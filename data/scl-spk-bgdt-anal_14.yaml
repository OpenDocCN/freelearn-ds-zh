- en: Time to Put Some Order - Cluster Your Data with Spark MLlib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 是时候整理一下 - 使用 Spark MLlib 聚类你的数据
- en: '*"If you take a galaxy and try to make it bigger, it becomes a cluster of galaxies,
    not a galaxy. If you try to make it smaller than that, it seems to blow itself
    apart"*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*“如果你试图把一个星系做大，它就变成了一个星系团，而不是一个星系。如果你试图让它变得比这个还小，它似乎会自我分裂。”*'
- en: '- Jeremiah P. Ostriker'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 杰里迈·P·奥斯特里克'
- en: 'In this chapter, we will delve deeper into machine learning and find out how
    we can take advantage of it to cluster records belonging to a certain group or
    class for a dataset of unsupervised observations. In a nutshell, the following
    topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨机器学习，并了解如何利用它将属于某个特定组或类别的记录聚类到无监督观察数据集中的方式。简而言之，本章将涵盖以下主题：
- en: Unsupervised learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Clustering techniques
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类技术
- en: Hierarchical clustering (HC)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类（HC）
- en: Centroid-based clustering (CC)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于质心的聚类（CC）
- en: Distribution-based clustering (DC)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于分布的聚类（DC）
- en: Determining number of clusters
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定聚类数目
- en: A comparative analysis between clustering algorithms
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类算法的比较分析
- en: Submitting jobs on computing clusters
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提交计算集群上的任务
- en: Unsupervised learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In this section, we will provide a brief introduction to unsupervised machine
    learning technique with appropriate examples. Let's start the discussion with
    a practical example. Suppose you have a large collection of not-pirated-totally-legal
    mp3s in a crowded and massive folder on your hard drive. Now, what if you can
    build a predictive model that helps automatically group together similar songs
    and organize them into your favorite categories such as country, rap, rock, and
    so on. This act of assigning an item to a group such that a mp3 to is added to
    the respective playlist in an unsupervised way. In the previous chapters, we assumed
    you're given a training dataset of correctly labeled data. Unfortunately, we don't
    always have that extravagance when we collect data in the real-world. For example,
    suppose we would like to divide up a large amount of music into interesting playlists.
    How could we possibly group together songs if we don't have direct access to their
    metadata? One possible approach could be a mixture of various machine learning
    techniques, but clustering is often at the heart of the solution.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要介绍无监督机器学习技术，并提供适当的示例。让我们从一个实际例子开始讨论。假设你在硬盘上的一个拥挤且庞大的文件夹里有大量未被盗版的、完全合法的
    mp3 文件。现在，如果你能够构建一个预测模型，帮助自动将相似的歌曲分组，并将它们组织到你最喜欢的类别中，比如乡村、说唱、摇滚等等，那该多好。这个将项目分配到某一组的行为，就像是将
    mp3 文件添加到相应的播放列表中，是一种无监督的方式。在前几章中，我们假设你得到了一个标注正确的训练数据集。然而，现实世界中，我们并不总是能够拥有这样的奢侈。例如，假设我们想要将大量音乐分成有趣的播放列表。如果我们无法直接访问它们的元数据，那我们如何可能将这些歌曲分组呢？一种可能的方法是将多种机器学习技术结合使用，但聚类通常是解决方案的核心。
- en: In short, iIn unsupervised machine learning problem, correct classes of the
    training dataset are not available or unknown. Thus, classes have to be deduced
    from the structured or unstructured datasets as shown in *Figure 1*. This essentially
    implies that the goal of this type of algorithm is to preprocess the data in some
    structured ways. In other words, the main objective of the unsupervised learning
    algorithms is to explore the unknown/hidden patterns in the input data that are
    unlabeled*.* Unsupervised learning, however, also comprehends other techniques
    to explain the key features of the data in an exploratory way toward finding the
    hidden patterns. To overcome this challenge, clustering techniques are used widely
    to group unlabeled data points based on certain similarity measures in an unsupervised
    way.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在无监督机器学习问题中，训练数据集的正确类别是不可用或未知的。因此，类别必须从结构化或非结构化数据集中推导出来，如*图 1*所示。这本质上意味着，这种类型的算法的目标是以某种结构化方式对数据进行预处理。换句话说，无监督学习算法的主要目标是探索输入数据中未标注的*隐藏模式*。然而，无监督学习还包括其他技术，以探索数据的关键特征，从而找到隐藏的模式。为了解决这一挑战，聚类技术被广泛应用于根据某些相似性度量以无监督的方式对未标注的数据点进行分组。
- en: 'For an in-depth theoretical knowledge of how unsupervised algorithms work,
    please refer to the following three books: *Bousquet*, *O.; von Luxburg*, *U.;
    Raetsch*, *G., eds* (2004). *Advanced Lectures on Machine Learning*. *Springer-Verlag*.
    ISBN 978-3540231226\. Or *Duda*, *Richard O.*; *Hart*, *Peter E.*; *Stork*, *David
    G*. (2001). *Unsupervised Learning and Clustering*. *Pattern classification* (2nd
    Ed.). *Wiley*. ISBN 0-471-05669-3 and *Jordan*, *Michael I.*; *Bishop*, *Christopher
    M*. (2004) *Neural Networks*. In *Allen B. Tucker* *Computer Science Handbook,
    Second Edition* (Section VII: Intelligent Systems). *Boca Raton*, FL: Chapman
    and Hall/CRC Press LLC. ISBN 1-58488-360-X.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解无监督算法的理论知识，请参考以下三本书：*Bousquet*, *O.; von Luxburg*, *U.; Raetsch*, *G.,
    eds*（2004）。*高级机器学习讲座*。*Springer-Verlag*。ISBN 978-3540231226，或 *Duda*, *Richard
    O.*；*Hart*, *Peter E.*；*Stork*, *David G*。（2001）。*无监督学习与聚类*。*模式分类*（第2版）。*Wiley*。ISBN
    0-471-05669-3 和 *Jordan*, *Michael I.*；*Bishop*, *Christopher M*。（2004）*神经网络*。收录于
    *Allen B. Tucker* *计算机科学手册，第2版*（第七部分：智能系统）。*佛罗里达州博卡拉顿*：Chapman and Hall/CRC Press
    LLC。ISBN 1-58488-360-X。
- en: '![](img/00263.jpeg)**Figure 1:** Unsupervised learning with Spark'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00263.jpeg)**图 1：** 使用 Spark 的无监督学习'
- en: Unsupervised learning example
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习示例
- en: 'In clustering tasks, an algorithm groups related features into categories by
    analyzing similarities between input examples where similar features are clustered
    and marked using circles around. Clustering uses include but are not limited to
    the following: search result grouping such as grouping customers, anomaly detection
    for suspicious pattern finding, text categorization for finding useful pattern
    in tests, social network analysis for finding coherent groups, data center computing
    clusters for finding a way to put related computers together, astronomic data
    analysis for galaxy formation, and real estate data analysis to identify neighborhoods
    based on similar features. We will show a Spark MLlib-based solution for the last
    use cases.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类任务中，算法通过分析输入示例之间的相似性，将相关特征分组成类别，其中相似的特征被聚集并用圆圈标出。聚类的应用包括但不限于以下几个方面：搜索结果分组，如客户分组，异常检测用于发现可疑模式，文本分类用于在文本中发现有用模式，社交网络分析用于发现一致的群体，数据中心计算机集群用于将相关计算机组合在一起，天文数据分析用于银河系形成，房地产数据分析用于根据相似特征识别邻里。我们将展示一个基于
    Spark MLlib 的解决方案，适用于最后一个用例。
- en: Clustering techniques
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类技术
- en: In this section, we will discuss clustering techniques along with challenges
    and suitable examples. A brief overview of hierarchical clustering, centroid-based
    clustering, and distribution-based clustering will be provided too.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论聚类技术、挑战以及适用的示例。还将简要概述层次聚类、基于质心的聚类和基于分布的聚类。
- en: Unsupervised learning and the clustering
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习与聚类
- en: Clustering analysis is about dividing data samples or data points and putting
    them into corresponding homogeneous classes or clusters. Thus a trivial definition
    of clustering can be thought as the process of organizing objects into groups
    whose members are similar in some way.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析是将数据样本或数据点划分并放入相应的同质类或聚类中的过程。因此，聚类的一个简单定义可以被认为是将对象组织成在某种方式上相似的组。
- en: A *cluster* is, therefore, a collection of objects that are *similar* between
    them and are *dissimilar* to the objects belonging to other clusters. As shown
    in *Figure 2*, if a collection of objects is given, clustering algorithms put
    those objects into a group based on similarity. A clustering algorithm such as
    K-means has then located the centroid of the group of data points. However, to
    make the clustering accurate and effective, the algorithm evaluates the distance
    between each point from the centroid of the cluster. Eventually, the goal of clustering
    is to determine the intrinsic grouping in a set of unlabeled data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个*聚类*是一个对象集合，这些对象在彼此之间是*相似的*，而与属于其他聚类的对象是*不同的*。如*图 2*所示，如果给定一组对象，聚类算法会根据相似性将这些对象分组。像
    K-means 这样的聚类算法会定位数据点组的质心。然而，为了使聚类更加准确有效，算法需要评估每个点与聚类质心之间的距离。最终，聚类的目标是确定一组未标记数据中的内在分组。
- en: '![](img/00059.jpeg)**Figure 2:** Clustering raw data'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00059.jpeg)**图 2：** 聚类原始数据'
- en: Spark supports many clustering algorithms such as **K-means**, **Gaussian mixture**,
    **power iteration clustering** (**PIC**), l**atent dirichlet allocation** (**LDA**),
    **bisecting K-means**, and **Streaming K-means**. LDA is used for document classification
    and clustering commonly used in text mining. PIC is used for clustering vertices
    of a graph consisting of pairwise similarities as edge properties. However, to
    keep the objective of this chapter clearer and focused, we will confine our discussion
    to the K-means, bisecting K-means, and Gaussian mixture algorithms.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Spark支持许多聚类算法，如**K-means**、**高斯混合**、**幂迭代聚类**（**PIC**）、**潜在狄利克雷分配**（**LDA**）、**二分K-means**和**流式K-means**。LDA常用于文档分类和聚类，广泛应用于文本挖掘。PIC用于聚类图的顶点，该图的边属性由成对相似度表示。然而，为了让本章的目标更加清晰和集中，我们将仅讨论K-means、二分K-means和高斯混合算法。
- en: Hierarchical clustering
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类
- en: The hierarchical clustering technique is based on the fundamental idea of objects
    or features that are more related to those nearby than others far away. Bisecting
    K-means is an example of such hierarchical clustering algorithm that connects
    data objects to form clusters based on their corresponding distance.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类技术基于这样一个基本思想：对象或特征与附近的对象或特征相比，更相关，而与远离的对象或特征的相关性较低。二分K-means是这样一种层次聚类算法的例子，它根据数据对象之间的对应距离将数据对象连接成簇。
- en: In the hierarchical clustering technique, a cluster can be described trivially
    by the maximum distance needed to connect parts of the cluster. As a result, different
    clusters will be formed at different distances. Graphically, these clusters can
    be represented using a dendrogram. Interestingly, the common name hierarchical
    clustering evolves from the concept of the dendrogram.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在层次聚类技术中，聚类可以通过连接聚类各部分所需的最大距离来简单地描述。因此，不同的聚类会在不同的距离下形成。从图形上看，这些聚类可以使用树状图表示。有趣的是，层次聚类这一常见名称来源于树状图的概念。
- en: Centroid-based clustering
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于中心的聚类
- en: In centroid-based clustering technique, clusters are represented by a central
    vector. However, the vector itself may not necessarily be a member of the data
    points. In this type of learning, a number of the probable clusters must be provided
    prior to training the model. K-means is a very famous example of this learning
    type, where, if you set the number of clusters to a fixed integer to say K, the
    K-means algorithm provides a formal definition as an optimization problem, which
    is a separate problem to be resolved to find the K cluster centers and assign
    the data objects the nearest cluster center. In short, this is an optimization
    problem where the objective is to minimize the squared distances from the clusters.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于中心的聚类技术中，聚类通过一个中心向量来表示。然而，这个向量本身不一定是数据点的成员。在这种类型的学习中，必须在训练模型之前提供一个预设的聚类数目。K-means是这种学习类型的一个非常著名的例子，其中，如果你将聚类数目设置为一个固定的整数K，K-means算法就会将其定义为一个优化问题，这是一个独立的问题，用于找到K个聚类中心，并将数据对象分配到距离它们最近的聚类中心。简而言之，这是一个优化问题，目标是最小化聚类间的平方距离。
- en: Distribution-based clustestering
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于分布的聚类
- en: Distribution-based clustering algorithms are based on statistical distribution
    models that provide more convenient ways to cluster related data objects to the
    same distribution. Although the theoretical foundations of these algorithms are
    very robust, they mostly suffer from overfitting. However, this limitation can
    be overcome by putting constraints on the model complexity.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 基于分布的聚类算法是基于统计分布模型的，这些模型提供了更便捷的方式，将相关的数据对象聚类到相同的分布中。尽管这些算法的理论基础非常稳健，但它们大多存在过拟合的问题。然而，通过对模型复杂度施加约束，可以克服这一限制。
- en: Centroid-based clustering (CC)
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于中心的聚类（CC）
- en: In this section, we discuss the centroid-based clustering technique and its
    computational challenges. An example of using K-means with Spark MLlib will be
    shown for a better understanding of the centroid-based clustering.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将讨论基于中心的聚类技术及其计算挑战。将通过使用Spark MLlib的K-means示例，帮助更好地理解基于中心的聚类。
- en: Challenges in CC algorithm
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CC算法中的挑战
- en: As discussed previously, in a centroid-based clustering algorithm like K-means,
    setting the optimal value of the number of clusters K is an optimization problem.
    This problem can be described as NP-hard (that is non-deterministic polynomial-time
    hard) featuring high algorithmic complexities, and thus the common approach is
    trying to achieve only an approximate solution. Consequently, solving these optimization
    problems imposes an extra burden and consequently nontrivial drawbacks. Furthermore,
    the K-means algorithm expects that each cluster has approximately similar size.
    In other words, data points in each cluster have to be uniform to get better clustering
    performance.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在像 K-means 这样的基于质心的聚类算法中，设定聚类数 K 的最优值是一个优化问题。这个问题可以被描述为 NP-hard（即非确定性多项式时间困难），具有较高的算法复杂性，因此常见的方法是尝试仅得到一个近似解。因此，解决这些优化问题会增加额外的负担，并因此带来不容忽视的缺点。此外，K-means
    算法假设每个聚类的大小大致相同。换句话说，为了获得更好的聚类效果，每个聚类中的数据点必须是均匀的。
- en: Another major drawback of this algorithm is that this algorithm tries to optimize
    the cluster centers but not cluster borders, and this often tends to inappropriately
    cut the borders in between the clusters. However, sometimes, we can have the advantage
    of visual inspection, which is often not available for data on hyperplanes or
    multidimensional data. Nonetheless, a complete section on how to find the optimal
    value of K will be discussed later in this chapter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的另一个主要缺点是，它试图优化聚类中心而不是聚类边界，这常常会导致错误地切割聚类之间的边界。然而，有时我们可以通过视觉检查来弥补这一点，但这通常不适用于超平面上的数据或多维数据。尽管如此，关于如何找到
    K 的最优值的完整内容将在本章后面讨论。
- en: How does K-means algorithm work?
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means 算法是如何工作的？
- en: 'Suppose we have *n* data points *x[i]*, *i=1...n* that need to be partitioned
    into *k* clusters. Now that the target here is to assign a cluster to each data
    point. K-means then aims to find the positions *μ[i],i=1...k* of the clusters
    that minimize the distance from the data points to the cluster. Mathematically,
    the K-means algorithm tries to achieve the goal by solving the following equation,
    that is, an optimization problem:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有 *n* 个数据点 *x[i]*，*i=1...n*，需要将它们划分为 *k* 个聚类。现在目标是为每个数据点分配一个聚类。K-means 算法的目标是通过求解以下方程来找到聚类的位置
    *μ[i],i=1...k*，以最小化数据点到聚类的距离。数学上，K-means 算法通过解决以下优化问题来实现这一目标：
- en: '![](img/00320.jpeg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00320.jpeg)'
- en: In the preceding equation, *c[i]* is the set of data points assigned to cluster
    *i*, and *d(x,μ[i]) =||x−μ[i]||²[2]* is the Euclidean distance to be calculated
    (we will explain why we should use this distance measurement shortly). Therefore,
    we can understand that the overall clustering operation using K-means is not a
    trivial one but an NP-hard optimization problem. This also means that the K-means
    algorithm not only tries to find the global minima but also often gets stuck in
    different solutions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，*c[i]* 是分配给聚类 *i* 的数据点集合，*d(x,μ[i]) =||x−μ[i]||²[2]* 是要计算的欧几里得距离（我们稍后会解释为什么要使用这种距离度量）。因此，我们可以理解，使用
    K-means 进行的整体聚类操作并非一个简单的问题，而是一个 NP-hard 优化问题。这也意味着 K-means 算法不仅仅是寻找全局最小值，还经常会陷入不同的局部解。
- en: 'Now, let''s see how we could formulate the algorithm before we can feed the
    data to the K-means model. At first, we need to decide the number of tentative
    clusters, *k* priory. Then, typically, you need to follow these steps:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看在将数据输入 K-means 模型之前，我们如何能制定算法。首先，我们需要预先决定聚类数 *k*。然后，通常你需要遵循以下步骤：
- en: '![](img/00367.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00367.jpeg)'
- en: Here *|c|* is the number of elements in *c*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 *|c|* 表示 *c* 中元素的数量。
- en: Clustering using the K-means algorithm begins by initializing all the coordinates
    to centroids. With every pass of the algorithm, each point is assigned to its
    nearest centroid based on some distance metric, usually *Euclidean distance*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 K-means 算法进行聚类时，首先将所有坐标初始化为质心。随着算法的每次迭代，每个点会根据某种距离度量（通常是*欧几里得距离*）分配到离它最近的质心。
- en: '**Distance calculation:** Note that there are other ways to calculate the distance
    too, for example:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**距离计算：** 请注意，还有其他方法可以计算距离，例如：'
- en: '*Chebyshev distance* can be used to measure the distance by considering only
    the most notable dimensions.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*切比雪夫距离* 可以用来通过只考虑最显著的维度来度量距离。'
- en: The *Hamming distance* algorithm can identify the difference between two strings.
    On the other hand, to make the distance metric scale-undeviating, *Mahalanobis
    distance* can be used to normalize the covariance matrix. The *Manhattan distance*
    is used to measure the distance by considering only axis-aligned directions. The
    *Minkowski distance* algorithm is used to make the Euclidean distance, Manhattan
    distance, and Chebyshev distance. The *Haversine distance* is used to measure
    the great-circle distances between two points on a sphere from the location, that
    is, longitudes and latitudes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*哈明距离*算法可以识别两个字符串之间的差异。另一方面，为了使距离度量具有尺度不变性，可以使用*马氏距离*来标准化协方差矩阵。*曼哈顿距离*用于通过仅考虑轴对齐的方向来衡量距离。*闵可夫斯基距离*算法用于统一欧几里得距离、曼哈顿距离和切比雪夫距离。*哈弗辛距离*用于测量球面上两点之间的大圆距离，也就是经度和纬度之间的距离。'
- en: 'Considering these distance-measuring algorithms, it is clear that the Euclidean
    distance algorithm would be the most appropriate to solve our purpose of distance
    calculation in the K-means algorithm. The centroids are then updated to be the
    centers of all the points assigned to it in that iteration. This repeats until
    there is a minimal change in the centers. In short, the K-means algorithm is an
    iterative algorithm and works in two steps:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些距离测量算法，可以清楚地看出，欧几里得距离算法将是解决K-means算法中距离计算问题的最合适选择。接着，质心将被更新为该次迭代中分配给它的所有点的中心。这一过程将重复，直到质心变化最小。简而言之，K-means算法是一个迭代算法，分为两个步骤：
- en: '**Cluster assignment step**: K-means goes through each of the m data points
    in the dataset which is assigned to a cluster that is represented by the closest
    of the k centroids. For each point, the distances to each centroid is then calculated
    and simply pick the least distant one.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类分配步骤**：K-means算法会遍历数据集中的每一个m个数据点，并将其分配到最接近的k个质心所代表的聚类中。对于每个点，计算它到每个质心的距离，并简单地选择距离最小的一个。'
- en: '**Update step**: For each cluster, a new centroid is calculated as the mean
    of all points in the cluster. From the previous step, we have a set of points
    which are assigned to a cluster. Now, for each such set, we calculate a mean that
    we declare a new centroid of the cluster.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新步骤**：对于每个聚类，计算一个新的质心，该质心是该聚类中所有点的均值。从前一步骤中，我们得到了一个已分配到聚类中的点集。现在，对于每一个这样的点集，我们计算均值，并将其声明为新的聚类质心。'
- en: An example of clustering using K-means of Spark MLlib
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark MLlib的K-means聚类示例
- en: 'To further demonstrate the clustering example, we will use the *Saratoga NY
    Homes* dataset downloaded from [http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html](http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html)
    as an unsupervised learning technique using Spark MLlib. The dataset contains
    several features of houses located in the suburb of the New York City. For example,
    price, lot size, waterfront, age, land value, new construct, central air, fuel
    type, heat type, sewer type, living area, pct.college, bedrooms, fireplaces, bathrooms,
    and the number of rooms. However, only a few features have been shown in the following
    table:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步展示聚类的例子，我们将使用从[Saratoga NY Homes数据集](http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html)下载的*萨拉托加纽约住宅*数据集，采用Spark
    MLlib进行无监督学习技术。该数据集包含了位于纽约市郊区的多栋住宅的若干特征。例如，价格、地块大小、临水、建筑年龄、土地价值、新建、中央空调、燃料类型、供暖类型、排污类型、居住面积、大学毕业率、卧室数量、壁炉数量、浴室数量以及房间数量。然而，以下表格中仅展示了部分特征：
- en: '| **Price** | **Lot Size** | **Water Front** | **Age** | **Land Value** | **Rooms**
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| **价格** | **地块大小** | **临水** | **建筑年龄** | **土地价值** | **房间数** |'
- en: '| 132,500 | 0.09 | 0 | 42 | 5,000 | 5 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 132,500 | 0.09 | 0 | 42 | 5,000 | 5 |'
- en: '| 181,115 | 0.92 | 0 | 0 | 22,300 | 6 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 181,115 | 0.92 | 0 | 0 | 22,300 | 6 |'
- en: '| 109,000 | 0.19 | 0 | 133 | 7,300 | 8 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 109,000 | 0.19 | 0 | 133 | 7,300 | 8 |'
- en: '| 155,000 | 0.41 | 0 | 13 | 18,700 | 5 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 155,000 | 0.41 | 0 | 13 | 18,700 | 5 |'
- en: '| 86,060 | 0.11 | 0 | 0 | 15,000 | 3 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 86,060 | 0.11 | 0 | 0 | 15,000 | 3 |'
- en: '| 120,000 | 0.68 | 0 | 31 | 14,000 | 8 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 120,000 | 0.68 | 0 | 31 | 14,000 | 8 |'
- en: '| 153,000 | 0.4 | 0 | 33 | 23,300 | 8 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 153,000 | 0.4 | 0 | 33 | 23,300 | 8 |'
- en: '| 170,000 | 1.21 | 0 | 23 | 146,000 | 9 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 170,000 | 1.21 | 0 | 23 | 146,000 | 9 |'
- en: '| 90,000 | 0.83 | 0 | 36 | 222,000 | 8 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 90,000 | 0.83 | 0 | 36 | 222,000 | 8 |'
- en: '| 122,900 | 1.94 | 0 | 4 | 212,000 | 6 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 122,900 | 1.94 | 0 | 4 | 212,000 | 6 |'
- en: '| 325,000 | 2.29 | 0 | 123 | 126,000 | 12 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 325,000 | 2.29 | 0 | 123 | 126,000 | 12 |'
- en: '**Table 1:** Sample data from the Saratoga NY Homes dataset'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**表1：** 来自萨拉托加纽约住宅数据集的示例数据'
- en: The target of this clustering technique here is to show an exploratory analysis
    based on the features of each house in the city for finding possible neighborhoods
    for the house located in the same area. Before performing feature extraction,
    we need to load and parse the Saratoga NY Homes dataset. This step also includes
    loading packages and related dependencies, reading the dataset as RDD, model training,
    prediction, collecting the local parsed data, and clustering comparing.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 该聚类技术的目标是基于每个房屋的特征，进行探索性分析，寻找可能的邻里区域，以便为位于同一地区的房屋找到潜在的邻居。在进行特征提取之前，我们需要加载并解析萨拉托加
    NY 房屋数据集。此步骤还包括加载包和相关依赖项，读取数据集作为RDD，模型训练、预测、收集本地解析数据以及聚类比较。
- en: '**Step 1**. Import-related packages:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1**. 导入相关包：'
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Step 2\. Create a Spark session - the entry point** - Here we at first set
    the Spark configuration by setting the application name and master URL. For simplicity,
    it''s standalone with all the cores on your machine:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2. 创建Spark会话 - 入口点** - 这里我们首先通过设置应用程序名称和主机URL来配置Spark。为了简化起见，它是独立运行，并使用您机器上的所有核心：'
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Step 3\. Load and parse the dataset** - Read, parse, and create RDDs from
    the dataset as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3. 加载和解析数据集** - 读取、解析并从数据集中创建RDD，如下所示：'
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that, to make the preceding code work, you should import the following
    package:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了使前面的代码正常工作，您应该导入以下包：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You will get the following output:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到如下输出：
- en: '![](img/00339.jpeg)**Figure 3:** A snapshot of the Saratoga NY Homes dataset'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00339.jpeg)**图 3：**萨拉托加 NY 房屋数据集快照'
- en: 'The following is the `parseLand` method that is used to create a `Land` class
    from an array of `Double` as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`parseLand`方法，用于从一个`Double`数组创建一个`Land`类，如下所示：
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And the `Land` class that reads all the features as a double is as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 读取所有特征为double类型的`Land`类如下所示：
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As you already know, to train the K-means model, we need to ensure all the
    data points and features to be numeric. Therefore, we further need to convert
    all the data points to double as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所知，训练K-means模型时，我们需要确保所有数据点和特征都是数值类型。因此，我们还需要将所有数据点转换为double类型，如下所示：
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Step 4\. Preparing the training set** - At first, we need to convert the
    data frame (that is, `landDF`) to an RDD of doubles and cache the data to create
    a new data frame to link the cluster numbers as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 4. 准备训练集** - 首先，我们需要将数据框（即`landDF`）转换为一个包含double类型数据的RDD，并缓存数据，以创建一个新的数据框来链接集群编号，如下所示：'
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now that we need to convert the preceding RDD of doubles into an RDD of dense
    vectors as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要将前面的RDD（包含double类型数据）转换为一个包含稠密向量的RDD，如下所示：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Step 5\. Train the K-means model** - Train the model by specifying 10 clusters,
    20 iterations, and 10 runs as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 5. 训练K-means模型** - 通过指定10个集群、20次迭代和10次运行来训练模型，如下所示：'
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The Spark-based implementation of K-means starts working by initializing a set
    of cluster centers using the K-means algorithm by *Bahmani et al.*, *Scalable
    K-Means++*, VLDB 2012\. This is a variant of K-means++ that tries to find dissimilar
    cluster centers by starting with a random center and then doing passes where more
    centers are chosen with a probability proportional to their squared distance to
    the current cluster set. It results in a provable approximation to an optimal
    clustering. The original paper can be found at [http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf](http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Spark的K-means实现通过使用*K-means++*算法初始化一组集群中心开始工作， *Bahmani等人*提出的*K-means++*，VLDB
    2012。这是K-means++的一种变体，试图通过从一个随机中心开始，然后进行多次选择，通过一个概率方法选择更多的中心，概率与它们到当前集群集合的平方距离成正比。它产生了一个可证明的接近最优聚类的结果。原始论文可以在[http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf](http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf)找到。
- en: '**Step 6\. Evaluate the model error rate** - The standard K-means algorithm
    aims at minimizing the sum of squares of the distance between the points of each
    set, that is, the squared Euclidean distance, which is the WSSSE''s objective.
    The K-means algorithm aims at minimizing the sum of squares of the distance between
    the points of each set (that is, the cluster center). However, if you really wanted
    to minimize the sum of squares of the distance between the points of each set,
    you would end up with a model where each cluster is its own cluster center; in
    that case, that measure would be 0.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤6：评估模型误差率** - 标准的K-means算法旨在最小化每组数据点之间的距离平方和，即平方欧几里得距离，这也是WSSSE的目标。K-means算法旨在最小化每组数据点（即聚类中心）之间的距离平方和。然而，如果你真的想最小化每组数据点之间的距离平方和，你最终会得到一个模型，其中每个聚类都是自己的聚类中心；在这种情况下，那个度量值将是0。'
- en: 'Therefore, once you have trained your model by specifying the parameters, you
    can evaluate the result by using **Within Set Sum of Squared Errors** (**WSSE**).
    Technically, it is something like the sum of the distances of each observation
    in each K cluster that can be computed as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一旦你通过指定参数训练了模型，你可以使用**集合内平方误差和**（**WSSE**）来评估结果。从技术上讲，它就像是计算每个K个聚类中每个观察值的距离总和，计算公式如下：
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The preceding model training set produces the value of WCSSS:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的模型训练集产生了WCSSS的值：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Step 7\. Compute and print the cluster centers** - At first, we get the prediction
    from the model with the ID so that we can link them back to other information
    related to each house. Note that we will use an RDD of rows that we prepared in
    step 4*:*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤7：计算并打印聚类中心** - 首先，我们从模型中获取预测结果和ID，以便可以将其与每个房子相关的其他信息进行关联。请注意，我们将使用在步骤4中准备的RDD行：'
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'However, it should be provided when a prediction is requested about the price.
    This should be done as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在请求有关价格的预测时，应该提供该数据。可以按照如下方式操作：
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'For better visibility and an exploratory analysis, convert the RDD to a DataFrame
    as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好的可视化和探索性分析，可以将RDD转换为DataFrame，代码如下：
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This should produce the output shown in the following figure:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成如下图所示的输出结果：
- en: '![](img/00044.gif)**Figure 4:** A snapshot of the clusters predicted'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00044.gif)**图4：** 聚类预测的快照'
- en: 'Since there''s no distinguishable ID in the dataset, we represented the `Price`
    field to make the linking. From the preceding figure, you can understand where
    does a house having a certain price falls, that is, in which cluster. Now for
    better visibility, let''s join the prediction DataFrame with the original DataFrame
    to know the individual cluster number for each house:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集中没有可区分的ID，我们使用`Price`字段来进行关联。从前面的图中，你可以了解某个价格的房子属于哪个聚类，即属于哪个簇。为了更好的可视化效果，我们将预测的DataFrame与原始的DataFrame进行合并，以便知道每个房子对应的具体聚类编号：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You should observe the output in the following figure:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该在下图中观察到输出结果：
- en: '![](img/00138.jpeg)**Figure 5:** A snapshot of the clusters predicted across
    each house'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00138.jpeg)**图5：** 每个房子预测的聚类快照'
- en: To make the analysis, we dumped the output in RStudio and generated the clusters
    shown in *Figure 6*. The R script can be found on my GitHub repositories at [https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics](https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics).
    Alternatively, you can write your own script and do the visualization accordingly.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行分析，我们将输出数据导入到RStudio中，并生成了如*图6*所示的聚类。R脚本可以在我的GitHub仓库中找到，网址是[https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics](https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics)。另外，你也可以编写自己的脚本并据此进行可视化。
- en: '![](img/00142.jpeg)**Figure 6:** Clusters of the neighborhoods'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00142.jpeg)**图6：** 社区的聚类'
- en: 'Now, for more extensive analysis and visibility, we can observe related statistics
    for each cluster. For example, below I printed thestatistics related to cluster
    3 and 4 in *Figure 8* and *Figure 9*, respectively:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了进行更广泛的分析和可视化，我们可以观察每个聚类的相关统计数据。例如，下面我打印了与聚类3和4相关的统计数据，分别在*图8*和*图9*中展示：
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now get the descriptive statistics for each cluster as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在获取每个聚类的描述性统计数据，见下：
- en: '[PRE17]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'At first, let''s observe the related statistics of cluster 3 in the following
    figure:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们观察聚类3的相关统计数据，见下图：
- en: '![](img/00353.jpeg)**Figure 7:** Statistics on cluster 3'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00353.jpeg)**图7：** 聚类3的统计数据'
- en: 'Now let''s observe the related statistics of cluster 4 in the following figure:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们观察聚类4的相关统计数据，见下图：
- en: '![](img/00377.jpeg)**Figure 8:** Statistics on cluster 4'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00377.jpeg)**图8：** 聚类4的统计数据'
- en: Note that, since the original screenshot was too large to fit in this page,
    the original images were modified and the column containing other variables of
    the houses were removed.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于原始截图太大，无法适应本页，因此原始图像已被修改，并且删除了包含其他房屋变量的列。
- en: 'Due to the random nature of this algorithm, you might receive different results
    for each successful iteration. However, you can lock the random nature of this
    algorithm by setting the seed as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该算法的随机性，每次成功迭代时可能会得到不同的结果。然而，您可以通过以下方法锁定该算法的随机性：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Step 8\. Stop the Spark session** - Finally, stop the Spark session using
    the stop method as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**第8步：停止Spark会话** - 最后，使用stop方法停止Spark会话，如下所示：'
- en: '[PRE19]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the preceding example, we dealt with a very small set of features; common-sense
    and visual inspection would also lead us to the same conclusions. From the above
    example using the K-means algorithm, we can understand that there are some limitations
    for this algorithm. For example, it's really difficult to predict the K-value,
    and with a global cluster it does not work well. Moreover, different initial partitions
    can result in different final clusters, and, finally, it does not work well with
    clusters of different sizes and densities.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们处理了一个非常小的特征集；常识和目视检查也会得出相同的结论。从上面的K-means算法示例中，我们可以理解该算法存在一些局限性。例如，很难预测K值，且全局簇表现不佳。此外，不同的初始分区可能会导致不同的最终簇，最后，它对不同大小和密度的簇表现不佳。
- en: 'To overcome these limitations, we have some more robust algorithms in this
    book like MCMC (Markov Chain Monte Carlo; see also at [https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo))
    presented in the book: *Tribble*, *Seth D.*, **Markov chain Monte Carlo** algorithms
    using completely uniformly distributed driving sequences, Diss. Stanford University,
    2007.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些局限性，本书中介绍了一些更强大的算法，如MCMC（马尔可夫链蒙特卡洛；参见[https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)）在书中呈现：*Tribble*,
    *Seth D.*, **马尔可夫链蒙特卡洛**算法使用完全均匀分布的驱动序列，斯坦福大学博士论文，2007年。
- en: Hierarchical clustering (HC)
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类（HC）
- en: In this section, we discuss the hierarchical clustering technique and its computational
    challenges. An example of using the bisecting K-means algorithm of hierarchical
    clustering with Spark MLlib will be shown too for a better understanding of hierarchical
    clustering.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论层次聚类技术及其计算挑战。还将展示一个使用Spark MLlib的层次聚类的双分K-means算法示例，以更好地理解层次聚类。
- en: An overview of HC algorithm and challenges
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类算法概述及挑战
- en: 'A hierarchical clustering technique is computationally different from the centroid-based
    clustering in the way the distances are computed. This is one of the most popular
    and widely used clustering analysis technique that looks to build a hierarchy
    of clusters. Since a cluster usually consists of multiple objects, there will
    be other candidates to compute the distance too. Therefore, with the exception
    of the usual choice of distance functions, you also need to decide on the linkage
    criterion to be used. In short, there are two types of strategies in hierarchical
    clustering:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类技术与基于质心的聚类在计算距离的方式上有所不同。这是最受欢迎和广泛使用的聚类分析技术之一，旨在构建一个簇的层次结构。由于一个簇通常包含多个对象，因此还会有其他候选项来计算距离。因此，除了通常选择的距离函数外，还需要决定使用的连接标准。简而言之，层次聚类中有两种策略：
- en: '**Bottom-up approach**: In this approach, each observation starts within its
    own cluster. After that, the pairs of clusters are merged together and one moves
    up the hierarchy.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自底向上方法**：在这种方法中，每个观察从其自身簇开始。之后，簇的对会合并在一起，然后向上移动到层次结构中。'
- en: '**Top-down approach**: In this approach, all observations start in one cluster,
    splits are performed recursively, and one moves down the hierarchy.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自顶向下方法**：在这种方法中，所有观察从一个簇开始，分裂是递归进行的，然后向下移动到层次结构中。'
- en: These bottom-up or top-down approaches are based on the s**ingle-linkage clustering**
    (**SLINK**) technique, which considers the minimum object distances, the **complete
    linkage clustering** (**CLINK**), which considers the maximum of object distances,
    and the u**nweighted pair group method with arithmetic mean** (**UPGMA**). The
    latter is also known as **average-linkage clustering**. Technically, these methods
    will not produce unique partitions out of the dataset (that is, different clusters).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这些自底向上或自顶向下的方法基于**单链聚类**（**SLINK**）技术，该技术考虑最小的对象距离；**完全链聚类**（**CLINK**），该方法考虑对象距离的最大值；以及**无权重配对组法平均法**（**UPGMA**）。后者也被称为**平均链聚类**。从技术上讲，这些方法不会从数据集中产生唯一的划分（即不同的簇）。
- en: A comparative analysis on these three approaches can be found at [https://nlp.stanford.edu/IR-book/completelink.html.](https://nlp.stanford.edu/IR-book/completelink.html)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对这三种方法的比较分析可以在[https://nlp.stanford.edu/IR-book/completelink.html.](https://nlp.stanford.edu/IR-book/completelink.html)找到。
- en: 'However, the user still needs to choose appropriate clusters from the hierarchy
    for better cluster prediction and assignment. Although algorithms of this class
    like bisecting K-means are computationally faster than the K-means algorithm,
    there are three disadvantages to this type of algorithm:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，用户仍然需要从层次结构中选择合适的簇，以获得更好的聚类预测和分配。虽然这一类算法（如二分 K-means）在计算上比 K-means 算法更快，但这种类型的算法也有三个缺点：
- en: First, these methods are not very robust toward outliers or datasets containing
    noise or missing values. This disadvantage either imposes additional clusters
    or even causes other clusters to merge. This problem is commonly referred to as
    the chaining phenomenon, especially for single-linkage clustering.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，这些方法对于异常值或包含噪声或缺失值的数据集并不是非常稳健。这个缺点会导致附加的簇，甚至可能导致其他簇合并。这个问题通常被称为链式现象，尤其在单链聚类（single-linkage
    clustering）中比较常见。
- en: Second, from the algorithmic analysis, the complexity is for agglomerative clustering
    and for divisive clustering, which makes them too slow for large data sets.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，从算法分析来看，聚合型聚类和分裂型聚类的复杂度较高，这使得它们对于大数据集来说过于缓慢。
- en: Third, SLINK and CLINK were previously used widely in data mining tasks as theoretical
    foundations of cluster analysis, but nowadays they are considered obsolete.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三，SLINK 和 CLINK 曾经在数据挖掘任务中广泛使用，作为聚类分析的理论基础，但如今它们被认为是过时的。
- en: Bisecting K-means with Spark MLlib
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spark MLlib 实现二分 K-means
- en: Bisecting K-means can often be much faster than regular K-means, but it will
    generally produce a different clustering. A bisecting K-means algorithm is based
    on the paper, *A comparison of document clustering* techniques by *Steinbach*,
    *Karypis*, and *Kumar*, with modification to fit with Spark MLlib.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 二分 K-means 通常比常规 K-means 更快，但它通常会产生不同的聚类结果。二分 K-means 算法基于论文《*A comparison of
    document clustering*》中的方法，作者为 *Steinbach*、*Karypis* 和 *Kumar*，并经过修改以适应 Spark MLlib。
- en: Bisecting K-means is a kind of divisive algorithm that starts from a single
    cluster that contains all the data points. Iteratively, it then finds all the
    divisible clusters on the bottom level and bisects each of them using K-means
    until there are K leaf clusters in total or no leaf clusters divisible. After
    that, clusters on the same level are grouped together to increase the parallelism.
    In other words, bisecting K-means is computationally faster than the regular K-means
    algorithm. Note that if bisecting all the divisible clusters on the bottom level
    results in more than K leaf clusters, larger clusters will always get higher priority.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 二分 K-means 是一种分裂型算法，它从一个包含所有数据点的单一簇开始。然后，它迭代地找到底层所有可分的簇，并使用 K-means 对每个簇进行二分，直到总共有
    K 个叶子簇，或者没有可分的叶子簇为止。之后，同一层次的簇会被组合在一起，以增加并行性。换句话说，二分 K-means 在计算上比常规的 K-means 算法更快。需要注意的是，如果对底层所有可分簇进行二分后得到的叶子簇数量超过
    K，则较大的簇会优先被选择。
- en: 'Note that if the bisecting of all the divisible clusters on the bottom level
    results in more than K leaf clusters, larger clusters will always get higher priority.
    The following parameters are used in the Spark MLlib implementation:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果对底层所有可分簇进行二分后得到的叶子簇数量超过 K，则较大的簇会优先被选择。以下是 Spark MLlib 实现中使用的参数：
- en: '**K**: This is the desired number of leaf clusters. However, the actual number
    could be smaller if there are no divisible leaf clusters left during the computation.
    The default value is 4.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K**：这是期望的叶子聚类数量。然而，如果在计算过程中没有可分割的叶子聚类，实际数量可能会更少。默认值为 4。'
- en: '**MaxIterations**: This is the max number of K-means iterations to split the
    clusters. The default value is 20.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MaxIterations**：这是 K-means 算法中用于分割聚类的最大迭代次数。默认值为 20。'
- en: '**MinDivisibleClusterSize**: This is the minimum number of points. The default
    value is set as 1.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MinDivisibleClusterSize**：这是最小的点数。默认值为 1。'
- en: '**Seed**: This is a random seed that disallows random clustering and tries
    to provide almost similar result in each iteration. However, it is recommended
    to use a long seed value like 12345 and so on.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Seed**：这是一个随机种子，禁止随机聚类，并尽量在每次迭代中提供几乎相同的结果。然而，建议使用较长的种子值，如 12345 等。'
- en: Bisecting K-means clustering of the neighborhood using Spark MLlib
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spark MLlib 对邻里进行二分 K-means 聚类
- en: 'In the previous section, we saw how to cluster similar houses together to determine
    the neighborhood. The bisecting K-means is also similar to regular K-means except
    that the model training that takes different training parameters as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到如何将相似的房屋聚集在一起，以确定邻里。二分 K-means 算法与常规 K-means 算法类似，不同之处在于模型训练使用了不同的训练参数，如下所示：
- en: '[PRE20]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You should refer to the previous example and just reuse the previous steps
    to get the trained data. Now let''s evaluate clustering by computing WSSSE as
    follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该参考前面的示例并重新使用前面的步骤来获取训练数据。现在让我们通过计算 WSSSE 来评估聚类，方法如下：
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You should observe the following output: `Within-Cluster Sum of Squares = 2.096980212594632E11`.
    Now for more analysis, please refer to step 5 in the previous section.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该观察到以下输出：`Within-Cluster Sum of Squares = 2.096980212594632E11`。现在，若要进行进一步分析，请参阅上一节的第
    5 步。
- en: Distribution-based clustering (DC)
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于分布的聚类（DC）
- en: In this section, we will discuss the distribution-based clustering technique
    and its computational challenges. An example of using **Gaussian mixture models**
    (**GMMs**) with Spark MLlib will be shown for a better understanding of distribution-based
    clustering.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将讨论基于分布的聚类技术及其计算挑战。为了更好地理解基于分布的聚类，将展示一个使用**高斯混合模型**（**GMMs**）与 Spark
    MLlib 的示例。
- en: Challenges in DC algorithm
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DC 算法中的挑战
- en: A distribution-based clustering algorithm like GMM is an expectation-maximization
    algorithm. To avoid the overfitting problem, GMM usually models the dataset with
    a fixed number of Gaussian distributions. The distributions are initialized randomly,
    and the related parameters are iteratively optimized too to fit the model better
    to the training dataset. This is the most robust feature of GMM and helps the
    model to be converged toward the local optimum. However, multiple runs of this
    algorithm may produce different results.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 像 GMM 这样的基于分布的聚类算法是一种期望最大化算法。为了避免过拟合问题，GMM 通常使用固定数量的高斯分布来建模数据集。这些分布是随机初始化的，并且相关参数也会进行迭代优化，以便更好地将模型拟合到训练数据集。这是
    GMM 最强大的特点，有助于模型向局部最优解收敛。然而，算法的多次运行可能会产生不同的结果。
- en: In other words, unlike the bisecting K-means algorithm and soft clustering,
    GMM is optimized for hard clustering, and in order to obtain of that type, objects
    are often assigned to the Gaussian distribution. Another advantageous feature
    of GMM is that it produces complex models of clusters by capturing all the required
    correlations and dependence between data points and attributes.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，与二分 K-means 算法和软聚类不同，GMM 是针对硬聚类进行优化的，为了获得这种类型，通常会将对象分配到高斯分布中。GMM 的另一个优势是，它通过捕捉数据点和属性之间所需的所有相关性和依赖关系，生成复杂的聚类模型。
- en: 'On the down-side, GMM has some assumptions about the format and shape of the
    data, and this puts an extra burden on us (that is, users). More specifically,
    if the following two criteria do not meet, performance decreases drastically:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，GMM 对数据的格式和形状有一些假设，这就给我们（即用户）增加了额外的负担。更具体地说，如果以下两个标准不满足，性能会急剧下降：
- en: 'Non-Gaussian dataset: The GMM algorithm assumes that the dataset has an underlying
    Gaussian, which is generative distribution. However, many practical datasets do
    not satisfy this assumption that is subject to provide low clustering performance.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非高斯数据集：GMM 算法假设数据集具有潜在的高斯分布，这是生成性分布。然而，许多实际数据集不满足这一假设，可能导致较差的聚类性能。
- en: If the clusters do not have even sizes, there is a high chance that small clusters
    will be dominated by larger ones.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果聚类的大小不均，较小的聚类很可能会被较大的聚类所主导。
- en: How does a Gaussian mixture model work?
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯混合模型是如何工作的？
- en: 'Using GMM is a popular technique of soft clustering. GMM tries to model all
    the data points as a finite mixture of Gaussian distributions; the probability
    that each point belongs to each cluster is computed along with the cluster related
    statistics and represents an amalgamate distribution: where all the points are
    derived from one of *K* Gaussian subdistributions having own probability. In short,
    the functionality of GMM can be described in a three-steps pseudocode:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GMM 是一种流行的软聚类技术。GMM 试图将所有数据点建模为有限的高斯分布混合体；计算每个点属于每个聚类的概率，并与聚类相关的统计数据一起表示一个合成分布：所有点都来自
    *K* 个具有自身概率的高斯子分布之一。简而言之，GMM 的功能可以用三步伪代码描述：
- en: '**Objective function:** Compute and maximize the log-likelihood using expectation–maximization
    (EM) as a framework'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Objective function（目标函数）**：使用期望最大化（EM）作为框架，计算并最大化对数似然。'
- en: '**EM algorithm:**'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**EM 算法：**'
- en: '**E step:** Compute the posterior probability of membership -i.e. nearer data
    points'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**E 步骤（E step）**：计算后验概率 - 即靠近的数据点。'
- en: '**M step:** Optimize the parameters.'
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**M 步骤（M step）**：优化参数。'
- en: '**Assignment:** Perform soft assignment during step E.'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Assignment（分配）**：在 E 步骤中执行软分配。'
- en: Technically, when a statistical model is given, parameters of that model (that
    is, when applied to a data set) are estimated using the **maximum-likelihood estimation**
    (**MLE**). On the other hand, **EM** algorithm is an iterative process of finding
    maximum likelihood.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，当给定一个统计模型时，该模型的参数（即应用于数据集时）是通过 **最大似然估计** (**MLE**) 来估计的。另一方面，**EM** 算法是一个迭代过程，用于寻找最大似然。
- en: Since the GMM is an unsupervised algorithm, GMM model depends on the inferred
    variables. Then EM iteration rotates toward performing the expectation (E) and
    maximization (M) step.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 GMM 是一种无监督算法，GMM 模型依赖于推断的变量。然后，EM 迭代会转向执行期望（E）和最大化（M）步骤。
- en: 'The Spark MLlib implementation uses the expectation-maximization algorithm
    to induce the maximum-likelihood model from a given a set of data points. The
    current implementation uses the following parameters:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib 实现使用期望最大化算法从给定的数据点集中引导最大似然模型。当前的实现使用以下参数：
- en: '**K** is the number of desired clusters to cluster your data points'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K** 是所需聚类数，用于聚类你的数据点。'
- en: '**ConvergenceTol** is the maximum change in log-likelihood at which we consider
    convergence achieved.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ConvergenceTol（收敛容忍度）** 是我们认为收敛已达成时，最大对数似然的变化量。'
- en: '**MaxIterations** is the maximum number of iterations to perform without reaching
    the convergence point.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MaxIterations（最大迭代次数）** 是在没有达到收敛点的情况下执行的最大迭代次数。'
- en: '**InitialModel** is an optional starting point from which to start the EM algorithm.
    If this parameter is omitted, a random starting point will be constructed from
    the data.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**InitialModel** 是一个可选的起始点，用于启动 EM 算法。如果省略此参数，将从数据中构造一个随机起始点。'
- en: An example of clustering using GMM with Spark MLlib
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spark MLlib 进行 GMM 聚类的示例
- en: 'In the previous sections, we saw how to cluster the similar houses together
    to determine the neighborhood. Using GMM, it is also possible to cluster the houses
    toward finding the neighborhood except the model training that takes different
    training parameters as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了如何将相似的房屋聚集在一起以确定邻里。使用 GMM，也可以将房屋聚集在一起以寻找邻里，除了模型训练会采用不同的训练参数，如下所示：
- en: '[PRE22]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You should refer to the previous example and just reuse the previous steps
    of getting the trained data. Now to evaluate the model''s performance, GMM does
    not provide any performance metrics like WCSS as a cost function. However, GMM
    provides some performance metrics like mu, sigma, and weight. These parameters
    signify the maximum likelihood among different clusters (five clusters in our
    case). This can be demonstrated as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该参考之前的示例，并重用获取训练数据的先前步骤。现在为了评估模型的性能，GMM 并没有提供像 WCSS 这样的性能指标作为代价函数。然而，GMM 提供了一些性能指标，比如
    mu、sigma 和权重。这些参数表示不同聚类之间的最大似然（我们这里有五个聚类）。这可以如下演示：
- en: '[PRE23]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You should observe the following output:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该观察到以下输出：
- en: '![](img/00154.jpeg)**Figure 9:** Cluster 1![](img/00017.jpeg)**Figure 10:**
    Cluster 2![](img/00240.jpeg)**Figure 11:** Cluster 3![](img/00139.jpeg)**Figure
    12:** Cluster 4![](img/00002.jpeg)**Figure 13:** Cluster 5'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00154.jpeg)**图9：** 簇1![](img/00017.jpeg)**图10：** 簇2![](img/00240.jpeg)**图11：**
    簇3![](img/00139.jpeg)**图12：** 簇4![](img/00002.jpeg)**图13：** 簇5'
- en: The weight of clusters 1 to 4 signifies that these clusters are homogeneous
    and significantly different compared with cluster 5.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 簇1到簇4的权重表明这些簇是均质的，并且与簇5相比存在显著差异。
- en: Determining number of clusters
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定簇的数量
- en: The beauty of clustering algorithms like K-means algorithm is that it does the
    clustering on the data with an unlimited number of features. It is a great tool
    to use when you have a raw data and would like to know the patterns in that data.
    However, deciding the number of clusters prior to doing the experiment might not
    be successful but may sometimes lead to an overfitting or underfitting problem.
    On the other hand, one common thing to all three algorithms (that is, K-means,
    bisecting K-means, and Gaussian mixture) is that the number of clusters must be
    determined in advance and supplied to the algorithm as a parameter. Hence, informally,
    determining the number of clusters is a separate optimization problem to be solved.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 像K-means算法这样的聚类算法的优点在于，它可以对具有无限特征的数据进行聚类。当你有原始数据并希望了解数据中的模式时，这是一个非常好的工具。然而，在实验之前确定簇的数量可能并不成功，有时还可能导致过拟合或欠拟合问题。另一方面，K-means、二分K-means和高斯混合模型这三种算法的共同之处在于，簇的数量必须事先确定，并作为参数提供给算法。因此，非正式地说，确定簇的数量是一个独立的优化问题，需要解决。
- en: In this section, we will use a heuristic approach based on the Elbow method.
    We start from K = 2 clusters, and then we ran the K-means algorithm for the same
    data set by increasing K and observing the value of cost function **Within-Cluster
    Sum of Squares** (**WCSS**). At some point, a big drop in cost function can be
    observed, but then the improvement became marginal with the increasing value of
    K. As suggested in cluster analysis literature, we can pick the K after the last
    big drop of WCSS as an optimal one.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用基于肘部法则的启发式方法。我们从K = 2个簇开始，然后通过增加K并观察成本函数**簇内平方和（Within-Cluster Sum
    of Squares）**（**WCSS**）的值，运行K-means算法处理相同的数据集。在某些时刻，可以观察到成本函数有一个大的下降，但随着K值的增加，改进变得微乎其微。如聚类分析文献所建议的，我们可以选择WCSS最后一次大幅下降后的K值作为最优值。
- en: 'By analysing below parameters, you can find out the performance of K-means:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析以下参数，你可以找出K-means的性能：
- en: '**Betweenness:** This is the between sum of squares also called as *intracluster
    similarity.*'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中介性（Betweenness）：** 这是中介平方和，也称为*簇内相似度（intracluster similarity）。*'
- en: '**Withiness:** This is the within sum of square also called *intercluster similarity.*'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**簇内平方和（Withiness）：** 这是簇内平方和，也叫做*簇间相似度（intercluster similarity）。*'
- en: '**Totwithinss:** This is the sum of all the withiness of all the clusters also
    called *total intracluster similarity.*'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总簇内平方和（Totwithinss）：** 这是所有簇内的平方和的总和，也叫做*总簇内相似度（total intracluster similarity）。*'
- en: It is to be noted that a robust and accurate clustering model will have a lower
    value of withiness and a higher value of betweenness. However, these values depend
    on the number of clusters, that is, K that is chosen before building the model.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，一个稳健且准确的聚类模型将具有较低的簇内平方和和较高的中介性值。然而，这些值取决于簇的数量，即K值，这个值需要在构建模型之前选择。
- en: Now let us discuss how to take advantage of the Elbow method to determine the
    number of clusters. As shown in the following, we calculated the cost function
    WCSS as a function of a number of clusters for the K-means algorithm applied to
    home data based on all the features. It can be observed that a big drop occurs
    when K = 5\. Therefore, we chose the number of clusters as 5, as shown in *Figure
    10*. Basically, this is the one after the last big drop.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论如何利用肘部法则来确定簇的数量。如下面所示，我们计算了K-means算法应用于家庭数据（基于所有特征）时，聚类数与成本函数WCSS的关系。可以观察到，当K
    = 5时，出现了一个大幅下降。因此，我们选择了5作为簇的数量，如*图10*所示。基本上，这是最后一次大幅下降之后的值。
- en: '![](img/00303.jpeg)**Figure 14:** Number of clusters as a function of WCSS'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00303.jpeg)**图14：** 聚类数与WCSS的关系'
- en: A comparative analysis between clustering algorithms
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类算法的比较分析
- en: 'Gaussian mixture is used mainly for expectation minimization, which is an example
    of an optimization algorithm. Bisecting K-means, which is faster than regular
    K-means, also produces slightly different clustering results. Below we try to
    compare these three algorithms. We will show a performance comparison in terms
    of model building time and the computional cost for each algorithm. As shown in
    the following code, we can compute the cost in terms of WCSS. The following lines
    of code can be used to compute the WCSS for the K-means and **b**isecting algorithms:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合模型主要用于期望最小化，这是优化算法的一个例子。与普通 K-means 算法相比，二分 K-means 更快，并且产生略微不同的聚类结果。下面我们尝试对比这三种算法。我们将展示每种算法在模型构建时间和计算成本方面的性能对比。如以下代码所示，我们可以通过
    WCSS 计算成本。以下代码行可以用来计算 K-means 和**二分**算法的 WCSS：
- en: '[PRE24]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'For the dataset we used throughout this chapter, we got the following values
    of WCSS:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章使用的数据集，我们得到了以下 WCSS 的值：
- en: '[PRE25]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This means that K-means shows slightly better performance in terms of the compute
    cost. Unfortunately, we don''t have any metrics like WCSS for the GMM algorithm.
    Now let''s observe the model building time for these three algorithms. We can
    start the system clock before starting model training and stop it immediately
    after the training has been finished as follows (for K-means):'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在计算成本方面，K-means 的表现稍微好一些。不幸的是，我们没有像 WCSS 这样的度量指标来评估 GMM 算法。现在让我们观察这三种算法的模型构建时间。我们可以在开始模型训练前启动系统时钟，并在训练结束后立即停止时钟，如下所示（对于
    K-means）：
- en: '[PRE26]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'For the training set we used throughout this chapter, we got the following
    values of model building time:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章使用的训练集，我们得到了以下模型构建时间的值：
- en: '[PRE27]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In different research articles, it has been found that the bisecting K-means
    algorithm has been shown to result in better cluster assignment for data points.
    Moreover, compared to K-means, bisecting K-means, alos converges well towards
    global minima. K-means on the other hand, gets stuck in local minima. In other
    words, using bisecting K-means algorithm, we can avoid the local minima that K-means
    can suffer from.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的研究文章中发现，二分 K-means 算法在数据点的聚类分配上表现得更好。此外，与 K-means 相比，二分 K-means 也能更好地收敛到全局最小值。而
    K-means 则容易陷入局部最小值。换句话说，使用二分 K-means 算法，我们可以避免 K-means 可能遭遇的局部最小值问题。
- en: Note that you might observe different values of the preceding parameters depending
    upon your machine's hardware configuration and the random nature of the dataset.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，根据机器的硬件配置和数据集的随机性，您可能会观察到前述参数的不同值。
- en: More details analysis is up to the readers from the theoretical views. Interested
    readers should also refer to Spark MLlib-based clustering techniques at [https://spark.apache.org/docs/latest/mllib-clustering.html](https://spark.apache.org/docs/latest/mllib-clustering.html)
    to get more insights.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细的分析留给读者从理论角度进行。感兴趣的读者还应参考基于 Spark MLlib 的聚类技术，详情请见 [https://spark.apache.org/docs/latest/mllib-clustering.html](https://spark.apache.org/docs/latest/mllib-clustering.html)
    以获得更多见解。
- en: Submitting Spark job for cluster analysis
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提交 Spark 作业进行聚类分析
- en: 'The examples shown in this chapter can be made scalable for the even larger
    dataset to serve different purposes. You can package all three clustering algorithms
    with all the required dependencies and submit them as a Spark job in the cluster.
    Now use the following lines of code to submit your Spark job of K-means clustering,
    for example (use similar syntax for other classes), for the Saratoga NY Homes
    dataset:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 本章展示的例子可以扩展到更大的数据集以服务于不同的目的。您可以将所有三种聚类算法与所需的依赖项一起打包，并将它们作为 Spark 作业提交到集群中。现在，使用以下代码行来提交您的
    K-means 聚类 Spark 作业，例如（对其他类使用类似语法），以处理 Saratoga NY Homes 数据集：
- en: '[PRE28]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Summary
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we delved even deeper into machine learning and found out how
    we can take advantage of machine learning to cluster records belonging to a dataset
    of unsupervised observations. Consequently, you learnt the practical know-how
    needed to quickly and powerfully apply supervised and unsupervised techniques
    on available data to new problems through some widely used examples based on the
    understandings from the previous chapters. The examples we are talking about will
    be demonstrated from the Spark perspective. For any of the K-means, bisecting
    K-means, and Gaussian mixture algorithms, it is not guaranteed that the algorithm
    will produce the same clusters if run multiple times. For example, we observed
    that running the K-means algorithm multiple times with the same parameters generated
    slightly different results at each run.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们进一步深入探讨了机器学习，并了解了如何利用机器学习对无监督观测数据集中的记录进行聚类。因此，你学习了通过前几章的理解，如何快速而有力地将有监督和无监督技术应用于新问题。我们将展示的例子将从
    Spark 的角度进行说明。对于 K-means、二分 K-means 和高斯混合算法，无法保证算法在多次运行时产生相同的聚类结果。例如，我们观察到，使用相同参数多次运行
    K-means 算法时，每次运行产生的结果略有不同。
- en: 'For a performance comparison between K-means and Gaussian mixture, see *Jung.
    et. al and cluster analysis* lecture notes. In addition to K-means, bisecting
    K-means, and Gaussian mixture, MLlib provides implementations of three other clustering
    algorithms, namely, PIC, LDA, and streaming K-means. One thing is also worth mentioning
    is that to fine tune clustering analysis, often we need to remove unwanted data
    objects called outlier or anomaly. But using distance based clustering it''s really
    difficult to identify such data pints. Therefore, other distance metrics other
    than Euclidean can be used. Nevertheless, these links would be a good resource
    to start with:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 K-means 和高斯混合模型的性能对比，请参见*Jung 等人的聚类分析*讲义。除了 K-means、二分 K-means 和高斯混合模型外，MLlib
    还提供了另外三种聚类算法的实现，分别是 PIC、LDA 和流式 K-means。值得一提的是，为了精细调优聚类分析，我们通常需要去除一些被称为离群点或异常值的无效数据对象。但使用基于距离的聚类方法时，确实很难识别这些数据点。因此，除了欧氏距离外，还可以使用其他距离度量。无论如何，这些链接将是一个很好的起点资源：
- en: '[https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html](https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html)'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html](https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html)'
- en: '[https://github.com/keiraqz/anomaly-detection](https://github.com/keiraqz/anomaly-detection)'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://github.com/keiraqz/anomaly-detection](https://github.com/keiraqz/anomaly-detection)'
- en: '[http://www.dcc.fc.up.pt/~ltorgo/Papers/ODCM.pdf](https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html)'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[http://www.dcc.fc.up.pt/~ltorgo/Papers/ODCM.pdf](https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html)'
- en: In the next chapter, we will dig even deeper into tuning Spark applications
    for better performance. We will see some best practice to optimize the performance
    of Spark applications.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨如何调优 Spark 应用以提高性能。我们将看到一些优化 Spark 应用性能的最佳实践。
