- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Mastering Data Governance in the Lakehouse with Unity Catalog
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Unity Catalog掌握湖仓中的数据治理
- en: In this chapter, we’ll dive into the implementation of effective data governance
    for the lakehouse using Unity Catalog. We’ll cover enabling Unity Catalog on an
    existing Databricks workspace, implementing data cataloging for data discovery,
    enforcing fine-grained data access at the table, row and column levels, as well
    as tracking data lineage. By the end of the chapter, you’ll be armed with industry
    best practices around data governance and will have gained real-world insights
    for enhancing data security and compliance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章节中，我们将深入探讨如何使用Unity Catalog实施湖仓中的有效数据治理。我们将介绍如何在现有的Databricks工作空间启用Unity
    Catalog，如何为数据发现实施数据目录管理，如何在表、行和列级别执行细粒度的数据访问控制，以及如何跟踪数据血统。到章节结束时，你将掌握数据治理的行业最佳实践，并获得提升数据安全性和合规性的真实世界经验。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章节中，我们将涵盖以下主要主题：
- en: Understanding data governance in the l akehouse
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解湖仓中的数据治理
- en: Enabling Unity Catalog on an existing Databricks workspace
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在现有Databricks工作空间启用Unity Catalog
- en: Identity federation in Unity Catalog
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unity Catalog中的身份联合
- en: Data discovery and cataloging
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据发现与目录管理
- en: Hands-on lab – data masking healthcare datasets
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动手实验室 – 数据掩码处理医疗数据集
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To follow along in this chapter, you’ll need Databricks workspace permissions
    to create and start an all-purpose cluster so that you can execute the chapter’s
    accompanying notebooks. It’s also recommended that your Databricks user be elevated
    to an account admin and a metastore admin so that you can deploy a new Unity Catalog
    metastore and attach it to your existing Databricks workspace. All code samples
    can be downloaded from this chapter’s GitHub repository located at [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter05](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter05)
    . This chapter will create and run several new notebooks, estimated to consume
    around 5–10 **Databricks** **units** ( **DBUs** ).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随本章节的内容，你需要拥有Databricks工作空间的权限，能够创建和启动通用集群，以便执行本章节附带的笔记本。还建议将你的Databricks用户提升为账户管理员和元存储管理员，这样你才能部署新的Unity
    Catalog元存储并将其附加到现有的Databricks工作空间。所有代码示例可以从本章节的GitHub仓库下载，链接地址为[https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter05](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter05)。本章节将创建并运行几个新的笔记本，预计会消耗约5到10
    **Databricks** **单位** (**DBU** )。
- en: Understanding data governance in a lakehouse
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解湖仓中的数据治理
- en: It’s quite common for a lakehouse implementation to leverage multiple processing
    engines for different use cases. However, each processing engine comes with its
    own data security implementation and, often, these different data security solutions
    don’t integrate with one another. Where most lakehouses fall short is that due
    to these multiple security layers, implementing consistent, global data security
    policies is nearly impossible. Ensuring that the data in your lakehouse is completely
    and consistently secured and private and that access is only granted to the correct
    set of users is of the utmost importance when building a data lakehouse. Therefore,
    having a simple data governance solution that covers all the data in your organization’s
    lakehouse is critical for your organization’s success.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 湖仓实现中通常会利用多个处理引擎来应对不同的使用场景。然而，每个处理引擎都有自己独特的数据安全实现，并且这些不同的数据安全解决方案往往无法互相集成。大多数湖仓系统的不足之处在于，由于有多个安全层，实施一致的、全球性的安全政策几乎是不可能的。确保湖仓中的数据完全且一致地安全、私密，并且仅向正确的用户授予访问权限，是建设数据湖仓时最为重要的任务。因此，拥有一个简单的数据治理解决方案，能够覆盖你组织湖仓中的所有数据，对组织的成功至关重要。
- en: Introducing the Databricks Unity Catalog
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍Databricks Unity Catalog
- en: Unity Catalog is a centralized data governance solution that simplifies securing
    the data in your lakehouse by organizing workspace object access policies into
    a single administrative “pane of glass.” In addition to access policies, Unity
    Catalog was designed with strong auditing in mind, allowing administrators to
    capture all user access patterns to workspace objects so that administrators can
    observe access patterns, workspace usage, and billing patterns across all Databricks
    workspaces. Furthermore, Unity Catalog was designed to allow data professionals
    to discover datasets across your organization, track data lineage, view entity
    relationship diagrams, share curated datasets, and monitor the health of systems.
    One of the major strong suites of Unity Catalog is that once your organization’s
    data is in Unity Catalog, it’s secured by default – no process, whether it’s internal
    within the Databricks workspace or an external process that interacts with data
    from outside of the Databricks Data Intelligence Platform, has access to the data
    unless access has been explicitly granted by a data administrator. Unity Catalog
    was designed to span across the perimeter of your lakehouse from workspace to
    workspace and beyond the Databricks workspace, sitting on top of your organization’s
    data so that a single governance model can be simply and consistently applied
    to all parties accessing your organization’s data in the l akehouse.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Unity Catalog是一个集中式的数据治理解决方案，通过将工作区对象访问策略组织到一个单一的管理“统一视图”中，简化了湖仓中数据的安全保护。除了访问策略，Unity
    Catalog的设计还充分考虑了强大的审计功能，允许管理员捕捉所有用户对工作区对象的访问模式，以便管理员可以观察访问模式、工作区使用情况以及跨所有Databricks工作区的计费模式。此外，Unity
    Catalog的设计还允许数据专业人员在组织内部发现数据集、追踪数据血缘、查看实体关系图、分享精选数据集以及监控系统健康状况。Unity Catalog的一个重要优势是，一旦组织的数据进入Unity
    Catalog，它会默认得到安全保护——无论是Databricks工作区内的内部过程，还是与Databricks数据智能平台外部数据交互的外部过程，除非数据管理员明确授予访问权限，否则任何过程都无法访问这些数据。Unity
    Catalog的设计旨在跨越湖仓的边界，从工作区到工作区，并超越Databricks工作区，坐落在组织的数据之上，以便能够将一个单一的治理模型简单且一致地应用于所有访问组织湖仓数据的方。
- en: '![Figure 5.1 – Unity Catalog provides a single, unified governance layer on
    top of your organization’s cloud data](img/B22011_05_1.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – Unity Catalog 在组织的云数据之上提供了一个单一的统一治理层](img/B22011_05_1.png)'
- en: Figure 5.1 – Unity Catalog provides a single, unified governance layer on top
    of your organization’s cloud data
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – Unity Catalog 在组织的云数据之上提供了一个单一的统一治理层
- en: However, having a global data and workspace object security layer wasn’t always
    as seamless as it is in Unity Catalog today. Let’s travel back in time to the
    lessons learned from the previous security model in Databricks and how Unity Catalog
    came to fruition today.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，拥有一个全球范围的数据和工作区对象安全层并不总是像今天在Unity Catalog中那样无缝。让我们回顾一下Databricks以前的安全模型中的经验教训，以及Unity
    Catalog如何最终实现今天的成果。
- en: A problem worth solving
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个值得解决的问题
- en: Previously, data access control policies were defined per workspace using a
    mechanism known as table **access control lists** ( **ACLs** ). When these were
    enforced properly, table ACLs provided a powerful data governance solution. Administrators
    could define data access policies for different users and groups within a workspace
    and those access policies could be enforced by the Databricks cluster when executing
    Spark code that accessed the underlying datasets registered in the **Hive** **Metastore**
    ( **HMS** ).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，数据访问控制策略是通过一种称为表**访问控制列表**（**ACLs**）的机制在每个工作区定义的。当这些策略被正确执行时，表ACLs提供了强大的数据治理解决方案。管理员可以为工作区内的不同用户和组定义数据访问策略，并且在执行访问底层数据集的Spark代码时，Databricks集群可以强制执行这些访问策略，这些数据集是在**Hive**
    **Metastore**（**HMS**）中注册的。
- en: However, four major problems quickly arose from the table ACL security model.
    The first problem was that data access policies defined using the table ACL security
    model needed to be repeated for each unique Databricks workspace. Most organizations
    prefer to have separate workspaces for each logical work environment – for example,
    a single workspace for development, another workspace for acceptance testing,
    and finally, a workspace dedicated to running production workloads. Aside from
    the repetition of the same shared data access policies, if a data access policy
    happened to change in one workspace, it often meant that the data access policies
    across *all* workspaces needed to be changed as well. This led to unnecessary
    maintenance overhead, as there was not a single location where these data access
    patterns could be easily defined within the Databricks Data Intelligence Platform.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，来自表ACL安全模型的四个主要问题迅速浮现。第一个问题是，使用表ACL安全模型定义的数据访问策略需要为每个独立的Databricks工作区重复定义。大多数组织更倾向于为每个逻辑工作环境拥有单独的工作区——例如，一个用于开发的工作区，一个用于验收测试的工作区，最后是一个专门用于运行生产工作负载的工作区。除了重复相同的共享数据访问策略外，如果某个工作区中的数据访问策略发生变化，通常意味着所有工作区的数据访问策略也需要更改。这导致了不必要的维护开销，因为在Databricks数据智能平台中并没有一个单一的位置可以轻松定义这些数据访问模式。
- en: Secondly, table ACLs were only enforced on the underlying data when interactive
    notebooks or automated jobs were executed on a table ACL-enabled cluster. A cluster
    without the table ACL security model enabled could directly access the underlying
    dataset, bypassing the security model entirely! While cluster policies (covered
    in [*Chapter 1*](B22011_01.xhtml#_idTextAnchor014) ) could be used to mitigate
    this issue and prevent any potential nefarious access to privileged datasets,
    cluster policies are complex to write. They require knowledge of the cluster policy
    schema as well as experience expressing configuration as JSON, making it difficult
    to scale across an organization. More often than not, it was quite common for
    a user to complain to their organization’s leaders that they needed administrative
    workspace access to spin up a cluster of their own liking and complete their day-to-day
    activities. Once the user was granted administrator workspace access, they too
    could grant administrator access to other users, and, like a snowball effect,
    there would be an unreasonable number of administrators for a workspace. This
    type of bad practice can easily lead to a data leak by side-stepping the table
    ACL security model using a cluster without table ACLs enabled.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，表ACL仅在交互式笔记本或自动化作业在启用了表ACL的集群上执行时，才会强制执行对底层数据的访问控制。没有启用表ACL安全模型的集群可以直接访问底层数据集，完全绕过安全模型！虽然集群策略（详见[*第1章*](B22011_01.xhtml#_idTextAnchor014)）可以用来缓解这个问题，并防止任何潜在的恶意访问特权数据集，但集群策略的编写十分复杂。它们需要了解集群策略架构，并且需要有将配置表示为JSON的经验，这使得在整个组织中扩展变得困难。通常情况下，用户常常向组织领导抱怨，称他们需要管理员工作区访问权限，以便启动自己喜欢的集群并完成日常活动。一旦用户获得了管理员工作区访问权限，他们也能将管理员权限授予其他用户，从而形成像滚雪球一样的效应，导致一个工作区有过多的管理员。此类不当操作很容易绕过启用了表ACL的集群，导致数据泄露。
- en: Furthermore, due to the isolation issues of running **Java Virtual Machine**
    ( **JVM** ) languages on a shared computational resource, table ACL-enabled clusters
    limited end users to only running workloads using either the SQL or Python programming
    languages. Users wanting to execute workloads using the Scala, Java, or R programming
    languages would need to be granted an exception to use a non-table ACL-enabled
    cluster, opening a huge hole in the organization’s data governance solution.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于在共享计算资源上运行**Java虚拟机**（**JVM**）语言的隔离问题，启用表ACL的集群限制最终用户只能使用SQL或Python编程语言运行工作负载。希望使用Scala、Java或R编程语言执行工作负载的用户，需要被授予例外权限，使用未启用表ACL的集群，这为组织的数据治理解决方案打开了一个巨大的漏洞。
- en: The fourth major problem that arose had to do with the ability of the HMS to
    scale. The Databricks Data Intelligence Platform leveraged the HMS to register
    datasets across a workspace, which allowed users to create new datasets from scratch,
    organize them in schemas, and even share access to users and groups across an
    organization. However, as a workspace onboarded thousands of users, those users
    would need to execute ad hoc queries concurrently, while also executing hundreds
    or even thousands of scheduled jobs. Eventually, the HMS struggled to keep up
    with the level of concurrency needed for the most demanding workspaces.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个出现的主要问题与 HMS 的可扩展性有关。Databricks 数据智能平台利用 HMS 在工作区中注册数据集，这使得用户可以从头开始创建新数据集，将其组织到架构中，甚至跨组织共享用户和组的访问权限。然而，随着一个工作区逐步增加数千名用户，这些用户需要同时执行临时查询，并且还需要执行数百甚至数千个定时任务。最终，HMS
    很难跟上最苛刻工作区所需的并发处理能力。
- en: It was clear that there needed to be a huge change and so Databricks set out
    to completely redesign a data governance solution from scratch.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，需要进行一次巨大的变革，因此 Databricks 开始着手从零开始完全重新设计数据治理解决方案。
- en: An overview of the Unity Catalog architecture
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Unity Catalog 架构概述
- en: One of the primary pain points that Unity Catalog aimed to solve was to implement
    a complete end-to-end data governance solution that spans all of an organization’s
    workspaces, removing the redundancy of having to redefine data access policies
    for each Databricks workspace. Instead, with Unity Catalog, data administrators
    can define data access controls *once* in a centralized location and have the
    peace of mind that they will be consistently applied across an organization no
    matter what computational resource or processing engine is used to interact with
    datasets in Unity Catalog.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Unity Catalog 旨在解决的一个主要痛点是实施一个完整的端到端数据治理解决方案，覆盖组织的所有工作区，消除需要为每个 Databricks 工作区重新定义数据访问策略的冗余问题。相反，通过
    Unity Catalog，数据管理员可以在一个集中位置定义数据访问控制*一次*，并且可以放心，数据访问策略将在整个组织中一致地应用，无论使用何种计算资源或处理引擎与
    Unity Catalog 中的数据集交互。
- en: '![Figure 5.2 – Unity Catalog centralizes data access policies that are then
    consistently applied across multiple Databricks workspaces](img/B22011_05_2.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – Unity Catalog 集中管理数据访问策略，并在多个 Databricks 工作区中一致应用这些策略](img/B22011_05_2.png)'
- en: Figure 5.2 – Unity Catalog centralizes data access policies that are then consistently
    applied across multiple Databricks workspaces
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – Unity Catalog 集中管理数据访问策略，并在多个 Databricks 工作区中一致应用这些策略
- en: 'In addition to centralized data governance, Unity Catalog has several other
    key motivating factors that make it an ideal data governance solution for the
    modern-day l akehouse:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 除了集中式数据治理外，Unity Catalog 还有多个其他关键驱动因素，使其成为现代湖仓环境理想的数据治理解决方案：
- en: '**Secure by default** : Users will not have access to read data from any form
    of compute without using a Unity Catalog-enabled cluster (Unity Catalog clusters
    are covered in the following section) and having been granted specific access
    to use and select data from a particular dataset.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**默认安全**：用户在未使用启用 Unity Catalog 的集群（Unity Catalog 集群将在下节中介绍）且未被授予特定数据集的使用和选择权限的情况下，无法访问任何计算资源中的数据。'
- en: '**Comfortable administrator interfaces** : Data access policies in Unity Catalog
    are tightly integrated with **American National Standards Institute** ( **ANSI**
    ) SQL, allowing administrators to express data access permissions on familiar
    database objects such as catalogs, databases, tables, functions, and views. Data
    access permissions can also be set using the administrator UI from the Databricks
    web app, or automated deployment tools such as Terraform.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**舒适的管理员界面**：Unity Catalog 中的数据访问策略与**美国国家标准学会**（**ANSI**）SQL 紧密集成，使管理员可以在熟悉的数据库对象上表达数据访问权限，例如目录、数据库、表、函数和视图。数据访问权限还可以通过
    Databricks Web 应用程序中的管理员 UI 设置，或者使用自动化部署工具，如 Terraform。'
- en: '**Data discovery** : Unity Catalog makes it easy for data stewards to tag datasets
    with descriptive metadata, allowing users across your organization to search and
    discover datasets available to them.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据发现**：Unity Catalog 使数据管理员可以轻松地为数据集添加描述性元数据，从而使组织中的用户能够搜索和发现可用的数据集。'
- en: '**Strong auditing** : Unity Catalog automatically captures user-level access
    patterns and data operations, allowing administrators to view and audit user behavior
    as they interact with the data in your l akehouse.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强大的审计功能**：Unity Catalog 会自动捕捉用户级别的访问模式和数据操作，允许管理员查看和审计用户在与湖仓数据交互时的行为。'
- en: '**Data lineage tracking** : Tracing how tables and columns are generated from
    upstream sources is important in ensuring that downstream datasets are formed
    using trusted sources. Unity Catalog makes tracking data and workspace assets
    simple through its strong data lineage APIs and system tables.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据血缘追踪**：追踪表和列如何从上游源生成，对于确保下游数据集是通过受信任的源形成的至关重要。通过强大的数据血缘 API 和系统表，Unity
    Catalog 使数据和工作空间资产的追踪变得简单。'
- en: '**Observability** : Because Unity Catalog spans multiple Databricks workspaces,
    it can aggregate system metrics and auditing events into a centralized set of
    read-only tables for monitoring and system observability called system tables
    (covered in greater detail in the *Observability with system* *tables* section).'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可观察性**：由于 Unity Catalog 跨多个 Databricks 工作空间，因此可以将系统指标和审计事件汇总到一组集中式的只读表中，以便进行监控和系统可观察性，这些表被称为系统表（在*系统表的可观察性*部分将详细介绍）。'
- en: To implement a security model where the data is secured by default and there
    is no ability to access the data externally without going through Unity Catalog,
    Databricks needed to design different clusters based on the user’s persona. Let’s
    look at the different cluster types available to users in a Unity Catalog-enabled
    workspace.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现一种默认数据安全且没有外部访问数据的能力，用户必须通过 Unity Catalog 来访问数据，Databricks 需要根据用户角色设计不同类型的集群。让我们来看一下在启用
    Unity Catalog 的工作空间中，用户可以使用的不同集群类型。
- en: Unity Catalog-enabled cluster types
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用 Unity Catalog 的集群类型
- en: 'There are three major types of clusters for a workspace with Unity Catalog
    enabled:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 启用 Unity Catalog 的工作空间有三种主要类型的集群：
- en: '**Single-user cluster** : Only a single user or service principal will have
    permission to execute notebook cells or workflows on this type of cluster. Workloads
    containing a mixture of Scala, Java, R, Python, and SQL languages can be executed
    on this type of cluster *only* . Datasets registered in Unity Catalog can be queried
    from this type of cluster.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单用户集群**：只有单个用户或服务主体才有权限在这种集群上执行笔记本单元格或工作流。包含 Scala、Java、R、Python 和 SQL 语言的工作负载只能在这种集群上执行。注册在
    Unity Catalog 中的数据集可以从这种集群中查询。'
- en: '**Shared cluster** : Multiple users or service principals can have permission
    to execute notebook cells or workflows on this type of cluster. This type of cluster
    is restricted to only Python, SQL, and Scala workloads. Datasets registered in
    Unity Catalog can be queried from this type of cluster.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共享集群**：多个用户或服务主体可以在这种集群上执行笔记本单元格或工作流。此类集群仅限于 Python、SQL 和 Scala 工作负载。注册在
    Unity Catalog 中的数据集可以从这种集群中查询。'
- en: '**Standalone cluster** : A single user or multiple users can attach a notebook
    and execute notebook cells to this type of cluster. However, datasets registered
    within the Unity Catalog *cannot* be queried by this type of cluster and will
    result in a runtime exception if a user attempts to query a dataset registered
    in Unity Catalog. This type of cluster can be used for reading datasets registered
    in the legacy HMS.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立集群**：单个用户或多个用户可以将笔记本附加到此类集群并执行笔记本单元格。然而，Unity Catalog 中注册的数据集 *无法* 被此类集群查询，若用户尝试查询
    Unity Catalog 中注册的数据集，将导致运行时异常。此类集群可用于读取在遗留 HMS 中注册的数据集。'
- en: Now that we’ve outlined the different types of computational resources you can
    use to interact with the data in Unity Catalog, let’s now turn our attention to
    how the data and other assets are organized within the Unity Catalog, mainly understanding
    the Unity Catalog object model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经概述了可以用来与 Unity Catalog 中的数据互动的不同计算资源类型，接下来让我们关注一下数据和其他资产在 Unity Catalog
    中是如何组织的，主要是理解 Unity Catalog 的对象模型。
- en: Unity Catalog object model
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Unity Catalog 对象模型
- en: It’s important to understand the object model within Unity Catalog as it will
    help users understand the types of objects that can be secured and governed by
    Unity Catalog. Furthermore, it will also help metastore administrators architect
    data access policies.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 Unity Catalog 中的对象模型非常重要，它将帮助用户理解哪些类型的对象可以被 Unity Catalog 安全管理和治理。此外，它还将帮助元存储管理员设计数据访问策略。
- en: One of the major changes that Unity Catalog introduced is the concept of a three-level
    namespace. Traditionally, in the HMS, users interacting with the data could reference
    datasets using a combination of a schema (or database) and a table name. However,
    Unity Catalog adds a third logical container, called the catalog, which can hold
    one or more schemas. To reference a fully qualified dataset in Unity Catalog,
    data practitioners will need to provide the name of the catalog, schema, and table.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Unity Catalog 引入的主要变化之一是三层命名空间的概念。传统上，在 HMS 中，用户与数据交互时可以通过模式（或数据库）和表名的组合来引用数据集。然而，Unity
    Catalog 增加了第三个逻辑容器，称为目录（catalog），它可以包含一个或多个模式。要在 Unity Catalog 中引用完全限定的数据集，数据工作者需要提供目录、模式和表的名称。
- en: '![Figure 5.3 – Unity Catalog consists of many different securable objects beyond
    just a dataset](img/B22011_05_3.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – Unity Catalog 包含了许多不同的可安全管理对象，不仅仅是数据集](img/B22011_05_3.jpg)'
- en: Figure 5.3 – Unity Catalog consists of many different securable objects beyond
    just a dataset
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – Unity Catalog 包含了许多不同的可安全管理对象，不仅仅是数据集
- en: 'Let’s dive more into the Unity Catalog object model starting with the objects
    related to organizing physical datasets:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解 Unity Catalog 对象模型，从组织物理数据集的相关对象开始：
- en: '**Metastore** : The “physical” implementation of Unity Catalog. A particular
    cloud region can at most contain a single metastore.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元存储**：Unity Catalog 的“物理”实现。一个特定的云区域最多只能包含一个元存储。'
- en: '**Catalog** : The top-level container for datasets in Unity Catalog. A catalog
    can contain a collection of one or more schema objects.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目录**：Unity Catalog 中数据集的顶级容器。一个目录可以包含一个或多个模式对象的集合。'
- en: '**Schema** : Serves as the second tier in Unity Catalog. A schema can contain
    a collection of one or more tables, views, functions, models, and volumes.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式**：作为 Unity Catalog 中的第二级对象。一个模式可以包含一个或多个表、视图、函数、模型和卷的集合。'
- en: '**Table** : The representation of a dataset containing a defined schema and
    organized into rows and columns.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表**：表示一个包含定义模式并按行和列组织的数据集。'
- en: '**View** : A calculated dataset that can be the result of joining together
    tables, filtering columns, or applying complex business logic. Furthermore, views
    are read-only and cannot have data written to them or data updated using **data
    manipulation language** ( **DML** ) statements.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视图**：一个计算的数据集，可能是通过联接表、筛选列或应用复杂的业务逻辑的结果。此外，视图是只读的，不能向其写入数据或通过 **数据操作语言**
    (**DML**) 语句更新数据。'
- en: '**Model** : A machine learning model that has been registered to the tracking
    server using MLflow.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：一个使用 MLflow 注册到跟踪服务器的机器学习模型。'
- en: '**Function** : A custom, user-defined function that often contains complex
    business logic that typically cannot be implemented using the built-in Spark functions
    alone.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**函数**：一个自定义的、用户定义的函数，通常包含复杂的业务逻辑，这些逻辑通常无法仅通过内置的 Spark 函数实现。'
- en: '**Volume** : A data storage location designed for storing structured, semi-structured,
    or unstructured data (we will cover volumes in greater detail in the following
    chapter, *Mastering Data Locations in* *Unity Catalog* ).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷**：用于存储结构化、半结构化或非结构化数据的数据存储位置（我们将在下一章中详细讲解卷，*精通 Unity Catalog 中的数据位置*）。'
- en: 'Next, let’s turn our attention to the objects within Unity Catalog that are
    used to interact with data outside of the Databricks Data Intelligence Platform:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将注意力转向 Unity Catalog 中用于与 Databricks 数据智能平台之外的数据交互的对象：
- en: '**Connection** : Represents a read-only connection containing the credentials
    to access data in a foreign **relational database management system** ( **RDBMS**
    ) or data warehouse .'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连接**：表示一个只读连接，包含访问外部 **关系型数据库管理系统** (**RDBMS**) 或数据仓库中的数据所需的凭据。'
- en: '**Storage credential** : Represents the authentication credential for accessing
    a cloud storage location .'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储凭证**：表示访问云存储位置的身份验证凭证。'
- en: '**External location** : Represents a cloud storage location outside of the
    Databricks-managed root storage location .'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**外部位置**：表示 Databricks 管理的根存储位置之外的云存储位置。'
- en: 'Lastly, let’s look at the elements of the Unity Catalog object model for sharing
    and receiving datasets using the Delta Sharing protocol:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看 Unity Catalog 对象模型中用于通过 Delta Sharing 协议共享和接收数据集的元素：
- en: '**Provider** : Represents a data provider. This entity creates a collection
    of one or more curated datasets into a logical grouping, called a *share* . The
    data provider can revoke access to a shared dataset at any moment.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供方**：表示数据提供方。此实体将一个或多个策划的数据集创建为一个逻辑分组，称为*共享*。数据提供方可以随时撤销对共享数据集的访问权限。'
- en: '**Share** : A logical grouping of one or more shared datasets that can be shared
    with a data recipient.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共享**：一个或多个共享数据集的逻辑分组，可以与数据接收方共享。'
- en: '**Recipient** : Represents a data recipient. This entity receives access to
    a data share and can query the datasets within their workspace.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**接收方**：表示数据接收方。此实体可以访问数据共享并查询其工作区中的数据集。'
- en: Now that we’ve outlined the major building blocks of Unity Catalog, let’s look
    at how we can enable Unity Catalog on an existing Databricks workspace so that
    administrators can begin taking full advantage of the strong data governance solution.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经概述了 Unity Catalog 的主要构建块，接下来我们来看如何在现有的 Databricks 工作区启用 Unity Catalog，以便管理员可以开始充分利用强大的数据治理解决方案。
- en: Enabling Unity Catalog on an existing Databricks workspace
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在现有的 Databricks 工作区启用 Unity Catalog
- en: Beginning in early November 2023, all new Databricks workspaces created on **Amazon
    Web Services** ( **AWS** ) or Azure are enabled with Unity Catalog by default,
    so there is no extra configuration needed if your workspace was created after
    this date on these two cloud providers. Similarly, at the time of the creation
    of a new Databricks workspace, a single regional Unity Catalog metastore will
    be provisioned for your workspace to use. Within the regional metastore is a default
    catalog having the same name as the workspace and is bound to that workspace only
    (we’ll cover catalog binding in the following section). Furthermore, all users
    of a newly created workspace will have read and write access to a schema called
    **default** within this workspace catalog.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从 2023 年 11 月初开始，在**亚马逊 Web 服务**（**AWS**）或 Azure 上创建的所有新 Databricks 工作区默认启用
    Unity Catalog，因此，如果您的工作区是在此日期之后在这两个云提供商上创建的，则无需额外配置。同样，在创建新的 Databricks 工作区时，将为您的工作区预配一个区域性的
    Unity Catalog 元存储。该区域元存储内有一个默认目录，名称与工作区相同，并且仅绑定到该工作区（我们将在下一节讨论目录绑定）。此外，新创建的工作区的所有用户将具有对该工作区目录中名为**default**的模式的读写访问权限。
- en: Important note
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Workspace administrators cannot disable Unity Catalog on a workspace once a
    Databricks workspace has been enabled for Unity Catalog. However, datasets can
    always be migrated back to the HMS implementation, but the workspace will always
    be enabled with a Unity Catalog metastore.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦为 Databricks 工作区启用了 Unity Catalog，工作区管理员无法禁用 Unity Catalog。然而，数据集始终可以迁移回 HMS
    实现，但该工作区将始终启用 Unity Catalog 元存储。
- en: 'The first step in upgrading an existing Databricks workspace with Unity Catalog
    is to deploy a new metastore. A metastore is the “physical” implementation of
    a Unity Catalog. Administrators will need to deploy a single metastore per cloud
    region for an organization. A metastore can be deployed through a variety of methods,
    but for simplicity’s sake, we’ll cover deploying a new metastore using the Databricks
    UI:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 升级现有 Databricks 工作区以使用 Unity Catalog 的第一步是部署一个新的元存储。元存储是 Unity Catalog 的“物理”实现。管理员需要为每个云区域部署一个元存储。元存储可以通过多种方式进行部署，但为了简便起见，我们将介绍如何通过
    Databricks UI 来部署新的元存储：
- en: First, ensure that you are logged in to the account console located at [https://accounts.cloud.databricks.com](https://accounts.cloud.databricks.com)
    .
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，确保您已登录到账户控制台，网址是 [https://accounts.cloud.databricks.com](https://accounts.cloud.databricks.com)。
- en: From the account console, click on the **Catalog** menu item in the sidebar
    and click the **Create metastore** button to begin deploying a new metastore.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在账户控制台中，点击侧边栏中的**目录**菜单项，然后点击**创建元存储**按钮，开始部署新的元存储。
- en: Enter a meaningful name for your metastore, choose the appropriate region, and,
    optionally, select a default storage path where managed datasets should be stored
    in.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为您的元存储输入一个有意义的名称，选择适当的区域，并可选地选择一个默认存储路径，以存储托管的数据集。
- en: Finally, click the **Create** button. After a few minutes, your new metastore
    will be provisioned in the cloud region of your choosing.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，点击**创建**按钮。几分钟后，您的新元存储将在您选择的云区域中进行配置。
- en: Note
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In Databricks, global configurations such as catalog binding, network configurations,
    or user provisioning are centralized in a single administrative console, sometimes
    shortened to the “ account console.”
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Databricks 中，像目录绑定、网络配置或用户配置等全局配置都集中在一个单一的管理控制台中，有时简称为“账户控制台”。
- en: 'Now that the metastore has been deployed, the only thing left is to choose
    which Databricks workspaces you’d like to link the metastore to, enabling Unity
    Catalog:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在元存储已经部署，剩下的就是选择您希望将元存储链接到哪些 Databricks 工作区，从而启用 Unity Catalog：
- en: From the account console, again choose the **Catalog** menu item from the sidebar.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从账户控制台中，再次选择侧边栏中的**目录**菜单项。
- en: Next, click the name of the newly created metastore, followed by the **Assign
    to** **workspaces** button.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，点击新创建的元存储的名称，然后点击**分配到** **工作区**按钮。
- en: Lastly, click the workspace you would like to enable Unity Catalog on and confirm
    your selection by clicking the **Assign** button.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，点击您希望启用 Unity Catalog 的工作区，并通过点击**分配**按钮确认您的选择。
- en: Congratulations! You’ve now enabled Unity Catalog for your existing Databricks
    workspace, and you can begin to enjoy the peace of mind that your lakehouse data
    will be governed by a complete data security solution. Equally as important, once
    you have attached a Unity Catalog metastore to a Databricks workspace, you have
    now enabled your workspace for identity federation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您现在已为现有的 Databricks 工作区启用了 Unity Catalog，并可以开始享受数据湖数据将由完整的数据安全解决方案进行治理的安心感。同样重要的是，一旦将
    Unity Catalog 元存储附加到 Databricks 工作区，您就启用了工作区的身份联合。
- en: Identity federation in Unity Catalog
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Unity Catalog 中的身份联合
- en: Whether you’ve deployed a brand new Databricks workspace or you’ve manually
    upgraded an existing workspace to use Unity Catalog, the natural next step is
    to set up new users so that they log in to the Databricks workspace and take advantage
    of the benefits of Unity Catalog.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是部署了全新的 Databricks 工作区，还是手动升级了现有工作区以使用 Unity Catalog，接下来的自然步骤是设置新用户，以便他们登录到
    Databricks 工作区并利用 Unity Catalog 的优势。
- en: Previously, user management in Databricks was managed within each workspace.
    Unity Catalog consolidates user management into a single centralized governance
    pane – the account console. Rather than manage the workspace identities at a workspace
    level, which can get repetitive if the same users have access to more than one
    workspace in a Databricks account, identity management is managed at the account
    level. This allows administrators to define users and their privileges once, and
    easily manage identity roles and permissions at a global level.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Databricks 中，用户管理以前是通过每个工作区来管理的。Unity Catalog 将用户管理集中到一个单一的集中式治理面板——账户控制台中。与其在工作区层级管理工作区身份（如果同一个用户访问多个
    Databricks 工作区，这会变得重复），不如在账户层级管理身份。这样，管理员只需定义一次用户及其权限，并可以在全局层级轻松管理身份角色和权限。
- en: Prior to Unity Catalog, workspace administrators would need to sync organizational
    identities from the identity provider, such as Okta, Ping, or **Azure Active Directory**
    ( **AAD** ). With Unity Catalog, the identities are synced across at the account
    level once using the **System for Cross-domain Identity Management** ( **SCIM**
    ). Unity Catalog will then take care of syncing across the identities to the appropriate
    workspace in a process known as **identity federation** . This allows an organization
    to continue to manage the identities within their organization’s identity provider
    while ensuring that changes are automatically propagated to the individual Databricks
    workspaces.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Unity Catalog 之前，工作区管理员需要从身份提供者（如 Okta、Ping 或 **Azure Active Directory**（**AAD**））同步组织身份。通过
    Unity Catalog，身份会在账户层级通过一次性的**跨域身份管理系统**（**SCIM**）同步。然后，Unity Catalog 会自动处理身份的同步，将其分配到适当的工作区，这个过程被称为**身份联合**。这使得组织可以继续在其组织的身份提供者中管理身份，同时确保更改会自动传播到各个
    Databricks 工作区。
- en: '![Figure 5.4 – Identities are managed at the account console and will automatically
    get federated across multiple Databricks workspaces](img/B22011_05_4.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 身份在账户控制台中管理，并会自动跨多个 Databricks 工作区进行身份联合](img/B22011_05_4.jpg)'
- en: Figure 5.4 – Identities are managed at the account console and will automatically
    get federated across multiple Databricks workspaces
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 身份在账户控制台中管理，并会自动跨多个 Databricks 工作区进行身份联合
- en: 'In fact, the word *administrator* is quite an overloaded term in the context
    of the Databricks Data Intelligence Platform. Let’s take a look at the different
    administrative roles and the level of entitlement each persona has in a Unity
    Catalog-enabled workspace:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，*管理员*在Databricks数据智能平台中是一个负载过重的术语。我们来看一下不同的管理角色，以及每个角色在启用Unity Catalog的工作区中所拥有的权限级别：
- en: '**Account owner** : The individual who originally opened the Databricks account.
    By default, this user will have access to the account console and will be added
    as both an account admin as well as a workspace admin.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**账户所有者**：最初创建Databricks账户的个人。默认情况下，该用户将有权访问账户控制台，并且将被添加为账户管理员和工作区管理员。'
- en: '**Account admin** : A power user who has the privileges to access the account
    console and make account-level changes, such as deploying a new Unity Catalog
    metastore, making network configuration changes, or adding users, groups, and
    service principals to workspaces. This user has the power to grant additional
    account admins and metastore admins.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**账户管理员**：一个拥有访问账户控制台权限并能够进行账户级别更改的高级用户，例如部署新的Unity Catalog元存储、更改网络配置，或将用户、组和服务主体添加到工作区等。此用户有权限授予其他账户管理员和元存储管理员权限。'
- en: '**Metastore admin** : An administrative user who has privileges to make metastore-level
    changes, such as changing catalog ownership, granting access to users to create
    or delete new catalogs, or configuring new datasets shared through the Delta Sharing
    protocol, to name a few. This user does not have access to the account console.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元存储管理员**：一个具有进行元存储级别更改权限的管理员用户，例如更改目录所有权、授予用户创建或删除新目录的权限，或配置通过Delta Sharing协议共享的新数据集等。此用户无法访问账户控制台。'
- en: '**Workspace admin** : An administrative user who has privileges to make workspace-level
    configuration changes, including creating cluster policies and instance pools,
    creating new clusters and DBSQL warehouses, or configuring workspace appearance
    settings, to name a few. This user does not have access to the account console.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作区管理员**：一个具有进行工作区级别配置更改权限的管理员用户，包括创建集群策略和实例池、创建新集群和DBSQL仓库，或配置工作区外观设置等。此用户无法访问账户控制台。'
- en: "![Figure 5.5 – Administrator-level privileges in the \uFEFFDatabricks Data\
    \ Intelligence Platform](img/B22011_05_5.jpg)"
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图5.5 – Databricks数据智能平台中的管理员级别权限](img/B22011_05_5.jpg)'
- en: Figure 5.5 – Administrator-level privileges in the Databricks Data Intelligence
    Platform
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 – Databricks数据智能平台中的管理员级别权限
- en: To begin provisioning new workspace users, you will need to log in to the account
    console, located at [https://accounts.cloud.databricks.com/login](https://accounts.cloud.databricks.com/login)
    . However, only account owners, the individual who originally opened the Databricks
    organization, or account admins will have access to the admin console.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始为新工作区用户配置环境，您需要登录到账户控制台，地址是[https://accounts.cloud.databricks.com/login](https://accounts.cloud.databricks.com/login)。然而，只有账户所有者、最初创建Databricks组织的个人，或账户管理员才能访问管理员控制台。
- en: The first step to begin onboarding new workspace users is to enable **single
    sign-on** ( **SSO** ) in the account console. This can be done by navigating to
    the **Settings** menu of the account console and providing the details of your
    organization’s identity provider. Once you’ve entered the configuration details,
    click on the **Test SSO** button to verify connectivity is successful.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 开始为新工作区用户入职的第一步是启用**单点登录**（**SSO**）功能。在账户控制台的**设置**菜单中提供您组织身份提供商的详细信息即可完成此操作。输入配置信息后，点击**测试SSO**按钮以验证连接是否成功。
- en: '![Figure 5.6 – SSO is required for identity federation to sync with your identity
    provider](img/B22011_05_6.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图5.6 – SSO需要进行身份联合，以便与您的身份提供商同步](img/B22011_05_6.jpg)'
- en: Figure 5.6 – SSO is required for identity federation to sync with your identity
    provider
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – SSO需要进行身份联合，以便与您的身份提供商同步
- en: After the identity provider integration has been verified successfully, the
    next step is to assign users and groups to the appropriate Databricks workspace.
    If you have a single Databricks workspace, then this is a trivial exercise. However,
    if there is more than one Databricks workspace, then it will be up to your organization
    to determine who has access to a particular workspace. You can assign individual
    users and groups to a Databricks workspace by navigating to the account console,
    then clicking on the **User Management** tab from the menu item, and assigning
    users to the appropriate workspace either at the user level or at the group level.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在身份提供者集成成功验证后，下一步是将用户和组分配到相应的Databricks工作区。如果只有一个Databricks工作区，那么这只是一个简单的操作。然而，如果有多个Databricks工作区，则由您的组织决定谁可以访问特定工作区。您可以通过导航到帐户控制台，点击菜单中的**用户管理**选项卡，然后在用户级别或组级别将用户分配到相应的工作区。
- en: Let’s look at how you can promote secure data exploration across your organization.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看如何在组织内推广安全的数据探索。
- en: Data discovery and cataloging
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据发现与目录管理
- en: 'Data tags are useful data cataloging constructs that permit data stewards to
    link descriptive metadata with datasets and other securable objects, such as catalogs,
    volumes, or machine learning models, within Unity Catalog. By attaching descriptive
    tags to datasets and other securable objects, users across your organization can
    search and discover data assets that may be helpful in their day-to-day activities.
    This helps to promote collaboration across teams, saving time and resources by
    not having to recreate similar data assets to reach the completion of a particular
    activity. Unity Catalog supports tags on the following data objects: catalogs,
    databases, tables, volumes, views, and machine learning models.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标签是有用的数据目录构造，允许数据管理员将描述性元数据与数据集和其他可安全访问的对象（如目录、卷或机器学习模型）关联，在Unity Catalog中使用。通过将描述性标签附加到数据集和其他可安全访问的对象，组织内的用户可以搜索和发现可能在日常活动中有用的数据资产。这有助于促进团队之间的协作，通过避免重复创建相似的数据资产来节省时间和资源，从而更快地完成特定活动。Unity
    Catalog支持以下数据对象的标签：目录、数据库、表、卷、视图和机器学习模型。
- en: Let’s look at an example of how we can apply descriptive tags to our existing
    taxi trip datasets that will make it easier for users across our organization
    to search, discover, and use our published datasets in Unity Catalog. Tags can
    be easily added to a table in Unity Catalog from a variety of methods. The easiest
    method is directly from the UI using Catalog Explorer in the Databricks Data Intelligence
    platform. Starting from Catalog Explorer, search for the catalog created in a
    previous chapter’s hands-on exercise that stored data from our DLT pipeline into
    our **yellow_taxi_raw** dataset. Next, expand the schema and select the **yellow_taxi_raw**
    dataset to bring up the dataset details. Finally, click on the **Add tags** button
    to begin adding tag data.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，展示如何将描述性标签应用于我们现有的出租车行程数据集，从而使组织内的用户更容易在Unity Catalog中搜索、发现和使用我们发布的数据集。可以通过多种方法轻松将标签添加到Unity
    Catalog中的表。最简单的方法是通过Databricks数据智能平台中的Catalog Explorer UI直接操作。从Catalog Explorer开始，搜索上一章实际操作中创建的目录，该目录将我们DLT管道中的数据存储到**yellow_taxi_raw**数据集中。接下来，展开架构并选择**yellow_taxi_raw**数据集，查看数据集详情。最后，点击**添加标签**按钮以开始添加标签数据。
- en: '![Figure 5.7 – Tags can be added to datasets directly from Catalog Explorer](img/B22011_05_7.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图5.7 – 标签可以直接从Catalog Explorer添加到数据集中](img/B22011_05_7.jpg)'
- en: Figure 5.7 – Tags can be added to datasets directly from Catalog Explorer
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 – 标签可以直接从Catalog Explorer添加到数据集中
- en: Tags are added as key-value pairs, with the key serving as a unique identifier,
    such as a category, and the value containing the contents that you’d like to assign
    to the securable object. In this case, we’d like to add a few tags to mark the
    data sensitivity of our dataset as well as a tag for the dataset owner. Add a
    few tags of your own choosing and click the **Save tags** button to persist your
    changes to the dataset.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 标签作为键值对添加，其中键作为唯一标识符，例如类别，值则包含您想要分配给可安全访问对象的内容。在此案例中，我们希望添加一些标签来标记数据集的敏感性，以及一个数据集所有者的标签。添加一些您自己选择的标签，并点击**保存标签**按钮，以便将您的更改保存到数据集。
- en: '![Figure 5.8 – Tags are key-value pairs that help distinguish the dataset from
    others in Unity Catalog](img/B22011_05_8.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图5.8 – 标签是键值对，帮助区分Unity Catalog中的数据集](img/B22011_05_8.jpg)'
- en: Figure 5.8 – Tags are key-value pairs that help distinguish the dataset from
    others in Unity Catalog
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 – 标签是帮助区分Unity Catalog中数据集与其他数据集的关键值对
- en: 'Similarly, tag data can be added, changed, or removed using SQL syntax as well.
    In the next example, create a new notebook within your workspace home directory
    in Databricks, and in the first cell of the notebook, add the following SQL statement.
    In this example, we’ll update the dataset description tag of our dataset:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，标签数据也可以使用SQL语法进行添加、更改或移除。在下一个示例中，在您的Databricks工作区主目录中创建一个新的笔记本，并在笔记本的第一个单元格中添加以下SQL语句。在此示例中，我们将更新我们数据集的数据集描述标签：
- en: '[PRE0]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Lastly, tags support finer granularity and can be added down to the column level
    for datasets in Unity Catalog. This is useful in scenarios when you might want
    to distinguish the data sensitivity of a column so that you can dynamically apply
    a data mask for a view in Unity Catalog.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，标签支持更精细的粒度，并且可以添加到Unity Catalog中数据集的列级别。在这种情况下非常有用，当您需要区分列的数据敏感性时，可以动态地为Unity
    Catalog中的视图应用数据蒙版。
- en: '![Figure 5.9 – Tags can be added at the column level for datasets in Unity
    Catalog](img/B22011_05_9.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图5.9 – 标签可以添加到Unity Catalog中数据集的列级别](img/B22011_05_9.jpg)'
- en: Figure 5.9 – Tags can be added at the column level for datasets in Unity Catalog
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 – 标签可以添加到Unity Catalog中数据集的列级别
- en: 'Conversely, users can search for views, tables, or columns that have tags applied
    by using the following syntax in the **Search** text field of Catalog Explorer:
    **tag:<case_sensitive_name_of_tag>** .'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，用户可以使用目录资源管理器的**搜索**文本字段中的以下语法搜索应用了标签的视图、表或列：**tag:<区分大小写的标签名称>**。
- en: As you can see, tags are extremely useful in helping promote the discoverability
    of datasets across your organization and help users distinguish datasets in Unity
    Catalog from one another.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，标签在帮助促进组织中数据集的可发现性方面非常有用，并帮助用户区分Unity Catalog中的数据集。
- en: In addition to discovering datasets across your organization, it’s also imperative
    to know how a dataset is formed and whether upstream sources are trusted. Data
    lineage is one such method for users to know exactly how the datasets they discover
    in Unity Catalog are formed and where the different columns originate from.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 除了发现组织中的数据集之外，了解数据集如何形成以及上游源是否可信也是至关重要的。数据血统是用户了解他们在Unity Catalog中发现的数据集确切形成方式以及各列来源的一种方法。
- en: Tracking dataset relationships using lineage
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用血统追踪跟踪数据集关系
- en: As data is transformed in your lakehouse by data pipelines, the contents of
    your organization’s datasets can go through a series of evolutions by a variety
    of processes. This can include processes such as data cleansing, data type casting,
    column transformation, or data enrichment, to name a few. As you can imagine,
    the data can deviate quite far from when it was originally ingested from its originating
    source. It’s important for downstream consumers of the data in your lakehouse
    to be able to verify the validity of your datasets. Data lineage is one mechanism
    for such validation by allowing users to trace the origin of tables and columns
    so that you can ensure that you are using data from trusted sources.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据通过数据管道在您的湖屋中进行转换时，组织数据集的内容可以通过多种过程经历一系列的演变。这可能包括数据清洗、数据类型转换、列转换或数据增强等过程。可以想象，数据与最初从其源头摄取时相比，可能会有很大的偏差。对于使用您湖屋中数据的下游消费者来说，能够验证数据集的有效性非常重要。数据血统是一种验证机制，允许用户跟踪表和列的来源，从而确保您正在使用来自可信源的数据。
- en: '![Figure 5.10 – A lakehouse table may be the result of a combination of multiple
    upstream tables](img/B22011_05_10.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图5.10 – 一个湖屋表可能是多个上游表组合的结果](img/B22011_05_10.jpg)'
- en: Figure 5.10 – A lakehouse table may be the result of a combination of multiple
    upstream tables
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 – 一个湖屋表可能是多个上游表组合的结果
- en: 'Data lineage can be viewed from a variety of mechanisms in the Databricks Data
    Intelligence Platform:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 数据血统可以从Databricks数据智能平台的多种机制中查看：
- en: Directly from Catalog Explorer by viewing the lineage graph
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过查看血统图直接从目录资源管理器获取
- en: Retrieved using the Lineage Tracking REST API
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Lineage Tracking REST API检索
- en: Querying the Lineage Tracking system tables in Unity Catalog
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Unity Catalog中查询Lineage Tracking系统表
- en: Let’s look at how we might be able to trace the origin of a few columns in our
    downstream table to the upstream sources in Databricks. If you haven’t already
    done so, you can clone this chapter’s sample notebooks from the GitHub repository
    located at [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter05](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter05)
    . Begin by importing the sample notebook titled **Data Lineage Demo.sql** into
    your Databricks workspace. Attach the notebook to a running all-purpose cluster
    and execute all of the notebook cells. The notebook will generate two tables –
    a parent table and a child table whose columns are constructed from the upstream
    parent table.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何追踪下游表中的一些列的来源，追溯到 Databricks 中的上游数据源。如果你还没有这样做，你可以从 GitHub 仓库克隆本章的示例笔记本，地址为
    [https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter05](https://github.com/PacktPublishing/Building-Modern-Data-Applications-Using-Databricks-Lakehouse/tree/main/chapter05)。首先，将名为**Data
    Lineage Demo.sql**的示例笔记本导入你的 Databricks 工作区。将该笔记本附加到一个正在运行的多用途集群，并执行所有笔记本单元格。该笔记本将生成两个表——一个父表和一个子表，其列是从上游父表中构建的。
- en: Once the notebook has been executed and the tables have been saved to Unity
    Catalog, navigate to Catalog Explorer by clicking the **Catalog** menu item from
    the left-hand navigation menu. From Catalog Explorer, search for the child table
    by entering the table name in the **Search** text field. Click on the child table
    to reveal the table details. Finally, click on the blue button titled **See lineage
    graph** to generate a lineage diagram. You’ll notice that the diagram clearly
    depicts the relationship between the two data assets – the parent table and the
    child table. Next, click on the column titled **car_description** in the child
    table. You’ll notice that the lineage diagram is updated, clearly illustrating
    which columns from the parent table are used to construct this column in the downstream
    child table.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦笔记本执行完成并且表已保存至 Unity Catalog，请通过点击左侧导航菜单中的**Catalog**菜单项进入 Catalog Explorer。在
    Catalog Explorer 中，输入表名到**搜索**文本框来查找子表。点击子表以查看表的详细信息。最后，点击标有**查看 lineage 图**的蓝色按钮来生成
    lineage 图。你会注意到，图中清晰地展示了两个数据资产——父表和子表之间的关系。接下来，点击子表中标为**car_description**的列。你会看到
    lineage 图已更新，清晰地说明了父表中的哪些列被用来构建下游子表中的此列。
- en: '![Figure 5.11 – The table lineage graph can be viewed directly from Catalog
    Explorer](img/B22011_05_11.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.11 – 可以直接从 Catalog Explorer 查看表的 lineage 图](img/B22011_05_11.jpg)'
- en: Figure 5.11 – The table lineage graph can be viewed directly from Catalog Explorer
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – 可以直接从 Catalog Explorer 查看表的 lineage 图
- en: In fact, thanks to the unification nature of Unity Catalog, data lineage can
    be used to trace data relationships across multiple workspaces. Furthermore, data
    lineage will capture relationship information in near-real time, so that users
    can always have an up-to-date view of dataset relationships no matter the Databricks
    workspace they are using.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，得益于 Unity Catalog 的统一特性，数据 lineage 可以用于追踪跨多个工作区的数据关系。此外，数据 lineage 会实时捕捉关系信息，确保用户无论使用哪个
    Databricks 工作区，都能始终获得最新的数据集关系视图。
- en: Observability with system tables
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用系统表进行可观察性
- en: Strong auditing and system observability is another core strength of Unity Catalog
    and is implemented in Databricks using system tables. System tables are a set
    of read-only tables in a Databricks workspace that capture the operational data
    about activities within your Databricks workspaces. Furthermore, systems tables
    record data across all workspaces within a Databricks account, serving as a single
    source of truth to retrieve operational data pertaining to your Databricks workspaces.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的审计和系统可观察性是 Unity Catalog 的另一大核心优势，它通过系统表在 Databricks 中实现。系统表是 Databricks
    工作区中的一组只读表，用于捕捉关于活动操作的数据。此外，系统表会记录 Databricks 账户中所有工作区的数据，作为获取与 Databricks 工作区相关的操作数据的单一真实数据源。
- en: 'System tables record observability information about the following aspects
    of your Databricks workspaces (the latest list of available system tables can
    be found at [https://docs.databricks.com/en/admin/system-tables/index.html#which-system-tables-are-available](https://docs.databricks.com/en/admin/system-tables/index.html#which-system-tables-are-available)
    ):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 系统表记录关于你 Databricks 工作区以下方面的可观察性信息（最新的系统表可用列表可以在 [https://docs.databricks.com/en/admin/system-tables/index.html#which-system-tables-are-available](https://docs.databricks.com/en/admin/system-tables/index.html#which-system-tables-are-available)
    中找到）：
- en: '| **Category** | **Service** | **Description** |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **服务** | **描述** |'
- en: '| System billing | Billable usage | Captures billing information about utilized
    computational resources such as warehouses and clusters |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 系统计费 | 可计费使用 | 捕获关于使用的计算资源（如仓库和集群）的计费信息 |'
- en: '| Pricing | Captures historical changes to system service pricing (or **stock-keeping**
    **unit** ( **SKU** )) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 定价 | 捕获系统服务定价的历史变化（或 **库存单元**（**SKU**）） |'
- en: '| System access | System audit | Contains event data from workspace services
    including jobs, workflows, clusters, notebooks, repos, secrets, and more |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 系统访问 | 系统审计 | 包含来自工作区服务的事件数据，包括作业、工作流、集群、笔记本、仓库、密钥等 |'
- en: '| Table lineage | Captures data about reads/writes to and from a table in Unity
    Catalog |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 表格血缘 | 捕获 Unity Catalog 中表格的读写数据 |'
- en: '| Column lineage information | Captures data about reads/writes to and from
    a column within a Unity Catalog table |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 列血缘信息 | 捕获 Unity Catalog 表格中列的读写数据 |'
- en: '| Compute | Clusters | Captures information about clusters – for example, configuration
    changes over time |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 计算 | 集群 | 捕获关于集群的信息——例如，随着时间推移的配置变化 |'
- en: '| Node type information | Includes hardware information about cluster node
    types |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 节点类型信息 | 包含关于集群节点类型的硬件信息 |'
- en: '| SQL warehouse events | Captures changes made to SQL warehousing such as scaling
    events |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| SQL 仓库事件 | 捕获对 SQL 仓库所做的更改，例如扩展事件 |'
- en: '| System storage | Predictive optimizations | Captures predicative I/O optimizations
    as they occur during data processing |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 系统存储 | 预测优化 | 捕获在数据处理过程中发生的预测 I/O 优化 |'
- en: '| Marketplace | Marketplace funnel events | Captures marketplace analytical
    information such as number of impressions and funnel data about dataset listing
    |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 市场 | 市场漏斗事件 | 捕获市场分析信息，例如展示次数和关于数据集列表的漏斗数据 |'
- en: '| Marketplace listing events | Records marketplace consumer information about
    dataset listings |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 市场列表事件 | 记录关于数据集列表的市场消费者信息 |'
- en: Table 5.1 – System tables will record operational information across various
    parts of the Databricks Data Intelligence Platform
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.1 – 系统表将记录 Databricks 数据智能平台各个部分的操作信息
- en: 'Like all tables in Unity Catalog, there is no access to the system tables by
    default. Instead, a metastore administrator will need to grant read access ( **SELECT**
    permissions) to these tables to the appropriate users and groups. For example,
    to grant permissions for department leaders to track their warehouse scaling events
    throughout the workday, a metastore admin would need to explicitly grant permissions
    for the group, **dept_leads** , to query the SQL warehouse system table:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Unity Catalog 中的所有表格一样，系统表默认是不可访问的。相反，元存储管理员需要授予适当用户和组对这些表的读取权限（**SELECT**
    权限）。例如，为了授权部门领导在工作日跟踪他们的仓库扩展事件，元存储管理员需要明确授权给组 **dept_leads** 查询 SQL 仓库系统表的权限：
- en: '[PRE1]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can imagine, very active Databricks workspaces will record many events
    throughout the day, and over time, these tables can grow to be quite large. To
    prevent observability information from accumulating to the point of creating a
    large cloud storage bill, instead, the system information will only be retained
    for a maximum of one year within your Databricks account.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所想象的那样，非常活跃的 Databricks 工作区将在一天中记录许多事件，随着时间的推移，这些表格可能会变得非常庞大。为了防止可观察性信息积累到产生巨大的云存储账单，系统信息将在你的
    Databricks 账户中最多保留一年。
- en: For use cases where the auditing information is required to be retained in the
    order of many years, you will need to set up a secondary process to copy the system
    information into a long-term archival system, for example. Tracking changes to
    datasets is critical to ensuring strong observability in your lakehouse. Another
    strong suite of Unity Catalog is that observability extends beyond just datasets
    and covers all objects that can be secured under Unity Catalog governance.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要保存多年的审计信息的使用场景，你需要设置一个次要流程，将系统信息复制到长期归档系统中。例如，跟踪数据集的变化对于确保湖仓的强大可观察性至关重要。Unity
    Catalog 的另一个强大功能是可观察性不仅仅局限于数据集，还涵盖了所有可以在 Unity Catalog 管理下保护的对象。
- en: Tracing the lineage of other assets
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 追溯其他资产的血缘关系
- en: As previously mentioned, Unity Catalog implements a single governance solution
    over your organization’s data assets, which extends beyond just tables. With Unity
    Catalog, you can trace the lineage of other data assets such as workflows, notebooks,
    and machine learning models, to name a few.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Unity Catalog 实现了一个统一的治理解决方案，涵盖了组织的数据资产，不仅仅是表格。通过 Unity Catalog，你可以追溯其他数据资产的血缘关系，比如工作流、笔记本和机器学习模型等。
- en: Let’s turn our attention to how Unity Catalog can dynamically generate different
    result sets to queries by evaluating the user and group permissions of a given
    Databricks user.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们关注 Unity Catalog 如何通过评估给定 Databricks 用户的用户和组权限，动态生成不同的查询结果集。
- en: Fine-grained data access
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 细粒度数据访问
- en: 'Dynamic views are a special type of view within the Databricks Data Intelligence
    Platform that provides data administrators with the ability to control fine-grained
    access to data within a dataset. For example, administrators can specify which
    rows and columns a particular individual may have access to depending upon their
    group membership. The Databricks Data Intelligence Platform introduces several
    built-in functions for dynamically evaluating group membership when a particular
    user queries the contents of a view:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 动态视图是 Databricks 数据智能平台中的一种特殊类型的视图，它为数据管理员提供了在数据集内控制细粒度数据访问的能力。例如，管理员可以根据用户的组成员身份指定某个特定个人可以访问哪些行和列。Databricks
    数据智能平台引入了几个内置函数，用于在特定用户查询视图内容时动态评估组成员身份：
- en: '**current_user()** returns the email address of the user querying the view'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**current_user()** 返回查询视图的用户的电子邮件地址。'
- en: '**is_member()** returns a Boolean ( **True** or **False** ) of whether the
    user querying a view is a member of a Databricks workspace-level group'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**is_member()** 返回一个布尔值（**True** 或 **False**），表示查询视图的用户是否是 Databricks 工作区级别组的成员。'
- en: '**is_account_group_member()** returns a Boolean ( **True** or **False** ) of
    whether the user querying a view is a member of a Databricks account-level group
    (as opposed to a workspace-level group)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**is_account_group_member()** 返回一个布尔值（**True** 或 **False**），表示查询视图的用户是否是 Databricks
    账户级别组的成员（而不是工作区级别组的成员）。'
- en: Important note
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: For dynamic views created against tables and views in Unity Catalog, it’s recommended
    to use the **is_account_group_member()** function to evaluate a user’s membership
    to a group as it will evaluate group membership at the Databricks account level.
    On the other hand, the **is_member()** function will evaluate a user’s membership
    to a group that is local to a particular workspace and may provide false or unintended
    results.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在 Unity Catalog 中创建的针对表和视图的动态视图，建议使用 **is_account_group_member()** 函数来评估用户是否是某个组的成员，因为它会在
    Databricks 账户级别评估组成员身份。另一方面，**is_member()** 函数将评估用户是否是某个特定工作区本地组的成员，可能会提供错误或意外的结果。
- en: Furthermore, dynamic views also enable data administrators to obfuscate specific
    column values so that sensitive data is not exfiltrated by accident. Using built-in
    Spark SQL functions such as **concat()** , **regexp_extract()** , or even **lit()**
    is a simple yet powerful tool for protecting the contents of the most sensitive
    datasets on the Databricks platform.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，动态视图还允许数据管理员模糊化特定列的值，以防敏感数据被意外泄露。使用内置的 Spark SQL 函数，如 **concat()**、**regexp_extract()**，甚至是
    **lit()**，是一种简单而强大的工具，用于保护 Databricks 平台上最敏感数据集的内容。
- en: In the next section, we’ll look at how we can leverage dynamic views to permit
    members of a data science team to perform ad hoc data wrangling of a sensitive
    dataset while simultaneously protecting the contents of columns with **personally
    identifiable information** ( **PII** ) data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，我们将查看如何利用动态视图，允许数据科学团队的成员对敏感数据集进行临时数据清洗，同时保护包含**个人可识别信息**（**PII**）数据的列内容。
- en: Hands-on example – data masking healthcare datasets
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实操示例 – 数据屏蔽医疗保健数据集
- en: In this example, we’ll be creating a dynamic view to restrict data access to
    certain rows and columns within a dataset. We’ll be using the COVID sample dataset
    located within the Databricks datasets at **/databricks-datasets/COVID/covid-19-data/us-counties.csv**
    . The dataset contains COVID-19 infection data for US counties during the 2020
    global pandemic. Since this dataset can contain sensitive data, we’ll apply a
    simple data mask to prevent the exposure of sensitive data to non-privileged users.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们将创建一个动态视图，限制对数据集中特定行和列的访问。我们将使用位于 Databricks 数据集中的 COVID 示例数据集，路径为**/databricks-datasets/COVID/covid-19-data/us-counties.csv**。该数据集包含了2020年全球疫情期间美国各县的
    COVID-19 感染数据。由于该数据集可能包含敏感数据，我们将应用一个简单的数据屏蔽，防止将敏感数据暴露给非特权用户。
- en: 'Let’s start by defining a few global variables, as well as the catalog and
    schema that will hold our dynamic views:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先定义一些全局变量，以及将用于存储动态视图的目录和模式：
- en: '[PRE2]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we’ll need to define a persistent table object in Unity Catalog that
    we will use to create views. Let’s start by creating a new table using the sample
    US Counties COVID dataset:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要在 Unity Catalog 中定义一个持久化表对象，用于创建视图。让我们开始使用示例的美国县级 COVID 数据集创建一个新表：
- en: '[PRE3]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, let’s query the newly created table in Unity Catalog. Note that all columns
    and rows are returned since we didn’t specify any qualifying criteria that would
    filter the data:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们查询 Unity Catalog 中新创建的表。请注意，由于我们没有指定任何筛选条件，所有列和行都将被返回：
- en: '[PRE4]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let’s create a view that will dynamically evaluate the querying user’s group
    membership in Unity Catalog. In this case, we want to restrict access to certain
    columns if the user is not a member of the **admins** group. Based upon the group
    membership, we can give access to a user or we could limit access to the data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个视图，动态评估查询用户在 Unity Catalog 中的组成员身份。在此情况下，我们希望如果用户不是**admins**组的成员，则限制对某些列的访问。根据组成员身份，我们可以授予用户访问权限，或者限制其访问数据。
- en: 'Let’s also leverage a built-in Spark SQL function to apply a simple yet powerful
    data mask to sensitive data columns, allowing only privileged members of the **admins**
    group access to view the text:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还利用内置的 Spark SQL 函数，对敏感数据列应用一个简单而强大的数据屏蔽，仅允许**admins**组的特权成员查看文本：
- en: '[PRE5]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In the previous view definition, we’ve limited data access to a particular
    set of columns within the US Counties COVID dataset. Using dynamic views, we can
    also limit access to a particular set of rows using a query predicate. In the
    final view definition, we’ll limit which US states a particular user can view
    based on a user’s membership to the **admins** group:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的视图定义中，我们已将数据访问限制为美国县级 COVID 数据集中某些特定列。使用动态视图，我们还可以通过查询谓词限制对特定行的访问。在最终的视图定义中，我们将根据用户是否属于**admins**组，限制用户可以查看哪些美国州：
- en: '[PRE6]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, members of other groups can perform ad hoc data exploration and other
    data experimentation. However, we won’t inadvertently expose any sensitive healthcare
    data. For example, let’s imagine that there is another group called **data-science**
    . This group can query the dynamic view, but the results will be different from
    if a member of the **admins** group queried the view. For example, the following
    aggregation will return different result sets depending on whether a user is in
    the **admins** group or the **data-science** group:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，其他组的成员可以进行临时数据探索和其他数据实验。然而，我们不会无意中暴露任何敏感的医疗保健数据。例如，假设有一个名为**data-science**的组。该组可以查询动态视图，但其结果将与**admins**组成员查询视图时的结果不同。例如，以下聚合查询将返回不同的结果集，具体取决于用户是否属于**admins**组或**data-science**组：
- en: '[PRE7]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We get the following results:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下结果：
- en: '![Figure 5.12 – Dynamic views can generate customized results based on group
    membership](img/B22011_05_12.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.12 – 动态视图可以根据组成员身份生成定制化的结果](img/B22011_05_12.jpg)'
- en: Figure 5.12 – Dynamic views can generate customized results based on group membership
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 – 动态视图可以根据组成员身份生成定制化的结果
- en: By now, you should be able to realize the power of dynamic views within the
    Databricks Data Intelligence Platform. With just a few built-in functions, we
    can implement strong data governance across various users and groups interacting
    with your organization’s data in the l akehouse.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该能够意识到 Databricks 数据智能平台中动态视图的强大功能。仅凭几个内置函数，我们就能在湖仓中实现强大的数据治理，管理不同用户和团队与组织数据的互动。
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the challenges specific to lakehouse data governance
    and how Unity Catalog solves these challenges. We also covered how to enable Unity
    Catalog within an existing Databricks workspace and how metastore admins can establish
    connections with external data sources. Lastly, we covered techniques for discovering
    and cataloging data assets within the lakehouse and how annotating data assets
    with metadata tags can create a searchable and well-organized data catalog.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了湖仓数据治理的具体挑战以及 Unity Catalog 如何解决这些挑战。我们还介绍了如何在现有的 Databricks 工作空间中启用
    Unity Catalog，以及如何通过元存储管理员与外部数据源建立连接。最后，我们讲解了在湖仓中发现和编目数据资产的技术，以及如何通过元数据标签注释数据资产，从而创建一个可搜索且井井有条的数据目录。
- en: In the next chapter, we’ll explore how to effectively manage input and output
    data locations using Unity Catalog. You’ll learn how to govern data access across
    various roles and departments within an organization, ensuring security and auditability
    within the Databricks Data Intelligence Platform.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何使用 Unity Catalog 有效管理输入和输出数据位置。你将学习如何在组织内跨角色和部门治理数据访问，确保 Databricks
    数据智能平台中的安全性和可审计性。
