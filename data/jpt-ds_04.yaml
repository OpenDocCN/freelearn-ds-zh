- en: Data Mining and SQL Queries
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据挖掘与 SQL 查询
- en: PySpark exposes the Spark programming model to Python. Spark is a fast, general
    engine for large-scale data processing. We can use Python under Jupyter. So, we
    can use Spark in Jupyter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 将 Spark 编程模型暴露给 Python。Spark 是一个快速的、通用的大规模数据处理引擎。我们可以在 Jupyter 中使用 Python。因此，我们可以在
    Jupyter 中使用 Spark。
- en: 'Installing Spark requires the following components to be installed on your
    machine:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 Spark 需要在你的机器上安装以下组件：
- en: Java JDK.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java JDK。
- en: Scala from [http://www.scala-lang.org/download/](http://www.scala-lang.org/download/).
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 [http://www.scala-lang.org/download/](http://www.scala-lang.org/download/)
    下载 Scala。
- en: Python recommend downloading Anaconda with Python (from [http://continuum.io](http://continuum.io)).
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐从 [http://continuum.io](http://continuum.io) 下载带 Python 的 Anaconda。
- en: Spark from [https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html).
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 [https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)
    下载 Spark。
- en: '`winutils`: This is a command-line utility that exposes Linux commands to Windows.
    There are 32-bit and 64-bit versions available at:'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`winutils`：这是一个命令行工具，它将 Linux 命令暴露给 Windows。32 位和 64 位版本可以在以下链接找到：'
- en: 32-bit `winutils.exe` at [https://code.google.com/p/rrd-hadoop-win32/source/checkout](https://code.google.com/p/rrd-hadoop-win32/source/checkout)
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 32 位 `winutils.exe` 可在 [https://code.google.com/p/rrd-hadoop-win32/source/checkout](https://code.google.com/p/rrd-hadoop-win32/source/checkout)
    下载
- en: 64-bit `winutils.exe` at [https://github.com/steveloughran/winutils/tree/master/hadoop-2.6.0/bin](https://github.com/steveloughran/winutils/tree/master/hadoop-2.6.0/bin)
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 64 位 `winutils.exe` 可在 [https://github.com/steveloughran/winutils/tree/master/hadoop-2.6.0/bin](https://github.com/steveloughran/winutils/tree/master/hadoop-2.6.0/bin)
    下载
- en: 'Then set environment variables that show the position of the preceding components:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然后设置环境变量，指明前述组件的位置：
- en: '`JAVA_HOME`: The bin directory where you installed JDK'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`JAVA_HOME`：你安装 JDK 的 bin 目录'
- en: '`PYTHONPATH`: Directory where Python was installed'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PYTHONPATH`：Python 安装目录'
- en: '`HADOOP_HOME`: Directory where `winutils` resides'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HADOOP_HOME`：`winutils` 所在的目录'
- en: '`SPARK_HOME`: Where Spark is installed'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SPARK_HOME`：Spark 安装的位置'
- en: These components are readily available over the internet for a variety of operating
    systems. I have successfully installed these previous components in a Windows
    environment and a Mac environment.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件可以通过互联网轻松获得，适用于各种操作系统。我已经在 Windows 和 Mac 环境下成功安装了这些组件。
- en: 'Once you have these installed you should be able to run the command, `pyspark`,
    from a command line window and a Jupyter Notebook with Python (with access to
    Spark) can be used. In my installation I used the command:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你安装了这些组件，你应该能够在命令行窗口中运行命令 `pyspark`，并可以使用 Python（并且能够访问 Spark）的 Jupyter Notebook。在我的安装中，我使用了以下命令：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As I had installed Spark in the root with the `\spark` directory. Yes, `pyspark`
    is a built-in tool for use by Spark.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我将 Spark 安装在根目录的 `\spark` 目录下。是的，`pyspark` 是 Spark 的内置工具。
- en: Special note for Windows installation
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Windows 安装的特别说明
- en: Spark (really Hadoop) needs a temporary storage location for its working set
    of data. Under Windows this defaults to the `\tmp\hive` location. If the directory
    does not exist when Spark/Hadoop starts it will create it. Unfortunately, under
    Windows, the installation does not have the correct tools built-in to set the
    access privileges to the directory.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Spark（其实是 Hadoop）需要一个临时存储位置来存放其工作数据集。在 Windows 上，默认位置是 `\tmp\hive`。如果在 Spark/Hadoop
    启动时该目录不存在，它会自动创建。不幸的是，在 Windows 上，安装程序并未内置正确的工具来设置该目录的访问权限。
- en: You should be able to run `chmod` under `winutils` to set the access privileges
    for the `hive` directory. However, I have found that the `chmod` function does
    not work correctly.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能够在 `winutils` 中运行 `chmod` 来设置 `hive` 目录的访问权限。然而，我发现 `chmod` 功能并未正确运行。
- en: A better idea has been to create the `tmp\hive` directory yourself in admin
    mode. And then grant full privileges to the hive directory to all users, again
    in admin mode.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更好的做法是以管理员模式自行创建 `tmp\hive` 目录。然后，再次以管理员模式将该目录的完全权限授予所有用户。
- en: Without this change, Hadoop fails right away. When you start `pyspark`, the
    output (including any errors) are displayed in the command line window. One of
    the errors will be insufficient access to this directory.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有这个更改，Hadoop 会立刻失败。当你启动 `pyspark` 时，输出（包括任何错误）会显示在命令行窗口中。其中一个错误会是该目录的访问权限不足。
- en: Using Spark to analyze data
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spark 分析数据
- en: The first thing to do in order to access Spark is to create a `SparkContext`.
    The `SparkContext` initializes all of Spark and sets up any access that may be
    needed to Hadoop, if you are using that as well.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 Spark 的第一步是创建一个 `SparkContext`。`SparkContext` 初始化所有 Spark 并设置对 Hadoop 的任何访问（如果你也在使用
    Hadoop）。
- en: The initial object used to be a `SQLContext`, but that has been deprecated recently
    in favor of `SparkContext`, which is more open-ended.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最初使用的对象是 `SQLContext`，但最近已被弃用，推荐使用 `SparkContext`，它更为开放。
- en: 'We could use a simple example to just read through a text file as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一个简单的示例来读取文本文件，如下所示：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In this example:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中：
- en: We obtain a `SparkContext`
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们获取一个 `SparkContext`
- en: With the context, read in a file (the Jupyter file for this example)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用上下文，读取一个文件（本示例中的 Jupyter 文件）
- en: We use a Hadoop `map` function to split up the text file into different lines
    and gather the lengths
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 Hadoop `map` 函数将文本文件拆分为不同的行并收集行的长度。
- en: We use a Hadoop `reduce` function to calculate the length of all the lines
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 Hadoop `reduce` 函数计算所有行的长度。
- en: We display our results
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们展示我们的结果
- en: 'Under Jupyter this looks like the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter 中，效果如下所示：
- en: '![](img/bbaf558a-ad63-46c7-ae00-83fe90b04571.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bbaf558a-ad63-46c7-ae00-83fe90b04571.png)'
- en: Another MapReduce example
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另一个 MapReduce 示例
- en: 'We can use MapReduce in another example where we get the word counts from a
    file. A standard problem, but we use MapReduce to do most of the heavy lifting.
    We can use the source code for this example. We can use a script similar to this
    to count the word occurrences in a file:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在另一个示例中使用 MapReduce，从文件中获取单词计数。这是一个标准问题，但我们使用 MapReduce 来完成大部分繁重的工作。我们可以使用此示例的源代码。我们可以使用类似这样的脚本来计算文件中单词的出现次数：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We have the same preamble to the coding.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有相同的代码前言。
- en: Then we load the text file into memory.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将文本文件加载到内存中。
- en: '`text_file` is a Spark **RDD** (**Resilient Distributed Dataset**), not a data
    frame.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`text_file` 是一个 Spark **RDD**（**弹性分布式数据集**），而不是数据框。'
- en: It is assumed to be massive and the contents distributed over many handlers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 假设文件是巨大的，且内容分布在多个处理程序中。
- en: Once the file is loaded we split each line into words, and then use a `lambda`
    function to tick off each occurrence of a word. The code is truly creating a new
    record for each word occurrence, such as *at appears 1*, *at appears 1*. For example,
    if the word *at* appears twice each occurrence would have a record added like
    *at* *appears 1*. The idea is to not aggregate results yet, just record the appearances
    that we see. The idea is that this process could be split over multiple processors
    where each processor generates these low-level information bits. We are not concerned
    with optimizing this process at all.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文件加载完成，我们将每行拆分成单词，然后使用 `lambda` 函数标记每个单词的出现。代码实际上为每个单词出现创建了一个新记录，例如 *at appears
    1*，*at appears 1*。例如，如果单词 *at* 出现了两次，每次出现都会添加一个记录，像 *at* *appears 1*。这个想法是暂时不聚合结果，只记录我们看到的出现情况。这个过程可以被多个处理器分担，每个处理器生成这些低级信息。我们并不关心优化这个过程。
- en: Once we have all of these records we reduce/summarize the record set according
    to the word occurrences mentioned.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有所有这些记录，我们就根据提到的单词出现次数来减少/汇总记录集。
- en: The `counts` object is also RDD in Spark. The last `for` loop runs a `collect()`
    against the RDD. As mentioned, this RDD could be distributed among many nodes.
    The `collect()` function pulls in all copies of the RDD into one location. Then
    we loop through each record.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`counts` 对象在 Spark 中也是 RDD。最后的 `for` 循环会对 RDD 执行 `collect()`。如前所述，这个 RDD 可能分布在多个节点上。`collect()`
    函数会将 RDD 的所有副本拉到一个位置。然后我们遍历每条记录。'
- en: 'When we run this in Jupyter we see something akin to this display:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在 Jupyter 中运行时，我们会看到类似这样的显示：
- en: '![](img/f0c88072-d9fa-4076-a14c-502f89c2dca1.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0c88072-d9fa-4076-a14c-502f89c2dca1.png)'
- en: The listing is abbreviated as the list of words continues for some time.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 该列表被简化，因为单词列表会继续一段时间。
- en: The previous example doesn't work well with Python 3\. There is a workaround
    when coding directly in Python, but not for Jupyter.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例在 Python 3 中效果不佳。直接在 Python 中编写代码时有解决方法，但在 Jupyter 中没有。
- en: Using SparkSession and SQL
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SparkSession 和 SQL
- en: 'Spark exposes many SQL-like actions that can be taken upon a data frame. For
    example, we could load a data frame with product sales information in a CSV file:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了许多类似 SQL 的操作，可以对数据框执行。例如，我们可以加载一个包含产品销售信息的 CSV 文件数据框：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The example:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: Starts a `SparkSession` (needed for most data access)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动一个 `SparkSession`（大多数数据访问需要）
- en: Uses the session to read a CSV formatted file, that contains a header record
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用会话读取一个包含头部记录的CSV格式文件
- en: Displays initial rows
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示初始行
- en: '![](img/3b841e8d-1388-4d3e-b9e1-295886ea97fa.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b841e8d-1388-4d3e-b9e1-295886ea97fa.png)'
- en: 'We have a few interesting columns in the sales data:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在销售数据中，我们有一些有趣的列：
- en: Actual sales for the products by division
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按部门划分的产品实际销售额
- en: Predicted sales for the products by division
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按部门划分的产品预测销售额
- en: 'If this were a bigger file, we could use SQL to determine the extent of the
    product list. Then the following is the Spark SQL to determine the product list:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是一个更大的文件，我们可以使用SQL来确定产品列表的范围。接下来是用Spark SQL确定产品列表的代码：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The data frame `groupBy` function works very similar to the SQL `Group By` clause.
    `Group By` collects the items in the dataset according to the values in the column
    specified. In this case the `PRODUCT` column. The `Group By` results in a dataset
    being established with the results. As a dataset, we can query how many rows are
    in each with the `count()` function.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框的`groupBy`函数与SQL中的`Group By`子句非常相似。`Group By`根据指定列中的值将数据集中的项目进行分组。在本例中是`PRODUCT`列。`Group
    By`的结果是生成一个包含结果的数据集。作为数据集，我们可以使用`count()`函数查询每个组中的行数。
- en: 'So, the result of the `groupBy` is a count of the number of items that correspond
    to the grouping element. For example, we grouped the items by `CHAIR` and found
    288 of them:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，`groupBy`的结果是对应分组元素的项目数量。例如，我们按`CHAIR`分组，发现有288个这样的项目：
- en: '![](img/3da579b9-b3d4-4abd-b655-a021a33f727a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3da579b9-b3d4-4abd-b655-a021a33f727a.png)'
- en: So, we obviously do not have real product data. It is unlikely that any company
    has the exact same number of products in each line.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，显然我们没有真实的产品数据。任何公司不太可能在每个产品线中有完全相同数量的产品。
- en: 'We can look into the dataset to determine how the different divisions performed
    in actual versus predicted sales using the `filter()` command in this example:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看数据集，以使用`filter()`命令查看不同部门在实际销售与预测销售方面的表现，示例如下：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We pass a logical test to the `filter` command that will be operated against
    every row in the dataset. If the data in that row passes the test then the row
    is returned. Otherwise, the row is dropped from the results.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将一个逻辑测试传递给`filter`命令，该命令将在数据集的每一行上执行。如果该行数据通过测试，则返回该行；否则，该行会被从结果中删除。
- en: Our test is only interested in sales where the actual sales figure exceeds the
    predicted values.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试只关心实际销售额超过预测值的情况。
- en: 'Under Jupyter this looks as like the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jupyter下，显示如下：
- en: '![](img/9ffb9eb1-9a13-497c-992c-63465c614db1.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ffb9eb1-9a13-497c-992c-63465c614db1.png)'
- en: So, we get a reduced result set. Again, this was produced by the `filter` function
    as a data frame and can then be called upon to `show` as any other data frame.
    Notice the third record from the previous display is not present as its actual
    sales were less than predicted. It is always a good idea to use a quick survey
    to make sure you have the correct results.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到了一个缩小的结果集。再次强调，这个结果是由`filter`函数生成的一个数据框，并可以像其他数据框一样调用`show`。注意，之前显示的第三条记录不再出现，因为它的实际销售额低于预测值。通常，做一个快速调查是个好主意，以确保得到正确的结果。
- en: What if we wanted to pursue this further and determine which were the best performing
    areas within the company?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想进一步追踪，找出公司内表现最佳的区域该怎么办？
- en: If this were a database table we could create another column that stored the
    difference between actual and predicted sales and then sort our display on that
    column. We can perform very similar steps in Spark.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是一个数据库表格，我们可以创建另一列，存储实际销售与预测销售之间的差异，然后根据这一列对结果进行排序。在Spark中我们可以执行类似的操作。
- en: 'Using a data frame we could use coding like this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据框，我们可以用类似这样的代码：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The first statement is creating a view/data frame within the context for further
    manipulation. This view is lazy evaluated, will not persist unless specific steps
    are taken, and most importantly can be accessed as a hive view. The view is available
    directly from the `SparkContext`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第一条语句是在当前上下文中创建一个视图/数据框，以便进一步操作。此视图是惰性计算的，除非采取特定步骤，否则不会持久化，最重要的是，可以作为Hive视图访问。该视图可以直接通过`SparkContext`访问。
- en: 'We then create a new data frame with the computed new column using the new
    sales view that we created. Under Jupyter this looks as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用创建的新销售视图创建一个新的数据框，并计算新的列。 在Jupyter下，显示如下：
- en: '![](img/17685618-ddb1-4d13-8423-b1176f2f9b41.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17685618-ddb1-4d13-8423-b1176f2f9b41.png)'
- en: Again, I don't think we have realistic values as the differences are very far
    off from predicted values.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我认为我们没有现实的值，因为差异与预测值相差甚远。
- en: The data frames created are immutable, unlike database tables.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 创建的数据框是不可变的，不像数据库表。
- en: Combining datasets
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合并数据集
- en: So, we have seen moving a data frame into Spark for analysis. This appears to
    be very close to SQL tables. Under SQL it is standard practice not to reproduce
    items in different tables. For example, a product table might have the price and
    an order table would just reference the product table by product identifier, so
    as not to duplicate data. So, then another SQL practice is to join or combine
    the tables to come up with the full set of information needed. Keeping with the
    order analogy, we combine all of the tables involved as each table has pieces
    of data that are needed for the order to be complete.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们已经看到将数据框移动到 Spark 进行分析。这看起来非常接近 SQL 表。在 SQL 中，标准做法是不在不同的表中重复项。例如，产品表可能会有价格，订单表只会通过产品标识符引用产品表，以避免重复数据。因此，另一个
    SQL 做法是将表连接或合并，以便获得完整的所需信息。继续使用订单的类比，我们将所有涉及的表合并，因为每个表都有订单完成所需的数据。
- en: 'How difficult would it be to create a set of tables and join them using Spark?
    We will use example tables of `Product`, `Order`, and `ProductOrder`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一组表并使用 Spark 连接它们会有多难？我们将使用 `Product`、`Order` 和 `ProductOrder` 示例表：
- en: '| **Table** | **Columns** |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| **表格** | **列** |'
- en: '| Product | Product ID,Description,Price |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 产品 | 产品 ID, 描述, 价格 |'
- en: '| Order | Order ID,Order Date |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 订单 | 订单 ID, 订单日期 |'
- en: '| ProductOrder | Order ID,Product ID,Quantity |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 产品订单 | 订单 ID, 产品 ID, 数量 |'
- en: So, an `Order` has a list of `Product`/`Quantity` values associated.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，一个 `Order` 关联有一组 `Product`/`Quantity` 值。
- en: 'We can populate the data frames and move them into Spark:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以填充数据框并将它们移入 Spark：
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we can attempt to perform an SQL-like `JOIN` operation among them:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以尝试在它们之间执行类似 SQL 的 `JOIN` 操作：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Doing all of this in Jupyter results in the display as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter 中执行这些操作的结果如下所示：
- en: '![](img/b653416b-1c6d-42df-9606-7d9883e10a3e.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b653416b-1c6d-42df-9606-7d9883e10a3e.png)'
- en: Our standard imports obtain a `SparkContext` and initialize a `SparkSession`.
    Note, the `getOrCreate` of the `SparkContext`. If you were to run this code outside
    of Jupyter there would be no context and a new one would be created. Under Jupyter,
    the startup for Spark in Jupyter initializes a context for all scripts. We can
    use that context at will with any Spark script, rather than have to create one
    ourselves.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的标准导入获取了一个 `SparkContext` 并初始化了一个 `SparkSession`。注意 `SparkContext` 的 `getOrCreate`
    方法。如果你在 Jupyter 外运行这段代码，则没有上下文，会创建一个新的上下文。在 Jupyter 中，Spark 启动时会为所有脚本初始化一个上下文。我们可以在任何
    Spark 脚本中使用该上下文，而不必自己创建一个。
- en: 'Load our `product` table:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 加载我们的 `product` 表：
- en: '![](img/f2e2fb89-1932-4714-9c4b-104974f5b136.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2e2fb89-1932-4714-9c4b-104974f5b136.png)'
- en: 'Load the `order` table:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 加载 `order` 表：
- en: '![](img/d26f6dd2-656c-4fa2-b545-904e59e648d0.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d26f6dd2-656c-4fa2-b545-904e59e648d0.png)'
- en: 'Load the `orderproduct` table. Note that at least one of the orders has multiple
    products:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 加载 `orderproduct` 表。请注意，至少有一个订单包含多个产品：
- en: '![](img/3909e3fb-6f95-4072-9f1e-c63225e75526.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3909e3fb-6f95-4072-9f1e-c63225e75526.png)'
- en: 'We have the `orderid` column from `order` and `orderproduct` in the result
    set. We could be more selective in our query and specify the exact columns we
    want to be returned:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在结果集中有来自 `order` 和 `orderproduct` 的 `orderid` 列。我们可以在查询中进行更精确的筛选，指定我们想要返回的确切列：
- en: '![](img/7253a504-d37c-47f7-85e8-3c10b30a2ba3.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7253a504-d37c-47f7-85e8-3c10b30a2ba3.png)'
- en: I had tried to use the Spark `join()` command with no luck.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾尝试使用 Spark 的 `join()` 命令，但未能成功。
- en: The documentation and examples I found on the internet are old, sparse, and
    incorrect.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我在网上找到的文档和示例都是过时、稀疏且不正确的。
- en: Using the command also presented the persistent error of a task not returning
    results in time. From the underlying Hadoop, I expect that processing tasks are
    normally broken up into separate tasks. I assume that Spark is breaking up functions
    into separate threads for completion similarly. It is not clear why such minor
    tasks are not completing as I was not asking it to perform anything extraordinary.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用该命令也显示了一个持续的错误，即任务没有及时返回结果。从底层的 Hadoop 看，我预计处理任务通常会被拆分为独立的任务。我猜 Spark 也会类似地将函数拆分到不同的线程中去完成。为什么这些小任务没有完成并不清楚，因为我并没有要求它执行任何超常的操作。
- en: Loading JSON into Spark
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 JSON 加载到 Spark 中
- en: 'Spark can also access JSON data for manipulation. Here we have an example that:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 还可以访问 JSON 数据进行操作。这里我们有一个示例：
- en: Loads a JSON file into a Spark data frame
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将JSON文件加载到Spark数据框中
- en: Examines the contents of the data frame and displays the apparent schema
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查数据框的内容并显示其明显的模式
- en: Like the other preceding data frames, moves the data frame into the context
    for direct access by the Spark session
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与前面提到的其他数据框一样，将数据框移入上下文以便Spark会话可以直接访问
- en: Shows an example of accessing the data frame in the Spark context
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示一个在Spark上下文中访问数据框的示例
- en: 'The listing is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 列表如下：
- en: 'Our standard includes for Spark:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的标准包括Spark的内容：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Read in the JSON and display what we found:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 读取JSON并显示我们找到的内容：
- en: '[PRE10]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](img/3248ff85-7185-4ba5-8b38-1facfcdc429b.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3248ff85-7185-4ba5-8b38-1facfcdc429b.png)'
- en: I had a difficult time getting a standard JSON to load into Spark. Spark appears
    to expect one record of data per list of the JSON file versus most JSON I have
    seen pretty much formats the record layouts with indentation and the like.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我在将标准JSON加载到Spark中时遇到了一些困难。Spark似乎期望JSON文件中的每个列表记录都是一条数据记录，而我所见的许多JSON文件格式将记录布局进行缩进等处理。
- en: Notice the use of null values where an attribute was not specified in an instance.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意使用了null值，表示某个实例未指定属性。
- en: 'Display the interpreted schema for the data:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 显示数据的解释模式：
- en: '[PRE11]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](img/78bf59a2-f9ee-4ccd-89f1-2a5455b9355e.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78bf59a2-f9ee-4ccd-89f1-2a5455b9355e.png)'
- en: The default for all columns is `nullable`. You can change an attribute of a
    column, but you cannot change the value of a column as the data values are immutable.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 所有列的默认值为`nullable`。你可以更改列的属性，但不能更改列的值，因为数据值是不可变的。
- en: 'Move the data frame into the context and access it from there:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据框移入上下文并从中访问：
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/269c578a-deec-424a-bd72-3a7f60ab54be.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/269c578a-deec-424a-bd72-3a7f60ab54be.png)'
- en: At this point, the `people` table works like any other temporary SQL table in
    Spark.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，`people`表像Spark中的任何其他临时SQL表一样工作。
- en: Using Spark pivot
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark透视表
- en: The `pivot()` function allows you to translate rows into columns while performing
    aggregation on some of the columns. If you think about it you are physically adjusting
    the axes of a table about a pivot point.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`pivot()`函数允许你将行转换为列，同时对某些列执行聚合操作。如果你仔细想想，这就像是围绕一个支点调整表格的坐标轴。'
- en: I thought of an easy example to show how this all works. I think it is one of
    those features that once you see it in action you realize the number of areas
    that you could apply it.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我想到了一个简单的例子来展示这一切是如何工作的。我认为这是一项功能，一旦你看到它的实际应用，就会意识到可以在很多地方应用它。
- en: In our example, we have some raw price points for stocks and we want to convert
    that table about a pivot to produce average prices per year per stock.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们有一些原始的股票价格数据，我们希望通过透视表将其转换，按股票每年生成平均价格。
- en: 'The code in our example is:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们示例中的代码是：
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This looks as follows in Jupyter:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jupyter中的展示如下：
- en: '![](img/592144ef-0026-42d2-9cdd-11e153424779.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/592144ef-0026-42d2-9cdd-11e153424779.png)'
- en: 'All the standard includes what we need for Spark to initialize the `SparkContext`
    and the `SparkSession`:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 所有标准包括我们需要的内容，以便Spark初始化`SparkContext`和`SparkSession`：
- en: '![](img/7f5525e2-f9d0-415e-b807-073728a1eb17.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7f5525e2-f9d0-415e-b807-073728a1eb17.png)'
- en: 'We load the stock price information from a CSV file. It is important that at
    least one of the stocks have more than one price for the same year:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从CSV文件加载股票价格信息。重要的是，至少有一只股票在同一年份有多个价格：
- en: '![](img/fd3422fc-e60a-4d88-8a36-3ec0c706b61e.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd3422fc-e60a-4d88-8a36-3ec0c706b61e.png)'
- en: We are grouping the information by stock symbol. The pivot is on the year that
    has two values, 2012 and 2013, in our dataset. We are computing an average price
    for each year.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按股票符号对信息进行分组。透视表基于数据集中包含两个值的年份——2012年和2013年。我们正在计算每年每只股票的平均价格。
- en: Summary
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we got familiar with obtaining a `SparkContext`. We saw examples
    of using Hadoop MapReduce. We used SQL with Spark data. We combined data frames
    and operated on the resulting set. We imported JSON data and manipulated it with
    Spark. Lastly, we looked at using a pivot to gather information about a data frame.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们熟悉了如何获取`SparkContext`。我们看到了使用Hadoop MapReduce的示例。我们用Spark数据进行了SQL操作。我们结合了数据框，并对结果集进行了操作。我们导入了JSON数据并用Spark进行了处理。最后，我们查看了如何使用透视表来汇总数据框的信息。
- en: In the next chapter, we will look at using R programming under Jupyter.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习如何在Jupyter中使用R编程。
