- en: Combining Pandas Objects
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合并Pandas对象
- en: A wide variety of options are available to combine two or more DataFrames or
    Series together. The `append` method is the least flexible and only allows for
    new rows to be appended to a DataFrame. The `concat` method is very versatile
    and can combine any number of DataFrames or Series on either axis. The `join`
    method provides fast lookups by aligning a column of one DataFrame to the index
    of others. The `merge` method provides SQL-like capabilities to join two DataFrames
    together.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种选项可以将两个或多个DataFrame或Series组合在一起。`append`方法是最不灵活的，它只允许向DataFrame添加新行。`concat`方法非常灵活，可以在任意轴上合并任意数量的DataFrame或Series。`join`方法通过将一个DataFrame的列与其他DataFrame的索引对齐来提供快速查找。`merge`方法提供类似SQL的功能，用于将两个DataFrame合并。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Appending new rows to DataFrames
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向DataFrame追加新行
- en: Concatenating multiple DataFrames together
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合并多个DataFrame
- en: Comparing President Trump's and Obama's approval ratings
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较特朗普总统和奥巴马总统的支持率
- en: Understanding the differences between `concat`, `join`, and `merge`
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解`concat`、`join`和`merge`之间的区别
- en: Connecting to SQL databases
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接到SQL数据库
- en: Appending new rows to DataFrames
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向DataFrame追加新行
- en: When performing a data analysis, it is far more common to create new columns
    than new rows. This is because a new row of data usually represents a new observation
    and, as an analyst, it is typically not your job to continually capture new data.
    Data capture is usually left to other platforms like relational database management
    systems. Nevertheless, it is a necessary feature to know as it will crop up from
    time to time.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行数据分析时，创建新列比创建新行更为常见。这是因为新的一行数据通常代表一个新的观察结果，而作为分析师，通常不是你负责不断捕获新数据。数据捕获通常由其他平台，如关系数据库管理系统，来处理。然而，作为一项必要功能，你仍然需要掌握它，因为它有时会用到。
- en: Getting ready
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: In this recipe, we will begin by appending rows to a small dataset with the
    `.loc` indexer and then transition to using the `append` method.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将首先使用`.loc`索引器将行添加到一个小数据集中，然后过渡到使用`append`方法。
- en: How to do it...
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Read in the names dataset, and output it:'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读名字数据集，并输出：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](img/4954144c-e882-4317-8aea-1ed33e15dc93.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4954144c-e882-4317-8aea-1ed33e15dc93.png)'
- en: 'Let''s create a list that contains some new data and use the `.loc` indexer
    to set a single row label equal to this new data:'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个包含一些新数据的列表，并使用`.loc`索引器将一个行标签设置为这些新数据：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](img/1da729da-b2ad-4cb5-abc6-aa45ba1f1c00.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1da729da-b2ad-4cb5-abc6-aa45ba1f1c00.png)'
- en: 'The `.loc` indexer uses labels to refer to the rows. In this case, the row
    labels exactly match the integer location. It is possible to append more rows
    with non-integer labels:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.loc`索引器使用标签来引用行。在这种情况下，行标签完全匹配整数位置。也可以使用非整数标签追加更多行：'
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](img/2a083be8-7f33-46f1-a95f-316aaa26f946.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a083be8-7f33-46f1-a95f-316aaa26f946.png)'
- en: 'To be more explicit in associating variables to values, you may use a dictionary.
    Also, in this step, we can dynamically choose the new index label to be the length
    of the DataFrame:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了更明确地将变量与值关联，你可以使用字典。此外，在这一步中，我们可以动态选择新的索引标签为DataFrame的长度：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](img/8ede39cb-b883-4529-9504-08a6509b4762.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ede39cb-b883-4529-9504-08a6509b4762.png)'
- en: 'A Series can hold the new data as well and works exactly the same as a dictionary:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个Series也可以保存新的数据，并且与字典完全相同：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](img/7a5690e9-355e-44db-936a-2513551b28ec.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a5690e9-355e-44db-936a-2513551b28ec.png)'
- en: 'The preceding operations all use the `.loc` indexing operator to make changes
    to the `names` DataFrame in-place. There is no separate copy of the DataFrame
    that is returned. In the next few steps, we will look at the `append` method,
    which does not modify the calling DataFrame. Instead, it returns a new copy of
    the DataFrame with the appended row(s). Let''s begin with the original `names`
    DataFrame and attempt to append a row. The first argument to `append` must be
    either another DataFrame, Series, dictionary, or a list of these, but not a list
    like the one in step 2\. Let''s see what happens when we attempt to use a dictionary
    with `append`:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述操作都使用`.loc`索引运算符对`names` DataFrame进行原地修改。没有返回DataFrame的副本。在接下来的几个步骤中，我们将了解`append`方法，它不会修改调用的DataFrame。相反，它会返回一个新的DataFrame副本，附加了行。让我们从原始的`names`
    DataFrame开始，尝试追加一行。`append`的第一个参数必须是另一个DataFrame、Series、字典或这些的列表，而不是像步骤2中的列表。让我们看看当我们尝试用字典和`append`一起使用时会发生什么：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This error message appears to be slightly incorrect. We are passing a DataFrame
    and not a Series but nevertheless, it gives us instructions on how to correct
    it:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个错误信息看起来稍有不准确。我们传递的是一个 DataFrame 而不是 Series，但它仍然给出了如何修正的指示：
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](img/d1209a67-6e9c-400b-878f-3b4c6454fa05.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1209a67-6e9c-400b-878f-3b4c6454fa05.png)'
- en: 'This works but `ignore_index` is a sneaky parameter. When set to `True`, the
    old index will be removed completely and replaced with a `RangeIndex` from 0 to
    n-1\. For instance, let''s specify an index for the `names` DataFrame:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这虽然可行，但 `ignore_index` 是一个狡猾的参数。当设置为 `True` 时，旧的索引将被完全移除，并被从 0 到 n-1 的 `RangeIndex`
    所替代。例如，下面我们为 `names` DataFrame 指定一个索引：
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/5ce47674-b461-431f-ac0e-0deb1442f0f1.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5ce47674-b461-431f-ac0e-0deb1442f0f1.png)'
- en: Rerun the code from step 7 and you will get the same result. The original index
    is completely ignored.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新运行步骤 7 中的代码，你将得到相同的结果。原始索引会被完全忽略。
- en: 'Let''s continue with this `names` dataset with these country strings in the
    index and use a Series that has a `name` attribute with the `append` method:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续使用这个 `names` 数据集，其中索引是这些国家字符串，并使用一个带有 `name` 属性的 Series 来使用 `append` 方法：
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](img/c54f0095-e1c3-4527-b793-a90c3efa3aa3.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c54f0095-e1c3-4527-b793-a90c3efa3aa3.png)'
- en: 'The `append` method is more flexible than the `.loc` indexer. It supports appending
    multiple rows at the same time. One way to accomplish this is with a list of Series:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`append` 方法比 `.loc` 索引器更灵活。它支持同时附加多行数据。实现这一点的一种方式是使用 Series 列表：'
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/49370d22-759a-4752-8eff-b1253447829e.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49370d22-759a-4752-8eff-b1253447829e.png)'
- en: 'Small DataFrames with only two columns are simple enough to manually write
    out all the column names and values. When they get larger, this process will be
    quite painful. For instance, let''s take a look at the 2016 baseball dataset:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只有两列的小型 DataFrame 足够简单，可以手动写出所有的列名和值。当它们变大时，这个过程将变得相当痛苦。例如，来看一下 2016 年的棒球数据集：
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](img/5c669c06-e152-40f1-a4ce-af3f4f91803a.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c669c06-e152-40f1-a4ce-af3f4f91803a.png)'
- en: 'This dataset contains 22 columns and it would be easy to mistype a column name
    or forget one altogether if you were manually entering new rows of data. To help
    protect against these mistakes, let''s select a single row as a Series and chain
    the `to_dict` method to it to get an example row as a dictionary:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个数据集包含 22 列，如果你手动输入新行数据时，很容易输入错误的列名或者完全忘记某些列名。为了避免这些错误，下面我们选择一行作为 Series，并链式调用
    `to_dict` 方法，将这一行作为字典提取出来：
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Clear the old values with a dictionary comprehension assigning any previous
    string value as an empty string and all others, missing values. This dictionary
    can now serve as a template for any new data you would like to enter:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用字典推导式清除旧值，将任何之前的字符串值设为空字符串，其他所有的值设为缺失值。这个字典现在可以作为你想输入任何新数据的模板：
- en: '[PRE12]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: How it works...
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `.loc` indexing operator is used to select and assign data based on the
    row and column labels. The first value passed to it represents the row label.
    In step 2, `names.loc[4]` refers to the row with a label equal to the integer
    4\. This label does not currently exist in the DataFrame. The assignment statement
    creates a new row with data provided by the list. As was mentioned in the recipe,
    this operation modifies the `names` DataFrame itself. If there was a previously
    existing row with a label equal to the integer 4, this command would have written
    over it. This modification in-place makes this indexing operator riskier to use
    than the `append` method, which never modifies the original calling DataFrame.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`.loc` 索引操作符用于根据行和列标签选择并分配数据。传递给它的第一个值代表行标签。在步骤 2 中，`names.loc[4]` 指的是标签等于整数
    4 的那一行。这个标签在 DataFrame 中当前并不存在。赋值语句通过列表提供的数据创建了一行新的数据。正如食谱中所提到的，这个操作会修改 `names`
    DataFrame 本身。如果之前存在标签等于整数 4 的行，这个命令会覆盖它。就地修改使得这个索引操作符比 `append` 方法更具风险，后者永远不会修改原始的调用
    DataFrame。'
- en: Any valid label may be used with the `.loc` indexing operator, as seen in step
    3\. Regardless of what the new label value actually is, the new row will always
    be appended at the end. Even though assigning with a list works, for clarity it's
    best to use a dictionary so that we know exactly which columns are associated
    with each value, as done in step 4.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 任何有效的标签都可以与 `.loc` 索引操作符一起使用，如步骤 3 所示。不管新的标签值是什么，新的行总是会附加到最后。即使用列表赋值有效，为了清晰起见，最好使用字典，这样我们可以清楚地知道每个值与哪些列关联，就像在步骤
    4 中所做的那样。
- en: Step 5 shows a little trick to dynamically set the new label to be the current
    number of rows in the DataFrame. Data stored in a Series will also get assigned
    correctly as long as the index labels match the column names.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第 5 步展示了一个小技巧，可以动态地将新标签设置为 DataFrame 当前的行数。存储在 Series 中的数据也会被正确分配，只要索引标签与列名匹配。
- en: The rest of the steps use the `append` method, which is a simple method that
    only appends new rows to DataFrames. Most DataFrame methods allow both row and
    column manipulation through an `axis` parameter. One exception is with `append`,
    which can only append rows to DataFrames.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的步骤使用 `append` 方法，这是一种简单的方法，只能将新行附加到 DataFrame 中。大多数 DataFrame 方法都可以通过 `axis`
    参数同时处理行和列。唯一的例外是 `append`，它只能向 DataFrame 添加行。
- en: Using a dictionary of column names mapped to values isn't enough information
    for append to work, as seen by the error message in step 6\. To correctly append
    a dictionary without a row name, you will have to set the `ignore_index` parameter
    to `True`. Step 10 shows you how to keep the old index by simply converting your
    dictionary to a Series. Make sure to use the `name` parameter, which is then used
    as the new index label. Any number of rows may be added with append in this manner
    by passing a list of Series as the first argument.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用列名映射到值的字典不足以使 append 操作生效，正如第 6 步的错误信息所示。要正确地附加没有行名的字典，你必须将 `ignore_index`
    参数设置为 `True`。第 10 步展示了如何通过将字典转换为 Series 来保留原始索引。确保使用 `name` 参数，这将作为新的索引标签。在这种方式下，通过将
    Series 列表作为第一个参数传递，可以添加任意数量的行。
- en: When wanting to append rows in this manner with a much larger DataFrame, you
    can avoid lots of typing and mistakes by converting a single row to a dictionary
    with the `to_dict` method and then using a dictionary comprehension to clear out
    all the old values replacing them with some defaults.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想要以这种方式附加大量的行到一个 DataFrame 时，你可以通过将单行转换为字典，并使用字典推导式清除所有旧值，将它们替换为一些默认值，从而避免大量的输入错误。
- en: There's more...
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Appending a single row to a DataFrame is a fairly expensive operation and if
    you find yourself writing a loop to append single rows of data to a DataFrame,
    then you are doing it wrong. Let''s first create 1,000 rows of new data as a list
    of Series:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 向 DataFrame 中添加单行是一个相当昂贵的操作，如果你发现自己正在写循环逐行添加数据到 DataFrame，那么你做错了。首先，我们创建 1,000
    行新数据作为 Series 列表：
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s time how long it takes to loop through each item making one append at
    a time:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来计时，看看循环通过每一项并逐行附加需要多少时间：
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'That took nearly five seconds for only 1,000 rows. If we instead pass in the
    entire list of Series, we get an enormous speed increase:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 仅 1,000 行数据就花了近五秒钟。如果我们改为传递整个 Series 列表，速度会大大提高：
- en: '[PRE15]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: By passing in the list of Series, the time has been reduced to under one-tenth
    of a second. Internally, pandas converts the list of Series to a single DataFrame
    and then makes the append.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过传递 Series 列表，时间被减少到了不到一秒的十分之一。Pandas 内部将 Series 列表转换为一个单一的 DataFrame，然后进行
    append 操作。
- en: Concatenating multiple DataFrames together
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将多个 DataFrame 连接在一起
- en: The versatile `concat` function enables concatenating two or more DataFrames
    (or Series) together, both vertically and horizontally. As per usual, when dealing
    with multiple pandas objects simultaneously, concatenation doesn't happen haphazardly
    but aligns each object by their index.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 多功能的 `concat` 函数可以将两个或更多的 DataFrame（或 Series）垂直或水平地连接在一起。通常，处理多个 pandas 对象时，连接不会随意发生，而是根据索引对齐每个对象。
- en: Getting ready
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备好
- en: In this recipe, we combine DataFrames both horizontally and vertically with
    the `concat` function and then change the parameter values to yield different
    results.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用 `concat` 函数水平和垂直地合并 DataFrame，然后通过更改参数值来得到不同的结果。
- en: How to do it...
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Read in the 2016 and 2017 stock datasets, and make their ticker symbol the
    index:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取 2016 年和 2017 年的股票数据集，并将它们的股票代码设置为索引：
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](img/78a928de-3aa5-444b-b6c3-8f74e590a2ca.png)    ![](img/82f29089-c05d-4bf1-8208-be3212c8b259.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78a928de-3aa5-444b-b6c3-8f74e590a2ca.png)    ![](img/82f29089-c05d-4bf1-8208-be3212c8b259.png)'
- en: 'Place all the `stock` datasets into a single list, and then call the `concat`
    function to concatenate them together:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有 `stock` 数据集放入一个列表中，然后调用 `concat` 函数将它们连接在一起：
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/d273561a-b8ee-4400-84c6-6fe99b2870ec.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d273561a-b8ee-4400-84c6-6fe99b2870ec.png)'
- en: 'By default, the `concat` function concatenates DataFrames vertically, one on
    top of the other. One issue with the preceding DataFrame is that there is no way
    to identify the year of each row. The `concat` function allows each piece of the
    resulting DataFrame to be labeled with the `keys` parameter. This label will appear
    in the outermost index level of the concatenated frame and force the creation
    of a MultiIndex. Also, the `names` parameter has the ability to rename each index
    level for clarity:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认情况下，`concat`函数将DataFrame垂直连接，将其一一叠加。前述的DataFrame存在一个问题，就是无法识别每一行的年份。`concat`函数允许通过`keys`参数为结果DataFrame中的每个部分加上标签。这个标签会出现在连接后的框架的最外层索引级别，并强制创建一个多重索引。此外，`names`参数可以重命名每个索引级别，以便更清晰地显示：
- en: '[PRE18]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](img/010d420f-feba-4c21-9d7c-3f60a806cf2e.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/010d420f-feba-4c21-9d7c-3f60a806cf2e.png)'
- en: 'It is also possible to concatenate horizontally by changing the `axis` parameter
    to *columns* or *1*:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 也可以通过将`axis`参数设置为*columns*或*1*来进行水平连接：
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](img/6d248ec3-bfb5-496c-96ae-2b5d267e5c77.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d248ec3-bfb5-496c-96ae-2b5d267e5c77.png)'
- en: 'Notice that missing values appear whenever a stock symbol is present in one
    year but not the other. The `concat` function, by default, uses an outer join,
    keeping all rows from each DataFrame in the list.  However, it gives us options
    to only keep rows that have the same index values in both DataFrames. This is
    referred to as an inner join. We set the `join` parameter to *inner* to change
    the behavior:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，每当某个股票代码在某一年出现而在另一年没有出现时，缺失值会出现。默认情况下，`concat`函数使用外连接，保留列表中每个DataFrame的所有行。然而，它也提供了仅保留两个DataFrame中索引值相同的行的选项，这被称为内连接。我们可以将`join`参数设置为*inner*来改变这一行为：
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/4740c498-d459-403c-bd63-17d7554797d5.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4740c498-d459-403c-bd63-17d7554797d5.png)'
- en: How it works...
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The first argument is the only argument required for the `concat` function and
    it must be a sequence of pandas objects, typically a list or dictionary of DataFrames
    or Series. By default, all these objects will be stacked vertically one on top
    of the other. In this recipe, only two DataFrames are concatenated, but any number
    of pandas objects work. When we were concatenating vertically, the DataFrames
    align by their column names.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是`concat`函数唯一需要的参数，必须是一个包含pandas对象的序列，通常是一个DataFrame或Series的列表或字典。默认情况下，所有这些对象会被垂直堆叠在一起。在这个例子中，仅连接了两个DataFrame，但任何数量的pandas对象都可以工作。当我们进行垂直连接时，DataFrame会根据列名对齐。
- en: In this dataset, all the column names were the same so each column in the 2017
    data lined up precisely under the same column name in the 2016 data. However,
    when they were concatenated horizontally, as in step 4, only two of the index
    labels matched from both years--*AAPL* and *TSLA*. Therefore, these ticker symbols
    had no missing values for either year. There are two types of alignment possible
    using `concat`, *outer* (the default) and *inner* referred to by the `join` parameter.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，所有列名相同，因此2017年的每一列都准确地与2016年对应的列对齐。然而，当它们像第4步那样进行水平连接时，只有两个索引标签在两年间匹配——*AAPL*和*TSLA*。因此，这些股票代码在两年之间都没有缺失值。使用`concat`可以进行两种类型的对齐，分别是*outer*（默认）和*inner*，由`join`参数指定。
- en: There's more...
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'The `append` method is a heavily watered down version of `concat` that can
    only append new rows to a DataFrame. Internally, `append` just calls the `concat`
    function. For instance, step 2 from this recipe may be duplicated with the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`append`方法是`concat`的简化版本，仅能将新行追加到DataFrame中。内部实现上，`append`只是调用了`concat`函数。例如，这个示例中的第二步可以通过以下方式实现：'
- en: '[PRE21]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Comparing President Trump's and Obama's approval ratings
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较特朗普和奥巴马的支持率
- en: Public support of the current President of the United States is a topic that
    frequently makes it into news headlines and is formally measured through opinion
    polls. In recent years, there has been a rapid increase in the frequency of these
    polls and lots of new data rolls in each week. There are many different pollsters
    that each have their own questions and methodology to capture their data, and
    thus there exists quite a bit of variability among the data. The American Presidency
    Project from the University of California, Santa Barbara, provides an aggregate
    approval rating down to a single data point each day.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 美国现任总统的公众支持度是一个经常出现在新闻头条的话题，并通过民意调查正式测量。近年来，这些民意调查的频率急剧增加，每周都会有大量的新数据发布。不同的民意调查公司有不同的问题和方法来收集数据，因此数据之间存在相当大的差异。加利福尼亚大学圣巴巴拉分校的美国总统项目提供了一个综合的支持度数据，每天更新一次。
- en: Unlike most of the recipes in this book, the data is not readily available in
    a CSV file. Often, as a data analyst, you will need to find data on the web, and
    use a tool that can scrape it into a format that you can then parse through your
    local workstation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书中的大多数示例不同，这些数据并不是直接以 CSV 文件的形式提供的。作为数据分析师，你通常需要在网上寻找数据，并使用一个能够抓取这些数据并转化为你能够在本地工作站解析的格式的工具。
- en: Getting ready
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: In this recipe, we will use the `read_html` function, which comes heavily equipped
    to scrape data from tables online and turn them into DataFrames. You will also
    learn how to inspect web pages to find the underlying HTML for certain elements.
    I used Google Chrome as my browser and suggest you use it, or Firefox, for the
    web-based steps.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 `read_html` 函数，它具有强大的功能，可以从网上的表格中抓取数据并将其转化为 DataFrame。你还将学习如何检查网页，找到某些元素的底层
    HTML。我使用的是 Google Chrome 浏览器，建议你也使用它，或者 Firefox 浏览器，进行网页相关的操作。
- en: How to do it...
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Navigate to *The American Presidency Project* approval page for President Donald
    Trump ([http://www.presidency.ucsb.edu/data/popularity.php?pres=45](http://www.presidency.ucsb.edu/data/popularity.php?pres=45)).
    You should get a page that contains a time series plot with the data in a table
    directly following it:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 *美国总统项目* 中关于总统唐纳德·特朗普的支持度页面（[http://www.presidency.ucsb.edu/data/popularity.php?pres=45](http://www.presidency.ucsb.edu/data/popularity.php?pres=45)）。你应该会看到一个包含时间序列图的数据页面，紧跟其后的是一个表格：
- en: '![](img/aa1d10d8-b146-408e-a53e-c0949b87086e.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa1d10d8-b146-408e-a53e-c0949b87086e.png)'
- en: 'The `read_html` function is able to scrape tables off web pages and place their
    data into DataFrames. It works best with simple HTML tables and provides some
    useful parameters to select the exact table you desire in case there happen to
    be multiple tables on the same page. Let''s go ahead and use `read_html` with
    its default values, which will return all the tables as DataFrames in a list:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`read_html` 函数能够从网页中抓取表格并将其数据放入 DataFrame。它在处理简单的 HTML 表格时效果最佳，并提供了一些有用的参数，帮助你在同一页面有多个表格时选择你想要的具体表格。我们可以直接使用
    `read_html` 的默认值，这将返回所有表格作为 DataFrame 的列表：'
- en: '[PRE22]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The function has returned 14 tables, which seems preposterous at first, as
    the web page appears to show only a single element that most people would recognize
    as a table. The `read_html` function formally searches for HTML table elements
    that begin with *<table*. Let''s inspect the HTML page by right-clicking on the
    approval data table and selecting inspect or inspect element:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该函数返回了 14 个表格，乍一看似乎很不可思议，因为网页上似乎只显示了一个大家普遍认为是表格的元素。`read_html` 函数正式搜索以 *<table*
    开头的 HTML 表格元素。让我们通过右键点击批准数据表格并选择检查或检查元素来检查 HTML 页面：
- en: '![](img/e0ac10b2-f06d-41b7-8812-e98ff764ef59.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0ac10b2-f06d-41b7-8812-e98ff764ef59.png)'
- en: 'This opens up the console, which is a very powerful tool for web development.
    For this recipe, we will only need it for a few tasks. All consoles allow you
    to search the HTML for a specific word. Let''s search for the word `table`. My
    browser found 15 different HTML tables, very close to the number returned by `read_html`:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会打开控制台，这是一个非常强大的网页开发工具。在本节中，我们只需要它来完成几个任务。所有控制台都允许你在 HTML 中搜索特定的词汇。让我们搜索 `table`
    这个词。我的浏览器找到了 15 个不同的 HTML 表格，与 `read_html` 返回的数量非常接近：
- en: '![](img/fa149f24-7f39-49b5-9ea0-17bb8a32f54c.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa149f24-7f39-49b5-9ea0-17bb8a32f54c.png)'
- en: 'Let''s begin inspecting the DataFrames in `df_list`:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们开始检查 `df_list` 中的 DataFrame：
- en: '[PRE23]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](img/f2c5a7e0-0994-47a1-89a9-dc3872414d23.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2c5a7e0-0994-47a1-89a9-dc3872414d23.png)'
- en: 'Looking back at the web page, there is a row in the approval table for nearly
    each day beginning January 22, 2017, until the day the data was scraped--September
    25, 2017\. This is a little more than eight months or 250 rows of data, which
    is somewhat close to the 308 lines in that first table. Scanning through the rest
    of the tables, you can see that lots of empty meaningless tables were discovered,
    as well as tables for different parts of the web page that don''t actually resemble
    tables. Let''s use some of the parameters of the `read_html` function to help
    us select the table we desire. We can use the `match` parameter to search for
    a specific string in the table. Let''s search for a table with the word *Start
    Date* in it:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回顾网页，从2017年1月22日开始，直到数据抓取的日期——2017年9月25日，审批表中几乎每天都有一行数据。大约是八个月或250行数据，接近第一个表格中的308行。浏览其余的表格时，你会发现很多空的无意义的表格，以及一些实际上不像表格的网页其他部分。让我们使用`read_html`函数的一些参数，帮助我们选择我们想要的表格。我们可以使用`match`参数搜索表格中的特定字符串。让我们搜索包含*Start
    Date*的表格：
- en: '[PRE24]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'By searching for a specific string in the table, we have reduced the number
    of tables down to just three. Another useful parameter is `attrs`, which accepts
    a dictionary of HTML attributes paired with their value. We would like to find
    some unique attributes for our particular table. To do this, let''s right-click
    again in our data table. This time, make sure to click at the very top in one
    of the table headers. For example, right click on *President,* and select inspect
    or inspect element again:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在表格中搜索特定的字符串，我们将表格的数量减少到了只有三个。另一个有用的参数是`attrs`，它接受一组HTML属性及其对应的值的字典。我们希望为我们的特定表格找到一些独特的属性。为此，让我们再次右键点击数据表。这次，确保点击在表头的最上方。例如，右键点击*President*，然后再次选择“检查”或“检查元素”：
- en: '![](img/8453c5fd-4db1-4cb7-9373-7c92d335c3ce.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8453c5fd-4db1-4cb7-9373-7c92d335c3ce.png)'
- en: 'The element that you selected should be highlighted. This is actually not the
    element we are interested in. Keep looking until you come across an HTML tag beginning
    with *<table*. All the words to the left of the equal signs are the attributes
    or `attrs` and to the right are the values. Let''s use the *align* attribute with
    its value *center* in our search:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你选择的元素应该被高亮显示。但这实际上不是我们感兴趣的元素。继续查找，直到你遇到一个以*<table*开头的HTML标签。等号左侧的所有词是属性或`attrs`，右侧是它们的值。让我们在搜索时使用*align*属性，其值为*center*：
- en: '[PRE25]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](img/42221dad-03c0-4e9b-9bf7-2e6aec18e4d6.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/42221dad-03c0-4e9b-9bf7-2e6aec18e4d6.png)'
- en: 'We only matched with one table and the number of rows is very close to the
    total days between the first and last dates. Looking at the data, it appears that
    we have indeed found the table we are looking for. The six column names appear
    to be on line 4\. We can go even further and precisely select the rows we want
    to skip and which row we would like to use for the column names with the `skiprows`
    and `header` parameters. We can also make sure that the start and end dates are
    coerced correctly to the right data type with the `parse_dates` parameter:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只匹配了一个表格，而且行数非常接近第一个和最后一个日期之间的总天数。查看数据后，似乎我们确实找到了我们想要的表格。六个列名似乎出现在第4行。我们可以更进一步，精确选择我们想跳过的行以及我们想用作列名的行，使用`skiprows`和`header`参数。我们还可以使用`parse_dates`参数确保开始日期和结束日期被正确转换为合适的数据类型：
- en: '[PRE26]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](img/99e47525-f4c0-44cf-84be-62eceaa334de.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99e47525-f4c0-44cf-84be-62eceaa334de.png)'
- en: 'This is almost exactly what we want, except for the columns with missing values.
    Let''s use the `dropna` method to drop columns with all values missing:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这几乎正是我们想要的，除了那些有缺失值的列。我们可以使用`dropna`方法删除所有值缺失的列：
- en: '[PRE27]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](img/907ada91-650c-4666-8dac-f7347b19b9fe.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/907ada91-650c-4666-8dac-f7347b19b9fe.png)'
- en: 'Let''s fill the missing values in the `President` column in a forward direction
    with the `ffill` method. Let''s first check whether there are any missing values
    in the other columns:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`ffill`方法按顺序填充`President`列中的缺失值。让我们先检查其他列中是否有缺失值：
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![](img/da9fc9fb-8b2e-4221-893d-105d8bca02d1.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/da9fc9fb-8b2e-4221-893d-105d8bca02d1.png)'
- en: 'Finally, it is important to check the data types to ensure they are correct:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，检查数据类型是否正确非常重要：
- en: '[PRE29]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s build a function with all the steps combined into one to automate the
    process of retrieving approval data for any President:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个函数，将所有步骤合并为一个，自动化获取任何总统的审批数据的过程：
- en: '[PRE30]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The only parameter, `pres_num`, denotes the order number of each president.
    Barack Obama was the 44th President of the United States; pass 44 to the `get_pres_appr`
    function to retrieve his approval numbers:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 唯一的参数`pres_num`表示每位总统的顺序编号。巴拉克·奥巴马是美国的第44任总统；传递44给`get_pres_appr`函数以获取他的支持率：
- en: '[PRE31]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![](img/6247a2a4-700b-48c1-97ee-065dee3a30d6.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6247a2a4-700b-48c1-97ee-065dee3a30d6.png)'
- en: 'There is Presidential approval rating data dating back to 1941 during President
    Franklin Roosevelt''s third term. With our custom function along with the `concat`
    function, it is possible to grab all the presidential approval rating data from
    this site. For now, let''s just grab the approval rating data for the last five
    presidents and output the first three rows for each President:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以追溯到1941年总统富兰克林·罗斯福的第三任期的总统支持率数据。通过自定义函数和`concat`函数的结合，我们可以从该网站获取所有总统的支持率数据。现在，我们只获取过去五任总统的支持率数据，并输出每位总统的前三行数据：
- en: '[PRE32]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![](img/b5510c6f-12c7-4faa-8711-6c8ece33933d.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5510c6f-12c7-4faa-8711-6c8ece33933d.png)'
- en: 'Before continuing, let''s determine if there are any dates with multiple approval
    ratings:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在继续之前，让我们确定是否有任何日期对应多个支持率：
- en: '[PRE33]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Only a few of the days have duplicate values. To help simplify our analysis,
    let''s keep only the first row where the duplicate date exists:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只有少数几天有重复值。为了简化分析，我们只保留重复日期的第一行：
- en: '[PRE34]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let''s get a few summary statistics on the data:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们获取一些数据的摘要统计：
- en: '[PRE35]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](img/118778a4-f5d5-4d65-a816-2a35a1024802.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/118778a4-f5d5-4d65-a816-2a35a1024802.png)'
- en: 'Let''s plot each President''s approval rating on the same chart. To do this,
    we will group by each President, iterate through each group, and individually
    plot the approval rating for each date:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将每位总统的支持率绘制在同一张图表上。为此，我们将按每位总统分组，遍历每个组，并单独绘制每个日期的支持率：
- en: '[PRE36]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![](img/8b6bbb45-0f79-4cbd-95f5-e47498c98c9e.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b6bbb45-0f79-4cbd-95f5-e47498c98c9e.png)'
- en: 'This chart places all the Presidents sequentially one after the other. We can
    compare them on a simpler scale by plotting their approval rating against the
    number of days in office. Let''s create a new variable to represent the number
    of days in office:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这张图表将所有总统按顺序排列。我们可以通过将他们的支持率与在任天数进行对比，简化比较。让我们创建一个新的变量来表示在任天数：
- en: '[PRE37]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](img/b63c31b6-e297-40e4-b17a-49bddb24e807.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b63c31b6-e297-40e4-b17a-49bddb24e807.png)'
- en: 'We have successfully given each row a relative number of days since the start
    of the presidency. It''s interesting that the new column, `Days in Office`, has
    a string representation of its value. Let''s check its data type:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经成功地给每一行添加了自总统任期开始以来的相对天数。有趣的是，新列`Days in Office`的值是以字符串形式表示的。让我们检查一下它的数据类型：
- en: '[PRE38]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The `Days in Office` column is a `timedelta64` object with nanosecond precision.
    This is far more precision than is needed. Let''s change the data type to integer
    by getting just the days:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Days in Office`列是一个`timedelta64`对象，精度为纳秒。这远远超过了所需的精度。让我们通过只取天数，将数据类型更改为整数：'
- en: '[PRE39]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We could plot this data in a similar fashion to what we did in step 19, but
    there is a completely different method that doesn''t involve any looping. By default,
    when calling the `plot` method on a DataFrame, pandas attempts to plot each column
    of data as a line plot and uses the index as the x-axis. Knowing this, let''s
    pivot our data so that each President has his own column for approval rating:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以像第19步中那样绘制这组数据，但还有一种完全不同的方法，它不涉及任何循环。默认情况下，当在DataFrame上调用`plot`方法时，pandas会尝试将每一列数据作为折线图绘制，并使用索引作为x轴。了解这一点后，让我们将数据透视，以便每位总统都有自己的一列支持率数据：
- en: '[PRE40]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![](img/16d28567-1ef0-412b-9a27-918e22994808.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16d28567-1ef0-412b-9a27-918e22994808.png)'
- en: 'Now that each President has his own column of approval ratings, we can plot
    each column directly without grouping. To reduce the clutter in the plot, we will
    only plot Barack Obama and Donald J. Trump:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在每位总统都有自己的一列支持率数据，我们可以直接绘制每一列，而无需分组。为了减少图表中的杂乱，我们将仅绘制巴拉克·奥巴马和唐纳德·J·特朗普的数据：
- en: '[PRE41]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![](img/82e84686-c9f1-4e28-bdfc-393437ce7825.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82e84686-c9f1-4e28-bdfc-393437ce7825.png)'
- en: How it works...
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何工作...
- en: It is typical to call `read_html` multiple times before arriving at the table
    (or tables) that you desire. There are two primary parameters at your disposal
    to specify a table, `match` and `attrs`. The string provided to `match` is used
    to find an exact match for the actual text in the table. This is text that will
    show up on the web page itself. The `attrs` parameter, on the other hand, searches
    for HTML table attributes found directly after the start of the table tag, `<table`.
    To see more of the table attributes, visit this page from W3 Schools ([http://bit.ly/2hzUzdD](https://www.w3schools.com/TagS/tag_table.asp)).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通常会多次调用`read_html`，直到找到你想要的表格（或表格）。你可以使用两个主要参数来指定表格，`match`和`attrs`。提供给`match`的字符串用于精确匹配表格中的实际文本。这是网页上显示的文本。而`attrs`参数则是用来查找HTML表格属性，这些属性位于表格标签`<table>`后面。要查看更多表格属性，请访问W3
    Schools的这个页面（[http://bit.ly/2hzUzdD](https://www.w3schools.com/TagS/tag_table.asp)）。
- en: Once we find our table in step 8, we can still take advantage of some other
    parameters to simplify things. HTML tables don't typically translate directly
    to nice DataFrames. There are often missing column names, extra rows, and misaligned
    data. In this recipe, `skiprows` is passed a list of row numbers to skip over
    when reading the file. They correspond to the rows of missing values in the DataFrame
    output from step 8\. The `header` parameter is also used to specify the location
    of the column names. Notice that `header` is equal to zero, which may seem wrong
    at first. Whenever the header parameter is used in conjunction with `skiprows`,
    the rows are skipped first resulting in a new integer label for each row. The
    correct column names are in row 4 but as we skipped rows 0 through 3, the new
    integer label for it is 0.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在步骤8中找到了表格，我们仍然可以利用一些其他参数来简化操作。HTML表格通常不能直接转化为漂亮的DataFrame。常常会缺少列名、额外的行以及数据错位。在这个步骤中，`skiprows`接收一个包含要跳过的行号的列表。当读取文件时，这些行号对应于步骤8中DataFrame输出中的缺失值行。`header`参数也用来指定列名的位置。注意，`header`等于零，乍一看可能会觉得是错误的。每当`header`参数与`skiprows`一起使用时，行会先被跳过，这样每行会得到一个新的整数标签。正确的列名在第4行，但由于我们跳过了第0到第3行，它的新的整数标签是0。
- en: In step 11, the `ffill` method fills any missing values vertically, going down
    with the last non-missing value. This method is just a shortcut for `fillna(method='ffill')`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤11中，`ffill`方法通过用最后一个非缺失值向下填充来填补任何缺失值。这个方法只是`fillna(method='ffill')`的快捷方式。
- en: Step 13 builds a function composed of all the previous steps to automatically
    get approval ratings from any President, provided you have the order number. There
    are a few differences in the function. Instead of applying the `ffill` method
    to the entire DataFrame, we only apply it to the `President` column. In Trump's
    DataFrame, the other columns had no missing data but this does not guarantee that
    all the scraped tables will have no missing data in their other columns. The last
    line of the function sorts the dates in a more natural way for data analysis from
    the oldest to newest. This changes the order of the index too, so we discard it
    with `reset_index` to have it begin from zero again.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤13构建了一个包含所有前面步骤的函数，用来自动获取任何总统的支持率，前提是你有订单号。这个函数有一些不同之处。我们不是对整个DataFrame应用`ffill`方法，而是只对`President`列应用它。在特朗普的DataFrame中，其他列没有缺失数据，但这并不保证所有抓取的表格在其他列中也没有缺失数据。函数的最后一行将日期按更自然的方式从旧到新排序，这样会改变索引的顺序，因此我们通过`reset_index`将其重置，使其重新从零开始。
- en: Step 16 shows a common pandas idiom for collecting multiple, similarly indexed
    DataFrames into a list before combining them together with the `concat` function.
    After concatenation into a single DataFrame, we should visually inspect it to
    ensure its accuracy. One way to do this is to take a glance at the first few rows
    from each President's section by grouping the data and then using the `head` method
    on each group.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤16展示了一种常见的pandas用法，先将多个索引相似的DataFrame收集到一个列表中，然后用`concat`函数将它们合并在一起。合并成一个DataFrame后，我们应该对其进行视觉检查，确保其准确性。检查的一种方法是通过对数据进行分组，然后在每个组上使用`head`方法来快速查看每个总统部分的前几行。
- en: The summary statistics in step 18 are interesting as each successive President
    has had lower median approval than the last. Extrapolating the data would lead
    to naively predicting a negative approval rating within the next several Presidents.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤18中的摘要统计数据很有趣，因为每位继任总统的中位支持率都低于前任。如果对数据进行外推，可能会天真地预测未来几位总统的支持率为负数。
- en: The plotting code in step 19 is fairly complex. You might be wondering why we
    need to iterate through a `groupby` object, to begin with. In the DataFrame's
    current structure, it has no ability to plot different groups based on values
    in a single column. However, step 23 shows you how to set up your DataFrame so
    that pandas can directly plot each President's data without a loop like this.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 第19步中的绘图代码相当复杂。你可能会想，为什么一开始就需要遍历一个`groupby`对象。在当前的DataFrame结构中，它无法基于单列的值绘制不同组的数据。然而，第23步会展示如何设置DataFrame，以便pandas可以直接绘制每个总统的数据，而不需要像这样的循环。
- en: To understand the plotting code in step 19, you must first be aware that a `groupby`
    object is iterable and, when iterating through, yields a tuple containing the
    current group (here it's just the name of the President) and the sub-DataFrame
    for just that group. This `groupby` object is zipped together with values controlling
    the color and linestyle of the plot. We import the colormap module, `cm`, from
    matplotlib which contains dozens of different colormaps. Passing a float between
    0 and 1 chooses a specific color from that colormap and we use it in our `plot`
    method with the `color` parameter. It is also important to note that we had to
    create the figure, `fig`, along with a plotting surface, `ax`, to ensure that
    each approval line was placed on the same plot. At each iteration in the loop,
    we use the same plotting surface with the identically named parameter, `ax`.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解第19步中的绘图代码，首先你需要知道`groupby`对象是可迭代的，在遍历时，它会返回一个包含当前组（这里就是总统的名字）和该组对应的子DataFrame的元组。这个`groupby`对象与控制绘图颜色和线条样式的值一起被打包。我们从matplotlib导入了色彩图模块`cm`，它包含了几十种不同的色彩图。传递一个0到1之间的浮动值可以从色彩图中选择一个特定的颜色，我们在`plot`方法中使用`color`参数来应用它。还需要注意的是，我们必须创建图形`fig`和绘图表面`ax`，以确保每条支持率线都绘制在同一张图上。在循环的每次迭代中，我们使用相同的绘图表面和同名的参数`ax`。
- en: To make a better comparison between Presidents, we create a new column equal
    to the number of days in office. We subtract the first date from the rest of the
    dates per President group. When two `datetime64` columns are subtracted, the result
    is a `timedelta64` object, which represents some length of time, days in this
    case. If we leave the column with nanosecond precision, the x-axis will similarly
    display too much precision by using the special `dt` accessor to return the number
    of days.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地比较各位总统，我们创建了一个新的列，表示在职天数。我们从每位总统组的其他日期中减去第一天的日期。当两个`datetime64`列相减时，结果是一个`timedelta64`对象，表示某段时间的长度，这里是天数。如果我们保持纳秒级的精度，x轴也会显示过多的精度，因此使用特殊的`dt`访问器返回天数。
- en: A crucial step comes in step 23\. We structure the data such that each President
    has a unique column for their approval rating. Pandas makes a separate line for
    each column. Finally, in step 24, we use the `.loc` indexer to simultaneously
    select the first 250 days (rows) along with only the columns for just Trump and
    Obama. The `ffill` method is used in the rare instances that one of the Presidents
    has a missing value for a particular day. In Python, it is possible to pass dictionaries
    that contain the parameter names and their values to functions by preceding them
    with `**` in a process called **dictionary unpacking**.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 关键步骤出现在第23步。我们重新结构化数据，使每个总统都有一个专门的列来表示其支持率。Pandas为每一列绘制了一条单独的线。最后，在第24步，我们使用`.loc`索引器同时选择前250天（行）以及特朗普和奥巴马的列。`ffill`方法用于在总统的某一天有缺失值时进行填充。在Python中，可以通过在函数前加`**`的方式将包含参数名称及其值的字典传递给函数，这个过程叫做**字典解包**。
- en: There's more...
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: The plot from step 19 shows quite a lot of noise and the data might be easier
    to interpret if it were smoothed. One common smoothing method is called the **rolling
    average**. Pandas offers the `rolling` method for DataFrames and `groupby` objects.
    It works analogously to the `groupby` method by returning an object waiting for
    an additional action to be performed on it. When creating it, you must pass the
    size of the window as the first argument, which can either be an integer or a
    date offset string.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 第19步中的绘图显示了相当多的噪音，如果对数据进行平滑处理，可能会更容易解释。一个常见的平滑方法叫做**滚动平均**。Pandas为DataFrame和`groupby`对象提供了`rolling`方法。它的工作方式类似于`groupby`方法，通过返回一个等待执行额外操作的对象。当创建时，你必须将窗口大小作为第一个参数传递，这个大小可以是一个整数或者一个日期偏移字符串。
- en: 'In this example, we take a 90-day moving average with the date offset string
    *90D*. The `on` parameter specifies the column from which the rolling window is
    calculated:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用日期偏移字符串*90D*计算90天的移动平均值。`on`参数指定了滚动窗口计算的列：
- en: '[PRE42]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'From here, we can restructure the data so that it looks similar to the output
    from step 23 with the `unstack` method, and then make our plot:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，我们可以使用 `unstack` 方法重新结构化数据，使其与步骤23的输出类似，然后制作我们的图表：
- en: '[PRE43]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![](img/d120b3da-ec23-4575-b869-80eb4a9d486c.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d120b3da-ec23-4575-b869-80eb4a9d486c.png)'
- en: See also
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: Colormap references for matplotlib ([http://bit.ly/2yJZOvt](https://matplotlib.org/examples/color/colormaps_reference.html))
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: matplotlib 的 colormap 参考（[http://bit.ly/2yJZOvt](https://matplotlib.org/examples/color/colormaps_reference.html)）
- en: A list of all the date offsets and their aliases ([http://bit.ly/2xO5Yg0](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases))
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有日期偏移及其别名的列表（[http://bit.ly/2xO5Yg0](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases)）
- en: Understanding the differences between concat, join, and merge
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 `concat`、`join` 和 `merge` 之间的区别
- en: 'The `merge` and `join` DataFrame (and not Series) methods and the `concat`
    function all provide very similar functionality to combine multiple pandas objects
    together. As they are so similar and they can replicate each other in certain
    situations, it can get very confusing when and how to use them correctly. To help
    clarify their differences, take a look at the following outline:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`merge` 和 `join` 的 DataFrame 方法（而不是 Series）以及 `concat` 函数都提供了非常相似的功能，用于将多个
    pandas 对象合并在一起。由于它们如此相似，并且在某些情况下可以互相替代，因此理解何时以及如何正确使用它们可能会让人困惑。为了帮助澄清它们之间的差异，以下是一个简要的概述：'
- en: '`concat`:'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`concat`：'
- en: Pandas function
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas 函数
- en: Combines two or more pandas objects vertically or horizontally
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 垂直或水平合并两个或更多 pandas 对象
- en: Aligns only on the index
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅在索引上对齐
- en: Errors whenever a duplicate appears in the index
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每当索引中出现重复项时会报错
- en: Defaults to outer join with option for inner
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认执行外连接，并可选择执行内连接
- en: '`join`:'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`join`：'
- en: DataFrame method
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame 方法
- en: Combines two or more pandas objects horizontally
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平合并两个或更多 pandas 对象
- en: Aligns the calling DataFrame's column(s) or index with the other objects' index
    (and not the columns)
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将调用的 DataFrame 的列/索引与其他对象的索引（而非列）对齐
- en: Handles duplicate values on the joining columns/index by performing a cartesian
    product
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过执行笛卡尔积处理连接列/索引中的重复值
- en: Defaults to left join with options for inner, outer, and right
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认执行左连接，并可选择执行内连接、外连接和右连接
- en: '`merge`:'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merge`：'
- en: DataFrame method
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame 方法
- en: Combines exactly two DataFrames horizontally
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确地水平合并两个 DataFrame
- en: Aligns the calling DataFrame's column(s)/index with the other DataFrame's column(s)/index
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将调用的 DataFrame 的列/索引与另一个 DataFrame 的列/索引对齐
- en: Handles duplicate values on the joining columns/index by performing a cartesian
    product
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过执行笛卡尔积处理连接列/索引中的重复值
- en: Defaults to inner join with options for left, outer, and right
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认执行内连接，并可选择执行左连接、外连接和右连接
- en: The first parameter to the join method is `other` which can either be a single
    DataFrame/Series or a list of any number of DataFrames/Series.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`join` 方法的第一个参数是 `other`，它可以是单个 DataFrame/Series，或者是任何数量的 DataFrame/Series
    列表。'
- en: Getting ready
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: In this recipe, we will do what is required to combine DataFrames. The first
    situation is simpler with `concat` while the second is simpler with `merge`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将完成合并 DataFrame 所需的步骤。第一种情况使用 `concat` 更为简单，而第二种情况则使用 `merge` 更为简单。
- en: How to do it...
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何执行...
- en: 'Let''s read in stock data for 2016, 2017, and 2018 into a list of DataFrames
    using a loop instead of three different calls to the `read_csv` function. Jupyter
    notebooks currently only allow a single DataFrame to be displayed on one line.
    However, there is a way to customize the HTML output with help from the `IPython`
    library. The user-defined `display_frames` function accepts a list of DataFrames
    and outputs them all in a single row:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们读取2016年、2017年和2018年的股票数据，并使用循环将它们放入一个 DataFrame 列表中，而不是通过三次不同的 `read_csv`
    调用。当前，Jupyter notebooks 只允许在一行上显示一个 DataFrame。但有一种方法可以借助 `IPython` 库自定义 HTML 输出。用户定义的
    `display_frames` 函数接受一个 DataFrame 列表并将它们全部输出在同一行上：
- en: '[PRE44]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![](img/334685b5-02c5-4a06-a159-caccb833fc01.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/334685b5-02c5-4a06-a159-caccb833fc01.png)'
- en: 'The `concat` function is the only one able to combine DataFrames vertically.
    Let''s do this by passing it the list `stock_tables`:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`concat` 函数是唯一能够垂直合并 DataFrame 的函数。让我们通过传递 `stock_tables` 列表来实现：'
- en: '[PRE45]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![](img/0c817ede-3963-45a8-961e-4ba60ab03d7c.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c817ede-3963-45a8-961e-4ba60ab03d7c.png)'
- en: 'It can also combine DataFrames horizontally by changing the `axis` parameter
    to `columns`:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还可以通过将`axis`参数更改为`columns`来水平合并DataFrame：
- en: '[PRE46]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![](img/761c169f-57ab-4ac7-aae1-e5e83bda06a3.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/761c169f-57ab-4ac7-aae1-e5e83bda06a3.png)'
- en: 'Now that we have started combining DataFrames horizontally, we can use the
    `join` and `merge` methods to replicate this functionality of `concat`. Here,
    we use the `join` method to combine the `stock_2016` and `stock_2017` DataFrames.
    By default, the DataFrames align on their index. If any of the columns have the
    same names, then you must supply a value to the `lsuffix` or `rsuffix` parameters
    to distinguish them in the result:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经开始水平合并DataFrame，可以使用`join`和`merge`方法来复制`concat`的这一功能。在这里，我们使用`join`方法来合并`stock_2016`和`stock_2017`两个DataFrame。默认情况下，DataFrame会根据它们的索引进行对齐。如果某些列有相同的名称，则必须为`lsuffix`或`rsuffix`参数提供值，以便在结果中区分它们：
- en: '[PRE47]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![](img/7003e693-6693-46b5-9880-13e184c2612d.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7003e693-6693-46b5-9880-13e184c2612d.png)'
- en: 'To exactly replicate the output of the `concat` function from step 3, we can
    pass a list of DataFrames to the `join` method:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了精确复制第3步中`concat`函数的输出，我们可以将一个DataFrame列表传递给`join`方法：
- en: '[PRE48]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](img/146587e7-f778-4917-8eac-06e4f8208f7a.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/146587e7-f778-4917-8eac-06e4f8208f7a.png)'
- en: 'Let''s check whether they actually are exactly equal:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查它们是否确实完全相等：
- en: '[PRE49]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, let''s turn to `merge` that, unlike `concat` and `join`, can combine exactly
    two DataFrames together. By default, `merge` attempts to align the values in the
    columns that have the same name for each of the DataFrames. However, you can choose
    to have it align on the index by setting the boolean parameters `left_index` and
    `right_index` to `True`. Let''s merge the 2016 and 2017 stock data together:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下`merge`，它与`concat`和`join`不同，能够将两个DataFrame精确地合并在一起。默认情况下，`merge`会尝试根据每个DataFrame中相同名称的列来对齐列中的值。不过，你也可以选择通过将布尔参数`left_index`和`right_index`设置为`True`，让它根据索引进行对齐。让我们将2016年和2017年的库存数据合并在一起：
- en: '[PRE50]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '![](img/075849a5-3ab9-4b9c-a914-be9e715ea643.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/075849a5-3ab9-4b9c-a914-be9e715ea643.png)'
- en: 'By default, merge uses an inner join and automatically supplies suffixes for
    identically named columns. Let''s change to an outer join and then perform another
    outer join of the 2018 data to exactly replicate `concat`:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认情况下，`merge`使用内连接，并自动为同名列提供后缀。让我们改用外连接，然后再执行一次外连接，将2018年的数据合并在一起，从而完全复制`concat`的功能：
- en: '[PRE51]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now let''s turn our comparison to datasets where we are interested in aligning
    together the values of columns and not the index or column labels themselves.
    The `merge` method is built exactly for this situation. Let''s take a look at
    two new small datasets, `food_prices` and `food_transactions`:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们转向我们关心对齐列值而非索引或列标签的数据集。`merge`方法正是为这种情况而设计的。让我们来看一下两个新的小数据集，`food_prices`和`food_transactions`：
- en: '[PRE52]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![](img/f98e02af-0bc9-4214-bff8-783bdda27391.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f98e02af-0bc9-4214-bff8-783bdda27391.png)'
- en: 'If we wanted to find the total amount of each transaction, we would need to
    join these tables on the `item` and `store` columns:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们想要找到每笔交易的总金额，我们需要在`item`和`store`列上连接这些表：
- en: '[PRE53]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '![](img/32a7e87a-018f-40bb-bb3e-123be98fdf88.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32a7e87a-018f-40bb-bb3e-123be98fdf88.png)'
- en: 'The price is now aligned correctly with its corresponding item and store, but
    there is a problem. Customer 2 has a total of four `steak` items. As the `steak`
    item appears twice in each table for store `B`, a Cartesian product takes place
    between them, resulting in four rows. Also, notice that the item, `coconut`, is
    missing because there was no corresponding price for it. Let''s fix both of these
    issues:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 价格现在已经与对应的商品和商店正确对齐，但存在一个问题。客户2购买了四个`steak`商品。由于`steak`商品在商店`B`的每个表格中都出现了两次，它们之间发生了笛卡尔积，导致了四行数据的出现。另请注意，`coconut`商品缺失，因为没有相应的价格。我们来解决这两个问题：
- en: '[PRE54]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '![](img/cbeec36e-7160-4664-a8a2-983ad7e70c41.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cbeec36e-7160-4664-a8a2-983ad7e70c41.png)'
- en: 'We can replicate this with the `join` method but we must first put the joining
    columns of the `food_prices` DataFrame into the index:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`join`方法来复制这一过程，但必须首先将`food_prices` DataFrame的连接列放入索引中：
- en: '[PRE55]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![](img/a6e73c76-1791-4dd5-b906-a5f9b0f386b1.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6e73c76-1791-4dd5-b906-a5f9b0f386b1.png)'
- en: 'The `join` method only aligns with the index of the passed DataFrame but can
    use the index or the columns of the calling DataFrame. To use columns for alignment
    on the calling DataFrame, you will need to pass them to the `on` parameter:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`join`方法仅与传递的DataFrame的索引对齐，但可以使用调用DataFrame的索引或列。要使用列进行对齐，你需要将它们传递给`on`参数：'
- en: '[PRE56]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output matches the result from step 11 exactly. To replicate this with
    the `concat` method, you would need to put the item and store columns into the
    index of both DataFrames. However, in this particular case, an error would be
    produced as a duplicate index value occurs in at least one of the DataFrames (with
    item `steak` and store `B`):'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出与第11步的结果完全匹配。要使用`concat`方法复制此操作，您需要将`item`和`store`列放入两个DataFrame的索引中。然而，在这个特定的案例中，由于至少一个DataFrame中存在重复的索引值（`item`为`steak`且`store`为`B`），因此会产生错误。
- en: '[PRE57]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: How it works...
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: It can get tedious to repeatedly write the `read_csv` function when importing
    many DataFrames at the same time. One way to automate this process is to put all
    the file names in a list and iterate through them with a for loop. This was done
    in step 1 with a list comprehension.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在同时导入多个DataFrame时，重复编写`read_csv`函数可能会变得繁琐。自动化这个过程的一种方法是将所有文件名放入列表中，并使用`for`循环遍历它们。这在第1步中通过列表推导实现。
- en: The rest of this step builds a function to display multiple DataFrames on the
    same line of output in a Jupyter notebook. All DataFrames have a `to_html` method,
    which returns a raw HTML string representation of the table. The CSS (cascading
    style sheet) of each table is changed by altering the `display` attribute to *inline*
    so that elements get displayed horizontally next to one another rather than vertically.
    To properly render the table in the notebook, you must use the helper function
    `read_html` provided by the IPython library.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 本步骤的其余部分构建了一个函数，用于在Jupyter notebook中将多个DataFrame显示在同一行输出上。所有DataFrame都有一个`to_html`方法，它返回表格的原始HTML字符串表示。每个表格的CSS（层叠样式表）通过修改`display`属性为*inline*，使得元素水平并排显示，而不是垂直显示。为了在notebook中正确渲染表格，必须使用IPython库提供的辅助函数`read_html`。
- en: At the end of step 1, we unpack the list of DataFrames into their own appropriately
    named variables so that each individual table may be easily and clearly referenced.
    The nice thing about having a list of DataFrames is that, it is the exact requirement
    for the `concat` function, as seen in step 2\. Notice how step 2 uses the `keys`
    parameter to name each chunk of data. This can be also be accomplished by passing
    a dictionary to `concat`, as done in step 3.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步结束时，我们将DataFrame列表解包到各自适当命名的变量中，以便每个单独的表格可以轻松且清晰地引用。拥有DataFrame列表的好处是，它正好符合`concat`函数的需求，如第2步所示。注意第2步如何使用`keys`参数为每个数据块命名。通过将字典传递给`concat`，也可以实现这一点，如第3步所示。
- en: In step 4, we must change the type of `join` to `outer` to include all of the
    rows in the passed DataFrame that do not have an index present in the calling
    DataFrame. In step 5, the passed list of DataFrames cannot have any columns in
    common. Although there is an `rsuffix` parameter, it only works when passing a
    single DataFrame and not a list of them. To work around this limitation, we change
    the names of the columns beforehand with the `add_suffix` method, and then call
    the `join` method.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4步，我们必须将`join`的类型更改为`outer`，以包括传递的DataFrame中那些没有在调用DataFrame中找到的索引的所有行。在第5步，传递的DataFrame列表不能有任何重复的列。虽然有`rsuffix`参数，它仅在传递单个DataFrame时有效，而非传递DataFrame列表。为了解决这个限制，我们事先使用`add_suffix`方法更改列名，然后调用`join`方法。
- en: In step 7, we use `merge`, which defaults to aligning on all column names that
    are the same in both DataFrames. To change this default behavior, and align on
    the index of either one or both, set the `left_index` or `right_index` parameters
    to `True`. Step 8 finishes the replication with two calls to merge. As you can
    see, when you are aligning multiple DataFrames on their index, `concat` is usually
    going to be a far better choice than merge.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7步，我们使用`merge`，它默认按照两个DataFrame中相同的列名进行对齐。若要更改此默认行为，并按照某个或两个DataFrame的索引进行对齐，可以将`left_index`或`right_index`参数设置为`True`。第8步通过两次调用`merge`完成了复制。如你所见，当你根据索引对齐多个DataFrame时，`concat`通常是比`merge`更好的选择。
- en: In step 9, we switch gears to focus on a situation where `merge` has the advantage.
    The `merge` method is the only one capable of aligning both the calling and passed
    DataFrame by column values. Step 10 shows you how easy it is to merge two DataFrames.
    The `on` parameter is not necessary but provided for clarity.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在第9步，我们转变思路，聚焦于`merge`占优势的情况。`merge`方法是唯一能够根据列值对调用的DataFrame和传递的DataFrame进行对齐的方法。第10步展示了合并两个DataFrame的简单方法。`on`参数并非必需，但为了清晰起见提供。
- en: Unfortunately, it is very easy to duplicate or drop data when combining DataFrames,
    as shown in step 10\. It is vital to take some time to do some sanity checks after
    combining data. In this instance, the `food_prices` dataset had a duplicate price
    for `steak` in store `B` so we eliminated this row by querying for only the current
    year in step 11\. We also change to a left join to ensure that each transaction
    is kept regardless if a price is present or not.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，当合并数据帧时很容易复制或丢弃数据，如第10步所示。在合并数据后进行一些合理性检查非常重要。在这个例子中，`food_prices`数据集在商店`B`中对`steak`的价格有重复，因此我们在第11步通过查询仅保留当前年份的行来删除了这一行。我们还改为左连接以确保每笔交易都被保留，无论价格是否存在。
- en: It is possible to use join in these instances but all the columns in the passed
    DataFrame must be moved into the index first. Finally, `concat` is going to be
    a poor choice whenever you intend to align data by values in their columns.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下可以使用联接，但是传递的数据帧中的所有列都必须首先移动到索引中。最后，当您打算根据其列中的值对齐数据时，使用`concat`将是一个不好的选择。
- en: There's more...
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: 'It is possible to read all files from a particular directory into DataFrames
    without knowing their names. Python provides a few ways to iterate through directories,
    with the `glob` module being a popular choice. The gas prices directory contains
    five different CSV files, each having weekly prices of a particular grade of gas
    beginning from 2007\. Each file has just two columns--the date for the week and
    the price. This is a perfect situation to iterate through all the files, read
    them into DataFrames, and combine them all together with the `concat` function.
    The `glob` module has the `glob` function, which takes a single parameter--the
    location of the directory you would like to iterate through as a string. To get
    all the files in the directory, use the string ***. In this example, **.csv* returns
    only files that end in *.csv*. The result from the `glob` function is a list of
    string filenames, which can be directly passed to the `read_csv` function:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在不知道文件名的情况下从特定目录读取所有文件到数据帧中。Python提供了几种迭代目录的方法，其中`glob`模块是一个流行的选择。天然气价格目录包含五个不同的CSV文件，每个文件从2007年开始每周记录一个特定等级的天然气价格。每个文件只有两列——每周的日期和价格。这是一个完美的情况，可以通过`concat`函数迭代所有文件，并将它们全部组合在一起。`glob`模块具有`glob`函数，它接受一个参数——要迭代的目录的位置作为字符串。要获取目录中的所有文件，请使用字符串 ***。在这个例子中，**.csv*
    仅返回以 *.csv* 结尾的文件。`glob`函数的结果是一组字符串文件名，可以直接传递给`read_csv`函数：
- en: '[PRE58]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '![](img/8a01d07f-d580-4dce-9d3c-f47cf9a4c0ea.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a01d07f-d580-4dce-9d3c-f47cf9a4c0ea.png)'
- en: See also
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: IPython official documentation of the `read_html` function ([http://bit.ly/2fzFRzd](http://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.display_html))
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IPython `read_html` 函数的官方文档（[http://bit.ly/2fzFRzd](http://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.display_html)）
- en: Refer to the *Exploding indexes* recipe from [Chapter 12](a5777e1a-6de5-44f6-b291-429cbceb505f.xhtml),
    *Index Alignment*
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考*Exploding indexes*章节的食谱（来自[第12章](a5777e1a-6de5-44f6-b291-429cbceb505f.xhtml)，*索引对齐*）
- en: Connecting to SQL databases
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接到SQL数据库
- en: To become a serious data analyst, you will almost certainly have to learn some
    amount of SQL. Much of the world's data is stored in databases that accept SQL
    statements. There are many dozens of relational database management systems, with
    SQLite being one of the most popular and easy to use.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 要成为一名严肃的数据分析师，你几乎肯定需要学习一定量的SQL。世界上大部分数据存储在接受SQL语句的数据库中。有许多关系型数据库管理系统，其中SQLite是最流行和易于使用的之一。
- en: Getting ready
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will be exploring the Chinook sample database provided by SQLite that contains
    11 tables of data for a music store. One of the best things to do when first diving
    into a proper relational database is to study a database diagram (sometimes called
    an entity relationship diagram) to better understand how tables are related. The
    following diagram will be immensely helpful when navigating through this recipe:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探索由SQLite提供的Chinook样例数据库，其中包含11张音乐商店数据表。首次深入研究合适的关系数据库时，最好做的事情之一是研究数据库图表（有时称为实体关系图），以更好地理解表之间的关系。在浏览此处的食谱时，以下图表将非常有帮助：
- en: '![](img/bfb39fe0-ea36-4d28-a0d2-245d6385b744.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bfb39fe0-ea36-4d28-a0d2-245d6385b744.png)'
- en: In order for this recipe to work, you will need to have the `sqlalchemy` Python
    package installed. If you installed the Anaconda distribution, then it should
    already be available to you. SQLAlchemy is the preferred pandas tool when making
    connections to databases. In this recipe, you will learn how to connect to a SQLite
    database. You will then ask two different queries, and answer them by joining
    together tables with the `merge` method.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个步骤有效，您需要安装`sqlalchemy` Python包。如果您安装了Anaconda发行版，那么它应该已经可用。在与数据库建立连接时，SQLAlchemy是首选的pandas工具。在这个步骤中，您将学习如何连接到SQLite数据库。然后，您将提出两个不同的查询，并通过使用`merge`方法将表格连接在一起来回答它们。
- en: How to do it...
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'Before we can begin reading tables from the `chinook` database, we need to
    set up our SQLAlchemy engine:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们开始从`chinook`数据库读取表格之前，我们需要设置我们的SQLAlchemy引擎：
- en: '[PRE59]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We can now step back into the world of pandas and remain there for the rest
    of the recipe. Let''s complete a simple command and read in the `tracks` table
    with the `read_sql_table` function. The name of the table is the first argument
    and the SQLAlchemy engine is the second:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以回到pandas的世界，并在剩余的步骤中继续待在那里。让我们完成一个简单的命令，并使用`read_sql_table`函数读取`tracks`表格。表格的名称是第一个参数，SQLAlchemy引擎是第二个参数：
- en: '[PRE60]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '![](img/e34917a4-21a5-4bba-b011-70d354bc6d96.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e34917a4-21a5-4bba-b011-70d354bc6d96.png)'
- en: 'For the rest of the recipe, we will answer a couple of different specific queries
    with help from the database diagram. To begin, let''s find the average length
    of song per genre:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将通过数据库图表回答一些不同的具体查询。首先，让我们找出每种流派歌曲的平均长度：
- en: '[PRE61]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '![](img/41707069-07b1-4426-a5c2-ffa53f5c92de.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41707069-07b1-4426-a5c2-ffa53f5c92de.png)'
- en: 'Now we can easily find the average length of each song per genre. To help ease
    interpretation, we convert the `Milliseconds` column to the `timedelta` data type:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以轻松地找出每种流派歌曲的平均长度。为了帮助解释，我们将`Milliseconds`列转换为`timedelta`数据类型：
- en: '[PRE62]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Now let''s find the total amount spent per customer. We will need the `customers`,
    `invoices`, and `invoice_items` tables all connected to each other:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们找出每位顾客的总花费金额。我们将需要将`customers`、`invoices`和`invoice_items`表格连接在一起：
- en: '[PRE63]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '![](img/6e860d89-e3e3-4d5e-8ed2-c768bf668f23.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e860d89-e3e3-4d5e-8ed2-c768bf668f23.png)'
- en: 'We can now multiply the quantity by the unit price and then find the total
    amount spent per customer:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以将数量乘以单价，然后找出每位顾客的总花费金额：
- en: '[PRE64]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: How it works...
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它的工作原理...
- en: 'The `create_engine` function requires a connection string in order to work
    properly. The connection string for SQLite is very simple, and is just the location
    of the database, which is located in the data directory. Other relational database
    management systems have more complex connection strings. You will need to provide
    a username, password, hostname, port, and optionally, a database. You will also
    need to supply the SQL dialect and the driver. The general form for the connection
    string is as follows: `dialect+driver://username:password@host:port/database`.
    The driver for your particular relational database might need to be installed
    separately.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_engine`函数需要一个连接字符串才能正常工作。SQLite的连接字符串非常简单，只是数据库的位置，它位于数据目录中。其他关系型数据库管理系统具有更复杂的连接字符串。您需要提供用户名、密码、主机名、端口号，以及可选的数据库。您还需要提供SQL方言和驱动程序。连接字符串的一般形式如下：`dialect+driver://username:password@host:port/database`。您的特定关系型数据库的驱动程序可能需要单独安装。'
- en: Once we have created the engine, selecting entire tables into DataFrames is
    very easy with the `read_sql_table` function in step 2\. Each of the tables in
    the database has a primary key uniquely identifying each row. It is identified
    graphically with a key symbol in the diagram. In step 3, we link genres to tracks
    through `GenreId`. As we only care about the track length, we trim the tracks
    DataFrame down to just the columns we need before performing the merge. Once the
    tables have merged, we can answer the query with a basic `groupby` operation.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了引擎，在第二步中使用`read_sql_table`函数将整个表格选择到DataFrames中非常容易。数据库中的每个表格都有一个主键，用于在图表中唯一标识每一行。在第三步中，我们通过`GenreId`将流派链接到音轨。由于我们只关心音轨长度，因此在执行合并之前，我们将音轨DataFrame减少到所需的列。表格合并后，我们可以通过基本的`groupby`操作回答查询。
- en: We go one step further and convert the integer milliseconds into a Timedelta
    object that is far easier to read. The key is passing in the correct unit of measurement
    as a string. Now that we have a Timedelta Series, we can use the `dt` attribute
    to access the `floor` method, which rounds the time down to the nearest second.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更进一步，将整数形式的毫秒转换为更易于阅读的Timedelta对象。关键是将正确的度量单位作为字符串传递。现在我们有了一个Timedelta Series，可以使用`dt`属性访问`floor`方法，它会将时间舍入到最接近的秒。
- en: 'The query required to answer step 5 involves three tables. We can trim the
    tables down significantly to only the columns we need by passing them to the `columns`
    parameter. When using `merge`, the joining columns are not kept when they have
    the same name. In step 6, we could have assigned a column for the price times
    quantity with the following:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 第五步所需的查询涉及三个表。我们可以通过将它们传递给`columns`参数，大大减少表格内容，仅保留我们需要的列。在使用`merge`时，如果连接列的名称相同，则这些列不会被保留。第六步中，我们本可以通过以下方式分配一个表示价格乘以数量的列：
- en: '[PRE65]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: There is nothing wrong with assigning columns in this manner. We chose to dynamically
    create a new column with the assign method to allow a continuous chain of methods.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式分配列是完全没有问题的。我们选择动态创建一个新的列，使用assign方法以便支持连续的方法链。
- en: There's more...
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'If you are adept with SQL, you can write a SQL query as a string and pass it
    to the `read_sql_query` function. For example, the following will reproduce the
    output from step 4:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉SQL，可以将SQL查询写成字符串并传递给`read_sql_query`函数。例如，以下查询将重现第四步的输出：
- en: '[PRE66]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '![](img/ccb4217b-d635-47d8-bfd3-c5952e29082b.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ccb4217b-d635-47d8-bfd3-c5952e29082b.png)'
- en: 'To reproduce the answer from step 6, use the following SQL query:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 要重现第六步的答案，请使用以下SQL查询：
- en: '[PRE67]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '![](img/307645d3-c9aa-4981-8e8c-eb934a124bf9.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](img/307645d3-c9aa-4981-8e8c-eb934a124bf9.png)'
- en: See also
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: All engine configurations for *SQLAlchemy* ([http://bit.ly/2kb07vV](http://docs.sqlalchemy.org/en/latest/core/engines.html))
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有*SQLAlchemy*的引擎配置（[http://bit.ly/2kb07vV](http://docs.sqlalchemy.org/en/latest/core/engines.html)）
- en: Pandas official documentation on *SQL Queries* ([http://bit.ly/2fFsOQ8](http://pandas.pydata.org/pandas-docs/stable/io.html#sql-queries)
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas官方文档关于*SQL查询*（[http://bit.ly/2fFsOQ8](http://pandas.pydata.org/pandas-docs/stable/io.html#sql-queries)）
