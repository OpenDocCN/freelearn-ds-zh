- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Acquiring and Processing Time Series Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取和处理时间序列数据
- en: In the previous chapter, we learned what a time series is and established some
    standard notation and terminology. Now, let’s switch tracks from theory to practice.
    In this chapter, we are going to get our hands dirty and start working with data.
    Although we said time series data is everywhere, we are yet to start working with
    a few time series datasets. We are going to start working on the dataset we will
    use throughout this book, process it in the right way, and learn about a few techniques
    to deal with missing values.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们学习了什么是时间序列，并建立了一些标准符号和术语。现在，让我们从理论转向实践。在本章中，我们将动手处理数据。虽然我们说时间序列数据无处不在，但我们还没有开始处理一些时间序列数据集。我们将开始处理本书中将要使用的数据集，正确地处理它，并学习一些处理缺失值的技术。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding the time series dataset
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解时间序列数据集
- en: pandas datetime operations, indexing, and slicing—a refresher
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas日期时间操作，索引和切片——刷新一下
- en: Handling missing data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缺失数据
- en: Mapping additional information
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 映射附加信息
- en: Saving and loading files to disk
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文件保存到磁盘和加载文件
- en: Handling longer periods of missing data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理较长时间的缺失数据
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need to set up the **Anaconda** environment, following the instructions
    in the *Preface* of the book, to get a working environment with all the libraries
    and datasets required for the code in this book. Any additional library will be
    installed while running the notebooks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要设置**Anaconda**环境，按照书中*前言*的说明进行操作，以获得包含本书代码所需的所有库和数据集的工作环境。运行笔记本时将安装任何额外的库。
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter02](https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter02).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在[https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter02](https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter02)找到。
- en: Handling time series data is like handling other tabular datasets, only with
    a focus on the temporal dimension. As with any tabular dataset, `pandas` is perfectly
    equipped to handle time series data as well.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 处理时间序列数据就像处理其他表格数据集一样，只是更加注重时间维度。与任何表格数据集一样，`pandas`同样适合处理时间序列数据。
- en: Let’s start getting our hands dirty and work through a dataset from the beginning.
    We are going to use the *London Smart Meters* dataset throughout this book. If
    you have not downloaded the data already as part of the environment setup, go
    to the *Preface* and do that now.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从头开始处理一个数据集。本书将始终使用*伦敦智能电表*数据集。如果您尚未在环境设置中下载数据，请转到*前言*并执行此操作。
- en: Understanding the time series dataset
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解时间序列数据集
- en: This is the key first step in any new dataset you come across, even before **Exploratory
    Data Analysis** (**EDA**), which we will cover in *Chapter 3*, *Analyzing and
    Visualizing Time Series Data*. Understanding where the data comes from, the data
    generating process behind it, and the source domain is essential to having a good
    understanding of the dataset.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您遇到任何新数据集的关键第一步，甚至在进行**探索性数据分析**（**EDA**）之前，我们将在*第三章* *分析和可视化时间序列数据*中进行讨论。了解数据的来源，背后的数据生成过程以及源域对于对数据集有一个良好的理解是至关重要的。
- en: London Data Store, a free and open data-sharing portal, provided this dataset,
    which was collected and enriched by Jean-Michel D and uploaded on Kaggle.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦数据存储库，一个免费开放的数据共享门户，由Jean-Michel D收集并丰富了此数据集，并上传到Kaggle。
- en: 'The dataset contains energy consumption readings for a sample of 5,567 London
    households that took part in the UK Power Networks-led Low Carbon London project
    between November 2011 and February 2014\. Readings were taken at half-hourly intervals.
    Some metadata about the households is also available as part of the dataset. Let’s
    look at what metadata is available as part of the dataset:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含5,567个伦敦家庭的能耗读数，这些家庭参与了英国电力网络主导的低碳伦敦项目，时间跨度从2011年11月到2014年2月。读数间隔为半小时。数据集的一部分还包括一些有关家庭的元数据。让我们看看数据集的元数据包含哪些信息：
- en: CACI UK segmented the UK’s population into demographic types, called Acorn.
    For each household in the data, we have the corresponding Acorn classification.
    The Acorn classes (Lavish Lifestyles, City Sophisticates, Student Life, and so
    on) are grouped into parent classes (Affluent Achievers, Rising Prosperity, Financially
    Stretched, and so on). A full list of Acorn classes can be found in *Table 2.1*.
    The complete documentation detailing each class is available at [https://acorn.caci.co.uk/downloads/Acorn-User-guide.pdf](https://acorn.caci.co.uk/downloads/Acorn-User-guide.pdf).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CACI UK 将英国人口划分为不同的群体，称为 Acorn。对于数据中的每个家庭，我们都有对应的 Acorn 分类。Acorn 类别（奢华生活方式、城市精英、学生生活等）被归类为上级类别（富裕成就者、上升中的繁荣、经济紧张等）。完整的
    Acorn 类别列表可以在*表 2.1*中找到。详细列出每个类别的完整文档可以在 [https://acorn.caci.co.uk/downloads/Acorn-User-guide.pdf](https://acorn.caci.co.uk/downloads/Acorn-User-guide.pdf)
    中查看。
- en: The dataset contains two groups of customers—one group who was subjected to
    **dynamic time-of-use** (**dToU**) energy prices throughout 2013, and another
    group who were on flat-rate tariffs. The tariff prices for the dToU were given
    a day ahead, via the smart meter IHD or text message.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集包含两组客户——一组在2013年使用**动态时间段电价**（**dToU**），另一组则采用固定费率电价。dToU的电价会提前一天通过智能电表的IHD或短信告知。
- en: Jean-Michel D also enriched the dataset with weather and UK bank holiday data.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jean-Michel D 还为数据集增加了天气和英国法定假期的数据。
- en: 'The following table shows the Acorn classes:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 下表显示了 Acorn 分类：
- en: '| **Acorn Group** | **Acorn Class** |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **Acorn 分组** | **Acorn 分类** |'
- en: '| Affluent Achievers | A-Lavish Lifestyles |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 富裕成就者 | A-奢华生活方式 |'
- en: '| B-Executive Wealth |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| B-高管财富 |'
- en: '| C-Mature Money |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| C-成熟财富 |'
- en: '| Rising Prosperity | D-City Sophisticates |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 上升中的繁荣 | D城精英 |'
- en: '| E-Career Climbers |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| E-职业攀升者 |'
- en: '| Comfortable Communities | F-Countryside Communities |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 舒适社区 | F-乡村社区 |'
- en: '| G-Successful Suburbs |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| G-成功的郊区 |'
- en: '| H-Steady Neighborhoods |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| H-稳定的社区 |'
- en: '| I-Comfortable Seniors |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| I-舒适老年人 |'
- en: '| J-Starting Out |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| J-起步阶段 |'
- en: '| Financially Stretched | K-Student Life |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 经济紧张 | K-学生生活 |'
- en: '| L-Modest Means |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| L-普通收入 |'
- en: '| M-Striving Families |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| M-奋斗中的家庭 |'
- en: '| N-Poorer Pensioners |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| N-贫困的退休人员 |'
- en: '| Urban Adversity | O-Young Hardship |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 城市困境 | O-Young艰难困苦 |'
- en: '| P-Struggling Estates |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| P-挣扎中的地产 |'
- en: '| Q-Difficult Circumstances |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Q-困境中的人群 |'
- en: 'Table 2.1: Acorn classification'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.1：Acorn 分类
- en: The Kaggle dataset also preprocesses the time series data daily and combines
    all the separate files. Here, we will ignore those files and start with the raw
    files, which can be found in the `hhblock_dataset` folder. Learning to work with
    raw files is an integral part of working with real-world datasets in the industry.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle 数据集也对时间序列数据进行了每日预处理，并将所有单独的文件合并在一起。在这里，我们将忽略这些文件，从原始文件开始，这些文件可以在 `hhblock_dataset`
    文件夹中找到。学习处理原始文件是从事实际行业数据集工作的重要部分。
- en: Preparing a data model
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据模型
- en: Once we understand where the data comes from, we can look at it, understand
    the information present in the different files, and figure out a mental model
    of how to relate the different files. You may call it old-school, but Microsoft
    Excel is an excellent tool for gaining this first-level understanding. If the
    file is too big to open in Excel, we can also read it in Python, save a sample
    of the data to an Excel file, and open it. However, keep in mind that Excel sometimes
    messes with the format of the data, especially dates, so we need to take care
    to not save the file and write back the formatting changes Excel made. If you
    are allergic to Excel, you can do it in Python as well, albeit with a lot more
    keystrokes. The purpose of this exercise is to see what the different data files
    contain, explore the relationship between the different files, and so on.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们理解了数据的来源，就可以查看它，理解不同文件中所包含的信息，并构建一个思维模型来关联不同的文件。你可以称之为传统方式，但Microsoft Excel
    是一个非常适合获取这种第一层次理解的工具。如果文件太大以至于无法在Excel中打开，我们也可以在Python中读取它，将一部分数据保存到Excel文件中，然后打开它。然而，请记住，Excel有时会乱改数据格式，尤其是日期格式，所以我们需要小心不要保存文件并写回Excel所做的格式更改。如果你对Excel有抵触，可以使用Python完成，尽管会需要更多的键盘操作。这个练习的目的是查看不同的数据文件包含了什么，探索不同文件之间的关系等。
- en: 'We can make this more formal and explicit by drawing a data model, similar
    to the one shown in the following diagram:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过绘制一个数据模型，使其更正式和明确，类似于以下图示：
- en: '![Figure 2.1 – Data model of the London Smart Meters dataset ](img/B22389_02_01.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 – 伦敦智能电表数据集的数据模型](img/B22389_02_01.png)'
- en: 'Figure 2.1: Data model of the London Smart Meters dataset'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：伦敦智能电表数据集的数据模型
- en: The data model is more for us to understand the data rather than any data engineering
    purpose. Therefore, it only contains bare-minimum information, such as the key
    columns on the left and the sample data on the right. We also have arrows connecting
    different files, with keys used to link the files.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据模型主要是帮助我们理解数据，而不是任何数据工程的目的。因此，它只包含最基本的信息，如左侧的关键列和右侧的示例数据。我们还有箭头连接不同的文件，并使用键来关联这些文件。
- en: 'Let’s look at a few key column names and their meanings:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下几个关键列名及其含义：
- en: '`LCLid`: The unique consumer ID for a household'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LCLid`：家庭的唯一消费者 ID'
- en: '`stdorTou`: Whether the household has dToU or standard tarriff'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stdorTou`：家庭是否使用 dToU 或标准收费'
- en: '`Acorn`: The ACORN class'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Acorn`：ACORN 类'
- en: '`Acorn_grouped`: The ACORN group'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Acorn_grouped`：ACORN 组'
- en: '`file`: The block number'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`file`：块编号'
- en: Each `LCLid` has a unique time series attached to it. The time series file is
    formatted in a slightly tricky format—each day, there will be 48 observations
    at a half-hourly frequency in the columns of the file.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 `LCLid` 都有一个唯一的时间序列与之关联。该时间序列文件采用一种稍微复杂的格式——每天，文件的列中会有 48 个观测值，以半小时为频率。
- en: '**Notebook alert**:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提示**：'
- en: To follow along with the complete code, use the `01-Pandas_Refresher_&_Missing_Values_Treatment.ipynb`
    notebook in the `Chapter01` folder.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随完整的代码，请使用 `Chapter01` 文件夹中的 `01-Pandas_Refresher_&_Missing_Values_Treatment.ipynb`
    笔记本。
- en: Before we start working with our dataset, there are a few concepts we need to
    establish. One of them is a concept in pandas DataFrames, which is of utmost importance—the
    pandas datetime properties and index. Let’s quickly look at a few `pandas` concepts
    that will be useful.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始处理数据集之前，有几个概念需要我们明确。其中一个在 pandas DataFrame 中至关重要的概念是 pandas 日期时间属性和索引。让我们快速回顾几个对我们有用的
    `pandas` 概念。
- en: If you are familiar with the datetime manipulations in pandas, feel free to
    skip ahead to the next section.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉 pandas 中的日期时间操作，可以跳到下一节。
- en: pandas datetime operations, indexing, and slicing—a refresher
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: pandas 日期时间操作、索引和切片——回顾
- en: 'Instead of using our dataset, which is slightly complex, let’s pick an easy,
    well-formatted stock exchange price dataset from the UCI Machine Learning Repository
    and look at the functionality of pandas:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不使用稍微复杂的数据集，而是选择一个简单且格式良好的股票交易价格数据集，来自 UCI 机器学习库，来查看 pandas 的功能：
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The DataFrame that we read looks as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们读取的 DataFrame 如下所示：
- en: '![Figure 2.2 – The DataFrame with stock exchange prices ](img/B22389_02_02.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – 包含股票交易价格的 DataFrame](img/B22389_02_02.png)'
- en: 'Figure 2.2: The DataFrame with stock exchange prices'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：包含股票交易价格的 DataFrame
- en: Now that we have read the `DataFrame`, let’s start manipulating it.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经读取了 `DataFrame`，让我们开始处理它。
- en: Converting the date columns into pd.Timestamp/DatetimeIndex
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将日期列转换为 `pd.Timestamp`/`DatetimeIndex`
- en: 'First, we must convert the date column (which may not always be parsed as dates
    automatically by pandas) into `pandas` datetime format. For that, `pandas` has
    a handy function called `pd.to_datetime`. It infers the datetime format automatically
    and converts the input into a `pd.Timestamp`, if the input is a `string`, or into
    a `DatetimeIndex`, if the input is a `list` of strings. So if we pass a single
    date as a string, `pd.to_datetime` converts it into `pd.Timestamp`, while if we
    pass a list of dates, it converts it into `DatetimeIndex`. Let’s also use a handy
    function, `strftime`, which formats the date representation into a format we specify.
    It uses `strftime` conventions to specify the format of the data. For instance,
    `%d` means a zero-padded date, `%B` means a month’s full name, and `%Y` means
    a year in four digits. A full list of `strftime` conventions can be found at [https://strftime.org/](https://strftime.org/):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须将日期列（该列可能并不会被 pandas 自动解析为日期）转换为 `pandas` 的日期时间格式。为此，`pandas` 提供了一个非常方便的函数叫做
    `pd.to_datetime`。它会自动推断日期时间格式，并将输入转换为 `pd.Timestamp`（如果输入是一个 `string`）或 `DatetimeIndex`（如果输入是一个字符串的
    `list`）。因此，如果我们传入单一的日期字符串，`pd.to_datetime` 会将其转换为 `pd.Timestamp`，而如果我们传入一组日期，它会将其转换为
    `DatetimeIndex`。我们还可以使用一个方便的函数，`strftime`，它可以将日期格式化为我们指定的格式。它使用 `strftime` 的约定来指定数据的格式。例如，`%d`
    代表零填充的日期，`%B` 代表月份的全名，`%Y` 代表四位数字的年份。`strftime` 约定的完整列表可以在 [https://strftime.org/](https://strftime.org/)
    查找：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let’s look at a case where the automatic parsing fails. The date is January
    4, 1987\. Let’s see what happens when we pass the string to the function:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一个自动解析失败的案例。日期是 1987 年 1 月 4 日。让我们看看将该字符串传递给函数时会发生什么：
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Well, that wasn’t expected, right? But if you think about it, anyone can make
    that mistake because we are not telling the computer whether the month or the
    day comes first, and pandas assumes the month comes first. Let’s rectify that:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，那并不是预期的结果，对吧？不过仔细想想，任何人都可能犯这个错误，因为我们没有告诉计算机是月在前还是日，在这种情况下 pandas 假定月份在前。让我们来纠正一下：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Another case where automatic date parsing fails is when the date string is
    in a non-standard form. In that case, we can provide a `strftime`-formatted string
    to help pandas parse the dates correctly:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 自动日期解析失败的另一个情况是当日期字符串采用非标准形式时。在这种情况下，我们可以提供一个`strftime`格式化字符串，帮助 pandas 正确解析日期：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A full list of `strftime` conventions can be found at [https://strftime.org/](https://strftime.org/).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`strftime` 格式化规范的完整列表可以在[https://strftime.org/](https://strftime.org/)找到。'
- en: '**Practioner’s tip**:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**实践者提示**：'
- en: 'Because of the wide variety of data formats, pandas may infer the time incorrectly.
    While reading a file, pandas will try to parse dates automatically and create
    an error. There are many ways we can control this behavior: we can use the `parse_dates`
    flag to turn off date parsing, the `date_parser` argument to pass in a custom
    date parser, and `year_first` and `day_first` to easily denote two popular formats
    of dates. From version 2.0, pandas supports `date_format`, which can be used to
    pass in the exact format of the date as a Python dictionary, with the column name
    as the key.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据格式的多样性，pandas 可能会错误地推断时间。在读取文件时，pandas 会尝试自动解析日期并创建错误。有许多方法可以控制这种行为：我们可以使用
    `parse_dates` 标志来关闭日期解析，使用 `date_parser` 参数传入自定义日期解析器，以及使用 `year_first` 和 `day_first`
    来轻松表示两种常见的日期格式。从 2.0 版本开始，pandas 支持 `date_format`，可以用来传入日期的确切格式，作为一个 Python 字典，列名作为键。
- en: Out of all these options, I prefer to use `date_format`, if using `pandas` >=2.0\.
    We can keep `parse_dates=True` and then pass in the exact date format, using `strftime`
    conventions. This ensures that the date is parsed in the way we want it to be.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些选项中，如果使用 `pandas` >=2.0，我更倾向于使用 `date_format`。我们可以保持 `parse_dates=True`，然后传入确切的日期格式，使用
    `strftime` 格式化规范。这确保了日期按照我们希望的方式被解析。
- en: If working with `pandas` <2.0, then I prefer to keep `parse_dates=False` in
    both `pd.read_csv` and `pd.read_excel` to make sure that pandas does not parse
    the data automatically. After that, you can convert the date using the `format`
    parameter, which lets you explicitly set the date format of the column using `strftime`
    conventions. There are two other parameters in `pd.to_datetime` that will also
    make inferring dates less error-prone—`yearfirst` and `dayfirst`. If you don’t
    provide an explicit date format, at least provide one of these.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用的是 `pandas` <2.0，那么我更倾向于在 `pd.read_csv` 和 `pd.read_excel` 中都保持 `parse_dates=False`，以确保
    pandas 不会自动解析数据。之后，你可以使用 `format` 参数转换日期，该参数允许你显式设置列的日期格式，采用 `strftime` 格式化规范。`pd.to_datetime`
    中还有两个参数可以减少推断日期时的错误——`yearfirst` 和 `dayfirst`。如果没有提供显式的日期格式，至少要提供其中一个。
- en: 'Now, let’s convert the date column in our stock prices dataset into datetime:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们把我们股票价格数据集中的日期列转换为日期时间格式：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, the `'date'` column, `dtype`, should be either `datetime64[ns]` or `<M8[ns]`,
    which are both pandas/NumPy-native datetime formats. But why do we need to do
    this?
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`'date'` 列的 `dtype` 应该是 `datetime64[ns]` 或 `<M8[ns]`，这两者都是 pandas/NumPy 本地的日期时间格式。但为什么我们需要这么做呢？
- en: 'It’s because of the wide range of additional functionalities this unlocks.
    The traditional `min()` and `max()` functions will start working because pandas
    knows it is a datetime column:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为它解锁了大量额外的功能。传统的 `min()` 和 `max()` 函数开始生效，因为 pandas 知道这是一个日期时间列：
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s look at a few cool features that the datetime format gives us.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下日期时间格式带来的一些酷炫特性。
- en: Using the .dt accessor and datetime properties
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 `.dt` 访问器和日期时间属性
- en: 'Since the column is now in date format, all the semantic information that is
    encoded in the date can be used through pandas datetime properties. We can access
    many datetime properties, such as `month`, `day_of_week`, `day_of_year`, and so
    on, using the `.dt` accessor:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于列现在是日期格式，所有包含在日期中的语义信息都可以通过 pandas datetime 属性使用。我们可以使用 `.dt` 访问器访问许多日期时间属性，例如
    `month`、`day_of_week`、`day_of_year` 等等：
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As of pandas 1.1.0, `week_of_year` has been deprecated because of the inconsistencies
    it produces at the end/start of the year. Instead, the ISO calendar standards
    (which are commonly used in government and business) have been adopted, and we
    can access the ISO calendar to get the ISO weeks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从 pandas 1.1.0 开始，`week_of_year` 已被弃用，因为它在年末/年初产生不一致的结果。取而代之的是采用了 ISO 日历标准（该标准广泛应用于政府和企业中），我们可以访问
    ISO 日历以获取 ISO 周。
- en: Indexing and slicing
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 索引和切片
- en: 'The real fun starts when we make the date column the index of the DataFrame.
    By doing this, you can use all the fancy slicing operations that pandas supports
    but on the datetime axis. Let’s take a look at a few of them:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将日期列设为 DataFrame 的索引时，真正有趣的部分开始了。通过这样做，你可以在 datetime 轴上使用 pandas 支持的所有花式切片操作。我们来看看其中的几个操作：
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In addition to the semantic information and intelligent indexing and slicing,
    `pandas` also provide tools to create and manipulate date sequences.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 除了语义信息和智能索引及切片功能外，`pandas` 还提供了创建和操作日期序列的工具。
- en: Creating date sequences and managing date offsets
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建日期序列和管理日期偏移
- en: 'If you are familiar with `range` in Python and `np.arange` in NumPy, then you
    will know they help us create `integer/float` sequences by providing a start point
    and an end point. pandas has something similar for datetime—`pd.date_range`. The
    function accepts start and end dates, along with a frequency (daily, monthly,
    and so on), and creates the sequence of dates in between. Let’s look at a couple
    of ways to create a sequence of dates:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉 Python 中的 `range` 和 NumPy 中的 `np.arange`，那么你就知道它们帮助我们通过提供起始点和结束点来创建 `整数/浮动`
    序列。pandas 也有类似的功能来处理日期时间——`pd.date_range`。该函数接受起始日期和结束日期，以及频率（日、月等），并在这两个日期之间创建日期序列。我们来看看几种创建日期序列的方式：
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can also add or subtract days, months, and other values to/from dates using
    `pd.TimeDelta`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 `pd.TimeDelta` 来给日期加减天数、月份和其他值：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: There are a lot of these aliases in `pandas`, including `W`, `W-MON`, `MS`,
    and others. The full list can be found at [https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases](https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `pandas` 中有很多这样的别名，包括 `W`、`W-MON`、`MS` 等。完整列表可以在 [https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases](https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases)
    查找到。
- en: In this section, we looked at a few useful features and operations we can perform
    on datetime indices and know how to manipulate DataFrames with datetime columns.
    Now, let’s review a few techniques we can use to deal with missing data.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们了解了在 datetime 索引上可以执行的一些有用特性和操作，并且知道如何操作包含 datetime 列的 DataFrame。现在，让我们回顾一下几种可以处理缺失数据的技术。
- en: Handling missing data
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理缺失数据
- en: 'While dealing with large datasets in the wild, you are bound to encounter missing
    data. If it is not part of the time series, it may be part of the additional information
    you collect and map. Before we jump the gun and fill it with a mean value or drop
    those rows, let’s consider a few aspects:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大规模数据集时，难免会遇到缺失数据。如果它不是时间序列的一部分，可能是你收集和映射的附加信息的一部分。在我们仓促地用均值填充或删除这些行之前，先考虑一下几个方面：
- en: The first consideration should be whether the missing data we are worried about
    is missing or not. For that, we need to think about the **Data Generating Process**
    (**DGP**) (the process that generates the time series). As an example, let’s look
    at sales at a local supermarket. You have been given the **point-of-sale** (**POS**)
    transactions for the last 2 years, and you are processing the data into a time
    series. While analyzing the data, you found that there are a few products where
    there aren’t any transactions for a few days. Now, what you need to think about
    is whether the missing data is missing or whether there is some information that
    this missingness gives you. If you don’t have any transactions for a particular
    product for a day, it will appear as missing data while you are processing it,
    even though it is not missing. What that tells us is that there were no sales
    for that item and that you should fill such missing data with zeros.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先需要考虑的是，我们担心的缺失数据到底是缺失还是其他原因。为此，我们需要考虑**数据生成过程**（**DGP**）（生成时间序列的过程）。举个例子，假设我们在处理某个本地超市的销售数据。你已经得到了过去2年的**销售点**（**POS**）交易数据，并正在将数据处理成时间序列。在分析数据时，你发现有一些产品几天内没有交易记录。现在，你需要考虑的是，这些缺失数据是真的缺失，还是缺失本身就能提供一些信息。如果某个产品一天没有交易记录，它在处理时会被视为缺失数据，尽管实际上它并不缺失。实际上，这意味着那天没有销售，因此你应该用零来填补这些缺失数据。
- en: Now, what if you see that, every Sunday, the data is missing—that is, there
    is a pattern to the missingness? This becomes tricky because how you fill in such
    gaps depends on the model that you intend to use. If you fill in such gaps with
    zeros, a model that looks at the immediate past to predict the future might be
    thrown off, especially for Monday. However, if you tell the model that the previous
    day was Sunday, then the model still can learn to tell the difference.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，如果你发现每个星期天数据都缺失——也就是说，缺失有规律可循，怎么办？这就变得有些棘手，因为如何填补这些空缺取决于你打算使用的模型。如果你用零填补这些空缺，那么一个基于近期数据预测未来的模型可能会受到影响，尤其是在周一的预测中。然而，如果你告诉模型前一天是星期天，那么模型仍然能够学会区分这些情况。
- en: Lastly, what if you see zero sales on one of the best-selling products that
    always gets sold? This can happen because of something such as a POS machine malfunction,
    a data entry mistake, or an out-of-stock situation. These types of missing values
    can be imputed with a few techniques.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，如果你看到某个热销产品的销售量为零，而它通常总是有销售，这该怎么办？这可能是因为某些原因，如POS机故障、数据录入错误或缺货等情况。可以使用一些技术来填补这类缺失值。
- en: Let’s look at an Air Quality dataset published by the ACT Government, Canberra,
    Australia, under the CC by Attribution 4.0 International License ([https://www.data.act.gov.au/Environment/Air-Quality-Monitoring-Data/94a5-zqnn](https://www.data.act.gov.au/Environment/Air-Quality-Monitoring-Data/94a5-zqnn))
    and see how we can impute such values using pandas (there are more sophisticated
    techniques available, all of which will be covered later in this chapter).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看澳大利亚堪培拉ACT政府发布的一个空气质量数据集（根据CC by Attribution 4.0国际许可证发布，链接：[https://www.data.act.gov.au/Environment/Air-Quality-Monitoring-Data/94a5-zqnn](https://www.data.act.gov.au/Environment/Air-Quality-Monitoring-Data/94a5-zqnn)），并看看我们如何使用
    pandas 填充这些缺失值（本章稍后将介绍更多复杂的技术）。
- en: '**Practitioner’s tip**:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**实践者提示**：'
- en: When reading data using a method such as `read_csv`, pandas provides a few handy
    ways to handle missing values. pandas treats values such as `#N/A`, `null`, and
    so on as `NaN` by default. We can control this list of allowable `NaN` values
    using the `na_values` and `keep_default_na` parameters.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`read_csv`等方法读取数据时，pandas 提供了几种便捷的方式来处理缺失值。默认情况下，pandas 会将`#N/A`、`null`等值视为`NaN`。我们可以使用`na_values`和`keep_default_na`参数来控制允许的`NaN`值列表。
- en: 'We have chosen region **Monash** and **PM2.5** readings, and artificially introduced
    some missing values, as shown in the following diagram:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了**Monash**地区和**PM2.5**的读数，并人为地引入了一些缺失值，如下图所示：
- en: '![Figure 2.3 – Missing values in the Air Quality dataset ](img/B22389_02_03.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图2.3 – 空气质量数据集中的缺失值](img/B22389_02_03.png)'
- en: 'Figure 2.3: Missing values in the Air Quality dataset'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3：空气质量数据集中的缺失值
- en: 'Now, let’s look at a few simple techniques we can use to fill in the missing
    values:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一些可以用来填充缺失值的简单技术：
- en: '**Last Observation Carried Forward or Forward Fill**: This imputation technique
    takes the last observed value, using that to fill in all the missing values until
    it finds the next observation. This is also called forward fill. We can do this
    like so:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最后观察值前向填充或前向填充**：这个插补技术使用最后一个观察值填充所有缺失值，直到遇到下一个观察值。这也被称为前向填充。我们可以这样操作：'
- en: '[PRE11]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Next Observation Carried Backward of Backward Fill**: This imputation technique
    takes the next observation and backtracks to fill in all the missing values with
    this value. This is also called backward fill. Let’s see how we can do this in
    pandas:'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下一个观察值后向填充或后向填充**：这个插补技术使用下一个观察值，回溯并用该值填充所有缺失值。这也被称为后向填充。让我们来看看如何在 pandas
    中实现这一点：'
- en: '[PRE12]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Mean Value Fill**: This imputation technique is also pretty simple. We calculate
    the mean of the entire series, and wherever we find missing values, we fill it
    with the mean value:'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均值填充**：这个插补技术也相当简单。我们计算整个序列的均值，并在遇到缺失值时用均值填充：'
- en: '[PRE13]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s plot the imputed lines we get from using these three techniques:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制使用这三种技术得到的插补线：
- en: '![Figure 2.4 – Imputed missing values using forward, backward, and mean value
    fill ](img/B22389_02_04.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.4 – 使用前向填充、后向填充和均值填充插补缺失值](img/B22389_02_04.png)'
- en: 'Figure 2.4: Imputed missing values using forward, backward, and mean value
    fill'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.4: 使用前向填充、后向填充和均值填充插补缺失值'
- en: 'Another family of imputation techniques covers interpolation:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类插补技术涉及插值：
- en: '**Linear Interpolation**: Linear interpolation is just like drawing a line
    between the two observed points and filling in the missing values so that they
    lie on this line. This is how we do it:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性插值**：线性插值就像是在两个观察点之间画一条直线，并填充缺失值使其位于这条直线上。我们可以这样操作：'
- en: '[PRE14]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Nearest Interpolation**: This is intuitively like a combination of the forward
    and backward fill. For each missing value, the closest observed value is found
    and used to fill in the missing value:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最近邻插值**：这直观上就像是前向填充和后向填充的结合。对于每个缺失值，找到最接近的观察值并用它来填充缺失值：'
- en: '[PRE15]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s plot the two interpolated lines:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制这两条插值线：
- en: '![Figure 2.5 – Imputed missing values using linear and nearest interpolation
    ](img/B22389_02_05.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.5 – 使用线性插值和最近邻插值插补缺失值](img/B22389_02_05.png)'
- en: 'Figure 2.5: Imputed missing values using linear and nearest interpolation'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.5: 使用线性插值和最近邻插值插补缺失值'
- en: 'There are a few non-linear interpolation techniques as well:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些非线性插值技术：
- en: '**Spline, Polynomial, and Other Interpolations**: In addition to linear interpolation,
    pandas also supports non-linear interpolation techniques that call a SciPy routine
    at the backend. Spline and polynomial interpolations are similar. They fit a spline/polynomial
    of a given order to the data and use that to fill in missing values. While using
    `spline` or `polynomial` as the method in `interpolate`, we should always provide
    `order` as well. The higher the order, the more flexible the function that is
    used to fit the observed points will be. Let’s see how we can use spline and polynomial
    interpolation:'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样条插值、多项式插值和其他插值方法**：除了线性插值外，pandas 还支持非线性插值技术，这些技术会在后台调用 SciPy 函数。样条插值和多项式插值相似，它们会为数据拟合一个给定阶数的样条/多项式，并用它来填补缺失值。在使用
    `spline` 或 `polynomial` 作为 `interpolate` 方法时，我们应该始终提供 `order` 参数。阶数越高，拟合观察点的函数就越灵活。让我们来看一下如何使用样条插值和多项式插值：'
- en: '[PRE16]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s plot these two non-linear interpolation techniques:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制这两种非线性插值技术：
- en: '![Figure 2.6 – Imputed missing values using spline and polynomial interpolation
    ](img/B22389_02_06.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.6 – 使用样条插值和多项式插值插补缺失值](img/B22389_02_06.png)'
- en: 'Figure 2.6: Imputed missing values using spline and polynomial interpolation'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.6: 使用样条插值和多项式插值插补缺失值'
- en: For a complete list of interpolation techniques supported by `interpolate`,
    go to [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html)
    and [https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html#scipy.interpolate.interp1d](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html#scipy.interpolate.interp1d).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 有关`interpolate`支持的所有插值技术，请访问[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html)和[https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html#scipy.interpolate.interp1d](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html#scipy.interpolate.interp1d)。
- en: Now that we are more comfortable with the way pandas manages datetime, let’s
    go back to our dataset and convert the data into a more manageable form.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经更熟悉pandas如何处理datetime类型的数据，接下来让我们回到数据集，并将数据转换成更易于管理的形式。
- en: '**Notebook alert**:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提示**：'
- en: To follow along with the complete code for pre-processing, use the `02-Preprocessing_London_Smart_Meter_Dataset.ipynb`
    notebook in the `Chapter02` folder.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 若要查看完整的预处理代码，请使用`Chapter02`文件夹中的`02-Preprocessing_London_Smart_Meter_Dataset.ipynb`笔记本。
- en: Converting the half-hourly block-level data (hhblock) into time series data
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将半小时区块数据（hhblock）转换为时间序列数据
- en: 'Before we start processing, let’s understand a few general categories of information
    we will find in a time series dataset:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始处理之前，让我们了解一下时间序列数据集中可能会出现的几类信息：
- en: '**Time Series Identifiers**: These are identifiers for a particular time series.
    It can be a name, an ID, or any other unique feature—for example, the SKU name
    or the ID of a retail sales dataset or the consumer ID in the energy dataset that
    we are working with are all time series identifiers.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列标识符**：这些是特定时间序列的标识符。它可以是一个名称、一个ID，或任何其他唯一的特征——例如，我们正在处理的零售销售数据集中的SKU名称、消费者ID，或者能源数据集中的消费者ID，都是时间序列标识符。'
- en: '**Metadata or Static Features**: This information does not vary with time.
    An example of this is the ACORN classification of the household in our dataset.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元数据或静态特征**：这些信息不随时间变化。例如，我们数据集中的ACORN家庭分类就是静态特征。'
- en: '**Time-Varying Features**: This information varies with time—for example, the
    weather information. For each point in time, we have a different value for weather,
    unlike the Acorn classification.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间变化特征**：这些信息会随时间变化——例如，天气信息。对于每一个时间点，我们都有不同的天气值，而不像Acorn分类那样固定。'
- en: Next, let’s discuss formatting of a dataset.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论一下数据集的格式化。
- en: Compact, expanded, and wide forms of data
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 紧凑格式、扩展格式和宽格式数据
- en: There are many ways to format a time series dataset, especially a dataset with
    many related time series, like the one we have now. A standard way of doing this
    is **wide** data. This is where the date column becomes sort of an index and each
    time series occupies a different column. And if there are a million time series,
    it will have a million and one columns (hence the term wide). Apart from the standard
    **wide** data, we can also look at two non-standard ways to format time series
    data. Although there is no standard nomenclature for them, we will refer to them
    as **compact** and **expanded** in this book. The expanded form is also referred
    to as **long** in some literature.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多方式可以格式化时间序列数据集，特别是像我们现在处理的这种包含多个相关时间序列的数据集。一种标准的方式是**宽格式**数据。这里日期列相当于索引，每个时间序列占据一个不同的列。如果有一百万个时间序列，就会有一百万零一个列（因此称为“宽格式”）。除了标准的**宽格式**数据，我们还可以考虑两种非标准的时间序列数据格式。尽管这些格式没有统一的命名，我们将在本书中将其称为**紧凑格式**和**扩展格式**。扩展格式在一些文献中也被称为**长格式**。
- en: 'Compact-form data is when any particular time series occupies only a single
    row in the pandas DataFrame—that is, the time dimension is managed as an array
    within a DataFrame row. The time series identifiers and the metadata occupy the
    columns with scalar values and then the time series values; other time-varying
    features occupy the columns with an array. Two additional columns are included
    to extrapolate time—`start_datetime` and `frequency`. If we know the start datetime
    and the frequency of the time series, we can easily construct the time and recover
    the time series from the DataFrame. This only works for regularly sampled time
    series. The advantage is that the DataFrames take up much less memory and are
    easy and faster to work with:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 紧凑格式数据是指在 pandas DataFrame 中，每个时间序列只占据一行——也就是说，时间维度作为一个数组被管理在 DataFrame 的一行中。时间序列标识符和元数据占据列并包含标量值，接着是时间序列的值；其他随时间变化的特征占据列并包含数组。还会额外添加两列来推断时间——`start_datetime`
    和 `frequency`。如果我们知道时间序列的开始日期和频率，就可以轻松构造时间并从 DataFrame 中恢复时间序列。这仅适用于定期采样的时间序列。其优点是
    DataFrame 占用的内存更少，处理起来也更加容易和快速：
- en: '![Figure 2.7 – Compact form data ](img/B22389_02_07.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.7 – 紧凑格式数据](img/B22389_02_07.png)'
- en: 'Figure 2.7: Compact-form data'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7：紧凑格式数据
- en: 'The expanded form is when the time series is expanded along the rows of a DataFrame.
    If there are *n* steps in the time series, it occupies *n* rows in the DataFrame.
    The time series identifiers and the metadata get repeated along all the rows.
    The time-varying features also get expanded along the rows. Also, instead of the
    start date and frequency, we have the timestamp as a column:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展格式是指时间序列沿着 DataFrame 的行进行展开。如果时间序列中有 *n* 步，它将在 DataFrame 中占据 *n* 行。时间序列标识符和元数据会沿着所有行重复。随时间变化的特征也会沿着行展开。此外，代替开始日期和频率，我们将使用时间戳作为一列：
- en: '![Figure 2.8 – Expanded form data ](img/B22389_02_08.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.8 – 扩展格式数据](img/B22389_02_08.png)'
- en: 'Figure 2.8: Expanded-form data'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8：扩展格式数据
- en: If the compact form had a time series identifier as the key, the time series
    identifier and the datetime column would be combined and become the key.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果紧凑格式中有时间序列标识符作为键，那么时间序列标识符和日期时间列将被组合并成为键。
- en: Wide-format data is more common in traditional time series literature. It can
    be considered a legacy format, which is limiting in many ways. Do you remember
    the stock data we saw earlier (*Figure 2.2*)? We have the date as an index or
    one of the columns, and the different time series as different columns of the
    DataFrame. As the number of time series increases, they become wider and wider,
    hence the name. This data format does not allow us to include any metadata about
    the time series. For instance, in our data, we have information about whether
    a particular household is under standard or dynamic pricing. There is no way for
    us to include such metadata in the wide format. From an operational perspective,
    the wide format also does not play well with relational databases because we have
    to keep adding columns to a table when we get new time series. We won’t be using
    this format in this book.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 宽格式数据在传统时间序列文献中更为常见。它可以视为一种遗留格式，在许多方面都有局限性。你还记得我们之前看到的股票数据（*图 2.2*）吗？我们将日期作为索引或列之一，将不同的时间序列作为
    DataFrame 的不同列。随着时间序列数量的增加，它们变得越来越宽，因此得名。这种数据格式无法包含任何关于时间序列的元数据。例如，在我们的数据中，我们有关于某个家庭是否使用标准定价或动态定价的信息。在宽格式中我们无法包含这种元数据。从操作角度来看，宽格式也不太适合关系型数据库，因为每当我们获取新的时间序列时，我们就必须不断向表中添加列。在本书中，我们将不会使用这种格式。
- en: Enforcing regular intervals in time series
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强制时间序列的定期间隔
- en: One of the first things you should check and correct is whether the regularly
    sampled time series data that you have has equal intervals of time. In practice,
    even regularly sampled time series have some samples missing in between, due to
    some data collection error or some other peculiar way data is collected. So while
    working with the data, we will make sure we enforce regular intervals in the time
    series.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该首先检查和纠正的事情之一是你所拥有的定期采样时间序列数据是否具有相等的时间间隔。实际上，即使是定期采样的时间序列，也可能由于数据收集错误或其他数据采集方式的特殊性，出现缺失的样本。因此，在处理数据时，我们会确保强制执行时间序列中的定期间隔。
- en: '**Best practice**:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**：'
- en: While working with datasets with multiple time series, it is best practice to
    check the end dates of all the time series. If they are not uniform, we can align
    them with the latest date across all the time series in the dataset.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理包含多个时间序列的数据集时，最好检查所有时间序列的结束日期。如果它们不一致，我们可以将它们对齐到数据集中所有时间序列的最新日期。
- en: In our smart meters dataset, some `LCLid` columns end much earlier than the
    rest. Maybe the household opted out of the program, or they moved out and left
    the house empty; the reason could be anything. However, we need to handle that
    while we enforce regular intervals.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的智能电表数据集中，一些 `LCLid` 列比其他列结束得早。也许是家庭选择退出该计划，或者他们搬走了并且房子空置了，原因可以有很多。但我们需要处理这一点，同时保持规则的时间间隔。
- en: We will learn how to convert the dataset into a time series format in the next
    section. The code for this process can be found in the `02-Preprocessing_London_Smart_Meter_Dataset.ipynb`
    notebook.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节学习如何将数据集转换为时间序列格式。此过程的代码可以在 `02-Preprocessing_London_Smart_Meter_Dataset.ipynb`
    笔记本中找到。
- en: Converting the London Smart Meters dataset into a time series format
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将伦敦智能电表数据集转换为时间序列格式
- en: For each dataset that you come across, the steps you would have to take to convert
    it into either a compact or expanded form would be different. It depends on how
    the original data is structured. Here, we will look at how the London Smart Meters
    dataset can be transformed so that we can transfer those learnings to other datasets.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个你遇到的数据集，转换成紧凑形式或扩展形式的步骤会有所不同。这取决于原始数据的结构。这里，我们将展示如何转换伦敦智能电表数据集，以便将这些经验转移到其他数据集。
- en: 'There are two steps we need to do before we can start processing the data into
    either compact or expanded form:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始处理数据为紧凑或扩展形式之前，我们需要完成两个步骤：
- en: '**Find the Global End Date**: We must find the maximum date across all the
    block files so that we know the global end date of the time series.'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**找到全局结束日期**：我们必须找到所有区块文件中的最大日期，以便知道时间序列的全局结束日期。'
- en: '**Basic Preprocessing**: If you remember how `hhblock_dataset` is structured,
    you will remember that each row had a date and that, along the columns, we have
    half-hourly blocks. We need to reshape that into a long form, where each row has
    a date and a single half-hourly block. It’s easier to handle that way.'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基本预处理**：如果你记得 `hhblock_dataset` 的结构，你会记得每一行都有一个日期，而列中有半小时的区块。我们需要将其重塑为长格式，其中每行有一个日期和一个半小时的区块。这样处理更方便。'
- en: Now, let’s define separate functions to convert data into compact and expanded
    forms and `apply` those functions to each of the `LCLid` columns. We will do this
    for each `LCLid` separately, since the start date for each `LCLid` is different.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义分别转换数据为紧凑形式和扩展形式的函数，并将这些函数 `apply` 到每个 `LCLid` 列。我们将为每个 `LCLid` 分别执行此操作，因为每个
    `LCLid` 的开始日期不同。
- en: Expanded form
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展形式
- en: 'The function to convert data into the expanded form does the following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据转换为扩展形式的函数执行以下操作：
- en: Finds the start date.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找开始日期。
- en: Create a standard DataFrame using the start date and the global end date.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用开始日期和全局结束日期创建标准的 DataFrame。
- en: Left-merges the DataFrame for `LCLid` with the standard DataFrame, leaving the
    missing data as `np.nan`.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 `LCLid` 的 DataFrame 进行左连接与标准 DataFrame，缺失的数据填充为 `np.nan`。
- en: Returns the merged DataFrame.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回合并后的 DataFrame。
- en: 'Once we have all the `LCLid` DataFrames, we must perform a couple of additional
    steps to complete the expanded form processing:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有所有的 `LCLid` DataFrame，我们需要执行几个额外的步骤来完成扩展形式的处理：
- en: Concatenate all the DataFrames into a single DataFrame.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有 DataFrame 合并为一个单一的 DataFrame。
- en: Create a column called offset, which is the numerical representation of the
    half-hour blocks; for example, `hh_3` → `3`.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 offset 的列，它是半小时区块的数值表示；例如，`hh_3` → `3`。
- en: Create a timestamp by adding a 30-minute offset to the day and dropping the
    unnecessary columns.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过为日期加上 30 分钟的偏移来创建时间戳，并删除不必要的列。
- en: For one block, this representation takes up ~47 MB of memory.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个区块，这种表示占用大约 47 MB 的内存。
- en: Compact form
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 紧凑形式
- en: 'The function for converting into compact form does the following:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据转换为紧凑形式的函数执行以下操作：
- en: Finds the start date and time series identifiers.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找开始日期和时间序列标识符。
- en: Creates a standard DataFrame using the start date and the global end date.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用开始日期和全局结束日期创建标准的 DataFrame。
- en: Left merges the DataFrame for `LCLid` to the standard DataFrame, leaving the
    missing data as `np.nan`.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 `LCLid` 的 DataFrame 进行左连接与标准 DataFrame，缺失的数据填充为 `np.nan`。
- en: Sorts the values on the date.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按日期对值进行排序。
- en: Returns the time series array, along with the time series identifier, start
    date, and the length of the time series.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回时间序列数组，以及时间序列标识符、开始日期和时间序列的长度。
- en: Once we have this information for each `LCLid`, we can compile it into a DataFrame
    and add 30min as the frequency.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们为每个 `LCLid` 获取了这些信息，我们可以将它们编译成一个 DataFrame，并将频率设置为 30 分钟。
- en: For one block, this representation takes up only ~0.002 MB of memory.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个区块，这种表示形式仅占用约0.002 MB的内存。
- en: We are going to use the compact form because it is easy to work with and much
    less resource-hungry.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用紧凑形式，因为它更易于处理，且占用资源较少。
- en: Mapping additional information
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 映射附加信息
- en: 'From the data model that we prepared earlier, we know that there are three
    key files that we have to map: *Household Information*, *Weather*, and *Bank Holidays*.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们之前准备的数据模型中，我们知道有三个关键文件需要映射：*家庭信息*、*天气* 和 *银行假期*。
- en: The `informations_households.csv` file contains metadata about the household.
    There are static features that are not dependent on time. For this, we just need
    to left-merge `informations_households.csv` with the compact form based on `LCLid`,
    which is the time series identifier.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`informations_households.csv` 文件包含关于家庭的元数据。这里有一些与时间无关的静态特征。为此，我们只需基于 `LCLid`（时间序列标识符）将
    `informations_households.csv` 与紧凑形式进行左连接。'
- en: '**Best practice**:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**：'
- en: While doing a pandas `merge`, one of the most common and unexpected outcomes
    is that the number of rows before and after the operation is not the same (even
    if you are doing a left merge). This typically happens because there are duplicates
    in the keys on which you are merging. As a best practice, you can use the `validate`
    parameter in the pandas merge, which takes in inputs such as `one_to_one` and
    `many_to_one` so that this check is done while merging and will throw an error
    if the assumption is not met. For more information, go to [https://pandas.pydata.org/docs/reference/api/pandas.merge.html](https://pandas.pydata.org/docs/reference/api/pandas.merge.html).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行 pandas 的 `merge` 时，一个最常见且意外的结果是操作前后行数不相同（即使你是进行左连接）。这通常发生在合并的键存在重复值时。作为最佳实践，可以在
    pandas 合并时使用 `validate` 参数，它接受 `one_to_one` 和 `many_to_one` 等输入，这样可以在合并时进行检查，如果不符合假设，将抛出错误。更多信息，请访问
    [https://pandas.pydata.org/docs/reference/api/pandas.merge.html](https://pandas.pydata.org/docs/reference/api/pandas.merge.html)。
- en: Bank Holidays and Weather, on the other hand, are time-varying features and
    should be dealt with accordingly. The most important aspect to keep in mind is
    that while we map this information, it should perfectly align with the time series
    that we have already stored as an array.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 银行假期和天气是时变特征，应当根据情况进行处理。需要牢记的最重要的一点是，在映射这些信息时，它们应该与我们已经存储的时间序列数组完美对齐。
- en: '`uk_bank_holidays.csv` is a file that contains the dates of the holidays and
    the kind of holiday. The holiday information is quite important here because the
    energy consumption patterns would be different on a holiday when the family members
    are at home spending time with each other, watching television, and so on. Follow
    these steps to process this file:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`uk_bank_holidays.csv` 是一个包含假期日期和假期类型的文件。假期信息在这里非常重要，因为在假期期间，家庭成员通常待在家里，一起度过时间、看电视等，这会影响能源消耗模式。按照以下步骤处理该文件：'
- en: Convert the date column into the datetime format and set it as the index of
    the DataFrame.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日期列转换为 datetime 格式，并将其设置为 DataFrame 的索引。
- en: Using the `resample` function we saw earlier, we must ensure that the index
    is resampled every 30 minutes, which is the frequency of the times series.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们之前看到的`resample`函数，我们必须确保索引每30分钟重采样一次，这是时间序列的频率。
- en: Forward fill the holidays within a day and fill in the rest of the `NaN` values
    with `NO_HOLIDAY`.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对假期数据进行前向填充，并将其余的 `NaN` 值填充为 `NO_HOLIDAY`。
- en: Now, we have converted the holiday file into a DataFrame that has a row for
    each 30-minute interval. On each row, we have a column that specifies whether
    that day was a holiday or not.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经将假期文件转换成了一个 DataFrame，且每行代表一个30分钟的时间间隔。每行中有一列指定该天是否为假期。
- en: '`weather_hourly_darksky.csv` is a file that is, once again, at the daily frequency.
    We need to downsample it to a 30-minute frequency because the data that we need
    to map to this is at a half-hourly frequency. If we don’t do this, the weather
    will only be mapped to the hourly timestamps, leaving the half-hourly timestamps
    empty.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`weather_hourly_darksky.csv`是一个每日频率的文件，我们需要将其下采样至30分钟频率，因为我们需要映射到的数据是半小时频率的。如果不进行此操作，天气数据将只映射到按小时计算的时间戳，导致半小时的时间戳没有数据。'
- en: 'The steps we must follow to process this file are also similar to the way we
    processed holidays:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们处理该文件的步骤也类似于处理假期的方式：
- en: Convert the date column into the datetime format and set it as the index of
    the DataFrame.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日期列转换为datetime格式，并将其设置为DataFrame的索引。
- en: Using the `resample` function, we must ensure that the index is resampled every
    30 minutes, which is the frequency of the times series.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`resample`函数时，我们必须确保每30分钟对索引进行重采样，这也是时间序列的频率。
- en: Forward fill the weather features to fill in the missing values that were created
    while resampling.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前向填充方法填充由于重采样而产生的缺失值。
- en: Now that you have made sure the alignment between the time series and the time-varying
    features is ensured, you can loop over each of the time series and extract the
    weather and bank holiday array before storing it in the corresponding row of the
    DataFrame.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经确保了时间序列和随时间变化的特征之间的对齐，你可以遍历每个时间序列，并提取天气和公共假日数组，然后将其存储在DataFrame的相应行中。
- en: Saving and loading files to disk
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存和加载文件到磁盘。
- en: The fully merged DataFrame in its compact form takes up only ~10 MB. However,
    saving this file requires a little bit of engineering. If we try to save the file
    in the CSV format, it will not work because of the way we have stored arrays in
    pandas columns (since the data is in its compact form). We can save it in `pickle`
    or `parquet` format, or any of the binary forms of file storage. This can work,
    depending on the size of the RAM available on our machines. Although the fully
    merged DataFrame is just ~10 MB, saving it in `pickle` format will make the size
    explode to ~15 GB.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 完全合并的DataFrame在紧凑形式下仅占~10 MB。然而，保存此文件需要一点工程工作。如果我们尝试以CSV格式保存文件，由于在pandas列中存储数组的方式（因为数据以紧凑形式存储），这将无法成功。我们可以将其保存为`pickle`或`parquet`格式，或者任何二进制文件格式。根据我们机器上可用的内存大小，这种方式是可行的。虽然完全合并的DataFrame只有~10
    MB，但以`pickle`格式保存时，文件大小会暴增至~15 GB。
- en: What we can do is save this as a text file while making a few tweaks to accommodate
    the column names, column types, and other metadata that is required to read the
    file back into memory. The resulting file size on disk still comes out to ~15
    GB, but since we are doing it as an I/O operation, we do not keep all that data
    in our memory. We call this the time series (`.ts`) format. The functions to save
    a compact form in the`.ts` format, read the `.ts` format, and convert the compact
    form into the expanded form are available in this book’s GitHub repository under
    `src/data_utils.py`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做的是将其作为文本文件保存，同时进行一些调整，以适应列名、列类型和其他读取文件所需的元数据。最终保存的文件大小仍然是~15 GB，但由于这是I/O操作，我们并不将所有数据保存在内存中。我们将这种格式称为时间序列（`.ts`）格式。保存紧凑形式到`.ts`格式、读取`.ts`格式并将紧凑形式转换为展开形式的功能可在本书的GitHub仓库中的`src/data_utils.py`文件找到。
- en: 'If you don’t need to store all of the DataFrame in a single file, you can split
    it into multiple chunks and save them individually in a binary format, such as
    `parquet`. For our datasets, let’s follow this route and split the whole DataFrame
    into chunks of blocks and save them as `parquet` files. This is the best route
    for us for a few reasons:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不需要将整个DataFrame存储在一个文件中，可以将其拆分成多个块，并以二进制格式单独保存，如`parquet`格式。对于我们的数据集，我们可以选择这个方法，将整个DataFrame拆分成多个块并保存为`parquet`文件。这是最适合我们的方案，原因有以下几点：
- en: It leverages the compression that comes with the format
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它利用了该格式自带的压缩特性。
- en: It reads in parts of the whole data for quick iteration and experimentation
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它按块读取整个数据，以便快速迭代和实验。
- en: The data types are retained between the read and write operations, leading to
    less ambiguity
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据类型在读写操作之间得以保留，从而减少了歧义。
- en: For very large datasets, we can use some pandas alternatives, which makes it
    easier to process datasets that are out of memory. Polars is a great library that
    has lazy loading and is very fast. And for truly huge datasets, PySpark with a
    distributed cluster might be the right choice.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非常大的数据集，我们可以使用一些pandas的替代库，这样可以更容易处理内存溢出的数据集。Polars是一个非常棒的库，支持懒加载且运行非常快。对于真正庞大的数据集，使用带有分布式集群的PySpark可能是合适的选择。
- en: Now that we have processed the dataset and stored it on disk, let’s read it
    back into memory and look at a few more techniques to handle missing data.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经处理完数据集并将其存储到磁盘上，让我们将其重新读取到内存中，并探讨一些处理缺失数据的技术。
- en: Handling longer periods of missing data
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理长时间缺失数据
- en: We saw some techniques to handle missing data earlier—forward and backward filling,
    interpolation, and so on. Those techniques usually work if there are one or two
    missing data points. But if a large section of data is missing, then these simple
    techniques fall short.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到了一些处理缺失数据的技巧——前向填充、后向填充、插值等。如果只有一个或两个数据点缺失，这些技巧通常有效。但是，如果缺失的是一大段数据，那么这些简单的技巧就显得不足了。
- en: '**Notebook alert**:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本提示**：'
- en: To follow along with the complete code for missing data imputation, use the
    `03-Handling_Missing_Data_(Long_Gaps).ipynb` notebook in the `Chapter02` folder.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 若要跟随缺失数据填充的完整代码，请使用`Chapter02`文件夹中的`03-Handling_Missing_Data_(Long_Gaps).ipynb`笔记本。
- en: 'Let’s read blocks `0–7 parquet` from memory:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从内存中读取`0–7 parquet`区块：
- en: '[PRE17]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The data that we have saved is in the compact form. We need to convert it into
    the expanded form because it is easier to work with time series data in that form.
    Since we only need a subset of the time series (for faster demonstration purposes),
    we will just extract one block from these seven blocks. To convert the compact
    form into the expanded form, we can use a helpful function in `src/utils/data_utils.py`
    called `compact_to_expanded`:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们保存的数据是紧凑形式的。我们需要将其转换为扩展形式，因为在这种形式下处理时间序列数据更为方便。由于我们只需要时间序列的一个子集（为了更快的演示），我们将从这七个区块中提取一个区块。为了将紧凑形式转换为扩展形式，我们可以使用`src/utils/data_utils.py`中一个名为`compact_to_expanded`的有用函数：
- en: '[PRE18]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'One of the best ways to visualize the missing data in a group of related time
    series is by using a very helpful package called `missingno`:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化一组相关时间序列中缺失数据的最佳方法之一是使用一个非常有用的包，叫做`missingno`：
- en: '[PRE19]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The preceding code produces the following output:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码会产生以下输出：
- en: '![Figure 2.9 – Visualization of the missing data in block 7 ](img/B22389_02_09.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.9 – 第7区块中缺失数据的可视化](img/B22389_02_09.png)'
- en: 'Figure 2.9: Visualization of the missing data in block 7'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9：第7区块中缺失数据的可视化
- en: Only attempt the `missingno` visualization on related time series where there
    are less than 25 time series. If you have a dataset that contains thousands of
    time series (such as in our full dataset), applying this visualization will give
    us an illegible plot and a frozen computer.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 仅在相关的时间序列中尝试`missingno`可视化，并且这些时间序列数量应少于25个。如果你有一个包含成千上万条时间序列的数据集（例如我们的完整数据集），应用这种可视化会导致我们得到一个无法辨认的图形，并使计算机卡住。
- en: This visualization tells us a lot of things at a single glance. The *Y*-axis
    contains the dates that we are plotting the visualization for, while the *X*-axis
    contains the columns, which in this case are the different households. We know
    that all the time series are not perfectly aligned—that is, not all of them start
    at the same time and end at the same time. The big white gaps we can see at the
    beginning of many of the time series show that data collection for those consumers
    started later than the others. We can also see that a few time series finish earlier
    than the rest, which means either they stopped being consumers or the measurement
    phase stopped. There are also a few smaller white lines in many time series, which
    are real missing values. We can also notice a sparkline to the right, which is
    a compact representation of the number of missing columns for each row. If there
    are no missing values (all time series have some value), then the sparkline would
    be at the far right. Finally, if there are a lot of missing values, the line will
    be to the left.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这张可视化图表一眼就能告诉我们很多信息。*Y*轴显示的是我们所绘制可视化图表的日期，而*X*轴则包含了列，这里是不同的家庭。我们知道，并非所有时间序列都完全对齐——也就是说，并非所有的时间序列都在相同的时间开始或结束。我们可以看到，在许多时间序列的开头都有明显的白色空隙，这表明这些消费者的数据收集开始得比其他消费者晚。我们还可以看到有一些时间序列比其他序列提前结束，这意味着这些消费者要么停止了消费，要么测量阶段停止了。许多时间序列中还有一些较小的白色线条，它们代表着真实的缺失值。我们还可以注意到右侧有一个迷你图，它是每一行缺失列数量的紧凑表示。如果没有缺失值（即所有时间序列都有数据），那么迷你图会显示在最右侧。如果缺失值很多，迷你图的线条会出现在最左侧。
- en: Just because there are missing values, we are not going to fill/impute them
    because the decision of whether to impute missing data or not comes later in the
    workflow. For some models, we do not need to do the imputation, while for others,
    we do. There are multiple ways of imputing missing data, and which one to choose
    is another decision we cannot make beforehand.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅因为存在缺失值，我们并不会立即填充/估算这些数据，因为是否填充缺失数据的决策会在后续流程中做出。对于某些模型，我们不需要进行填充，而对于其他模型则需要。填充缺失数据有多种方法，选择哪种方法是我们在此之前无法做出的决定。
- en: 'So for now, let’s pick one `LCLid` and dig deeper. We already know that there
    are some missing values between `2012-09-30` and `2012-10-31`. Let’s visualize
    that period:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 那么现在，让我们选择一个`LCLid`，深入分析。我们已经知道在`2012-09-30`到`2012-10-31`之间存在一些缺失值。让我们可视化这一时期的数据：
- en: '[PRE20]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The preceding code produces the following output:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将产生如下输出：
- en: '![Figure 2.10 – Visualization of missing data of MAC000193 between 2012-09-30
    and 2012-10-31 ](img/B22389_02_10.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图2.10 – 2012-09-30到2012-10-31之间MAC000193缺失数据的可视化](img/B22389_02_10.png)'
- en: 'Figure 2.10: Visualization of missing data of MAC000193 between 2012-09-30
    and 2012-10-31'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10：2012-09-30到2012-10-31之间MAC000193缺失数据的可视化
- en: Here, we can see that the missing data is between `2012-10-18` and `2012-10-19`.
    Normally, we would go ahead and impute the missing data in this period, but since
    we are looking at this with an academic lens, we will take a slightly different
    route.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到缺失数据出现在`2012-10-18`到`2012-10-19`之间。通常情况下，我们会继续填充这一时期的缺失数据，但由于我们以学术的视角来看待这个问题，我们将采取略微不同的路径。
- en: 'Let’s introduce an artificial missing data section, see how the different techniques
    we are going to look at impute the missing data, and compute a metric to see how
    close we are to the real time series (We are going to use a metric called **Mean
    Absolute Error** (**MAE**) to do the comparison, and it’s nothing but the average
    of the absolute error across the time steps. Just understand that it is a lower-the-better
    metric that we will talk about in detail later in the book.):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们引入一个人工缺失数据部分，看看我们将要使用的不同技术如何填充缺失数据，并计算一个指标来评估我们与真实时间序列的接近程度（我们将使用一个叫做**平均绝对误差**（**MAE**）的指标来做比较，它不过是所有时间点的绝对误差的平均值。只需要明白它是一个越小越好的指标，我们将在书中的后续章节详细讨论它。）：
- en: '[PRE21]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, let’s plot the missing area in the time series:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在时间序列中绘制缺失区域：
- en: '![Figure 2.11 – The energy consumption of MAC000193 between 2012-10-05 and
    2012-10-10 ](img/B22389_02_11.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图2.11 – 2012-10-05到2012-10-10之间MAC000193的能耗](img/B22389_02_11.png)'
- en: 'Figure 2.11: The energy consumption of MAC000193 between 2012-10-05 and 2012-10-10'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11：2012-10-05到2012-10-10之间MAC000193的能耗
- en: We are missing 2 whole days of energy consumption readings, which means there
    are 96 missing data points (half-hourly). If we use one of the techniques we saw
    earlier, such as interpolation, we will see that it will mostly be a straight
    line because none of the methods are complex enough to capture the pattern over
    a long time.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们缺少了整整两天的能耗数据，这意味着有 96 个缺失的数据点（每半小时一个数据点）。如果我们使用之前看到的一些方法，例如插值法，我们会发现结果大多是直线，因为这些方法不够复杂，无法捕捉长期的模式。
- en: There are a few techniques that we can use to fill in such large missing gaps
    in data. We will cover these now.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些技术可以用来填补如此大的数据缺口。我们现在将介绍这些方法。
- en: Imputing with the previous day
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用前一天数据进行填补
- en: Since this is a half-hourly time series of energy consumption, it stands to
    reason that there might be a pattern that repeats day after day. The energy consumption
    between 9:00 A.M. and 10:00 A.M. might be higher as everybody gets ready to go
    to the office, slumping during the day when most houses may be empty.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个半小时一次的能耗时间序列，可以合理推测，每天可能会有重复的模式。上午 9:00 到 10:00 之间的能耗可能较高，因为每个人都在准备去办公室，而白天大部分时间住宅可能是空的，能耗较低。
- en: 'So the simplest way to fill in the missing data would be to use the previous
    day’s energy readings so that the energy reading at 10:00 A.M, 2012-10-18, can
    be filled with the energy reading at 10:00 A.M, 2012-10-17:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，填补缺失数据最简单的方法就是使用前一天的能耗数据，这样 2012-10-18 10:00 A.M. 的能耗可以用 2012-10-17 10:00
    A.M. 的能耗来填补：
- en: '[PRE22]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let’s see what the imputation looks like:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看填补后的效果如何：
- en: '![Figure 2.12 – Imputing with the previous day ](img/B22389_02_12.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.12 – 使用前一天数据进行填补](img/B22389_02_12.png)'
- en: 'Figure 2.12: Imputing with the previous day'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.12：使用前一天数据进行填补
- en: While this looks better, this is also very brittle. When we copy the previous
    day, we also assume that any kind of variation or anomalous behavior is also repeated.
    We can already see that the patterns for the day before and the day after are
    not the same.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这看起来更好，但这种方法也很脆弱。当我们复制前一天的数据时，我们也假设任何变化或异常行为都会被复制。我们已经可以看到前一天和后一天的模式并不相同。
- en: Hourly average profile
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 每小时平均曲线
- en: 'A better approach would be to calculate an hourly profile from the data—the
    mean consumption for every hour—and use the average to fill in the missing data:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的方法是从数据中计算出每小时曲线——每个小时的平均消耗——并用这个平均值来填补缺失数据：
- en: '[PRE23]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s see if this is better:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这是否更好：
- en: '![Figure 2.13 – Imputing with an hourly profile ](img/B22389_02_13.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.13 – 使用每小时曲线进行填补](img/B22389_02_13.png)'
- en: 'Figure 2.13: Imputing with an hourly profile'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13：使用每小时曲线进行填补
- en: This gives us a much more generalized curve that does not have the spikes that
    we saw for the individual days. The hourly ups and downs have also been captured
    as per our expectations. The MAE is also lower than before.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这样我们得到了一个更加通用的曲线，没有我们在单独日期中看到的尖峰波动。每小时的起伏也已经按预期被捕捉到了。MAE（平均绝对误差）也比之前低。
- en: The hourly average for each weekday
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 每个工作日的每小时平均值
- en: 'We can further refine this rule by introducing a specific profile for each
    weekday. It stands to reason that the usage pattern on a weekday is not going
    to be the same on a weekend. Hence, we can calculate the average hourly consumption
    for each weekday separately so that we have one profile for Monday, another for
    Tuesday, and so on:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过引入每个工作日的特定曲线进一步完善这个规则。可以合理推测，工作日的使用模式与周末并不相同。因此，我们可以分别计算每个工作日的每小时平均能耗，这样我们就可以为星期一、星期二等每一天得到一条独立的曲线：
- en: '[PRE24]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let’s see what this looks like:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这看起来如何：
- en: '![Figure 2.14 – Imputing the hourly average for each weekday ](img/B22389_02_14.png)Figure
    2.14: Imputing the hourly average for each weekday'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.14 – 填补每个工作日的每小时平均值](img/B22389_02_14.png) 图 2.14：填补每个工作日的每小时平均值'
- en: This looks very similar to the other one, but this is because the day we are
    imputing is a weekday and the weekday profiles are similar. The MAE is also lower
    than the day profile. The weekend profile is slightly different, which you can
    see in the associated Jupyter notebook.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来和之前的结果非常相似，但这是因为我们正在填补的这一天是工作日，而工作日的曲线非常相似。MAE 也比单一日期的曲线低。周末的曲线稍有不同，您可以在相关的
    Jupyter 笔记本中看到。
- en: Seasonal interpolation
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 季节性插值
- en: 'Although calculating seasonal profiles and using them to impute works well,
    there are instances, especially when there is a trend in the time series, where
    such a simple technique falls short. The simple seasonal profile doesn’t capture
    the trend at all and ignores it completely. For such cases, we can do the following:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管计算季节性配置文件并用它进行填充效果很好，但在某些情况下，特别是当时间序列中存在趋势时，这种简单的技术就不够用了。简单的季节性配置文件根本没有捕捉到趋势，完全忽略了它。在这种情况下，我们可以采取以下措施：
- en: Calculate the seasonal profile, similar to how we calculated the averages earlier.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算季节性配置文件，类似于我们之前计算平均值的方式。
- en: Subtract the seasonal profile and apply any of the interpolation techniques
    we saw earlier.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减去季节性配置文件并应用我们之前看到的任何插值技术。
- en: Return the seasonal profile to the interpolated series.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将季节性配置文件返回到插值后的序列中。
- en: 'This process has been implemented in this book’s GitHub repository in the `src/imputation/interpolation.py`
    file. We can use it as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程已经在本书的 GitHub 仓库中的 `src/imputation/interpolation.py` 文件中实现。我们可以按以下方式使用它：
- en: '[PRE25]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The key parameter here is `seasonal_period`, which tells the algorithm to look
    for patterns that repeat every `seasonal_period`. If we mention `seasonal_period=48`,
    it will look for patterns that repeat every 48 data points. In our case, they
    are after each day (because we have 48 half-hour timesteps in a day). In addition
    to this, we need to specify what kind of interpolation we need to perform.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键参数是 `seasonal_period`，它告诉算法寻找每 `seasonal_period` 个数据点重复的模式。如果我们指定 `seasonal_period=48`，它会寻找每
    48 个数据点重复的模式。在我们的例子中，这是每天之后的模式（因为一天有 48 个半小时的时间步）。除了这个参数，我们还需要指定需要执行哪种插值方法。
- en: '**Additional information**:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '**附加信息**：'
- en: Internally, we use something called seasonal decomposition (`statsmodels.tsa.seasonal.seasonal_decompose`),
    which will be covered in *Chapter 3*, *Analyzing and Visualizing Time Series Data*,
    to isolate the seasonality component.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，我们使用了一种叫做季节性分解的方法（`statsmodels.tsa.seasonal.seasonal_decompose`），它将在*第 3
    章*《分析和可视化时间序列数据》中讲解，用来分离季节性成分。
- en: 'Here, we have done seasonal interpolation using 48 (half-hourly) and 48*7 (weekday
    to half-hourly) and plotted the resulting imputation:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 48（半小时）和 48*7（工作日到半小时）进行了季节性插值，并绘制了得到的填充结果：
- en: '![Figure 2.15 – Imputing with seasonal interpolation ](img/B22389_02_15.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.15 – 使用季节性插值进行填充](img/B22389_02_15.png)'
- en: 'Figure 2.15: Imputing with seasonal interpolation'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.15：使用季节性插值进行填充
- en: Here, we can see that both have captured the seasonality patterns, but the half-hourly
    profile every weekday has captured the peaks of the first day better, so they
    have a lower MAE. There is no improvement in terms of hourly averages, mostly
    because there are no strong increasing or decreasing patterns in the time series.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到两者都捕捉到了季节性模式，但每个工作日的半小时配置文件更好地捕捉了第一天的峰值，因此它们的 MAE 较低。在每小时平均值方面没有改进，主要是因为时间序列中没有强烈的上升或下降模式。
- en: With this, we have come to the end of this chapter. We are now officially into
    the nitty-gritty of juggling, cleaning, and processing time series data. Congratulations
    on finishing this chapter!
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，本章内容已经结束。我们现在正式进入了时间序列数据的清洗、处理和操控的细节。恭喜你完成本章的学习！
- en: Summary
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, after a short refresher on pandas DataFrames, especially on
    the datetime manipulations and simple techniques to handle missing data, we learned
    about the two forms of storing and working with time series data—compact and expanded.
    With all this knowledge, we took our raw dataset and built a pipeline to convert
    it into the compact form. If you have run the accompanying notebook, you should
    have the preprocessed dataset saved on disk. We also had an in-depth look at some
    techniques to handle long gaps of missing data.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，在简要回顾 pandas DataFrame，特别是 datetime 操作和处理缺失数据的简单技巧之后，我们学习了存储和处理时间序列数据的两种形式——紧凑型和展开型。通过这些知识，我们将原始数据集处理并构建了一个管道，将其转换为紧凑型。如果你已经运行了附带的笔记本，你应该已经将预处理后的数据集保存到磁盘。我们还深入探讨了处理长时间缺失数据的几种技术。
- en: Now that we have the processed datasets, in the next chapter, we will learn
    how to visualize and analyze a time series dataset.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了处理后的数据集，在下一章中，我们将学习如何可视化和分析时间序列数据集。
- en: Join our community on Discord
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussions with authors and other readers:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者讨论：
- en: '[https://packt.link/mts](https://packt.link/mts)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mts](https://packt.link/mts)'
- en: '![](img/QR_Code15080603222089750.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code15080603222089750.png)'
