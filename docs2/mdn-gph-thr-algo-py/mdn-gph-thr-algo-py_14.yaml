- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Machine Learning for Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络机器学习
- en: In this chapter, we’ll consider **machine learning** (**ML**) models typically
    used on relational data and their applications within network science. While many
    network-specific tools provide good insights into network structure and prediction
    of spread across a network, ML tools allow us to leverage additional information
    about individuals in the network to construct a more complete view of relationships,
    spreading processes, and key outcomes related to the network or its individuals.
    We’ll consider friendship networks and metadata associated with individuals and
    their connections to other individuals to explore ML on networks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将考虑在关系数据上通常使用的**机器学习**（**ML**）模型及其在网络科学中的应用。虽然许多特定于网络的工具提供了对网络结构和网络内传播预测的良好见解，但机器学习工具使我们能够利用关于网络中个体的额外信息，构建一个更全面的关系、传播过程和与网络或其个体相关的关键结果视图。我们将考虑友谊网络以及与个人及其与其他个人的联系相关的元数据，以探索网络上的机器学习。
- en: We’ll first return to network construction based on shared activities and traits
    of individuals, move on to clustering based on both network and metadata features,
    and finally predict individual and friendship network outcomes based on networks
    and their metadata. You’ll learn how to combine network metrics with metadata
    and how to build several types of ML models using network data, upon which we
    will build in the remaining chapters of this book. Let’s dive into some friendship
    networks and their metadata.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将回到基于共享活动和个体特征的网络构建，然后转向基于网络和元数据特征的聚类，最后根据网络及其元数据预测个体和友谊网络的结果。您将学习如何将网络度量与元数据相结合，以及如何使用网络数据构建几种类型的机器学习模型，这些模型将在本书剩余章节中构建。让我们深入探讨一些友谊网络及其元数据。
- en: 'Specifically, we will cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，本章将涵盖以下主题：
- en: Introduction to friendship networks and friendship relational datasets
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 友谊网络和友谊关系数据集简介
- en: ML on networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络上的机器学习
- en: SDL on networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络上的SDL
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The code for the practical examples presented in this chapter can be found
    here: [https://github.com/PacktPublishing/Modern-Graph-Theory-Algorithms-with-Python](https://github.com/PacktPublishing/Modern-Graph-Theory-Algorithms-with-Python)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中展示的实用示例代码可以在以下位置找到：[https://github.com/PacktPublishing/Modern-Graph-Theory-Algorithms-with-Python](https://github.com/PacktPublishing/Modern-Graph-Theory-Algorithms-with-Python)
- en: Introduction to friendship networks and friendship relational datasets
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 友谊网络和友谊关系数据集简介
- en: In this section, we’ll consider a friendship network based on student behavior
    factors to form a network. We’ll then apply **unsupervised learning** (**UL**)
    methods, namely clustering, to group individuals into friendship groups to compare
    performance before and after adding extra network structural information.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将考虑基于学生行为因素的友谊网络，以形成一个网络。然后，我们将应用**无监督学习**（**UL**）方法，即聚类，将个体分组到友谊群体中，以比较添加额外网络结构信息前后的性能。
- en: Friendship network introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 友谊网络简介
- en: Let’s consider a group of classmates in a small school with enrollment based
    on age and geography, as is common in the United States. Classmates may participate
    in the same extracurricular activities, such as sports teams, the school paper,
    or a concert band. They may also study together, share meals, or get together
    to hang out on weekends. Some may form a core group of friends who take some of
    the same classes, participate in the same extracurriculars, study together, and
    hang out together outside of school-related activities. Strong social ties such
    as these often form an integral source of social support and lasting social relationships.
    These tend to be very important to individual life decisions and outcomes, particularly
    in adolescence and early adulthood, where peers play an important role in psychosocial
    development.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个以年龄和地理为基础的小学校的学生群体，这在美国很常见。同学可能参加相同的课外活动，如运动队、校报或管弦乐队。他们也可能一起学习、共享餐食或在周末聚在一起。有些人可能形成一个核心的友谊群体，他们上一些相同的课程，参加相同的课外活动，一起学习，并在与学校相关活动之外一起闲逛。这种强大的社会联系通常是一个重要的社会支持和社会关系的来源。这些往往对个人的生活决策和结果非常重要，尤其是在青春期和成年早期，同龄人在心理社会发展中扮演着重要角色。
- en: Other groups of friends may only study together or play on the same team, with
    few other shared interests or interactions. Weak social ties such as these also
    play an important role in society, connecting individuals with a wide range of
    resources across a community and exposing young people to a wider variety of viewpoints
    and new ideas. Social change often comes from weak ties across diverse communities,
    such as playing on the same sports teams, sharing classes in school, and participating
    in religious activities. While weak social ties often don’t provide strong social
    support, they serve a bridging function within networks and can introduce individuals
    to others who will become strong social ties.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 其他朋友群体可能只一起学习或同一个团队打球，共享的其他兴趣或互动很少。这种弱社会联系在社会中也发挥着重要作用，将个人与社区内广泛的各种资源联系起来，使年轻人接触到更广泛的各种观点和新思想。社会变革通常来自不同社区之间的弱联系，例如在同一个体育队伍中打球、在学校共享课程以及参与宗教活动。虽然弱社会联系通常不会提供强大的社会支持，但它们在网络中起着桥梁作用，可以将个人介绍给将成为强社会联系的其他人。
- en: 'In our first friendship network, we’ll consider both weak and strong social
    ties. Strong social ties mainly occur within a group of seven friends who mostly
    play on the same team, share some classes, study together, and play sports before
    school and at weekends:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一个友谊网络中，我们将考虑弱社会联系和强社会联系。强社会联系主要发生在由七个朋友组成的群体中，他们大多数时间都在同一个团队打球，上一些相同的课程，一起学习，并在上学前和周末进行体育活动：
- en: '![Figure 9.1 – An illustration of a group of boys playing basketball before
    school](img/B21087_09_01.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – 一群男孩在上学前打篮球的插图](img/B21087_09_01.jpg)'
- en: Figure 9.1 – An illustration of a group of boys playing basketball before school
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – 一群男孩在上学前打篮球的插图
- en: '*Figure 9**.1* shows three of the strong-social-tie boys playing basketball
    before school. We’d expect ideas, behaviors, and communicable diseases to spread
    quickly through this part of the network, as this group spends most of its time
    together.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9**.1* 展示了三个强社会联系男孩在上学前打篮球。我们预计想法、行为和传染性疾病会迅速通过这一部分网络传播，因为这个群体大部分时间都在一起。'
- en: 'In contrast, weak social ties within the network consist of occasional interactions
    that might include core courses or one shared interest that brings individuals
    together for short periods of time, such that they recognize each other and might
    know something about fellow students but probably don’t know much about other
    students’ interests, home life, or aspirations:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，网络中的弱社会联系包括偶尔的互动，可能包括核心课程或一个共同兴趣将个人聚集在一起短暂的时间，这样他们可以互相认识，并且可能对同学有所了解，但可能不太了解其他学生的兴趣、家庭生活或抱负：
- en: '![Figure 9.2 – An illustration of students in the same classroom for a course
    who may not interact outside of the classroom](img/B21087_09_02.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2 – 同一课堂中的学生在课程期间可能不会互动的插图](img/B21087_09_02.jpg)'
- en: Figure 9.2 – An illustration of students in the same classroom for a course
    who may not interact outside of the classroom
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 – 同一课堂中的学生在课程期间可能不会互动的插图
- en: '*Figure 9**.2* shows students in the same classroom who may not interact outside
    of that single class. Weak social ties such as these expose students to different
    ideas, different interests, seasonal flu, and more but have less influence on
    an individual than on the group of individuals as a whole.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9**.2* 展示了同一课堂中的学生，他们可能不会在那个单一的课程之外互动。这种弱社会联系使学生接触到不同的想法、不同的兴趣、季节性流感等，但对个人的影响不如对整个个体群体的影响大。'
- en: In this chapter, we’ll infer groups of students likely to share strong social
    ties through UL algorithms on both network metrics and metadata related to student
    demographics. We’ll also analyze social network risk on randomly generated networks
    to understand different epidemic risks for different types of networks through
    **supervised learning** (**SL**) with GNNs. Let’s explore our initial dataset
    a bit before diving into some analytics.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过在学生人口统计数据和网络指标以及与学生人口统计数据相关的元数据上使用UL算法来推断可能共享强社会联系的学生群体。我们还将通过使用GNN的**监督学习**（**SL**）来分析随机生成的网络中的社会网络风险，以了解不同类型网络的不同流行病风险。在深入分析之前，让我们先探索一下我们的初始数据集。
- en: Friendship demographic and school factor dataset
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 友谊人口统计和学校因素数据集
- en: 'In this chapter, we’ll mainly work with a dataset containing information about
    a group of 25 students who are connected by many different lifestyle factors:
    team membership, casual workouts, weekend sports activities, game attendance,
    and homework study group membership. Demographic and socioeconomic factors, as
    well as class assignments, also connect these students by registration in four
    elective courses, gender, neighborhood of residence, and prior attendance at one
    of two local junior high schools.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将主要使用一个包含关于25名学生信息的数据集，这些学生通过许多不同的生活方式因素相互连接：团队会员资格、休闲锻炼、周末体育活动、游戏出席率和家庭作业学习小组会员资格。人口统计和社会经济因素，以及课程分配，也通过四门选修课程的注册、性别、居住区的邻里和之前在两所当地初中之一就读的经历将学生联系起来。
- en: This dataset was derived from Farrelly’s secondary school diary over the course
    of a month in her freshman year. Farrelly herself is individual *#7*. To create
    a weighted network, we’ll sum up connections across factors between pairs of students.
    This will give us an approximation of which students are most connected to each
    other. We’ll first explore clustering to discern friendship groups.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集是从Farrelly在大学一年级的一个月日记中提取的。Farrelly本人是编号*#7*的个体。为了创建一个加权网络，我们将计算学生之间因子的连接总和。这将给出一个近似值，表明哪些学生彼此之间联系最紧密。我们首先将探索聚类以辨别友谊群体。
- en: Let’s see how we can cluster this network based on metadata alone before we
    move into clustering on both metadata and network metrics.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们转向同时基于元数据和网络指标进行聚类之前，让我们看看如何仅根据元数据来聚类这个网络。
- en: ML on networks
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络上的机器学习
- en: Now that we have explored friendship data a bit, let’s see how clustering algorithm
    performance varies depending on whether or not we include structural information
    about the network. We’ll start by considering just student factors.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对友谊数据进行了探索，让我们看看聚类算法的性能如何根据是否包含关于网络的结构信息而变化。我们将首先考虑仅学生因素。
- en: Clustering based on student factors
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于学生因素的聚类
- en: For our first attempt at clustering, we’ll focus on the dataset itself, which
    contains metadata regarding student demographics and social activities. One of
    the simplest clustering algorithms is *k-means clustering*, which partitions data
    iteratively to minimize within-cluster variance and maximize between-cluster variance.
    This means that students clustered together have more in common with students
    in that same cluster than with students in other clusters. K-means clustering
    is a simple algorithm that works well in most cases. However, one needs to specify
    the number of expected clusters, which is typically not known ahead of time. We’ll
    use a cluster size of `3` and assess model fit; in addition, we’ll restart the
    algorithm five times to ensure that we have an optimal three-cluster solution
    regardless of algorithm start point and random error.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们第一次尝试聚类时，我们将关注数据集本身，该数据集包含有关学生人口统计和社会活动的元数据。最简单的聚类算法之一是*k-means聚类*，它通过迭代地划分数据以最小化簇内方差并最大化簇间方差。这意味着聚在一起的学生的共同点比与其他簇中的学生更多。k-means聚类是一个简单的算法，在大多数情况下都表现良好。然而，需要指定期望的簇数，这通常在事先并不知道。我们将使用`3`个簇的大小并评估模型拟合度；此外，我们将重新启动算法五次，以确保无论算法的起始点如何以及随机误差如何，我们都能得到一个最优的三簇解决方案。
- en: Important note
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If you are on a Windows machine, you may get a warning that does not impact
    results; some of the packages on `scikit-learn` are not updated with the new Windows
    operating systems in mind. New releases of operating systems and updates to package
    dependencies tend to trigger these warnings.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Windows机器，你可能会收到一个不影响结果的警告；`scikit-learn`上的一些包没有考虑到新的Windows操作系统。操作系统的最新版本和包依赖关系的更新往往会触发这些警告。
- en: 'Let’s dive into the k-means clustering code with `Script 9.1`:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用`Script 9.1`深入探讨k-means聚类代码：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The clustering results suggest that the three-cluster solution is a good fit.
    One cluster group (*#0*) includes individuals `1`-`7` and individual `10`; this
    group mostly does homework together, attends games, works out together on weekends,
    and plays on the same team. Cluster *#0* is characterized by a tight-knit group
    of friends who share many of the same activities and are near each other most
    of the week. We’d be concerned about an epidemic starting and spreading with this
    group. Likely, they share the same protective behaviors, such as healthy eating,
    regular physical activity, and social engagement. However, an infectious disease
    or risk behavior that might lead to physical injury (trying a dangerous take on
    a sports move, taking dares…) is a concern, as the behavior is likely to spread
    through the entire group of friends.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类结果建议三聚类解决方案是一个很好的拟合。一个聚类组（**#0**）包括个体`1`-`7`和个体`10`；这个组主要一起做作业，参加比赛，周末一起锻炼，并在同一个团队中玩耍。聚类**#0**的特点是紧密的朋友圈，他们共享许多相同的活动，并且在一周的大部分时间里都在一起。我们会担心在这个组中开始并传播流行病。很可能，他们共享相同的保护行为，如健康饮食、定期体育锻炼和社会参与。然而，可能导致身体伤害的传染病或风险行为（尝试危险的体育动作，接受挑战…）是一个担忧，因为这种行为很可能会在整个朋友圈中传播。
- en: Another group (*#1*) includes individuals `8`-`9`, `12`-`14`, `16`, `19`, and
    `23`-`25`; these individuals usually share *class 2*, don’t work out or play sports
    together outside of school, don’t do homework together, and don’t share many other
    classes. Cluster *#1* is characterized by a lack of involvement and engagement
    with others in our sample. This group is low risk for both protective behavior
    and risk behavior spreading as they don’t have strong social ties to others in
    our sample. Likely, they wouldn’t be influenced or influence others with behavior.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个组（**#1**）包括个体`8`-`9`、`12`-`14`、`16`、`19`和`23`-`25`；这些个体通常共享*课程2*，不在学校外一起锻炼或运动，不一起做作业，也不共享许多其他课程。集群**#1**的特点是缺乏与我们样本中其他人的参与和互动。这个组在保护行为和风险行为传播方面风险较低，因为他们与我们样本中的其他人没有强烈的社交联系。很可能，他们不会受到或影响他人的行为。
- en: The last group (*#2*) includes individuals `11`, `15`, `17`-`18`, and `20`-`22`;
    this group is heterogeneous and includes teammates who don’t have much else in
    common, individuals who share a few classes, and isolated individuals with few
    connections to others. In general, this group is low risk for epidemic or behavior
    spread like cluster *#1*; however, they are more active within the sample and
    may be influenced somewhat by teammates or those with whom they share multiple
    classes.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个组（**#2**）包括个体`11`、`15`、`17`-`18`和`20`-`22`；这个组是异质的，包括没有太多共同点的队友，一些共享几门课程的个体，以及与其他人联系较少的孤立个体。总的来说，这个组对于像集群**#1**那样的流行病或行为传播风险较低；然而，他们在样本中更为活跃，可能会受到队友或与他们共享多门课程的个体的影响。
- en: Clustering based on student factors and network metrics
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于学生因素和网络指标的聚类
- en: 'Now, let’s create a network based on thresholded Pearson correlations, which
    represents the similarity of activities/classes across individuals by adding to
    `Script 9.1`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们基于阈值化的皮尔逊相关系数创建一个网络，这通过在`Script 9.1`中添加内容来表示个体之间活动/课程的相似性：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Running this addition to `Script 9.1` yields a plot of the friendship network,
    as shown in *Figure 9**.3*:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个添加到`Script 9.1`的代码，会得到如图*图9**.3*所示的友谊网络图：
- en: '![Figure 9.3 – A network plot of the thresholded friendship dataset](img/B21087_09_03.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3 – 阈值化友谊数据集的网络图](img/B21087_09_03.jpg)'
- en: Figure 9.3 – A network plot of the thresholded friendship dataset
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – 阈值化友谊数据集的网络图
- en: '*Figure 9**.3* shows two separate groups, with one very small group consisting
    of two individuals and a much larger group with sparse and dense connectivity
    among individuals in the group. We’d expect the degree and PageRank centralities
    to vary quite a bit among individuals, given the connectivity patterns of our
    friendship dataset. Let’s add to `Script 9.1` and append our feature matrix to
    rerun our k-means analysis, including both demographic factors and two scaled
    centrality metrics, to see how our clustering changes:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9**.3*显示了两个独立的组，一个由两个个体组成的小组，以及一个由组内个体之间稀疏和密集连接组成的大组。鉴于我们友谊数据集的连接模式，我们预计个体之间的度数和PageRank中心性会有很大的变化。让我们在`Script
    9.1`中添加内容，并将我们的特征矩阵附加到重新运行我们的k-means分析中，包括人口统计因素和两个缩放中心性指标，以查看我们的聚类如何变化：'
- en: Note
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You may find a warning about copying objects; this does not impact the analysis
    or object.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会发现关于复制对象的警告；这不会影响分析或对象。
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can see some changes in our clustering results compared to our initial k-means
    model. In cluster *#0*, individual `19` is added (a teammate who does homework
    with the initial *#0* cluster and attends the game). Our initial cluster *#1*
    shows individuals `8`-`9`, `12`, `16`, and `23`-`25`; individuals `13`, `14`,
    and `19` are no longer assigned to this cluster but other individuals remain.
    In the remaining cluster, individuals `13` and `14` join our initial cluster,
    both of whom seem to have more connectivity than initial cluster *#1*, fitting
    better with cluster *#2* based on centrality metrics. It seems that adding network
    connectivity metrics improves k-means clustering results, as individuals who may
    not share every activity but show similar group connections are reassigned to
    groups that more closely fit their positions within the social network.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们最初的k-means模型相比，我们可以看到聚类结果中的一些变化。在聚类*#0*中，个体`19`被添加（一个与初始*#0*聚类一起做作业并参加比赛的同队成员）。我们的初始聚类*#1*显示了个体`8`-`9`、`12`、`16`和`23`-`25`；个体`13`、`14`和`19`不再分配到这个聚类，但其他个体仍然保留。在剩余的聚类中，个体`13`和`14`加入了我们的初始聚类，他们似乎比初始聚类*#1*有更多的连接性，根据中心性指标更适合聚类*#2*。似乎添加网络连接性指标提高了k-means聚类结果，因为那些可能不共享每个活动但显示出类似群体连接的个体被重新分配到更接近他们在社交网络中位置的群体。
- en: Let’s now see how we can use a semi-supervised clustering algorithm that we
    first encountered in [*Chapter 5*](B21087_05.xhtml#_idTextAnchor066)—spectral
    clustering—to obtain a semi-supervised solution to our friendship network clustering.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看如何使用我们在[*第五章*](B21087_05.xhtml#_idTextAnchor066)中首次遇到的半监督聚类算法——谱聚类——来获得我们友谊网络聚类的半监督解决方案。
- en: Spectral clustering on the friendship network
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 友谊网络的谱聚类
- en: 'As we saw in [*Chapter 5*](B21087_05.xhtml#_idTextAnchor066), spectral clustering
    offers a clustering option to partition either an adjacency matrix or a distance
    matrix; this can be done as a UL or `Script 9.1` to run an unsupervised spectral
    clustering with three clusters and five initializations (similar to our k-means
    runs) on our friendship dataset to compare with our k-means results by adding
    to `Script 9.1`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第五章*](B21087_05.xhtml#_idTextAnchor066)中看到的，谱聚类提供了一个聚类选项，可以分割邻接矩阵或距离矩阵；这可以通过UL或`Script
    9.1`来完成，以在我们的友谊数据集上运行具有三个聚类和五个初始化（类似于我们的k-means运行）的无监督谱聚类，并与我们的k-means结果进行比较，通过向`Script
    9.1`添加以下内容：
- en: Note
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Again, you may encounter a Windows warning from scikit-learn or a warning about
    the graph not being fully connected (assessed via Laplacian, which results in
    a different approach to the clustering than would be run for a fully connected
    network). Neither of these warnings will impact the result.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，你可能会遇到来自scikit-learn的Windows警告，或者关于图未完全连接的警告（通过拉普拉斯矩阵评估，这会导致与完全连接网络不同的聚类方法）。这两个警告都不会影响结果。
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: These results differ significantly compared to the k-means solutions we obtained
    in the previous subsection. Given that both k-means models consider specific activities
    and course schedules rather than just a correlation summary, this difference makes
    sense. The spectral clustering solution focuses solely on network connectivity
    rather than the factors included in the friendship dataset or a combination of
    connectivity and factors. In this case, the k-means solutions make more sense
    given our data—particularly the second k-means solution, which includes network
    metrics and the original factors.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在前一小节中获得的k-means解决方案相比，这些结果有显著差异。鉴于k-means模型都考虑了特定的活动和课程表，而不仅仅是相关性的总结，这种差异是有意义的。谱聚类解决方案专注于网络连接性，而不是友谊数据集中包含的因素或连接性和因素的组合。在这种情况下，k-means解决方案更符合我们的数据——尤其是第二个k-means解决方案，它包括网络指标和原始因素。
- en: The selection of unsupervised versus semi-supervised clustering algorithms is
    highly specific to the task at hand. For very large networks, k-means algorithms
    have solutions that scale well, and adding network connectivity metrics that scale
    well may improve k-means solutions without sacrificing efficiency. For problems
    that involve a pure network connectivity solution, spectral clustering may be
    preferable, particularly if the factors used to construct the network were not
    collected or are unknown for a third-party network. However, spectral clustering
    can also take partially labeled data as input, allowing for SSL that can guide
    the learning process given what is known already about the data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督与半监督聚类算法的选择高度特定于当前任务。对于非常大的网络，k-means 算法有很好的扩展性解决方案，并且添加良好的扩展性网络连通性度量可以改善
    k-means 解决方案，而不会牺牲效率。对于涉及纯网络连通性解决方案的问题，谱聚类可能更可取，尤其是如果构建网络所使用的因素未收集或对于第三方网络是未知的。然而，谱聚类也可以接受部分标记的数据作为输入，允许
    SSL 指导学习过程，考虑到已知的数据信息。
- en: Now that we’ve seen how UL and SSL algorithms can be used on network datasets,
    let’s turn our attention to SL algorithms, focusing on an exciting new type of
    **deep learning** (**DL**) algorithm specifically designed to take network datasets
    as their input.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何将 UL 和 SSL 算法用于网络数据集，让我们将注意力转向 SL 算法，重点关注一种特别设计的**深度学习**（**DL**）算法，该算法专门用于将网络数据集作为其输入。
- en: DL on networks
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络上的深度学习
- en: In this section, we’ll consider a new type of DL model called GNNs, which process
    and operate on networks by embedding vertex, edge, or global properties of the
    network to learn outcomes related to individual networks, vertex properties within
    a network, or edge properties within a network. Essentially, the DL architecture
    evolves the topology of these embeddings to find key topological features in the
    input data that are predictive of the outcome. This can be done in a fully supervised
    or semi-supervised fashion. In this example, we’ll focus on SSL, where only some
    of the labels are known; however, by providing all labels as input, this can be
    changed to an SL setting.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将考虑一种新的深度学习模型，称为 GNNs，它通过嵌入网络顶点、边或全局属性来处理和操作网络，以学习与单个网络、网络内的顶点属性或网络内的边属性相关的结果。本质上，深度学习架构通过演变这些嵌入的拓扑结构来找到输入数据中的关键拓扑特征，这些特征可以预测结果。这可以通过完全监督或半监督的方式进行。在这个例子中，我们将专注于
    SSL，其中只有一些标签是已知的；然而，通过提供所有标签作为输入，这可以改变为 SL 设置。
- en: Before we dive into the technical details of GNNs, let’s explore their use cases
    in more depth. Classifying networks themselves often yields important insight
    into problems such as image features or type, molecular compound toxicity or potential
    use as a pharmaceutical agent, or potential for epidemic spread within a country
    of interest given travel routes and population hubs.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨 GNNs 的技术细节之前，让我们更深入地探讨它们的使用案例。对网络本身进行分类通常可以提供关于图像特征或类型、分子化合物毒性或作为药物剂的潜在用途，或给定旅行路线和人口枢纽，在感兴趣的国家的潜在流行病传播等重要问题的见解。
- en: Typically, data such as molecules or images is transformed into network structure
    prior to the network embedding step of GNNs. Within the context of molecular compounds,
    atoms that share a covalent bound, for instance, are represented as vertices connected
    by an edge. Each compound, then, results in a unique network based on the molecular
    structure of that compound. For proteins, amino acids can serve as vertices, with
    connections existing between amino acids sharing a bond (such as a cysteine bridge
    resulting from a disulfide bond).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在 GNNs 的网络嵌入步骤之前，数据如分子或图像会被转换成网络结构。在分子化合物的背景下，共享共价键的原子，例如，被表示为通过边连接的顶点。因此，每个化合物都基于该化合物的分子结构产生一个独特的网络。对于蛋白质，氨基酸可以作为顶点，共享键（如由二硫键产生的半胱氨酸桥）的氨基酸之间存在连接。
- en: When screening potential compounds for use in pharmaceutical development, we
    often want to predict if the compound might have toxic effects. Using known databases
    of toxic compounds and compounds with no toxic effects, we can develop a GNN to
    predict the toxic effects of new compounds in development based on the molecular
    structures of the new compounds, given what we know about molecules that are known
    to be or not to be toxic. This allows for quick screening of potential new drugs
    for toxicity prior to animal or human trials.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在筛选用于药物开发的潜在化合物时，我们通常想要预测该化合物是否可能具有毒性。使用已知的毒性化合物数据库和无毒性化合物数据库，我们可以开发一个 GNN，根据新化合物的分子结构以及我们关于已知或未知毒性的分子的知识，预测开发中的新化合物的毒性。这允许在动物或人体试验之前快速筛选潜在的新药毒性。
- en: GNNs are also able to learn vertex labels given an input network, which is the
    focus of this chapter. For instance, within a crime or terrorism network, we may
    wish to identify potential leadership within the network given some knowledge
    of leaders and non-leaders from collected intelligence data. Incomplete information
    is common within intelligence data, and learning from what is known can be valuable
    in identifying key players in the network who are not known and who may be difficult
    to identify from informants or undercover agents. Since vertex prediction involves
    a network that has been constructed, we typically skip to the embedding steps
    of the GNN rather than wrangle the data. However, it might be necessary to add
    vertex labels to the graph to denote known information about leadership structure
    in the network.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: GNN 还能够根据输入网络学习顶点标签，这是本章的重点。例如，在一个犯罪或恐怖主义网络中，我们可能希望根据收集到的情报数据中关于领导者和非领导者的知识，在网络中识别潜在的领导层。情报数据中通常存在不完整的信息，从已知信息中进行学习可以帮助识别网络中未知的关键玩家，这些玩家可能难以从线人或卧底特工中识别。由于顶点预测涉及已经构建的网络，我们通常跳到
    GNN 的嵌入步骤，而不是整理数据。然而，可能需要在图中添加顶点标签，以表示网络中关于领导结构已知的信息。
- en: Edge learning with GNNs mirrors vertex learning, typically through the use of
    an existing network with complete or incomplete information about edge properties
    (such as communication frequency or importance across members of a terrorist network
    that might involve coordinating a terrorist attack or recruiting new members in
    a geographic region). We embed the edges rather than the vertices in this case
    before proceeding with the GNN training.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GNN 进行边学习与顶点学习类似，通常通过使用具有关于边属性（如恐怖主义网络成员之间的通信频率或重要性，可能涉及协调恐怖袭击或在该地理区域内招募新成员）的完整或不完整信息的现有网络。在这种情况下，我们在进行
    GNN 训练之前将边嵌入，而不是顶点。
- en: Now that we know a bit about problems we can tackle with GNNs, let’s learn more
    about the architecture and mathematical operations used to build a GNN.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了一些可以用 GNN 解决的问题，让我们更多地了解构建 GNN 所使用的架构和数学运算。
- en: GNN introduction
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GNN 简介
- en: GNN construction involves a few key steps. In the prior subsection, we mentioned
    data transformation as a potential first step. GNNs require a network or tensor
    of networks as input to the embedding step of the algorithm, so data must contain
    network-structured data and some outcome label associated with the networks themselves
    or edges/vertices in the network of interest. Some data engineering may be required
    to wrangle image(s), molecule(s), or other data sources into network structures.
    In the prior subsection, we overviewed how molecule or protein data can be transformed
    into a network structure. Many common types of data have standard transformation
    methods to transform them into network data; for example, in prior chapters, we’ve
    transformed spatial and time series data into network structures that could be
    used as input for a GNN.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: GNN 构建涉及几个关键步骤。在前一小节中，我们提到了数据转换作为可能的第一个步骤。GNN 需要一个网络或网络张量作为算法嵌入步骤的输入，因此数据必须包含网络结构化的数据以及与网络本身或感兴趣网络中的边/顶点相关的一些结果标签。可能需要一些数据工程来将图像、分子或其他数据源整理成网络结构。在前一小节中，我们概述了分子或蛋白质数据如何转换为网络结构。许多常见类型的数据都有标准的转换方法，可以将它们转换为网络数据；例如，在前几章中，我们将空间和时间序列数据转换为可用于
    GNN 输入的网络结构。
- en: Once our data exists in a network structure with a set of labels for networks,
    edges, or vertices, we’re ready to embed the relevant structures at a network,
    edge, or vertex level. Embeddings aim to find a low-dimensional representation
    of relevant network geometry at the level of embedding (network, edge, or vertex).
    They can also include other relevant information, such as other attributes of
    networks, edges, or vertices. Sometimes, it’s advantageous to create these embeddings
    manually to include both relevant network structure and attribute information.
    For instance, in our friendship network, we have data on many activities in which
    individuals participate; we may wish to create an embedding that captures not
    only the network centrality metrics but also the activity participation for individuals
    represented as a network vertex. In our k-means example, including both types
    of information (network structure and collected activity data) improved k-means
    performance in finding groups we hypothesized to exist.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的数据存在于一个具有网络、边或顶点标签的网络结构中时，我们就准备好在网络、边或顶点级别嵌入相关结构了。嵌入的目标是在嵌入级别（网络、边或顶点）找到相关网络几何的低维表示。它们还可以包括其他相关信息，例如网络、边或顶点的其他属性。有时，手动创建这些嵌入是有利的，以便包含相关的网络结构和属性信息。例如，在我们的友谊网络中，我们有许多个人参与的活动数据；我们可能希望创建一个嵌入，不仅能够捕捉网络中心性指标，还能够捕捉作为网络顶点表示的个人活动参与度。在我们的k-means示例中，包括这两种类型的信息（网络结构和收集到的活动数据）提高了k-means在寻找我们假设存在的组时的性能。
- en: Many GNN packages in Python, such as PyTorch (which we will use later in the
    section), have functions that summarize network properties at the network, edge,
    and vertex level to create an automatic embedding at a specified dimensionality.
    How we embed data prior to GNN training greatly impacts results, so this step
    is important to consider when building a GNN. Even with package functions such
    as the PyTorch one that we’ll use, specifying a dimensionality impacts algorithm
    performance. We don’t want too low of a dimensionality (missing key features relevant
    to the outcome of interest), but we also don’t want too high of a dimensionality
    (which might include a lot of noise). In practice, this parameter is often optimized
    through grid search.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Python中的许多GNN包，如PyTorch（我们将在后续部分中使用），都有函数可以总结网络在网络、边和顶点级别的属性，以在指定的维度上创建自动嵌入。我们在GNN训练之前如何嵌入数据对结果有很大影响，因此在构建GNN时考虑这一步很重要。即使使用我们将会使用的PyTorch等包函数，指定维度也会影响算法性能。我们不想维度太低（遗漏了与感兴趣的结果相关的关键特征），但也不希望维度太高（可能包含很多噪声）。在实践中，这个参数通常通过网格搜索进行优化。
- en: Once we have the embedding, we can define the outcomes as target labels. We
    may need to employ one-hot encoding to transform text labels into a sequence of
    binary outcomes. Just as other DL algorithms can handle multiclass classification
    problems, continuous outcomes, or other types of distributions, GNNs can fit many
    different outcomes of interest. This flexibility makes them ideal for modeling
    outcomes across network classification/regression problems.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了嵌入，我们就可以定义结果为目标标签。我们可能需要使用one-hot编码将文本标签转换为一系列二进制结果。正如其他深度学习算法可以处理多类分类问题、连续结果或其他类型的分布一样，GNN可以适应许多感兴趣的不同结果。这种灵活性使它们非常适合建模网络分类/回归问题中的结果。
- en: 'The DL architecture itself is not unique. Readers who are familiar with **convolutional
    neural networks** (**CNNs**) will recognize many of the components and backfitting
    algorithms we’ll discuss, as they are identical within the context of GNNs. We
    start with an input layer with a dimension equal to the embedding dimension, and
    we end with an output layer with a dimension equal to the number of classes of
    our outcome (for classification problems, which we’ll consider in this chapter).
    When only the input and output layers exist, **neural networks** (**NNs**) approximate
    linear regression, with a learned map between input matrices and output vectors.
    However, between these layers, we typically include hidden layers that further
    process features between the input and output layers, as shown in *Figure 9**.4*:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习架构本身并不独特。熟悉**卷积神经网络**（**CNNs**）的读者将认识到我们将讨论的许多组件和反向拟合算法，因为它们在GNN的上下文中是相同的。我们从一个维度等于嵌入维度的输入层开始，并以一个维度等于我们结果类别数（对于分类问题，我们将在本章中考虑）的输出层结束。当只有输入层和输出层存在时，**神经网络**（**NNs**）近似线性回归，输入矩阵和输出向量之间存在一个学习到的映射。然而，在这些层之间，我们通常包括隐藏层，以进一步处理输入层和输出层之间的特征，如图*9**.4*所示：
- en: '![Figure 9.4 – A summary of the GNN life cycle, including data engineering
    and DL architecture steps](img/B21087_09_04.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 – GNN生命周期总结，包括数据工程和深度学习架构步骤](img/B21087_09_04.jpg)'
- en: Figure 9.4 – A summary of the GNN life cycle, including data engineering and
    DL architecture steps
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – GNN生命周期总结，包括数据工程和深度学习架构步骤
- en: Hidden layers refine the topological maps between the input and output layers,
    often pooling topological features found during the training process to feed into
    the next hidden layer. For small networks and small samples of networks, the number
    of hidden layers should be small to maintain the stability of the solution and
    obtain good performance. For larger networks or sets of networks, more hidden
    layers can be added to improve performance without encountering instability of
    solutions and fit.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层细化输入层和输出层之间的拓扑映射，通常在训练过程中汇总发现的拓扑特征，以输入到下一个隐藏层。对于小型网络和小型网络样本，隐藏层的数量应该较小，以保持解的稳定性和获得良好的性能。对于大型网络或网络集，可以添加更多隐藏层以提高性能，而不会遇到解的不稳定性或拟合问题。
- en: Hidden layers typically employ a non-linear mapping function, called an *activation
    function*, between the input layer and output layer connected to that specific
    hidden layer. In practice, only a few activation functions are common, including
    the *ReLU function*, which returns 0 for negative or zero input values and the
    input value for positive input values.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层通常使用一个非线性映射函数，称为**激活函数**，在输入层和连接到该特定隐藏层的输出层之间。在实践中，只有少数激活函数是常见的，包括**ReLU函数**，对于负数或零输入值返回0，对于正输入值返回输入值本身。
- en: '*Convolution layers*, also commonly used in hidden layers, apply a filter function
    (typically a kernel) to the input layer to transform it through the defined kernel
    function. Typically, a convolution layer will reduce the dimensionality of the
    matrix or tensor, so zero padding to maintain dimensionality may be used to avoid
    the problem of transforming layers in a way that limits the number of layers possible
    given the dimensionality of the output. For small datasets, such as the one we
    consider, this is not necessarily a problem, as shallow networks tend to be the
    only stable GNNs that we can train given the limited data size.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积层**，也常用于隐藏层，将一个滤波函数（通常是核）应用于输入层，通过定义的核函数对其进行转换。通常，卷积层会降低矩阵或张量的维度，因此可能使用零填充以保持维度，以避免以限制给定输出维度可能层数的方式转换层。对于小型数据集，例如我们考虑的，这并不一定是问题，因为浅层网络往往是我们在有限数据量下可以训练的唯一稳定的GNN。'
- en: The theory of building effective architectures is beyond the scope of this book,
    and readers interested in DL who do not have a background can obtain this knowledge
    by reading through the references provided at the end of this chapter.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 构建有效架构的理论超出了本书的范围，对深度学习（DL）感兴趣但没有背景知识的读者可以通过阅读本章末尾提供的参考文献来获取这些知识。
- en: 'Once an architecture is defined (either through expert guessing or, again,
    grid search to optimize architecture), we must fit the parameters connecting nodes
    in each layer and across layers (called *weights*). There are many options to
    do this, and it’s possible to define custom fitting algorithms. However, we’ll
    focus on the two most common options within the PyTorch package used in our example:
    **Adam optimizers** and **stochastic gradient** **descent** (**SGD**).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了架构（无论是通过专家猜测还是再次通过网格搜索来优化架构），我们必须拟合连接每一层节点以及层间节点的参数（称为 *权重*）。有许多选项可以做到这一点，并且可以定义自定义拟合算法。然而，我们将关注
    PyTorch 包中我们示例中使用的两个最常见选项：**Adam 优化器**和**随机梯度下降**（**SGD**）。
- en: '*SGD* fits weights between nodes within and across layers by exploring the
    gradient function defined on the NN in much the way that gradient boosting fits
    a linear regression model. A *learning rate* is defined to control the exploration
    of the gradient function. A steeper learning rate fits a model more quickly but
    may not find global minima or maxima. One caveat of SGD is that the algorithm
    can get stuck in local optima, resulting in lower accuracies of results than what
    is possible given the input data, mapping functions between layers, and the outcome
    data. It also tends to be slower, requiring more algorithm iterations and potentially
    more processing power to even fit the model.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*SGD* 通过探索定义在神经网络上的梯度函数，在节点之间以及层与层之间调整权重，这与梯度提升法拟合线性回归模型的方式非常相似。定义了一个 *学习率*
    来控制梯度函数的探索。较大的学习率可以更快地拟合模型，但可能无法找到全局最小值或最大值。SGD 的一个缺点是算法可能会陷入局部最优，导致结果准确率低于给定输入数据、层间映射函数和输出数据所能达到的准确率。它还倾向于较慢，需要更多的算法迭代，甚至可能需要更多的处理能力来拟合模型。'
- en: '*Adam optimizers* allow for flexible learning rates for different weights between
    nodes, leading to faster model fits and avoiding local optima by allowing the
    learning rate to adjust to the local gradient landscape. Adam also allows for
    decay rates, further customizing local learning of weights. Many Adam optimizers
    have evolved since the initial Adam optimizer was developed, and it’s likely more
    will be developed for GNNs and other DL architectures. One drawback is that Adam
    optimizers tend to be memory-intensive. When training large GNNs, it may be preferable
    to use SGD to avoid memory issues during training.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*Adam 优化器* 允许节点之间具有灵活的学习率，从而加快模型拟合速度，并通过允许学习率调整到局部梯度景观来避免局部最优。Adam 还允许衰减率，进一步定制局部权重学习。自从最初的
    Adam 优化器开发以来，已经发展了许多 Adam 优化器，并且很可能会有更多针对 GNN 和其他深度学习架构的开发。一个缺点是 Adam 优化器通常占用内存较多。在训练大型
    GNN 时，可能更倾向于使用 SGD 以避免训练过程中的内存问题。'
- en: In practice, it’s difficult to know which optimizer is best for fitting the
    weights of the defined architecture, and grid search, again, is typically employed
    for industry GNN models to optimize this choice. Once a fitting algorithm is selected,
    a predefined (again, usually optimized by grid search) number of iterations is
    run, or the algorithm runs until meeting a stopping criterion. Adam optimizers
    tend to converge more quickly than SGD optimizers, but performance can vary depending
    on the data and architecture.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，很难知道哪种优化器最适合拟合定义的架构的权重，因此通常会采用网格搜索来优化这一选择。一旦选择了拟合算法，就会运行预定义的（通常通过网格搜索优化）迭代次数，或者算法会运行直到满足停止标准。Adam
    优化器通常比 SGD 优化器收敛得更快，但性能可能会根据数据和架构而变化。
- en: Now that we understand a bit about the building blocks of GNNs, let’s explore
    an example using an open source sports network consisting of students assigned
    to two different teachers (our outcome of interest).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 GNN 的构建块有了一些了解，让我们通过一个开源体育网络示例来探索，该网络由分配给两位不同老师的两名学生组成（这是我们感兴趣的结果）。
- en: Example GNN classifying the Karate Network dataset
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 GNN 对 Karate Network 数据集进行分类
- en: 'For our example, we’ll predict vertex-level attributes in a common open source
    network: Zachary’s `Karate Network` dataset. This dataset consists of 34 individuals
    connected by 78 edges in a karate training network who ended up splitting between
    an administrator and one of the instructors when a conflict between the administrator
    and instructor occurred. One of the primary tasks for vertex classification and
    learning problems on this network is to predict which individuals ended up siding
    with which person in the conflict (the administrator or the instructor). We will
    predict vertex labels through a semi-supervised GNN model approach.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，我们将预测一个常见开源网络中的顶点级属性：Zachary的`Karate Network`数据集。这个数据集由34个个体组成，他们通过78条边连接在一个空手道训练网络中，当管理员和教练之间发生冲突时，他们最终分裂成管理员和教练两派。在这个网络上的顶点分类和学习问题中的一个主要任务是预测哪些个体最终站在冲突中的哪一方（管理员或教练）。我们将通过半监督GNN模型方法来预测顶点标签。
- en: 'We’ll first install the needed packages and import our dataset. If you don’t
    have the necessary packages installed, please install them on your machine prior
    to running our code. We’ve provided this step as an option in `Script 9.2`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将安装所需的包并导入我们的数据集。如果您还没有安装必要的包，请在运行我们的代码之前在您的机器上安装它们。我们已经在`Script 9.2`中提供了这一步骤作为选项：
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To embed our vertex data, we’ll use PyTorch’s default embedding algorithm with
    a dimensionality of `6`. Anything that is in the `4`-`6` dimension range should
    work reasonably well, given the size of our network. Let’s add to `Script 9.2`
    to embed our vertices:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了嵌入我们的顶点数据，我们将使用PyTorch的默认嵌入算法，其维度为`6`。考虑到我们网络的大小，任何位于`4`至`6`维度范围内的内容都应该能合理地工作。让我们在`Script
    9.2`中添加代码以嵌入我们的顶点：
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This piece of the script should output embedding vectors for each vertex in
    our network. Now that we have our vertices embedded, we can create our labels.
    Given that we wish to demonstrate a semi-supervised approach, we’ll feed our network
    information on six vertices (`1`, `3`, `5`, `12`, `15`, and `32`). You can play
    around with this part of the script to see how fewer or more vertices impact the
    performance and stability of our chosen architecture. Let’s add the label information
    by adding to `Script 9.2`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这段脚本应该为我们的网络中的每个顶点输出嵌入向量。现在我们已经嵌入顶点，我们可以创建我们的标签。鉴于我们希望展示半监督方法，我们将向网络提供关于六个顶点（`1`、`3`、`5`、`12`、`15`和`32`）的信息。您可以尝试调整脚本的这一部分，看看顶点数量的增减如何影响我们选择架构的性能和稳定性。让我们通过添加到`Script
    9.2`来添加标签信息：
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we’ll need to build our GNN architecture and define training parameters.
    Many of the papers and tutorials on GNNs using this dataset employ a very shallow
    network architecture and Adam optimizers. For the sake of comparison and demonstration
    of other options for building GNNs, we’ll use two hidden layers instead of one
    (including convolution layers coupled with ReLU functions), employ small layers
    (eight and six nodes, respectively for hidden layers), an SGD fitting algorithm
    (with a learning rate of `0.01` and a momentum driving the algorithm of `0.8`,
    which is close to the default value), and `990` iterations. Many examples that
    exist online use Adam optimizers and a single hidden layer with more nodes than
    our architecture, allowing for fewer training iterations. However, for larger
    network vertex-label prediction problems, a more complex architecture is likely
    to perform better, so we will show a way to include more hidden layers and a way
    to use a different fitting algorithm than Adam. Let’s define our architecture
    and fit our weights by adding to `Script 9.2`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要构建我们的GNN架构并定义训练参数。许多使用此数据集的GNN论文和教程采用非常浅的网络架构和Adam优化器。为了比较和展示构建GNN的其他选项，我们将使用两个隐藏层而不是一个（包括与ReLU函数耦合的卷积层），使用小型层（隐藏层分别为八个和六个节点），SGD拟合算法（学习率为`0.01`，动量为推动算法的`0.8`，接近默认值），以及`990`次迭代。许多在线的例子使用Adam优化器和比我们的架构节点更多的单隐藏层，允许更少的训练迭代。然而，对于更大的网络顶点标签预测问题，更复杂的架构可能表现更好，因此我们将展示如何包含更多隐藏层以及如何使用不同于Adam的拟合算法。让我们通过添加到`Script
    9.2`来定义我们的架构并拟合我们的权重：
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You should see the loss function (logistic regression link function here) that
    decreases across iterations as your output. Typical accuracies from GNN architectures
    fall into the 95%-100% range. Because this dataset is small and our architecture
    is large, your output accuracies may vary quite a bit between runs of the algorithm.
    This has to do with random sampling within the fitting steps of the algorithm
    and the coarseness of the underlying gradient landscape. Let’s add to `Script
    9.2` to find our accuracy:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到损失函数（此处为逻辑回归链接函数）随着迭代次数的增加而降低，这是你的输出。典型的 GNN 架构准确率在 95%-100% 范围内。由于这个数据集较小，我们的架构较大，你的算法运行准确率可能会有很大差异。这与算法拟合步骤中的随机采样和底层梯度景观的粗糙度有关。让我们在`脚本
    9.2`中添加一些内容来找到我们的准确率：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Our run of the algorithm gives an accuracy of ~97% in this run of the algorithm.
    That is on par with the performance of other GNN architectures. However, don’t
    be surprised if your accuracy is significantly lower in one or more runs of the
    script, as we don’t have a large enough sample size to fit this type of architecture.
    Changing the embedding dimension, architecture, and training algorithm parameters
    will impact accuracy, and interested readers are encouraged to revise `Script
    9.2` as a way to see how different choices impact accuracy and stability of fit.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行该算法的准确率在此次运行中达到了 ~97%。这与其他 GNN 架构的性能相当。然而，如果你的准确率在一个或多个脚本运行中显著降低，请不要感到惊讶，因为我们没有足够大的样本量来适应这种架构。改变嵌入维度、架构和训练算法参数将影响准确率，并且鼓励感兴趣的读者修改`脚本
    9.2`，以了解不同选择如何影响准确率和拟合的稳定性。
- en: In general, GNN classifiers work much better and show better stability on larger
    networks and with more labels fed into semi-supervised usage. The Zachary Karate
    Network dataset is small enough that other methods are recommended to classify
    the network. However, learning labels on a huge social network (such as those
    found on social media platforms) or a large geographic network (such as a United
    States city network with connections defined by roads connecting cities larger
    than 50,000 people) would result in a more stable GNN solution, and it would be
    possible to create a very deep architecture. However, to fit these large models,
    we often need a cloud computing platform, as the large datasets and large number
    of iterations can be difficult on a laptop.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，GNN 分类器在较大的网络和更多的标签输入到半监督使用时工作得更好，并且表现出更好的稳定性。Zachary 拳击网络数据集足够小，因此建议使用其他方法来分类网络。然而，在大型社交网络（如社交媒体平台上的那些）或大型地理网络（如由连接城市超过
    50,000 人的道路定义的美国城市网络）上学习标签将导致更稳定的 GNN 解决方案，并且可以创建一个非常深的架构。但是，为了拟合这些大型模型，我们通常需要一个云计算平台，因为大型数据集和大量迭代对笔记本电脑来说可能很难处理。
- en: GNNs have shown great promise in network-based classification problems in many
    different fields, and it is likely that they will continue to evolve and solve
    pressing problems with large networks and collections of networks. However, cloud
    computing solutions are often needed, and this requires expertise working with
    data and Python notebook solutions on the cloud computing platform used to store
    data and fit the GNN.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: GNN 在许多不同领域的基于网络的分类问题中显示出巨大的潜力，并且它们很可能将继续发展和解决与大型网络和网络集合相关的紧迫问题。然而，云计算解决方案通常是必需的，这需要与数据以及云平台上的
    Python 笔记本解决方案合作的专业知识，该平台用于存储数据和拟合 GNN。
- en: Summary
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we considered several use cases of ML algorithms on network
    datasets. This included UL on a friendship network through fitting k-means and
    spectral clustering. We considered k-means clustering on both the original dataset
    of activities in which individuals participated and the original dataset, with
    added network metrics to improve clustering accuracy. We then turned to SL and
    SSL on networks and collections of networks through a type of DL algorithm called
    GNNs. We accurately predicted the labels of individuals in Zachary’s Karate Network
    dataset through a shallow GNN and compared results with other existing solutions
    to this network classification problem. In [*Chapter 10*](B21087_10.xhtml#_idTextAnchor128),
    we'll mine educational data for causal relationships using network tools related
    to conditional probability.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们考虑了网络数据集上机器学习算法的几个用例。这包括通过拟合k-means和谱聚类在友谊网络上的UL。我们考虑了对个人参与的活动原始数据集以及添加了网络度量以改进聚类精度的原始数据集进行k-means聚类。然后，我们通过一种称为GNNs的深度学习算法类型，在网络上和网络的集合上进行SL和SSL。我们通过浅层GNN准确预测了Zachary的空手道网络数据集中个人的标签，并将结果与其他现有的网络分类问题解决方案进行了比较。在[*第10章*](B21087_10.xhtml#_idTextAnchor128)中，我们将使用与条件概率相关的网络工具挖掘教育数据中的因果关系。
- en: References
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Acharya, D. B., & Zhang, H. (2021). *Weighted Graph Nodes Clustering via Gumbel
    Softmax*. arXiv preprint arXiv:2102.10775.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Acharya, D. B., & Zhang, H. (2021). *加权图节点聚类通过Gumbel Softmax*。arXiv预印本 arXiv:2102.10775。
- en: Bongini, P., Bianchini, M., & Scarselli, F. (2021). Molecular generative graph
    neural networks for drug discovery. *Neurocomputing,* *450, 242-252.*
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Bongini, P., Bianchini, M., & Scarselli, F. (2021)。用于药物发现的分子生成图神经网络。*神经计算*，*450，242-252。*
- en: Fan, W., Ma, Y., Li, Q., He, Y., Zhao, E., Tang, J., & Yin, D. (2019, May).
    Graph neural networks for social recommendation. *In The World Wide Web Conference
    (**pp. 417-426).*
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Fan, W., Ma, Y., Li, Q., He, Y., Zhao, E., Tang, J., & Yin, D. (2019, 五月)。图神经网络在社交推荐中的应用。*世界万维网会议(**第417-426页)。*
- en: 'Hartigan, J. A., & Wong, M. A. (1979). Algorithm AS 136: A k-means clustering
    algorithm. *Journal of the Royal Statistical Society. Series C (Applied Statistics),*
    *28(1), 100-108.*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Hartigan, J. A., & Wong, M. A. (1979)。算法AS 136：一个k-means聚类算法。*皇家统计学会系列C（应用统计）*，*28(1)，100-108。*
- en: 'Imambi, S., Prakash, K. B., & Kanagachidambaresan, G. R. (2021). PyTorch. *Programming
    with TensorFlow: Solution for Edge Computing* *Applications, 87-104.*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Imambi, S., Prakash, K. B., & Kanagachidambaresan, G. R. (2021)。PyTorch。*使用TensorFlow进行编程：边缘计算应用解决方案*，*87-104。*
- en: Kumar, V. (2020). *An Investigation Into Graph Neural Networks (Doctoral dissertation,
    Trinity College* *Dublin, Ireland).*
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Kumar, V. (2020)。*对图神经网络的研究（博士论文，都柏林爱尔兰的Trinity College）。*
- en: 'Labonne, M. (2023). *Hands-On Graph Neural Networks Using Python: Practical
    techniques and architectures for building powerful graph and deep learning apps
    with PyTorch. Packt* *Publishing Ltd.*'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Labonne, M. (2023)。*使用Python进行图神经网络实践：使用PyTorch构建强大的图和深度学习应用程序的技术和架构。Packt出版社*。
- en: Liang, F., Qian, C., Yu, W., Griffith, D., & Golmie, N. (2022). Survey of graph
    neural networks and applications. *Wireless Communications and Mobile* *Computing,
    2022.*
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Liang, F., Qian, C., Yu, W., Griffith, D., & Golmie, N. (2022)。图神经网络及其应用的综述。*无线通信和移动计算*，2022。
- en: Mantzaris, A. V., Chiodini, D., & Ricketson, K. (2021). Utilizing the simple
    graph convolutional neural network as a model for simulating influence spread
    in networks. *Computational Social Networks,* *8, 1-17.*
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Mantzaris, A. V., Chiodini, D., & Ricketson, K. (2021)。利用简单的图卷积神经网络作为模拟网络中影响传播的模型。*计算社交网络*，*8，1-17。*
- en: Min, S., Gao, Z., Peng, J., Wang, L., Qin, K., & Fang, B. (2021). STGSN—A Spatial–Temporal
    Graph Neural Network framework for time-evolving social networks. *Knowledge-Based
    Systems,* *214, 106746.*
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Min, S., Gao, Z., Peng, J., Wang, L., Qin, K., & Fang, B. (2021). STGSN—一个用于时间演变社交网络的时空图神经网络框架。*知识系统*，*214，106746.*
- en: 'Ng, A., Jordan, M., & Weiss, Y. (2001). On spectral clustering: Analysis and
    an algorithm. *Advances in neural information processing* *systems, 14.*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Ng, A., Jordan, M., & Weiss, Y. (2001)。关于谱聚类的分析和一个算法。*神经网络信息处理系统进展*，*14。*
- en: Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., & Monfardini, G. (2008).
    The graph neural network model. *IEEE transactions on neural networks,* *20(1),
    61-80.*
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., & Monfardini, G. (2008).
    图神经网络模型。*IEEE神经网络Transactions*，*20(1)，61-80.*
- en: 'Wieder, O., Kohlbacher, S., Kuenemann, M., Garon, A., Ducrot, P., Seidel, T.,
    & Langer, T. (2020). A compact review of molecular property prediction with graph
    neural networks. *Drug Discovery Today: Technologies,* *37, 1-12.*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Wieder, O., Kohlbacher, S., Kuenemann, M., Garon, A., Ducrot, P., Seidel, T.,
    & Langer, T. (2020). A compact review of molecular property prediction with graph
    neural networks. *《药物发现今日：技术》*，*37*，1-12.
- en: Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., & Philip, S. Y. (2020). A comprehensive
    survey on graph neural networks. *IEEE transactions on neural networks and learning
    systems,* *32(1), 4-24.*
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., & Philip, S. Y. (2020). A comprehensive
    survey on graph neural networks. *《IEEE神经网络与学习系统汇刊》*，*32*(1)，4-24.
- en: Zachary, W. W. (1977). An information flow model for conflict and fission in
    small groups. *Journal of Anthropological Research,* *33(4), 452-473.*
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Zachary, W. W. (1977). An information flow model for conflict and fission in
    small groups. *《人类学研究杂志》*，*33*(4)，452-473.
- en: Zhang, L., Xu, J., Pan, X., Ye, J., Wang, W., Liu, Y., & Wei, Q. (2023). Visual
    analytics of route recommendation for tourist evacuation based on graph neural
    network. *Scientific Reports,* *13(1), 17240.*
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang, L., Xu, J., Pan, X., Ye, J., Wang, W., Liu, Y., & Wei, Q. (2023). Visual
    analytics of route recommendation for tourist evacuation based on graph neural
    network. *《科学报告》*，*13*(1)，17240.
- en: 'Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., ... & Sun, M. (2020).
    *Graph neural networks: A review of methods and applications. AI open,* *1, 57-81.*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., ... & Sun, M. (2020).
    *《图神经网络：方法与应用综述》*，*AI open*，*1*，57-81.
