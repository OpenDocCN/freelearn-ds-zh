- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Mock Test 2
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟测试 2
- en: Questions
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Try your hand at these practice questions to test your knowledge of Apache
    Spark:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试回答这些问题以测试你对 Apache Spark 的了解：
- en: '**Question 1**:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 1**:'
- en: What is a task in Spark?
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 中的任务是什么？
- en: The unit of work performed for each data partition within a task is the slots
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在任务中为每个数据分区执行的工作单元是插槽
- en: A task is the second-smallest entity that can be executed within Spark
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任务是 Spark 中可以执行的第二小实体
- en: Tasks featuring wide dependencies can be combined into a single task
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 具有宽依赖的任务可以合并为单个任务
- en: A task is the smallest component that can be executed within Spark
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任务是 Spark 中可以执行的最小组件
- en: '**Question 2**:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 2**:'
- en: What is the role of an executor in Spark?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 执行器在 Spark 中的角色是什么？
- en: The executor’s role is to request the transformation of operations into a directed
    acyclic graph (DAG)
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器的角色是请求将操作转换为有向无环图 (DAG)
- en: There can only be one executor within a Spark environment
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 环境中只能有一个执行器
- en: Executors are tasked with executing the assignments provided to them by the
    driver
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器负责执行驱动程序分配给它们的任务
- en: The executor schedules queries for execution
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器安排查询以执行
- en: '**Question 3**:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 3**:'
- en: Which of the following is one of the tasks of Adaptive Query Execution in Spark?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个是自适应查询执行在 Spark 中的任务之一？
- en: Adaptive Query Execution collects runtime statistics during query execution
    to optimize query plans
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自适应查询执行在查询执行期间收集运行时统计信息以优化查询计划
- en: Adaptive Query Execution is responsible for distributing tasks to executors
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自适应查询执行负责将任务分配给执行器
- en: Adaptive Query Execution is responsible for wide operations in Spark
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自适应查询执行负责 Spark 中的宽操作
- en: Adaptive Query Execution is responsible for fault tolerance in Spark
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自适应查询执行负责 Spark 中的容错
- en: '**Question 4**:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 4**:'
- en: Which is the lowest level in Spark’s execution hierarchy?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 执行层次结构中的最低级别是什么？
- en: Task
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任务
- en: Slot
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插槽
- en: Job
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作业
- en: Stage
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阶段
- en: '**Question 5**:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 5**:'
- en: Which one of these operations is an action?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个操作是动作？
- en: '`DataFrame.count()`'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame.count()`'
- en: '`DataFrame.filter()`'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame.filter()`'
- en: '`DataFrame.select()`'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame.select()`'
- en: '`DataFrame.groupBy()`'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame.groupBy()`'
- en: '**Question 6**:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 6**:'
- en: Which of the following describes the characteristics of the DataFrame API?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个描述了 DataFrame API 的特性？
- en: The DataFrame API is based on resilient distributed dataset (RDD) at the backend
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DataFrame API 在后端基于弹性分布式数据集 (RDD)
- en: The DataFrame API is available in Scala, but it is not available in Python
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DataFrame API 在 Scala 中可用，但在 Python 中不可用
- en: The DataFrame API does not have data manipulation functions
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DataFrame API 没有数据操作函数
- en: The DataFrame API is used for distributing tasks in executors
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DataFrame API 用于在执行器中分配任务
- en: '**Question 7**:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 7**:'
- en: Which of the following statements is accurate about executors?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个关于执行器的陈述是准确的？
- en: Slots are not a part of an executor
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插槽不是执行器的一部分
- en: Executors are able to run tasks in parallel via slots
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器能够通过插槽并行运行任务
- en: Executors are always equal to tasks
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器始终等于任务
- en: An executor is responsible for distributing tasks for a job
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器负责为作业分配任务
- en: '**Question 8**:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 8**:'
- en: Which of the following statements is accurate about the Spark driver?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个关于 Spark 驱动程序的陈述是准确的？
- en: There are multiple drivers in a Spark application
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 应用程序中有多个驱动程序
- en: Slots are a part of a driver
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插槽是驱动程序的一部分
- en: Drivers execute tasks in parallel
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 驱动程序并行执行任务
- en: It is the responsibility of the Spark driver to transform operations into DAG
    computations
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将操作转换为 DAG 计算的责任在于 Spark 驱动程序
- en: '**Question 9**:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 9**:'
- en: Which one of these operations is a wide transformation?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个操作是宽转换？
- en: '`DataFrame.show()`'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame.show()`'
- en: '`DataFrame.groupBy()`'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame.groupBy()`'
- en: '`DataFrame.repartition()`'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame.repartition()`'
- en: '`DataFrame.select()`'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame.select()`'
- en: '`DataFrame.filter()`'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame.filter()`'
- en: '**Question 10**:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 10**:'
- en: Which of the following statements is correct about lazy evaluation?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个关于惰性评估的陈述是正确的？
- en: Execution is triggered by transformations
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行是由转换触发的
- en: Execution is triggered by actions
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行是由动作触发的
- en: Statements are executed as they appear in the code
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 语句按照代码中的顺序执行
- en: Spark distributes tasks to different executors
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 将任务分配给不同的执行器
- en: '**Question 11**:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 11**:'
- en: Which of the following is true about DAGs in Spark?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个关于 Spark 中的 DAGs 的陈述是正确的？
- en: DAGs are lazily evaluated
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DAGs 是惰性评估的
- en: DAGs can be scaled horizontally in Spark
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DAGs 可以在 Spark 中水平扩展
- en: DAGs are responsible for processing partitions in an optimized and distributed
    fashion
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DAGs负责以优化和分布式的方式处理分区
- en: DAG is comprised of tasks that can run in parallel
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DAG由可以并行运行的任务组成
- en: '**Question 12**:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 12**:'
- en: Which of the following statements is true about Spark’s fault tolerance mechanism?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个关于Spark容错机制的陈述是正确的？
- en: Spark achieves fault tolerance via DAGs
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark通过DAGs实现容错能力
- en: It is the responsibility of the executor to enable fault tolerance in Spark
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使Spark具备容错能力是执行器的责任
- en: Because of fault tolerance, Spark can recompute any failed RDD
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于容错能力，Spark可以重新计算任何失败的RDD
- en: Spark builds a fault-tolerant layer on top of the legacy RDD data system, which
    by itself is not fault tolerant
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark在传统的RDD数据系统之上构建了一个容错层，而RDD本身并不具备容错能力
- en: '**Question 13**:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 13**:'
- en: What is the core of Spark’s fault-tolerant mechanism?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Spark容错机制的核心是什么？
- en: RDD is at the core of Spark, which is fault tolerant by design
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RDD是Spark的核心，它设计上具有容错能力
- en: Data partitions, since data can be recomputed
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据分区，因为数据可以被重新计算
- en: DataFrame is at the core of Spark since it is immutable
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DataFrame是Spark的核心，因为它是不变的
- en: Executors ensure that Spark remains fault tolerant
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器确保Spark保持容错能力
- en: '**Question 14**:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 14**:'
- en: What is accurate about jobs in Spark?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中的作业有哪些准确之处？
- en: Different stages in a job may be executed in parallel
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作业的不同阶段可以并行执行
- en: Different stages in a job cannot be executed in parallel
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作业的不同阶段不能并行执行
- en: A task consists of many jobs
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个任务由许多作业组成
- en: A stage consists of many jobs
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个阶段由许多作业组成
- en: '**Question 15**:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 15**:'
- en: What is accurate about a shuffle in Spark?
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中的shuffle有哪些准确之处？
- en: In a shuffle, data is sent to multiple partitions to be processed
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在shuffle过程中，数据被发送到多个分区进行处理
- en: In a shuffle, data is sent to a single partition to be processed
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在shuffle过程中，数据被发送到单个分区进行处理
- en: A shuffle is an action that triggers evaluation in Spark
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Shuffle是一个触发Spark评估的操作
- en: In a shuffle, all data remains in memory to be processed
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在shuffle过程中，所有数据都保留在内存中以便处理
- en: '**Question 16**:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 16**:'
- en: What is accurate about the cluster manager in Spark?
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中的集群管理器有哪些准确之处？
- en: The cluster manager is responsible for managing resources for Spark
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群管理器负责管理Spark的资源
- en: The cluster manager is responsible for working with executors directly
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群管理器负责直接与执行器协同工作
- en: The cluster manager is responsible for creating query plans
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群管理器负责创建查询计划
- en: The cluster manager is responsible for optimizing DAGs
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群管理器负责优化DAGs
- en: '**Question 17**:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 17**:'
- en: 'The following code block needs to take the sum and average of the `salary`
    column for each department in the `df` DataFrame. Then, it should calculate the
    sum and maximum value for the `bonus` column:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块需要计算`df` DataFrame中每个部门的`salary`列的总和和平均值。然后，它应该计算`bonus`列的总和和最大值：
- en: '[PRE0]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Choose the answer that correctly fills the blanks in the code block to accomplish
    this:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的答案来填充代码块中的空白，以完成此任务：
- en: '`groupBy`'
  id: totrans-105
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`groupBy`'
- en: '`agg`'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`agg`'
- en: '`avg`'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`avg`'
- en: '`max`'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`max`'
- en: '`filter`'
  id: totrans-109
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`filter`'
- en: '`agg`'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`agg`'
- en: '`avg`'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`avg`'
- en: '`max`'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`max`'
- en: '`groupBy`'
  id: totrans-113
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`groupBy`'
- en: '`avg`'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`avg`'
- en: '`agg`'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`agg`'
- en: '`max`'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`max`'
- en: '`groupBy`'
  id: totrans-117
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`groupBy`'
- en: '`agg`'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`agg`'
- en: '`avg`'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`avg`'
- en: '`avg`'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`avg`'
- en: '**Question 18**:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 18**:'
- en: 'The following code block contains an error. The code block needs to join the
    `salaryDf` DataFrame with the bigger `employeeDf` DataFrame on the `employeeID`
    column:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块中包含一个错误。代码块需要将`salaryDf` DataFrame与较大的`employeeDf` DataFrame在`employeeID`列上连接：
- en: '[PRE1]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Identify the error:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 识别错误：
- en: Instead of `join`, the code should use `innerJoin`
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码应该使用`innerJoin`而不是`join`
- en: '`broadcast` is not a `join` type in Spark for joining two DataFrames'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`broadcast`不是Spark中用于连接两个DataFrames的`join`类型'
- en: '`salaryDf` and `employeeDf` should be swapped'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf`和`employeeDf`应该交换'
- en: In the `how` parameter, `crossJoin` should be used instead of `broadcast`
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`how`参数中，应该使用`crossJoin`而不是`broadcast`
- en: '**Question 19**:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 19**:'
- en: Which of the following code blocks shuffles the `df` DataFrame to have 20 partitions
    instead of 5 partitions?
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块将`df` DataFrame的shuffle操作从5个分区变为20个分区？
- en: '`df.repartition(5)`'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.repartition(5)`'
- en: '`df.repartition(20)`'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.repartition(20)`'
- en: '`df.coalesce(20)`'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.coalesce(20)`'
- en: '`df.coalesce(5)`'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.coalesce(5)`'
- en: '**Question 20**:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 20**:'
- en: Which of the following operations will trigger evaluation?
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个操作将触发评估？
- en: '`df.filter()`'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.filter()`'
- en: '`df.distinct()`'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.distinct()`'
- en: '`df.intersect()`'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.intersect()`'
- en: '`df.join()`'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.join()`'
- en: '`df.count()`'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.count()`'
- en: '**Question 21**:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 21**:'
- en: Which of the following code blocks returns unique values for the `age` and `name`
    columns in the `df` DataFrame in its respective columns where all values are unique
    in these columns?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回`df` DataFrame中`age`和`name`列的唯一值，并在各自的列中保持所有值唯一？
- en: '`df.select(''age'').join(df.select(''name''),` `col(state)==col(''name''),
    ''inner'').show()`'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select(''age'').join(df.select(''name''), col(state) == col(''name''),
    ''inner'').show()`'
- en: '`df.select(col(''age''),` `col(''name'')).agg({''*'': ''count''}).show()`'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select(col(''age''), col(''name'')).agg({''*'': ''count''}).show()`'
- en: '`df.select(''age'', ''name'').distinct().show()`'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select(''age'', ''name'').distinct().show()`'
- en: '`df.select(''age'').unionAll(df.select(''name'')).distinct().show()`'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select(''age'').unionAll(df.select(''name'')).distinct().show()`'
- en: '**Question 22**:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题22**：'
- en: Which of the following code blocks returns the count of the total number of
    rows in the `df` DataFrame?
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回`df` DataFrame中总行数的计数？
- en: '`df.count()`'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.count()`'
- en: '`df.select(col(''state''),` `col(''department'')).agg({''*'': ''count''}).show()`'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select(col(''state''), col(''department'')).agg({''*'': ''count''}).show()`'
- en: '`df.select(''state'', ''department'').distinct().show()`'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select(''state'', ''department'').distinct().show()`'
- en: '`df.select(''state'').union(df.select(''department'')).distinct().show()`'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select(''state'').union(df.select(''department'')).distinct().show()`'
- en: '**Question 23**:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题23**：'
- en: 'The following code block contains an error. The code block should save the
    `df` DataFrame at the `filePath` path as a new parquet file:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。代码块应该将`df` DataFrame保存为新的parquet文件到`filePath`路径：
- en: '[PRE2]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Identify the error:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 识别错误：
- en: The code block should have `overwrite` instead of `append` as an option
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码块应该有`overwrite`选项而不是`append`
- en: The code should be `write.parquet` instead of `write.mode`
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码应该是`write.parquet`而不是`write.mode`
- en: The `df.write` operation cannot be called directly from the DataFrame
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不能直接从DataFrame中调用`df.write`操作
- en: The first part of the code should be `df.write.mode(append)`
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码的第一部分应该是`df.write.mode(append)`
- en: '**Question 24**:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题24**：'
- en: Which of the following code blocks adds a `salary_squared` column to the `df`
    DataFrame that is the square of the `salary` column?
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块向`df` DataFrame中添加了一个`salary_squared`列，该列是`salary`列的平方？
- en: '`df.withColumnRenamed("salary_squared",` `pow(col("salary"), 2))`'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumnRenamed("salary_squared", pow(col("salary"), 2))`'
- en: '`df.withColumn("salary_squared", col("salary"*2))`'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("salary_squared", col("salary"*2))`'
- en: '`df.withColumn("salary_squared",` `pow(col("salary"), 2))`'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("salary_squared", pow(col("salary"), 2))`'
- en: '`df.withColumn("salary_squared", square(col("salary")))`'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("salary_squared", square(col("salary")))`'
- en: '**Question 25**:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题25**：'
- en: Which of the following code blocks performs a join in which the small `salaryDf`
    DataFrame is sent to all executors so that it can be joined with the `employeeDf`
    DataFrame on the `employeeSalaryID` and `EmployeeID` columns, respectively?
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块执行了一个连接操作，其中小的`salaryDf` DataFrame被发送到所有执行器，以便可以在`employeeSalaryID`和`EmployeeID`列上与`employeeDf`
    DataFrame进行连接？
- en: '`employeeDf.join(salaryDf, "employeeDf.employeeID ==` `salaryDf.employeeSalaryID",
    "inner")`'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`employeeDf.join(salaryDf, "employeeDf.employeeID == salaryDf.employeeSalaryID",
    "inner")`'
- en: '`employeeDf.join(salaryDf, "employeeDf.employeeID ==` `salaryDf.employeeSalaryID",
    "broadcast")`'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`employeeDf.join(salaryDf, "employeeDf.employeeID == salaryDf.employeeSalaryID",
    "broadcast")`'
- en: '`employeeDf.join(broadcast(salaryDf), employeeDf.employeeID ==` `salaryDf.employeeSalaryID)`'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`employeeDf.join(broadcast(salaryDf), employeeDf.employeeID == salaryDf.employeeSalaryID)`'
- en: '`salaryDf.join(broadcast(employeeDf), employeeDf.employeeID ==` `salaryDf.employeeSalaryID)`'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf.join(broadcast(employeeDf), employeeDf.employeeID == salaryDf.employeeSalaryID)`'
- en: '**Question 26**:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题26**：'
- en: Which of the following code blocks performs an outer join between the `salarydf`
    DataFrame and the `employeedf` DataFrame, using the `employeeID` and `salaryEmployeeID`
    columns as join keys respectively?
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块在`salarydf` DataFrame和`employeedf` DataFrame之间执行了外连接，使用`employeeID`和`salaryEmployeeID`列作为连接键分别？
- en: '`Salarydf.join(employeedf, "outer", salarydf.employeedf ==` `employeeID.salaryEmployeeID)`'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Salarydf.join(employeedf, "outer", salarydf.employeedf == employeeID.salaryEmployeeID)`'
- en: '`salarydf.join(employeedf, employeeID ==` `salaryEmployeeID)`'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salarydf.join(employeedf, employeeID == salaryEmployeeID)`'
- en: '`salarydf.join(employeedf, salarydf.salaryEmployeeID ==` `employeedf.employeeID,
    "outer")`'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salarydf.join(employeedf, salarydf.salaryEmployeeID == employeedf.employeeID,
    "outer")`'
- en: '`salarydf.join(employeedf, salarydf.employeeID ==` `employeedf.salaryEmployeeID,
    "outer")`'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salarydf.join(employeedf, salarydf.employeeID == employeedf.salaryEmployeeID,
    "outer")`'
- en: '**Question 27**:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题27**：'
- en: Which of the following pieces of code would print the schema of the `df` DataFrame?
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块会打印出`df` DataFrame的模式？
- en: '`df.rdd.printSchema`'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.rdd.printSchema`'
- en: '`df.rdd.printSchema()`'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.rdd.printSchema()`'
- en: '`df.printSchema`'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.printSchema`'
- en: '`df.printSchema()`'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.printSchema()`'
- en: '**Question 28**:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题28**：'
- en: Which of the following code blocks performs a left join between the `salarydf`
    DataFrame and the `employeedf` DataFrame, using the `employeeID` column?
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块在 `salarydf` DataFrame 和 `employeedf` DataFrame 之间执行左连接，使用 `employeeID`
    列？
- en: '`salaryDf.join(employeeDf, salaryDf["employeeID"] ==` `employeeDf["employeeID"],
    "outer")`'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf.join(employeeDf, salaryDf["employeeID"] ==` `employeeDf["employeeID"],
    "outer")`'
- en: '`salaryDf.join(employeeDf, salaryDf["employeeID"] ==` `employeeDf["employeeID"],
    "left")`'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf.join(employeeDf, salaryDf["employeeID"] ==` `employeeDf["employeeID"],
    "left")`'
- en: '`salaryDf.join(employeeDf, salaryDf["employeeID"] ==` `employeeDf["employeeID"],
    "inner")`'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf.join(employeeDf, salaryDf["employeeID"] ==` `employeeDf["employeeID"],
    "inner")`'
- en: '`salaryDf.join(employeeDf, salaryDf["employeeID"] ==` `employeeDf["employeeID"],
    "right")`'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf.join(employeeDf, salaryDf["employeeID"] ==` `employeeDf["employeeID"],
    "right")`'
- en: '**Question 29**:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题29**:'
- en: Which of the following code blocks aggregates the `bonus` column of the `df`
    DataFrame in ascending order with `nulls` being last?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块按升序聚合了`df` DataFrame中的`bonus`列，并且`nulls`值排在最后？
- en: '`df.agg(asc_nulls_last("bonus").alias("bonus_agg"))`'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.agg(asc_nulls_last("bonus").alias("bonus_agg"))`'
- en: '`df.agg(asc_nulls_first("bonus").alias("bonus_agg"))`'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.agg(asc_nulls_first("bonus").alias("bonus_agg"))`'
- en: '`df.agg(asc_nulls_last("bonus", asc).alias("bonus_agg"))`'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.agg(asc_nulls_last("bonus", asc).alias("bonus_agg"))`'
- en: '`df.agg(asc_nulls_first("bonus", asc).alias("bonus_agg"))`'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.agg(asc_nulls_first("bonus", asc).alias("bonus_agg"))`'
- en: '**Question 30**:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题30**:'
- en: The following code block contains an error. The code block should return a DataFrame
    by joining the `employeeDf` and `salaryDf` DataFrames on the `employeeID` and
    `employeeSalaryID` columns, respectively, excluding the `bonus` and `department`
    columns from the `employeeDf` DataFrame and the `salary` column from the `salaryDf`
    DataFrame in the final DataFrame.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。该代码块应该通过在 `employeeID` 和 `employeeSalaryID` 列上分别连接 `employeeDf`
    和 `salaryDf` DataFrame 来返回一个 DataFrame，同时从最终的 DataFrame 中排除 `employeeDf` DataFrame
    中的 `bonus` 和 `department` 列以及 `salaryDf` DataFrame 中的 `salary` 列。
- en: '[PRE3]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Identify the error:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 识别错误：
- en: '`groupBy` should be replaced with the `innerJoin` operator'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`groupBy` 应该替换为 `innerJoin` 操作符'
- en: '`groupBy` should be replaced with a `join` operator and `delete` should be
    replaced with `drop`'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`groupBy` 应该替换为一个 `join` 操作符，并且 `delete` 应该替换为 `drop`'
- en: '`groupBy` should be replaced with the `crossJoin` operator and `delete` should
    be replaced with `withColumn`'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`groupBy` 应该替换为 `crossJoin` 操作符，并且 `delete` 应该替换为 `withColumn`'
- en: '`groupBy` should be replaced with a `join` operator and `delete` should be
    replaced with `withColumnRenamed`'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`groupBy` 应该替换为一个 `join` 操作符，并且 `delete` 应该替换为 `withColumnRenamed`'
- en: '**Question 31**:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题31**:'
- en: Which of the following code blocks reads a `/loc/example.csv` CSV file as a
    `df` DataFrame?
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块将 `/loc/example.csv` CSV 文件作为 `df` DataFrame 读取？
- en: '`df =` `spark.read.csv("/loc/example.csv")`'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df =` `spark.read.csv("/loc/example.csv")`'
- en: '`df =` `spark.mode("csv").read("/loc/example.csv")`'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df =` `spark.mode("csv").read("/loc/example.csv")`'
- en: '`df =` `spark.read.path("/loc/example.csv")`'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df =` `spark.read.path("/loc/example.csv")`'
- en: '`df =` `spark.read().csv("/loc/example.csv")`'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df =` `spark.read().csv("/loc/example.csv")`'
- en: '**Question 32**:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题32**:'
- en: Which of the following code blocks reads a parquet file at the `my_path` location
    using a schema file named `my_schema`?
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块使用名为 `my_schema` 的模式文件在 `my_path` 位置读取一个 parquet 文件？
- en: '`spark.read.schema(my_schema).format("parquet").load(my_path)`'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spark.read.schema(my_schema).format("parquet").load(my_path)`'
- en: '`spark.read.schema("my_schema").format("parquet").load(my_path)`'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spark.read.schema("my_schema").format("parquet").load(my_path)`'
- en: '`spark.read.schema(my_schema).parquet(my_path)`'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spark.read.schema(my_schema).parquet(my_path)`'
- en: '`spark.read.parquet(my_path).schema(my_schema)`'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spark.read.parquet(my_path).schema(my_schema)`'
- en: '**Question 33**:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题33**:'
- en: We want to find the number of records in the resulting DataFrame when we join
    the `employeedf` and `salarydf` DataFrames on the `employeeID` and `employeeSalaryID`
    columns respectively. Which code blocks should be executed to achieve this?
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要找到在将`employeedf`和`salarydf` DataFrame在`employeeID`和`employeeSalaryID`列上分别连接时，结果DataFrame中的记录数。应该执行哪些代码块来实现这一点？
- en: '`.``filter(~isnull(col(department)))`'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.``filter(~isnull(col(department)))`'
- en: '`.``count()`'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.``count()`'
- en: '`employeedf.join(salarydf, col("employeedf.employeeID")==col("salarydf.employeeSalaryID"))`'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`employeedf.join(salarydf, col("employeedf.employeeID")==col("salarydf.employeeSalaryID"))`'
- en: '`employeedf.join(salarydf, employeedf. employeeID ==salarydf.` `employeeSalaryID,
    how=''inner'')`'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`employeedf.join(salarydf, employeedf. employeeID ==salarydf.` `employeeSalaryID,
    how=''inner'')`'
- en: '`.``filter(col(department).isnotnull())`'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.``filter(col(department).isnotnull())`'
- en: '`.``sum(col(department))`'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.``sum(col(department))`'
- en: 3, 1, 6
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3, 1, 6
- en: 3, 1, 2
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3, 1, 2
- en: 4, 2
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4, 2
- en: 3, 5, 2
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3, 5, 2
- en: '**Question 34**:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题34**:'
- en: Which of the following code blocks returns a copy of the `df` DataFrame where
    the name of the `state` column has been changed to `stateID`?
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回一个 `df` DataFrame 的副本，其中 `state` 列的名称已更改为 `stateID`？
- en: '`df.withColumnRenamed("state", "stateID")`'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumnRenamed("state", "stateID")`'
- en: '`df.withColumnRenamed("stateID", "state")`'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumnRenamed("stateID", "state")`'
- en: '`df.withColumn("state", "stateID")`'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("state", "stateID")`'
- en: '`df.withColumn("stateID", "state")`'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("stateID", "state")`'
- en: '**Question 35**:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 35**:'
- en: Which of the following code blocks returns a copy of the `df` DataFrame where
    the `salary` column has been converted to `integer`?
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回一个 `df` DataFrame 的副本，其中 `salary` 列已转换为 `integer`？
- en: '`df.col("salary").cast("integer"))`'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.col("salary").cast("integer"))`'
- en: '`df.withColumn("salary", col("salary").castType("integer"))`'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("salary", col("salary").castType("integer"))`'
- en: '`df.withColumn("salary", col("salary").convert("integerType()"))`'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("salary", col("salary").convert("integerType()"))`'
- en: '`df.withColumn("salary", col("salary").cast("integer"))`'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("salary", col("salary").cast("integer"))`'
- en: '**Question 36**:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 36**:'
- en: Which of the following code blocks splits a `df` DataFrame in half with the
    exact same values even when the code is run multiple times?
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块将 `df` DataFrame 分成两半，即使代码多次运行，值也完全相同？
- en: '`df.randomSplit([0.5,` `0.5], seed=123)`'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.randomSplit([0.5, 0.5], seed=123)`'
- en: '`df.split([0.5,` `0.5], seed=123)`'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.split([0.5, 0.5], seed=123)`'
- en: '`df.split([0.5, 0.5])`'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.split([0.5, 0.5])`'
- en: '`df.randomSplit([0.5, 0.5])`'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.randomSplit([0.5, 0.5])`'
- en: '**Question 37**:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 37**:'
- en: Which of the following code blocks sorts the `df` DataFrame by two columns,
    `salary` and `department`, where `salary` is in ascending order and `department`
    is in descending order?
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块按两个列，`salary` 和 `department`，排序 `df` DataFrame，其中 `salary` 是升序，`department`
    是降序？
- en: '`df.sort("salary", asc("department"))`'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.sort("salary", asc("department"))`'
- en: '`df.sort("salary", desc(department))`'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.sort("salary", desc(department))`'
- en: '`df.sort(col(salary)).desc(col(department))`'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.sort(col(salary)).desc(col(department))`'
- en: '`df.sort("salary", desc("department"))`'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.sort("salary", desc("department"))`'
- en: '**Question 38**:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 38**:'
- en: Which of the following code blocks calculates the average of the `bonus` column
    from the `salaryDf` DataFrame and adds that in a new column called `average_bonus`?
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块从 `salaryDf` DataFrame 的 `bonus` 列计算平均值，并将其添加到名为 `average_bonus` 的新列中？
- en: '`salaryDf.avg("bonus").alias("average_bonus"))`'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf.avg("bonus").alias("average_bonus"))`'
- en: '`salaryDf.agg(avg("bonus").alias("average_bonus"))`'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf.agg(avg("bonus").alias("average_bonus"))`'
- en: '`salaryDf.agg(sum("bonus").alias("average_bonus"))`'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf.agg(sum("bonus").alias("average_bonus"))`'
- en: '`salaryDf.agg(average("bonus").alias("average_bonus"))`'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf.agg(average("bonus").alias("average_bonus"))`'
- en: '**Question 39**:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 39**:'
- en: Which of the following code blocks saves the `df` DataFrame in the `/FileStore/file.csv`
    location as a CSV file and throws an error if a file already exists in the location?
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块将 `df` DataFrame 保存到 `/FileStore/file.csv` 位置作为 CSV 文件，如果位置中已存在文件则抛出错误？
- en: '`df.write.mode("error").csv("/FileStore/file.csv")`'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.write.mode("error").csv("/FileStore/file.csv")`'
- en: '`df.write.mode.error.csv("/FileStore/file.csv")`'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.write.mode.error.csv("/FileStore/file.csv")`'
- en: '`df.write.mode("exception").csv("/FileStore/file.csv")`'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.write.mode("exception").csv("/FileStore/file.csv")`'
- en: '`df.write.mode("exists").csv("/FileStore/file.csv")`'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.write.mode("exists").csv("/FileStore/file.csv")`'
- en: '**Question 40**:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 40**:'
- en: Which of the following code blocks reads the `my_csv.csv` CSV file located at
    `/my_path/` into a DataFrame?
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块读取位于 `/my_path/` 的 `my_csv.csv` CSV 文件到 DataFrame 中？
- en: '`spark.read().mode("csv").path("/my_path/my_csv.csv")`'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spark.read().mode("csv").path("/my_path/my_csv.csv")`'
- en: '`spark.read.format("csv").path("/my_path/my_csv.csv")`'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spark.read.format("csv").path("/my_path/my_csv.csv")`'
- en: '`spark.read("csv", "/my_path/my_csv.csv")`'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spark.read("csv", "/my_path/my_csv.csv")`'
- en: '`spark.read.csv("/my_path/my_csv.csv")`'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spark.read.csv("/my_path/my_csv.csv")`'
- en: '**Question 41**:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 41**:'
- en: Which of the following code blocks displays the top 100 rows of the `df` DataFrame,
    where the `salary` column is present, in descending order?
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块显示 `df` DataFrame 的前 100 行，其中包含 `salary` 列，按降序排列？
- en: '`df.sort(asc(value)).show(100)`'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.sort(asc(value)).show(100)`'
- en: '`df.sort(col("value")).show(100)`'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.sort(col("value")).show(100)`'
- en: '`df.sort(col("value").desc()).show(100)`'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.sort(col("value").desc()).show(100)`'
- en: '`df.sort(col("value").asc()).print(100)`'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.sort(col("value").asc()).print(100)`'
- en: '**Question 42**:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 42**:'
- en: Which of the following code blocks creates a DataFrame that shows the mean of
    the `salary` column of the `salaryDf` DataFrame based on the `department` and
    `state` columns, where `age` is greater than `35` and the returned DataFrame should
    be sorted in ascending order by the `employeeID` column such that there are no
    nulls in that column?
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块创建了一个 DataFrame，它显示了基于 `department` 和 `state` 列的 `salary` 列的平均值，其中 `age`
    大于 `35`，并且返回的 DataFrame 应该按 `employeeID` 列升序排序，以确保该列没有空值？
- en: '`salaryDf.filter(col("age") >` `35)`'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salaryDf.filter(col("age") >` `35)`'
- en: '`.``filter(col("employeeID")`'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.``filter(col("employeeID")`'
- en: '`.``filter(col("employeeID").isNotNull())`'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.``filter(col("employeeID").isNotNull())`'
- en: '`.``groupBy("department")`'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.``groupBy("department")`'
- en: '`.``groupBy("department", "state")`'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.``groupBy("department", "state")`'
- en: '`.``agg(avg("salary").alias("mean_salary"))`'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.``agg(avg("salary").alias("mean_salary"))`'
- en: '`.``agg(average("salary").alias("mean_salary"))`'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.``agg(average("salary").alias("mean_salary"))`'
- en: '`.``orderBy("employeeID")`'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.``orderBy("employeeID")`'
- en: 1, 2, 5, 6, 8
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1, 2, 5, 6, 8
- en: 1, 3, 5, 6, 8
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1, 3, 5, 6, 8
- en: 1, 3, 6, 7, 8
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1, 3, 6, 7, 8
- en: 1, 2, 4, 6, 8
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1, 2, 4, 6, 8
- en: '**Question 43**:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 43**:'
- en: The following code block contains an error. The code block should return a new
    DataFrame without the `employee` and `salary` columns and with an additional `fixed_value`
    column, which has a value of `100`.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。代码块应该返回一个新的 DataFrame，不包含 `employee` 和 `salary` 列，并添加一个 `fixed_value`
    列，其值为 `100`。
- en: '[PRE4]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Identify the error:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 确定错误：
- en: '`withcolumnRenamed` should be replaced with `withcolumn` and the `lit()` function
    should be used to fill the `100` value'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`withcolumnRenamed` 应该替换为 `withcolumn`，并且应该使用 `lit()` 函数来填充 `100` 的值'
- en: '`withcolumnRenamed` should be replaced with `withcolumn`'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`withcolumnRenamed` 应该替换为 `withcolumn`'
- en: '`employee` and `salary` should be swapped in a `drop` function'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `drop` 函数中应该交换 `employee` 和 `salary`
- en: The `lit()` function call is missing
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`lit()` 函数调用缺失'
- en: '**Question 44**:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 44**:'
- en: Which of the following code blocks returns the basic statistics for numeric
    and string columns of the `df` DataFrame?
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回了 `df` DataFrame 中数值和字符串列的基本统计信息？
- en: '`df.describe()`'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.describe()`'
- en: '`df.detail()`'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.detail()`'
- en: '`df.head()`'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.head()`'
- en: '`df.explain()`'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.explain()`'
- en: '**Question 45**:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 45**:'
- en: Which of the following code blocks returns the top 5 rows of the `df` DataFrame?
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回了 `df` DataFrame 的前 5 行？
- en: '`df.select(5)`'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select(5)`'
- en: '`df.head(5)`'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.head(5)`'
- en: '`df.top(5)`'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.top(5)`'
- en: '`df.show()`'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.show()`'
- en: '**Question 46**:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 46**:'
- en: Which of the following code blocks creates a new DataFrame with the `department`,
    `age`, and `salary` columns from the `df` DataFrame?
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块创建了一个新的 DataFrame，包含来自 `df` DataFrame 的 `department`、`age` 和 `salary`
    列？
- en: '`df.select("department", "``age", "salary")`'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select("department", "``age", "salary")`'
- en: '`df.drop("department", "``age", "salary")`'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.drop("department", "``age", "salary")`'
- en: '`df.filter("department", "``age", "salary")`'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.filter("department", "``age", "salary")`'
- en: '`df.where("department", "``age", "salary")`'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.where("department", "``age", "salary")`'
- en: '**Question 47**:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 47**:'
- en: Which of the following code blocks creates a new DataFrame with three columns,
    `department`, `age`, and `max_salary`, which has the maximum salary for each employee
    from each department and each age group from the `df` DataFrame?
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块创建了一个新的 DataFrame，包含三个列，`department`、`age` 和 `max_salary`，其中每个部门以及每个年龄组的最高工资来自
    `df` DataFrame？
- en: '[PRE5]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Identify the correct answer:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 确定正确答案：
- en: '`filter`'
  id: totrans-322
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: filter
- en: '`agg`'
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: agg
- en: '`max`'
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: max
- en: groupBy
  id: totrans-325
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: groupBy
- en: agg
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: agg
- en: max
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: max
- en: filter
  id: totrans-328
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: filter
- en: agg
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: agg
- en: sum
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: sum
- en: groupBy
  id: totrans-331
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: groupBy
- en: agg
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: agg
- en: sum
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: sum
- en: '**Question 48**:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 48**:'
- en: The following code block contains an error. The code block should return a new
    DataFrame, filtered by the rows, where the `salary` column is greater than or
    equal to `1000` in the `df` DataFrame.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。代码块应该返回一个新的 DataFrame，通过行筛选，其中 `salary` 列在 `df` DataFrame 中大于或等于
    `1000`。
- en: '[PRE6]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Identify the error:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 确定错误：
- en: Instead of `filter()`, `where()` should be used
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该使用 `where()` 而不是 `filter()`
- en: The `F(salary)` operation should be replaced with `F.col("salary")`
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该将 `F(salary)` 操作替换为 `F.col("salary")`
- en: Instead of `>=`, the `>` operator should be used
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该使用 `>` 操作符而不是 `>=`
- en: The argument to the `where` method should be `"salary >` `1000"`
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`where` 方法的参数应该是 `"salary >` `1000"`'
- en: '**Question 49**:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 49**:'
- en: Which of the following code blocks returns a copy of the `df` DataFrame where
    the `department` column has been renamed `business_unit`?
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回了一个 `df` DataFrame 的副本，其中 `department` 列已被重命名为 `business_unit`？
- en: '`df.withColumn(["department", "business_unit"])`'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn(["department", "business_unit"])`'
- en: '`itemsDf.withColumn("department").alias("business_unit")`'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`itemsDf.withColumn("department").alias("business_unit")`'
- en: '`itemsDf.withColumnRenamed("department", "business_unit")`'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`itemsDf.withColumnRenamed("department", "business_unit")`'
- en: '`itemsDf.withColumnRenamed("business_unit", "department")`'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`itemsDf.withColumnRenamed("business_unit", "department")`'
- en: '**Question 50**:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题50**:'
- en: Which of the following code blocks returns a DataFrame with the total count
    of employees in each department from the `df` DataFrame?
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块从`df` DataFrame返回包含每个部门员工总数的数据帧？
- en: '`df.groupBy("department").agg(count("*").alias("total_employees"))`'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.groupBy("department").agg(count("*").alias("total_employees"))`'
- en: '`df.filter("department").agg(count("*").alias("total_employees"))`'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.filter("department").agg(count("*").alias("total_employees"))`'
- en: '`df.groupBy("department").agg(sum("*").alias("total_employees"))`'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.groupBy("department").agg(sum("*").alias("total_employees"))`'
- en: '`df.filter("department").agg(sum("*").alias("total_employees"))`'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.filter("department").agg(sum("*").alias("total_employees"))`'
- en: '**Question 51**:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题51**:'
- en: Which of the following code blocks returns a DataFrame with the `employee` column
    from the `df` DataFrame case to the `string` type?
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块从`df` DataFrame返回将`employee`列转换为字符串类型的DataFrame？
- en: '`df.withColumn("employee", col("employee").cast_type("string"))`'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("employee", col("employee").cast_type("string"))`'
- en: '`df.withColumn("employee", col("employee").cast("string"))`'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("employee", col("employee").cast("string"))`'
- en: '`df.withColumn("employee", col("employee").cast_type("stringType()"))`'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("employee", col("employee").cast_type("stringType()"))`'
- en: '`df.withColumnRenamed("employee", col("employee").cast("string"))`'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumnRenamed("employee", col("employee").cast("string"))`'
- en: '**Question 52**:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题52**:'
- en: Which of the following code blocks returns a DataFrame with a new `fixed_value`
    column, which has `Z` in all rows in the df DataFrame?
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回一个新的DataFrame，其中包含一个新的`fixed_value`列，该列在`df` DataFrame的所有行中都有`Z`？
- en: '`df.withColumn("fixed_value", F.lit("Z"))`'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("fixed_value", F.lit("Z"))`'
- en: '`df.withColumn("fixed_value", F("Z"))`'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn("fixed_value", F("Z"))`'
- en: '`df.withColumnRenamed("fixed_value", F.lit("Z"))`'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumnRenamed("fixed_value", F.lit("Z"))`'
- en: '`df.withColumnRenamed("fixed_value", lit("Z"))`'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumnRenamed("fixed_value", lit("Z"))`'
- en: '**Question 53**:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题53**:'
- en: Which of the following code blocks returns a new DataFrame with a new `upper_string`
    column, which is the capitalized version of the `employeeName` column in the `df`
    DataFrame?
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回一个新的DataFrame，其中包含一个新的`upper_string`列，它是`df` DataFrame中`employeeName`列的大写版本？
- en: '`df.withColumnRenamed(''employeeName'', upper(df.upper_string))`'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumnRenamed(''employeeName'', upper(df.upper_string))`'
- en: '`df.withColumnRenamed(''upper_string'', upper(df.employeeName))`'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumnRenamed(''upper_string'', upper(df.employeeName))`'
- en: '`df.withColumn(''upper_string'', upper(df.employeeName))`'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn(''upper_string'', upper(df.employeeName))`'
- en: '`df.withColumn(''` `employeeName'', upper(df.upper_string))`'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.withColumn(''` `employeeName'', upper(df.upper_string))`'
- en: '**Question 54**:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题54**:'
- en: 'The following code block contains an error. The code block is supposed to capitalize
    the employee names using a udf:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。该代码块本应使用udf将员工姓名转换为大写：
- en: '[PRE7]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Identify the error:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 识别错误：
- en: The `capitalize_udf` function should be called instead of `capitalize`
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该使用`capitalize_udf`函数而不是`capitalize`。
- en: The `udf` function, `capitalize_udf`, is not capitalizing correctly
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`udf`函数`capitalize_udf`没有正确地转换为大写。'
- en: Instead of `StringType()`, `IntegerType()` should be used
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该使用`IntegerType()`而不是`StringType()`。
- en: Instead of `df.withColumn("capitalized_name", capitalize("employee"))`, it should
    use `df.withColumn("employee", capitalize("capitalized_name"))`
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该使用`df.withColumn("employee", capitalize("capitalized_name"))`代替`df.withColumn("capitalized_name",
    capitalize("employee"))`，而不是`df.withColumn("capitalized_name", capitalize("employee"))`。
- en: '**Question 55**:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题55**:'
- en: The following code block contains an error. The code block is supposed to sort
    the `df` DataFrame by salary in ascending order. Then, it should sort based on
    the `bonus` column, putting `nulls` last.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。该代码块本应按薪资升序对`df` DataFrame进行排序。然后，它应该根据`bonus`列进行排序，将`nulls`放在最后。
- en: '[PRE8]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Identify the error:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 识别错误：
- en: The `salary` column should be sorted in descending order and `desc_nulls_last`
    should be used instead of `asc_nulls_first`. Moreover, it should be wrapped in
    a `col()` operator.
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salary`列应该以降序排序，并使用`desc_nulls_last`代替`asc_nulls_first`。此外，它应该被`col()`运算符包裹。'
- en: The `salary` column should be wrapped by the `col()` operator.
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`salary`列应该被`col()`运算符包裹。'
- en: The `bonus` column should be sorted in a descending way, putting nulls last.
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`奖金`列应该以降序排序，并将null值放在最后。'
- en: The `bonus` column should be sorted by `desc_nulls_first()` instead.
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`奖金`列应该按照`desc_nulls_first()`进行排序。'
- en: '**Question 56**:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题56**:'
- en: The following code block contains an error. The code block needs to group the
    `df` DataFrame based on the `department` column and calculate the total salary
    and average salary for each department.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。该代码块需要根据 `department` 列对 `df` DataFrame 进行分组，并计算每个部门的总工资和平均工资。
- en: '[PRE9]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Identify the error:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 识别错误：
- en: The `avg` method should also be called through the `agg` function
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`avg` 方法也应该通过 `agg` 函数调用'
- en: Instead of `filter`, `groupBy` should be used
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该使用 `groupBy` 而不是 `filter`
- en: The `agg` method syntax is incorrect
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`agg` 方法的语法不正确'
- en: Instead of filtering on `department`, the code should filter on `salary`
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该在 `salary` 上进行过滤，而不是在 `department` 上进行过滤
- en: '**Question 57**:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 57**：'
- en: Which code block will write the `df` DataFrame as a parquet file on the `filePath`
    path partitioning it on the `department` column?
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个代码块将 `df` DataFrame 写入到 `filePath` 路径上的 parquet 文件，并按 `department` 列进行分区？
- en: '`df.write.partitionBy("department").parquet(filePath)`'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.write.partitionBy("department").parquet(filePath)`'
- en: '`df.write.partition("department").parquet(filePath)`'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.write.partition("department").parquet(filePath)`'
- en: '`df.write.parquet("department").partition(filePath)`'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.write.parquet("department").partition(filePath)`'
- en: '`df.write.coalesce("department").parquet(filePath)`'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.write.coalesce("department").parquet(filePath)`'
- en: '**Question 58**:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 58**：'
- en: The `df` DataFrame contains columns `[employeeID, salary, department]`. Which
    of the following pieces of code would return the `df` DataFrame with only columns
    `[``employeeID, salary]`?
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '`df` DataFrame 包含列 `[employeeID, salary, department]`。以下哪段代码将返回只包含列 `[employeeID,
    salary]` 的 `df` DataFrame？'
- en: '`df.drop("department")`'
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.drop("department")`'
- en: '`df.select(col(employeeID))`'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select(col(employeeID))`'
- en: '`df.drop("department", "salary")`'
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.drop("department", "salary")`'
- en: '`df.select("employeeID", "department")`'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.select("employeeID", "department")`'
- en: '**Question 59**:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 59**：'
- en: Which of the following code blocks returns a new DataFrame with the same columns
    as the `df` DataFrame, except for the `salary` column?
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码块返回一个新的 DataFrame，其列与 `df` DataFrame 相同，除了 `salary` 列？
- en: '`df.drop(col("salary"))`'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.drop(col("salary"))`'
- en: '`df.delete(salary)`'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.delete(salary)`'
- en: '`df.drop(salary)`'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.drop(salary)`'
- en: '`df.delete("salary")`'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`df.delete("salary")`'
- en: '**Question 60**:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 60**：'
- en: The following code block contains an error. The code block should return the
    `df` DataFrame with `employeeID` renamed as `employeeIdColumn`.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含一个错误。该代码块应该返回将 `employeeID` 重命名为 `employeeIdColumn` 的 `df` DataFrame。
- en: '[PRE10]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Identify the error:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 识别错误：
- en: Instead of `withColumnRenamed`, the `withColumn` method should be used
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代替 `withColumnRenamed`，应该使用 `withColumn` 方法
- en: Instead of `withColumnRenamed`, the `withColumn` method should be used and the
    `"employeeIdColumn"` argument should be swapped with the `"``employeeID"` argument
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该使用 `withColumn` 方法代替 `withColumnRenamed`，并且将 `"employeeIdColumn"` 参数与 `"employeeID"`
    参数交换
- en: The `"employeeIdColumn"` and `"employeeID"` arguments should be swapped
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`"employeeIdColumn"` 和 `"employeeID"` 参数应该交换'
- en: '`withColumnRenamed` is not a method for DataFrames'
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`withColumnRenamed` 不是一个 DataFrame 的方法'
- en: Answers
  id: totrans-422
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 答案
- en: D
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: C
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: A
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: A
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: A
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: A
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: B
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: D
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: C
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: B
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: C
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: C
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: A
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: B
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: A
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: A
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: A
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: B
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: B
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: E
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: E
- en: C
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: A
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: A
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: C
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: C
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: D
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: D
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: B
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: A
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: B
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: A
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: A
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: C
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: A
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: D
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: A
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: D
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: B
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: A
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: D
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: C
  id: totrans-463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: B
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: A
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: A
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: B
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: A
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: B
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: B
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: C
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: A
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: B
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: A
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: C
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
- en: A
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: A
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: B
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B
- en: A
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: A
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: A
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: C
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C
