# 1

# 认证指南和考试的概述

准备任何任务最初涉及彻底理解手头的问题，然后制定一个应对挑战的策略。在这个规划阶段，为应对挑战的每个方面创建一个逐步的方法是有效的方法。这种方法使得可以单独处理较小的任务，有助于系统地通过挑战，而无需感到不知所措。

本章旨在展示通过 Spark 认证考试逐步方法。在本章中，我们将涵盖以下主题：

+   认证考试的概述

+   考试中可能遇到的不同类型的问题

+   本书其余章节的概述

我们首先概述一下认证考试。

# 认证考试的概述

考试由 **60 个问题** 组成。你被给予的时间来尝试这些问题是 **120 分钟**。这给你大约 **每题 2 分钟**。

要通过考试，你需要得到 **70% 的分数**，这意味着你需要在 60 个问题中正确回答 42 个才能通过。

如果你准备充分，这段时间应该足够你回答问题，并在时间结束前进行复习。

接下来，我们将看到这些问题如何在整个考试中分布。

## 问题的分布

考试问题被分为以下几个广泛的类别。以下表格根据不同类别提供了问题的细分：

| **主题** | **考试百分比** | **问题数量** |
| --- | --- | --- |
| Spark 架构：概念理解 | 17% | 10 |
| Spark 架构：应用理解 | 11% | 7 |
| Spark DataFrame API 应用 | 72% | 43 |

表 1.1：考试细分

通过观察这个分布，你可能会想要在考试准备中更加关注 Spark DataFrame API，因为这个部分涵盖了大约 72% 的考试（大约 43 个问题）。如果你能正确回答这些问题，通过考试就会变得更容易。

但这并不意味着你不应该关注 Spark 架构领域。Spark 架构问题难度各异，有时可能会令人困惑。同时，它们允许你轻松得分，因为架构问题通常很简单。

让我们看看一些其他可用的资源，这些资源可以帮助你为这次考试做准备。

## 准备考试的资源

当你开始计划参加认证考试时，你必须做的第一件事是掌握 Spark 概念。这本书将帮助你理解这些概念。一旦你完成了这些，进行模拟考试将很有用。这本书中提供了两个模拟考试供你利用。

此外，Databricks 提供了模拟考试，这对考试准备非常有用。您可以在以下位置找到它：[`files.training.databricks.com/assessments/practice-exams/PracticeExam-DCADAS3-Python.pdf`](https://files.training.databricks.com/assessments/practice-exams/PracticeExam-DCADAS3-Python.pdf).

## 考试期间可用的资源

在考试期间，您将能够访问 Spark 文档。这是通过 **Webassessor** 实现的，其界面与您在互联网上找到的常规 Spark 文档略有不同。熟悉这个界面会很好。您可以在 [`www.webassessor.com/zz/DATABRICKS/Python_v2.html`](https://www.webassessor.com/zz/DATABRICKS/Python_v2.html) 找到该界面。我建议您浏览它，并尝试通过此文档找到 Spark 的不同包和函数，以便在考试期间更舒适地导航。

接下来，我们将查看如何注册考试。

# 注册您的考试

Databricks 是准备这些考试和认证的公司。您可以在此处注册考试：[`www.databricks.com/learn/certification/apache-spark-developer-associate`](https://www.databricks.com/learn/certification/apache-spark-developer-associate).

接下来，我们将查看考试的一些先决条件。

## 考试先决条件

在您参加考试之前，需要一些先决条件，以便您能够成功通过认证。以下是一些主要的先决条件：

+   掌握 Spark 架构的基本原理，包括自适应查询执行的原则。

+   熟练使用 Spark DataFrame API 进行各种数据操作任务，如下所示：

    +   执行列操作，例如选择、重命名和操作

    +   执行行操作，包括过滤、删除、排序和聚合数据

    +   执行与 DataFrame 相关的任务，例如连接、读取、写入和实现分区策略

    +   展示使用 **用户定义函数**（**UDFs**）和 Spark SQL 函数的熟练程度

+   虽然没有明确测试，但预期您对 Python 或 Scala 有功能性的理解。考试可用两种编程语言进行。

希望到这本书的结尾，您能够完全掌握所有这些概念，并且已经足够练习，以便对考试充满信心。

现在，让我们讨论一下在线监考考试期间可以期待什么。

## 在线监考考试

Spark 认证考试是一个在线监考考试。这意味着您将在家中参加考试，但有人将在网上监考。我鼓励您提前了解监考考试的程序和规则。这将为您在考试时节省很多麻烦和焦虑。

为了给你一个概述，在整个考试过程中，以下程序将生效：

+   Webassessor 监考员将进行网络摄像头监控，以确保考试诚信。

+   你需要出示带有照片的有效身份证明。

+   你需要独自进行考试。

+   你的桌子需要整理干净，并且除了你用于考试的笔记本电脑外，房间里不应有其他电子设备。

+   房间的墙上不应有任何可能帮助你考试的海报或图表。

+   监考员在考试期间也会监听你，所以你想要确保你坐在一个安静舒适的环境中。

+   建议不要使用你的工作笔记本电脑参加这次考试，因为它需要安装软件，并且需要禁用你的防病毒软件和防火墙。

监考员的职责如下：

+   监督你的考试过程以保持考试诚信。

+   解决与考试交付过程相关的任何疑问。

+   如有必要，提供技术支持。

+   需要注意的是，监考员不会就考试内容提供任何形式的帮助。

我建议你在考试前有足够的时间设置你将进行考试的环境。这将确保一个顺畅的在线考试流程，让你可以专注于问题，不必担心其他任何事情。

现在，让我们谈谈考试中可能出现的不同类型的题目。

# 题目类型。

考试中你会遇到不同类别的题目。它们可以大致分为理论题和代码题。在本节中，我们将探讨这两个类别及其各自的子类别。

## 理论题。

理论题是那些会要求你对某些主题的概念性理解的问题。理论题可以进一步细分为不同的类别。让我们看看这些类别，以及从之前的考试中选取的属于这些类别的示例问题。

### 解释题。

解释题是需要定义和解释某事的问题。它也可以包括某物是如何工作的以及它做什么。让我们看看一个例子。

以下哪项描述了一个工作节点？

1.  工作节点是集群中执行计算的节点。

1.  工作节点与执行器是同义词。

1.  工作节点总是与执行器保持一对一的关系。

1.  工作节点是 Spark 执行层次结构中最细粒度的执行级别。

1.  工作节点是 Spark 执行层次结构中最粗粒度的执行级别。

### 连接题。

连接题是需要定义不同事物之间是如何相互关联的，或者它们是如何相互区别的问题。让我们通过一个例子来展示这一点。

以下哪项描述了工作节点与执行器之间的关系？

1.  执行器是在工作节点上运行的 Java 虚拟机（JVM）。

1.  工作节点是在执行器上运行的 JVM。

1.  总是存在比执行器更多的工作节点。

1.  总是存在相同数量的执行器和工作节点。

1.  执行器和工作节点之间没有关系。

### 场景问题

场景问题涉及定义在不同 if-else 场景中事物是如何工作的——例如，“如果 ______ 发生，那么 _____ 就会发生。”此外，它还包括关于场景的陈述不正确的问题。让我们通过一个例子来展示这一点。

如果 Spark 以集群模式运行，以下关于节点的说法中哪一个是错误的？

1.  有一个包含 Spark 驱动程序和执行器的工作节点。

1.  Spark 驱动程序在其自己的非工作节点上运行，没有任何执行器。

1.  每个执行器都是工作节点内部运行的 JVM。

1.  总是存在多个节点。

1.  可能会有比总节点更多的执行器，或者比执行器更多的总节点。

### 分类问题

分类问题是这样的问题，你需要描述某个事物所属的类别。让我们通过一个例子来展示这一点。

以下哪个说法准确地描述了阶段？

1.  阶段内的任务可以由多台机器同时执行。

1.  作业中的各个阶段可以并发运行。

1.  阶段包括一个或多个作业。

1.  阶段在提交之前暂时存储事务。

### 配置问题

配置问题是这样的问题，你需要根据不同的集群配置概述事物的行为。让我们通过一个例子来展示这一点。

以下哪个说法准确地描述了 Spark 的集群执行模式？

1.  集群模式在网关节点上运行执行器进程。

1.  集群模式涉及驱动程序托管在网关机器上。

1.  在集群模式下，Spark 驱动程序和集群管理器不在同一位置。

1.  集群模式下的驱动程序位于工作节点上。

接下来，我们将探讨基于代码的问题及其子类别。

## 基于代码的问题

下一个类别是基于代码的问题。大量基于 Spark API 的问题属于这个类别。基于代码的问题是你会得到一个代码片段，然后你会被问及关于它的问题。基于代码的问题可以进一步细分为不同的类别。让我们看看这些类别，以及从之前的考试中选取的属于这些不同子类别的示例问题。

### 函数识别问题

函数识别问题是这样的问题，你需要定义某个事物是由哪个函数执行的。了解 Spark 中用于数据操作的不同函数及其语法非常重要。让我们通过一个例子来展示这一点。

以下哪个代码块返回了 `df` DataFrame 的副本，其中 `column` 的 `salary` 已重命名为 `employeeSalary`？

1.  `df.withColumn(["salary", "employeeSalary"])`

1.  `df.withColumnRenamed("salary").alias("employeeSalary ")`

1.  `df.withColumnRenamed("salary", "` `employeeSalary ")`

1.  `df.withColumn("salary", "` `employeeSalary ")`

### 填空题

填空题是这样的问题，您需要通过填写空白来完成代码块。让我们通过一个示例来演示这一点。

以下代码块应返回一个 DataFrame，其中包含`transactionsDf` DataFrame 中的`employeeId`、`salary`、`bonus`和`department`列。请选择正确填充空白的答案来完成此操作。

```py
df.__1__(__2__)
```

1.  1.  `drop`

    1.  `"employeeId", "salary", "``bonus", "department"`

1.  1.  `filter`

    1.  `"employeeId, salary,` `bonus, department"`

1.  1.  `select`

    1.  `["employeeId", "salary", "``bonus", "department"]`

1.  1.  `select`

    1.  `col(["employeeId", "salary", "``bonus", "department"])`

### 代码行顺序问题

代码行顺序问题是这样的问题，您需要将代码行按特定顺序排列，以便正确执行操作。让我们通过一个示例来演示这一点。

以下哪个代码块创建了一个 DataFrame，该 DataFrame 显示了基于`department`和`state`列的`salary`列的平均值，其中`age`大于 35？

1.  `salaryDf.filter(col("age") >` `35)`

1.  `.``filter(col("employeeID")`

1.  `.``filter(col("employeeID").isNotNull())`

1.  `.``groupBy("department")`

1.  `.``groupBy("department", "state")`

1.  `.``agg(avg("salary").alias("mean_salary"))`

1.  `.``agg(average("salary").alias("mean_salary"))`

1.  i, ii, v, vi

1.  i, iii, v, vi

1.  i, iii, vi, vii

1.  i, ii, iv, vi

# 摘要

本章提供了认证考试的概述。到目前为止，您已经知道考试中可以期待什么，以及如何最好地准备它。为此，我们介绍了您将遇到的不同类型的问题。

今后，本书的每一章都将为您提供实用的知识和动手实践示例，以便您能够利用 Apache Spark 进行各种数据处理和分析任务。

# 第二部分：介绍 Spark

本部分将为您提供对 Spark 功能和操作原理的全面了解。它将涵盖 Spark 是什么，为什么它很重要，以及 Spark 最有用的应用领域。它将介绍可以从 Spark 中受益的不同类型的用户。它还将涵盖 Spark 的基本架构以及如何在 Spark 中导航应用程序。它将详细说明窄和宽 Spark 转换，并讨论 Spark 中的懒加载评估。这种理解很重要，因为 Spark 的工作方式与其他传统框架不同。

本部分包含以下章节：

+   *第二章**，理解 Apache Spark 及其应用*

+   *第三章**，Spark 架构和转换*
