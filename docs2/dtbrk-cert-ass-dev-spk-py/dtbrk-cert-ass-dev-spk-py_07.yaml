- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Structured Streaming in Spark
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark中的结构化流
- en: The world of data processing has evolved rapidly as data volume and data velocity
    increase every day. With that, the need to analyze and derive insights from real-time
    data is becoming increasingly crucial. Structured Streaming, a component of Apache
    Spark, has emerged as a powerful framework to process and analyze data streams
    in real time. This chapter delves into the realm of Structured Streaming, exploring
    its capabilities, features, and real-world applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据量和数据速度的每日增长，数据处理的世界已经迅速发展。因此，从实时数据中分析和提取洞察的需求变得越来越关键。作为Apache Spark的一个组件，结构化流已经出现成为一个强大的框架，用于实时处理和分析数据流。本章深入探讨结构化流的领域，探索其功能、特性和实际应用。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Real-time data processing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时数据处理
- en: The fundamentals of streaming
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式处理的基本原理
- en: Streaming architectures
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式架构
- en: Spark Streaming
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: Structured Streaming
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化流
- en: Streaming sources and sinks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式源和接收器
- en: Advanced topics in Structured Streaming
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化流的进阶主题
- en: Joins in Structured Streaming
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化流中的连接
- en: By the end of this chapter, you will understand Spark Streaming and the power
    of real-time data insights.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解Spark Streaming和实时数据洞察的力量。
- en: We will start by looking at what real-time data processing means.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先探讨实时数据处理的意义。
- en: Real-time data processing
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时数据处理
- en: Real-time data processing has become increasingly critical in today’s fast-paced
    and data-driven world. Organizations need to analyze and derive insights from
    data as it arrives, enabling them to make timely decisions and take immediate
    action. Spark Streaming, a powerful component of Apache Spark, addresses this
    need by providing a scalable and fault-tolerant framework to process real-time
    data streams.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今快节奏和以数据驱动为主的世界中，实时数据处理变得越来越关键。组织需要分析并从到达的数据中提取洞察，使他们能够及时做出决策并采取即时行动。Apache
    Spark的一个强大组件Spark Streaming通过提供一个可扩展和容错的框架来处理实时数据流来满足这一需求。
- en: Real-time data processing has gained immense importance in various industries,
    ranging from finance and e-commerce to the Internet of Things (IoT) and social
    media. Traditional batch processing approaches, while suitable for many scenarios,
    fall short when immediate insights and actions are required. Real-time data processing
    fills this gap by enabling the analysis and processing of data as it arrives,
    allowing organizations to make timely decisions and respond quickly to changing
    conditions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 实时数据处理在各个行业中获得了巨大的重要性，从金融和电子商务到物联网（IoT）和社交媒体。虽然传统的批量处理方法在许多场景中适用，但在需要即时洞察和行动时却力不从心。实时数据处理通过允许在数据到达时进行分析和处理来填补这一空白，使组织能够及时做出决策并迅速应对变化的情况。
- en: Real-time data processing involves the continuous ingestion, processing, and
    analysis of streaming data. Unlike **batch processing**, which operates on static
    datasets, real-time data processing systems handle data that is generated and
    updated in real time. This data can be sourced from various channels, including
    sensors, logs, social media feeds, and financial transactions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 实时数据处理涉及对流数据的持续摄取、处理和分析。与**批量处理**不同，批量处理在静态数据集上运行，实时数据处理系统处理的是实时生成和更新的数据。这些数据可以来自各种渠道，包括传感器、日志、社交媒体流和金融交易。
- en: 'The key characteristics of real-time data processing are as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 实时数据处理的关键特性如下：
- en: '**Low latency**: Real-time data processing aims to minimize the time delay
    between data generation and processing. It requires fast and efficient processing
    capabilities to provide near-instantaneous insights and responses.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低延迟**：实时数据处理旨在最小化数据生成和处理之间的时间延迟。它需要快速高效的处理能力，以提供近乎瞬间的洞察和响应。'
- en: '**Scalability**: Real-time data processing systems must be able to handle high-volume
    and high-velocity data streams. The ability to scale horizontally and distribute
    processing across multiple nodes is essential to accommodate the increasing data
    load.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：实时数据处理系统必须能够处理高容量和高速度的数据流。能够水平扩展并在多个节点上分配处理能力对于适应不断增长的数据负载是至关重要的。'
- en: '**Fault tolerance**: Given the continuous nature of streaming data, real-time
    processing systems need to be resilient to failures. They should have mechanisms
    in place to recover from failures and ensure uninterrupted processing.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错性**：鉴于流数据的连续性，实时处理系统需要能够抵御故障。它们应具备恢复失败的机制，并确保处理不间断。'
- en: '**A streaming data model**: Real-time data processing systems operate on **streaming
    data**, which is an unbounded sequence of events or records. Streaming data models
    are designed to handle the continuous flow of data and provide mechanisms for
    event-time and window-based computations.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流数据模型**：实时数据处理系统在**流数据**上运行，这是一个无界的事件或记录序列。流数据模型旨在处理数据的连续流动，并提供基于事件时间和窗口计算的机制。'
- en: 'These characteristics of real-time data processing lead to several advantages,
    including the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 实时数据处理的特点导致以下几项优势：
- en: '**A rapid response**: Real-time processing enables organizations to respond
    quickly to changing conditions, events, or opportunities. It allows for timely
    actions, such as fraud detection, anomaly detection, real-time monitoring, and
    alerting.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**快速响应**：实时处理使组织能够快速响应变化的情况、事件或机会。它允许及时采取行动，例如欺诈检测、异常检测、实时监控和警报。'
- en: '**Personalization**: Real-time processing enables personalized experiences
    by analyzing and acting on user behavior in real time. It powers real-time recommendations,
    dynamic pricing, targeted advertising, and content personalization.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个性化**：实时处理通过实时分析和采取用户行为，实现个性化体验。它推动了实时推荐、动态定价、定向广告和内容个性化。'
- en: '**Operational efficiency**: Real-time processing provides insights into operational
    processes, allowing organizations to optimize their operations, identify bottlenecks,
    and improve efficiency in real time. It facilitates predictive maintenance, supply
    chain optimization, and real-time resource allocation.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作效率**：实时处理提供了对操作流程的洞察，使组织能够优化其操作，识别瓶颈，并在实时内提高效率。它促进了预测性维护、供应链优化和实时资源分配。'
- en: '**Situational awareness**: Real-time data processing helps organizations gain
    situational awareness by continuously analyzing and aggregating data from various
    sources. It enables real-time analytics, monitoring, and decision making in domains
    such as cybersecurity, financial markets, and emergency response systems.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情境感知**：实时数据处理通过持续分析和汇总来自各种来源的数据，帮助组织获得情境感知。它使网络安全、金融市场和应急响应系统等领域的实时分析、监控和决策成为可能。'
- en: In summary, real-time streaming involves the continuous transmission and processing
    of data, enabling immediate insights and rapid decision-making. It has a wide
    range of applications across different industries and utilizes various technologies
    to facilitate efficient and reliable streaming.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，实时流涉及数据的持续传输和处理，从而实现即时洞察和快速决策。它在不同行业中具有广泛的应用，并利用各种技术来促进高效和可靠的流处理。
- en: In the next section, we will explore the basics of streaming and understand
    how streaming is useful for real-time operations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨流式的基础知识，并了解流式如何对实时操作有益。
- en: What is streaming?
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是流？
- en: Streaming refers to the continuous and real-time processing of data as it is
    generated or received. Unlike batch processing, where data is processed in chunks
    or batches at fixed intervals, streaming enables the processing of data continuously
    and incrementally. It allows applications to ingest, process, and analyze data
    in real time, enabling timely decision-making and immediate responses to events.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 流是指数据的连续和实时处理，这些数据是在生成或接收时产生的。与在固定间隔以块或批量处理数据的批处理不同，流允许连续和增量地处理数据。它允许应用程序实时摄取、处理和分析数据，从而实现及时决策和对事件的即时响应。
- en: Different types of streaming architectures are available to handle streaming
    data. We will look at them next.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种类型的流式架构可供处理流数据。我们将在下一节中探讨它们。
- en: Streaming architectures
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式架构
- en: 'Streaming architectures are designed to handle the continuous and high-velocity
    nature of streaming data. They typically consist of three key components:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 流式架构旨在处理流数据的连续和高速度特性。它们通常由三个关键组件组成：
- en: '**Streaming sources**: These are the origins of the streaming data, such as
    IoT devices, sensors, logs, social media feeds, or messaging systems. Streaming
    sources continuously produce and emit data in real time.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流式处理源**：这些是流数据的来源，例如物联网设备、传感器、日志、社交媒体流或消息系统。流式源持续产生和实时发射数据。'
- en: '**A streaming processing engine**: The streaming processing engine is responsible
    for ingesting, processing, and analyzing streaming data. It provides the necessary
    infrastructure and computational capabilities to handle the continuous and incremental
    nature of streaming data.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流处理引擎**：流处理引擎负责摄取、处理和分析流数据。它提供了处理流数据的连续性和增量性质所必需的基础设施和计算能力。'
- en: '**Streaming sinks**: Streaming sinks are destinations where the processed data
    is stored, visualized, or acted upon. They can be databases, data warehouses,
    dashboards, or external systems that consume the processed data.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流式处理接收器**：流式处理接收器是处理后的数据存储、可视化或采取行动的目的地。它们可以是数据库、数据仓库、仪表板或消费处理数据的系统。'
- en: 'There are various streaming architectures, including the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 存在着各种流处理架构，包括以下几种：
- en: '**Event-driven architecture**: In event-driven architecture, events are generated
    by sources and then captured and processed by the engine, leading to immediate
    reactions and triggering actions or updates in real time. This framework facilitates
    real-time event processing, supports the development of event-driven microservices,
    and contributes to the creation of reactive systems.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件驱动架构**：在事件驱动架构中，事件由源生成，然后由引擎捕获和处理，导致即时反应并触发实时动作或更新。这个框架促进了实时事件处理，支持事件驱动微服务的发展，并有助于创建反应式系统。'
- en: Event-driven architecture’s advantages lie in its ability to provide responsiveness,
    scalability, and flexibility. This allows for prompt reactions to events as they
    unfold, fostering agility in system responses.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 事件驱动架构的优势在于其提供响应性、可扩展性和灵活性。这允许对事件进行快速反应，从而在系统响应中培养敏捷性。
- en: '**Lambda architecture**: The lambda architecture seamlessly integrates batch
    and stream processing to effectively manage both historical and real-time data.
    This involves parallel processing of data streams to enable real-time analysis,
    coupled with offline batch processing for in-depth and comprehensive analytics.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Lambda架构**：Lambda架构无缝集成批处理和流处理，有效地管理历史数据和实时数据。这涉及到数据流的并行处理，以实现实时分析，并辅以离线批处理进行深入和全面的分析。'
- en: This approach is particularly well-suited for applications that require a balance
    of real-time insights and thorough historical analysis. The lambda architecture’s
    strengths lie in its provision of fault tolerance, scalability, and capacity to
    handle substantial data volumes. This is achieved by harnessing the combined power
    of both the batch and stream processing techniques.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种方法特别适合需要平衡实时洞察和深入历史分析的应用。Lambda架构的优势在于其提供容错性、可扩展性和处理大量数据的能力。这是通过利用批处理和流处理技术的综合力量实现的。
- en: '**Unified streaming architecture**: Unified streaming architectures, such as
    Apache Spark’s Structured Streaming, aim to provide a unified API and processing
    model for both batch and stream processing. They simplify the development and
    deployment of real-time applications by abstracting away the complexities of managing
    separate batch and stream processing systems.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统一流处理架构**：统一流处理架构，如Apache Spark的Structured Streaming，旨在为批处理和流处理提供统一的API和处理模型。它们通过抽象管理单独的批处理和流处理系统的复杂性，简化了实时应用程序的开发和部署。'
- en: This architecture abstracts complexities by offering a simplified approach to
    developing and deploying real-time applications. This is ideal for scenarios where
    simplicity and ease of development are crucial, allowing developers to focus more
    on business logic than intricate technicalities.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种架构通过提供一种简化的方法来开发和部署实时应用程序，抽象了复杂性。这对于简单性和易于开发至关重要的场景来说非常理想，允许开发者更多地关注业务逻辑而不是复杂的技术细节。
- en: The advantages are that it simplifies development, reduces operational overhead,
    and ensures consistency across batch and stream processing.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优点在于它简化了开发，减少了运营开销，并确保批处理和流处理的一致性。
- en: These architectures cater to different needs based on the specific requirements
    of a given application. Event-driven is ideal for real-time reactions, lambda
    for a balance between real-time and historical data, and unified streaming for
    a streamlined, unified approach to both batch and stream processing. Each approach
    has its strengths and trade-offs, making them suitable for various scenarios based
    on the specific needs of a system.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些架构根据特定应用的特定需求满足不同的需求。事件驱动适用于实时反应，lambda在实时和历史数据之间取得平衡，统一流处理则提供了一种简化和统一的方法来处理批处理和流处理。每种方法都有其优势和权衡，使它们根据系统的特定需求适用于各种场景。
- en: In the following sections, we will delve into the specifics of Structured Streaming,
    its key concepts, and how it compares to Spark Streaming. We will also explore
    stateless and stateful streaming, streaming sources, and sinks, providing code
    examples and practical illustrations to enhance understanding.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们将深入探讨结构化流的具体内容、其关键概念以及它与Spark Streaming的比较。我们还将探索无状态和有状态流处理、流源和流目的地，提供代码示例和实际说明以增强理解。
- en: Introducing Spark Streaming
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Spark Streaming
- en: As you’ve seen so far, Spark Streaming is a powerful real-time data processing
    framework built on Apache Spark. It extends the capabilities of the Spark engine
    to support high-throughput, fault-tolerant, and scalable stream processing. Spark
    Streaming enables developers to process real-time data streams using the same
    programming model as batch processing, making it easy to transition from batch
    to streaming workloads.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如您迄今为止所看到的，Spark Streaming是基于Apache Spark构建的强大实时数据处理框架。它扩展了Spark引擎的功能，以支持高吞吐量、容错和可扩展的流处理。Spark
    Streaming使开发者能够使用与批处理相同的编程模型来处理实时数据流，从而简化从批处理到流工作负载的过渡。
- en: At its core, Spark Streaming divides the real-time data stream into small batches
    or micro-batches, which are then processed using Spark’s distributed computing
    capabilities. Each micro-batch is treated as a **Resilient Distributed Dataset**
    (**RDD**), Spark’s fundamental abstraction for distributed data processing. This
    approach allows developers to leverage Spark’s extensive ecosystem of libraries,
    such as Spark SQL, MLlib, and GraphX, for real-time analytics and machine learning
    tasks.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在核心上，Spark Streaming将实时数据流划分为小批量或微批量，然后使用Spark的分布式计算能力进行处理。每个微批量被视为**弹性分布式数据集**（**RDD**），这是Spark用于分布式数据处理的根本抽象。这种方法允许开发者利用Spark广泛的生态系统中的库，如Spark
    SQL、MLlib和GraphX，用于实时分析和机器学习任务。
- en: Exploring the architecture of Spark Streaming
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索Spark Streaming的架构
- en: 'Spark Streaming follows a **master-worker architecture**, where the driver
    program serves as the master and the worker nodes process the data. The high-level
    architecture consists of the following components:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming遵循**主从架构**，其中驱动程序作为主节点，工作节点处理数据。高级架构包括以下组件：
- en: '**The driver program**: The driver program runs the main application and manages
    the overall execution of the Spark Streaming application. It divides the data
    stream into batches, schedules tasks on the worker nodes, and coordinates the
    processing.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**驱动程序**: 驱动程序运行主应用程序并管理Spark Streaming应用程序的整体执行。它将数据流划分为批量，在工作节点上调度任务，并协调处理。'
- en: '**Receivers**: Receivers are responsible for connecting to the streaming data
    sources and receiving the data. They run on worker nodes and pull the data from
    sources such as Kafka, Flume, or TCP sockets. The received data is then stored
    in the memory of the worker nodes.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**接收器**: 接收器负责连接到流数据源并接收数据。它们在工作节点上运行，并从Kafka、Flume或TCP套接字等源拉取数据。接收到的数据随后存储在工作节点的内存中。'
- en: '**Discretized Stream (DStream)**: DStream is the basic abstraction in Spark
    Streaming. It represents a continuous stream of data divided into small, discrete
    RDDs. DStream provides a high-level API to perform transformations and actions
    on the streaming data.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**离散流（DStream）**: DStream是Spark Streaming中的基本抽象。它表示被划分为小而离散的RDD的数据连续流。DStream提供了一个高级API来对流数据进行转换和操作。'
- en: '`map`, `filter`, and `reduceByKey`, are applied to each RDD in the DStream.
    Actions, such as `count`, `saveAsTextFiles`, and `foreachRDD`, trigger the execution
    of the streaming computation and produce results.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map`、`filter`和`reduceByKey`应用于DStream中的每个RDD。例如`count`、`saveAsTextFiles`和`foreachRDD`等操作触发流计算的执行并产生结果。'
- en: '**Output operations**: Output operations allow the processed data to be written
    to external systems or storage. Spark Streaming supports various output operations,
    such as writing to files, databases, or sending to dashboards for visualization.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出操作**：输出操作允许将处理后的数据写入外部系统或存储。Spark Streaming支持各种输出操作，例如写入文件、数据库或将数据发送到仪表板进行可视化。'
- en: Key concepts
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键概念
- en: 'To effectively use Spark Streaming, it is important to understand some key
    concepts:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地使用Spark Streaming，理解一些关键概念是很重要的：
- en: '**DStreams**: As mentioned earlier, DStreams represent the continuous stream
    of data in Spark Streaming. They are a sequence of RDDs, where each RDD contains
    data from a specific time interval. DStreams support various transformations and
    actions, enabling complex computations on the stream.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DStreams**：如前所述，DStreams表示Spark Streaming中的连续数据流。它们是一系列RDD，其中每个RDD包含特定时间间隔的数据。DStreams支持各种转换和操作，使得在流上执行复杂计算成为可能。'
- en: '**Window operations**: Window operations allow you to apply transformations
    on a sliding window of data in the stream. It enables computations over a fixed
    window size or based on time durations, enabling tasks such as windowed aggregations
    or time-based joins.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**窗口操作**：窗口操作允许你在数据流中的滑动窗口上应用转换。它支持固定窗口大小或基于时间段的计算，使得窗口聚合或基于时间的连接等任务成为可能。'
- en: '**Stateful operations**: Spark Streaming gives you the ability to maintain
    stateful information across batches. It enables operations that require maintaining
    and updating state, such as cumulative counts.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有状态操作**：Spark Streaming允许你在批次之间维护有状态信息。它使得需要维护和更新状态的操作成为可能，例如累积计数。'
- en: '**Checkpointing**: Checkpointing is a crucial mechanism in Spark Streaming
    to ensure fault tolerance and recovery. It periodically saves the metadata about
    the streaming application, including the configuration, DStream operations, and
    the processed data. It enables the recovery of the application if there are failures.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Checkpointing**：Checkpointing是Spark Streaming中确保容错和恢复的关键机制。它定期保存有关流应用程序的元数据，包括配置、DStream操作和已处理的数据。它使得在出现故障时能够恢复应用程序。'
- en: Advantages
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优势
- en: 'Now, we will see the different advantages of using Spark Streaming in real-time
    operations:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看到在实时操作中使用Spark Streaming的不同优势：
- en: '**A unified processing model**: One of the significant advantages of Spark
    Streaming is its integration with the larger Spark ecosystem. It leverages the
    same programming model as batch processing, allowing users to seamlessly transition
    between batch and real-time processing. This unified processing model simplifies
    development and reduces the learning curve for users familiar with Spark.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统一的处理模型**：Spark Streaming的一个显著优势是其与更大的Spark生态系统的集成。它利用与批量处理相同的编程模型，使用户能够无缝地在批处理和实时处理之间切换。这种统一的处理模型简化了开发，并降低了熟悉Spark的用户的学习曲线。'
- en: '**High-level abstractions**: Spark Streaming provides high-level abstractions
    such as DStreams to represent streaming data. DStreams are designed to handle
    continuous data streams and enable easy integration with existing Spark APIs,
    libraries, and data sources. These abstractions provide a familiar and expressive
    programming interface to process real-time data.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级抽象**：Spark Streaming提供了高级抽象，如DStreams来表示流数据。DStreams旨在处理连续数据流，并允许轻松集成现有的Spark
    API、库和数据源。这些抽象提供了一个熟悉且表达性强的编程接口，用于处理实时数据。'
- en: '**Fault tolerance and scalability**: Spark Streaming offers fault-tolerant
    processing by leveraging Spark’s RDD abstraction. It automatically recovers from
    failures by recomputing lost data, ensuring the processing pipeline remains resilient
    and robust. Additionally, Spark Streaming can scale horizontally by distributing
    a workload across a cluster of machines, allowing it to handle large-scale data
    streams effectively.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错性和可伸缩性**：Spark Streaming通过利用Spark的RDD抽象提供容错处理。它通过重新计算丢失的数据来自动从故障中恢复，确保处理管道保持弹性和健壮。此外，Spark
    Streaming可以通过在机器集群中分配工作负载来实现水平扩展，从而有效地处理大规模数据流。'
- en: '**Windowed computations**: Spark Streaming supports windowed computations,
    which enable time-based analysis over sliding or tumbling windows of data. Window
    operations provide flexibility in performing aggregations, time-series analysis,
    and window-level transformations. This capability is particularly useful when
    analyzing streaming data based on temporal characteristics or patterns.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**窗口计算**：Spark Streaming 支持窗口计算，这允许在滑动或翻滚窗口的数据上执行基于时间的分析。窗口操作提供了在执行聚合、时间序列分析和窗口级转换方面的灵活性。这种能力在根据时间特征或模式分析流式数据时特别有用。'
- en: '**A wide range of data sources**: Spark Streaming seamlessly integrates with
    various data sources, including Kafka, Flume, Hadoop Distributed File System (HDFS),
    and Amazon S3\. This broad range of data sources allows users to ingest data from
    multiple streams and integrate it with existing data pipelines. Spark Streaming
    also supports custom data sources, enabling integration with proprietary or specialized
    streaming platforms.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广泛的数据源**：Spark Streaming 可以无缝集成到各种数据源中，包括 Kafka、Flume、Hadoop 分布式文件系统 (HDFS)
    和 Amazon S3。这一广泛的数据源允许用户从多个流中摄取数据并将其与现有数据管道集成。Spark Streaming 还支持自定义数据源，使得可以与专有或专业化的流式平台集成。'
- en: While Spark Streaming offers powerful real-time data processing capabilities,
    there are certain challenges to consider.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Spark Streaming 提供了强大的实时数据处理能力，但也有一些挑战需要考虑。
- en: Challenges
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 挑战
- en: 'Building streaming architectures for applications comes with its set of challenges,
    such as the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为应用程序构建流式架构带来了一系列挑战，如下所述：
- en: '**End-to-end latency**: Latency is introduced when Spark Streaming processes
    data in micro-batches. The end-to-end latency can vary based on factors such as
    the batch interval, the data source, and the complexity of the computations.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端到端延迟**：当 Spark Streaming 以微批处理方式处理数据时，会引入延迟。端到端延迟可能因批处理间隔、数据源和计算复杂度等因素而变化。'
- en: '**Fault tolerance**: Spark Streaming provides fault tolerance through RDD lineage
    and checkpointing. However, failures in receivers or the driver program can still
    disrupt the stream processing. Handling and recovering from failures is an important
    consideration to ensure the reliability of Spark Streaming applications.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错性**：Spark Streaming 通过 RDD 线性和检查点提供容错性。然而，接收器或驱动程序程序中的故障仍然可能干扰流式处理。处理和恢复故障是确保
    Spark Streaming 应用程序可靠性的重要考虑因素。'
- en: '**Scalability**: Scaling Spark Streaming applications to handle large volumes
    of data and meet high throughput requirements can be a challenge. Proper resource
    allocation, tuning, and cluster management are crucial to achieve scalability.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：将 Spark Streaming 应用程序扩展以处理大量数据并满足高吞吐量要求可能是一个挑战。适当的资源分配、调整和集群管理对于实现可扩展性至关重要。'
- en: '**Data ordering**: Spark Streaming processes data in parallel across multiple
    worker nodes, which can affect the order of events. Ensuring the correctness of
    the order of events becomes important in certain use cases, and developers need
    to consider this when designing their applications.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据排序**：Spark Streaming 在多个工作节点上并行处理数据，这可能会影响事件的顺序。在某些用例中，确保事件顺序的正确性变得非常重要，开发者在设计应用程序时需要考虑这一点。'
- en: In summary, Spark Streaming brings the power of Apache Spark to real-time data
    processing. Its integration with the Spark ecosystem, high-level abstractions,
    fault tolerance, scalability, and support for windowed computations make it a
    compelling choice for processing streaming data. By harnessing the advantages
    of Spark Streaming, organizations can unlock valuable insights and make informed
    decisions in real time.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，Spark Streaming 将 Apache Spark 的强大功能带到了实时数据处理中。它与 Spark 生态系统的集成、高级抽象、容错性、可扩展性和对窗口计算的支撑使其成为处理流式数据的诱人选择。通过利用
    Spark Streaming 的优势，组织可以解锁宝贵的见解并在实时做出明智的决策。
- en: In the next section, we will explore Structured Streaming, a newer and more
    expressive streaming API in Apache Spark that overcomes some of the limitations
    and challenges of Spark Streaming. We will discuss its core concepts, the differences
    from Spark Streaming, and its benefits for real-time data processing.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨 Structured Streaming，这是 Apache Spark 中一个更新、更具有表现力的流式 API，它克服了 Spark
    Streaming 的一些限制和挑战。我们将讨论其核心概念、与 Spark Streaming 的区别以及其在实时数据处理方面的优势。
- en: Introducing Structured Streaming
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入 Structured Streaming
- en: Structured Streaming is a revolutionary addition to Apache Spark that brings
    a new paradigm for real-time data processing. It introduces a high-level API that
    seamlessly integrates batch and streaming processing, providing a unified programming
    model. Structured Streaming treats streaming data as an unbounded table or DataFrame,
    enabling developers to express complex computations using familiar SQL-like queries
    and transformations.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Structured Streaming是Apache Spark的一个革命性补充，为实时数据处理带来了新的范式。它引入了一个高级API，无缝集成批处理和流处理，提供了一个统一的编程模型。Structured
    Streaming将流数据视为一个无界的表或DataFrame，使开发者能够使用熟悉的类似SQL的查询和转换来表示复杂的计算。
- en: Unlike the micro-batch processing model of Spark Streaming, Structured Streaming
    follows a continuous processing model. It processes data incrementally as it arrives,
    providing low-latency and near-real-time results. This shift toward continuous
    processing opens up new possibilities for interactive analytics, dynamic visualizations,
    and real-time decision-making.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与Spark Streaming的微批处理模型不同，Structured Streaming遵循连续处理模型。它随着数据的到来增量处理数据，提供低延迟和接近实时的结果。这种向连续处理的转变为交互式分析、动态可视化和实时决策打开了新的可能性。
- en: Key features and advantages
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键特性和优势
- en: 'Structured Streaming offers several key features and advantages over traditional
    stream processing frameworks:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Structured Streaming在几个关键方面优于传统的流处理框架：
- en: '**An expressive API**: Structured Streaming provides a declarative API that
    allows developers to express complex streaming computations using SQL queries,
    DataFrame operations, and Spark SQL functions. This enables developers with SQL
    or DataFrame expertise to easily transition to real-time data processing.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表达性API**：Structured Streaming提供了一个声明式API，允许开发者使用SQL查询、DataFrame操作和Spark SQL函数来表示复杂的流计算。这使得具有SQL或DataFrame专长的开发者可以轻松过渡到实时数据处理。'
- en: '**Fault tolerance and exactly-once semantics**: Structured Streaming guarantees
    end-to-end fault tolerance and exactly-once semantics by maintaining the necessary
    metadata and state information. It handles failures gracefully and ensures that
    data is processed exactly once, even in the presence of failures or retries.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错性和精确一次语义**：Structured Streaming通过维护必要的元数据和状态信息来保证端到端容错性和精确一次语义。它优雅地处理故障，并确保即使在出现故障或重试的情况下，数据也只被处理一次。'
- en: '**Scalability**: Structured Streaming leverages the scalability of the Spark
    engine, enabling horizontal scaling by adding more worker nodes to the cluster.
    It can handle high-throughput data streams and scale seamlessly as the data volume
    increases.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可伸缩性**：Structured Streaming利用Spark引擎的可伸缩性，通过向集群添加更多工作节点来实现水平扩展。它可以处理高吞吐量的数据流，并在数据量增加时无缝扩展。'
- en: '**Unified batch and streaming**: With Structured Streaming, developers can
    use the same API and programming model for both batch and streaming processing.
    This unification simplifies the development and maintenance of applications, as
    there is no need to manage separate batch and stream processing code bases.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统一批处理和流处理**：使用Structured Streaming，开发者可以为批处理和流处理使用相同的API和编程模型。这种统一简化了应用程序的开发和维护，因为不需要管理单独的批处理和流处理代码库。'
- en: '**Ecosystem integration**: Structured Streaming seamlessly integrates with
    the broader Spark ecosystem, enabling the use of libraries such as Spark SQL,
    MLlib, and GraphX for real-time analytics, machine learning, and graph processing
    on streaming data.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生态系统集成**：Structured Streaming无缝集成到更广泛的Spark生态系统中，使得可以使用Spark SQL、MLlib和GraphX等库进行实时分析、机器学习和流数据的图处理。'
- en: Now, let’s take a look at some of the differences between Structured Streaming
    and Spark Streaming.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看Structured Streaming和Spark Streaming之间的一些区别。
- en: Structured Streaming versus Spark Streaming
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Structured Streaming与Spark Streaming的比较
- en: 'Structured Streaming differs from Spark Streaming in several fundamental ways:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Structured Streaming在几个基本方面与Spark Streaming不同：
- en: '**The processing model**: Spark Streaming processes data in micro-batches,
    where each batch is treated as a discrete RDD. In contrast, Structured Streaming
    processes data incrementally in a continuous manner, treating the stream as an
    unbounded table or DataFrame.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理模型**：Spark Streaming以微批处理的方式处理数据，其中每个批次都被视为一个独立的RDD。相比之下，Structured Streaming以连续的方式增量处理数据，将流视为一个无界的表或DataFrame。'
- en: '**The API and query language**: Spark Streaming primarily offers a low-level
    API based on RDD transformations and actions. Structured Streaming, on the other
    hand, provides a higher-level API with SQL-like queries, DataFrame operations,
    and Spark SQL functions. This makes it easier to express complex computations
    and leverage the power of SQL for real-time analytics.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API 和查询语言**：Spark Streaming 主要提供基于 RDD 转换和操作的底层 API。而 Structured Streaming
    则提供了一种更高级的 API，具有类似 SQL 的查询、DataFrame 操作和 Spark SQL 函数。这使得表达复杂的计算并利用 SQL 的力量进行实时分析变得更加容易。'
- en: '**Fault tolerance**: Both Spark Streaming and Structured Streaming provide
    fault tolerance. However, Structured Streaming’s fault tolerance is achieved by
    maintaining the necessary metadata and state information, whereas Spark Streaming
    relies on RDD lineage and checkpointing for fault recovery.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错性**：Spark Streaming 和 Structured Streaming 都提供容错性。然而，Structured Streaming
    的容错性是通过维护必要的元数据和状态信息来实现的，而 Spark Streaming 则依赖于 RDD 线性和检查点来进行故障恢复。'
- en: '**Data processing guarantees**: Spark Streaming provides at-least-once processing
    guarantees by default, where some duplicates may be processed if there are failures.
    Structured Streaming, on the other hand, provides exactly-once processing semantics,
    ensuring that each event is processed exactly once, even in the presence of failures
    or retries.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据处理保证**：Spark Streaming 默认提供至少一次处理保证，即在出现故障时可能会处理一些重复数据。另一方面，Structured
    Streaming 提供了精确一次处理语义，确保即使在出现故障或重试的情况下，每个事件也只被处理一次。'
- en: Limitations and considerations
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制和注意事项
- en: 'While Structured Streaming offers significant advantages, there are certain
    limitations and considerations to keep in mind:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Structured Streaming 提供了显著的优势，但也有一些限制和注意事项需要考虑：
- en: '**Event time handling**: Proper handling of event time, such as timestamp extraction,
    watermarking, and late data handling, is essential in Structured Streaming. Care
    should be taken to ensure the correct processing and handling of out-of-order
    events.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件时间处理**：在 Structured Streaming 中，正确处理事件时间（如时间戳提取、水印和迟到数据处理）至关重要。应确保正确处理乱序事件。'
- en: '**State management**: Structured Streaming allows you to maintain stateful
    information across batches, which can introduce challenges related to state management
    and scalability. Monitoring memory usage and configuring appropriate state retention
    policies are crucial for optimal performance.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态管理**：Structured Streaming 允许你在批次之间维护状态信息，这可能会引入与状态管理和可扩展性相关的挑战。监控内存使用量和配置适当的状态保留策略对于最佳性能至关重要。'
- en: '**Ecosystem compatibility**: While Structured Streaming integrates well with
    the Spark ecosystem, certain libraries and features might not be fully compatible
    with real-time streaming use cases. It is important to evaluate the compatibility
    of specific libraries and functionalities before using them in a Structured Streaming
    application.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生态系统兼容性**：虽然 Structured Streaming 与 Spark 生态系统集成良好，但某些库和功能可能不完全兼容实时流式处理用例。在使用
    Structured Streaming 应用程序之前，评估特定库和功能之间的兼容性非常重要。'
- en: '**Performance considerations**: Structured Streaming’s continuous processing
    model introduces different performance considerations compared to micro-batch
    processing. Factors such as the event rate, processing time, and resource allocation
    need to be carefully monitored and optimized for efficient real-time data processing.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能考虑**：Structured Streaming 的连续处理模型与微批处理相比引入了不同的性能考虑因素。事件速率、处理时间和资源分配等因素需要仔细监控和优化，以实现高效的实时数据处理。'
- en: In the next section, we will delve deeper into the concepts of stateless and
    stateful streaming, exploring their differences and use cases in the context of
    Structured Streaming.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将深入探讨无状态和有状态流式处理的概念，探讨它们在 Structured Streaming 上下文中的差异和用例。
- en: Streaming fundamentals
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式处理基础
- en: Let’s start by looking at some of the fundamental concepts in streaming that
    will help us get familiarized with different paradigms in streaming.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从查看一些流式处理的基本概念开始，这些概念将帮助我们熟悉流式处理的不同范式。
- en: Stateless streaming – processing one event at a time
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无状态流式处理 – 一次处理一个事件
- en: Stateless streaming refers to the processing of each event in isolation, without
    considering any context or history from previous events. In this approach, each
    event is treated independently, and the processing logic does not rely on any
    accumulated state or information from past events.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态流指的是独立处理每个事件，不考虑任何来自先前事件的上下文或历史。在这种方法中，每个事件都是独立处理的，处理逻辑不依赖于任何累积的状态或来自过去事件的信息。
- en: Stateless streaming is well-suited for scenarios where each event can be processed
    independently, and the output is solely determined by the content of the event
    itself. This approach is often used for simple filtering, transformation, or enrichment
    operations that do not require you to maintain any contextual information across
    events.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态流非常适合每个事件可以独立处理的情况，输出完全由事件本身的内容决定。这种方法通常用于简单的过滤、转换或增强操作，不需要你在事件之间维护任何上下文信息。
- en: Stateful streaming – maintaining stateful information
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 状态流 – 维护状态信息
- en: Stateful streaming involves maintaining and utilizing contextual information
    or state across multiple events during processing. The processing logic considers
    the history of events and uses accumulated information to make decisions or perform
    computations. Stateful streaming enables more sophisticated analysis and complex
    computations that rely on context or accumulated knowledge.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 状态流涉及在处理过程中维护和利用跨多个事件的环境信息或状态。处理逻辑考虑事件的历史，并使用累积的信息做出决策或执行计算。状态流使更复杂的分析和依赖上下文或累积知识的复杂计算成为可能。
- en: Stateful streaming requires you to maintain and update state information as
    new events arrive. The state can be as simple as a running count or more complex,
    involving aggregations, windowed computations, or maintaining session information.
    Proper management of state is essential to ensure correctness, scalability, and
    fault tolerance in stateful streaming applications.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 状态流要求你在新事件到达时维护和更新状态信息。状态可以很简单，如运行计数，也可以更复杂，涉及聚合、窗口计算或维护会话信息。在状态流应用程序中，适当的状态管理对于确保正确性、可扩展性和容错性至关重要。
- en: Let’s understand the differences between stateless and stateful streaming.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解无状态流和状态流之间的区别。
- en: The differences between stateless and stateful streaming
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无状态流和状态流之间的区别
- en: 'The main differences between stateless and stateful streaming can be summarized
    as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态流和状态流之间的主要区别可以总结如下：
- en: Stateless streaming processes events independently, while stateful streaming
    maintains and uses accumulated state information across events.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无状态流独立处理事件，而状态流在事件之间维护和使用累积的状态信息。
- en: Stateless streaming is suitable for simple operations that don’t rely on past
    events, while stateful streaming enables complex computations that require context
    or accumulated knowledge.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无状态流适用于不依赖过去事件的简单操作，而状态流使复杂的计算成为可能，这些计算需要上下文或累积的知识。
- en: Stateless streaming is generally simpler to implement and reason about, while
    stateful streaming introduces additional challenges in managing state, fault tolerance,
    and scalability.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无状态流通常更容易实现和推理，而状态流在管理状态、容错性和可扩展性方面引入了额外的挑战。
- en: Stateless streaming is often used for real-time filtering, transformation, or
    basic aggregations, while stateful streaming is necessary for windowed computations,
    sessionization, and stateful joins.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无状态流通常用于实时过滤、转换或基本聚合，而状态流对于窗口计算、会话化和状态化连接是必要的。
- en: Understanding the distinction between stateless and stateful streaming is crucial
    when designing real-time data processing systems, as it helps determine the appropriate
    processing model and requirements for a given use case.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计实时数据处理系统时，理解无状态流和状态流之间的区别至关重要，因为它有助于确定给定用例的适当处理模型和需求。
- en: Now, let’s take a look at some of the fundamentals of Structured Streaming.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看结构化流的一些基本概念。
- en: Structured Streaming concepts
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化流概念
- en: To understand Structured Streaming, it’s important for us to understand the
    different operations that take place in a near-real-time scenario when data arrives.
    We will understand them in the following section.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解结构化流，了解数据到达时在近实时场景中发生的不同操作对我们来说很重要。我们将在下一节中理解它们。
- en: Event time and processing time
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 事件时间和处理时间
- en: 'In Structured Streaming, there are two important notions of time – event time
    and processing time:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在Structured Streaming中，有两个重要的时间概念——事件时间和处理时间：
- en: '**Event time**: Event time refers to the time when an event occurred or was
    generated. It is typically embedded within the data itself, representing the timestamp
    or a field indicating when the event occurred in the real world. Event time is
    crucial for analyzing data based on its temporal order or performing window-based
    computations.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件时间**: 事件时间指的是事件发生或生成的时刻。它通常嵌入在数据本身中，代表时间戳或一个字段，指示事件在现实世界中发生的时刻。事件时间对于基于时间顺序分析数据或执行基于窗口的计算至关重要。'
- en: '**Processing time**: Processing time, on the other hand, refers to the time
    when an event is processed by the streaming application. It is determined by the
    system clock or the time at which the event is ingested by the processing engine.
    Processing time is useful for tasks that require low latency or an immediate response
    but may not accurately reflect the actual event order.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理时间**: 另一方面，处理时间指的是事件被流应用程序处理的时间。它由系统时钟或事件被处理引擎摄取的时间决定。处理时间对于需要低延迟或即时响应的任务很有用，但可能无法准确反映实际事件顺序。'
- en: Based on these different time concepts, we can determine which one works best
    for a given use case. It’s important to understand the difference between the
    two. Based on that, the strategy for data processing can be determined.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些不同的时间概念，我们可以确定哪个最适合给定的用例。理解两者之间的区别很重要。基于这一点，可以确定数据处理策略。
- en: Watermarking and late data handling
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 水印和迟到数据处理
- en: 'Now, we will discuss how to handle data that doesn’t arrive at the defined
    time in real-time applications. There are different ways to handle that situation.
    Structured Streaming has a built-in mechanism to handle this type of data. These
    mechanisms include:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将讨论如何处理在实时应用程序中未在定义时间到达的数据。有不同方式来处理这种情况。Structured Streaming有一个内置机制来处理这类数据。这些机制包括：
- en: '**Watermarking**: Watermarking is a mechanism in Structured Streaming used
    to deal with event time and handle delayed or late-arriving data. A watermark
    is a threshold timestamp that indicates the maximum event time seen by a system
    up to a certain point. It allows the system to track the progress of event time
    and determine when it is safe to emit results for a specific window.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**水印**: 水印是Structured Streaming中用于处理事件时间和处理延迟或迟到数据的机制。水印是一个阈值时间戳，指示系统在某个时刻之前看到的最大事件时间。它允许系统跟踪事件时间的进度并确定何时可以安全地发出特定窗口的结果。'
- en: '**Late data handling**: Late-arriving data refers to events that have timestamps
    beyond the watermark threshold. Structured Streaming provides options to handle
    late data, such as discarding it, updating existing results, or storing it separately
    for further analysis.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理延迟数据**: 迟到数据指的是时间戳超过水印阈值的事件。Structured Streaming提供了处理迟到数据的选择，例如丢弃它、更新现有结果或将它单独存储以供进一步分析。'
- en: These built-in mechanisms save users a lot of time and efficiently handle late-arriving
    data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这些内置机制为用户节省了大量时间，并有效地处理迟到数据。
- en: Next, we will see, once the data arrives, how we start the operations on it
    in streaming.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到，一旦数据到达，我们将如何开始对其进行流处理操作。
- en: Triggers and output modes
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 触发器和输出模式
- en: 'Triggers determine when a streaming application should emit results or trigger
    the execution of the computation. Structured Streaming supports different types
    of triggers:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 触发器决定了流应用程序何时应发出结果或触发计算的执行。Structured Streaming支持不同类型的触发器：
- en: '**Event time triggers**: Event time triggers operate based on the arrival of
    new events or when a watermark advances beyond a certain threshold. They enable
    more accurate and efficient processing, based on event time semantics.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件时间触发器**: 事件时间触发器基于新事件的到达或水印超过某个阈值时进行操作。它们基于事件时间语义，实现更准确和高效的处理。'
- en: '**Processing time triggers**: These triggers operate based on processing time,
    allowing you to specify time intervals or durations at which the computation should
    be executed.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理时间触发器**: 这些触发器基于处理时间进行操作，允许您指定计算应执行的时间间隔或持续时间。'
- en: 'Structured Streaming also offers different output modes. The output modes determine
    how data is updated in the sink. A sink is where we would write the output after
    the streaming operation:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Structured Streaming还提供了不同的输出模式。输出模式决定了数据在接收器中的更新方式。接收器是我们会在流操作后写入输出的地方：
- en: '**Complete mode**: In this mode, the entire updated result, including all the
    rows in the output, is written to the sink. This mode provides the most comprehensive
    view of data but can be memory-intensive for large result sets.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整模式**：在此模式下，整个更新后的结果，包括输出中的所有行，都被写入到接收器。这种模式提供了最全面的数据视图，但对于大型结果集来说可能非常消耗内存。'
- en: '**Append mode**: In append mode, only the new rows appended to the result table
    since the last trigger are written to the sink. This mode is suitable for cases
    where the result is an append-only stream.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**追加模式**：在追加模式下，只有自上次触发器以来添加到结果表中的新行被写入到接收器。这种模式适用于结果是一个只追加流的情况。'
- en: '**Update mode**: Update mode only writes the changed rows to the sink, preserving
    the existing rows that haven’t changed since the last trigger. This mode is useful
    for cases where the result table is updated incrementally.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新模式**：更新模式只将更改的行写入到接收器，保留自上次触发器以来未更改的现有行。这种模式适用于结果表是增量更新的情况。'
- en: Now, let’s take a look at the different types of aggregate operations we can
    do on streaming data.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看在流数据上可以执行的不同类型的聚合操作。
- en: Windowing operations
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 窗口操作
- en: Windowing operations in Structured Streaming allow you to group and aggregate
    data over specific time windows. Windows for these operations can be defined based
    on either event time or processing time, and they provide a way to perform computations
    over a subset of events within a given time range.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Structured Streaming 中的窗口操作允许您在特定的时间窗口内对数据进行分组和聚合。这些操作的窗口可以根据事件时间或处理时间定义，并提供了一种在给定时间范围内对事件子集进行计算的方法。
- en: 'The common types of windowing operations include the following:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的窗口操作类型包括以下几种：
- en: '**Tumbling windows**: Tumbling windows divide a stream into non-overlapping
    fixed-size windows. Each event falls into exactly one window, and computations
    are performed independently for each window.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滚动窗口**：滚动窗口将流划分为非重叠的固定大小窗口。每个事件恰好属于一个窗口，并且每个窗口的计算是独立进行的。'
- en: '**Sliding windows**: Sliding windows create overlapping windows that slide
    or move over a stream at regular intervals. Each event can contribute to multiple
    windows, and computations can be performed on the overlapping parts.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滑动窗口**：滑动窗口创建重叠窗口，这些窗口以固定的时间间隔在流上滑动或移动。每个事件可以贡献给多个窗口，并且可以在重叠部分进行计算。'
- en: '**Session windows**: Session windows group events that are close in time or
    belong to the same session, based on a specified session timeout. A session is
    defined as a series of events within a certain time threshold of each other.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**会话窗口**：会话窗口根据指定的会话超时将时间上接近或属于同一会话的事件分组。会话被定义为一系列彼此之间在一定时间阈值内的事件。'
- en: The next operation that we frequently use in streaming is the join operation.
    Now, we will see how we can use joins with streaming data.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在流处理中，我们经常使用的下一个操作是连接操作。现在，我们将看看如何使用连接操作处理流数据。
- en: Joins and aggregations
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接和聚合
- en: 'Structured Streaming supports joins and aggregations on streaming data, enabling
    complex analytics and data transformations:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Structured Streaming 支持在流数据上执行连接和聚合，从而实现复杂的分析和数据转换：
- en: '**Joins**: Streaming joins allow you to combine two or more streams or a stream
    with static/reference data, based on a common key or condition. The join operation
    can be performed using event time or processing time, and it supports different
    join types such as inner join, outer join, and left/right join.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连接**：流连接允许您根据公共键或条件结合两个或多个流或一个流与静态/参考数据。连接操作可以使用事件时间或处理时间执行，并支持不同的连接类型，如内连接、外连接和左/右连接。'
- en: '`count`, `sum`, `average`, `min`, and `max`.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`count`, `sum`, `average`, `min`, 和 `max`.'
- en: Structured Streaming’s flexible and expressive API for handling event time,
    triggers, output modes, windowing operations, joins, and aggregations allows developers
    to perform comprehensive real-time analytics and computations on streaming data.
    By understanding these concepts, developers can build sophisticated streaming
    applications with ease and precision.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Structured Streaming 提供了灵活且表达丰富的 API 来处理事件时间、触发器、输出模式、窗口操作、连接和聚合，这使得开发者能够对流数据进行全面的实时分析和计算。通过理解这些概念，开发者可以轻松且精确地构建复杂的流应用程序。
- en: In the next section, we will explore how to read and write data with streaming
    sources and sinks.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何使用流数据源和接收器读取和写入数据。
- en: Streaming sources and sinks
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流数据源和接收器
- en: Streaming sources and sinks are essential components in a streaming system that
    enable the ingestion of data from external systems and the output of processed
    data to external destinations. They form the connectors between the streaming
    application and the data sources or sinks.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 流式源和汇是流式系统中不可或缺的组件，它们使得从外部系统摄取数据并将处理后的数据输出到外部目标成为可能。它们构成了流式应用程序与数据源或汇之间的连接器。
- en: Streaming sources retrieve data from various input systems, such as message
    queues, filesystems, databases, or external APIs, and make it available for processing
    in a streaming application. On the other hand, streaming sinks receive processed
    data from the application and write it to external storage, databases, filesystems,
    or other systems for further analysis or consumption.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 流式源从各种输入系统中检索数据，如消息队列、文件系统、数据库或外部API，并将数据提供给流式应用程序进行处理。另一方面，流式汇接收应用程序处理后的数据并将其写入外部存储、数据库、文件系统或其他系统以进行进一步分析或消费。
- en: There are different types of streaming sources and sinks. We will explore some
    of them next.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 存在着不同类型的流式源和汇。接下来我们将探索其中的一些。
- en: Built-in streaming sources
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内置流式源
- en: 'Structured Streaming provides built-in support for a variety of streaming sources,
    making it easy to integrate with popular data systems. Some of the commonly used
    built-in streaming sources include the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流为各种流式源提供了内置支持，使得与流行的数据系统集成变得容易。一些常用的内置流式源包括以下内容：
- en: '**The file source**: The file source allows you to read data from files in
    a directory or a file stream from a file-based system, such as HDFS or Amazon
    S3.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文件源**：文件源允许您从目录中的文件或基于文件系统的文件流（如HDFS或Amazon S3）中读取数据。'
- en: '**KafkaSource**: KafkaSource enables the consumption of data from Apache Kafka,
    a distributed streaming platform. It provides fault-tolerant, scalable, and high-throughput
    ingestion of data streams.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KafkaSource**：KafkaSource允许从Apache Kafka（一个分布式流平台）中消费数据。它提供了容错、可伸缩和高吞吐量的数据流摄取。'
- en: '**The socket source**: The socket source allows a streaming application to
    read data from a **Transmission Control Protocol** (**TCP**) socket. It is useful
    for scenarios where data is sent through network connections, such as log streaming
    or data sent by external systems.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**套接字源**：套接字源允许流式应用程序从**传输控制协议**（**TCP**）套接字读取数据。这对于数据通过网络连接发送的场景很有用，例如日志流或外部系统发送的数据。'
- en: '**The Structured Streaming source**: The Structured Streaming source allows
    developers to define their own streaming sources by extending the built-in source
    interfaces. It provides the flexibility to integrate with custom or proprietary
    data sources.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构化流源**：结构化流源允许开发者通过扩展内置源接口来定义自己的流式源。它提供了与自定义或专有数据源集成的灵活性。'
- en: Custom streaming sources
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义流式源
- en: In addition to the built-in streaming sources, Structured Streaming allows developers
    to create custom streaming sources to ingest data from any system that can be
    accessed programmatically. Custom streaming sources can be implemented by extending
    the `Source` interface provided by the Structured Streaming API.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 除了内置的流式源之外，结构化流允许开发者创建自定义流式源，以从任何可以通过编程访问的系统摄取数据。自定义流式源可以通过扩展结构化流API提供的`Source`接口来实现。
- en: When implementing a custom streaming source, developers need to consider aspects
    such as data ingestion, event-time management, fault tolerance, and scalability.
    They must define how data is fetched, how it is partitioned and distributed among
    workers, and how to handle late-arriving data and schema evolution.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现自定义流式源时，开发者需要考虑数据摄取、事件时间管理、容错性和可伸缩性等方面。他们必须定义如何获取数据，如何将其分区和分配给工作节点，以及如何处理迟到数据和模式演变。
- en: Similar to different streaming sources, we have streaming sinks as well. Let’s
    explore them next.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 与不同的流式源类似，我们也有流式汇。接下来让我们来探索它们。
- en: Built-in streaming sinks
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内置流式汇
- en: 'Structured Streaming provides built-in support for various streaming sinks,
    enabling the output of processed data to different systems. Some of the commonly
    used built-in streaming sinks include the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流为各种流式汇提供了内置支持，使得将处理后的数据输出到不同的系统变得容易。一些常用的内置流式汇包括以下内容：
- en: '**The console sink**: The console sink writes the output data to the console
    or standard output. It is useful for debugging and quick prototyping but not suitable
    for production use.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制台源**: 控制台源将输出数据写入控制台或标准输出。它对于调试和快速原型设计很有用，但不适合生产使用。'
- en: '**The file sink**: The file sink writes the output data to files in a directory
    or a file-based system such as HDFS or Amazon S3\. It allows data to be stored
    and consumed later for batch processing or archival purposes.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文件源**: 文件源将输出数据写入目录或基于文件的系统（如 HDFS 或 Amazon S3）。它允许数据被存储并在以后用于批量处理或归档目的。'
- en: '**The Kafka sink**: The Kafka sink enables you to write data to Apache Kafka
    topics. It provides fault-tolerant, scalable, and high-throughput output to Kafka
    for consumption by other systems.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kafka 源**: Kafka 源允许您将数据写入 Apache Kafka 主题。它为 Kafka 提供了容错、可扩展和高吞吐量的输出，以便其他系统进行消费。'
- en: '`ForeachWriter` interface. It provides the flexibility to write data to external
    systems or perform custom operations on the output data.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ForeachWriter` 接口。它提供了将数据写入外部系统或对输出数据进行自定义操作的灵活性。'
- en: Custom streaming sinks
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义流式源
- en: Similar to custom streaming sources, developers can implement custom streaming
    sinks in Structured Streaming by extending the `Sink` interface. There are instances
    when you would need to write the data back to a system that might not support
    streaming. It could be a database or a file-based storage system. Custom streaming
    sinks enable integration with external systems or databases that are not supported
    by the built-in sinks.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 与自定义流式源类似，开发者可以通过扩展 `Sink` 接口在结构化流中实现自定义流式源。在某些情况下，您可能需要将数据写回到可能不支持流式写入的系统。这可能是一个数据库或基于文件的存储系统。自定义流式源允许与内置源不支持的外部系统或数据库集成。
- en: When implementing a custom streaming sink, developers need to define how output
    data is written or processed by the external system. This may involve establishing
    connections, handling batching or buffering, and ensuring fault tolerance and
    exactly-once semantics.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当实现自定义流式源时，开发者需要定义外部系统如何写入或处理输出数据。这可能涉及建立连接、处理批处理或缓冲，并确保容错和一次精确语义。
- en: In the next section, we will talk about advanced techniques in Structured Streaming.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论结构化流的高级技术。
- en: Advanced techniques in Structured Streaming
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化流的高级技术
- en: There are certain built-in capabilities of Structured Streaming that makes it
    the default choice for even some batch operations. Instead of architecting things
    yourself, Structured Streaming handles these properties for you. Some of them
    are as follows.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流具有某些内置功能，使其成为某些批量操作的默认选择。您不必自己设计架构，结构化流会为您处理这些属性。以下是一些例子。
- en: Handling fault tolerance
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理容错
- en: 'Fault tolerance is crucial in streaming systems to ensure data integrity and
    reliability. Structured Streaming provides built-in fault tolerance mechanisms
    to handle failures in both streaming sources and sinks:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在流式系统中，容错至关重要，以确保数据完整性和可靠性。结构化流提供了内置的容错机制来处理流式源和源端的故障：
- en: '**Source fault tolerance**: Structured Streaming ensures end-to-end fault tolerance
    in sources, by tracking the progress of event time using watermarks and checkpointing
    the metadata related to the stream. If there are failures, the system can recover
    and resume processing from the last consistent state.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**源容错**: 结构化流通过使用水印跟踪事件时间进度并检查点与流相关的元数据，确保源端端到端的容错。如果出现故障，系统可以恢复并从最后一个一致状态继续处理。'
- en: '**Sink fault tolerance**: Fault tolerance in sinks depends on the guarantees
    provided by the specific sink implementation. Some sinks may inherently provide
    exactly-once semantics, while others may rely on idempotent writes or deduplication
    techniques to achieve at-least-once semantics. Sink implementations should be
    carefully chosen to ensure data consistency and reliability.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**源容错**: 源的容错依赖于特定源实现提供的保证。某些源可能天生提供一次精确语义，而其他源可能依赖于幂等写入或去重技术来实现至少一次语义。源实现应仔细选择，以确保数据一致性和可靠性。'
- en: Developers should consider the fault tolerance characteristics of the streaming
    sources and sinks they use and configure appropriate checkpointing intervals,
    retention policies, and recovery mechanisms to ensure the reliability of their
    streaming applications.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者应该考虑他们使用的流源和流的容错特性，并配置适当的检查点间隔、保留策略和恢复机制，以确保其流应用的可靠性。
- en: Structured Streaming has built-in support for schema evolution as well. Let’s
    explore that in the next section.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Structured Streaming 同样内置了对模式演化的支持。让我们在下一节中探讨这一点。
- en: Handling schema evolution
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理模式演化
- en: Structured Streaming provides support for handling schema evolution in streaming
    data sources. Schema evolution refers to changes in the structure or schema of
    incoming data over time.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Structured Streaming 为处理流数据源中的模式演化提供了支持。模式演化指的是随时间变化的数据结构或模式的变化。
- en: Structured Streaming can handle schema evolution by applying the concept of
    schema inference or schema merging. When reading from streaming sources, the initial
    schema is inferred from the incoming data. As the data evolves, subsequent DataFrames
    are merged with the initial schema, accommodating any new or changed fields.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Structured Streaming 可以通过应用模式推断或模式合并的概念来处理模式演化。在从流源读取时，初始模式从传入的数据中推断出来。随着数据的发展，后续的
    DataFrame 将与初始模式合并，以适应任何新或更改的字段。
- en: 'The following code snippet demonstrates handling schema evolution in Structured
    Streaming:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了在 Structured Streaming 中处理模式演化的方法：
- en: '[PRE0]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, the initial schema is provided explicitly using the `schema`
    method. As new data arrives with additional fields, such as `new_col`, it can
    be selected and merged into the stream using the `selectExpr` method.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，初始模式通过 `schema` 方法显式提供。当新数据到达并带有额外的字段，如 `new_col` 时，可以使用 `selectExpr`
    方法选择并将其合并到流中。
- en: Handling schema evolution is crucial to ensure compatibility and flexibility
    in streaming applications where the data schema may change or evolve over time.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 处理模式演化对于确保流应用中的兼容性和灵活性至关重要，在这些应用中，数据模式可能会随时间变化或演化。
- en: Different joins in Structured Streaming
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Structured Streaming 中的不同连接方式
- en: One of the key features of Structured Streaming is its ability to join different
    types of data streams together in one sink.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Structured Streaming 的一个关键特性是它能够在一个汇集中将不同类型的数据流连接在一起。
- en: Stream-stream joins
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流-流连接
- en: Stream-stream joins, also known as **stream-stream co-grouping** or **stream-stream
    correlation**, involve joining two or more streaming data sources based on a common
    key or condition. In this type of join, each incoming event from the streams is
    matched with events from other streams that share the same key or satisfy the
    specified condition.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 流-流连接，也称为 **流-流组** 或 **流-流关联**，涉及根据公共键或条件将两个或多个流数据源连接起来。在这种类型的连接中，每个来自流的传入事件都与具有相同键或满足指定条件的其他流的事件相匹配。
- en: Stream-stream joins enable real-time data correlation and enrichment, making
    it possible to combine multiple streams of data to gain deeper insights and perform
    complex analytics. However, stream-stream joins present unique challenges compared
    to batch or stream-static joins, due to the unbounded nature of streaming data
    and potential event-time skew.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 流-流连接允许实时数据关联和丰富，使得能够将多个数据流合并以获得更深入的见解和执行复杂分析。然而，与批处理或流-静态连接相比，流-流连接由于流数据的无界性和潜在的事件时间偏移，带来了独特的挑战。
- en: One common approach to stream-stream joins is the use of windowing operations.
    By defining overlapping or tumbling windows on the streams, events within the
    same window can be joined based on their keys. Careful consideration of window
    size, watermarking, and event time characteristics is necessary to ensure accurate
    and meaningful joins.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 流-流连接的一种常见方法是使用窗口操作。通过在流上定义重叠或滚动窗口，可以基于它们的键将同一窗口内的事件连接起来。为了确保准确和有意义的连接，需要对窗口大小、水印和事件时间特性进行仔细考虑。
- en: 'Here’s an example of a stream-stream join using Structured Streaming:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个使用 Structured Streaming 进行流-流连接的示例：
- en: '[PRE1]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this example, two Kafka streams, `stream1` and `stream2`, are read from different
    topics. The `join` method is then applied to perform the join operation, based
    on the `common_key` field shared by both streams.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，从不同的主题读取了两个 Kafka 流，`stream1` 和 `stream2`。然后应用 `join` 方法，基于两个流共有的 `common_key`
    字段执行连接操作。
- en: Stream-static joins
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流-静态连接
- en: Stream-static joins, also known as **stream-batch joins**, involve joining a
    streaming data source to a static or reference dataset. The static dataset typically
    represents reference data, such as configuration data or dimension tables, that
    remains constant over time.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 流式静态连接，也称为**流式批量连接**，涉及将流数据源与静态或参考数据集连接起来。静态数据集通常表示随时间保持恒定的参考数据，例如配置数据或维度表。
- en: Stream-static joins are useful for enriching streaming data with additional
    information or attributes from the static dataset. For example, you might join
    a stream of user activity events with a static user profile table to enrich each
    event with user-related details.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 流式静态连接用于使用来自静态数据集的附加信息或属性丰富流数据。例如，您可以将用户活动事件流与静态用户配置文件表连接起来，以丰富每个事件的用户相关细节。
- en: To perform a stream-static join in Structured Streaming, you can load the static
    dataset as a static DataFrame and then use the join method to perform the join
    with the streaming DataFrame. Since the static dataset does not change, the join
    operation can be performed using the default “right outer join” mode.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Structured Streaming 中执行流式静态连接时，您可以加载静态数据集作为静态 DataFrame，然后使用连接方法与流 DataFrame
    执行连接操作。由于静态数据集不会改变，连接操作可以使用默认的“右外连接”模式执行。
- en: 'Here’s an example of a stream-static join in Structured Streaming:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个 Structured Streaming 中流式静态连接的示例：
- en: '[PRE2]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this example, the streaming data is read from a Kafka source, and the static
    dataset is loaded from a CSV file. The join method is then used to perform the
    stream-static join based on the “`common_key`” field.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，流数据是从 Kafka 源读取的，静态数据集是从 CSV 文件加载的。然后使用连接方法根据“`common_key`”字段执行流式静态连接。
- en: Both stream-stream and stream-static joins provide powerful capabilities for
    real-time data analysis and enrichment. When using these join operations, it is
    essential to carefully manage event time characteristics, windowing options, and
    data consistency to ensure accurate and reliable results. Additionally, performance
    considerations should be considered to handle large volumes of data and meet low-latency
    requirements in real-time streaming applications.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 流式流连接和流式静态连接都为实时数据分析和数据增强提供了强大的功能。当使用这些连接操作时，必须仔细管理事件时间特性、窗口选项和数据一致性，以确保准确可靠的结果。此外，还应考虑性能因素，以处理大量数据并满足实时流应用中的低延迟要求。
- en: Final thoughts and future developments
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后的想法和未来的发展
- en: Structured Streaming has emerged as a powerful framework for real-time data
    processing in Apache Spark. Its unified programming model, fault tolerance, and
    seamless integration with the Spark ecosystem make it an attractive choice for
    building scalable and robust streaming applications.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Structured Streaming 已经成为 Apache Spark 中实时数据处理的一个强大框架。其统一的编程模型、容错性和与 Spark 生态系统的无缝集成使其成为构建可扩展和健壮流式应用的理想选择。
- en: 'As Structured Streaming continues to evolve, there are several areas that hold
    promise for future developments. These include the following:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Structured Streaming 的不断发展，有几个领域有望在未来得到发展。以下是一些：
- en: '**Enhanced support for streaming sources and sinks**: Providing more built-in
    connectors for popular streaming systems and databases, as well as improving the
    integration and compatibility with custom sources and sinks.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强对流式源和目的地的支持**：提供更多内置连接器以支持流行的流式系统和数据库，以及改进与自定义源和目的地的集成和兼容性。'
- en: '**Advanced event time handling**: Introducing more advanced features for event
    time handling, including support for event-time skew detection and handling, event
    deduplication, and watermark optimizations.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级事件时间处理**：引入更多高级事件时间处理功能，包括支持事件时间偏斜检测和处理、事件去重和水印优化。'
- en: '**Performance optimization**: Continuously improving the performance of Structured
    Streaming, especially in scenarios with high data volumes and complex computations.
    This could involve optimizations in memory management, query planning, and query
    optimization techniques.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能优化**：持续改进 Structured Streaming 的性能，特别是在数据量高和计算复杂的场景中。这可能包括内存管理、查询计划和查询优化技术的优化。'
- en: '**Integration with AI and machine learning**: Further integrating Structured
    Streaming with AI and machine learning libraries in Spark, such as MLlib and TensorFlow,
    to enable real-time machine learning and predictive analytics on streaming data.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与AI和机器学习的集成**：进一步将结构化流与Spark中的AI和机器学习库（如MLlib和TensorFlow）集成，以实现流式数据的实时机器学习和预测分析。'
- en: '**Seamless integration with streaming data warehouses**: Providing better integration
    with streaming data warehouses or data lakes, such as Apache Iceberg or Delta
    Lake, to enable scalable and efficient storage and the querying of streaming data.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与流式数据仓库的无缝集成**：提供与流式数据仓库或数据湖（如Apache Iceberg或Delta Lake）更好的集成，以实现可扩展和高效的存储以及流式数据的查询。'
- en: In conclusion, Structured Streaming offers a modern and expressive approach
    to real-time data processing in Apache Spark. Its ease of use, scalability, fault
    tolerance, and integration with the Spark ecosystem make it a valuable tool for
    building robust and scalable streaming applications. By leveraging the concepts
    and techniques covered in this chapter, developers can unlock the full potential
    of real-time data processing with Structured Streaming.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，结构化流（Structured Streaming）为Apache Spark中的实时数据处理提供了一种现代且表达力强的方法。其易用性、可扩展性、容错性和与Spark生态系统的集成使其成为构建健壮和可扩展流式应用的有价值工具。通过利用本章涵盖的概念和技术，开发者可以充分发挥结构化流实时数据处理的潜力。
- en: Summary
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Throughout this chapter, we have explored the fundamental concepts and advanced
    techniques in Structured Streaming.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了结构化流的基本概念和高级技术。
- en: We started by understanding the fundamentals of Structured Streaming, its advantages,
    and the core concepts that underpin its operation. Then, we talked about Spark
    Streaming and what it has to offer.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先理解了结构化流的基本原理、其优势以及支撑其运作的核心概念。然后，我们讨论了Spark Streaming及其提供的功能。
- en: After that, we dived into the core functionalities of Structured Streaming.
    Then, we further delved into advanced topics, such as windowed operations in Structured
    Streaming. We explored sliding and tumbling windows, which enable us to perform
    aggregations and computations over a specified time window, allowing for time-based
    analysis of the streaming data. Additionally, we explored stateful streaming processing,
    which involves maintaining and updating state in streaming applications and integrating
    external libraries and APIs to enhance the capabilities of Structured Streaming.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们深入探讨了结构化流的核心功能。然后，我们进一步深入到高级主题，例如结构化流中的窗口操作。我们探讨了滑动窗口和滚动窗口，这些窗口使我们能够在指定的时间窗口内执行聚合和计算，从而允许对流式数据进行基于时间的分析。此外，我们还探讨了有状态流式处理，这涉及到在流式应用中维护和更新状态，以及集成外部库和API以增强结构化流的功能。
- en: Finally, we explored emerging trends in real-time data processing and concluded
    the chapter by summarizing the key takeaways and insights gained.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们探讨了实时数据处理的最新趋势，并在总结关键要点和洞察中结束本章。
- en: In the next chapter, we will look into machine learning techniques and how to
    use Spark with machine learning.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨机器学习技术以及如何使用Spark进行机器学习。
