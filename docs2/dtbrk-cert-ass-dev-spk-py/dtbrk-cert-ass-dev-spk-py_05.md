

# 第五章：Spark 的高级操作和优化

在本章中，我们将深入探讨 Apache Spark 的高级功能，为您提供优化数据处理工作流程所需的知识和技术。从 Catalyst 优化器的内部工作原理到不同类型连接的复杂性，我们将探索高级 Spark 操作，让您能够充分利用这个强大框架的全部潜力。

本章将涵盖以下主题：

+   在 Spark DataFrame 中对数据进行分组的不同选项。

+   Spark 中的各种连接类型，包括内连接、左连接、右连接、外连接、交叉连接、广播连接和洗牌连接，每种连接都有其独特的用例和影响

+   Shuffle 和广播连接，重点关注广播哈希连接和洗牌排序合并连接，以及它们的应用和优化策略

+   使用不同的数据格式（如 CSV、Parquet 等）在 Spark 中读取和写入数据

+   使用 Spark SQL 进行不同操作

+   Catalyst 优化器，Spark 查询执行引擎中的一个关键组件，它采用基于规则和基于成本的优化来提高查询性能

+   Spark 中窄变换和宽变换的区别以及何时使用每种类型以实现最佳并行性和资源效率

+   数据持久化和缓存技术以减少重复计算并加速数据处理，以及高效内存管理的最佳实践

+   通过 repartition 和 coalesce 进行数据分区，以及如何使用这些操作来平衡工作负载和优化数据分布

+   **用户定义函数**（**UDFs**）和自定义函数，允许您实现专门的数据处理逻辑，以及何时以及如何有效地利用它们

+   使用 Catalyst 优化器和**自适应查询执行**（**AQE**）进行 Spark 的高级优化

+   基于数据的优化技术及其好处

每个部分都将提供深入见解、实际示例和最佳实践，确保您能够应对 Apache Spark 中的复杂数据处理挑战。到本章结束时，您将掌握利用 Spark 高级功能的知识和技能，并解锁其在数据驱动项目中的全部潜力。

# 在 Spark 中对数据进行分组和不同的 Spark 连接

我们将从最重要的数据处理技术之一开始：分组和连接数据。当我们进行数据探索时，根据不同标准对数据进行分组成为数据分析的关键。我们将探讨如何使用`groupBy`对不同的数据进行分组。

## 在 DataFrame 中使用 groupBy

我们可以根据不同的标准在 DataFrame 中对数据进行分组——例如，我们可以根据 DataFrame 中的不同列对数据进行分组。我们还可以应用不同的聚合，如`sum`或`average`，来获取数据切片的整体视图。

为了这个目的，在 Spark 中，我们有`groupBy`操作。`groupBy`操作与 SQL 中的`groupBy`类似，因为我们可以在这些分组数据集上执行分组操作。此外，我们可以在一个`groupBy`语句中指定多个`groupBy`标准。以下示例展示了如何在 PySpark 中使用`groupBy`。我们将使用上一章中创建的 DataFrame 薪水数据。

在以下`groupBy`语句中，我们根据`Department`列对薪水数据进行分组：

```py
salary_data.groupby('Department')
```

因此，这个操作返回了一个按`Department`列分组的分组数据对象：

```py
<pyspark.sql.group.GroupedData at 0x7fc8495a3c10>
```

这可以分配给一个单独的 DataFrame，并且可以对这组数据执行更多操作。所有聚合操作也可以用于 DataFrame 的不同组。

我们将使用以下语句来获取`salary_data` DataFrame 中不同部门的平均薪水：

```py
salary_data.groupby(‘Department’).avg().show()
```

这里是结果：

```py
+----------+------------------+  
|Department| avg(Salary)      |  
+----------+------------------+  
| null     |            3750.0|  
| Sales    |            2600.0|  
| Field-eng| 4166.666666666667|  
| Finance  |3333.3333333333335|  
+----------+------------------+ 
```

在这个例子中，我们可以看到每个部门的平均薪水是基于`salary_data` DataFrame 中的`salary`列计算的。包括`null`（因为我们 DataFrame 中有空值），所有四个部门都包含在结果 DataFrame 中。

现在，让我们看看如何将复杂的`groupBy`操作应用于 PySpark DataFrame 中的数据。

## 一个复杂的 groupBy 语句

`groupBy`可以在复杂的数据操作中使用，例如在单个`groupBy`语句中进行多个聚合。

在下面的代码片段中，我们将使用`groupBy`操作，通过对每个部门的薪水列求和。然后，我们将刚刚创建的`sum(Salary)`列四舍五入到小数点后两位。之后，我们将`sum(Salary)`列重命名为`Salary`。所有这些操作都在一个`groupBy`语句中完成：

```py
from pyspark.sql.functions import col, round
salary_data.groupBy('Department')\
  .sum('Salary')\
  .withColumn('sum(Salary)',round(col('sum(Salary)'), 2))\
  .withColumnRenamed('sum(Salary)', 'Salary')\
  .orderBy('Department')\
  .show()
```

因此，我们将看到以下 DataFrame，它显示了基于每个部门的`Salary`列的聚合总和：

```py
+----------+------------------+
|Department|    sum(Salary)   |
+----------+------------------+
| null     |              7500|
| Field-eng|             12500|
| Finance  |             10000|
| Sales    |              5200|
+----------+------------------+
```

在这个例子中，我们可以看到每个部门的总薪水被计算在一个名为`sum(Salary)`的新列中，之后我们将这个总数四舍五入到两位小数。在下一个语句中，我们将`sum(Salary)`列重命名为`Salary`，然后根据`Department`对结果 DataFrame 进行排序。在结果 DataFrame 中，我们可以看到每个部门的薪水总和被计算在新列中。

现在我们知道了如何使用不同的聚合函数来分组数据，让我们看看如何在 Spark 中将两个 DataFrame 合并在一起。

# Spark 中的 DataFrame 连接

连接操作是数据处理任务的基本操作，也是 Apache Spark 的核心组件之一。Spark 提供了多种类型的连接操作，用于将来自不同 DataFrame 或数据集的数据合并在一起。在本节中，我们将探讨不同的 Spark 连接操作以及何时使用每种类型。

连接操作用于根据公共列将两个或多个数据帧中的数据合并在一起。这些操作对于合并数据集、聚合信息和执行关系操作等任务至关重要。

在 Spark 中，执行连接的主要语法是使用 `.join()` 方法，该方法接受以下参数：

+   `other`: 要与之连接的其他数据帧

+   `on`: 要连接数据帧的列

+   `how`: 要执行的连接类型（内部、外部、左连接或右连接）

+   `suffixes`: 在两个数据帧中具有相同名称的列中添加的后缀

这些参数在连接操作的主要语法中使用，如下所示：

```py
Dataframe1.join(Dataframe2, on, how)
```

在这里，`Dataframe1` 将位于连接的左侧，而 `Dataframe2` 将位于连接的右侧。

数据帧或数据集可以根据数据帧内的公共列进行连接，连接查询的结果是一个新的数据帧。

我们将在两个新的数据帧上演示连接操作。首先，让我们创建这些数据帧。第一个数据帧称为 `salary_data_with_id`：

```py
salary_data_with_id = [(1, "John", "Field-eng", 3500), \
    (2, "Robert", "Sales", 4000), \
    (3, "Maria", "Finance", 3500), \
    (4, "Michael", "Sales", 3000), \
    (5, "Kelly", "Finance", 3500), \
    (6, "Kate", "Finance", 3000), \
    (7, "Martin", "Finance", 3500), \
    (8, "Kiran", "Sales", 2200), \
  ]
columns= ["ID", "Employee", "Department", "Salary"]
salary_data_with_id = spark.createDataFrame(data = salary_data_with_id, schema = columns)
salary_data_with_id.show()
```

结果数据帧，命名为 `salary_data_with_id`，看起来如下：

```py
+---+--------+----------+------+
|ID |Employee|Department|Salary|
+---+--------+----------+------+
| 1 | John   | Field-eng|  3500|
| 2 | Robert | Sales    |  4000|
| 3 | Maria  | Finance  |  3500|
| 4 | Michael| Sales    |  3000|
| 5 | Kelly  | Finance  |  3500|
| 6 | Kate   | Finance  |  3000|
| 7 | Martin | Finance  |  3500|
| 8 | Kiran  | Sales    |  2200|
+---+--------+----------+------+
```

现在，我们将创建另一个名为 `employee_data` 的数据帧：

```py
employee_data = [(1, "NY", "M"), \
    (2, "NC", "M"), \
    (3, "NY", "F"), \
    (4, "TX", "M"), \
    (5, "NY", "F"), \
    (6, "AZ", "F") \
  ]
columns= ["ID", "State", "Gender"]
employee_data = spark.createDataFrame(data = employee_data, schema = columns)
employee_data.show()
```

结果数据帧，命名为 `employee_data`，看起来如下：

```py
+---+-----+------+
| ID|State|Gender|
+---+-----+------+
|  1|   NY|     M|
|  2|   NC|     M|
|  3|   NY|     F|
|  4|   TX|     M|
|  5|   NY|     F|
|  6|   AZ|     F|
+---+-----+------+
```

现在，假设我们想要根据 `ID` 列将这两个数据帧连接在一起。

如我们之前提到的，Spark 提供了不同类型的连接操作。我们将在本章中探索其中的一些。让我们从内部连接开始。

### 内部连接

当我们想要根据两个数据帧中共同存在的值来连接两个数据帧时，使用**内部连接**。任何在任何一个数据帧中不存在的值都不会是结果数据帧的一部分。默认情况下，Spark 中的连接类型是内部连接。

#### 用例

内部连接在合并数据时非常有用，当你对两个数据帧中的共同元素感兴趣时——例如，将销售数据与客户数据连接起来，以查看哪些客户进行了购买。

以下代码演示了如何使用我们之前创建的数据帧进行内部连接：

```py
salary_data_with_id.join(employee_data,salary_data_with_id.ID ==  employee_data.ID,"inner").show()
```

结果数据帧现在包含两个数据帧的所有列——`salary_data_with_id` 和 `employee_data`——在单个数据帧中连接在一起。它只包括两个数据帧中都有的行。下面是它的样子：

```py
+---+--------+----------+------+---+-----+------+
| ID|Employee|Department|Salary| ID|State|Gender|
+---+--------+----------+------+---+-----+------+
|  1|    John| Field-eng|  3500|  1|   NY|     M|
|  2|  Robert|     Sales|  4000|  2|   NC|     M|
|  3|   Maria|   Finance|  3500|  3|   NY|     F|
|  4| Michael|     Sales|  3000|  4|   TX|     M|
|  5|   Kelly|   Finance|  3500|  5|   NY|     F|
|  6|    Kate|   Finance|  3000|  6|   AZ|     F|
+---+--------+----------+------+---+-----+------+
```

你会注意到，`how` 参数定义了此语句中正在进行的连接类型。目前，它显示为 `inner`，因为我们想要数据帧根据内部连接进行连接。我们还可以看到 ID `7` 和 `8` 缺失。原因是 `employee_data` 数据帧不包含 ID `7` 和 `8`。由于我们使用的是内部连接，它只基于两个数据帧中共同的数据元素进行连接。任何在任一数据帧中不存在的数据都不会是结果数据帧的一部分。

接下来，我们将探索外部连接。

### 外部连接

一个 `null`。

当我们想要根据两个 DataFrame 中都存在的值来连接两个 DataFrame，无论这些值是否存在于另一个 DataFrame 中时，我们应该使用外连接。任何存在于任何一个 DataFrame 中的值都将成为结果 DataFrame 的一部分。

#### 用例

外连接适用于需要包含两个 DataFrame 中的所有记录，同时容纳不匹配值的情况——例如，当合并员工数据与项目数据以查看哪些员工被分配到哪些项目时，包括那些目前没有被分配到任何项目的员工。

以下代码演示了我们如何使用之前创建的 DataFrame 进行外连接：

```py
salary_data_with_id.join(employee_data,salary_data_with_id.ID ==  employee_data.ID,"outer").show()
```

结果 DataFrame 包含 `salary_data_with_id` 和 `employee_data` DataFrame 中所有员工的资料。它看起来是这样的：

```py
+---+--------+----------+------+----+-----+------+
| ID|Employee|Department|Salary|  ID|State|Gender|
+---+--------+----------+------+----+-----+------+
|  1|    John| Field-eng|  3500|   1|   NY|     M|
|  2|  Robert|     Sales|  4000|   2|   NC|     M|
|  3|   Maria|   Finance|  3500|   3|   NY|     F|
|  4| Michael|     Sales|  3000|   4|   TX|     M|
|  5|   Kelly|   Finance|  3500|   5|   NY|     F|
|  6|    Kate|   Finance|  3000|   6|   AZ|     F|
|  7|  Martin|   Finance|  3500|null| null|  null|
|  8|   Kiran|     Sales|  2200|null| null|  null|
+---+--------+----------+------+----+-----+------+
```

你会注意到 `how` 参数已更改，并显示为 `outer`。在结果 DataFrame 中，现在存在 ID `7` 和 `8`。然而，也请注意，ID `7` 和 `8` 的 `ID`、`State` 和 `Gender` 列是 `null`。原因是 `employee_data` DataFrame 不包含 ID `7` 和 `8`。任何在两个 DataFrame 中都不存在的数据将成为结果 DataFrame 的一部分，但对应列将显示为 `null`，正如员工 ID `7` 和 `8` 的情况所示。

接下来，我们将探讨左连接。

### 左连接

左连接返回左侧 DataFrame 的所有行和右侧 DataFrame 的匹配行。如果右侧 DataFrame 中没有匹配项，则结果将包含 `null` 值。

#### 用例

当你想要保留左侧 DataFrame 的所有记录，并且只保留右侧 DataFrame 的匹配记录时，左连接很有用——例如，当合并客户数据与交易数据以查看哪些客户进行了购买时。

以下代码演示了我们如何使用之前创建的 DataFrame 进行左连接：

```py
salary_data_with_id.join(employee_data,salary_data_with_id.ID ==  employee_data.ID,"left").show()
```

结果 DataFrame 包含来自左侧 DataFrame 的所有数据——即 `salary_data_with_id`。它看起来是这样的：

```py
+---+--------+----------+------+----+-----+------+
| ID|Employee|Department|Salary|  ID|State|Gender|
+---+--------+----------+------+----+-----+------+
|  1|    John| Field-eng|  3500|   1|   NY|     M|
|  2|  Robert|     Sales|  4000|   2|   NC|     M|
|  3|   Maria|   Finance|  3500|   3|   NY|     F|
|  4| Michael|     Sales|  3000|   4|   TX|     M|
|  5|   Kelly|   Finance|  3500|   5|   NY|     F|
|  6|    Kate|   Finance|  3000|   6|   AZ|     F|
|  7|  Martin|   Finance|  3500|null| null|  null|
|  8|   Kiran|     Sales|  2200|null| null|  null|
+---+--------+----------+------+----+-----+------+
```

注意，`how` 参数已更改，并显示为 `left`。现在，ID `7` 和 `8` 存在。然而，也请注意，ID `7` 和 `8` 的 `ID`、`State` 和 `Gender` 列是 `null`。原因是 `employee_data` DataFrame 不包含 ID `7` 和 `8`。由于 `salary_data_with_id` 是连接语句中的左侧 DataFrame，其值在连接中具有优先权。

所有来自左侧 DataFrame 的记录都包含在结果 DataFrame 中，并且包含来自右侧 DataFrame 的匹配记录。右侧 DataFrame 中的不匹配条目在结果中用 `null` 值填充。

接下来，我们将探讨右连接。

### 右连接

右连接与左连接类似，但它返回右侧 DataFrame 的所有行和左侧 DataFrame 的匹配行。左侧 DataFrame 中的不匹配行包含 `null` 值。

#### 用例

右连接是左连接的相反，当您想保留右侧 DataFrame 的所有记录并包含来自左侧 DataFrame 的匹配记录时使用。

以下代码演示了如何使用我们之前创建的 DataFrame 进行右连接：

```py
salary_data_with_id.join(employee_data,salary_data_with_id.ID ==  employee_data.ID,"right").show()
```

结果 DataFrame 包含右侧 DataFrame 的所有数据——即 `employee_data`。它看起来如下：

```py
+---+--------+----------+------+---+-----+------+
| ID|Employee|Department|Salary| ID|State|Gender|
+---+--------+----------+------+---+-----+------+
|  1|    John| Field-eng|  3500|  1|   NY|     M|
|  2|  Robert|     Sales|  4000|  2|   NC|     M|
|  3|   Maria|   Finance|  3500|  3|   NY|     F|
|  4| Michael|     Sales|  3000|  4|   TX|     M|
|  5|   Kelly|   Finance|  3500|  5|   NY|     F|
|  6|    Kate|   Finance|  3000|  6|   AZ|     F|
+---+--------+----------+------+---+-----+------+
```

注意到 `how` 参数已更改，现在显示为 `right`。结果 DataFrame 显示 IDs `7` 和 `8` 不存在。原因是 `employee_data` DataFrame 不包含 IDs `7` 和 `8`。由于 `employee_data` 是连接语句中的右侧 DataFrame，其值在连接中具有优先权。

所有来自右侧 DataFrame 的记录都包含在结果 DataFrame 中，并且包含来自左侧 DataFrame 的匹配记录。左侧 DataFrame 中的不匹配条目在结果中用 `null` 值填充。

接下来，我们将探索交叉连接。

### 交叉连接

**交叉连接**，也称为**笛卡尔连接**，将左侧 DataFrame 的每一行与右侧 DataFrame 的每一行组合。这导致了一个大型的笛卡尔积 DataFrame。

#### 用例

由于它们可能生成大量数据集，因此应谨慎使用交叉连接。它们通常用于探索所有可能的数据组合，例如在生成测试数据时。

接下来，我们将探索并集选项以连接两个 DataFrame。

### 并集

并集用于连接具有相似模式的两个 DataFrame。为了说明这一点，我们将创建另一个名为 `salary_data_with_id_2` 的 DataFrame，其中包含一些额外的值。这个 DataFrame 的模式与 `salary_data_with_id` 的模式相同：

```py
salary_data_with_id_2 = [(1, "John", "Field-eng", 3500), \
    (2, "Robert", "Sales", 4000), \
    (3, "Aliya", "Finance", 3500), \
    (4, "Nate", "Sales", 3000), \
  ]
columns2= ["ID", "Employee", "Department", "Salary"]
salary_data_with_id_2 = spark.createDataFrame(data = salary_data_with_id_2, schema = columns2)
salary_data_with_id_2.printSchema()
salary_data_with_id_2.show(truncate=False)
```

因此，你将首先看到 DataFrame 的模式，然后是实际的 DataFrame 及其值：

```py
root
 |-- ID: long (nullable = true)
 |-- Employee: string (nullable = true)
 |-- Department: string (nullable = true)
 |-- Salary: long (nullable = true)
+---+--------+----------+------+
|ID |Employee|Department|Salary|
+---+--------+----------+------+
|1  |John    |Field-eng |3500  |
|2  |Robert  |Sales     |4000  |
|3  |Aliya   |Finance   |3500  |
|4  |Nate    |Sales     |3000  |
+---+--------+----------+------+
```

一旦我们有了这个 DataFrame，我们就可以使用 `union()` 函数将 `salary_data_with_id` 和 `salary_data_with_id_2` DataFrame 连接在一起。以下示例说明了这一点：

```py
unionDF = salary_data_with_id.union(salary_data_with_id_2)
unionDF.show(truncate=False)
```

结果 DataFrame，命名为 `unionDF`，看起来如下：

```py
+---+--------+----------+------+
|ID |Employee|Department|Salary|
+---+--------+----------+------+
|1  |John    |Field-eng |3500  |
|2  |Robert  |Sales     |4000  |
|3  |Maria   |Finance   |3500  |
|4  |Michael |Sales     |3000  |
|5  |Kelly   |Finance   |3500  |
|6  |Kate    |Finance   |3000  |
|7  |Martin  |Finance   |3500  |
|8  |Kiran   |Sales     |2200  |
|1  |John    |Field-eng |3500  |
|2  |Robert  |Sales     |4000  |
|3  |Aliya   |Finance   |3500  |
|4  |Nate    |Sales     |3000  |
+---+--------+----------+------+
```

如您所见，两个 DataFrame 已连接在一起，因此结果 DataFrame 中添加了新行。最后四行来自 `salary_data_with_id_2` 并添加到 `salary_data_with_id` 的行中。这是将两个 DataFrame 连接在一起的一种方法。

在本节中，我们探讨了不同类型的 Spark 连接及其适用场景。选择正确的连接类型对于确保 Spark 中高效的数据处理至关重要，并且理解每种类型的含义将帮助你在数据分析和处理任务中做出明智的决定。

现在，让我们看看如何在 Spark 中读取和写入数据。

# 读取和写入数据

当我们使用 Spark 并在 Spark 中进行数据操作的所有操作时，我们需要做的最重要的事情之一是将数据读写到磁盘上。记住，Spark 是一个内存框架，这意味着所有操作都在计算或集群的内存中执行。一旦这些操作完成，我们就会希望将数据写入磁盘。同样，在我们操作任何数据之前，我们可能还需要从磁盘读取数据。

Spark 支持多种数据格式用于读取和写入不同类型的数据文件。在本章中，我们将讨论以下格式。

+   **逗号分隔值** (**CSV**)

+   **Parquet**

+   **优化行** **列式** (**ORC**)

请注意，这些并不是 Spark 支持的唯一格式，但这是一个非常流行的格式子集。Spark 还支持许多其他格式，例如 Avro、文本、JDBC、Delta 等。

在下一节中，我们将讨论 CSV 文件格式以及如何读取和写入 CSV 格式的数据文件。

## 读取和写入 CSV 文件

在本节中，我们将讨论如何从 CSV 文件格式读取和写入数据。在这个文件格式中，数据由逗号分隔。这是一个非常流行的数据格式，因为它易于使用且简单。

让我们通过运行以下代码来查看如何使用 Spark 写入 CSV 文件：

```py
salary_data_with_id.write.csv('salary_data.csv', mode='overwrite', header=True)
spark.read.csv('/salary_data.csv', header=True).show()
```

生成的 DataFrame，命名为 `salary_data_with_id`，如下所示：

```py
+---+--------+----------+------+
|ID |Employee|Department|Salary|
+---+--------+----------+------+
| 1 | John   | Field-eng|  3500|
| 2 | Robert | Sales    |  4000|
| 3 | Maria  | Finance  |  3500|
| 4 | Michael| Sales    |  3000|
| 5 | Kelly  | Finance  |  3500|
| 6 | Kate   | Finance  |  3000|
| 7 | Martin | Finance  |  3500|
| 8 | Kiran  | Sales    |  2200|
+---+--------+----------+------+
```

在这里我们可以看到 `dataframe.write.csv()` 函数中的一些参数。第一个参数是我们需要写入磁盘的 DataFrame 名称。第二个参数 `header` 指定我们需要写入的文件是否应该带有标题行。

在 `dataframe.read.csv()` 函数中，有一些参数我们需要讨论。第一个参数是我们需要读取的文件的 `path/name` 值。第二个参数 `header` 指定文件是否有标题行需要读取。

在第一个语句中，我们将 `salary_data` DataFrame 写入名为 `salary_data.csv` 的 CSV 文件。

在下一个语句中，我们正在读取我们写入的相同文件以查看其内容。我们可以看到，生成的文件包含我们写入的相同数据。

让我们看看另一个可以用于使用 Spark 读取 CSV 文件的函数：

```py
from pyspark.sql.types import *
filePath = '/salary_data.csv'
columns= ["ID", "State", "Gender"] 
schema = StructType([
      StructField("ID", IntegerType(),True),
  StructField("State",  StringType(),True),
  StructField("Gender",  StringType(),True)
])
read_data = spark.read.format("csv").option("header","true").schema(schema).load(filePath)
read_data.show()
```

生成的 DataFrame，命名为 `read_data`，如下所示：

```py
+---+--------+----------+------+
|ID |Employee|Department|Salary|
+---+--------+----------+------+
| 1 | John   | Field-eng|  3500|
| 2 | Robert | Sales    |  4000|
| 3 | Maria  | Finance  |  3500|
| 4 | Michael| Sales    |  3000|
| 5 | Kelly  | Finance  |  3500|
| 6 | Kate   | Finance  |  3000|
| 7 | Martin | Finance  |  3500|
| 8 | Kiran  | Sales    |  2200|
+---+--------+----------+------+
```

在 `spark.read.format()` 函数中，有一些参数。首先，我们指定需要读取的文件格式。然后，我们可以为不同的选项执行不同的函数调用。在下一个调用中，我们指定文件有标题，因此 DataFrame 预期会有标题。然后，我们指定我们需要为这些数据有一个模式，该模式在 `schema` 变量中定义。最后，在 `load` 函数中，我们定义要加载的文件的路径。

接下来，我们将学习如何使用 Spark 读取和写入 Parquet 文件。

## 读取和写入 Parquet 文件

在本节中，我们将讨论 Parquet 文件格式。Parquet 是一种列式文件格式，使得数据读取和写入非常高效。它也是一种紧凑的文件格式，有助于加快读取和写入速度。

让我们通过运行以下代码来学习如何使用 Spark 写入 Parquet 文件：

```py
salary_data_with_id.write.parquet('salary_data.parquet', mode='overwrite')
spark.read.parquet(' /salary_data.parquet').show()
```

生成的 DataFrame，命名为`salary_data_with_id`，看起来如下：

```py
+---+--------+----------+------+
|ID |Employee|Department|Salary|
+---+--------+----------+------+
| 1 | John   | Field-eng|  3500|
| 2 | Robert | Sales    |  4000|
| 3 | Maria  | Finance  |  3500|
| 4 | Michael| Sales    |  3000|
| 5 | Kelly  | Finance  |  3500|
| 6 | Kate   | Finance  |  3000|
| 7 | Martin | Finance  |  3500|
| 8 | Kiran  | Sales    |  2200|
+---+--------+----------+------+
```

在`dataframe.write()`函数中，有一些参数我们可以在这里看到。第一个调用是`parquet`函数，用于定义文件类型。然后，作为下一个参数，我们指定了需要将这个 Parquet 文件写入的路径。

在下一个语句中，我们正在读取我们写入的相同文件以查看其内容。我们可以看到，生成的文件包含了我们写入的数据。

接下来，我们将探讨如何使用 Spark 读取和写入 ORC 文件。

## 读取和写入 ORC 文件

在本节中，我们将讨论 ORC 文件格式。与 Parquet 类似，ORC 也是一种列式和紧凑的文件格式，使得数据读取和写入非常高效。

让我们通过运行以下代码来学习如何使用 Spark 写入 ORC 文件：

```py
salary_data_with_id.write.orc('salary_data.orc', mode='overwrite')
spark.read.orc(' /salary_data.orc').show()
```

生成的 DataFrame，命名为`salary_data_with_id`，看起来如下：

```py
+---+--------+----------+------+
|ID |Employee|Department|Salary|
+---+--------+----------+------+
| 1 | John   | Field-eng|  3500|
| 2 | Robert | Sales    |  4000|
| 3 | Maria  | Finance  |  3500|
| 4 | Michael| Sales    |  3000|
| 5 | Kelly  | Finance  |  3500|
| 6 | Kate   | Finance  |  3000|
| 7 | Martin | Finance  |  3500|
| 8 | Kiran  | Sales    |  2200|
+---+--------+----------+------+
```

在`dataframe.write()`函数中，有一些参数我们可以看到。第一个调用是`orc`函数，用于定义文件类型。然后，作为下一个参数，我们指定了需要将这个 Parquet 文件写入的路径。

在下一个语句中，我们正在读取我们写入的相同文件以查看其内容。我们可以看到，生成的文件包含了我们写入的相同数据。

接下来，我们将探讨如何使用 Spark 读取和写入 Delta 文件。

## 读取和写入 Delta 文件

Delta 文件格式是一种比 Parquet 和其他列式格式更优化的开放格式。当数据以 Delta 格式存储时，你会注意到底层文件是 Parquet 格式的。Delta 格式在 Parquet 文件之上添加了一个事务日志，使得数据读取和写入更加高效。

让我们通过运行以下代码来学习如何使用 Spark 读取和写入 Delta 文件：

```py
salary_data_with_id.write.format("delta").save("/FileStore/tables/salary_data_with_id", mode='overwrite')
df = spark.read.load("/FileStore/tables/salary_data_with_id")
df.show()
```

生成的 DataFrame，命名为`salary_data_with_id`，看起来如下：

```py
+---+--------+----------+------+
| ID|Employee|Department|Salary|
+---+--------+----------+------+
|  7|  Martin|   Finance|  3500|
|  4| Michael|     Sales|  3000|
|  6|    Kate|   Finance|  3000|
|  2|  Robert|     Sales|  4000|
|  1|    John| Field-eng|  3500|
|  5|   Kelly|   Finance|  3500|
|  3|   Maria|   Finance|  3500|
|  8|   Kiran|     Sales|  2200|
+---+--------+----------+------+
```

在这个示例中，我们将`salary_data_with_id`写入一个 Delta 文件。我们在`format`函数中添加了`delta`参数，之后将文件保存到某个位置。

在下一个语句中，我们正在将我们写入的相同 Delta 文件读取到一个名为`df`的 DataFrame 中。文件的内容与用于写入它的 DataFrame 保持一致。

现在我们知道了如何在 Spark 中使用高级操作来操作和合并数据，我们将探讨如何使用 SQL 与 Spark DataFrames 进行交互，以在 Python 和 SQL 之间切换语言。这为 Spark 用户提供了很大的权力，因为它允许他们根据用例和他们对不同语言的知识使用多种语言。

# 在 Spark 中使用 SQL

在*第二章*中，我们讨论了 Spark Core 以及它是如何跨 Spark 的不同组件共享的。DataFrame 和 Spark SQL 也可以互换使用。我们还可以使用 DataFrame 中存储的数据进行 Spark SQL 查询。

以下代码演示了如何利用此功能：

```py
salary_data_with_id.createOrReplaceTempView("SalaryTable")
spark.sql("SELECT count(*) from SalaryTable").show()
```

结果 DataFrame 看起来像这样：

```py
+--------+
|count(1)|
+--------+
|       8|
+--------+
```

`createOrReplaceTempView`函数用于将 DataFrame 转换为名为`SalaryTable`的表。一旦完成此转换，我们就可以在此表上运行常规 SQL 查询。我们正在运行一个`count *`查询来计算表中的元素总数。

在下一节中，我们将了解 UDF 是什么以及如何在 Spark 中使用它。

# Apache Spark 中的 UDFs

UDFs（用户定义函数）是 Apache Spark 中的一个强大功能，它允许你通过定义自定义函数来扩展 Spark 的功能。UDFs 对于以 Spark 内置函数不支持的方式转换和操作数据至关重要。在本节中，我们将深入探讨 UDFs 在 Spark 中的概念、实现和最佳实践。

## 什么是 UDFs？

UDFs 是由用户创建的用于在 Spark 中对数据进行特定操作的定制函数。UDFs 扩展了你可以应用于数据转换和操作的范围，使 Spark 在多样化的用例中更加灵活。

这里是 UDFs 的一些关键特性：

+   **用户自定义逻辑**：UDFs 允许你将用户特定的逻辑或自定义算法应用于你的数据

+   **支持多种语言**：Spark 支持用 Scala、Python、Java 和 R 等多种编程语言编写的 UDFs

+   **与 DataFrame 和弹性分布式数据集（RDD）的兼容性**：UDFs 可以与 DataFrame 和 RDD 一起使用

+   **利用外部库**：你可以在 UDFs 中使用外部库来执行高级操作

让我们看看如何创建 UDFs。

## 创建和注册 UDFs

要在 Spark 中使用 UDFs，你需要创建并注册它们。这个过程涉及定义一个函数并将其注册到 Spark 中。你可以为 SQL 和 DataFrame 操作定义 UDFs。在本节中，你将看到在 Spark 中定义 UDF 的基本语法，然后将其注册到 Spark 中。你可以在 UDF 中编写任何自定义 Python 代码以用于你的应用程序逻辑。第一个例子是 Python；下一个例子是 Scala。

### 在 Python 中创建 UDFs

我们可以使用以下代码在 Python 中创建一个 UDF：

```py
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType
# Define a UDF in Python
def my_udf_function(input_param):
# Your custom logic here
return processed_value
# Register the UDF with Spark
my_udf = udf(my_udf_function, IntegerType())
# Using the UDF in a DataFrame operation
df = df.withColumn("new_column", my_udf(df["input_column"]))
```

### 在 Scala 中创建 UDFs

我们可以使用以下代码在 Scala 中创建一个 UDF：

```py
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
// Define a UDF in Scala
val myUDF: UserDefinedFunction = udf((inputParam: InputType) => {
// Your custom logic here
processedValue }, OutputType)
// Using the UDF in a DataFrame operation
val df = df.withColumn("newColumn", myUDF(col("inputColumn")))
```

## UDFs 的用例

UDFs 非常灵活，可以在广泛的应用场景中使用，包括但不限于以下内容：

+   **数据转换**：应用自定义逻辑以转换数据，例如数据丰富、清洗和特征工程

+   **复杂计算**：实现 Spark 标准函数中不可用的复杂数学或统计操作

+   **字符串操作**：解析和格式化字符串、正则表达式和文本处理

+   **机器学习**：在机器学习工作流程中创建用于特征提取、预处理或后处理的自定义函数

+   **特定领域逻辑**：实现特定于您用例的特定领域相关逻辑

## 使用 UDF 的最佳实践

当在 Spark 中使用 UDF 时，请考虑以下最佳实践：

+   **避免性能瓶颈**：UDF 可能会影响性能，尤其是在与大数据集一起使用时。分析并监控您的应用程序以识别性能瓶颈。

+   **最小化 UDF 复杂性**：保持 UDF 简单和高效，以避免减慢您的 Spark 应用程序。复杂的操作可能导致更长的执行时间。

+   **检查数据类型兼容性**：确保 UDF 的输出数据类型与列数据类型匹配，以避免错误和数据类型不匹配。

+   **优化数据处理**：尽可能使用内置的 Spark 函数，因为它们针对分布式数据处理进行了高度优化。

+   **使用矢量化 UDF**：在某些 Spark 版本中，可用的矢量化 UDF 可以通过一次处理多个值来显著提高 UDF 性能。

+   **测试和验证**：在将 UDF（用户定义函数）应用于整个数据集之前，先在数据的小子集上彻底测试它们。确保它们产生预期的结果。

+   **记录 UDF**：使用注释和描述记录您的 UDF，使您的代码更易于维护和理解他人。

在本节中，我们探讨了 Apache Spark 中 UDF（用户定义函数）的概念。UDF 是扩展 Spark 功能并执行自定义数据转换和操作的强大工具。当谨慎且高效地使用时，UDF 可以帮助您解决 Spark 中广泛的数据处理挑战。

现在我们已经涵盖了 Spark 的高级操作，我们将深入探讨 Spark 优化的概念。

# Apache Spark 的优化

以其分布式计算能力而闻名的 Apache Spark 提供了一套高级优化技术，这些技术对于最大化性能、提高资源利用率和增强数据处理作业的效率至关重要。这些技术超越了基本优化，使用户能够针对最佳执行对 Spark 应用程序进行微调和优化。

## 理解 Spark 中的优化

Spark 中的优化旨在微调作业的执行以提高速度、资源利用率和整体性能。

以其强大的优化能力而闻名的 Apache Spark，显著提高了分布式数据处理任务的性能。在这个优化框架的核心是 Catalyst 优化器，这是一个关键组件，在提高查询执行效率方面发挥着至关重要的作用。这是在查询执行之前完成的。

Catalyst 优化器主要在查询编译期间生成的静态优化计划上工作。然而，AQE（在 Spark 3.0 中引入）是一种动态和自适应的方法，在运行时根据实际数据特性和执行环境优化查询计划。我们将在下一节中了解更多关于这两种范例的信息。

## Catalyst 优化器

Catalyst 优化器是 Apache Spark 查询执行引擎的一个关键部分。它是一个强大的工具，使用高级技术来优化查询计划，从而提高 Spark 应用程序的性能。术语“*催化剂*”指的是它激发查询计划中的转换并使其更高效的能力。

让我们看看 Catalyst 优化器的一些关键特性：

+   **基于规则的优化**：Catalyst 优化器采用一组规则和优化来转换和增强查询计划。这些规则涵盖了广泛的查询优化场景。

+   **逻辑和物理查询计划**：它同时适用于逻辑和物理查询计划。逻辑计划表示查询的抽象结构，而物理计划概述了如何执行它。

+   **可扩展性**：用户可以定义自定义规则和优化。这种可扩展性允许你根据特定的用例定制优化器。

+   **基于成本的优化**：Catalyst 优化器可以评估不同查询计划的成本，并根据成本估计选择最有效的一个。这在处理复杂查询时特别有用。

让我们看看构成 Catalyst 优化器的不同组件。

### Catalyst 优化器组件

要深入了解 Catalyst 优化器，必须检查其核心组件。

#### 逻辑查询计划

逻辑查询计划表示查询的高级、抽象结构。它定义了你想要完成的事情，而不指定如何实现它。Spark 的 Catalyst 优化器与这个逻辑计划一起工作，以确定最佳物理计划。

#### 基于规则的优化

基于规则的优化是 Catalyst 优化器的核心。它包括一组规则，将逻辑查询计划转换为一个更高效的版本。每个规则都专注于优化的一个特定方面，例如谓词下沉、常量折叠或列剪枝。

#### 物理查询计划

物理查询计划定义了如何执行查询。一旦逻辑计划使用基于规则的技巧优化，它就被转换为一个物理计划，考虑到可用的资源和执行环境。这一阶段确保计划可以在分布式和并行方式下执行。

#### 基于成本的优化

除了基于规则的优化外，Catalyst 优化器还可以使用基于成本的优化。它估计不同执行计划的成本，考虑数据分布、连接策略和可用资源等因素。这种方法有助于 Spark 根据实际的执行特征选择最有效的计划。

#### Catalyst 优化器在行动

为了见证 Catalyst 优化器的实际应用，让我们考虑一个使用 Spark SQL API 的实际示例。

在这个代码示例中，我们正在从 CSV 文件加载数据，应用选择操作以选择特定列，并根据条件过滤行。通过在生成的 DataFrame 上调用`explain()`，我们可以看到由 Catalyst 优化器生成的优化查询计划。输出提供了关于 Spark 将执行的物理执行步骤的见解：

```py
# SparkSession setup
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("CatalystOptimizerExample").getOrCreate()
# Load data
df = spark.read.csv("/salary_data.csv", header=True, inferSchema=True) 
# Query with Catalyst Optimizer 
result_df = df.select("employee", "department").filter(df["salary"] > 3500) 
# Explain the optimized query plan 
result_df.explain() 
```

这个来自`explain()`方法的解释通常包括关于物理执行计划、特定优化使用以及查询执行所选策略的详细信息。

通过检查查询计划并理解 Catalyst 优化器如何增强它，你可以获得关于 Spark 优化引擎内部工作原理的宝贵见解。

本节提供了对 Catalyst 优化器、其组件和实际示例的坚实基础介绍。你可以在此基础上进一步深入研究基于规则和基于成本的优化技术，并讨论 Catalyst 优化器在查询性能上产生重大影响的实际场景。

接下来，我们将看到 AQE 如何将优化提升到 Spark 的下一个层次。

## 自适应查询执行（AQE）

Apache Spark，一个强大的分布式计算框架，提供了多种优化技术来增强数据处理作业的性能。其中一项高级优化功能是 AQE，这是一种动态方法，可以显著提高查询处理效率。

AQE 根据实际数据统计信息和硬件条件在运行时动态调整执行计划。它收集并利用运行时统计信息来优化连接策略、分区方法和广播操作。

让我们看看它的关键组件：

+   **运行时统计信息收集**：AQE 在查询执行期间收集运行时统计信息，如数据大小、偏差和分区。

+   **自适应优化规则**：它利用收集到的统计信息动态调整和优化连接策略、分区方法和广播操作。

现在，让我们考虑它的好处和重要性：

+   **性能提升**：AQE 通过动态优化执行计划，显著提高了性能，从而实现了更好的资源利用和减少的执行时间。

+   **处理可变性**：它在查询执行期间有效地处理数据大小、数据分布偏差和硬件条件的变化。

+   **高效资源利用**：它实时优化查询计划，从而提高资源利用率和减少执行时间

### AQE 工作流程

让我们看看 AQE 如何在 Spark 3.0 中优化工作流程：

+   **运行时统计收集**：在查询执行期间，Spark 收集与数据分布、分区大小和连接键基数相关的统计信息

+   **自适应优化**：利用收集到的统计信息，Spark 动态调整查询执行计划，优化连接策略、分区方法和数据重分布技术

+   **增强性能**：自适应优化确保 Spark 适应不断变化的数据和运行时条件，从而提高查询性能和资源利用率

AQE 在 Apache Spark 中代表了查询优化的重要进步，它超越了静态规划，以适应运行时条件和数据特征。通过根据实时统计信息动态调整执行计划，它优化查询性能，确保大规模数据集的高效和可扩展处理。

接下来，我们将看到 Spark 如何进行基于成本的优化。

### 基于成本的优化

Spark 根据数据大小、连接操作和洗牌阶段等因素估计执行不同查询计划的成本。它利用成本估计来选择最有效的查询执行计划。

这里是其好处：

+   **最优计划选择**：基于成本的优化在选择最经济的执行计划时考虑因素，如连接策略和数据分布

+   **性能提升**：最小化不必要的洗牌和计算提高查询性能

接下来，我们将看到 Spark 如何利用内存管理和调整进行优化。

### 内存管理和调整

Spark 还应用了有效的内存分配策略，包括存储和执行内存，以避免不必要的溢出并提高处理效率。它微调垃圾收集设置以最小化中断并提高整体作业性能。

这里是其好处：

+   **降低开销**：优化的内存使用最小化不必要的磁盘溢出，降低开销并提高作业性能

+   **稳定性和可靠性**：调整垃圾收集设置提高稳定性并减少暂停，确保更一致的作业执行

包括 AQE、基于成本的优化、Catalyst 优化器和内存管理在内的高级 Spark 优化技术，在提高 Spark 作业性能、资源利用率和整体效率方面发挥着至关重要的作用。通过利用这些技术，用户可以优化 Spark 应用程序以满足不同的数据处理需求，并提高其可扩展性和性能。

到目前为止，我们已经看到了 Spark 如何内部优化其查询计划。然而，还有其他用户可以实施以进一步提高 Spark 性能的优化。接下来，我们将讨论一些这些优化。

# Apache Spark 中的基于数据的优化

除了 Spark 的内部优化之外，我们还可以在实现方面做一些事情来使 Spark 更加高效。这些是用户控制的优化。如果我们了解这些挑战以及如何在现实世界的数据应用中处理它们，我们就可以充分利用 Spark 的分布式架构。

我们将从分布式框架中一个非常常见的问题——小文件问题开始讨论。

## 解决 Apache Spark 中的小文件问题

小文件问题在分布式计算框架（如 Apache Spark）中提出了重大挑战，因为它影响了性能和效率。当数据存储在众多小文件中而不是合并成大文件时，会导致开销增加和资源利用不充分。在本节中，我们将深入探讨 Spark 中小文件问题的含义，并探讨缓解其影响的有效解决方案。

与小文件问题相关的主要挑战如下：

+   **增加的元数据开销**：将数据存储在众多小文件中会导致更高的元数据开销，因为每个文件都占用单独的块并产生额外的文件处理 I/O 操作。

+   **吞吐量降低**：处理众多小文件效率较低，因为它涉及打开、读取和关闭文件的高开销，导致吞吐量降低。

+   **资源利用效率低下**：Spark 的并行性依赖于数据分区，而小文件可能导致分区不足，资源利用率低，并阻碍并行处理。

既然我们已经讨论了主要挑战，让我们讨论一些缓解小文件问题的解决方案：

+   **文件连接或合并**：将小文件合并成大文件可以显著缓解小文件问题。通过手动或自动化流程进行文件连接或合并等技术有助于减少单个文件的数量。

+   **文件压缩或合并**：将小文件压缩或合并成更少、更实质性的文件的工具或流程可以简化数据存储。这种整合减少了元数据开销并提高了数据访问效率。

+   **文件格式优化**：选择高效的文件格式，如 Parquet 或 ORC，这些格式支持列式存储和压缩，可以减少小文件的影响。这些格式便于高效的数据访问并减少存储空间。

+   **分区策略**：在 Spark 的数据摄入或处理过程中应用适当的分区策略可以减轻小文件问题的影响。这涉及到将数据组织成更大的分区以提高并行性。

+   **数据预取或缓存**：在处理之前将小文件预取或缓存到内存中可以最小化 I/O 开销。使用 Spark 的能力进行缓存或加载数据等技术可以提高性能。

+   **AQE**: 利用 Spark 的 AQE 功能可以帮助根据运行时统计信息优化查询计划。这可以在查询执行期间减轻小文件的影响。

+   **数据湖架构变更**: 重新评估数据湖架构并采用最小化小文件创建的数据摄取策略，可以从源头上防止问题。

让我们来看看处理小文件的最佳实践：

+   **定期监控和清理**: 实施定期监控和清理流程，以识别和合并随着时间的推移生成的小文件

+   **优化存储布局**: 设计数据存储布局，在考虑块大小和文件系统设置等因素的同时，最大限度地减少小文件的创建。

+   **自动化流程**: 使用自动化流程或工具来有效地合并和管理小文件，减少人工工作量。

+   **教育数据生产者**: 教育数据生产者关于小文件的影响，并鼓励生成更大文件或优化文件创建的实践。

通过采用这些策略和最佳实践，组织可以有效地缓解 Apache Spark 中的小文件问题，确保性能提升、资源利用增强以及高效的数据处理能力。这些方法赋予用户克服小文件问题带来的挑战，并优化其 Spark 工作流程以实现最佳性能和可扩展性。

接下来，我们将看到数据倾斜如何影响 Spark 的性能。

## 解决 Apache Spark 中的数据倾斜问题

**数据倾斜**在分布式数据处理框架（如 Apache Spark）中是一个重大挑战，导致工作负载分布不均并阻碍并行性。

当某些键或分区持有的数据量显著多于其他分区时，就会发生数据倾斜。这种不平衡导致不同分区的处理时间不均，造成延迟。倾斜的数据分布可能导致某些工作节点过载，而其他节点利用率不足，导致资源分配效率低下。处理倾斜数据分区的任务需要更长的时间完成，从而造成作业执行延迟并影响整体性能。

这里有一些我们可以用来解决数据倾斜的解决方案：

+   **分区技术**:

    +   **Salting**: 通过向键添加盐来引入随机性，以更均匀地分布数据到分区中。这有助于防止热点并平衡工作负载。

    +   **自定义分区**: 通过不同地分组键来实现自定义分区逻辑，以重新分配倾斜数据，确保分区之间分布更加平衡。

+   **倾斜感知算法**: 利用诸如倾斜连接优化等技术，这些技术将倾斜键与常规连接分开处理，更有效地重新分配和处理它们。

+   **复制小倾斜数据**: 在多个节点上复制小倾斜分区，以并行处理并减轻单个节点的负载

+   **AQE**：利用 Spark 的 AQE 功能根据运行时统计信息动态调整执行计划，减轻数据偏差的影响

+   **采样和过滤**：应用采样和过滤技术事先识别偏差数据分区，允许在处理过程中主动处理偏差键

+   **动态资源分配**：实现动态资源分配，为处理偏差数据分区的任务分配额外资源，优化资源利用

让我们讨论处理数据偏差的最佳实践：

+   **定期分析**：持续分析并监控数据分布，以在处理管道早期识别并解决偏差问题

+   **优化分区**：根据数据特性选择合适的分区策略，以防止或减轻数据偏差

+   **分布式处理**：利用分布式处理框架将偏差数据分布到多个节点以实现并行执行

+   **任务重试机制**：为处理偏差数据的任务实现重试机制，以适应潜在的延迟并避免作业失败

+   **数据预处理**：在数据处理前应用预处理技术以减轻偏差，确保更均衡的工作负载

通过采用这些策略和最佳实践，组织可以有效应对 Apache Spark 中的数据偏差，确保更均衡的工作负载、改进的资源利用，并在分布式数据处理工作流程中提高整体性能。这些方法赋予用户克服数据偏差带来的挑战，并优化 Spark 应用以实现高效和可扩展的数据处理。

在 Apache Spark 中解决数据偏差对于优化性能和确保分布式计算环境中的资源高效利用至关重要。通过理解数据偏差的原因和影响，并采用缓解策略，用户可以显著提高 Spark 作业的效率和可靠性，减轻数据偏差的负面影响。

在下一节中，我们将讨论 Spark 中的数据溢出以及如何管理它们。

## 在 Apache Spark 中管理数据溢出

数据溢出，这是在 Apache Spark 等分布式处理框架中经常遇到的问题，当正在处理的数据超过可用内存容量时发生，导致数据被写入磁盘。这种现象可能会显著影响性能和整体效率。在本节中，我们将深入探讨 Spark 中数据溢出的影响以及缓解其影响的有效策略，以优化数据处理。

数据溢出发生在 Spark 的内存容量超出时，导致过多的数据写入磁盘操作，这比内存操作慢得多。将数据写入磁盘会产生高 I/O 开销，由于延迟增加，导致处理性能显著下降。数据溢出可能导致资源竞争，因为磁盘操作与其他计算任务竞争，导致资源利用效率低下。

这里是我们可以实施的解决数据溢出的一些方案：

+   **内存管理技术**：

    +   **增加执行器内存**：为 Spark 执行器分配更多内存可以帮助减少数据溢出的可能性，通过在内存中容纳更大的数据集来实现

    +   **调整内存配置**：优化 Spark 的内存配置，例如调整存储和执行内存的分数，以更好地利用可用内存

+   **分区和缓存策略**：

    +   **重新分区**：将数据重新分区到最佳分区数，可以帮助管理内存使用并最小化数据溢出，确保节点间更好的数据分布

    +   **缓存中间结果**：在内存中缓存或持久化中间数据集可以防止重复计算，并减少后续操作中数据溢出的可能性

+   **高级优化技术**：

    +   **Shuffle 调整**：通过调整如 shuffle 分区和缓冲区大小等参数来调整 shuffle 操作，以降低在 shuffle 阶段数据溢出的可能性

    +   **数据压缩**：在内存或磁盘上存储中间数据时利用数据压缩技术，以减少存储占用并缓解内存压力

+   **AQE**：利用 Spark 的 AQE 能力，根据运行时统计信息动态调整执行计划，优化内存使用并减少溢出。

+   **处理任务和数据倾斜**：应用技术来减轻任务和数据倾斜。倾斜的数据会加剧内存压力并增加数据溢出的可能性。

这里是处理数据溢出的最佳实践：

+   **资源监控**：定期监控内存使用和资源分配，以识别和预防潜在的数据溢出问题

+   **优化数据结构**：利用优化的数据结构和格式（如 Parquet 或 ORC）以减少内存开销和存储需求

+   **高效的缓存策略**：策略性地缓存或持久化中间结果，以最小化重复计算并降低数据溢出的概率

+   **增量处理**：采用增量处理技术，以可管理的块处理大数据集，减少内存压力

通过采用这些策略和最佳实践，组织可以有效管理 Apache Spark 中的数据溢出，确保高效内存利用、优化处理性能，并在分布式数据处理工作流程中提高整体可伸缩性。这些方法使用户能够主动解决数据溢出挑战，并优化 Spark 应用程序以提高效率和性能。

在下一节中，我们将讨论什么是数据洗牌以及如何处理它以优化性能。

## 在 Apache Spark 中管理数据洗牌

数据洗牌，在 Apache Spark 等分布式处理框架中的基本操作，涉及在集群节点间移动数据。虽然洗牌操作对于各种转换，如连接和聚合，是必不可少的，但它们也可能引入性能瓶颈和资源开销。在本节中，我们将探讨 Spark 中数据洗牌的影响以及优化和减轻其影响的有效策略，以实现高效的数据处理。

数据洗牌涉及大量的网络和磁盘 I/O 操作，导致延迟增加和资源利用率提高。在节点间移动大量数据可能会因为数据移动和处理过多而引入性能瓶颈。密集的洗牌操作可能导致节点间的资源竞争，影响整体集群性能。

让我们讨论优化数据洗牌的解决方案：

+   **数据分区技术**：实施优化的数据分区策略以减少洗牌开销，确保更平衡的工作负载分配

+   **倾斜处理**：通过采用盐分或自定义分区等技术来减轻数据倾斜，防止热点并平衡数据分布

+   **洗牌分区调整**：根据数据特性和作业要求调整洗牌分区的数量，以优化洗牌性能并减少开销

+   **内存管理**：优化洗牌操作的内存分配，以最小化数据溢出到磁盘并提高整体洗牌性能

+   **数据过滤和修剪**：应用过滤或修剪技术以减少节点间洗牌的数据量，仅关注相关数据子集

+   **连接优化**：

    +   **广播连接**：对于较小的数据集，利用广播连接在节点间复制数据，最小化数据洗牌并提高连接性能

    +   **排序合并连接**：对于大型数据集，采用排序合并连接算法以最小化连接操作中的数据移动

+   **AQE**：利用 Spark 的 AQE 功能，根据运行时统计和数据分布动态优化洗牌操作

管理数据洗牌的最佳实践如下：

+   **分析和监控**：持续分析和监控洗牌操作，以识别瓶颈并优化配置

+   **优化的分区大小**: 根据数据特性确定最佳分区大小，并相应地调整 shuffle 分区

+   **缓存和持久化**: 缓存或持久化中间 shuffle 结果以减少重复计算并减轻 shuffle 开销

+   **常规调优**: 根据工作负载需求和集群资源，定期调整与 shuffle 操作相关的 Spark 配置

通过实施这些策略和最佳实践，组织可以有效地优化 Apache Spark 中的数据 shuffle 操作，确保性能提升、减少资源竞争，并提高分布式数据处理工作流程的整体效率。这些方法赋予用户主动管理和优化 shuffle 操作的能力，以实现数据处理的简化并提高集群性能。

尽管用户需要了解所有与数据相关的挑战，但 Spark 在其内部工作中有某些类型的 join 可供我们利用以获得更好的性能。我们将接下来查看这些内容。

Shuffle 和广播 join

Apache Spark 提供了两种基本方法来执行 join 操作：shuffle join 和广播 join。每种方法都有其优势和用例，了解何时使用它们对于优化 Spark 应用程序至关重要。请注意，这些 join 操作是由 Spark 自动执行的，以将不同的数据集连接在一起。您可以在代码中强制执行某些 join 类型，但 Spark 负责执行。

## Shuffle joins

Shuffle join 是分布式计算环境中连接大型数据集的常用方法。这些 join 将数据重新分配到分区中，确保匹配的键最终位于同一 worker 节点上。Spark 通过其底层执行引擎高效地执行 shuffle join。

下面是 shuffle join 的一些关键特性：

+   **数据重分布**: Shuffle join 重新分配数据以确保具有匹配键的行位于同一 worker 节点上。此过程可能需要大量的网络和磁盘 I/O。

+   **适用于大型数据集**: Shuffle join 非常适合连接大小相当的大型 DataFrame。

+   **数据复制**: 在 shuffle join 过程中，数据可能会在 worker 节点上临时复制，以方便高效地执行 join 操作。

+   **网络和磁盘 I/O 成本高昂**: 由于数据 shuffle，shuffle join 可能非常消耗资源，这使得它们与其他 join 技术相比在处理较小数据集时速度较慢。

+   **示例**: 内连接、左连接、右连接和全外连接通常作为 shuffle join 实现。

### 用例

Shuffle joins 通常用于连接两个大小没有显著差异的大型 DataFrame。

## Shuffle sort-merge joins

Shuffle sort-merge join 是一种 shuffle join，它利用排序和合并技术的组合来执行 join 操作。它根据 join 键对两个 DataFrame 进行排序，然后高效地合并它们。

下面是洗牌排序合并连接的一些关键特性：

+   **数据排序**：洗牌排序合并连接在两边对数据进行排序，以确保有效的合并

+   **适用于大型数据集**：它们在连接具有倾斜数据分布的大型 DataFrame 时效率很高

+   **复杂性**：这种类型的洗牌连接比简单的洗牌连接更复杂，因为它涉及到排序操作

### **用例**

洗牌排序合并连接对于大规模连接非常有效，尤其是在数据分布不均匀时，确保数据在分区之间平衡分布至关重要。

让我们来看看广播连接。

## 广播连接

广播连接是将小 DataFrame 与较大 DataFrame 连接的一种高度有效技术。在这种方法中，较小的 DataFrame 被广播到所有工作节点，消除了在网络中洗牌数据的需求。广播连接是一种特定的优化技术，可以在其中一个 DataFrame 足够小，可以放入内存时应用。在这种情况下，小 DataFrame 被广播到所有工作节点，避免了昂贵的洗牌。

让我们来看看广播连接的一些关键特性：

+   **小 DataFrame 广播**：较小的 DataFrame 被广播到所有工作节点，确保它可以在本地使用

+   **减少网络开销**：广播连接显著减少了网络和磁盘 I/O，因为它们避免了数据洗牌

+   **适用于维度表**：广播连接在将事实表与较小的维度表连接时常用，例如在数据仓库场景中

+   **适用于小到大型连接**：它们在其中一个 DataFrame 远小于另一个 DataFrame 的连接操作中效率很高

### **用例**

当你将一个大型 DataFrame 与一个远小于它的 DataFrame 连接时，广播连接非常有用，例如在数据仓库中将事实表与维度表连接。

## 广播哈希连接

广播哈希连接是一种特定的广播连接。在这种情况下，较小的 DataFrame 被广播为一个哈希表到所有工作节点，这允许在大 DataFrame 中进行有效的查找。

### **用例**

**广播哈希连接适用于以下场景**：其中一个 DataFrame 足够小，可以广播，并且需要执行基于等式的连接。

在本节中，我们讨论了 Spark 中的两种基本连接技术——洗牌连接和广播连接——包括特定的变体，如广播哈希连接和洗牌排序合并连接。选择正确的连接方法取决于你的 DataFrame 的大小、数据分布和网络考虑因素，并且做出明智的决定对于优化你的 Spark 应用程序至关重要。在下一节中，我们将介绍 Spark 中存在的不同类型的转换。

# Apache Spark 中的窄和宽转换

如*第三章*所述，变换是处理数据的核心操作。变换分为两大类：窄变换和宽变换。理解这两种变换之间的区别对于优化 Spark 应用程序的性能至关重要。

## 窄变换

窄变换是一种不需要数据洗牌或跨分区进行大量数据移动的操作。它们可以在单个分区上执行，无需与其他分区通信。这种固有的局部性使得窄变换非常高效且执行速度快。

以下是一些窄变换的关键特性：

+   **单分区处理**：窄变换独立地对数据的单个分区进行操作，这最小化了通信开销。

+   **速度和效率**：由于它们的分区特性，窄变换既快又高效。

`map()`、`filter()`、`union()` 和 `groupBy()` 是窄变换的典型例子。

## 宽变换

相比之下，宽变换涉及数据洗牌，这需要分区之间的数据交换。这些变换需要多个分区之间的通信，并且可能非常资源密集。因此，它们通常执行速度较慢，在计算方面成本更高。

下面是宽变换的一些关键特性：

+   **数据洗牌**：宽变换涉及跨分区重新组织数据，需要不同工作者之间的数据交换。

+   **执行速度较慢**：由于需要洗牌，宽变换相对于窄变换来说，执行速度较慢且资源密集。

`groupByKey()`、`reduceByKey()` 和 `join()` 是宽变换的常见例子。

让我们讨论根据操作选择哪种变换效果最好。

## 选择窄变换和宽变换

选择合适的变换类型取决于具体用例和现有数据。以下是选择窄变换和宽变换时的一些考虑因素：

+   **数据大小**：如果你的数据足够小，可以舒适地放入单个分区，那么使用窄变换是首选。这可以最小化与洗牌相关的开销。

+   **数据分布**：如果你的数据在分区之间分布不均匀，可能需要宽变换来重新组织和平衡数据。

+   **性能**：窄变换通常更快、更高效，因此如果性能是关键关注点，则更倾向于选择它们。

+   **复杂操作**：一些操作，例如连接大型 DataFrame，通常需要宽变换。在这种情况下，性能权衡是不可避免的。

+   **集群资源**：考虑可用的集群资源。资源密集型的广泛转换可能导致共享集群中的资源争用。

接下来，我们将学习如何在必要时优化广泛转换。

## 优化广泛转换

虽然对于某些操作来说，广泛的转换是必要的，但优化它们以减少对性能的影响至关重要。以下是一些优化广泛转换的策略：

+   **最小化数据洗牌**：尽可能使用技术来最小化数据洗牌。例如，考虑使用广播连接来处理小的数据框。

+   **分区**：仔细选择分区数量和分区键以确保数据均匀分布，减少大量洗牌的需求。

+   **缓存和持久化**：缓存频繁使用的 DataFrame 可以帮助减少后续阶段中重新计算和洗牌的需求。

+   **调整集群资源**：调整集群配置，例如执行器和内存分配的数量，以满足广泛转换的需求。

+   **分析和监控**：定期分析和监控您的 Spark 应用程序以识别性能瓶颈，尤其是在广泛转换的情况下。

在本节中，我们探讨了 Apache Spark 中窄转换和广泛转换的概念。理解何时以及如何使用这些转换对于优化 Spark 应用程序的性能至关重要，尤其是在处理大型数据集和复杂操作时。

在下一节中，我们将介绍 Spark 中的持久化和缓存操作。

# Spark 中的持久化和缓存

在 Apache Spark 中，优化数据处理操作的性能至关重要，尤其是在处理大型数据集和复杂工作流时。缓存和持久化是允许您在内存或磁盘上存储中间或频繁使用数据的技术，从而减少重新计算的需求并提高整体性能。本节探讨了 Spark 中持久化和缓存的概念。

## 理解数据持久化

数据持久化是将 Spark 转换的中间或最终结果存储在内存或磁盘上的过程。通过持久化数据，您可以减少从源数据重新计算的需求，从而提高查询性能。

以下关键概念与数据持久化相关：

+   **存储级别**：Spark 提供多种数据存储级别，从仅内存到磁盘，根据您的需求而定。每个存储级别都带来了速度和持久性方面的权衡。

+   **延迟评估**：Spark 遵循延迟评估模型，这意味着转换只有在调用操作时才会执行。数据持久化确保中间结果可用于重用，而无需重新计算。

+   **缓存与持久化**：缓存是一种特定形式的数据持久化，它将数据存储在内存中，而持久化则包括内存和磁盘上的存储。

## 缓存数据

缓存是一种数据持久化形式，它将 DataFrame、RDD 或数据集存储在内存中以实现快速访问。它是一种重要的优化技术，可以提高 Spark 应用程序的性能，尤其是在处理迭代算法或重复计算时。

要缓存 DataFrame 或 RDD，您可以在指定存储级别时使用`.cache()`或`.persist()`方法：

+   `.cache()`或`.persist(StorageLevel.MEMORY_ONLY)`.

+   `.persist(StorageLevel.MEMORY_ONLY_SER)`.

+   `.persist(StorageLevel.MEMORY_AND_DISK)`.

+   `.persist(StorageLevel.DISK_ONLY)`.

缓存在以下场景中特别有益：

+   **迭代算法**：缓存对于迭代算法至关重要，如机器学习、图处理和优化问题，在这些算法中，相同的数据被反复使用

+   **多个操作**：当 DataFrame 用于多个操作时，在第一次操作后缓存它可以提高性能

+   **避免重复计算**：缓存有助于避免在多个转换依赖于它时重复计算相同的数据

+   **交互式查询**：在交互式数据探索或查询中，缓存常用的中间结果可以加快临时分析

## 取消持久化数据

缓存会消耗内存，在集群环境中，高效管理内存至关重要。您可以使用`.unpersist()`方法从内存中释放缓存数据。此方法允许您指定是立即释放数据还是仅在不再需要时释放。

下面是一个取消持久化数据的示例：

```py
# Cache a DataFrame
df.cache()
# Unpersist the cached DataFrame
df.unpersist()
```

## 最佳实践

要在您的 Spark 应用程序中有效地使用缓存和持久化，请考虑以下最佳实践：

+   **仅缓存必要的数据**：缓存会消耗内存，因此仅缓存频繁使用或计算成本高的数据

+   **监控内存使用**：定期监控内存使用情况，以避免内存耗尽或磁盘溢出

+   **自动化取消持久化**：如果您有有限的内存资源，请自动化较少使用的数据的取消持久化，以释放内存用于更关键的操作

+   **考虑序列化**：根据您的用例，考虑使用序列化存储级别以减少内存开销

在本节中，我们探讨了 Apache Spark 中持久化和缓存的概念。缓存和持久化是优化 Spark 应用程序性能的强大技术，尤其是在处理迭代算法或重复使用相同数据的场景中。了解何时以及如何使用这些技术可以显著提高您数据处理工作流程的效率。

在下一节中，我们将学习如何在 Spark 中实现分区和合并。

# Apache Spark 中的分区和合并

在 Apache Spark 中优化数据处理工作流程时，高效的数据分区起着至关重要的作用。重新分区和合并是允许您控制数据在分区间分布的操作。在本节中，我们将探讨重新分区和合并的概念及其在 Spark 应用程序中的重要性。

## 理解数据分区

Apache Spark 中的数据分区涉及将数据集划分为更小、更易于管理的单元，称为分区。每个分区包含数据的一个子集，并由分布式集群中不同的工作节点独立处理。适当的数据分区可以显著影响 Spark 应用程序的效率和性能。

## 重新分区数据

重新分区是将数据重新分配到不同数量的分区中的过程。这个操作可以帮助平衡数据分布，提高并行性，并优化数据处理。您可以使用`.repartition()`方法来指定所需的分区数。

下面是关于重新分区数据的一些关键点：

+   **增加或减少分区**：重新分区允许您根据处理需求增加或减少分区数。

+   **数据洗牌**：重新分区通常涉及数据洗牌，这可能非常消耗资源。因此，应谨慎使用。

+   **均匀数据分布**：当原始数据在分区中分布不均匀，导致工作负载倾斜时，重新分区很有用。

+   **优化连接操作**：在执行连接操作时，重新分区可以有益于最小化数据洗牌。

下面是重新分区数据的示例：

```py
# Repartition a DataFrame into 8 partitions
df.repartition(8)
```

## 合并数据

合并是在保持数据局部性的同时减少分区数量的过程。它比重新分区更高效，因为它尽可能避免不必要的洗牌操作。您可以使用`.coalesce()`方法来指定目标分区数。

下面是关于合并数据的一些关键点：

+   **减少分区**：当您想减少分区数量以优化数据处理时，使用合并。

+   **最小化数据移动**：与重新分区不同，合并通过尽可能在本地合并分区来最小化洗牌操作。

+   **高效的数据减少**：当您需要减少分区数量而不承担数据洗牌的全部成本时，合并操作是高效的。

下面是合并数据的示例：

```py
# Coalesce a DataFrame to 4 partitions
df.coalesce(4)
```

## 重新分区和合并的使用案例

理解何时重新分区和合并对于优化您的 Spark 应用程序至关重要。

以下是一些重新分区的用例：

+   **数据倾斜**：当数据在分区中倾斜时，重新分区可以帮助平衡工作负载。

+   **连接优化**：通过确保连接键是本地化的来优化连接操作。

+   **并行度控制**：调整并行度以优化资源利用率。

现在，让我们看看合并的一些用例：

+   **减少数据**: 当你需要减少分区数量以节省内存和降低开销时

+   **最小化洗牌**: 为了避免不必要的洗牌并最小化网络通信

+   **后过滤**: 在应用过滤器或转换显著减少数据集大小之后

## 最佳实践

为了在 Spark 应用程序中有效地重新分区和合并，请考虑以下最佳实践：

+   **分析和监控**: 分析你的应用程序以识别与数据分区相关的性能瓶颈。使用 Spark 的 UI 和监控工具跟踪数据洗牌。

+   **考虑数据大小**: 在决定分区数量时，考虑你的数据集大小和可用的集群资源。

+   **平衡工作负载**: 旨在使分区之间的工作负载分布平衡，以优化并行性。

+   **尽可能合并**: 在减少分区数量时，优先选择合并而不是重新分区，以最小化数据洗牌。

+   **规划连接**: 在执行连接操作时，规划最佳分区数量以最小化洗牌开销。

在本节中，我们探讨了 Apache Spark 中的重新分区和合并的概念。了解如何有效地控制数据分区可以显著影响 Spark 应用程序的性能，尤其是在处理大型数据集和复杂操作时。

# 摘要

在本章中，我们深入探讨了 Apache Spark 的高级数据处理功能，增强了你对关键概念和技术理解。我们探讨了 Spark 的 Catalyst 优化器的复杂性，不同类型 Spark 连接的力量，数据持久化和缓存的重要性，窄转换和宽转换的意义，以及使用重新分区和合并进行数据分区的作用。此外，我们还发现了 UDFs 的灵活性和实用性。

随着你使用 Apache Spark 的旅程不断深入，这些高级功能将证明对优化和定制数据处理工作流程非常有价值。通过利用 Catalyst 优化器的潜力，你可以微调查询执行以改善性能。了解 Spark 连接的细微差别使你能够针对特定用例做出明智的决定选择哪种类型的连接。当你寻求减少重复计算和加速迭代过程时，数据持久化和缓存变得不可或缺。

窄转换和宽转换在 Spark 应用程序中实现所需的并行性和资源效率中起着关键作用。通过重新分区和合并进行适当的数据分区确保了平衡的工作负载和最佳的数据分布。

UDFs（用户定义函数）打开了无限可能的大门，使你能够实现自定义数据处理逻辑，从数据清洗和特征工程到复杂计算和特定领域的操作。然而，明智地使用 UDFs 至关重要，优化它们以获得性能，并遵循最佳实践。

通过本章的知识，你将更好地应对 Apache Spark 中的复杂数据处理挑战，使你能够高效且有效地从数据中提取有价值的见解。这些高级功能使你能够充分利用 Spark 的潜力，并在数据驱动的努力中实现最佳性能。

在下一章中，我们将介绍 SparkSQL，并学习如何在 Spark 中创建和操作 SQL 查询。

# 样题问题

**问题 1:**

以下哪个代码块返回了一个 DataFrame，显示了按`department`列分组的`df` DataFrame 中`salary`列的平均值？

1.  `df.groupBy("department").agg(avg("salary"))`

1.  `df.groupBy(col(department).avg())`

1.  `df.groupBy("department").avg(col("salary"))`

1.  `df.groupBy("department").agg(average("salary"))`

**问题 2:**

以下哪个代码块返回了`df`中`state`和`department`列的所有唯一值？

1.  `df.select(state).join(transactionsDf.select('department'), col(state) == col('department'), 'outer').show()`

1.  `df.select(col('state'), col('department')).agg({'*': 'count'}).show()`

1.  `df.select('state', 'department').distinct().show()`

1.  `df.select('state').union(df.select('department')).distinct().show()`

## 答案

1.  A

1.  D
