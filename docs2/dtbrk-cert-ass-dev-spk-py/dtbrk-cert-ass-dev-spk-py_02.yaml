- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Understanding Apache Spark and Its Applications
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Apache Spark及其应用
- en: With the advent of machine learning and data science, the world is seeing a
    paradigm shift. A tremendous amount of data is being collected every second, and
    it’s hard for computing power to keep up with this pace of rapid data growth.
    To make use of all this data, Spark has become a de facto standard for big data
    processing. Migrating data processing to Spark is not only a question of saving
    resources that will allow you to focus on your business; it’s also a means of
    modernizing your workloads to leverage the capabilities of Spark and the modern
    technology stack to create new business opportunities.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习和数据科学的兴起，世界正在经历一个范式转变。每秒钟都在收集大量的数据，而计算能力难以跟上这种快速数据增长的速度。为了利用所有这些数据，Spark已经成为大数据处理的事实标准。将数据处理迁移到Spark不仅是一个节省资源的问题，让你能专注于你的业务；它也是一种现代化你的工作负载，利用Spark的能力和现代技术堆栈来创造新的商业机会的手段。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: What is Apache Spark?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是Apache Spark？
- en: Why choose Apache Spark?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么选择Apache Spark？
- en: Different components of Spark
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark的不同组件
- en: What are the Spark use cases?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark有哪些用例？
- en: Who are the Spark users?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark的用户是谁？
- en: What is Apache Spark?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是Apache Spark？
- en: Apache Spark is an open-source big data framework that is used for multiple
    big data applications. The strength of Spark lies in its superior parallel processing
    capabilities that makes it a leader in its domain.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个开源的大数据框架，用于多个大数据应用。Spark的强大之处在于其卓越的并行处理能力，使其在其领域成为领导者。
- en: According to its website ([https://spark.apache.org/](https://spark.apache.org/)),
    “*The most widely-used engine for* *scalable computing.*”
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其网站([https://spark.apache.org/](https://spark.apache.org/))，“*最广泛使用的可扩展计算引擎*”
- en: The history of Apache Spark
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Spark的历史
- en: Apache Spark started as a research project at the UC Berkeley AMPLab in 2009
    and moved to an open source license in 2010\. Later, in 2013, it came under the
    Apache Software Foundation ([https://spark.apache.org/](https://spark.apache.org/)).
    It gained popularity after 2013, and today, it serves as a backbone for a large
    number of big data products across various Fortune 500 companies and has thousands
    of developers actively working on it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark始于2009年在加州大学伯克利分校AMPLab的一个研究项目，并于2010年转为开源许可。后来，在2013年，它成为Apache软件基金会的一部分([https://spark.apache.org/](https://spark.apache.org/))。它在2013年之后获得了流行，如今，它已成为众多财富500强公司大数据产品的支柱，有成千上万的开发者正在积极为其工作。
- en: Spark came into being because of limitations in the Hadoop MapReduce framework.
    MapReduce’s main premise was to read data from disk, distribute that data for
    parallel processing, apply map functions to the data, and then reduce those functions
    and save them back to disk. This back-and-forth reading and saving to disk becomes
    time-consuming and costly very quickly.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的出现是由于Hadoop MapReduce框架的限制。MapReduce的主要前提是从磁盘读取数据，将数据分发以进行并行处理，对数据应用map函数，然后将这些函数减少并保存回磁盘。这种来回读取和保存到磁盘的过程很快就会变得耗时且成本高昂。
- en: To overcome this limitation, Spark introduced the concept of in-memory computation.
    On top of that, Spark has several capabilities that came as a result of different
    research initiatives. You will read more about them in the next section.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一限制，Spark引入了内存计算的概念。除此之外，Spark还拥有由不同研究倡议带来的几个能力。你将在下一节中了解更多关于它们的信息。
- en: Understanding Spark differentiators
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解Spark的不同之处
- en: Spark’s foundation lies in its major capabilities such as in-memory computation,
    lazy evaluation, fault tolerance, and support for multiple languages such as Python,
    SQL, Scala, and R. We will discuss each one of them in detail in the following
    section.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的基础在于其主要能力，如内存计算、延迟评估、容错性和支持Python、SQL、Scala和R等多种语言。我们将在下一节中详细讨论每一个。
- en: Let’s start with in-memory computation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从内存计算开始。
- en: In-memory computation
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存计算
- en: The first major differentiator technology that Spark’s foundation is built on
    is that it utilizes in-memory computations. Remember when we discussed Hadoop
    MapReduce technology? One of its major limitations is to write back to disk at
    each step. Spark saw this as an opportunity for improvement and introduced the
    concept of in-memory computation. The main idea is that the data remains in memory
    as long as it is worked on. If we can work with the size of data that can be stored
    in the memory at once, we can eliminate the need to write to disk at each step.
    As a result, the complete computation cycle can be done in memory if we can work
    with all computations on that amount of data. Now, the thing to note here is that
    with the advent of big data, it’s hard to contain all the data in memory. Even
    if we look at heavyweight servers and clusters in the cloud computing world, memory
    remains finite. This is where Spark’s internal framework of parallel processing
    comes into play. Spark framework utilizes the underlying hardware resources in
    the most efficient manner. It distributes the computations across multiple cores
    and utilizes the hardware capabilities to the maximum.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Spark基础构建的第一个主要区分性技术是它利用内存计算。记得我们讨论Hadoop MapReduce技术时吗？它的一个主要限制是在每个步骤都将数据写回磁盘。Spark将其视为改进的机会，并引入了内存计算的概念。主要思想是数据在处理过程中始终保持在内存中。如果我们能够处理一次性存储在内存中的数据量，我们就可以消除在每个步骤中写入磁盘的需要。因此，如果我们能够处理所有计算的数据量，整个计算周期就可以在内存中完成。现在，需要注意的是，随着大数据的出现，很难将所有数据都包含在内存中。即使我们在云计算世界的重型服务器和集群中看，内存仍然是有限的。这就是Spark并行处理内部框架发挥作用的地方。Spark框架以最有效的方式利用底层硬件资源。它将计算分布在多个核心上，并充分利用硬件能力。
- en: This tremendously reduces the computation time, since the overhead of writing
    to disk and reading it back for the subsequent step is minimized as long as the
    data can be fit in the memory of Spark compute.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这极大地减少了计算时间，因为只要数据可以适应Spark计算内存，写入磁盘和读取回磁盘的开销就会最小化。
- en: Lazy evaluation
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 延迟评估
- en: 'Generally, when we work with programming frameworks, the backend compilers
    look at each statement and execute it. While this works great for programming
    paradigms, with big data and parallel processing, we need to shift to a look-ahead
    kind of model. Spark is well known for its parallel processing capabilities. To
    achieve even better performance, Spark doesn’t execute code as it reads it, but
    once the code is there and we submit a Spark statement to execute, the first step
    is that Spark builds a logical map of the queries. Once that map is built, then
    it plans what the best path of execution is. You will read more about its intricacies
    in the Spark architecture chapters. Once the plan is established, only then will
    the execution begin. Once the execution begins, even then, Spark holds off executing
    all statements until it hits an “action” statement. There are two types of statements
    in Spark:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当我们使用编程框架时，后端编译器会查看每个语句并执行它。虽然这对于编程范式来说效果很好，但在大数据和并行处理的情况下，我们需要转向一种前瞻性的模型。Spark因其并行处理能力而闻名。为了实现更好的性能，Spark不会在读取代码时执行代码，而是在代码存在并且我们提交Spark语句以执行时，第一步是Spark构建查询的逻辑映射。一旦这个映射建立，然后它会规划最佳的执行路径。你将在Spark架构章节中了解更多关于其复杂性的内容。一旦计划确定，执行才会开始。即使执行开始，Spark也会推迟执行所有语句，直到遇到一个“操作”语句。Spark中有两种类型的语句：
- en: Transformations
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换
- en: Actions
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作
- en: 'You will learn more about the different types of Spark statements in detail
    in [*Chapter 3*](B19176_03.xhtml#_idTextAnchor053), where we discuss Spark architecture.
    Here are a few advantages of lazy evaluation:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在[*第3章*](B19176_03.xhtml#_idTextAnchor053)中详细了解Spark语句的不同类型，其中我们讨论了Spark架构。以下是延迟评估的一些优点：
- en: Efficiency
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 效率
- en: Code manageability
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码可管理性
- en: Query and resource optimization
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询和资源优化
- en: Reduced complexities
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简化复杂性
- en: Resilient datasets/fault tolerance
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 弹性数据集/容错性
- en: Spark’s foundation is built on **resilient distributed datasets** (**RDDs**).
    It is an immutable distributed collection of objects that represent a set of records.
    RDDs are distributed across a number of servers, and they are computed in parallel
    across multiple cluster nodes. RDDs can be generated with code. When we read data
    from an external storage location into Spark, RDDs hold that data. This data can
    be shared across multiple clusters and can be computed in parallel, thus giving
    Spark a very efficient way of running computations on RDD data. RDDs are loaded
    in memory for processing; therefore, loading to and from memory computations is
    not required, unlike Hadoop.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的基础是建立在**弹性分布式数据集**（**RDDs**）之上的。它是一个不可变的分布式对象集合，代表了一组记录。RDDs分布在多个服务器上，它们在多个集群节点上并行计算。RDDs可以通过代码生成。当我们从外部存储位置读取数据到Spark中时，RDDs会保存这些数据。这些数据可以在多个集群之间共享，并且可以并行计算，从而为Spark提供了非常高效的在RDD数据上运行计算的方法。RDDs被加载到内存中进行处理；因此，与Hadoop不同，不需要将数据加载到和从内存中进行计算。
- en: RDDs are fault-tolerant. This means that if there are failures, RDDs have the
    ability to self-recover. Spark achieves that by distributing these RDDs to different
    worker nodes while keeping in view what task is performed by which worker node.
    This handling of worker nodes is done by the Spark driver. We will discuss this
    in detail in upcoming chapters.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: RDDs具有容错性。这意味着如果出现故障，RDDs有自我恢复的能力。Spark通过将这些RDD分布到不同的工作节点上，同时考虑到每个工作节点执行的任务，来实现这一点。这种对工作节点的处理由Spark驱动器完成。我们将在后续章节中详细讨论这一点。
- en: RDDs give a lot of power to Spark in terms of resilience and fault-tolerance.
    This capability, along with other features, makes Spark the tool of choice for
    any production-grade applications.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: RDDs在弹性和容错性方面赋予了Spark很大的能力。这种能力，结合其他特性，使得Spark成为任何生产级应用的工具选择。
- en: Multiple language support
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多语言支持
- en: Spark supports multiple languages for development such as Java, R, Scala, and
    Python. This gives users the flexibility to use any language of choice to build
    applications in Spark.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Spark支持多种开发语言，如Java、R、Scala和Python。这使用户能够灵活地使用任何选择的编程语言在Spark中构建应用程序。
- en: The components of Spark
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark的组件
- en: Let’s talk about the different components Spark has. As you can see in *Figure
    1**.1*, Spark Core is the backbone of operations in Spark and spans across all
    the other components that Spark has. Other components that we’re going to discuss
    in this section are Spark SQL, Spark Streaming, Spark MLlib, and GraphX.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来谈谈Spark的不同组件。正如你在*图1.1*中可以看到的，Spark Core是Spark操作的核心，横跨Spark的所有其他组件。本节我们将讨论的其他组件包括Spark
    SQL、Spark Streaming、Spark MLlib和GraphX。
- en: '![Figure 2.1: Spark components](img/B19176_02_01.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1：Spark组件](img/B19176_02_01.jpg)'
- en: 'Figure 2.1: Spark components'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：Spark组件
- en: Let’s look at the first component of Spark.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看Spark的第一个组件。
- en: Spark Core
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark Core
- en: Spark Core is central to all the other components of Spark. It provides functionalities
    and core features for all the different components. Spark SQL, Spark Streaming,
    Spark MLlib, and GraphX all make use of Spark Core as their base. All the functionality
    and features of Spark are controlled by Spark Core. It provides in-memory computing
    capabilities to deliver speed, a generalized execution model to support a wide
    variety of applications, and Java, Scala, and Python APIs for ease of development.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Core是Spark所有其他组件的核心。它为所有不同的组件提供功能和核心特性。Spark SQL、Spark Streaming、Spark
    MLlib和GraphX都使用Spark Core作为其基础。Spark的所有功能和特性都由Spark Core控制。它提供了内存计算能力以提供速度，一个通用的执行模型以支持广泛的各类应用，以及Java、Scala和Python
    API以简化开发。
- en: In all of these different components, you can write queries in supported languages.
    Spark will then convert these queries to **directed acyclic graphs** (**DAGs**),
    and Spark Core has the responsibility of executing them.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些不同的组件中，你可以使用支持的语言编写查询。然后Spark将这些查询转换为**有向无环图**（**DAGs**），而Spark Core负责执行它们。
- en: 'The key responsibilities of Spark Core are as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Core的关键职责如下：
- en: Interacting with storage systems
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与存储系统交互
- en: Memory management
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存管理
- en: Task distribution
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务分配
- en: Task scheduling
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务调度
- en: Task monitoring
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务监控
- en: In-memory computation
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存计算
- en: Fault tolerance
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容错性
- en: Optimization
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化
- en: Spark Core contains an API for RDDs which are an integral part of Spark. It
    also provides different APIs to interact and work with RDDs. All the components
    of Spark work with underlying RDDs for data manipulation and processing. RDDs
    make it possible for Spark to have a lineage for data, since they are immutable.
    This means that every time an operation is run on an RDD that requires changes
    in it, Spark will create a new RDD for it. Hence, it maintains the lineage information
    of RDDs and their corresponding operations.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Core包含一个用于RDDs的API，RDDs是Spark的组成部分。它还提供了不同的API来交互和工作与RDDs。Spark的所有组件都使用底层的RDDs进行数据处理。RDDs使得Spark能够拥有数据的历史记录，因为它们是不可变的。这意味着每次对RDD执行需要更改的操作时，Spark都会为它创建一个新的RDD。因此，它维护RDD及其对应操作的历史记录信息。
- en: Spark SQL
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark SQL
- en: SQL is the most popular language for database and data warehouse applications.
    Analysts use this language for all their exploratory data analysis on relational
    databases and their counterparts in traditional data warehouses. Spark SQL adds
    this advantage to the Spark ecosystem. Spark SQL is used to query structured data
    in SQL using the DataFrame API.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: SQL是数据库和数据仓库应用中最流行的语言。分析师使用这种语言进行所有基于关系数据库和传统数据仓库的探索性数据分析。Spark SQL将这一优势添加到Spark生态系统中。Spark
    SQL用于使用DataFrame API以SQL查询结构化数据。
- en: As its name represents, Spark SQL gives SQL support to Spark. This means we
    can query the data present in RDDs and other external sources, such as Parquet
    files. This is a powerful capability of Spark, since it gives developers the flexibility
    to use a relational table structure on top of RDDs and other file formats and
    write SQL queries on top of it. This also adds the capabilities of using SQL where
    necessary and unifies it with analytics applications and use cases, thus providing
    unification of the platforms.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名称所示，Spark SQL为Spark提供了SQL支持。这意味着我们可以使用DataFrame API查询RDD和其他外部源中的数据。这是Spark的一个强大功能，因为它为开发者提供了在RDD和其他文件格式之上使用关系表结构的灵活性，并允许在上面编写SQL查询。这也增加了在必要时使用SQL的能力，并将其与分析应用程序和用例统一，从而提供了平台的统一。
- en: 'With Spark SQL, developers are able to do the following with ease:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark SQL，开发者可以轻松完成以下操作：
- en: They can read data from different file formats and different sources into RDDs
    and DataFrames
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们可以从不同的文件格式和不同的来源读取数据到RDDs和DataFrames中
- en: They can run SQL queries on top of the data present in DataFrames, thus giving
    flexibility to the developers to use programming languages or SQL to process data
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们可以在DataFrame中的数据上运行SQL查询，从而为开发者提供使用编程语言或SQL处理数据的灵活性
- en: Once they’re done with the processing of the data, they have the capability
    to write RDDs and DataFrames to external sources
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦完成数据处理，他们就有能力将RDDs和DataFrames写入外部源
- en: Spark SQL consists of a cost-based optimizer that optimizes queries, keeping
    in view the resources; it also has the capability to generate code for these optimizations,
    which makes these queries very fast and efficient. To support even faster query
    times, it can scale to multiple nodes with the help of Spark Core and also provides
    features such as fault tolerance and resiliency. This is known as the Catalyst
    optimizer. We will read more about it in [*Chapter 5*](B19176_05.xhtml#_idTextAnchor115).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL包含一个基于成本的优化器，它优化查询，同时考虑资源；它还具有生成这些优化代码的能力，这使得这些查询非常快速和高效。为了支持更快的查询时间，它可以在Spark
    Core的帮助下扩展到多个节点，并提供诸如容错和弹性等特性。这被称为Catalyst优化器。我们将在[*第五章*](B19176_05.xhtml#_idTextAnchor115)中了解更多关于它。
- en: 'The most noticeable features of Sparks SQL are as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL最显著的特点如下：
- en: It provides an engine for high-level structured APIs
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了一个高级结构化API的引擎
- en: Reads/writes data to and from a large number of file formats such as Avro, Delta,
    **Comma-Separated Values** (**CSV**), and Parquet
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取/写入到和从大量文件格式，如Avro、Delta、**逗号分隔值**（**CSV**）和Parquet
- en: Provides **Open Database Connectivity** (**ODBC**)/**Java Database Connectivity**
    (**JDBC**) connectors to **business intelligence** (**BI**) tools such as PowerBI
    and Tableau, as well as popular **relational** **databases** (**RDBMs**)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供了**开放数据库连接**（**ODBC**）和**Java数据库连接**（**JDBC**）连接器，用于连接商业智能（**BI**）工具，如PowerBI和Tableau，以及流行的**关系数据库**（**RDBMs**）
- en: Provides a way to query structured data in files as tables and views
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供了一种将文件中的结构化数据查询为表和视图的方法
- en: It supports ANSI SQL:2003-compliant commands and HiveQL
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持符合ANSI SQL:2003命令和HiveQL
- en: Now that we have covered SparkSQL, let’s discuss the Spark Streaming component.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经涵盖了SparkSQL，让我们来讨论Spark Streaming组件。
- en: Spark Streaming
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: 'We have talked about the rapid growth of data in today’s times. If we were
    to divide this data into groups, there are two types of datasets in practice,
    batch and streaming:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了当今时代数据的快速增长。如果我们将这些数据分组，实际上有两种数据集类型，批处理和流式处理：
- en: '**Batch data** is when there’s a chunk of data present that you have to ingest
    and then transform all at once. Think of when you want to get a sales report of
    all the sales in a month. You would have the monthly data available as a batch
    and process it all at once.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批处理数据**是指存在一块数据，你必须一次性摄取并转换。想象一下，当你想要获取一个月内所有销售的报告时。你将拥有作为批处理的月度数据，并一次性处理它。'
- en: '**Streaming data** is when you need output of that data in real time. To serve
    this requirement, you would have to ingest and process that data in real time.
    This means every data point can be ingested as a single data element, and we would
    not wait for it to be ingested after a block of data is collected. Think of when
    self-driving cars need to make decisions in real time based on the data they collect.
    All the data needs to be ingested and processed in real time for the car to make
    effective decisions in a given moment.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流数据**是指你需要实时数据的输出。为了满足这一需求，你必须实时摄取和处理这些数据。这意味着每个数据点都可以作为一个单独的数据元素摄取，我们不会等待收集到数据块后再进行摄取。想象一下，自动驾驶汽车需要根据收集到的数据实时做出决策。所有数据都需要实时摄取和处理，以便汽车在特定时刻做出有效的决策。'
- en: There are a large number of industries generating streaming data. To make use
    of this data, you need real-time ingestion, processing, and management of this
    data. It has become essential for organizations to use streaming data as it arrives
    for real-time analytics and other use cases. This gives them an edge over their
    competitors, as this allows them to make decisions in real time.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多行业正在生成流数据。为了利用这些数据，你需要实时摄取、处理和管理这些数据。对于组织来说，使用流数据作为实时分析和其他用例已经成为一项基本要求。这使他们比竞争对手具有优势，因为这使他们能够实时做出决策。
- en: Spark Streaming enables organizations to make use of streaming data. One of
    the most important factors of Spark Streaming is its ease of use alongside batch
    data processing. You can combine batch and stream data within one framework and
    use it to augment your analytics applications. Spark Streaming also inherits Spark
    Core’s features of resilience and fault tolerance, giving it a dominant position
    in the industry. It integrates with a large number of streaming data sources such
    as HDFS, Kafka, and Flume.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming使组织能够利用流数据。Spark Streaming最重要的因素之一是它易于使用，并且可以与批处理数据一起使用。你可以在一个框架内结合批处理和流数据，并使用它来增强你的分析应用程序。Spark
    Streaming还继承了Spark Core的弹性和容错特性，使其在行业中占据主导地位。它集成了大量流数据源，如HDFS、Kafka和Flume。
- en: The beauty of Spark Streaming is that batch data can be processed as streams
    to take advantage of built-in paradigms of streaming data and look-back capabilities.
    There are certain factors that need to be taken into consideration when we work
    with real-time data. When we work with real-time data streams, there’s a chance
    that some of the data may get missed due to system hiccups or failures altogether.
    Spark Streaming takes care of this in a seamless way. To cater to these requirements,
    it has a built-in mechanism called **checkpoints**. The purpose of these checkpoints
    is to keep track of the incoming data, knowing what was processed downstream and
    which data is still left to be processed in the next cycle. We will learn more
    about this in [*Chapter 7*](B19176_07.xhtml#_idTextAnchor183) when we discuss
    Spark Streaming in more detail.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming的美丽之处在于，批处理数据可以作为流进行处理，以利用流数据的内置范式和回溯能力。当我们处理实时数据时，需要考虑某些因素。当我们处理实时数据流时，可能会因为系统故障或完全失败而错过一些数据。Spark
    Streaming以无缝的方式处理这个问题。为了满足这些需求，它有一个内置机制，称为**检查点**。这些检查点的目的是跟踪传入的数据，了解下游处理了什么数据，以及在下一次周期中还有哪些数据需要处理。我们将在详细讨论Spark
    Streaming的[第7章](B19176_07.xhtml#_idTextAnchor183)中了解更多关于这一点。
- en: This makes Spark resilient to failures. If there are any failures, you need
    minimal work to reprocess old data. You can also define mechanisms and algorithms
    for missing data or late processed data. This gives a lot of flexibility to the
    data pipelines and makes them easier to maintain in large production environments.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得Spark对故障具有弹性。如果有任何故障，您需要做最少的工作来重新处理旧数据。您还可以定义缺失数据或延迟处理数据的机制和算法。这为数据管道提供了很大的灵活性，并使它们在大规模生产环境中更容易维护。
- en: Spark MLlib
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark MLlib
- en: Spark provides a framework for distributed and scalable machine learning. It
    distributes the computations across different nodes, thus resulting in better
    performance for model training. It also distributes hyperparameter tuning. You
    will learn more about hyperparameter tuning in [*Chapter 8*](B19176_08.xhtml#_idTextAnchor220),
    where we talk about machine learning. Because Spark can scale to large datasets,
    it is the framework of choice for machine learning production pipelines. When
    you build products, execution and computation speed matter a lot. Spark gives
    you the ability to work with large amounts of data and build state-of-the-art
    machine learning models that can run very efficiently. Instead of working with
    models that take days to train, Spark reduces that time to hours. In addition,
    working with more data results in better-performing models in most cases.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了一个用于分布式和可扩展机器学习的框架。它将计算分布在不同的节点上，从而在模型训练方面实现更好的性能。它还分布了超参数调整。您将在[*第8章*](B19176_08.xhtml#_idTextAnchor220)中了解更多关于超参数调整的内容，我们将讨论机器学习。因为Spark可以扩展到大型数据集，所以它是机器学习生产管道的首选框架。当您构建产品时，执行和计算速度非常重要。Spark让您能够处理大量数据，并构建运行非常高效的先进机器学习模型。与需要数天训练的模型相比，Spark将时间缩短到数小时。此外，处理更多数据在大多数情况下会导致性能更好的模型。
- en: 'Most of the commonly used machine learning algorithms are part of Spark’s libraries.
    There are two machine learning packages available in Spark:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数常用的机器学习算法都是Spark库的一部分。Spark中有两个机器学习包可用：
- en: Spark MLlib
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark MLlib
- en: Spark ML
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark ML
- en: The major difference between these two is the type of data they work with. Spark
    MLlib is built on top of RDDs while Spark ML works with DataFrames. Spark MLlib
    is the older library and has now entered maintenance mode. The more up-to-date
    library is Spark ML. You should also note that Spark ML is not the official name
    of the library itself, but it is commonly used to refer to the DataFrame-based
    API in Spark. The official name is still Spark MLlib. However, it’s important
    to know the differences.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个之间的主要区别是它们处理的数据类型。Spark MLlib建立在RDD之上，而Spark ML与DataFrame一起工作。Spark MLlib是较旧的库，现在已进入维护模式。更先进的库是Spark
    ML。您还应注意，Spark ML不是库本身的官方名称，但它通常用于指代Spark中基于DataFrame的API。官方名称仍然是Spark MLlib。然而，了解这些差异是很重要的。
- en: Spark MLlib contains the most commonly used machine learning libraries for **classification**,
    **regression**, **clustering**, and **recommendation systems**. It also has some
    support for frequent pattern mining algorithms.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib包含了最常用的机器学习库，用于**分类**、**回归**、**聚类**和**推荐系统**。它还支持一些频繁模式挖掘算法。
- en: When there is a need to serve these models to millions and billions of users,
    Spark is also helpful. You can distribute and parallelize both data processing
    (**Extract, Transform, Load** (**ETL**)) and model scoring with Spark.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要将这些模型服务于数百万甚至数十亿用户时，Spark也非常有帮助。您可以使用Spark分发和并行化数据处理（**提取、转换、加载**（**ETL**））和模型评分。
- en: GraphX
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GraphX
- en: GraphX is Spark’s API for graphs and graph-parallel computation. GraphX extends
    Spark’s RDD to work with graphs and allows you to run parallel computations with
    graph objects. This speeds up the computations significantly.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: GraphX是Spark的图和图并行计算的API。GraphX扩展了Spark的RDD以支持图，并允许您使用图对象运行并行计算。这显著提高了计算速度。
- en: Here’s a network graph that represents what a graph looks like.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个表示图外观的网络图。
- en: '![Figure 2.2: A network graph](img/B19176_02_02.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图2.2：一个网络图](img/B19176_02_02.jpg)'
- en: 'Figure 2.2: A network graph'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：一个网络图
- en: A graph is an object with vertices and edges. Properties are attached to each
    vertex and edge. There are primary graph operations that Spark supports, such
    as `subgraph` and `joinVertices`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图是一个具有顶点和边的对象。属性附加到每个顶点和边上。Spark支持一些主要的图操作，例如`subgraph`和`joinVertices`。
- en: 'The main premise is that you can use GraphX for exploratory analysis and ETL
    and transform and join graphs with RDDs efficiently. There are two types of operator—
    `Graph` and `GraphOps`. On top of that, graph aggregation operators are also available.
    Spark also includes a number of graph algorithms that are used in common use cases.
    Some of the most popular algorithms are as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 主要前提是你可以使用GraphX进行探索性分析和ETL，并使用RDD高效地转换和连接图。有两种类型的操作符——`Graph`和`GraphOps`。在此基础上，还有图聚合操作符。Spark还包括许多在常见用例中使用的图算法。以下是一些最受欢迎的算法：
- en: PageRank
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PageRank
- en: Connected components
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连通分量
- en: Label propagation
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签传播
- en: SVD++
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVD++
- en: Strongly connected components
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强连通分量
- en: Triangle count
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三角形计数
- en: Now, let’s discuss why we want to use Spark in our applications and what some
    of the features it provides are.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论为什么我们想在应用中使用Spark以及它提供的一些特性。
- en: Why choose Apache Spark?
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择Apache Spark？
- en: In this section, we will discuss the applications of Apache Spark and its features,
    such as speed, reusability, in-memory computations, and how Spark is a unified
    platform.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论Apache Spark的应用及其特性，例如速度、可重用性、内存计算以及Spark是如何成为一个统一平台的。
- en: Speed
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 速度
- en: Apache Spark is one of the fastest processing frameworks for data available
    today. It beats Hadoop MapReduce by a large margin. The main reason is its in-memory
    computation capabilities and lazy evaluation. We will learn more about this when
    we discuss Spark architecture in the next chapter.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是目前可用的最快数据处理框架之一。它比Hadoop MapReduce快得多。主要原因在于其内存计算能力和延迟评估。我们将在下一章讨论Spark架构时了解更多关于这一点。
- en: Reusability
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可重用性
- en: Reusability is a very important consideration for large organizations making
    use of modern platforms. Spark can join batch and stream data seamlessly. Moreover,
    you can augment datasets with historical data to serve your use cases better.
    This gives a large historical view of data to run queries or build modern analytical
    systems.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 可重用性对于使用现代平台的大型组织来说是一个非常重要的考虑因素。Spark可以无缝地连接批处理和流数据。此外，你可以通过添加历史数据来增强数据集，以更好地满足你的用例。这为运行查询或构建现代分析系统提供了大量历史数据视图。
- en: In-memory computation
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存计算
- en: With in-memory computation, all the overhead of reading and writing to disks
    is eliminated. The data is cached, and at each step, the required data is already
    present in memory. At the end of the processing, results are aggregated and sent
    back to the driver for further steps.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用内存计算，消除了读取和写入磁盘的所有开销。数据被缓存，在每一步中，所需的数据已经存在于内存中。在处理结束时，结果被汇总并发送回驱动程序以进行后续步骤。
- en: All of this is facilitated by the process of DAG creation that Spark performs
    inherently. Before execution, Spark creates a DAG of the necessary steps and prioritizes
    them based on its internal algorithms. We will learn more about this in the next
    chapter. These capabilities support in-memory computation, resulting in fast processing
    speeds.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都得益于Spark固有的DAG创建过程。在执行之前，Spark创建必要的步骤的DAG并根据其内部算法对其进行优先排序。我们将在下一章中了解更多关于这一点。这些功能支持内存计算，从而实现快速处理速度。
- en: A unified platform
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统一平台
- en: Spark provides a unified platform for data engineering, data science, machine
    learning, analytics, streaming, and graph processing. All of these components
    are integrated with Spark Core. The core engine is very high-speed and generalizes
    the commonly needed tasks for its other components. This gives Spark an advantage
    over other platforms because of the unification of its different components. These
    components can work in conjunction with each other, providing a unified experience
    for software applications. In modern applications, this unification makes it easy
    to use, and different parts of the application can make use of the core capabilities
    of these components without compromising on features.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了一个统一的数据工程、数据科学、机器学习、分析、流处理和图处理平台。所有这些组件都与Spark Core集成。核心引擎非常高速，并概括了其其他组件所需的一些常用任务。这使得Spark在与其他平台相比时具有优势，因为其不同组件的统一。这些组件可以协同工作，为软件应用提供统一的体验。在现代应用中，这种统一使得使用变得容易，并且应用的不同部分可以充分利用这些组件的核心功能，而不会牺牲功能。
- en: Now that you understand the benefits of using Spark, let’s talk about the different
    use cases of Spark in the industry.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了使用Spark的好处，让我们来谈谈Spark在行业中的不同用例。
- en: What are the Spark use cases?
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark有哪些用例？
- en: In this section, we will learn about how Spark is used in the industry. There
    are various use cases of Spark prevalent today, some of which include big data
    processing, machine learning applications, near-real-time and real-time streaming,
    and using graph analytics.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解Spark在行业中的应用。目前Spark有各种用例，包括大数据处理、机器学习应用、近实时和实时流处理，以及使用图分析。
- en: Big data processing
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大数据处理
- en: One of the most popular use cases for Spark is big data processing. You might
    be wondering what big data is, so let’s take a look at the components that mark
    data as big data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Spark最流行的用例之一是大数据处理。你可能想知道什么是大数据，那么让我们来看看标记数据为大数据的组成部分。
- en: The first component of big data is the **volume of data**. By volume, we mean
    that the data is very large in size, often amounting to terabytes, petabytes,
    and beyond in some cases. Organizations have collected a large amount of data
    over the years. This data can be used for analysis. However, the first step in
    this activity is to process these large amounts of data. Also, it’s only recently
    that computing power has grown to now be able to process such vast volumes of
    data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据的第一个组成部分是**数据量**。按数据量来说，数据非常大，在某些情况下，数据量可能达到太字节、拍字节甚至更多。多年来，组织收集了大量的数据。这些数据可以用于分析。然而，在这个活动的第一步是处理这些大量的数据。此外，只有最近，计算能力才增长到能够处理如此庞大的数据量。
- en: The second component of big data is the **velocity of data**. The velocity of
    data refers to the speed of its generation, ingestion, and distribution. This
    means that the speed with which this data is generated has increased manifold
    in recent years. Take, for example, data generated by your smart appliance that
    sends data every second to a server. In this process, the server also needs to
    keep up with the ingestion of this data, and then distributing this across different
    sources might be the next step.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据的第二个组成部分是**数据速度**。数据速度指的是数据生成、摄取和分布的速度。这意味着近年来数据生成的速度已经大幅增加。以你的智能设备为例，它每秒向服务器发送数据。在这个过程中，服务器还需要跟上数据的摄取，然后可能需要将数据分布到不同的来源。
- en: The third component of big data is the **variety of data**. The variety of data
    refers to the different sources that generate the data. It also refers to different
    types of data that are generated. Gone are the days when data was only generated
    in structured formats that could be saved as tables in databases. Currently, data
    can be structured, semi-structured, or unstructured. The systems now have to work
    with all these different data types, and tools should be able to manipulate these
    different data types. Think of images that need to be processed or audio and video
    files that can be analyzed with advanced analytics.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据的第三个组成部分是**数据多样性**。数据多样性指的是生成数据的不同来源。它还指的是生成的不同类型的数据。那些只有结构化格式可以保存为数据库中的表的数据生成时代已经过去了。目前，数据可以是结构化的、半结构化的或非结构化的。现在的系统必须处理所有这些不同的数据类型，工具应该能够操作这些不同的数据类型。想想需要处理的照片或可以使用高级分析进行分析的音频和视频文件。
- en: Some other components can be added to the original three Vs as well, such as
    veracity and value. However, these components are out of the scope of our discussion.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以将其他一些组件添加到原始的三个V中，例如真实性和价值。然而，这些组件超出了我们讨论的范围。
- en: Big data is too large for regular machines to process it. That’s why it’s called
    big data. Big data that is high-volume, high-velocity, and high-variety needs
    to be processed with advanced analytical tools such as Spark, which can distribute
    the workload across different machines or clusters and does processing in parallel
    to make use of all the resources available on a machine. So, instead of using
    only a single machine and loading all the data into one node, Spark gives us the
    ability to divide the data up into different parts and process them in parallel
    and across different machines. This massively speeds up the whole process and
    makes use of all the available resources.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据太大，常规机器无法处理。这就是为什么它被称为大数据。高容量、高速度、高多样性的大数据需要使用像Spark这样的高级分析工具进行处理，Spark可以在不同的机器或集群之间分配工作负载，并并行处理以利用机器上的所有可用资源。因此，Spark使我们能够将数据分成不同的部分，并在不同的机器上并行处理。这极大地加快了整个过程，并利用了所有可用资源。
- en: For all of the aforementioned reasons, Spark is one of the most widely used
    big data processing technologies. Large organizations make use of Spark to analyze
    and manipulate their big data stacks. Spark serves as a backbone for big data
    processing in complex analytical use cases.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 由于上述所有原因，Spark是使用最广泛的的大数据处理技术之一。大型组织利用Spark来分析和操作他们的大数据堆栈。Spark在复杂分析用例中作为大数据处理的基础。
- en: 'Some examples of big data use cases are as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些大数据用例的例子：
- en: Business intelligence for reporting and dashboarding
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 报告和仪表板业务智能
- en: Data warehousing for complex applications
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂应用的数据仓库
- en: Operational analytics for application monitoring
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用监控的操作分析
- en: It is important to note here that working with Spark requires a mindset shift
    from single-node processing to big data-processing paradigms. You now have to
    start thinking about how to best utilize and optimize the use of large clusters
    for processing and what some of the best practices around parallel processing
    are.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要特别注意的是，与Spark合作需要从单节点处理模式转变为大数据处理模式。你现在必须开始思考如何最好地利用和优化大型集群进行处理，以及并行处理的一些最佳实践。
- en: Machine learning applications
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习应用
- en: As data grows, so does the need for machine learning models to make use of more
    and more data. The general understanding in the machine learning community today
    is that the more data is provided to the models, the better the models will be.
    This resulted in the need for massive amounts of data to be given to a model for
    predictive analytics. When we deal with massive amounts of data, the challenges
    for training machine learning models become more complex than data processing.
    The reason is that machine learning models crunch data and run statistical estimations
    to come to a minimum error point. To get to that minimum error, the model must
    do complex mathematical operations such as matrix multiplication. These computations
    require large amounts of data to be available in memory and then computations
    to run on it. This serves as the case for parallel processing in machine learning.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据的增长，对机器学习模型利用更多数据的需要也在增加。目前在机器学习社区中普遍认为，提供给模型的数据越多，模型就越好。这导致了需要大量数据提供给模型进行预测分析的需求。当我们处理大量数据时，训练机器学习模型的挑战比数据处理更加复杂。原因是机器学习模型通过压缩数据并运行统计估计来达到最小误差点。为了达到这个最小误差，模型必须执行复杂的数学运算，如矩阵乘法。这些计算需要在内存中提供大量数据，并在其上运行计算。这为机器学习中的并行处理提供了案例。
- en: Machine learning adds an element of prediction to products. Instead of reacting
    to the changes that have already taken place, we can proactively look for ways
    to improve our products and services based on historical data and trends. Every
    aspect of an organization can make use of machine learning for predictive analytics.
    Machine learning can be applied to a number of industries, from hospitals to retail
    stores to manufacturing organizations. All of us have encountered some kind of
    machine learning algorithms when we do tasks on the internet, such as online buying
    and selling, browsing and searching for websites, and using social media platforms.
    Machine learning has become a major part of our lives knowingly or unknowingly.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习为产品增加了预测元素。我们不再只是对已经发生的变化做出反应，而是可以根据历史数据和趋势主动寻找改进我们产品和服务的途径。组织的每个方面都可以利用机器学习进行预测分析。机器学习可以应用于许多行业，从医院到零售店再到制造组织。我们在互联网上完成任务时，如在线买卖、浏览和搜索网站、使用社交媒体平台，都遇到过某种机器学习算法。不知不觉中，机器学习已经成为我们生活的重要组成部分。
- en: 'Although there’s a large number of use cases that an organization can make
    use of in terms of machine learning, I’m highlighting only a few here:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在机器学习方面，组织可以利用大量用例，但我在这里只突出强调几个：
- en: Personalized shopping
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个性化购物
- en: Website searches and ranking
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网站搜索和排名
- en: Fraud detection for banking and insurance
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 银行和保险业的欺诈检测
- en: Customer sentiment analysis
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户情感分析
- en: Customer segmentation
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户细分
- en: Recommendation engines
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐引擎
- en: Price optimization
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 价格优化
- en: Predictive maintenance and support
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测性维护和支持
- en: Text and video analytics
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本和视频分析
- en: Customer/patient 360
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户/患者360度
- en: Let’s move on to cover real-time streaming next.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续讨论实时流处理。
- en: Real-time streaming
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实时流处理
- en: Real-time streaming is one of the use cases where Spark really shines. There
    are very few competing frameworks that offer the flexibility that Spark Streaming
    has.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 实时流是 Spark 真正发光的用例之一。提供与 Spark Streaming 相同灵活性的竞争框架非常少。
- en: Spark Streaming provides a mechanism to ingest data from multiple streaming
    data sources, such as Kafka and Amazon Kinesis. Once the data is ingested, it
    can be processed in real time with very efficient Spark Streaming processing.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 提供了一种机制，可以从多个流数据源（如 Kafka 和 Amazon Kinesis）中摄取数据。一旦数据被摄取，就可以使用非常高效的
    Spark Streaming 处理实时处理。
- en: 'There are a large number of real-time use cases that can make use of Spark
    Streaming. Some of them are as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多实时用例可以利用 Spark Streaming。以下是一些例子：
- en: Self-driving cars
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动驾驶汽车
- en: Real-time reporting and analysis
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时报告和分析
- en: Providing updates on stock market data
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供股市数据的更新
- en: Internet of Things (IoT) data ingestion and processing
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物联网（IoT）数据摄取和处理
- en: Real-time news data processing
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时新闻数据处理
- en: Real-time analytics for optimization of inventory and operations
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时分析以优化库存和运营
- en: Real-time fraud detection systems for credit cards
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信用卡实时欺诈检测系统
- en: Real-time event detection
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时事件检测
- en: Real-time recommendations
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时推荐
- en: Large global organizations make use of Spark Streaming to process billion and
    trillions of data rows in real time. We see some of this in action in our everyday
    life. Your credit card blocking a transaction while you’re out shopping is one
    such example of real-time fraud detection in action. Netflix and YouTube use real-time
    interactions, with the video platforms recommending users what to watch next.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 大型全球组织利用 Spark Streaming 实时处理数十亿甚至数万亿的数据行。我们在日常生活中也能看到一些这样的应用。例如，当你外出购物时，你的信用卡阻止了一笔交易，这就是实时欺诈检测的一个例子。Netflix
    和 YouTube 使用实时交互，视频平台推荐用户观看下一部视频。
- en: As we move to a world of every device sending data back to its server for analysis,
    there’s an increased need for streaming and real-time analysis. One of the main
    advantages of using Spark Streaming for this kind of data is the built-in capabilities
    it has, for look-back and late processing of data. We discussed the usefulness
    of this approach earlier as well, and a lot of manual pipeline processing work
    is removed due to these capabilities. We will learn more about this when we discuss
    Spark Streaming in [*Chapter 7*](B19176_07.xhtml#_idTextAnchor183).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们进入一个每个设备都将数据发送回其服务器进行分析的世界，对流和实时分析的需求增加。使用 Spark Streaming 进行此类数据的主要优势之一是其内置的回溯和延迟处理数据的能力。我们之前也讨论了这种方法的实用性，并且由于这些能力，大量手动管道处理工作被移除。当我们讨论
    Spark Streaming 时，我们将了解更多关于这一点，[第 7 章](B19176_07.xhtml#_idTextAnchor183) 将会涉及。
- en: Graph analytics
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图分析
- en: Graph analytics provides a unique way of looking at data by analyzing relationships
    between different entities. The vertices of a graph represent the entities and
    the edges of a graph represent the relationship between two entities. Think of
    your social network on Facebook or Instagram. You represent one entity, and the
    people you are connected to represent another entity. The relationship (connection)
    between you and your friends is the edge. Similarly, your interests on your social
    media could all be different edges. Then, there can be a location category, for
    which all people who belong to one location would have an edge (a relationship)
    with that location, and so on. Therefore, connections can be made with any different
    type of entities. The more connected you are, the higher the chance that you are
    connected to like-minded people or interests. This is one method of measuring
    relationships between different entities. There are several uses for these kinds
    of graphs. The beauty of Spark is the distributed processing of these graphs to
    find these relationships very quickly. There can be millions and billions of connections
    for billions of entities. Spark has the capability to distribute these workloads
    and compute complex algorithms very fast.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图分析通过分析不同实体之间的关系，提供了一种独特的数据观察方式。图中的顶点代表实体，图的边代表两个实体之间的关系。以你在Facebook或Instagram上的社交网络为例。你代表一个实体，而你连接的人代表另一个实体。你和你朋友之间的联系（连接）就是边。同样，你在社交媒体上的兴趣可能都是不同的边。然后，可以有一个位置类别，属于同一位置的所有人都会与该位置有一个边（关系），依此类推。因此，可以与任何不同类型的实体建立联系。你连接得越多，你与志同道合的人或兴趣相连接的可能性就越高。这是衡量不同实体之间关系的一种方法。这些类型的图有几种用途。Spark的美丽之处在于，它可以通过分布式处理这些图来快速找到这些关系。对于数十亿个实体，可能会有数百万甚至数十亿个连接。Spark具有分配这些工作负载和快速计算复杂算法的能力。
- en: 'The following are some use cases of graph analytics:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些图分析的应用场景：
- en: Social network analysis
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社交网络分析
- en: Fraud detection
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欺诈检测
- en: Page ranking based on relevance
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于相关性的页面排名
- en: Weather prediction
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天气预测
- en: Search engine optimization
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索引擎优化
- en: Supply chain analysis
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 供应链分析
- en: Finding influencers on social media
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在社交媒体上寻找影响者
- en: Money laundering and fraud detection
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 洗钱和欺诈检测
- en: With a growing number of use cases of graph analytics, this proves to be a critical
    use case in the industry today where we need to analyze networks of relationships
    among entities.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 随着图分析用例数量的不断增长，这证明在当今行业中，我们需要分析实体之间关系的网络，这是一个关键的应用场景。
- en: In the next section, we’re going to discuss who the Spark users are and what
    their typical role is within an organization.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论Spark用户是谁以及他们在组织中的典型角色。
- en: Who are the Spark users?
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark用户是谁？
- en: As the world moves toward data-driven decision-making approaches, the role of
    data and the different types of users who can leverage it for critical business
    decisions has become paramount. There are different types of users in data who
    can leverage Spark for different purposes. I will introduce some of those different
    users in this section. This is not an exhaustive list, but it should give you
    an idea of the different roles that exist in data-driven organizations today.
    However, as the industry grows, many more new roles are coming up that are similar
    to the ones present in the following sections, although each may have its own
    separate role.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 随着世界向数据驱动的决策方法转变，数据和能够利用它做出关键业务决策的不同类型用户的作用变得至关重要。在数据中存在不同类型的用户，他们可以利用Spark实现不同的目的。在本节中，我将介绍其中一些不同的用户。这不是一个详尽的列表，但它应该能给你一个关于今天数据驱动组织中存在的不同角色的概念。然而，随着行业的发展，许多新的角色正在出现，它们与以下章节中提到的角色相似，尽管每个角色可能都有其独特的职责。
- en: We’ll start with the role of data analysts.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从数据分析师的角色开始。
- en: Data analysts
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分析师
- en: The more traditional role in data today is a data analyst. The data analyst
    is typically the first-tier role in data. What this means is that data analysts
    are at the core of decision making in organizations. This role spans across different
    business units in an organization, and oftentimes, data analysts have to interact
    with multiple business stakeholders to put across their requirements. This requires
    knowledge of the business domain as well as its processes. When an analyst has
    an understanding of the business and its goals, only then can they perform their
    duties best. Moreover, a lot of times, the requirement is to make current processes
    more efficient, which results in a better bottom line for the business. Therefore,
    having an understanding of not just the business goals but also how it all works
    together is one of the main requirements for this role.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今的数据领域，更传统的角色是数据分析师。数据分析师通常是数据的第一层级角色。这意味着数据分析师是组织决策的核心。这个角色跨越组织的不同业务部门，并且通常，数据分析师需要与多个业务利益相关者互动，以传达他们的需求。这需要了解业务领域及其流程。当分析师对业务及其目标有了解时，他们才能最好地履行他们的职责。此外，很多时候，需求是使当前流程更有效率，这最终会为业务带来更好的底线。因此，不仅需要了解业务目标，还需要了解它们是如何协同工作的，这是这个角色的一项主要要求。
- en: 'A typical job role for a data analyst may look as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析师的一个典型工作角色可能如下所示：
- en: When data analysts are given a project in an organization, the first step in
    the project is to gather requirements from multiple stakeholders. Let’s work with
    an example here. Say you joined an organization as a data analyst. This organization
    makes and sells computer hardware. You are given the task of reporting on the
    revenue each month for the last 10 years. The first step for you would be to gather
    all requirements. It is possible that some stakeholders want to know how many
    units of certain products are sold each month, while others may want to know whether
    the revenues are consistently growing or not. Remember, the end users of your
    reports might work in different business units of the organization.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当数据分析师在一个组织中接到一个项目时，项目的第一步是从多个利益相关者那里收集需求。让我们用一个例子来说明。假设你加入了一个组织作为数据分析师。这个组织生产和销售计算机硬件。你被分配的任务是报告过去10年中每个月的收入。对你来说，第一步就是收集所有需求。可能有些利益相关者想知道每个月销售了多少个特定产品的单位，而其他人可能想知道收入是否持续增长。记住，你的报告的最终用户可能工作在组织的不同业务部门。
- en: Once you have all the requirements gathered from all the concerned stakeholders,
    then you move on to the next step, which is to look for the relevant data sources
    to answer the questions that you are tasked with. You may need to talk with database
    administrators in the organization or platform architects to know where the different
    data sources reside that have relevant information for you to extract.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你收集了所有相关利益相关者的需求，接下来就是进行下一步，也就是寻找相关的数据源来回答你所负责的问题。你可能需要与组织或平台架构师中的数据库管理员交谈，以了解不同数据源的位置，这些数据源中包含对你提取信息有用的相关信息。
- en: Once you have all the relevant sources, then you want to connect with those
    sources programmatically (in most cases) and clean and join some data together
    to come up with relevant statistics, based on your requirements. This is where
    Spark would help you connect to these different data sources and also read and
    manipulate the data most efficiently. You also want to slice and dice the data
    based on your business requirements. Once the data is clean and statistics are
    generated, you want to generate some reports based on these statistics. There
    are different tools in the market to generate reports, such as Qlik and Tableau,
    that you can work with. Once the reports are generated, you may want to share
    your results with the stakeholders. You could present your results to them or
    share the reports with them, depending on what the preferred medium is. This will
    help stakeholders make informed business-critical decisions that are data-driven
    in nature.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你有了所有相关的来源，那么你想要以编程方式（在大多数情况下）与这些来源连接，清理并合并一些数据，以满足你的要求，从而得出相关的统计数据。这就是Spark可以帮助你连接到这些不同的数据来源，并高效地读取和操作数据的地方。你还需要根据业务需求对数据进行切割和细分。一旦数据清理完成并生成了统计数据，你想要基于这些统计数据生成一些报告。市场上有很多生成报告的工具，如Qlik和Tableau，你可以使用它们。一旦生成了报告，你可能想要与利益相关者分享你的结果。你可以向他们展示你的结果，或者与他们分享报告，具体取决于首选的媒介。这将帮助利益相关者做出基于数据的、信息化的、业务关键性的决策。
- en: Collaboration across different roles also plays an important role for data analysts.
    Since organizations have been collecting data for a long time, the most important
    thing is working with all the data that has been collected over the years and
    making sense of it, helping businesses with critical decision making. Helping
    with data-driven decision making is the key to being a successful data analyst.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 不同角色之间的协作对于数据分析师来说也起着重要的作用。由于组织已经收集数据很长时间了，最重要的事情是与多年来收集的所有数据进行工作，并从中找到意义，帮助企业在关键决策中做出贡献。帮助进行数据驱动的决策是成为一名成功的数据分析师的关键。
- en: 'Here’s a summary of the steps taken in a project, as discussed in the previous
    paragraphs:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前几段讨论的项目中采取的步骤的总结：
- en: Gather requirements from stakeholders.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从利益相关者那里收集需求。
- en: Identify the relevant data sources.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定相关的数据来源。
- en: Collaborate with subject matter experts (SMEs).
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与主题专家（SMEs）合作。
- en: Slice and dice data.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切割和细分数据。
- en: Generate reports.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成报告。
- en: Share the results.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分享结果。
- en: Let’s look at data engineers next. This role is gaining a lot of traction in
    the industry today.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来让我们看看数据工程师。这个角色在当今行业中获得了很大的关注。
- en: Data engineers
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据工程师
- en: The next role that is getting more and more prevalent in the industry is a data
    engineer. This is a relatively new role but has gained immense popularity in recent
    times. The reason for this is that data is growing at tremendous levels. We have
    more data being generated per second now than in a whole month a few years ago.
    Working with all this data requires specialized skills. The data can no longer
    be contained in the modest memory of most computers, so we have to make use of
    the massive scale of cloud computing to serve this purpose. As data needs are
    becoming a lot more complex, we need complex architectures to process and use
    this data for business decision making. This is where the role of the data engineer
    comes into play. The main job of the data engineer is to prepare data for ingestion
    for different purposes. The downstream systems that leverage this prepared data
    could be dashboards that run reports based on this data, or it could be a predictive
    analytics solution that works with advanced machine learning algorithms to make
    proactive decisions based on the data.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在行业中越来越普遍的新角色是数据工程师。这是一个相对较新的角色，但近年来获得了巨大的流行度。原因是数据增长速度极快。现在每秒钟产生的数据比几年前一个月产生的数据还要多。处理所有这些数据需要专业的技能。这些数据已经无法被大多数计算机的适度内存所容纳，因此我们必须利用云计算的巨大规模来满足这一需求。随着数据需求变得更加复杂，我们需要复杂的架构来处理和使用这些数据以进行商业决策。这就是数据工程师角色发挥作用的地方。数据工程师的主要工作是准备数据以供不同目的的摄取。利用这些准备好的数据的下游系统可能是基于这些数据运行报告的仪表板，也可能是与高级机器学习算法合作进行预测分析的解决方案，以便根据数据做出主动决策。
- en: More broadly, data engineers are responsible for creating, maintaining, optimizing,
    and monitoring data pipelines that serve different use cases in an organization.
    These pipelines are typically known as Extract, Transform, Load (ETL) pipelines.
    The major differentiator is the sheer scale of data that data engineers have to
    work with. When there are downstream needs for data for BI reporting, advanced
    analytics, and/or machine learning, that is where data pipelines come into play
    for large projects.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 更广泛地说，数据工程师负责创建、维护、优化和监控为组织中的不同用例提供服务的管道。这些管道通常被称为提取、转换、加载（ETL）管道。主要区别在于数据工程师必须处理的数据规模之大。当有下游需求用于BI报告、高级分析以及/或机器学习时，这就是数据管道在大项目中发挥作用的地方。
- en: 'A typical job role for a data engineer in an organization may look as follows.
    When data engineers are given a task to create a data pipeline for a project,
    the first thing they need to consider is the overall architecture of an application.
    There might be data architects in some organizations to help with some of the
    architecture requirements, but that might not always be the case. So, a data engineer
    would ask questions such as the following:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 组织中数据工程师的一个典型工作角色可能如下。当数据工程师被分配创建一个项目数据管道的任务时，他们首先需要考虑的是应用程序的整体架构。在一些组织中可能有数据架构师来帮助处理一些架构需求，但这并不总是如此。因此，数据工程师会提出如下问题：
- en: What are the different sources of data?
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的不同来源有哪些？
- en: What is the size of the data?
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的大小是多少？
- en: Where does the data reside today?
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据目前存储在哪里？
- en: Do we need to migrate the data between different tools?
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否需要在不同的工具之间迁移数据？
- en: How do we connect to the data?
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何连接到数据？
- en: What kind of transformations are required for the data?
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要什么样的数据转换？
- en: How often does the data get updated?
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据多久更新一次？
- en: Should we expect a schema change in the new data?
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否应该预期新数据会有模式变更？
- en: How do we monitor the pipelines if there are failures?
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果出现故障，我们如何监控管道？
- en: Do we need to create a notification system for failures?
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否需要创建一个用于故障的通知系统？
- en: Do we need to add a retry mechanism for failures?
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否需要为故障添加重试机制？
- en: What is the timeout strategy for failures?
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故障的超时策略是什么？
- en: How do we run back-dated pipelines if there are failures?
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果出现故障，我们如何运行过期的管道？
- en: How do we deal with bad data?
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何处理不良数据？
- en: What strategy we should follow – ETL or ELT?
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该遵循什么策略——ETL还是ELT？
- en: How can we save the costs of computation?
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何节省计算成本？
- en: Once they have answers to these questions, then they start working on a resilient
    architecture to build data pipelines. Once those pipelines are run and tested,
    the next step is to maintain these pipelines and make the processing more efficient
    and visible for failure detection. The goal is to build these pipelines so that
    once everything is run, the end state of data is consistent for different downstream
    use cases. Too often, data engineers have to collaborate with data analysts and
    data scientists to come up with correct data transformation requirements, based
    on the required use cases.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦他们回答了这些问题，他们就开始着手构建一个具有弹性的架构来构建数据管道。一旦这些管道运行并经过测试，下一步就是维护这些管道，使处理更加高效和易于故障检测。目标是构建这些管道，以便一旦运行完毕，数据的最终状态对于不同的下游用例是一致的。过于频繁的是，数据工程师必须与数据分析师和数据科学家合作，根据所需用例制定正确的数据转换要求。
- en: Let’s talk about data scientists now, a job that has been advertised as “*the
    sexiest job of the 21st century*” on multiple forums.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈数据科学家，这是一个在多个论坛上被宣传为“*21世纪最性感的工作*”的职位。
- en: Data scientists
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据科学家
- en: Traditionally, data has been used for decision making based on what has happened
    in the past. This means that organizations have been reactive, based on the data.
    Now, there’s been a paradigm shift in advanced and predictive analytics. This
    means instead of being reactive, organizations can be proactive in their decision
    making. They achieve this with the help of all the data that is available to organizations
    now. To make effective use of this data, data scientists play a major part. They
    take analytics to the next level, where instead of just looking at what has happened
    in the past, they have sophisticated machine learning algorithms to predict what
    could take place in the future as well. All this is based on the huge amounts
    of data that is available to them.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，数据一直被用于基于过去发生的事情进行决策。这意味着组织是基于数据做出反应的。现在，在高级和预测分析方面已经发生了范式转变。这意味着组织在决策上可以变得主动，而不是被动。他们通过现在组织可用的所有数据来实现这一点。为了有效地利用这些数据，数据科学家扮演着重要的角色。他们将分析提升到下一个层次，而不是仅仅查看过去发生的事情，他们拥有复杂的机器学习算法来预测未来可能发生的事情。所有这些都是基于他们可用的海量数据。
- en: A typical job role for a data scientist in an organization may look as follows.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个组织中，数据科学家的典型工作角色可能如下所示。
- en: The data scientist is given a problem to solve or a question to answer. The
    first task is to see what kind of data is available to them that would help them
    answer this question. They would create a few hypotheses to test with the given
    data. If the results are positive and the data is able to answer some of the problem
    statements, then they move on to experimenting with the data and seek ways to
    more effectively answer the questions at hand. For this purpose, they would join
    different datasets together, and they would also transform the data to make it
    ready for some machine learning algorithms to consume. At this stage, they would
    also need to decide what kind of machine learning problem they aim to solve.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家被分配了一个需要解决的问题或一个需要回答的问题。首要任务是查看他们可以用来回答这个问题的数据类型。他们会基于给定的数据提出一些假设进行测试。如果结果积极，并且数据能够回答一些问题陈述，那么他们就会继续通过实验来处理数据，并寻求更有效地回答手头问题的方法。为此，他们会将不同的数据集合并在一起，并且也会转换数据，使其适合某些机器学习算法使用。在这个阶段，他们还需要决定他们旨在解决的机器学习问题类型。
- en: 'There are three major types of machine learning techniques that they can use:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 他们可以使用三种主要的机器学习技术：
- en: Regression
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归
- en: Classification
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类
- en: Clustering
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类
- en: Based on the technique decided and data transformations, they would then move
    to prototype with a few machine learning algorithms to create a baseline model.
    A baseline model is a very basic model that serves to answer the original question.
    Based on this baseline model, other models can be created that would be able to
    answer the question better. In some cases, some predefined rules can also serve
    as a baseline model. What this means is that the business might already be operating
    on some predefined rules that can serve as a baseline to compare the machine learning
    model. Once the initial prototyping is done, then the data scientist moves on
    to more advanced optimizations in terms of models. They can work with different
    hyperparameters of the model or experiment with different data transformations
    and sample sizes. All of this can be done in Spark or other tools and languages,
    depending on their preference. Spark has the edge to run these algorithms in a
    parallel fashion, making the whole process very efficient. Once the data scientist
    is happy with the model results based on different metrics, they would then move
    that model to a production environment where these models can be served to customers
    solving specific problems. At this point, they would hand over these models to
    machine learning engineers to start incorporating them into the pipelines.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 根据决定的技术和数据转换，他们接下来会使用几个机器学习算法进行原型设计，以创建一个基线模型。基线模型是一个非常基本的模型，用于回答原始问题。基于这个基线模型，可以创建其他模型，这些模型能够更好地回答问题。在某些情况下，一些预定义的规则也可以作为基线模型。这意味着企业可能已经在一些预定义的规则上运行，这些规则可以作为基准来比较机器学习模型。一旦完成初步的原型设计，数据科学家就会继续进行更高级的模型优化。他们可以与模型的不同的超参数一起工作，或者尝试不同的数据转换和样本大小。所有这些都可以在Spark或其他工具和语言中完成，具体取决于他们的偏好。Spark具有并行运行这些算法的优势，使整个过程非常高效。一旦数据科学家根据不同的指标对模型结果感到满意，他们就会将模型移动到生产环境，在这些环境中，这些模型可以服务于客户解决特定问题。在这个阶段，他们会将这些模型交给机器学习工程师，开始将它们集成到管道中。
- en: 'Here’s a summary of the steps taken in a project, as discussed in the previous
    paragraph:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前一段讨论的项目中采取的步骤总结：
- en: Create and test a hypothesis.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建并测试一个假设。
- en: Transform the data.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换数据。
- en: Decide on a machine learning algorithm.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们决定使用哪种机器学习算法？
- en: Prototype with different machine learning models.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用不同的机器学习模型进行原型设计。
- en: Create a baseline model.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个基线模型。
- en: Tune the model.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整模型。
- en: Tune the data.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整数据。
- en: Transition models to production.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型过渡到生产环境。
- en: Let’s discuss the role of machine learning engineers next.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论机器学习工程师的角色。
- en: Machine learning engineers
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习工程师
- en: Like data engineers, machine learning engineers also build pipelines, but these
    pipelines are primarily built for machine learning model deployment. Machine learning
    engineers typically take prototyped models created by data scientists and build
    machine learning pipelines around them. We will discuss what machine learning
    pipelines are and what some of the questions that need to be answered to build
    these pipelines are.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据工程师一样，机器学习工程师也构建管道，但这些管道主要是为机器学习模型部署而构建的。机器学习工程师通常使用数据科学家创建的原型模型，并围绕它们构建机器学习管道。我们将讨论机器学习管道是什么，以及构建这些管道需要回答的一些问题。
- en: 'Machine learning models are built to solve complex problems and provide advanced
    analytic methods to serve a business. After prototyping, these models need to
    run in the production environments of the organizations and be deployed to serve
    customers. For deployment, there are several considerations that need to be taken
    into account:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型是为了解决复杂问题并提供高级分析方法以服务于企业而构建的。在原型设计之后，这些模型需要在组织的生产环境中运行并部署以服务于客户。对于部署，需要考虑以下几个因素：
- en: How much data is there for model training?
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于模型训练的数据有多少？
- en: How many customers do we plan to serve concurrently?
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们计划同时服务于多少客户？
- en: How often do we need to retrain the models?
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要多久重新训练一次模型？
- en: How often do we expect the data to change?
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们预计数据多久会变化一次？
- en: How do we scale the pipeline up and down based on demand?
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何根据需求进行管道的扩展和缩减？
- en: How do we monitor failures in model training?
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何监控模型训练中的失败？
- en: Do we need notifications for failures?
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否需要为失败发送通知？
- en: Do we need to add a retry mechanism for failures?
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否需要为失败添加重试机制？
- en: What is the timeout strategy for failures?
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 失败的超时策略是什么？
- en: How do we measure model performance in production?
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何在生产中衡量模型性能？
- en: How do we tackle data drift?
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何应对数据漂移？
- en: How do we tackle model drift?
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何应对模型漂移？
- en: Once these questions are answered, the next step is to build a pipeline around
    these models. The main purpose of the pipeline would be such that when new data
    comes in, the pre-trained models are able to answer questions based on new datasets.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这些问题得到解答，下一步就是围绕这些模型构建一个管道。管道的主要目的是，当新数据到来时，预训练的模型能够根据新的数据集回答问题。
- en: 'Let’s use an example to better understand these pipelines. We’ll continue with
    the first example of an organization selling computer hardware:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来更好地理解这些管道。我们将继续使用一个组织销售计算机硬件的第一个例子：
- en: Suppose the organization wants to build a recommender system on its website
    that recommends to users which products to buy.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设该组织希望在网站上构建一个推荐系统，向用户推荐购买哪些产品。
- en: The data scientists have built a prototype model that works well with test data.
    Now, they want to deploy it to production.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据科学家已经构建了一个与测试数据表现良好的原型模型。现在，他们想将其部署到生产环境中。
- en: To deploy this model, the machine learning engineers would have to see how they
    can incorporate this model on the website.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了部署这个模型，机器学习工程师需要看看他们如何将这个模型整合到网站上。
- en: They would start by getting the data ingested from the website to get the user
    information.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 他们将从从网站上获取数据开始，以获取用户信息。
- en: Once they have the information, they pass it through the data pipeline to clean
    and join the data.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦他们有了信息，他们就会将其通过数据管道进行清洗和合并数据。
- en: They might also want to add some precomputed features to the model, such as
    the time of the year, to get a better idea of whether it’s a holiday season and
    some special deals are going on.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 他们还可能想将一些预计算的特征添加到模型中，例如一年中的时间，以更好地了解是否是假日季节以及是否有一些特别优惠正在进行。
- en: Then, they would need a REST API endpoint to get the latest recommendations
    for each user on the website.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，他们需要一个REST API端点来获取网站上每个用户的最新推荐。
- en: After that, the website needs to be connected to the REST endpoint to serve
    the actual customers.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，网站需要连接到REST端点以服务实际客户。
- en: Once these models are deployed on live systems (the website, in our example),
    there needs to be a monitoring system for any errors and changes in either the
    model or the data. This is known as **model drift** and **data** **drift**, respectively.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦这些模型部署到实际系统中（在我们的例子中是网站），就需要有一个监控系统来监控模型或数据中的任何错误和变化。这分别被称为**模型漂移**和**数据漂移**。
- en: Data drift
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据漂移
- en: Data may change over time. In our example, people’s preferences may change with
    time, or with seasonality, data may be different. For example, during a holiday
    season, people’s preferences might slightly change because they are looking to
    get presents for their friends and family, so recommending relevant products based
    on these preferences is of paramount importance for a business. Monitoring these
    trends and changes in the data would result in better models over time and would
    ultimately benefit the business.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可能会随时间变化。在我们的例子中，人们的偏好可能会随着时间的推移或季节性而变化，数据可能不同。例如，在假日季节，人们的偏好可能会略有变化，因为他们正在寻找为朋友和家人购买礼物，因此根据这些偏好推荐相关产品对业务至关重要。监控这些趋势和数据变化将随着时间的推移产生更好的模型，并最终有利于业务。
- en: Model drift
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型漂移
- en: Similar to data drift, we also have the concept of model drift. This means that
    the model changes over time, and the old model that was initially built is not
    the most performant in terms of recommending items to website visitors. With changing
    data, the model also needs to be updated from time to time. To get a sense of
    when a model needs to be updated, we need to have monitoring in place for models
    as well. This monitoring would constantly compare old model results with the new
    data and see whether model performance is degrading. If that’s the case, it’s
    time to update the model.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据漂移类似，我们还有模型漂移的概念。这意味着模型会随时间变化，最初构建的旧模型在向网站访客推荐项目方面不再是表现最佳的。随着数据的变化，模型也需要不时地进行更新。为了了解何时需要更新模型，我们需要对模型进行监控。这种监控会持续比较旧模型的结果与新数据，看模型性能是否下降。如果是这样，那就需要更新模型了。
- en: This whole life cycle of model deployment is typically the responsibility of
    machine learning engineers. Note that the process would slightly vary for different
    problems, but the overall idea remains the same.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型部署的全生命周期通常是机器学习工程师的责任。请注意，对于不同的问题，这个过程会有所不同，但总体思路保持不变。
- en: Summary
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about the basics of Apache Spark and why Spark is
    becoming a lot more prevalent in the industry for big data applications. We also
    learned about the different components of Spark and how these components are helpful
    in terms of application development. Then, we discussed the different roles that
    are present in the industry today and who can make use of Spark’s capabilities.
    Finally, we discussed the modern-day uses of Spark in different industry use cases.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了Apache Spark的基础知识以及为什么Spark在工业界的大数据应用中变得越来越普遍。我们还学习了Spark的不同组件以及这些组件在应用开发方面的帮助。然后，我们讨论了当前行业中存在的不同角色以及谁可以利用Spark的能力。最后，我们讨论了Spark在不同行业用例中的现代应用。
- en: Sample questions
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 样题
- en: 'Although these questions are not part of the Spark certification, it’s good
    to answer these to assess your understanding of the basics of Spark:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些问题不是Spark认证的一部分，但回答这些问题以评估你对Spark基础知识的理解是很好的：
- en: What are the core components of Spark?
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark的核心组件有哪些？
- en: When do we want to use Spark Streaming?
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在什么时候想使用Spark Streaming？
- en: What is a look-back mechanism in Spark Streaming?
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark Streaming中的回溯机制是什么？
- en: What are some good use cases for Spark?
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark有哪些好的用例？
- en: Which roles in an organization should use Spark?
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在组织中哪些角色应该使用Spark？
