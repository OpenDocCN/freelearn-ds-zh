- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Ingesting Data from Structured and Unstructured Databases
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从结构化和非结构化数据库中摄取数据
- en: Nowadays, we can store and retrieve data from multiple sources, and the optimal
    storage method depends on the type of information being processed. For example,
    most APIs make data available in an unstructured format as this allows the sharing
    of data of multiple formats (for example, audio, video, and image) and has low
    storage costs via the use of data lakes. However, if we want to make quantitative
    data available for use with several tools to support analysis, then the most reliable
    option might be structured data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以从多个来源存储和检索数据，最佳存储方法取决于正在处理的信息类型。例如，大多数 API 以非结构化格式提供数据，因为这允许共享多种格式的数据（例如，音频、视频和图像），并且通过使用数据湖具有低存储成本。然而，如果我们想使定量数据可用于与多个工具一起支持分析，那么最可靠的选择可能是结构化数据。
- en: Ultimately, whether you are a data analyst, scientist, or engineer, it is essential
    to understand how to manage both structured and unstructured data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，无论你是数据分析师、科学家还是工程师，了解如何管理结构化和非结构化数据都是至关重要的。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将介绍以下食谱：
- en: Configuring a JDBC connection
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 JDBC 连接
- en: Ingesting data from a JDBC database using SQL
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SQL 从 JDBC 数据库中摄取数据
- en: Connecting to a NoSQL database (MongoDB)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接到 NoSQL 数据库（MongoDB）
- en: Creating our NoSQL table in MongoDB
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 MongoDB 中创建我们的 NoSQL 表
- en: Ingesting data from MongoDB using PySpark
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PySpark 从 MongoDB 中摄取数据
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the code from this chapter in the GitHub repository at [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从本章的 GitHub 仓库 [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook)
    中找到代码。
- en: 'Using the **Jupyter Notebook** is not mandatory but allows us to explore the
    code interactively. Since we will execute both Python and PySpark code, Jupyter
    can help us to understand the scripts better. Once you have Jupyter installed,
    you can execute it using the following line:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 **Jupyter Notebook** 不是强制性的，但它允许我们以交互式的方式探索代码。由于我们将执行 Python 和 PySpark 代码，Jupyter
    可以帮助我们更好地理解脚本。一旦你安装了 Jupyter，你可以使用以下行来执行它：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It is recommended to create a separate folder to store the Python files or notebooks
    we will cover in this chapter; however, feel free to organize it in the most appropriate
    way for you.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 建议创建一个单独的文件夹来存储本章中将要介绍的 Python 文件或笔记本；然而，请随意以最适合你的方式组织它们。
- en: Configuring a JDBC connection
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置 JDBC 连接
- en: Working with different systems brings the challenge of finding an efficient
    way to connect the systems. An adaptor, or a driver, is the solution to this communication
    problem, creating a bridge to translate information from one system to another.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与不同的系统合作带来了找到一种有效方式连接系统的挑战。适配器或驱动程序是解决这种通信问题的解决方案，它创建了一个桥梁来翻译一个系统到另一个系统的信息。
- en: '**JDBC**, or **Java Database Connectivity**, is used to facilitate communication
    between Java-based systems and databases. This recipe covers configuring JDBC
    in SparkSession to connect to a PostgreSQL database, using best practices as always.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**JDBC**，或 **Java 数据库连接**，用于促进基于 Java 的系统与数据库之间的通信。本食谱涵盖了在 SparkSession 中配置
    JDBC 以连接到 PostgreSQL 数据库，一如既往地使用最佳实践。'
- en: Getting ready
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Before configuring SparkSession, we need to download the `.jars` file (Java
    Archive). You can do this at [https://jdbc.postgresql.org/](https://jdbc.postgresql.org/)
    on the PostgreSQL official site.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置 SparkSession 之前，我们需要下载 `.jars` 文件（Java 存档）。你可以在 PostgreSQL 官方网站 [https://jdbc.postgresql.org/](https://jdbc.postgresql.org/)
    上完成此操作。
- en: 'Select **Download**, and you will be redirected to another page:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 **下载**，你将被重定向到另一个页面：
- en: '![Figure 5.1 – PostgreSQL JDBC home page](img/Figure_5.1_B19453.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – PostgreSQL JDBC 主页](img/Figure_5.1_B19453.jpg)'
- en: Figure 5.1 – PostgreSQL JDBC home page
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – PostgreSQL JDBC 主页
- en: Then, select the **Java 8** **Download** button.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，选择 **Java 8** **下载** 按钮。
- en: Keep this `.jar` file somewhere safe, as you will need it later. I suggest keeping
    it inside the folder where your code is.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 请将此 `.jar` 文件保存在安全的地方，因为你稍后需要它。我建议将其保存在你的代码所在的文件夹中。
- en: For the PostgreSQL database, you can use a Docker image or the instance we created
    on Google Cloud in *Chapter 4*. If you opt for the Docker image, ensure it is
    up and running.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 PostgreSQL 数据库，你可以使用 Docker 镜像或我们在第 4 章中创建的实例。如果你选择 Docker 镜像，请确保它已启动并运行。
- en: The final preparatory step for this recipe is to import a dataset to be used.
    We will use the `word_population.csv` file (which you can find in the GitHub repository
    of this book, at [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets)).
    Import it using DBeaver or any other SQL IDE of your choice. We will use this
    dataset with SQL in the *Ingesting data from a JDBC database using SQL* recipe
    later in this chapter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此菜谱的最终准备步骤是导入一个要使用的数据集。我们将使用 `world_population.csv` 文件（您可以在本书的 GitHub 仓库中找到，位于
    [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets)）。使用
    DBeaver 或您选择的任何其他 SQL IDE 导入它。我们将在本章后面的 *使用 SQL 从 JDBC 数据库中获取数据* 菜谱中使用此数据集。
- en: To import data into DBeaver, create a table with the name of your choice under
    the Postgres database. I chose to give my table the exact name of the CSV file.
    You don’t need to insert any columns for now.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要将数据导入 DBeaver，在 Postgres 数据库下创建一个您选择的表名。我选择给我的表取与 CSV 文件完全相同的名字。目前您不需要插入任何列。
- en: 'Then, right-click on the table and select **Import Data**, as shown in the
    following screenshot:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，右键单击表并选择 **导入数据**，如下面的截图所示：
- en: '![Figure 5.2 – Importing data on a table using DBeaver](img/Figure_5.2_B19453.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 使用 DBeaver 在表格中导入数据](img/Figure_5.2_B19453.jpg)'
- en: Figure 5.2 – Importing data on a table using DBeaver
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 使用 DBeaver 在表格中导入数据
- en: 'A new window will open, showing the options to use a CSV file or a database
    table. Select **CSV** and then **Next**, as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 将打开一个新窗口，显示使用 CSV 文件或数据库表选项。选择 **CSV** 然后选择 **下一步**，如下所示：
- en: '![Figure 5.3 – Importing CSV files into a table using DBeaver](img/Figure_5.3_B19453.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 使用 DBeaver 将 CSV 文件导入到表格中](img/Figure_5.3_B19453.jpg)'
- en: Figure 5.3 – Importing CSV files into a table using DBeaver
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 使用 DBeaver 将 CSV 文件导入到表格中
- en: 'A new window will open where you can select the file. Choose the `world_population.csv`
    file and select the **Next** button, leaving the default settings shown as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 将打开一个新窗口，您可以在其中选择文件。选择 `world_population.csv` 文件并选择 **下一步** 按钮，保留如下所示的默认设置：
- en: '![Figure 5.4 – CSV file successfully imported into the world_population table](img/Figure_5.4_B19453.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – CSV 文件成功导入到 world_population 表中](img/Figure_5.4_B19453.jpg)'
- en: Figure 5.4 – CSV file successfully imported into the world_population table
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – CSV 文件成功导入到 world_population 表中
- en: 'If all succeeds, you should be able to see the `world_population` table populated
    with the columns and data:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，您应该能够看到 `world_population` 表已填充了列和数据：
- en: '![Figure 5.5 – The world_population table populated with data from the CSV](img/Figure_5.5_B19453.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – 已用 CSV 数据填充的 world_population 表](img/Figure_5.5_B19453.jpg)'
- en: Figure 5.5 – The world_population table populated with data from the CSV
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 已用 CSV 数据填充的 world_population 表
- en: How to do it…
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'I will use a Jupyter notebook to insert and execute the code to make this exercise
    more dynamic. Here is how we do it:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用 Jupyter notebook 来插入和执行代码，使这个练习更加动态。以下是我们的操作方法：
- en: '`SparkSession`, we will need an additional class called `SparkConf` to set
    our new configuration:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`SparkSession`，我们需要一个额外的类 `SparkConf` 来设置我们的新配置：'
- en: '[PRE1]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`SparkConf(`), which we instantiated, we can set the path to the `.jar` with
    `spark.jars`:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`SparkConf(`)，我们实例化后，我们可以使用 `spark.jars` 设置 `.jar` 路径：'
- en: '[PRE2]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You will see a `SparkConf` object created, as shown in the following output:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到创建了一个 `SparkConf` 对象，如下面的输出所示：
- en: '![Figure 5.6 – SparkConf object](img/Figure_5.6_B19453.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – SparkConf 对象](img/Figure_5.6_B19453.jpg)'
- en: Figure 5.6 – SparkConf object
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – SparkConf 对象
- en: '`SparkSession` and create it:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `SparkSession` 并初始化它：
- en: '[PRE3]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If a warning message appears as in the following screenshot, you can ignore
    it:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果出现如下截图所示的警告信息，您可以忽略它：
- en: '![Figure 5.7 – SparkSession initialization warning messages](img/Figure_5.7_B19453.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7 – SparkSession 初始化警告信息](img/Figure_5.7_B19453.jpg)'
- en: Figure 5.7 – SparkSession initialization warning messages
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – SparkSession 初始化警告信息
- en: '**Connecting to our database**: Finally, we can connect to the PostgreSQL database
    by passing the required credentials including host, database name, username, and
    password as follows:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**连接到我们的数据库**：最后，我们可以通过传递所需的凭据（包括主机、数据库名称、用户名和密码）来连接到 PostgreSQL 数据库，如下所示：'
- en: '[PRE4]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If the credentials are correct, we should expect no output here.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果凭据正确，我们应在此处不期望有任何输出。
- en: '`.printSchema()`, it is possible now to see the table columns:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.printSchema()`，现在可以查看表列：'
- en: '[PRE5]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Executing the code will show the following output:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码将显示以下输出：
- en: '![Figure 5.8 – DataFrame of the world_population schema](img/Figure_5.8_B19453.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图5.8 – world_population架构的DataFrame](img/Figure_5.8_B19453.jpg)'
- en: Figure 5.8 – DataFrame of the world_population schema
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 – world_population架构的DataFrame
- en: How it works…
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: We can observe that PySpark (and Spark) require additional configuration to
    create a connection with a database. In this recipe, using the PostgreSQL `.jars`
    file is essential to make it work.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到PySpark（以及Spark）需要额外的配置来与数据库建立连接。在这个配方中，使用PostgreSQL的`.jars`文件是使其工作所必需的。
- en: 'Let’s understand what kind of configuration Spark requires by looking at our
    code:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过查看我们的代码来了解Spark需要什么样的配置：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We started by instantiating the `SparkConf()` method, responsible for defining
    configurations used in SparkSession. After instantiating the class, we used the
    `set()` method to pass a key-value pair parameter: `spark.jars`. If more than
    one `.jars` file was used, the paths could be passed on the value parameter separated
    by commas. It is also possible to define more than one `conf.set()`method; they
    just need to be included one after the other.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先实例化了`SparkConf()`方法，该方法负责定义在SparkSession中使用的配置。实例化类后，我们使用`set()`方法传递一个键值对参数：`spark.jars`。如果使用了多个`.jars`文件，路径可以通过逗号分隔的值参数传递。也可以定义多个`conf.set()`方法；它们只需要依次包含即可。
- en: 'It is on the second line of SparkSession where the set of configurations is
    passed, as you can see in the following code:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在以下代码中看到的，配置的集合是在SparkSession的第二行传递的：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, with our SparkSession instantiated, we can use it to read our database,
    as you can see in the following code:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，随着我们的SparkSession实例化，我们可以使用它来读取我们的数据库，如下面的代码所示：
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Since we are handling a third-party application, we must set the format for
    reading the output using the `.format()` method. The `.options()` method will
    carry the authentication values and the driver.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在处理第三方应用程序，我们必须使用`.format()`方法设置读取输出的格式。`.options()`方法将携带认证值和驱动程序。
- en: Note
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: With time you will observe that there are a few diverse ways to declare the
    `.options()` key-value pairs. For example, another frequently used format is .`options("driver",
    "org.postgresql.Driver)`. Both ways are correct depending on the *taste* of the
    developer.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，您将观察到声明`.options()`键值对有几种不同的方式。例如，另一种常用的格式是.`options("driver", "org.postgresql.Driver")`。这两种方式都是正确的，取决于开发者的*口味*。
- en: There’s more…
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: This recipe covered how to use a JDBC driver, and the same logic applies to
    **Open Database Connectivity** (**ODBC**). However, determining the criteria for
    using JDBC or ODBC requires understanding which data source we are ingesting data
    from.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方涵盖了如何使用JDBC驱动程序，同样的逻辑也适用于**开放数据库连接**（**ODBC**）。然而，确定使用JDBC或ODBC的标准需要了解我们从哪个数据源摄取数据。
- en: 'The ODBC connection in Spark is usually associated with Spark Thrift Server,
    a Spark SQL extension from Apache HiveServer2 that allows users to execute SQL
    queries in **Business Intelligence** (**BI**) tools such as MS PowerBI or Tableau.
    See the following diagram for an outline of this relationship:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中的ODBC连接通常与Spark Thrift Server相关联，这是Apache HiveServer2的一个Spark SQL扩展，允许用户在**商业智能**（**BI**）工具（如MS
    PowerBI或Tableau）中执行SQL查询。以下图表概述了这种关系：
- en: '![Figure 5.9 – Spark Thrift architecture, provided by Cloudera documentation
    (https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/developing-spark-applications/content/using_spark_sql.xhtml)](img/Figure_5.9_B19453.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图5.9 – Spark Thrift架构，由Cloudera文档提供（https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/developing-spark-applications/content/using_spark_sql.xhtml）](img/Figure_5.9_B19453.jpg)'
- en: Figure 5.9 – Spark Thrift architecture, provided by Cloudera documentation ([https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/developing-spark-applications/content/using_spark_sql.xhtml](https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/developing-spark-applications/content/using_spark_sql.xhtml))
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 – Spark Thrift架构，由Cloudera文档提供（[https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/developing-spark-applications/content/using_spark_sql.xhtml](https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/developing-spark-applications/content/using_spark_sql.xhtml)）
- en: By contrast to JDBC, ODBC is used in real-life projects that are smaller and
    more specific to certain system integrations. It also requires the use of another
    Python library called `pyodbc`. You can read more about it at [https://kontext.tech/article/290/connect-to-sql-server-in-spark-pyspark](https://kontext.tech/article/290/connect-to-sql-server-in-spark-pyspark).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 与 JDBC 相比，ODBC 在实际项目中使用较多，这些项目规模较小，且更具体于某些系统集成。它还需要使用另一个名为 `pyodbc` 的 Python
    库。您可以在 [https://kontext.tech/article/290/connect-to-sql-server-in-spark-pyspark](https://kontext.tech/article/290/connect-to-sql-server-in-spark-pyspark)
    上了解更多信息。
- en: Debugging connection errors
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调试连接错误
- en: PySpark errors can be very confusing and lead to misinterpretations. It happens
    because the errors are often related to a problem on the JVM, and Py4J (a Python
    interpreter that communicates dynamically with the JVM) consolidates the message
    with other Python errors that may have occurred.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 错误可能非常令人困惑，并可能导致误解。这是因为错误通常与 JVM 上的问题有关，而 Py4J（一个与 JVM 动态通信的 Python
    解释器）将消息与其他可能发生的 Python 错误合并。
- en: 'Some error messages are prevalent and can easily be identified when managing
    database connections. Let’s take a look at an error that occurred when using the
    following code:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一些错误信息在管理数据库连接时很常见，并且很容易识别。让我们看看以下代码使用时发生的错误：
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here is the error message that resulted:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是产生的错误信息：
- en: '![Figure 5.10 – Py4JJavaError message](img/Figure_5.10_B19453.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10 – Py4JJavaError 信息](img/Figure_5.10_B19453.jpg)'
- en: Figure 5.10 – Py4JJavaError message
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – Py4JJavaError 信息
- en: 'In the first line, we see `Py4JJavaError` informing us of an error when calling
    the load function. Continuing to the second line, we can see the message: `java.sql.SQLException:
    No suitable driver`. It informs us that even though the `.jars` file is configured
    and set, PySpark doesn’t know which drive to use to load data from PostgreSQL.
    This can be easily fixed by adding the `driver` parameter under `.options()`.
    Refer to the following code:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '在第一行，我们看到 `Py4JJavaError` 通知我们在调用加载函数时出现错误。继续到第二行，我们可以看到消息：“java.sql.SQLException:
    No suitable driver”。它通知我们，尽管 `.jars` 文件已配置并设置，但 PySpark 仍然不知道使用哪个驱动程序来从 PostgreSQL
    加载数据。这可以通过在 `.options()` 下添加 `driver` 参数来轻松解决。请参考以下代码：'
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: See also
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: Find more about Spark Thrift Server at [https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-thrift-server.xhtml](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-thrift-server.xhtml).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多关于 Spark Thrift 服务器的信息，请访问 [https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-thrift-server.xhtml](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-thrift-server.xhtml)。
- en: Ingesting data from a JDBC database using SQL
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SQL 从 JDBC 数据库中摄取数据
- en: With the connection tested and SparkSession configured, the next step is to
    ingest the data from PostgreSQL, filter it, and save it in an analytical format
    called a Parquet file. Don’t worry about how Parquet files work for now; we will
    cover it in the following chapters.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试连接并配置 SparkSession 后，下一步是从 PostgreSQL 中摄取数据，过滤它，并将其保存为称为 Parquet 文件的分析格式。现在不用担心
    Parquet 文件是如何工作的；我们将在接下来的章节中介绍。
- en: This recipe aims to use the connection we created with our JDBC database and
    ingest the data from the `world_population` table.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个菜谱旨在使用我们与 JDBC 数据库创建的连接，并从 `world_population` 表中摄取数据。
- en: Getting ready
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe will use the same dataset and code as the *Configuring a JDBC connection*
    recipe to connect to the PostgreSQL database. Ensure your Docker container is
    running or your PostgreSQL server is up.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这个菜谱将使用与 *配置 JDBC 连接* 菜谱相同的数据集和代码来连接到 PostgreSQL 数据库。请确保您的 Docker 容器正在运行或您的
    PostgreSQL 服务器已启动。
- en: This recipe continues from the content presented in *Configuring a JDBC connection*.
    We will now learn how to ingest the data inside the Postgres database.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个菜谱从 *配置 JDBC 连接* 中继续。我们现在将学习如何摄取 Postgres 数据库内部的数据。
- en: How to do it…
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Following on from our previous code, let’s read the data in our database as
    follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的代码基础上，让我们按照以下方式读取数据库中的数据：
- en: '`world_population` table:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`world_population` 表：'
- en: '[PRE11]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Creating a TempView**: Using the exact name of our table (for organization
    purposes), we create a temporary view in the Spark default database from the DataFrame:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建临时视图**：为了组织目的，我们使用表的准确名称，从 DataFrame 中在 Spark 默认数据库中创建一个临时视图：'
- en: '[PRE12]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: There is no output expected here.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这里不应该有输出。
- en: '`spark` variable:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spark` 变量：'
- en: '[PRE13]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Depending on the size of your monitor, the output may look confusing, as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的显示器大小，输出可能看起来很混乱，如下所示：
- en: '![Figure 5.11 – world_population view using Spark SQL](img/Figure_5.11_B19453.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.11 – 使用 Spark SQL 的 world_population 视图](img/Figure_5.11_B19453.jpg)'
- en: Figure 5.11 – world_population view using Spark SQL
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – 使用 Spark SQL 的 world_population 视图
- en: '**Filtering data**: Using a SQL statement, let’s filter only the South American
    countries in our DataFrame:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**过滤数据**：使用 SQL 语句，让我们仅过滤 DataFrame 中的南美国家：'
- en: '[PRE14]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Since we attribute the results to a variable, there is no output.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将结果分配给一个变量，因此没有输出。
- en: '`.toPandas()` function to bring a more user-friendly view:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.toPandas()` 函数以更用户友好的方式查看：'
- en: '[PRE15]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This is how the result appears:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是结果呈现的样子：
- en: '![Figure 5.12 – south_america countries with toPandas() visualization](img/Figure_5.12_B19453.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.12 – 使用 toPandas() 可视化的 south_america 国家](img/Figure_5.12_B19453.jpg)'
- en: Figure 5.12 – south_america countries with toPandas() visualization
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 – 使用 toPandas() 可视化的 south_america 国家
- en: '**Saving our work**: Now, we can save our filtered data as follows:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**保存我们的工作**：现在，我们可以如下保存我们的过滤数据：'
- en: '[PRE16]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Looking at your script’s folder, you should see a folder named `south_america_population`.
    Inside, you should see the following output:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 查看您的脚本文件夹，您应该看到一个名为 `south_america_population` 的文件夹。在里面，您应该看到以下输出：
- en: '![Figure 5.13 – south_america data in the Parquet file](img/Figure_5.13_B19453.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.13 – Parquet 文件中的 south_america 数据](img/Figure_5.13_B19453.jpg)'
- en: Figure 5.13 – south_america data in the Parquet file
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 – Parquet 文件中的 south_america 数据
- en: This is our filtered and ingested DataFrame in an analytical format.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的过滤和导入的 DataFrame，以分析格式呈现。
- en: How it works…
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: A significant advantage of working with Spark is the possibility of using SQL
    statements to filter and query data from a DataFrame. It allows data analytics
    and BI teams to help the data engineers by handling queries. This helps to build
    analytical data and insert it into data warehouses.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Spark 一起工作的一个显著优势是使用 SQL 语句从 DataFrame 中过滤和查询数据。这允许数据分析和 BI 团队通过处理查询来帮助数据工程师。这有助于构建分析数据并将其插入数据仓库。
- en: 'Nevertheless, there are some considerations we need to take to execute a SQL
    statement properly. One of them is using `.createOrReplaceTempView()`, as seen
    in this line of code:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们需要注意一些事项来正确执行 SQL 语句。其中之一是使用 `.createOrReplaceTempView()`，如代码中的这一行所示：
- en: '[PRE17]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Behind the scenes, this temporary view will work as a SQL table and organize
    the data from the DataFrame without needing physical files.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，这个临时视图将作为一个 SQL 表来工作，并从 DataFrame 中组织数据，而不需要物理文件。
- en: 'Then we used the instantiated `SparkSession` variable to execute the SQL statements.
    Note that the name of the table is the same as the temporary view:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用实例化的 `SparkSession` 变量来执行 SQL 语句。请注意，表名与临时视图的名称相同：
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After doing the SQL queries we required, we proceeded to save our files using
    the .`write()` method, as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行所需的 SQL 查询后，我们继续使用 `.write()` 方法保存我们的文件，如下所示：
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The parameter inside the `parquet()` method defines the file’s path and name.
    Several other configurations are available when writing Parquet files, which we
    will cover later, in [*Chapter 7*](B19453_07.xhtml#_idTextAnchor227).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`parquet()` 方法内部的参数定义了文件的路径和名称。在写入 Parquet 文件时，还有其他一些配置可用，我们将在后面的 [*第 7 章*](B19453_07.xhtml#_idTextAnchor227)
    中介绍。'
- en: There’s more…
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: 'Although we used a temporary view to make our SQL statements, it is also possible
    to use the filtering and aggregation functions from the DataFrame. Let’s use the
    example from this recipe by filtering only the South American countries:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们使用了临时视图来编写我们的 SQL 语句，但也可以使用 DataFrame 中的过滤和聚合函数。让我们通过仅过滤南美国家来使用本食谱中的示例：
- en: '[PRE20]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You should see the following output:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下输出：
- en: '![Figure 5.14 – South American countries filtered using DataFrame operations](img/Figure_5.14_B19453.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.14 – 使用 DataFrame 操作过滤的南美国家](img/Figure_5.14_B19453.jpg)'
- en: Figure 5.14 – South American countries filtered using DataFrame operations
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14 – 使用 DataFrame 操作过滤的南美国家
- en: It is essential to understand that not all SQL functions can be used as DataFrame
    operations. You can see more practical examples of filtering and aggregation functions
    using DataFrame operations at [https://spark.apache.org/docs/2.2.0/sql-programming-guide.xhtml](https://spark.apache.org/docs/2.2.0/sql-programming-guide.xhtml).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解并非所有 SQL 函数都可以用作 DataFrame 操作。您可以在 [https://spark.apache.org/docs/2.2.0/sql-programming-guide.xhtml](https://spark.apache.org/docs/2.2.0/sql-programming-guide.xhtml)
    找到更多使用 DataFrame 操作进行过滤和聚合函数的实用示例。
- en: See also
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '*TowardsDataScience* has a fantastic blog post about SQL functions using PySpark,
    at [https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53](https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*TowardsDataScience* 有关于使用 PySpark 的 SQL 函数的精彩博客文章，请参阅 [https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53](https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53)。'
- en: Connecting to a NoSQL database (MongoDB)
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接到 NoSQL 数据库（MongoDB）
- en: MongoDB is an open source, unstructured, document-oriented database made in
    C++. It is well known in the data world for its scalability, flexibility, and
    speed.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB 是一个用 C++ 编写的开源、非结构化、面向文档的数据库。它在数据界因其可扩展性、灵活性和速度而闻名。
- en: As someone who will work with data (or maybe already does), it is essential
    to know how to explore a MongoDB (or any other unstructured) database. MongoDB
    has some peculiarities, which we will explore practically here.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于将处理数据（或者可能已经正在处理数据）的人来说，了解如何探索 MongoDB（或任何其他非结构化）数据库是至关重要的。MongoDB 有一些独特的特性，我们将在实践中进行探索。
- en: In this recipe, you will learn how to create a connection to access MongoDB
    documents via Studio 3T Free, a MongoDB GUI.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，您将学习如何创建连接以通过 Studio 3T Free 访问 MongoDB 文档，这是一个 MongoDB 图形用户界面。
- en: Getting ready
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To start our work with this robust database, first, we need to install and
    create a MongoDB server on our local machine. We already configured a MongoDB
    Docker container in [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022), so let’s
    get it up and running. You can do this using Docker Desktop or via the command
    line using the following command:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始使用这个强大的数据库，我们首先需要在本地机器上安装并创建一个 MongoDB 服务器。我们已经在 [*第 1 章*](B19453_01.xhtml#_idTextAnchor022)
    中配置了一个 MongoDB Docker 容器，所以让我们启动它。您可以使用 Docker Desktop 或通过以下命令在命令行中完成此操作：
- en: '[PRE21]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Don’t forget to change the variables using the username and password of your
    choice.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记使用您选择的用户名和密码更改变量。
- en: 'On Docker Desktop, you should see the following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Docker Desktop 上，您应该看到以下内容：
- en: '![Figure 5.15 – MongoDB Docker container running](img/Figure_5.15_B19453.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.15 – 运行中的 MongoDB Docker 容器](img/Figure_5.15_B19453.jpg)'
- en: Figure 5.15 – MongoDB Docker container running
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 – 运行中的 MongoDB Docker 容器
- en: The next step is to download and configure Studio 3T Free, free software the
    development community uses to connect to MongoDB servers. You can download this
    software from [https://studio3t.com/download-studio3t-free](https://studio3t.com/download-studio3t-free)
    and follow the installer’s steps for your given OS.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是下载并配置 Studio 3T Free，这是开发社区用来连接到 MongoDB 服务器的免费软件。您可以从 [https://studio3t.com/download-studio3t-free](https://studio3t.com/download-studio3t-free)
    下载此软件，并按照安装程序为您的操作系统提供的步骤进行操作。
- en: During the installation, a message may appear like that shown in the following
    figure. If so, you can leave the fields blank. We don’t need password encryption
    for local or testing purposes.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装过程中，可能会出现类似于以下图所示的提示信息。如果是这样，您可以留空字段。我们不需要为本地或测试目的进行密码加密。
- en: '![Figure 5.16 – Studio 3T Free password encryption message](img/Figure_5.16_B19453.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.16 – Studio 3T Free 密码加密消息](img/Figure_5.16_B19453.jpg)'
- en: Figure 5.16 – Studio 3T Free password encryption message
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 – Studio 3T Free 密码加密消息
- en: 'When the installation process is finished, you will see the following window:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 安装过程完成后，您将看到以下窗口：
- en: '![Figure 5.17 – Studio 3T Free connection window](img/Figure_5.17_B19453.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.17 – Studio 3T Free 连接窗口](img/Figure_5.17_B19453.jpg)'
- en: Figure 5.17 – Studio 3T Free connection window
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17 – Studio 3T Free 连接窗口
- en: We are now ready to connect our MongoDB instance to the IDE.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已准备好将我们的 MongoDB 实例连接到 IDE。
- en: How to do it…
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'Now that we have Studio 3T installed, let’s connect to our local MongoDB instance:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了 Studio 3T，让我们连接到我们的本地 MongoDB 实例：
- en: '**Creating the connection**: Right after you open Studio 3T, a window will
    appear and ask you to insert the connection string or manually configure it. Select
    the second option and click on **Next**.'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建连接**：在您打开 Studio 3T 后，将出现一个窗口，要求您输入连接字符串或手动配置它。选择第二个选项，然后点击 **下一步**。'
- en: 'You will have something like this:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到类似以下内容：
- en: '![Figure 5.18 – Studio 3T New Connection initial options](img/Figure_5.18_B19453.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.18 – Studio 3T 新连接初始选项](img/Figure_5.18_B19453.jpg)'
- en: Figure 5.18 – Studio 3T New Connection initial options
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.18 – Studio 3T 新连接初始选项
- en: '`localhost` and `27017`.'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`localhost` 和 `27017`。'
- en: 'Your screen should look as follows for now:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 目前您的屏幕应该看起来如下：
- en: '![Figure 5.19 – New Connection server information](img/Figure_5.19_B19453.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.19 – 新连接服务器信息](img/Figure_5.19_B19453.jpg)'
- en: Figure 5.19 – New Connection server information
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.19 – 新连接服务器信息
- en: Now select the **Authentication** tab under the **Connection group** field,
    and from the **Authentication Mode** drop-down menu, choose **Basic**.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在**连接组**字段下选择**身份验证**选项卡，然后从**身份验证模式**下拉菜单中选择**基本**。
- en: Three fields will appear—`admin` in the **Authentication** **DB** field.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 将出现三个字段——在**身份验证** **数据库**字段中为`admin`。
- en: '![Figure 5.20 – New Connection Authentication information](img/Figure_5.20_B19453.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.20 – 新连接身份验证信息](img/Figure_5.20_B19453.jpg)'
- en: Figure 5.20 – New Connection Authentication information
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.20 – 新连接身份验证信息
- en: '**Testing our connection**: With this configuration, we should be able to test
    our database connection. In the lower-left corner, select the **Test** **Connection**
    button.'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**测试我们的连接**：使用此配置，我们应该能够测试我们的数据库连接。在左下角，选择**测试** **连接**按钮。'
- en: 'If the credentials you provided are correct, you will see the following output:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您提供的凭据正确，您将看到以下输出：
- en: '![Figure 5.21 – Test connection successful](img/Figure_5.21_B19453.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.21 – 测试连接成功](img/Figure_5.21_B19453.jpg)'
- en: Figure 5.21 – Test connection successful
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.21 – 测试连接成功
- en: Click on the **Save** button, and the window will close.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**保存**按钮，窗口将关闭。
- en: '**Connecting to our database**: After we save our configuration, a window with
    the available connections will appear, including our newly created one:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**连接到我们的数据库**：在保存我们的配置后，将出现一个包含可用连接的窗口，包括我们新创建的连接：'
- en: '![Figure 5.22 – Connection manager with the connection created](img/Figure_5.22_B19453.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.22 – 创建连接后的连接管理器](img/Figure_5.22_B19453.jpg)'
- en: Figure 5.22 – Connection manager with the connection created
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.22 – 创建连接后的连接管理器
- en: 'Select the **Connect** button, and three default databases will appear: **admin**,
    **config**, and **local**, as shown in the following screenshot:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 选择**连接**按钮，将出现三个默认数据库：**admin**、**config**和**local**，如下截图所示：
- en: '![Figure 5.23 – The main page of the local MongoDB with the default databases
    on the server](img/Figure_5.23_B19453.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.23 – 服务器上默认数据库的本地 MongoDB 主页](img/Figure_5.23_B19453.jpg)'
- en: Figure 5.23 – The main page of the local MongoDB with the default databases
    on the server
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.23 – 服务器上默认数据库的本地 MongoDB 主页
- en: We have now finished our MongoDB configuration and are ready for the following
    recipes in this chapter and others, including *Chapters 6*, *11*, and *12*.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成了 MongoDB 的配置，并准备好进行本章以及其他章节的后续食谱，包括**第 6 章**、**第 11 章**和**第 12 章**。
- en: How it works…
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: 'Like available databases, creating and running MongoDB through a Docker container
    is straightforward. Check the following commands:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 就像可用的数据库一样，通过 Docker 容器创建和运行 MongoDB 是直接了当的。检查以下命令：
- en: '[PRE22]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As we saw in the *Getting ready* section, the most crucial information to be
    passed is the username and password (using the `-e` parameter), the ports over
    which to connect (using the `-p` parameter), and the container image version,
    which is the latest available.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在**准备就绪**部分所看到的，需要传递的最关键信息是用户名和密码（使用 `-e` 参数），连接的端口（使用 `-p` 参数），以及容器镜像版本，这是最新可用的。
- en: 'The architecture of the MongoDB container connected to Studio 3T Free is even
    more straightforward. Once the connection port is available, we can easily access
    the database. You can see the architectural representation as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到 Studio 3T Free 的 MongoDB 容器的架构甚至更加简单。一旦连接端口可用，我们就可以轻松访问数据库。您可以在以下架构表示中看到：
- en: '![Figure 5.24 – MongoDB with Docker image connected to Studio 3T Free](img/Figure_5.24_B19453.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.24 – 使用 Docker 镜像连接到 Studio 3T Free 的 MongoDB](img/Figure_5.24_B19453.jpg)'
- en: Figure 5.24 – MongoDB with Docker image connected to Studio 3T Free
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.24 – 使用 Docker 镜像连接到 Studio 3T Free 的 MongoDB
- en: 'As described at the beginning of this recipe, MongoDB is a document-oriented
    database. Its structure is similar to a JSON file, except each line is interpreted
    as a document and has its own `ObjectId`, as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如本食谱开头所述，MongoDB 是一个面向文档的数据库。其结构类似于 JSON 文件，除了每一行都被解释为一个文档，并且每个文档都有自己的 `ObjectId`，如下所示：
- en: '![Figure 5.25 – MongoDB document format](img/Figure_5.25_B19453.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.25 – MongoDB 文档格式](img/Figure_5.25_B19453.jpg)'
- en: Figure 5.25 – MongoDB document format
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.25 – MongoDB 文档格式
- en: 'The group of documents is referred to as a *collection*, which is better understood
    as a table representation in a structured database. You can see how it is hierarchically
    organized in the schema shown here:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 文档的集合被称为**集合**，这可以更好地理解为结构化数据库中的表表示。您可以在以下架构中看到它是如何按层次结构组织的：
- en: '![Figure 5.26 – MongoDB data structure](img/Figure_5.26_B19453.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图5.26 – MongoDB数据结构](img/Figure_5.26_B19453.jpg)'
- en: Figure 5.26 – MongoDB data structure
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.26 – MongoDB数据结构
- en: 'As we observed when logging in to MongoDB using Studio 3T Free, there are three
    default databases: `admin`, `config`, and `local`. For now, let’s disregard the
    last two since they pertain to operational working of the data. The `admin` database
    is the main one created by the `root` user. That’s why we provided this database
    for the **Authentication DB** option in *step 3*.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在使用Studio 3T Free登录MongoDB时观察到的，有三个默认数据库：`admin`、`config`和`local`。目前，让我们忽略最后两个，因为它们与数据的操作工作有关。`admin`数据库是由`root`用户创建的主要数据库。这就是为什么我们在*步骤3*中提供了这个数据库作为**认证数据库**选项的原因。
- en: Creating a user to ingest data and access specific databases or collections
    is generally recommended. However, we will keep using root access here and in
    the following recipes in this book for demonstration purposes.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个用户以导入数据并访问特定的数据库或集合通常是被推荐的。然而，为了演示目的，我们在这里以及本书接下来的食谱中将继续使用root访问权限。
- en: There’s more…
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容…
- en: The connection string will vary depending on how your MongoDB server is configured.
    For instance, when *replicas* or *sharded clusters* are in place, we need to specify
    which instances we want to connect to.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 连接字符串将根据你的MongoDB服务器配置而有所不同。例如，当存在*副本集*或*分片集群*时，我们需要指定我们想要连接到哪些实例。
- en: Note
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Sharded clusters are a complex and interesting topic. You can read more and
    go deeper on the topic in MongoDB’s official documentation at [https://www.mongodb.com/docs/manual/core/sharded-cluster-components/](https://www.mongodb.com/docs/manual/core/sharded-cluster-components/).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 分片集群是一个复杂且有趣的话题。你可以在MongoDB的官方文档中了解更多关于这个话题的内容，并深入探讨[https://www.mongodb.com/docs/manual/core/sharded-cluster-components/](https://www.mongodb.com/docs/manual/core/sharded-cluster-components/)。
- en: 'Let’s see an example of a standalone server string connection using basic authentication
    mode:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个使用基本认证模式的独立服务器字符串连接的例子：
- en: '[PRE23]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As you can see, it is similar to other database connections. If we wanted to
    connect to a local server, we would change the host to `localhost`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，它与其他数据库连接类似。如果我们想要连接到本地服务器，我们将主机更改为`localhost`。
- en: 'Now, for a replica or sharded cluster, the string connection looks like this:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于副本集或分片集群，字符串连接看起来是这样的：
- en: '[PRE24]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `authSource=admin` parameter in this URI is essential to inform MongoDB
    that we want to authenticate using the administration user of the database. Without
    it, an error or authentication will be raised, like the following output:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个URI中的`authSource=admin`参数对于通知MongoDB我们想要使用数据库的管理员用户进行认证是必不可少的。没有它，将会引发错误或认证问题，如下面的输出所示：
- en: '[PRE25]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Another way to avoid this error is to create a specific user to access the database
    and collection.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 避免这种错误的另一种方式是创建一个特定的用户来访问数据库和集合。
- en: SRV URI connection
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SRV URI连接
- en: MongoDB introduced the **Domain Name System** (**DNS**) seed list connection,
    constructed by a **DNS Service Record** (**SRV**) specification of data in the
    DNS, to try to solve this verbose string. We saw the possibility of using an SRV
    URI to configure the MongoDB connection in the first step of this recipe.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB引入了**域名系统**（**DNS**）种子列表连接，它通过DNS中的**服务记录**（**SRV**）规范构建数据，以尝试解决这种冗长的字符串。我们在本食谱的第一步中看到了使用SRV
    URI配置MongoDB连接的可能性。
- en: 'Here’s an example of how it looks:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个例子：
- en: '[PRE26]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: It is similar to the standard connection string format we saw earlier. However,
    we need to indicate the use of SRV at the beginning and then provide the DNS entry.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 它与我们在之前看到的标准连接字符串格式相似。然而，我们需要在开头指示使用SRV，然后提供DNS条目。
- en: This type of connection is advantageous when handling replicas or nodes since
    the SRV creates a single identity for the cluster. You can find a more detailed
    explanation of this, along with an outline of how to configure it, in the MongoDB
    official documentation at [https://www.mongodb.com/docs/manual/reference/connection-string/#dns-seed-list-connection-format](https://www.mongodb.com/docs/manual/reference/connection-string/#dns-seed-list-connection-format).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理副本或节点时，这种类型的连接具有优势，因为SRV为集群创建了一个单一的身份。你可以在MongoDB官方文档中找到更详细的解释，以及如何配置它的概述，请参阅[https://www.mongodb.com/docs/manual/reference/connection-string/#dns-seed-list-connection-format](https://www.mongodb.com/docs/manual/reference/connection-string/#dns-seed-list-connection-format)。
- en: See also
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'If you are interested, other MongoDB GUI tools are available on the market:
    [https://www.guru99.com/top-20-mongodb-tools.xhtml](https://www.guru99.com/top-20-mongodb-tools.xhtml).'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您感兴趣，市场上还有其他MongoDB图形用户界面工具可供选择：[https://www.guru99.com/top-20-mongodb-tools.xhtml](https://www.guru99.com/top-20-mongodb-tools.xhtml)。
- en: Creating our NoSQL table in MongoDB
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在MongoDB中创建我们的NoSQL表
- en: After successfully connecting and understanding how Studio 3T works, we will
    now import some MongoDB collections. We have seen in the *Connecting to a NoSQL
    database (MongoDB)* recipe how to get started with MongoDB, and in this recipe,
    we will import a MongoDB database and come to understand its structure. Although
    MongoDB has a specific format to organize data internally, understanding how a
    NoSQL database behaves is crucial when working with data ingestion.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功连接并了解Studio 3T的工作原理后，我们现在将导入一些MongoDB集合。我们在*连接到NoSQL数据库（MongoDB）*的配方中看到了如何开始使用MongoDB，在这个配方中，我们将导入一个MongoDB数据库并了解其结构。尽管MongoDB有特定的格式来组织内部数据，但在处理数据摄取时了解NoSQL数据库的行为至关重要。
- en: We will practice by ingesting the imported collections in the following recipes
    in this chapter.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的以下配方中通过摄取导入的集合进行练习。
- en: Getting ready
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe, we will use a sample dataset of Airbnb reviews called `listingsAndReviews.json`.
    You can find this dataset in the GitHub repository of this book at [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets/sample_airbnb](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets/sample_airbnb).
    After downloading it, put the file into our `mongo-local` directory, created in
    [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个配方，我们将使用一个名为`listingsAndReviews.json`的Airbnb评论样本数据集。您可以在本书的GitHub存储库中找到此数据集：[https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets/sample_airbnb](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets/sample_airbnb)。下载后，将文件放入我们创建的`mongo-local`目录中，如[*第1章*](B19453_01.xhtml#_idTextAnchor022)所示。
- en: 'I kept mine inside the `sample_airbnb` folder just for organization purposes,
    as you can see in the following screenshot:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我将我的数据库放在`sample_airbnb`文件夹中，只是为了组织目的，如以下截图所示：
- en: '![Figure 5.27 – Command line with listingsAndReviews.json](img/Figure_5.27_B19453.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图5.27 – 带有listingsAndReviews.json的命令行](img/Figure_5.27_B19453.jpg)'
- en: Figure 5.27 – Command line with listingsAndReviews.json
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.27 – 带有listingsAndReviews.json的命令行
- en: 'After downloading the dataset, we need to install `pymongo`, a Python library
    to connect to and manage MongoDB operations. To install it, use the following
    command:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 下载数据集后，我们需要安装`pymongo`，这是一个用于连接和管理MongoDB操作的Python库。要安装它，请使用以下命令：
- en: '[PRE27]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Feel free to create `virtualenv` for this installation.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以自由地为这个安装创建`virtualenv`。
- en: We are now ready to start inserting data into MongoDB. Don’t forget to check
    that your Docker image is up and running before we begin.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好开始向MongoDB中插入数据。在我们开始之前，别忘了检查您的Docker镜像是否正在运行。
- en: How to do it…
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Here are the steps to perform this recipe:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此配方的步骤如下：
- en: '`pymongo`, we can easily establish a connection with the MongoDB database.
    Refer to the following code:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pymongo`，我们可以轻松地与MongoDB数据库建立连接。请参考以下代码：
- en: '[PRE28]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Defining our database and collection**: We will create a database and collection
    instance using the client connection we instantiated.'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义我们的数据库和集合**：我们将使用实例化的客户端连接创建数据库和集合实例。'
- en: 'For the `json_collection` variable, insert the path where you put the Airbnb
    sample dataset:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`json_collection`变量，插入您放置Airbnb样本数据集的路径：
- en: '[PRE29]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`bulk_write` function, we will insert all the documents inside the JSON file
    into the sales collection we created and close the connection:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`bulk_write`函数，我们将把JSON文件中的所有文档插入到我们创建的销售集合中，并关闭连接：
- en: '[PRE30]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: No output is expected from this operation, but we can check the database to
    see if it is successful.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 此操作不应有任何输出，但我们可以检查数据库以查看操作是否成功。
- en: '**Checking the MongoDB database results**: Let’s check our database to see
    if the data was inserted correctly.'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检查MongoDB数据库结果**：让我们检查我们的数据库，看看数据是否已正确插入。'
- en: 'Open Studio 3T Free and refresh the connection (right-click on the connection
    name and select **Refresh All**). You should see a new database named **db_airbnb**
    has been created, containing a **reviews** collection, as shown in the following
    screenshot:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 打开Studio 3T Free并刷新连接（右键单击连接名称并选择**刷新所有**）。您应该会看到一个名为**db_airbnb**的新数据库已创建，其中包含一个**reviews**集合，如以下截图所示：
- en: '![Figure 5.28 – Database and collection successfully created on MongoDB](img/Figure_5.28_B19453.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.28 – 在 MongoDB 上成功创建数据库和集合](img/Figure_5.28_B19453.jpg)'
- en: Figure 5.28 – Database and collection successfully created on MongoDB
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.28 – 在 MongoDB 上成功创建数据库和集合
- en: With the collection now created and containing some data, let’s go deeper into
    how the code works.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在集合已经创建并包含了一些数据，让我们更深入地了解代码是如何工作的。
- en: How it works…
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: As you can see, the code we implemented is straightforward, using just a few
    lines to create and insert data in our database. However, there are important
    points to pay attention to due to the particularities of MongoDB.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们实现的代码非常简单，只用几行代码就能在我们的数据库中创建和插入数据。然而，由于 MongoDB 的特殊性，有一些重要的点需要注意。
- en: 'Let’s examine the code line by line now:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们逐行检查代码：
- en: '[PRE31]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This line defines the connection to our MongoDB database, and from this instance,
    we can create a new database and its collections.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码定义了与我们的 MongoDB 数据库的连接，并且从这个实例，我们可以创建一个新的数据库及其集合。
- en: Note
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Observe that the URI connection contains hardcoded values for the username and
    password. This must be avoided in real applications, and even development servers.
    It is recommended to store those values as environment variables or use a secret
    manager vault.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 URI 连接包含用户名和密码的硬编码值。在实际应用中，甚至在开发服务器上都必须避免这样做。建议将这些值存储为环境变量或使用秘密管理器保险库。
- en: Next, we define the database and collection names; you may have noticed we didn’t
    create them previously in our database. At the time of execution of the code,
    MongoDB checks whether the database exists; if not, MongoDB will create it. The
    same rule applies to the **reviews** collection.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义数据库和集合的名称；您可能已经注意到我们之前没有在数据库中创建它们。在代码执行时，MongoDB 会检查数据库是否存在；如果不存在，MongoDB
    将创建它。同样的规则适用于 **reviews** 集合。
- en: 'Notice the collection derives from the `db_cookbook` instance, which makes
    it clear that it is linked to the `db_airbnb` database:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 注意集合是从 `db_cookbook` 实例派生的，这使得它明确地与 `db_airbnb` 数据库相关联：
- en: '[PRE32]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Following the code, the next step is to open the JSON file and parse every
    line. Here we start to see some tricky peculiarities of MongoDB:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 按照代码，下一步是打开 JSON 文件并解析每一行。这里我们开始看到 MongoDB 的一些棘手的特殊性：
- en: '[PRE33]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'It is common to wonder why we actually need to parse the lines of JSON, since
    MongoDB accepts this format. Let’s check our `listingsAndReviews.json` file, as
    shown in the following screenshot:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 人们常常想知道为什么我们实际上需要解析 JSON 行，因为 MongoDB 接受这种格式。让我们检查下面的截图中的 `listingsAndReviews.json`
    文件：
- en: '![Figure 5.29 – JSON file with MongoDB document lines](img/Figure_5.29_B19453.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.29 – 包含 MongoDB 文档行的 JSON 文件](img/Figure_5.29_B19453.jpg)'
- en: Figure 5.29 – JSON file with MongoDB document lines
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.29 – 包含 MongoDB 文档行的 JSON 文件
- en: 'If we use any tool to verify this as valid JSON, it will certainly say it’s
    not a valid format. This happens because each line of this file represents one
    document of the MongoDB collection. Trying to open that file using only the conventional
    `open()` and `json.loads()` methods will produce an error like the following:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用任何工具来验证这是有效的 JSON，它肯定会说这不是有效的格式。这是因为这个文件的每一行代表 MongoDB 集合中的一个文档。仅使用传统的
    `open()` 和 `json.loads()` 方法尝试打开该文件将产生如下错误：
- en: '[PRE34]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: To make it acceptable to the Python interpreter, we need to open and read each
    line individually and append it to the `requesting_collection` list. Also, the
    `InsertOne()` method will ensure that each line is inserted separately. A problem
    that occurs while inserting a specific row will be much easier to identify.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使 Python 解释器能够接受，我们需要逐行打开和读取并追加到 `requesting_collection` 列表中。此外，`InsertOne()`
    方法将确保每行单独插入。在插入特定行时出现的问题将更容易识别。
- en: 'Finally, the `bulk_write()` will take the list of documents and insert them
    into the MongoDB database:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`bulk_write()` 将文档列表插入 MongoDB 数据库：
- en: '[PRE35]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This operation will finish without returning any output or error messages if
    everything is OK.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正常，这个操作将完成而不会返回任何输出或错误信息。
- en: There’s more…
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We have seen how simple it is to create a Python script to insert data into
    our MongoDB server. Nevertheless, MongoDB has database tools to provide the same
    result and can be executed via the command line. The `mongoimport` command is
    used to insert data into our database, as you can see in the following code:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到创建一个 Python 脚本来将数据插入我们的 MongoDB 服务器是多么简单。尽管如此，MongoDB 提供了数据库工具来提供相同的结果，并且可以通过命令行执行。`mongoimport`
    命令用于将数据插入我们的数据库，如下面的代码所示：
- en: '[PRE36]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: If you are interested to learn more about the other database tools and commands
    available, check the official MongoDB documentation at [https://www.mongodb.com/docs/database-tools/installation/installation/](https://www.mongodb.com/docs/database-tools/installation/installation/).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于其他数据库工具和命令的信息，请查看官方MongoDB文档，链接为[https://www.mongodb.com/docs/database-tools/installation/installation/](https://www.mongodb.com/docs/database-tools/installation/installation/).
- en: Restrictions on field names
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 字段名称的限制
- en: 'When loading data into MongoDB, one big problem is the restrictions on characters
    used in the field names. Due to MongoDB server versions or programming language
    specificities, sometimes the key names of fields come with a `$` prefix, and,
    by default, MongoDB is not compatible with it, creating an error like the following
    output:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 当将数据加载到MongoDB时，一个主要问题是对字段名称中使用的字符的限制。由于MongoDB服务器版本或编程语言特定性，有时字段的键名会带有`$`前缀，并且默认情况下，MongoDB与它不兼容，会创建如下错误输出：
- en: '[PRE37]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In this case, a JSON file dump was exported from a MongoDB server, and the
    reference of `ObjectID` came with the `$` prefix. Even though the more recent
    versions of MongoDB have started to accept these characters (see the thread here:
    [https://jira.mongodb.org/browse/SERVER-41628?fbclid=IwAR1t5Ld58LwCi69SrMCcDbhPGf2EfBWe_AEurxGkEWHpZTHaEIde0_AZ-uM%5D](https://jira.mongodb.org/browse/SERVER-41628?fbclid=IwAR1t5Ld58LwCi69SrMCcDbhPGf2EfBWe_AEurxGkEWHpZTHaEIde0_AZ-uM%5D)),
    it is a good practice to avoid using them where possible. In this case, we have
    two main options: remove all the restricted characters using a script, or encode
    the JSON file into a **Binary JavaScript Object Notation** (**BSON**) file. You
    can find out more about encoding the file into BSON format at [https://kb.objectrocket.com/mongo-db/how-to-use-python-to-encode-a-json-file-into-mongodb-bson-documents-545](https://kb.objectrocket.com/mongo-db/how-to-use-python-to-encode-a-json-file-into-mongodb-bson-documents-545).'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，从MongoDB服务器导出了一个JSON文件，`ObjectID`的引用带有`$`前缀。尽管MongoDB的最新版本已经开始接受这些字符（见此处线程：[https://jira.mongodb.org/browse/SERVER-41628?fbclid=IwAR1t5Ld58LwCi69SrMCcDbhPGf2EfBWe_AEurxGkEWHpZTHaEIde0_AZ-uM%5D](https://jira.mongodb.org/browse/SERVER-41628?fbclid=IwAR1t5Ld58LwCi69SrMCcDbhPGf2EfBWe_AEurxGkEWHpZTHaEIde0_AZ-uM%5D)），但在可能的情况下避免使用它们是一个好的做法。在这种情况下，我们有两个主要选项：使用脚本删除所有受限字符，或者将JSON文件编码成**二进制JavaScript对象表示法**（**BSON**）文件。你可以在[https://kb.objectrocket.com/mongo-db/how-to-use-python-to-encode-a-json-file-into-mongodb-bson-documents-545](https://kb.objectrocket.com/mongo-db/how-to-use-python-to-encode-a-json-file-into-mongodb-bson-documents-545)了解更多关于将文件编码成BSON格式的信息。
- en: See also
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: You can read more about the MongoDB restrictions on field names at [https://www.mongodb.com/docs/manual/reference/limits/#mongodb-limit-Restrictions-on-Field-Names](https://www.mongodb.com/docs/manual/reference/limits/#mongodb-limit-Restrictions-on-Field-Names).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://www.mongodb.com/docs/manual/reference/limits/#mongodb-limit-Restrictions-on-Field-Names](https://www.mongodb.com/docs/manual/reference/limits/#mongodb-limit-Restrictions-on-Field-Names)了解更多关于MongoDB对字段名称的限制。
- en: Ingesting data from MongoDB using PySpark
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PySpark从MongoDB导入数据
- en: Even though it seems impractical to create and ingest the data ourselves, this
    exercise can be applied to real-life projects. People who work with data are often
    involved in the architectural process of defining the type of database, helping
    other engineers to insert data from applications into a database server, and later
    ingesting only the relevant information for dashboards or other analytical tools.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然自己创建和导入数据似乎不太实际，但这个练习可以应用于实际项目。与数据打交道的人通常参与定义数据库类型的架构过程，帮助其他工程师将应用程序中的数据插入到数据库服务器中，然后仅导入仪表板或其他分析工具的相关信息。
- en: So far, we have created and evaluated our server and then created collections
    inside our MongoDB instance. With all this preparation, we can now ingest our
    data using PySpark.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经创建并评估了我们的服务器，然后在MongoDB实例内部创建了集合。有了所有这些准备工作，我们现在可以使用PySpark导入我们的数据。
- en: Getting ready
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe requires the execution of the *Creating our NoSQL table in MongoDB*
    recipe due to data insertion. However, you can create and insert other documents
    into the MongoDB database and use them here. If you do this, ensure you set the
    suitable configurations to make it run properly.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这个菜谱需要执行*在MongoDB中创建我们的NoSQL表*菜谱，因为需要数据插入。然而，你可以创建并插入其他文档到MongoDB数据库中，并在此处使用它们。如果你这样做，确保设置合适的配置以使其正常运行。
- en: Also, as in the *Creating our NoSQL table in MongoDB* recipe, check that the
    Docker container is up and running since this is our MongoDB instance’s primary
    data source. Let’s proceed to the ingesting!
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，正如在*在MongoDB中创建我们的NoSQL表*食谱中一样，检查Docker容器是否正在运行，因为这是我们MongoDB实例的主要数据源。让我们继续进行数据摄取！
- en: '![Figure 5.30 – Docker container for MongoDB is running](img/Figure_5.30._B19453.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![图5.30 – 运行中的MongoDB Docker容器](img/Figure_5.30._B19453.jpg)'
- en: Figure 5.30 – Docker container for MongoDB is running
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.30 – 运行中的MongoDB Docker容器
- en: How to do it…
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'You need to perform the following steps to try this recipe:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 要尝试这个食谱，你需要执行以下步骤：
- en: '`SparkSession`, but this time passing specific configurations for reading our
    MongoDB database, `db_airbnb`, such as the URI and the `.jars`:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`SparkSession`，但这次传递读取我们的MongoDB数据库`db_airbnb`的特定配置，例如URI和`.jars`：'
- en: '[PRE38]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We should expect a significant output here since Spark downloads the package
    and sets the rest of the configuration we passed:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该期望这里有一个显著的结果，因为Spark下载了包并设置了我们传递的其余配置：
- en: '![Figure 5.31 – SparkSession being initialized with MongoDB configurations](img/Figure_5.31._B19453.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![图5.31 – 使用MongoDB配置初始化SparkSession](img/Figure_5.31._B19453.jpg)'
- en: Figure 5.31 – SparkSession being initialized with MongoDB configurations
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.31 – 使用MongoDB配置初始化SparkSession
- en: '`SparkSession` we instantiated. Here, no output is expected because the `SparkSession`
    is set only to send logs at the `WARN` level:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们实例化的`SparkSession`。在这里，我们不应该期望有任何输出，因为`SparkSession`仅设置为在`WARN`级别发送日志：
- en: '[PRE39]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '**Getting our DataFrame schema**: We can see the collection’s schema using
    the print operation on the DataFrame:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**获取我们的DataFrame模式**：我们可以通过在DataFrame上执行打印操作来查看集合的模式：'
- en: '[PRE40]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'You should observe the following output:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该观察到以下输出：
- en: '![Figure 5.32 – Reviews DataFrame collection schema printed](img/Figure_5.32._B19453.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![图5.32 – Reviews DataFrame集合模式打印](img/Figure_5.32._B19453.jpg)'
- en: Figure 5.32 – Reviews DataFrame collection schema printed
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.32 – Reviews DataFrame集合模式打印
- en: As you can observe, the structure is similar to a JSON file with nested objects.
    Unstructured data is usually presented in this form and can hold a large amount
    of information to create a Python script to insert data into our data. Now, let’s
    go deeper and understand our code.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，结构类似于一个嵌套对象的JSON文件。非结构化数据通常以这种形式呈现，并且可以包含大量信息，以便创建一个Python脚本来将数据插入我们的数据中。现在，让我们更深入地了解我们的代码。
- en: How it works…
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'MongoDB required a few additional configurations in `SparkSession` to execute
    the `.read` function. It is essential to understand why we used the configurations
    instead of just using code from the documentation. Let’s explore the code for
    it:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB在`SparkSession`中需要一些额外的配置来执行`.read`函数。理解为什么我们使用配置而不是仅仅使用文档中的代码是至关重要的。让我们来探索相关的代码：
- en: '[PRE41]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note the use of `spark.mongodb.input.uri`, which tells our `SparkSession` that
    a *read* operation needs to be performed using a MongoDB URI. If, for instance,
    we wanted to do a *write* operation (or both read and write), we would just need
    to add the `spark.mongodb.output.uri` configuration.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 注意使用`spark.mongodb.input.uri`，它告诉我们的`SparkSession`需要进行一个使用MongoDB URI的*读取*操作。如果我们想进行*写入*操作（或两者都进行），我们只需要添加`spark.mongodb.output.uri`配置。
- en: Next, we pass the URI containing the user and password information, the name
    of the database, and the authentication source. Since we use the root user to
    retrieve the data, this last parameter is set to `admin`.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们传递包含用户和密码信息、数据库名称和认证源的URI。由于我们使用root用户检索数据，因此最后一个参数设置为`admin`。
- en: 'Next, we define the name of our collection to be used in the read operation:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义在读取操作中使用的集合名称：
- en: '[PRE42]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Note
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Even though it might seem odd to define these parameters in the SparkSession,
    and it is possible to set the database and collection, this is a good practice
    that has been adopted by the community when manipulating MongoDB connections.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在SparkSession中定义这些参数看起来可能有些奇怪，并且可以设置数据库和集合，但这是一种社区在操作MongoDB连接时采用的良好实践。
- en: '[PRE43]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Another new configuration here is `spark.jars.packages`. When using this key
    with the `.config()` method, Spark will search its available online packages,
    download them, and place them in the `.jar` folders to be used. Although this
    is an advantageous way to set the `.jar` connectors, this is not available for
    all databases.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里还有一个新的配置是`spark.jars.packages`。当使用这个键与`.config()`方法一起时，Spark将搜索其可用的在线包，下载它们，并将它们放置在`.jar`文件夹中以供使用。虽然这是一种设置`.jar`连接器的有利方式，但这并不是所有数据库都支持的。
- en: 'Once the connection is established, the reading process is remarkably similar
    to the JDBC: we pass the `.format()` of the database (here, `mongo`), and since
    the database and collection name are already set, we don’t need to configure `.option()`:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦建立连接，读取过程与 JDBC 非常相似：我们传递数据库的 `.format()`（这里为 `mongo`），由于数据库和集合名称已经设置，我们不需要配置
    `.option()`：
- en: '[PRE44]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'When executing `.load()`, Spark will verify whether the connection is valid
    and throw an error if not. In the following screenshot, you can see an example
    of the error message when the credentials are not correct:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行 `.load()` 时，Spark 将验证连接是否有效，如果无效则抛出错误。在下面的截图中，你可以看到一个错误信息的示例，当凭证不正确时：
- en: '![Figure 5.33 – Py4JJavaError: Authentication error to MongoDB connection](img/Figure_5.33._B19453.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.33 – Py4JJavaError：MongoDB 连接认证错误](img/Figure_5.33._B19453.jpg)'
- en: 'Figure 5.33 – Py4JJavaError: Authentication error to MongoDB connection'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.33 – 当 MongoDB 包未在配置中设置时引发的 Py4JJavaError：MongoDB 连接认证错误
- en: Even though we are handling an unstructured data format, as soon as PySpark
    transforms our collection into a DataFrame, all the filtering, cleaning, and manipulating
    of data is pretty much the same as PySpark data.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们正在处理非结构化数据格式，但一旦 PySpark 将我们的集合转换为 DataFrame，所有过滤、清理和操作数据的过程基本上与 PySpark
    数据相同。
- en: There’s more…
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: As we saw previously, PySpark error messages can be confusing and cause discomfort
    at first glance. Let’s explore other common errors when ingesting data from a
    MongoDB database without the proper configuration.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，PySpark 错误信息可能会让人困惑，并且一开始看起来会让人不舒服。让我们探索在没有适当配置的情况下从 MongoDB 数据库中摄取数据时出现的其他常见错误。
- en: 'In this example, let’s not set `spark.jars.packages` in the `SparkSession`
    configuration:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，让我们不要在 `SparkSession` 配置中设置 `spark.jars.packages`：
- en: '[PRE45]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'If you try to execute the preceding code (passing the rest of the memory settings),
    you will get the following output:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试执行前面的代码（传递剩余的内存设置），你将得到以下输出：
- en: '![Figure 5.34 – java.lang.ClassNotFoundException error when the MongoDB package
    is not set in the configuration](img/Figure_5.34._B19453.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.34 – 当 MongoDB 包未在配置中设置时引发的 java.lang.ClassNotFoundException 错误](img/Figure_5.34._B19453.jpg)'
- en: Figure 5.34 – java.lang.ClassNotFoundException error when the MongoDB package
    is not set in the configuration
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.34 – 当 MongoDB 包未在配置中设置时引发的 java.lang.ClassNotFoundException 错误
- en: Looking carefully at the second line, which begins with `java.lang.ClassNotFoundException`,
    the JVM highlights a missing package or class that needs to be searched for in
    a third-party repository. The package contains the connector code to our JVM and
    establishes communication with the database server.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细查看以 `java.lang.ClassNotFoundException` 开头的第二行，JVM 强调了一个需要搜索第三方存储库的缺失包或类。该包包含连接到我们的
    JVM 的连接器代码，并建立与数据库服务器的通信。
- en: 'Another widespread error message is `IllegalArgumentException`. This type of
    error indicates to the developer that an argument was wrongly passed to a method
    or class. Usually, when related to database connections, it refers to an invalid
    string connection, as in the following screenshot:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的错误信息是 `IllegalArgumentException`。此类错误向开发者表明，方法或类中传递了一个错误的参数。通常，当与数据库连接相关时，它指的是无效的字符串连接，如下面的截图所示：
- en: '![Figure 5.35 – IllegalArgumentException error when the URI is invalid](img/Figure_5.35._B19453.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.35 – 当 URI 无效时引发的 IllegalArgumentException 错误](img/Figure_5.35._B19453.jpg)'
- en: Figure 5.35 – IllegalArgumentException error when the URI is invalid
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.35 – 当 URI 无效时引发的 IllegalArgumentException 错误
- en: Although it seems unclear, there is a typo in the URI, where `db_aibnb/?` contains
    an extra forward slash. Removing it and running `SparkSession` again will make
    this error disappear.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然看起来不太清楚，但 URI 中存在一个拼写错误，其中 `db_aibnb/?` 包含一个额外的正斜杠。移除它并再次运行 `SparkSession`
    将会使此错误消失。
- en: Note
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is recommended to shut down and restart the kernel processes when re-defining
    the SparkSession configurations because SparkSession tends to append to the processes
    rather than replacing them.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 建议在重新定义 SparkSession 配置时关闭并重新启动内核进程，因为 SparkSession 倾向于向进程追加而不是替换它们。
- en: See also
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'MongoDB Spark connector documentation: [https://www.mongodb.com/docs/spark-connector/current/configuration/](https://www.mongodb.com/docs/spark-connector/current/configuration/)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MongoDB Spark 连接器文档：[https://www.mongodb.com/docs/spark-connector/current/configuration/](https://www.mongodb.com/docs/spark-connector/current/configuration/)
- en: 'You can check the MongoDB documentation for a full explanation of how the MongoDB
    connector behaves with PySpark: [https://www.mongodb.com/docs/spark-connector/current/read-from-mongodb/](https://www.mongodb.com/docs/spark-connector/current/read-from-mongodb/)'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在MongoDB文档中找到MongoDB连接器与PySpark交互的完整说明：[https://www.mongodb.com/docs/spark-connector/current/read-from-mongodb/](https://www.mongodb.com/docs/spark-connector/current/read-from-mongodb/)
- en: 'There are also some interesting use cases of MongoDB here: [https://www.mongodb.com/use-cases](https://www.mongodb.com/use-cases)'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里也有一些MongoDB的有趣用例：[https://www.mongodb.com/use-cases](https://www.mongodb.com/use-cases)
- en: Further reading
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[https://www.talend.com/resources/structured-vs-unstructured-data/](https://www.talend.com/resources/structured-vs-unstructured-data/)'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.talend.com/resources/structured-vs-unstructured-data/](https://www.talend.com/resources/structured-vs-unstructured-data/)'
- en: '[https://careerfoundry.com/en/blog/data-analytics/structured-vs-unstructured-data/](https://careerfoundry.com/en/blog/data-analytics/structured-vs-unstructured-data/)'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://careerfoundry.com/en/blog/data-analytics/structured-vs-unstructured-data/](https://careerfoundry.com/en/blog/data-analytics/structured-vs-unstructured-data/)'
- en: '[https://www.dba-ninja.com/2022/04/is-mongodbsrv-necessary-for-a-mongodb-connection.xhtml](https://www.dba-ninja.com/2022/04/is-mongodbsrv-necessary-for-a-mongodb-connection.xhtml)'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.dba-ninja.com/2022/04/is-mongodbsrv-necessary-for-a-mongodb-connection.xhtml](https://www.dba-ninja.com/2022/04/is-mongodbsrv-necessary-for-a-mongodb-connection.xhtml)'
- en: '[https://www.mongodb.com/docs/manual/reference/connection-string/#connection-string-options](https://www.mongodb.com/docs/manual/reference/connection-string/#connection-string-options)'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.mongodb.com/docs/manual/reference/connection-string/#connection-string-options](https://www.mongodb.com/docs/manual/reference/connection-string/#connection-string-options)'
- en: '[https://sparkbyexamples.com/spark/spark-createorreplacetempview-explained/](https://sparkbyexamples.com/spark/spark-createorreplacetempview-explained/)'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://sparkbyexamples.com/spark/spark-createorreplacetempview-explained/](https://sparkbyexamples.com/spark/spark-createorreplacetempview-explained/)'
