- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Logging and Monitoring Your Data Ingest in Airﬂow
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Airflow 中记录和监控您的数据摄取
- en: We already know how vital logging and monitoring are to manage applications
    and systems, and Airflow is no different. In fact, **Apache Airflow** already
    has built-in modules to create logs and export them. But what about improving
    them?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道日志记录和监控对于管理应用程序和系统是多么重要，Airflow 也不例外。事实上，**Apache Airflow** 已经内置了创建日志并导出的模块。但如何改进它们呢？
- en: In the previous chapter, *Putting Everything Together with Airﬂow*, we covered
    the fundamental aspects of Airflow, how to start our data ingestion, and how to
    orchestrate a pipeline and use the best data development practices. Now, let’s
    put into practice the best techniques to enhance logging and monitor Airflow pipelines.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，*使用 Airflow 整合一切*，我们介绍了 Airflow 的基本方面，如何启动我们的数据摄取，以及如何编排管道和使用最佳数据开发实践。现在，让我们将最佳技术付诸实践，以增强日志记录并监控
    Airflow 管道。
- en: 'In this chapter, you will learn the following recipes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习以下食谱：
- en: Creating basic logs in Airﬂow
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Airflow 中创建基本日志
- en: Storing log files in a remote location
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在远程位置存储日志文件
- en: Configuring logs in `airflow.cfg`
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `airflow.cfg` 中配置日志
- en: Designing advanced monitoring
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计高级监控
- en: Using notiﬁcation operators
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用通知操作员
- en: Using SQL operators for data quality
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SQL 操作员进行数据质量
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can find the code for this chapter in the GitHub repository here: [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此 GitHub 仓库中找到本章的代码：[https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook)。
- en: Installing and running Airflow
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装和运行 Airflow
- en: This chapter requires that Airflow is installed on your local machine. You can
    install it directly on your **operating system** (**OS**) or by using a Docker
    image. For more information, refer to the *Configuring Docker for Airflow* recipe
    in [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求您的本地机器上安装了 Airflow。您可以直接在您的 **操作系统**（**OS**）上安装它，或者使用 Docker 镜像。有关更多信息，请参阅
    [*第 1 章*](B19453_01.xhtml#_idTextAnchor022) 中的 *配置 Docker 以用于 Airflow* 食谱。
- en: 'After following the steps described in [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022),
    ensure your Airflow runs correctly. You can do that by checking the Airflow UI
    here: `http://localhost:8080`.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在遵循 [*第 1 章*](B19453_01.xhtml#_idTextAnchor022) 中描述的步骤之后，请确保您的 Airflow 运行正确。您可以通过检查
    Airflow UI 来做到这一点：`http://localhost:8080`。
- en: 'If you are using a Docker container (as I am) to host your Airflow application,
    you can check its status on the terminal by running the following command:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您像我一样使用 Docker 容器来托管您的 Airflow 应用程序，您可以通过运行以下命令在终端检查其状态：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can see the command running here:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到这里正在运行的命令：
- en: '![Figure 10.1 – Airflow containers running](img/Figure_9.01_B19453.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.1 – 运行的 Airflow 容器](img/Figure_9.01_B19453.jpg)'
- en: Figure 10.1 – Airflow containers running
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – 运行的 Airflow 容器
- en: 'For Docker, check the container status on **Docker Desktop**, as shown in the
    following screenshot:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Docker，请检查 **Docker Desktop** 上的容器状态，如下面的截图所示：
- en: '![Figure 10.2 – The Docker Desktop version of Airflow containers running](img/Figure_10.02_B19453.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.2 – 运行的 Docker Desktop 版本 Airflow 容器](img/Figure_10.02_B19453.jpg)'
- en: Figure 10.2 – The Docker Desktop version of Airflow containers running
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 – 运行的 Docker Desktop 版本 Airflow 容器
- en: Airflow environment variables in docker-compose
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: docker-compose 中的 Airflow 环境变量
- en: This section is aimed at users with Airflow running in a Docker container. If
    you install it directly on your machine, you can skip it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本节针对在 Docker 容器中运行 Airflow 的用户。如果您直接在您的机器上安装它，您可以跳过这一部分。
- en: We need to configure or change Airflow environment variables to complete most
    of the recipes in this chapter. This kind of configuration is supposed to be done
    by editing the `airflow.cfg` file. However, this can be tricky if you opt to run
    your Airflow application using `docker-compose`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要配置或更改 Airflow 环境变量来完成本章的大部分食谱。这种配置应该通过编辑 `airflow.cfg` 文件来完成。然而，如果您选择使用
    `docker-compose` 运行您的 Airflow 应用程序，这可能会很棘手。
- en: 'Ideally, we should be able to access the `airflow.cfg` file by mounting a volume
    in `docker-compose.yaml`, as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们应该能够通过在 `docker-compose.yaml` 中挂载卷来访问 `airflow.cfg` 文件，如下所示：
- en: '![Figure 10.3 – docker-compose.yaml volumes](img/Figure_10.03_B19453.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.3 – docker-compose.yaml 卷](img/Figure_10.03_B19453.jpg)'
- en: Figure 10.3 – docker-compose.yaml volumes
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 – docker-compose.yaml 卷
- en: Nevertheless, instead of reflecting the file in the local machine, it creates
    a directory named `airflow.cfg`. It is a bug known by the community (see [https://github.com/puckel/docker-airflow/issues/571](https://github.com/puckel/docker-airflow/issues/571))
    with no resolution.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它不是在本地机器上反映文件，而是创建一个名为`airflow.cfg`的目录。这是社区已知的一个错误（见[https://github.com/puckel/docker-airflow/issues/571](https://github.com/puckel/docker-airflow/issues/571)），但没有解决方案。
- en: 'To work around it, we will set all the `airflow.cfg` configurations in `docker-compose.yaml`
    using the environment variables, as shown in the following example:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们将使用环境变量在`docker-compose.yaml`中设置所有`airflow.cfg`配置，如下例所示：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For users who install and run Airflow directly on their local machine, you can
    proceed by following the steps that instruct you how to edit the `airflow.cfg`
    file.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于直接在本地机器上安装和运行Airflow的用户，您可以按照指示如何编辑`airflow.cfg`文件的步骤进行操作。
- en: Creating basic logs in Airﬂow
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Airﬂow中创建基本日志
- en: The internal Airflow logging library is based on the Python built-in logs, which
    provide flexible and configurable forms to capture and store log messages using
    different components of **directed acyclic graphs** (**DAGs**). Let’s start this
    chapter by covering the basic concepts of how Airflow logs work. This knowledge
    will allow us to apply more advanced concepts and create mature data ingestion
    pipelines in real-life projects.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow的内部日志库基于Python内置的日志，它提供了灵活和可配置的形式来捕获和存储使用**有向无环图**（**DAGs**）不同组件的日志消息。让我们从介绍Airflow日志的基本概念开始这一章。这些知识将使我们能够应用更高级的概念，并在实际项目中创建成熟的数据摄取管道。
- en: In this recipe, we will create a simple DAG to generate logs based on the default
    configurations of Airflow. We will also understand how Airflow internally sets
    the logging architecture.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将创建一个简单的DAG，根据Airflow的默认配置生成日志。我们还将了解Airflow内部如何设置日志架构。
- en: Getting ready
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the *Technical requirements* section for this recipe, since we will
    handle it with the same technology.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 参考此食谱的*技术要求*部分，因为我们将以相同的技术处理它。
- en: 'Since we will create a new DAG, let’s create a folder under the `dag/` directory
    called `basic_logging` and a file inside it called `basic_logging_dag.py` to insert
    our script. By the end, your folder structure should look like the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将创建一个新的DAG，让我们在`dag/`目录下创建一个名为`basic_logging`的文件夹，并在其中创建一个名为`basic_logging_dag.py`的文件来插入我们的脚本。最终，您的文件夹结构应该如下所示：
- en: '![Figure 10.4 – An Airflow directory with a basic_logging DAG structure](img/Figure_10.04_B19453.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图10.4 – 带有basic_logging DAG结构的Airflow目录](img/Figure_10.04_B19453.jpg)'
- en: Figure 10.4 – An Airflow directory with a basic_logging DAG structure
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 – 带有basic_logging DAG结构的Airflow目录
- en: How to do it…
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The goal is to understand how to create logs in Airflow properly so that the
    DAG script will be pretty straightforward:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是理解如何在Airflow中正确创建日志，以便DAG脚本将非常简单明了：
- en: 'Let’s start by importing the Airflow and Python libraries:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从导入Airflow和Python库开始：
- en: '[PRE2]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, let’s get the log configuration we want to use:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，让我们获取我们想要使用的日志配置：
- en: '[PRE3]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, let’s define `default_args` and the DAG object which Airflow can create:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义`default_args`和Airflow可以创建的DAG对象：
- en: '[PRE4]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Unlike in [*Chapter 9*](B19453_09.xhtml#_idTextAnchor319), here we will define
    which tasks belong to this DAG by assigning them to the operator instantiation
    in *step 5* of this recipe.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与[*第9章*](B19453_09.xhtml#_idTextAnchor319)不同，在这里我们将通过将任务分配到本食谱的*步骤5*中的操作实例化来定义哪些任务属于这个DAG。
- en: 'Now, let’s create three example functions only to return log messages. The
    functions will be named after the ETL steps, as you can see here:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建三个仅返回日志消息的示例函数。这些函数将根据ETL步骤命名，正如您在这里所看到的：
- en: '[PRE5]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Feel free to insert more log levels if you want to.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想的话，可以插入更多的日志级别。
- en: 'For each function, we will set a task using `PythonOperator` and the execution
    order:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个函数，我们将使用`PythonOperator`设置一个任务，并指定执行顺序：
- en: '[PRE6]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can see that we referred the DAG to each task by assigning the `dag` object
    (defined in *step 4*) to a `dag` parameter.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到我们通过将*步骤4*中定义的`dag`对象分配给`dag`参数来引用DAG到每个任务。
- en: Save the file and go to the Airflow UI.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 保存文件并转到Airflow UI。
- en: 'In the Airflow UI, look for the **basic_logging_dag** DAG and enable it by
    clicking the toggle button. The job will start right away, and if you check the
    **Graph** vision of the DAG, you should see something similar to the following
    screenshot:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Airflow UI中，查找**basic_logging_dag** DAG，并通过点击切换按钮来启用它。作业将立即开始，如果您检查DAG的**图形**视图，您应该会看到以下截图类似的内容：
- en: '![Figure 10.5 – The DAG Graph view showing the successful state of the tasks](img/Figure_10.05_B19453.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.5 – 显示任务成功状态的 DAG 图视图](img/Figure_10.05_B19453.jpg)'
- en: Figure 10.5 – The DAG Graph view showing the successful state of the tasks
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 – 显示任务成功状态的 DAG 图视图
- en: It means the pipeline ran successfully!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着管道运行成功！
- en: Let’s check the `logs/` directory on our local machine. This directory is at
    the same level as the `DAGs` folder, where we put our scripts.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查本地机器上的 `logs/` 目录。这个目录与 `DAGs` 文件夹处于同一级别，我们将脚本放在那里。
- en: You can see more folders inside if you open the `logs/` folder. Look for the
    one beginning with `dag_id= basic_logging` and open it.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您打开 `logs/` 文件夹，您可以看到更多文件夹。寻找以 `dag_id= basic_logging` 开头的文件夹并打开它。
- en: '![Figure 10.6 – The Airflow logs folder for the basic_logging DAG and its tasks](img/Figure_10.06_B19453.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.6 – basic_logging DAG 及其任务的 Airflow 日志文件夹](img/Figure_10.06_B19453.jpg)'
- en: Figure 10.6 – The Airflow logs folder for the basic_logging DAG and its tasks
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 – basic_logging DAG 及其任务的 Airflow 日志文件夹
- en: 'Now, select the folder named `task_id=transform_data` and open the log file
    inside. You should see something like the following screenshot:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，选择名为 `task_id=transform_data` 的文件夹并打开其中的日志文件。您应该会看到以下截图中的内容：
- en: '![Figure 10.7 – Log messages for the transform_data task](img/Figure_10.07_B19453.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.7 – transform_data 任务的日志消息](img/Figure_10.07_B19453.jpg)'
- en: Figure 10.7 – Log messages for the transform_data task
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 – transform_data 任务的日志消息
- en: As you can see, the logs were printed on the output and even colored accordingly
    with the log level, where **INFO** is in green and **ERROR** is in red.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，日志被打印在输出上，并且根据日志级别进行了相应的着色，其中 **INFO** 为绿色，**ERROR** 为红色。
- en: How it works…
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: This exercise was straightforward, but what if I told you that many developers
    struggle to understand how Airflow creates its logs? It often happens for two
    reasons – developers are used to inserting `print()` functions instead of logging
    methods and only check the records in the Airflow UI.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习很简单，但如果我告诉你许多开发者都难以理解 Airflow 如何创建其日志呢？这通常有两个原因——开发者习惯于插入 `print()` 函数而不是日志方法，并且只检查
    Airflow UI 中的记录。
- en: Depending on the Airflow configuration, it will not show `print()` messages
    on the UI, and messages used to debug or find where the code ran can be lost.
    Also, the Airflow UI has a limit on the number of record lines to show, and Spark
    error messages can be easily omitted in this case.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Airflow 的配置，它不会在 UI 上显示 `print()` 消息，用于调试或查找代码运行位置的日志消息可能会丢失。此外，Airflow UI
    对显示的记录行数有限制，在这种情况下，Spark 错误消息很容易被省略。
- en: 'That’s why it is vital to understand that, by default, Airflow stores all its
    logs under a `logs/` directory, even organizing it by `dag_id`, `run_id`, and
    each task separately, as we saw in *step 7*. This folder structure can also be
    changed or improved depending on your needs, and all you need to do is alter the
    `log_filename_template` variable in `airflow.cfg`. The following is how it is
    set by default:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正因如此，理解默认情况下 Airflow 将所有日志存储在 `logs/` 目录下至关重要，甚至按照 `dag_id`、`run_id` 和每个任务分别组织，正如我们在
    *步骤 7* 中所看到的。这个文件夹结构也可以根据您的需求进行更改或改进，您只需修改 `airflow.cfg` 中的 `log_filename_template`
    变量。以下是其默认设置：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, looking inside the log file, you can see that it is the same as what is
    on the UI, as shown in the following screenshot:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，查看日志文件，你可以看到它与 UI 上的内容相同，如下面的截图所示：
- en: '![Figure 10.8 – A complete log message stored in a log file found in the local
    Airflow log folder](img/Figure_10.08_B19453.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.8 – 存储在本地 Airflow 日志文件夹中的日志文件中的完整日志消息](img/Figure_10.08_B19453.jpg)'
- en: Figure 10.8 – A complete log message stored in a log file found in the local
    Airflow log folder
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8 – 存储在本地 Airflow 日志文件夹中的日志文件中的完整日志消息
- en: In the first lines, it is possible to see the internal calls Airflow makes to
    start a task, and even the specific function names, such as `taskinstance.py`
    or `standard_task_runner.py`. Those are all internal scripts. Then, we can see
    our log messages below in the file.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几行中，我们可以看到 Airflow 启动任务时调用的内部调用，甚至可以看到特定的函数名称，例如 `taskinstance.py` 或 `standard_task_runner.py`。这些都是内部脚本。然后，我们可以在文件下方看到我们的日志消息。
- en: 'If you look closely, you can see that the format for our logs is similar to
    the Airflow core. It happens for two reasons:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仔细观察，您可以看到我们的日志格式与 Airflow 核心类似。这有两个原因：
- en: 'At the beginning of our code, we used the `getLogger()` method to retrieve
    the configuration used by the `airflow.task` module, as you can see here:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的代码开头，我们使用了 `getLogger()` 方法来检索 `airflow.task` 模块使用的配置，如下所示：
- en: '[PRE8]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`airflow.task` uses the Airflow default configuration to format all logs, which
    can also be found inside the `airflow.cfg` file. Don’t worry about this now; we
    will cover it later in the *Configuring logs in* *airflow.cfg* recipe.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`airflow.task` 使用 Airflow 默认配置来格式化所有日志，这些配置也可以在 `airflow.cfg` 文件中找到。现在不用担心这个问题；我们将在
    *在 airflow.cfg 中配置日志* 菜谱中稍后介绍。'
- en: After defining the `logger` variable and setting the logging class configurations,
    the rest of the script is straightforward.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义 `logger` 变量和设置日志类配置之后，脚本的其他部分就很简单了。
- en: See also
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'You can read more details about Airflow logs on the Astronomer page here: [https://docs.astronomer.io/learn/logging](https://docs.astronomer.io/learn/logging).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 Astronomer 页面这里了解更多关于 Airflow 日志的详细信息：[https://docs.astronomer.io/learn/logging](https://docs.astronomer.io/learn/logging)。
- en: Storing log files in a remote location
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在远程位置存储日志文件
- en: By default, Airflow stores and organizes its logs in a local folder with easy
    access for developers, which facilitates the debugging process when something
    does not go as expected. However, working with larger projects or teams makes
    giving everyone access to an Airflow instance or server almost impracticable.
    Besides looking at the DAG console output, there are other ways to allow access
    to the logging folder without granting access to Airflow’s server.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Airflow 将其日志存储和组织在本地文件夹中，便于开发者访问，这有助于在预期之外出现问题时的调试过程。然而，对于较大的项目或团队来说，让每个人都能够访问
    Airflow 实例或服务器几乎是不切实际的。除了查看 DAG 控制台输出外，还有其他方法可以允许访问日志文件夹，而无需授予对 Airflow 服务器的访问权限。
- en: One of the most straightforward solutions is to export logs to external storage,
    such as S3 or **Google Cloud Storage**. The good news is that Airflow already
    has native support to export records to cloud resources.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的一种解决方案是将日志导出到外部存储，例如 S3 或 **Google Cloud Storage**。好消息是 Airflow 已经原生支持将记录导出到云资源。
- en: In this recipe, we will set a configuration in our `airflow.cfg` file that allows
    the use of the remote logging feature and test it using an example DAG.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将在 `airflow.cfg` 文件中设置一个配置，允许使用远程日志功能，并使用示例 DAG 进行测试。
- en: Getting ready
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the *Technical requirements* section for this recipe.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅此菜谱的 *技术要求* 部分。
- en: AWS S3
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS S3
- en: 'To complete this exercise, it is necessary to create an **AWS S3** bucket.
    Here are the steps required to accomplish it:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个练习，需要创建一个 **AWS S3** 存储桶。以下是完成此任务的步骤：
- en: 'Create an AWS account by following the steps here: [https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-creating.xhtml](https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-creating.xhtml)'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下步骤创建 AWS 账户：[https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-creating.xhtml](https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-creating.xhtml)
- en: 'Then, proceed to create an S3 bucket, guided by the AWS documentation here:
    [https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.xhtml](https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.xhtml)'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，根据 AWS 文档在此处创建 S3 存储桶：[https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.xhtml](https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.xhtml)
- en: 'In my case, I will create an S3 bucket called `airflow-cookbook` for use in
    this recipe, as you can see in the following screenshot:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，我将创建一个名为 `airflow-cookbook` 的 S3 存储桶，用于本菜谱，如下截图所示：
- en: '![Figure 10.9 – The AWS S3 Create bucket page](img/Figure_10.09_B19453.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.9 – AWS S3 创建存储桶页面](img/Figure_10.09_B19453.jpg)'
- en: Figure 10.9 – The AWS S3 Create bucket page
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9 – AWS S3 创建存储桶页面
- en: Airflow DAG code
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Airflow DAG 代码
- en: To avoid redundancy and focus on the goal of this recipe, which is to configure
    remote logging in Airflow, we will use the same DAG as the *Creating basic logs
    in Airﬂow* recipe. However, feel free to create another DAG with a different name
    but the same code.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免冗余并专注于本菜谱的目标，即配置 Airflow 的远程日志，我们将使用与 *在 Airflow 中创建基本日志* 菜谱相同的 DAG，但你可以自由地创建另一个具有不同名称但相同代码的
    DAG。
- en: How to do it…
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Here are the steps to perform this recipe:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是执行此菜谱的步骤：
- en: 'First, let’s create a programmatic user in our AWS account. Airflow will use
    this user to authenticate on AWS and will be able to write the logs. On your AWS
    console, select **IAM services**, and you will be redirected to a page similar
    to this:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们在我们的AWS账户中创建一个程序化用户。Airflow将使用此用户在AWS上进行身份验证，并将能够写入日志。在您的AWS控制台中，选择**IAM服务**，您将被重定向到一个类似于以下页面：
- en: '![Figure 10.10 – The AWS IAM main page](img/Figure_10.10_B19453.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图10.10 – AWS IAM主页面](img/Figure_10.10_B19453.jpg)'
- en: Figure 10.10 – The AWS IAM main page
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10 – AWS IAM主页面
- en: Since this is a test account with a strict purpose, I will ignore the alerts
    on the IAM dashboard.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这是一个具有严格目的的测试账户，我将忽略IAM仪表板上的警报。
- en: 'Then, select **Users** and **Add users**, as shown here:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，选择**用户**和**添加用户**，如图所示：
- en: '![Figure 10.11 – The AWS IAM Users page](img/Figure_10.11_B19453.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图10.11 – AWS IAM用户页面](img/Figure_10.11_B19453.jpg)'
- en: Figure 10.11 – The AWS IAM Users page
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11 – AWS IAM用户页面
- en: 'On the **Create user** page, insert a username that is easy to remember, as
    shown here:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在**创建用户**页面，插入一个易于记忆的用户名，如图所示：
- en: '![Figure 10.12 – The AWS IAM new user details](img/Figure_10.12_B19453.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图10.12 – AWS IAM新用户详情](img/Figure_10.12_B19453.jpg)'
- en: Figure 10.12 – The AWS IAM new user details
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12 – AWS IAM新用户详情
- en: Leave the checkbox unmarked and select **Next** to add the access policies.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 保持复选框未勾选，并选择**下一步**以添加访问策略。
- en: 'On the **Set permissions** page, select **Attach policies directly** and then
    look for **AmazonS3FullAccess** in the **Permission** **policies** checkbox:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**设置权限**页面，选择**直接附加策略**，然后在**权限策略**复选框中查找**AmazonS3FullAccess**：
- en: '![Figure 10.13 – AWS IAM set permissions for user creation](img/Figure_10.13_B19453.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图10.13 – AWS IAM为用户创建设置权限](img/Figure_10.13_B19453.jpg)'
- en: Figure 10.13 – AWS IAM set permissions for user creation
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.13 – AWS IAM为用户创建设置权限
- en: Since this is a testing exercise, we can use full access to the S3 resource.
    However, remember to attach specific policies to access the resources in a production
    environment.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个测试练习，我们可以使用对S3资源的完全访问权限。然而，请记住，在生产环境中访问资源时附加特定的策略。
- en: Select **Next** and then click on the **Create** **user** button.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 选择**下一步**，然后点击**创建用户**按钮。
- en: 'Now, retrieve the access key by selecting the user you created, go to **Security
    credentials**, and scroll down until you see the **Access keys** box. Then, create
    a new one and save the CSV file in an easily accessible place:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，通过选择您创建的用户，转到**安全凭证**，然后向下滚动直到您看到**访问密钥**框。然后创建一个新的，并将CSV文件保存在易于访问的地方：
- en: '![Figure 10.14 – Access key creation for a user](img/Figure_10.14_B19453.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图10.14 – 用户访问密钥创建](img/Figure_10.14_B19453.jpg)'
- en: Figure 10.14 – Access key creation for a user
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14 – 用户访问密钥创建
- en: Now, back in Airflow, let’s configure the connection between Airflow and our
    AWS account.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，回到Airflow，让我们配置Airflow与我们的AWS账户之间的连接。
- en: 'Create a new connection using the Airflow UI, and in the **Connection Type**
    field, select **Amazon S3**. In the **Extra** field, insert the following line
    with the credentials retrieved in *step 4*:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Airflow UI创建一个新的连接，并在**连接类型**字段中选择**Amazon S3**。在**额外**字段中，插入以下行，该行是在*步骤4*中检索到的凭证：
- en: '[PRE9]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Your page will look like the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 您的页面将看起来如下所示：
- en: '![Figure 10.15 – The Airflow UI on adding a new AWS S3 connector](img/Figure_10.15_B19453.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图10.15 – 添加新的AWS S3连接时的Airflow UI](img/Figure_10.15_B19453.jpg)'
- en: Figure 10.15 – The Airflow UI on adding a new AWS S3 connector
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.15 – 添加新的AWS S3连接时的Airflow UI
- en: Save it, and open your code editor in your Airflow directory.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 保存它，并在您的Airflow目录中打开您的代码编辑器。
- en: 'Now, let’s add the configurations to our `airflow.cfg` file. If you are using
    Docker to host Airflow, add the following lines to your `docker-compose.yaml file`,
    under the environment settings:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将配置添加到我们的`airflow.cfg`文件中。如果您使用Docker托管Airflow，请在环境设置下将以下行添加到您的`docker-compose.yaml`文件中：
- en: '[PRE10]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Your `docker-compose.yaml` file will look similar to this:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 您的`docker-compose.yaml`文件将类似于以下内容：
- en: '![Figure 10.16 – Remote logging configuration in docker-compose.yaml](img/Figure_10.16_B19453.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图10.16 – docker-compose.yaml中的远程日志配置](img/Figure_10.16_B19453.jpg)'
- en: Figure 10.16 – Remote logging configuration in docker-compose.yaml
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.16 – docker-compose.yaml中的远程日志配置
- en: 'If you installed Airflow directly on your local machine, you can instantly
    change the `airflow.cfg` file. Change the following lines in `airflow.cfg` and
    save it:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您直接在本地机器上安装了Airflow，您可以立即更改`airflow.cfg`文件。在`airflow.cfg`中更改以下行并保存它：
- en: '[PRE11]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: After the preceding changes, restart your Airflow application.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的更改之后，重新启动您的Airflow应用程序。
- en: 'With your refreshed Airflow, run `basic_logging_dag` and open your AWS S3\.
    Select the bucket you created in the *Getting ready* section, and you should see
    a new object inside of it, as follows:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你更新的 Airflow，运行 `basic_logging_dag` 并打开你的 AWS S3\. 选择你在 *准备就绪* 部分创建的桶，你应该能在其中看到一个新对象，如下所示：
- en: '![Figure 10.17 – The AWS S3 airflow-cookbook bucket objects](img/Figure_10.17_B19453.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.17 – AWS S3 airflow-cookbook 中的 bucket 对象](img/Figure_10.17_B19453.jpg)'
- en: Figure 10.17 – The AWS S3 airflow-cookbook bucket objects
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.17 – AWS S3 airflow-cookbook 中的 bucket 对象
- en: 'Then, select the object created, and you should be able to see more folders
    related to the tasks executed, as follows:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，选择创建的对象，你应该能看到更多与执行的任务相关的文件夹，如下所示：
- en: '![Figure 10.18 – AWS S3 airflow-cookbook showing the remote logs](img/Figure_10.18_B19453.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.18 – AWS S3 airflow-cookbook 显示远程日志](img/Figure_10.18_B19453.jpg)'
- en: Figure 10.18 – AWS S3 airflow-cookbook showing the remote logs
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.18 – AWS S3 airflow-cookbook 显示远程日志
- en: Finally, if you select one of the folders, you will see the same file you saw
    in the *Creating basic logs in Airﬂow* recipe. We successfully wrote logs in a
    remote location!
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，如果你选择其中一个文件夹，你将看到与在 *在 Airflow 中创建基本日志* 菜单中看到相同的文件。我们在远程位置成功写入了日志！
- en: How it works…
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: If you look at this recipe overall, it may seem considerable work. However,
    remember that we are making a configuration from zero, which generally takes time.
    Since we are somewhat used to creating an AWS S3 bucket and executing DAGs (see
    [*Chapter 2*](B19453_02.xhtml#_idTextAnchor064) and [*Chapter 9*](B19453_09.xhtml#_idTextAnchor319),
    respectively), let’s focus on setting the remote log configurations.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从整体上查看这个菜谱，可能会觉得这是一项相当大的工作。然而，记住我们是从零开始进行配置，这通常需要时间。由于我们已经习惯了创建 AWS S3 桶和执行
    DAG（分别见 [*第 2 章*](B19453_02.xhtml#_idTextAnchor064) 和 [*第 9 章*](B19453_09.xhtml#_idTextAnchor319)），让我们专注于设置远程日志配置。
- en: Our first action started with creating a connection in Airflow using the access
    keys generated on AWS. This step is required because, internally, Airflow will
    use those keys to authenticate in AWS and prove its identity.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步是在 Airflow 中创建一个连接，使用在 AWS 上生成的访问密钥。这一步是必需的，因为，内部上，Airflow 将使用这些密钥在 AWS
    中进行身份验证并证明其身份。
- en: 'Then, we changed the following Airflow configurations as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们按以下方式更改了以下 Airflow 配置：
- en: '[PRE12]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The two first lines are string configurations to set on Airflow whether remote
    logging is enabled and which bucket path will be used. The last two lines are
    related to the name of the connection we created on the `True` if we handle sensitive
    information.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 前两行是字符串配置，用于在 Airflow 中设置是否启用远程日志以及将使用哪个 bucket 路径。最后两行与我们在 `True` 时创建的连接名称有关，如果我们处理敏感信息。
- en: After restarting Airflow, the configurations will be reflected in our application,
    and by executing a DAG, we can already see the logs written in the S3 bucket.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在重启 Airflow 后，配置将在我们的应用程序中体现出来，通过执行 DAG，我们就可以看到在 S3 桶中写入的日志。
- en: As mentioned in the introduction of this recipe, this type of configuration
    is beneficial not only in big projects but also as a good practice when using
    Airflow, allowing developers to debug or retrieve information about code output
    without accessing the cluster or server.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如本菜谱介绍中所述，这种配置不仅在大项目中有益，而且在使用 Airflow 时也是一种良好的实践，允许开发者在不访问集群或服务器的情况下调试或检索有关代码输出的信息。
- en: 'Here, we covered an example using AWS S3, but it is also possible to use **Google
    Cloud Storage** or **Azure Blog Storage**. You can read more here: [https://airflow.apache.org/docs/apache-airflow/1.10.13/howto/write-logs.xhtml](https://airflow.apache.org/docs/apache-airflow/1.10.13/howto/write-logs.xhtml).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们介绍了一个使用 AWS S3 的示例，但也可以使用 **Google Cloud Storage** 或 **Azure Blob Storage**。你可以在这里了解更多信息：[https://airflow.apache.org/docs/apache-airflow/1.10.13/howto/write-logs.xhtml](https://airflow.apache.org/docs/apache-airflow/1.10.13/howto/write-logs.xhtml)。
- en: Note
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you don’t want to use remote logging anymore, you can simply remove the environment
    variables from your `docker-compose.yaml` or set `REMOTE_LOGGING` back to `False`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不再想使用远程日志，你可以简单地从你的 `docker-compose.yaml` 中移除环境变量，或者将 `REMOTE_LOGGING` 设置回
    `False`。
- en: See also
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'You can read more about remote logging in S3 on the Apache Airflow official
    documentation page here: [https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/logging/s3-task-handler.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/logging/s3-task-handler.xhtml).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 Apache Airflow 官方文档页面上了解更多关于 S3 中的远程日志信息：[https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/logging/s3-task-handler.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/logging/s3-task-handler.xhtml)。
- en: Configuring logs in airflow.cfg
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 airflow.cfg 中配置日志
- en: We had our first contact with the `airflow.cfg` file in the *Storing log files
    in a remote location* recipe. At a glance, we saw how powerful and handy this
    configuration file is. There are many ways to customize and improve Airflow just
    by editing it.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *将日志文件存储在远程位置* 配方中第一次接触了 `airflow.cfg` 文件。一眼望去，我们看到了这个配置文件是多么强大和方便。只需编辑它，就有许多方法可以自定义和改进
    Airflow。
- en: This exercise will teach how you to enhance your logs by setting applicable
    configurations in the `airflow.cfg` file.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习将教会你如何通过在 `airflow.cfg` 文件中设置适当的配置来增强你的日志。
- en: Getting ready
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the *Technical requirements* section for this recipe, since we will
    handle it with the same technology.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅本配方的 *技术要求* 部分，因为我们将以相同的技术来处理它。
- en: Airflow DAG code
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Airflow DAG 代码
- en: To avoid redundancy and focus on the goal of this recipe, which is to configure
    remote logging in Airflow, we will use the same DAG as the *Creating basic logs
    in Airﬂow* recipe. However, feel free to create another DAG with a different name
    but the same code.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免冗余并专注于本配方的目标，即配置 Airflow 的远程日志，我们将使用与 *在 Airflow 中创建基本日志* 配方相同的 DAG。然而，你也可以创建另一个具有不同名称但相同代码的
    DAG。
- en: How to do it…
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Since we will use the same DAG code from *Creating basic logs in Airﬂow,* let’s
    jump right to the required configuration to format our logs:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用与 *在 Airflow 中创建基本日志* 相同的 DAG 代码，让我们直接跳到格式化日志所需配置：
- en: 'Let’s begin by setting the configuration in our `docker-compose.yaml`. In the
    environment section, insert the following line and save the file:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从在我们的 `docker-compose.yaml` 中设置配置开始。在环境部分，插入以下行并保存文件：
- en: '[PRE13]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Your `docker-compose` file should look like this:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你的 `docker-compose` 文件应该看起来像这样：
- en: '![Figure 10.19 – Formatting log configuration in docker-compose.yaml](img/Figure_10.19_B19453.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.19 – 在 docker-compose.yaml 中格式化日志配置](img/Figure_10.19_B19453.jpg)'
- en: Figure 10.19 – Formatting log configuration in docker-compose.yaml
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.19 – 在 docker-compose.yaml 中格式化日志配置
- en: 'If you directly edit the `airflow.cfg` file, search for the `log_format` variable,
    and change it to the following line:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你直接编辑 `airflow.cfg` 文件，搜索 `log_format` 变量，并将其更改为以下行：
- en: '[PRE14]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Your code will look like this:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你的代码将看起来像这样：
- en: '![Figure 10.20 – log_format inside airflow.cfg](img/Figure_10.20_B19453.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.20 – airflow.cfg 中的 log_format](img/Figure_10.20_B19453.jpg)'
- en: Figure 10.20 – log_format inside airflow.cfg
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.20 – airflow.cfg 中的 log_format
- en: Save it, and go to the next step.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 保存它，然后进行下一步。
- en: We added a few more items in the log line, which we will cover later.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在日志行中添加了一些更多项目，我们将在稍后介绍。
- en: Note
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Be very attentive here. In the `airflow.cfg` file, the `%` character is doubled,
    unlike in the `docker-compose` file.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里要非常注意。在 `airflow.cfg` 文件中，`%` 字符是双写的，与 `docker-compose` 文件不同。
- en: 'Now, let’s restart Airflow. You can do it by stopping the Docker container
    and rerunning it with the following commands:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们重新启动 Airflow。你可以通过停止 Docker 容器并使用以下命令重新运行它来完成：
- en: '[PRE15]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, let’s head up to the Airflow UI and run our DAG called `basic_logging_dag`.
    On the DAG page, look in the top-right corner and select the play button (depicted
    by an arrow), followed by **Trigger DAG**, as follows:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，让我们前往 Airflow UI 并运行我们称为 `basic_logging_dag` 的 DAG。在 DAG 页面上，查看右上角并选择播放按钮（由箭头表示），然后选择
    **触发 DAG**，如下所示：
- en: '![Figure 10.21 – basic_logging_dag trigger button on the right side of the
    page](img/Figure_10.21_B19453.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.21 – 页面右侧的基本日志触发按钮](img/Figure_10.21_B19453.jpg)'
- en: Figure 10.21 – basic_logging_dag trigger button on the right side of the page
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.21 – 页面右侧的基本日志触发按钮
- en: The DAG will start to run immediately.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: DAG 将立即开始运行。
- en: 'Now, let’s see the logs generated by one task. I will pick the `extract_data`
    task, and the log will look like this:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看一个任务生成的日志。我将选择 `extract_data` 任务，日志将看起来像这样：
- en: '![Figure 10.22 – The formatted log output for extract_data task](img/Figure_10.22_B19453.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.22 – extract_data 任务的格式化日志输出](img/Figure_10.22_B19453.jpg)'
- en: Figure 10.22 – The formatted log output for extract_data task
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.22 – extract_data 任务的格式化日志输出
- en: If you look closely, you will see that we now have the process number displayed
    on the output.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察，你会看到现在输出中显示了进程号。
- en: Note
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you opt to maintain continuity from the last recipe, *Storing log files in
    a remote location*, remember that your logs are stored in a remote location.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择从上一个配方中保持连续性，*将日志文件存储在远程位置*，请记住你的日志存储在远程位置。
- en: How it works…
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'As we can see, altering any logging information is simple, since Airflow uses
    the Python logging library behind the scenes. Now, let’s take a look at our output:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，更改任何日志信息都很简单，因为Airflow在幕后使用Python日志库。现在，让我们看看我们的输出：
- en: '![Figure 10.23 – The formatted log output for the extract_data task](img/Figure_10.23_B19453.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图10.23 – extract_data任务的格式化日志输出](img/Figure_10.23_B19453.jpg)'
- en: Figure 10.23 – The formatted log output for the extract_data task
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.23 – extract_data任务的格式化日志输出
- en: As you can see, before the process name (for example, `airflow.task`), we also
    have the number of the running process. It can be helpful information when running
    multiple processes simultaneously, allowing us to understand which one is taking
    longer to complete and what is running.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在进程名称（例如，`airflow.task`）之前，我们还有运行进程的编号。当同时运行多个进程时，这可以是有用的信息，使我们能够了解哪个进程完成得较慢以及正在运行什么。
- en: 'Let’s look at the code we inserted:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们插入的代码：
- en: '[PRE16]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As you can see, variables such as `asctime`, `process`, and `filename` are
    identical to the ones we saw in [*Chapter 8*](B19453_08.xhtml#_idTextAnchor280).
    Also, since a core Python function operates behind the scenes, we can add more
    information based on the allowed attributes. You can find the list here: [https://docs.python.org/3/library/logging.xhtml#logrecord-attributes](https://docs.python.org/3/library/logging.xhtml#logrecord-attributes).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，变量如`asctime`、`process`和`filename`与我们之前在[*第8章*](B19453_08.xhtml#_idTextAnchor280)中看到的相同。此外，由于底层是一个核心Python函数，我们可以根据允许的属性添加更多信息。您可以在这里找到列表：[https://docs.python.org/3/library/logging.xhtml#logrecord-attributes](https://docs.python.org/3/library/logging.xhtml#logrecord-attributes)。
- en: Going deeper in airflow.cfg
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深入了解airflow.cfg
- en: Now, let’s go deeper into Airflow configurations. As you can observe, Airflow
    resources are orchestrated by the `airflow.cfg` file. Using a single file, we
    can determine how to send email notifications (we will cover this in the *Using
    notiﬁcations operators* recipe), when DAGs will reflect a code change, how logs
    will be displayed, and so on.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更深入地了解Airflow配置。如您所观察到的，Airflow资源是由`airflow.cfg`文件编排的。使用单个文件，我们可以确定如何发送电子邮件通知（我们将在*使用通知操作符*配方中介绍），DAG何时反映代码更改，日志如何显示等等。
- en: 'It is also possible to set these configurations by exporting environment variables,
    and this has priority over the configuration setting on `airflow.cfg`. This prioritization
    happens because, internally, Airflow translates the content from `airflow.cfg`
    to environment variables, broadly speaking. You can read more here: [https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.xhtml#environment-variable](https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.xhtml#environment-variable).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这些配置也可以通过导出环境变量来设置，并且这比`airflow.cfg`上的配置设置具有优先级。这种优先级的发生是因为，从内部来说，Airflow将`airflow.cfg`的内容转换为环境变量，广义上讲。您可以在这里了解更多：[https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.xhtml#environment-variable](https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.xhtml#environment-variable)。
- en: 'Let’s look at the logging configuration in the Airflow **REFERENCES** section.
    We can see many other customization possibilities, such as coloring, a specific
    format for DAG processors, and extra logs for third-party applications, as shown
    here:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看Airflow的**引用**部分中的日志配置。我们可以看到许多其他定制可能性，例如着色、DAG处理器的特定格式以及第三方应用的额外日志，如这里所示：
- en: '![Figure 10.24 – Airflow documentation for the logging configuration](img/Figure_10.24_B19453.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图10.24 – Airflow的日志配置文档](img/Figure_10.24_B19453.jpg)'
- en: Figure 10.24 – Airflow documentation for the logging configuration
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.24 – Airflow的日志配置文档
- en: 'The fantastic part of this documentation is that we have references to configure
    directly in `airflow.cfg` or environment variables. You can see the complete reference
    list here: [https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml#logging](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml#logging).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这份文档的精彩之处在于，我们可以在`airflow.cfg`或环境变量中直接配置引用。您可以在这里查看完整的引用列表：[https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml#logging](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml#logging)。
- en: After we get used to the Airflow dynamics, testing new configurations or formats
    is straightforward, especially when we have a testing server to do so. However,
    simultaneously, we need to be cautious when changing anything internally; otherwise,
    we can impair our whole application.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们熟悉了Airflow的动态之后，测试新的配置或格式变得简单，尤其是当我们有一个测试服务器来做这件事时。然而，同时，我们在更改任何内部内容时需要谨慎；否则，我们可能会损害整个应用程序。
- en: There’s more…
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容…
- en: In *step 1*, we mentioned avoiding the use of double `%` characters when setting
    the variables in `docker-compose` – let’s now cover this!
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤1*中，我们提到了在`docker-compose`中设置变量时避免使用双`%`字符 – 现在我们来解决这个问题！
- en: 'The `string` variable we pass for `docker-compose` will be read by an internal
    Python logging function, which will not recognize the double `%` pattern. Instead,
    it will understand the default format for the logs in Airflow needs to be equal
    to that string variable, and all the DAG logs will look like this:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们传递给`docker-compose`的`string`变量将被一个内部的Python日志功能读取，它不会识别双`%`模式。相反，它将理解Airflow日志的默认格式需要等于该字符串变量，所有的DAG日志都将看起来像这样：
- en: '![Figure 10.25 – An error when the environment variable for log_format is not
    correctly set](img/Figure_10.25_B19453.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图10.25 – 当log_format环境变量设置不正确时的错误](img/Figure_10.25_B19453.jpg)'
- en: Figure 10.25 – An error when the environment variable for log_format is not
    correctly set
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.25 – 当log_format环境变量设置不正确时的错误
- en: Now, inside the `airflow.cfg` file, the double `%` character is a Bash format
    pattern that works like a modulo operator.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在`airflow.cfg`文件中，双`%`字符是一个Bash格式模式，它像模运算符一样工作。
- en: See also
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'See the whole list of configurations for Airflow here: [https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml).'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里查看Airflow的完整配置列表：[https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml)。
- en: Designing advanced monitoring
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计高级监控
- en: After spending some time learning and practicing logging concepts, we can advance
    a little more in the subject of monitoring. We can monitor results from all our
    logging collection work and generate insightful monitoring dashboards and alerts,
    with the right monitoring message stored.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在花费一些时间学习和实践日志概念之后，我们可以在监控主题上更进一步。我们可以监控来自所有日志收集工作的结果，并生成有洞察力的监控仪表板和警报，同时存储正确的监控信息。
- en: In this recipe, we will cover the Airflow metrics integrated with StatsD, a
    platform that collects system statistics, and their purpose to help us achieve
    a mature pipeline.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本菜谱中，我们将介绍与StatsD集成的Airflow指标，StatsD是一个收集系统统计信息的平台，以及它们的目的，帮助我们实现成熟的管道。
- en: Getting ready
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This exercise will focus on bringing clarity to the Airflow monitoring metrics
    and how to build a robust architecture to structure it.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习将专注于使Airflow监控指标更加清晰，以及如何构建一个健壮的架构来组织它。
- en: 'As a requirement for this recipe, it is vital to keep in mind the following
    basic Airflow architecture:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本菜谱的要求，牢记以下基本Airflow架构至关重要：
- en: '![Figure 10.26 – An Airflow high-level architecture diagram](img/Figure_10.26_B19453.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图10.26 – Airflow高级架构图](img/Figure_10.26_B19453.jpg)'
- en: Figure 10.26 – An Airflow high-level architecture diagram
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.26 – Airflow高级架构图
- en: 'Airflow components, from a high-level perspective, are composed of the following:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次的角度来看，Airflow组件由以下组成：
- en: A **web server**, where we can access the Airflow UI.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**Web服务器**，我们可以访问Airflow UI。
- en: A relational database to store metadata and other helpful information for use
    in the DAGs or tasks. To keep it simple, we will work with just one type of database;
    however, there can be more than one.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个关系型数据库，用于存储元数据和DAG或任务中使用的其他有用信息。为了简化，我们将只使用一种类型的数据库；然而，可能会有多个。
- en: The **scheduler**, which will consult the information inside the database to
    send it to the workers.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调度器**，将咨询数据库中的信息并将其发送给工作者。'
- en: A **Celery** application, responsible for queueing the requests sent from the
    scheduler and the workers.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**Celery**应用程序，负责排队来自调度器和工人的请求。
- en: The **workers**, which will execute the DAG and tasks.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作者**，将执行DAG和任务。'
- en: With this in mind, we can proceed to the next section.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，我们可以继续到下一节。
- en: How to do it…
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Let’s see the main items to design advanced monitoring:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看设计高级监控的主要项目：
- en: '**Counters**: As the name suggests, this metric will provide information about
    the counts of actions inside Airflow. This metric provides a count of running
    tasks, failed tasks, and so on. In the following figure, you can see some examples:'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计数器**：正如其名所示，这个指标将提供关于Airflow内部操作计数的详细信息。此指标提供了正在运行的任务、失败的任务等的计数。在下面的图中，您可以看到一些示例：'
- en: '![Figure 10.27 – A list of counter metric examples to monitor Airflow workflows](img/Figure_10.27_B19453.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图10.27 – 监控Airflow工作流的计数器指标示例列表](img/Figure_10.27_B19453.jpg)'
- en: Figure 10.27 – A list of counter metric examples to monitor Airflow workflows
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.27 – 监控Airflow的计数器指标示例列表
- en: '**Timers**: This metric tells us how long a task or DAG takes to complete or
    load a file. In the following figure, you can see more:'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定时器**：这个指标告诉我们任务或DAG完成或加载文件所需的时间。在下面的图中，您可以看到更多：'
- en: '![Figure 10.28 – A list of timer examples to monitor Airflow workflows](img/Figure_10.28_B19453.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图10.28 – 监控Airflow工作流的定时器示例列表](img/Figure_10.28_B19453.jpg)'
- en: Figure 10.28 – A list of timer examples to monitor Airflow workflows
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.28 – 监控Airflow工作流的定时器示例列表
- en: '**Gauges**: Finally, the last metric type gives us a more visual overview.
    Gauges use timers or counters metrics to illustrate whether we are reaching a
    defined threshold. In the following figure, there are some examples of gauges:'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**量规**：最后，最后一种指标类型给我们提供了一个更直观的概览。量规使用定时器或计数器指标来表示我们是否达到了定义的阈值。在下面的图中，有一些量规的示例：'
- en: '![Figure 10.29 – A list of gauge examples to be used to monitor Airflow](img/Figure_10.29_B19453.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图10.29 – 用于监控Airflow的量规示例列表](img/Figure_10.29_B19453.jpg)'
- en: Figure 10.29 – A list of gauge examples to be used to monitor Airflow
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.29 – 用于监控Airflow的量规示例列表
- en: With the metrics defined and on our radar, we can proceed with the architecture
    design to integrate it.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了指标并将其纳入我们的视线后，我们可以继续进行架构设计以集成它。
- en: '**StatsD**: Now, let’s add **StatsD** to the architecture drawing we saw in
    the *Getting ready* section. You will have something like this:'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**StatsD**：现在，让我们将**StatsD**添加到我们在“准备就绪”部分看到的架构图中。您将得到如下内容：'
- en: '![Figure 10.30 – StatsD integration and coverage for the Airflow components
    architecture](img/Figure_10.30_B19453.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图10.30 – StatsD集成和覆盖Airflow组件架构](img/Figure_10.30_B19453.jpg)'
- en: Figure 10.30 – StatsD integration and coverage for the Airflow components architecture
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.30 – StatsD集成和覆盖Airflow组件架构
- en: StatsD can collect the metrics from all the components inside the dotted rectangle
    and direct them to a monitoring tool.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: StatsD可以从虚线框内的所有组件中收集指标并将它们直接发送到监控工具。
- en: '**Prometheus and Grafana**: Then, we can plug StatsD into Prometheus, which
    serves as one of Grafana’s data sources. Adding these tools into our architecture
    will look something like this:'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Prometheus和Grafana**：然后，我们可以将StatsD连接到Prometheus，它作为Grafana的数据源之一。将这些工具添加到我们的架构中看起来将如下所示：'
- en: '![Figure 10.31 – A Prometheus and Grafana integration with StatsD and Airflow
    diagram](img/Figure_10.31_B19453.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图10.31 – Prometheus和Grafana与StatsD和Airflow的集成图](img/Figure_10.31_B19453.jpg)'
- en: Figure 10.31 – A Prometheus and Grafana integration with StatsD and Airflow
    diagram
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.31 – Prometheus和Grafana与StatsD和Airflow的集成图
- en: Now, let’s understand the components behind this architecture.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解这个架构背后的组件。
- en: How it works…
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Let’s start understanding what StatsD is. StatsD is a daemon developed by the
    Etsy company to aggregate and collect application metrics. Generally, any application
    can send metrics using a simple protocol, such as **User Datagram Protocol** (**UDP**).
    With this protocol, the sender doesn’t need to wait for a response from StatsD,
    making the process simple. After listening and aggregating data for some time,
    StatsD will send the metrics to output storage, which is Prometheus.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始了解StatsD是什么。StatsD是由Etsy公司开发的一个守护进程，用于聚合和收集应用程序指标。通常，任何应用程序都可以使用简单的协议，如**用户数据报协议**（**UDP**）发送指标。使用此协议，发送者不需要等待StatsD的响应，这使得过程变得简单。在监听和聚合数据一段时间后，StatsD将指标发送到输出存储，即Prometheus。
- en: 'The StatsD integration and installation can be done using the following command:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: StatsD的集成和安装可以使用以下命令完成：
- en: '[PRE17]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If you want to know more about it, you can refer to the Airflow documentation
    here: [https://airflow.apache.org/docs/apache-airflow/2.5.1/administration-and-deployment/logging-monitoring/metrics.xhtml#counters](https://airflow.apache.org/docs/apache-airflow/2.5.1/administration-and-deployment/logging-monitoring/metrics.xhtml#counters).'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多，可以参考 Airflow 文档此处：[https://airflow.apache.org/docs/apache-airflow/2.5.1/administration-and-deployment/logging-monitoring/metrics.xhtml#counters](https://airflow.apache.org/docs/apache-airflow/2.5.1/administration-and-deployment/logging-monitoring/metrics.xhtml#counters)。
- en: Then, Prometheus and Grafana will gather the metrics and transform them into
    a more visual resource. You don’t need to worry about this now; we will learn
    more about it in [*Chapter 12*](B19453_12.xhtml#_idTextAnchor433).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，Prometheus 和 Grafana 将收集指标并将它们转换成更直观的资源。你现在不需要担心这个问题；我们将在[*第12章*](B19453_12.xhtml#_idTextAnchor433)中了解更多。
- en: 'For each metric we saw in the three first steps in the *How to do it…* section,
    we can set a threshold to trigger an alert when it has trespassed. All the metrics
    are presented in the *How to do it…* section, and some more can be found here:
    https://airflow.apache.org/docs/apache-airflow/2.5.1/administration-and-deployment/logging-monitoring/metrics.xhtml#counters.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在“如何做…”部分的前三个步骤中看到的每个指标，我们都可以设置一个阈值，当它超过阈值时触发警报。所有指标都在“如何做…”部分中展示，更多指标可以在这里找到：https://airflow.apache.org/docs/apache-airflow/2.5.1/administration-and-deployment/logging-monitoring/metrics.xhtml#counters。
- en: There’s more…
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'Besides StatsD, there are other tools we can plug into Airflow to track specific
    metrics or statuses. For example, for a deep error track, we can use **Sentry**,
    a specialized tool used by IT operations teams to provide support and insights.
    You can learn more about this integration here: [https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/errors.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/errors.xhtml).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 StatsD，我们还可以将其他工具集成到 Airflow 中以跟踪特定的指标或状态。例如，对于深度错误跟踪，我们可以使用 **Sentry**，这是一个由
    IT 运维团队使用的专业工具，用于提供支持和见解。你可以在这里了解更多关于此集成：[https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/errors.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/errors.xhtml)。
- en: 'On the other hand, if tracking users’ activities is a concern, it is possible
    to integrate Airflow with Google Analytics. You can learn more here: [https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/tracking-user-activity.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/tracking-user-activity.xhtml).'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果跟踪用户活动是一个关注点，可以将 Airflow 与 Google Analytics 集成。你可以在这里了解更多：[https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/tracking-user-activity.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/tracking-user-activity.xhtml)。
- en: See also
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'Learn more about Airflow architecture here: [https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/logging-architecture.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/logging-architecture.xhtml)'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多关于 Airflow 架构的信息请见此处：[https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/logging-architecture.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/logging-architecture.xhtml)
- en: 'More information about StatsD is here: [https://www.datadoghq.com/blog/statsd/](https://www.datadoghq.com/blog/statsd/)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于 StatsD 的更多信息请见此处：[https://www.datadoghq.com/blog/statsd/](https://www.datadoghq.com/blog/statsd/)
- en: Using notiﬁcation operators
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用通知操作员
- en: So far, we have focused on ensuring that code is well logged and has enough
    information to provide valid monitoring. Nevertheless, the purpose of having mature
    and structured pipelines is to avoid the necessity of manual intervention. With
    busy agendas and other projects, it is hard to constantly look at monitoring dashboards
    to check whether everything is fine.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直专注于确保代码有良好的日志记录，并且有足够的信息来提供有效的监控。然而，拥有成熟和结构化的管道的目的是避免手动干预的需要。在忙碌的日程和其他项目之间，持续查看监控仪表板以检查一切是否正常是很困难的。
- en: Thankfully, Airflow also has native operators to trigger alerts depending on
    their configured situation. In this recipe, we will configure an email operator
    to trigger a message every time a pipeline succeeds or fails, allowing us to remediate
    the problem rapidly.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Airflow 还具有本机操作员，可以根据其配置的情况触发警报。在这个菜谱中，我们将配置一个电子邮件操作员，以便在管道成功或失败时触发消息，使我们能够快速解决问题。
- en: Getting ready
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the *Technical requirements* section for this recipe, since we will
    handle it with the same technology.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考此菜谱的*技术要求*部分，因为我们将以相同的技术来处理它。
- en: 'In addition to that, you need to create an app password for your Google account.
    This password will allow our application to authenticate and use the **Simple
    Mail Transfer Protocol** (**SMTP**) host from Google to trigger emails. You can
    generate the app password in your Google account at the following link: [https://security.google.com/settings/security/apppasswords](https://security.google.com/settings/security/apppasswords).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还需要为你的 Google 账户创建一个应用密码。这个密码将允许我们的应用程序进行身份验证并使用 Google 的 **简单邮件传输协议** (**SMTP**)
    主机来触发电子邮件。你可以在以下链接中生成应用密码：[https://security.google.com/settings/security/apppasswords](https://security.google.com/settings/security/apppasswords)。
- en: 'Once you access the link, you will be asked to authenticate using your Google
    credentials, and a new page will appear, similar to the following:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你访问了链接，你将需要使用你的 Google 凭据进行身份验证，并将出现一个新页面，类似于以下内容：
- en: '![Figure 10.32 – The Google app password generation page](img/Figure_10.32_B19453.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.32 – Google 应用密码生成页面](img/Figure_10.32_B19453.jpg)'
- en: Figure 10.32 – The Google app password generation page
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.32 – Google 应用密码生成页面
- en: In the first box, select **Mail**, and in the second box, select the device
    that will use the app password. Since I am using a Macbook, I will select **Mac**,
    as shown in the preceding screenshot. Then, click on **GENERATE**.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个框中，选择 **邮件**，在第二个框中，选择将使用应用密码的设备。由于我使用的是 Macbook，所以我将选择 **Mac**，如前述截图所示。然后，点击
    **生成**。
- en: 'A window similar to the following will appear:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 将会出现一个类似于以下窗口：
- en: '![Figure 10.33 – The Google generated app password pop-up window](img/Figure_10.33_B19453.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.33 – Google 生成的应用密码弹出窗口](img/Figure_10.33_B19453.jpg)'
- en: Figure 10.33 – The Google generated app password pop-up window
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.33 – Google 生成的应用密码弹出窗口
- en: Follow the steps on the page and save the password in a place you can remember.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 按照页面上的步骤操作，并将密码保存在你可以记住的地方。
- en: Airflow DAG code
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Airflow DAG 代码
- en: To avoid redundancy and focus on the goal of this recipe, which is to configure
    remote logging in Airflow, we will use the same DAG as the *Creating basic logs
    in Airﬂow* recipe. However, feel free to create another DAG with a different name
    but the same code.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免冗余并专注于此菜谱的目标，即配置 Airflow 中的远程日志，我们将使用与 *在 Airflow 中创建基本日志* 菜谱相同的 DAG。然而，你也可以创建另一个具有不同名称但相同代码的
    DAG。
- en: 'Nonetheless, you can always find the final code in the GitHub repository here:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，你仍然可以在以下 GitHub 仓库中找到最终的代码：
- en: '[https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_notifications_operators](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_notifications_operators)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_notifications_operators](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_notifications_operators)'
- en: How to do it…
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Perform the following steps to try this recipe:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来尝试这个菜谱：
- en: 'Let’s start by configuring the SMTP server in Airflow. Insert the following
    lines in your `docker-compose.yaml` file under the environment section:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先从在 Airflow 中配置 SMTP 服务器开始。在你的 `docker-compose.yaml` 文件的环境部分插入以下行：
- en: '[PRE18]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Your file should look like this:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 你的文件应该看起来像这样：
- en: '![Figure 10.34 – docker-compose.yaml with SMTP environment variables](img/Figure_10.34_B19453.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.34 – 包含 SMTP 环境变量的 docker-compose.yaml](img/Figure_10.34_B19453.jpg)'
- en: Figure 10.34 – docker-compose.yaml with SMTP environment variables
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.34 – 包含 SMTP 环境变量的 docker-compose.yaml
- en: 'If you directly edit the `airflow.cfg` file, edit the following lines:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你直接编辑 `airflow.cfg` 文件，编辑以下行：
- en: '[PRE19]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Don’t forget to restart Airflow after these configurations are saved.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在保存这些配置后，不要忘记重新启动 Airflow。
- en: 'Now, let’s edit our `basic_logging_dag` DAG to allow it to send emails using
    `EmailOperator`. Let’s add to our imports the following line:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们编辑我们的 `basic_logging_dag` DAG，以便它可以使用 `EmailOperator` 发送电子邮件。让我们向我们的导入中添加以下行：
- en: '[PRE20]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The imports will be organized like this:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 导入将按照以下方式组织：
- en: '[PRE21]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In `default_args`, we will add three new parameters – `email`, `email_on_failure`,
    and `email_on_retry`. You can see here what it looks like:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `default_args` 中，我们将添加三个新参数 – `email`、`email_on_failure` 和 `email_on_retry`。你在这里可以看到它的样子：
- en: '[PRE22]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You don’t need to worry about these new parameters for now. We will cover them
    in the *How it* *works…* section.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 目前你不需要担心这些新参数。我们将在 *它是如何工作的* 部分介绍它们。
- en: 'Then, let’s add a new task to our DAG called `success_task`. If all the other
    tasks are successful, this one will trigger `EmailOperator` to alert us. Add the
    following code to the `basic_logging_dag` script:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，让我们在我们的DAG中添加一个名为`success_task`的新任务。如果所有其他任务都成功，这个任务将触发`EmailOperator`来提醒我们。将以下代码添加到`basic_logging_dag`脚本中：
- en: '[PRE23]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, at the end of your script, let’s add the workflow:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在脚本末尾，让我们添加工作流程：
- en: '[PRE24]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Don’t forget that you can always check how the final code looks here: [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_noti%EF%AC%81cations_operators](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_noti%EF%AC%81cations_operators)'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记，您始终可以在这里检查最终代码的样式：[https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_noti%EF%AC%81cations_operators](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_noti%EF%AC%81cations_operators)
- en: If you check your DAG graph, you can see that a new task called `success_task`
    appears. It shows our operator is ready to be used. Let’s trigger our DAG by selecting
    the play button in the top-right corner, as we did in *step 3* of the *Configuring
    logs in* *airflow.cfg* recipe.
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你检查你的DAG图，你可以看到出现了一个名为`success_task`的新任务。它表明我们的操作符已经准备好使用。让我们通过在右上角选择播放按钮来触发我们的DAG，就像我们在*步骤3*中配置*airflow.cfg*日志时做的那样。
- en: 'Your Airflow UI should look like this:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 你的Airflow UI应该看起来像这样：
- en: '![Figure 10.35 – basic_logging_dag showing successful runs for all the tasks](img/Figure_10.35_B19453.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![图10.35 – basic_logging_dag显示所有任务的成功运行](img/Figure_10.35_B19453.jpg)'
- en: Figure 10.35 – basic_logging_dag showing successful runs for all the tasks
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.35 – basic_logging_dag显示所有任务的成功运行
- en: 'Then, let’s check our email. If everything is well configured, you should see
    an email similar to the following:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，让我们检查我们的电子邮件。如果一切配置正确，你应该会看到以下类似的电子邮件：
- en: '![Figure 10.36 – An email with a Hello World! Message, indicating that success_task
    worked](img/Figure_10.36_B19453.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![图10.36 – 包含Hello World!信息的电子邮件，表明success_task已成功执行](img/Figure_10.36_B19453.jpg)'
- en: Figure 10.36 – An email with a Hello World! Message, indicating that success_task
    worked
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.36 – 包含Hello World!信息的电子邮件，表明success_task已成功执行
- en: Our `EmailOperator` works exactly as expected!
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`EmailOperator`工作得完全符合预期！
- en: How it works…
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Let’s start explaining the code by defining what an SMTP server is. An SMTP
    server is a key component of an email system that enables the transmission of
    email messages between servers and from clients to servers.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过定义什么是SMTP服务器来开始解释代码。SMTP服务器是电子邮件系统的一个关键组件，它使得服务器之间以及客户端到服务器的电子邮件消息传输成为可能。
- en: In our case, Google works both as a sender and receiver. We borrow a Gmail host
    to help send an email from our local machine. However, you don’t need to worry
    about this when working on a company project; your IT operations team will take
    care of it.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，Google既作为发送者又作为接收者工作。我们借用一个Gmail主机来帮助我们从本地机器发送电子邮件。然而，当你在公司项目上工作时，你不需要担心这一点；你的IT运维团队会处理它。
- en: 'Now, back to Airflow – once we understand how the SMTP works, its configuration
    is straightforward. Consulting the reference page for the configurations in Airflow
    ([https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml)),
    we can see that there is a section dedicated to SMTP, as you can see here:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回到Airflow – 一旦我们理解了SMTP的工作原理，其配置就很简单了。查阅Airflow配置的参考页面([https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml))，我们可以看到有一个专门针对SMTP的部分，正如你在这里看到的：
- en: '![Figure 10.37 – The Airflow documentation page for the SMTP environment variables](img/Figure_10.37_B19453.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![图10.37 – Airflow SMTP环境变量的文档页面](img/Figure_10.37_B19453.jpg)'
- en: Figure 10.37 – The Airflow documentation page for the SMTP environment variables
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.37 – Airflow SMTP环境变量的文档页面
- en: 'Then, all we needed to do was to set the required parameters to allow the connection
    between the host (`smtp.gmail.com`) and Airflow, as you can see here:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们只需要设置必要的参数，以允许主机（`smtp.gmail.com`）和Airflow之间的连接，正如你在这里看到的：
- en: '![Figure 10.38 – A close look at the docker-compose.yaml SMTP settings](img/Figure_10.38_B19453.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![图10.38 – 仔细查看docker-compose.yaml的SMTP设置](img/Figure_10.38_B19453.jpg)'
- en: Figure 10.38 – A close look at the docker-compose.yaml SMTP settings
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.38 – 仔细查看docker-compose.yaml的SMTP设置
- en: 'Once this step is completed, we will go to our DAG and declare `EmailOperator`,
    as shown in the following code:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成这一步，我们将转到我们的DAG并声明`EmailOperator`，如下面的代码所示：
- en: '[PRE25]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The parameters of the email are very intuitive and can be set accordingly to
    whatever is needed. If we delve deeper, we can see that there are plenty of possibilities
    to make those fields’ values more abstract to adapt to different function results.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件的参数非常直观，可以根据需要相应地设置。如果我们进一步深入，我们可以看到有很多可能性使这些字段的值更加抽象，以适应不同的功能结果。
- en: 'It is also possible to use a formatted email template in `html_content` and
    even attach a complete error or log message. You can see more of the allowed parameters
    here: [https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/email/index.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/email/index.xhtml).'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用`html_content`中的格式化电子邮件模板，甚至附加完整的错误或日志消息。你可以在这里看到更多允许的参数：[https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/email/index.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/email/index.xhtml)。
- en: 'In our case, this operator was triggered when all tasks successfully ran. But
    what about if there is an error? Let’s go back to *step 3* and see `default_args`:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，这个操作员是在所有任务成功运行时触发的。但如果出现错误怎么办？让我们回到*步骤3*并查看`default_args`：
- en: '[PRE26]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The two new parameters added (`email_on_failure` and `email_on_retry`) address
    scenarios where the DAG failed or retries a task. The values inside the `email`
    parameter list are the recipients of these emails.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 新增的两个参数（`email_on_failure`和`email_on_retry`）解决了DAG失败或重试任务的情况。`email`参数列表中的值是这些电子邮件的收件人。
- en: 'A default email triggered by an error message looks like this:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 由错误消息触发的默认电子邮件看起来像这样：
- en: '![Figure 10.39 – The Airflow default email for error in a task instance](img/Figure_10.39_B19453.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![图10.39 – Airflow任务实例中的默认错误电子邮件](img/Figure_10.39_B19453.jpg)'
- en: Figure 10.39 – The Airflow default email for error in a task instance
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.39 – Airflow任务实例中的默认错误电子邮件
- en: There’s more…
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The Airflow notification system is not limited to sending emails and counts,
    offering useful integrations with Slack, Teams, and Telegram.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow的通知系统不仅限于发送电子邮件和计数，还提供了与Slack、Teams和Telegram的有用集成。
- en: 'TowardsDataScience has a fantastic blog post about how to integrate Airflow
    with Slack, and you can find it here: [https://towardsdatascience.com/automated-alerts-for-airflow-with-slack-5c6ec766a823](https://towardsdatascience.com/automated-alerts-for-airflow-with-slack-5c6ec766a823).'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: TowardsDataScience有一篇关于如何将Airflow与Slack集成的精彩博客文章，你可以在这里找到它：[https://towardsdatascience.com/automated-alerts-for-airflow-with-slack-5c6ec766a823](https://towardsdatascience.com/automated-alerts-for-airflow-with-slack-5c6ec766a823)。
- en: 'Not limited to corporate tools, Airflow also has a Discord hook: [https://airflow.apache.org/docs/apache-airflow-providers-discord/stable/_api/airflow/providers/discord/hooks/discord_webhook/index.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-discord/stable/_api/airflow/providers/discord/hooks/discord_webhook/index.xhtml).'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅限于企业工具，Airflow还有一个Discord钩子：[https://airflow.apache.org/docs/apache-airflow-providers-discord/stable/_api/airflow/providers/discord/hooks/discord_webhook/index.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-discord/stable/_api/airflow/providers/discord/hooks/discord_webhook/index.xhtml)。
- en: The best advice I can give is always to look at Airflow community documentation.
    As an open source and active platform, there is always a new implementation to
    help automate and make our daily work easier.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我能给出的最好建议是始终查看Airflow社区文档。作为一个开源和活跃的平台，总有新的实现来帮助我们自动化并使我们的日常工作更轻松。
- en: Using SQL operators for data quality
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SQL操作员进行数据质量
- en: Good **data quality** is crucial for an organization to ensure the effectiveness
    of its data systems. By performing quality checks within the DAG, it is possible
    to stop pipelines and notify stakeholders before erroneous data is introduced
    into a production lake or warehouse.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 优秀的**数据质量**对于一个组织来说至关重要，以确保其数据系统的有效性。通过在DAG中执行质量检查，可以在错误数据被引入生产湖或仓库之前停止管道并通知利益相关者。
- en: Although plenty of available tools in the market provide **data quality checks**,
    one of the most popular ways to do this is by running SQL queries. As you may
    have already guessed, Airflow has providers to support those operations.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管市场上有很多可用的工具提供**数据质量检查**，但最受欢迎的方法之一是通过运行SQL查询来完成。正如你可能已经猜到的，Airflow提供了支持这些操作的服务提供者。
- en: This recipe will cover the data quality principal topics in the data ingestion
    process, pointing out the best `SQLOperator` type to run in those situations.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱将涵盖数据摄入过程中的数据质量主要主题，指出在这些情况下运行的最佳`SQLOperator`类型。
- en: Getting ready
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Before starting our exercise, let’s create a simple `customers` table. You
    can see here how it looks:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始我们的练习之前，让我们创建一个简单的`customers`表。你可以看到它的样子如下：
- en: '![Figure 10.40 – An example of customers table columns](img/Figure_10.40_B19453.jpg)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![图10.40 – 客户表列的一个示例](img/Figure_10.40_B19453.jpg)'
- en: Figure 10.40 – An example of customers table columns
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.40 – 客户表列的一个示例
- en: 'And the same table is represented with its schema:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的表用其模式表示：
- en: '[PRE27]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: You don’t need to worry about creating this table in a SQL database. This exercise
    will focus on the data quality factors to be checked, using this table as an example.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要担心在SQL数据库中创建此表。这个练习将专注于要检查的数据质量因素，并以这个表为例。
- en: How to do it…
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Here are the steps to perform this recipe:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是执行此食谱的步骤：
- en: 'Let’s start by defining the essential data quality checks that apply as follows:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先定义以下基本数据质量检查：
- en: '![Figure 10.41 – Data quality essential points](img/Figure_10.41_B19453.jpg)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![图10.41 – 数据质量基本要点](img/Figure_10.41_B19453.jpg)'
- en: Figure 10.41 – Data quality essential points
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.41 – 数据质量基本要点
- en: 'Let’s imagine implementing it using `SQLColumnCheckOperator`, integrated and
    installed in our Airflow platform. Let’s now create a simple task to check whether
    our table has unique IDs and whether all customers have `first_name`. Our example
    code looks like this:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们想象使用集成并安装在我们Airflow平台中的`SQLColumnCheckOperator`来实现它。现在，让我们创建一个简单的任务来检查我们的表是否有唯一的ID，以及所有客户是否都有`first_name`。我们的示例代码如下：
- en: '[PRE28]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, let’s validate whether we ingest the required count of rows using `SQLTableCheckOperator`,
    as follows:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们验证是否使用`SQLTableCheckOperator`摄入了所需的行数，如下所示：
- en: '[PRE29]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, let’s ensure the customers in our database have at least one order.
    Our example code looks like this:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们确保数据库中的客户至少有一个订单。我们的示例代码如下：
- en: '[PRE30]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The `geq_to` key stands for **great or** **equal to**.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '`geq_to`键代表**大于或等于**。'
- en: How it works…
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Data quality is a complex topic encompassing many variables, such as the project
    or company context, business models, and **Service Level Agreements** (**SLAs**)
    between teams. Based on this, the goal of this recipe was to offer the core concept
    of data quality and demonstrate how to first approach using Airflow SQLOperators.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量是一个复杂的话题，涉及许多变量，如项目或公司背景、商业模式以及团队之间的**服务水平协议**（**SLAs**）。基于此，本食谱的目标是提供数据质量的核心概念，并展示如何首先使用Airflow
    SQLOperators进行尝试。
- en: 'Let’s start with the essential topics in *step 1*, as follows:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从*步骤1*中的基本主题开始，如下所示：
- en: '![Figure 10.42 – Data quality essential points](img/Figure_10.42_B19453.jpg)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![图10.42 – 数据质量基本要点](img/Figure_10.42_B19453.jpg)'
- en: Figure 10.42 – Data quality essential points
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.42 – 数据质量基本要点
- en: In a generic scenario, those items are the principal topics to be approached
    and implemented. They will guarantee the minimum data reliability, based on whether
    the columns are the ones we expected, creating an average value for the row count,
    ensuring the IDs are unique, and having control of the `null` and distinct values
    in specific columns.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个通用场景中，这些项目是我们要处理和实施的主要主题。它们将保证基于列是否是我们预期的、创建行数的平均值、确保ID唯一以及控制特定列中的`null`和唯一值的最小数据可靠性。
- en: Using Airflow, we used the SQL approach to check data. As mentioned at the beginning
    of this recipe, SQL checks are widespread and popular due to their simplicity
    and flexibility. Unfortunately, to simulate a scenario like this, we would be
    required to set up a hard-working local infrastructure, and the best we could
    come up with is simulating the tasks in Airflow.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Airflow，我们采用了SQL方法来检查数据。正如本食谱开头所述，SQL检查因其简单性和灵活性而广泛流行。不幸的是，为了模拟此类场景，我们需要设置一个勤奋的本地基础设施，而我们能想到的最好的办法是在Airflow中模拟任务。
- en: 'Here, we used two `SQLOperator` subtypes – `SQLColumnCheckOperator` and `SQLTableCheckOperator`.
    As the name suggests, the first operator is more focused on verifying the column’s
    content by checking whether there are null or distinct values. In the case of
    `customer_id`, we verified both scenarios and only null values for `first_name`,
    as you can see here:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了两种`SQLOperator`子类型——`SQLColumnCheckOperator`和`SQLTableCheckOperator`。正如其名称所示，第一个操作员更专注于通过检查是否存在null或唯一值来验证列的内容。在`customer_id`的情况下，我们验证了两种情况，并且对于`first_name`只有null值，如下所示：
- en: '[PRE31]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '`SQLTableCheckOperator` will perform validations across the whole table. It
    allows the insertion of a SQL query to make counts or other operations, as we
    did to validate the expected number of rows in *step 3*, as you can see in the
    piece of code here:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '`SQLTableCheckOperator`将对整个表进行验证。它允许插入一个SQL查询来进行计数或其他操作，就像我们在*步骤3*中验证预期行数一样，如以下代码片段所示：'
- en: '[PRE32]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'However, `SQLOperator` is not limited to these two. In the Airflow documentation,
    you can see other examples and the complete list of accepted parameters for these
    functions: [https://airflow.apache.org/docs/apache-airflow/2.1.4/_api/airflow/operators/sql/index.xhtml#module-airflow.operators.sql](https://airflow.apache.org/docs/apache-airflow/2.1.4/_api/airflow/operators/sql/index.xhtml#module-airflow.operators.sql).'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，`SQLOperator`并不局限于这两者。在Airflow文档中，你可以看到其他示例和这些函数接受的完整参数列表：[https://airflow.apache.org/docs/apache-airflow/2.1.4/_api/airflow/operators/sql/index.xhtml#module-airflow.operators.sql](https://airflow.apache.org/docs/apache-airflow/2.1.4/_api/airflow/operators/sql/index.xhtml#module-airflow.operators.sql)。
- en: A fantastic operator to check out is `SQLIntervalCheckOperator`, used to validate
    historical data and ensure the stored information is concise.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 一个值得关注的出色操作符是`SQLIntervalCheckOperator`，用于验证历史数据并确保存储的信息简洁。
- en: In your data career, you will see that data quality is a daily topic and concern
    among teams. The best advice here is to continually search for tools and methods
    to improve this methodology.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的数据生涯中，你会发现数据质量是团队日常讨论和关注的话题。在这里最好的建议是持续寻找工具和方法来改进这一方法。
- en: There’s more…
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: We can use additional tools to enhance our data quality checks. One of the recommended
    tools for this use is **GreatExpectations**, an open source platform made in Python
    with plenty of integrations, with resources such as Airflow, **AWS S3**, and **Databricks**.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用额外的工具来增强我们的数据质量检查。为此推荐的工具之一是**GreatExpectations**，这是一个用Python编写的开源平台，具有丰富的集成，包括Airflow、**AWS
    S3**和**Databricks**等资源。
- en: 'Although it is a platform you can install in any cluster, **GreatExpectations**
    is expanding toward a managed cloud version. You can check more about it on the
    official page here: [https://greatexpectations.io/integrations](https://greatexpectations.io/integrations).'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是一个你可以安装在任何集群上的平台，但**GreatExpectations**正在扩展到托管云版本。你可以在官方页面了解更多信息：[https://greatexpectations.io/integrations](https://greatexpectations.io/integrations)。
- en: See also
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '*Yu Ishikawa* has a nice blog post about other checks you can do using SQL
    in Airflow: [https://yu-ishikawa.medium.com/apache-airflow-as-a-data-quality-checker-416ca7f5a3ad](https://yu-ishikawa.medium.com/apache-airflow-as-a-data-quality-checker-416ca7f5a3ad)'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*石川裕*有一篇关于在Airflow中使用SQL进行其他检查的精彩博客文章：[https://yu-ishikawa.medium.com/apache-airflow-as-a-data-quality-checker-416ca7f5a3ad](https://yu-ishikawa.medium.com/apache-airflow-as-a-data-quality-checker-416ca7f5a3ad)'
- en: 'More information about data quality in Airflow is available here: [https://docs.astronomer.io/learn/data-quality](https://docs.astronomer.io/learn/data-quality)'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于Airflow中数据质量的更多信息，请在此处查看：[https://docs.astronomer.io/learn/data-quality](https://docs.astronomer.io/learn/data-quality)
- en: Further reading
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[https://www.oak-tree.tech/blog/airflow-remote-logging-s3](https://www.oak-tree.tech/blog/airflow-remote-logging-s3)'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.oak-tree.tech/blog/airflow-remote-logging-s3](https://www.oak-tree.tech/blog/airflow-remote-logging-s3)'
- en: '[https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/connections/aws.xhtml#examples](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/connections/aws.xhtml#examples)'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/connections/aws.xhtml#examples](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/connections/aws.xhtml#examples)'
- en: '[https://airflow.apache.org/docs/apache-airflow/stable/howto/email-config.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/howto/email-config.xhtml)'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://airflow.apache.org/docs/apache-airflow/stable/howto/email-config.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/howto/email-config.xhtml)'
- en: https://docs.astronomer.io/learn/logging
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: https://docs.astronomer.io/learn/logging
- en: https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/metrics.xhtml#setup
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/metrics.xhtml#setup
- en: https://hevodata.com/learn/airflow-monitoring/#aam
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: https://hevodata.com/learn/airflow-monitoring/#aam
- en: https://servian.dev/developing-5-step-data-quality-framework-with-apache-airflow-972488ddb65f
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: https://servian.dev/developing-5-step-data-quality-framework-with-apache-airflow-972488ddb65f
