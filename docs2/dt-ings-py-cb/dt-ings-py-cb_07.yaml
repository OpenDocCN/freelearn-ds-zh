- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Ingesting Analytical Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摄入分析数据
- en: Analytical data is a bundle of data that serves various areas (such as finances,
    marketing, and sales) in a company, university, or any other institution, to facilitate
    decision-making, especially for strategic matters. When transposing analytical
    data to a data pipeline or a usual **Extract, Transform, and Load** (**ETL**)
    process, it corresponds to the final step, where data is already ingested, cleaned,
    aggregated, and has other transformations accordingly to business rules.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 分析数据是一组数据，它为公司、大学或任何其他机构中的多个领域（如财务、营销和销售）提供服务，以促进决策，尤其是在战略问题上。当将分析数据转换为数据管道或常规的
    **提取、转换和加载**（**ETL**）过程时，它对应于最终步骤，其中数据已经摄取、清理、聚合，并根据业务规则进行了其他转换。
- en: There are plenty of scenarios where data engineers must retrieve data from a
    data warehouse or any other storage containing analytical data. The objective
    of this chapter is to learn how to read analytical data and its standard formats
    and cover practical use cases related to the reverse ETL concept.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多场景中，数据工程师必须从包含分析数据的数据仓库或其他存储中检索数据。本章的目标是学习如何读取分析数据和其标准格式，并涵盖与反向 ETL 概念相关的实际用例。
- en: 'In this chapter, we will learn about the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: Ingesting Parquet ﬁles
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摄入 Parquet 文件
- en: Ingesting Avro files
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摄入 Avro 文件
- en: Applying schemas to analytical data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模式应用于分析数据
- en: Filtering data and handling common issues
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤数据和处理常见问题
- en: Ingesting partitioned data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摄入分区数据
- en: Applying reverse ETL
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用反向 ETL
- en: Selecting analytical data for reverse ETL
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择用于反向 ETL 的分析数据
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Like [*Chapter 6*](B19453_06.xhtml#_idTextAnchor195), in this chapter too,
    some recipes will need `SparkSession` initialized, and you can use the same session
    for all of them. You can use the following code to create your session:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如同 [*第 6 章*](B19453_06.xhtml#_idTextAnchor195)，在本章中，一些菜谱也需要初始化 `SparkSession`，并且您可以使用相同的会话来执行所有这些。您可以使用以下代码来创建您的会话：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: A `WARN` message as output is expected in some cases, especially if you are
    using WSL on Windows, so you don’t need to worry if you receive one.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您可能会看到 `WARN` 消息作为输出，尤其是如果您在 Windows 上使用 WSL，所以如果您收到一个，您不需要担心。
- en: 'You can also find the code from this chapter in its GitHub repository here:
    [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在此章节的 GitHub 仓库中找到代码：[https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook).
- en: Ingesting Parquet ﬁles
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摄入 Parquet 文件
- en: '**Apache Parquet** is a columnar storage format that is open source and designed
    to support fast processing. It is available to any project in a **Hadoop ecosystem**
    and can be read in different programming languages.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apache Parquet** 是一种开源的列式存储格式，旨在支持快速处理。它在 **Hadoop 生态系统** 中的任何项目中都可用，并且可以用不同的编程语言读取。'
- en: Due to its compression and fastness, this is one of the most used formats when
    needing to analyze data in great volume. The objective of this recipe is to understand
    how to read a collection of Parquet files using **PySpark** in a real-world scenario.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其压缩和快速处理能力，这是在需要大量分析数据时最常用的格式之一。本菜谱的目标是了解如何在现实场景中使用 **PySpark** 读取一组 Parquet
    文件。
- en: Getting ready
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe, we will need `SparkSession` to be initialized. You can use
    the code provided at the beginning of this chapter to do so.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个菜谱，我们需要初始化 `SparkSession`。您可以使用本章开头提供的代码来完成此操作。
- en: 'The dataset for this recipe will be *Yellow Taxi Trip Records from New York*.
    You can download it by accessing the **NYC Government website** and selecting
    **2022** | **January** | **Yellow Taxi Trip Records** or using this link:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本菜谱的数据集将是 *纽约黄色出租车行程记录*。您可以通过访问 **纽约市政府网站** 并选择 **2022** | **一月** | **黄色出租车行程记录**
    或使用此链接下载：
- en: '[https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet](https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet](https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet)'
- en: Feel free to execute the code with a Jupyter notebook or a PySpark shell session.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以自由地使用 Jupyter 笔记本或 PySpark 壳会话执行代码。
- en: How to do it…
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Here are the steps to perform this recipe:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此菜谱的步骤如下：
- en: '**Setting the Parquet file path**: To create a DataFrame based on the Parquet
    file, we have two options: pass the filename or the path of the Parquet file.
    In the following code block, you can see an example of passing only the name of
    the file:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置 Parquet 文件路径**：为了根据 Parquet 文件创建 DataFrame，我们有两种选择：传递文件名或 Parquet 文件的路径。在下面的代码块中，你可以看到一个只传递文件名的示例：'
- en: '[PRE1]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For the second option, we remove the Parquet filename, and Spark handles the
    rest. You can see how the code looks in the following code block:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二种情况，我们移除了 Parquet 文件名，Spark 处理其余部分。你可以在下面的代码块中看到代码的样式：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`.printSchema()` function to see whether the DataFrame was created successfully:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.printSchema()` 函数用于查看 DataFrame 是否成功创建：'
- en: '[PRE3]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You should see the following output:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '![Figure 7.1 – Yellow taxi trip DataFrame schema](img/Figure_7.1_B19453.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – 黄色出租车行程 DataFrame 模式](img/Figure_7.1_B19453.jpg)'
- en: Figure 7.1 – Yellow taxi trip DataFrame schema
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 黄色出租车行程 DataFrame 模式
- en: '**Visualizing with pandas**: This is an optional step since it requires your
    local machine to have enough processing capacity and can freeze your kernel trying
    to process it.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用 pandas 可视化**：这是一个可选步骤，因为它要求你的本地机器有足够的处理能力，并且可能会在尝试处理时冻结内核。'
- en: 'To take a better look at the DataFrame, let’s use `.toPandas()`, as shown:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地查看 DataFrame，让我们使用 `.toPandas()`，如下所示：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You should see the following output:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '![Figure 7.2 – Yellow taxi trip DataFrame with pandas visualization](img/Figure_7.2_B19453.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 黄色出租车行程 DataFrame 与 pandas 可视化](img/Figure_7.2_B19453.jpg)'
- en: Figure 7.2 – Yellow taxi trip DataFrame with pandas visualization
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 黄色出租车行程 DataFrame 与 pandas 可视化
- en: How it works…
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: As we can observe in the preceding code, reading Parquet files is straightforward.
    Like many Hadoop tools, PySpark natively supports reading and writing Parquet.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，读取 Parquet 文件很简单。像许多 Hadoop 工具一样，PySpark 本地支持读取和写入 Parquet。
- en: 'Similar to JSON and CSV files, we used a function that derives from the `.read`
    method to inform PySpark that a Parquet file will be read, as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与 JSON 和 CSV 文件类似，我们使用一个从 `.read` 方法派生的函数来通知 PySpark 将读取 Parquet 文件，如下所示：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We also saw two ways of reading, passing only the folder where the Parquet file
    is or passing the path with the Parquet filename. The best practice is to use
    the first case since there is usually more than one Parquet file, and reading
    just one may cause several errors. This is because each Parquet file inside the
    respective Parquet folder corresponds to a piece of the data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到了两种读取方式，只传递 Parquet 文件所在的文件夹或传递带有 Parquet 文件名的路径。最佳实践是使用第一种情况，因为通常有多个 Parquet
    文件，只读取一个可能会导致多个错误。这是因为每个 Parquet 文件都对应于相应 Parquet 文件夹中的数据的一部分。
- en: After reading and transforming the dataset into a DataFrame, we printed its
    schema using the `.printSchema()` method. As the name suggests, it will print
    and show the schema of the DataFrame. Since we didn’t specify the schema we want
    for the DataFrame, Spark will infer it based on the data pattern inside the columns.
    Don’t worry about this now; we will cover this further in the *Applying schemas
    to analytical* *data* recipe.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取并将数据集转换为 DataFrame 后，我们使用 `.printSchema()` 方法打印了其模式。正如其名所示，它将打印并显示 DataFrame
    的模式。由于我们没有指定我们想要的 DataFrame 模式，Spark 将根据列内的数据模式推断它。现在不用担心这个问题；我们将在 *将模式应用于分析数据*
    菜谱中进一步介绍。
- en: Using the `.printSchema()` method before doing any operations in the DataFrame
    is an excellent practice for understanding the best ways to handle the data inside.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DataFrame 中进行任何操作之前使用 `.printSchema()` 方法是理解处理 DataFrame 内部数据的最佳方式的优秀实践。
- en: Finally, as the last step of the recipe, we used the `.toPandas()` method to
    visualize our data better since the `.show()` Spark method is not intended to
    bring friendly visualizations like pandas. However, we must be cautious when using
    the `.toPandas()` method since it needs computational and memory power to translate
    the Spark DataFrame into a pandas DataFrame.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，作为菜谱的最后一步，我们使用了 `.toPandas()` 方法来更好地可视化我们的数据，因为 `.show()` Spark 方法并不旨在提供像
    pandas 那样的友好可视化。然而，在使用 `.toPandas()` 方法时必须谨慎，因为它需要计算能力和内存来将 Spark DataFrame 转换为
    pandas DataFrame。
- en: There’s more…
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容…
- en: 'Now, let’s understand why `parquet` is an optimized file format for big data.
    Parquet files are column-oriented, stored in data blocks and in small chunks (a
    data fragment), allowing optimized reading and writing. In the following diagram,
    you can visually observe how `parquet` organizes data at a high level:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解为什么 `parquet` 是大数据优化的文件格式。Parquet 文件是列式存储的，以数据块和小块（数据片段）的形式存储，允许优化的读写操作。在下面的图中，你可以从高层次上观察
    `parquet` 如何组织数据：
- en: '![Figure 7.3 – Parquet file structure diagram by Darius Kharazi (https://dkharazi.github.io/blog/parquet)](img/Figure_7.3_B19453.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 由 Darius Kharazi 制作的 Parquet 文件结构图（https://dkharazi.github.io/blog/parquet）](img/Figure_7.3_B19453.jpg)'
- en: Figure 7.3 – Parquet file structure diagram by Darius Kharazi (https://dkharazi.github.io/blog/parquet)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 由 Darius Kharazi 制作的 Parquet 文件结构图（https://dkharazi.github.io/blog/parquet）
- en: 'Parquet files can frequently be found in a compressed form. This adds another
    layer of efficiency to improve data storage and transfer. A compressed Parquet
    file will look like this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 文件通常以压缩形式存在。这为提高数据存储和传输效率增加了另一层。压缩的 Parquet 文件看起来像这样：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `snappy` name informs us of the compression type and is crucial when creating
    a table in `gzip` format but more optimized for a massive volume of data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`snappy` 名称告诉我们压缩类型，在创建 `gzip` 格式的表时至关重要，但更适用于大量数据。'
- en: See also
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考内容
- en: 'You can read more about Apache Parquet in the official documentation: [https://parquet.apache.org/docs/overview/motivation/](https://parquet.apache.org/docs/overview/motivation/)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在官方文档中了解更多关于 Apache Parquet 的信息：[https://parquet.apache.org/docs/overview/motivation/](https://parquet.apache.org/docs/overview/motivation/)
- en: 'If you want to test other Parquet files and explore more data from *Open Targets
    Platform*, access this link: [http://ftp.ebi.ac.uk/pub/databases/opentargets/platform/22.11/output/etl/parquet/hpo/](http://ftp.ebi.ac.uk/pub/databases/opentargets/platform/22.11/output/etl/parquet/hpo/)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想要测试其他 Parquet 文件并探索来自 *Open Targets Platform* 的更多数据，请访问此链接：[http://ftp.ebi.ac.uk/pub/databases/opentargets/platform/22.11/output/etl/parquet/hpo/](http://ftp.ebi.ac.uk/pub/databases/opentargets/platform/22.11/output/etl/parquet/hpo/)
- en: Ingesting Avro files
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入 Avro 文件
- en: Like Parquet, **Apache Avro** is a widely used format to store analytical data.
    Apache Avro is a leading method of serialization to record data and relies on
    schemas. It also provides **Remote Procedure Calls** (**RPCs**), making transmitting
    data easier and resolving problems such as missing fields, extra fields, and naming
    fields.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Parquet 类似，**Apache Avro** 是一种广泛使用的格式，用于存储分析数据。Apache Avro 是记录数据的主要序列化方法，依赖于模式。它还提供了
    **远程过程调用**（**RPCs**），使得数据传输更加容易，并解决诸如字段缺失、额外字段和字段命名等问题。
- en: In this recipe, we will understand how to read an Avro file properly and later
    comprehend how it works.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将了解如何正确读取 Avro 文件，然后理解它是如何工作的。
- en: Getting ready
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'This recipe will require `SparkSession` with some different configurations
    from the previous *Ingesting Parquet ﬁles* recipe. If you are already running
    `SparkSession`, stop it using the following command:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此菜谱将需要具有与之前 *导入 Parquet 文件* 菜谱中不同的配置的 `SparkSession`。如果你已经运行了 `SparkSession`，请使用以下命令停止它：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We will create another session in the *How to do* *it…* section.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 *如何操作* 部分创建另一个会话。
- en: 'The dataset used here can be found at this link: [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_7/ingesting_avro_files](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_7/ingesting_avro_files).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里使用的数据集可以在以下链接找到：[https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_7/ingesting_avro_files](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_7/ingesting_avro_files).
- en: Feel free to execute the code in a Jupyter notebook or your PySpark shell session.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 随意执行 Jupyter notebook 或 PySpark shell 会话中的代码。
- en: How to do it…
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Here are the steps to perform this recipe:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是执行此菜谱的步骤：
- en: '`avro` file, we need to import a `.jars` file in our `SparkSession` configuration,
    as follows:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要处理 `avro` 文件，我们需要在我们的 `SparkSession` 配置中导入一个 `.jars` 文件，如下所示：
- en: '[PRE8]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'When executed, this code will provide an output similar to this:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 执行时，此代码将提供类似以下输出：
- en: '![Figure 7.4 – SparkSession logs when downloading an Avro file package](img/Figure_7.4_B19453.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4 – 下载 Avro 文件包时 SparkSession 的日志](img/Figure_7.4_B19453.jpg)'
- en: Figure 7.4 – SparkSession logs when downloading an Avro file package
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – 下载 Avro 文件包时 SparkSession 的日志
- en: It means the `avro` package was successfully downloaded and ready to use. We
    will later cover how it works.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 `avro` 包已成功下载并准备好使用。我们将在后面介绍它是如何工作的。
- en: '`.jars` file configured, we will pass the file format to `.read` and add the
    file’s path:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置了`.jars`文件后，我们将传递文件格式到`.read`并添加文件的路径：
- en: '[PRE9]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`.printSchema()`, let’s retrieve the schema of this DataFrame:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.printSchema()`，让我们检索这个DataFrame的架构：'
- en: '[PRE10]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You will observe the following output:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你将观察到以下输出：
- en: '![Figure 7.5 – DataFrame schema from the Avro file](img/Figure_7.5_B19453.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – 从Avro文件中获取的DataFrame架构](img/Figure_7.5_B19453.jpg)'
- en: Figure 7.5 – DataFrame schema from the Avro file
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 从Avro文件中获取的DataFrame架构
- en: As we can observe, this DataFrame contains the same data as the Parquet file
    covered in the last recipe, *Ingesting* *Parquet ﬁles*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所观察到的，这个DataFrame包含与上一道菜中提到的Parquet文件相同的数据，即*摄取* *Parquet文件*。
- en: How it works…
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: As you can observe, we started this recipe slightly differently by creating
    `SparkSession` with a custom configuration. This is because, since version 2.4,
    Spark does not natively provide an internal API to read or write Avro files.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所观察到的，我们通过创建具有自定义配置的`SparkSession`来开始这道菜，这与Spark自2.4版本以来没有提供读取或写入Avro文件的内部API有关。
- en: 'If you try to read the file used here without downloading the `.jars` file,
    you will get the following error message:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试读取这里使用的文件而不下载`.jars`文件，你将得到以下错误信息：
- en: '![Figure 7.6 – Error message when Spark cannot find the Avro file package](img/Figure_7.6_B19453.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – 当Spark找不到Avro文件包时的错误信息](img/Figure_7.6_B19453.jpg)'
- en: Figure 7.6 – Error message when Spark cannot find the Avro file package
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 当Spark找不到Avro文件包时的错误信息
- en: 'Reading the error message, we can notice it is recommended to search for a
    third-party (or external) source called `avro` file. Check out the Spark third-parties
    documentation, which can be found here: [https://spark.apache.org/docs/latest/sql-data-sources-avro.xhtml#data-source-option](https://spark.apache.org/docs/latest/sql-data-sources-avro.xhtml#data-source-option).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从错误信息中，我们可以注意到建议搜索一个名为`avro`的第三方（或外部）源。查看Spark第三方文档，可以在以下位置找到：[https://spark.apache.org/docs/latest/sql-data-sources-avro.xhtml#data-source-option](https://spark.apache.org/docs/latest/sql-data-sources-avro.xhtml#data-source-option)。
- en: Even though the documentation has some helpful information about how to set
    it for different languages, such as `org.apache.spark:spark-avro_2.12:3.3.1.jars`
    file incompatible with some PySpark versions, and so the recommendation is to
    use `org.apache.spark:spark-avro_2.12:2.4.4`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管文档中提供了一些关于如何为不同语言设置它的有用信息，例如`org.apache.spark:spark-avro_2.12:3.3.1.jars`文件与某些PySpark版本不兼容，因此建议使用`org.apache.spark:spark-avro_2.12:2.4.4`。
- en: '`.jars` file to be downloaded, but it is also incompatible with some versions
    of PySpark: `com.databricks:spark-avro_2.11:4.0.0`.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 需要下载`.jars`文件，但它也与某些版本的PySpark不兼容：`com.databricks:spark-avro_2.11:4.0.0`。
- en: 'Due to the non-existence of an internal API to handle this file, we need to
    inform Spark of the format of the file, as shown here:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有内部API来处理此文件，我们需要通知Spark文件的格式，如下所示：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We did the same thing when reading **MongoDB** collections, as seen in [*Chapter
    5*](B19453_05.xhtml#_idTextAnchor161), in the *Ingesting data from MongoDB using*
    *PySpark* recipe.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取**MongoDB**集合时，我们做了同样的事情，如[第5章](B19453_05.xhtml#_idTextAnchor161)中所述，在*使用PySpark从MongoDB中摄取数据*的配方中。
- en: Once our file is converted into a DataFrame, all other functionalities and operations
    are identical without prejudice. As we saw, Spark will infer the schema and transform
    it into a columnar format.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的文件被转换为DataFrame，所有其他功能和操作都是相同的，没有偏见。正如我们所见，Spark将推断架构并将其转换为列格式。
- en: There’s more…
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Now that we know about both Apache Parquet and Apache Avro, you might wonder
    when to use each. Even though both are used to store analytical data, some key
    differences exist.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了Apache Parquet和Apache Avro，你可能想知道何时使用哪一个。尽管两者都用于存储分析数据，但存在一些关键差异。
- en: The main difference is how they store data. Parquet stores are in a columnar
    format, while Avro stores data in rows, which can be very efficient if you want
    to retrieve the entire row or dataset. However, columnar formats are much more
    optimized for aggregations or larger datasets, and `parquet` also supports more
    efficient queries using large-scale data and compression.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 主要区别在于它们存储数据的方式。Parquet存储在列格式中，而Avro以行格式存储数据，如果你想要检索整个行或数据集，这可以非常高效。然而，列格式在聚合或更大的数据集方面进行了更多优化，`parquet`还支持使用大规模数据和压缩进行更有效的查询。
- en: '![Figure 7.7 – Columnar versus row format](img/Figure_7.7_B19453.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图7.7 – 列格式与行格式](img/Figure_7.7_B19453.jpg)'
- en: Figure 7.7 – Columnar versus row format
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 列格式与行格式
- en: 'On the other hand, Avro files are commonly used for data streaming. A good
    example is when using **Kafka** with **Schema Registry**, it will allow Kafka
    to verify the file’s expected schema in real time. You can see some example code
    in the Databricks documentation here: [https://docs.databricks.com/structured-streaming/avro-dataframe.xhtml](https://docs.databricks.com/structured-streaming/avro-dataframe.xhtml).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Avro 文件通常用于数据流。一个很好的例子是使用 **Kafka** 和 **Schema Registry**，它将允许 Kafka 在实时验证文件的预期架构。您可以在
    Databricks 文档中看到一些示例代码：[https://docs.databricks.com/structured-streaming/avro-dataframe.xhtml](https://docs.databricks.com/structured-streaming/avro-dataframe.xhtml)。
- en: See also
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'Read more about how Apache Avro works and its functionalities on the official
    documentation page here: [https://avro.apache.org/docs/](https://avro.apache.org/docs/).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在官方文档页面上了解更多关于 Apache Avro 的工作原理及其功能：[https://avro.apache.org/docs/](https://avro.apache.org/docs/)。
- en: Applying schemas to analytical data
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将架构应用于分析数据
- en: In the previous chapter, we saw how to apply schemas to structured and unstructured
    data, but the application of a schema is not limited to raw files.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到了如何将架构应用于结构化和非结构化数据，但架构的应用不仅限于原始文件。
- en: Even when working with already processed data, there will be cases when we need
    to cast the values of a column or change column names to be used by another department.
    In this recipe, we will learn how to apply a schema to Parquet files and how it
    works.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在处理已处理的数据时，也可能会遇到需要将列的值转换为其他部门使用的格式或更改列名的情况。在本菜谱中，我们将学习如何将架构应用于 Parquet 文件以及它是如何工作的。
- en: Getting ready
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will need `SparkSession` for this recipe. Ensure you have a session that
    is up and running. We will use the same dataset as in the *Ingesting Parquet*
    *ﬁles* recipe.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要 `SparkSession` 来完成此菜谱。请确保您有一个正在运行和启动的会话。我们将使用与 *Ingesting Parquet 文件* 菜谱中相同的数据集。
- en: Feel free to execute the code using a Jupyter notebook or your PySpark shell
    session.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以自由地使用 Jupyter notebook 或您的 PySpark shell 会话执行代码。
- en: How to do it…
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Here are the steps to perform this recipe:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此菜谱的步骤如下：
- en: '**Looking at our columns**: As seen in the *Ingesting Parquet ﬁles* recipe,
    we can list the columns and their inferred data types. You can see the list as
    follows:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**查看我们的列**：如 *Ingesting Parquet 文件* 菜谱中所示，我们可以列出列及其推断的数据类型。您可以看到以下列表：'
- en: '[PRE12]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`VendorID`, to a more readable form. Refer to the following code:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `VendorID` 转换为更易读的形式。请参考以下代码：
- en: '[PRE13]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`parquet` file:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`parquet` 文件：'
- en: '[PRE14]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Printing the new DataFrame schema**: When printing the schema, we can see
    the name of some columns changed as we set them on the schema object in *step
    1*:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**打印新的 DataFrame 架构**：当打印架构时，我们可以看到一些列的名称已更改，正如我们在 *步骤 1* 中在架构对象上设置的那样：'
- en: '[PRE15]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Executing the preceding code will provide the following output:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码将提供以下输出：
- en: '![Figure 7.8 – DataFrame with new schema applied](img/Figure_7.8_B19453.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 – 应用新架构的 DataFrame](img/Figure_7.8_B19453.jpg)'
- en: Figure 7.8 – DataFrame with new schema applied
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 应用新架构的 DataFrame
- en: '`.toPandas()` function, as follows:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.toPandas()` 函数，如下所示：'
- en: '[PRE16]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output looks like this:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 输出看起来像这样：
- en: '![Figure 7.9 – Yellow taxi trip DataFrame visualization with pandas](img/Figure_7.9_B19453.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9 – 使用 pandas 可视化的黄色出租车行程 DataFrame](img/Figure_7.9_B19453.jpg)'
- en: Figure 7.9 – Yellow taxi trip DataFrame visualization with pandas
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – 使用 pandas 可视化的黄色出租车行程 DataFrame
- en: As you can see, no numerical data has changed, and therefore the data integrity
    remains.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，没有数值数据发生变化，因此数据完整性保持不变。
- en: How it works…
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: As we can observe in this exercise, defining and setting the schema for a DataFrame
    is not complex. However, it can be a bit laborious when we think about knowing
    the data types or declaring each column of the DataFrame.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所观察到的，为 DataFrame 定义和设置架构并不复杂。然而，当我们考虑了解数据类型或声明 DataFrame 的每一列时，它可能会有些费时。
- en: The first step to start the schema definition is understanding the dataset we
    need to handle. This can be done by consulting a data catalog or even someone
    in more contact with the data. As a last option, you can create the DataFrame,
    let Spark infer the schema, and make adjustments when re-creating the DataFrame.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 开始架构定义的第一步是了解我们需要处理的数据集。这可以通过咨询数据目录或甚至与数据有更多联系的人来完成。作为最后的选项，您可以创建 DataFrame，让
    Spark 推断架构，并在重新创建 DataFrame 时进行调整。
- en: 'When creating the schema structure in Spark, there are a few items we need
    to pay attention to, as you can see here:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 当在 Spark 中创建架构结构时，有一些事项我们需要注意，正如您在这里所看到的：
- en: '`StructType` object, which represents the schema of a list of `StructField`.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StructType` 对象，它表示 `StructField` 列表的模式。'
- en: '`StructField` will define the name, data type, and whether the column allows
    null or empty fields.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StructField` 将定义名称、数据类型以及列是否允许空或空字段。'
- en: '**Data types**: The last thing to bear in mind is where we will define the
    column’s data type; as you can imagine, a few data types are available. You can
    always consult the documentation to see the supported data types here: [https://spark.apache.org/docs/latest/sql-ref-datatypes.xhtml](https://spark.apache.org/docs/latest/sql-ref-datatypes.xhtml).'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据类型**：最后要注意的是，我们将在哪里定义列的数据类型；正如你所想象的那样，有几种数据类型可用。你总是可以查阅文档以查看这里支持的数据类型：[https://spark.apache.org/docs/latest/sql-ref-datatypes.xhtml](https://spark.apache.org/docs/latest/sql-ref-datatypes.xhtml)。'
- en: Once we have defined the schema object, we can easily attribute it to the function
    that creates the DataFrame using the `.schema()` method, as we saw in *step 3*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了模式对象，就可以很容易地使用 `.schema()` 方法将其分配给创建 DataFrame 的函数，就像我们在 *步骤 3* 中看到的那样。
- en: With the DataFrame created, all the following commands remain the same.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 DataFrame 后，所有以下命令保持不变。
- en: There’s more…
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容…
- en: 'Let’s do an experiment where instead of using `TimestampType()`, we will use
    `DateType()`. See the following portion of the changed code:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一个实验，在这个实验中，我们不是使用 `TimestampType()`，而是使用 `DateType()`。请看以下更改后的代码片段：
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If we repeat the steps using the preceding code change, an error message will
    appear when we try to visualize the data:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们重复使用前面的代码更改步骤，当我们尝试可视化数据时将出现错误信息：
- en: '![Figure 7.10 – Error reading the DataFrame when attributing an incompatible
    data type](img/Figure_7.10_B19453.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.10 – 在分配不兼容的数据类型时读取 DataFrame 出错](img/Figure_7.10_B19453.jpg)'
- en: Figure 7.10 – Error reading the DataFrame when attributing an incompatible data
    type
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 – 在分配不兼容的数据类型时读取 DataFrame 出错
- en: The reason behind this is the incompatibility of these two data types when formatting
    the data inside the column. `DateType()` uses the `yyyy-MM-dd` format, while `TimestampType()`
    uses `yyy-MM-dd HH:mm:ss.SSSS`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这背后的原因是这两种数据类型在格式化列内数据时的不兼容性。`DateType()` 使用 `yyyy-MM-dd` 格式，而 `TimestampType()`
    使用 `yyy-MM-dd HH:mm:ss.SSSS`。
- en: Looking closely at both columns, we see hour, minute, and second information.
    If we try to force it into another format, it could corrupt the data.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察这两个列，我们看到小时、分钟和秒的信息。如果我们尝试将其强制转换为另一种格式，可能会损坏数据。
- en: See also
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'Learn more about the Spark data types here: [https://spark.apache.org/docs/3.0.0-preview/sql-ref-datatypes.xhtml](https://spark.apache.org/docs/3.0.0-preview/sql-ref-datatypes.xhtml).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里了解更多关于 Spark 数据类型的信息：[https://spark.apache.org/docs/3.0.0-preview/sql-ref-datatypes.xhtml](https://spark.apache.org/docs/3.0.0-preview/sql-ref-datatypes.xhtml)。
- en: Filtering data and handling common issues
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过滤数据和处理常见问题
- en: Filtering data is a process of excluding or selecting only the necessary information
    to be used or stored. Even analytical data must be re-filtered to meet a specific
    need. An excellent example is **data marts** (we will cover them later in this
    recipe).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤数据是一个排除或仅选择必要信息用于或存储的过程。即使是分析数据也必须重新过滤以满足特定需求。一个很好的例子是 **数据集市**（我们将在本食谱的后面部分介绍）。
- en: This recipe aims to understand how to create and apply filters to our data using
    a real-world example.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱旨在通过一个实际例子了解如何创建和应用过滤数据。
- en: Getting ready
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe requires `SparkSession`, so ensure yours is up and running. You
    can use the code provided at the beginning of the chapter or create your own.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 此食谱需要 `SparkSession`，请确保你的已经启动并运行。你可以使用章节开头提供的代码或创建自己的代码。
- en: The dataset used here will be the same as in the *Ingesting Parquet* *ﬁles*
    recipe.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的数据集将与 *Ingesting Parquet* *文件* 食谱中使用的相同。
- en: 'To make this exercise more practical, let’s imagine we want to analyze two
    scenarios: how many trips each vendor made and what hour of the day there are
    more pickups. We will create some aggregations and filter our dataset to carry
    out those analyses.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个练习更加实用，让我们假设我们想要分析两个场景：每个供应商完成了多少次行程以及一天中的哪个小时段有更多的接单。我们将创建一些聚合并过滤我们的数据集以执行这些分析。
- en: How to do it…
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'Here are the steps to perform this recipe:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此食谱的步骤如下：
- en: '**Reading the Parquet file**: Let’s start by reading our Parquet file, as the
    following code shows:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**读取 Parquet 文件**：让我们从读取我们的 Parquet 文件开始，如下面的代码所示：'
- en: '[PRE18]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`vendorId` instances and how many trips each vendor carried out in January
    (the timeframe of our dataset). We can use a `.groupBy()` function with `.count()`,
    as follows:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`vendorId` 实例以及每个供应商在 1 月（我们的数据集的时间范围）中执行了多少次行程。我们可以使用 `.groupBy()` 函数和 `.count()`，如下所示：'
- en: '[PRE19]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This is what the vendor trip count looks like:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是供应商行程计数的样子：
- en: '![Figure 7.11 – vendorId trips count](img/Figure_7.11_B19453.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.11 – vendorId 行程计数](img/Figure_7.11_B19453.jpg)'
- en: Figure 7.11 – vendorId trips count
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 – vendorId 行程计数
- en: '`tpep_pickup_datetime` column, as shown in the following code. Then, we make
    a count and order it in an ascending flow:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tpep_pickup_datetime` 列，如下面的代码所示。然后，我们进行计数并按升序排序：'
- en: '[PRE20]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This is what the output looks like:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是输出看起来像这样：
- en: '![Figure 7.12 – Count of trips per hour](img/Figure_7.12_B19453.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.12 – 每小时的行程计数](img/Figure_7.12_B19453.jpg)'
- en: Figure 7.12 – Count of trips per hour
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 – 每小时的行程计数
- en: As you can observe, at `14` hours and `19` hours, there is an increase in the
    number of pickups. We can think of some possible reasons for this, such as lunchtime
    and rush hour.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所观察到的，在 `14` 小时和 `19` 小时，接货数量有所增加。我们可以考虑一些可能的原因，例如午餐时间和高峰时段。
- en: How it works…
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Since we are already very familiar with the reading operation to create a DataFrame,
    let’s go straight to *step 2*:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经非常熟悉用于创建 DataFrame 的读取操作，让我们直接进入 *步骤 2*：
- en: '[PRE21]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As you can observe, the chain of functions here closely resembles SQL operations.
    This is because, behind the scenes, we are using the native SQL methods a DataFrame
    supports.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所观察到的，这里的函数链非常类似于 SQL 操作。这是因为，在幕后，我们正在使用 DataFrame 支持的本地 SQL 方法。
- en: Note
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Like the SQL operations, the order of the methods in the chain will influence
    the result, and even whether it will result in a success or not.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 与 SQL 操作类似，链中方法的顺序会影响结果，甚至可能影响操作的成功与否。
- en: 'In *step 3*, we added a little bit more complexity by extracting the hour value
    from the `tpep_pickup_datetime` column. That was only possible because this column
    is of the timestamp data type. Also, we ordered by the count column this time,
    which was created once we invoked the `.count()` function, similar to the SQL.
    You can see this in the following code:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *步骤 3* 中，我们通过从 `tpep_pickup_datetime` 列中提取小时值增加了一些复杂性。这仅因为此列是时间戳数据类型。此外，我们这次按计数列排序，该列是在调用
    `.count()` 函数后创建的，类似于 SQL。您可以在下面的代码中看到这一点：
- en: '[PRE22]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'There are plenty of other functions, such as `.filter()` and `.select()`. You
    can find more PySpark functions here: [https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.xhtml](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.xhtml).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他函数，例如 `.filter()` 和 `.select()`。您可以在以下位置找到更多 PySpark 函数：[https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.xhtml](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.xhtml)。
- en: There’s more…
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: The analysis in this recipe was carried out using SQL functions natively supported
    by PySpark. However, these functions are not a good fit for more complex queries.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中进行的分析使用了 PySpark 本地支持的 SQL 函数。然而，这些函数并不适合更复杂的查询。
- en: 'In those cases, the best practice is to use the **SQL API** of Spark. Let’s
    see how to do it in the code that follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，最佳实践是使用 Spark 的 **SQL API**。让我们看看接下来的代码是如何实现的：
- en: '`.createOrReplaceTempView()` method and pass a name to our temporary view,
    as follows:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.createOrReplaceTempView()` 方法并传递一个临时视图的名称，如下所示：'
- en: '[PRE23]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`SparkSession` variable (`spark`), we will invoke `.sql()` and pass a multi-lined
    string containing the desired SQL code. To make it easier to visualize the results,
    let’s also attribute it to a variable called `vendor_groupby`.'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`SparkSession` 变量 (`spark`)，我们将调用 `.sql()` 并传递一个包含所需 SQL 代码的多行字符串。为了更容易可视化结果，我们还将它分配给一个名为
    `vendor_groupby` 的变量。'
- en: 'Observe we use the temporary view name to indicate where the query will be
    made:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到我们使用临时视图名称来指示查询将在哪里进行：
- en: '[PRE24]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Executing this code will not generate an output.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码不会生成输出。
- en: '`.show()` method will work to bring the results, as shown in the following
    code:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.show()` 方法将用于显示结果，如下面的代码所示：'
- en: '[PRE25]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This is the output:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '![Figure 7.13 – vendorId counts of trips using SQL code](img/Figure_7.13_B19453.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.13 – 使用 SQL 代码的 vendorId 行程计数](img/Figure_7.13_B19453.jpg)'
- en: Figure 7.13 – vendorId counts of trips using SQL code
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 – 使用 SQL 代码的 vendorId 行程计数
- en: 'The downside of using the SQL API is that the error logs might sometimes be
    unclear. See the following screenshot:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SQL API 的缺点是错误日志有时可能不清楚。请参见以下截图：
- en: '![Figure 7.14 – Spark error when SQL does not have the right syntax](img/Figure_7.14_B19453.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.14 – 当 SQL 语法错误时的 Spark 错误](img/Figure_7.14_B19453.jpg)'
- en: Figure 7.14 – Spark error when SQL does not have the right syntax
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 – 当 SQL 语法错误时的 Spark 错误
- en: This screenshot shows the result of a query where the syntax was incorrect when
    grouping by the `tpep_pickup_datetime` column. In scenarios like this, the best
    approach is to debug using baby steps, executing the query operations and conditionals
    one by one. If your DataFrame comes from a table in a database, try to reproduce
    the query directly on the database and see whether there is a more intuitive error
    message.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 此截图显示了当按 `tpep_pickup_datetime` 列分组时语法错误的查询结果。在这种情况下，最佳方法是分步调试，逐个执行查询操作和条件。如果您的
    DataFrame 来自数据库中的表，请尝试直接在数据库中重现查询，并查看是否有更直观的错误消息。
- en: Data marts
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集市
- en: As mentioned at the beginning of this recipe, one common use case for ingesting
    and re-filtering analytical data is to use it in a data mart.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如本配方开头所述，导入和重新过滤分析数据的一个常见用例是将其用于数据集市。
- en: 'Data marts are a smaller version of a data warehouse, with data concentrated
    on one subject, such as from a financial or sales department. The following diagram
    shows how they tend to be organized:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集市是数据仓库的一个较小版本，数据集中在单一主题上，例如来自财务或销售部门。以下图示显示了它们的组织方式：
- en: '![Figure 7.15 – Data marts diagram](img/Figure_7.15_B19453.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.15 – 数据集市图](img/Figure_7.15_B19453.jpg)'
- en: Figure 7.15 – Data marts diagram
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.15 – 数据集市图
- en: Implementing the data mart concept has many benefits, such as reaching specific
    information or guaranteeing temporary access to a strict piece of data for a project
    without managing the security access of several users to the data warehouse.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 实施数据集市概念有许多好处，例如获取特定信息或确保在项目中对严格的数据片段进行临时访问，而无需管理多个用户对数据仓库的安全访问。
- en: See also
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: 'Find out more about data marts and data warehouse concepts on the Panoply.io
    blog: [https://panoply.io/data-warehouse-guide/data-mart-vs-data-warehouse/](https://panoply.io/data-warehouse-guide/data-mart-vs-data-warehouse/).'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Panoply.io 博客上了解更多关于数据集市和数据仓库概念：[https://panoply.io/data-warehouse-guide/data-mart-vs-data-warehouse/](https://panoply.io/data-warehouse-guide/data-mart-vs-data-warehouse/).
- en: Ingesting partitioned data
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入分区数据
- en: 'The practice of partitioning data is not recent. It was implemented in databases
    to distribute data across multiple disks or tables. Actually, data warehouses
    can partition data according to the purpose and use of the data inside. You can
    read more here: [https://www.tutorialspoint.com/dwh/dwh_partitioning_strategy.htm](https://www.tutorialspoint.com/dwh/dwh_partitioning_strategy.htm).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分区的实践并非最近才出现。它在数据库中实现，用于在多个磁盘或表中分配数据。实际上，数据仓库可以根据数据内部的目的和使用情况来分区数据。您可以在此处了解更多信息：[https://www.tutorialspoint.com/dwh/dwh_partitioning_strategy.htm](https://www.tutorialspoint.com/dwh/dwh_partitioning_strategy.htm).
- en: In our case, partitioning data is related to how our data will be split into
    small chunks and processed.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，数据分区与我们的数据将被分割成小块并处理的方式有关。
- en: In this recipe, we will learn how to ingest data that is already partitioned
    and how it can affect the performance of our code.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们将学习如何导入已分区数据以及它如何影响我们代码的性能。
- en: Getting ready
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe requires an initialized `SparkSession`. You can create your own
    or use the code provided at the beginning of this chapter.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方需要初始化的 `SparkSession`。您可以创建自己的或使用本章开头提供的代码。
- en: 'The data required to complete the steps can be found here: [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_7/ingesting_partitioned_data](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_7/ingesting_partitioned_data).'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 完成步骤所需的数据可以在此处找到：[https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_7/ingesting_partitioned_data](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_7/ingesting_partitioned_data).
- en: You can use a Jupyter notebook or a PySpark shell session to execute the code.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 Jupyter 笔记本或 PySpark 壳会话来执行代码。
- en: How to do it…
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Use the following steps to perform this recipe:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下步骤执行此配方：
- en: '**Creating the DataFrame for the February data**: Let’s use the usual way for
    creating a DataFrame from a Parquet file, but this time passing only the month
    we want to read:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建 2 月数据的 DataFrame**：让我们使用从 Parquet 文件创建 DataFrame 的常规方法，但这次只传递我们想要读取的月份：'
- en: '[PRE26]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: You should see no output from this execution.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看不到此执行的任何输出。
- en: '**Using pandas to show the results**: Once the DataFrame is created, we can
    better visualize the results using pandas:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用pandas显示结果**：一旦创建DataFrame，我们可以使用pandas更好地可视化结果：'
- en: '[PRE27]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You should observe this output:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该观察以下输出：
- en: '![Figure 7.16 – Yellow taxi trip DataFrame visualization using partitioned
    data](img/Figure_7.16_B19453.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图7.16 – 使用分区数据可视化黄色出租车行程DataFrame](img/Figure_7.16_B19453.jpg)'
- en: Figure 7.16 – Yellow taxi trip DataFrame visualization using partitioned data
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16 – 使用分区数据可视化黄色出租车行程DataFrame
- en: Observe that in the `tpep_pickup_datetime` column, there is only data from February,
    and now we don’t need to be very preoccupied with the processing capacity of our
    local machine.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到在`tpep_pickup_datetime`列中，只有2月份的数据，现在我们不需要过分关注我们本地机器的处理能力。
- en: How it works…
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: This was a very simple recipe, but there are some important concepts that we
    need to understand a bit.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的配方，但有一些重要的概念我们需要稍微了解一下。
- en: 'As you can observe, all the magic happens during the creation of the DataFrame,
    where we pass not only the path where our Parquet files are stored but also the
    name of another subfolder containing the month reference. Let’s take a look at
    how this folder is organized in the following screenshot:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所观察到的，所有的魔法都在DataFrame的创建过程中发生，我们不仅传递了存储Parquet文件的路径，还传递了包含月份参考的另一个子文件夹的名称。让我们看看以下截图中的文件夹是如何组织的：
- en: '![Figure 7.17 – Folder showing data partitioned by month](img/Figure_7.17_B19453.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图7.17 – 按月份分区的文件夹](img/Figure_7.17_B19453.jpg)'
- en: Figure 7.17 – Folder showing data partitioned by month
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17 – 按月份分区的文件夹
- en: Note
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The `_SUCCESS` file indicates that the partitioning write process was successfully
    made.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`_SUCCESS`文件表示分区写入过程已成功完成。'
- en: Inside the `chapter7_partitioned_files` folder, there are other subfolders with
    a number of references. Each of these subfolders represents a partition, in this
    case, categorized by month.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在`chapter7_partitioned_files`文件夹内，存在其他带有多个参考的子文件夹。这些子文件夹中的每一个代表一个分区，在这种情况下，按月份进行分类。
- en: 'If we look inside a subfolder, we can observe one or more Parquet files. Refer
    to the following screenshot:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看子文件夹内部，我们可以观察到一个或多个Parquet文件。参考以下截图：
- en: '![Figure 7.18 – Parquet file for February](img/Figure_7.18_B19453.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图7.18 – 2月份的Parquet文件](img/Figure_7.18_B19453.jpg)'
- en: Figure 7.18 – Parquet file for February
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18 – 2月份的Parquet文件
- en: Partitions are an optimized form of reading or writing a specific amount of
    data from a dataset. That’s why using pandas to visualize the DataFrame was faster
    this time.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 分区是从数据集中读取或写入特定数量数据的优化形式。这就是为什么这次使用pandas可视化DataFrame更快。
- en: 'Partitioning also makes the execution of transformations faster since data
    will be processed using parallelism across the Spark internal worker nodes. You
    can visualize it better in the following figure:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 分区还使得转换的执行更快，因为数据将通过Spark内部工作节点之间的并行处理来处理。你可以在以下图中更好地可视化它：
- en: '![Figure 7.19 – Partitioning parallelism diagram](img/Figure_7.19_B19453.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图7.19 – 分区并行性图](img/Figure_7.19_B19453.jpg)'
- en: Figure 7.19 – Partitioning parallelism diagram
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.19 – 分区并行性图
- en: There’s more…
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容…
- en: 'As we saw, working with partitions to save data on a large scale brings several
    benefits. However, knowing how to partition your data is the key to reading and
    writing data in a performative way. Let’s list the three most important best practices
    when writing partitions:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，使用分区在大规模上保存数据带来了许多好处。然而，知道如何分区你的数据是高效读取和写入数据的关键。让我们列出编写分区时最重要的三个最佳实践：
- en: '`month`, but it is possible to partition over any column and even to use a
    column with year, month, or day to bring more granularity. Normally, partitioning
    reflects what the best way to retrieve the data will be.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`month`，但也可以对任何列进行分区，甚至可以使用包含年、月或日的列来提供更多粒度。通常，分区反映了检索数据最佳的方式。'
- en: '![Figure 7.20 – Partition folders by month](img/Figure_7.20_B19453.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图7.20 – 按月份分区文件夹](img/Figure_7.20_B19453.jpg)'
- en: Figure 7.20 – Partition folders by month
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.20 – 按月份分区文件夹
- en: '`SparkSession` is configured or where it will write the final files, Spark
    can create small `parquet`/`avro` files. From a large data scale perspective,
    reading these small files can prejudice performance.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SparkSession`配置或它将写入最终文件的位置，Spark可以创建小的`parquet`/`avro`文件。从大数据规模的角度来看，读取这些小文件可能会影响性能。'
- en: 'A good practice is to use `coalesce()` while invoking the `write()` function
    to aggregate the files into a small amount. Here is an example:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的做法是在调用`write()`函数时使用`coalesce()`来聚合文件成少量。以下是一个示例：
- en: '[PRE28]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You can find a good article about it here: [https://www.educba.com/pyspark-coalesce/](https://www.educba.com/pyspark-coalesce/).'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到一篇关于它的好文章：[https://www.educba.com/pyspark-coalesce/](https://www.educba.com/pyspark-coalesce/)。
- en: '**Avoid over-partitioning**: This follows the same logic as the previous one.
    Over-partitioning will create small files since we split them using a granularity
    rule, and then Spark''s parallelism will be slowed.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**避免过度分区**：这与前面的逻辑相同。过度分区将创建小文件，因为我们使用粒度规则将它们分割，然后Spark的并行性会减慢。'
- en: See also
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: 'You can find more good practices here: [https://climbtheladder.com/10-spark-partitioning-best-practices/](https://climbtheladder.com/10-spark-partitioning-best-practices/).'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到更多最佳实践：[https://climbtheladder.com/10-spark-partitioning-best-practices/](https://climbtheladder.com/10-spark-partitioning-best-practices/)。
- en: 'Related to the topic of partitioning, we also have the *database sharding*
    concept. It is a very interesting topic, and the MongoDB official documentation
    has a very nice post about it here: [https://www.mongodb.com/features/database-sharding-explained](https://www.mongodb.com/features/database-sharding-explained).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 与分区主题相关，我们还有*数据库分片*的概念。这是一个非常有趣的话题，MongoDB官方文档中有一篇非常好的文章介绍它：[https://www.mongodb.com/features/database-sharding-explained](https://www.mongodb.com/features/database-sharding-explained)。
- en: Applying reverse ETL
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用反向ETL
- en: As the name suggests, **reverse ETL** takes data from a data warehouse and inserts
    it into a business application such as **HubSpot** or **Salesforce**. The reason
    behind this is to make data more operational and use business tools to bring more
    insights to data that is already in a format ready for analysis or analytical
    format.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，**反向ETL**从数据仓库中提取数据并将其插入到如**HubSpot**或**Salesforce**这样的业务应用程序中。这样做的原因是为了使数据更具操作性，并使用业务工具为已经以分析或分析格式准备好的数据带来更多见解。
- en: This recipe will teach us how to architect a reverse ETL pipeline and about
    the commonly used tools.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱将教会我们如何构建反向ETL管道，以及常用的工具。
- en: Getting ready
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: There are no technical requirements for this recipe. However, it is encouraged
    to use a whiteboard or a notepad to take notes.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱没有技术要求。然而，鼓励使用白板或记事本来记录笔记。
- en: Here, we will work with a scenario where we are ingesting data from an **e-learning
    platform**. Imagine we received a request from the marketing department to better
    understand user actions on the platform using the Salesforce system.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将处理一个场景，即我们从**在线学习平台**中摄取数据。想象一下，我们收到了市场部门的请求，希望使用Salesforce系统更好地了解平台上的用户行为。
- en: The objective here will be to create a diagram showing the data flow process
    from a source of data to the Salesforce platform.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 此处的目标将是创建一个从数据源到Salesforce平台的数据流过程图。
- en: How to do it…
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'To make this exercise more straightforward, we will assume we already have
    data stored in the database for the e-learning platform:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个练习更直接，我们假设我们已经在数据库中存储了在线学习平台的数据：
- en: '**Ingesting user action data from the website**: Let’s imagine we have a frontend
    API that sends useful information about our user’s actions and behavior on the
    e-learning platform to our backend databases. Refer to the following diagram to
    see what it looks like:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**从网站摄取用户行为数据**：让我们假设我们有一个前端API，它将有关用户在在线学习平台上的行为和操作的有用信息发送到我们的后端数据库。参考以下图表，看看它是什么样子：'
- en: '![ Figure 7.21 – Data flow from the frontend to an API in the backend](img/Figure_7.21_B19453.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图7.21 – 前端到后端API的数据流](img/Figure_7.21_B19453.jpg)'
- en: Figure 7.21 – Data flow from the frontend to an API in the backend
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.21 – 前端到后端API的数据流
- en: '**Processing it using ETL**: With the available data, we can pick the necessary
    information that the marketing department needs and put it into the ETL process.
    We will ingest it from the backend database, apply any cleansing or transformations
    needed, and then load it into our data warehouse.'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用ETL进行处理**：有了可用的数据，我们可以挑选出市场部门需要的信息，并将其放入ETL流程中。我们将从后端数据库中摄取它，应用所需的任何清洗或转换，然后将它加载到我们的数据仓库中。'
- en: '![ Figure 7.22 – Diagram showing backend storage to the data warehouse](img/Figure_7.22_B19453.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![图7.22 – 显示后端存储到数据仓库的图](img/Figure_7.22_B19453.jpg)'
- en: Figure 7.22 – Diagram showing backend storage to the data warehouse
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.22 – 显示后端存储到数据仓库的图
- en: '**Storing data in a data warehouse**: After the data is ready and transformed
    into an analytical format, it will be stored in the data warehouse. We don’t need
    to worry here about how data is modeled. Let’s assume a new analytical table will
    be created just for this processing purpose.'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在数据仓库中存储数据**：数据准备好并转换为分析格式后，它将被存储在数据仓库中。我们在这里不需要担心数据是如何建模的。让我们假设将创建一个新的分析表，专门用于此处理目的。'
- en: '**ETL to Salesforce**: Once data is populated in the data warehouse, we need
    to insert it into the Salesforce system. Let’s do this using PySpark, as you can
    see in the following figure:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ETL到Salesforce**：一旦数据在数据仓库中填充，我们需要将其插入Salesforce系统中。如下图所示，让我们使用PySpark来完成这个操作：'
- en: '![Figure 7.23 – Data warehouse data flow to Salesforce](img/Figure_7.23_B19453.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![图7.23 – 数据仓库数据流向Salesforce](img/Figure_7.23_B19453.jpg)'
- en: Figure 7.23 – Data warehouse data flow to Salesforce
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.23 – 数据仓库数据流向Salesforce
- en: With data inside Salesforce, we can advise the marketing team and automate this
    process to be triggered on a necessary schedule.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在Salesforce内部有数据的情况下，我们可以向营销团队提供建议，并自动化此过程，使其在必要的时间表上触发。
- en: How it works…
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: Although it seems complicated, the reverse ETL process is similar to an ingest
    job. In some cases, adding a few more transformations to fit the final application
    model might be necessary, but isn’t complex. Now, let’s take a closer look at
    what we did in the recipe.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然看起来很复杂，但反向ETL过程与数据摄取作业类似。在某些情况下，可能需要添加一些额外的转换以适应最终的应用模型，但这并不复杂。现在，让我们更详细地看看我们在配方中做了什么。
- en: '![Figure 7.24 – Reverse ETL diagram overview](img/Figure_7.24_B19453.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![图7.24 – 反向ETL概述图](img/Figure_7.24_B19453.jpg)'
- en: Figure 7.24 – Reverse ETL diagram overview
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.24 – 反向ETL概述
- en: First, we need to understand whether we already have the requested raw data
    stored in our internal database to meet the marketing department’s needs. If we
    don’t, the data team is responsible for reaching out to the responsible developers
    to verify how to accomplish that.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要了解我们是否已经在我们内部数据库中存储了所需的原始数据，以满足营销部门的需求。如果没有，数据团队负责联系负责的开发人员以验证如何完成这项工作。
- en: Once this is checked, we proceed with the usual ETL pipeline. Normally, there
    will be SQL transformations to filter or group information based on the needs
    of the analysis. Then, we store it in a *source of truth*, such as a main data
    warehouse.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦检查无误，我们就继续进行常规的ETL管道。通常，会有SQL转换来根据分析需求过滤或分组信息。然后，我们将它存储在*事实来源*中，例如主数据仓库。
- en: It is in *step 4* that the reverse ETL occurs. The origin of this name is because,
    normally, the ETL process involves retrieving data from an application such as
    Salesforce and storing it in a data warehouse. However, in recent years, these
    tools have become another form of better understanding how users are behaving
    or interacting with our applications.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤4*中发生反向ETL。这个名称的来源是因为，通常，ETL过程涉及从应用程序（如Salesforce）检索数据并将其存储在数据仓库中。然而，近年来，这些工具已成为更好地理解用户如何行为或与我们的应用程序交互的另一种形式。
- en: With user-centric feedback solutions with analytical data, we can get better
    insights into and access to specific results. Another example besides Salesforce
    can be a **machine learning** solution to predict whether some change in the e-learning
    platform would result in improved user retention.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以用户为中心的分析数据反馈解决方案，我们可以更好地了解并访问具体结果。除了Salesforce之外，另一个例子可以是**机器学习**解决方案，用于预测电子学习平台的一些变化是否会导致用户保留率的提高。
- en: There’s more…
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容……
- en: To carry out reverse ETL, we can create our own solution or use a commercial
    one. Plenty of solutions on the market retrieve data from data warehouses and
    connect dynamically with business solutions. Some can also generate reports to
    provide feedback to the data warehouse again, improving the quality of information
    sent and even creating other analyses. The cons of these tools are that most are
    paid solutions, and free tiers tend to include one or few connections.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行反向ETL，我们可以创建自己的解决方案或使用商业解决方案。市场上有很多解决方案可以从数据仓库中检索数据，并动态地与业务解决方案连接。一些还可以生成报告，向数据仓库提供反馈，从而提高发送信息的质量，甚至创建其他分析。这些工具的缺点是大多数都是付费解决方案，免费层通常包括一个或几个连接。
- en: 'One of the most used reverse ETL tools is **Hightouch**; you can find out more
    here: [https://hightouch.com/](https://hightouch.com/).'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的反向ETL工具之一是**Hightouch**；您可以在此处了解更多信息：[https://hightouch.com/](https://hightouch.com/).
- en: See also
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'You can read more about reverse ETL at *Astasia Myers’* Medium blog: [https://medium.com/memory-leak/reverse-etl-a-primer-4e6694dcc7fb](https://medium.com/memory-leak/reverse-etl-a-primer-4e6694dcc7fb).'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*Astasia Myers*的Medium博客上了解更多关于反向ETL的信息：[https://medium.com/memory-leak/reverse-etl-a-primer-4e6694dcc7fb](https://medium.com/memory-leak/reverse-etl-a-primer-4e6694dcc7fb)。
- en: Selecting analytical data for reverse ETL
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择用于反向ETL的分析数据
- en: Now that we know what reverse ETL is, the next step is to understand which types
    of analytical data are a good use case to load into a Salesforce application,
    for example.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了反向ETL是什么，下一步就是了解哪些类型的分析数据适合加载到Salesforce应用程序中，例如。
- en: This recipe continues from the previous one, *Applying reverse ETL*, intending
    to illustrate a real scenario of deciding what data will be transferred into a
    Salesforce application.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这个菜谱从上一个菜谱“应用反向ETL”继续，目的是展示一个决定将哪些数据传输到Salesforce应用程序的真实场景。
- en: Getting ready
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe has no technical requirements, but you can use a whiteboard or a
    notepad for annotations.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这个菜谱没有技术要求，但你可以用白板或便签纸进行注释。
- en: Still using the example of a scenario where the marketing department requested
    data to be loaded into their Salesforce account, we will now go a little deeper
    to see what information is relevant for their analysis.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然以市场营销部门请求将数据加载到他们的Salesforce账户的情景为例，我们现在将进一步深入，看看哪些信息对他们的分析是相关的。
- en: We received a request from the marketing team to understand the user journey
    in the e-learning platform. They want to understand which courses are watched
    most and whether some need improvement. Currently, they don’t know what information
    we have in our backend databases.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收到了市场营销团队的一个请求，希望了解电子学习平台中的用户旅程。他们想了解哪些课程被观看最多，以及是否有一些需要改进。目前，他们不知道我们后端数据库中有什么信息。
- en: How to do it…
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let’s work on this scenario in small steps. The objective here will be to understand
    what data we need to accomplish the task:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分步骤来处理这个情景。这里的目的是了解我们需要哪些数据来完成这项任务：
- en: '**Consulting the data catalog**: To simplify our work, let’s assume our data
    engineers worked on creating a data catalog with the user information collected
    and stored in the backend databases. In the following diagram, we can better see
    how the information is stored:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**咨询数据目录**：为了简化我们的工作，让我们假设我们的数据工程师已经创建了一个包含收集并存储在后端数据库中的用户信息的数据目录。在下面的图中，我们可以更好地看到信息是如何存储的：'
- en: '![Figure 7.25 – Tables of interest for reverse ETL highlighted](img/Figure_7.25_B19453.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![图7.25 – 高亮显示的用于反向ETL的感兴趣表](img/Figure_7.25_B19453.jpg)'
- en: Figure 7.25 – Tables of interest for reverse ETL highlighted
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.25 – 高亮显示的用于反向ETL的感兴趣表
- en: 'As we can see, there are three tables with potentially relevant information
    to be retrieved: `user_data`, `course_information`, and `videos`.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，有三个表可能包含需要检索的相关信息：`user_data`、`course_information`和`videos`。
- en: '**Selecting the raw data**: We can see highlighted in the following diagram
    the columns that can provide the information needed for the analysis:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择原始数据**：在下面的图中，我们可以看到高亮显示的列可以提供分析所需的信息：'
- en: '![Figure 7.26 – Tables and respective columns highlighted as relevant for reverse
    ETL](img/Figure_7.26_B19453.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![图7.26 – 高亮显示的表及其相关列，这些列对反向ETL相关](img/Figure_7.26_B19453.jpg)'
- en: Figure 7.26 – Tables and respective columns highlighted as relevant for reverse
    ETL
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.26 – 高亮显示的表及其相关列，这些列对反向ETL相关
- en: '**Transforming and filtering data**: Since we need a single table to load data
    into Salesforce, we can make a SQL filter and join the information.'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**转换和过滤数据**：由于我们需要一个单独的表来加载Salesforce中的数据，我们可以创建一个SQL过滤器并将信息连接起来。'
- en: How it works…
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: As mentioned at the beginning of the recipe, the marketing team wants to understand
    the user journey in the e-learning application. First, let’s understand what a
    user journey is.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 如同菜谱开头所述，市场营销团队希望了解电子学习应用中的用户旅程。首先，让我们了解一下什么是用户旅程。
- en: A user journey is all the actions and interactions a user carries out on a system
    or application, from when they opt to use or buy a service until they log out
    or leave it. Information such as what type of content they have watched and for
    how long is very important in this case.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 用户旅程是指用户在系统或应用上执行的所有动作和交互，从他们选择使用或购买一项服务，直到他们登出或离开为止。在这种情况下，诸如他们观看的内容类型以及观看时长等信息非常重要。
- en: Let’s see the fields we collected and why they are important. The first six
    columns will give us an idea of the user and where they live. We can use these
    pieces of information later to see whether there are any patterns for the predilection
    of content.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们收集到的字段以及它们为什么重要。前六列将给我们一个关于用户和他们居住地的概念。我们可以在以后使用这些信息来查看内容偏好的任何模式。
- en: '![Figure 7.27 – user_data relevant columns for reverse ETL](img/Figure_7.27_B19453.jpg)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![图7.27 – 反向ETL的相关用户数据列](img/Figure_7.27_B19453.jpg)'
- en: Figure 7.27 – user_data relevant columns for reverse ETL
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.27 – 反向ETL的相关用户数据列
- en: Then, the last columns provide information about the content this user is watching
    and whether there is any relationship between the types of content. For example,
    if they bought a Python course and a SQL course, we can use a tag (for example,
    `programming course`) from the content metadata to make a filer of correlation.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，最后几列提供了关于用户正在观看的内容以及内容类型之间是否存在任何关系的详细信息。例如，如果他们购买了一门Python课程和一门SQL课程，我们可以使用内容元数据中的一个标签（例如，`编程课程`）来创建一个相关性过滤器。
- en: '![Figure 7.28 – course_information and videos with columns highlighted](img/Figure_7.28_B19453.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![图7.28 – 高亮显示列的课程信息和视频](img/Figure_7.28_B19453.jpg)'
- en: Figure 7.28 – course_information and videos with columns highlighted
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.28 – 高亮显示列的课程信息和视频
- en: 'Feeding back all this information into Salesforce can help to answer the following
    questions about the user’s journey:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些信息反馈到Salesforce可以帮助回答关于用户旅程的以下问题：
- en: Is there a tendency to finish one course before starting another?
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户是否有在完成一个课程之前开始另一个课程的倾向？
- en: Do users tend to watch multiple courses at the same time?
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户是否倾向于同时观看多个课程？
- en: Does the educational team need to reformulate a course because of a high turnover?
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教育团队是否需要因为高流失率而重新制定课程？
- en: See also
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'Find more use cases here: [https://www.datachannel.co/blogs/reverse-etl-use-cases-common-usage-patterns](https://www.datachannel.co/blogs/reverse-etl-use-cases-common-usage-patterns).'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里可以找到更多用例：[https://www.datachannel.co/blogs/reverse-etl-use-cases-common-usage-patterns](https://www.datachannel.co/blogs/reverse-etl-use-cases-common-usage-patterns)。
- en: Further reading
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Here is a list of websites you can refer to, to enhance your knowledge further:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一份您可以参考的网站列表，以进一步扩展您的知识：
- en: '[https://segment.com/blog/reverse-etl/](https://segment.com/blog/reverse-etl/)'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://segment.com/blog/reverse-etl/](https://segment.com/blog/reverse-etl/)'
- en: '[https://hightouch.com/blog/reverse-etl](https://hightouch.com/blog/reverse-etl)'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://hightouch.com/blog/reverse-etl](https://hightouch.com/blog/reverse-etl)'
- en: '[https://www.oracle.com/br/autonomous-database/what-is-data-mart/](https://www.oracle.com/br/autonomous-database/what-is-data-mart/)'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.oracle.com/br/autonomous-database/what-is-data-mart/](https://www.oracle.com/br/autonomous-database/what-is-data-mart/)'
- en: '[https://www.netsuite.com/portal/resource/articles/ecommerce/customer-lifetime-value-clv.shtml](https://www.netsuite.com/portal/resource/articles/ecommerce/customer-lifetime-value-clv.shtml)'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.netsuite.com/portal/resource/articles/ecommerce/customer-lifetime-value-clv.shtml](https://www.netsuite.com/portal/resource/articles/ecommerce/customer-lifetime-value-clv.shtml)'
- en: '[https://www.datachannel.co/blogs/reverse-etl-use-cases-common-usage-patterns](https://www.datachannel.co/blogs/reverse-etl-use-cases-common-usage-patterns)'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.datachannel.co/blogs/reverse-etl-use-cases-common-usage-patterns](https://www.datachannel.co/blogs/reverse-etl-use-cases-common-usage-patterns)'
- en: 'Part 2: Structuring the Ingestion Pipeline'
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：结构化摄取管道
- en: In the book’s second part, you will be introduced to the monitoring practices
    and see how to create your very first data ingestion pipeline using the recommended
    tools on the market, all the while applying the best data engineering practices.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第二部分，您将了解监控实践，并了解如何使用市场上推荐的工具创建您自己的第一个数据摄取管道，同时应用最佳数据工程实践。
- en: 'This part has the following chapters:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 8*](B19453_08.xhtml#_idTextAnchor280), *Designing Monitored Data
    Workﬂows*'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B19453_08.xhtml#_idTextAnchor280)，*设计监控数据工作流*'
- en: '[*Chapter 9*](B19453_09.xhtml#_idTextAnchor319), *Putting Everything Together
    with Airﬂow*'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B19453_09.xhtml#_idTextAnchor319)，*使用Airflow整合所有内容*'
- en: '[*Chapter 10*](B19453_10.xhtml#_idTextAnchor364), *Logging and Monitoring Your
    Data Ingest in Airﬂow*'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B19453_10.xhtml#_idTextAnchor364)，*在Airflow中记录和监控您的数据摄取*'
- en: '[*Chapter 11*](B19453_11.xhtml#_idTextAnchor402)*,* *Automating Your Data Ingestion
    Pipelines*'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第11章*](B19453_11.xhtml#_idTextAnchor402)，*自动化您的数据摄取管道*'
- en: '[*Chapter 12*](B19453_12.xhtml#_idTextAnchor433), *Using Data Observability
    for Debugging, Error Handling, and Preventing Downtime*'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第12章*](B19453_12.xhtml#_idTextAnchor433)，*使用数据可观察性进行调试、错误处理和预防停机*'
