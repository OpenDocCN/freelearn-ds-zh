- en: 5 Using Regular Expressions in Power BI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 在Power BI中使用正则表达式
- en: Often, many data cleansing tasks involve carrying out complex searches and substitutions
    between strings. The usual search and replace tools are sometimes not enough to
    get the desired results. For instance, let's suppose you need to match strings,
    not in an exact way (for instance, via equality conditions) but using similar
    criteria between them. Knowing how to use tools such as regular expressions (alias
    regex) or fuzzy string searches can make all the difference in projects that require
    high-quality data. Thanks to R and Python, you can add these tools to your arsenal.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，许多数据清洗任务涉及在字符串之间执行复杂的搜索和替换。通常的搜索和替换工具有时不足以获得所需的结果。例如，假设你需要匹配字符串，不是通过精确的方式（例如，通过相等条件）而是使用它们之间的相似标准。了解如何使用正则表达式（别名为regex）或模糊字符串搜索可以在需要高质量数据的项目中起到关键作用。多亏了R和Python，你可以将这些工具添加到你的工具库中。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: A brief introduction to regexes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则表达式的简要介绍
- en: Validating data using regex in Power BI
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Power BI中使用正则表达式验证数据
- en: Loading complex log files using regex in Power BI
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Power BI中使用正则表达式加载复杂的日志文件
- en: Extracting values from text using regex in Power BI
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Power BI中使用正则表达式从文本中提取值
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter requires you to have a working internet connection and **Power
    BI Desktop** already installed on your machine. You need to properly configure
    the R and Python engines and IDEs, as outlined in *Chapter 2*, *Configuring R
    with Power BI*, and *Chapter 3*, *Configuring Python with Power BI*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求你拥有一个正常工作的互联网连接，并且**Power BI桌面版**已经安装在你的机器上。你需要按照*第2章*、*配置Power BI中的R*和*第3章*、*配置Power
    BI中的Python*中概述的方式正确配置R和Python引擎和IDE。
- en: A brief introduction to regexes
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则表达式的简要介绍
- en: A **regular expression** (usually shortened to **regex**) is defined by a series
    of characters that *identify an abstract search pattern*. Essentially, it is a
    mathematical technique that was developed in 1951 by experts of formal language
    and theoretical computer science. It is used to **validate** input data or to
    *search for and extract* information from texts.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则表达式**（通常简称为**regex**）由一系列字符定义，这些字符*标识一个抽象的搜索模式*。本质上，它是一种在1951年由形式语言和理论计算机科学专家开发的数学技术。它用于**验证**输入数据或从文本中*搜索和提取*信息。'
- en: 'If you don''t know the syntax of a regex, at first glance, it might look really
    tricky:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不知道正则表达式的语法，乍一看，它可能看起来非常复杂：
- en: '![Figure 5.1 – An example of a regex pattern](img/file112.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图5.1 – 正则表达式模式的示例](img/file112.png)'
- en: Figure 5.1 – An example of a regex pattern
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 正则表达式模式的示例
- en: 'Fortunately, there are online regex visualization tools that make it easier
    to understand patterns (you can find one of them at [https://regexper.com](https://regexper.com)).
    For example, the regex highlighted in *Figure 5.1* can be visualized as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一些在线正则表达式可视化工具可以使理解模式变得更加容易（你可以在[https://regexper.com](https://regexper.com)找到其中之一）。例如，*图5.1*中高亮显示的正则表达式可以如下可视化：
- en: '![Figure 5.2 – A visualization of a regex](img/file113.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图5.2 – 正则表达式的可视化](img/file113.png)'
- en: Figure 5.2 – A visualization of a regex
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – 正则表达式的可视化
- en: From *Figure 5.2*, it is enough to intuit that the regex in *Figure 5.1* will
    identify email addresses in a piece of text.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图5.2*中，可以直观地推断出*图5.1*中的正则表达式将识别文本中的电子邮件地址。
- en: Learning how to use regexes like a pro is certainly not easy, and it is not
    the purpose of this section. Here, we will explain the basic rules that will allow
    you to create simple, yet effective, search patterns. For more details, please
    refer to the *References* section at the end of this chapter.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们当然不会深入讲解如何像专业人士一样使用正则表达式，这不是本节的目的。在这里，我们将解释一些基本规则，这将使你能够创建简单而有效的搜索模式。更多细节，请参阅本章末尾的*参考文献*部分。
- en: The basics of regexes
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则表达式的基础知识
- en: We will try to explain the basic principles of regexes through the use of examples,
    which is perhaps the most immediate way to start using them. Each subsequent subsection
    will explain a function of regexes. To test our regex, we will use the tool made
    available at [https://www.regexpal.com/](https://www.regexpal.com/). Let's get
    started!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用示例来尝试解释正则表达式的基本原理，这可能是开始使用它们的直接方式。每个后续的小节将解释正则表达式的一个功能。为了测试我们的正则表达式，我们将使用在[https://www.regexpal.com/](https://www.regexpal.com/)提供的工具。让我们开始吧！
- en: Literal characters
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 文本字符
- en: 'To include one or more literal characters in a regex, it is necessary to make
    use of the "search" feature. Let''s try searching for the *owe* string inside
    the *May the power of extending Power BI with Python and R be with you!* text:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要在正则表达式中包含一个或多个字面字符，必须使用“搜索”功能。让我们尝试在 *May the power of extending Power BI with
    Python and R be with you!* 文本中搜索 *owe* 字符串：
- en: '![Figure 5.3 – Searching for "owe" using a regex](img/file114.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 使用正则表达式搜索 "owe"](img/file114.png)'
- en: Figure 5.3 – Searching for "owe" using a regex
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 使用正则表达式搜索 "owe"
- en: 'Note that the tool uses the **Global search** **flag** in the search by default.
    A specific flag is indicated with a letter (in our case, **g**) right after the
    regex delimiters, **/.../**. The possible flags that can be used are as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，该工具在默认的搜索中使用了 **Global search** **标志**。特定的标志用字母（在我们的情况下，**g**）表示，紧随正则表达式分隔符
    **/.../** 之后。可用的标志如下：
- en: '**g (global)**: This will match all of the occurrences, keeping the index of
    the last match.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**g (global)**：这将匹配所有出现，并保留最后一个匹配的索引。'
- en: '**m (multiline)**: When enabled, the string anchors (you''ll see them later)
    will match the start and end of a line instead of the whole string.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**m (multiline)**：当启用时，字符串锚定符（你稍后会看到它们）将匹配行的开始和结束，而不是整个字符串。'
- en: '**i (ignore case)**: This searches the pattern irrespective of the case (lower
    or upper) of the string.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**i (ignore case)**：这将忽略字符串的大小写（大写或小写）来搜索模式。'
- en: 'Bear in mind that not all programming languages use flag syntax, as mentioned
    earlier. For example, Python''s `re` package (the default one for regexes) provides
    parameters in the `search`, `match`, and `sub` functions:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，并非所有编程语言都使用标志语法，如前所述。例如，Python 的 `re` 包（正则表达式的默认包）在 `search`、`match` 和 `sub`
    函数中提供参数：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This is the same for R''s `regex()` function of the `stringr` package:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这与 R 的 `stringr` 包中的 `regex()` 函数相同：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can also use **global modifiers** directly in line with your regex pattern.
    These are `(?i)` for case-insensitive and `(?m)` for multiline. For example, in
    R, you can also run the following script:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以直接在你的正则表达式模式中使用 **global modifiers**。这些是 `(?i)` 用于不区分大小写和 `(?m)` 用于多行。例如，在
    R 中，你也可以运行以下脚本：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that Python doesn’t allow inline global modifiers.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Python 不允许使用内联全局修饰符。
- en: Special characters
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特殊字符
- en: Regex uses 12 special characters (also called **metacharacters**) where each
    has a special meaning. They are the pipe, `|`; the backslash, `\`; the dollar
    sign, `$`; the question mark, `?`; the caret, `^`; the asterisk, `*`; the plus
    sign, `+`; the dot, `.`; the parentheses, `(` and `)`; the opening square bracket,
    `[`; and the opening curly bracket, `{`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式使用 12 个特殊字符（也称为 **元字符**），每个都有特殊含义。它们是管道 `|`；反斜杠 `\`；美元符号 `$`；问号 `?`；脱字符
    `^`；星号 `*`；加号 `+`；点 `.`；括号 `(` 和 `)`；开方括号 `[` 和开花括号 `{`。
- en: If you need to search for one of the previously mentioned characters, you have
    to escape it using the backslash. So, if you want to match exactly `123$`, you
    need to use `123\$` as the regex pattern.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要搜索之前提到的一个字符，你必须使用反斜杠来转义它。所以，如果你想精确匹配 `123$`，你需要使用 `123\$` 作为正则表达式模式。
- en: Next, you will learn about the meaning and use of metacharacters.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将学习元字符的含义和使用方法。
- en: The ^ and $ anchors
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ^ 和 $ 锚定符
- en: 'Anchor characters are special characters in that they are used to place the
    regex match at a certain position in the string. The caret, `^`, is used to indicate
    *the beginning of the string* (or line), and the dollar sign, `$`, is used to
    indicate *the end of the string* (or line). An example visualization is worth
    a thousand words:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 锚定字符是特殊字符，因为它们用于将正则表达式匹配放置在字符串的特定位置。脱字符 `^` 用于指示 *字符串的开始*（或行），而美元符号 `$` 用于指示
    *字符串的结束*（或行）。一个示例可视化胜过千言万语：
- en: '![Figure 5.4 – Case-insensitive and global search](img/file115.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 不区分大小写的全局搜索](img/file115.png)'
- en: Figure 5.4 – Case-insensitive and global search
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 不区分大小写的全局搜索
- en: 'In *Figure 5.4*, the "ignore case" flag is set by clicking on the **flags**
    icon and then checking "ignore case." In this way, both the occurrences are matched.
    Now, add a caret, `^`, before the `m` character:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 5.4* 中，通过点击 **flags** 图标并勾选“ignore case”来设置“忽略大小写”标志。这样，两个出现都会被匹配。现在，在
    `m` 字符之前添加一个脱字符 `^`：
- en: '![Figure 5.5 – Case-insensitive and global search using the caret, ^](img/file116.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – 使用脱字符 ^ 进行不区分大小写的全局搜索](img/file116.png)'
- en: Figure 5.5 – Case-insensitive and global search using the caret, ^
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 使用脱字符 ^ 进行不区分大小写的全局搜索
- en: In this case, only the first occurrence (that is, the one at the beginning of
    the string) is matched.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，只有第一次出现（即字符串的开头）的匹配项会被匹配。
- en: If you also add a dollar sign at the end of the regex, nothing will be matched,
    as you are asking for a match of the `may the power` string that is at the beginning
    and that also ends the text.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你也在正则表达式的末尾添加一个美元符号，则不会进行匹配，因为你正在请求匹配以`may the power`字符串开始并且也结束文本的匹配项。
- en: OR operators
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: OR运算符
- en: You might need to match one of the individual sets of characters or strings.
    For example, to match any of the `s` and `t` characters after the `ye` string,
    you should use the **character class** of `[st]` inside the `ye[st]` regex. This
    is so that it will match both the `yes` and `yet` strings. Character classes can
    also be used to match an occurrence in a range of characters using the hyphen,
    `-`. For example, `[0-9]` matches a single digit between 0 and 9, while `[A-Z]`
    matches a single uppercase letter from A to Z. Additionally, you can combine multiple
    ranges into one character class. For instance, `[A-Z0-9]` matches only a digit
    or an uppercase letter.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要匹配单个字符集或字符串集。例如，为了匹配`ye`字符串之后的任何`s`和`t`字符，你应该在`ye[st]`正则表达式内部使用**字符类**`[st]`。这样它将匹配`yes`和`yet`字符串。字符类还可以用来匹配字符范围内的一个出现，使用连字符`-`。例如，`[0-9]`匹配0到9之间的单个数字，而`[A-Z]`匹配从A到Z的单个大写字母。此外，你还可以将多个范围组合成一个字符类。例如，`[A-Z0-9]`只匹配数字或大写字母。
- en: 'In order to match one of two strings, you can use the pipe, `|`, to separate
    them within opening and closing parentheses, such as `(string1|string2)`. Here
    is a complete example:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了匹配两个字符串中的一个，你可以在括号内使用管道符`|`来分隔它们，例如`(string1|string2)`。以下是一个完整的示例：
- en: '![Figure 5.6 – A complete example of OR operators](img/file117.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图5.6 – OR运算符的完整示例](img/file117.png)'
- en: Figure 5.6 – A complete example of OR operators
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – OR运算符的完整示例
- en: The character class can also be used to match any character different from a
    specific character that is given. This is thanks to the **Negated Character Class**.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 字符类还可以用来匹配任何不同于特定字符的字符。这是由于**否定字符类**的作用。
- en: Negated character classes
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 否定字符类
- en: The caret that appears just after the opening square bracket negates the content
    of the character class. For example, the `[^"]` regex matches every character
    that isn't a double quote.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 开方括号后出现的撇号会否定字符类的内容。例如，`[^"]`正则表达式匹配所有不是双引号的字符。
- en: Shorthand character classes
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 简写字符类
- en: 'There are some character classes that are used very often. For this reason,
    we have defined some abbreviations to allow you to include them in regexes quickly.
    Here is a list of the most used ones:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有些字符类使用得非常频繁。因此，我们定义了一些缩写，以便你能够快速地将它们包含在正则表达式中。以下是最常用的列表：
- en: '`\w`: This matches an alphanumeric character, including the underscore.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`\w`: 这会匹配一个字母数字字符，包括下划线。'
- en: '`\W`: This is the opposite of `\w`, so it matches a single non-alphanumeric
    character, excluding the underscore. For example, it can match spacing and punctuation
    marks.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`\W`: 这是`\w`的对立面，因此它匹配一个非字母数字字符，不包括下划线。例如，它可以匹配空格和标点符号。'
- en: '`\d`: This matches a single digit.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`\d`: 这会匹配一个单个数字。'
- en: '`\D`: This is the opposite of `\d`, so it matches a single non-digit character.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`\D`: 这是`\d`的对立面，因此它匹配一个非数字字符。'
- en: '`\s`: This matches "whitespace characters" such as space, tab, newline, and
    carriage return.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`\s`: 这会匹配“空白字符”，例如空格、制表符、换行符和回车符。'
- en: We'll use shorthand character classes relatively often throughout this chapter.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中相对频繁地使用简写字符类。
- en: Quantifiers
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量词
- en: 'Quantifiers indicate the number of times a *character* or *expression* must
    be matched. Here is a list of the most used ones:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 量词表示一个**字符**或**表达式**必须匹配的次数。以下是最常用的列表：
- en: '`+`: This matches what precedes it one or more times. For example, `test\d+`
    will match the `test` string followed by one or more digits. The `test(-\d\d)+`
    regex will match the `test` string followed by one or more times a dash that is
    then followed by two digits:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`+`: 这会匹配其前面的内容一次或多次。例如，`test\d+`将匹配`test`字符串后跟一个或多个数字。`test(-\d\d)+`正则表达式将匹配`test`字符串后跟一个或多个由连字符和两个数字组成的序列：'
- en: '![Figure 5.7 – Repeating a group of characters using +](img/file118.png)'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图5.7 – 使用`+`重复一组字符](img/file118.png)'
- en: Figure 5.7 – Repeating a group of characters using +
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.7 – 使用`+`重复一组字符
- en: '`{n}`: This matches what precedes it *n* times. For example, `\d{4}` will match
    any integer number made up of 4 digits.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`{n}`: 这将匹配其前面的内容重复 *n* 次。例如，`\d{4}` 将匹配由 4 位数字组成的任何整数。'
- en: '`{n,m}`: This matches what precedes it between *n* and *m* times. For example,
    `prod-\d{2,6}` will match the `prod-` string followed by an integer number made
    up of between 2 and 6 digits.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`{n,m}`: 这将匹配其前面的内容，重复 *n* 到 *m* 次。例如，`prod-\d{2,6}` 将匹配 `prod-` 字符串后跟由 2 到
    6 位数字组成的整数。'
- en: '`{n,}`: This matches *n* or more times what precedes it.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`{n,}`: 这将匹配 *n* 次或更多次其前面的内容。'
- en: '`?`: This matches one or zero times what precedes it. For example, `Mar(ch)?`
    will match both `March` and `Mar`. Alternatively, the `colou?red` regex will match
    both `colored` and `coloured`.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`?`: 这将匹配其前面的一个或零次。例如，`Mar(ch)?` 将匹配 `March` 和 `Mar`。或者，`colou?red` 正则表达式将匹配
    `colored` 和 `coloured`。'
- en: '`*`: This matches zero or more times what precedes it. For example, `code\d*`
    will match `code`, `code1`, or `code173846`.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*`: 这将匹配其前面的内容零次或多次。例如，`code\d*` 将匹配 `code`、`code1` 或 `code173846`。'
- en: The dot
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 点
- en: The dot corresponds to a single character, regardless of what that character
    is, except for line break characters. It's a very powerful regex metacharacter
    and gives you a chance to be lazy. This is precisely why you have to be careful
    not to abuse it because, sometimes, you might include unintended results in the
    matches.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 点对应于单个字符，无论该字符是什么，除了换行符。这是一个非常强大的正则表达式元字符，并给你一个偷懒的机会。这正是为什么你必须小心不要滥用它，因为有时你可能会在匹配中包含意外结果。
- en: Greedy and lazy matches
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贪婪和懒惰匹配
- en: 'The `+`, `*`, and repetition of `{…}` are **greedy quantifiers**. Greedy means
    that *they will consume the longest possible string*. Let''s suppose that you
    only want to match the tags used in the `<em>Power BI rocks</em>` string. The
    first attempt a beginner would make is to use the `<.+>` regex, which, expressed
    in words, becomes "get the `<`, then get any non-newline character one or more
    times, and finally, in the end, get the `>`." The expected result is made by two
    matches, `<em>` and `</em>`. Let''s take a look at the result:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`+`、`*` 和 `{…}` 的重复是**贪婪量词**。贪婪意味着它们将消耗尽可能长的字符串。假设你只想匹配 `<em>Power BI rocks</em>`
    字符串中使用的标签。初学者可能会尝试使用 `<.+>` 正则表达式，用文字表达就是“获取 `<`，然后获取一个或多个非换行符，最后，在末尾获取 `>`。”预期的结果是两个匹配项，`<em>`
    和 `</em>`。让我们看看结果：'
- en: '![Figure 5.8 – The greediness of .+](img/file119.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8 – .+ 的贪婪性](img/file119.png)'
- en: Figure 5.8 – The greediness of .+
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – .+ 的贪婪性
- en: It is evident that the combination of `.+` captures everything contained between
    *the first* occurrence of `<` and the last occurrence of `>`, hence the definition
    of the **greediness** of the quantifiers.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，`.+` 的组合捕获了从 `<` 的第一次出现到 `>` 的最后一次出现之间的所有内容，因此量词的贪婪性得到了定义。
- en: 'So, is it possible to force a greedy quantifier to stop at the first detected
    occurrence of the next character, preventing it from "eating" anything until the
    last occurrence of the same? In other words, is it possible to turn a greedy quantifier
    into a **lazy** one? The answer is "yes," it is possible to do so by adding the
    `?` metacharacter just after the `+`. So, the `<.+>` regex becomes `<.+?>`. Here
    is the result:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，是否可以强制贪婪量词在检测到下一个字符的第一个出现时停止，防止它“吃掉”任何内容，直到相同字符的最后一个出现？换句话说，是否可以将贪婪量词转换为**懒惰**量词？答案是“可以”，可以通过在
    `+` 后面添加 `?` 元字符来实现。因此，`<.+>` 正则表达式变为 `<.+?>`。以下是结果：
- en: '![Figure 5.9 – Making a greedy quantifier lazy thanks to the ? metacharacter](img/file120.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – 利用 ? 元字符使贪婪量词变得懒惰](img/file120.png)'
- en: Figure 5.9 – Making a greedy quantifier lazy thanks to the ? metacharacter
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 利用 ? 元字符使贪婪量词变得懒惰
- en: 'Bear in mind, however, that *lazy quantifiers are underperforming*. Whenever
    possible, it is always preferable to *use negated character classes instead*.
    In our example, simply using the `<[^>]+>` regex (that is, a `<` character, any
    non-`>` character one or more times, and a `>` character) will achieve the same
    result without consuming computational resources:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，**懒惰量词性能较差**。只要可能，总是更倾向于使用否定字符类。在我们的例子中，只需使用 `<[^>]+>` 正则表达式（即一个 `<`
    字符，一个或多个非 `>` 字符，以及一个 `>` 字符）就可以达到相同的结果，而不会消耗计算资源：
- en: '![Figure 5.9 – Using a negated character class instead of a lazy quantifier](img/file121.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – 使用否定字符类代替懒惰量词](img/file121.png)'
- en: Figure 5.9 – Using a negated character class instead of a lazy quantifier
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 使用否定字符类代替懒惰量词
- en: So far, with what you've learned about regexes, you have the minimum foundation
    required to understand the more complex regexes that we'll be using in the next
    few examples.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你关于正则表达式所学的知识，是你理解我们将在接下来的几个例子中使用的更复杂正则表达式的最低基础。
- en: Checking the validity of email addresses
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查电子邮件地址的有效性
- en: 'If you were asked to validate an email address using the concepts you just
    learned, one of your first attempts might look like the following: `^.+@.+\..+$`.
    Translating this regex into spoken language gives us the following:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你被要求使用你刚刚学到的概念来验证电子邮件地址，你的第一次尝试可能看起来像以下这样：`^.+@.+\..+$`。将这个正则表达式翻译成口语，我们得到以下内容：
- en: '`^`: This matches the beginning of the string or a line if the multiline flag
    is enabled.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`^`: 如果启用了多行标志，则匹配字符串的开始或一行。'
- en: '`.+`: This matches any character one or more times, except the line break.'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.+`: 匹配任何字符一次或多次，除了换行符。'
- en: '`@`: This matches an "@" character.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`@`: 匹配一个 "@" 字符。'
- en: '`.+`: This matches any character one or more times, except the line break.'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.+`: 匹配任何字符一次或多次，除了换行符。'
- en: '`\.`: This matches a "." character.'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`\.`: 匹配一个 "." 字符。'
- en: '`.+`: This matches any character one or more times, except the line break.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.+`: 匹配任何字符一次或多次，除了换行符。'
- en: 'Of course, this regex will validate a correct email address. But are you sure
    it can also detect the obvious syntactic errors of bad emails? Let''s perform
    a test in [https://www.regexpal.com/](https://www.regexpal.com/) with the wrong
    email, `example@example.c` (the top-level domain, that is, the portion of the
    domain after the dot, must contain a minimum of two characters):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个正则表达式将验证正确的电子邮件地址。但你确定它也能检测到明显错误的电子邮件地址的语法错误吗？让我们在 [https://www.regexpal.com/](https://www.regexpal.com/)
    上进行测试，使用错误的电子邮件地址 `example@example.c`（顶级域名，即点之后的部分，必须包含至少两个字符）：
- en: '![Figure 5.10 – Using a simple regex to validate a wrong email address](img/file122.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10 – 使用简单的正则表达式验证错误的电子邮件地址](img/file122.png)'
- en: Figure 5.10 – Using a simple regex to validate a wrong email address
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – 使用简单的正则表达式验证错误的电子邮件地址
- en: 'Well, that''s not much of an outcome: an obviously wrong email would pass as
    correct. For this reason, it is often necessary to use more complex regexes that
    can respect well-defined syntactic rules.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这并不是什么结果：一个明显错误的电子邮件地址会被错误地认为是正确的。因此，通常需要使用更复杂的正则表达式，这些正则表达式可以尊重定义良好的语法规则。
- en: 'In this specific case, we''ll use a specific regex for email validation that
    we often adopt in production. It also takes into account whether the domain IP
    is used. For the purpose of displaying it in full, the regex is as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的情况下，我们将使用我们在生产中经常采用的特定正则表达式来验证电子邮件地址。它还考虑了是否使用了域名 IP。为了完整地显示它，正则表达式如下：
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: At first glance, this regex is sure to cause confusion. However, if we attempt
    to break it down into its essential parts, it becomes much more readable.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，这个正则表达式肯定会引起混淆。然而，如果我们尝试将其分解为其基本部分，它就变得容易阅读多了。
- en: However, if you think that this regex is really complex, take a look at the
    one that takes into account all, and I mean all, the syntactic rules provided
    by the *Standard for the Format of Arpa Internet Text Messages*, which you can
    find at [http://www.ex-parrot.com/pdw/Mail-RFC822-Address.html](http://www.ex-parrot.com/pdw/Mail-RFC822-Address.html)!
    Pretty impressive, huh?
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你认为这个正则表达式真的很复杂，那么请看看考虑了所有（是的，所有）由 *Arpa Internet 文本消息格式标准* 提供的语法规则的那个正则表达式，你可以在
    [http://www.ex-parrot.com/pdw/Mail-RFC822-Address.html](http://www.ex-parrot.com/pdw/Mail-RFC822-Address.html)
    找到它！相当令人印象深刻，不是吗？
- en: 'The format of an email address is defined as *local-part@domain*. You can find
    the complete specifications on Wikipedia at [https://en.wikipedia.org/wiki/Email_address](https://en.wikipedia.org/wiki/Email_address).
    We are going to match the minimum number of rules (not all of them!) that will
    allow us to validate a significantly different number of email addresses. So,
    considering we''re going to match the domain name or the domain IP, the general
    structure of the regex is as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件地址的格式定义为 *本地部分@域名*。你可以在维基百科上找到完整的规范 [https://en.wikipedia.org/wiki/Email_address](https://en.wikipedia.org/wiki/Email_address)。我们将匹配允许我们验证大量不同电子邮件地址的最小规则（不是所有规则！）所以，考虑到我们将匹配域名或域名
    IP，正则表达式的一般结构如下：
- en: '![Figure 5.11 – The structure of the complex regex for email validation](img/file123.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.11 – 用于电子邮件验证的复杂正则表达式的结构](img/file123.png)'
- en: Figure 5.11 – The structure of the complex regex for email validation
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – 用于电子邮件验证的复杂正则表达式的结构
- en: 'In *Figure 5.11*, the `{0}`, `{1}`, and `{2}` strings are just placeholders,
    not characters to match. That said, let''s start by defining each token:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 5.11* 中，`{0}`、`{1}` 和 `{2}` 字符串只是占位符，不是要匹配的字符。话虽如此，让我们首先定义每个标记：
- en: 'The `{0}` token matches the **local-part** regex of the email. In this case,
    it''s much easier to explain what the *local-part* regex does with a diagram rather
    than with words. In *Figure 5.12*, the labels explain every single detail of the
    subparts:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`{0}` 符号匹配电子邮件的**本地部分**正则表达式。在这种情况下，使用图表比用文字解释本地部分正则表达式要容易得多。在 *图 5.12* 中，标签解释了每个子部分的每个细节：'
- en: '![Figure 5.12 – The local-part regex explained in detail](img/file124.png)'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.12 – 详细解释的本地部分正则表达式](img/file124.png)'
- en: Figure 5.12 – The local-part regex explained in detail
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.12 – 详细解释的本地部分正则表达式
- en: Bear in mind that *parentheses group regex patterns together*. Parentheses allow
    you to apply regex operators to the entire grouped expression and to get a part
    of the match as a separate item in the result array. The text that corresponds
    to the regex expression is captured within them as a **numbered group**, and it
    can be reused with a numbered backreference. You'll learn how to use this feature
    later.Remember that, in order to have the match of a metacharacter be a real character,
    the escape backslash must be placed before it. So, for example, `\]` will be the
    `]` character.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请记住，*括号将正则表达式模式组合在一起*。括号允许你将正则表达式运算符应用于整个分组表达式，并将匹配的一部分作为结果数组中的单独项获取。与正则表达式表达式对应的文本被捕获在其中作为**编号组**，并且可以使用编号反向引用来重用。你将在以后学习如何使用此功能。记住，为了使元字符的匹配成为一个真实字符，必须在它之前放置转义反斜杠。所以，例如，`\]`
    将是 `]` 字符。
- en: 'The `{1}` token matches the **domain name** of the email. Again, we will use
    a diagram to explain what the domain name regex does:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`{1}` 符号匹配电子邮件的**域名**。同样，我们将使用图表来解释域名正则表达式的作用：'
- en: '![Figure 5.13 – The domain name regex explained in detail](img/file125.png)'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.13 – 详细解释的域名正则表达式](img/file125.png)'
- en: Figure 5.13 – The domain name regex explained in detail
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.13 – 详细解释的域名正则表达式
- en: 'The `{2}` token matches the **domain IP** of the email. It is the easier sub-regex,
    and you can find out what it matches by looking at *Figure 5.14*:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`{2}` 符号匹配电子邮件的**域名 IP**。这是较简单的子正则表达式，你可以通过查看 *图 5.14* 来了解它匹配的内容：'
- en: '![Figure 5.14 – The domain IP regex explained in detail](img/file126.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.14 – 详细解释的域名 IP 正则表达式](img/file126.png)'
- en: Figure 5.14 – The domain IP regex explained in detail
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14 – 详细解释的域名 IP 正则表达式
- en: 'If you want to view the whole regex in a visualization, please refer to the
    following one, which is taken from [https://jex.im/regulex](https://jex.im/regulex):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要查看整个正则表达式的可视化，请参考以下链接，它来自 [https://jex.im/regulex](https://jex.im/regulex)：
- en: '![Figure 5.14 – A visualization of the whole email regex](img/file127.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.14 – 整个电子邮件正则表达式的可视化](img/file127.png)'
- en: Figure 5.14 – A visualization of the whole email regex
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14 – 整个电子邮件正则表达式的可视化
- en: 'Now, let''s proceed with the validation of another important type of information,
    which is very often subject to typing errors: **dates**.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续验证另一种重要类型的信息的验证，这种信息经常受到输入错误的影响：**日期**。
- en: Checking the validity of dates
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查日期的有效性
- en: 'Even in the case of dates, the clueless regex developer might think that the
    following regex is enough to validate dates (in the format of `dd-mm-yyyy`): `^\d{1,2}([\-\/])\d{1,2}\1(?:\d{4}|\d{2})$`.
    There are two new expressions, never before encountered, that are worth exploring:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在日期的情况下，无知的正则表达式开发者可能会认为以下正则表达式足以验证日期（格式为 `dd-mm-yyyy`）：`^\d{1,2}([\-\/])\d{1,2}\1(?:\d{4}|\d{2})$`。有两个新的表达式，以前从未遇到过，值得探索：
- en: '`\1`: This is the **backreference** to group 1\. As we explained in the previous
    section, parentheses also help capture a portion of the string, which you can
    reference during the rest of the regex. In this case, the `\1` syntax indicates
    that, at exactly that position, you can expect the same portion of the string
    matched by the first pair of parentheses. Bear in mind that, in Python, you need
    to use the `\g<1>` syntax instead of `\1`.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`\1`: 这是对组 1 的**反向引用**。正如我们在上一节中解释的那样，括号还可以帮助捕获字符串的一部分，你可以在正则表达式的其余部分中引用它。在这种情况下，`\1`
    语法表示，在确切的位置，你可以期望与第一对括号匹配的相同字符串部分。请注意，在 Python 中，你需要使用 `\g<1>` 语法而不是 `\1`。'
- en: '`(?: … )`: This is the so-called **non-capturing group**. Sometimes, you need
    parentheses to correctly apply a quantifier, but you don''t want their contents
    to be reported in the results.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(?: … )`：这是所谓的**非捕获组**。有时，你需要括号来正确应用量词，但你不想在结果中报告其内容。'
- en: 'Translating the whole regex into spoken language gives us the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 将整个正则表达式翻译成口语，我们得到以下内容：
- en: '`^`: This matches the beginning of the string or a line if the multiline flag
    is enabled.'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`^`：这匹配字符串的开始或一行，如果启用了多行标志。'
- en: '`\d{1,2}`: This matches any digit between 1 and 2 repetitions.'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`\d{1,2}`：这匹配 1 到 2 次重复的任何数字。'
- en: '`([\-\/])`: This matches any character between `-` and `/`, and captures the
    result as group 1.'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`([\-\/])`：这匹配位于 `-` 和 `/` 之间的任何字符，并将结果捕获为组 1。'
- en: '`\d{1,2}`: This matches any digit between 1 and 2 repetitions.'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`\d{1,2}`：这匹配 1 到 2 次重复的任何数字。'
- en: '`\1`: This backreferences to the captured group 1\. So, it expects any character
    between `-` and `/`.'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`\1`：这指的是捕获组 1。因此，它期望任何位于 `-` 和 `/` 之间的字符。'
- en: '`.+`: This matches any character one or more times, except the line break.'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.+`：这匹配一个或多个字符，除了换行符。'
- en: '`(?:\d{4}|\d{2})`: This matches one of the following two alternatives: any
    digit for exactly 4 repetitions and any digit for exactly 2 repetitions.'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`(?:\d{4}|\d{2})`：这匹配以下两种选择之一：恰好 4 次重复的任何数字和恰好 2 次重复的任何数字。'
- en: 'You can visualize the whole regex as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以这样可视化整个正则表达式：
- en: '![Figure 5.15 – A visualization of the first attempt of a regex for validating
    dates](img/file128.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.15 – 验证日期的正则表达式的第一次尝试的可视化](img/file128.png)'
- en: Figure 5.15 – A visualization of the first attempt of a regex for validating
    dates
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 – 验证日期的正则表达式的第一次尝试的可视化
- en: 'As you might have guessed, this regex validates dates in the following formats:
    `dd-mm-yyyy`, `dd-mm-yy`, `d-m-yyyy`, and `d-m-yy`, using `/` or `-` as separators.
    However, it does not account for errors due to invalid dates, such as February
    30 or September 31.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所猜，这个正则表达式验证以下格式的日期：`dd-mm-yyyy`、`dd-mm-yy`、`d-m-yyyy` 和 `d-m-yy`，使用 `/` 或
    `-` 作为分隔符。然而，它不考虑由于无效日期（如 2 月 30 日或 9 月 31 日）引起的错误。
- en: 'If you want a regex that also accounts for these errors, then you must use
    the following:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要一个也考虑这些错误的正则表达式，那么你必须使用以下：
- en: '[PRE4]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Again, viewed in this way, this regex is difficult to interpret. However, looking
    at it "from above" a bit more, you realize that it consists of four alternatives:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，以这种方式查看，这个正则表达式很难解释。然而，如果你稍微“从上面”看，你会意识到它由四个选择项组成：
- en: '![Figure 5.16 – The structure of the complex regex for date validation](img/file129.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.16 – 复杂日期验证正则表达式的结构](img/file129.png)'
- en: Figure 5.16 – The structure of the complex regex for date validation
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 – 复杂日期验证正则表达式的结构
- en: Also, in this case, the `{0}`, `{1}`, `{2}`, and `{3}` strings are just placeholders,
    not characters to match. That said, let's start by defining the `{0}` token.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在这种情况下，`{0}`, `{1}`, `{2}`, 和 `{3}` 字符串只是占位符，不是用于匹配的字符。话虽如此，让我们首先定义 `{0}`
    令牌。
- en: 'The `{0}` token matches the dates that have the 31st day. As in previous cases,
    it''s much easier to explain what this regex does with a visualization rather
    than with words. In *Figure 5.17*, the labels explain every single detail of the
    subparts:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`{0}` 令牌匹配具有 31 日的日期。与之前的案例一样，用可视化来解释这个正则表达式比用文字来说明要容易得多。在 *图 5.17* 中，标签解释了每个子部分的每一个细节：'
- en: '![Figure 5.17 – The regex part for dates that have the 31st day explained in
    detail](img/file130.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.17 – 详细解释具有 31 日的日期的正则表达式部分](img/file130.png)'
- en: Figure 5.17 – The regex part for dates that have the 31st day explained in detail
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17 – 详细解释具有 31 日的日期的正则表达式部分
- en: 'The regexes used for the other placeholders are very similar to the one we
    just explained. Therefore, we will leave the explanation of the others as an exercise
    for the reader. If you want to see the whole regex in a visualization, please
    refer to the following diagram, which is taken from [https://jex.im/regulex](https://jex.im/regulex):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 用于其他占位符的正则表达式与我们刚才解释的非常相似。因此，我们将把其他正则表达式的解释留给读者作为练习。如果你想看到整个正则表达式的可视化，请参考以下图表，该图表来自
    [https://jex.im/regulex](https://jex.im/regulex)：
- en: '![Figure 5.18 – The visualization of the whole date regex](img/file131.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.18 – 整个日期正则表达式的可视化](img/file131.png)'
- en: Figure 5.18 – The visualization of the whole date regex
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.18 – 整个日期正则表达式的可视化
- en: The regex that we just examined allows the validation of the `dd-mm-yyyy` format
    with all of its variants. In the code, we will demonstrate how you can implement
    date validation in Power BI. Additionally, you will find regexes that allow you
    to validate dates in the `mm-dd-yyyy` and `yyyy-mm-dd` formats with all of their
    variants (where the year is made of two digits, the month is made of one digit,
    and so on).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才检查的正则表达式允许验证 `dd-mm-yyyy` 格式及其所有变体。在代码中，我们将演示如何在 Power BI 中实现日期验证。此外，你还将找到允许你验证
    `mm-dd-yyyy` 和 `yyyy-mm-dd` 格式及其所有变体的正则表达式（其中年份由两位数字组成，月份由一位数字组成，等等）。
- en: Now that you understand what's behind the complex regexes presented earlier,
    let's move on to implementing them in Power BI to validate your data.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了之前展示的复杂正则表达式的原理，让我们继续在 Power BI 中实现它们以验证你的数据。
- en: Validating data using regex in Power BI
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Power BI 中使用正则表达式验证数据
- en: To date, Power BI has no native feature in Power Query to perform operations
    via regexes. There are cases when you can't avoid using regexes to extract useful
    information from data in text form. The only way to be able to use regexes is
    through R scripts or Python scripts. The only cons you have in this case is that,
    if you need to publish the report on the Power BI service, to allow Power Query
    to use external R or Python engines, you must also install the on-premises data
    gateway in personal mode.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，Power BI 在 Power Query 中没有原生的功能来执行正则表达式操作。有些情况下，你无法避免使用正则表达式从文本形式的数据中提取有用信息。能够使用正则表达式的唯一方法是使用
    R 脚本或 Python 脚本。在这种情况下，你唯一的缺点是，如果你需要将报告发布到 Power BI 服务，为了让 Power Query 能够使用外部
    R 或 Python 引擎，你必须也以个人模式安装本地数据网关。
- en: However, let's get right into it with real-world examples.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，让我们直接进入现实世界的例子。
- en: Let's suppose you work at a retail company where there is a team dedicated to
    identifying fraudulent customers. As soon as a team member identifies a fraudster,
    they fill out an Excel spreadsheet, in which the *Email* and *BannedDate* columns
    are included along with others. Your task is to load the data from this Excel
    file into Power BI and, from other data sources, select only the fraudster's information
    in order to carry out specific analysis on their purchases.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在一个零售公司工作，公司里有一个团队专门负责识别欺诈客户。一旦团队成员识别出一名欺诈者，他们就会填写一个 Excel 表格，其中包含 *Email*
    和 *BannedDate* 列以及其他列。你的任务是把这个 Excel 文件中的数据加载到 Power BI 中，并从其他数据源中选择欺诈者的信息，以便对这些购买进行特定分析。
- en: Having the correct fraudster emails within the Excel file is critically important
    to be able to properly join with the other data. Having the correct ban dates
    is also important in order to know whether further orders from that fraudster
    have slipped through the cracks after that date. As you know, the filling in of
    an Excel file by several users is done without any kind of validation of the entered
    data; therefore, it is subject to human errors. So, identifying any errors when
    filling out certain fields and highlighting them allows the fraud team to be able
    to correct them. It is precisely in this case that regexes come to your aid.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Excel 文件中拥有正确的欺诈者电子邮件对于能够正确地与其他数据连接至关重要。拥有正确的禁用日期也很重要，以便知道在那之后该欺诈者是否有进一步的订单漏网。正如你所知，多个用户填写
    Excel 文件时没有任何数据验证；因此，它容易受到人为错误的影响。因此，在填写某些字段时识别任何错误并突出显示它们，可以让欺诈团队能够纠正它们。正是在这种情况下，正则表达式才会对你有所帮助。
- en: Using regex in Power BI to validate emails with Python
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Power BI 中使用 Python 使用正则表达式验证电子邮件
- en: 'In the repository that comes with this book, you can find the `Users.xlsx`
    Excel file inside the `Chapter05` folder. Its content is similar to *Figure 5.15*:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书附带的重构库中，你可以在 `Chapter05` 文件夹中找到 `Users.xlsx` Excel 文件。其内容类似于 *图 5.15*：
- en: '![Figure 5.19 – The content of the Users.xlsx file](img/file132.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.19 – Users.xlsx 文件的内容](img/file132.png)'
- en: Figure 5.19 – The content of the Users.xlsx file
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.19 – Users.xlsx 文件的内容
- en: In this section, we will focus exclusively on the `Email` column. This contains
    the email addresses of fraudsters entered manually by the fraud team, which was
    described at the beginning of the section. These email addresses are not all syntactically
    correct. Moreover, in the Excel file, there is also the `IsEmailValidByDefinition`
    column, whose values (*1=yes*; *0=no*) indicate whether the email in correspondence
    of that value is actually valid or not.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将专注于**电子邮件**列。这包含欺诈团队手动输入的欺诈者的电子邮件地址，这在节的开头已经描述过。这些电子邮件地址并非都是语法正确的。此外，在
    Excel 文件中，还有一个名为 `IsEmailValidByDefinition` 的列，其值（*1=是*；*0=否*）表示对应值的电子邮件实际上是否有效。
- en: Python has a built-in package, called `re`, which contains all the functions
    you need to work with regexes. Additionally, in `pandas`, there are several methods
    for a series or dataframe object, which accept regexes to find a pattern in a
    string. These methods work in the same way as the ones you will find in Python's
    `re` module. We will be using the `match` method shortly.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Python 有一个内置的包，称为 `re`，其中包含你处理正则表达式所需的所有函数。此外，在 `pandas` 中，有几个用于序列或数据框对象的方法，这些方法接受正则表达式以在字符串中查找模式。这些方法的工作方式与你在
    Python 的 `re` 模块中找到的方法相同。我们很快将使用 `match` 方法。
- en: You will learn about the use of the `r'...'` syntax to create strings. This
    is a **raw string** that allows you to treat the backslash (`\`) as a literal
    character and not as an escape character.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习如何使用 `r'...'` 语法来创建字符串。这是一个**原始字符串**，允许你将反斜杠（`\`）视为一个字面字符，而不是一个转义字符。
- en: 'So, open your Power BI Desktop, make sure the Python environment you use is
    `pbi_powerquery_env`, and let''s get started:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，打开你的 Power BI Desktop，确保你使用的 Python 环境是 `pbi_powerquery_env`，然后让我们开始吧：
- en: 'From the ribbon, click on the **Excel** icon to import data from Excel:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从功能区点击**Excel**图标以从 Excel 导入数据：
- en: '![Figure 5.20 – Importing data from Excel](img/file133.png)'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.20 – 从 Excel 导入数据](img/file133.png)'
- en: Figure 5.20 – Importing data from Excel
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.20 – 从 Excel 导入数据
- en: From the **Open** dialog box, select the previously mentioned `Users.xlsx` file.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**打开**对话框中，选择之前提到的 `Users.xlsx` 文件。
- en: 'From the **Navigator** window, select the **Users** sheet and then click on
    **Transform Data**:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**导航器**窗口中，选择**用户**表，然后点击**转换数据**：
- en: '![Figure 5.21 – Selecting the Users sheet and clicking on Transform Data](img/file134.png)'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.21 – 选择用户表并点击转换数据](img/file134.png)'
- en: Figure 5.21 – Selecting the Users sheet and clicking on Transform Data
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.21 – 选择用户表并点击转换数据
- en: Click on the **Transform** menu, and then click on **Run Python Script**.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**转换**菜单，然后点击**运行 Python 脚本**。
- en: 'Then, copy and paste the following code into the Python script editor and click
    on **OK**:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将以下代码复制并粘贴到 Python 脚本编辑器中，并点击**确定**：
- en: '[PRE5]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can also find this Python script in the `01-validate-emails-with-regex-with-python.py`
    file, which is inside the repository that comes with the book, in the `Chapter05\validating-data-using-regex`
    folder.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在书附带仓库的 `Chapter05\validating-data-using-regex` 文件夹中的 `01-validate-emails-with-regex-with-python.py`
    文件中找到这个 Python 脚本。
- en: 'Power BI Desktop might display an alert that says the following: **Information
    is required about data privacy**. If so, click on **Continue**, and follow *Step
    2*; otherwise, you can jump to *Step 3*.'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Power BI Desktop 可能会显示一个如下所示的警告：**需要提供有关数据隐私的信息**。如果是这样，请点击**继续**，并遵循*步骤 2*；否则，你可以跳到*步骤
    3*。
- en: The **Privacy levels** window pops up. Here, you'll specify an isolation level
    that defines the degree to which one data source will be isolated from other data
    sources. You could select to *ignore Privacy Levels checks*, but this might expose
    confidential data to unauthorized persons. You will be asked to select a privacy
    level for both the Python script and the dataset loaded from Excel. If you select
    the **Organizational** level for both, everything works fine on Power BI Desktop.
    However, if you plan to publish your reports to the *Power BI service (or Embedded)*,
    you *must use the "Public" level*. For more details, please refer to [http://bit.ly/pbi-privacy-levels](http://bit.ly/pbi-privacy-levels).
    For now, select the **Organizational** level for both options.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**隐私级别**窗口弹出。在这里，您将指定一个隔离级别，该级别定义了一个数据源将与其他数据源隔离的程度。您可以选择忽略**隐私级别检查**，但这可能会使机密数据暴露给未经授权的人员。您将被要求为Python脚本和从Excel加载的数据集选择隐私级别。如果您为两者都选择**组织**级别，Power
    BI桌面上的所有操作都正常。然而，如果您计划将报告发布到*Power BI服务（或嵌入式）*，您*必须使用“公共”级别*。有关更多详细信息，请参阅[http://bit.ly/pbi-privacy-levels](http://bit.ly/pbi-privacy-levels)。目前，为这两个选项选择**组织**级别。'
- en: 'For now, we are only interested in the `df` dataset. So, click on its **Table**
    value:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前，我们只对`df`数据集感兴趣。因此，点击其**表**值：
- en: '![Figure 5.22 – Selecting the df dataset as a result of the Python script transformation](img/file135.png)'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图5.22 – 选择Python脚本转换后的df数据集](img/file135.png)'
- en: Figure 5.22 – Selecting the df dataset as a result of the Python script transformation
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.22 – 选择Python脚本转换后的df数据集
- en: 'As you can see, the **isEmailValidFromRegex** column is added and it contains
    the Boolean values resulting from the validation of the emails through your regex.
    If you do a check, you will see that they coincide with the values given by definition
    in the **IsEmailValidByDefinition** column:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如您所见，新增了**isEmailValidFromRegex**列，它包含通过正则表达式验证电子邮件得到的布尔值。如果您进行检查，您会发现它们与**IsEmailValidByDefinition**列中定义的值一致：
- en: '![Figure 5.23 – The regex validation results for emails](img/file136.png)'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图5.23 – 电子邮件正则表达式验证结果](img/file136.png)'
- en: Figure 5.23 – The regex validation results for emails
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.23 – 电子邮件正则表达式验证结果
- en: Your regex did a great job! Now you can go back to the **Home** menu and click
    on **Close & Apply**.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您的正则表达式做得很好！现在您可以回到**主页**菜单并点击**关闭并应用**。
- en: Thanks to the **isEmailValidFromRegex** column, you can now appropriately filter
    the correct and incorrect email addresses, perhaps even reporting the matter to
    the fraud team in a dedicated report.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了**isEmailValidFromRegex**列，您现在可以适当地过滤正确的和错误的电子邮件地址，也许甚至可以将问题报告给专门的欺诈团队。
- en: Using regex in Power BI to validate emails with R
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Power BI中使用正则表达式和R验证电子邮件
- en: If you want to use R for your email validation using regex, the process is pretty
    much the same except for a few things.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想使用R通过正则表达式进行电子邮件验证，过程基本上相同，只是有几处不同。
- en: First of all, *R only allows the use of raw strings as of version 4.0.0*. Additionally,
    the syntax for raw strings is slightly different. Instead of `r'...'`, you can
    use `r'(...)'`, `r'[...]'`, or `r'{...}'` indiscriminately. Additionally, instead
    of using numeric placeholders in curly brackets inside the string and then assigning
    them via the `format()` function, as you would in Python, in R, you can simply
    use the variable names in curly brackets directly as placeholders.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从4.0.0版本开始，*R只允许使用原始字符串*。此外，原始字符串的语法略有不同。您可以使用`r'(...)'`、`r'[...]'`或`r'{...}'`来无差别地使用，而不是`r'...'`。另外，与Python中在字符串内部的括号中使用数字占位符并通过`format()`函数分配不同，在R中，您可以直接在括号中使用变量名作为占位符。
- en: 'That said, the second thing you need to pay attention to is the following:
    you have to be careful so that, in R, not only `]` is considered a metacharacter,
    but also `[`. Therefore, when you want to use both square brackets as literal
    characters, you must prepend the backslash escape character (`\`) for both. Therefore,
    the part of the regex that identifies the character class in the local-part regex
    of the email is slightly different:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，您需要注意的第二件事是：在R中，不仅`]`被视为元字符，`[`也是。因此，当您想使用方括号作为普通字符时，您必须为两者都添加反斜杠转义字符（`\`）。因此，正则表达式中识别电子邮件本地部分正则表达式中的字符类的部分略有不同：
- en: '![Figure 5.24 – You must escape the opened square bracket when it is a literal
    character in R](img/file137.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图5.24 – 在R中将开方括号作为普通字符使用时必须进行转义](img/file137.png)'
- en: Figure 5.24 – You must escape the opened square bracket when it is a literal
    character in R
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.24 – 在 R 中，当开方括号是字面字符时，必须对其进行转义
- en: 'R:Base provides two functions that enable you to use regexes: `grep()` and
    `grepl()`:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: R:Base 提供了两个函数，使您能够使用正则表达式：`grep()` 和 `grepl()`：
- en: '`grepl()` returns a Boolean value based on whether a pattern exists in a character
    string.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grepl()` 根据模式是否存在于字符字符串中返回一个布尔值。'
- en: '`grep()` returns the indexes of the occurrences in the vector of characters
    that contains a match or the specific strings that have the match.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grep()` 返回包含匹配或特定字符串匹配的字符向量中的索引。'
- en: Since we want to adopt the *Tidyverse paradigm*, we will use the wrapper functions
    provided by the **stringr** package, which are `str_detect()` and `str_which()`,
    respectively.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们想采用 *Tidyverse 模式*，我们将使用 **stringr** 包提供的包装函数，分别是 `str_detect()` 和 `str_which()`。
- en: 'Having clarified these differences, the process to validate the emails present
    in the `Users.xlsx` Excel file in Power BI using the R script is practically the
    same as that discussed in the previous section where we used Python:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 明确了这些差异后，使用 R 脚本在 Power BI 中验证 `Users.xlsx` Excel 文件中存在的电子邮件的过程实际上与上一节中我们使用
    Python 讨论的过程相同：
- en: Repeat *Steps 1 to 3* of the previous section to import the data contained in
    the `Users.xlsx` file.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复上一节中的 *步骤 1 到 3* 以导入 `Users.xlsx` 文件中的数据。
- en: Click on the **Transform** menu, and then click on **Run R Script**.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **变换** 菜单，然后点击 **运行 R 脚本**。
- en: 'Then, copy and paste the following code into the R script editor and click
    on **OK**:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将以下代码复制并粘贴到 R 脚本编辑器中，并点击 **确定**：
- en: '[PRE6]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You also can find this R script in the `02-validate-emails-with-regex-with-r.R`
    file, which is the repository that comes with the book, in the `Chapter05\validating-data-using-regex`
    folder.
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您还可以在随书提供的 `02-validate-emails-with-regex-with-r.R` 文件中找到此 R 脚本，该文件位于 `Chapter05\validating-data-using-regex`
    文件夹中。
- en: 'Power BI Desktop might display a notice that says the following: **Information
    is required about data privacy**. If so, click on **Continue**, and follow the
    instructions here. Otherwise, you can jump to *Step 5*. Also, select the *Organizational*
    level for R scripts. Sometimes, you might find that the compatibility levels of
    datasets and analytical scripts are not compatible with each other. In this case,
    Power BI might give you an alert, such as **Formula.Firewall: Query ''XXX'' (step
    ''YYY'') is accessing data sources that have privacy levels which cannot be used
    together. Please rebuild this data combination**. In this case, simply open the
    **Data source settings** window, as follows:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Power BI Desktop 可能会显示一条通知，内容如下：**需要提供关于数据隐私的信息**。如果是这样，请点击 **继续**，并遵循此处说明。否则，您可以跳转到
    *步骤 5*。此外，选择 R 脚本的 *组织* 级别。有时，您可能会发现数据集和分析脚本的兼容级别不兼容。在这种情况下，Power BI 可能会向您发出警报，例如
    **公式.防火墙：查询 'XXX'（步骤 'YYY'）正在访问具有无法一起使用的隐私级别的数据源。请重新构建此数据组合**。在这种情况下，只需按照以下方式打开
    **数据源设置** 窗口：
- en: '![Figure 5.25 – Opening the Power Query Data source settings window](img/file138.png)'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.25 – 打开 Power Query 数据源设置窗口](img/file138.png)'
- en: Figure 5.25 – Opening the Power Query Data source settings window
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.25 – 打开 Power Query 数据源设置窗口
- en: 'After that, you have to make sure that all data sources have the same level
    of privacy (in our case, *Organizational*), changing it for each of them, if necessary,
    through the **Edit Permissions…** option:'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 之后，您必须确保所有数据源具有相同的隐私级别（在我们的案例中为 *组织*），如果需要，通过 **编辑权限…** 选项为每个数据源更改它：
- en: '![Figure 5.26 – Editing the privacy permissions for the data sources](img/file139.png)'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.26 – 编辑数据源的隐私权限](img/file139.png)'
- en: Figure 5.26 – Editing the privacy permissions for the data sources
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.26 – 编辑数据源的隐私权限
- en: At this point, you can refresh the preview data by clicking on **Refresh Preview**.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 到目前为止，您可以通过点击 **刷新预览** 来刷新预览数据。
- en: 'Additionally, in this case, we are only interested in the `df` dataset. So,
    click on its **Table** value:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，在这种情况下，我们只对 `df` 数据集感兴趣。因此，点击其 **表** 值：
- en: '![Figure 5.27 – Selecting the df dataset as a result of the R script transformation](img/file140.png)'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.27 – 选择 R 脚本转换后的 df 数据集](img/file140.png)'
- en: Figure 5.27 – Selecting the df dataset as a result of the R script transformation
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.27 – 选择 R 脚本转换后的 df 数据集
- en: As you can see, the `isEmailValidFromRegex` column has been added, and it contains
    the Boolean values (transformed to 1 and 0) resulting from the validation of the
    emails through your regex. If you do a check, they coincide with the values given
    by definition in the `IsEmailValidByDefinition` column. Your regex did a great
    job! Now you can go back to the **Home** menu and click on **Close & Apply**.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如您所见，已添加了`isEmailValidFromRegex`列，它包含通过您的正则表达式验证电子邮件得到的布尔值（转换为1和0）。如果您进行检查，它们将与`IsEmailValidByDefinition`列中定义的值相符。您正则表达式做得非常出色！现在您可以返回到**主页**菜单，然后点击**关闭并应用**。
- en: Thanks to the `isEmailValidFromRegex` column, you can now appropriately filter
    the correct and incorrect email addresses in your reports.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了`isEmailValidFromRegex`列，您现在可以适当地过滤报告中的正确和错误电子邮件地址。
- en: Now, let's take a look at how to validate dates in Power BI with Python.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何在Power BI中使用Python验证日期。
- en: Using regex in Power BI to validate dates with Python
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Python在Power BI中验证日期的正则表达式
- en: As an example of the dates to validate, we will use the `Users.xlsx` Excel file
    that we used earlier. It contains the *BannedDate* column that has string values
    representing dates in the `mm/dd/yyyy` format with all its variants. Moreover,
    in the Excel file, there is also the *IsDateValidByDefinition* column, whose values
    (*1=yes*; *0=no*) indicate whether the date matching that value is valid or not.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 作为验证日期的示例，我们将使用之前使用的`Users.xlsx` Excel文件。它包含一个名为*BannedDate*的列，该列包含表示`mm/dd/yyyy`格式的日期的字符串值，包括所有变体。此外，在Excel文件中，还有一个名为*IsDateValidByDefinition*的列，其值（*1=是*；*0=否*）指示匹配该值的日期是否有效。
- en: 'By now, you already know the Python functions needed to use a regex. So, let’s
    get started:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经知道了使用正则表达式所需的Python函数。那么，让我们开始吧：
- en: Repeat *Steps 1 to 3* of the *Using regex in Power BI to validate emails with
    Python* section to import the data contained in the `Users.xlsx` file.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*使用Python在Power BI中验证电子邮件的步骤1到3*部分，以导入`Users.xlsx`文件中的数据。
- en: Click on the **Transform** menu and then click on **Run Python Script**.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**转换**菜单，然后点击**运行Python脚本**。
- en: 'Then, copy and paste the following code into the Python script editor and click
    on **OK**:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将以下代码复制并粘贴到Python脚本编辑器中，然后点击**确定**：
- en: '[PRE7]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can find a more exhaustive Python script in the `03-validate-dates-with-regex-with-python.py`
    file, which can be found in the repository that comes with the book, in the `Chapter05\validating-data-using-regex`
    folder. That script handles dates in the formats of `mm-dd-yyyy`, `dd-mm-yyyy`,
    and `yyyy-mm-dd` with all their variances, including both `-` and `/` as separators.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以在与本书一起提供的存储库中的`Chapter05\validating-data-using-regex`文件夹中找到的`03-validate-dates-with-regex-with-python.py`文件中找到一个更全面的Python脚本。该脚本处理`mm-dd-yyyy`、`dd-mm-yyyy`和`yyyy-mm-dd`格式的日期，包括所有变体，包括`-`和`/`作为分隔符。
- en: If Power BI requires you to provide it with data privacy information, you already
    know how to proceed based on what we've discussed in the previous sections.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果Power BI要求您提供数据隐私信息，您已经知道如何根据我们之前讨论的内容进行操作。
- en: 'As you can see, the `isValidDateFromRegex` column has been added, and it contains
    the Boolean values resulting from the validation of the emails through your regex.
    If you do a check, they coincide with the values given by definition in the `IsDateValidByDefinition`
    column:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如您所见，已添加了`isValidDateFromRegex`列，它包含通过您的正则表达式验证电子邮件得到的布尔值。如果您进行检查，它们将与`IsDateValidByDefinition`列中定义的值相符：
- en: '![Figure 5.28 – The regex validation results for the dates](img/file141.png)'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图5.28 – 日期的正则表达式验证结果](img/file141.png)'
- en: Figure 5.28 – The regex validation results for the dates
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.28 – 日期的正则表达式验证结果
- en: Your regex did a great job! Now you can go back to the **Home** menu and click
    on **Close & Apply**.
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您的正则表达式做得非常出色！现在您可以返回到**主页**菜单并点击**关闭并应用**。
- en: Thanks to the `isValidDateFromRegex` column, you can now filter the correct
    and incorrect email addresses and work appropriately with them.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了`isValidDateFromRegex`列，您现在可以过滤正确和错误的电子邮件地址，并适当地处理它们。
- en: Using regex in Power BI to validate dates with R
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用R在Power BI中验证日期的正则表达式
- en: 'If you want to use R for your date validation using regex, in this case, the
    process is pretty much the same except for what you have already learned in the
    *Using regex in Power BI to validate emails with R* section. Starting from the
    same `Users.xlsx` Excel file that we used in the previous section, here are the
    steps to follow:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要使用 R 语言通过正则表达式进行日期验证，在这种情况下，过程基本上与你在 *使用 R 语言在 Power BI 中验证电子邮件的正则表达式*
    部分中学到的相同，只是有所不同。从我们在上一节中使用的相同的 `Users.xlsx` Excel 文件开始，以下是需要遵循的步骤：
- en: Repeat *Steps 1 to 3* of the *Using regex in Power BI to validate emails with
    Python* section to import the data contained in the `Users.xlsx` file.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *使用 Python 在 Power BI 中验证电子邮件的正则表达式* 部分的 *步骤 1 到 3* 以导入 `Users.xlsx` 文件中的数据。
- en: Click on the **Transform** menu, and then click on **Run R Script**.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **转换** 菜单，然后点击 **运行 R 脚本**。
- en: 'Then, copy and paste the following code into the R script editor and click
    on **OK**:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将以下代码复制并粘贴到 R 脚本编辑器中，并点击 **确定**：
- en: '[PRE8]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You can find a more exhaustive R script in the `04-validate-dates-with-regex-with-r.R`
    file, which can be found in the repository that comes with the book, in the `Chapter05\validating-data-using-regex`
    folder. That script handles dates in the formats of `mm-dd-yyyy`, `dd-mm-yyyy`,
    and `yyyy-mm-dd` with all their variances, including both `-` and `/` as separators.
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在与本书一起提供的存储库中的 `Chapter05\validating-data-using-regex` 文件夹中找到更详尽的 R 脚本 `04-validate-dates-with-regex-with-r.R`
    文件。该脚本处理 `mm-dd-yyyy`、`dd-mm-yyyy` 和 `yyyy-mm-dd` 格式的日期，包括所有变体，包括 `-` 和 `/` 作为分隔符。
- en: If Power BI requires you to provide it with data privacy information, you already
    know how to proceed based on what we've discussed in the previous sections.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 Power BI 需要你提供数据隐私信息，你根据我们在前几节中讨论的内容已经知道如何操作。
- en: As you can see, the **isValidDateFromRegex** column has been added, and it contains
    the Boolean values resulting from the validation of the emails through your regex.
    If you do a check, they coincide with the values given by definition in the **IsDateValidByDefinition**
    column. Your regex did a great job! Now you can go back to the **Home** menu and
    click on **Close & Apply**.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如你所见，已添加了 **isValidDateFromRegex** 列，它包含通过你的正则表达式验证电子邮件产生的布尔值。如果你进行检查，它们与 **IsDateValidByDefinition**
    列中定义的值相一致。你的正则表达式做得很好！现在你可以回到 **主页** 菜单并点击 **关闭并应用**。
- en: Thanks to the **isDateValidFromRegexegex** column, you can now appropriately
    filter the correct and incorrect dates in your reports.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了 **isDateValidFromRegex** 列，你现在可以适当地过滤报告中的正确和错误日期。
- en: In the next section, you will learn how to import the contents of a semi-structured
    log file using Python and R.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将学习如何使用 Python 和 R 语言导入半结构化日志文件的内容。
- en: Loading complex log files using regex in Power BI
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用正则表达式在 Power BI 中加载复杂日志文件
- en: Log files are a very useful tool for developers and administrators of computer
    systems. They record what happened to the system, when it happened, and which
    user actually generated the event. Thanks to these files, you can find information
    about any system failure, thus allowing a faster diagnosis of the causes of these
    faults.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 日志文件是开发人员和计算机系统管理员非常有用的工具。它们记录了系统发生的事情、何时发生以及哪个用户实际生成了事件。多亏了这些文件，你可以找到有关任何系统故障的信息，从而允许更快地诊断这些故障的原因。
- en: Logs are often **semi-structured data**, that is, information that cannot be
    persisted in a relational database in the format in which it is generated. In
    order to be analyzed with the usual tools, first, this data must be transformed
    into a more suitable format.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 日志通常是 **半结构化数据**，即不能以生成时的格式持久化在关系数据库中的信息。为了能够使用常规工具进行分析，首先，必须将此数据转换为更合适的格式。
- en: Since they are not structured data, it is difficult for them to be imported
    into Power BI as is, unless someone has developed a custom connector to do so.
    It is in these scenarios that using a regex in languages such as Python or R can
    help us get the desired results.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们不是结构化数据，直接导入 Power BI 是困难的，除非有人开发了专门的连接器来处理这种情况。在这些场景中，使用 Python 或 R 等语言中的正则表达式可以帮助我们获得所需的结果。
- en: Apache access logs
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Apache 访问日志
- en: 'Let''s suppose your company has a website published through an Apache web server.
    Your manager asks you to carry out an analysis regarding which web pages of the
    site are the most clicked on. The only way to get this information is to analyze
    the *access log file*. This file records data about all requests made to the web
    server. Here is an example of an Apache access log:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的公司通过 Apache 网络服务器发布了一个网站。你的经理要求你进行一项分析，以确定网站上哪些网页被点击得最多。获取这些信息的唯一方法是分析 *访问日志文件*。该文件记录了所有对网络服务器的请求。以下是一个
    Apache 访问日志的示例：
- en: '![Figure 5.29 – An example of an Apache access log](img/file142.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.29 – Apache 访问日志的一个示例](img/file142.png)'
- en: Figure 5.29 – An example of an Apache access log
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.29 – Apache 访问日志的一个示例
- en: As you can see, at first glance, there is a fairly organized structure to the
    information in this log. If no one has customized the output of the Apache log
    files, it uses the **Common Log Format** (**CLF**) by default. You can find a
    real example of an Apache access log in the `apache_logs.txt` file, which is inside
    the repository that comes with this book, in the `Chapter05\loading-complex-log-files-using-regex`
    folder. We found it in the GitHub repository at [http://bit.ly/apache-access-log](http://bit.ly/apache-access-log)
    (click on **Download** to view it).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，乍一看，这个日志中的信息结构相当有组织。如果没有人定制 Apache 日志文件的输出，它默认使用 **通用日志格式**（**CLF**）。你可以在与本书一起提供的存储库中的
    `apache_logs.txt` 文件中找到一个 Apache 访问日志的真实示例，该文件位于 `Chapter05\loading-complex-log-files-using-regex`
    文件夹中。我们在 GitHub 存储库中找到了它，[http://bit.ly/apache-access-log](http://bit.ly/apache-access-log)（点击
    **Download** 查看它）。
- en: 'If you go ahead and read the documentation of those log files, you will deduce
    that the information recorded in the access log follows the *NCSA extended/combined
    log format*. So, the data that is recorded is as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你继续阅读那些日志文件的文档，你会推断出访问日志中记录的信息遵循 *NCSA 扩展/组合日志格式*。因此，记录的数据如下：
- en: The remote hostname (the IP address).
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 远程主机名（IP 地址）。
- en: The remote logname (if empty, you'll find a dash; it is not used in the sample
    file).
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 远程日志名（如果为空，你会看到一个破折号；在样本文件中未使用）。
- en: The remote user if the request was authenticated (if empty, you'll find a dash).
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果请求被认证的远程用户（如果为空，你会看到一个破折号）。
- en: The datetime that the request was received, in the `[18/Sep/2011:19:18:28 -0400]`
    format.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请求接收的日期时间，格式为 `[18/Sep/2011:19:18:28 -0400]`。
- en: The first line of the request made to the server between double quotes.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请求服务器之间的双引号中的第一行请求。
- en: The HTTP status code for the request.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请求的 HTTP 状态码。
- en: The size of the response in bytes, excluding the HTTP headers (could be a dash).
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 响应的大小（以字节为单位），不包括 HTTP 头部（可能是一个破折号）。
- en: The `Referer` HTTP request header, which contains the absolute or partial address
    of the page making the request.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Referer` HTTP 请求头部，其中包含发起请求的页面的绝对或部分地址。'
- en: The `User-Agent` HTTP request header, which contains a string that identifies
    the application, operating system, vendor, and/or version of the requesting user
    agent.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`User-Agent` HTTP 请求头部，其中包含一个字符串，用于标识请求用户代理的应用程序、操作系统、供应商和/或版本。'
- en: Once you know both the nature of the information written in the log and the
    form in which it is written, you can take advantage of the powerful tools provided
    by regexes to better structure this information and import it into Power BI.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你知道日志中写入信息的性质以及它的形式，你就可以利用正则表达式提供的强大工具来更好地结构化这些信息并将其导入 Power BI。
- en: Importing Apache access logs in Power BI with Python
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Python 将 Apache 访问日志导入 Power BI
- en: As mentioned earlier, you can find a real example of an Apache access log in
    the `apache_logs.txt` file, which is inside the repository that comes with this
    book, in the `Chapter05\loading-complex-log-files-using-regex` folder. You will
    load the information in this file using a Python script, not a Power BI connector.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，你可以在与本书一起提供的存储库中的 `apache_logs.txt` 文件中找到一个 Apache 访问日志的真实示例，该文件位于 `Chapter05\loading-complex-log-files-using-regex`
    文件夹中。你将使用 Python 脚本来加载这个文件中的信息，而不是使用 Power BI 连接器。
- en: 'Compared to what you''ve learned before about regexes and Python, in the `01-apache-access-log-parser-python.py`
    Python script (which you''ll find in the preceding folder), you''ll encounter
    these new constructs:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 与你之前关于正则表达式和 Python 的学习相比，在 `01-apache-access-log-parser-python.py` Python 脚本（你可以在前面的文件夹中找到）中，你会遇到这些新的结构：
- en: To be able to read a text file line by line in Python, you'll use the `open(file,
    mode)` functions and the `readlines()` method. Specifically, you're going to read
    the `apache_logs.txt` file as read-only (`'r'`) and read each of its lines to
    store them in a list.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要在 Python 中逐行读取文本文件，你将使用 `open(file, mode)` 函数和 `readlines()` 方法。具体来说，你将按顺序读取
    `apache_logs.txt` 文件，将其作为只读文件（`'r'`）读取，并将每一行存储在列表中。
- en: 'In regexes, it is possible to refer to groups identified by round brackets
    not only by a numerical index but also *by a name.* This is thanks to **named
    capturing groups**. Usually, the regex syntax that is used to assign a name to
    a group is `(?<group-name>…)`. In Python, it is `(?P<group-name>…)`:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在正则表达式中，除了可以通过数字索引引用由圆括号标识的组之外，还可以通过名称来引用。这要归功于**命名捕获组**。通常，用于给组分配名称的正则表达式语法是
    `(?<group-name>...)`。在 Python 中是 `(?P<group-name>...)`：
- en: 'In Python, you can define a list of regex parts that can be merged together
    (`join`) using a separator, which is defined by a regex itself (`\s+`):'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 中，你可以定义一个可以合并在一起的正则表达式部分列表（使用分隔符 `join`），分隔符由正则表达式本身定义（`\s+`）：
- en: '[PRE9]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that, in this case, the `re.compile()` function is used since the match
    must be done many times on all lines of the log; therefore, precompiling the regex
    could have computational advantages.
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，在这种情况下，使用了 `re.compile()` 函数，因为必须在日志的所有行上多次进行匹配；因此，预编译正则表达式可能具有计算优势。
- en: 'Pattern matching is done for each line in the log:'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对日志中的每一行都进行模式匹配：
- en: '[PRE10]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `groupdict()` method returns a dictionary with the group names as the key
    and the matched strings as the value for that key. All the dictionaries for each
    line are appended to the `log_data` list.
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`groupdict()` 方法返回一个字典，其中以组名为键，以匹配字符串为值。每行的所有字典都附加到 `log_data` 列表中。'
- en: We leave it to the reader to interpret how each individual regex part goes about
    capturing the desired string.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将如何解释每个正则表达式部分如何捕获所需字符串的细节留给读者去理解。
- en: 'Now that we''ve clarified a few points in the code, let''s import the log into
    Power BI:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经澄清了代码中的几个要点，让我们将日志导入 Power BI：
- en: In Power BI Desktop, be sure to use the `pbi_powerquery_env` environment.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Power BI Desktop 中，务必使用 `pbi_powerquery_env` 环境。
- en: Go to **Get Data** and select the Python script.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往 **获取数据** 并选择 Python 脚本。
- en: Copy and paste the script from the `01-apache-access-log-parser-python.py` file
    into the Python script editor and click on **OK**.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `01-apache-access-log-parser-python.py` 文件中的脚本复制并粘贴到 Python 脚本编辑器中，然后点击 **确定**。
- en: 'Then, select the **df** dataframe from the **Navigator** window and click on
    **Load**:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，从 **导航器** 窗口中选择 **df** 数据框，并点击 **加载**：
- en: '![Figure 5.30 – Selecting the df dataframe returned by the Python script](img/file143.png)'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.30 – 选择由 Python 脚本返回的 df 数据框](img/file143.png)'
- en: Figure 5.30 – Selecting the df dataframe returned by the Python script
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.30 – 选择由 Python 脚本返回的 df 数据框
- en: 'If you click on the **Data** icon, you can view the entire log loaded as a
    structured table:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你点击 **数据** 图标，你可以查看作为结构化表格加载的整个日志：
- en: '![Figure 5.31 – The Apache access log is loaded in Power BI with Python](img/file144.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.31 – 使用 Python 在 Power BI 中加载 Apache 访问日志](img/file144.png)'
- en: Figure 5.31 – The Apache access log is loaded in Power BI with Python
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.31 – 使用 Python 在 Power BI 中加载 Apache 访问日志
- en: Awesome! Thanks to the power of regex, you've just managed to easily import
    into Power BI what looked like a complex log file to manage.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！多亏了正则表达式的强大功能，你刚刚轻松地将看似复杂的日志文件导入到 Power BI 中。
- en: Importing Apache access logs in Power BI with R
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 R 将 Apache 访问日志导入 Power BI
- en: In this section, you will load the information of the `apache_logs.txt` file,
    but this time, using an R script.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将使用 R 脚本来加载 `apache_logs.txt` 文件的信息。
- en: 'Compared to what you''ve learned previously about regexes in R, in the `02-apache-access-log-parser-r.R`
    script (which you''ll find in the same preceding folder), you''ll encounter these
    new constructs:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 与你之前在 R 中学习的关于正则表达式的知识相比，在 `02-apache-access-log-parser-r.R` 脚本（你可以在同一前一个文件夹中找到）中，你会遇到这些新的结构：
- en: To be able to read a text file line by line in R, you'll use the `read_lines()`
    function from the `readr` package. Specifically, you're going to read each line
    of the `apache_logs.txt` file in order to persist them to a vector.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要在 R 中逐行读取文本文件，你将使用 `read_lines()` 函数从 `readr` 包中。具体来说，你将按顺序读取 `apache_logs.txt`
    文件的每一行，以便将它们持久化到向量中。
- en: 'In order to take full advantage of named capturing groups in R, you need to
    install and use the features of a new package, called **namedCapture**. Thanks
    to this package, both regex syntaxes for named groups are allowed: the standard
    `(?<group-name>…)` regex syntax and the `(?P<group-name>…)` regex syntax.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了充分利用 R 中的命名捕获组，你需要安装并使用一个名为 **namedCapture** 的新包的功能。多亏了这个包，命名组的正则表达式语法都允许：标准的
    `(?<group-name>…)` 正则表达式语法和 `(?P<group-name>…)` 正则表达式语法。
- en: 'Just as we did in the Python script, in R, you''ll also define a vector of
    regex parts, which you''ll merge with the `paste(..., collapse = ''...'')` function.
    This function has the task of joining regex parts together through the `\s+` separator.
    After merging all of the parts, the `$` character is added to the end of the resulting
    string using the `paste0(…)` function. Remember that raw strings have a different
    syntax in R than in Python. In this case, we will use the `r''{...}''` syntax:'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就像我们在 Python 脚本中所做的那样，在 R 中，你也将定义一个正则表达式部分的向量，然后使用 `paste(..., collapse = '...')`
    函数将其合并。这个函数的任务是通过 `\s+` 分隔符将正则表达式部分连接起来。合并所有部分后，使用 `paste0(…)` 函数将 `$` 字符添加到结果字符串的末尾。记住，R
    中的原始字符串语法与 Python 不同。在这种情况下，我们将使用 `r'{...}'` 语法：
- en: '[PRE11]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Pattern matching is done using the `str_match_named()` function of the `namedCapture`
    package over the whole log vector, using a single-line command:'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `namedCapture` 包的 `str_match_named()` 函数在整个日志向量上执行模式匹配，通过一条单行命令：
- en: '[PRE12]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Again, we leave it to the reader to interpret how each individual regex part
    goes about capturing the desired string.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们将解释每个正则表达式部分如何捕获所需字符串的任务留给读者。
- en: 'Now that we''ve clarified a few points in the code, let''s import the log into
    Power BI:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经澄清了代码中的几个要点，让我们将日志导入 Power BI：
- en: 'First, you need to install the `namedCapture` package. So, open RStudio and
    make sure that the engine being referenced is the latest one in **Global Options**.
    Then, run the following code in a new script to temporarily set CRAN as the repository
    to download packages from:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你需要安装 `namedCapture` 包。因此，打开 RStudio 并确保在 **全局选项** 中引用的是最新版本。然后，在新的脚本中运行以下代码以临时将
    CRAN 设置为下载包的存储库：
- en: '[PRE13]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: All this has been done to download the latest version of the `namedCapture`
    package.
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有这些都是为了下载 `namedCapture` 包的最新版本。
- en: 'Now, go to the console and enter and run the following code:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，转到控制台并输入并运行以下代码：
- en: '[PRE14]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Open Power BI Desktop, go to **Get Data**, and select the R script.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 Power BI Desktop，转到 **获取数据**，并选择 R 脚本。
- en: Copy and paste the script from the `02-apache-access-log-parser-r.R` file into
    the R script editor and then click on **OK**.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `02-apache-access-log-parser-r.R` 文件中的脚本复制并粘贴到 R 脚本编辑器中，然后点击 **确定**。
- en: 'Then, select the **df** dataframe from the **Navigator** window and click on
    **Load**:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，从 **导航器** 窗口中选择 **df** 数据框，并点击 **加载**：
- en: '![Figure 5.32 – Selecting the df dataframe returned by the R script](img/file145.png)'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.32 – 选择由 R 脚本返回的 df 数据框](img/file145.png)'
- en: Figure 5.32 – Selecting the df dataframe returned by the R script
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.32 – 选择由 R 脚本返回的 df 数据框
- en: 'If you click on the **Data** icon, you can view the entire log loaded as a
    structured table:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你点击 **数据** 图标，你可以查看加载为结构化表的整个日志：
- en: '![Figure 5.33 – The Apache access log loaded in Power BI with R](img/file146.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.33 – 使用 R 在 Power BI 中加载的 Apache 访问日志](img/file146.png)'
- en: Figure 5.33 – The Apache access log loaded in Power BI with R
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.33 – 使用 R 在 Power BI 中加载的 Apache 访问日志
- en: Great job! You were able to import a semi-structured log file into Power BI
    even with R.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！你甚至使用 R 成功地将半结构化日志文件导入 Power BI。
- en: Extracting values from text using regex in Power BI
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Power BI 中使用正则表达式从文本中提取值
- en: 'The last use case we want to present happens very often when dealing with shipments
    of goods to customers. Sometimes, it happens that a fraudster manages to steal
    the goods addressed to a customer; therefore, the customer must be refunded by
    the company. The defrauded customer then contacts Customer Care to request a refund.
    If the management system provided to the Customer Care operator who has to manage
    the case does not allow you to enter the information of the refund in a structured
    way, the operator must resort to the only possible method: the entry of a *free
    text note* associated with the order, which specifies the *amount*, the *reason*
    and the *date* of the refund.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要展示的最后一种用例在处理向客户发货时非常常见。有时，会发生欺诈者设法窃取寄给客户的货物的情况；因此，公司必须向客户退款。被欺诈的客户随后联系客户服务部要求退款。如果提供给客户服务操作员的管理系统不允许以结构化的方式输入退款信息，操作员必须求助于唯一可能的方法：与订单关联的*自由文本备注*，其中指定了*金额*、*原因*和*退款日期*。
- en: You already know that information entered in free text is every analyst's nightmare,
    especially when your boss asks you to analyze the very information entered in
    these infamous notes.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经知道，以自由文本形式输入的信息是每位分析师的噩梦，尤其是当你的老板要求你分析这些臭名昭著的备注中的信息时。
- en: 'In the repository that comes with this book, you can find the `OrderNotes.xlsx`
    Excel file inside the `Chapter05` folder. Its content is similar to the content
    shown in *Figure 5.34*:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书附带的重构库中，你可以在`Chapter05`文件夹中找到`OrderNotes.xlsx` Excel文件。其内容与图5.34所示的内容类似：
- en: '![Figure 5.34 – Free text notes entered by the operator for some orders](img/file147.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![图5.34 – 操作员为某些订单输入的自由文本备注](img/file147.png)'
- en: Figure 5.34 – Free text notes entered by the operator for some orders
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.34 – 操作员为某些订单输入的自由文本备注
- en: 'As you can see, by looking at the contents of the Excel file, the relevant
    information to extract from the notes is as follows:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，通过查看Excel文件的内容，从备注中需要提取的相关信息如下：
- en: The refund amount
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 退款金额
- en: The refund reason
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 退款原因
- en: The refund date
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 退款日期
- en: 'The problem is that the Customer Care operators used a lot of imagination to
    enter this information, without the slightest predetermined rule of how to structure
    it. From this, we can see the following:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于客户服务操作员在输入这些信息时使用了大量的想象力，而没有一丝预先设定的结构化规则。从这一点我们可以看出以下情况：
- en: The refund amount was entered as *EUR xx.yy*, *EURxx.yy*, *xx.yy EUR*, *€ xx.yy*,
    *xx.yy€*, and *xx.yy €*.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 退款金额被输入为*EUR xx.yy*、*EURxx.yy*、*xx.yy EUR*、*€ xx.yy*、*xx.yy€*和*xx.yy €*。
- en: The "separator" between all of the pieces of information can be made by one
    or more whitespaces or by a dash surrounded by one or more spaces.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有信息片段之间的“分隔符”可以由一个或多个空格或由一个或多个空格包围的破折号组成。
- en: The refund date is always in the `dd/mm/yyyy` format (you are lucky here!).
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 退款日期始终以`dd/mm/yyyy`格式（你很幸运！）。
- en: The refund reason could contain any text.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 退款原因可以包含任何文本。
- en: Given all this generality of the entered notes, is it possible to correctly
    extract the information needed for the analysis requested by your boss? The answer
    is certainly "yes" if you know how to best use regexes.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到输入备注的这种普遍性，你是否能够正确提取老板要求分析所需的信息？如果你知道如何最好地使用正则表达式，答案无疑是“是的”。
- en: One regex to rule them all
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个正则表达式统治一切
- en: 'With the experience you gathered during the previous sections, you will immediately
    understand the solution we are going to propose. Consider the following regex
    parts:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几节中积累的经验将使你立即理解我们将要提出的解决方案。考虑以下正则表达式部分：
- en: '**Currency**: `(?:EUR|€)`'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**货币**: `(?:EUR|€)`'
- en: '**Separator**: `(?:\s+)?-?(?:\s+)?`'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分隔符**: `(?:\s+)?-?(?:\s+)?`'
- en: '**Refund amount**: `\d{1,}\.?\d{0,2}`'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**退款金额**: `\d{1,}\.?\d{0,2}`'
- en: '**Refund reason**: `.*?`'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**退款原因**: `.*?`'
- en: '**Refund date**: `\d{2}[\-\/]\d{2}[\-\/]\d{4}`'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**退款日期**: `\d{2}[\-\/]\d{2}[\-\/]\d{4}`'
- en: 'Remember the syntax of a **non-capturing group**, `(?:…)`? Well, with this
    syntax, you''re explicitly saying to the regex engine that you don''t want to
    capture the content inside those brackets, as this isn''t important information
    to extract. That said, the final regex is nothing more than multiple alternative
    combinations of these parts, such as the one you can see in *Figure 5.35*:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 记住非捕获组的语法 `(?:…)`?。使用这种语法，你明确告诉正则表达式引擎你不想捕获括号内的内容，因为这些内容不是提取的重要信息。话虽如此，最终的正则表达式不过是这些部分的多个替代组合，就像你在图5.35中看到的那样：
- en: '![Figure 5.35 – The full regex structure for extracting information from notes](img/file148.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![图5.35 – 提取笔记信息的完整正则表达式结构](img/file148.png)'
- en: Figure 5.35 – The full regex structure for extracting information from notes
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.35 – 提取笔记信息的完整正则表达式结构
- en: 'If you''re curious to see it in full, the final complete regex is as follows:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看完整的正则表达式，最终的完整正则表达式如下：
- en: '[PRE15]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Let's implement it in Power BI using Python.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在Power BI中使用Python实现它。
- en: Using regex in Power BI to extract values with Python
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Python在Power BI中提取值
- en: 'As you saw from *Figure 5.35*, our regex contains named groups that are *reused
    multiple times* within it. Unfortunately, the reuse of the same named group within
    a regex is not supported by the Python `re` module, which, by the way, is the
    module that is also used behind the scenes in `pandas`. In order to use more advanced
    features of regex, such as the previously mentioned *identically named groups*
    or *lookbehind* and *lookahead* syntaxes (which are not explored in this chapter),
    you must use the `regex` module. So, first of all, you have to install it within
    your *pbi_powerquery_env* environment. Then, you have to load the `OrderNotes.xlsx`
    Excel file, which you can find in the `Chapter05` folder, into Power BI Desktop.
    After that, you can transform that dataset using a Python script. So, let''s get
    started:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在*图5.35*中看到的，我们的正则表达式包含被多次*重复使用*的命名组。不幸的是，Python `re`模块不支持在正则表达式中重复使用相同的命名组，顺便说一下，这个模块也是`pandas`背后使用的模块。为了使用更高级的正则表达式功能，例如之前提到的*同名组*或*向后查找*和*向前查找*语法（这些内容在本章中未探讨），您必须使用`regex`模块。因此，首先，您必须在您的*pbi_powerquery_env*环境中安装它。然后，您必须将位于`Chapter05`文件夹中的`OrderNotes.xlsx`
    Excel文件加载到Power BI Desktop中。之后，您可以使用Python脚本转换该数据集。那么，让我们开始吧：
- en: 'Open your Anaconda Prompt, switch to your `pbi_powerquery_env` environment
    using the `conda activate pbi_powerquery_env` command, and then install the `regex`
    package using this code: `pip install regex`.'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开您的Anaconda Prompt，使用`conda activate pbi_powerquery_env`命令切换到您的`pbi_powerquery_env`环境，然后使用以下代码安装`regex`包：`pip
    install regex`。
- en: Open your Power BI Desktop and make sure the referenced Python environment is
    `pbi_powerquery_env` in the **Options**.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开您的Power BI Desktop，确保在**选项**中引用的Python环境是`pbi_powerquery_env`。
- en: From the ribbon, click on the **Excel** icon to import data from Excel and open
    the `OrderNotes.xlsx` file.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从功能区点击**Excel**图标，从Excel导入数据并打开`OrderNotes.xlsx`文件。
- en: 'Select the **Sheet1** dataset from the **Navigator** window and click on **Transform
    Data**:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**导航器**窗口中选择**Sheet1**数据集，并点击**转换数据**：
- en: '![Figure 5.36 – Loading the order notes from Excel and transforming the data](img/file149.png)'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图5.36 – 从Excel加载订单笔记并转换数据](img/file149.png)'
- en: Figure 5.36 – Loading the order notes from Excel and transforming the data
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.36 – 从Excel加载订单笔记并转换数据
- en: 'Declare the first row of loaded data as column headers by clicking on **Use
    First Row as Headers**:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击**使用第一行作为标题**，将加载的数据的第一行声明为列标题：
- en: '![Figure 5.37 – The Use First Row as Headers button](img/file150.png)'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图5.37 – 使用第一行作为标题的按钮](img/file150.png)'
- en: Figure 5.37 – The Use First Row as Headers button
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.37 – 使用第一行作为标题的按钮
- en: Then, go to the **Transform** menu and click on **Run Python script**.
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，转到**转换**菜单并点击**运行Python脚本**。
- en: Open the Python script that you can find inside the `01-order-notes-parser-python.py`
    file, which is in the `Chapter05\extracting-values-from-text-using-regex` folder.
    Copy and paste the script into the **Run Python script** editor and then click
    on **OK**.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开位于`Chapter05\extracting-values-from-text-using-regex`文件夹中的`01-order-notes-parser-python.py`文件中的Python脚本，将其复制并粘贴到**运行Python脚本**编辑器中，然后点击**确定**。
- en: If there are any issues with the compatibility levels of the datasets, simply
    open the **Data source settings** window and set the permissions of each dataset
    to *Organizational* using **Edit Permissions…**. Then, click on **Refresh Preview**.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果数据集的兼容性级别存在问题，只需打开**数据源设置**窗口，使用**编辑权限…**将每个数据集的权限设置为*组织*。然后，点击**刷新预览**。
- en: 'We are only interested in the `df` dataset. So, click on its **Table** value:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只对`df`数据集感兴趣。因此，点击其**表**值：
- en: '![Figure 5.38 – Selecting the df dataset as a result of the Python script transformation](img/file151.png)'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图5.38 – 选择Python脚本转换后的df数据集](img/file151.png)'
- en: Figure 5.38 – Selecting the df dataset as a result of the Python script transformation
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.38 – 选择Python脚本转换后的df数据集
- en: 'You can see that the resulting table has three more columns, where each one
    is related to a named group:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以看到，结果表多了三列，每一列都与一个命名组相关：
- en: '![Figure 5.39 – The values extracted from free notes using regex with Python](img/file152.png)'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.39 – 使用 Python 和正则表达式从自由笔记中提取的值](img/file152.png)'
- en: Figure 5.39 – The values extracted from free notes using regex with Python
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.39 – 使用 Python 和正则表达式从自由笔记中提取的值
- en: Finally, go to the **Home** menu and click on **Close & Apply**.
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，转到 **主页** 菜单并点击 **关闭并应用**。
- en: Awesome! You've just managed to reorganize the data contained in the order notes
    using regexes in Python. Your boss will be more than satisfied!
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！你刚刚成功地使用 Python 中的正则表达式重新组织了订单笔记中的数据。你的老板一定会非常满意！
- en: Using regex in Power BI to extract values with R
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 R 在 Power BI 中通过正则表达式提取值
- en: 'In R, we can continue to use the `namedCapture` package to manage named groups
    that are reused multiple times in the same regex. We can do this by putting the
    `(?J)` modifier in front of it (this allows multiple named capturing groups to
    share the same name). Unlike Python, in R, the `str_match_named()` function of
    the `namedCapture` package does not return a one-time result captured by the named
    group. It returns as many columns as the number of times it was used:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在 R 中，我们仍然可以使用 `namedCapture` 包来管理在同一个正则表达式中多次重复使用的命名组。我们可以通过在其前面放置 `(?J)` 修饰符来实现这一点（这允许多个命名捕获组共享相同的名称）。与
    Python 不同，在 R 中，`namedCapture` 包的 `str_match_named()` 函数不会返回由命名组捕获的一次性结果。它返回的列数与它被使用的次数一样多：
- en: '![Figure 5.40 – Returning as many columns as the number of times the named
    group is used](img/file153.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.40 – 返回与命名组使用次数相同的列数](img/file153.png)'
- en: Figure 5.40 – Returning as many columns as the number of times the named group
    is used
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.40 – 返回与命名组使用次数相同的列数
- en: For this reason, we had to further manipulate the result; first, by replacing
    the empty characters with the null value of `NA`, and second, by applying the
    `coalesce()` function of `dplyr`, which merges multiple columns into one by keeping
    the non-null values.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不得不进一步处理结果；首先，通过将空字符替换为 `NA` 的空值，其次，通过应用 `dplyr` 的 `coalesce()` 函数，该函数通过保留非空值将多个列合并为一列。
- en: '**Important note**'
  id: totrans-365
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**重要提示**'
- en: ''
  id: totrans-366
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We pointed out this limitation to Toby Dylan Hocking, the author of the `namedCapture`
    package, who recently implemented the feature inside the new version of the package,
    named `nc`. You can find details of the implementation at [https://github.com/tdhock/namedCapture/issues/15](https://github.com/tdhock/namedCapture/issues/15).
    The new version of the `nc` package has not yet been published onto CRAN at the
    time of writing. Therefore, we thought it appropriate to keep the use of the `namedCapture`
    package in our code. However, feel free to adopt the new `nc` package for your
    future projects.
  id: totrans-367
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们将这个限制指出了 `namedCapture` 包的作者 Toby Dylan Hocking，他最近在包的新版本中实现了这个功能，名为 `nc`。你可以在
    [https://github.com/tdhock/namedCapture/issues/15](https://github.com/tdhock/namedCapture/issues/15)
    找到实现的详细信息。在撰写本文时，`nc` 包的新版本尚未发布到 CRAN。因此，我们认为在代码中保持使用 `namedCapture` 包是合适的。然而，你可以自由地在你未来的项目中采用新的
    `nc` 包。
- en: 'That said, let''s start to extract values from the order notes using R in Power
    BI:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们开始使用 R 在 Power BI 中从订单笔记中提取值：
- en: Open your Power BI Desktop and make sure the referenced R engine is the latest
    one (in our case, this is *MRO 4.0.2*).
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你的 Power BI Desktop，并确保引用的 R 引擎是最新版本（在我们的例子中，这是 *MRO 4.0.2*）。
- en: From the ribbon, click on the **Excel** icon to import data from Excel, and
    open the `OrderNotes.xlsx` file, which you can find in the `Chapter05` folder.
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从功能区点击 **Excel** 图标以从 Excel 导入数据，并打开位于 `Chapter05` 文件夹中的 `OrderNotes.xlsx` 文件。
- en: 'Select the **Sheet1** dataset from the **Navigator** window, and click on **Transform
    Data**:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 **导航器** 窗口中选择 **Sheet1** 数据集，并点击 **转换数据**：
- en: '![Figure 5.41 – Loading the order notes from Excel and transforming the data](img/file154.png)'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.41 – 从 Excel 加载订单笔记并转换数据](img/file154.png)'
- en: Figure 5.41 – Loading the order notes from Excel and transforming the data
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.41 – 从 Excel 加载订单笔记并转换数据
- en: Declare the first row of loaded data as column headers by clicking on **Use
    First Row as Headers**.
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击 **使用第一行作为标题** 将加载数据的第 一行声明为列标题。
- en: Then, go to the **Transform** menu and click on **Run R script**.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，转到 **转换** 菜单并点击 **运行 R 脚本**。
- en: Open the R script that you can find inside the `02-order-notes-parser-r.R` file,
    which is in the `Chapter05\extracting-values-from-text-using-regex` folder. Copy
    and paste the script into the **Run R script** editor and then click on **OK**.
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开位于 `Chapter05\extracting-values-from-text-using-regex` 文件夹中的 `02-order-notes-parser-r.R`
    文件中的 R 脚本，将其复制并粘贴到**运行 R 脚本**编辑器中，然后点击**确定**。
- en: 'We are only interested in the `df` dataset. So, click on its **Table** value:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只对 `df` 数据集感兴趣。因此，点击其**表**值：
- en: '![Figure 5.42 – Selecting the df dataset as a result of the R script transformation](img/file155.png)'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.42 – 选择 R 脚本转换后的 df 数据集](img/file155.png)'
- en: Figure 5.42 – Selecting the df dataset as a result of the R script transformation
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.42 – 选择 R 脚本转换后的 df 数据集
- en: 'You can see that the resulting table has three more columns, each one related
    to a named group:'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以看到，结果表有另外三列，每一列都与一个命名组相关：
- en: '![Figure 5.43 – The values extracted from the free notes using regex with Python](img/file156.png)'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.43 – 使用 Python 正则表达式从自由笔记中提取的值](img/file156.png)'
- en: Figure 5.43 – The values extracted from the free notes using regex with Python
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.43 – 使用 Python 正则表达式从自由笔记中提取的值
- en: Finally, go to the **Home** menu and click on **Close & Apply**.
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，转到**首页**菜单并点击**关闭并应用**。
- en: Amazing! You've just demonstrated that you know how to rearrange data contained
    in the order notes using regexes even with R.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！你刚刚证明了你知道如何使用正则表达式重新排列订单笔记中的数据，即使使用 R。
- en: Summary
  id: totrans-385
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you were introduced to the basics of how to use regexes. Using
    the bare minimum, you were able to effectively validate strings representing email
    addresses and dates in Power BI, using both Python and R.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你被介绍了如何使用正则表达式的基础知识。使用最基本的方法，你能够有效地在 Power BI 中使用 Python 和 R 验证表示电子邮件地址和日期的字符串。
- en: Additionally, you learned how to extract information from semi-structured log
    files through the use of regexes, and how to import the extracted information,
    in a structured way, into Power BI.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还学会了如何通过使用正则表达式从半结构化日志文件中提取信息，以及如何以结构化的方式将提取的信息导入 Power BI。
- en: Finally, you learned how to use regex in Python and R to extract information
    from seemingly unprocessable free text thanks to the real-world case of notes
    associated with sales orders.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你通过销售订单相关笔记的实际情况，学会了如何在 Python 和 R 中使用正则表达式从看似无法处理的自由文本中提取信息。
- en: In the next chapter, you'll learn how to use some de-identification techniques
    in Power BI to anonymize or pseudonymize datasets that show sensitive data about
    individuals in plain text before they are imported into Power BI.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习如何在 Power BI 中使用一些脱敏技术来匿名化或伪匿名化在导入 Power BI 之前以纯文本形式显示敏感个人数据的数据集。
- en: References
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'For additional reading, please refer to the following books and articles:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步阅读，请参考以下书籍和文章：
- en: '*Regular Expressions: The Complete Tutorial, Jan Goyvaerts* ([https://www.princeton.edu/~mlovett/reference/Regular-Expressions.pdf](https://www.princeton.edu/~mlovett/reference/Regular-Expressions.pdf))'
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*正则表达式：完整教程，作者：Jan Goyvaerts* ([https://www.princeton.edu/~mlovett/reference/Regular-Expressions.pdf](https://www.princeton.edu/~mlovett/reference/Regular-Expressions.pdf))'
- en: '*Data Privacy Settings In Power BI/Power Query, Part 1: Performance Implications*
    ([https://blog.crossjoin.co.uk/2017/05/24/data-privacy-settings-in-power-bipower-query-part-1-performance-implications/](https://blog.crossjoin.co.uk/2017/05/24/data-privacy-settings-in-power-bipower-query-part-1-performance-implications/))'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Power BI/Power Query 中的数据隐私设置，第 1 部分：性能影响* ([https://blog.crossjoin.co.uk/2017/05/24/data-privacy-settings-in-power-bipower-query-part-1-performance-implications/](https://blog.crossjoin.co.uk/2017/05/24/data-privacy-settings-in-power-bipower-query-part-1-performance-implications/))'
