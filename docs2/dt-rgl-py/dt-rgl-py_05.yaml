- en: '*Chapter 5*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第五章*'
- en: Getting Comfortable with Different Kinds of Data Sources
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 适应不同类型的数据源
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Read CSV, Excel, and JSON files into pandas DataFrames
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将CSV、Excel和JSON文件读入pandas DataFrame
- en: Read PDF documents and HTML tables into pandas DataFrames
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将PDF文档和HTML表格读入pandas DataFrame
- en: Perform basic web scraping using powerful yet easy to use libraries such as
    Beautiful Soup
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用如Beautiful Soup等强大且易于使用的库执行基本的网络爬取
- en: Extract structured and textual information from portals
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从门户网站提取结构和文本信息
- en: In this chapter, you will be exposed to real-life data wrangling techniques,
    as applied to web scraping.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将接触到应用于网络爬取的实际数据整理技术。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简介
- en: So far in this book, we have focused on learning pandas DataFrame objects as
    the main data structure for the application of wrangling techniques. Now, we will
    learn about various techniques by which we can read data into a DataFrame from
    external sources. Some of those sources could be text-based (CSV, HTML, JSON,
    and so on), whereas some others could be binary (Excel, PDF, and so on), that
    is, not in ASCII format. In this chapter, we will learn how to deal with data
    that is present in web pages or HTML documents. This holds very high importance
    in the work of a data practitioner.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这本书中，我们一直专注于学习pandas DataFrame对象作为应用整理技术的主体数据结构。现在，我们将学习各种技术，通过这些技术我们可以从外部来源将数据读入DataFrame。其中一些来源可能是基于文本的（CSV、HTML、JSON等），而另一些则可能是二进制的（Excel、PDF等），即不是ASCII格式。在本章中，我们将学习如何处理存在于网页或HTML文档中的数据。这对于数据从业者的工作非常重要。
- en: Note
  id: totrans-11
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Since we have gone through a detailed example of basic operations with NumPy
    and pandas, in this chapter, we will often skip trivial code snippets such as
    viewing a table, selecting a column, and plotting. Instead, we will focus on showing
    code examples for the new topics we aim to learn about here.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经详细介绍了使用NumPy和pandas的基本操作示例，在本章中，我们将经常跳过诸如查看表格、选择列和绘图等琐碎的代码片段。相反，我们将专注于展示我们在此处旨在学习的新主题的代码示例。
- en: Reading Data from Different Text-Based (and Non-Text-Based) Sources
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从不同的基于文本（和非基于文本）来源读取数据
- en: One of the most valued and widely used skills of a data wrangling professional
    is the ability to extract and read data from a diverse array of sources into a
    structured format. Modern analytics pipelines depend on their ability to scan
    and absorb a variety of data sources to build and analyze a pattern-rich model.
    Such a feature-rich, multi-dimensional model will have high predictive and generalization
    accuracy. It will be valued by stakeholders and end users alike for any data-driven
    product.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据整理专业人士最宝贵且最广泛使用的技能之一是能够从各种来源提取和读取数据到结构化格式。现代分析管道依赖于它们扫描和吸收各种数据源的能力，以构建和分析一个富模式模型。这样一个功能丰富、多维度的模型将具有高度的预测和泛化准确性。它将被利益相关者和最终用户同样重视，对于任何数据驱动的产品。
- en: In the first topic of this chapter, we will go through various data sources
    and how they can be imported into pandas DataFrames, thus imbuing wrangling professionals
    with extremely valuable data ingestion knowledge.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一个主题中，我们将探讨各种数据源以及它们如何导入到pandas DataFrame中，从而为整理专业人士提供极其宝贵的数据摄取知识。
- en: Data Files Provided with This Chapter
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章提供的数据文件
- en: Because this topic is about reading from various data sources, we will use small
    files of various types in the following exercises. All of the data files are provided
    along with the Jupyter notebook in the code repository.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这个主题是关于从各种数据源读取的，所以接下来的练习中我们将使用各种类型的小文件。所有数据文件都随Jupyter笔记本一起提供在代码仓库中。
- en: Libraries to Install for This Chapter
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章需要安装的库
- en: Because this chapter deals with reading various file formats, we need to have
    the support of additional libraries and software platforms to accomplish our goals.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因为本章涉及读取各种文件格式，我们需要额外的库和软件平台的支持来实现我们的目标。
- en: 'Execute the following codes in your Jupyter notebook cells (don''t forget the
    ! before each line of code) to install the necessary libraries:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的Jupyter笔记本单元格中执行以下代码（不要忘记每行代码前的!）来安装必要的库：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Exercise 60: Reading Data from a CSV File Where Headers Are Missing'
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习60：从缺少标题的CSV文件中读取数据
- en: The pandas library provides a simple direct method called `read_csv` to read
    data in a tabular format from a comma-separated text file, or CSV. This is particularly
    useful because a CSV is a lightweight yet extremely handy data exchange format
    for many applications, including such domains as machine-generated data. It is
    not a proprietary format and therefore is universally used by a variety of data-generating
    sources.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 库提供了一个简单的直接方法，称为 `read_csv`，用于从逗号分隔的文本文件或 CSV 中读取表格格式的数据。这特别有用，因为 CSV
    是一种轻量级但极其方便的数据交换格式，适用于许多应用，包括机器生成数据等域。它不是专有格式，因此被各种数据生成源普遍使用。
- en: 'At times, headers may be missing from a CSV file and you may have to add proper
    headers/column names of your own. Let''s have a look at how this can be done:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，CSV 文件可能缺少标题，你可能需要添加自己的正确标题/列名。让我们看看如何做到这一点：
- en: 'Read the example CSV file (with a proper header) using the following code and
    examine the resulting DataFrame, as follows:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码读取示例 CSV 文件（带有合适的标题）并检查生成的 DataFrame，如下所示：
- en: '[PRE1]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output is as follows:'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.1: Output of example CSV file](img/C11065_05_01.jpg)'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.1：示例 CSV 文件的输出](img/C11065_05_01.jpg)'
- en: 'Figure 5.1: Output of example CSV file'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.1：示例 CSV 文件的输出
- en: 'Read a `.csv` file with no header using a pandas DataFrame:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 pandas DataFrame 读取没有标题的 `.csv` 文件：
- en: '[PRE2]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output is as follows:'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.2: Output of the .csv being read using a DataFrame](img/C11065_05_02.jpg)'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.2：使用 DataFrame 读取的 .csv 的输出](img/C11065_05_02.jpg)'
- en: 'Figure 5.2: Output of the .csv being read using a DataFrame'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.2：使用 DataFrame 读取的 .csv 的输出
- en: Certainly, the top data row has been mistakenly read as the column header. You
    can specify `header=None` to avoid this.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当然，顶部数据行被错误地读取为列标题。你可以指定 `header=None` 来避免这种情况。
- en: 'Read the `.csv` file by mentioning the header `None`, as follows:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过指定 `header=None` 来读取 `.csv` 文件，如下所示：
- en: '[PRE3]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'However, without any header information, you will get back the following output.
    The default headers will be just some default numeric indices starting from 0:'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，如果没有标题信息，你将得到以下输出。默认标题将是一些从 0 开始的默认数值索引：
- en: '![Figure 5.3: CSV file with a numeric column header](img/C11065_05_03.jpg)'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.3：具有数值列标题的 CSV 文件](img/C11065_05_03.jpg)'
- en: 'Figure 5.3: CSV file with a numeric column header'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.3：具有数值列标题的 CSV 文件
- en: This may be fine for data analysis purposes, but if you want the DataFrame to
    truly reflect the proper headers, then you will have to add them using the `names`
    argument.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这可能适合数据分析目的，但如果你想使 DataFrame 真正反映正确的标题，那么你必须使用 `names` 参数添加它们。
- en: 'Add the `names` argument to get the correct headers:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加 `names` 参数以获取正确的标题：
- en: '[PRE4]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, you will get a DataFrame that''s as follows:'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，你将得到如下所示的 DataFrame：
- en: '![Figure 5.4: CSV file with correct column header](img/C11065_05_04.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4：具有正确列标题的 CSV 文件](img/C11065_05_04.jpg)'
- en: 'Figure 5.4: CSV file with correct column header'
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.4：具有正确列标题的 CSV 文件
- en: 'Exercise 61: Reading from a CSV File where Delimiters are not Commas'
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 61：从非逗号分隔符的 CSV 文件中读取
- en: 'Although CSV stands for comma-separated-values, it is fairly common to encounter
    raw data files where the separator/delimiter is a character other than a comma:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 CSV 代表逗号分隔值，但遇到分隔符/定界符不是逗号的原始数据文件相当常见：
- en: 'Read a `.csv` file using pandas DataFrames:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 pandas DataFrame 读取 `.csv` 文件：
- en: '[PRE5]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output will be as follows:![Figure 5.5: A DataFrame that has a semi-colon
    as a separator](img/C11065_05_05.jpg)'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出将如下所示：![图 5.5：以分号作为分隔符的 DataFrame](img/C11065_05_05.jpg)
- en: 'Figure 5.5: A DataFrame that has a semi-colon as a separator'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.5：以分号作为分隔符的 DataFrame
- en: 'Clearly, the *;* separator was not expected, and the reading is flawed. A simple
    work around is to specify the separator/delimiter explicitly in the read function:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显然，*;* 分隔符不是预期的，读取有误。一个简单的解决方案是在读取函数中明确指定分隔符/定界符：
- en: '[PRE6]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is as follows:'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.6: Semicolons removed from the DataFrame](img/C11065_05_06.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6：从 DataFrame 中移除的分号](img/C11065_05_06.jpg)'
- en: 'Figure 5.6: Semicolons removed from the DataFrame'
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.6：从 DataFrame 中移除的分号
- en: 'Exercise 62: Bypassing the Headers of a CSV File'
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 62：跳过 CSV 文件的标题
- en: 'If your CSV file already comes with headers but you want to bypass them and
    put in your own, you have to specifically set `header` `=` `0` to make it happen.
    If you try to set the names variable to your header list, unexpected things can
    happen:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的 CSV 文件已经包含标题，但你想跳过它们并添加自己的标题，你必须特别设置 `header` `=` `0` 以实现这一点。如果你尝试将名称变量设置为你的标题列表，可能会发生意外的情况：
- en: 'Add names to a .csv file that has headers, as follows:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为具有标题的 .csv 文件添加名称，如下所示：
- en: '[PRE7]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output is as follows:'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.7: CSV file with headers overlapped](img/C11065_05_07.jpg)'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.7：标题重叠的 CSV 文件](img/C11065_05_07.jpg)'
- en: 'Figure 5.7: CSV file with headers overlapped'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.7：标题重叠的 CSV 文件
- en: 'To avoid this, set `header` to zero and provide a names list:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了避免这种情况，将 `header` 设置为零并提供名称列表：
- en: '[PRE8]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.8: CSV file with defined headers](img/C11065_05_08.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8：具有定义标题的 CSV 文件](img/C11065_05_08.jpg)'
- en: 'Figure 5.8: CSV file with defined headers'
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.8：具有定义标题的 CSV 文件
- en: 'Exercise 63: Skipping Initial Rows and Footers when Reading a CSV File'
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 63：读取 CSV 文件时跳过初始行和页脚
- en: 'Skipping initial rows is a widely useful method because, most of the time,
    the first few rows of a CSV data file are metadata about the data source or similar
    information, which is not read into the table:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 跳过初始行是一种广泛使用的方法，因为大多数情况下，CSV 数据文件的前几行是关于数据源或类似信息的元数据，这些信息不会被读入表格：
- en: '![Figure 5.9: Contents of the CSV file](img/C11065_05_09.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9：CSV 文件的内容](img/C11065_05_09.jpg)'
- en: 'Figure 5.9: Contents of the CSV file'
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.9：CSV 文件的内容
- en: Note
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The first two lines in the CSV file are irrelevant data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: CSV 文件的前两行是不相关的数据。
- en: 'Read the CSV file and examine the results:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取 CSV 文件并检查结果：
- en: '[PRE9]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.10: DataFrame with an unexpected error](img/C11065_05_10.jpg)'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.10：带有意外错误的 DataFrame](img/C11065_05_10.jpg)'
- en: 'Figure 5.10: DataFrame with an unexpected error'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.10：带有意外错误的 DataFrame
- en: 'Skip the first two rows and read the file:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跳过前两行并读取文件：
- en: '[PRE10]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.11: Expected DataFrame after skipping two rows](img/C11065_05_11.jpg)'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.11：跳过两行后的预期 DataFrame](img/C11065_05_11.jpg)'
- en: 'Figure 5.11: Expected DataFrame after skipping two rows'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.11：跳过两行后的预期 DataFrame
- en: 'Similar to skipping the initial rows, it may be necessary to skip the footer
    of a file. For example, we do not want to read the data at the end of the following
    file:![Figure 5.12: Contents of the CSV file](img/C11065_05_12.jpg)'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与跳过初始行类似，可能需要跳过文件的页脚。例如，我们不想读取以下文件末尾的数据：![图 5.12：CSV 文件的内容](img/C11065_05_12.jpg)
- en: 'Figure 5.12: Contents of the CSV file'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.12：CSV 文件的内容
- en: We have to use `skipfooter` and the `engine='python'` option to enable this.
    There are two engines for these CSV reader functions – based on C or Python, of
    which only the Python engine supports the `skipfooter` option.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们必须使用 `skipfooter` 和 `engine='python'` 选项来启用此功能。这些 CSV 读取函数有两个引擎——基于 C 或 Python，其中只有
    Python 引擎支持 `skipfooter` 选项。
- en: 'Use the `skipfooter` option in Python:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Python 中使用 `skipfooter` 选项：
- en: '[PRE11]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is as follows:'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.13: DataFrame without a footer](img/C11065_05_13.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.13：没有页脚的 DataFrame](img/C11065_05_13.jpg)'
- en: 'Figure 5.13: DataFrame without a footer'
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.13：没有页脚的 DataFrame
- en: Reading Only the First N Rows (Especially Useful for Large Files)
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 只读取前 N 行（特别是对于大文件非常有用）
- en: In many situations, we may not want to read a whole data file but only the first
    few rows. This is particularly useful for extremely large data files, where we
    may just want to read the first couple of hundred rows to check an initial pattern
    and then decide to read the whole data later on. Reading the entire file can take
    a long time and slow down the entire data wrangling pipeline.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，我们可能不想读取整个数据文件，而只想读取前几行。这对于非常大的数据文件尤其有用，我们可能只想读取前几百行来检查初始模式，然后决定稍后读取整个数据。读取整个文件可能需要很长时间，并减慢整个数据处理流程。
- en: 'A simple option, called `nrows`, in the `read_csv` function enables us to do
    just that:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_csv` 函数中的一个简单选项，称为 `nrows`，使我们能够做到这一点：'
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/C11065_05_14.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C11065_05_14.jpg)'
- en: 'Figure 5.14: DataFrame with the first few rows of the CSV file'
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.14：CSV 文件的前几行 DataFrame
- en: 'Exercise 64: Combining Skiprows and Nrows to Read Data in Small Chunks'
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 64：结合 Skiprows 和 Nrows 以小块读取数据
- en: 'Continuing our discussion about reading a very large data file, we can cleverly
    combine `skiprows` and `nrows` to read in such a large file in smaller chunks
    of pre-determined sizes. The following code demonstrates just that:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续讨论读取非常大的数据文件时，我们可以巧妙地结合 `skiprows` 和 `nrows` 来以预定的较小块读取这样的大文件。以下代码演示了这一点：
- en: 'Create a list where DataFrames will be stored:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个列表来存储 DataFrames：
- en: '[PRE13]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Store the number of rows to be read into a variable:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将要读取的行数存储到变量中：
- en: '[PRE14]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Create a variable to store the number of chunks to be read:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个变量来存储要读取的块数：
- en: '[PRE15]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Create a dummy DataFrame to get the column names:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个虚拟的 DataFrame 以获取列名：
- en: '[PRE16]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Loop over the CSV file to read only a fixed number of rows at a time:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历 CSV 文件以一次读取固定数量的行：
- en: '[PRE17]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note how the `iterator` variable is set up inside the `range` function to break
    it into chunks. Say the number of chunks is 5 and the rows per chunk is 10\. Then,
    the iterator will have a range of (0,5*10,10), where the final 10 is step-size,
    that is, it will iterate with indices of (0,9,19,29,39,49).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `iterator` 变量如何在 `range` 函数内部设置以将其分成块。假设块的数量是 5，每个块中的行数是 10。那么，迭代器将有一个范围
    (0,5*10,10)，其中最后的 10 是步长，即它将以索引 (0,9,19,29,39,49) 迭代。
- en: Setting the skip_blank_lines Option
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置 skip_blank_lines 选项
- en: 'By default, `read_csv` ignores blank lines. But sometimes, you may want to
    read them in as NaN so that you can count how many such blank entries were present
    in the raw data file. In some situations, this is an indicator of the default
    data streaming quality and consistency. For this, you have to disable the `skip_blank_lines`
    option:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`read_csv` 会忽略空白行。但有时，您可能希望将它们读取为 NaN，以便您可以计算原始数据文件中存在多少这样的空白条目。在某些情况下，这是默认数据流质量一致性的指标。为此，您必须禁用
    `skip_blank_lines` 选项：
- en: '[PRE18]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.15: DataFrame that has blank rows of a .csv file](img/C11065_05_15.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.15：具有空白行的 .csv 文件的 DataFrame](img/C11065_05_15.jpg)'
- en: 'Figure 5.15: DataFrame that has blank rows of a .csv file'
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.15：具有空白行的 .csv 文件的 DataFrame
- en: Read CSV from a Zip file
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从 Zip 文件中读取 CSV
- en: This is an awesome feature of pandas, in that it allows you to read directly
    from a compressed file such as `.zip`, `.gz`, `.bz2`, or `.xz`. The only requirement
    is that the intended data file (CSV) should be the only file inside the compressed
    file.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这是以 pandas 为例的一个很棒的功能，因为它允许您直接从压缩文件（如 `.zip`、`.gz`、`.bz2` 或 `.xz`）读取。唯一的要求是，目标数据文件（CSV）应该是压缩文件中唯一的文件。
- en: 'In this example, we compressed the example CSV file with a 7-Zip program and
    read from it directly using the `read_csv` method:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用 7-Zip 程序压缩了示例 CSV 文件，并直接使用 `read_csv` 方法读取：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output is as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/C11065_05_16.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/C11065_05_16.jpg)'
- en: 'Figure 5.16: DataFrame of a compressed CSV'
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.16：压缩 CSV 的 DataFrame
- en: Reading from an Excel File Using sheet_name and Handling a Distinct sheet_name
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 sheet_name 读取 Excel 文件并处理不同的 sheet_name
- en: Next, we will turn our attention to a Microsoft Excel file. It turns out that
    most of the options and methods we learned about in the previous exercises with
    the CSV file apply directly to the reading of Excel files too. Therefore, we will
    not repeat them here. Instead, we will focus on their differences. An Excel file
    can consist of multiple worksheets and we can read a specific sheet by passing
    in a particular argument, that is, `sheet_name`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将关注 Microsoft Excel 文件。事实证明，我们在之前的 CSV 文件练习中学习的许多选项和方法也直接适用于 Excel 文件的读取。因此，我们在这里不会重复它们。相反，我们将关注它们之间的差异。Excel
    文件可以由多个工作表组成，我们可以通过传递特定的参数来读取特定的工作表，即 `sheet_name`。
- en: 'For example, in the associated data file, `Housing_data.xlsx`, we have three
    tabs, and the following code reads them one by one in three separate DataFrames:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在相关的数据文件 `Housing_data.xlsx` 中，我们有三个标签页，以下代码将它们逐个读取到三个单独的 DataFrame 中：
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If the Excel file has multiple distinct sheets but the `sheet_name` argument
    is set to `None`, then an ordered dictionary will be returned by the `read_excel`
    function. Thereafter, we can simply iterate over that dictionary or its keys to
    retrieve individual DataFrames.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Excel 文件有多个不同的工作表，但 `sheet_name` 参数设置为 `None`，那么 `read_excel` 函数将返回一个有序字典。之后，我们可以简单地遍历该字典或其键来检索单个
    DataFrame。
- en: 'Let''s consider the following example:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下示例：
- en: '[PRE21]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE22]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Exercise 65: Reading a General Delimited Text File'
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 65：读取通用分隔文本文件
- en: 'General text files can be read as easily as we read CSV files. However, you
    have to pass on the proper separator if it is anything other than a whitespace
    or a tab:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通用文本文件可以像读取 CSV 文件一样轻松读取。然而，如果您使用的是除空格或制表符之外的分隔符，您必须传递正确的分隔符：
- en: 'A comma-separated file, saved with the `.txt` extension, will result in the
    following DataFrame if read without explicitly setting the separator:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以逗号分隔的文件，保存为 `.txt` 扩展名，如果未显式设置分隔符读取，将生成以下 DataFrame：
- en: '[PRE23]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is as follows:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/C11065_05_17.jpg)'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](img/C11065_05_17.jpg)'
- en: 'Figure 5.17: DataFrame that has a comma-separated CSV file'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.17：具有逗号分隔的 CSV 文件的 DataFrame
- en: 'In this case, we have to set the separator explicitly, as follows:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种情况下，我们必须显式设置分隔符，如下所示：
- en: '[PRE24]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output is follows:'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.18: DataFrame read using comma seperator](img/C11065_05_18.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.18：使用逗号分隔符读取的 DataFrame](img/C11065_05_18.jpg)'
- en: 'Figure 5.18: DataFrame read using a comma separator'
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.18：使用逗号分隔符读取的 DataFrame
- en: Reading HTML Tables Directly from a URL
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 直接从 URL 读取 HTML 表格
- en: The pandas library allows us to read HTML tables directly from a URL. This means
    that they already have some kind of built-in HTML parser that processes the HTML
    content of a given page and tries to extract various tables in the page.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 库允许我们直接从 URL 读取 HTML 表格。这意味着它们已经内置了一些 HTML 解析器，可以处理给定页面的 HTML 内容，并尝试提取页面中的各种表格。
- en: Note
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The `read_html` method returns a list of DataFrames (even if the page has a
    single DataFrame) and you have to extract the relevant tables from the list.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_html` 方法返回一个 DataFrame 列表（即使页面只有一个 DataFrame），你必须从列表中提取相关的表格。'
- en: 'Consider the following example:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下示例：
- en: '[PRE25]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'These results are shown in the following DataFrame:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果如下所示：
- en: '![Figure 5.19: Results of reading HTML tables](img/C11065_05_19.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.19：读取 HTML 表格的结果](img/C11065_05_19.jpg)'
- en: 'Figure 5.19: Results of reading HTML tables'
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.19：读取 HTML 表格的结果
- en: 'Exercise 66: Further Wrangling to Get the Desired Data'
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 66：进一步整理以获取所需数据
- en: 'As discussed in the preceding exercise, this HTML-reading function almost always
    returns more than one table for a given HTML page and we have to further parse
    through the list to extract the particular table we are interested in:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述的练习中讨论的，这个 HTML 读取函数几乎总是为给定的 HTML 页面返回多个表格，我们必须进一步解析列表以提取我们感兴趣的特定表格：
- en: 'For example, if we want to get the table of the 2016 summer Olympics medal
    tally (by nation), we can easily search to get a page on Wikipedia that we can
    pass on to pandas. We can do this by using the following command:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，如果我们想获取 2016 年夏季奥运会奖牌榜（按国家）的表格，我们可以轻松地搜索到一个页面，并将其传递给 Pandas。我们可以通过以下命令来完成：
- en: '[PRE26]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'If we check the length of the list returned, we will see it is 6:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们检查返回列表的长度，我们会看到它是 6：
- en: '[PRE27]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is as follows:'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE28]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'To look for the table, we can run a simple loop:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了查找表格，我们可以运行一个简单的循环：
- en: '[PRE29]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output is as follows:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/C11065_05_20.jpg)'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C11065_05_20.jpg)'
- en: 'Figure 5.20: Shape of the tables'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.20：表格的形状
- en: 'It looks like the second element in this list is the table we are looking for:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看起来这个列表中的第二个元素就是我们正在寻找的表格：
- en: '[PRE30]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output is as follows:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.21: Output of the data in the second table](img/C11065_05_21.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.21：第二张表格中的数据输出](img/C11065_05_21.jpg)'
- en: 'Figure 5.21: Output of the data in the second table'
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.21：第二张表格中的数据输出
- en: 'Exercise 67: Reading from a JSON File'
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 67：从 JSON 文件中读取
- en: Over the last 15 years, JSON has become a ubiquitous choice for data exchange
    on the web. Today, it is the format of choice for almost every publicly available
    web API, and it is frequently used for private web APIs as well. It is a schema-less,
    text-based representation of structured data that is based on key-value pairs
    and ordered lists.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的 15 年里，JSON 已经成为网络数据交换的通用选择。如今，它几乎成为所有公开可用的网络 API 的首选格式，同时也常用于私有网络 API。它是一种基于键值对和有序列表的无模式、基于文本的结构化数据表示。
- en: 'The pandas library provides excellent support for reading data from a JSON
    file directly into a DataFrame. To practice with this chapter, we have included
    a file called `movies.json`. This file contains the cast, genre, title, and year
    (of release) information for almost all major movies since 1900:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 库提供了将数据直接从 JSON 文件读取到 DataFrame 中的出色支持。为了练习本章内容，我们包含了一个名为 `movies.json`
    的文件。该文件包含自 1900 年以来几乎所有主要电影的演员、类型、标题和发行年份信息：
- en: 'Extract the cast list for the 2012 Avengers movie (from Marvel comics):'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取 2012 年复仇者联盟电影的演员列表（来自漫威漫画）：
- en: '[PRE31]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.22: DataFrame displaying Avengers movie cast](img/C11065_05_22.jpg)'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.22：显示复仇者联盟电影演员的 DataFrame](img/C11065_05_22.jpg)'
- en: 'Figure 5.22: DataFrame displaying the Avengers movie cast'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.22：显示复仇者联盟电影演员的 DataFrame
- en: 'To look for the cast where the title is "Avengers", we can use filtering:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了查找标题为 "复仇者联盟" 的演员列表，我们可以使用过滤功能：
- en: '[PRE32]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output will be as follows:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果将如下所示：
- en: '[PRE33]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Reading a Stata File
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 读取 Stata 文件
- en: The pandas library provides a direct reading function for Stata files, too.
    Stata is a popular statistical modeling platform that's used in many governmental
    and research organizations, especially by economists and social scientists.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 库还提供了直接读取 Stata 文件的函数。Stata 是一个流行的统计建模平台，被许多政府机构和研究组织使用，尤其是经济学家和社会科学家。
- en: 'The simple code to read in a Stata file (`.dta` format) is as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 读取 Stata 文件（`.dta` 格式）的简单代码如下：
- en: '[PRE34]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Exercise 68: Reading Tabular Data from a PDF File'
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 68：从 PDF 文件中读取表格数据
- en: Among the various types of data sources, the PDF format is probably the most
    difficult to parse in general. While there are some popular packages in Python
    for working with PDF files for general page formatting, the best library to use
    for table extraction from PDF files is `tabula-py`.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种数据源类型中，PDF 格式可能是最难以解析的。虽然有一些流行的 Python 包用于处理 PDF 文件的一般页面格式，但用于从 PDF 文件中提取表格的最佳库是
    `tabula-py`。
- en: From the GitHub page of this package, `tabula-py` is a simple Python wrapper
    of `tabula-java`, which can read a table from a PDF. You can read tables from
    PDFs and convert them into pandas DataFrames. The `tabula-py` library also enables
    you to convert a PDF file into a CSV/TSV/JSON file.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个包的 GitHub 页面来看，`tabula-py` 是 `tabula-java` 的简单 Python 封装，可以从 PDF 中读取表格。您可以从
    PDF 中读取表格并将它们转换为 pandas DataFrame。`tabula-py` 库还允许您将 PDF 文件转换为 CSV/TSV/JSON 文件。
- en: 'You will need the following packages installed on your system before you can
    run this, but they are free and easy to install:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在您运行此代码之前，需要在您的系统上安装以下包，但它们是免费的且易于安装：
- en: urllib3
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: urllib3
- en: pandas
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas
- en: pytest
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pytest
- en: flake8
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: flake8
- en: distro
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: distro
- en: pathlib
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pathlib
- en: 'Find the PDF file in the following link: https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Chapter05/Exercise60-68/Housing_data.xlsx.
    The following code retrieves the tables from two pages and joins them to make
    one table:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下链接中找到 PDF 文件：https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Chapter05/Exercise60-68/Housing_data.xlsx。以下代码从两页中检索表格并将它们连接成一个表格：
- en: '[PRE35]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output is as follows:'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.23: DataFrame with a table derived by merging a table flowing over
    two pages in a PDF](img/C11065_05_23.jpg)'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.23：通过合并 PDF 中跨越两页的表格得到的 DataFrame](img/C11065_05_23.jpg)'
- en: 'Figure 5.23: DataFrame with a table derived by merging a table flowing over
    two pages in a PDF'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.23：通过合并 PDF 中跨越两页的表格得到的 DataFrame
- en: 'Retrieve the table from another page of the same PDF by using the following
    command:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从同一 PDF 的另一页检索表格：
- en: '[PRE36]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output is as follows:'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.24: DataFrame displaying a table from another page](img/C11065_05_24.jpg)'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.24：显示来自另一页的表格的 DataFrame](img/C11065_05_24.jpg)'
- en: 'Figure 5.24: DataFrame displaying a table from another page'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.24：显示来自另一页的表格的 DataFrame
- en: 'To concatenate the tables that were derived from the first two steps, execute
    the following code:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要连接从前两个步骤中得到的表格，请执行以下代码：
- en: '[PRE37]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.25: DataFrame derived by concatenating two tables](img/C11065_05_25.jpg)'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.25：通过连接两个表格得到的 DataFrame](img/C11065_05_25.jpg)'
- en: 'Figure 5.25: DataFrame derived by concatenating two tables'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.25：通过连接两个表格得到的 DataFrame
- en: 'With PDF extraction, most of the time, headers will be difficult to extract
    automatically. You have to pass on the list of headers with the `names` argument
    in the `read-pdf` function as `pandas_option`, as follows:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 PDF 提取过程中，大多数情况下，标题将难以自动提取。您必须通过 `read-pdf` 函数中的 `names` 参数作为 `pandas_option`
    传递标题列表，如下所示：
- en: '[PRE38]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is as follows:'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.26: DataFrame with correct column headers for PDF data](img/C11065_05_26.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.26：具有正确列标题的 PDF 数据 DataFrame](img/C11065_05_26.jpg)'
- en: 'Figure 5.26: DataFrame with correct column headers for PDF data'
  id: totrans-220
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.26：具有正确列标题的 PDF 数据 DataFrame
- en: We will have a full activity on reading tables from a PDF report and processing
    them at the end of this chapter.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章末尾，我们将进行一个完整的活动，阅读 PDF 报告中的表格并进行处理。
- en: Introduction to Beautiful Soup 4 and Web Page Parsing
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Beautiful Soup 4 和网页解析简介
- en: The ability to read and understand web pages is one of paramount interest for
    a person collecting and formatting data. For example, consider the task of gathering
    data about movies and then formatting it for a downstream system. Data for the
    movies is best obtained by the websites such as IMDB and that data does not come
    pre-packaged in nice forms(CSV, JSON< and so on), so you need to know how to download
    and read web page.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 读取和理解网页的能力对于收集和格式化数据的人来说是至关重要的。例如，考虑收集有关电影数据并将其格式化以供下游系统使用的任务。电影数据最好通过像IMDB这样的网站获得，而这些数据并不是以预包装的格式（CSV、JSON等）提供的，因此你需要知道如何下载和读取网页。
- en: Furthermore, you also need to be equipped with the knowledge of the structure
    of a web page so that you can design a system that can search for (query) a particular
    piece of information from a whole web page and get the value of it. This involves
    understanding the grammar of markup languages and being able to write something
    that can parse them. Doing this, and keeping all the edge cases in mind, for something
    like HTML is already incredibly complex, and if you extend the scope of the bespoke
    markup language to include XML as well, then it becomes full-time work for a team
    of people.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还需要具备网页结构的知识，这样你才能设计一个系统，可以从整个网页中搜索（查询）特定信息并获取其值。这涉及到理解标记语言语法，并能够编写可以解析它们的代码。做这件事，同时考虑到所有边缘情况，对于像HTML这样的东西已经非常复杂，如果你将定制标记语言的范围扩展到包括XML，那么这将成为一个团队的全职工作。
- en: Thankfully, we are using Python, and Python has a very mature and stable library
    to do all of the complicated jobs for us. This library is called `BeautifulSoup`
    (it is, at present, in version 4 and thus we will call it `bs4` in short from
    now on). `bs4` is a library for getting data from HTML or XML documents, and it
    gives you a nice, normalized, idiomatic way of navigating and querying a document.
    It does not include a parser but it supports different ones.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们正在使用Python，Python有一个非常成熟和稳定的库来为我们完成所有复杂的任务。这个库叫做`BeautifulSoup`（目前处于第4版，因此从现在起我们将简称为`bs4`）。`bs4`是一个从HTML或XML文档中获取数据的库，它为你提供了一种优雅、规范、惯用的方式来导航和查询文档。它不包含解析器，但它支持不同的解析器。
- en: Structure of HTML
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: HTML结构
- en: 'Before we jump into `bs4` and start working with it, we need to examine the
    structure of a HTML document. **H**yper **T**ext **M**arkup **L**anguage is a
    structured way of telling web browsers about the organization of a web page, meaning
    which kind of elements (text, image, video, and so on) come from where, in which
    place inside the page they should appear, what they look like, what they contain,
    and how they will behave with user input. HTML5 is the latest version of HTML.
    An HTML document can be viewed as a tree, as we can see from the following diagram:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入研究`bs4`并开始使用它之前，我们需要检查HTML文档的结构。**超**文**字**标**记**语**言是一种向网页浏览器告知网页组织结构的方式，意味着哪种类型的元素（文本、图像、视频等）来自哪里，它们在页面内部的哪个位置出现，它们的样式是什么，它们包含什么，以及它们如何响应用户输入。HTML5是HTML的最新版本。一个HTML文档可以被视为一棵树，正如我们可以在以下图中看到的那样：
- en: '![Figure 5.27: HTML structure](img/C11065_05_27.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图5.27：HTML结构](img/C11065_05_27.jpg)'
- en: 'Figure 5.27: HTML structure'
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.27：HTML结构
- en: Each node of the tree represents one element in the document. An element is
    anything that starts with `<` and ends with `>`. For example, `<html>`, `<head>`,
    `<p>`, `<br>`, `<img>`, and so on are various HTML elements. Some elements have
    a start and end element, where the end element begins with "</" and has the same
    name as the start element, such as `<p>` and `</p>`, and they can contain an arbitrary
    number of elements of other types in them. Some elements do not have an ending
    part, such as the `<br />` element, and they cannot contain anything within them.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 树中的每个节点代表文档中的一个元素。元素是以`<`开头并以`>`结尾的任何东西。例如，`<html>`、`<head>`、`<p>`、`<br>`、`<img>`等等都是各种HTML元素。一些元素有起始和结束元素，其中结束元素以`</`开头，并且与起始元素具有相同的名称，例如`<p>`和`</p>`，它们可以包含任意数量的其他类型的元素。一些元素没有结束部分，例如`<br
    />`元素，它们不能包含任何内容。
- en: 'The only other thing that we need to know about an element at this point is
    the fact that elements can have attributes, which are there to modify the default
    behavior of an element. An `<a>` element requires a `href` attribute to tell the
    browser which website it should navigate to when that particular `<a>` is clicked,
    like this: `<a href="http://cnn.com">`. The CNN news channel, `</a>`, will take
    you to cnn.com when clicked:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们还需要了解关于元素的其他唯一事实是，元素可以有属性，这些属性用于修改元素的默认行为。一个 `<a>` 元素需要一个 `href` 属性来告诉浏览器当点击特定的
    `<a>` 时应该导航到哪个网站，例如：`<a href="http://cnn.com">`。点击 CNN 新闻频道 `<a>` 将带你到 cnn.com：
- en: '![Figure 5.28: CNN news channel hyperlink](img/C11065_05_28.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.28：CNN 新闻频道超链接](img/C11065_05_28.jpg)'
- en: 'Figure 5.28: CNN news channel hyperlink'
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.28：CNN 新闻频道超链接
- en: So, when you are at a particular element of the tree, you can visit all the
    children of that element to get the contents and attributes of them.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当你处于树中的特定元素时，你可以访问该元素的子元素以获取它们的内容和属性。
- en: Equipped with this knowledge, let's see how we can read and query data from
    a HTML document.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 带着这些知识，让我们看看我们如何从 HTML 文档中读取和查询数据。
- en: In this topic, we will cover the reading and parsing of web pages, but we do
    not request them from a live website. Instead, we read them from disk. A section
    on reading them from the internet will follow in a future chapter.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个主题中，我们将介绍网页的读取和解析，但我们不会从实时网站请求它们。相反，我们从磁盘读取它们。关于从互联网读取它们的章节将在未来的章节中介绍。
- en: 'Exercise 69: Reading an HTML file and Extracting its Contents Using BeautifulSoup'
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 69：使用 BeautifulSoup 读取 HTML 文件并提取其内容
- en: 'In this exercise, we will do the simplest thing possible. We will import the
    `BeautifulSoup` library and then use it to read an HTML document. Then, we will
    examine the different kinds of objects it returns. While doing the exercises for
    this topic, you should have the example HTML file open in a text editor all the
    time so that you can check for the different tags and their attributes and contents:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将做最简单的事情。我们将导入 `BeautifulSoup` 库，然后使用它来读取一个 HTML 文档。然后，我们将检查它返回的不同类型的对象。在练习这个主题时，你应该始终在文本编辑器中打开示例
    HTML 文件，以便你可以检查不同的标签及其属性和内容：
- en: 'Import the `bs4` library:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `bs4` 库：
- en: '[PRE39]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Please download the following test HTML file and save it on your disk and the
    use bs4 to read it from the disk:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请下载以下测试 HTML 文件并将其保存到您的磁盘上，然后使用 bs4 从磁盘读取它：
- en: '[PRE40]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output is as follows:'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE41]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: You can pass a file handler directly to the constructor of the `BeautifulSoup`
    object and it will read the contents from the file that the handler is attached
    to. We will see that return-type is an instance of `bs4.BeautifulSoup`. This class
    holds all the methods we need to navigate through the DOM tree that the document
    represents.
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以直接将文件句柄传递给 `BeautifulSoup` 对象的构造函数，它将从句柄附加的文件中读取内容。我们将看到返回类型是 `bs4.BeautifulSoup`
    的一个实例。这个类包含了我们需要导航文档表示的 DOM 树的所有方法。
- en: 'Print the contents of the file in a nice way by using the `prettify` method
    from the class like this:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用类中的 `prettify` 方法以这种方式打印文件的漂亮内容：
- en: '[PRE42]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output is as follows:'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.29: Contents of the HTML file](img/C11065_05_29.jpg)'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.29：HTML 文件内容](img/C11065_05_29.jpg)'
- en: 'Figure 5.29: Contents of the HTML file'
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.29：HTML 文件内容
- en: 'The same information can also be obtained by using the `soup.contents` member
    variable. The differences are: first, it won''t print anything pretty and, second,
    it is essentially a list.'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 同样的信息也可以通过使用 `soup.contents` 成员变量来获取。区别在于：首先，它不会打印出任何美观的内容，其次，它本质上是一个列表。
- en: If we look carefully at the contents of the HTML file in a separate text editor,
    we will see that there are many paragraph tags, or `<p>` tags. Let's read content
    from one such `<p>` tag. We can do that using the simple `.` access modifier as
    we would have done for a normal member variable of a class.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果我们仔细查看单独的文本编辑器中的 HTML 文件内容，我们会看到有许多段落标签，或 `<p>` 标签。让我们从一个这样的 `<p>` 标签中读取内容。我们可以使用简单的
    `.` 访问修饰符来完成，就像我们会对类的普通成员变量做的那样。
- en: 'The magic of `bs4` is the fact that it gives us this excellent way to dereference
    tags as member variables of the `BeautifulSoup` class instance:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`bs4` 的魔力在于它给我们提供了这样一种优秀的方式来取消引用标签，作为 `BeautifulSoup` 类实例的成员变量：'
- en: '[PRE43]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output is as follows:'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.30: Text from the <p> tag ](img/C11065_05_30.jpg)'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.30：<p> 标签中的文本](img/C11065_05_30.jpg)'
- en: 'Figure 5.30: Text from the <p> tag'
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.30：<p> 标签中的文本
- en: As we can see, this is the content of a `<p>` tag.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如我们所见，这是 `<p>` 标签的内容。
- en: We saw how to read a tag in the last exercise, but we can easily see the problem
    with this approach. When we look into our HTML document, we can see that we have
    more than one `<p>` tag there. How can we access all the `<p>` tags? It turns
    out that this is easy.
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在上一个练习中，我们看到了如何读取一个标签，但我们可以很容易地看到这种方法的问题。当我们查看我们的 HTML 文档时，我们可以看到那里有多个 `<p>`
    标签。我们如何访问所有的 `<p>` 标签？结果证明这是很容易的。
- en: 'Use the `findall` method to extract the content from the tag:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `findall` 方法从标签中提取内容：
- en: '[PRE44]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is as follows:'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE45]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This will print 6, which is exactly the number of `<p>`tags in the document.
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印出 6，这正是文档中 `<p>` 标签的数量。
- en: We have seen how to access all the tags of the same type. We have also seen
    how to get the content of the entire HTML document.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们已经看到了如何访问相同类型的所有标签。我们也看到了如何获取整个 HTML 文档的内容。
- en: 'Now, we will see how to get the contents under a particular HTML tag, as follows:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将看到如何获取特定 HTML 标签下的内容，如下所示：
- en: '[PRE46]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output is as follows:'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.31: Content under the <table> tag ](img/C11065_05_31.jpg)'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.31：`<table>` 标签下的内容](img/C11065_05_31.jpg)'
- en: 'Figure 5.31: Content under the <table> tag'
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.31：`<table>` 标签下的内容
- en: Here, we are getting the (first) table from the document and then using the
    same "`.`" notation, to get the contents under that tag.
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们正在从文档中获取（第一个）表格，然后使用相同的 "`.`" 表示法，来获取该标签下的内容。
- en: We saw in the previous exercise that we can access the entire content under
    a particular tag. However, HTML is represented as a tree and we are able to traverse
    the children of a particular node. There are a few ways to do this.
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在上一个练习中，我们看到了如何访问特定标签下的全部内容。然而，HTML 被表示为树，我们能够遍历特定节点的子标签。有几种方法可以做到这一点。
- en: 'The first way is by using the `children` generator from any `bs4` instance,
    as follows:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一种方法是通过使用任何 `bs4` 实例的 `children` 生成器，如下所示：
- en: '[PRE47]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'When we execute the code, we will see something like the following:'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们执行代码时，我们将看到如下所示的内容：
- en: '![Figure 5.32: Traversing children of a table node](img/C11065_05_32.jpg)'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.32：遍历表格节点的子标签](img/C11065_05_32.jpg)'
- en: 'Figure 5.32: Traversing the children of a table node'
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.32：遍历表格节点的子标签
- en: It seems that the loop has only been executed twice! Well, the problem with
    the "`children`" generator is that it only takes into account the immediate children
    of the tag. We have `<tbody>` under the `<table>` and our whole table structure
    is wrapped in it. That's why it was considered a single child of the `<table>`
    tag.
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 看起来循环只执行了两次！嗯，`"children"` 生成器的问题在于它只考虑了标签的直接子标签。我们在 `<table>` 下有 `<tbody>`，并且整个表格结构都被它包裹着。这就是为什么它被视为
    `<table>` 标签的单个子标签。
- en: We looked into how to browse the immediate children of a tag. We will see how
    we can browse all the possible children of a tag and not only the immediate one.
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们探讨了如何浏览一个标签的直接子标签。我们将看到如何浏览一个标签的所有可能子标签，而不仅仅是直接子标签。
- en: 'To do that, we use the `descendants` generator from the `bs4` instance, as
    follows:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们使用 `bs4` 实例中的 `descendants` 生成器，如下所示：
- en: '[PRE48]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output is as follows:'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE49]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The comparison print at the end of the code block will show us the difference
    between `children` and `descendants`. The length of the list we got from `children`
    is only 9, whereas the length of the list we got from `descendants` is 61.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块末尾的比较打印将显示 `children` 和 `descendants` 之间的差异。我们从 `children` 获取的列表长度仅为 9，而从
    `descendants` 获取的列表长度为 61。
- en: 'Exercise 70: DataFrames and BeautifulSoup'
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 70：DataFrames 和 BeautifulSoup
- en: 'So far, we have seen some basic ways to navigate the tags inside a HTML document
    using `bs4`. Now, we are going to go one step further and use the power of `bs4`
    combined with the power of pandas to generate a DataFrame out of a plain HTML
    table. This particular knowledge is very useful for us. With the knowledge we
    will acquire now, it will be fairly easy for us to prepare a pandas DataFrame
    to perform EDA (exploratory data analysis) or modeling. We are going to show this
    process on a simple small table from the test HTML file, but the exact same concept
    applies to any arbitrarily large table as well:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了一些使用 `bs4` 在 HTML 文档内部导航标签的基本方法。现在，我们将更进一步，利用 `bs4` 的力量与 pandas
    的力量相结合，从纯 HTML 表格生成一个 DataFrame。这种特定的知识对我们非常有用。通过我们现在将获得的知识，我们将能够轻松地准备一个 pandas
    DataFrame 来执行 EDA（探索性数据分析）或建模。我们将在测试 HTML 文件的一个简单小表格上展示这个过程，但这个概念同样适用于任何任意大的表格：
- en: 'Import `pandas` and read the document, as follows:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`并按以下方式读取文档：
- en: '[PRE50]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output is as follows:'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE51]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Check the original table structure in the HTML source. You will see that the
    first row is the column headings and all of the following rows are the data. We
    assign two different variables for the two sections, as follows:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查HTML源中的原始表格结构。你会看到第一行是列标题，所有后续的行都是数据。我们为两个部分分配了两个不同的变量，如下所示：
- en: '[PRE52]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output is as follows:'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE53]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Note
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Keep in mind that the art of scraping a HTML page goes hand in hand with an
    understanding of the source HTML structure. So, whenever you want to scrape a
    page, the first thing you need to do is right-click on it and then use "View Source"
    from the browser to see the source HTML.
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请记住，抓取HTML页面的艺术与理解源HTML结构密不可分。因此，每次你想抓取一个页面时，你首先需要做的就是右键点击它，然后从浏览器中使用“查看源代码”来查看源HTML。
- en: 'Once we have separated the two sections, we need two list comprehensions to
    make them ready to go in a DataFrame. For the header, this is easy:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们分离了这两个部分，我们需要两个列表推导式来使它们准备好进入DataFrame。对于标题，这很简单：
- en: '[PRE54]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output is as follows:'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE55]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Data preparation is a bit tricky for a pandas DataFrame. You need to have a
    two-dimensional list, which is a list of lists. We accomplish that in the following
    way:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于pandas DataFrame的数据准备有点棘手。你需要有一个二维列表，即列表的列表。我们以下这种方式完成：
- en: '[PRE56]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output is as follows:'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/C11065_05_33.jpg)'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](img/C11065_05_33.jpg)'
- en: 'Figure 5.33: Output as a two-dimensional list'
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.33：以二维列表的形式输出
- en: 'Invoke the `pd.DataFrame` method and supply the right arguments by using the
    following code:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`pd.DataFrame`方法，并使用以下代码提供正确的参数：
- en: '[PRE57]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '![](img/C11065_05_34.jpg)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/C11065_05_34.jpg)'
- en: 'Figure 5.34: Output in tabular format with column headers'
  id: totrans-309
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.34：带有列标题的表格格式输出
- en: 'Exercise 71: Exporting a DataFrame as an Excel File'
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习71：将DataFrame导出为Excel文件
- en: 'In this exercise, we will see how we can save a DataFrame as an Excel file.
    Pandas can natively do this, but it needs the help of the `openpyxl` library to
    achieve this goal:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将看到如何将DataFrame保存为Excel文件。Pandas可以原生地做到这一点，但需要`openpyxl`库的帮助来实现这个目标：
- en: 'Install the `openpyxl` library by using the following command:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装`openpyxl`库：
- en: '[PRE58]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'To save the DataFrame as an Excel file, use the following command from inside
    of the Jupyter notebook:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要将DataFrame保存为Excel文件，请在Jupyter笔记本中使用以下命令：
- en: '[PRE59]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output is as follows:'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE60]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Exercise 72: Stacking URLs from a Document using bs4'
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习72：使用bs4从文档中堆叠URL
- en: Previously (while discussing stack), we explained how important it is to have
    a stack that we can push the URLs from a web page to so that we can pop them at
    a later time to follow each of them. Here, in this exercise, we will see how that
    works.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前（讨论堆栈时），我们解释了拥有一个堆栈的重要性，我们可以将网页中的URL推入堆栈，以便稍后弹出以跟踪每个URL。在这里，在这个练习中，我们将看到它是如何工作的。
- en: 'In the given test, HTML file links or `<a>` tags are under a `<ul>` tag, and
    each of them is contained inside a `</li>` tag:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的测试中，HTML文件链接或`<a>`标签位于`<ul>`标签下，并且每个都包含在`</li>`标签内：
- en: 'Find all the `<a>` tags by using the following command:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令查找所有`<a>`标签：
- en: '[PRE61]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Define a stack before you start the loop. Then, inside the loop, use the `append`
    method to push the links in the stack:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在循环开始之前定义一个堆栈。然后在循环内部，使用`append`方法将链接推入堆栈：
- en: '[PRE62]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Print the stack:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印堆栈：
- en: '![](img/C11065_05_35.jpg)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/C11065_05_35.jpg)'
- en: 'Figure 5.35: Output of the stack'
  id: totrans-327
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.35：堆栈的输出
- en: 'Activity 7: Reading Tabular Data from a Web Page and Creating DataFrames'
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动七：从网页中读取表格数据并创建DataFrame
- en: 'In this activity, you have been given a Wikipedia page where you have the GDP
    of all countries listed. You have been asked to create three `DataFrames` from
    the three sources mentioned in the page ([https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)](https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal))):'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，你已经得到了一个包含所有国家GDP的维基百科页面。你被要求从页面中提到的三个来源创建三个`DataFrame`（[https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)](https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal))）：
- en: 'You will have to do the following:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须做以下事情：
- en: Open the page in a separate Chrome/Firefox tab and use something like an **Inspect
    Element** tool to view the source HTML and understand its structure
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在单独的Chrome/Firefox标签页中打开页面，并使用类似**检查元素**的工具来查看源HTML并理解其结构
- en: Read the page using bs4
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用bs4读取页面
- en: Find the table structure you will need to deal with (how many tables there are?)
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到你需要处理的表格结构（有多少个表格？）
- en: Find the right table using bs4
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用bs4找到正确的表格
- en: Separate the source names and their corresponding data
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将源名称及其对应的数据分开
- en: Get the source names from the list of sources you have created
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从你创建的源列表中获取源名称
- en: Separate the header and data from the data that you separated before for the
    first source only, and then create a DataFrame using that
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅对第一个源将标题和数据从之前分开的数据中分离出来，然后使用这些数据创建一个DataFrame
- en: Repeat the last task for the other two data sources
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对其他两个数据源重复最后一个任务
- en: Note
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 308.
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第308页找到。
- en: Summary
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this topic, we looked at the structure of an HTML document. HTML documents
    are the cornerstone of the World Wide Web and, given the amount of data that's
    contained on it, we can easily infer the importance of HTML as a data source.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个主题中，我们研究了HTML文档的结构。HTML文档是万维网的基础，考虑到其包含的数据量，我们可以轻易推断出HTML作为数据源的重要性。
- en: We learned about bs4 (BeautifulSoup4), a Python library that gives us Pythonic
    ways to read and query HTML documents. We used bs4 to load an HTML document and
    also explored several different ways to navigate the loaded document. We also
    got necessary information about the difference between all of these methods.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了bs4（BeautifulSoup4），这是一个Python库，它为我们提供了以Python方式读取和查询HTML文档的方法。我们使用bs4加载了一个HTML文档，并探索了多种不同的导航加载文档的方法。我们还获得了关于这些方法之间差异的必要信息。
- en: We looked at how we can create a pandas DataFrame from an HTML document (which
    contains a table). Although there are some built-in ways to do this job in pandas,
    they fail as soon as the target table is encoded inside a complex hierarchy of
    elements. So, the knowledge we gathered in this topic by transforming an HTML
    table into a pandas DataFrame in a step-by-step manner is invaluable.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了如何从HTML文档（其中包含一个表格）创建pandas DataFrame。尽管pandas中有一些内置的方法来完成这项工作，但一旦目标表格被编码在一个复杂的元素层次结构中，它们就会失败。因此，我们在本主题中通过逐步将HTML表格转换为pandas
    DataFrame所获得的知识是无价的。
- en: Finally, we looked at how we can create a stack in our code, where we push all
    the URLs that we encounter while reading the HTML file and then use them at a
    later time. In the next chapter, we will discuss list comprehensions, zip, format
    and outlier detection and cleaning.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们探讨了如何在代码中创建一个栈，将我们在读取HTML文件时遇到的全部URL推入栈中，然后在稍后时间使用它们。在下一章中，我们将讨论列表推导式、zip、格式化和异常值检测与清理。
