- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: Natural Language Processing is used everywhere—in search engines, spell checkers,
    mobile phones, computer games, and even in your washing machine. Python's Natural
    Language Toolkit (NLTK) suite of libraries has rapidly emerged as one of the most
    efficient tools for Natural Language Processing. You want to employ nothing less
    than the best techniques in Natural Language Processing—and this book is your
    answer.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）无处不在——在搜索引擎、拼写检查器、移动电话、计算机游戏，甚至是在您的洗衣机中。Python的自然语言工具包（NLTK）套件迅速成为自然语言处理中最有效的工具之一。您希望采用不亚于最佳技术的自然语言处理技术——这本书就是您的答案。
- en: '*Python Text Processing with NLTK 2.0 Cookbook* is your handy and illustrative
    guide, which will walk you through all the Natural Language Processing techniques
    in a step-by-step manner. It will demystify the advanced features of text analysis
    and text mining using the comprehensive NLTK suite.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python Text Processing with NLTK 2.0 Cookbook* 是您的实用指南，它将逐步引导您了解所有自然语言处理技术。它将揭示使用NLTK全面套件进行文本分析和文本挖掘的高级功能。'
- en: This book cuts short the preamble and lets you dive right into the science of
    text processing with a practical hands-on approach.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本书省略了序言部分，让您直接通过实践动手的方式深入探索文本处理科学。
- en: 'Get started off with learning tokenization of text. Receive an overview of
    WordNet and how to use it. Learn the basics as well as advanced features of stemming
    and lemmatization. Discover various ways to replace words with simpler and more
    common (read: more searched) variants. Create your own corpora and learn to create
    custom corpus readers for data stored in MongoDB. Use and manipulate POS taggers.
    Transform and normalize parsed chunks to produce a canonical form without changing
    their meaning. Dig into feature extraction and text classification. Learn how
    to easily handle huge amounts of data without any loss in efficiency or speed.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 从学习文本分词开始。了解WordNet的概述及其使用方法。学习词干提取和词形还原的基本和高级功能。发现用更简单、更常见的（即：更常搜索的）变体替换单词的各种方法。创建您自己的语料库，并学习为存储在MongoDB中的数据创建自定义语料库读取器。使用和操作词性标注器。转换和归一化解析的短语，以产生不改变其含义的规范形式。深入研究特征提取和文本分类。学习如何轻松处理大量数据，而不会损失效率或速度。
- en: This book will teach you all that and beyond, in a hands-on learn-by-doing manner.
    Make yourself an expert in using the NLTK for Natural Language Processing with
    this handy companion.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将教会您所有这些以及更多，以动手实践、边做边学的方式。通过这本实用的伴侣书籍，让自己成为NLTK在自然语言处理（NLP）方面的专家。
- en: What this book covers
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书涵盖内容
- en: '[Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing
    Text and WordNet Basics*, covers the basics of tokenizing text and using WordNet.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[第1章](ch01.html "第1章。文本分词和WordNet基础知识")，*文本分词和WordNet基础知识*，涵盖了文本分词的基本知识和如何使用WordNet。'
- en: '[Chapter 2](ch02.html "Chapter 2. Replacing and Correcting Words"), *Replacing
    and Correcting Words*, discusses various word replacement and correction techniques.
    The recipes cover the gamut of linguistic compression, spelling correction, and
    text normalization.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[第2章](ch02.html "第2章。替换和纠正单词")，*替换和纠正单词*，讨论了各种单词替换和纠正技术。这些配方涵盖了语言压缩、拼写纠正和文本归一化的范围。'
- en: '[Chapter 3](ch03.html "Chapter 3. Creating Custom Corpora"), *Creating Custom
    Corpora*, covers how to use corpus readers and create custom corpora. At the same
    time, it explains how to use the existing corpus data that comes with NLTK.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[第3章](ch03.html "第3章。创建自定义语料库")，*创建自定义语料库*，涵盖了如何使用语料库读取器创建自定义语料库。同时，它还解释了如何使用NLTK附带的存在语料库数据。'
- en: '[Chapter 4](ch04.html "Chapter 4. Part-of-Speech Tagging"), *Part-of-Speech
    Tagging*, explains the process of converting a sentence, in the form of a list
    of words, into a list of tuples. It also explains taggers, which are trainable.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[第4章](ch04.html "第4章。词性标注")，*词性标注*，解释了将句子（以单词列表的形式）转换为元组列表的过程。它还解释了标注器，这些标注器是可以训练的。'
- en: '[Chapter 5](ch05.html "Chapter 5. Extracting Chunks"), *Extracting Chunks*,
    explains the process of extracting short phrases from a part-of-speech tagged
    sentence. It uses Penn Treebank corpus for basic training and testing chunk extraction,
    and the CoNLL 2000 corpus as it has a simpler and more flexible format that supports
    multiple chunk types.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[第5章](ch05.html "第5章。提取短语")，*提取短语*，解释了从词性标注句子中提取短语的流程。它使用宾州树库（Penn Treebank）作为基本训练和测试短语提取的语料库，并使用CoNLL
    2000语料库，因为它具有更简单、更灵活的格式，支持多种短语类型。'
- en: '[Chapter 6](ch06.html "Chapter 6. Transforming Chunks and Trees"), *Transforming
    Chunks and Trees*, shows you how to do various transforms on both chunks and trees.
    The functions detailed in these recipes modify data, as opposed to learning from
    it.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[第6章](ch06.html "第6章. 块和树的转换"), *转换块和树*，展示了如何在块和树上执行各种转换。这些菜谱中详细说明的函数修改数据，而不是从数据中学习。'
- en: '[Chapter 7](ch07.html "Chapter 7. Text Classification"), *Text Classification*,
    describes a way to categorize documents or pieces of text and, by examining the
    word usage in a piece of text, classifiers decide what class label should be assigned
    to it.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[第7章](ch07.html "第7章. 文本分类"), *文本分类*，描述了一种对文档或文本片段进行分类的方法，通过检查文本中的单词使用情况，分类器决定应该将其分配给哪个类别标签。'
- en: '[Chapter 8](ch08.html "Chapter 8. Distributed Processing and Handling Large
    Datasets"), *Distributed Processing and Handling Large Datasets*, discusses how
    to use execnet to do parallel and distributed processing with NLTK. It also explains
    how to use the Redis data structure server/database to store frequency distributions.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[第8章](ch08.html "第8章. 分布式处理和大型数据集处理"), *分布式处理和大型数据集处理*，讨论了如何使用execnet在NLTK中进行并行和分布式处理。它还解释了如何使用Redis数据结构服务器/数据库来存储频率分布。'
- en: '[Chapter 9](ch09.html "Chapter 9. Parsing Specific Data"), *Parsing Specific
    Data*, covers parsing specific kinds of data, focusing primarily on dates, times,
    and HTML.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[第9章](ch09.html "第9章. 解析特定数据"), *解析特定数据*，涵盖了解析特定类型的数据，主要关注日期、时间和HTML。'
- en: '[Appendix](apa.html "Appendix A. Penn Treebank Part-of-Speech Tags"), *Penn
    Treebank Part-of-Speech Tags*, lists a table of all the part-of-speech tags that
    occur in the `treebank` corpus distributed with NLTK.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[附录](apa.html "附录A. Penn Treebank 词性标注"), *Penn Treebank 词性标注*，列出了在NLTK附带`treebank`语料库中出现的所有词性标注。'
- en: What you need for this book
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你需要这本书什么
- en: 'In the course of this book, you will need the following software utilities
    to try out various code examples listed:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的过程中，你需要以下软件工具来尝试各种代码示例：
- en: NLTK
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLTK
- en: MongoDB
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MongoDB
- en: PyMongo
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyMongo
- en: Redis
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redis
- en: redis-py
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: redis-py
- en: execnet
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: execnet
- en: Enchant
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Enchant
- en: PyEnchant
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyEnchant
- en: PyYAML
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyYAML
- en: dateutil
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dateutil
- en: chardet
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: chardet
- en: BeautifulSoup
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BeautifulSoup
- en: lxml
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lxml
- en: SimpleParse
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SimpleParse
- en: mxBase
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mxBase
- en: lockfile
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lockfile
- en: Who this book is for
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这本书的适用对象
- en: This book is for Python programmers who want to quickly get to grips with using
    the NLTK for Natural Language Processing. Familiarity with basic text processing
    concepts is required. Programmers experienced in the NLTK will find it useful.
    Students of linguistics will find it invaluable.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书是为想要快速掌握使用NLTK进行自然语言处理的Python程序员而写的。需要熟悉基本的文本处理概念。NLTK经验丰富的程序员会发现它很有用。语言学专业的学生会发现它非常有价值。
- en: Conventions
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 习惯用法
- en: In this book, you will find a number of styles of text that distinguish between
    different kinds of information. Here are some examples of these styles, and an
    explanation of their meaning.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，你会找到许多不同风格的文本，以区分不同类型的信息。以下是一些这些风格的示例，以及它们含义的解释。
- en: 'Code words in text are shown as follows: "Now we want to split `para` into
    sentences. First we need to import the sentence tokenization function, and then
    we can call it with the paragraph as an argument."'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 文本中的代码词如下所示："现在我们想要将`para`分割成句子。首先我们需要导入句子分词函数，然后我们可以用段落作为参数调用它。"
- en: 'A block of code is set as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一段代码如下设置：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**New terms** and **important words** are shown in bold.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**新术语**和**重要词汇**以粗体显示。'
- en: Note
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Warnings or important notes appear in a box like this.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 警告或重要注意事项以这种方式出现在框中。
- en: Tip
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Tips and tricks appear like this.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士和技巧看起来像这样。
- en: Reader feedback
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读者反馈
- en: Feedback from our readers is always welcome. Let us know what you think about
    this book—what you liked or may have disliked. Reader feedback is important for
    us to develop titles that you really get the most out of.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们始终欢迎读者的反馈。让我们知道你对这本书的看法——你喜欢什么或可能不喜欢什么。读者反馈对我们开发你真正能从中获得最大价值的标题非常重要。
- en: To send us general feedback, simply send an e-mail to `<[feedback@packtpub.com](mailto:feedback@packtpub.com)>`,
    and mention the book title via the subject of your message.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要向我们发送一般反馈，只需发送一封电子邮件到`<[feedback@packtpub.com](mailto:feedback@packtpub.com)>`，并在邮件主题中提及书名。
- en: If there is a book that you need and would like to see us publish, please send
    us a note in the **SUGGEST A TITLE** form on [www.packtpub.com](http://www.packtpub.com)
    or e-mail `<[suggest@packtpub.com](mailto:suggest@packtpub.com)>`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要一本书并且希望我们出版，请通过 [www.packtpub.com](http://www.packtpub.com) 上的 **建议书名**
    表格或发送电子邮件至 `<[suggest@packtpub.com](mailto:suggest@packtpub.com)>` 给我们留言。
- en: If there is a topic that you have expertise in and you are interested in either
    writing or contributing to a book, see our author guide on [www.packtpub.com/authors](http://www.packtpub.com/authors).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在某个领域有专业知识，并且对撰写或为书籍做出贡献感兴趣，请参阅我们在 [www.packtpub.com/authors](http://www.packtpub.com/authors)
    上的作者指南。
- en: Customer support
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户支持
- en: Now that you are the proud owner of a Packt book, we have a number of things
    to help you to get the most from your purchase.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经是 Packt 书籍的骄傲拥有者，我们有一些事情可以帮助您从您的购买中获得最大收益。
- en: Note
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Downloading the example code for this book**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**下载本书的示例代码**'
- en: You can download the example code files for all Packt books you have purchased
    from your account at [http://www.PacktPub.com](http://www.PacktPub.com). If you
    purchased this book elsewhere, you can visit [http://www.PacktPub.com/support](http://www.PacktPub.com/support)
    and register to have the files e-mailed directly to you.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从您在 [http://www.PacktPub.com](http://www.PacktPub.com) 的账户下载您购买的所有 Packt
    书籍的示例代码文件。如果您在其他地方购买了这本书，您可以访问 [http://www.PacktPub.com/support](http://www.PacktPub.com/support)
    并注册，以便将文件直接通过电子邮件发送给您。
- en: Errata
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 勘误
- en: Although we have taken every care to ensure the accuracy of our content, mistakes
    do happen. If you find a mistake in one of our books—maybe a mistake in the text
    or the code—we would be grateful if you would report this to us. By doing so,
    you can save other readers from frustration and help us improve subsequent versions
    of this book. If you find any errata, please report them by visiting [http://www.packtpub.com/support](http://www.packtpub.com/support),
    selecting your book, clicking on the **errata** **submission** **form** link,
    and entering the details of your errata. Once your errata are verified, your submission
    will be accepted and the errata will be uploaded on our website, or added to any
    list of existing errata, under the Errata section of that title. Any existing
    errata can be viewed by selecting your title from [http://www.packtpub.com/support](http://www.packtpub.com/support).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经尽最大努力确保内容的准确性，但错误仍然可能发生。如果您在我们的书中发现错误——可能是文本或代码中的错误——如果您能向我们报告这一点，我们将不胜感激。这样做可以避免其他读者感到沮丧，并帮助我们改进本书的后续版本。如果您发现任何勘误，请通过访问
    [http://www.packtpub.com/support](http://www.packtpub.com/support)，选择您的书籍，点击 **勘误提交**
    表格链接，并输入您的勘误详情来报告。一旦您的勘误得到验证，您的提交将被接受，勘误将被上传到我们的网站，或添加到该标题的勘误部分下的现有勘误列表中。任何现有勘误都可以通过从
    [http://www.packtpub.com/support](http://www.packtpub.com/support) 选择您的标题来查看。
- en: Piracy
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 盗版
- en: Piracy of copyright material on the Internet is an ongoing problem across all
    media. At Packt, we take the protection of our copyright and licenses very seriously.
    If you come across any illegal copies of our works, in any form, on the Internet,
    please provide us with the location address or website name immediately so that
    we can pursue a remedy.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在互联网上，版权材料的盗版是一个持续存在的问题，涉及所有媒体。在 Packt，我们非常重视我们版权和许可证的保护。如果您在互联网上发现我们作品的任何非法副本，无论形式如何，请立即提供位置地址或网站名称，以便我们可以寻求补救措施。
- en: Please contact us at `<[copyright@packtpub.com](mailto:copyright@packtpub.com)>`
    with a link to the suspected pirated material.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请通过 `<[copyright@packtpub.com](mailto:copyright@packtpub.com)>` 与我们联系，并提供涉嫌盗版材料的链接。
- en: We appreciate your help in protecting our authors, and our ability to bring
    you valuable content.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢您在保护我们作者和我们为您提供有价值内容的能力方面提供的帮助。
- en: Questions
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You can contact us at `<[questions@packtpub.com](mailto:questions@packtpub.com)>`
    if you are having a problem with any aspect of the book, and we will do our best
    to address it.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在本书的任何方面遇到问题，可以通过 `<[questions@packtpub.com](mailto:questions@packtpub.com)>`
    联系我们，我们将尽力解决。
