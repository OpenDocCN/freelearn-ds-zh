- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Simplifying Data Processing Using Snowpark
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Snowpark 简化数据处理
- en: In the previous chapter, we learned how to set up a development environment
    for Snowpark, as well as various Snowpark components, such as DataFrames, UDFs,
    and stored procedures. We also covered how to operate those objects and run them
    in Snowflake. In this chapter, we will cover data processing with Snowpark and
    learn how to load, prepare, analyze, and transform data using Snowpark.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何为 Snowpark 设置开发环境，以及各种 Snowpark 组件，如 DataFrames、UDFs 和存储过程。我们还介绍了如何操作这些对象并在
    Snowflake 中运行它们。在本章中，我们将介绍使用 Snowpark 进行数据处理，并学习如何使用 Snowpark 加载数据、准备、分析和转换数据。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Data ingestion
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据摄取
- en: Data exploration and transformation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据探索和转换
- en: Data grouping and analysis
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分组和分析
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, you require an active Snowflake account and Python installed
    with Anaconda configured locally. You can refer to the following documentation
    for installation instructions:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，您需要一个有效的 Snowflake 账户，并且需要在本地安装了 Anaconda 的 Python。您可以参考以下文档获取安装说明：
- en: You can sign up for a Snowflake Trial account at [https://signup.snowflake.com/](https://signup.snowflake.com/)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在 [https://signup.snowflake.com/](https://signup.snowflake.com/) 注册 Snowflake
    试用账户。
- en: To configure Anaconda, follow the guide at https://conda.io/projects/conda/en/latest/user-guide/getting-started.html
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要配置 Anaconda，请遵循 https://conda.io/projects/conda/en/latest/user-guide/getting-started.html
    中的指南
- en: To install and set up Python for VS Code, follow the guide at [https://code.visualstudio.com/docs/python/python-tutorial](https://code.visualstudio.com/docs/python/python-tutorial)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要安装和设置 VS Code 中的 Python，请遵循 [https://code.visualstudio.com/docs/python/python-tutorial](https://code.visualstudio.com/docs/python/python-tutorial)
    中的指南
- en: To learn how to operate Jupyter Notebook in VS Code, go to [https://code.visualstudio.com/docs/datascience/jupyter-notebooks](https://code.visualstudio.com/docs/datascience/jupyter-notebooks)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解如何在 VS Code 中操作 Jupyter Notebook，请访问 [https://code.visualstudio.com/docs/datascience/jupyter-notebooks](https://code.visualstudio.com/docs/datascience/jupyter-notebooks)
- en: The supporting materials for this chapter are available in this book’s GitHub
    repository at [https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的支持材料可在本书的 GitHub 仓库中找到，网址为 [https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark)。
- en: Data ingestion
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据摄取
- en: The first part of the data engineering process is data ingestion – it is crucial
    to get all the different data into a usable format in Snowflake for analytics.
    In the previous chapter, we learned how Snowpark can access data through a DataFrame.
    This DataFrame can access data from Snowflake tables, views, and objects, such
    as streams, if we run a query against it. Snowpark supports structured data in
    various formats, such as Excel and CSV, as well as semi-structured data, such
    as JSON, XML, Parquet, Avro, and ORC; specialized formats, such as HL7 and DICOM,
    and unstructured data, such as images and media, can be ingested and handled in
    Snowpark. Snowpark enables secure and programmatic access to files in Snowflake
    stages.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程流程的第一部分是数据摄取——将所有不同的数据以可用的格式放入 Snowflake 进行分析至关重要。在上一章中，我们学习了 Snowpark 如何通过
    DataFrame 访问数据。如果对这个 DataFrame 执行查询，它可以从 Snowflake 表、视图以及流等对象中访问数据。Snowpark 支持多种格式的结构化数据，如
    Excel 和 CSV，以及半结构化数据，如 JSON、XML、Parquet、Avro 和 ORC；特殊格式，如 HL7 和 DICOM，以及非结构化数据，如图像和媒体，也可以在
    Snowpark 中摄取和处理。Snowpark 使您能够安全且程序化地访问 Snowflake 阶段中的文件。
- en: The flexibility of Snowpark Python allows you to adapt to changing data requirements
    effortlessly. Suppose you start with a CSV file as your data source; you can switch
    to a JSON or packet format at a later stage. With Snowpark, you don’t need to
    rewrite your entire code base. Instead, you can make minor adjustments or configuration
    changes to accommodate the new structure while keeping the core logic intact.
    This flexibility saves you valuable time and effort, enabling you to switch between
    different data formats as your needs evolve quickly.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark Python 的灵活性允许您轻松适应不断变化的数据需求。假设您最初以 CSV 文件作为数据源；您可以在稍后阶段切换到 JSON 或数据包格式。使用
    Snowpark，您无需重写整个代码库。相反，您可以通过进行少量调整或配置更改来适应新的结构，同时保持核心逻辑不变。这种灵活性为您节省了宝贵的时间和精力，使您能够根据需求快速演变在不同数据格式之间切换。
- en: By leveraging Snowpark’s capabilities, you can focus more on analyzing and utilizing
    data rather than worrying about the intricacies of data format handling. This
    streamlined approach empowers you to experiment with different data sources, adapt
    to evolving data requirements, and efficiently load data into Snowflake tables,
    all with minimal code changes and maximum flexibility.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用Snowpark的功能，您可以更多地专注于分析和利用数据，而不是担心数据格式处理的复杂性。这种简化的方法使您能够对不同数据源进行实验，适应不断变化的数据需求，并高效地将数据加载到Snowflake表中，所有这些只需进行最少的代码更改和最大的灵活性。
- en: So, let’s delve into the power of Snowpark Python and its ability to effortlessly
    handle different data formats, allowing you to work with diverse sources without
    cumbersome code modifications. You will experience the freedom to explore, analyze,
    and extract insights from your data while enjoying a seamless and flexible integration
    process.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们深入探讨Snowpark Python的力量及其轻松处理不同数据格式的能力，让您能够与各种来源协同工作而无需进行繁琐的代码修改。您将体验到探索、分析和从数据中提取见解的自由，同时享受无缝和灵活的集成过程。
- en: 'The data ingestion scripts are provided in this book’s GitHub repository: [https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark).
    These scripts will simplify the process of uploading any new dataset that will
    be used for analysis, ensuring a smooth and efficient workflow. Following a similar
    approach to what was outlined in the preceding chapters, you can effortlessly
    upload new datasets and explore Snowflake’s data engineering and machine learning
    functionalities. The provided data ingestion scripts will act as your guide, making
    the process seamless and hassle-free.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据摄取脚本提供在本书的GitHub仓库中：[https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark)。这些脚本将简化上传任何新数据集的过程，确保工作流程的顺畅和高效。遵循前几章中概述的类似方法，您可以轻松上传新数据集并探索Snowflake的数据工程和机器学习功能。提供的数据摄取脚本将作为您的指南，使过程无缝且无烦恼。
- en: Important note on datasets
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重要数据集注意事项
- en: The dataset we’ll be using in this chapter provides unique insights into customer
    behavior, campaign responses, and complaints, enabling data-driven decision-making
    and customer satisfaction improvement. The original dataset is from the Kaggle
    platform ([https://www.kaggle.com/datasets/rodsaldanha/arketing-campaign](https://www.kaggle.com/datasets/rodsaldanha/arketing-campaign)).
    However, the datasets that will be discussed in this section are not directly
    accessible via a Kaggle link. Instead, we started with a base dataset and generated
    new data formats to illustrate loading various dataset formats using Snowpark.
    These datasets can be found in this book’s GitHub repository under the `datasets`
    folder.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们将使用的数据集提供了对客户行为、活动响应和投诉的独特见解，从而支持数据驱动的决策和客户满意度提升。原始数据集来自Kaggle平台([https://www.kaggle.com/datasets/rodsaldanha/arketing-campaign](https://www.kaggle.com/datasets/rodsaldanha/arketing-campaign))。然而，本节中讨论的数据集不能直接通过Kaggle链接访问。相反，我们从一个基础数据集开始，并生成新的数据格式，以展示如何使用Snowpark加载各种数据集格式。这些数据集可以在本书的GitHub仓库的`datasets`文件夹中找到。
- en: 'The datasets include purchase history in CSV format, campaign information in
    JSON format, and complaint information in Parquet format. These datasets provide
    valuable information about customer behavior, campaign responses, and complaints:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包括CSV格式的购买历史，JSON格式的活动信息，以及Parquet格式的投诉信息。这些数据集提供了关于客户行为、活动响应和投诉的有价值信息：
- en: '**Purchase history (CSV)**: This file contains customer information, such as
    ID, education, marital status, and purchase metrics. The dataset offers insights
    into customer buying habits and can be further analyzed for data-driven decisions.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**购买历史（CSV）**：此文件包含客户信息，如ID、教育程度、婚姻状况和购买指标。该数据集提供了对客户购买习惯的见解，并可进一步分析以支持数据驱动的决策。'
- en: '**Campaign information (JSON)**: The JSON dataset includes data on campaign
    acceptance and customer responses. Analyzing this dataset will help you refine
    marketing strategies and understand campaign effectiveness.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**活动信息（JSON）**：JSON数据集包括关于活动接受度和客户响应的数据。分析此数据集将帮助您优化营销策略并了解活动效果。'
- en: '**Complaint information (Parquet)**: This file contains details about customer
    complaints, including contact and revenue metrics. This dataset aids in tracking
    and addressing customer complaints for improved satisfaction.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**投诉信息（Parquet）**：此文件包含有关客户投诉的详细信息，包括联系和收入指标。此数据集有助于跟踪和解决客户投诉，以改善满意度。'
- en: Note
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Moving forward, we will be utilizing our local development environment to execute
    all Snowpark code, rather than relying on Snowflake worksheets. This approach
    offers greater flexibility and control over the development and testing of Snowpark
    scripts. When worksheets are used for specific tasks, we will explicitly call
    out their usage for clarity and context.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的工作中，我们将利用本地开发环境来执行所有 Snowpark 代码，而不是依赖于 Snowflake 工作表。这种方法提供了更大的灵活性和对 Snowpark
    脚本开发和测试的控制。当使用工作表执行特定任务时，我们将明确指出其使用情况，以确保清晰和上下文。
- en: Ingesting a CSV file into Snowflake
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 CSV 文件导入 Snowflake
- en: 'Snowflake supports ingesting data easily using CSV files. We will load the
    purchase history data into the `PURCHASE_HISTORY` table as a CSV file. We’ll upload
    `purchase_history.csv` into an internal stage by using a Snowpark session, as
    shown here:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Snowflake 支持使用 CSV 文件轻松导入数据。我们将以 CSV 文件的形式将购买历史数据加载到 `PURCHASE_HISTORY` 表中。我们将使用
    Snowpark 会话将 `purchase_history.csv` 上传到内部阶段，如下所示：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'With that, the file has been uploaded to the internal stage. We will reference
    this directly in Snowpark. The data schema for the marketing table can also be
    directly defined as a Snowpark type. The following code provides the necessary
    columns and data types to create the table in Snowflake:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，文件已上传到内部阶段。我们将在 Snowpark 中直接引用此文件。营销表的数据模式也可以直接定义为 Snowpark 类型。以下代码提供了创建
    Snowflake 表所需的必要列和数据类型：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this code snippet, we take our first step toward understanding the structure
    of our data by defining a schema for our purchase history dataset. Using the Snowflake
    Snowpark library, we establish the fields and corresponding data types, setting
    the foundation for our data analysis journey. This code serves as a starting point,
    guiding us in defining and working with structured data. This is not the only
    way we can load the dataset using Snowpark. We will continue to explore different
    methodologies to load other tabular datasets as we progress.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码片段中，我们通过为购买历史数据集定义一个模式来迈出理解我们数据结构的第一步。使用 Snowflake Snowpark 库，我们建立字段和对应的数据类型，为我们数据分析之旅奠定基础。此代码作为起点，指导我们定义和使用结构化数据。这不是我们使用
    Snowpark 加载数据集的唯一方法。随着我们的进展，我们将继续探索不同的方法来加载其他表格数据集。
- en: 'This code imports the necessary types from the Snowflake Snowpark library.
    It creates a variable called `purchase_history_schema` and assigns it a `StructType`
    object, representing a structured schema for the dataset. The `StructType` object
    contains multiple `StructField` objects, each representing a field in the dataset.
    Each `StructField` object specifies the name of the area and its corresponding
    data type using the types provided by Snowflake Snowpark. The following code reads
    the file:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码从 Snowflake Snowpark 库导入必要的类型。它创建了一个名为 `purchase_history_schema` 的变量，并将其分配给一个
    `StructType` 对象，该对象代表数据集的结构化模式。`StructType` 对象包含多个 `StructField` 对象，每个对象代表数据集中的一个字段。每个
    `StructField` 对象使用 Snowflake Snowpark 提供的类型指定区域的名称及其对应的数据类型。以下代码读取文件：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The CSV file is read with file format options such as `FIELD_DELIMITER`, `SKIP_HEADER`,
    and others, all of which are specified alongside the schema defined in the preceding
    definition. The `PURCHASE_HISTORY` table was created with the data from the CSV
    file, which is now ready for processing:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: CSV 文件使用文件格式选项（如 `FIELD_DELIMITER`、`SKIP_HEADER` 等）进行读取，所有这些选项都与前面定义中定义的模式一起指定。使用来自
    CSV 文件的数據创建了 `PURCHASE_HISTORY` 表，现在该表已准备好进行处理：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code shows the output of the `PURCHASE_HISTORY` table:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码显示了 `PURCHASE_HISTORY` 表的输出：
- en: '![Figure 3.1 – The PURCHASE_HISTORY table](img/B19923_03_1.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – PURCHASE_HISTORY 表](img/B19923_03_1.jpg)'
- en: Figure 3.1 – The PURCHASE_HISTORY table
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – PURCHASE_HISTORY 表
- en: The CSV is easy to load as it uses the file format options available in Snowflake.
    Now, let’s see how we can load JSON files into Snowflake.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: CSV 文件易于加载，因为它使用了 Snowflake 中可用的文件格式选项。现在，让我们看看如何将 JSON 文件加载到 Snowflake 中。
- en: Ingesting JSON into Snowflake
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 JSON 数据导入 Snowflake
- en: 'Snowflake allows JSON structures to be ingested and processed via the `Variant`
    data type. We can ingest JSON similar to how we would ingest a CSV file – by uploading
    it into the internal stage. The `campaign_info.json` file contains data about
    marketing campaigns. We can load this into the `CAMPAIGN_INFO` table by using
    the following code:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Snowflake 允许通过 `Variant` 数据类型摄取和处理 JSON 结构。我们可以像摄取 CSV 文件一样摄取 JSON 文件——通过将其上传到内部阶段。`campaign_info.json`
    文件包含有关营销活动的数据。我们可以使用以下代码将其加载到 `CAMPAIGN_INFO` 表中：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With that, the file has been uploaded to the internal stage; we will reference
    it in Snowpark. Snowpark can access the file to load it into a table:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由此，文件已上传到内部阶段；我们将在 Snowpark 中引用它。Snowpark 可以访问该文件并将其加载到表中：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The contents of the JSON file are read into the DataFrame as JSON objects.
    This DataFrame can be written into a table as a variant:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 文件的内容被读入 DataFrame 作为 JSON 对象。这个 DataFrame 可以作为一个变体写入到表中：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `CAMPAIGN_INFO_TEMP` table contains the JSON data. We can query the table
    to view the data:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`CAMPAIGN_INFO_TEMP` 表包含 JSON 数据。我们可以查询该表来查看数据：'
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding command displays the JSON data from the DataFrame:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令显示 DataFrame 中的 JSON 数据：
- en: '![Figure 3.2 – The Campaign Info table](img/B19923_03_2.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – 营销信息表](img/B19923_03_2.jpg)'
- en: Figure 3.2 – The Campaign Info table
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 营销信息表
- en: 'The following code snippet utilizes the Snowpark library in Snowflake to manipulate
    a DataFrame:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段利用 Snowflake 中的 Snowpark 库来操作 DataFrame：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The preceding code selects specific columns from an existing DataFrame and renames
    them using the `col` function. The transformed DataFrame is then saved as a new
    table in Snowflake. The code performs data **extraction, transformation, and loading**
    (**ETL**) operations by selecting and renaming columns within the DataFrame and
    saving the result as a new table in Snowflake.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码从现有的 DataFrame 中选择特定的列，并使用 `col` 函数重命名它们。转换后的 DataFrame 然后作为新表保存到 Snowflake
    中。该代码通过在 DataFrame 中选择和重命名列来执行数据 **提取、转换和加载**（**ETL**）操作，并将结果保存为新表。
- en: 'The `CAMPAIGN_INFO` table now contains the flattened data, with the data in
    separate columns so that it’s easier to process. Let’s have a look at the data:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`CAMPAIGN_INFO` 表现在包含扁平化数据，数据分别存储在单独的列中，以便更容易处理。让我们看一下数据：'
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding code shows the output of the `CAMPAIGN_INFO` table:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码显示了 `CAMPAIGN_INFO` 表的输出：
- en: '![Figure 3.3 – The CAMPAIGN_INFO table](img/B19923_03_3.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3 – `CAMPAIGN_INFO` 表](img/B19923_03_3.jpg)'
- en: Figure 3.3 – The CAMPAIGN_INFO table
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – `CAMPAIGN_INFO` 表
- en: Loading and processing JSON files in Snowpark becomes easier when using the
    `Variant` column. Next, we will cover how to load a Parquet file into Snowflake
    using Snowpark.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `Variant` 列，在 Snowpark 中加载和处理 JSON 文件变得更容易。接下来，我们将介绍如何使用 Snowpark 将 Parquet
    文件加载到 Snowflake 中。
- en: Ingesting Parquet files into Snowflake
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 Parquet 文件摄取到 Snowflake
- en: 'Parquet is a popular open source format for storing data licensed under Apache.
    The column-oriented format is lighter to store and faster to process. Parquet
    also supports complex data types since the data and the column information are
    stored in Parquet format. The `COMPLAINT_INFO` table consists of customer complaint
    information. Let’s load this into Snowflake:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 是一种流行的开源数据存储格式，由 Apache 许可。列导向的格式存储更轻便，处理更快。Parquet 还支持复杂的数据类型，因为数据和列信息都存储在
    Parquet 格式中。`COMPLAINT_INFO` 表由客户投诉信息组成。让我们将其加载到 Snowflake 中：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The file will be uploaded into the internal stage. Snowpark can access it to
    process and load it into a table:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 文件将被上传到内部阶段。Snowpark 可以访问它以处理并将其加载到表中：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The Parquet file is read into the DataFrame and then copied into the `COMPLAINT_INFO`
    table. Since the Parquet file already contains the table metadata information,
    it defines the table structure. We can query the table to view the data:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Parquet 文件读入 DataFrame，然后复制到 `COMPLAINT_INFO` 表中。由于 Parquet 文件已经包含表元数据信息，它定义了表结构。我们可以查询该表来查看数据：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will output the following `COMPLAINT_INFO` table:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下 `COMPLAINT_INFO` 表：
- en: '![Figure 3.4 – The COMPLAINT_INFO table](img/B19923_03_4.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – `COMPLAINT_INFO` 表](img/B19923_03_4.jpg)'
- en: Figure 3.4 – The COMPLAINT_INFO table
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – `COMPLAINT_INFO` 表
- en: Parquet is one of the preferred formats for Snowflake since it’s the format
    that’s used by Apache Iceberg. Parquet stands out in data engineering and data
    science for its columnar storage, which optimizes compression and query performance.
    Its support for schema evolution and partitioning ensures flexibility and efficiency
    in handling evolving data structures. With broad compatibility across various
    data processing frameworks, Parquet enables seamless integration into existing
    workflows, making it a cornerstone format in modern data pipelines. In the next
    section, we will cover how easy it is to load unstructured data, such as an image,
    into Snowflake.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 是 Snowflake 的一种首选格式，因为它是由 Apache Iceberg 使用的格式。Parquet 在数据工程和数据分析中因其列式存储而脱颖而出，这优化了压缩和查询性能。其对模式演变和分区的支持确保了在处理不断变化的数据结构时的灵活性和效率。由于其与各种数据处理框架的广泛兼容性，Parquet
    能够无缝集成到现有工作流程中，成为现代数据管道中的基石格式。在下一节中，我们将介绍如何轻松地将非结构化数据，如图片，加载到 Snowflake 中。
- en: Important note
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: We’ve chosen to maintain separate stages for handling images and text, although
    it’s not mandatory to do so. The **MY_TEXT** and **MY_IMAGES** stages can be prepared
    using the same methods we outlined earlier.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择为处理图片和文本维护单独的阶段，尽管这样做不是强制性的。**MY_TEXT** 和 **MY_IMAGES** 阶段可以使用我们之前概述的方法准备。
- en: Ingesting images into Snowpark
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将图片导入 Snowpark
- en: Snowflake supports versatile data, such as images, that can be uploaded into
    a stage and executed directly in Snowpark without the need to manage dependencies
    as well.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Snowflake 支持多种数据类型，如图片，可以直接上传到阶段并在 Snowpark 中执行，无需管理依赖项。
- en: 'Platforms such as Amazon S3, Google Cloud Storage, and Azure Blob Storage are
    commonly preferred for managing and storing image data due to their scalability
    and reliability. However, it’s worth noting that Snowpark also offers flexibility
    in loading image files, making it a versatile option for handling image data in
    data engineering and data science workflows. We will be loading a bunch of sample
    images that can be used for processing:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 平台如 Amazon S3、Google Cloud Storage 和 Azure Blob Storage 常常被用于管理和存储图片数据，因为它们具有可扩展性和可靠性。然而，值得注意的是，Snowpark
    也提供了在数据工程和数据分析工作流程中处理图片数据的灵活性，使其成为一种多功能的选项。我们将加载一些样本图片，这些图片可以用于处理：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code loads the images from the local folder to the internal stage.
    The path can support wildcard entries to upload all the images in a particular
    folder. The folder in the stage can be queried to get the list of images that
    were uploaded:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码展示了如何将图片从本地文件夹加载到内部阶段。路径可以支持通配符输入，以便上传特定文件夹中的所有图片。可以在阶段中查询文件夹以获取已上传图片的列表：
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The preceding code shows a list of all the images that are present in the stage:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码展示了阶段中所有现有图片的列表：
- en: '![Figure 3.5 – List of images](img/B19923_03_5.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – 图片列表](img/B19923_03_5.jpg)'
- en: Figure 3.5 – List of images
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 图片列表
- en: 'Once the image has been uploaded, it can be directly accessed via Snowpark.
    Snowpark supports the `get_stream` function to stream the file’s contents as bytes
    from the stage. We can use a library such as Pillow to read the file from the
    bytes stream:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦图片上传成功，就可以通过 Snowpark 直接访问。Snowpark 支持使用 `get_stream` 函数从阶段以字节形式流式传输文件的內容。我们可以使用像
    Pillow 这样的库从字节流中读取文件：
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This will output the following image:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下图片：
- en: '![Figure 3.6 – Rendering images](img/B19923_03_6.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – 渲染图片](img/B19923_03_6.jpg)'
- en: Figure 3.6 – Rendering images
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 渲染图片
- en: The image is displayed directly in the notebook. Snowpark’s native support for
    images supports capabilities for use cases such as image classification, image
    processing, and image recognition. Snowpark also supports rendering images dynamically.
    We will cover this in the next section.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图片直接在笔记本中显示。Snowpark 对图片的原生支持支持图像分类、图像处理和图像识别等用例。Snowpark 还支持动态渲染图片。我们将在下一节中介绍这一点。
- en: Reading files dynamically with Snowpark
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Snowpark 动态读取文件
- en: 'Snowpark contains the `files` module and the `SnowflakeFile` class, both of
    which provide access to files dynamically and stream them for processing. These
    dynamic files are also helpful for reading multiple files as we can iterate over
    them. `open()` extends the `IOBase` file objects and provides the functionality
    to open a file. The `SnowflakeFile` object also supports other `IOBase` methods
    for processing the file. The following code shows an example of reading multiple
    files using a relative path from the internal stage:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark 包含 `files` 模块和 `SnowflakeFile` 类，这两个都提供了动态访问文件并流式传输以进行处理的接口。这些动态文件对于读取多个文件也很有帮助，因为我们能够遍历它们。`open()`
    扩展了 `IOBase` 文件对象，并提供了打开文件的函数。`SnowflakeFile` 对象也支持其他 `IOBase` 方法来处理文件。以下代码展示了如何使用内部阶段的相对路径读取多个文件的示例：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The preceding code iterates over the `@MY_TEXTS` stage location and calculates
    the length of each file using the `SnowflakeFile` method. The path is passed as
    the input to the UDF. We can execute the function to get the output:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码遍历 `@MY_TEXTS` 阶段位置，并使用 `SnowflakeFile` 方法计算每个文件的长度。路径作为输入传递给 UDF。我们可以执行该函数以获取输出：
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding code produces the following result:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码产生以下结果：
- en: '![Figure 3.7 – Dynamic files within Snowpark](img/B19923_03_7.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.7 – Snowpark 中的动态文件](img/B19923_03_7.jpg)'
- en: Figure 3.7 – Dynamic files within Snowpark
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – Snowpark 中的动态文件
- en: The files in the stage are displayed as output. In this section, we covered
    ingesting different types of files into Snowflake using Snowpark. In the next
    section, we will learn how to perform data preparation and transformations using
    Snowpark.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段中的文件以输出形式显示。在本节中，我们介绍了如何使用 Snowpark 将不同类型的文件导入 Snowflake。在下一节中，我们将学习如何使用 Snowpark
    进行数据准备和转换。
- en: Data exploration and transformation
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据探索和转换
- en: Once the data has been loaded, the next step is to prepare the data so that
    it can be transformed. In this section, we will cover how to perform data exploration
    so that we understand how the modify the data as necessary.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加载完成后，下一步是准备数据，以便进行转换。在本节中，我们将介绍如何进行数据探索，以便我们了解如何根据需要修改数据。
- en: Data exploration
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据探索
- en: '**Data exploration** is a critical step in data analysis as it sets the stage
    for successful insights and informed decision-making. By delving into the data,
    analysts can deeply understand its characteristics, uncover underlying patterns,
    and identify potential issues or outliers. Exploring the data provides valuable
    insights into its structure, distribution, and relationships, enabling analysts
    to choose the appropriate data transformation techniques.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据探索** 是数据分析中的关键步骤，因为它为成功的见解和明智的决策奠定了基础。通过深入研究数据，分析师可以深入了解其特征，揭示潜在的规律，并识别潜在的问题或异常。探索数据提供了关于其结构、分布和关系的宝贵见解，使分析师能够选择适当的数据转换技术。'
- en: Understanding the data’s characteristics and patterns helps analysts determine
    the appropriate transformations and manipulations needed to clean, reshape, or
    derive new variables from the data. Additionally, data exploration aids in identifying
    subsets of data that are relevant to the analysis, facilitating the filtering
    and sub-setting operations required for specific analytical objectives.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 理解数据的特征和模式有助于分析师确定所需的适当转换和操作，以清理、重塑或从数据中派生新变量。此外，数据探索有助于识别与分析相关的数据子集，便于进行特定分析目标所需的过滤和子集操作。
- en: 'Before embarking on data transformation, we must understand the data we have
    in place. By comprehensively understanding the data, we can effectively identify
    its structure, quality, and patterns. This understanding is a solid foundation
    for informed decision-making during the data transformation process, enabling
    us to extract meaningful insights and derive maximum value from the data. Take
    a look at the following code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始数据转换之前，我们必须了解我们现有的数据。通过全面理解数据，我们可以有效地识别其结构、质量和模式。这种理解是数据转换过程中做出明智决策的坚实基础，使我们能够提取有意义的见解并从数据中获得最大价值。请看以下代码：
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here, we loaded the necessary tables into a session. These tables are now available
    in the Snowpark session for further data preparation. We will start by preparing
    the `PURCHASE_HISTORY` table:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将必要的表加载到会话中。这些表现在在 Snowpark 会话中可用，以便进行进一步的数据准备。我们将从准备 `PURCHASE_HISTORY`
    表开始：
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `show()` method returns the data from the DataFrame. The preceding code
    produces the top 5 rows from the `PURCHASE_HISTORY` table:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`show()`方法返回DataFrame中的数据。前面的代码生成了`PURCHASE_HISTORY`表的前5行：'
- en: '![Figure 3.8 – PURCHASE_HISTORY – top 5 rows](img/B19923_03_8.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图3.8 – 购买历史 – 前5行](img/B19923_03_8.jpg)'
- en: Figure 3.8 – PURCHASE_HISTORY – top 5 rows
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 – 购买历史 – 前5行
- en: 'We can use the `collect()` method to display the data in the notebook:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`collect()`方法在笔记本中显示数据：
- en: '[PRE20]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The records from the `PURCHASE_HISTORY` table are shown in the JSON array:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`PURCHASE_HISTORY`表中的记录以JSON数组的形式显示：'
- en: '![Figure 3.9 – PURCHASE_HISTORY – full table](img/B19923_03_9.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图3.9 – 购买历史 – 完整表](img/B19923_03_9.jpg)'
- en: Figure 3.9 – PURCHASE_HISTORY – full table
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 – 购买历史 – 完整表
- en: The difference between collect() and show()
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: collect()和show()之间的区别
- en: 'In Snowpark Python, there are two essential functions: **collect()** and **show()**.
    These functions serve different purposes in processing and displaying data. The
    **collect()** function in Snowpark Python is used to gather or retrieve data from
    a specified source, such as a table, file, or API. It allows you to perform queries,
    apply filters, and extract the desired information from the data source. The collected
    data is stored in a variable or structure, such as a DataFrame, for further analysis
    or manipulation.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在Snowpark Python中，有两个基本函数：**collect()**和**show()**。这些函数在数据处理和显示数据时具有不同的用途。Snowpark
    Python中的**collect()**函数用于从指定的源（如表、文件或API）收集或检索数据。它允许您执行查询、应用过滤器并从数据源中提取所需信息。收集到的数据存储在变量或结构（如DataFrame）中，以便进行进一步的分析或操作。
- en: On the other hand, the **show()** function in Snowpark Python is primarily used
    to display the contents of a DataFrame or any other data structure in a tabular
    format. It provides a convenient way to visualize and inspect the data at different
    stages of the data processing pipeline. The **show()** function presents the data
    in a human-readable manner, showing the rows and columns in a structured table-like
    format. It can be helpful for debugging, understanding the data’s structure, or
    performing exploratory data analysis.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Snowpark Python中的**show()**函数主要用于以表格格式显示DataFrame或任何其他数据结构的内 容。它提供了一个方便的方式来可视化并检查数据处理管道不同阶段的数据。**show()**函数以人类可读的方式呈现数据，以结构化表格格式显示行和列。它对于调试、理解数据结构或进行数据探索分析可能很有帮助。
- en: In short, the **collect()** function focuses on gathering and retrieving data
    from a source, while the **show()** function displays the data in a readable format.
    Both functions play essential roles in Snowpark Python when it comes to working
    with data, but they serve distinct purposes in the data processing workflow.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，**collect()**函数专注于从源中收集和检索数据，而**show()**函数以可读的格式显示数据。这两个函数在Snowpark Python处理数据时都发挥着重要作用，但在数据处理工作流程中它们具有不同的用途。
- en: 'Next, we will use the `count()` method to get the total count of the rows in
    the table:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`count()`方法来获取表中行的总数：
- en: '[PRE21]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: From the resulting output, we can see that the `PURCHASE_HISTORY` table contains
    around 2,000 rows of data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果输出中，我们可以看到`PURCHASE_HISTORY`表包含大约2,000行数据。
- en: 'We can now check the columns of the table to understand more about this data:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以检查表的列以了解有关这些数据的更多信息：
- en: '[PRE22]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This returns the column information, which helps us understand the data better.
    The column information contains the data related to customer purchase history:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回列信息，有助于我们更好地理解数据。列信息包含与客户购买历史相关的数据：
- en: '![Figure 3.10 – PURCHASE_HISTORY columns](img/B19923_03_10.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图3.10 – 购买历史 – 列](img/B19923_03_10.jpg)'
- en: Figure 3.10 – PURCHASE_HISTORY columns
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 – 购买历史 – 列
- en: 'We can now filter the data to slice and dice it. We can use the following code
    to filter specific rows or a single row:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以过滤数据以进行切片和切块。我们可以使用以下代码来过滤特定行或单行：
- en: '[PRE23]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This returns the column. where `id` is set to `1`. We can pass multiple values
    in the column filter to perform additional row-level operations:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回`id`设置为`1`的列。我们可以在列过滤器中传递多个值以执行额外的行级操作：
- en: '![Figure 3.11 – PURCHASE_HISTORY ID filter](img/B19923_03_11.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图3.11 – 购买历史 ID 过滤](img/B19923_03_11.jpg)'
- en: Figure 3.11 – PURCHASE_HISTORY ID filter
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 – 购买历史 ID 过滤
- en: 'If we need to add multiple filter values, we can use the `&` operation to pass
    multiple column filter values to the method:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要添加多个过滤器值，可以使用`&`操作符将多个列过滤器值传递给方法：
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The preceding code provides data about those with `MARITAL_STATUS` set to `Married`
    and who have kids at home (`KIDHOME`):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码提供了将`MARITAL_STATUS`设置为`Married`并且有孩子在家（`KIDHOME`）的人的数据：
- en: '![Figure 3.12 – PURCHASE_HISTORY filters](img/B19923_03_12.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图3.12 – 购买历史过滤器](img/B19923_03_12.jpg)'
- en: Figure 3.12 – PURCHASE_HISTORY filters
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 – 购买历史过滤器
- en: 'This helps us understand the purchase history pattern of married customers
    with kids. We can also filter it to the year of birth by passing the year of birth
    range between 1964 and 1980:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于我们了解有孩子的已婚客户的购买历史模式。我们还可以通过传递1964年至1980年的出生年份范围来过滤到出生年份：
- en: '[PRE25]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This displays the purchase data for customers born between 1964 and 1980:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了1964年至1980年出生的客户的购买数据：
- en: '![Figure 3.13 – PURCHASE_HISTORY filters](img/B19923_03_13.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图3.13 – 购买历史过滤器](img/B19923_03_13.jpg)'
- en: Figure 3.13 – PURCHASE_HISTORY filters
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13 – 购买历史过滤器
- en: 'This data helps us understand their purchases. We can also use the `select()`
    method to select only the columns that are required for analysis:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这项数据有助于我们了解他们的购买情况。我们还可以使用`select()`方法仅选择分析所需的列：
- en: '[PRE26]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The preceding returns only the customer’s ID, year, and education status:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码仅返回客户的ID、年份和教育状况：
- en: '![Figure 3.14 – PURCHASE_HISTORY columns](img/B19923_03_14.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图3.14 – 购买历史列](img/B19923_03_14.jpg)'
- en: Figure 3.14 – PURCHASE_HISTORY columns
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14 – 购买历史列
- en: In the upcoming chapters, we will delve deeper into data exploration, uncovering
    more techniques to gain insights from our data.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将更深入地探讨数据探索，揭示更多从数据中获取洞察的技术。
- en: Building upon these basic exploration steps, we will dive into the realm of
    data transformation operations. By combining our understanding of the data and
    the power of transformation techniques, we will unlock the full potential of our
    data and extract valuable insights for informed decision-making.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些基本探索步骤的基础上，我们将深入数据转换操作领域。通过结合我们对数据的理解以及转换技术的力量，我们将释放数据的全部潜力，并为明智的决策提取有价值的见解。
- en: In the next section, we will discuss how to perform data transformation using
    this data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论如何使用这些数据执行数据转换。
- en: Data transformations
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据转换
- en: '**Data transformation** is a fundamental process that involves modifying and
    reshaping data to make it more suitable for analysis or other downstream tasks,
    such as the machine learning model building process. It entails applying a series
    of operations to the data, such as cleaning, filtering, aggregating, and reformatting,
    to ensure its quality, consistency, and usability. Data transformation allows
    us to convert raw data into a structured and organized format that can be easily
    interpreted and analyzed.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据转换**是一个基本过程，涉及修改和重塑数据，使其更适合分析或其他下游任务，例如机器学习模型构建过程。它包括对数据应用一系列操作，如清理、过滤、聚合和重新格式化，以确保其质量、一致性和可用性。数据转换使我们能够将原始数据转换为易于解释和分析的结构化和组织化格式。'
- en: 'The data requires minimal transformation, and we will cover it extensively
    in the coming chapters. Our goal for this section is to combine data from different
    sources, creating a unified table for further processing that we will use in the
    next chapter. We will leverage Snowpark’s robust join and union capabilities to
    accomplish this. By utilizing joins, we can merge data based on standard columns
    or conditions. Unions, on the other hand, allow us to append data from multiple
    sources vertically. These techniques will enable us to integrate and consolidate
    our data efficiently, setting the stage for comprehensive analysis and insights.
    Let’s explore how Snowpark’s join and union capabilities can help us achieve this
    data combination:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 数据需要最小的转换，我们将在接下来的章节中对其进行详细阐述。本节的目标是将来自不同来源的数据结合起来，创建一个用于进一步处理的统一表格，我们将在下一章中使用它。我们将利用Snowpark强大的连接和合并功能来完成这项工作。通过使用连接，我们可以根据标准列或条件合并数据。另一方面，合并允许我们垂直地追加来自多个来源的数据。这些技术将使我们能够有效地整合和合并我们的数据，为全面分析和洞察奠定基础。让我们探索Snowpark的连接和合并功能如何帮助我们实现这种数据组合：
- en: '[PRE27]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here, we are joining the purchase history to campaign information to establish
    the relationship between purchases and campaigns. The standard ID column is used
    to select the join and defaults to an inner join:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将购买历史与活动信息连接起来，以建立购买与活动之间的关系。标准ID列用于选择连接，默认为内部连接：
- en: '[PRE28]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We are dropping the extra ID column from the joined result. The DataFrame now
    contains just a single ID column:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在从合并的结果中删除额外的 ID 列。DataFrame 现在只包含一个 ID 列：
- en: '[PRE29]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This displays the data of the purchase campaign combined with the purchase
    history and the campaign information:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了购买活动数据与购买历史和活动信息相结合的数据：
- en: '![Figure 3.15 – Purchase campaign data](img/B19923_03_15.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.15 – 购买活动数据](img/B19923_03_15.jpg)'
- en: Figure 3.15 – Purchase campaign data
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.15 – 购买活动数据
- en: 'Let’s combine this with the complaint information to get the complete data:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些数据与投诉信息结合起来以获得完整的数据：
- en: '[PRE30]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here, we are combining the result of the purchase campaign along with the complaint
    information by using the standard ID column. The resultant DataFrame contains
    the complete data required for data analysis. We are dropping the extra ID column
    from the joined result. The DataFrame now has just a single ID column:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过使用标准的 ID 列将购买活动的结果与投诉信息结合起来。结果 DataFrame 包含了数据分析所需的完整数据。我们正在从合并的结果中删除额外的
    ID 列。DataFrame 现在只有一个 ID 列：
- en: '[PRE31]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This displays the final data combined from all three tables. We can now write
    this data into the table for further analysis:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了从所有三个表中综合得到的最终数据。我们现在可以将这些数据写入表格以进行进一步分析：
- en: '[PRE32]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Here, the data is written into the `MARKETING_DATA` table, at which point it
    will be available inside Snowflake. We need to append this data with the additional
    marketing data that must be loaded into this table.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，数据被写入 `MARKETING_DATA` 表，此时它将在 Snowflake 中可用。我们需要将此数据与必须加载到该表中的附加营销数据一起追加。
- en: The difference between joins and unions
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 连接和联合的区别
- en: Joins combine data from two or more tables based on a shared column or condition.
    In Snowflake Snowpark, you can perform different types of joins, such as inner
    join, left join, right join, and full outer join. Joins allow you to merge data
    horizontally by aligning rows based on matching values in the specified columns.
    This enables you to combine related data from multiple tables, resulting in a
    combined dataset that includes information from all the joined tables.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 连接基于共享列或条件从两个或多个表中合并数据。在 Snowflake Snowpark 中，您可以执行不同类型的连接，如内连接、左连接、右连接和全外连接。连接允许您通过根据指定列中的匹配值对齐行来水平合并数据。这使得您能够将来自多个表的相关数据合并，从而生成包含所有连接表信息的合并数据集。
- en: On the other hand, unions are used to append data from multiple tables vertically,
    or result sets into a single dataset. Unlike joins, unions do not require any
    specific conditions or matching columns. Instead, they stack rows on top of each
    other, concatenating the data vertically. This is useful when you have similar
    datasets with the same structure and want to consolidate them into a single dataset.
    Unions can be performed in Snowflake Snowpark to create a new dataset that contains
    all the rows from the input tables or result sets.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，联合用于垂直追加来自多个表的数据，或将结果集追加到单个数据集中。与连接不同，联合不需要任何特定条件或匹配列。相反，它们将行堆叠在一起，垂直连接数据。这在您有相同结构且希望将它们合并到单个数据集中的类似数据集时非常有用。在
    Snowflake Snowpark 中可以执行联合，以创建包含输入表或结果集中所有行的新的数据集。
- en: In summary, joins in Snowflake Snowpark are used to combine data horizontally
    by matching columns, while unions are used to stack data vertically without any
    specific conditions. Joins merge related data from multiple tables, while unions
    append similar datasets into a single dataset.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在 Snowflake Snowpark 中，连接用于通过匹配列水平地组合数据，而联合用于垂直堆叠数据，而不需要任何特定条件。连接将来自多个表的相关数据合并，而联合将类似的数据集追加到单个数据集中。
- en: Appending data
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 追加数据
- en: The Snowflake Snowpark `UNION` function is vital in combining and integrating
    new data into a Snowflake database. The importance of the `UNION` function lies
    in its ability to append rows from different data sources vertically, or result
    sets into a single consolidated dataset. When new data is added to the database,
    it is often necessary to merge or combine it with existing data for comprehensive
    analysis. The `UNION` function allows us to seamlessly integrate the newly added
    data with the existing dataset, creating a unified view encompassing all relevant
    information.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Snowflake Snowpark 的 `UNION` 函数在将新数据合并和集成到 Snowflake 数据库中起着至关重要的作用。`UNION` 函数的重要性在于其能够垂直追加来自不同数据源或结果集的行，或将结果集合并到单个综合数据集中。当向数据库添加新数据时，通常需要将其与现有数据合并或结合，以便进行综合分析。`UNION`
    函数使我们能够无缝地将新添加的数据与现有数据集集成，创建一个包含所有相关信息的统一视图。
- en: This capability of the `UNION` function is precious in scenarios where data
    is received or updated periodically. For example, suppose we receive daily sales
    data or log files. In that case, the `UNION` function enables us to effortlessly
    append the new records to the existing dataset, ensuring that our analysis reflects
    the most up-to-date information. Additionally, it ensures data consistency and
    allows for seamless continuity in data analysis, enabling us to derive accurate
    insights and make informed decisions based on the complete and unified dataset.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`UNION`函数的这一功能在数据定期接收或更新的场景中非常宝贵。例如，假设我们接收每日的销售数据或日志文件。在这种情况下，`UNION`函数使我们能够轻松地将新记录附加到现有数据集，确保我们的分析反映最新的信息。此外，它确保数据一致性，并允许数据分析的无缝连续性，使我们能够从完整和统一的数据集中得出准确的见解并做出明智的决策。'
- en: 'The additional marketing data is available in the `MARKETING_ADDITIONAL` table.
    Let’s see how we can leverage Snowpark’s `UNION` function to include this additional
    data for processing:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的营销数据可在`MARKETING_ADDITIONAL`表中找到。让我们看看我们如何利用Snowpark的`UNION`函数来处理这些额外数据：
- en: '[PRE33]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding code displays the data from the `MARKETING_ADDITIONAL` table:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码显示了`MARKETING_ADDITIONAL`表中的数据：
- en: '![Figure 3.16 – The MARKETING_ADDITIONAL table](img/B19923_03_16.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图3.16 – MARKETING_ADDITIONAL表](img/B19923_03_16.jpg)'
- en: Figure 3.16 – The MARKETING_ADDITIONAL table
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16 – MARKETING_ADDITIONAL表
- en: 'With that, the table has been loaded into the DataFrame. Let’s look at the
    number of rows in our original and appended tables:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，该表已加载到DataFrame中。让我们看看原始表和附加表的行数：
- en: '[PRE34]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This code displays the total number of rows in the `MARKETING_ADDITIONAL` and
    `PURCHASE_HISTORY` tables:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码显示了`MARKETING_ADDITIONAL`和`PURCHASE_HISTORY`表中的总行数：
- en: '![Figure 3.17 – Data row count](img/B19923_03_17.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图3.17 – 数据行数](img/B19923_03_17.jpg)'
- en: Figure 3.17 – Data row count
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17 – 数据行数
- en: 'The `MARKETING_ ADDITIONAL` table contains 240 rows of new data that must be
    appended with the `PURCHASE_HISTORY` table, which contains 2,000 rows of data.
    Since the column names are identical, the data can be appended by using `union_by_name`:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`MARKETING_ADDITIONAL`表包含240行新数据，这些数据必须与包含2,000行数据的`PURCHASE_HISTORY`表合并。由于列名相同，可以使用`union_by_name`进行合并：'
- en: '[PRE35]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, the DataFrame contains the appended data. Let’s look at the number of
    rows in this DataFrame:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，DataFrame包含附加的数据。让我们看看这个DataFrame中的行数：
- en: '[PRE36]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The preceding code shows the final data that’s in the DataFrame:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码显示了DataFrame中的最终数据：
- en: '![Figure 3.18 – The MARKETING_FINAL table](img/B19923_03_18.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图3.18 – MARKETING_FINAL表](img/B19923_03_18.jpg)'
- en: Figure 3.18 – The MARKETING_FINAL table
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18 – MARKETING_FINAL表
- en: 'The total count of the rows is 2,240\. With that, the new data has been appended.
    Now, we will write this data into the `MARKETING_FINAL` table in Snowflake:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 总行数为2,240行。这样，新数据已经附加。现在，我们将这些数据写入Snowflake中的`MARKETING_FINAL`表：
- en: '[PRE37]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The `MARKETING_DATA` table is now available in Snowflake and can be consumed.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`MARKETING_DATA`表现在在Snowflake中可用并可被消费。'
- en: The difference between union and union_by_name
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`union`和`union_by_name`之间的区别'
- en: 'Two methods are available for combining data: **union_by_name** and **union**.
    Both methods allow multiple datasets to be merged, but they differ in their approach
    and functionality.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以合并数据：**union_by_name**和**union**。这两种方法都允许合并多个数据集，但它们在方法和功能上有所不同。
- en: The **union_by_name** method in Snowpark Python is specifically designed to
    combine datasets by matching and merging columns based on their names. This method
    ensures that the columns with the same name from different datasets are merged,
    creating a unified dataset. It is beneficial when you have datasets with similar
    column structures and want to consolidate them while preserving the column names.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark Python中的`union_by_name`方法专门设计用于通过匹配和合并列名来组合数据集。此方法确保来自不同数据集的具有相同名称的列被合并，创建一个统一的数据集。当您有具有相似列结构的数据集并希望合并它们同时保留列名时，这非常有用。
- en: On the other hand, the **union** method in Snowpark Python combines datasets
    by simply appending them vertically, regardless of column names or structures.
    This method concatenates the rows from one dataset with the rows from another,
    resulting in a single dataset with all the rows from both sources. The **union**
    method is suitable for stacking datasets vertically without considering column
    names or matching structures. However, note that in certain cases, the column
    type matters, such as when casting a string column to a numeric value.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Snowpark Python 中的 **union** 方法通过简单地垂直追加数据集来合并数据集，而不考虑列名或结构。此方法将一个数据集的行与另一个数据集的行连接起来，从而生成包含两个来源所有行的单个数据集。**union**
    方法适用于垂直堆叠数据集，而不考虑列名或匹配结构。然而，请注意，在某些情况下，列类型很重要，例如在将字符串列转换为数值类型时。
- en: Data grouping and analysis
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分组和分析
- en: Now that the data is ready and has been transformed, the next step is to see
    how we can group data to understand important patterns and analyze it. In this
    section, we will aggregate this data and analyze it.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经准备好并已转换，下一步是看看我们如何分组数据以了解重要的模式和进行分析。在本节中，我们将汇总这些数据并进行分析。
- en: Data grouping
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分组
- en: In data analysis, understanding patterns within datasets is crucial for gaining
    insights and making informed decisions. One powerful tool that aids in this process
    is the `group_by` function in Snowpark Python. This function allows us to group
    data based on specific criteria, enabling us to dissect and analyze the dataset
    in a structured manner.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析中，理解数据集中的模式对于获得见解和做出明智的决策至关重要。在这个过程中，一个强大的工具是 Snowpark Python 中的 `group_by`
    函数。此函数允许我们根据特定标准对数据进行分组，使我们能够以结构化的方式剖析和分析数据集。
- en: By utilizing the `group_by` function, we can uncover valuable insights into
    how data is distributed and correlated across different categories or attributes.
    For example, we can group sales data by product category to analyze sales trends,
    or group customer data by demographics to understand buying behavior.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用 `group_by` 函数，我们可以揭示数据在不同类别或属性中的分布和相关性方面的宝贵见解。例如，我们可以按产品类别对销售数据进行分组，以分析销售趋势，或按人口统计对客户数据进行分组，以了解购买行为。
- en: Furthermore, the `group_by` function can be combined with other data manipulation
    and visualization techniques to gain deeper insights. For instance, we can create
    visualizations such as bar charts or heatmaps to visually represent the aggregated
    data, making it easier to spot patterns and trends.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`group_by` 函数可以与其他数据处理和可视化技术结合使用，以获得更深入的见解。例如，我们可以创建条形图或热图等可视化，以直观地表示汇总数据，使其更容易发现模式和趋势。
- en: 'To facilitate grouping and conducting deeper analysis, we’ll utilize the `MARKETING_FINAL`
    table we established earlier:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于分组和进行更深入的分析，我们将利用之前建立的 `MARKETING_FINAL` 表：
- en: '[PRE38]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Here, we are loading the data from the `MARKETING_FINAL` table into the DataFrame.
    We will use this DataFrame to perform aggregations:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将从 `MARKETING_FINAL` 表中加载数据到 DataFrame 中。我们将使用此 DataFrame 来执行聚合：
- en: '[PRE39]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This returns the average income by `EDUCATION`. People with PhDs have the highest
    average income, and people with primary education have the lowest average income:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回了按 `EDUCATION` 的平均收入。拥有博士学位的人平均收入最高，而拥有初等教育的人平均收入最低：
- en: '![Figure 3.19 – Average income by education](img/B19923_03_19.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.19 – 按教育水平的平均收入](img/B19923_03_19.jpg)'
- en: Figure 3.19 – Average income by education
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.19 – 按教育水平的平均收入
- en: 'Now, we can create an alias for the column:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以为列创建一个别名：
- en: '[PRE40]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The average income is displayed as an alias – `AVG_INCOME`:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 平均收入以别名显示 – `AVG_INCOME`：
- en: '![Figure 3.20 – The AVG_INCOME alias](img/B19923_03_20.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.20 – AVG_INCOME 别名](img/B19923_03_20.jpg)'
- en: Figure 3.20 – The AVG_INCOME alias
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.20 – AVG_INCOME 别名
- en: 'We can also achieve similar results by using the `function()` method to pass
    the respective operation from Snowpark functions:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过使用 `function()` 方法传递 Snowpark 函数中的相应操作来达到类似的结果：
- en: '[PRE41]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This prints the following output:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印以下输出：
- en: '![Figure 3.21 – Sum of revenue by marital status](img/B19923_03_21.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.21 – 按婚姻状况的收益总和](img/B19923_03_21.jpg)'
- en: Figure 3.21 – Sum of revenue by marital status
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.21 – 按婚姻状况的收益总和
- en: 'Here, we can see that married customers generate the highest revenue. We can
    also use `agg()` to perform this particular aggregation. Let’s calculate the maximum
    income by marital status:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到已婚客户产生了最高的收入。我们还可以使用 `agg()` 来执行这种特定的聚合。让我们按婚姻状况计算最大收入：
- en: '[PRE42]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This generates the following output:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '![Figure 3.22 – Income by marital status](img/B19923_03_22.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.22 – 按婚姻状况划分的收入](img/B19923_03_22.jpg)'
- en: Figure 3.22 – Income by marital status
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.22 – 按婚姻状况划分的收入
- en: 'Here, we can see that customers who are together and married as a family have
    the maximum income to spend, and hence they generate the maximum revenue. Next,
    we will find the count of different types of graduates and their maximum income:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们可以看到一起作为家庭并已婚的客户有最高的收入用于消费，因此他们产生了最大的收入。接下来，我们将找出不同类型毕业生的数量及其最高收入：
- en: '[PRE43]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The preceding code produces the following output:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码产生了以下输出：
- en: '![Figure 3.23 – Count of category](img/B19923_03_23.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.23 – 类别计数](img/B19923_03_23.jpg)'
- en: Figure 3.23 – Count of category
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.23 – 类别计数
- en: Here, we can see that `PhD` has a maximum income of `162397`, and that people
    with `Basic` income have the lowest maximum income – that is, `34445`.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们可以看到 `PhD` 的最高收入为 `162397`，而 `Basic` 收入的人最高收入最低，即 `34445`。
- en: 'We can also perform complex multi-level aggregations in Snowpark. Let’s find
    out how people with different educations and marital statuses spend:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在 Snowpark 中执行复杂的多层次聚合。让我们找出不同教育水平和婚姻状况的人如何消费：
- en: '[PRE44]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Here’s the output:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 3.24 – Multi-level aggregation](img/B19923_03_24.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.24 – 多层次聚合](img/B19923_03_24.jpg)'
- en: Figure 3.24 – Multi-level aggregation
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.24 – 多层次聚合
- en: 'Let’s determine the relationship between `EDUCATION`, `MARITAL_STATUS`, and
    `SUM_PURCHASE`. People who are graduates and married spend the most compared to
    single people. We can also sort the results by using the `sort()` function:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确定 `EDUCATION`、`MARITAL_STATUS` 和 `SUM_PURCHASE` 之间的关系。与单身人士相比，已婚的毕业生花费最多。我们还可以使用
    `sort()` 函数对结果进行排序：
- en: '[PRE45]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Here’s the output:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 3.25 – Sorted result](img/B19923_03_25.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.25 – 排序结果](img/B19923_03_25.jpg)'
- en: Figure 3.25 – Sorted result
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.25 – 排序结果
- en: Here, we are sorting the results in ascending order by purchase amount after
    the aggregation is completed. The following section will cover some standard data
    analysis that can be performed on this data.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们在聚合完成后按购买金额升序排序结果。下一节将介绍可以在这些数据上执行的一些标准数据分析。
- en: Data analysis
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分析
- en: In the previous sections, we delved into data exploration, transformation, and
    aggregation, where we learned about various techniques we can use to find out
    what our data is all about and how we can combine different datasets. Armed with
    a solid foundation of general dataset exploration, we are ready to dive deeper
    into data analysis using Snowpark Python.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们深入探讨了数据探索、转换和聚合，学习了我们可以使用的各种技术来了解我们的数据是什么以及我们如何结合不同的数据集。在掌握了坚实的通用数据集探索基础后，我们准备更深入地使用
    Snowpark Python 进行数据分析。
- en: This section focuses on leveraging the power of statistical functions, sampling
    techniques, pivoting operations, and converting data into a pandas DataFrame for
    advanced analysis. We will explore applying statistical functions to extract meaningful
    information from our data. Then, we will learn about different sampling techniques
    to work efficiently with large datasets. Additionally, we will discover how to
    reshape our data using pivoting operations to facilitate in-depth analysis.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重点介绍利用统计函数、采样技术、交叉操作以及将数据转换为 pandas DataFrame 进行高级分析的能力。我们将探讨如何应用统计函数从我们的数据中提取有意义的信息。然后，我们将了解不同的采样技术以高效地处理大型数据集。此外，我们将发现如何使用交叉操作重塑我们的数据，以促进深入分析。
- en: Moreover, we will explore the seamless integration of Snowpark Python with pandas,
    a widely used data manipulation library. We will understand how to convert our
    Snowpark data into a pandas DataFrame, enabling us to leverage pandas’ extensive
    analytical and visualization capabilities.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将探讨 Snowpark Python 与广泛使用的数据处理库 pandas 的无缝集成。我们将了解如何将 Snowpark 数据转换为 pandas
    DataFrame，从而利用 pandas 的广泛分析和可视化功能。
- en: The following section provides a glimpse into the capabilities of Snowpark Python
    for data analysis; we will delve deeper into each topic in the subsequent chapter.
    Here, we aim to provide a foundational understanding of the key concepts and techniques
    of analyzing data using Snowpark Python. In the next chapter, we will explore
    these topics in greater detail, unraveling the full potential of Snowpark Python
    for data analysis.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分提供了 Snowpark Python 在数据分析方面的能力概述；我们将在下一章中深入探讨每个主题。在这里，我们旨在提供使用 Snowpark
    Python 分析数据的关键概念和技术的基础理解。在下一章中，我们将更详细地探讨这些主题，揭示 Snowpark Python 在数据分析方面的全部潜力。
- en: Describing the data
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 描述数据
- en: The first step in our analysis is understanding how our data is distributed.
    The `describe()` function in pandas is a valuable tool that helps us gain insights
    into the statistical properties of our numerical data. When we apply `describe()`
    to a DataFrame, it computes various descriptive statistics, including the count,
    mean, standard deviation, minimum, quartiles, and maximum values for each numerical
    column.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析的第一步是理解我们的数据是如何分布的。pandas 中的 `describe()` 函数是一个非常有用的工具，它帮助我们深入了解数值数据的统计特性。当我们对
    DataFrame 应用 `describe()` 时，它会计算各种描述性统计量，包括每个数值列的计数、平均值、标准差、最小值、四分位数和最大值。
- en: 'This summary comprehensively overviews our data’s distribution and central
    tendencies. By examining these statistics, we can quickly identify key characteristics,
    such as the range of values, the spread of the data, and any potential outliers.
    This initial exploration sets the stage for more advanced analysis techniques
    and allows us to make informed decisions based on a solid understanding of our
    dataset’s distribution:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 此总结全面概述了我们的数据分布和中心趋势。通过检查这些统计量，我们可以快速识别关键特征，例如值的范围、数据的分布以及任何潜在的异常值。这种初步探索为更高级的分析技术奠定了基础，并允许我们根据对数据集分布的深入了解做出明智的决策：
- en: '[PRE46]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The preceding code shows the data from the `MARKETING_FINAL` table:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码显示了 `MARKETING_FINAL` 表的数据：
- en: '![Figure 3.26 – MARKETING_FINAL DataFrame](img/B19923_03_26.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![图3.26 – MARKETING_FINAL DataFrame](img/B19923_03_26.jpg)'
- en: Figure 3.26 – MARKETING_FINAL DataFrame
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.26 – MARKETING_FINAL DataFrame
- en: The result shows the different columns and the data in the `MARKETING_FINAL`
    table.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示了 `MARKETING_FINAL` 表的不同列和数据。
- en: Finding distinct data
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查找独特数据
- en: 'In Snowpark DataFrames, the `distinct()` function is crucial in identifying
    unique values within a column or set of columns. When applied to a Snowpark DataFrame,
    `distinct()` eliminates duplicate records, resulting in a new DataFrame that contains
    only distinct values. This function is particularly useful for dealing with large
    datasets or extracting unique records for analysis or data processing:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Snowpark DataFrames 中，`distinct()` 函数对于在列或列集中识别唯一值至关重要。当应用于 Snowpark DataFrame
    时，`distinct()` 会消除重复记录，从而生成一个新的 DataFrame，其中只包含唯一值。此函数特别适用于处理大型数据集或提取用于分析或数据处理的分析记录：
- en: '[PRE47]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The preceding code shows the total count of the `MARKETING_FINAL` table:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码显示了 `MARKETING_FINAL` 表的总计数：
- en: '![Figure 3.27 – MARKETING_FINAL count](img/B19923_03_27.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图3.27 – MARKETING_FINAL 计数](img/B19923_03_27.jpg)'
- en: Figure 3.27 – MARKETING_FINAL count
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.27 – MARKETING_FINAL 计数
- en: In our case, the entire dataset is returned since we do not have any duplicate
    rows. `distinct()` preserves the original rows of the DataFrame and only filters
    out repeated values within the specified columns.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，由于我们没有重复行，整个数据集被返回。`distinct()` 函数保留 DataFrame 的原始行，并且只过滤掉指定列中的重复值。
- en: Dropping duplicates
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 删除重复项
- en: '`drop_duplicates()` removes duplicate rows from a Snowpark DataFrame. It analyzes
    the entire row and compares it with other rows in the DataFrame. If a row is found
    to be an exact duplicate of another row, `drop_duplicates()` will remove it, keeping
    only the first occurrence. By default, this function considers all columns in
    the DataFrame for duplicate detection:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '`drop_duplicates()` 函数从 Snowpark DataFrame 中删除重复行。它分析整行并与 DataFrame 中的其他行进行比较。如果发现某行是另一行的完全重复，`drop_duplicates()`
    将会删除它，只保留第一次出现。默认情况下，此函数将 DataFrame 中的所有列都考虑为重复检测的依据：'
- en: '[PRE48]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This will display the following output:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示以下输出：
- en: '![Figure 3.28 – Marketing duplicates removed](img/B19923_03_28.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![图3.28 – 已移除的营销重复项](img/B19923_03_28.jpg)'
- en: Figure 3.28 – Marketing duplicates removed
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.28 – 已移除的营销重复项
- en: Note that you can specify specific columns using the `subset` parameter to check
    for duplicates based on those columns alone. `drop_duplicates()` modifies the
    original DataFrame by removing duplicate rows.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您可以使用 `subset` 参数指定特定的列，仅基于这些列检查重复项。`drop_duplicates()` 方法通过删除重复行来修改原始 DataFrame。
- en: Crosstab analysis
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 列联分析
- en: 'Once we have identified the unique combinations of the `EDUCATION` and `MARITAL_STATUS`
    columns in our dataset, we might still be curious about how frequently each combination
    occurs. We can utilize the `crosstab` function to determine the occurrence of
    these unique combinations. By applying the `crosstab` function to our dataset,
    we can generate a cross-tabulation or contingency table that displays the frequency
    distribution of the unique combinations of `EDUCATION` and `MARITAL_STATUS`:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定了数据集中 `EDUCATION` 和 `MARITAL_STATUS` 列的唯一组合，我们可能会对每个组合出现的频率仍然感到好奇。我们可以利用
    `crosstab` 函数来确定这些唯一组合的出现次数。通过将 `crosstab` 函数应用于我们的数据集，我们可以生成一个交叉表或列联表，显示 `EDUCATION`
    和 `MARITAL_STATUS` 的唯一组合的频率分布：
- en: '[PRE49]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The preceding code shows the crosstab data in the DataFrame:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码显示了 DataFrame 中的列联表数据：
- en: '![Figure 3.29 – Crosstab data](img/B19923_03_29.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.29 – 列联表数据](img/B19923_03_29.jpg)'
- en: Figure 3.29 – Crosstab data
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.29 – 列联表数据
- en: This table provides a comprehensive overview of how often each unique combination
    occurs in the dataset, allowing us to gain valuable insights into the relationships
    between these variables. The `crosstab` function aids us in understanding the
    distribution and occurrence patterns of the unique combinations, further enhancing
    our data analysis capabilities.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 此表提供了关于数据集中每个唯一组合出现频率的全面概述，使我们能够深入了解这些变量之间的关系。`crosstab` 函数帮助我们理解唯一组合的分布和出现模式，进一步增强了我们的数据分析能力。
- en: Pivot analysis
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 旋转分析
- en: Upon using the `crosstab` function to examine the unique combinations of the
    `EDUCATION` and `MARITAL_STATUS` columns in our dataset, we might encounter certain
    combinations with zero occurrences. We can construct a pivot table to gain a more
    comprehensive understanding of the data and further investigate the relationships
    between these variables.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 `crosstab` 函数检查我们的数据集中 `EDUCATION` 和 `MARITAL_STATUS` 列的唯一组合时，我们可能会遇到某些零次出现的组合。我们可以构建一个透视表来更全面地了解数据，并进一步研究这些变量之间的关系。
- en: 'Constructing a pivot table allows us to summarize and analyze the data more
    dynamically and flexibly. Unlike the `crosstab` function, which only provides
    the frequency distribution of unique combinations, a pivot table allows us to
    explore additional aggregate functions, such as sum, average, or maximum values.
    This enables us to delve deeper into the dataset and obtain meaningful insights:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 构建透视表使我们能够更动态和灵活地总结和分析数据。与仅提供唯一组合频率分布的 `crosstab` 函数不同，透视表允许我们探索额外的聚合函数，如总和、平均值或最大值。这使得我们能够更深入地研究数据集，并获得有意义的见解：
- en: '[PRE50]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The preceding code shows the data in the DataFrame:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码显示了 DataFrame 中的数据：
- en: '![Figure 3.30 – Pivot table](img/B19923_03_30.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.30 – 透视表](img/B19923_03_30.jpg)'
- en: Figure 3.30 – Pivot table
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.30 – 透视表
- en: By constructing a pivot table for the `EDUCATION` and `MARITAL_STATUS` columns,
    we can uncover the occurrence counts and various statistical measures or calculations
    associated with each combination. This expanded analysis provides a more comprehensive
    view of the data and allows for a more nuanced and detailed exploration.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为 `EDUCATION` 和 `MARITAL_STATUS` 列构建透视表，我们可以揭示每个组合的出现次数以及与每个组合相关的各种统计度量或计算。这种扩展分析提供了对数据的更全面视图，并允许进行更细致和详细的研究。
- en: Note
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When the **crosstab** function displays zero occurrences for certain combinations
    of variables, it is essential to note that those combinations will be represented
    as **NULL** values instead of zeros when constructing a pivot table.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 当**列联**函数显示某些变量的组合为零次出现时，需要注意的是，在构建透视表时，这些组合将用**NULL**值表示，而不是零。
- en: Unlike **crosstab**, which explicitly highlights zero counts for combinations
    absent in the dataset, a pivot table considers all possible combinations of the
    variables. Consequently, if a variety does not exist in the dataset, the corresponding
    cell in the pivot table will be represented as a **NULL** value rather than a
    zero.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 与**列联**不同，后者明确突出显示数据集中不存在的组合的零计数，透视表考虑了变量的所有可能组合。因此，如果数据集中不存在某种组合，透视表中的相应单元格将用**NULL**值表示，而不是零。
- en: The presence of **NULL** values in the pivot table highlights the absence of
    data for those particular combinations. Interpreting and handling these **NULL**
    values appropriately during subsequent data analysis processes, such as data cleaning,
    imputation, or further statistical calculations, is essential.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 数据透视表中**NULL**值的存在突出了那些特定组合数据缺失的情况。在后续数据分析过程中，如数据清理、插补或进一步的统计计算中，适当地解释和处理这些**NULL**值是至关重要的。
- en: Dropping missing values
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 删除缺失值
- en: 'The `dropna()` function in pandas is a powerful tool for handling missing values
    in a DataFrame. In this case, we will be utilizing the `dropna()` functionality
    of Snowpark, which allows us to remove rows or columns that contain missing or
    `NULL` values, helping to ensure the integrity and accuracy of our data. The `dropna()`
    function offers several parameters that provide flexibility in controlling the
    operation’s behavior:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: pandas中的`dropna()`函数是处理DataFrame中缺失值的有力工具。在这种情况下，我们将利用Snowpark的`dropna()`功能，它允许我们删除包含缺失或`NULL`值的行或列，有助于确保数据的完整性和准确性。`dropna()`函数提供了几个参数，提供了在控制操作行为方面的灵活性：
- en: '[PRE51]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The preceding code shows the data with the applied filter from the DataFrame:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码显示了DataFrame中应用了过滤器的数据：
- en: '![Figure 3.31 – Pivot table – dropna()](img/B19923_03_31.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![图3.31 – 数据透视表 – dropna()](img/B19923_03_31.jpg)'
- en: Figure 3.31 – Pivot table – dropna()
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.31 – 数据透视表 – dropna()
- en: 'The `how` parameter determines the criteria that are used to drop rows or columns.
    It accepts the input as `any` and `all`: `any` drops the row or column if it contains
    any missing value, and `all` drops the row or column only if all its values are
    missing.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '`how`参数决定了用于删除行或列的准则。它接受`any`和`all`作为输入：`any`会在行或列包含任何缺失值时删除该行或列，而`all`只有在所有值都缺失时才会删除行或列。'
- en: 'The `thresh` parameter specifies the minimum number of non-null values required
    to keep a row or column. The row or column is dropped if the *non-null values
    exceed* the threshold:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '`thresh`参数指定了保留行或列所需的最小非空值数量。如果非空值超过阈值，则删除行或列：'
- en: '[PRE52]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The preceding code shows the data with the applied filter from the DataFrame:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码显示了DataFrame中应用了过滤器的数据：
- en: '![Figure 3.32 – Pivot threshold](img/B19923_03_32.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![图3.32 – 数据透视阈值](img/B19923_03_32.jpg)'
- en: Figure 3.32 – Pivot threshold
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.32 – 数据透视阈值
- en: 'The `subset` parameter allows us to specify a subset of columns or rows for
    missing value removal. It accepts a list of column or row labels. By default,
    `dropna()` checks all columns or rows for missing values. However, with a subset,
    we can focus on specific columns or rows for the operation:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '`subset`参数允许我们指定用于缺失值删除的列或行的子集。它接受列或行标签的列表。默认情况下，`dropna()`检查所有列或行中的缺失值。然而，使用子集，我们可以专注于特定列或行进行操作：'
- en: '[PRE53]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The preceding code drops any rows from the `market_pivot` DataFrame where the
    `Graduation` column has missing values and then displays the resulting DataFrame:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码从`market_pivot` DataFrame中删除了`Graduation`列有缺失值的任何行，然后显示了结果DataFrame：
- en: '![Figure 3.33 – Pivot subset](img/B19923_03_33.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![图3.33 – 数据透视子集](img/B19923_03_33.jpg)'
- en: Figure 3.33 – Pivot subset
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.33 – 数据透视子集
- en: This shows the data with the applied filter from the DataFrame.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了应用了过滤器的DataFrame中的数据。
- en: Note
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When working with pivot tables, it is crucial to handle **NULL** values appropriately
    because they can impact the accuracy and reliability of subsequent analyses. This
    allows us to ensure that we have complete data for further analysis and calculations.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用数据透视表时，适当地处理**NULL**值至关重要，因为它们可能会影响后续分析的准确性和可靠性。这使我们能够确保我们有完整的数据用于进一步的分析和计算。
- en: Having **NULL** values in the pivot result can lead to incorrect interpretations
    or calculations since **NULL** values can propagate through the analysis and affect
    subsequent aggregations, statistics, or visualizations. By replacing **NULL**
    values with a specific value, such as 0, we can provide a meaningful representation
    of the data in the pivot table, allowing us to perform reliable analysis and make
    informed decisions based on complete information.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据透视结果中存在**NULL**值可能会导致错误的解释或计算，因为**NULL**值可能会在分析中传播并影响后续的聚合、统计或可视化。通过将**NULL**值替换为特定值，例如0，我们可以在数据透视表中提供有意义的数值表示，从而允许我们进行可靠的分析并基于完整信息做出明智的决策。
- en: Filling missing values
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 填充缺失值
- en: 'The `fillna()` function allows us to replace null values with specific values
    or apply various techniques for imputation. It also allows us to fill in the missing
    values in a DataFrame, ensuring that we maintain the integrity of the data structure.
    We can specify the values for filling nulls, such as a constant value, or values
    derived from statistical calculations such as mean, median, or mode. The `fillna()`
    function is useful when we’re treating null values while considering the data’s
    nature and the desired analysis:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '`fillna()` 函数允许我们用特定值替换空值或应用各种插补技术。它还允许我们在 DataFrame 中填充缺失值，确保我们保持数据结构的完整性。我们可以指定用于填充空值的值，例如一个常数，或者从统计计算中得出的值，如平均值、中位数或众数。当我们在考虑数据的性质和期望的分析时处理空值时，`fillna()`
    函数非常有用：'
- en: '[PRE54]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The preceding code fills any null values in the `market_pivot` DataFrame with
    a value of `0` and then displays the resulting DataFrame:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将 `market_pivot` DataFrame 中的任何空值填充为 `0`，然后显示结果 DataFrame：
- en: '![Figure 3.34 – Missing values](img/B19923_03_34.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.34 – 缺失值](img/B19923_03_34.jpg)'
- en: Figure 3.34 – Missing values
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.34 – 缺失值
- en: This is a handy function that fills in missing values that need to be used for
    calculations.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个方便的功能，用于填充需要用于计算的缺失值。
- en: Variable interaction
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变量交互
- en: 'The `corr()` function calculates the correlation coefficient, which measures
    the strength and direction of the linear relationship between two variables. It
    returns a value between -1 and 1, where -1 represents a perfect negative correlation,
    1 illustrates a perfect positive correlation, and 0 indicates no linear correlation:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '`corr()` 函数计算相关系数，它衡量两个变量之间线性关系的强度和方向。它返回一个介于 -1 和 1 之间的值，其中 -1 表示完美的负相关，1
    表示完美的正相关，0 表示没有线性相关：'
- en: '[PRE55]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'By executing this code, we obtain the correlation coefficient between the `INCOME`
    and `NUMSTOREPURCHASES` columns, providing insights into the potential relationship
    between income levels and the number of store purchases in the dataset:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码后，我们获得 `INCOME` 和 `NUMSTOREPURCHASES` 列之间的相关系数，从而提供了关于收入水平和数据集中商店购买数量之间潜在关系的见解：
- en: '![Figure 3.35 – Correlation value](img/B19923_03_35.jpg)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.35 – 相关值](img/B19923_03_35.jpg)'
- en: Figure 3.35 – Correlation value
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.35 – 相关值
- en: 'The `cov()` function, on the other hand, calculates the covariance, which measures
    the degree of association between two variables without normalizing for scale:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`cov()` 函数计算协方差，它衡量两个变量之间关联的程度，而不对规模进行归一化：
- en: '[PRE56]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Here’s the output:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 3.36 – Covariance value](img/B19923_03_36.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.36 – 协方差值](img/B19923_03_36.jpg)'
- en: Figure 3.36 – Covariance value
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.36 – 协方差值
- en: The covariance between the `INCOME` and `NUMSTOREPURCHASES` columns helps us
    understand how changes in income levels correspond to changes in the number of
    store purchases in the dataset.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '`INCOME` 和 `NUMSTOREPURCHASES` 列之间的协方差帮助我们了解收入水平的变化如何与数据集中商店购买数量的变化相对应。'
- en: Note
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: While both **corr()** and **cov()** help analyze relationships between variables,
    it is essential to note that in Snowpark Python, these functions only support
    the analysis of two variables at a time. This limitation means we can only calculate
    the correlation or covariance between two columns in a DataFrame, and not simultaneously
    across multiple variables. Additional techniques or functions may be required
    to overcome this limitation and perform correlation or covariance analysis for
    various variables.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 **corr()** 和 **cov()** 都有助于分析变量之间的关系，但重要的是要注意，在 Snowpark Python 中，这些函数仅支持同时分析两个变量。这种限制意味着我们只能计算
    DataFrame 中两列之间的相关性或协方差，而不能跨多个变量同时进行。可能需要额外的技术或函数来克服这种限制，并执行多个变量的相关性或协方差分析。
- en: Operating with pandas DataFrame
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 pandas DataFrame 操作
- en: Converting a Snowpark DataFrame into a pandas DataFrame is a valuable step that
    opens up a wide range of analysis capabilities. Snowpark provides seamless integration
    with pandas, allowing us to leverage pandas’ extensive data manipulation, analysis,
    and visualization functionalities. By converting a Snowpark DataFrame into a pandas
    DataFrame, we gain access to a vast ecosystem of tools and libraries that are
    designed explicitly for data analysis.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Snowpark DataFrame 转换为 pandas DataFrame 是一个有价值的步骤，它为分析能力打开了广泛的大门。Snowpark
    提供了与 pandas 的无缝集成，使我们能够利用 pandas 的广泛的数据操作、分析和可视化功能。通过将 Snowpark DataFrame 转换为
    pandas DataFrame，我们获得了专门为数据分析设计的工具和库的庞大生态系统。
- en: This transition enables us to leverage pandas’ rich functions and methods, such
    as statistical calculations, advanced filtering, grouping operations, and time
    series analysis. pandas also provide many visualization options, such as generating
    insightful plots, charts, and graphs that are more accessible, to visualize the
    data. With pandas, we can create meaningful visual representations of our data,
    facilitating the exploration of patterns, trends, and relationships. Additionally,
    working with pandas allows us to utilize its extensive community support and resources.
    The pandas library has a vast user community, making finding documentation, tutorials,
    and helpful discussions on specific data analysis tasks more accessible.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这种转换使我们能够利用 pandas 丰富的函数和方法，例如统计计算、高级过滤、分组操作和时间序列分析。pandas 还提供了许多可视化选项，例如生成有洞察力的图表、图表和图形，使数据可视化更加容易访问。使用
    pandas，我们可以创建有意义的可视化数据表示，便于探索模式、趋势和关系。此外，使用 pandas 允许我们利用其广泛的社区支持和资源。pandas 库拥有庞大的用户社区，这使得查找特定数据分析任务的文档、教程和有帮助的讨论变得更加容易。
- en: Limitations of pandas DataFrames
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: pandas DataFrame 的局限性
- en: Converting a Snowpark DataFrame into a pandas DataFrame can have its limitations,
    mainly when dealing with large datasets. The primary constraint is memory consumption
    as converting the entire dataset simultaneously may exceed available memory resources.
    This can hinder the analysis process and potentially lead to system crashes or
    performance issues.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Snowpark DataFrame 转换为 pandas DataFrame 可能存在局限性，尤其是在处理大型数据集时。主要约束是内存消耗，因为同时转换整个数据集可能会超过可用的内存资源。这可能会阻碍分析过程，并可能导致系统崩溃或性能问题。
- en: However, these limitations can be mitigated by breaking the DataFrame into batches
    and sampling the data. We’ll discuss this shortly.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些局限性可以通过将 DataFrame 分批处理和采样数据来缓解。我们将在稍后讨论这一点。
- en: Data analysis using pandas
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 pandas 进行数据分析
- en: 'Converting a Snowpark DataFrame into a pandas DataFrame empowers us to seamlessly
    transition from Snowpark’s powerful data processing capabilities to pandas’ feature-rich
    environment. This interoperability expands our analytical possibilities and enables
    us to perform advanced analysis and gain deeper insights from our data:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Snowpark DataFrame 转换为 pandas DataFrame 使我们能够无缝地从 Snowpark 强大的数据处理能力过渡到 pandas
    丰富的环境。这种互操作性扩展了我们的分析可能性，并使我们能够执行高级分析，从数据中获得更深入的见解：
- en: '[PRE57]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The preceding code converts the `marketing_final` Snowpark DataFrame into a
    pandas DataFrame, allowing us to work with the data using pandas’ extensive data
    analysis and manipulation functionalities. It will print out the following output:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将 `marketing_final` Snowpark DataFrame 转换为 pandas DataFrame，使我们能够使用 pandas
    的广泛数据分析和操作功能来处理数据。它将输出以下内容：
- en: '![Figure 3.37 – The resulting pandas DataFrame](img/B19923_03_37.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.37 – 结果 pandas DataFrame](img/B19923_03_37.jpg)'
- en: Figure 3.37 – The resulting pandas DataFrame
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.37 – 结果 pandas DataFrame
- en: This shows the data that has been converted into the pandas DataFrame.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了已转换为 pandas DataFrame 的数据。
- en: Correlation in pandas
  id: totrans-347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: pandas 中的相关性
- en: 'In pandas, calculating correlations among multiple columns is straightforward:
    it involves selecting the desired columns and applying the `corr()` function.
    It generates a correlation matrix, allowing us to examine the relationships between
    each pair of columns simultaneously:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在 pandas 中，计算多个列之间的相关性很简单：涉及选择所需的列并应用 `corr()` 函数。它生成一个相关性矩阵，使我们能够同时检查每一对列之间的关系：
- en: '[PRE58]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The preceding code calculates the correlation matrix among the `INCOME`, `KIDHOME`,
    and `RECENCY` columns in the `pandas_df` pandas DataFrame. It computes the pairwise
    correlation coefficients between these columns, providing insights into their
    relationships. The output is as follows:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码计算 `pandas_df` pandas DataFrame 中 `INCOME`、`KIDHOME` 和 `RECENCY` 列之间的相关性矩阵。它计算这些列之间的成对相关系数，提供对这些关系见解。输出如下：
- en: '![Figure 3.38 – Pandas correlation](img/B19923_03_38.jpg)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.38 – Pandas 相关性](img/B19923_03_38.jpg)'
- en: Figure 3.38 – Pandas correlation
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.38 – Pandas 相关性
- en: Next, we’ll look at frequency distribution.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看频率分布。
- en: Frequency distribution
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 频率分布
- en: 'Calculating the frequency of values in a single column is simpler in pandas
    than in Snowpark Python. We can quickly obtain the frequency distribution in pandas
    by using the `value_counts()` function on a specific column. It returns a Series
    with unique values as indices and their corresponding counts as values. This concise
    method allows us to quickly understand the distribution and prevalence of each
    unique value in the column. On the other hand, in Snowpark Python, obtaining the
    frequency of values in a single column requires more steps and additional coding.
    We typically need to group the DataFrame by the desired column and then perform
    aggregation operations to count the occurrences of each unique value. Although
    this can be achieved in Snowpark Python, it involves more complex syntax and multiple
    transformations, making the process more cumbersome compared to pandas:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在 pandas 中计算单列的值频率比在 Snowpark Python 中简单。我们可以通过在特定列上使用 `value_counts()` 函数快速获取
    pandas 中的频率分布。它返回一个 Series，其中唯一值作为索引，其对应的计数作为值。这种简洁的方法使我们能够快速了解列中每个唯一值的分布和普遍性。另一方面，在
    Snowpark Python 中，获取单列的值频率需要更多步骤和额外的编码。我们通常需要按所需的列对 DataFrame 进行分组，然后执行聚合操作以计算每个唯一值的出现次数。尽管在
    Snowpark Python 中可以实现这一点，但它涉及更复杂的语法和多个转换，使得与 pandas 相比，这个过程更加繁琐：
- en: '[PRE59]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '`frequency = pandas_df.EDUCATION.value_counts()` calculates the frequency distribution
    of unique values in the `EDUCATION` column of the `pandas_df` pandas DataFrame
    and assigns the result to the `frequency` variable. The output is as follows:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '`frequency = pandas_df.EDUCATION.value_counts()` 计算了 pandas DataFrame 中 `pandas_df`
    的 `EDUCATION` 列中唯一值的频率分布，并将结果赋值给 `frequency` 变量。输出如下：'
- en: '![Figure 3.39 – Pandas data frequency](img/B19923_03_39.jpg)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.39 – Pandas 数据频率](img/B19923_03_39.jpg)'
- en: Figure 3.39 – Pandas data frequency
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.39 – Pandas 数据频率
- en: This shows the data frequency values in the pandas DataFrame.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了 pandas DataFrame 中的数据频率值。
- en: Visualization in pandas
  id: totrans-361
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: pandas 中的可视化
- en: Creating visualizations is made easy with pandas due to its seamless integration
    with popular visualization libraries such as Matplotlib and Seaborn. pandas provides
    a simple and intuitive interface to generate various visualizations, including
    line plots, bar charts, histograms, scatter plots, and more.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 由于与流行的可视化库（如 Matplotlib 和 Seaborn）的无缝集成，使用 pandas 创建可视化变得非常简单。pandas 提供了一个简单直观的界面来生成各种可视化，包括线图、条形图、直方图、散点图等等。
- en: 'By leveraging pandas’ built-in plotting functions, we can effortlessly transform
    our data into insightful visual representations, enabling us to explore patterns,
    trends, and relationships within our dataset. With just a few lines of code, pandas
    *empowers* us to produce visually appealing and informative plots, facilitating
    the communication and interpretation of our data:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用 pandas 内置的绘图函数，我们可以轻松地将我们的数据转换为有洞察力的视觉表示，使我们能够探索数据集中的模式、趋势和关系。只需几行代码，pandas
    就能让我们产生视觉上吸引人且信息丰富的图表，从而促进我们数据的沟通和解读：
- en: '[PRE60]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The preceding code creates a horizontal bar plot from the frequency distribution
    data stored in the `frequency` variable, where each unique value is represented
    by a bar with a length proportional to its count, and the plot has a customized
    size of 8 inches in width and 3 inches in height:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码从存储在 `frequency` 变量中的频率分布数据创建了一个水平条形图，其中每个唯一值由一个长度与其计数成比例的条形表示，该图宽度为 8 英寸，高度为
    3 英寸的自定义尺寸：
- en: '![Figure 3.40 – Frequency plot](img/B19923_03_40.jpg)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.40 – 频率图](img/B19923_03_40.jpg)'
- en: Figure 3.40 – Frequency plot
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.40 – 频率图
- en: 'Similarly, we can generate a Hexbin plot by changing `kind` to `hexbin`:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以通过将 `kind` 改为 `hexbin` 生成 Hexbin 图：
- en: '[PRE61]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The preceding code creates a Hexbin plot that visualizes the relationship between
    the `INCOME` and `MNTGOLDPRODS` columns in the `pandas_df` pandas DataFrame:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码创建了一个 Hexbin 图，用于可视化 `pandas_df` pandas DataFrame 中 `INCOME` 和 `MNTGOLDPRODS`
    列之间的关系：
- en: '![Figure 3.41 – Hexbin plot](img/B19923_03_41.jpg)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.41 – Hexbin 图](img/B19923_03_41.jpg)'
- en: Figure 3.41 – Hexbin plot
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.41 – Hexbin 图
- en: Here, the *X*-axis represents income values and the *Y*-axis represents the
    number of gold products. The plot is limited to X-axis limits of 0 to 100,000
    and Y-axis limits of 0 to 100, with a customized size of 8 inches in width and
    3 inches in height.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*X* 轴表示收入值，*Y* 轴表示黄金产品的数量。该图限制在 X 轴范围为 0 到 100,000，Y 轴范围为 0 到 100，宽度为 8
    英寸，高度为 3 英寸的自定义尺寸。
- en: Breaking a DataFrame into batches
  id: totrans-374
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将 DataFrame 分成批次
- en: 'The `to_pandas_batches()` function converts a Snowpark DataFrame into multiple
    smaller pandas DataFrames to be processed in batches. This approach reduces memory
    usage by converting the data into manageable portions, enabling efficient analysis
    of large datasets:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '`to_pandas_batches()` 函数将 Snowpark DataFrame 转换为多个较小的 pandas DataFrame，以便批量处理。这种方法通过将数据转换为可管理的部分来减少内存使用，从而能够高效地分析大型数据集：'
- en: '[PRE62]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Here’s the output:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '![Figure 3.42 – DataFrame batches](img/B19923_03_42.jpg)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.42 – DataFrame 批量](img/B19923_03_42.jpg)'
- en: Figure 3.42 – DataFrame batches
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.42 – DataFrame 批量
- en: The preceding code demonstrates how to analyze a large dataset in batches using
    the `to_pandas_batches()` function in Snowpark Python. By iterating over the `to_pandas_batches()`
    function, the code processes the dataset in manageable batches rather than loading
    the entire dataset into memory at once. In each iteration, a batch of the dataset
    is converted into a pandas DataFrame and stored in the `batch` variable. The `print(batch.shape)`
    statement provides the shape of each batch, indicating the number of rows and
    columns in that specific batch.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了如何使用 Snowpark Python 中的 `to_pandas_batches()` 函数批量分析大型数据集。通过遍历 `to_pandas_batches()`
    函数，代码以可管理的批量处理数据集，而不是一次性将整个数据集加载到内存中。在每次迭代中，数据集的一个批次被转换为 pandas DataFrame 并存储在
    `batch` 变量中。`print(batch.shape)` 语句提供了每个批次的形状，指示该特定批次的行数和列数。
- en: Analyzing the dataset in batches allows for more efficient memory utilization,
    enabling us to process large datasets that might otherwise exceed available memory
    resources. This approach facilitates the analysis of large datasets by breaking
    them into smaller, more manageable portions, allowing for faster computations
    and reducing the risk of memory-related issues.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 以批量的方式分析数据集可以更有效地利用内存，使我们能够处理可能超出可用内存资源的大型数据集。这种方法通过将数据集分解为更小、更易于管理的部分，从而便于快速计算并降低与内存相关问题的风险。
- en: Sampling a DataFrame
  id: totrans-382
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 采样 DataFrame
- en: 'The `sample()` function in Snowpark Python allows us to retrieve a random subset
    of data from the Snowpark DataFrame. By specifying the desired fraction or number
    of rows, we can efficiently extract a representative sample for analysis. This
    technique reduces the memory footprint required for conversion and subsequent
    analysis while providing meaningful insights:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark Python 中的 `sample()` 函数允许我们从 Snowpark DataFrame 中检索随机子集的数据。通过指定所需的分数或行数，我们可以有效地提取用于分析的代表性样本。这种技术减少了转换和后续分析所需的内存占用，同时提供了有意义的见解：
- en: '[PRE63]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Here’s the output:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '![Figure 3.43 – Sampling data](img/B19923_03_43.jpg)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.43 – 采样数据](img/B19923_03_43.jpg)'
- en: Figure 3.43 – Sampling data
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.43 – 采样数据
- en: The preceding code selects a random sample of 50% of the rows from the `marketing_final`
    DataFrame and assigns it to the `sample_df` DataFrame. The final count step produces
    slightly different output each time you run the code segment as it involves sampling
    the original table. The subsequent `sample_df.count()` function calculates the
    count of non-null values in each column of the `sample_df` DataFrame.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码从 `marketing_final` DataFrame 中选择 50% 的随机样本并将其分配给 `sample_df` DataFrame。每次运行代码段时，最终计数步骤都会产生略微不同的输出，因为它涉及到对原始表的采样。随后的
    `sample_df.count()` 函数计算 `sample_df` DataFrame 中每列的非空值计数。
- en: By utilizing the methods we covered here in Snowpark Python, we can overcome
    the limitations of converting large Snowpark DataFrames into pandas DataFrames,
    allowing for practical analysis while efficiently managing memory resources. These
    functions provide flexibility and control, enabling us to work with sizable datasets
    in a manageable and optimized manner.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在 Snowpark Python 中使用我们介绍的方法，我们可以克服将大型 Snowpark DataFrame 转换为 pandas DataFrame
    的限制，从而在有效管理内存资源的同时进行实际分析。这些函数提供了灵活性和控制力，使我们能够以可管理和优化的方式处理大型数据集。
- en: Summary
  id: totrans-390
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Snowpark provides different data processing capabilities and supports various
    techniques. It provides us with an easy and versatile way to ingest different
    structured and unstructured file formats, and Snowpark’s DataFrames support various
    data transformation and analysis operations. We covered various Snowpark session
    variables and different data operations that can be performed using Snowpark.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: Snowpark 提供了不同的数据处理能力并支持各种技术。它为我们提供了一种简单且通用的方式来摄取不同的结构化和非结构化文件格式，并且 Snowpark
    的 DataFrame 支持各种数据转换和分析操作。我们介绍了 Snowpark 会话变量以及可以使用 Snowpark 执行的不同数据操作。
- en: In the next chapter, we will cover how to build data engineering pipelines with
    Snowpark.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍如何使用Snowpark构建数据工程管道。
