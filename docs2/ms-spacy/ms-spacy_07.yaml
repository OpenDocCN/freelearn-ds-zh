- en: 'Chapter 5: Working with Word Vectors and Semantic Similarity'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章：处理词向量和语义相似度
- en: '**Word vectors** are handy tools and have been the hot topic of NLP for almost
    a decade. A word vector is basically a dense representation of a word. What''s
    surprising about these vectors is that semantically similar words have similar
    word vectors. Word vectors are great for semantic similarity applications, such
    as calculating the similarity between words, phrases, sentences, and documents.
    At a word level, word vectors provide information about synonymity, semantic analogies,
    and more. We can build semantic similarity applications by using word vectors.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**词向量**是方便的工具，并且几乎有十年时间是自然语言处理的热门话题。词向量基本上是一个词的密集表示。这些向量令人惊讶的地方在于，语义相似的词具有相似的词向量。词向量非常适合语义相似度应用，例如计算词、短语、句子和文档之间的相似度。在词级别上，词向量提供了关于同义性、语义类比等方面的信息。我们可以通过使用词向量来构建语义相似度应用。'
- en: Word vectors are produced by algorithms that make use of the fact that similar
    words appear in similar contexts. To capture the meaning of a word, a word vector
    algorithm collects information about the surrounding words that the target word
    appears with. This paradigm of capturing semantics for words by their surrounding
    words is called **distributional semantics**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量是由利用相似词出现在相似上下文中的事实的算法产生的。为了捕捉一个词的意义，词向量算法收集了目标词出现的周围词的信息。通过周围词捕捉词的语义的这种范例被称为**分布语义**。
- en: In this chapter, we will introduce the **distributional semantics paradigm**
    and its associated **semantic similarity methods**. We will start by taking a
    conceptual look at **text vectorization** so that you know what NLP problems word
    vectors solve.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍**分布语义范式**及其相关的**语义相似度方法**。我们将首先从概念上了解**文本向量化**，以便你知道词向量解决哪些NLP问题。
- en: Next, we will become familiar with word vector computations such as **distance
    calculation**, **analogy calculations**, and **visualization**. Then, we will
    learn how to benefit from spaCy's pretrained word vectors, as well as import and
    use third-party vectors. Finally, we will go through advanced semantic similarity
    methods using spaCy.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将熟悉词向量计算，如**距离计算**、**类比计算**和**可视化**。然后，我们将学习如何从spaCy的预训练词向量中受益，以及如何导入和使用第三方向量。最后，我们将通过spaCy深入了解高级语义相似度方法。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Understanding word vectors
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解词向量
- en: Using spaCy's pretrained vectors
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用spaCy的预训练向量
- en: Using third-party word vectors
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用第三方词向量
- en: Advanced semantic similarity methods
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级语义相似度方法
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we have used some external Python libraries besides spaCy
    for code visualization purposes. If you want to generate word vector visualizations
    in this chapter, you will need the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们除了spaCy之外还使用了某些外部Python库来进行代码可视化。如果您想在本章中生成词向量可视化，您将需要以下库：
- en: NumPy
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: scikit-learn
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn
- en: Matplotlib
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib
- en: 'You can find this chapter''s code in this book''s GitHub repository: [https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter05](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter05).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此书的GitHub仓库中找到本章的代码：[https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter05](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter05).
- en: Understanding word vectors
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解词向量
- en: The invention of word vectors (or **word2vec**) has been one of the most thrilling
    advancements in the NLP world. Those of you who are practicing NLP have definitely
    heard of word vectors at some point. This chapter will help you understand the
    underlying idea that caused the invention of word2vec, what word vectors look
    like, and how to use them in NLP applications.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量（或**word2vec**）的发明是自然语言处理领域最令人激动的进步之一。那些在实践自然语言处理的你们肯定在某一点上听说过词向量。本章将帮助你们理解导致词2vec发明的底层理念，了解词向量是什么样的，以及如何在自然语言处理应用中使用它们。
- en: The statistical world works with numbers, and all statistical methods, including
    statistical NLP algorithms, work with vectors. As a result, while working with
    statistical methods, we need to represent every real-world quantity as a vector,
    including text. In this section, we will learn about the different ways we can
    represent text as vectors and discover how word vectors provide semantic representation
    for words.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 统计世界使用数字，包括统计自然语言处理算法在内的所有统计方法都使用向量。因此，在处理统计方法时，我们需要将每个现实世界的数量表示为向量，包括文本。在本节中，我们将了解我们可以用不同的方式将文本表示为向量，并发现词向量如何为单词提供语义表示。
- en: 'We will start by discovering text vectorization by covering the simplest implementation
    possible: one-hot encoding.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先通过涵盖最简单的实现方式——独热编码——来发现文本向量化。
- en: One-hot encoding
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独热编码
- en: '**One-hot encoding** is a simple and straightforward way to assign vectors
    to words: assign an index value to each word in the vocabulary and then encode
    this value into a **sparse vector**. Let''s look at an example. Here, we will
    consider the vocabulary of a pizza ordering application; we can assign an index
    to each word in the order they appear in the vocabulary:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**独热编码**是将向量分配给单词的一种简单直接的方法：给词汇表中的每个单词分配一个索引值，然后将此值编码到一个**稀疏向量**中。让我们看一个例子。在这里，我们将考虑一个披萨订购应用的词汇表；我们可以按照它们在词汇表中出现的顺序给每个单词分配一个索引：'
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, the vector of a vocabulary word will be 0, except for the position of
    the word''s corresponding index value:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，词汇表中单词的向量将为0，除了单词对应索引值的位：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we can represent a sentence as a matrix, where each row corresponds to
    one word. For example, the sentence *I want a pizza* can be represented by the
    following matrix:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将一个句子表示为一个矩阵，其中每一行对应一个单词。例如，句子*I want a pizza*可以表示为以下矩阵：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As we can see from the preceding vocabulary and indices, the length of the vectors
    is equal to the number of the words in the vocabulary. Each dimension is devoted
    to one word explicitly. When we apply one-hot encoding vectorization to our text,
    each word is replaced by its vector and the sentence is transformed into a `(N,
    V)` matrix, where `N` is the number of words in the sentence and `V` is the vocabulary's
    size.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们从先前的词汇和索引中可以看到，向量的长度等于词汇表中的单词数量。每个维度都专门对应一个单词。当我们对文本应用独热编码向量化时，每个单词都被其向量所替代，句子被转换成一个
    `(N, V)` 矩阵，其中 `N` 是句子中的单词数量，`V` 是词汇表的大小。
- en: 'This way of representing text is straightforward to compute, as well as easy
    to debug and understand. This looks good so far, but there are some potential
    problems here, such as the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示文本的方式计算简单，调试和理解也容易。到目前为止看起来不错，但这里存在一些潜在问题，例如以下内容：
- en: The vectors are sparse. Each vector contains many 0s but only one `1`. Obviously,
    this is a waste of space if we know that words with similar meanings can be grouped
    together and share some dimensions. Also, numerical algorithms don't really like
    high-dimensional and sparse vectors in general.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些向量是稀疏的。每个向量包含许多0，但只有一个`1`。显然，如果我们知道具有相似意义的单词可以分组并共享一些维度，这将是一种空间浪费。此外，通常数值算法并不喜欢高维和稀疏向量。
- en: Secondly, what if the vocabulary size is over 1 million words? Obviously, we
    would need to use 1 million dimensional vectors, which is not really feasible
    in terms of memory and computation.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，如果词汇表的大小超过一百万个单词怎么办？显然，我们需要使用一百万维的向量，这在内存和计算方面实际上是不可行的。
- en: Another problem is that the vectors are not *meaningful* at all. Similar words
    are not assigned similar vectors somehow. In the preceding vocabulary, the words
    `cheese`, `topping`, `salami`, and `pizza` actually carry related meanings, but
    their vectors are not related in any way. These vectors are indeed assigned randomly,
    depending on the corresponding word's index in the vocabulary. The one-hot encoded
    vectors don't capture any semantic relationships at all.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个问题是没有向量具有任何*意义*。相似单词并没有以某种方式分配到相似的向量中。在先前的词汇中，单词`cheese`、`topping`、`salami`和`pizza`实际上携带相关的意义，但它们的向量在没有任何方式上相关。这些向量确实是随机分配的，取决于词汇表中相应单词的索引。独热编码的向量根本不捕捉任何语义关系。
- en: Word vectors were invented to answer the preceding list of concerns.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量是为了回答上述问题而发明的。
- en: Word vectors
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词向量
- en: 'Word vectors are the solution to the preceding problems. A word vector is a
    **fixed-size**, **dense**, **real-valued** vector. From a broader perspective,
    a word vector is a learned representation of the text where semantically similar
    words have similar vectors. The following is what a word vector looks like. This
    has been extracted from **Glove English vectors** (we''ll look at Glove in detail
    in the *How word vectors are produced* section):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量是解决前面问题的解决方案。词向量是一个**固定大小**、**密集**、**实值**的向量。从更广泛的角度来看，词向量是文本的学习表示，其中语义相似的单词具有相似的向量。以下是一个词向量看起来像什么。这已被从**Glove英语向量**中提取出来（我们将在*如何生成词向量*部分详细探讨Glove）：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is a 50-dimensional vector for the word `the`. As you can see, the dimensions
    are floating points. But what do the dimensions represent? These individual dimensions
    typically don't have inherent meanings. Instead, they represent locations in the
    vector space, and the distance between these vectors indicates the similarity
    of the corresponding words' meanings. Hence, a word's meaning is distributed across
    the dimensions. This way of representing a word's meaning is called **distributional
    semantics**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是单词“the”的50维向量。正如你所见，维度是浮点数。但这些维度代表什么呢？这些单个维度通常没有固有的含义。相反，它们代表向量空间中的位置，这些向量之间的距离表示了对应单词意义的相似性。因此，一个单词的意义分布在维度上。这种表示单词意义的方式被称为**分布语义**。
- en: 'We''ve already mentioned that semantically similar words have similar representations.
    Let''s look at the vectors of the different words and how they offer semantic
    representations. We can use the word vector visualizer for TensorFlow at [https://projector.tensorflow.org/](https://projector.tensorflow.org/)
    for this. On this website, Google offers word vectors for 10,000 words. Each vector
    is 200-dimensional and projected onto three dimensions for visualization. Let''s
    look at the representation of the word `cheese` from our humble pizza ordering
    vocabulary:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到，语义相似的单词具有相似的表达。让我们看看不同单词的向量以及它们如何提供语义表示。为此，我们可以使用TensorFlow的词向量可视化工具，网址为[https://projector.tensorflow.org/](https://projector.tensorflow.org/)。在这个网站上，Google提供了10,000个单词的词向量。每个向量是200维的，并投影到三个维度进行可视化。让我们看看我们谦逊的披萨订购词汇中单词“cheese”的表示：
- en: '![Figure 5.1 – The vector representation of the word “cheese” and semantically
    similar words'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.1 – “cheese”单词及其语义相似单词的向量表示'
- en: '](img/B16570_5_1.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B16570_5_1.jpg)'
- en: Figure 5.1 – The vector representation of the word "cheese" and semantically
    similar words
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – “cheese”单词及其语义相似单词的向量表示
- en: 'As we can see, the word `cheese` is semantically grouped with the other words
    about food. These are the words that are used together with the word `cheese`
    quite often: sauce, cola, food, and so on. In the following screenshot, we can
    see the closest words sorted by their cosine distance (think of cosine distance
    as a way of calculating the distance between vectors):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，单词“cheese”在语义上与其他关于食物的单词分组。这些是经常与单词“cheese”一起使用的单词：酱汁、可乐、食物等等。在下面的截图中，我们可以看到按余弦距离排序的最近单词（将余弦距离视为计算向量之间距离的一种方式）：
- en: '![Figure 5.2 – Closest points to “cheese” in the three-dimensional space'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.2 – “cheese”在三维空间中的最近点'
- en: '](img/B16570_5_2.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B16570_5_2.jpg)'
- en: Figure 5.2 – Closest points to "cheese" in the three-dimensional space
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – 三维空间中“cheese”的最近点
- en: 'How about some proper nouns? Word vectors are trained on a huge corpus, such
    as Wikipedia, which is why the representations of some proper nouns are also learned.
    For example, the proper noun **elizabeth** is represented by the following vector:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，关于专有名词呢？词向量是在大型语料库上训练的，例如维基百科，这就是为什么一些专有名词的表示也是学习得到的。例如，专有名词**elizabeth**由以下向量表示：
- en: '![Figure 5.3 – Vector representation of elizabeth'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.3 – elizabeth的向量表示'
- en: '](img/B16570_5_3.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B16570_5_3.jpg)'
- en: Figure 5.3 – Vector representation of elizabeth
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – elizabeth的向量表示
- en: Note
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: Notice that all the words in the preceding screenshot are in lowercase. Most
    of the word vector algorithms make all the vocabulary input words lowercase to
    avoid there being two representations of the same word.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，前一个截图中的所有单词都是小写的。大多数词向量算法会将所有词汇输入单词转换为小写，以避免存在两个相同单词的表示。
- en: Here, we can see that **elizabeth** indeed points to Queen Elizabeth of England.
    The surrounding words are **monarch**, **empress**, **princess**, **royal**, **lord**,
    **lady**, **crown**, **England**, **Tudor**, **Buckingham**, her mother's name,
    **anne**, her father's name, **henry**, and even her mother's rival queen's name,
    **catherine**! Both ordinary words such as **crown** and proper nouns such as
    **henry** are grouped together with **elizabeth**. We can also see that the syntactic
    category of all the neighbor words is noun; verbs don't go together with nouns.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到 **elizabeth** 确实指向英格兰女王伊丽莎白。周围的词包括 **monarch**、**empress**、**princess**、**royal**、**lord**、**lady**、**crown**、**England**、**Tudor**、**Buckingham**、她母亲的名字
    **anne**、她父亲的名字 **henry**，甚至她母亲竞争对手的女王名字 **catherine**！**crown** 这样的普通词和 **henry**
    这样的专有名词都与 **elizabeth** 一起分组。我们还可以看到，所有邻近词的句法类别都是名词；动词不会与名词一起使用。
- en: Word vectors can capture synonyms, antonyms, and semantic categories such as
    animals, places, plants, names, and abstract concepts. Next, we'll dive deep into
    semantics and explore a surprising feature provided by word vectors – **word analogies**.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量可以捕捉同义词、反义词以及诸如动物、地点、植物、人名和抽象概念等语义类别。接下来，我们将深入探讨语义，并探索词向量提供的令人惊讶的功能——**词类比**。
- en: Analogies and vector operations
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类比和向量运算
- en: We've already seen that learned representations can capture semantics. What's
    more, word vectors support vector operations, such as vector addition and subtraction,
    in a meaningful way. Indeed, adding and subtracting word vectors is one way to
    support analogies.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，学习到的表示可以捕捉语义。更重要的是，词向量以有意义的方式支持向量运算，如向量加法和减法。实际上，添加和减去词向量是支持类比的一种方式。
- en: A word analogy is a semantic relationship between a pair of words. There are
    many types of relationship, such as synonymity, anonymity, and wholepart relation.
    Some example pairs are (King – man, Queen – woman), (airplane – air, ship - sea),
    (fish – sea, bird - air), (branch – tree, arm – human), (forward – backward, absent
    – present), and so on.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 词类比是一对词之间的语义关系。存在许多类型的关系，例如同义性、反义性和整体部分关系。一些例子包括（King – man, Queen – woman）、（airplane
    – air, ship - sea）、（fish – sea, bird - air）、（branch – tree, arm – human）、（forward
    – backward, absent – present）等等。
- en: 'For example, we can represent gender mapping between the Queen and King as
    `Queen – Woman + Man = King`. Here, if we subtract *woman* from *Queen* and add
    *man* instead, we get *King*. Then, this analogy reads as, *queen is to king as
    woman is to man*. Embeddings can generate remarkable analogies such as gender,
    tense, and capital city. The following diagram shows these analogies:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以将女王和国王之间的性别映射表示为 `Queen – Woman + Man = King`。在这里，如果我们从 *Queen* 中减去 *woman*
    并加上 *man*，我们得到 *King*。然后，这个类比可以读作，*queen is to king as woman is to man*。嵌入可以生成诸如性别、时态和首都等显著的类比。以下图表显示了这些类比：
- en: '![Figure 5.4 – Analogies created by the word vectors (Source: https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space)
    ](img/B16570_5_4.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 由词向量创建的类比（来源：https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space)
    ](img/B16570_5_4.jpg)'
- en: 'Figure 5.4 – Analogies created by the word vectors (Source: https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 由词向量创建的类比（来源：https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space）
- en: Obviously, word vectors provide great semantic capabilities for NLP developers,
    but how are they produced? We'll learn more about word vector generation algorithms
    in the next section.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，词向量为 NLP 开发者提供了强大的语义能力，但它们是如何产生的呢？我们将在下一节中了解更多关于词向量生成算法的内容。
- en: How word vectors are produced
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词向量是如何产生的
- en: 'There is more than one way to produce word vectors. Let''s look at the most
    popular pretrained vectors and how they are trained:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 产生词向量的方法不止一种。让我们看看最流行的预训练向量以及它们的训练方式：
- en: '**word2vec** is the name of the statistical algorithm that was created by Google
    to produce word vectors. Word vectors are trained with a neural network architecture,
    which processes windows of words and predicts the vector for each word, depending
    on the surrounding words. These pretrained word vectors can be downloaded from
    [https://developer.syn.co.in/tutorial/bot/oscova/pretrained-vectors.html#word2vec-and-glove-models](https://developer.syn.co.in/tutorial/bot/oscova/pretrained-vectors.html#word2vec-and-glove-models).
    We won''t go into the details here, but you can read the excellent blog at [https://jalammar.github.io/illustrated-word2vec/](https://jalammar.github.io/illustrated-word2vec/)
    for more details about the algorithm and data preparation steps.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**word2vec**是谷歌创建的用于生成词向量的统计算法。词向量是通过神经网络架构训练的，该架构处理单词窗口并预测每个单词的向量，这取决于周围的单词。这些预训练的词向量可以从[https://developer.syn.co.in/tutorial/bot/oscova/pretrained-vectors.html#word2vec-and-glove-models](https://developer.syn.co.in/tutorial/bot/oscova/pretrained-vectors.html#word2vec-and-glove-models)下载。这里我们不会深入细节，但你可以阅读关于算法和数据准备步骤的出色博客[https://jalammar.github.io/illustrated-word2vec/](https://jalammar.github.io/illustrated-word2vec/)以获取更多信息。'
- en: '**Glove** vectors are trained in another way and were invented by the Stanford
    NLP group. This method depends on singular value decomposition, which is used
    on the word co-occurrences matrix. A comprehensive guide to the Glove algorithm
    is available at [https://www.youtube.com/watch?v=Fn_U2OG1uqI](https://www.youtube.com/watch?v=Fn_U2OG1uqI).
    The pretrained vectors are available at [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Glove**向量以另一种方式训练，是由斯坦福NLP小组发明的。这种方法依赖于奇异值分解，它用于单词共现矩阵。关于Glove算法的全面指南可在[https://www.youtube.com/watch?v=Fn_U2OG1uqI](https://www.youtube.com/watch?v=Fn_U2OG1uqI)找到。预训练的向量可在[https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)找到。'
- en: '**fastText** was created by Facebook Research and is similar to word2vec, but
    offers more. word2vec predicts words based on their surrounding context, while
    fastText predicts subwords; that is, character n-grams. For example, the word
    *chair* generates the following subwords:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**fastText**是由Facebook Research创建的，与word2vec类似，但提供了更多功能。word2vec根据周围的上下文预测单词，而fastText预测子词；也就是说，字符n-gram。例如，单词*椅子*生成了以下子词：'
- en: '[PRE4]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: fastText produces a vector for each subword, including misspelled words, numbers,
    partial words, and single characters. fastText is robust when it comes to misspelled
    words and rare words. It can compute a vector for the tokens that are not proper
    lexicon words.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: fastText为每个子词生成一个向量，包括拼写错误、数字、部分单词和单个字符。在处理拼写错误和罕见单词时，fastText非常稳健。它可以计算非标准词典单词的向量。
- en: Facebook Research published pretrained fastText vectors for 157 languages. You
    can find these models at [https://fasttext.cc/docs/en/crawl-vectors.html](https://fasttext.cc/docs/en/crawl-vectors.html)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook Research发布了157种语言的预训练fastText向量。你可以在[https://fasttext.cc/docs/en/crawl-vectors.html](https://fasttext.cc/docs/en/crawl-vectors.html)找到这些模型。
- en: 'All the preceding algorithms follow the same idea: similar words occur in a
    similar context. The context – the surrounding words around a word – is key to
    generating the word vector for a specific word in any case. All the pretrained
    word vectors that are generated with the preceding three algorithms are trained
    on a huge corpus such as Wikipedia, the news, or Twitter.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的先前算法都遵循相同的思想：相似单词出现在相似语境中。语境——围绕一个单词的周围单词——在任何情况下都是生成特定单词的词向量的关键。所有使用先前三种算法生成的预训练词向量都是在像维基百科、新闻或推特这样的大型语料库上训练的。
- en: Pro tip
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'When we say similar words, the first concept that comes to mind is **synonymity**.
    Synonym words occur in a similar context; for example, *freedom* and *liberty*
    both mean the same thing:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说到相似单词时，首先想到的概念是**同义性**。同义词出现在相似的语境中；例如，*自由*和*自由*都意味着相同的事情：
- en: We want free healthcare, education, and liberty.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望拥有免费的健康医疗、教育和自由。
- en: We want free healthcare, education, and freedom.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望拥有免费的健康医疗、教育和自由。
- en: 'How about antonyms? Antonyms can be used in the same context. Take *love* and
    *hate*, for example:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，反义词呢？反义词可以在相同的语境中使用。以*爱*和*恨*为例：
- en: I hate cats.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我讨厌猫。
- en: I love cats.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢猫。
- en: As you can see, antonyms also appear in similar contexts; hence, usually, their
    vectors are also similar. If your downstream NLP task is sensitive in this aspect,
    be careful while using word vectors. In this case, always either train your own
    vectors or refine your word vectors by training them in the downstream task as
    well. You can train your own word vectors with the Gensim package ([https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)).
    The Keras library allows word vectors to be trained on downstream tasks. We'll
    revisit this issue in [*Chapter 8*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)*,
    Text Classification with spaCy*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，反义词也出现在类似的环境中；因此，通常它们的向量也是相似的。如果您的下游 NLP 任务在这方面很敏感，使用词向量时要小心。在这种情况下，始终要么训练自己的向量，要么通过在下游任务中训练来改进您的词向量。您可以使用
    Gensim 包（[https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/））来训练自己的词向量。Keras
    库允许在下游任务上训练词向量。我们将在 [*第 8 章*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)*，使用
    spaCy 进行文本分类* 中重新讨论这个问题。
- en: Now that we know more about word vectors, let's look at how to use spaCy's pretrained
    word vectors.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对词向量有了更多的了解，让我们看看如何使用 spaCy 的预训练词向量。
- en: Using spaCy's pretrained vectors
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 spaCy 的预训练向量
- en: We installed a medium-sized English spaCy language model in [*Chapter 1*](B16570_01_Final_JM_ePub.xhtml#_idTextAnchor014)*,
    Getting Started with spaCy*, so that we can directly use word vectors. Word vectors
    are part of many spaCy language models. For instance, the `en_core_web_md` model
    ships with 300-dimensional vectors for 20,000 words, while the `en_core_web_lg`
    model ships with 300-dimensional vectors with a 685,000 word vocabulary.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [*第 1 章*](B16570_01_Final_JM_ePub.xhtml#_idTextAnchor014)*，spaCy 入门* 中安装了一个中等大小的英语
    spaCy 语言模型，这样我们就可以直接使用词向量。词向量是许多 spaCy 语言模型的一部分。例如，`en_core_web_md` 模型包含 20,000
    个单词的 300 维向量，而 `en_core_web_lg` 模型包含 685,000 个单词词汇的 300 维向量。
- en: Typically, small models (those whose names end with `sm`) do not include any
    word vectors but include context-sensitive tensors. You can still make the following
    semantic similarity calculations, but the results won't be as accurate as word
    vector computations.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，小型模型（那些以 `sm` 结尾的）不包含任何词向量，但包含上下文敏感的张量。您仍然可以进行以下语义相似度计算，但结果不会像词向量计算那样准确。
- en: 'You can reach a word''s vector via the `token.vector` method. Let''s look at
    this method in an example. The following code queries the word vector for banana:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过 `token.vector` 方法访问一个单词的向量。让我们通过一个例子来看看这个方法。以下代码查询了 banana 的单词向量：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following screenshot was taken within the Python shell:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是在 Python 壳中拍摄的：
- en: '![Figure 5.5 – Word vector for the word “banana”'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.5 – 单词 "banana" 的词向量'
- en: '](img/B16570_5_5.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_5_5.jpg)'
- en: Figure 5.5 – Word vector for the word "banana"
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 单词 "banana" 的词向量
- en: '`token.vector` returns a NumPy `ndarray`. You can call `numpy` methods on the
    result:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`token.vector` 返回一个 NumPy `ndarray`。您可以在结果上调用 `numpy` 方法：'
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In this code segment, first, we queried the Python type of the word vector.
    Then, we invoked the `shape()` method of the NumPy array on the vector.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码段中，首先，我们查询了单词向量的 Python 类型。然后，我们在向量上调用了 NumPy 数组的 `shape()` 方法。
- en: 'The `Doc` and `Span` objects also have vectors. The vector of a sentence or
    a span is the average of its words'' vectors. Run the following code and view
    the results:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`Doc` 和 `Span` 对象也有向量。句子或 span 的向量是其单词向量的平均值。运行以下代码并查看结果：'
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Only the words in the model''s vocabulary have vectors; words that are not
    in the vocabulary are called `token.is_oov` and `token.has_vector` are two methods
    we can use to query whether a token is in the model''s vocabulary and has a word
    vector:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 只有模型词汇表中的单词才有向量；不在词汇表中的单词称为 `token.is_oov` 和 `token.has_vector`，这两个方法可以用来查询一个标记是否在模型的词汇表中并且有单词向量：
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This is basically how we use spaCy's pretrained word vectors. Next, we'll discover
    how to invoke spaCy's semantic similarity method on `Doc`, `Span`, and `Token`
    objects.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上就是我们使用 spaCy 的预训练词向量的方法。接下来，我们将探讨如何调用 spaCy 的语义相似度方法在 `Doc`、`Span` 和 `Token`
    对象上。
- en: The similarity method
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相似度方法
- en: In spaCy, every container type object has a similarity method that allows us
    to calculate the semantic similarity of other container objects by comparing their
    word vectors.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在 spaCy 中，每个容器类型对象都有一个相似度方法，允许我们通过比较它们的词向量来计算其他容器对象的语义相似度。
- en: 'We can calculate the semantic similarity between two container objects, even
    though they are different types of containers. For instance, we can compare a
    `Token` object to a `Doc` object and a `Doc` object to a `Span` object. The following
    example computes how similar two `Span` objects are:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以计算两个容器对象之间的语义相似度，即使它们是不同类型的容器。例如，我们可以比较一个`Token`对象和一个`Doc`对象，以及一个`Doc`对象和一个`Span`对象。以下示例计算了两个`Span`对象之间的相似度：
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can compare the two `Token` objects, `London` and `England`, as well:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以比较两个`Token`对象，`伦敦`和`英格兰`：
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The sentence''s similarity is computed by calling `similarity()` on the `Doc`
    objects:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 句子的相似度是通过在`Doc`对象上调用`similarity()`来计算的：
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The preceding code segment calculates the semantic similarity between the two
    sentences `I visited England.` and `I went to London.`. The similarity score is
    high enough that it considers both sentences are similar (the degree of similarity
    ranges from `0` to `1`, with `0` being unrelated and `1` being identical).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码段计算了两个句子`我访问了英格兰`和`我去了伦敦`之间的语义相似度。相似度得分足够高，以至于它认为这两个句子是相似的（相似度的范围从`0`到`1`，其中`0`表示无关，`1`表示相同）。
- en: 'Not surprisingly, the `similarity()` method returns `1` when you compare an
    object to itself:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，当你比较一个对象与自身时，`similarity()`方法返回`1`：
- en: '[PRE12]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Judging the distance with numbers is difficult sometimes, but looking at the
    vectors on paper can also help us understand how our vocabulary words are grouped.
    The following code snippet visualizes a simple vocabulary of two semantic classes.
    The first class of words is for animals, while the second class is for food. We
    expect these two classes of words to become two groups on the graphics:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 有时用数字判断距离是困难的，但查看纸上的向量也可以帮助我们理解我们的词汇词是如何分组的。以下代码片段可视化了一个简单的词汇，包含两个语义类别。第一个类别是动物，而第二个类别是食物。我们预计这两个类别的词将在图形上形成两个组：
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This code snippet achieves a lot. Let''s take a look:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码片段实现了很多功能。让我们来看看：
- en: First, we imported the matplotlib library for creating our graphic.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入了matplotlib库来创建我们的图形。
- en: The next two imports are for calculating the vectors.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面的两个导入是为了计算向量。
- en: We imported `spacy` and created an `nlp` object as usual.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们像往常一样导入了`spacy`并创建了一个`nlp`对象。
- en: Then, we created a `Doc` object from our vocabulary.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们从我们的词汇表中创建了一个`Doc`对象。
- en: Next, we stacked our word vectors vertically by calling `np.vstack`.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通过调用`np.vstack`将词向量垂直堆叠。
- en: Since the vectors are 300-dimensional, we needed to project them into a two-dimensional
    space for visualization purposes. We made this projection by extracting the two
    principal components via **principal component analysis** (**PCA**).
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于向量是300维的，我们需要将它们投影到二维空间以进行可视化。我们通过提取两个主成分来执行这种投影，这被称为**主成分分析**（**PCA**）。
- en: The rest of the code deals with matplotlib function calls to create a scatter
    plot.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码的其余部分处理matplotlib函数调用以创建散点图。
- en: 'The resulting visual looks as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的视觉如下所示：
- en: '![Figure 5.6 – Two semantic classes grouped'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.6 – 两个语义类别分组]'
- en: '](img/B16570_5_6.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_5_6.jpg]'
- en: Figure 5.6 – Two semantic classes grouped
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – 两个语义类别分组
- en: Voil•! Our spaCy word vectors really worked! Here, we can see the two semantic
    classes that were grouped on the visualization. Notice that the distance between
    the animals is less and more uniformly distributed, while the food class formed
    groups inside the group.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！我们的spaCy词向量真的起作用了！在这里，我们可以看到在可视化中分组的两类语义。注意，动物之间的距离更短，分布更均匀，而食物类别在组内形成了群体。
- en: Previously, we mentioned that we can create our own word vectors or refine them
    on our own corpus. Once we've done that, can we use them within spaCy? The answer
    is yes! In the next section, we'll learn how to load custom word vectors into
    spaCy.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们提到过我们可以创建自己的词向量或者在我们自己的语料库上对它们进行优化。一旦我们完成了这些，我们能否在spaCy中使用它们呢？答案是肯定的！在下一节中，我们将学习如何将自定义词向量加载到spaCy中。
- en: Using third-party word vectors
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用第三方词向量
- en: We can also use third-party word vectors within spaCy. In this section, we'll
    learn how to import a third-party word vector package into spaCy. We'll use fastText's
    subword-based pretrained vectors from the Facebook AI. You can view the list of
    all the available English pretrained vectors at [https://fasttext.cc/docs/en/english-vectors.html](https://fasttext.cc/docs/en/english-vectors.html).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在 spaCy 中使用第三方词向量。在本节中，我们将学习如何将第三方词向量包导入 spaCy。我们将使用来自 Facebook AI 的 fastText
    的基于子词的预训练向量。您可以在 [https://fasttext.cc/docs/en/english-vectors.html](https://fasttext.cc/docs/en/english-vectors.html)
    查看所有可用的英语预训练向量的列表。
- en: The name of the package identifies the vector's dimension, the vocabulary size,
    and the corpus genre that the vectors will be trained on. For instance, `wiki-news-300d-1M-subword.vec.zip`
    indicates that it contains 1 million 300-dimensional word vectors that have been
    trained on a Wikipedia corpus.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 包的名字标识了向量的维度、词汇量大小以及向量将要训练的语料库类型。例如，`wiki-news-300d-1M-subword.vec.zip` 表示它包含在维基百科语料库上训练的100万300维度的词向量。
- en: 'Let''s start downloading the vectors:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始下载向量：
- en: 'In your terminal, type the following command. Alternatively, you can copy and
    paste the URL into your browser and the download should start:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的终端中，输入以下命令。或者，您可以将 URL 复制并粘贴到浏览器中，下载应该会开始：
- en: '[PRE14]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The preceding line will download the 300-dimensional word vectors onto your
    machine.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前一行将下载300维度的词向量到您的机器上。
- en: 'Next, we will unzip the following `.zip` file. You can either unzip it by right-clicking
    or by using the following code:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将解压以下 `.zip` 文件。您可以通过右键单击或使用以下代码进行解压：
- en: '[PRE15]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we''re ready to use spaCy''s `init-model` command:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好了使用 spaCy 的 `init-model` 命令：
- en: '[PRE16]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If everything goes well, you should see the following message:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一切顺利，您应该会看到以下消息：
- en: '[PRE17]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'With that, we''ve created the language model. Now, we can load it:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过这样，我们已经创建了语言模型。现在，我们可以加载它：
- en: '[PRE18]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, we can create a `doc` object with this `nlp` object, just like we did
    with spaCy''s default language models:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用这个 `nlp` 对象创建一个 `doc` 对象，就像我们使用 spaCy 的默认语言模型一样：
- en: '[PRE19]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The model we just created is an empty model that was initiated with the word
    vectors, so it does not contain any other pipeline components. For instance, making
    a call to `doc.ents` will fail with an error. So, be careful while working with
    third-party vectors and favor built-in spaCy vectors whenever possible.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚创建的模型是一个使用词向量初始化的空模型，因此它不包含任何其他管道组件。例如，调用 `doc.ents` 将会失败并显示错误。所以，当与第三方向量一起工作时，请小心，并在可能的情况下优先使用内置的
    spaCy 向量。
- en: Advanced semantic similarity methods
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级语义相似性方法
- en: In this section, we'll discover advanced semantic similarity methods for word,
    phrase, and sentence similarity. We've already learned how to calculate semantic
    similarity with spaCy's **similarity** method and obtained some scores. But what
    do these scores mean? How are they calculated? Before we look at more advanced
    methods, first, we'll learn how semantic similarity is calculated.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨用于单词、短语和句子相似性的高级语义相似性方法。我们已经学习了如何使用 spaCy 的 **similarity** 方法计算语义相似性并获得了一些分数。但这些分数意味着什么？它们是如何计算的？在我们查看更高级的方法之前，首先，我们将学习语义相似性是如何计算的。
- en: Understanding semantic similarity
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解语义相似性
- en: When we collect text data (any sort of data), we want to see how some examples
    are similar, different, or related. We want to measure how similar two pieces
    of text are by calculating their similarity scores. Here, the term *semantic similarity*
    comes into the picture; **semantic similarity** is a **metric** that's defined
    over texts, where the distance between two texts is based on their semantics.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们收集文本数据（任何类型的数据）时，我们希望看到一些例子是如何相似、不同或相关的。我们希望通过计算它们的相似度分数来衡量两段文本的相似度。在这里，术语
    *语义相似性* 出现了；**语义相似性**是一个定义在文本上的 **度量**，其中两个文本之间的距离基于它们的语义。
- en: A metric in mathematics is basically a distance function. Every metric induces
    a topology on the vector space. Word vectors are vectors, so we want to calculate
    the distance between them and use this as a similarity score.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 数学中的度量基本上是一个距离函数。每个度量在向量空间上诱导一个拓扑。词向量是向量，因此我们想要计算它们之间的距离，并将其用作相似度分数。
- en: 'Now, we''ll learn about two commonly used distance functions: **Euclidian distance**
    and **cosine distance**. Let''s start with Euclidian distance.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将了解两种常用的距离函数：**欧几里得距离**和**余弦距离**。让我们从欧几里得距离开始。
- en: Euclidian distance
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: 'The Euclidian distance between two points in a k-dimensional space is the length
    of the path between them. The distance between two points is calculated by the
    Pythagorean theorem. We calculate this distance by summing the difference of each
    coordinate''s square and then taking the square root of this sum. The following
    diagram shows the Euclidian distance between two vectors, dog and cat:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在 k 维空间中，两点之间的欧几里得距离是它们之间路径的长度。两点之间的距离是通过勾股定理计算的。我们通过求和每个坐标差的平方，然后取这个和的平方根来计算这个距离。以下图显示了两个向量（狗和猫）之间的欧几里得距离：
- en: '![Figure 5.7 – Euclidian distance between two vectors, dog and cat'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.7 – 狗和猫向量之间的欧几里得距离'
- en: '](img/B16570_5_7.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_5_7.jpg)'
- en: Figure 5.7 – Euclidian distance between two vectors, dog and cat
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – 狗和猫向量之间的欧几里得距离
- en: What does Euclidian distance mean for word vectors? First, Euclidian distance
    has no idea of **vector orientation**; what matters is the **vector magnitude**.
    If we take a pen and draw a vector from the origin to the **dog** point (let's
    call it **dog vector**) and do the same for the **cat** point (let's call it **cat
    vector**) and subtract one vector from and other, then the distance is basically
    the magnitude of this difference vector.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离对词向量意味着什么？首先，欧几里得距离没有向量方向的概念；重要的是**向量的大小**。如果我们拿一支笔从原点画一个向量到 **dog** 点（让我们称它为
    **dog vector**）并同样对 **cat** 点（让我们称它为 **cat vector**）做同样的操作，然后从其中一个向量减去另一个向量，那么距离基本上就是这个差向量的幅度。
- en: What happens if we add two more semantically similar words (*canine*, *terrier*)
    to **dog** and make it a text of three words? Obviously, the dog vector will now
    grow in magnitude, possibly in the same direction. This time, the distance will
    be much bigger due to geometry (as shown in the following diagram), although the
    semantics of the first piece of text (now **dog canine terrier**) remain the same.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在 **dog** 中添加两个语义相似的词（*canine*，*terrier*）并使其成为三个词的文本会发生什么？显然，狗向量现在将增长幅度，可能是在同一方向上。这次，由于几何形状（如下所示），距离将更大，尽管第一段文本（现在是
    **dog canine terrier**）的语义保持不变。
- en: 'This is the main drawback of using Euclidian distance for semantic similarity
    – the orientation of the two vectors in the space is not taken into account. The
    following diagram illustrates the distance between **dog** and **cat** and the
    distance between **dog** **canine terrier** and **cat**:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是使用欧几里得距离进行语义相似度计算的主要缺点 – 空间中两个向量的方向没有被考虑。以下图说明了 **dog** 和 **cat** 之间的距离以及
    **dog** **canine terrier** 和 **cat** 之间的距离：
- en: '![Figure 5.8 – Distance between “dog” and “cat,” as well as the distance between
    “dog canine terrier” and “cat”](img/B16570_5_8.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8 – “dog”和“cat”之间的距离，以及“dog canine terrier”和“cat”之间的距离](img/B16570_5_8.jpg)'
- en: Figure 5.8 – Distance between "dog" and "cat," as well as the distance between
    "dog canine terrier" and "cat"
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – “dog”和“cat”之间的距离，以及“dog canine terrier”和“cat”之间的距离
- en: How can we fix this problem? There's another way of calculating similarity that
    addresses this problem, called **cosine similarity**. Let's take a look.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解决这个问题？有一种计算相似度的另一种方法可以解决这个问题，称为**余弦相似度**。让我们看看。
- en: Cosine distance and cosine similarity
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 余弦距离和余弦相似度
- en: 'Contrary to Euclidian distance, cosine distance is more concerned with the
    orientation of the two vectors in the space. The cosine similarity of two vectors
    is basically the cosine of the angle that''s created by these two vectors. The
    following diagram shows the angle between the **dog** and **cat** vectors:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与欧几里得距离相反，余弦距离更关注空间中两个向量的方向。两个向量的余弦相似度基本上是这两个向量所形成的角度的余弦。以下图显示了 **dog** 和 **cat**
    向量之间的角度：
- en: '![Figure 5.9 – The angle between the dog and cat vectors. Here, the semantic
    similarity is calculated by cos(θ)](img/B16570_5_9.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – 狗和猫向量的夹角。在这里，语义相似度是通过 cos(θ) 计算的](img/B16570_5_9.jpg)'
- en: Figure 5.9 – The angle between the dog and cat vectors. Here, the semantic similarity
    is calculated by cos(θ)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 狗和猫向量之间的夹角。在这里，语义相似度是通过 cos(θ) 计算的
- en: The maximum similarity score that's allowed by cosine similarity is `1`. This
    is obtained when the angle between two vectors is 0 degrees (hence, the vectors
    coincide). The similarity between two vectors is 0 when the angle between them
    is 90 degrees.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度允许的最大相似度分数是 `1`。这是在两个向量之间的角度为 0 度时获得的（因此，向量重合）。当两个向量之间的角度为 90 度时，两个向量的相似度为
    0。
- en: Cosine similarity provides us with scalability when the vectors grow in magnitude.
    We will refer to *Figure 5.8* again here. If we grow one of the input vectors,
    the angle between them remains the same, so the cosine similarity score is the
    same.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似性在向量增长时提供了可扩展性。我们在这里再次引用*图5.8*。如果我们增长其中一个输入向量，它们之间的角度保持不变，因此余弦相似度得分相同。
- en: Note that here, we are calculating the semantic similarity score, not the distance.
    The highest possible value is 1 when the vectors coincide, while the lowest score
    is 0 when two vectors are perpendicular. The cosine distance is 1 – cos(θ), which
    is a distance function.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这里，我们计算的是语义相似度得分，而不是距离。当向量重合时，最高可能值为1，而当两个向量垂直时，最低得分为0。余弦距离是1 – cos(θ)，这是一个距离函数。
- en: spaCy uses cosine similarity to calculate semantic similarity. Hence, calling
    the `similarity` method helps us make cosine similarity calculations.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy使用余弦相似性来计算语义相似度。因此，调用`similarity`方法帮助我们进行余弦相似度计算。
- en: 'So far, we''ve learned how to calculate similarity scores, but we still haven''t
    discovered words we should look for meaning in. Obviously, not all the words in
    a sentence have the same impact on the semantics of the sentence. The similarity
    method will calculate the semantic similarity score for us, but for the results
    of that calculation to be useful, we need to choose the right keywords to compare.
    To understand why, consider the following text snippet:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何计算相似度得分，但我们还没有发现我们应该寻找意义的单词。显然，句子中的所有单词对句子的语义影响并不相同。相似度方法会为我们计算语义相似度得分，但为了使该计算的结果有用，我们需要选择正确的关键词进行比较。为了理解这一点，考虑以下文本片段：
- en: '[PRE20]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If we're interested in finding the biggest mammals on the planet, the phrases
    `biggest mammals` and `in the world` will be the key words. Comparing these phrases
    with the search phrases *largest mammals* and *on the planet* should give us a
    high similarity score. But if we're interested in finding out about some places
    in the world, `California` will be the keyword. `California` is semantically similar
    to the word *geography* and, even better, the entity type is a geographical noun.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们感兴趣的是寻找地球上最大的哺乳动物，短语“最大的哺乳动物”和“在世界上”将是关键词。将这些短语与搜索短语“最大的哺乳动物”和“在地球上”进行比较应该会给出一个高相似度得分。但如果我们感兴趣的是了解世界上的某些地方，那么“加利福尼亚”将是关键词。“加利福尼亚”在语义上与单词“地理”相似，而且更好，实体类型是地理名词。
- en: We have already learned *how* to calculate the similarity score. In the next
    section, we'll learn about *where* to look for the *meaning*. We'll extract the
    key phrases and named entities from the sentences and then use them in similarity
    score calculations. We'll start by covering a case study on text categorization
    before improving the task results via key phrase extraction.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了如何计算相似度得分。在下一节中，我们将学习如何查找意义所在。我们将从句子中提取关键短语和命名实体，然后将其用于相似度得分计算。我们将首先通过一个文本分类案例研究来介绍，然后通过关键短语提取来提高任务结果。
- en: Categorizing text with semantic similarity
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用语义相似性对文本进行分类
- en: 'Determining two sentence''s semantic similarity can help you categorize texts
    into predefined categories or spot only the relevant texts. In this case study,
    we''ll filter all user comments in an e-commerce website related to the word *perfume*.
    Suppose you need to evaluate the following user comments:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 确定两个句子的语义相似度可以帮助你将文本分类到预定义的类别中，或者仅突出相关的文本。在本案例研究中，我们将过滤掉所有与单词“香水”相关的电子商务网站的用户评论。假设你需要评估以下用户评论：
- en: '[PRE21]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here, we can see that only the second sentence is related. This is because it
    contains the word `fragrance`, as well as the adjectives describing scents. To
    understand which sentences are related, we can try several comparison strategies.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到只有第二句话是相关的。这是因为它包含了单词“香气”，以及描述香味的形容词。为了理解哪些句子是相关的，我们可以尝试几种比较策略。
- en: 'First, we can compare `perfume` to each sentence. Recall that spaCy generates
    a word vector for a sentence by averaging the word vector of its tokens. The following
    code snippet compares the preceding sentences to the `perfume` search key:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以将“香水”与每个句子进行比较。回想一下，spaCy通过平均其标记的词向量来为句子生成一个词向量。以下代码片段将前面的句子与“香水”搜索关键字进行比较：
- en: '[PRE22]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here, we performed the following steps:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们执行了以下步骤：
- en: First, we created a `Doc` object with the three preceding sentences.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建了包含前三个句子的`Doc`对象。
- en: Then, for each sentence, we calculated the similarity score with `perfume`.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，对于每个句子，我们计算了它与`perfume`的相似度得分。
- en: Then, we printed the score by invoking the `similarity()` method on the sentence.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们通过在句子上调用`similarity()`方法来打印得分。
- en: The degree of similarity between `perfume` and the first sentence is small,
    indicating that this sentence is not very relevant to our search key. The second
    sentence looks relevant, which means that we correctly spotted the semantic similarity.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`perfume`和第一句话的相似度很小，这表明这句话与我们搜索的关键词不太相关。第二句话看起来相关，这意味着我们正确地识别了语义相似度。'
- en: How about the third sentence? The script identified that the third sentence
    is relevant somehow, most probably because it includes the word `bottle` and perfumes
    are sold in bottles. The word `bottle` appears in similar contexts with the word
    `perfume`. For this reason, the similarity score of this sentence and the search
    key is not low enough; also, the scores of the second sentence and the third sentence
    are not far away enough to make the second sentence significant.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 第三句话怎么样？脚本确定第三句话在某种程度上是相关的，最可能的原因是它包含了单词`bottle`，而香水通常装在瓶子中销售。单词`bottle`与单词`perfume`出现在相似的环境中。因此，这个句子和搜索关键词的相似度得分并不足够低；此外，第二句和第三句的得分也没有足够接近，以至于第二句变得重要。
- en: There's another potential problem with comparing the key to the whole sentence.
    In practice, we occasionally deal with quite long texts, such as web documents.
    Averaging over a very long text lowers the importance of key words.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 与将关键词与整个句子进行比较相比，还有一个潜在问题。在实践中，我们偶尔会处理相当长的文本，例如网页文档。对非常长的文本进行平均会降低关键词的重要性。
- en: To improve performance, we can extract the *important* words. Let's look at
    how we can spot the key phrases in a sentence.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高性能，我们可以提取*重要*的词语。让我们看看我们如何在句子中找到关键短语。
- en: Extracting key phrases
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取关键短语
- en: A better way to do semantic categorization is to extract the important words/phrases
    and compare only them to the search key. Instead of comparing the key to the different
    parts of speech, we can compare the key to just the noun phrases. Noun phrases
    are the subjects, direct objects, and indirect objects of the sentences and carry
    a big percentage of the sentence's semantics on their shoulders.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 进行语义分类的更好方法是提取重要的词语/短语，并将它们与搜索关键词进行比较。我们不需要将关键词与不同的词性进行比较，而是可以将关键词仅与名词短语进行比较。名词短语是句子的主语、直接宾语和间接宾语，承担着句子大部分语义的比重。
- en: For example, in the sentence *Blue whales live in California.*, you'd probably
    like to focus on *blue whales*, *whales*, *California*, or *whales in California*.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在句子*Blue whales live in California.*中，你可能想关注*blue whales*、*whales*、*California*或*whales
    in California*。
- en: Similarly, in the preceding sentence about perfume, we focused on picking out
    the noun, *fragrance*. In different semantic tasks, you might need other context
    words such as verbs to decide what the sentence is about, but for semantic similarity,
    noun phrases carry most weight.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在关于香水的上一句话中，我们专注于挑选出名词，*fragrance*。在不同的语义任务中，你可能需要其他上下文词，如动词，来判断句子的主题，但对于语义相似度来说，名词短语占的比重最大。
- en: 'What is a noun phrase, then? A **noun phrase** (**NP**) is a group of words
    that consist of a noun and its modifiers. Modifiers are usually pronouns, adjectives,
    and determiners. The following phrases are noun phrases:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，名词短语是什么？一个**名词短语**（**NP**）是由一个名词及其修饰语组成的一组词语。修饰语通常是代词、形容词和限定词。以下短语是名词短语：
- en: '[PRE23]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'spaCy extracts noun phases by parsing the output of the dependency parser.
    We can see the noun phrases of a sentence by using the `doc.noun_chunks` method:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy通过解析依存句法分析器的输出来提取名词短语。我们可以通过使用`doc.noun_chunks`方法来查看句子的名词短语：
- en: '[PRE24]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s modify the preceding code snippet a bit. Instead of comparing the search
    key *perfume* to the entire sentence, this time, we will only compare it with
    the sentence''s noun chunks:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微修改一下前面的代码片段。这次，我们不是将搜索关键词*perfume*与整个句子进行比较，而是只将其与句子的名词短语进行比较：
- en: '[PRE25]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In the preceding code, we did the following:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们做了以下操作：
- en: First, we iterated over the sentences.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们遍历了句子。
- en: Then, for each sentence, we extracted the noun chunks and stored them in a Python
    list.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，对于每个句子，我们提取了名词短语并将它们存储在一个Python列表中。
- en: Next, we joined the noun chunks in the list into a Python string and converted
    it into a `Doc` object.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将列表中的名词短语连接成一个Python字符串，并将其转换为`Doc`对象。
- en: Finally, we compared this `Doc` object of noun chunks to the search key *perfume*
    to determine their semantic similarity score.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将名词短语的`Doc`对象与搜索关键词*perfume*进行比较，以确定它们的语义相似度分数。
- en: If we compare these scores to the previous scores, we will see that the first
    sentence is still irrelevant, so its score went down slightly. The second sentence's
    score increased significantly. Now, the second sentence's and the third sentence's
    scores look so far away from each other for us to confidently say that the second
    sentence is the most related sentence here.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这些分数与之前的分数进行比较，我们会看到第一句话仍然不相关，所以它的分数略有下降。第二句话的分数显著增加。现在，第二句话和第三句话的分数看起来离我们很远，以至于我们可以自信地说第二句话是最相关的句子。
- en: Extracting and comparing named entities
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取和比较命名实体
- en: 'In some cases, instead of extracting every noun, we will only focus on the
    proper nouns; hence, we want to extract the named entities. Let''s say we want
    to compare the following paragraphs:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们不会提取每个名词，而只会关注专有名词；因此，我们想要提取命名实体。假设我们想要比较以下段落：
- en: '[PRE26]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Our code should be able to recognize that the first two paragraphs are about
    large technology companies and their products, while the third paragraph is about
    a geographic location.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码应该能够识别出前两段是关于大型科技公司及其产品的，而第三段是关于地理位置的。
- en: 'Comparing all the noun phrases in these sentences may not be very helpful because
    many of them, such as `volume`, aren''t relevant to the categorization. The topics
    of these paragraphs are determined by the phrases within them; that is, `Google
    Search`, `Google`, `Microsoft Bing`, `Microsoft`, `Windows`, `Dead Sea`, `Jordan
    Valley`, and `Israel`. spaCy can spot these entities:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 比较这些句子中的所有名词短语可能并不很有帮助，因为其中许多，如`volume`，与分类不相关。这些段落的主题由其内的短语决定；也就是说，`Google
    Search`、`Google`、`Microsoft Bing`、`Microsoft`、`Windows`、`Dead Sea`、`Jordan Valley`和`Israel`。spaCy可以识别这些实体：
- en: '[PRE27]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now that we have extracted the words we want to compare, let''s calculate the
    similarity scores:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经提取了我们想要比较的单词，让我们计算相似度分数：
- en: '[PRE28]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Looking at these figures, we can see that the highest level of similarity exists
    between the first and the second paragraph, which are both about large tech companies.
    The third paragraph is not really similar to the other paragraphs. How did we
    get this calculation by just using word vectors? Probably because the words *Google*
    and *Microsoft* often appear together in news and other social media text corpuses,
    hence creating similar word vectors.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 观察这些图表，我们可以看到第一段和第二段之间的相似度最高，这两段都是关于大型科技公司的。第三段与其他段落并不真正相似。我们是如何仅使用词向量就得到这个计算的？可能是因为单词*Google*和*Microsoft*经常出现在新闻和其他社交媒体文本语料库中，从而创建了相似的词向量。
- en: Congratulations! You've reached the end of the *Advanced semantic similarity
    methods* section! You explored different ways of combining word vectors with linguistic
    features such as key phrases and named entities. By finishing this section, we
    are now ready to conclude this chapter.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经到达了*高级语义相似度方法*部分的结尾！你探索了将词向量与诸如关键词和命名实体等语言特征结合的不同方法。通过完成这一部分，我们现在可以总结本章内容。
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you worked with word vectors, which are floating-point vectors
    that represent word semantics. First, you learned about the different ways to
    perform text vectorization, as well as how to use word vectors and distributed
    semantics. Then, you explored the vector operations that word vectors allow and
    what semantics these operations bring.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你使用了词向量，这些是表示词义的字节浮点向量。首先，你学习了不同的文本向量化方法，以及如何使用词向量和分布式语义。然后，你探索了词向量允许的向量操作以及这些操作带来的语义。
- en: You also learned how to use spaCy's built-in word vectors and how to import
    third-party vectors into spaCy. Finally, you learned about vector-based semantic
    similarity and how to blend linguistic concepts with word vectors to get the best
    out of these semantics.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 你还学习了如何使用spaCy的内置词向量，以及如何将第三方向量导入spaCy。最后，你学习了基于向量的语义相似度以及如何将语言概念与词向量结合以获得最佳语义效果。
- en: The next chapter is full of surprises – we'll look at a real-word case-based
    study that allows you to blend what you've learned about in the past five chapters.
    Let's see what spaCy can do when it comes to real-world problems!
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章充满了惊喜——我们将研究一个基于实际案例的研究，这将使你能够将前五章学到的知识结合起来。让我们看看spaCy在处理现实世界问题时能做什么！
