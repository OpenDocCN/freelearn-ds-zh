- en: Chapter 6. Sentiment Analysis – Categorizing Hotel Reviews
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 情感分析 – 对酒店评论进行分类
- en: People talk about a lot of things online. There are forums and communities for
    almost everything under the sun, and some of them may be about your product or
    service. People may complain, or they may praise, and you would want to know which
    of the two they're doing.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 人们在网上谈论很多话题。几乎有关于太阳下所有事情的论坛和社区，其中一些可能与你产品或服务有关。人们可能会抱怨，或者他们可能会赞扬，你想要知道他们是在做哪一种。
- en: This is where sentiment analysis helps. It can automatically track whether the
    reviews and discussions are positive or negative overall, and it can pull out
    items from either category to make them easier to respond to or draw attention
    to.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是情感分析发挥作用的地方。它可以自动跟踪评论和讨论总体上是正面还是负面，并且可以从任一类别中提取项目，使其更容易回应或引起注意。
- en: 'Over the course of this chapter, we''ll cover a lot of ground. Some of it will
    be a little hazy, but in general, here''s what we''ll cover:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的整个过程中，我们将涵盖很多内容。其中一些可能有些模糊，但总的来说，以下是我们将涵盖的内容：
- en: Exploring and preparing the data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索和准备数据
- en: Understanding the classifiers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解分类器
- en: Running the experiment
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行实验
- en: Examining the error rates
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查错误率
- en: Before we go any further, let's learn what sentiment analysis is.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们先了解什么是情感分析。
- en: Understanding sentiment analysis
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解情感分析
- en: 'Sentiment analysis is a form of text categorization that works on opinions
    instead of topics. Often, texts are categorized according to the subject they
    discuss. For example, sentiment analysis attempts to categorize texts according
    to the opinions or emotions of the writers, whether the text is about cars or
    pets. Often, these are cast in binary terms: good or bad, like or dislike, positive
    or negative, and so on. Does this person love Toyotas or hate them? Are Pugs the
    best or German Shepherds? Would they go back to this restaurant? Questions like
    these have proven to be an important area of research, simply because so many
    companies want to know what people say about their goods and services online.
    This provides a way for companies'' marketing departments to monitor people''s
    opinions about their products or services as they talk on Twitter and other online
    public forums. They can reach out to unhappy customers to provide better, more
    proactive customer service or reach out to satisfied ones to strengthen their
    relationships and opinions.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是一种基于观点而非主题的文本分类形式。通常，文本是根据它们讨论的主题进行分类的。例如，情感分析试图根据作者的看法或情绪来对文本进行分类，无论文本是关于汽车还是宠物。通常，这些分类是二元的：好或坏，喜欢或不喜欢，正面或负面，等等。这个人喜欢丰田车还是讨厌它们？是哈士奇最好还是德国牧羊犬？他们会回这家餐厅吗？这类问题已被证明是一个重要的研究领域，仅仅因为许多公司都想了解人们在网络上对他们的商品或服务的看法。这为公司营销部门提供了一个途径，以监控人们在Twitter和其他在线公共论坛上对他们的产品或服务的看法。他们可以联系不满意的客户，提供更好的、更主动的客户服务，或者联系满意的客户，加强他们的关系和看法。
- en: As you can imagine, categorizing based on opinion than on topics is much more
    difficult. Even basic words tend to take on multiple meanings that are very dependent
    on their contexts.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所想，基于观点而非主题进行分类比基于主题分类要困难得多。即使是基本词汇也往往具有多种含义，这些含义非常依赖于它们的上下文。
- en: For example, take the word *good*. In a review, I can say that something is
    *good*. I can also say that it's not good, no good, or so far from good that It
    can almost see it on a clear day. On the other hand, I can say that something's
    *bad*. Or can I say that it's *not bad*. Or, if I'm stuck in the '80s, I can say
    that "I love it, it's so bad."
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以单词“好”为例。在评论中，我可以说某件事是“好”的。我也可以说它不好，毫无价值，或者离好差得远，几乎在晴朗的日子里都能看得见。另一方面，我可以说某件事是“坏”的。或者，我可以说它“不坏”。或者，如果我被困在20世纪80年代，我可以说“我喜欢它，它太糟糕了。”
- en: This is a very important and interesting problem, so people have been working
    on it for a number of years. An early paper on this topic came in 2002, *Thumbs
    up? Sentiment classification using machine learning techniques*, published by
    *Bo Pang*, *Lillian Lee*, and *Shivakumar Vaithyanathan*. In this paper, they
    compared movie reviews using naive Bayes' maximum entropy and support vector machines
    to categorize movie reviews into positive and negative. They also compared a variety
    of feature types such as unigrams, bigrams, and other combinations. In general,
    they found that support vector machines with single tokens performed best, although
    the difference wasn't usually huge.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常重要且有趣的问题，因此人们已经研究这个问题多年。关于这个主题的早期论文是在2002年发表的，由*Bo Pang*、*Lillian Lee*和*Shivakumar
    Vaithyanathan*撰写的*Thumbs up? Sentiment classification using machine learning techniques*。在这篇论文中，他们使用朴素贝叶斯的最大熵和支持向量机比较了电影评论，以将电影评论分类为正面或负面。他们还比较了各种特征类型，如unigrams、bigrams和其他组合。总的来说，他们发现使用单个标记的支持向量机表现最佳，尽管差异通常不是很大。
- en: Together and separately, *Bo Pang*, *Lillian Lee*, and many others have extended
    sentiment analysis in interesting ways. They've attempted to go beyond simple
    binary classifications toward predicting finer-grained sentiments. For example,
    they've worked on systems to predict from a document the number of stars the author
    of the review would give the reviewed service or object on a four-star or five-star
    rating system.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是集体还是单独，*Bo Pang*、*Lillian Lee*以及许多其他人都在以有趣的方式扩展情感分析。他们试图超越简单的二分类，预测更细致的情感。例如，他们致力于开发系统，从文档中预测评论者会给予被评论服务或对象的四星或五星评分系统中的星级数量。
- en: Part of what makes this interesting is that the baseline is how well the system
    explicitly agrees with the judgment of the human raters. However, in research,
    human raters only agree about 79 percent of the time, so a system that agrees
    with human raters 60 or 70 percent of the time is doing pretty well.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使这个问题有趣的部分在于基准是系统与人类评分者判断的一致性。然而，在研究中，人类评分者只有79%的时间达成一致，因此一个与人类评分者60%或70%时间达成一致的系统表现相当不错。
- en: Getting hotel review data
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取酒店评论数据
- en: For this chapter, we'll look at the **OpinRank Review** dataset ([http://archive.ics.uci.edu/ml/datasets/OpinRank+Review+Dataset](http://archive.ics.uci.edu/ml/datasets/OpinRank+Review+Dataset)).
    This is a dataset that contains almost 260,000 reviews for hotels ([http://tripadvisor.com/](http://tripadvisor.com/))
    from around the world on **TripAdvisor** as well as more than 42,000 car reviews
    ([http://edmunds.com/](http://edmunds.com/)) from 2007, 2008, and 2009 on **Edmunds**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将查看**OpinRank评论**数据集([http://archive.ics.uci.edu/ml/datasets/OpinRank+Review+Dataset](http://archive.ics.uci.edu/ml/datasets/OpinRank+Review+Dataset))。这是一个包含大约260,000条酒店评论的数据集，这些评论来自世界各地的**TripAdvisor**，以及2007年、2008年和2009年超过42,000条汽车评论([http://edmunds.com/](http://edmunds.com/))，这些评论来自**Edmunds**。
- en: Exploring the data
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据
- en: If we look at some of these reviews, we can see just how difficult categorizing
    the reviews as positive or negative is, even for humans.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看一些这些评论，我们可以看到将评论归类为正面或负面是多么困难，即使是对于人类来说也是如此。
- en: 'For instance, some words are used in ways that aren''t associated with their
    straightforward meaning. For example, look at the use of the term *greatest* in
    the following quote from a review for a Beijing hotel:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一些词的使用方式与其直接含义不相关。例如，看看以下来自北京酒店评论的引用中对术语*greatest*的使用：
- en: '*"Not the greatest area but no problems, even at 3:00 AM."*'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*"不是最好的区域，但没问题，即使在凌晨3点。"*'
- en: 'Also, many reviews recount both good and bad aspects of the hotel that they''re
    discussing, even if the final review decidedly comes down one way or the other.
    This review of a London hotel starts off listing the positives, but then it pivots:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，许多评论会描述他们讨论的酒店的好与坏两方面，即使最终评论明确地偏向一方或另一方。这篇关于伦敦酒店的评论一开始列举了优点，但随后发生了转变：
- en: '*"… These are the only real positives. Everything else was either average or
    below average...."*'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*"… 这些是唯一真正的优点。其他一切都是平均或低于平均水平..."*'
- en: 'Another reason why reviews are difficult to classify is that many reviews just
    don''t wholeheartedly endorse whatever it is they''re reviewing. Instead, the
    review will be tepid, or the reviewers qualify their conclusions as they did in
    this review for a Las Vegas hotel:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个评论难以分类的原因是，许多评论并没有全心全意地推荐他们所评论的内容。相反，评论可能是温和的，或者评论者会像在这篇拉斯维加斯酒店的评论中那样对他们的结论进行限定：
- en: '*"It''s faded, but it''s fine. If you''re on a budget and want to stay on the
    Strip, this is the place. But for a really great inexpensive experience, try the
    Main Street Station downtown."*'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*"它已经褪色了，但还可以。如果你预算有限，想在拉斯维加斯大道上住宿，这个地方很合适。但如果你想体验真正物有所值的服务，请尝试市中心的Main Street
    Station。"*'
- en: All of these factors contribute toward making this task more difficult than
    standard document classification problems.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些因素都使得这个任务比标准的文档分类问题更困难。
- en: Preparing the data
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: For this experiment, I've randomly selected 500 hotel reviews and classified
    them manually. A better option might be to use Amazon's Mechanical Turk ([https://www.mturk.com/mturk/](https://www.mturk.com/mturk/))
    to get more reviews classified than any one person might be able to do easily.
    Really, a few hundred is about the minimum that we'd like to use as both the training
    and test sets need to come from this. I made sure that the sample contained an
    equal number of positive and negative reviews. (You can find the sample in the
    `data` directory of the code download.)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实验，我随机选择了500条酒店评论并手动进行了分类。更好的选择可能是使用亚马逊的Mechanical Turk ([https://www.mturk.com/mturk/](https://www.mturk.com/mturk/))
    来获取比任何一个人都能轻松完成的更多评论分类。实际上，几百条评论是我们想要的最低数量，因为训练集和测试集都需要从这个样本中获取。我确保样本中包含相同数量的正面和负面评论。（您可以在代码下载的`data`目录中找到样本。）
- en: 'The data files are **tab-separated values** (**TSV**). After being manually
    classified, each line had four fields: the classification as a `+` or `-` sign,
    the date of the review, the title of the review, and the review itself. Some of
    the reviews are quite long.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 数据文件是**制表符分隔值**（**TSV**）。在手动分类后，每一行有四个字段：分类作为一个`+`或`-`符号、评论的日期、评论的标题以及评论本身。有些评论相当长。
- en: 'After tagging the files, we''ll take those files and create feature vectors
    from the vocabulary of the title and create a review for each one. For this chapter,
    we''ll see what works best: unigrams (single tokens), bigrams, trigrams, or part-of-speech
    annotated unigrams. These features comprise several common ways to extract features
    from the text:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记文件后，我们将从标题的词汇表中创建特征向量，并为每个标题创建一个评论。在本章中，我们将看到哪些方法效果最好：unigrams（单个标记）、bigrams、trigrams或词性标注的unigrams。这些特征包括从文本中提取特征的几种常见方式：
- en: Unigrams are single tokens, for example, features from the preceding sentence
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unigrams是单个标记，例如，来自前一句的特征
- en: Bigrams are two tokens next to each other, for example, *features comprise*
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bigrams是相邻的两个标记，例如，*features comprise*
- en: Trigrams are three tokens next to each other, for example, *features comprise
    several*
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trigrams是相邻的三个标记，例如，*features comprise several*
- en: Part-of-speech annotated unigrams would look something like `features_N`, which
    just means that the unigram features is a noun.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词性标注的unigrams看起来像`features_N`，这仅仅意味着unigram特征是一个名词。
- en: We'll also use these features to train a variety of classifiers on the reviews.
    Just like *Bo Pang* and *Lillian Lee* did, we'll try experiments with naive Bayes
    maximum entropy classifiers. To compare how well each of these does, we'll use
    cross validation to train and test our classifier multiple times.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用这些特征在评论上训练各种分类器。就像*Bo Pang*和*Lillian Lee*所做的那样，我们将尝试使用朴素贝叶斯最大熵分类器进行实验。为了比较这些分类器的表现，我们将使用交叉验证来多次训练和测试我们的分类器。
- en: Tokenizing
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词
- en: 'Before we get started on the code for this chapter, note that the Leiningen
    2 `project.clj` file looks like the following code:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始编写本章的代码之前，请注意，Leiningen 2的`project.clj`文件看起来像以下代码：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: First, let's create some functions to handle tokenization. Under the cover's,
    we'll use methods from the **OpenNLP** library ([http://opennlp.apache.org/](http://opennlp.apache.org/))
    to process the next methods from the **Weka machine learning** library ([http://www.cs.waikato.ac.nz/ml/weka/](http://www.cs.waikato.ac.nz/ml/weka/))
    to perform the sentiment analysis. However, we'll wrap these to provide a more
    natural, Clojure-like interface.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一些处理分词的函数。在幕后，我们将使用**OpenNLP**库（[http://opennlp.apache.org/](http://opennlp.apache.org/））的方法以及**Weka机器学习**库（[http://www.cs.waikato.ac.nz/ml/weka/](http://www.cs.waikato.ac.nz/ml/weka/））的下一个方法来执行情感分析。然而，我们将包装这些方法以提供一个更自然、类似Clojure的接口。
- en: 'Let''s start in the `src/sentiment/tokens.clj` file, which will begin in the
    following way:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`src/sentiment/tokens.clj`文件开始，它将以以下方式开始：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Our tokenizer will use `SimpleTokenizer` from the OpenNLP library and normalize
    all characters to lowercase:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分词器将使用OpenNLP库中的`SimpleTokenizer`并将所有字符转换为小写：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'I''ve aliased the `sentiment.tokens` namespace to `t` in the REPL. This function
    is used to break an input string into a sequence of token substrings:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我在交互式解释器中将 `sentiment.tokens` 命名空间别名为 `t`。此函数用于将输入字符串分割成一系列标记子字符串：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, we'll take the token streams and create feature vectors from them.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从标记流中创建特征向量。
- en: Creating feature vectors
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建特征向量
- en: A feature vector is a vector that summarizes an observation or document. Each
    vector contains the values associated with each variable or feature. The values
    may be boolean, indicating the presence or absence with 0 or 1, they may be raw
    counts, or they may be proportions scaled by the size of the overall document.
    As much of machine learning is based on linear algebra, vectors and matrices are
    very convenient data structures.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量是一个向量，它总结了观察或文档。每个向量包含与每个变量或特征关联的值。这些值可能是布尔值，表示存在或不存在，用 0 或 1 表示，也可能是原始计数，或者可能是按整体文档大小缩放的比率。由于机器学习的大部分基于线性代数，因此向量和矩阵是非常方便的数据结构。
- en: In order to maintain consistent indexes for each feature, we have to maintain
    a mapping from feature to indexes. Whenever we encounter a new feature, we need
    to assign it to a new index.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持每个特征的索引一致，我们必须维护一个从特征到索引的映射。每次我们遇到一个新特征时，我们需要将其分配到一个新索引。
- en: For example, the following table traces the steps to create a feature vector
    based on token frequencies from the phrase *the cat in the hat*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下表格追踪了根据短语 *the cat in the hat* 的标记频率创建特征向量的步骤。
- en: '| Step | Feature | Index | Feature Vector |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 步骤 | 特征 | 索引 | 特征向量 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | the | 0 | [1] |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 1 | the | 0 | [1] |'
- en: '| 2 | cat | 1 | [1, 1] |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 2 | cat | 1 | [1, 1] |'
- en: '| 3 | in | 2 | [1, 1, 1] |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 3 | in | 2 | [1, 1, 1] |'
- en: '| 4 | the | 0 | [2, 1, 1] |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 4 | the | 0 | [2, 1, 1] |'
- en: '| 5 | hat | 3 | [2, 1, 1, 1] |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 5 | hat | 3 | [2, 1, 1, 1] |'
- en: So, the final feature vector for *the cat in the hat* would be `[2, 1, 1, 1]`.
    In this case, we're counting the features. In other applications, we might use
    a bag-of-words approach that only tests the presence of the features. In that
    case, the feature vector would be `[1, 1, 1, 1]`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*the cat in the hat* 的最终特征向量将是 `[2, 1, 1, 1]`。在这种情况下，我们正在计算特征。在其他应用中，我们可能使用仅测试特征存在性的词袋方法。在这种情况下，特征向量将是
    `[1, 1, 1, 1]`。
- en: 'We''ll include the code to do this in the `sentiment.tokens` namespace. First,
    we''ll create a function that increments the value of a feature in the feature
    vector. It looks up the index of the feature in the vector from the feature index
    (`f-index`). If the feature hasn''t been seen yet, this function also allocates
    an index for it:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 `sentiment.tokens` 命名空间中包含执行此操作的代码。首先，我们将创建一个函数，该函数增加特征向量中特征值的值。它从特征索引（`f-index`）中查找向量中特征的位置。如果特征尚未被看到，此函数还会为它分配一个索引：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can use this function to convert a feature sequence into a feature vector.
    This function initially creates a vector of zeroes for the feature sequence, and
    then it reduces over the features, updating the feature index and vector as necessary:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用此函数将特征序列转换为特征向量。此函数最初为特征序列创建一个零向量，然后它遍历特征，根据需要更新特征索引和向量：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, for this task, we have several functions that we''ll look at together.
    The first function, `accum-features`, builds the index and the list of feature
    vectors. Each time it''s called, it takes the sequence of features passed to it
    and creates a feature vector. It appends this to the collection of feature vectors
    also passed into it. The next function, `pad-to`, makes sure that the feature
    vector has the same number of elements as the feature index. This makes it slightly
    easier to work with the feature vectors later on. The final function takes a list
    of feature vectors and returns the feature index and vectors for this data:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于这个任务，我们有一些函数将一起查看。第一个函数 `accum-features` 构建索引和特征向量列表。每次调用它时，它都会接收传递给它的特征序列，并创建一个特征向量。然后，它将这个向量追加到传递给它的特征向量集合中。下一个函数
    `pad-to` 确保特征向量具有与特征索引相同的元素数量。这使得稍后处理特征向量稍微容易一些。最后一个函数接受一个特征向量列表，并返回该数据的特征索引和向量：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can use these functions to build up a matrix of feature vectors from a set
    of input sentences. Let''s see how this works in the first few sentences of an
    *Emily Dickinson* poem:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些函数从一组输入句子中构建特征向量矩阵。让我们看看这是如何在 *Emily Dickinson* 诗歌的前几行中工作的。
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Notice that after tokenizing each document, we created a set of the tokens.
    This changes the system here to use a bag-of-words approach. We're only looking
    at the presence or absence of a feature, not its frequency. This does put the
    tokens out of order, `nobody` was evidently the first token indexed, but this
    doesn't matter.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在标记化每个文档之后，我们创建了一组标记。这改变了这里的系统，使其使用词袋方法。我们只关注特征的存在或不存在，而不是它们的频率。这确实会使标记顺序混乱，显然“nobody”是第一个索引的标记，但这并不重要。
- en: 'Now, by inverting the feature index, we can look up the words in a document
    from the features that it contains. This allows us to recreate a frequency map
    for each document as well as to recreate the tokens in each document. In this
    case, we''ll look up the words from the first feature vector, `I''m nobody`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过反转特征索引，我们可以从文档包含的特征中查找单词。这使我们能够为每个文档重新创建频率图，以及重新创建每个文档中的标记。在这种情况下，我们将从第一个特征向量`I'm
    nobody`中查找单词：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This block of code gets the indexes for each position in the feature vector,
    removes the features that didn't occur, and then looks up the index in the inverted
    feature index. This provides us with the sequence of features that occurred in
    that document. Notice that they're out of order. This is to be expected because
    neither the input sequence of features (in this case a set) nor the feature vector
    itself preserves the order of the features.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码获取特征向量中每个位置的索引，移除未出现的特征，然后在反转特征索引中查找索引。这为我们提供了在该文档中出现的特征序列。注意，它们是乱序的。这是可以预料的，因为既不是输入特征序列（在这种情况下是一个集合）也不是特征向量本身保留了特征的顺序。
- en: Creating feature vector functions and POS tagging
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建特征向量函数和POS标记
- en: We'll also include some functions to turn a list of tokens into a list of features.
    By wrapping these into functions, we make it easier to compose pipelines of processing
    functions and experiment with different feature sets.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将包括一些函数，将标记列表转换为特征列表。通过将这些包装成函数，我们使组合处理函数的管道和实验不同的特征集变得更加容易。
- en: 'The simplest and probably the most common type of feature is the unigram or
    a single token. As the `tokenize` function already outputs single functions, the
    `unigram` function is very simple to implement:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单且可能是最常见的特征类型是单语或单个标记。由于`tokenize`函数已经输出单个函数，因此`unigram`函数非常简单实现：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Another way to construct features is to use a number of consecutive tokens.
    In the abstract, these are called n-grams. Bigrams (two tokens) and trigrams (three
    tokens) are common instances of this type of function. We''ll define all of these
    as functions:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 构建特征的另一种方法是使用一系列连续的标记。在摘要中，这些被称为n-gram。二元组（两个标记）和三元组（三个标记）是这种类型函数的常见实例。我们将把这些都定义为函数：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'There are a number of different features we could create and experiment with,
    but we won''t show them all here. However, before we move on, here''s one more
    common type of feature: the token tagged with its **part of speech** (**POS**).
    POS is the category for words, which determines their range of uses in sentences.
    You probably remember these from elementary school. Nouns are people, places,
    and things. Verbs are actions.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建和实验多种不同的功能，但在这里我们不会展示所有。然而，在我们继续之前，这里还有一种更常见的功能类型：标记了其**词性**（**POS**）的标记。POS是词语的分类，它决定了它们在句子中的使用范围。你可能从小学时就已经记得这些了。名词是人、地点和事物。动词是动作。
- en: To get this information, we'll use OpenNLP's trained POS tagger. This takes
    a word and associates it with a part of speech. In order to use this, we need
    to download the training model file. You can find it at [http://opennlp.sourceforge.net/models-1.5/](http://opennlp.sourceforge.net/models-1.5/).
    Download **en POS tagger** (English) with a description of **Maxent model with
    tag dictionary**. The file itself is named `en-pos-maxent.bin`, and I put it into
    the `data` directory of my project.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取这些信息，我们将使用OpenNLP的预训练POS标记器。这个标记器将一个词与一个词性关联起来。为了使用它，我们需要下载训练模型文件。你可以在[http://opennlp.sourceforge.net/models-1.5/](http://opennlp.sourceforge.net/models-1.5/)找到它。下载带有**最大熵模型和标记字典**描述的**英语POS标记器**。文件本身命名为`en-pos-maxent.bin`，我将它放入了我项目的`data`目录中。
- en: This tagger uses the POS tags defined by the Penn Treebank ([http://www.cis.upenn.edu/~treebank/](http://www.cis.upenn.edu/~treebank/)).
    It uses a trained, probabilistic tagger to associate tags with each token from
    a sentence. For example, it might associate the token things with the `NNS` tag,
    which is the abbreviation for plural nouns. We'll create the string for this feature
    by putting these two together so that this feature would look like `things_NNS`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此标注器使用宾州树库（[http://www.cis.upenn.edu/~treebank/](http://www.cis.upenn.edu/~treebank/））定义的词性标签。它使用一个训练好的、概率性的标注器将标签与句子中的每个标记相关联。例如，它可能会将标记
    things 与 `NNS` 标签相关联，这是复数名词的缩写。我们将通过将这些两个结合起来创建这个特征的字符串，这样这个特征就会看起来像 `things_NNS`。
- en: 'Once we have the data file, we need to load it into a POS model. We''ll write
    a function to do this and return the tagger object:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了数据文件，我们需要将其加载到词性标注模型中。我们将编写一个函数来完成这个任务并返回标注器对象：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Using the tagger is pretty easy. We just call its tag method as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标注器相当简单。我们只需调用它的标注方法，如下所示：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now that we have these functions ready, let's take a short sentence and generate
    the features for it. For this set of examples, we'll use the clauses, `Time flies
    like an arrow; fruit flies like a banana`. To begin with, we'll define the input
    data and load the POS tagger.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了这些函数，让我们用一个简短的句子来生成它的特征。对于这组示例，我们将使用子句，`Time flies like an arrow;
    fruit flies like a banana`。首先，我们将定义输入数据和加载词性标注器。
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the last output, the words are associated with part-of-speech tags. This
    output uses the tags from the Penn Treebank ([http://www.cis.upenn.edu/~treebank/](http://www.cis.upenn.edu/~treebank/)).
    You can look at it for more information, but briefly, here are the tags used in
    the preceding code snippet:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后的输出中，单词与词性标签相关联。此输出使用来自宾州树库（[http://www.cis.upenn.edu/~treebank/](http://www.cis.upenn.edu/~treebank/））的标签。你可以查看它以获取更多信息，但简要来说，以下是前面代码片段中使用的标签：
- en: '`NN` means noun;'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NN` 表示名词；'
- en: '`VBZ` means the present tense verb, third person, singular;'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VBZ` 表示现在时动词，第三人称单数；'
- en: '`IN` means and, the preposition or subordinating conjunction'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IN` 表示和，介词或从属连词'
- en: '`DT` means the determiner.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DT` 表示限定词。'
- en: So we can see that the POS-tagged features provide the most data on the single
    tokens; however, the n-grams (bigrams and trigrams) provide more information about
    the context around each word. Later on, we'll see which one gets better results.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到，词性标注的特征提供了关于单个标记的最多的数据；然而，n-gram（双词和三词）提供了关于每个词周围上下文更多的信息。稍后，我们将看到哪一个能得到更好的结果。
- en: Now that we have the preprocessing out of way, let's turn our attention to the
    documents and how we want to structure the rest of the experiment.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经处理好了预处理，让我们将注意力转向文档以及我们想要如何构建实验的其余部分。
- en: Cross-validating the results
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证结果
- en: As I've already mentioned, the dataset for this chapter is a manually coded
    group of 500 hotel reviews taken from the OpinRank dataset. For this experiment,
    we'll break these into 10 chunks of 50 reviews each.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，本章的数据集是从 OpinRank 数据集中手动编码的 500 条酒店评论。对于这个实验，我们将它们分成 10 个包含 50 条评论的块。
- en: These chunks will allow us to use **K-fold cross validation** to test how our
    system is doing. Cross validation is a way of checking your algorithm and procedures
    by splitting your data up into equally sized chunks. You then train your data
    on all of the chunks but one; that is the training set. You calculate the error
    after running the trained system on the validation set. Then, you use the next
    chunk as a validation set and start over again. Finally, we can average the error
    for all of the trials.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些块将使我们能够使用 **K 折交叉验证**来测试我们的系统表现如何。交叉验证是一种通过将数据分成大小相等的块来检查你的算法和流程的方法。你然后在除了一个块之外的所有块上训练你的数据；这就是训练集。你在运行训练系统在验证集上后计算错误。然后，你使用下一个块作为验证集并重新开始。最后，我们可以对所有试验的平均错误进行平均。
- en: For example, the validation procedure uses four folds, A, B, C, and D. For the
    first run, A, B, and C would be the training set, and D would be the test set.
    Next, A, B, and D would be the training set, and C would be the test set. This
    would continue until every fold is used as the test set once.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，验证过程使用四个折，A、B、C 和 D。对于第一次运行，A、B 和 C 将是训练集，而 D 将是测试集。接下来，A、B 和 D 将是训练集，而 C
    将是测试集。这将一直持续到每个折都至少作为一次测试集使用。
- en: This may seem like a lot of work, but it helps us makes sure that we didn't
    just get lucky with our choice of training or validation data. It provides a much
    more robust way of estimating the error rates and accuracy of our classifier.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来像很多工作，但它有助于我们确保我们没有仅仅因为训练或验证数据的选择而侥幸成功。它提供了一种更稳健的方式来估计分类器的错误率和准确性。
- en: The main trick in implementing cross validation is that Clojure's native partitioning
    functions (`partition` and `partition-all`) don't handle extra items exactly the
    way we'd like. The `partition` function just throws the extras away, and `partition-all`
    sticks all of the extras to the end in a smaller group. What we'd like is to include
    the extras in the previous chunks. Each chunk should have one extra until all
    of the remainders are exhausted. To handle this, we'll define a function named
    `partition-spread`. It will partition the first part of the collection into larger
    chunks and the second part into smaller chunks.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 实现交叉验证的主要技巧是Clojure的本地分区函数（`partition`和`partition-all`）并不完全按照我们期望的方式处理额外项。`partition`函数只是丢弃额外项，而`partition-all`则将所有额外项粘附在较小的组末尾。我们希望的是将额外项包含在之前的块中。每个块应该有一个额外项，直到所有余数都耗尽。为了处理这个问题，我们将定义一个名为`partition-spread`的函数。它将集合的前一部分分成较大的块，后一部分分成较小的块。
- en: 'Unfortunately, we''ll need to know the size of the input collection. To do
    this, we must hold the entire collection in the memory at once, so this algorithm
    isn''t good for very large sequences:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们需要知道输入集合的大小。为此，我们必须一次性将整个集合保存在内存中，因此这个算法不适合非常大的序列：
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can now see how these partitioning functions differ:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以看到这些分区函数的不同之处：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can also see that the semantics of the first parameter have changed. Instead
    of indicating the size of the partitions, it specifies the number of partitions.
    Now the partitions are all of a roughly equal size.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到第一个参数的语义已经改变。它不再表示分区的大小，而是指定分区的数量。现在，所有分区的大小大致相等。
- en: Next, we'll create a couple of functions that pull out each chunk to use as
    the validation set and concatenates all the other chunks.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建几个函数，用于提取每个块作为验证集，并将所有其他块连接起来。
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, by partitioning into chunks with one element each, we can clearly see
    just how the K-fold partitioning works. Each time, a new chunk is selected as
    the validation set (the first item), and the rest of the chunks are concatenated
    into the training set (the second item):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过将数据分成每个元素一个块的块，我们可以清楚地看到K折分区是如何工作的。每次，一个新的块被选为验证集（第一个项目），其余的块被连接成训练集（第二个项目）：
- en: '[PRE17]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we can define a function that controls the K-fold validation process. It
    takes the training and error steps as function parameters, and it just handles
    partitioning the data into groups, calling the training and error functions, and
    combining their output into one result:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义一个函数来控制K折验证过程。它接受训练和错误步骤作为函数参数，它只是处理将数据分成组，调用训练和错误函数，并将它们的输出组合成一个结果：
- en: '[PRE18]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now we need to decide what constitutes an error and how we'll compute it.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要决定什么构成了错误以及我们将如何计算它。
- en: Calculating error rates
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算错误率
- en: 'To calculate the error rates on classification algorithms, we''ll keep count
    of several things. We''ll track how many positives are correctly and incorrectly
    identified as well as how many negatives are correctly and incorrectly identified.
    These values are usually called true positives, false positives, true negatives,
    and false negatives. The relationship of these values to the expected values and
    the classifier''s outputs and to each other can be seen in the following diagram:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算分类算法的错误率，我们需要记录几件事情。我们将跟踪有多少正例被正确和错误地识别，以及有多少负例被正确和错误地识别。这些值通常被称为真正例、假正例、真负例和假负例。这些值与预期值、分类器的输出以及彼此之间的关系可以在以下图表中看到：
- en: '![Calculating error rates](img/4139OS_06_01.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![计算错误率](img/4139OS_06_01.jpg)'
- en: From these numbers, we'll first calculate the precision of the algorithm. This
    is the ratio of true positives to the number of all identified positives (both
    true and false positives). This tells us how many of the items that it identified
    as positives actually are positives.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些数字中，我们首先计算算法的精确度。这是真正例与所有识别出的正例数量（包括真正例和假正例）的比率。这告诉我们它识别为正例的项目中有多少实际上是正例。
- en: We'll then calculate the recall. This is the ratio of true positives to all
    actual positives (true positives and false negatives). This gives us an idea of
    how many positives it's missing.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来将计算召回率。这是所有实际正例（真正例和假负例）中真正例的比例。这让我们了解到有多少正例被遗漏了。
- en: 'To calculate this, we''ll use a standard `reduce` loop. First, we''ll write
    the accumulator function for it. This will take a mapping of the counts that we
    need to tally and a pair of ratings, the expected and the actual. Depending on
    what they are and whether they match, we''ll increment one of the counts as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算这个，我们将使用标准的`reduce`循环。首先，我们将为它编写累加器函数。这将接受一个我们需要计数的映射和一对评分，预期的和实际的。根据它们是什么以及它们是否匹配，我们将按照以下方式增加一个计数：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Once we have the counts for a test set, we''ll need to summarize these counts
    into the figure for precision and recall:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了测试集的计数，我们需要将这些计数汇总成精确度和召回率的数值：
- en: '[PRE20]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With these two defined, the function to actually calculate the error is standard
    Clojure:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了这两个函数后，实际计算错误的函数是标准的Clojure：
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can do something similar to determine the mean error of a collection of
    precision/recall mappings. We could simply figure the value for each key separately,
    but rather than walking over the collection multiple times, we will do something
    more complicated and walk over it once while calculating the sums for each key:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过类似的方式确定一组精确度/召回率映射的平均误差。我们可以简单地分别计算每个键的值，但与其多次遍历集合，我们将会做更复杂的事情，只遍历一次集合同时计算每个键的总和：
- en: '[PRE22]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: These functions will give us a good grasp of the performance of our classifiers
    and how well they do at identifying the sentiments expressed in the data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数将帮助我们了解我们分类器的性能以及它们在识别数据中表达的情感方面做得如何。
- en: Using the Weka machine learning library
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Weka机器学习库
- en: We're going to test a couple of machine learning algorithms that are commonly
    used for sentiment analysis. Some of them are implemented in the OpenNLP library.
    However, they do not have anything for others algorithms. So instead, we'll use
    the Weka machine learning library ([http://www.cs.waikato.ac.nz/ml/weka/](http://www.cs.waikato.ac.nz/ml/weka/)).
    This doesn't have the classes to tokenize or segment the data that an application
    in a natural language processing requires, but it does have a more complete palette
    of machine learning algorithms.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将测试一些常用的机器学习算法，这些算法通常用于情感分析。其中一些在OpenNLP库中实现。然而，它们没有其他算法。因此，我们将使用Weka机器学习库（[http://www.cs.waikato.ac.nz/ml/weka/](http://www.cs.waikato.ac.nz/ml/weka/)）。这个库没有用于分词或分割数据的类，这些数据是自然语言处理应用所需的，但它确实提供了一套更完整的机器学习算法。
- en: All of the classes in the Weka library also have a standard, consistent interface.
    These classes are really designed to be used from the command line, so each takes
    its options as an array of strings with a command-line-like syntax. For example,
    the array for a naive Bayesian classifier may have a flag to indicate that it
    should use the kernel density estimator rather than the normal distribution. This
    would be indicated by the `-K` flag being included in the option array. Other
    options may include a parameter that would follow the option in the array. For
    example, the logistic regression classifier can take a parameter to indicate the
    maximum number of iterations it should run. This would include the items `-M`
    and `1000` (say) in the options array.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Weka库中的所有类也都有一个标准、一致的接口。这些类实际上是为从命令行使用而设计的，因此每个类都接受一个字符串数组作为选项，具有类似命令行的语法。例如，朴素贝叶斯分类器的数组可能有一个标志，表示它应该使用核密度估计器而不是正态分布。这将在选项数组中通过包含`-K`标志来表示。其他选项可能包括一个参数，该参数将跟随数组中的选项。例如，逻辑回归分类器可以接受一个参数来指示它应该运行的最大迭代次数。这将在选项数组中包括`-M`和`1000`（例如）。
- en: The Clojure interface functions for these classes are very regular. In fact,
    they're almost boilerplate. Unfortunately, they're also a little redundant. Option
    names are repeated in the functions' parameter list, the default values for those
    parameters, and where the parameters are fed into the options array. It would
    be better to have one place for a specification of each option, its name, its
    flag, its semantics, and its default value.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类的Clojure接口函数非常规范。实际上，它们几乎是模板化的。不幸的是，它们也有些冗余。选项名称在函数的参数列表、那些参数的默认值以及参数被输入到选项数组中的地方都被重复了。最好有一个地方可以指定每个选项，包括它的名称、标志、语义和默认值。
- en: This is a perfect application of Clojure's macro system. The data to create
    the functions can be transformed into the function definition, which is then compiled
    into the interface function.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是Clojure宏系统的完美应用。创建函数所需的数据可以转换成函数定义，然后编译成接口函数。
- en: The final product of this is the `defanalysis` macro, which takes the name of
    the function, the class, the method it's based on, and the options it accepts.
    We'll see several uses of it later in this chapter.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果是`defanalysis`宏，它接受函数名称、类、基于的方法以及它接受的选项。我们将在本章后面看到它的几个用法。
- en: Unfortunately, at almost 40 lines, this system is a little long and disruptive
    to include here, however interesting it may be. You can find this in the `src/sentiment/weka.clj`
    file in the code download, and I have discussed it in a bit more length in *Clojure
    Data Analysis Cookbook*, *Packt Publishing*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个系统大约有40行代码，虽然很有趣，但在这里包含它可能会有些长且分散注意力。您可以在代码下载中的`src/sentiment/weka.clj`文件中找到它，我在*Clojure
    Data Analysis Cookbook*，*Packt Publishing*中对其进行了更详细的讨论。
- en: We do still need to convert the `HotelReview` records that we loaded earlier
    into a `WekaInstances` collection. We'll need to do this several times as we train
    and test the classifiers, and this will provide us with a somewhat shorter example
    of interacting with Weka.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然需要将之前加载的`HotelReview`记录转换为`WekaInstances`集合。在训练和测试分类器时，我们需要多次进行此操作，这将为我们提供一个与Weka交互的简短示例。
- en: To store a data matrix, Weka uses an `Instances` object. This implements a number
    of standard Java collection interfaces, and it holds objects that implement the
    `Instance` interface, such as `DenseInstance` or `SparseInstance`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了存储数据矩阵，Weka使用`Instances`对象。它实现了多个标准的Java集合接口，并持有实现`Instance`接口的对象，如`DenseInstance`或`SparseInstance`。
- en: 'Instances also keep track of which fields each item has in its collection of
    `Attribute` objects. To create these, we''ll populate `ArrayList` with all of
    the features that we accumulated in the feature index. We''ll also create a feature
    for the ratings and add it to `ArrayList`. We''ll return both the full collection
    of the attributes and the single attribute for the review''s rating:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 实例还跟踪其`Attribute`对象集合中每个项目具有哪些字段。为了创建这些，我们将使用在特征索引中积累的所有特征填充`ArrayList`。我们还将创建一个用于评级的特征并将其添加到`ArrayList`中。我们将返回属性的全集以及评论评级的单个属性：
- en: '[PRE23]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: (At this point, we're hardcoding the markers for the sentiments as a plus sign
    and a negative sign. However, these could easily be made into parameters for a
    more flexible system.)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: （在这个阶段，我们硬编码了表示情感的标记为加号和减号。然而，这些可以很容易地变成更灵活系统的参数。）
- en: 'Each hotel review itself can be converted separately. As most documents will
    only have a fraction of the full number of features, we''ll use `SparseInstance`.
    Sparse vectors are more memory efficient if most of the values in the instance
    are zero. If the feature is nonzero in the feature vector, we''ll set it in `Instance`.
    Finally, we''ll also set the rating attribute as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 每个酒店评论本身可以单独转换。由于大多数文档将只有全部特征的一部分，我们将使用`SparseInstance`。如果实例中的大多数值都是零，稀疏向量在内存效率上更优。如果特征向量中的特征不为零，我们将将其设置在`Instance`中。最后，我们还将设置如下评级属性：
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'With these, we can populate `Instances` with the data from the `HotelReview`
    records:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些工具，我们可以将来自`HotelReview`记录的数据填充到`Instances`中：
- en: '[PRE25]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now we can define some functions to sit between the cross-validation functions
    we defined earlier and the Weka interface functions.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义一些函数，它们将位于我们之前定义的交叉验证函数和Weka接口函数之间。
- en: Connecting Weka and cross-validation
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接Weka和交叉验证
- en: 'The first of these functions will classify an instance and determine which
    rating symbol it is classified by (`+` or `-`), given the distribution of probabilities
    for each category. This function is used to run the classifier on all data in
    an `Instances` object:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数中的第一个将根据每个类别的概率分布对实例进行分类，并确定它被分类为哪个评级符号（`+`或`-`）。此函数用于在`Instances`对象中的所有数据上运行分类器：
- en: '[PRE26]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The next function defines the cross-validation procedure for a group of `HotelReview`
    records. This function actually takes a training function and returns a function
    that takes the feature index and collection of `HotelReview` records and actually
    performs the cross validation. This will allow us to create some wrapper functions
    for each type of classifier:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个函数定义了一组 `HotelReview` 记录的交叉验证过程。这个函数实际上接受一个训练函数，并返回一个函数，该函数接受特征索引和 `HotelReview`
    记录的集合，并执行交叉验证。这将使我们能够为每种类型的分类器创建一些包装函数：
- en: '[PRE27]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: When executed, this function will return a list of ten of whatever the `do-test`
    function returns. In this case, that means a list of ten precision and recall
    mappings. We can average the output of this to get a summary of each classifier's
    performance.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行时，这个函数将返回 `do-test` 函数返回的十个结果。在这种情况下，这意味着一个包含十个精确率和召回率映射的列表。我们可以平均这个输出，以获得每个分类器性能的摘要。
- en: Now we can start actually defining and testing classifiers.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们实际上可以开始定义和测试分类器了。
- en: Understanding maximum entropy classifiers
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解最大熵分类器
- en: Maximum entropy (maxent) classifiers are, in a sense, very conservative classifiers.
    They assume nothing about hidden variables and base their classifications strictly
    upon the evidence they've been trained on. They are consistent with the facts
    that they've seen, but all other distributions are assumed to be completely uniform
    otherwise. What does this mean?
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最大熵（maxent）分类器在某种程度上是非常保守的分类器。它们对隐藏变量没有任何假设，并且严格基于它们所训练的证据进行分类。它们与它们所看到的事实一致，但所有其他分布都被假定为完全均匀。这意味着什么？
- en: 'Let''s say that we have a set of reviews and positive or negative ratings,
    and we wish to be able to predict the value of ratings when the ratings are unavailable,
    given the tokens or other features in the reviews. The probability that a rating
    is positive would be p(+). Initially, before we see any actual evidence, we may
    intuit that this probability would be uniform across all possible features. So,
    for a set of five features, before training, we might expect the probability function
    to return these values:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一组评论和正面或负面评分，我们希望能够预测评分的值，当评分不可用，给定评论中的标记或其他特征时。评分是正面的概率将是 p(+)。最初，在我们看到任何实际证据之前，我们可能会直觉地认为这个概率将在所有可能特征上均匀分布。所以，对于一组五个特征，在训练之前，我们可能期望概率函数返回这些值：
- en: '| p(+) | ½ |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| p(+) | ½ |'
- en: '| p(-) | ½ |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| p(-) | ½ |'
- en: This is perfectly uniform but not very useful. We have to make observations
    from the data in order to train the classifier.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这完全均匀，但不太有用。我们必须从数据中做出观察，以便训练分类器。
- en: The process of training involves observing the features in each document and
    its rating and determining the probability of any given feature that is found
    in a document with a given rating. We'll denote this as p(x, y) or the probability
    as feature x and rating y.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程涉及观察每份文档的特征及其评分，并确定在具有给定评分的文档中找到的任何给定特征的概率。我们将这表示为 p(x, y) 或特征 x 和评分 y 的概率。
- en: These features impose constraints on our model. As we gather more and more constraints,
    figuring a consistent and uniform distribution for the non-constrained probabilities
    in the model becomes increasingly difficult.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征对我们的模型施加了约束。随着我们收集越来越多的约束，为模型中非约束概率找到一致和均匀的分布变得越来越困难。
- en: Essentially, this is the maxent algorithm's job. It takes into account all of
    the constraints imposed by the probabilities found in the training data, but it
    maintains a uniform distribution on everything that's unconstrained. This provides
    a more consistent, stronger algorithm overall, and it still performs very well,
    usually. Also, cross validation can help us evaluate its performance.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，这是最大熵算法的工作。它考虑了训练数据中找到的所有概率约束，但它在所有无约束的事物上保持均匀分布。这提供了一个更一致、更强的算法，通常表现也非常好。此外，交叉验证可以帮助我们评估其性能。
- en: Another benefit is that maxent doesn't make any assumptions about the relationships
    between different features. In a bit, we'll look at a naive Bayesian classifier,
    and it does make an assumption about the relationships between the features, an
    often unrealistic assumption. Because maxent does not make that assumption, it
    can better match the data involved.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个好处是，maxent 对不同特征之间的关系没有做出任何假设。稍后我们将查看一个朴素贝叶斯分类器，它确实对特征之间的关系做出了假设，这是一个通常不切实际的假设。因为
    maxent 没有做出那个假设，它可以更好地匹配涉及的数据。
- en: 'For this chapter, we''ll use the maxent classifier found in the Weka class,
    `weka.classifiers.functions.Logistic` (maxent is equivalent to the logistic regression,
    which attempts to classify data based on a binary categorical label, which is
    based on one or more features). We''ll use the `defanalysis` macro to define a
    utility function that cross validates a logistic regression classifier as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将使用 Weka 类中的 maxent 分类器，即 `weka.classifiers.functions.Logistic`（maxent
    等同于逻辑回归，它试图根据二元分类标签对数据进行分类，该标签基于一个或多个特征）。我们将使用 `defanalysis` 宏来定义一个效用函数，如下交叉验证逻辑回归分类器：
- en: '[PRE28]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now let's define something similar for a naive Bayesian classifier.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们为朴素贝叶斯分类器定义一个类似的概念。
- en: Understanding naive Bayesian classifiers
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解朴素贝叶斯分类器
- en: A common, generally well-performing classifier is the naive Bayesian classifier.
    It's naive because it makes an assumption about that data and the features; it
    assumes that the features are independent of each other. That is, the probability
    of, say, *good* occurring in a document is not influenced at all by the probability
    of any other token or feature, such as, say, *not*. Unfortunately, language doesn't
    work this way, and there are dependencies all through the features of any linguistic
    dataset.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见且通常表现良好的分类器是朴素贝叶斯分类器。它之所以被称为朴素，是因为它对数据以及特征做出了假设；它假设特征之间是相互独立的。也就是说，例如，*good*
    在文档中出现的概率不会受到任何其他标记或特征（例如，*not*）的概率的影响。不幸的是，语言并不按这种方式运作，任何语言数据集的特征之间都存在依赖关系。
- en: Fortunately, even when the data and features are not completely independent,
    this classifier often still performs quite well in practice. For example, in *An
    analysis of data characteristics that affect naive Bayes performance* by *Irina
    Rish*, *Joseph Hellerstein*, and *Jayram Thathachar*, it was found that Bayesian
    classifiers perform best with features that are completely independent or functionally
    dependent.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，即使数据和特征并不完全独立，这个分类器在实践中通常仍然表现相当好。例如，在 Irina Rish、Joseph Hellerstein 和 Jayram
    Thathachar 的文章《影响朴素贝叶斯性能的数据特征分析》中，发现贝叶斯分类器在特征完全独立或功能依赖时表现最佳。
- en: 'This classifier works by knowing several probabilities and then using Bayes''
    theorem to turn them around to predict the classification of the document. The
    following are the probabilities that it needs to know:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分类器通过知道几个概率，然后使用贝叶斯定理将它们转换过来以预测文档的分类。它需要知道以下概率：
- en: It needs to know the probability for each feature in the training set. We'll
    call this **p(F)**. Say the word *good* occurs in 40 percent of the documents.
    This is the evidence of the classification.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要知道训练集中每个特征的概率。我们将称之为 **p(F**)。比如说，单词 *good* 在 40% 的文档中出现。这是分类的证据。
- en: It needs to know the probability that a document will be part of a classification.
    We'll call this **p(C)**. Say that the rate of positive ratings in the corpus
    of reviews is 80 percent. This is the prior distribution.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要知道一个文档成为分类一部分的概率。我们将称之为 **p(C**)。比如说，在评论语料库中正面评价的比率是 80%。这是先验分布。
- en: Now it needs to know the probability that the good feature is in the document
    if the document is rated positively. This is **p(F|C)**. For this hypothetical
    example, say that *good* appears in 40 percent of the positive reviews. This is
    the likelihood.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在它需要知道，如果文档被正面评价，那么包含良好特征的概率是 **p(F|C**)。对于这个假设的例子，假设 *good* 在 40% 的正面评价中出现。这是似然。
- en: Bayes theorem allows us to turn this around and compute the probability that
    a document is positively rated, if it contains the feature *good*.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理允许我们将其反过来计算，如果文档包含特征 *good*，则该文档被正面评价的概率。
- en: '![Understanding naive Bayesian classifiers](img/4139OS_06_02.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![理解朴素贝叶斯分类器](img/4139OS_06_02.jpg)'
- en: For this example, this turns out to be `(0.8)(0.4) / 0.4`, or 0.8 (80 percent).
    So, if the document contains the feature *good*, it is very likely to be positively
    rated.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，结果是 `(0.8)(0.4) / 0.4`，即 0.8（80%）。所以，如果文档包含特征 *good*，它很可能被正面评价。
- en: Of course, things begin to get more and more interesting as we start to track
    more and more features. If the document contains both *not* and *good*, for instance,
    the probability that the review is positive may change drastically.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，当我们开始跟踪越来越多的特征时，事情开始变得更有趣。例如，如果文档中既有 *not* 又有 *good*，那么评论为正面的概率可能会发生剧烈变化。
- en: 'The Weka implementation of a naive Bayesian classifier is found in `weka.classifiers.bayes.NaiveBayes`,
    and we''ll wrap it in a manner that is similar to the one we used for the maxent
    classifier:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Weka 实现的朴素贝叶斯分类器位于 `weka.classifiers.bayes.NaiveBayes`，我们将以类似于我们用于最大熵分类器的方式包装它：
- en: '[PRE29]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now that we have both the classifiers in place, let's look again at the features
    we'll use and how we'll compare everything.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了分类器，让我们再次看看我们将使用的特征以及我们将如何比较一切。
- en: Running the experiment
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行实验
- en: 'Remember, earlier we defined functions to break a sequence of tokens into features
    of various sorts: unigrams, bigrams, trigrams, and POS-tagged unigrams. We can
    take these and automatically test both the classifiers against all of these types
    of features. Let''s see how.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们之前定义了函数来将一系列标记分解成各种类型的特征：单词、二元组、三元组和带词性标注的单词。我们可以使用这些特征并自动测试所有这些类型的特征对分类器的效果。让我们看看如何。
- en: 'First, we''ll define some top-level variables that associate label keywords
    with the functions that we want to test at that point in the process (that is,
    classifiers or feature-generators):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义一些顶层变量，将标签关键词与我们在该过程点想要测试的函数（即分类器或特征生成器）关联起来（即，分类器或特征生成器）：
- en: '[PRE30]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We can now iterate over both of these hash maps and cross-validate these classifiers
    on these features. We'll average the error information (the precision and recall)
    for all of them and return the averages. Once we've executed that, we can spend
    some time looking at the results.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以遍历这两个哈希表，并交叉验证这些分类器在这些特征上的表现。我们将对所有分类器的错误信息（精确度和召回率）进行平均，并返回平均值。一旦执行了这一步，我们就可以花些时间查看结果。
- en: 'For the inner-most loop of this process, we''ll take a collection of features
    and a classifier and cross validate them. This is pretty straightforward; it simply
    constructs an identifying key out of the keywords for the feature generator and
    the classifier, runs the cross validation, and averages the output error information
    as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此过程的内部循环，我们将取一组特征和一个分类器，并交叉验证它们。这很简单；它只是从特征生成器的关键词和分类器构建一个标识键，运行交叉验证，并按以下方式平均输出错误信息：
- en: '[PRE31]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, given a set of features, we''ll call `do-class` on each classifier one
    loop up. Constructing the loop this way by generating the features and then looping
    on the classifiers keeps us from needing to regenerate the same set of features
    multiple times:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，给定一组特征，我们将对每个分类器在上一个循环中调用 `do-class`。通过生成特征然后对分类器进行循环来构建这个循环，我们避免了需要多次重新生成相同特征集的需求：
- en: '[PRE32]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The controlling function for this process simply calls `do-features` on each
    set of feature-generating functions and stores all the outputs into a hash map:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程的控制函数简单地对每一组特征生成函数调用 `do-features`，并将所有输出存储到一个哈希表中：
- en: '[PRE33]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This takes a while to execute:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要一段时间来执行：
- en: '[PRE34]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now we can start looking at the data in more detail.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以更详细地查看数据。
- en: Examining the results
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查结果
- en: 'First, let''s examine the precision of the classifiers. Remember that the precision
    is how well the classifiers do at only returning positive reviews. This indicates
    the percentage of reviews that each classifier has identified as being positive
    is actually positive in the test set:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们检查分类器的精确度。记住，精确度是指分类器在仅返回正面评论方面做得有多好。这表明每个分类器识别为正面的评论在测试集中实际上是正面的百分比：
- en: '![Examining the results](img/4139OS_06_03.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![检查结果](img/4139OS_06_03.jpg)'
- en: We need to remember a couple of things while looking at this graph. First, sentiment
    analysis is difficult, compared to other categorization tasks. Most importantly,
    human raters only agree about 80 percent of the time. So, the bar seen in the
    preceding figure that almost reaches 65 percent is actually decent, if not great.
    Still, we can see that the naive Bayesian classifier generally outperforms the
    maxent one for this dataset, especially when using unigram features. It performed
    less well for the bigram and trigram features, and slightly lesser for the POS-tagged
    unigrams.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看此图表时，我们需要记住一些事情。首先，与其他分类任务相比，情感分析是困难的。最重要的是，人类评分员只有大约 80% 的时间达成一致。因此，前一个图表中几乎达到
    65% 的条形实际上相当不错，如果不是非常好的。尽管如此，我们可以看到，对于这个数据集，朴素贝叶斯分类器通常优于最大熵分类器，尤其是在使用单词特征时。对于二元组和三元组特征，它的表现较差，对于带词性标注的单词，表现略差。
- en: We didn't try tagging the bigram and trigrams with POS information, but that
    might have been an interesting experiment. Based on what we can see here, these
    feature generators would not get better results than what we've already tested,
    but it would be good to know that more definitively.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有尝试对二元和三元特征进行词性标注，但这可能是一个有趣的实验。根据我们在这里看到的情况，这些特征生成器不会比我们已经测试过的结果更好，但确定这一点会更好。
- en: 'It''s interesting to see that maxent performed best with trigrams. Generally,
    compared to unigrams, trigrams pack more information into each feature, as they
    encode some implicit syntactical information into each feature. However, each
    feature also occurs fewer times, which makes performing some statistical processes
    on it more difficult. Remember that recall is the percentage of positives in the
    test set that were correctly identified by each classifier. Now let''s look at
    the recall of these classifiers:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 看到最大熵分类器在三元特征上表现最佳很有趣。一般来说，与单语素相比，三元特征将更多的信息压缩到每个特征中，因为它们将一些隐含的句法信息编码到每个特征中。然而，每个特征出现的次数也更少，这使得对其进行某些统计处理变得更加困难。记住，召回率是测试集中每个分类器正确识别的正面的百分比。现在让我们看看这些分类器的召回率：
- en: '![Examining the results](img/4139OS_06_04.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![检查结果](img/4139OS_06_04.jpg)'
- en: First, while the naive Bayesian classifier still outperforms the maxent classifier,
    this time the bigram and trigram get much better results than the unigram or POS-tagged
    features.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，尽管朴素贝叶斯分类器仍然优于最大熵分类器，但这次二元和三元特征比单语素或词性标注特征得到了更好的结果。
- en: Also, the recall numbers on these two tests are better than any of the values
    for the precision. The best part is that the naive Bayes bigram test had a recall
    of just over 90 percent.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这两个测试的召回率比精确度的任何值都要好。最好的部分是，朴素贝叶斯二元测试的召回率略超过90%。
- en: In fact, just looking at the results, there appeared to be an inverse relationship
    between the precision and the recall, as there typically is. Tests with high precision
    tended to have lower recall numbers and vice versa. This makes intuitive sense.
    A classifier can get a high recall number by marking more reviews as positive,
    but that negatively impacts its precision. Or, a classifier can have better precision
    by being more selective in what it marks as positive but also noting that will
    drag down its recall.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，仅从结果来看，精确度和召回率之间似乎存在一种反向关系，这通常是如此。高精确度的测试往往召回率较低，反之亦然。这从直觉上是有道理的。一个分类器可以通过将更多评论标记为正面来获得高召回率，但这会对其精确度产生负面影响。或者，一个分类器可以通过更严格地标记正面来提高其精确度，但这也将降低其召回率。
- en: Combining the error rates
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合错误率
- en: 'We can combine these two into a single metric using the harmonic mean of the
    precision and recall, also known as the F-measure. We''ll compute this with the
    following function:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用精确度和召回率的调和平均值（也称为F度量）将这两个指标合并为一个。我们将使用以下函数来计算：
- en: '[PRE35]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This gives us a way to combine the precision and recall in a rational, meaningful
    manner. Let''s see what values it gives for the F-measure:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一种以合理、有意义的方式结合精确度和召回率的方法。让我们看看它为F度量给出的值：
- en: '![Combining the error rates](img/4139OS_06_05.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![结合错误率](img/4139OS_06_05.jpg)'
- en: So, as we've already noticed, the naive Bayesian classifier performed better
    than the maxent classifier in general, and on balance, the bigram features worked
    best for this classifier.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正如我们已经注意到的，朴素贝叶斯分类器在总体上比最大熵分类器表现更好，而且从平衡的角度来看，二元特征对这种分类器来说效果最佳。
- en: While this gives us a good starting point, we'll also want to consider why we're
    looking for this information, how we'll use it, and what penalties are involved.
    If it's vitally important that we get all the positive reviews, then we will definitely
    want to use the naive Bayesian classifier with the bigram features. However, if
    the cost of missing some isn't so high but the cost of having to sort through
    too many false results is high, then we'll probably want to use unigram features,
    which would minimize the number of false results we have to manually sort through
    later.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这为我们提供了一个良好的起点，但我们还需要考虑为什么我们要寻找这些信息，我们将如何使用它，以及可能涉及的惩罚。如果得到所有正面评价至关重要，那么我们肯定会想使用具有二元特征的朴素贝叶斯分类器。然而，如果错过一些信息的成本不是那么高，但必须处理大量错误结果的成本很高，那么我们可能更想使用单语素特征，这样可以最小化我们后来必须手动排序的错误结果数量。
- en: Improving the results
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高结果
- en: What could we do to improve these results?
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做些什么来提高这些结果？
- en: First, we should improve the test and training sets. It would be good to have
    multiple raters, say, have each review independently reviewed three times and
    use the rating that was chosen two or three times.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该改进测试集和训练集。最好有多个评分者，比如说，每个评论独立评审三次，并使用被选中两次或三次的评分。
- en: Most importantly, we'd like to have a larger and better test set and training
    set. For this type of problem, having 500 observations is really on the low end
    of what you can do anything useful with, and you can expect the results to improve
    with more observations. However, I do need to stress on the fact that more training
    data doesn't necessarily imply better results. It could help, but there are no
    guarantees.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，我们希望有一个更大、更好的测试集和训练集。对于这类问题，拥有500个观测值实际上是你能做任何有用事情的最低限度，而且你可以预期随着观测值的增加，结果会得到改善。然而，我必须强调，更多的训练数据并不一定意味着更好的结果。它可能有所帮助，但没有任何保证。
- en: We could also look at improving the features. We could select them more carefully,
    because having too many useless or unneeded features can make the classifier perform
    poorly. We could also select different features such as dates or information about
    the informants; if we had any data on them, it might be useful.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以考虑改进特征。我们可以更仔细地选择它们，因为拥有太多无用或不必要的特点可能会使分类器表现不佳。我们还可以选择不同的特征，如日期或关于信息提供者的信息；如果我们有任何关于它们的数据，这可能是有用的。
- en: There has also been more recent work in moving beyond polarity classification,
    such as looking at emotional classification. Another way of being more fine grained
    than binary categorization is to classify the documents on a scale. For instance,
    instead of positive or negative, these classifiers could try to predict how the
    user would rate the product on a five-star scale, such as what has become popular
    on **Amazon** and many websites that include user ratings and reviews.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 最近也进行了一些超越极性分类的工作，例如查看情感分类。另一种比二元分类更细粒度的方式是对文档进行评分。例如，而不是正面或负面，这些分类器可以尝试预测用户会在五星级评分上如何评分产品，比如在**亚马逊**和许多包含用户评分和评论的网站上变得流行。
- en: Once we have identified the positive or negative reviews, we can apply other
    analyses separately to those reviews, whether its topic modeling, named entity
    recognition, or something else.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定了正面或负面的评论，我们就可以分别对这些评论应用其他分析，无论是主题建模、命名实体识别还是其他。
- en: Summary
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In the end, sentiment analysis is a simple tool to analyze documents according
    to two complex, possibly ill-defined categories. Although language is used in
    complex ways, modern sentiment analysis techniques can do almost as well as humans,
    which, admittedly, isn't particularly efficient.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，情感分析是一种简单的工具，根据两个复杂、可能不明确的类别来分析文档。尽管语言被以复杂的方式使用，但现代情感分析技术几乎可以做到与人类一样好，尽管这并不特别高效。
- en: What's most powerful about these techniques is that they can provide answers
    to questions that cannot be answered in other ways. As such, they're an important
    part of the data analyst's toolbox.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术最强大的地方在于，它们可以提供其他方式无法解答的问题的答案。因此，它们是数据分析师工具箱中的重要组成部分。
- en: In the next chapter, we'll look at null hypothesis testing, which is a standard
    and foundational technique of traditional statistics. This informs how we approach
    many experiments and how we frame the questions that we're asking. By following
    these guides, we can make sure that our results are more valid and generalizable.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨零假设检验，这是传统统计学的一个标准、基础技术。这告诉我们如何处理许多实验以及如何构建我们提出的问题。通过遵循这些指南，我们可以确保我们的结果更加有效和具有普遍性。
