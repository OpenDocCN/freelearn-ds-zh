- en: Chapter 14. Analyzing the R Community
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章。分析R社区
- en: In this final chapter, I will try to summarize what you have learned in the
    past 13 chapters. To this end, we will create an actual case study, independent
    from the previously used `hflights` and `mtcars` datasets, and will now try to
    estimate the size of the R community. This is a rather difficult task as there
    is no list of R users around the world; thus, we will have to build some predicting
    models on a number of partial datasets.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一最后一章中，我将尝试总结你过去13章中学到的内容。为此，我们将创建一个实际案例研究，独立于之前使用的`hflights`和`mtcars`数据集，并尝试估计R社区的规模。这是一个相当困难的任务，因为世界上没有R用户的列表；因此，我们将在多个部分数据集上构建一些预测模型。
- en: 'To this end, we will do the following in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到这个目的，我们将在本章做以下几件事情：
- en: Collect live data from different data sources on the Internet
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从互联网上的不同数据源收集实时数据
- en: Cleanse the data and transform it to a standard format
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清洗数据并将其转换为标准格式
- en: Run some quick descriptive, exploratory analysis methods
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行一些快速描述性和探索性分析方法
- en: Visualize the extracted data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化提取的数据
- en: Build some log-linear models on the number of R users based on an independent
    list of names
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于独立名单构建一些基于R用户数量的对数线性模型
- en: R Foundation members
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R基金会成员
- en: 'One of the easiest things we can do is count the members of the R Foundation—the
    organization coordinating the development of the core R program. As the ordinary
    members of the Foundation include only the *R Development Core Team*, we had better
    check the supporting members. Anyone can become a supporting member of the Foundation
    by paying a nominal yearly fee— I highly suggest you do this, by the way. The
    list is available on the [http://r-project.org](http://r-project.org) site, and
    we will use the `XML` package (for more detail, see [Chapter 2](ch02.html "Chapter 2. Getting
    Data from the Web"), *Getting Data from the Web*) to parse the HTML page:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能做的最简单的事情之一是计算R基金会的成员数量——该组织协调核心R程序的开发。由于基金会的普通成员仅包括*R开发核心团队*，我们最好检查支撑成员。任何人都可以通过支付象征性的年度费用成为基金会的支撑成员——顺便说一句，我强烈建议你这样做。名单可在[http://r-project.org](http://r-project.org)网站上找到，我们将使用`XML`包（更多细节，请参阅第2章[“从网络获取数据”](ch02.html
    "Chapter 2. Getting Data from the Web")，*“从网络获取数据”*)来解析HTML页面：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now that we have the HTML page loaded into R, we can use the XML Path Language
    to extract the list of the supporting members of the Foundation, by reading the
    list after the `Supporting members` header:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将HTML页面加载到R中，我们可以使用XML路径语言提取基金会的支撑成员名单，通过读取“支撑成员”标题之后的列表：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Form this character vector of 279 names and countries, let''s extract the list
    of supporting members and the countries separately:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个包含279个名称和国家的字符向量中，让我们分别提取支撑成员名单和国家名单：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: So we first extracted the names by removing everything starting from the opening
    parenthesis in the strings, and then we matched the countries by the character
    positions computed from the number of characters in the names and the original
    strings.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们首先通过移除字符串中从开括号开始的所有内容来提取名称，然后通过从名称中的字符数和原始字符串中计算出的字符位置来匹配国家。
- en: 'Besides the name list of 279 supporting members of the R Foundation, we also
    know the proportion of the citizenship or residence of the members:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 除了R基金会279名支撑成员的名单外，我们还知道成员的国籍或居住地的比例：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Visualizing supporting members around the world
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化全球的支撑成员
- en: 'Probably it''s not that surprising that most supporting members are from the
    USA, and some European countries are also at the top of this list. Let''s save
    this table so that we can generate a map on this count data after some quick data
    transformations:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 可能并不令人惊讶，大多数支撑成员来自美国，一些欧洲国家也位于这个名单的顶端。让我们保存这个表格，以便在快速数据转换后，我们可以根据这个计数数据生成一张地图：
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As mentioned in [Chapter 13](ch13.html "Chapter 13. Data Around Us"), *Data
    Around Us*, the `rworldmap` package can render country-level maps in a very easy
    way; we just have to map the values with some polygons. Here, we will use the
    `joinCountryData2Map` function, first enabling the `verbose` option to see what
    country names have been missed:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如第13章[“我们周围的数据”](ch13.html "Chapter 13. Data Around Us")所述，“我们周围的数据”，`rworldmap`包可以非常容易地渲染国家级地图；我们只需将值映射到一些多边形上。在这里，我们将使用`joinCountryData2Map`函数，首先启用`verbose`选项以查看哪些国家名称被遗漏：
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'So we tried to match the country names stored in the countries data frame,
    but failed for the previously listed four strings. Although we could manually
    fix this, in most cases it''s better to automate what we can, so let''s pass all
    the failed strings to the Google Maps geocoding API and see what it returns:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们尝试将存储在`countries`数据框中的国家名称进行匹配，但前述四个字符串失败了。尽管我们可以手动修复这个问题，但在大多数情况下，最好自动化我们可以处理的事情，所以让我们将所有失败的字符串传递给谷歌地图地理编码API，看看它返回什么：
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now that we have fixed the country names with the help of the Google geocoding
    service, let''s regenerate the frequency table and map those values to the polygon
    names with the `rworldmap` package:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经借助谷歌地理编码服务固定了国家名称，让我们重新生成频率表，并使用`rworldmap`包将这些值映射到多边形名称：
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'These results are much more satisfying! Now we have the number of supporting
    members of the R Foundation mapped to the countries, so we can easily plot this
    data:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果令人满意得多！现在我们已经将R基金会的支持成员数量映射到各个国家，因此我们可以轻松地绘制这些数据：
- en: '[PRE8]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Visualizing supporting members around the world](img/2028OS_14_01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![可视化全球支持成员分布](img/2028OS_14_01.jpg)'
- en: Well, it's clear that most supporting members of the R Foundation are based
    in the USA, Europe, Australia, and New Zealand (where R was born more than 20
    years ago).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，很明显，R基金会的多数支持成员都位于美国、欧洲、澳大利亚和新西兰（R在这里诞生已有20多年）。
- en: But the number of supporters is unfortunately really low, so let's see what
    other data sources we can find and utilize in order to estimate the number of
    R users around the world.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 但支持者的数量非常低，遗憾的是，所以让我们看看我们可以找到和利用的其他数据源来估计全球R用户数量。
- en: R package maintainers
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R包维护者
- en: 'Another similarly straightforward data source might be the list of R package
    maintainers. We can download the names and e-mail addresses of the package maintainers
    from a public page of CRAN, where this data is stored in a nicely structured HTML
    table that is extremely easy to parse:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个类似简单直接的数据源可能是R包维护者的列表。我们可以从CRAN的公共页面下载包维护者的名称和电子邮件地址，这些数据存储在一个结构良好的HTML表中，非常容易解析：
- en: '[PRE9]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Extracting the names from the `Maintainer` column can be done via some quick
    data cleansing and transformations, mainly using regular expressions. Please note
    that the column name starts with a space—that''s why we quoted the column name:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从`Maintainer`列中提取名称可以通过一些快速的数据清洗和转换来完成，主要使用正则表达式。请注意，列名以空格开头——这就是为什么我们引用了列名：
- en: '[PRE10]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This list of almost 7,000 package maintainers includes some duplicated names
    (they maintain multiple packages). Let''s see the list of the top, most prolific
    R package developers:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个包含近7000个包维护者的列表中包含一些重复的名称（他们维护多个包）。让我们看看最顶尖、最多产的R包开发者的列表：
- en: '[PRE11]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Although there's an odd name in the preceding list (orphaned packages do not
    have a maintainer—it's worth mentioning that having only 26 packages out of the
    6,994 no longer actively maintained is a pretty good ratio), but the other names
    are indeed well known in the R community and work on a number of useful packages.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面列表中有一个奇怪的名字（孤儿包没有维护者——值得一提的是，在6994个不再积极维护的包中只有26个是一个相当好的比例），但其他名字在R社区中确实很知名，并且致力于开发多个有用的包。
- en: The number of packages per maintainer
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 每个维护者拥有的包的数量
- en: 'On the other hand, there are a lot more names in the list associated with only
    one or a few R packages. Instead of visualizing the number of packages per maintainer
    on a simple bar chart or histogram, let''s load the `fitdistrplus` package, which
    we will use on the forthcoming pages to fit various theoretical distributions
    on this analyzed dataset:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，列表中与仅一个或几个R包相关联的名称有很多。与其在简单的条形图或直方图上可视化每个维护者拥有的包的数量，不如加载`fitdistrplus`包，我们将在接下来的页面中使用它来拟合分析数据集上的各种理论分布：
- en: '[PRE12]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![The number of packages per maintainer](img/2028OS_14_02.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![每个维护者拥有的包的数量](img/2028OS_14_02.jpg)'
- en: 'The preceding plots also show that most people in the list maintain only one,
    but no more than two or three, packages. If we are interested in how long/heavy
    tailed this distribution is, we might want to call the `descdist` function, which
    returns some important descriptive statistics on the empirical distribution and
    also plots how different theoretical distributions fit our data on a skewness-kurtosis
    plot:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表还显示，列表中的大多数人只维护一个包，但不超过两个或三个包。如果我们对分布的尾部长度/重尾长度感兴趣，我们可能想调用`descdist`函数，该函数返回关于经验分布的一些重要描述性统计信息，并在偏度-峰度图上绘制不同的理论分布如何拟合我们的数据：
- en: '[PRE13]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![The number of packages per maintainer](img/2028OS_14_03.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![每个维护者拥有的包的数量](img/2028OS_14_03.jpg)'
- en: 'Our empirical distribution seems to be rather long-tailed with a very high
    kurtosis, and it seems that the gamma distribution is the best fit for this dataset.
    Let''s see the estimate parameters of this gamma distribution:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实证分布似乎非常长尾，峰度非常高，看起来伽马分布是此数据集的最佳拟合。让我们看看这个伽马分布的估计参数：
- en: '[PRE14]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can use these parameters to simulate a lot more R package maintainers with
    the `rgamma` function. Let''s see how many R packages would be available on CRAN
    with, for example, 100,000 package maintainers:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些参数通过`rgamma`函数模拟出更多的R包维护者。让我们看看在例如有10万名包维护者的情况下，CRAN上会有多少R包可用：
- en: '[PRE15]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![The number of packages per maintainer](img/2028OS_14_04.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![每个维护者拥有的包的数量](img/2028OS_14_04.jpg)'
- en: 'It''s rather clear that this distribution is not as long-tailed as our real
    dataset: even with 100,000 simulations, the largest number was below 10, as we
    can see in the preceding plot; in reality, though, the R package maintainers are
    a lot more productive with up to 20 or 30 packages.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，这个分布不像我们的实际数据集那样长尾：即使进行10万次模拟，最大的数量也低于10，正如我们可以在前面的图中看到的那样；然而，现实中R包维护者的生产力要高得多，可以达到20个或30个包。
- en: 'Let''s verify this by estimating the proportion of R package maintainers with
    no more than two packages based on the preceding gamma distribution:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过基于前面的伽马分布估计不超过两个包的R包维护者的比例来验证这一点：
- en: '[PRE16]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'But this percentage is a lot higher in the real dataset:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 但在实际数据集中，这个百分比要高得多：
- en: '[PRE17]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This may suggest trying to fit a longer-tailed distribution. Let''s see for
    example how Pareto distribution would fit our data. To this end, let''s follow
    the analytical approach by using the lowest value as the location of the distribution,
    and the number of values divided by the sum of the logarithmic difference of all
    these values from the location as the shape parameter:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能意味着尝试拟合一个更长尾的分布。让我们看看例如帕累托分布将如何拟合我们的数据。为此，让我们通过使用最低值作为分布的位置，以及所有这些值与位置的对数差之和除以值的数量作为形状参数来遵循分析方法：
- en: '[PRE18]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Unfortunately, there is no `ppareto` function in the base `stats` package,
    so we have to first load the `actuar` or `VGAM` package to compute the distribution
    function:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，基础`stats`包中没有`ppareto`函数，因此我们不得不首先加载`actuar`或`VGAM`包来计算分布函数：
- en: '[PRE19]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Well, now this is even higher than the real proportion! It seems that none
    of the preceding theoretical distributions fit our data perfectly—which is pretty
    normal by the way. But let''s see how these distributions fit our original data
    set on a joint plot:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，现在这个比例甚至更高了！看起来前面的理论分布没有一个能完美地拟合我们的数据——但事实上这也是很正常的。但让我们看看这些分布如何在联合图上拟合我们的原始数据集：
- en: '[PRE20]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![The number of packages per maintainer](img/2028OS_14_05.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![每个维护者拥有的包的数量](img/2028OS_14_05.jpg)'
- en: 'After all, it seems that the Pareto distribution is the closest fit to our
    long-tailed data. But more importantly, we know about more than 4,000 R users
    besides the previously identified 279 R Foundation supporting members:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，似乎帕累托分布是最接近我们长尾数据的拟合。但更重要的是，我们除了之前确定的279位R基金会支持成员外，还了解到了4000多位R用户：
- en: '[PRE21]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: What other data sources can we use to find information on the (number of) R
    users?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还能使用哪些数据源来获取关于（R用户数量）的信息？
- en: The R-help mailing list
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R-help邮件列表
- en: 'R-help is the official, main mailing list providing general discussion about
    problems and solutions using R, with many active users and several dozen e-mails
    every day. Fortunately, this public mailing list is archived on several sites,
    and we can easily download the compressed monthly files from, for example, ETH
    Zurich''s R-help archives:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: R-help是官方的、主要的邮件列表，提供有关使用R解决问题的通用讨论，有众多活跃用户，每天有几十封电子邮件。幸运的是，这个公开的邮件列表在几个网站上都有存档，我们可以轻松地从例如ETH
    Zurich的R-help存档中下载压缩的月度文件：
- en: '[PRE22]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now let''s extract the URL of the monthly compressed archives from this page
    via an XPath query:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过XPath查询从这个页面中提取月度压缩存档的URL：
- en: '[PRE23]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'And now let''s download these files to our computer for future parsing:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将这些文件下载到我们的计算机上以供将来解析：
- en: '[PRE24]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Depending on your operating system and R version, the `curl` option that we
    used to download files via the HTTPS protocol might not be available. In such
    cases, you can try other another method or update the query to use the `RCurl`,
    `curl`, or `httr` packages.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的操作系统和R版本，我们用于通过HTTPS协议下载文件的`curl`选项可能不可用。在这种情况下，您可以尝试其他方法或更新查询以使用`RCurl`、`curl`或`httr`包。
- en: 'Downloading these ~200 files takes some time and you might also want to add
    a `Sys.sleep` call in the loop so as not to overload the server. Anyway, after
    some time, you will have a local copy of the `R-help` mailing list in the `r-help`
    folder, ready to be parsed for some interesting data:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下载这些约200个文件需要一些时间，您可能还希望在循环中添加`Sys.sleep`调用，以避免服务器过载。无论如何，经过一段时间，您将在`r-help`文件夹中拥有`R-help`邮件列表的本地副本，准备好解析一些有趣的数据：
- en: '[PRE25]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Instead of loading all the text files into R and using `grep` there, I pre-filtered
    the files via the Linux command line `zgrep` utility, which can search in `gzipped`
    (compressed) text files efficiently. If you do not have `zgrep` installed (it
    is available on both Windows and the Mac), you can extract the files first and
    use the standard `grep` approach with the very same regular expression.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我不是将所有文本文件加载到R中并使用`grep`，而是通过Linux命令行`zgrep`实用程序预先过滤了文件，该实用程序可以有效地搜索`gzipped`（压缩）文本文件。如果您没有安装`zgrep`（它在Windows和Mac上都是可用的），您可以首先提取文件，然后使用带有相同正则表达式的标准`grep`方法。
- en: 'So we filtered for all lines of the e-mails and headers, starting with the
    `From` string, that hold information on the senders in the e-mail address and
    name. Out of the ~387,000 e-mails, we have found around ~110,000 unique e-mail
    sources. To understand the following regular expressions, let''s see how one of
    these lines looks:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们筛选了所有包含电子邮件地址和姓名中发送者信息的电子邮件和标题行，从`From`字符串开始。在约387,000封电子邮件中，我们找到了大约110,000个独特的电子邮件来源。为了理解以下正则表达式，让我们看看这些行中的一行是如何看的：
- en: '[PRE26]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now let''s process these lines by removing the static prefix and extracting
    the names found between parentheses after the e-mail address:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们通过移除静态前缀并提取电子邮件地址后面的括号中的名称来处理这些行：
- en: '[PRE27]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'And we can see the list of the most active `R-help` posters:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到最活跃的`R-help`发帖者列表：
- en: '[PRE28]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This list seems to be legitimate, right? Although my first guess was that Professor
    Brian Ripley with his brief messages will be the first one in this list. As a
    result of some earlier experiences, I know that matching names can be tricky and
    cumbersome, so let''s verify that our data is clean enough and there''s only one
    version of the Professor''s name:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表看起来似乎是合法的，对吧？尽管我最初的猜测是教授布莱恩·里普利（Brian Ripley）以其简短的邮件内容将是这个列表中的第一个。由于一些早期的经验，我知道匹配名称可能会很棘手且繁琐，所以让我们验证我们的数据是否足够干净，并且教授的姓名只有一个版本：
- en: '[PRE29]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Well, it seems that the Professor used some alternative `From` addresses as
    well, so a more valid estimate of the number of his messages should be something
    like:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，看起来教授还使用了某些替代的`From`地址，因此对他消息数量的更准确估计可能应该是这样的：
- en: '[PRE30]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'So using quick, regular expressions to extract the names from the e-mails returned
    most of the information we were interested in, but it seems that we have to spend
    a lot more time to get the whole information set. As usual, the Pareto rule applies:
    we can spend around 80 percent of our time on preparing data, and we can get 80
    percent of the data in around 20 percent of the whole project timeline.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用快速的正则表达式从电子邮件中提取名称返回了我们感兴趣的大部分信息，但似乎我们需要花费更多的时间来获取整个信息集。像往常一样，帕累托法则适用：我们可以将大约80%的时间用于准备数据，我们可以在整个项目时间线的约20%内获得大约80%的数据。
- en: Due to page limitations, we will not cover data cleansing on this dataset in
    greater detail at this point, but I highly suggest checking Mark van der Loo's
    `stringdist` package, which can compute string distances and similarities to,
    for example, merge similar names in cases like this.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于篇幅限制，我们在此不会更详细地介绍这个数据集的数据清洗，但我强烈建议检查Mark van der Loo的`stringdist`包，它可以计算字符串距离和相似度，例如，在这种情况下合并类似名称。
- en: Volume of the R-help mailing list
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: R-help邮件列表的容量
- en: But besides the sender, these e-mails also include some other really interesting
    data as well. For example, we can extract the date and time when the e-mail was
    sent—to model the frequency and temporal pattern of the mailing list.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 但是除了发送者之外，这些电子邮件还包含一些其他非常有趣的数据。例如，我们可以提取电子邮件发送的日期和时间——以模拟邮件列表的频率和时序模式。
- en: 'To this end, let''s filter for some other lines in the compressed text files:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个目的，让我们在压缩的文本文件中过滤一些其他行：
- en: '[PRE31]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This returns fewer lines when compared to the previously extracted `From` lines:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前提取的`From`行相比，这返回的行数更少：
- en: '[PRE32]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This is due to the various date and time formats used in the e-mail headers,
    as sometimes the day of the week was not included in the string or the order of
    year, month, and day was off compared to the vast majority of other mails. Anyway,
    we will only concentrate on this significant portion of mails with the standard
    date and time format but, if you are interested in transforming these other time
    formats, you might want to check Hadley Wickham's `lubridate` package to help
    your workflow. But please note that there's no general algorithm to guess the
    order of decimal year, month, and day—so you will end up with some manual data
    cleansing for sure!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为电子邮件标题中使用了各种日期和时间格式，有时字符串中不包括星期几，或者年、月、日的顺序与大多数其他邮件相比是错误的。无论如何，我们只会关注这部分具有标准日期和时间格式的邮件，但如果你对转换这些其他时间格式感兴趣，你可能想查看Hadley
    Wickham的`lubridate`包以帮助你的工作流程。但请注意，没有通用的算法来猜测十进制年、月和日的顺序——所以你肯定会进行一些手动数据清洗！
- en: 'Let''s see how these (subset of) lines look:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些（子集）行看起来如何：
- en: '[PRE33]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Then we can simply get rid of the `Date` prefix and parse the time stamps via
    `strptime`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以简单地去掉`Date`前缀，并通过`strptime`解析时间戳：
- en: '[PRE34]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now that the data is in a parsed format (even the local time-zones were converted
    to UTC), it''s relatively easy to see, for example, the number of e-mails on the
    mailing list per year:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经解析格式化（即使是本地时区也被转换成了UTC），相对容易看到，例如，每年邮件列表中的电子邮件数量：
- en: '[PRE35]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![Volume of the R-help mailing list](img/2028OS_14_06.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![R-help邮件列表的体积](img/2028OS_14_06.jpg)'
- en: Note
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Although the volume on the `R-help` mailing list seems to have decreased in
    the past few years, it''s not due to the lower R activity: R users, okay as is
    or no/. others on the Internet, nowadays tend to use other information channels
    more often than e-mail—for example: StackOverflow and GitHub (or even Facebook
    and LinkedIn). For a related research, please see the paper of Bogdan Vasilescu
    at al at [http://web.cs.ucdavis.edu/~filkov/papers/r_so.pdf](http://web.cs.ucdavis.edu/~filkov/papers/r_so.pdf).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然过去几年`R-help`邮件列表的体积似乎有所下降，但这并不是由于R活动的减少：R用户，无论是好是坏，还是互联网上的其他人，现在更倾向于使用其他信息渠道，而不是电子邮件——例如：StackOverflow和GitHub（甚至是Facebook和LinkedIn）。有关相关研究，请参阅Bogdan
    Vasilescu等人发表在[http://web.cs.ucdavis.edu/~filkov/papers/r_so.pdf](http://web.cs.ucdavis.edu/~filkov/papers/r_so.pdf)的论文。
- en: 'Well, we can do a lot better than this, right? Let''s massage our data a bit
    and visualize the frequency of mails based on the day of week and hour of the
    day via a more elegant graph—inspired by GitHub''s punch card plot:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，我们可以做得更好，对吧？让我们稍微调整一下我们的数据，并通过一个更优雅的图表来可视化基于星期几和一天中的小时数的邮件频率——灵感来自GitHub的打卡图：
- en: '[PRE36]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Visualizing this dataset is relatively straightforward with `ggplot`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`ggplot`可视化这个数据集相对简单：
- en: '[PRE37]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![Volume of the R-help mailing list](img/2028OS_14_07.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![R-help邮件列表的体积](img/2028OS_14_07.jpg)'
- en: As the times are by UTC, the early morning mails might suggest that where most
    `R-help` posters live has a positive GMT offset—if we suppose that most e-mails
    were written in business hours. Well, at least the lower number of e-mails on
    the weekends seems to suggest this statement.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 由于时间是按照UTC计算的，早上早些时候的邮件可能表明大多数`R-help`发件人所在的地区有正的GMT偏差——如果我们假设大多数电子邮件都是在工作时间写的。嗯，至少周末电子邮件数量较少似乎也支持这个说法。
- en: 'And it seems that the UTC, UTC+1, and UTC+2 time zones are indeed rather frequent,
    but the US time zones are also pretty common for the `R-help` posters:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来UTC、UTC+1和UTC+2时区确实相当常见，但美国时区对于`R-help`发件人来说也很常见：
- en: '[PRE38]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Forecasting the e-mail volume in the future
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测未来的电子邮件体积
- en: 'And we can also use this relatively clean dataset to forecast the future volume
    of the `R-help` mailing list. To this end, let''s aggregate the original dataset
    to count data daily, as we saw in [Chapter 3](ch03.html "Chapter 3. Filtering
    and Summarizing Data"), *Filtering and Summarizing Data*:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用这个相对干净的数据库来预测`R-help`邮件列表的未来体积。为此，让我们将原始数据库聚合起来，按日计数，就像我们在[第3章](ch03.html
    "第3章。过滤和汇总数据")中看到的那样，*过滤和汇总数据*：
- en: '[PRE39]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now let''s transform this `data.table` object into a time-series object by
    referencing the actual mail counts as values and the dates as the index:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过引用实际的邮件计数作为值和日期作为索引来将这个`data.table`对象转换成时间序列对象：
- en: '[PRE40]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Well, this daily dataset is a lot spikier than the previously rendered yearly
    graph:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这个每日数据集的波动性比之前渲染的年度图表要大得多：
- en: '[PRE41]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![Forecasting the e-mail volume in the future](img/2028OS_14_08.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![预测未来电子邮件量](img/2028OS_14_08.jpg)'
- en: 'But instead of smoothing or trying to decompose this time-series, like we did
    in [Chapter 12](ch12.html "Chapter 12. Analyzing Time-series"), *Analyzing Time-series*,
    let''s rather see how we can provide some quick estimates (based on historical
    data) on the forthcoming number of mails on this mailing list with some automatic
    models. To this end, we will use the `forecast` package:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不会像在 [第 12 章](ch12.html "第 12 章。分析时间序列") *分析时间序列* 中所做的那样，尝试平滑或分解这个时间序列，而是看看我们如何使用一些自动模型来提供一些基于历史数据的快速估计（关于这个邮件列表即将到来的邮件数量）。为此，我们将使用
    `forecast` 包：
- en: '[PRE42]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The `ets` function implements a fully automatic method that can select the
    optimal trend, season, and error type for the given time-series. Then we can simply
    call the `predict` or `forecast` function to see the specified number of estimates,
    only for the next day in this case:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`ets` 函数实现了一种完全自动的方法，可以选择给定时间序列的最佳趋势、季节和误差类型。然后我们可以简单地调用 `predict` 或 `forecast`
    函数来查看指定数量的估计，在这种情况下，仅针对下一天：'
- en: '[PRE43]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'So it seems that, for the next day, our model estimated around 28 e-mails with
    a confidence interval of 80 percent being somewhere between 10 and 47\. Visualizing
    predictions for a slightly longer period of time with some historical data can
    be done via the standard `plot` function with some useful new parameters:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于第二天，我们的模型估计大约有 28 封电子邮件，80% 的置信区间在 10 到 47 之间。通过使用标准 `plot` 函数和一些有用的新参数，可以可视化稍长时间段内的预测和历史数据：
- en: '[PRE44]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![Forecasting the e-mail volume in the future](img/2028OS_14_09.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![预测未来电子邮件量](img/2028OS_14_09.jpg)'
- en: Analyzing overlaps between our lists of R users
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析我们 R 用户列表之间的重叠
- en: But our original idea was to predict the number of R users around the world
    and not to focus on some minor segments, right? Now that we have multiple data
    sources, we can start building some models combining those to provide estimates
    on the global number of R users.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们的原始想法是预测全球 R 用户的数量，而不是关注一些较小的细分市场，对吧？现在我们有了多个数据源，我们可以开始构建一些模型，结合这些数据来提供全球
    R 用户数量的估计。
- en: The basic idea behind this approach is the capture-recapture method, which is
    well known in ecology, where we first try to identify the probability of capturing
    a unit from the population, and then we use this probability to estimate the number
    of not captured units.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法背后的基本思想是捕获-再捕获方法，这在生态学中是众所周知的，我们首先尝试确定从人群中捕获一个单位的概率，然后我们使用这个概率来估计未捕获的单位数量。
- en: 'In our current study, units will be R users and the samples are the previously
    captured name lists on the:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们当前的研究中，单位将是 R 用户，样本是之前捕获的以下名称列表：
- en: Supporters of the *R Foundation*
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R 基金会* 的支持者'
- en: R package maintainers who submitted at least one package to *CRAN*
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少向 *CRAN* 提交了一个软件包的 R 包维护者
- en: '*R-help* mailing list e-mail senders'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R-help* 邮件列表的邮件发送者'
- en: 'Let''s merge these lists with a tag referencing the data source:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一个引用数据源的标签来合并这些列表：
- en: '[PRE45]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next let''s see the number of names we can find in one, two or all three groups:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看我们可以在一个、两个或所有三个组中找到多少个名字：
- en: '[PRE46]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: So there are (at least) 40 persons who support the R Foundation, maintain at
    least one R package on CRAN, and have posted at least one mail to `R-help` since
    1997! I am happy and proud to be one of these guys -- especially with an accent
    in my name, which often makes matching of strings more complex.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，至少有 40 人支持 R 基金会，至少在 CRAN 上维护了一个 R 软件包，并且自 1997 年以来至少发布了一封邮件到 `R-help`！我很高兴和自豪能成为这些人中的一员——尤其是我的名字中带有口音，这通常会使字符串匹配更加复杂。
- en: Now, if we suppose these lists refer to the same population, namely R users
    around the world, then we can use these common occurrences to predict the number
    of R users who somehow missed supporting the R Foundation, maintaining a package
    on CRAN, and writing a mail to the R-help mailing list. Although this assumption
    is obviously off, let's run this quick experiment and get back to these outstanding
    questions later.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们假设这些列表指的是同一人群，即全球的 R 用户，那么我们可以使用这些共同出现的情况来预测那些以某种方式错过了支持 R 基金会、维护 CRAN
    上的软件包和向 R-help 邮件列表发送邮件的 R 用户数量。尽管这个假设显然是错误的，但让我们进行这个快速实验，稍后再回到这些悬而未决的问题。
- en: 'One of the best things in R is that we have a package for almost any problem.
    Let''s load the `Rcapture` package, which provides some sophisticated, yet easily
    accessible, methods for capture-recapture models:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: R中最好的事情之一是我们几乎为任何问题都有一个包。让我们加载`Rcapture`包，它提供了一些复杂但易于访问的捕获-再捕获模型方法：
- en: '[PRE47]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'These numbers from the first `fi` column are familiar from the previous table,
    and represent the number of R users identified on one, two, or all three lists.
    It''s a lot more interesting to fit some models on this data with a simple call
    such as:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这些来自第一列`fi`的数字与之前的表格中的数字相似，代表在一份、两份或三份列表上识别出的R用户数量。用简单的调用拟合一些模型会更有趣，例如：
- en: '[PRE48]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Once again, I have to emphasize that these estimates are not actually on the
    abundance of all R users around the world, because:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这些估计实际上并不是针对全球所有R用户的丰富程度，因为：
- en: Our non-independent lists refer to far more specific groups
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的非独立列表指的是更具体的群体
- en: The model assumptions do not stand
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型假设并不成立
- en: The R community is definitely not a closed population and some open-population
    models would be more reliable
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R社区肯定不是一个封闭的群体，一些开放群体模型可能会更可靠
- en: We missed some very important data-cleansing steps, as noted
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们遗漏了一些非常重要的数据清理步骤，正如所注
- en: Further ideas on extending the capture-recapture models
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于扩展捕获-再捕获模型的其他想法
- en: Although this playful example did not really help us to find out the number
    of R users around the world, with some extensions the basic idea is definitely
    viable. First of all, we might consider analyzing the source data in smaller chunks—for
    example, looking for the same e-mail addresses or names in different years of
    the R-help archives. This might help with estimating the number of persons who
    were thinking about submitting a question to `R-help`, but did not actually send
    the e-mail after all (for example, because another poster's question had already
    been answered or she/he resolved the problem without external help).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个轻松的例子并没有真正帮助我们找出全球R用户数量，但通过一些扩展，基本想法肯定是可行的。首先，我们可能考虑分析源数据的小块——例如，在R-help存档的不同年份中寻找相同的电子邮件地址或姓名。这可能有助于估计那些考虑向`R-help`提交问题但最终没有发送电子邮件的人数（例如，因为另一个发帖者的问题已经得到解答，或者她/他未寻求外部帮助就解决了问题）。
- en: On the other hand, we could also add a number of other data sources to the models,
    so that we can do more reliable estimates on some other R users who do not contribute
    to the R Foundation, CRAN, or R-help.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们也可以向模型中添加许多其他数据源，这样我们就可以对一些没有向R基金会、CRAN或R-help做出贡献的其他R用户进行更可靠的估计。
- en: 'I have been working on a similar study over the past 2 years, collecting data
    on the number of:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的两年里，我一直在进行一项类似的研究，收集以下数据：
- en: R Foundation ordinary and supporting members, donators and benefactors
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R基金会的普通和赞助会员、捐赠者和资助者
- en: Attendees at the annual R conference between 2004 and 2015
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2004年至2015年每年R会议的与会者人数
- en: CRAN downloads per package and country in 2013 and 2014
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2013年和2014年按包和国家的CRAN下载量
- en: R User Groups and meet-ups with the number of members
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R用户组和成员人数的聚会
- en: The [http://www.r-bloggers.com](http://www.r-bloggers.com) visitors in 2013
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2013年的[http://www.r-bloggers.com](http://www.r-bloggers.com)访客
- en: GitHub users with at least one repository with R source code
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少有一个包含R源代码存储库的GitHub用户
- en: Google search trends on R-related terms
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R相关术语的Google搜索趋势
- en: You can find the results on an interactive map and the country-level aggregated
    data in a CSV file at [http://rapporter.net/custom/R-activity](http://rapporter.net/custom/R-activity)
    and an offline data visualization presented in the past two *useR!* conferences
    at [http://bit.ly/useRs2015](http://bit.ly/useRs2015).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在交互式地图上找到结果，并在CSV文件中找到按国家汇总的数据，该文件位于[http://rapporter.net/custom/R-activity](http://rapporter.net/custom/R-activity)，以及在过去两届*useR!*会议上展示的离线数据可视化，位于[http://bit.ly/useRs2015](http://bit.ly/useRs2015)。
- en: The number of R users in social media
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 社交媒体中的R用户数量
- en: An alternative way to try to estimate the number of R users could be to analyze
    the occurrence of the related terms on social media. This is relatively easy on
    Facebook, where the marketing API allows us to query the size of the so-called
    target audiences, which we can use to define targets for some paid ads.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试估计R用户数量的另一种方法可能是分析社交媒体上相关术语的出现频率。在Facebook上这相对容易，因为营销API允许我们查询所谓的目标受众的大小，我们可以用这些信息来定义一些付费广告的目标。
- en: 'Well, we are not actually interested in creating a paid advertisement on Facebook
    right now, although this can be easily done with the `fbRads` package, but we
    can use this feature to see the estimated size of the *target* group of persons
    interested in R:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们现在实际上并不感兴趣在Facebook上创建付费广告，尽管这可以通过`fbRads`包轻松完成，但我们可以使用这个功能来查看对R感兴趣的人的*目标*群体的估计规模：
- en: '[PRE49]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Of course, to run this quick example you will need to have a (free) Facebook
    developer account, a registered application, and a generated token (please see
    the package docs for more details), but it is definitely worth it: we have just
    found out that there are more than 1.3M users around the world interested in R!
    That''s really impressive, although it seems to be rather high to me, especially
    when compared with some other statistical software, such as:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，要运行这个快速示例，你需要拥有一个（免费）的Facebook开发者账户、一个注册的应用程序以及一个生成的令牌（请参阅包文档以获取更多详细信息），但这绝对值得：我们刚刚发现，全世界有超过130万用户对R感兴趣！这真的很令人印象深刻，尽管对我来说这似乎相当高，尤其是与其他一些统计软件相比，例如：
- en: '[PRE50]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Having said this, comparing R with other programming languages suggests that
    the audience size might actually be correct:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，将R与其他编程语言进行比较表明，受众规模可能实际上是正确的：
- en: '[PRE51]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: There are many programmers around the world, it seems! But what are they talking
    about and what are the trending topics? We will cover these questions in the next
    section.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 世界上似乎有很多程序员！但他们都在谈论什么，哪些是热门话题？我们将在下一节中探讨这些问题。
- en: R-related posts in social media
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 社交媒体中的R相关帖子
- en: One option to collect posts from the past few days of social media is processing
    Twitter's global stream of Tweet data. This stream data and API provides access
    to around 1 percent of all tweets. If you are interested in all this data, then
    a commercial Twitter Firehouse account is needed. In the following examples, we
    will use the free Twitter search API, which provides access to no more than 3,200
    tweets based on any search query—but this will be more than enough to do some
    quick analysis on the trending topics among R users.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 收集过去几天社交媒体帖子的一种选择是处理Twitter的全局Tweet数据流。这些流数据和分析API提供了访问所有推文的大约1%的能力。如果你对所有这些数据感兴趣，那么需要一个商业Twitter
    Firehouse账户。在以下示例中，我们将使用免费的Twitter搜索API，它基于任何搜索查询提供不超过3,200条推文的访问权限——但这将足以对R用户中的热门话题进行一些快速分析。
- en: 'So let''s load the `twitteR` package and initialize the connection to the API
    by providing our application tokens and secrets, generated at [https://apps.twitter.com](https://apps.twitter.com):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们加载`twitteR`包，并通过提供我们的应用程序令牌和密钥来初始化与API的连接，这些令牌和密钥是在[https://apps.twitter.com](https://apps.twitter.com)生成的：
- en: '[PRE52]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Now we can start using the `searchTwitter` function to search tweets for any
    keywords, including hashtags and mentions. This query can be fine-tuned with a
    couple of arguments. `Since`, `until`, and *n* set the beginning and end date,
    also the number of tweets to return respectively. Language can be set with the
    `lang` attribute by the ISO 639-1 format—for example, use `en` for English.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始使用`searchTwitter`函数搜索任何关键词的推文，包括标签和提及。这个查询可以通过几个参数进行微调。`Since`、`until`和*n*分别设置开始和结束日期，以及要返回的推文数量。可以通过`lang`属性设置语言，使用ISO
    639-1格式——例如，使用`en`表示英语。
- en: 'Let''s search for the most recent tweet with the official R hashtag:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们搜索带有官方R标签的最近推文：
- en: '[PRE53]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: This is quite an impressive amount of information for a character string with
    no more than 140 characters, isn't it? Besides the text including the actual tweet,
    we got some meta-information as well—for example, the author, post time, the number
    of times other users favorited or retweeted the post, the Twitter client name,
    and the URLs in the post along with the shortened, expanded, and displayed format.
    The location of the tweet is also available in some cases, if the user enabled
    that feature.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个不超过140个字符的字符串来说，这确实是一个相当惊人的信息量。除了包括实际推文的文本外，我们还获得了一些元信息——例如，作者、发布时间、其他用户点赞或转发该帖子的次数、Twitter客户端名称以及帖子中的URL（包括缩短、展开和显示格式）。在某些情况下，如果用户启用了该功能，推文的地理位置信息也是可用的。
- en: 'Based on this piece of information, we could focus on the Twitter R community
    in very different ways. Examples include:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这条信息，我们可以以非常不同的方式关注Twitter R社区。以下是一些例子：
- en: Counting the users mentioning R
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算提及R的用户数量
- en: Analyzing social network or Twitter interactions
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析社交网络或Twitter互动
- en: Time-series analysis on the time of posts
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于帖子时间的时序分析
- en: Spatial analysis on the location of tweets
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推文位置的时空分析
- en: Text mining of the tweet contents
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推文内容的文本挖掘
- en: Probably a mixture of these (and other) methods would be the best approach,
    and I highly suggest you do that as an exercise to practice what you have learned
    in this book. However, in the following pages we will only concentrate on the
    last item.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 可能是这些（和其他）方法的混合体将是最佳方法，我强烈建议你作为练习来做这件事，以巩固你在本书中学到的知识。然而，在接下来的几页中，我们只会专注于最后一项。
- en: 'So first, we need some recent tweets on the R programming language. To search
    for `#rstats` posts, instead of providing the related hashtag (like we did previously),
    we can use the `Rtweets` wrapper function as well:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，首先，我们需要一些关于R编程语言的最新推文。为了搜索`#rstats`帖子，我们不仅可以提供相关的标签（就像我们之前做的那样），还可以使用`Rtweets`包装函数：
- en: '[PRE54]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This function returned 500 reference classes similar to those we saw previously.
    We can count the number of original tweets excluding retweets:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数返回了500个与之前看到的类似的参考类。我们可以计算不包括转发的原始推文数量：
- en: '[PRE55]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'But, as we are looking for the trending topics, we are interested in the original
    list of tweets, where the retweets are also important as they give a natural weight
    to the trending posts. So let''s transform the list of reference classes to a
    `data.frame`:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，因为我们正在寻找热门话题，所以我们感兴趣的是原始推文列表，其中转发也很重要，因为它们为热门帖子提供了自然的权重。所以让我们将参考类列表转换为`data.frame`：
- en: '[PRE56]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'This dataset consists of 500 rows (tweets) and 16 variables on the content,
    author, and location of the posts, as described previously. Now, as we are only
    interested in the actual text of the tweets, let''s load the `tm` package and
    import our corpus as seen in [Chapter 7](ch07.html "Chapter 7. Unstructured Data"),
    *Unstructured Data*:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含500行（推文）和16个变量，涉及推文的内容、作者和位置，如前所述。现在，因为我们只对推文的实际文本感兴趣，所以让我们加载`tm`包并将我们的语料库导入，正如在[第7章](ch07.html
    "第7章。非结构化数据")中看到的，*非结构化数据*：
- en: '[PRE57]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'As the data is in the right format, we can start to clean the data from the
    common English words and transform everything into lowercase format; we might
    also want to remove any extra whitespace:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据格式正确，我们可以开始从常见英语单词中清理数据，并将所有内容转换为小写格式；我们可能还想删除任何额外的空白：
- en: '[PRE58]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'It''s also wise to remove the R hashtag, as this is part of all tweets:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 还明智地删除R标签，因为这是所有推文的一部分：
- en: '[PRE59]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'And then we can use the `wordcloud` package to plot the most important words:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`wordcloud`包来绘制最重要的单词：
- en: '[PRE60]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '![R-related posts in social media](img/2028OS_14_10.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![社交媒体中的R相关帖子](img/2028OS_14_10.jpg)'
- en: Summary
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In the past few pages, I have tried to cover a variety of data science and
    R programming topics, although many important methods and questions were not addressed
    due to page limitation. To this end, I''ve compiled a short reading list in the
    *References* chapter of the book. And don''t forget: now it''s your turn to practice
    everything you learned in the previous chapters. I wish you a lot of fun and success
    in this journey!'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几页中，我试图涵盖各种数据科学和R编程主题，尽管由于篇幅限制，许多重要方法和问题都没有涉及。为此，我在书的*参考文献*章节中整理了一份简短的阅读清单。而且别忘了：现在轮到你自己练习前面章节中学到的所有内容了。我祝愿你在这一旅程中玩得开心，取得成功！
- en: And once again, thanks for reading this book; I hope you found it useful. If
    you have any questions, comments, or any kind of feedback, please feel free to
    get in touch, I'm looking forward to hearing from you!
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 再次感谢阅读这本书；我希望你觉得它有用。如果你有任何问题、评论或任何形式的反馈，请随时联系，我期待着你的回复！
