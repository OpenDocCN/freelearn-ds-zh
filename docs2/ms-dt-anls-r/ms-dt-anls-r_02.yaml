- en: Chapter 2. Getting Data from the Web
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章：从网络获取数据
- en: 'It happens pretty often that we want to use data in a project that is not yet
    available in our databases or on our disks, but can be found on the Internet.
    In such situations, one option might be to get the IT department or a data engineer
    at our company to extend our data warehouse to scrape, process, and load the data
    into our database as shown in the following diagram:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常会遇到这样的情况，即我们想在项目中使用的数据尚未存储在我们的数据库或磁盘上，但可以在互联网上找到。在这种情况下，一个选择可能是让IT部门或我们公司的数据工程师扩展我们的数据仓库，以爬取、处理并将数据加载到我们的数据库中，如下面的图所示：
- en: '![Getting Data from the Web](img/2028OS_02_01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![从网络获取数据](img/2028OS_02_01.jpg)'
- en: 'On the other hand, if we have no *ETL* system (*to Extract, Transform, and
    Load data*) or simply just cannot wait a few weeks for the IT department to implement
    our request, we are on our own. This is pretty standard for the data scientist,
    as most of the time we are developing prototypes that can be later transformed
    into products by software developers. To this end, a variety of skills are required
    in the daily round, including the following topics that we will cover in this
    chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们没有*ETL*系统（用于提取、转换和加载数据）或者简单地无法等待几周时间让IT部门实现我们的请求，我们就只能自己动手了。这对于数据科学家来说很常见，因为大多数时候我们都在开发可以由软件开发人员后来转化为产品的原型。为此，在日常工作需要各种技能，包括以下我们将在本章中涵盖的主题：
- en: Downloading data programmatically from the Web
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从网络中以编程方式下载数据
- en: Processing XML and JSON formats
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理XML和JSON格式
- en: Scraping and parsing data from raw HTML sources
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从原始HTML源中抓取和解析数据
- en: Interacting with APIs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与API交互
- en: 'Although being a *data scientist* was referred to as the sexiest job of the
    21st century (Source: [https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century/](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century/)),
    most data science tasks have nothing to do with data analysis. Worse, sometimes
    the job seems to be boring, or the daily routine requires just basic IT skills
    and no machine learning at all. Hence, I prefer to call this role a *data hacker*
    instead of *data scientist*, which also means that we often have to get our hands
    dirty.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管被称为21世纪最性感的工作之一（来源：[https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century/](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century/))，但大多数数据科学任务与数据分析无关。更糟糕的是，有时这份工作似乎很无聊，或者日常工作中只需要基本的IT技能，根本不需要机器学习。因此，我更愿意称这个角色为*数据黑客*而不是*数据科学家*，这也意味着我们经常不得不亲自动手。
- en: For instance, scraping and scrubbing data is the least sexy part of the analysis
    process for sure, but it's one of the most important steps; it is also said, that
    around 80 percent of data analysis is spent cleaning data. There is no sense in
    running the most advanced machine learning algorithm on junk data, so be sure
    to take your time to get useful and tidy data from your sources.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，抓取和清洗数据无疑是分析过程中最不性感的一部分，但它是最重要的步骤之一；据说，大约80%的数据分析时间都花在数据清洗上。在垃圾数据上运行最先进的机器学习算法是没有意义的，所以请确保花时间从你的数据源中获取有用且整洁的数据。
- en: Note
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: This chapter will also depend on extensive usage of Internet browser debugging
    tools with some R packages. These include Chrome `DevTools` or `FireBug` in Firefox.
    Although the steps to use these tools will be straightforward and also shown on
    screenshots, it's definitely worth mastering these tools for future usage; therefore,
    I suggest checking out a few tutorials on these tools if you are into fetching
    data from online sources. Some starting points are listed in the *References*
    section of the *Appendix* at the end of the book.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还将依赖于广泛使用互联网浏览器调试工具和一些R包。这些包括Chrome的`DevTools`或Firefox中的`FireBug`。尽管使用这些工具的步骤将非常直接，并且也会在屏幕截图上展示，但掌握这些工具绝对值得，因此我建议如果你对从在线来源获取数据感兴趣，可以查看一些关于这些工具的教程。一些起点列在书末附录的*参考文献*部分。
- en: For a quick overview and a collection of relevant R packages for scraping data
    from the Web and to interact with Web services, see the *Web Technologies and
    Services CRAN Task View* at [http://cran.r-project.org/web/views/WebTechnologies.html](http://cran.r-project.org/web/views/WebTechnologies.html).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要快速了解从网络抓取数据的相关R包以及与Web服务交互的信息，请参阅[http://cran.r-project.org/web/views/WebTechnologies.html](http://cran.r-project.org/web/views/WebTechnologies.html)上的*Web技术和服务CRAN任务视图*。
- en: Loading datasets from the Internet
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从互联网加载数据集
- en: 'The most obvious task is to download datasets from the Web and load those into
    our R session in two manual steps:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最明显的任务是从网络下载数据集，并通过两个手动步骤将其加载到我们的 R 会话中：
- en: Save the datasets to disk.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集保存到磁盘。
- en: Read those with standard functions, such as `read.table` or for example `foreign::read.spss`,
    to import `sav` files.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用标准函数，如 `read.table` 或例如 `foreign::read.spss`，来读取 `sav` 文件。
- en: 'But we can often save some time by skipping the first step and loading the
    flat text data files directly from the URL. The following example fetches a comma-separated
    file from the **Americas Open Geocode** (**AOG**) database at [http://opengeocode.org](http://opengeocode.org),
    which contains the government, national statistics, geological information, and
    post office websites for the countries of the world:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们通常可以通过跳过第一步并直接从 URL 加载平面文本数据文件来节省一些时间。以下示例从 [http://opengeocode.org](http://opengeocode.org)
    的 **美洲开放地理编码**（**AOG**）数据库中获取一个逗号分隔的文件，该数据库包含世界各国的政府、国家统计数据、地质信息和邮政网站：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, we passed a hyperlink to the `file` argument of `read.table`,
    which actually downloaded the text file before processing. The `url` function,
    used by `read.table` in the background, supports HTTP and FTP protocols, and can
    also handle proxies, but it has its own limitations. For example `url` does not
    support **Hypertext Transfer Protocol Secure** (**HTTPS**) except for a few exceptions
    on Windows, which is often a must to access Web services that handle sensitive
    data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们向 `read.table` 的 `file` 参数传递了一个超链接，这实际上在处理之前下载了文本文件。`read.table` 在后台使用的
    `url` 函数支持 HTTP 和 FTP 协议，并可以处理代理，但它有自己的限制。例如，`url` 除了在 Windows 上的一些例外情况外，不支持 **安全超文本传输协议**（**HTTPS**），这对于访问处理敏感数据的
    Web 服务通常是必需的。
- en: Note
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: HTTPS is not a separate protocol alongside HTTP, but instead HTTP over an encrypted
    SSL/TLS connection. While HTTP is considered to be insecure due to the unencrypted
    packets travelling between the client and server, HTTPS does not let third-parties
    discover sensitive information with the help of signed and trusted certificates.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: HTTPS 不是与 HTTP 并列的独立协议，而是通过加密的 SSL/TLS 连接在 HTTP 之上。虽然 HTTP 由于客户端和服务器之间传输的未加密数据包被认为是不安全的，但
    HTTPS 不允许第三方通过签名和受信任的证书发现敏感信息。
- en: 'In such situations, it''s wise, and used to be the only reasonable option,
    to install and use the `RCurl` package, which is an R client interface to curl:
    [http://curl.haxx.se](http://curl.haxx.se). Curl supports a wide variety of protocols
    and URI schemes and handles cookies, authentication, redirects, timeouts, and
    even more.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，安装并使用 `RCurl` 包是明智的，曾经这也是唯一合理的选项，该包是 R 的 curl 客户端接口：[http://curl.haxx.se](http://curl.haxx.se)。Curl
    支持广泛的协议和 URI 方案，并处理 cookies、身份验证、重定向、超时等。
- en: For example, let's check the U.S. Government's open data catalog at [http://catalog.data.gov/dataset](http://catalog.data.gov/dataset).
    Although the general site can be accessed without SSL, most of the generated download
    URLs follow the HTTPS URI scheme. In the following example, we will fetch the
    **Comma Separated Values** (**CSV**) file of the Consumer Complaint Database from
    the Consumer Financial Protection Bureau, which can be accessed at [http://catalog.data.gov/dataset/consumer-complaint-database](http://catalog.data.gov/dataset/consumer-complaint-database).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们检查美国政府的开放数据目录 [http://catalog.data.gov/dataset](http://catalog.data.gov/dataset)。尽管可以不使用
    SSL 访问一般网站，但大多数生成的下载 URL 都遵循 HTTPS URI 方案。在以下示例中，我们将从消费者金融保护局获取消费者投诉数据库的 **逗号分隔值**（**CSV**）文件，该数据库可通过
    [http://catalog.data.gov/dataset/consumer-complaint-database](http://catalog.data.gov/dataset/consumer-complaint-database)
    访问。
- en: Note
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: This CSV file contains metadata on around a quarter of a million of complaints
    about financial products and services since 2011\. Please note that the file is
    around 35-40 megabytes, so downloading it might take some time, and you would
    probably not want to reproduce the following example on mobile or limited Internet.
    If the `getURL` function fails with a certificate error (this might happen on
    Windows), please provide the path of the certificate manually by `options(RCurlOptions
    = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))` or
    try the more recently published `curl` package by Jeroen Ooms or `httr` (`RCurl`
    front-end) by Hadley Wickham—see later.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此 CSV 文件包含自 2011 年以来关于金融产品和服务的约 25 万条投诉的元数据。请注意，该文件大小约为 35-40 兆字节，因此下载可能需要一些时间，你可能不希望在移动设备或有限互联网环境下重现以下示例。如果
    `getURL` 函数因证书错误而失败（这可能在 Windows 上发生），请通过 `options(RCurlOptions = list(cainfo
    = system.file("CurlSSL", "cacert.pem", package = "RCurl")))` 手动提供证书路径，或者尝试由 Jeroen
    Ooms 或 Hadley Wickham（`RCurl` 前端）最近发布的 `curl` 软件包——详情见后。
- en: 'Let''s see the distribution of these complaints by product type after fetching
    and loading the CSV file directly from R:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看从 R 中直接获取和加载 CSV 文件后，按产品类型分布的这些投诉情况：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Although it's nice to know that most complaints were received about mortgages,
    the point here was to use curl to download the CSV file with a HTTPS URI and then
    pass the content to the `read.csv` function (or any other parser we discussed
    in the previous chapter) as text.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然知道大多数投诉是关于抵押贷款的很好，但这里的重点是使用 curl 下载具有 HTTPS URI 的 CSV 文件，然后将内容传递给 `read.csv`
    函数（或我们在上一章中讨论的任何其他解析器）作为文本。
- en: Note
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Besides `GET` requests, you can easily interact with RESTful API endpoints via
    `POST`, `DELETE`, or `PUT` requests as well by using the `postForm` function from
    the `RCurl` package or the `httpDELETE`, `httpPUT`, or `httpHEAD` functions— see
    details about the `httr` package later.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `GET` 请求外，您还可以通过使用 `RCurl` 软件包中的 `postForm` 函数或 `httpDELETE`、`httpPUT` 或
    `httpHEAD` 函数，轻松通过 `POST`、`DELETE` 或 `PUT` 请求与 RESTful API 端点进行交互——有关 `httr` 软件包的详细信息将在后面介绍。
- en: Curl can also help to download data from a secured site that requires authorization.
    The easiest way to do so is to login to the homepage in a browser, save the cookie
    to a text file, and then pass the path of that to `cookiefile` in `getCurlHandle`.
    You can also specify `useragent` among other options. Please see [http://www.omegahat.org/RCurl/RCurlJSS.pdf](http://www.omegahat.org/RCurl/RCurlJSS.pdf)
    for more details and an overall (and very useful) overview on the most important
    RCurl features.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Curl 还可以帮助从需要授权的安全站点下载数据。这样做最简单的方法是在浏览器中登录主页，将 cookie 保存到文本文件中，然后将该路径传递给 `getCurlHandle`
    中的 `cookiefile`。您还可以在其他选项中指定 `useragent`。有关更多详细信息以及最重要的 RCurl 功能的总体（且非常有用）概述，请参阅
    [http://www.omegahat.org/RCurl/RCurlJSS.pdf](http://www.omegahat.org/RCurl/RCurlJSS.pdf)。
- en: Although curl is extremely powerful, the syntax and the numerous options with
    the technical details might be way too complex for those without a decent IT background.
    The `httr` package is a simplified wrapper around `RCurl` with some sane defaults
    and much simpler configuration options for common operations and everyday actions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管curl功能非常强大，但其语法和众多选项以及技术细节可能对没有良好IT背景的人来说过于复杂。`httr` 软件包是 `RCurl` 的简化包装，提供了一些合理的默认值和更简单的配置选项，用于常见操作和日常行动。
- en: For example, cookies are handled automatically by sharing the same connection
    across all requests to the same website; error handling is much improved, which
    means easier debugging if something goes wrong; the package comes with various
    helper functions to, for instance, set headers, use proxies, and easily issue
    `GET`, `POST`, `PUT`, `DELETE`, and other methods. Even more, it also handles
    authentication in a much more user-friendly way—along with OAuth support.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，cookies 通过在所有对同一网站的请求中共享相同的连接来自动处理；错误处理得到了显著改善，这意味着如果出现问题，调试将更加容易；该软件包包含各种辅助函数，例如设置头部信息、使用代理以及轻松发出
    `GET`、`POST`、`PUT`、`DELETE` 和其他方法。更重要的是，它还以更加用户友好的方式处理身份验证——包括 OAuth 支持。
- en: Note
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: OAuth is the open standard for authorization with the help of intermediary service
    providers. This simply means that the user does not have to share actual credentials,
    but can rather delegate rights to access some of the stored information at the
    service providers. For example, one can authorize Google to share the real name,
    e-mail address, and so on with a third-party without disclosing any other sensitive
    information or any need for passwords. Most generally, OAuth is used for password-less
    login to various Web services and APIs. For more information, please see the [Chapter
    14](ch14.html "Chapter 14. Analyzing the R Community"), *Analyzing the R Community*,
    where we will use OAuth with Twitter to authorize the R session for fetching data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: OAuth 是一种通过中间服务提供商进行授权的开放标准。这简单意味着用户不必共享实际凭证，而是可以委托访问服务提供商存储的一些信息。例如，可以授权 Google
    与第三方共享真实姓名、电子邮件地址等信息，而无需披露任何其他敏感信息或密码。最普遍地，OAuth 用于各种 Web 服务和 API 的无密码登录。有关更多信息，请参阅第
    14 章，*分析 R 社区*，其中我们将使用 OAuth 与 Twitter 授权 R 会话以获取数据。
- en: But what if the data is not available to be downloaded as CSV files?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果数据无法以 CSV 文件的形式下载怎么办？
- en: Other popular online data formats
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他流行的在线数据格式
- en: Structured data is often available in XML or JSON formats on the Web. The high
    popularity of these two formats is due to the fact that both are human-readable,
    easy to handle from a programmatic point of view, and can manage any type of hierarchical
    data structure, not just a simple tabular design, as CSV files are.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化数据通常以 XML 或 JSON 格式在网络上可用。这两种格式之所以如此受欢迎，是因为它们都是可读的，从程序的角度来看易于处理，并且可以管理任何类型的分层数据结构，而不仅仅是简单的表格设计，就像
    CSV 文件那样。
- en: Note
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: JSON is originally derived from *JavaScript Object Notation*, which recently
    became one of the top, most-used standards for human-readable data exchange format.
    JSON is considered to be a low-overhead alternative to XML with attribute-value
    pairs, although it also supports a wide variety of object types such as number,
    string, boolean, ordered lists, and associative arrays. JSON is highly used in
    Web applications, services, and APIs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 最初来源于 *JavaScript 对象表示法*，最近已成为最受欢迎、最常用的数据交换格式标准之一。JSON 被认为是具有属性值对的 XML
    的低开销替代品，尽管它也支持多种对象类型，如数字、字符串、布尔值、有序列表和关联数组。JSON 在 Web 应用程序、服务和 API 中得到了广泛的使用。
- en: Of course, R also supports loading (and saving) data in JSON. Let's demonstrate
    that by fetching some data from the previous example via the Socrata API (more
    on that later in the *R packages to interact with data source APIs* section of
    this chapter), provided by the Consumer Financial Protection Bureau. The full
    documentation of the API is available at [http://www.consumerfinance.gov/complaintdatabase/technical-documentation](http://www.consumerfinance.gov/complaintdatabase/technical-documentation).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，R 也支持以 JSON 格式加载数据（以及保存数据）。让我们通过从上一个示例中通过 Socrata API 获取一些数据来演示这一点（关于这一点，本章的
    *与数据源 API 交互的 R 包* 部分将进行更多介绍），该 API 由消费者金融保护局提供。API 的完整文档可在 [http://www.consumerfinance.gov/complaintdatabase/technical-documentation](http://www.consumerfinance.gov/complaintdatabase/technical-documentation)
    获取。
- en: 'The endpoint of the API is a URL where we can query the background database
    without authentication is [http://data.consumerfinance.gov/api/views](http://data.consumerfinance.gov/api/views).
    To get an overall picture on the structure of the data, the following is the returned
    JSON list opened in a browser:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: API 的端点是 URL，我们可以在这里查询背景数据库而无需认证，即 [http://data.consumerfinance.gov/api/views](http://data.consumerfinance.gov/api/views)。为了获得数据的结构概览，以下是在浏览器中打开的返回的
    JSON 列表：
- en: '![Other popular online data formats](img/2028OS_02_02.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![其他流行的在线数据格式](img/2028OS_02_02.jpg)'
- en: 'As JSON is extremely easy to read, it''s often very helpful to skim through
    the structure manually before parsing. Now let''s load that tree list into R with
    the `rjson` package:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 JSON 非常易于阅读，在解析之前手动浏览其结构通常非常有帮助。现在让我们使用 `rjson` 包将这个树形列表加载到 R 中：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Well, it does not seem to be the same data we have seen before in the comma-separated
    values file! After a closer look at the documentation, it''s clear that the endpoint
    of the API returns metadata on the available views instead of the raw tabular
    data that we saw in the CSV file. So let''s see the view with the ID of `25ei-6bcr`
    now for the first five rows by opening the related URL in a browser:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这似乎与我们之前在逗号分隔值文件中看到的数据不同！经过仔细查看文档，我们可以清楚地看到 API 的端点返回的是可用视图的元数据，而不是我们在 CSV
    文件中看到的原始表格数据。所以，现在让我们通过在浏览器中打开相关 URL 来查看 ID 为 `25ei-6bcr` 的视图的前五行：
- en: '![Other popular online data formats](img/2028OS_02_03.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![其他流行的在线数据格式](img/2028OS_02_03.jpg)'
- en: 'The structure of the resulting JSON list has changed for sure. Now let''s read
    that hierarchical list into R:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的 JSON 列表结构确实发生了变化。现在让我们将这个分层列表读入 R：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We managed to fetch the data along with some further meta-information on the
    view, columns, and so on, which is not something that we are interested in at
    the moment. As `fromJSON` returned a `list` object, we can simply drop the metadata
    and work with the `data` rows from now on:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功地获取了数据以及关于视图、列等的进一步元信息，这不是我们目前感兴趣的东西。由于 `fromJSON` 返回了一个 `list` 对象，我们可以简单地删除元数据，并从现在开始处理
    `data` 行：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This is still a `list`, which we usually want to transform into a `data.frame`
    instead. So we have `list` with five elements, each holding 19 nested children.
    Please note that one of those, the 13th sub element, is list again with 5-5 vectors.
    This means that transforming the tree list into tabular format is not straightforward,
    even less so when we realize that one of those vectors holds multiple values in
    an unprocessed JSON format. So, for the sake of simplicity and proof of a concept
    demo, let''s simply ditch the location-related values now and transform all other
    values to `data.frame`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这仍然是一个 `list`，我们通常希望将其转换为 `data.frame`。所以，我们有包含五个元素的 `list`，每个元素包含 19 个嵌套子元素。请注意，其中之一，第
    13 个子元素，又是一个包含 5-5 向量的 `list`。这意味着将树形列表转换为表格格式并不简单，尤其是当我们意识到其中一个向量以未处理的 JSON 格式包含多个值时。所以，为了简单起见，以及作为概念验证演示，现在让我们简单地丢弃与位置相关的值，并将所有其他值转换为
    `data.frame`：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So we applied a simple function that drops location information from each element
    of the list (by removing the 13th element of each *x*), automatically simplified
    to `matrix` (by using `sapply` instead of `lapply` to iterate though each element
    of the list), transposed it (via `t`), and then coerced the resulting object to
    `data.frame`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们应用了一个简单的函数，从列表的每个元素中删除位置信息（通过删除每个 *x* 的第 13 个元素），自动简化为 `matrix`（通过使用 `sapply`
    而不是 `lapply` 来迭代列表中的每个元素），然后通过 `t` 进行转置，最后将结果对象强制转换为 `data.frame`。
- en: 'Well, we can also use some helper functions instead of manually tweaking all
    the list elements, as earlier. The `plyr` package (please find more details in
    [Chapter 3](ch03.html "Chapter 3. Filtering and Summarizing Data"), *Filtering
    and Summarizing Data* and [Chapter 4](ch04.html "Chapter 4. Restructuring Data"),
    *Restructuring Data*) includes some extremely useful functions to split and combine
    data:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，我们也可以使用一些辅助函数来代替手动调整所有列表元素，就像之前那样。`plyr` 包（请参阅第 3 章[过滤和汇总数据](ch03.html "Chapter
    3. Filtering and Summarizing Data")，*过滤和汇总数据*和第 4 章[重构数据](ch04.html "Chapter 4.
    Restructuring Data")，*重构数据*）包含一些非常实用的函数来分割和组合数据：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'It looks a lot more familiar now, although we miss the variable names, and
    all values were converted to character vectors or factors—even the dates that
    were stored as UNIX timestamps. We can easily fix these problems with the help
    of the provided metadata (`res$meta`): for example, let''s set the variable names
    by extracting (via the `[` operator) the name field of all columns except for
    the dropped (13th) location data:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看起来更熟悉了，尽管我们缺少变量名，所有值都被转换成了字符向量或因子——甚至存储为 UNIX 时间戳的日期也是如此。我们可以借助提供的元数据（`res$meta`）轻松地修复这些问题：例如，让我们通过提取（通过
    `[` 操作符）所有列（除了被删除的 13th 个位置数据）的名称字段来设置变量名：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: One might also identify the object classes with the help of the provided metadata.
    For example, the `renderTypeName` field would be a good start to check, and using
    `as.numeric` for number and `as.POSIXct` for all `calendar_date` fields would
    resolve most of the preceding issues.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 可以借助提供的元数据来识别对象类别。例如，`renderTypeName` 字段是一个很好的起点来检查，使用 `as.numeric` 对数字进行转换，以及使用
    `as.POSIXct` 对所有 `calendar_date` 字段进行转换，可以解决前面的大部分问题。
- en: Well, did you ever hear that around 80 percent of data analysis is spent on
    data preparation?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，你有没有听说过大约 80% 的数据分析时间都花在了数据准备上？
- en: Parsing and restructuring JSON and XML to `data.frame` can take a long time,
    especially when you are dealing with hierarchical lists primarily. The `jsonlite`
    package tries to overcome this issue by transforming R objects into a conventional
    JSON data structure and vice-versa instead of raw conversion. This means from
    a practical point of view that `jsonlite::fromJSON` will result in `data.frame`
    instead of raw list if possible, and it makes the interchange data format even
    more seamless. Unfortunately, we cannot always transform lists to a tabular format;
    in such cases, the list transformations can be speeded up by for example the `rlist`
    package. Please find more details on list manipulations in [Chapter 14](ch14.html
    "Chapter 14. Analyzing the R Community"), *Analyzing the R Community*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 解析和重构JSON和XML到`data.frame`可能需要很长时间，尤其是在你主要处理层次列表时。`jsonlite`包试图通过将R对象转换为传统的JSON数据结构以及相反的操作来克服这个问题，而不是进行原始转换。这意味着从实际的角度来看，如果可能的话，`jsonlite::fromJSON`将产生`data.frame`而不是原始列表，这使得数据交换格式更加无缝。不幸的是，我们并不总能将列表转换为表格格式；在这种情况下，可以通过例如`rlist`包来加速列表转换。请参阅[第14章](ch14.html
    "第14章. 分析R社区")，*分析R社区*中关于列表操作的更多细节。
- en: Note
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Extensible Markup Language** (**XML**) was originally developed by the World
    Wide Web Consortium in 1996 to store documents in a both human-readable and machine-readable
    format. This popular syntax is used in for example the Microsoft Office Open XML
    and Open/LibreOffice OpenDocument file formats, in RSS feeds, and in various configuration
    files. As the format is also highly used for the interchange of data over the
    Internet, data is often available in XML as the only option—especially with some
    older APIs.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**可扩展标记语言** (**XML**)最初于1996年由万维网联盟开发，用于以人类可读和机器可读的格式存储文档。这种流行的语法被用于例如Microsoft
    Office Open XML和Open/LibreOffice OpenDocument文件格式、RSS源和各种配置文件中。由于该格式也高度用于在互联网上交换数据，数据通常以XML作为唯一选项提供——特别是对于一些较旧的API。'
- en: 'Let us also see how we can handle another popular online data interchange format
    besides JSON. The XML API can be used in a similar way, but we must define the
    desired output format in the endpoint URL: [http://data.consumerfinance.gov/api/views.xml](http://data.consumerfinance.gov/api/views.xml),
    as you should be able to see in the following screenshot:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何处理除了JSON之外另一种流行的在线数据交换格式。XML API可以以类似的方式使用，但我们必须在端点URL中定义所需的输出格式：[http://data.consumerfinance.gov/api/views.xml](http://data.consumerfinance.gov/api/views.xml)，正如你可以在下面的屏幕截图中所看到的那样：
- en: '![Other popular online data formats](img/2028OS_02_04.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![其他流行的在线数据格式](img/2028OS_02_04.jpg)'
- en: 'It seems that the XML output of the API differs from what we have seen in the
    JSON format, and it simply includes the rows that we are interested in. This way,
    we can simply parse the XML document and extract the rows from the response then
    transform them to `data.frame`:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来API的XML输出与我们在JSON格式中看到的不同，它仅仅包括我们感兴趣的行。这样，我们就可以简单地解析XML文档，从响应中提取行，然后将它们转换为`data.frame`：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Although we could manually set the desired classes of the variables in the
    `colClasses` argument passed to `xmlToDataFrame`, just like in `read.tables` we
    can also fix this issue afterwards with a quick `helper` function:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以在传递给`xmlToDataFrame`的`colClasses`参数中手动设置变量的所需类别，就像在`read.tables`中一样，我们也可以通过一个快速的`helper`函数来修复这个问题：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: So we tried to guess if a column includes only numbers, and convert those to
    `numeric` if our helper function returns `TRUE`. Please note that we first convert
    the `factor` to `character` before transforming to number, as a direct conversion
    from `factor` to `numeric` would return the `factor` order instead of the real
    value. One might also try to resolve this issue with the `type.convert` function,
    which is used by default in `read.table`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们尝试猜测一列是否只包含数字，并在我们的辅助函数返回`TRUE`时将这些数字转换为`numeric`类型。请注意，我们在将`factor`转换为数字之前，首先将其转换为`character`类型，因为直接从`factor`转换为`numeric`会返回`factor`的顺序而不是实际值。有人也可能尝试使用`type.convert`函数解决这个问题，该函数是`read.table`默认使用的。
- en: Note
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: To test similar APIs and JSON or XML resources, you may find it interesting
    to check out the API of Twitter, GitHub, or probably any other online service
    provider. On the other hand, there is also another open-source service based on
    R that can return XML, JSON, or CSV files from any R code. Please find more details
    at [http://www.opencpu.org](http://www.opencpu.org).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试类似的API和JSON或XML资源，你可能对检查Twitter、GitHub或其他在线服务提供商的API感兴趣。另一方面，还有一个基于R的开源服务，可以从任何R代码返回XML、JSON或CSV文件。请参阅[http://www.opencpu.org](http://www.opencpu.org)获取更多详细信息。
- en: So now we can process structured data from various kinds of downloadable data
    formats but, as there are still some other data source options to master, I promise
    you it's worth it to keep reading.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们可以处理来自各种可下载数据格式的结构化数据，但还有一些其他数据源选项需要掌握，我保证继续阅读是值得的。
- en: Reading data from HTML tables
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从HTML表格中读取数据
- en: According to the traditional document formats on the World Wide Web, most texts
    and data are served in HTML pages. We can often find interesting pieces of information
    in for example HTML tables, from which it's pretty easy to copy and paste data
    into an Excel spreadsheet, save that to disk, and load it to R afterwards. But
    it takes time, it's boring, and can be automated anyway.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 根据万维网上的传统文档格式，大多数文本和数据都是以HTML页面的形式提供的。我们经常可以在HTML表格中找到有趣的信息，例如，很容易将数据复制粘贴到Excel电子表格中，将其保存到磁盘上，然后加载到R中。但这很耗时，很无聊，而且可以自动化。
- en: 'Such HTML tables can be easily generated with the help of the aforementioned
    API of the Customer Compliant Database. If we do not set the required output format
    for which we used XML or JSON earlier, then the browser returns a HTML table instead,
    as you should be able to see in the following screenshot:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这些HTML表格可以很容易地通过上述客户投诉数据库API生成。如果我们没有设置之前使用的XML或JSON所需的输出格式，那么浏览器将返回一个HTML表格，正如你可以在以下屏幕截图中所看到的那样：
- en: '![Reading data from HTML tables](img/2028OS_02_05.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![从HTML表格中读取数据](img/2028OS_02_05.jpg)'
- en: 'Well, in the R console it''s a bit more complicated as the browser sends some
    non-default HTTP headers while using curl, so the preceding URL would simply return
    a JSON list. To get HTML, let the server know that we expect HTML output. To do
    so, simply set the appropriate HTTP header of the query:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，在R控制台中，这要复杂一些，因为当使用curl时，浏览器会发送一些非默认的HTTP头信息，所以先前的URL会简单地返回一个JSON列表。要获取HTML，让服务器知道我们期望HTML输出。为此，只需设置查询的适当HTTP头信息：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `XML` package provides an extremely easy way to parse all the HTML tables
    from a document or specific nodes with the help of the `readHTMLTable` function,
    which returns a `list` of `data.frames` by default:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`XML`包提供了一个非常简单的方法，通过`readHTMLTable`函数从文档或特定节点中解析所有HTML表格，该函数默认返回`data.frames`的`list`：'
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To get only the first table on the page, we can filter `res` afterwards or
    pass the `which` argument to `readHTMLTable`. The following two R expressions
    have the very same results:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取页面上的第一个表格，我们可以在之后过滤`res`或者将`which`参数传递给`readHTMLTable`。以下两个R表达式具有完全相同的结果：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Reading tabular data from static Web pages
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从静态Web页面中读取表格数据
- en: 'Okay, so far we have seen a bunch of variations on the same theme, but what
    if we do not find a downloadable dataset in any popular data format? For example,
    one might be interested in the available R packages hosted at CRAN, whose list
    is available at [http://cran.r-project.org/web/packages/available_packages_by_name.html](http://cran.r-project.org/web/packages/available_packages_by_name.html).
    How do we scrape that? No need to call `RCurl` or to specify custom headers, still
    less do we have to download the file first; it''s enough to pass the URL to `readHTMLTable`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，到目前为止，我们已经看到了同一主题的许多变体，但如果我们没有在任何流行的数据格式中找到可下载的数据集怎么办？例如，有人可能对CRAN上托管的可用R包感兴趣，其列表可在[http://cran.r-project.org/web/packages/available_packages_by_name.html](http://cran.r-project.org/web/packages/available_packages_by_name.html)找到。我们如何抓取这些数据？不需要调用`RCurl`或指定自定义头信息，更不用说先下载文件了；只需将URL传递给`readHTMLTable`即可：
- en: '[PRE13]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: So `readHTMLTable` can directly fetch HTML pages, then it extracts all the HTML
    tables to `data.frame` R objects, and returns a `list` of those. In the preceding
    example, we got a `list` of only one `data.frame` with all the package names and
    descriptions as columns.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`readHTMLTable`可以直接获取HTML页面，然后将其中的所有HTML表格提取到R的`data.frame`对象中，并返回这些表格的`list`。在先前的例子中，我们得到了一个只包含所有软件包名称和描述列的`data.frame`的`list`。
- en: 'Well, this amount of textual information is not really informative with the
    `str` function. For a quick example of processing and visualizing this type of
    raw data, and to present the plethora of available features by means of R packages
    at CRAN, now we can create a word cloud of the package descriptions with some
    nifty functions from the `wordcloud` and the `tm` packages:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，使用`str`函数，这么多的文本信息实际上并不具有很高的信息量。为了快速展示处理和可视化这类原始数据，并展示CRAN上可用的众多R包功能，现在我们可以使用`wordcloud`和`tm`包的一些巧妙函数来创建包描述的词云：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This short command results in the following screenshot, which shows the most
    frequent words found in the R package descriptions. The position of the words
    has no special meaning, but the larger the font size, the higher the frequency.
    Please see the technical description of the plot following the screenshot:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简短的命令产生了以下屏幕截图，显示了在R包描述中找到的最频繁的单词。单词的位置没有特殊意义，但字体越大，频率越高。请参阅屏幕截图后的技术描述：
- en: '![Reading tabular data from static Web pages](img/2028OS_02_06.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![从静态网页中读取表格数据](img/2028OS_02_06.jpg)'
- en: So we simply passed all the strings from the second column of the first `list`
    element to the `wordcloud` function, which automatically runs a few text-mining
    scripts from the `tm` package on the text. You can find more details on this topic
    in [Chapter 7](ch07.html "Chapter 7. Unstructured Data"), *Unstructured Data*.
    Then, it renders the words with a relative size weighted by the number of occurrences
    in the package descriptions. It seems that R packages are indeed primarily targeted
    at building models and applying multivariate tests on data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们简单地将第一个`list`元素的第二列中的所有字符串传递给`wordcloud`函数，该函数自动在文本上运行`tm`包的一些文本挖掘脚本。你可以在[第7章](ch07.html
    "第7章。非结构化数据")*非结构化数据*中找到更多关于这个主题的详细信息。然后，它根据包描述中出现的次数以相对大小渲染单词。这似乎表明R包确实主要针对构建模型和在数据上应用多元测试。
- en: Scraping data from other online sources
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从其他在线来源抓取数据
- en: 'Although the `readHTMLTable` function is very useful, sometimes the data is
    not structured in tables, but rather it''s available only as HTML lists. Let''s
    demonstrate such a data format by checking all the R packages listed in the relevant
    CRAN Task View at [http://cran.r-project.org/web/views/WebTechnologies.html](http://cran.r-project.org/web/views/WebTechnologies.html),
    as you can see in the following screenshot:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`readHTMLTable`函数非常有用，但有时数据不是以表格形式结构化，而是仅作为HTML列表提供。让我们通过检查在相关CRAN任务视图[http://cran.r-project.org/web/views/WebTechnologies.html](http://cran.r-project.org/web/views/WebTechnologies.html)中列出的所有R包来演示这种数据格式，就像你在下面的屏幕截图中所看到的那样：
- en: '![Scraping data from other online sources](img/2028OS_02_07.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![从其他在线来源抓取数据](img/2028OS_02_07.jpg)'
- en: 'So we see a HTML list of the package names along with a URL pointing to the
    CRAN, or in some cases to the GitHub repositories. To proceed, first we have to
    get acquainted a bit with the HTML sources to see how we can parse them. You can
    do that easily either in Chrome or Firefox: just right-click on the **CRAN** packages
    heading at the top of the list, and choose **Inspect Element**, as you can see
    in the following screenshot:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们看到一个包含包名称的HTML列表，以及指向CRAN或在某些情况下指向GitHub仓库的URL。为了继续操作，我们首先需要熟悉一下HTML源代码，看看我们如何解析它们。你可以很容易地在Chrome或Firefox中做到这一点：只需在列表顶部的**CRAN**包标题上右键单击，然后选择**检查元素**，就像你在下面的屏幕截图中所看到的那样：
- en: '![Scraping data from other online sources](img/2028OS_02_08.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![从其他在线来源抓取数据](img/2028OS_02_08.jpg)'
- en: So we have the list of related R packages in an `ul` (unordered list) HTML tag,
    just after the `h3` (level 3 heading) tag holding the `CRAN packages` string.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在包含“CRAN 包”字符串的`h3`（三级标题）标签之后，有一个`ul`（无序列表）HTML标签中有了相关R包的列表。
- en: 'In short:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之：
- en: We have to parse this HTML file
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须解析这个HTML文件
- en: Look for the third-level heading holding the search term
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找包含搜索词的第三级标题
- en: Get all the list elements from the subsequent unordered HTML list
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从随后的无序列表HTML中获取所有列表元素
- en: This can be done by, for example, the XML Path Language, which has a special
    syntax to select nodes in XML/HTML documents via queries.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过例如XML路径语言来完成，它具有特殊的语法，可以通过查询在XML/HTML文档中选择节点。
- en: Note
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more details and R-driven examples, see *Chapter 4*, *XPath, XPointer, and
    XInclude* of the book *XML and Web Technologies for Data Sciences with R* written
    by Deborah Nolan and Duncan Temple Lang in the Use R! series from Springer. Please
    see more references in the *Appendix* at the end of the book.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 更多细节和R驱动的示例，请参阅Deborah Nolan和Duncan Temple Lang所著的《使用R进行数据科学中的XML和Web技术》一书的第4章，*XPath,
    XPointer, and XInclude*，Springer的Use R!系列。请参阅书末的*附录*中的更多参考资料。
- en: 'XPath can be rather ugly and complex at first glance. For example, the preceding
    list can be described with:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: XPath一开始看起来可能相当丑陋和复杂。例如，前面的列表可以用以下方式描述：
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let me elaborate a bit on this:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我详细说明一下：
- en: We are looking for a `h3` tag which has `CRAN packages` as its text, so we are
    searching for a specific node in the whole document with these attributes.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们正在寻找一个文本为`CRAN packages`的`h3`标签，因此我们在整个文档中搜索具有这些属性的特定节点。
- en: Then the `following-siblings` expression stands for all the subsequent nodes
    at the same hierarchy level as the chosen `h3` tag.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后`following-siblings`表达式代表与所选`h3`标签处于同一层次级别的所有后续节点。
- en: Filter to find only `ul` HTML tags.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过滤以找到仅包含`ul` HTML标签。
- en: As we have several of those, we select only the first of the further siblings
    with the index `(1)` between the brackets.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因为我们有多个这样的标签，所以我们只选择括号中索引为`(1)`的后续兄弟中的第一个。
- en: Then we simply select all `li` tags (the list elements) inside that.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们简单地选择那个内部的全部`li`标签（列表元素）。
- en: 'Let''s try it in R:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在R中试试：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'And we have the character vector of the related 118 R packages:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有相关118个R包的字符向量：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'XPath is really powerful for selecting and searching for nodes in HTML documents,
    so is `xpathApply`. The latter is the R wrapper around most of the XPath functionality
    in `libxml`, which makes the process rather quick and efficient. One might rather
    use the `xpathSApply` instead, which tries to simplify the returned list of elements,
    just like `sapply` does compared to the `lapply` function. So we can also update
    our previous code to save the `unlist` call:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: XPath在选择和搜索HTML文档中的节点方面非常强大，`xpathApply`也是如此。后者是`libxml`中大多数XPath功能的R包装器，这使得整个过程相当快速和高效。有人可能会更愿意使用`xpathSApply`，它试图简化返回的元素列表，就像`sapply`与`lapply`函数相比那样。因此，我们也可以更新之前的代码以保存`unlist`调用：
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The attentive reader must have noticed that the returned list was a simple character
    vector, while the original HTML list also included the URLs of the aforementioned
    packages. Where and why did those vanish?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细的读者一定已经注意到，返回的列表是一个简单的字符向量，而原始HTML列表还包括上述包的URL。这些URL在哪里，为什么消失了？
- en: We can blame `xmlValue` for this result, which we called instead of the default
    `NULL` as the evaluating function to extract the nodes from the original document
    at the `xpathSApply` call. This function simply extracts the raw text content
    of each leaf node without any children, which explains this behavior. What if
    we are rather interested in the package URLs?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以责怪`xmlValue`导致这个结果，我们调用它而不是默认的`NULL`作为在`xpathSApply`调用中从原始文档中提取节点时的评估函数。这个函数简单地提取每个叶节点的原始文本内容，没有任何子节点，这解释了这种行为。如果我们更感兴趣的是包的URL呢？
- en: 'Calling `xpathSapply` without a specified fun returns all the raw child nodes,
    which is of no direct help, and we shouldn''t try to apply some regular expressions
    on those. The help page of `xmlValue` can point us to some similar functions that
    can be very handy with such tasks. Here we definitely want to use `xmlAttrs`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有指定fun的情况下调用`xpathSapply`会返回所有原始子节点，这对我们没有直接帮助，我们也不应该尝试在这些节点上应用正则表达式。`xmlValue`的帮助页面可以指引我们一些在类似任务中非常有用的类似函数。在这里，我们肯定想使用`xmlAttrs`：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Please note that an updated path was used here, where now we selected all the
    `a` tags instead of the `li` parents. And, instead of the previously introduced
    `xmlValue`, now we called `xmlAttrs` with the `'href'` extra argument. This simply
    extracts all the `href` arguments of all the related `a` nodes.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里使用了更新的路径，现在我们选择了所有的`a`标签而不是之前的`li`父标签。而且，现在我们调用`xmlAttrs`而不是之前引入的`xmlValue`，并添加了`'href'`额外参数。这简单地提取了所有相关`a`节点的`href`参数。
- en: With these primitives, you will be able to fetch any publicly available data
    from online sources, although sometimes the implementation can end up being rather
    complex.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些原语，您将能够从在线来源获取任何公开可用的数据，尽管有时实现可能相当复杂。
- en: Note
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: On the other hand, please be sure to always consult the terms and conditions
    and other legal documents of all potential data sources, as fetching data is often
    prohibited by the copyright owner.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，请务必始终查阅所有潜在数据源的相关条款和条件以及其他法律文件，因为获取数据通常被版权所有者禁止。
- en: Beside the legal issues, it's also wise to think of fetching and crawling data
    from the technical point of view of the service provider. If you start to send
    a plethora of queries to a server without consulting with their administrators
    beforehand, this action might be construed as a network attack and/or might result
    in an unwanted load on the servers. To keep it simple, always use a sane delay
    between your queries. This should be for example, a 2-second pause between queries
    at a minimum, but it's better to check the *Crawl-delay* directive set in the
    site's *robot.txt*, which can be found in the root path if available. This file
    also contains other directives if crawling is allowed or limited. Most of the
    data provider sites also have some technical documentation on data crawling; please
    be sure to search for Rate limits and throttling.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 除了法律问题之外，从服务提供商的技术角度考虑获取和爬取数据也是明智的。如果你在事先没有咨询管理员的情况下开始向服务器发送大量查询，这种行为可能会被视为网络攻击，或者可能导致服务器上出现不希望的压力。为了简化问题，始终在查询之间使用合理的延迟。这应该是例如，在查询之间至少有2秒的暂停，但最好检查网站`robot.txt`中设置的`Crawl-delay`指令，如果有的话，可以在根路径中找到。此文件还包含其他指令，如果允许或限制爬取。大多数数据提供者网站也提供有关数据爬取的技术文档；请务必搜索速率限制和节流。
- en: And sometimes we are just simply lucky in that someone else has already written
    the tricky XPath selectors or other interfaces, so we can load data from Web services
    and homepages with the help of native R packages.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候我们只是简单地幸运，因为其他人已经编写了棘手的XPath选择器或其他接口，因此我们可以借助本机R包从Web服务和主页加载数据。
- en: R packages to interact with data source APIs
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R包用于与数据源API交互
- en: Although it's great that we can read HTML tables, CSV files and JSON and XML
    data, and even parse raw HTML documents to store some parts of those in a dataset,
    there is no sense in spending too much time developing custom tools until we have
    no other option. First, always start with a quick look on the Web Technologies
    and Services CRAN Task View; also search R-bloggers, StackOverflow, and GitHub
    for any possible solution before getting your hands dirty with custom XPath selectors
    and JSON list magic.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以读取HTML表格、CSV文件和JSON以及XML数据，甚至解析原始HTML文档以将其中一些部分存储在数据集中，但在没有其他选择之前，花费太多时间开发定制工具是没有意义的。首先，始终从Web技术和服务CRAN任务视图快速浏览开始；在动手使用自定义XPath选择器和JSON列表魔法之前，也请在R博客、StackOverflow和GitHub上搜索任何可能的解决方案。
- en: Socrata Open Data API
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Socrata 开放数据API
- en: 'Let''s do this for our previous examples by searching for Socrata, the Open
    Data Application Program Interface of the Consumer Financial Protection Bureau.
    Yes, there is a package for that:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过搜索消费者金融保护局的开放数据应用程序程序接口Socrata来为之前的示例做这件事。是的，为此有一个包：
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As a matter of fact, the `RSocrata` package uses the same JSON sources (or CSV
    files), as we did before. Please note the warning message, which says that `RSocrata`
    depends on another JSON parser R package rather than the one we used, so some
    function names are conflicting. It's probably wise to `detach('package:rjson')`
    before automatically loading the `RJSONIO` package.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，`RSocrata`包使用与我们之前相同的JSON源（或CSV文件）。请注意警告信息，它表示`RSocrata`依赖于另一个JSON解析R包，而不是我们使用的包，因此一些函数名称存在冲突。在自动加载`RJSONIO`包之前，可能明智的做法是`detach('package:rjson')`。
- en: 'Loading the Customer Complaint Database by the given URL is pretty easy with
    `RSocrata`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`RSocrata`通过给定的URL加载客户投诉数据库相当简单：
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We got `numeric` values for numbers, and the dates are also automatically processed
    to `POSIXlt`!
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了数字的`numeric`值，日期也被自动处理为`POSIXlt`！
- en: Similarly, the Web Technologies and Services CRAN Task View contains more than
    a hundred R packages to interact with data sources on the Web in natural sciences
    such as ecology, genetics, chemistry, weather, finance, economics, and marketing,
    but we can also find R packages to fetch texts, bibliography resources, Web analytics,
    news, and map and social media data besides some other topics. Due to page limitations,
    here we will only focus on the most-used packages.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，Web 技术和服务 CRAN 任务视图包含超过一百个 R 包，用于与自然科学（如生态学、遗传学、化学、天气、金融、经济学和营销）中的网络数据源进行交互，但我们还可以找到用于获取文本、参考文献资源、网络分析、新闻以及地图和社交媒体数据的
    R 包，除了其他一些主题。由于页面限制，这里我们只关注最常用的包。
- en: Finance APIs
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 金融 API
- en: 'Yahoo! and Google Finance are pretty standard free data sources for all those
    working in the industry. Fetching for example stock, metal, or foreign exchange
    prices is extremely easy with the `quantmod` package and the aforementioned service
    providers. For example, let us see the most recent stock prices for Agilent Technologies
    with the `A` ticker symbol:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Yahoo! 和 Google Finance 是所有在该行业工作的人的标准免费数据源。例如，使用 `quantmod` 包和上述服务提供商，获取股票、金属或外汇价格非常容易。例如，让我们看看
    Agilent Technologies 的最新股票价格，其交易代码为 `A`：
- en: '[PRE22]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: By default, `getSymbols` assigns the fetched results to the `parent.frame` (usually
    the global) environment with the name of the symbols, while specifying `NULL`
    as the desired environment simply returns the fetched results as an `xts` time-series
    object, as seen earlier.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`getSymbols` 将获取的结果分配给 `parent.frame`（通常是全局）环境，并以符号名称命名，而将 `NULL` 作为所需环境将简单地返回前面看到的
    `xts` 时间序列对象。
- en: 'Foreign exchange rates can be fetched just as easily:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 可以轻松获取外汇汇率：
- en: '[PRE23]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The returned string of `getSymbols` refers to the R variable in which the data
    was saved inside `.GlobalEnv`. To see all the available data sources, let''s query
    the related S3 methods:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`getSymbols` 返回的字符串指的是数据保存在 `.GlobalEnv` 中的 R 变量。要查看所有可用的数据源，让我们查询相关的 S3 方法：'
- en: '[PRE24]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'So besides some offline data sources, we can query Google, Yahoo!, and OANDA
    for recent financial information. To see the full list of available symbols, the
    already loaded `TTR` package might help:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，除了某些离线数据源外，我们还可以查询 Google、Yahoo! 和 OANDA 获取最新的金融信息。要查看可用符号的完整列表，已加载的 `TTR`
    包可能会有所帮助：
- en: '[PRE25]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Find more information on how to handle and analyze similar datasets in [Chapter
    12](ch12.html "Chapter 12. Analyzing Time-series"), *Analyzing Time-series*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 12 章](ch12.html "第 12 章。分析时间序列") *分析时间序列* 中找到更多关于如何处理和分析类似数据集的信息。
- en: Fetching time series with Quandl
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Quandl 获取时间序列
- en: 'Quandl provides access to millions of similar time-series data in a standard
    format, via a custom API, from around 500 data sources. In R, the `Quandl` package
    provides easy access to all these open data in various industries all around the
    world. Let us see for example the dividends paid by Agilent Technologies published
    by the U.S. Securities and Exchange Commission. To do so, simply search for "Agilent
    Technologies" at the [http://www.quandl.com](http://www.quandl.com) homepage,
    and provide the code of the desired data from the search results to the `Quandl`
    function:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Quandl 提供了通过自定义 API 从约 500 个数据源获取数百万类似时间序列数据的标准格式访问权限。在 R 中，`Quandl` 包提供了轻松访问全球各个行业中所有这些开放数据的途径。让我们以美国证券交易委员会发布的
    Agilent Technologies 分红为例。要做到这一点，只需在 [http://www.quandl.com](http://www.quandl.com)
    首页搜索“Agilent Technologies”，并将搜索结果中所需数据的代码提供给 `Quandl` 函数：
- en: '[PRE26]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As you can see, the API is rather limited without a valid authentication token,
    which can be redeemed at the `Quandl` homepage for free. To set your token, simply
    pass that to the `Quandl.auth` function.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，没有有效的身份验证令牌，API 相对有限，该令牌可以在 `Quandl` 首页免费兑换。要设置您的令牌，只需将其传递给 `Quandl.auth`
    函数。
- en: 'This package also lets you:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 此包还允许您：
- en: Fetch filtered data by time
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过时间过滤数据
- en: Perform some transformations of the data on the server side—such as cumulative
    sums and the first differential
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在服务器端对数据进行一些转换——例如累积总和和一阶微分
- en: Sort the data
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据进行排序
- en: Define the desired class of the returning object—such as `ts`, `zoo`, and `xts`
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义返回对象的所需类别——例如 `ts`、`zoo` 和 `xts`
- en: Download some meta-information on the data source
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载有关数据源的一些元信息
- en: 'The latter is saved as `attributes` of the returning R object. So, for example,
    to see the frequency of the queried dataset, call:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 后者被保存为返回 R 对象的 `attributes`。例如，要查看查询数据集的频率，请调用：
- en: '[PRE27]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Google documents and analytics
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google 文档和分析
- en: You might however be more interested in loading your own or custom data from
    Google Docs, to which end the `RGoogleDocs` package is a great help and is available
    for download at the [http://www.omegahat.org/](http://www.omegahat.org/) homepage.
    It provides authenticated access to Google spreadsheets with both read and write
    access.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能更感兴趣的是从 Google Docs 加载您自己的或自定义数据，为此 `RGoogleDocs` 包非常有帮助，并且可以在 [http://www.omegahat.org/](http://www.omegahat.org/)
    网站主页上下载。它提供了对 Google 电子表格的认证访问，具有读写权限。
- en: Unfortunately, this package is rather outdated and uses some deprecated API
    functions, so you might be better trying some newer alternatives, such as the
    recently released `googlesheets` package, which can manage Google Spreadsheets
    (but not other documents) from R.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个包相当过时，并使用了一些已弃用的 API 函数，因此您可能最好尝试一些较新的替代方案，例如最近发布的 `googlesheets` 包，它可以从
    R 中管理 Google 电子表格（但不能管理其他文档）。
- en: Similar packages are also available to interact with Google Analytics or Google
    Adwords for all those, who would like to analyze page visits or ad performance
    in R.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有希望使用 R 分析页面访问或广告性能的人来说，也有类似的包可以与 Google Analytics 或 Google Adwords 交互。
- en: Online search trends
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在线搜索趋势
- en: On the other hand, we interact with APIs to download public data. Google also
    provides access to some public data of the World Bank, IMF, US Census Bureau,
    and so on at [http://www.google.com/publicdata/directory](http://www.google.com/publicdata/directory)
    and also some of their own internal data in the form of search trends at [http://google.com/trends](http://google.com/trends).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们通过交互 API 下载公共数据。Google 还在 [http://www.google.com/publicdata/directory](http://www.google.com/publicdata/directory)
    提供了世界银行、国际货币基金组织、美国人口普查局等机构的公共数据访问权限，以及一些以搜索趋势形式存在的他们自己的内部数据 [http://google.com/trends](http://google.com/trends)。
- en: 'The latter can be queried extremely easily with the `GTrendsR` package, which
    is not yet available on CRAN, but we can at least practice how to install R packages
    from other sources. The `GTrendR` code repository can be found on `BitBucket`,
    from where it''s really convenient to install it with the `devtools` package:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `GTrendsR` 包可以非常容易地查询后者，该包尚未在 CRAN 上提供，但我们可以至少练习如何从其他来源安装 R 包。`GTrendR` 代码仓库可以在
    `BitBucket` 上找到，从那里使用 `devtools` 包安装它非常方便：
- en: Tip
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: To make sure you install the same version of `GTrensR` as used in the following,
    you can specify the `branch`, `commit`, or other reference in the `ref` argument
    of the `install_bitbucket` (or `install_github`) function. Please see the *References*
    section in the *Appendix* at the end of the book for the commit hash.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保您安装的 `GTrensR` 与以下使用的版本相同，您可以在 `install_bitbucket`（或 `install_github`）函数的
    `ref` 参数中指定 `branch`、`commit` 或其他引用。请参阅本书末尾附录中的 *参考文献* 部分以获取提交哈希值。
- en: '[PRE28]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'So installing R packages from BitBucket or GitHub is as easy as providing the
    name of the code repository and author''s username and allowing `devtools` to
    do the rest: downloading the sources and compiling them.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 从 BitBucket 或 GitHub 安装 R 包就像提供代码仓库的名称和作者的用户名，然后允许 `devtools` 做剩下的工作：下载源代码并编译它们。
- en: 'Windows users should install `Rtools` prior to compiling packages from the
    source: [http://cran.r-project.org/bin/windows/Rtools/](http://cran.r-project.org/bin/windows/Rtools/).
    We also enabled the quiet mode, to suppress compilation logs and the boring details.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Windows 用户在从源代码编译包之前应安装 `Rtools`：[http://cran.r-project.org/bin/windows/Rtools/](http://cran.r-project.org/bin/windows/Rtools/)。我们还启用了静默模式，以抑制编译日志和无聊的细节。
- en: 'After the package has been installed, we can load it in the traditional way:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在包安装完成后，我们可以以传统方式加载它：
- en: '[PRE29]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'First, we have to authenticate with a valid Google username and password before
    being able to query the Google Trends database. Our search term will be "how to
    install R":'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须使用有效的 Google 用户名和密码进行身份验证，然后才能查询 Google Trends 数据库。我们的搜索词将是 "如何安装 R"：
- en: Tip
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Please make sure you provide a valid username and password; otherwise the following
    query will fail.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保您提供有效的用户名和密码；否则，以下查询将失败。
- en: '[PRE30]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The returned dataset includes weekly metrics on the relative amount of search
    queries on R installation. The data shows that the highest activity was recorded
    in the middle of July, while only around 75 percent of those search queries were
    triggered at the beginning of the next month. So Google do not publish raw search
    query statistics, but rather comparative studies can be done with different search
    terms and time periods.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的数据集包括关于R安装相对搜索查询量的每周指标。数据显示，最高活动记录在七月中旬，而只有大约75%的这些搜索查询是在下个月初触发的。所以Google不发布原始搜索查询统计数据，而是可以通过不同的搜索词和时间周期进行对比研究。
- en: Historical weather data
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 历史天气数据
- en: 'There are also various packages providing access to data sources for all R
    users in Earth Science. For example, the `RNCEP` package can download historical
    weather data from the National Centers for Environmental Prediction for more than
    one hundred years in six hourly resolutions. The `weatherData` package provides
    direct access to [http://wunderground.com](http://wunderground.com). For a quick
    example, let us download the daily temperature averages for the last seven days
    in London:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，也有各种包为地球科学中的所有R用户提供访问数据源的方式。例如，`RNCEP`包可以从国家环境预测中心下载超过一百年的历史天气数据，分辨率为每六小时一次。`weatherData`包提供直接访问[http://wunderground.com](http://wunderground.com)。作为一个快速示例，让我们下载伦敦过去七天的每日平均温度：
- en: '[PRE31]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Please note that an unimportant part of the preceding output was suppressed,
    but what happened here is quite straightforward: the package fetched the specified
    URL, which is a CSV file by the way, then parsed that with some additional information.
    Setting `opt_detailed` to `TRUE` would also return intraday data with a 30-minute
    resolution.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面输出中的不重要部分已被抑制，但这里发生的事情相当简单：包抓取了指定的URL，顺便提一下，这是一个CSV文件，然后使用一些附加信息进行解析。将`opt_detailed`设置为`TRUE`也会返回日内数据，分辨率为30分钟。
- en: Other online data sources
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他在线数据源
- en: Of course, this short chapter cannot provide an overview of querying all the
    available online data sources and R implementations, but please consult the Web
    Technologies and Services CRAN Task View, R-bloggers, StackOverflow, and the resources
    in the *References* chapter at the end of the book to look for any already existing
    R packages or helper functions before creating your own crawler R scripts.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个简短的章节不能提供查询所有可用在线数据源和R实现的概述，但请在创建自己的R爬虫脚本之前，咨询Web技术和服务CRAN任务视图、R博客、StackOverflow以及本书末尾的*参考文献*章节中的资源，以寻找任何现有的R包或辅助函数。
- en: Summary
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter focused on how to fetch and process data directly from the Web,
    including some problems with downloading files, processing XML and JSON formats,
    parsing HTML tables, applying XPath selectors to extract data from HTML pages,
    and interacting with RESTful APIs.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍了如何直接从网络中获取和处理数据，包括下载文件、处理XML和JSON格式、解析HTML表格、应用XPath选择器从HTML页面中提取数据以及与RESTful
    API交互的问题。
- en: Although some examples in this chapter might appear to have been an idle struggle
    with the Socrata API, it turned out that the `RSocrata` package provides production-ready
    access to all those data. However, please bear in mind that you will face some
    situations without ready-made R packages; thus, as a data hacker, you will have
    to get your hands dirty with all the JSON, HTML and XML sources.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本章中的一些示例可能看起来是对Socrata API的无效斗争，但结果证明`RSocrata`包提供了对所有这些数据的生产级访问。然而，请记住，你将面临一些没有现成R包的情况；因此，作为一名数据黑客，你将不得不亲手处理所有的JSON、HTML和XML源。
- en: In the next chapter, we will discover how to filter and aggregate the already
    acquired and loaded data with the top, most-used methods for reshaping and restructuring
    data.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将发现如何使用重塑和重构数据的顶级、最常用方法来过滤和聚合已经获取和加载的数据。
