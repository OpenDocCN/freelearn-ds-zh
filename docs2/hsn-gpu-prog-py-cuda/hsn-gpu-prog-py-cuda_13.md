# 评估

# 第一章，为什么进行 GPU 编程？

1.  前两个 `for` 循环遍历每个像素，其输出对彼此是恒定的；因此，我们可以在这两个 `for` 循环上并行化。第三个 `for` 循环计算特定像素的最终值，这是本质上是递归的。

1.  Amdahl 定律没有考虑到在 GPU 和主机之间传输内存所需的时间。

1.  512 x 512 等于 262,144 像素。这意味着第一个 GPU 只能一次计算一半像素的输出，而第二个 GPU 可以一次计算所有像素；这意味着第二个 GPU 在这里将比第一个快大约两倍。第三个 GPU 有足够的内核一次计算所有像素，但正如我们在问题 1 中看到的，额外的内核在这里对我们没有帮助。因此，对于这个问题，第二个和第三个 GPU 的速度将一样快。

1.  关于 Amdahl 定律，将某段代码泛化为可并行化的代码有一个问题是，这假设如果处理器数量 *N* 非常大，这段代码的计算时间将接近 0。正如我们从最后一个问题中看到的，这并不成立。

1.  首先，一致地使用 *time* 可能会很麻烦，而且可能无法准确找到程序的性能瓶颈。其次，分析器可以从 Python 的角度告诉你所有代码的确切计算时间，因此你可以判断是某些库函数还是操作系统的后台活动出了问题，而不是你的代码。

# 第二章，设置您的 GPU 编程环境

1.  不，CUDA 只支持 Nvidia GPU，不支持 Intel HD 或 AMD Radeon

1.  本书仅使用 Python 2.7 示例

1.  设备管理器

1.  `lspci`

1.  `free`

1.  `.run`

# 第三章，开始使用 PyCUDA

1.  是的。

1.  主机/设备之间的内存传输和编译时间。

1.  你可以，但这将取决于你的 GPU 和 CPU 设置。

1.  使用 C 的 `?` 操作符对点积和归约操作都这样做。

1.  如果 `gpuarray` 对象超出作用域，其析构函数将被调用，这将自动释放（释放）它在 GPU 上表示的内存。

1.  `ReductionKernel` 可能会执行多余的运算，这取决于底层 GPU 代码的结构是否必要。一个 *中立元素* 将确保这些多余的运算不会改变任何值。

1.  我们应该将 `neutral` 设置为有符号 32 位整数可能的最小值。

# 第四章，内核、线程、块和网格

1.  尝试一下。

1.  并非所有线程都会同时运行在 GPU 上。这与 CPU 在操作系统之间切换任务非常相似，GPU 的各个核心会在不同的线程之间切换。

1.  O( n/640 log n)，即 O(n log n)。

1.  尝试一下。

1.  实际上，CUDA 中没有内部网格级别的同步——只有块级别的（使用 `__syncthreads`）。我们必须使用主机来同步超过单个块的所有内容。

1.  简单：129 次加法操作。工作高效：62 次加法操作。

1.  再次，如果我们需要在大型块网格上同步，我们不能使用`__syncthreads`。如果我们同步在主机上，我们还可以在每个迭代中启动较少的线程，从而为其他操作释放更多资源。

1.  在简单并行求和的情况下，我们可能会只处理少量数据点，这些数据点应该等于或小于 GPU 核心的总数，这很可能适合块的最大大小（1032）；由于单个块可以内部同步，我们应该这样做。只有当数据点的数量远大于 GPU 上可用的核心数量时，我们才应该使用工作高效的算法。

# 第五章，流、事件、上下文和并发

1.  对于两者，性能都有所提高；随着线程数量的增加，GPU 在这两种情况下都达到了峰值利用率，减少了使用流带来的收益。

1.  是的，你可以异步启动任意数量的内核，并使用`cudaDeviceSynchronize`来同步它们。

1.  打开你的文本编辑器并尝试一下！

1.  高标准差意味着 GPU 的使用不均匀，在某些点上会压倒 GPU，而在其他点上则会低效使用。低标准差意味着所有启动的操作都在一般地平稳运行。

1.  i. 主机通常可以处理比 GPU 少得多的并发线程。ii. 每个线程都需要自己的 CUDA 上下文。由于每个上下文都有自己的内存空间，并且必须处理自己的加载的可执行代码，因此 GPU 可能会因为过多的上下文而变得过载。

# 第六章，调试和性能分析您的 CUDA 代码

1.  在 CUDA 中，内存分配会自动同步。

1.  `lockstep`属性仅在大小为 32 或更小的单个块中成立。在这里，两个块会正确地发散，没有任何`lockstep`。

1.  这里也会发生同样的事情。这个 64 线程的块实际上会被分成两个 32 线程的 warp。

1.  Nvprof 可以计时单个内核启动、GPU 利用率和流使用；任何主机端分析器都只会看到启动的 CUDA 主机函数。

1.  对于相对较短、内联的内核的小规模项目，使用 Printf 通常更容易。如果你编写了一个非常复杂的 CUDA 内核，有数千行代码，那么你可能想使用 IDE 逐行逐步调试和调试你的内核。

1.  这告诉 CUDA 我们想要使用哪个 GPU。

1.  `cudaDeviceSynchronize`将确保相互依赖的内核启动和内存复制确实是同步的，并且它们不会在所有必要的操作完成之前启动。

# 第七章，使用 Scikit-CUDA 与 CUDA 库

1.  SBLAH 以 S 开头，因此这个函数使用 32 位实浮点数。ZBLEH 以 Z 开头，这意味着它使用 128 位复数浮点数。

1.  提示：设置`trans = cublas._CUBLAS_OP['T']`

1.  提示：使用 Scikit-CUDA 包装器到点积，`skcuda.cublas.cublasSdot`

1.  提示：基于上一个问题的答案进行构建。

1.  你可以将 cuBLAS 操作放入 CUDA 流中，并使用此流的事件对象来精确测量 GPU 上的计算时间。

1.  由于输入对 cuFFT 来说似乎很复杂，它将计算所有值作为 NumPy。

1.  暗色边缘是由于图像周围的零缓冲区。这可以通过在边缘上*镜像*图像而不是使用零缓冲区来缓解。

# 第八章，CUDA 设备函数库和 Thrust

1.  尝试一下。（实际上比你想象的要准确。）

1.  一个应用：高斯分布可以用来向样本添加`白噪声`，以增强机器学习中的数据集。

1.  不，因为它们来自不同的种子，如果我们将它们连接起来，这些列表可能会有很强的相关性。如果我们计划将它们连接起来，我们应该使用相同种子的子序列。

1.  尝试一下。

1.  提示：记住，矩阵乘法可以被视为一系列矩阵-向量乘法，而矩阵-向量乘法可以被视为一系列点积。

1.  使用`Operator()`来定义实际函数。

# 第九章，深度神经网络实现

1.  可能的问题之一是我们没有对训练输入进行归一化。另一个可能是训练率过大。

1.  如果训练率很小，一组权重可能会非常缓慢地收敛，或者根本不收敛。

1.  大的训练率可能导致一组权重过度拟合特定的批次值或这个训练集。它还可能导致数值溢出/下溢，就像第一个问题中那样。

1.  Sigmoid。

1.  Softmax。

1.  更多更新。

# 第十章，与编译的 GPU 代码一起工作

1.  只有 EXE 文件将包含宿主函数，但 PTX 和 EXE 都将包含 GPU 代码。

1.  `cuCtxDestroy`。

1.  带有任意输入参数的`printf`。（尝试查找`printf`原型。）

1.  使用 Ctypes 的`c_void_p`对象。

1.  这将允许我们从 Ctypes 链接到具有其原始名称的函数。

1.  设备内存分配和设备/主机之间的 memcopies 由 CUDA 自动同步。

# 第十一章，CUDA 性能优化

1.  `atomicExch`是线程安全的，但这并不保证所有线程都会同时执行此函数（因为网格中的不同块可以在不同时间执行）。

1.  大小为 100 的块将在多个 warp 上执行，除非我们使用`__syncthreads`，否则块内不会同步。因此，`atomicExch`可能会被多次调用。

1.  由于 warp 默认以锁步方式执行，且大小为 32 或更小的块使用单个 warp 执行，因此`__syncthreads`是不必要的。

1.  我们在 warp 内使用简单的并行求和，但除此之外，我们使用`atomicAdd`进行的求和与串行求和一样多。虽然 CUDA 自动并行化了许多这些`atomicAdd`调用，但我们可以通过实现一个工作高效的并行求和来减少所需的`atomicAdd`调用总数。

1.  肯定是 `sum_ker`。很明显，PyCUDA 的求和操作并没有使用我们使用的相同硬件技巧，因为我们的在较小的数组上表现更好，但通过将规模扩大得多，PyCUDA 版本更好的唯一解释是它执行了更少的多项式运算。

# 第十二章，下一步该去哪里

1.  两个例子：DNA 分析以及物理模拟。

1.  两个例子：OpenACC，Numba。

1.  TPU 仅用于机器学习操作，缺乏渲染图形所需的组件。

1.  以太网。
