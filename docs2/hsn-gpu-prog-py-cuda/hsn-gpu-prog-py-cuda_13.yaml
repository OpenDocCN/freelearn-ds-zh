- en: Assessment
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: Chapter 1, Why GPU Programming?
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章，为什么进行 GPU 编程？
- en: The first two `for` loops iterate over every pixel, whose outputs are invariant
    to each other; we can thus parallelize over these two `for` loops. The third `for`
    loop calculates the final value of a particular pixel, which is intrinsically
    recursive.
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前两个 `for` 循环遍历每个像素，其输出对彼此是恒定的；因此，我们可以在这两个 `for` 循环上并行化。第三个 `for` 循环计算特定像素的最终值，这是本质上是递归的。
- en: Amdahl's Law doesn't account for the time it takes to transfer memory between
    the GPU and the host.
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Amdahl 定律没有考虑到在 GPU 和主机之间传输内存所需的时间。
- en: 512 x 512 amounts to 262,144 pixels. This means that the first GPU can only
    calculate the outputs of half of the pixels at once, while the second GPU can
    calculate all of the pixels at once; this means the second GPU will be about twice
    as fast as the first here. The third GPU has more than sufficient cores to calculate
    all pixels at once, but as we saw in problem 1, the extra cores will be of no
    use to us here. So the second and third GPUs will be equally fast for this problem.
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 512 x 512 等于 262,144 像素。这意味着第一个 GPU 只能一次计算一半像素的输出，而第二个 GPU 可以一次计算所有像素；这意味着第二个
    GPU 在这里将比第一个快大约两倍。第三个 GPU 有足够的内核一次计算所有像素，但正如我们在问题 1 中看到的，额外的内核在这里对我们没有帮助。因此，对于这个问题，第二个和第三个
    GPU 的速度将一样快。
- en: One issue with generically designating a certain segment of code as parallelizable
    with regards to Amdahl's Law is that this makes the assumption that the computation
    time for this piece of code will be close to 0 if the number of processors, *N*,
    is very large. As we can see from the last problem, this is not the case.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于 Amdahl 定律，将某段代码泛化为可并行化的代码有一个问题是，这假设如果处理器数量 *N* 非常大，这段代码的计算时间将接近 0。正如我们从最后一个问题中看到的，这并不成立。
- en: First, using *time* consistently can be cumbersome, and it might not zero in
    on the bottlenecks of your program. Second, a profiler can tell you the exact
    computation time of all of your code from the perspective of Python, so you can
    tell whether some library function or background activity of your operating system
    is at fault rather than your code.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，一致地使用 *time* 可能会很麻烦，而且可能无法准确找到程序的性能瓶颈。其次，分析器可以从 Python 的角度告诉你所有代码的确切计算时间，因此你可以判断是某些库函数还是操作系统的后台活动出了问题，而不是你的代码。
- en: Chapter 2, Setting Up Your GPU Programming Environment
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章，设置您的 GPU 编程环境
- en: No, CUDA only supports Nvidia GPUs, not Intel HD or AMD Radeon
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不，CUDA 只支持 Nvidia GPU，不支持 Intel HD 或 AMD Radeon
- en: This book only uses Python 2.7 examples
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本书仅使用 Python 2.7 示例
- en: Device Manager
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设备管理器
- en: '`lspci`'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`lspci`'
- en: '`free`'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`free`'
- en: '`.run`'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.run`'
- en: Chapter 3, Getting Started with PyCUDA
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章，开始使用 PyCUDA
- en: Yes.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是的。
- en: Memory transfers between host/device, and compilation time.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主机/设备之间的内存传输和编译时间。
- en: You can, but this will vary depending on your GPU and CPU setup.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以，但这将取决于你的 GPU 和 CPU 设置。
- en: Do this using the C `?` operator for both the point-wise and reduce operations.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 C 的 `?` 操作符对点积和归约操作都这样做。
- en: If a `gpuarray` object goes out of scope its destructor is called, which will
    deallocate (free) the memory it represents on the GPU automatically.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 `gpuarray` 对象超出作用域，其析构函数将被调用，这将自动释放（释放）它在 GPU 上表示的内存。
- en: '`ReductionKernel` may perform superfluous operations, which may be necessary
    depending on how the underlying GPU code is structured. A *neutral element* will
    ensure that no values are altered as a result of these superfluous operations.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ReductionKernel` 可能会执行多余的运算，这取决于底层 GPU 代码的结构是否必要。一个 *中立元素* 将确保这些多余的运算不会改变任何值。'
- en: We should set `neutral` to the smallest possible value of a signed 32-bit integer.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该将 `neutral` 设置为有符号 32 位整数可能的最小值。
- en: Chapter 4, Kernels, Threads, Blocks, and Grids
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章，内核、线程、块和网格
- en: Try it.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试一下。
- en: All of the threads don't operate on the GPU simultaneously. Much like a CPU
    switching between tasks in an OS, the individual cores of the GPU switch between
    the different threads for a kernel.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 并非所有线程都会同时运行在 GPU 上。这与 CPU 在操作系统之间切换任务非常相似，GPU 的各个核心会在不同的线程之间切换。
- en: O( n/640 log n), that is, O(n log n).
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: O( n/640 log n)，即 O(n log n)。
- en: Try it.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试一下。
- en: There is actually no internal grid-level synchronization in CUDA—only block-level
    (with `__syncthreads).` We have to synchronize anything above a single block with
    the host.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实际上，CUDA 中没有内部网格级别的同步——只有块级别的（使用 `__syncthreads`）。我们必须使用主机来同步超过单个块的所有内容。
- en: 'Naive: 129 addition operations. Work-efficient: 62 addition operations.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 简单：129次加法操作。工作高效：62次加法操作。
- en: Again, we can't use `__syncthreads` if we need to synchronize over a large grid
    of blocks. We can also launch over fewer threads on each iteration if we synchronize
    on the host, freeing up more resources for other operations.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，如果我们需要在大型块网格上同步，我们不能使用`__syncthreads`。如果我们同步在主机上，我们还可以在每个迭代中启动较少的线程，从而为其他操作释放更多资源。
- en: In the case of a naive parallel sum, we will likely be working with only a small
    number of data points that should be equal to or less than the total number of
    GPU cores, which can likely fit in the maximum size of a block (1032); since a
    single block can be synchronized internally, we should do so. We should use the
    work-efficient algorithm only if the number of data points are far greater than
    the number of available cores on the GPU.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在简单并行求和的情况下，我们可能会只处理少量数据点，这些数据点应该等于或小于GPU核心的总数，这很可能适合块的最大大小（1032）；由于单个块可以内部同步，我们应该这样做。只有当数据点的数量远大于GPU上可用的核心数量时，我们才应该使用工作高效的算法。
- en: Chapter 5, Streams, Events, Contexts, and Concurrency
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章，流、事件、上下文和并发
- en: The performance improves for both; as we increase the number of threads, the
    GPU reaches peak utilization in both cases, reducing the gains made through using
    streams.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于两者，性能都有所提高；随着线程数量的增加，GPU在这两种情况下都达到了峰值利用率，减少了使用流带来的收益。
- en: Yes, you can launch an arbitrary number of kernels asynchronously and synchronize
    them to with `cudaDeviceSynchronize`.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是的，你可以异步启动任意数量的内核，并使用`cudaDeviceSynchronize`来同步它们。
- en: Open up your text editor and try it!
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你的文本编辑器并尝试一下！
- en: High standard deviation would mean that the GPU is being used unevenly, overwhelming
    the GPU at some points and under-utilizing it at others. A low standard deviation
    would mean that all launched operations are running generally smoothly.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高标准差意味着GPU的使用不均匀，在某些点上会压倒GPU，而在其他点上则会低效使用。低标准差意味着所有启动的操作都在一般地平稳运行。
- en: i. The host can generally handle far fewer concurrent threads than a GPU. ii.
    Each thread requires its own CUDA context. The GPU can become overwhelmed with
    excessive contexts, since each has its own memory space and has to handle its
    own loaded executable code.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: i. 主机通常可以处理比GPU少得多的并发线程。ii. 每个线程都需要自己的CUDA上下文。由于每个上下文都有自己的内存空间，并且必须处理自己的加载的可执行代码，因此GPU可能会因为过多的上下文而变得过载。
- en: Chapter 6, Debugging and Profiling Your CUDA Code
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章，调试和性能分析您的CUDA代码
- en: Memory allocations are automatically synchronized in CUDA.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在CUDA中，内存分配会自动同步。
- en: The `lockstep` property only holds in single blocks of size 32 or less. Here,
    the two blocks would properly diverge without any `lockstep`.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`lockstep`属性仅在大小为32或更小的单个块中成立。在这里，两个块会正确地发散，没有任何`lockstep`。'
- en: The same thing would happen here. This 64-thread block would actually be split
    into two 32-thread warps.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里也会发生同样的事情。这个64线程的块实际上会被分成两个32线程的warp。
- en: Nvprof can time individual kernel launches, GPU utilization, and stream usage;
    any host-side profiler would only see CUDA host functions being launched.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Nvprof可以计时单个内核启动、GPU利用率和流使用；任何主机端分析器都只会看到启动的CUDA主机函数。
- en: Printf is generally easier to use for small-scale projects with relatively short,
    inline kernels. If you write a very involved CUDA kernel with thousands of lines,
    then probably you would want to use the IDE to step through and debug your kernel
    line by line.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于相对较短、内联的内核的小规模项目，使用Printf通常更容易。如果你编写了一个非常复杂的CUDA内核，有数千行代码，那么你可能想使用IDE逐行逐步调试和调试你的内核。
- en: This tells CUDA which GPU we want to use.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这告诉CUDA我们想要使用哪个GPU。
- en: '`cudaDeviceSynchronize` will ensure that interdependent kernel launches and
    mem copies are indeed synchronized, and that they won''t launch before all necessary
    operations have finished.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cudaDeviceSynchronize`将确保相互依赖的内核启动和内存复制确实是同步的，并且它们不会在所有必要的操作完成之前启动。'
- en: Chapter 7, Using the CUDA Libraries with Scikit-CUDA
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章，使用Scikit-CUDA与CUDA库
- en: SBLAH starts with an S, so this function uses 32-bit real floats. ZBLEH starts
    with a Z, which means it works with 128-bit complex floats.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SBLAH以S开头，因此这个函数使用32位实浮点数。ZBLEH以Z开头，这意味着它使用128位复数浮点数。
- en: 'Hint: set `trans = cublas._CUBLAS_OP[''T'']`'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示：设置`trans = cublas._CUBLAS_OP['T']`
- en: 'Hint: use the Scikit-CUDA wrapper to the dot product, `skcuda.cublas.cublasSdot`'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示：使用Scikit-CUDA包装器到点积，`skcuda.cublas.cublasSdot`
- en: 'Hint: build upon the answer to the last problem.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示：基于上一个问题的答案进行构建。
- en: You can put the cuBLAS operations in a CUDA stream and use event objects with
    this stream to precisely measure the computation times on the GPU.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以将cuBLAS操作放入CUDA流中，并使用此流的事件对象来精确测量GPU上的计算时间。
- en: Since the input appears as being complex to cuFFT, it will calculate all of
    the values as NumPy.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于输入对cuFFT来说似乎很复杂，它将计算所有值作为NumPy。
- en: The dark edge is due to the zero-buffering around the image. This can be mitigated
    by *mirroring* the image on its edges rather than by using a zero-buffer.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 暗色边缘是由于图像周围的零缓冲区。这可以通过在边缘上*镜像*图像而不是使用零缓冲区来缓解。
- en: Chapter 8, The CUDA Device Function Libraries and Thrust
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章，CUDA设备函数库和Thrust
- en: Try it. (It's actually more accurate than you'd think.)
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试一下。（实际上比你想象的要准确。）
- en: 'One application: a Gaussian distribution can be used to add `white noise` to
    samples to augment a dataset in machine learning.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个应用：高斯分布可以用来向样本添加`白噪声`，以增强机器学习中的数据集。
- en: No, since they are from different seeds, these lists may have a strong correlation
    if we concatenate them together. We should use subsequences of the same seed if
    we plan to concatenate them together.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不，因为它们来自不同的种子，如果我们将它们连接起来，这些列表可能会有很强的相关性。如果我们计划将它们连接起来，我们应该使用相同种子的子序列。
- en: Try it.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试一下。
- en: 'Hint: remember that matrix multiplication can be thought of as a series of
    matrix-vector multiplications, while matrix-vector multiplication can be thought
    of as a series of dot products.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提示：记住，矩阵乘法可以被视为一系列矩阵-向量乘法，而矩阵-向量乘法可以被视为一系列点积。
- en: '`Operator()` is used to define the actual function.'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`Operator()`来定义实际函数。
- en: Chapter 9, Implementation of a Deep Neural Network
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章，深度神经网络实现
- en: One problem could be that we haven't normalized our training inputs. Another
    could be that the training rate was too large.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能的问题之一是我们没有对训练输入进行归一化。另一个可能是训练率过大。
- en: With a small training rate a set of weights might converge very slowly, or not
    at all.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果训练率很小，一组权重可能会非常缓慢地收敛，或者根本不收敛。
- en: A large training rate can lead to a set of weights being over-fit to particular
    batch values or this training set. Also, it can lead to numerical overflows/underflows
    as in the first problem.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大的训练率可能导致一组权重过度拟合特定的批次值或这个训练集。它还可能导致数值溢出/下溢，就像第一个问题中那样。
- en: Sigmoid.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sigmoid。
- en: Softmax.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Softmax。
- en: More updates.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更多更新。
- en: Chapter 10, Working with Compiled GPU Code
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章，与编译的GPU代码一起工作
- en: Only the EXE file will have the host functions, but both the PTX and EXE will
    contain the GPU code.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只有EXE文件将包含宿主函数，但PTX和EXE都将包含GPU代码。
- en: '`cuCtxDestory`.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cuCtxDestroy`。'
- en: '`printf` with arbitrary input parameters. (Try looking up the `printf` prototype.)'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带有任意输入参数的`printf`。（尝试查找`printf`原型。）
- en: With a Ctypes `c_void_p` object.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Ctypes的`c_void_p`对象。
- en: This will allow us to link to the function with its original name from Ctypes.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将允许我们从Ctypes链接到具有其原始名称的函数。
- en: Device memory allocations and memcopies between device/host are automatically
    synchronized by CUDA.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设备内存分配和设备/主机之间的memcopies由CUDA自动同步。
- en: Chapter 11, Performance Optimization in CUDA
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章，CUDA性能优化
- en: The fact that `atomicExch` is thread-safe doesn't guarantee that all threads
    will execute this function at the same time (which is not the case since different
    blocks in a grid can be executed at different times).
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`atomicExch`是线程安全的，但这并不保证所有线程都会同时执行此函数（因为网格中的不同块可以在不同时间执行）。'
- en: A block of size 100 will be executed over multiple warps, which will not be
    synchronized within the block unless we use `__syncthreads`. Thus, `atomicExch`
    may be called at multiple times.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大小为100的块将在多个warp上执行，除非我们使用`__syncthreads`，否则块内不会同步。因此，`atomicExch`可能会被多次调用。
- en: Since a warp executes in lockstep by default, and blocks of size 32 or less
    are executed with a single warp, `__syncthreads` would be unnecessary.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于warp默认以锁步方式执行，且大小为32或更小的块使用单个warp执行，因此`__syncthreads`是不必要的。
- en: We use a naïve parallel sum within the warp, but otherwise, we are doing as
    many sums with`atomicAdd` as we would do with a serial sum. While CUDA automatically
    parallelizes many of these `atomicAdd` invocations, we could reduce the total
    number of required `atomicAdd` invocations by implementing a work-efficient parallel
    sum.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在warp内使用简单的并行求和，但除此之外，我们使用`atomicAdd`进行的求和与串行求和一样多。虽然CUDA自动并行化了许多这些`atomicAdd`调用，但我们可以通过实现一个工作高效的并行求和来减少所需的`atomicAdd`调用总数。
- en: Definitely `sum_ker`. It's clear that PyCUDA's sum doesn't use the same hardware
    tricks as we do since ours performs better on smaller arrays, but by scaling the
    size to be much larger, the only explanation as to why PyCUDA's version is better
    is that it performs fewer addition operations.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 肯定是 `sum_ker`。很明显，PyCUDA 的求和操作并没有使用我们使用的相同硬件技巧，因为我们的在较小的数组上表现更好，但通过将规模扩大得多，PyCUDA
    版本更好的唯一解释是它执行了更少的多项式运算。
- en: Chapter 12, Where to Go from Here
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章，下一步该去哪里
- en: 'Two examples: DNA analysis and physics simulations.'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个例子：DNA分析以及物理模拟。
- en: 'Two examples: OpenACC, Numba.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个例子：OpenACC，Numba。
- en: TPUs are only used for machine learning operations and lack the components required
    to render graphics.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TPU 仅用于机器学习操作，缺乏渲染图形所需的组件。
- en: Ethernet.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以太网。
