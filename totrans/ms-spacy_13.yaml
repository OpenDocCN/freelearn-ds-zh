- en: 'Chapter 10: Putting Everything Together: Designing Your Chatbot with spaCy'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章：整合一切：使用spaCy设计您的聊天机器人
- en: In this chapter, you will use everything you have learned so far to design a
    chatbot. You will perform entity extraction, intent recognition, and context handling.
    You will use different ways of syntactic and semantic parsing, entity extraction,
    and text classification.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将利用迄今为止所学的一切来设计一个聊天机器人。您将执行实体提取、意图识别和上下文处理。您将使用不同的句法和语义解析方式、实体提取和文本分类方法。
- en: 'First, you''ll explore the dataset we''ll use to collect linguistic information
    about the utterances within it. Then, you''ll perform entity extraction by combining
    the spaCy `Matcher` class. After that, you''ll perform intent recognition with
    two different techniques: a pattern-based method and statistical text classification
    with TensorFlow and Keras. You''ll train a character-level LSTM to classify the
    utterance intents.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您将探索我们将用于收集其中话语的语言信息的数据集。然后，您将通过结合spaCy的`Matcher`类进行实体提取。之后，您将使用两种不同的技术进行意图识别：基于模式的方法和TensorFlow和Keras的统计文本分类。您将训练一个字符级LSTM来分类话语意图。
- en: The final section is a section dedicated to sentence- and dialog-level semantics.
    You'll take a deep dive into semantic subjects such as **anaphora resolution**,
    **grammatical question types**, and **differentiating subjects from objects**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一节是关于句子和对话级语义的章节。您将深入研究诸如**代词消解**、**语法疑问句类型**和**区分主语和宾语**等语义主题。
- en: By the end of this chapter, you'll be ready to design a real chatbot **natural
    language understanding** (**NLU**) pipeline. You will bring together what you
    learned in all previous chapters – linguistically and statistically – by combining
    several spaCy pipeline components such as **NER**, a **dependency parser**, and
    a **POS tagger**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将准备好设计一个真正的聊天机器人**自然语言理解**（**NLU**）管道。您将通过结合之前章节中学到的内容——语言和统计——结合几个spaCy管道组件，如**命名实体识别（NER**）、**依存句法分析器**和**词性标注器**。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Introduction to conversational AI
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对话式人工智能简介
- en: Entity extraction
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实体提取
- en: Intent recognition
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 意图识别
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we''ll be using NumPy, TensorFlow, and scikit-learn along
    with spaCy. You can install these libraries via `pip` using the following commands:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用NumPy、TensorFlow和scikit-learn以及spaCy。您可以通过以下命令使用`pip`安装这些库：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can find the chapter code and data at the book''s GitHub repository: [https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter10](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter10).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书的GitHub仓库中找到本章的代码和数据：[https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter10](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter10)。
- en: Introduction to conversational AI
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对话式人工智能简介
- en: We welcome you to our last and very exciting chapter, where you'll be designing
    a chatbot NLU pipeline with spaCy and TensorFlow. In this chapter, you'll learn
    the NLU techniques for extracting meaning from multiturn chatbot-user interactions.
    By learning and applying these techniques, you'll take a step into **conversational
    AI development**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们欢迎您来到我们最后一章，也是非常激动人心的一章，您将使用spaCy和TensorFlow设计一个聊天机器人NLU管道。在本章中，您将学习从多轮聊天机器人-用户交互中提取意义的方法。通过学习和应用这些技术，您将迈入**对话式人工智能开发**的步伐。
- en: 'Before diving into the technical details, there''s one fundamental question:
    what is a chatbot? Where can we find one? What exactly does conversational AI
    mean?'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入技术细节之前，有一个基本问题：什么是聊天机器人？我们可以在哪里找到它？对话式人工智能究竟是什么意思？
- en: '**Conversational artificial intelligence** (**conversational AI**) is a field
    of machine learning that aims to create technology that enables users to have
    text- or speech-based interactions with machines. Chatbots, virtual assistants,
    and voice assistants are typical conversational AI products.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**对话式人工智能**（**conversational AI**）是机器学习的一个领域，旨在创建能够使用户与机器进行基于文本或语音交互的技术。聊天机器人、虚拟助手和语音助手是典型的对话式人工智能产品。'
- en: A **chatbot** is a software application that is designed to make conversations
    with humans in chat applications. Chatbots are popular in a wide variety of commercial
    areas including HR, marketing and sales, banking, and healthcare, as well as in
    personal, non-commercial areas such as small talk. Many commercial companies,
    such as Sephora (Sephora owns two chatbots – a virtual make-up artist chatbot
    on Facebook messenger platform and a customer service chatbot again on Facebook
    messenger), IKEA (IKEA have a customer service chatbot called Anna), AccuWeather,
    and many more, own customer service and FAQ chatbots.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**聊天机器人**是一种设计用来在聊天应用中与人类进行对话的软件应用。聊天机器人在包括人力资源、市场营销和销售、银行和医疗保健在内的广泛商业领域以及个人、非商业领域（如闲聊）中都很受欢迎。许多商业公司，如Sephora（Sephora拥有两个聊天机器人——一个在Facebook消息平台上的虚拟化妆师聊天机器人和一个在Facebook消息平台上的客户服务聊天机器人）、IKEA（IKEA有一个名为Anna的客户服务聊天机器人）、AccuWeather等，都拥有客户服务和常见问题解答聊天机器人。'
- en: 'Instant messaging services such as Facebook Messenger and Telegram provide
    interfaces to developers for connecting their bots. These platforms provide detailed
    guidelines for developers as well, such as the Facebook Messenger API documentation:
    ([https://developers.facebook.com/docs/messenger-platform/getting-started/quick-start/](https://developers.facebook.com/docs/messenger-platform/getting-started/quick-start/))
    or the Telegram bot API documentation: ([https://core.telegram.org/bots](https://core.telegram.org/bots)).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 即时通讯服务如Facebook Messenger和Telegram为开发者提供了连接其机器人的接口。这些平台还为开发者提供了详细的指南，例如Facebook
    Messenger API文档：([https://developers.facebook.com/docs/messenger-platform/getting-started/quick-start/](https://developers.facebook.com/docs/messenger-platform/getting-started/quick-start/))
    或Telegram机器人API文档：([https://core.telegram.org/bots](https://core.telegram.org/bots))。
- en: A **virtual assistant** is also a software agent that performs some tasks upon
    user request or question. A well-known example is **Amazon Alexa**. Alexa is a
    voice-based virtual assistant and can perform many tasks, including playing music,
    setting alarms, reading audiobooks, playing podcasts, and giving real-time information
    for weather, traffic, sports, and so on. Alexa Home can control connected smart
    home devices and perform a variety of tasks, including switching the lights on
    and off, controlling the garage door, and so on.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**虚拟助手**也是一种软件代理，它会在用户请求或提问时执行一些任务。一个著名的例子是**亚马逊Alexa**。Alexa是一个基于语音的虚拟助手，可以执行许多任务，包括播放音乐、设置闹钟、阅读有声读物、播放播客，以及提供关于天气、交通、体育等方面的实时信息。Alexa
    Home可以控制连接的智能家居设备，并执行各种任务，例如开关灯光、控制车库门等。'
- en: Other well-known examples are Google Assistant and Siri. Siri is integrated
    into a number of Apple products, including iPhone, iPad, iPod, and macOS. On iPhone,
    Siri can make calls, answer calls, and send and receive text messages as well
    as WhatsApp messages. Google Assistant also can perform a wide variety of tasks,
    such as providing real-time flight, weather, and traffic information; sending
    and receiving text messages; setting alarms; providing device battery information;
    checking your email inbox; integrating with smart home devices; and so on. Google
    Assistant is available on Google Maps, Google Search, and standalone Android and
    iOS applications.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 其他知名的例子包括Google Assistant和Siri。Siri集成在苹果公司的多个产品中，包括iPhone、iPad、iPod和macOS。在iPhone上，Siri可以进行电话拨打、接听电话，以及发送和接收短信以及WhatsApp消息。Google
    Assistant也可以执行各种任务，例如提供实时航班、天气和交通信息；发送和接收短信；设置闹钟；提供设备电池信息；检查您的电子邮件收件箱；与智能家居设备集成等。Google
    Assistant可在Google Maps、Google Search以及独立的Android和iOS应用程序中使用。
- en: 'Here is a list of the most popular and well-known virtual assistants to give
    you some more ideas of what''s out there:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些最受欢迎和知名的虚拟助手列表，以给您更多关于市场上有哪些产品的想法：
- en: Amazon Alexa
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon Alexa
- en: AllGenie from Alibaba Group
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阿里巴巴集团的AllGenie
- en: Bixby from Samsung
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三星公司的Bixby
- en: Celia from Huawei
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 华为公司的Celia
- en: Duer from Baidu
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 百度公司的Duer
- en: Google Assistant
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Assistant
- en: Microsoft Cortana
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微软公司的Cortana
- en: Siri from Apple
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 苹果公司的Siri
- en: Xiaowei from Tencent
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 腾讯公司的Xiaowei
- en: All of these virtual assistants are voice-based and are usually invoked with
    a **wake word**. A wake word is a special word or phrase that is used to activate
    a voice assistant. Some examples are *Hey Alexa*, *Hey Google*, and *Hey Siri*,
    which are the wake words of Amazon Alexa, Google Assistant, and Siri, respectively.
    If you want to know more about the development details of these products, please
    refer to the *References* section of this chapter.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些虚拟助手都是基于语音的，通常通过一个**唤醒词**来激活。唤醒词是一个特殊的词或短语，用于激活语音助手。例如，“嘿，Alexa”、“嘿，Google”和“嘿，Siri”，分别是亚马逊Alexa、谷歌助手和Siri的唤醒词。如果您想了解更多关于这些产品的开发细节，请参阅本章的**参考文献**部分。
- en: Now, we come to the technical details. What are the NLP components of these
    products? Let's look at these NLP components in detail.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来探讨一下技术细节。这些产品的NLP组件是什么？让我们详细看看这些NLP组件。
- en: NLP components of conversational AI products
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对话式人工智能产品的NLP组件
- en: 'A typical voice-based conversational AI product consists of the following components:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的基于语音的对话式人工智能产品包括以下组件：
- en: '**Speech-to-text component**: Converts user speech into text. Input to this
    component is a WAV/mp3 file and the output is a text file containing the user
    utterance as a text.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音转文本组件**：将用户语音转换为文本。该组件的输入是一个WAV/mp3文件，输出是一个包含用户话语的文本文件。'
- en: '**Conversational NLU component**: This component performs intent recognition
    and entity extraction on the user utterance text. The output is the user intent
    and a list of entities. Resolving references in the current utterance to the previous
    utterances is done in this component (please refer to the *Anaphora resolution*
    section).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对话式NLU组件**：这个组件对用户话语文本执行意图识别和实体提取。输出是用户意图和实体列表。在当前话语中解决对先前话语的引用是在这个组件中完成的（请参阅**指代消解**部分）。'
- en: '**Dialog manager**: Keeps the conversation memory to make a meaningful and
    coherent chat. You can think of this component as the dialog memory as this component
    usually holds a **dialog state**. The dialog state is the state of the conversation:
    the entities that have appeared so far, the intents that have appeared so far,
    and so on. Input to this component is the previous dialog state and the current
    user parsed with intent and entities. The output of this component is the new
    dialog state.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对话管理器**：保持对话记忆，以进行有意义的连贯对话。您可以把这个组件看作是对话记忆，因为这个组件通常保存一个**对话状态**。对话状态是对话的状态：到目前为止出现的实体、到目前为止出现的意图等等。该组件的输入是先前的对话状态和当前用户解析的带有意图和实体的内容。该组件的输出是新的对话状态。'
- en: '**Answer generator**: Given all the inputs from the previous stages, generates
    the system''s answer to the user utterance.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回答生成器**：根据前几个阶段的全部输入，生成系统对用户话语的响应。'
- en: '**Text-to-speech**: This component generates a speech file (WAV or mp3) from
    the system''s answer.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本转语音**：这个组件将系统的答案生成语音文件（WAV或mp3）。'
- en: Each of the components is trained and evaluated separately. For example, the
    speech-to-text component is trained on an annotated speech corpus (training is
    done on speech files and the corresponding transcriptions). The NLU component
    is trained on intent and an entity labeled corpus (similar to the datasets we
    used in *Chapters 6, 7, 8,* and *9*). In this chapter, we'll focus on the NLU
    component tasks. For text-based products, the first and last components are not
    necessary and are replaced with email or chat client integration.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每个组件都是单独训练和评估的。例如，语音转文本组件是在标注的语音语料库上训练的（训练是在语音文件和相应的转录上进行的）。NLU组件是在意图和实体标注的语料库上训练的（类似于我们在第6、7、8和9章中使用的数据集）。在本章中，我们将重点关注NLU组件的任务。对于基于文本的产品，第一个和最后一个组件是不必要的，通常被电子邮件或聊天客户端集成所取代。
- en: There's another paradigm that is called **end-to-end spoken language understanding**
    (**SLU**). In SLU architectures, the system is trained end to end, which means
    that the input to the system is a speech file and the output is the system response.
    Each approach has pros and cons; you can refer to the *References* section for
    more material.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种被称为**端到端语音理解**（**SLU**）的范式。在SLU架构中，系统是端到端训练的，这意味着输入到系统的是语音文件，输出是系统的响应。每种方法都有其优缺点；您可以参考**参考文献**部分获取更多资料。
- en: 'As the author of this book, I''m happy to present this chapter to you with
    my domain experience. I''ve been working in the conversational AI area for quite
    some time and tackle challenges of language and speech processing every day for
    our product. Me and my colleagues are building the world''s first driver digital
    assistant, Chris (*Tips & Tricks: How to talk to Chris – basic voice commands*,
    [https://www.youtube.com/watch?v=Qwnjszu3exY](https://www.youtube.com/watch?v=Qwnjszu3exY)).
    Chris can make calls, answer incoming calls, read and write WhatsApp and text
    messages, play music, navigate, and make small talk. Here is Chris:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本书的作者，我很高兴能结合我的领域经验向您介绍这一章节。我在对话式人工智能领域工作了一段时间，并且每天都在为我们的产品解决语言和语音处理方面的挑战。我和我的同事们正在构建世界上第一个驾驶员数字助手Chris（*技巧与窍门：如何与Chris交流
    – 基本语音命令*，[https://www.youtube.com/watch?v=Qwnjszu3exY](https://www.youtube.com/watch?v=Qwnjszu3exY)）。Chris可以打电话、接听来电、阅读和发送WhatsApp和短信、播放音乐、导航和闲聊。以下是Chris：
- en: '![Figure 10.1 – In-car voice assistant Chris (this is the product that the
    author is working on](img/Figure_10_1.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1 – 车载语音助手Chris（这是作者正在开发的产品](img/Figure_10_1.jpg)'
- en: Figure 10.1 – In-car voice assistant Chris (this is the product that the author
    is working on)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – 车载语音助手Chris（这是作者正在开发的产品）
- en: As we see from the preceding examples, conversational AI has become a hot topic
    recently. As an NLP professional, it's quite likely that you'll work for a conversational
    product or work in a related area such as speech recognition, text-to-speech,
    or question answering. Techniques presented in this chapter such as intent recognition,
    entity extraction, and anaphora resolution are applicable to a wide set of NLU
    problems as well. Let's dive into the technical sections. We'll start by exploring
    the dataset that we'll use throughout this chapter.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的例子中我们可以看到，对话式人工智能最近已经成为一个热门话题。作为一名自然语言处理专业人士，你很可能会在一个对话产品或相关领域工作，比如语音识别、文本转语音或问答。本章中介绍的技术，如意图识别、实体提取和代词消解，也适用于广泛的NLU问题。让我们深入技术部分。我们将从探索本章中我们将使用的整个数据集开始。
- en: Getting to know the dataset
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解数据集
- en: 'In *Chapters 6*, *7*, *8*, and *9*, we worked on well-known real-world datasets
    for text classification and entity extraction purposes. In these chapters, we
    always explored our dataset as the very first task. The main point of data exploration
    is to understand the nature of the dataset text in order to develop strategies
    in our algorithms that can tackle this dataset. If we recall from [*Chapter 6*](B16570_06_Final_JM_ePub.xhtml#_idTextAnchor103)*,
    Putting Everything Together: Semantic Parsing with spaCy*, the following are the
    main points we should keep an eye on during our exploration:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第6章*、*第7章*、*第8章*和*第9章*中，我们针对文本分类和实体提取目的使用了著名的真实世界数据集。在这些章节中，我们总是将数据集探索作为首要任务。数据探索的主要目的是为了了解数据集文本的性质，以便在我们的算法中制定应对该数据集的策略。如果我们回顾一下[*第6章*](B16570_06_Final_JM_ePub.xhtml#_idTextAnchor103)*，使用spaCy进行语义解析：整合一切，以下是我们探索过程中应该关注的主要点：
- en: What kind of utterances there are? Are utterances short text or full sentences
    or long paragraphs or documents? What is the average utterance length?
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有哪些类型的语句？是简短的文本、完整的句子、长段落还是文档？语句的平均长度是多少？
- en: What sort of entities does the corpus include? Person names, organization names,
    geographical locations, street names? Which ones do we want to extract?
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语料库包含哪些实体？人名、组织名、地理位置、街道名？我们想要提取哪些？
- en: How is punctuation used? Is the text correctly punctuated or is no punctuation
    used at all?
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标点符号是如何使用的？文本是否正确使用了标点，或者完全没有使用标点？
- en: How are the grammatical rules followed? Is capitalization correct, and did the
    users follow the grammatical rules? Are there misspelled words?
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语法规则是如何遵循的？大写是否正确，用户是否遵循了语法规则？是否有拼写错误？
- en: The previous datasets we used consisted of `(text, class_label)` pairs to be
    used in text classification tasks or `(text, list_of_entities)` pairs to be used
    in entity extraction tasks. In this chapter, we'll tackle a much more complicated
    task, chatbot design. Hence, the dataset will be more structured and more complicated.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前使用的语料库由`(text, class_label)`对组成，用于文本分类任务，或者由`(text, list_of_entities)`对组成，用于实体提取任务。在本章中，我们将处理一个更复杂的任务，即聊天机器人设计。因此，数据集将更加结构化和复杂。
- en: 'Chatbot design datasets are usually in JSON format to maintain the dataset
    structure. Here, structure means the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人设计数据集通常以JSON格式存储，以保持数据集结构。在这里，结构意味着以下内容：
- en: Keeping the order of user and system utterances
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持用户和系统话语的顺序
- en: Marking slots of the user utterances
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记用户话语的槽位
- en: Labeling the intent of the user utterances
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标注用户话语的意图
- en: Throughout this chapter, we'll use Google Research's **The Schema-Guided Dialogue**
    dataset (**SGD**) ([https://github.com/google-research-datasets/dstc8-schema-guided-dialogue](https://github.com/google-research-datasets/dstc8-schema-guided-dialogue)).
    This dataset consists of annotated user-virtual assistant interactions. The original
    dataset contains over 20,000 dialog segments in several areas, including restaurant
    reservations, movie reservations, weather queries, and travel ticket booking.
    Dialogs include utterances of user and virtual assistant turn by turn. In this
    chapter, we won't use all of this massive dataset; instead, we'll use a subset
    about restaurant reservations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用谷歌研究团队的**《Schema-Guided Dialogue》**数据集（**SGD**）([https://github.com/google-research-datasets/dstc8-schema-guided-dialogue](https://github.com/google-research-datasets/dstc8-schema-guided-dialogue))。该数据集包含标注的用户与虚拟助手交互。原始数据集包含超过20,000个对话片段，涉及多个领域，包括餐厅预订、电影预订、天气查询和旅行票务预订。对话包括用户和虚拟助手的轮流话语。在本章中，我们不会使用这个庞大的数据集的全部；相反，我们将使用关于餐厅预订的子集。
- en: 'Let''s get started with downloading the dataset. You can download the dataset
    from the book''s GitHub repository at [https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/data/restaurants.json](https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/data/restaurants.json).
    Alternatively, you can write the following code:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始下载数据集。你可以从本书的GitHub仓库下载数据集：[https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/data/restaurants.json](https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/data/restaurants.json)。或者，你可以编写以下代码：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you open the file with a text editor and look at the first few lines, you''ll
    see the following:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你用文本编辑器打开文件并查看前几行，你会看到以下内容：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: First of all, the dataset consists of dialog segments and each dialog segment
    has a `dialogue_id` instance. Each dialog segment is an ordered list of turns
    and each turn belongs to the user or to the system. A `turns` field is a list
    of the user/system turns. Each element of the `turns` list is a turn. One turn
    consists of a speaker (user or system), the speaker's utterance, a list of slots,
    and an intent for the user utterances.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，数据集由对话片段组成，每个对话片段都有一个`dialogue_id`实例。每个对话片段是一个有序的轮流列表，每个轮流属于用户或系统。`turns`字段是一个用户/系统轮流的列表。`turns`列表的每个元素都是一个轮流。一个轮流包括说话者（用户或系统）、说话者的话语、槽位列表和用户话语的意图。
- en: 'Here are some example user utterances from the dataset:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是从数据集中的一些示例用户话语：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we see from these example utterances, capital letters and punctuation are
    used in the user utterances. Users can make typos, such as the word `afforadable`
    in the second sentence. There are some grammatical errors as well, such as the
    wrong usage of a capital letter in the word `Thanks` of the fifth sentence. Another
    capitalization mistake occurs in the sixth sentence, where the pronoun *I* is
    written as `i` twice.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些示例话语中我们可以看到，用户话语中使用了大写字母和标点符号。用户可能会犯拼写错误，例如第二句话中的单词`afforadable`。还有一些语法错误，例如第五句话中`Thanks`一词首字母大写的错误。另一个大写错误发生在第六句话中，其中代词*I*被错误地写成了`i`两次。
- en: Also, one utterance can contain multiple sentences. The first utterance starts
    with a greeting sentence and the last two sentences start with an affirmative
    or negative answer sentence each. The fourth sentence also starts with a `Yes`,
    but not as a standalone sentence; instead it's separated from the second sentence
    with a comma.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一个话语可以包含多个句子。第一个话语以问候句开始，最后两个句子各自以肯定或否定回答句开始。第四句话也以`Yes`开始，但不是作为一个独立的句子；相反，它与第二个句子用逗号隔开。
- en: Intent recognition for multiple sentence utterances is a point we need to pay
    attention to in general – these types of utterances can contain multiple intents.
    Also, answer generation for multi-sentence utterances is a bit tricky; sometimes
    we need to generate only one answer (such as for the second sentence in the preceding
    code) or sometimes we need to generate an answer per each user sentence (such
    as for the last sentence in the preceding code).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多句话语的意图识别，这是我们通常需要注意的一个点——这类话语可以包含多个意图。此外，对于多句话语的答案生成也有些棘手；有时我们只需要生成一个答案（例如，对于前面代码中的第二句话）或有时我们需要为每个用户句子生成一个答案（例如，对于前面代码中的最后一句话）。
- en: 'This is a dataset for restaurant reservations, so naturally it includes some
    slots in user utterances such as the location, cuisine, time, date, number of
    people, and so on. Our dataset includes the following slots:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个餐厅预订数据集，因此它自然包括用户话语中的某些槽位，如位置、菜系、时间、日期、人数等。我们的数据集包括以下槽位：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here are some example sentences with the preceding slot types and their values:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是包含先前槽位类型及其值的示例句子：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we come to the class labels for the intent recognition and the distribution
    of these class labels. Here''s the class labels distribution:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来讨论意图识别的类别标签及其分布。以下是类别标签的分布：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`NONE` is a special class label for utterances that indicate the end of a conversation
    or just saying thank you. This class of utterances is not related to restaurant
    reservation in general. Utterances that intend to list restaurants and get some
    information are labeled with the class label `FindRestaurants`, and utterances
    that include the intent to make a booking are labeled with `ReserveRestaurants`.
    Let''s see some example utterances of each class:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`NONE`是表示对话结束或只是说谢谢的话语的特殊类别标签。这类话语通常与餐厅预订无关。意图为列出餐厅并获取信息的话语被标记为`FindRestaurants`类别标签，而包含预订意图的话语被标记为`ReserveRestaurants`。让我们看看每个类别的示例话语：'
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We notice that the follow-up sentences, such as utterances 6, 8, and 9, are
    marked with the intents `FindRestaurants` and `ReserveRestaurant`. These utterances
    don't contain the intents of finding/reserving directly, but they continue the
    dialog about finding/reserving a restaurant and still make queries about the restaurant/reservation.
    Hence, although there are no explicit actions of finding/reserving stated in these
    utterances, still the intents are to find/reserve a restaurant.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到后续句子，例如第6、8和9句话，被标记为`FindRestaurants`和`ReserveRestaurant`意图。这些句子不直接包含寻找/预订的意图，但它们继续了关于寻找/预订餐厅的对话，并且仍然对餐厅/预订进行查询。因此，尽管这些句子中没有明确指出寻找/预订的动作，但意图仍然是寻找/预订餐厅。
- en: That's it – we collected enough insights about our dataset using the preliminary
    work of this section. With these insights, we're ready to build our NLU pipeline.
    We'll start with extracting the user utterance entities.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样——我们使用本节初步工作收集了关于我们数据集的足够见解。有了这些见解，我们准备构建我们的NLU管道。我们将从提取用户话语实体开始。
- en: Entity extraction
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实体提取
- en: 'In this section, we''ll implement the first step of our chatbot NLU pipeline
    and extract entities from the dataset utterances. The following are the entities
    marked in our dataset:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现我们聊天机器人NLU管道的第一步，并从数据集话语中提取实体。以下是我们数据集中标记的实体：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To extract the entities, we'll use the spaCy NER model and the spaCy `Matcher`
    class. Let's get started by extracting the `city` entities.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取实体，我们将使用spaCy NER模型和spaCy的`Matcher`类。让我们先从提取`city`实体开始。
- en: Extracting city entities
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取城市实体
- en: 'We''ll first extract the `city` entities. We''ll get started by recalling some
    information about the spaCy NER model and entity labels from [*Chapter 3*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)*,
    Linguistic Features*, and [*Chapter 6*](B16570_06_Final_JM_ePub.xhtml#_idTextAnchor103)*,
    Putting Everything Together: Semantic Parsing with spaCy*:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先提取`city`实体。我们将从回忆一些关于spaCy NER模型和实体标签的信息开始，这些信息来自[*第3章*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)*，语言特征*和[*第6章*](B16570_06_Final_JM_ePub.xhtml#_idTextAnchor103)*，使用spaCy进行语义解析：
- en: 'First, we recall that the spaCy named entity label for cities and countries
    is `GPE`. Let''s ask spaCy to explain what `GPE` label corresponds to once again:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们回忆一下spaCy中城市和国家的命名实体标签是`GPE`。让我们再次询问spaCy，`GPE`标签对应的是什么：
- en: '[PRE9]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Secondly, we also recall that we can access entities of a `Doc` object via
    the `ents` property. We can find all entities in an utterance that are labeled
    by the spaCy NER model as follows:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，我们还回忆起我们可以通过`ents`属性访问`Doc`对象的实体。我们可以找到以下标记为spaCy NER模型的实体：
- en: '[PRE10]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In this code segment, we listed all named entities of this utterance by calling
    `doc.ents`. Then, we examined the entity labels by calling `ent.label_`. Examining
    the output, we see that this utterance contains five entities – one cardinal number
    entity (`2`), one `TIME` entity (`11:30 am`), one `PRODUCT` entity (`Bird`, which
    is not an ideal label for a restaurant), one `CITY` entity (`Palo Alto`), and
    one `DATE` entity (`today`). The `GPE` type entity is what we're looking for;
    `Palo Alto` is a city in the US and hence is labeled by the spaCy NER model as
    `GPE`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码段中，我们通过调用`doc.ents`列出了这个话语中所有的命名实体。然后，我们通过调用`ent.label_`检查了实体标签。检查输出，我们看到这个话语包含五个实体
    - 一个序数实体（`2`），一个`TIME`实体（`11:30 am`），一个`PRODUCT`实体（`Bird`，这不是餐厅的理想标签），一个`CITY`实体（`Palo
    Alto`），和一个`DATE`实体（`today`）。`GPE`类型的实体是我们想要的；`Palo Alto`是美国的一个城市，因此被spaCy NER模型标记为`GPE`。
- en: The script at [https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/extract_city_ents.py](https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/extract_city_ents.py)
    in the book's GitHub outputs all the utterances that include a city entity together
    with the city entities. From the output of this script, we can see that the spaCy
    NER model performs very well on this corpus for `GPE` entities. We don't need
    to train the spaCy NER model with our custom data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 书中[https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/extract_city_ents.py](https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/extract_city_ents.py)的脚本在GitHub上输出了包含城市实体的所有话语。从该脚本的输出中，我们可以看到spaCy
    NER模型在这个语料库上的`GPE`实体表现非常好。我们不需要用我们的自定义数据训练spaCy NER模型。
- en: We extracted city entities, and our chatbot knows in which city to look for
    a restaurant. Now, we'll extract dates and times to allow our chatbot to make
    a real reservation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提取了城市实体，我们的聊天机器人知道在哪个城市寻找餐厅。现在，我们将提取日期和时间，以便我们的聊天机器人能够进行真正的预订。
- en: Extracting date and time entities
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取日期和时间实体
- en: Extracting `DATE` and `TIME` entities is similar to extracting `CITY` entities,
    which we saw in the previous section. We'll again go over the corpus utterances
    and see how successful the spaCy NER model is at extracting `DATE` and `TIME`
    entities from our corpus.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 提取`DATE`和`TIME`实体与提取`CITY`实体类似，我们在上一节中看到了。我们再次回顾语料库话语，看看spaCy NER模型在从我们的语料库中提取`DATE`和`TIME`实体方面的成功程度。
- en: 'Let''s see some example utterances from the corpus:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看语料库中的几个示例话语：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the following code, we''ll extract the entities of these example utterances:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们将提取这些示例话语的实体：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Looks good! The output looks quite successful:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错！输出相当成功：
- en: The time entities `11:30 am` and `1:30 pm` of the first and second sentences
    are extracted successfully.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一句和第二句中的时间实体`11:30 am`和`1:30 pm`被成功提取。
- en: 'The `DATE` entities `next friday` and `next Friday` of the third and fourth
    sentences are extracted as well. Notice the first entity includes a typo: `friday`
    should be written as *Friday* – still, the spaCy NER model successfully extracted
    this entity.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三句和第四句中的`DATE`实体`next friday`和`next Friday`也被提取出来。请注意，第一个实体包含一个拼写错误：`friday`应该写成*Friday*
    - 尽管如此，spaCy NER模型仍然成功提取了这个实体。
- en: 'The fifth sentence included both a `DATE` entity and a `TIME` entity. We can
    break the `DATE` entity `Monday next week` into two parts: `Monday` – a weekday
    and `next week` – a relative date (the exact date depends on the date of the utterance).
    This entity consists of two noun phrases: `Monday` (noun) and `next week` (adjective
    noun). spaCy can handle such multiword entities. The time entity, `half past 12`,
    of this utterance is also a multiword entity. This entity consists of a noun (`half`),
    a preposition (`past`), and a number (`12`).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第五句包含了`DATE`实体和`TIME`实体。我们可以将`DATE`实体`Monday next week`分为两部分：`Monday` - 一个星期几和`next
    week` - 一个相对日期（确切的日期取决于话语的日期）。这个实体由两个名词短语组成：`Monday`（名词）和`next week`（形容词名词）。spaCy可以处理这样的多词实体。这个话语的时间实体`half
    past 12`也是一个多词实体。这个实体由一个名词（`half`），一个介词（`past`）和一个数字（`12`）组成。
- en: 'The same goes for the sixth utterance''s multiword `TIME` entity, `A quarter
    past 5`. Here is the dependency tree of this entity:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于第六个话语的多词`TIME`实体`A quarter past 5`也是如此。以下是这个实体的依存句法树：
- en: '![Figure 10.2 – Dependency tree of the time entity "A quarter past 5"'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.2 – 时间实体“5点过一刻”的依存树'
- en: '](img/Figure_10_2.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10_2.jpg)'
- en: Figure 10.2 – Dependency tree of the time entity "A quarter past 5"
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 – 时间实体“5点过一刻”的依存树
- en: 'The preceding examples look quite good indeed, but how about the following
    utterances:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的例子确实看起来相当不错，但以下这些表述如何：
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Oops-a-daisy – looks like we have some `day`, as date entities incorrectly.
    What can we do here?
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀呀 – 看起来我们有一些`day`，因为日期实体错误地被识别了。我们在这里能做什么？
- en: 'Fortunately, these false matches don''t form a pattern such as `a good day`
    and `a wonderful day` of the third and fourth sentence are not labeled as entities.
    Only the word sequences `a great day` and `a nice day` are labeled as entities.
    Then, we can just filter the spaCy NER results with the following two patterns:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这些错误匹配并没有形成像第三和第四句中的“a good day”和“a wonderful day”这样的模式。只有单词序列“a great
    day”和“a nice day”被标记为实体。然后，我们可以用以下两种模式过滤spaCy NER的结果：
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The preceding code block performs the following steps:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块执行以下步骤：
- en: First, we defined a list of phrases that we don't want to come up as `DATE`
    entities.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们定义了一个我们不想被识别为`DATE`实体的短语列表。
- en: We extracted the `DATE` entities of the Doc object on the third line by iterating
    over all entities of `doc` and picking the entities whose labels were `DATE`.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过遍历`doc`中的所有实体，并选择标签为`DATE`的实体，在第3行提取了Doc对象的`DATE`实体。
- en: In the next line, we filtered the entities that didn't appear in the `wrong_matches`
    list.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一行，我们过滤了不在`wrong_matches`列表中的实体。
- en: We printed the result. As expected, the final result of the `date` entity is
    an empty list.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打印了结果。正如预期的那样，`date`实体的最终结果是空列表。
- en: Great, we have extracted `DATE` and `TIME` entities along with `CITY` entities.
    For all the three entity types, we used the spaCy NER model directly, because
    spaCy NER recognizes date, time, and location entities. How about `phone_number`
    entities? SpaCy NER doesn't include such a label at all. So, we'll use some `Matcher`
    class tricks to handle this entity type. Let's extract the phone numbers.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，我们已经提取了`DATE`、`TIME`和`CITY`实体。对于所有三种实体类型，我们直接使用了spaCy NER模型，因为spaCy NER可以识别日期、时间和地点实体。那么`phone_number`实体呢？spaCy
    NER根本不包含这样的标签。所以，我们将使用一些`Matcher`类的技巧来处理这种实体类型。让我们提取电话号码。
- en: Extracting phone numbers
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取电话号码
- en: 'We had some `Matcher` class practice on entities that include numbers in [*Chapter
    4*](B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069)*, Rule-Based Matching*. We
    can also recall from [*Chapter 4*](B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069)*,
    Rule-Based Matching*, that matching number type entities can be indeed quite tricky;
    extracting telephone numbers especially requires attention. Phone numbers can
    come in different formats, with dashes (212-44-44), area codes ((312) 790 12 31),
    country and area codes (+49 30 456 222), and the number of digits differing from
    country to country. As a result, we usually examine the following points:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第4章*](B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069)*，基于规则的匹配*中进行了`Matcher`类在包含数字的实体上的实践。我们还可以从[*第4章*](B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069)*，基于规则的匹配*中回忆起，匹配数字类型实体确实可能相当棘手；特别是提取电话号码需要特别注意。电话号码可以以不同的格式出现，包括带连字符的（212-44-44）、区号（(312)
    790 12 31）、国家及区号（+49 30 456 222），以及不同国家的数字位数。因此，我们通常检查以下几点：
- en: How many country formats are the corpus phone number entities written in?
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语料库中的电话号码实体是以多少种国家格式编写的？
- en: How are the digit blocks separated – with a dash, or whitespace, or both?
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字块是如何分隔的 – 是用连字符、空格，还是两者都用？
- en: Is there an area code block in some phone numbers?
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些电话号码中是否有区号块？
- en: Is there a country code block in some phone numbers?
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些电话号码中是否有国家代码块？
- en: Are the country code blocks preceded with a + or 00, or are both formats used?
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 国家代码块是否以+或00开头，或者两种格式都使用？
- en: 'Let''s examine some of our phone number entities, then:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一些我们的电话号码实体，然后：
- en: '[PRE15]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: All the phone-type entities occur in system utterances. The chatbot fetches
    phone numbers of restaurants and provides them to the users. The chatbot formed
    phone number entities by placing a dash between the digit blocks. Also, all the
    phone numbers are in USA phone number format. Hence the phone number format is
    uniform and is of the form `ddd-ddd-dddd`. This is very good for defining a Matcher
    pattern. We can define only one pattern and it matches all the phone number entities.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 所有电话类型的实体都出现在系统对话中。聊天机器人检索餐厅的电话号码并将其提供给用户。聊天机器人通过在数字块之间放置破折号来形成电话号码实体。此外，所有电话号码都采用美国电话号码格式。因此，电话号码格式是统一的，形式为
    `ddd-ddd-dddd`。这对于定义匹配模式非常有用。我们只需定义一个模式，就可以匹配所有电话号码实体。
- en: 'Let''s first see how an example phone number tokenizes:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看一个示例电话号码是如何进行分词的：
- en: '[PRE16]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Each digit block is tokenized as one token and each dash character is tokenized
    as one token as well. Hence, in our Matcher pattern, we''ll look for a sequence
    of five tokens: a three-digit number, a dash, a three-digit number again, a dash
    again, and finally a four-digit number. Then, our Matcher pattern should look
    like this:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数字块都被分词为一个标记，每个破折号字符也被分词为一个标记。因此，在我们的匹配器模式中，我们将寻找一个由五个标记组成的序列：一个三位数，一个破折号，再次是一个三位数，再次是一个破折号，最后是一个四位数。然后，我们的匹配器模式应该看起来像这样：
- en: '[PRE17]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If you recall from [*Chapter 4*](B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069)*,
    Rule-Based Matching*, the `SHAPE` attribute refers to the token shape. The token
    shape represents the shape of the characters: `d` means a digit, `X` means a capital
    character, and `x` means a lowercase character. Hence `{"SHAPE": "ddd"}` means
    a token that consists of three digits. This pattern will match five tokens of
    the form `ddd-ddd-dddd`. Let''s try our brand-new pattern on a corpus utterance:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '如果你还记得[第4章](B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069)中的“基于规则的匹配”，`SHAPE`
    属性指的是标记形状。标记形状表示字符的形状：`d` 表示数字，`X` 表示大写字母，而 `x` 表示小写字母。因此，`{"SHAPE": "ddd"}` 表示由三个数字组成的标记。这个模式将匹配形式为
    `ddd-ddd-dddd` 的五个标记。让我们用我们的全新模式对一个语料库对话进行尝试：'
- en: '[PRE18]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Voila! Our new pattern matched a phone number type entity as expected! Now,
    we'll deal with the cuisine type so that our chatbot can make a reservation. Let's
    see how to extract the cuisine type.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！我们的新模式如预期地匹配了一个电话号码类型实体！现在，我们将处理菜系类型，以便我们的聊天机器人可以预订。让我们看看如何提取菜系类型。
- en: Extracting cuisine types
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取菜系类型
- en: 'Extracting cuisine types is much easier than extracting a number of people
    or phone types; indeed, it''s similar to extracting city entities. We can use
    a spaCy NER label directly for cuisine types – `NORP`. The `NORP` entity label
    refers to ethnic or political groups:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 提取菜系类型比提取人数或电话类型要容易得多；实际上，它类似于提取城市实体。我们可以直接使用 spaCy NER 标签来提取菜系类型 – `NORP`。`NORP`
    实体标签指的是民族或政治团体：
- en: '[PRE19]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Fortunately, cuisine names in our corpus coincide with nationalities. So, cuisine
    names are labeled as NORP by spaCy's NER.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们语料库中的菜名与国籍相吻合。因此，菜名被 spaCy 的 NER 标记为 `NORP`。
- en: 'First, let''s have a look at some example utterances:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看一些示例对话：
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s extract the entities of these utterances and examine how spaCy''s NER
    labels cuisine types as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们提取这些对话的实体，并检查 spaCy 的 NER 标签如何将菜系类型标记如下：
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we are able to extract the city, date and time, number of people, and
    cuisine entities from user utterances. The result of the named entity extraction
    module we built here carries all the information the chatbot needs to provide
    to the reservation system. Here''s an example utterance annotated with extracted
    entities:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们能够从用户对话中提取城市、日期和时间、人数和菜系实体。我们构建的命名实体提取模块的结果包含了聊天机器人需要提供给预订系统的所有信息。以下是一个带有提取实体的示例对话：
- en: '[PRE22]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Here, we completed the first part of our semantic parsing, extracting entities.
    A full semantic parse needs an intent too. Now, we'll move on to the next section
    and do intent recognition with TensorFlow and Keras.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们完成了语义解析的第一部分，即提取实体。完整的语义解析还需要一个意图。现在，我们将进入下一部分，使用 TensorFlow 和 Keras 进行意图识别。
- en: Intent recognition
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 意图识别
- en: '**Intent recognition** (also called **intent classification**) is the task
    of classifying user utterances with predefined labels (intents). Intent classification
    is basically text classification. Intent classification is a well-known and common
    NLP task. GitHub and Kaggle host many intent classification datasets (please refer
    to the *References* section for the names of some example datasets).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**意图识别**（也称为**意图分类**）是将预定义标签（意图）分类到用户表述的任务。意图分类基本上是文本分类。意图分类是一个已知且常见的NLP任务。GitHub和Kaggle托管了许多意图分类数据集（请参阅*参考文献*部分以获取一些示例数据集的名称）。'
- en: 'In real-world chatbot applications, we first determine the domain our chatbot
    has to function in, such as finance and banking, healthcare, marketing, and so
    on. Then we perform the following loop of actions:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的聊天机器人应用中，我们首先确定聊天机器人必须运行的领域，例如金融和银行、医疗保健、市场营销等。然后我们执行以下循环动作：
- en: We determine a set of intents we want to support and prepare a labeled dataset
    of `(utterance, label)` pairs. We train our intent classifier on this dataset.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们确定了一组我们想要支持的意图，并准备了一个带有`(表述，标签)`对的标记数据集。我们在该数据集上训练我们的意图分类器。
- en: Next, we deploy our chatbot to the users and gather real user data.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将我们的聊天机器人部署给用户，并收集真实用户数据。
- en: Then we examine how our chatbot performed on real user data. At this stage,
    usually, we spot some new intents and some utterances our chatbot failed to recognize.
    We extend our set of intents with the new intents, add the unrecognized utterances
    to our training set, and retrain our intent classifier.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们检查我们的聊天机器人在真实用户数据上的表现。在这个阶段，通常我们会发现一些新的意图和一些聊天机器人未能识别的表述。我们将新的意图扩展到我们的意图集合中，将未识别的表述添加到我们的训练集中，并重新训练我们的意图分类器。
- en: We go to *step 2* and perform *steps 2-3* until chatbot NLU quality reaches
    a good level of accuracy (> 0.95)
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们进入*步骤2*并执行*步骤2-3*，直到聊天机器人NLU的质量达到良好的准确度水平（> 0.95）。
- en: Our dataset is a real-world dataset; it contains typos and grammatical mistakes.
    While designing our intent classifiers – especially while doing pattern-based
    classification – we need to be robust to such mistakes.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的语料库是一个真实世界的语料库；它包含拼写错误和语法错误。在设计我们的意图分类器时——尤其是在进行基于模式的分类时——我们需要对这样的错误有足够的鲁棒性。
- en: 'We''ll do the intent recognition in two steps: pattern-based text classification
    and statistical text classification. We saw how to do statistical text classification
    with TensorFlow and Keras in [*Chapter 8*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)*,
    Text Classification with spaCy*. In this section, we''ll work with Tensorflow
    and Keras again. Before that, we''ll see how to design a pattern-based text classifier.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分两步进行意图识别：基于模式的文本分类和统计文本分类。我们在[*第8章*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)*，使用spaCy进行文本分类*中看到了如何使用TensorFlow和Keras进行统计文本分类。在本节中，我们将再次使用TensorFlow和Keras。在此之前，我们将了解如何设计基于模式的文本分类器。
- en: Pattern-based text classification
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于模式的文本分类
- en: '**Pattern-based classification** means classifying text by matching a predefined
    list of patterns to the text. We compare a precompiled list of patterns against
    the utterances and check whether there''s a match.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于模式的分类**意味着通过将预定义的模式列表与文本匹配来对文本进行分类。我们将预编译的模式列表与表述进行比较，并检查是否存在匹配。'
- en: An immediate example is **spam classification**. If an email contains one of
    the patterns, such as *you won a lottery* and *I'm a Nigerian prince*, then this
    email should be classified as spam. Pattern-based classifiers are combined with
    **statistical classifiers** to boost the overall system accuracy.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一个直接的例子是**垃圾邮件分类**。如果一个电子邮件包含以下模式之一，例如*你中了彩票*和*我是尼日利亚王子*，那么这封电子邮件应该被分类为垃圾邮件。基于模式的分类器与**统计分类器**结合使用，以提高整个系统的准确度。
- en: Contrary to statistical classifiers, pattern-based classifiers are easy to build.
    We don't need to put any effort into training a TensorFlow model at all. We will
    compile a list of patterns from our corpus and feed them to Matcher. Then, Matcher
    can look for pattern matches in utterances.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 与统计分类器不同，基于模式的分类器很容易构建。我们根本不需要在训练TensorFlow模型上花费任何努力。我们将从我们的语料库中编译一个模式列表，并将其提供给Matcher。然后，Matcher可以在表述中查找模式匹配。
- en: 'To build a pattern-based classifier, we first need to collect some patterns.
    In this section, we''ll classify utterances with the `NONE` label. Let''s see
    some utterance examples first:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个基于模式的分类器，我们首先需要收集一些模式。在本节中，我们将对带有`NONE`标签的表述进行分类。让我们先看看一些表述示例：
- en: '[PRE23]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'By looking at these utterances, we see that the utterances with the `NONE`
    label follow some patterns:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Most of the utterances start with `No,` or `No.`.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patterns of saying *thank you* are also quite common. The patterns `Thanks`,
    `thank you`, and `thanks a lot` occur in most of the utterances in the preceding
    code.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some helper phrases such as `that is all`, `that'll be all`, `that's OK`, and
    `this should be enough` are also commonly used.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on this information, we can create three Matcher patterns as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s go over the patterns one by one:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: The first pattern matches token sequences `no,`, `no.`, `nope,`, `nope.`, `No,`,
    `No.`, `Nope,`, and `Nope.`. The first item matches two tokens `no` and `nope`
    either in capitals or small letters. The second item matches the punctuation marks
    `,` and `.`.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second pattern matches `thank`, `thank you`, `thanks`, and `thanks a lot`,
    either in capitals or small letters. The first item matches `thank` and `thanks`
    `s?`. In regex syntax, the `s` character is optional. The second item corresponds
    to the words `you` and `a lot`, which can possibly follow `thanks?`. The second
    item is optional; hence, the pattern matches `thanks` and `thank` as well. We
    used the operator `OP: *` to make the second item optional; recall from [*Chapter
    4*](B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069)*, Rule-Based Matching*, that
    Matcher supports operator syntax with different operators, such as `*` , `+`,
    and `?`.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third pattern matches the token sequences `that is all`, `that's all`, `thats
    all`, and so on. Notice that the first item includes some misspelled words, such
    as `thats` and `thatll`. We included the misspelled words on purpose, so the matching
    will be more robust to user typos.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different combinations of the preceding three patterns will match the utterances
    of the `NONE` class. You can try the patterns by adding them to a Matcher object
    and see how they match.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Pro tip
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: While designing a rule-based system, always keep in mind that user data is not
    perfect. User data contains typos, grammatical mistakes, and wrong capitalization.
    Always keep robustness as a high priority and test your patterns on user data.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: We made a statistical model-free classifier by making use of some common patterns
    and classified one intent successfully. How about the other two intents – `FindRestaurants`
    and `ReserveRestaurant`? Utterances of these two intents are semantically much
    more complicated, so we cannot cope with pattern lists. We need statistical models
    to recognize these two intents. Let's go ahead and train our statistical text
    classifiers with TensorFlow and Keras.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Classifying text with a character-level LSTM
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we''ll train a **character-level LSTM architecture** for recognizing
    the intents. We already practiced text classification with TensorFlow and Keras
    in [*Chapter 8*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)*, Text Classification
    with spaCy*. Recall from this chapter that LSTMs are sequential models that process
    one input at one time step. We fed one word at each time step as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Feeding one word to an LSTM at each time step'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10_3.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 – Feeding one word to an LSTM at each time step
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: As we remarked in [*Chapter 8*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)*,
    Text Classification with spaCy*, LSTMs have an internal state (you can think of
    it as a memory), so LSTMs can model the sequential dependencies in the input sequence
    by holding past information in their internal state.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we''ll train a character-level LSTM. As the name suggests,
    we''ll feed utterances character by character, not word by word. Each utterance
    will be represented as a sequence of characters. At each time step, we''ll feed
    one character. This is what feeding the utterance from *Figure 10.3* looks like:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Feeding the first two words of the utterance "I want Italian
    food"'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10_4.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.4 – Feeding the first two words of the utterance "I want Italian food"
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: We notice that the space character is fed as an input as well, because the space
    character is also a part of the utterance; for character-level tasks, there is
    no distinction between digits, spaces, and letters.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Let's start building the Keras model. We'll skip the data preparation stage
    here. You can find the complete code in the intent classification notebook [https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/Intent-classifier-char-LSTM.ipynb](https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/Intent-classifier-char-LSTM.ipynb).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll directly start with Keras'' Tokenizer to create a vocabulary. Recall
    from [*Chapter 8*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)*, Text Classification
    with spaCy*, that we use Tokenizer to do the following:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Create a vocabulary from the dataset sentences.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign a token ID to each token of the dataset.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transform input sentences to token IDs.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see how to perform each step:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'In [*Chapter 8*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)*, Text Classification
    with spaCy*, we tokenized the sentences into words and assigned token IDs to words.
    This time, we''ll break the input sentence into its characters, then assign token
    IDs to characters. Tokenizer provides a parameter named `char_level`. Here''s
    the Tokenizer code for character-level tokenization:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The preceding code segment will create a vocabulary from the input characters.
    We used the `lower=True` parameter, so all characters of the input sentence are
    made lowercase by Tokenizer. After initializing the `Tokenizer` object on our
    vocabulary, we can now examine its vocabulary. Here are the first 10 items of
    the Tokenizer vocabulary:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述代码段将创建一个从输入字符中提取的词汇表。我们使用了`lower=True`参数，因此Tokenizer将输入句子的所有字符转换为小写。在初始化我们的词汇表上的`Tokenizer`对象后，我们现在可以检查其词汇表。以下是Tokenizer词汇表的前10项：
- en: '[PRE26]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Just as with the word-level vocabulary, index `0` is reserved for a special
    token, which is the padding character. Recall from [*Chapter 8*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)*,
    Text Classification with spaCy*, that Keras cannot process variable-length sequences;
    each sentence in the dataset should be of the same length. Hence, we pad all sentences
    to a maximum length by appending a padding character to the sentence end or sentence
    start.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如与词级词汇表一样，索引`0`被保留用于一个特殊标记，即填充字符。回想一下[*第8章*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)*，使用spaCy进行文本分类*，Keras无法处理变长序列；数据集中的每个句子都应该具有相同的长度。因此，我们通过在句子末尾或句子开头添加填充字符来将所有句子填充到最大长度。
- en: 'Next, we''ll convert each dataset sentence into token IDs. This is achieved
    by calling the `texts_to_sequences` method of Tokenizer:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将每个数据集句子转换为标记ID。这是通过调用Tokenizer的`texts_to_sequences`方法实现的：
- en: '[PRE27]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we''ll pad all the input sentences to a length of `150`:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将所有输入句子填充到长度为`150`：
- en: '[PRE28]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We''re ready to feed our transformed dataset into our LSTM model. Our model
    is a simple yet very efficient one: we placed a dense layer on top of a bidirectional
    LSTM layer. Here''s the model architecture:'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们准备好将转换后的数据集输入到我们的LSTM模型中。我们的模型简单而非常高效：我们在双向LSTM层之上放置了一个密集层。以下是模型架构：
- en: '[PRE29]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'A bidirectional LSTM layer means two LSTMs stacked on top of each other. The
    first LSTM goes through the input sequence from left to right (in a forward direction)
    and the second LSTM goes through the input sequence right to left (in a backward
    direction). For each time step, the outputs of the forward LSTM and backward LSTM
    are concatenated to generate a single output vector. The following figure exhibits
    our architecture with a bidirectional LSTM:'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 双向LSTM层意味着两个LSTM堆叠在一起。第一个LSTM从左到右（正向）遍历输入序列，第二个LSTM从右到左（反向）遍历输入序列。对于每个时间步，正向LSTM和反向LSTM的输出被连接起来生成一个单独的输出向量。以下图展示了我们的具有双向LSTM的架构：
- en: '![Figure 10.5 – Bidirectional LSTM architecture'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![Figure 10.5 – 双向LSTM架构'
- en: '](img/Figure_10_5.jpg)'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/Figure_10_5.jpg]'
- en: Figure 10.5 – Bidirectional LSTM architecture
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图10.5 – 双向LSTM架构
- en: 'Next, we compile our model and train it on our dataset by calling `model.fit`:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通过调用`model.fit`来编译我们的模型并在我们的数据集上训练它：
- en: '[PRE30]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here, we compiled our model with the following:'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用以下内容编译我们的模型：
- en: a) Binary cross-entropy loss, because this is a binary classification task (we
    have two class labels).
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 二元交叉熵损失，因为这是一个二元分类任务（我们有两个类别标签）。
- en: b) The `Adam` optimizer, which will help the training procedure to run faster
    by arranging the size of the training steps. Please refer to the *References*
    section and [*Chapter 8*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)*, Text
    Classification with spaCy*, for more information about the `Adam` optimizer.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `Adam`优化器，它将通过调整训练步骤的大小来帮助训练过程更快地运行。请参阅*参考文献*部分和[*第8章*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)*，使用spaCy进行文本分类*，以获取有关`Adam`优化器的更多信息。
- en: c) Accuracy as our success metric. Accuracy is calculated by comparing how often
    the predicted label is equal to the actual label.
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 准确率作为我们的成功指标。准确率是通过比较预测标签与实际标签相等的频率来计算的。
- en: After fitting our model, our model gives a `0.8226` accuracy on the validation
    set, which is quite good.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整我们的模型后，我们的模型在验证集上给出了`0.8226`的准确率，这相当不错。
- en: 'Now, only one question remains: why did we prefer to train a character-level
    model this time? Character-level models definitely have some advantages:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，只剩下一个问题：为什么我们这次选择训练一个字符级模型？字符级模型确实有一些优点：
- en: Character-level models are highly misspelling-tolerant. Consider the misspelled
    word *charactr* – whether or not the *e* is missing does not affect the overall
    sentence semantics that much. For our dataset, we will benefit from this robustness,
    as we have already seen spelling mistakes by users in our dataset exploration.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符级模型对拼写错误高度容忍。考虑一下拼写错误的单词*charactr* – 是否缺少字母*e*并不太影响整个句子的语义。对于我们的数据集，我们将从这种鲁棒性中受益，因为我们已经在数据集探索中看到了用户的拼写错误。
- en: The vocabulary size is smaller than a word-level model. The number of characters
    in the alphabet (for any given language) is fixed and low (a maximum of 50 characters,
    including uppercase and lowercase letters, digits, and some punctuation); but
    the number of words in a language is much greater. As a result, model sizes can
    differ. The main difference lies at the embedding layer; an embedding table is
    of size `(vocabulary_size, output_dim)` (refer to the model code). Given that
    the output dimensions are the same, 50 rows is really small compared to thousands
    of rows.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇量小于词级模型。字母表中的字符数（对于任何给定语言）是固定的且较低（最多50个字符，包括大写和小写字母、数字和一些标点符号）；但一个语言中的单词数量要大得多。因此，模型大小可能会有所不同。主要区别在于嵌入层；嵌入表的大小为`(vocabulary_size,
    output_dim)`（参见模型代码）。鉴于输出维度相同，与数千行相比，50行确实很小。
- en: In this section, we were able to extract the user intent from the utterances.
    Intent recognition is the main step in understanding sentence semantics, but is
    there something more? In the next section, we'll dive into sentence-level and
    dialog-level semantics. More semantic parsing
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们能够从话语中提取用户意图。意图识别是理解句子语义的主要步骤，但还有更多吗？在下一节中，我们将深入研究句子级和对话级语义。更多语义分析
- en: This is a section solely on chatbot NLU. In this section, we'll explore sentence-level
    semantic and syntactic information to generate a deeper understanding of the input
    utterances, as well as providing clues to answer generation.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个专门关于聊天机器人NLU的章节。在本节中，我们将探索句子级语义和句法信息，以生成对输入话语的更深入理解，并提供答案生成的线索。
- en: In the rest of this section, please think of the answer generation component
    as a black box. We provide the semantic parse of the sentence and it generates
    an answer based on this semantic parse. Let's start by dissecting sentence syntax
    and examining the subjects and objects of the utterances.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，请将答案生成组件视为黑盒。我们提供句子的语义分析，并根据这个语义分析生成答案。让我们先剖析句子语法，并检查话语的主语和对象。
- en: Differentiating subjects from objects
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 区分主语和对象
- en: 'Recall from [*Chapter 3*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)*,
    Linguistic Features*, that a sentence has two important grammatical components:
    a **subject** and an **object**. The subject is the person or thing that performs
    the action given by the verb of the sentence:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[*第3章*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)*，语言特征*，一个句子有两个重要的语法成分：一个**主语**和一个**对象**。主语是执行句子动词所给动作的人或物：
- en: '[PRE31]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: A subject can be a noun, a pronoun, or a noun phrase.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 主语可以是名词、代词或名词短语。
- en: 'An object is the thing or person on which the subject performs the action given
    by the verb. An object can be a noun, a pronoun, or a noun phrase too. Here are
    some examples:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 一个对象是动词所给动作执行的对象或人。一个对象可以是名词、代词或名词短语。以下是一些例子：
- en: '[PRE32]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: So far, so good, but how does this information help us in our chatbot NLU?
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利，但这个信息如何帮助我们进行聊天机器人NLU？
- en: 'Extracting the subject and the object helps us understand the sentence structure,
    hence adding one more layer to the semantic parse of the sentence. Sentence subject
    and object information directly relates to answer generation. Let''s see some
    examples of utterances from our dataset:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 提取主语和对象有助于我们理解句子结构，从而为句子的语义分析增加一个层次。句子主语和对象信息直接与答案生成相关。让我们看看我们数据集中的一些话语示例：
- en: '[PRE33]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The following figure shows the dependency parse of this utterance. The subject
    is the noun phrase `this restaurant`:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了此话语的依存句法分析。主语是名词短语`这家餐厅`：
- en: '![Figure 10.6 – Dependency parse of the example utterance'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 10.6 – 示例话语的依存句法分析'
- en: '](img/Figure_10_6.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 10_6.jpg]'
- en: Figure 10.6 – Dependency parse of the example utterance
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6 – 示例话语的依存句法分析
- en: 'How can we generate an answer to this sentence? Obviously, the answer should
    have `this restaurant` (or the restaurant it refers to) as the subject. Some answers
    could be as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何生成这个句子的答案？显然，答案应该以`this restaurant`（或它所指的餐厅）为主语。以下是一些可能的答案：
- en: '[PRE34]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'What if the user puts `this restaurant` into the object role? Would the answer
    change? Let''s take some example utterances from the dataset:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户将`this restaurant`放入宾语角色，答案会改变吗？让我们从数据集中取一些示例语句：
- en: '[PRE35]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Obviously, the user is asking about the address of the restaurant again. The
    system needs to give the restaurant address information. However, this time, the
    subject of these sentences is `you`:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，用户再次询问餐厅的地址。系统需要提供餐厅地址信息。然而，这次，这些句子的主语是`你`：
- en: '![Figure 10.7 – Dependency parse of the first sentence'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.7 – 第一句话的依存句法分析'
- en: '](img/Figure_10_7.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10_7.jpg)'
- en: Figure 10.7 – Dependency parse of the first sentence
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7 – 第一句话的依存句法分析
- en: 'This question''s subject is `you`, so the answer can start with an *I*. This
    a question sentence, hence the answer can start with a *yes*/*no* or the answer
    can just provide the restaurant''s address directly. The following sentences are
    all possible answers:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的主语是`你`，所以答案可以以一个*I*开头。这是一个疑问句，因此答案可以以*yes*/*no*开头，或者答案可以直接提供餐厅的地址。以下句子都是可能的答案：
- en: '[PRE36]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The same phrase, `the restaurant`, being the subject or the object doesn''t
    affect the user''s intent, but it affects the sentence structure of the answer.
    Let''s look at the information more systematically. The semantic parses of the
    preceding example sentences look as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的短语`the restaurant`作为主语或宾语并不影响用户的意图，但它会影响答案的句子结构。让我们更系统地看看信息。前例句子的语义分析如下：
- en: '[PRE37]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: When we feed these semantic parses to the answer generator module, this module
    can generate answers by taking the current utterance, the dialog history, the
    utterance intent, and the utterance's sentence structure (for the time being,
    only the sentence subject information) into account.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将这些语义分析输入到答案生成模块时，该模块可以通过考虑当前语句、对话历史、语句意图和语句的句子结构（目前仅考虑句子主语信息）来生成答案。
- en: Here, we extracted the utterance's sentence structure information by looking
    at the utterance's dependency tree. Can a dependency parse provide us with more
    information about the utterance? The answer is yes. We'll see how to extract the
    sentence type in the next section.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过查看语句的依存树来提取语句的句子结构信息。依存句法分析能否为我们提供更多关于语句的信息？答案是肯定的。我们将在下一节中看到如何提取句子类型。
- en: Parsing the sentence type
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 句子类型解析
- en: 'In this section, we''ll extract the sentence type of the user utterances. The
    grammar has four main sentence types, classified by their purpose:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将提取用户语句的句子类型。语法有四种主要的句子类型，根据其目的进行分类：
- en: '[PRE38]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Sentence types in chatbot NLU are a bit different; we classify sentences according
    to the POS tag of the subject and objects as well as the purpose. Here are some
    sentence types that are used in chatbot NLU:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在聊天机器人NLU中，句子类型略有不同；我们根据主语和宾语的词性以及目的来分类句子。以下是聊天机器人NLU中使用的某些句子类型：
- en: '[PRE39]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Let's examine each sentence type and its structural properties. We start with
    question sentences.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查每种句子类型及其结构特性。我们首先从疑问句开始。
- en: Question sentences
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 疑问句
- en: 'A question sentence is used when the user wants to ask something. A question
    sentence can be formed in two ways, either by using an interrogative pronoun or
    by placing a modal/auxiliary verb at the beginning of the sentence:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户想要提问时，会使用疑问句。疑问句可以通过两种方式形成，要么使用疑问代词，要么将情态动词或助动词置于句首：
- en: '[PRE40]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Hence, we also divide the question sentences into two classes, **wh-questions**
    and **yes/no questions**. As the name suggests, wh-questions start with a **wh-word**
    (a wh-word means an interrogative pronoun, such as where, what, who, and how)
    and yes/no questions are formed by using a modal/auxiliary verb.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将疑问句分为两类，**wh-疑问句**和**是非疑问句**。正如其名所示，wh-疑问句以一个**wh词**（wh词指的是疑问代词，如哪里、什么、谁和如何）开头，而是非疑问句是通过使用情态动词或助动词来构成的。
- en: 'How will this classification help us? Syntactically, yes/no questions should
    be answered with a yes or no. Hence, if our chatbot NLU passes a yes/no question
    to the answer generation module, the answer generator should evaluate this information
    and generate an answer that starts with a yes/no. Wh-questions aim to get information
    about the subject or objects, hence the answer generator module should provide
    information about the sentence subject or objects. Consider the following utterance:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This utterance generates the following dependency parse:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Dependency parse of the example wh-question'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10_8.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.8 – Dependency parse of the example wh-question
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the subject of the utterance is `this restaurant`; hence the answer generator
    should generate an answer by relating `Where` and `this restaurant`. How about
    the following utterance:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The dependency parse of this utterance looks as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 -- Dependency parse of the example wh-question'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10_9.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.9 -- Dependency parse of the example wh-question
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Here, the sentence structure is a bit different. `Which city` is the subject
    of the sentence and `this restaurant` is the subject of the clause. Here, the
    answer generation module should generate an answer by relating `which city` and
    `this restaurant`.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: We will now move on to imperative sentence type.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Imperative sentence
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Imperative sentences** occur quite frequently in chatbot user utterances.
    An imperative sentence is formed by placing the main verb at the beginning of
    the sentence. Here are some utterance examples from our dataset:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'As we see, imperative utterances occur quite a lot in user utterances, because
    they''re succinct and to-the-point. We can spot these types of sentences by looking
    at the POS tags of the words: either the first word is a verb or the sentence
    starts with *please* and the second word is a verb. The following Matcher patterns
    match imperative utterances:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'How would the answer generator process these types of sentences? Imperative
    sentences usually include the syntactic and semantic elements to generate an answer;
    the main verb provides the action and is usually followed by a list of objects.
    Here''s an example parse for the utterance `Find me Ethiopian cuisine in Berkeley`:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Dependency parse of the example utterance'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10_10.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.10 – Dependency parse of the example utterance
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'From the figure, we can see the syntactic components of this sentence as `Find`
    (the action), `Ethiopian cuisine` (an object), and `Berkeley` (an object). These
    components provide a clear template to the answer generator for generating an
    answer to this utterance: the answer generator should ask the restaurants database
    for matches of `Ethiopian cuisine` and `Berkeley` and list the matching restaurants.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Now we move on to the next sentence type, wish sentences. Let's see look at
    sentences in detail.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Wish sentences
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Wish sentences** are semantically similar to imperative sentences. The difference
    is syntactic: wish sentences start with phrases such as *I''d like to*, *Can I*,
    *Can you*, and *May I*, pointing to a wish. Here are some examples from our dataset:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '**愿望句**在语义上与祈使句相似。区别在于句法：愿望句以诸如 *I''d like to*、*Can I*、*Can you* 和 *May I*
    等短语开头，指向一个愿望。以下是我们数据集中的几个例子：'
- en: '[PRE45]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Extracting the verb and the objects is similar to what we do for imperative
    sentences, hence the semantic parse is quite similar.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 提取动词和宾语与我们对祈使句所做的工作类似，因此语义分析相当相似。
- en: 'After extracting the sentence type, we can include it into our semantic parse
    result as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在提取句子类型后，我们可以将其包含到我们的语义分析结果中，如下所示：
- en: '[PRE46]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Now, we have a rich semantic and syntactic representation of the input utterance.
    In the next section, we'll go one step beyond the sentence-level semantics and
    go through the dialog-level semantics. Let's move on to the next section and see
    how we tackle dialog-level semantics.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了丰富的语义和句法表示的输入话语。在下一节中，我们将超越句子级语义，进入对话级语义。让我们继续到下一节，看看我们如何处理对话级语义。
- en: Anaphora resolution
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代词消解
- en: 'In this section, we''ll explore the linguistic concepts of **anaphora** and
    **cohesion**. In linguistics, cohesion means the grammatical links that glue a
    text together semantically. This text can be a single sentence, a paragraph, or
    a dialog segment. Consider the following two sentences:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨语言学概念**代词**和**连贯性**。在语言学中，连贯性意味着将文本在语义上粘合在一起的语法联系。这个文本可以是一个单独的句子、一个段落或一个对话片段。考虑以下两个句子：
- en: '[PRE47]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Here, the word `one` refers to the dress from the first sentence. A human can
    resolve this link easily. It's not so straightforward for software programs, though.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，单词 `one` 指的是第一句话中的连衣裙。人类可以轻松地解决这个问题，但对于软件程序来说，这并不那么直接。
- en: 'Also, consider the following dialog segment:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 还要考虑以下对话片段：
- en: '[PRE48]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The second sentence is completely understandable, though some parts of the
    sentence are missing:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 第二句话完全可以理解，尽管句子中的一些部分缺失：
- en: '[PRE49]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'In written and spoken language, we use such **shortcuts** every day. However,
    resolving such shortcuts needs attention while programming, especially in chatbot
    NLU. Consider these utterances and dialog segments from our dataset:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在书面和口头语言中，我们每天都在使用这样的**捷径**。然而，在编程时解决这些捷径需要引起注意，尤其是在聊天机器人自然语言理解（NLU）中。考虑以下来自我们数据集的话语和对话片段：
- en: 'Example 1:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '示例 1:'
- en: '[PRE50]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Example 2:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '示例 2:'
- en: '[PRE51]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Example 3:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '示例 3:'
- en: '[PRE52]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Example 4:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '示例 4:'
- en: '[PRE53]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Example 5:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '示例 5:'
- en: '[PRE54]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Example 6:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '示例 6:'
- en: '[PRE55]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: All the highlighted parts of the preceding sentences and dialogs are examples
    of a linguistic event named `one`, `more`, `same`, `it`, and so on. Anaphora resolution
    means to resolve exactly the phrases anaphoric words point to.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 前面句子和对话中所有加粗的部分都是名为 `one`、`more`、`same`、`it` 等的语言事件的例子。代词消解意味着解决代词所指的确切短语。
- en: How do we apply this information to our chatbot NLU then?
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将此信息应用到我们的聊天机器人 NLU 中呢？
- en: 'First of all, we need to determine whether an utterance involves an anaphora
    and whether we need an anaphora resolution. Consider the following dialog segment
    again:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要确定一个话语是否涉及代词消解，以及我们是否需要进行代词消解。再次考虑以下对话片段：
- en: '[PRE56]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The dependency parse of the second utterance looks like this:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个话语的依存句法分析如下：
- en: '![Figure 10.11 – Dependency parse of the example utterance'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.11 – 示例话语的依存句法分析'
- en: '](img/Figure_10_11.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10_11](img/Figure_10_11.jpg)'
- en: Figure 10.11 – Dependency parse of the example utterance
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11 – 示例话语的依存句法分析
- en: 'First of all, `one` appears as the direct object of the sentence and there
    are no other direct objects. This means that `one` should be an anaphora. In order
    to resolve what `one` refers to, we''ll look back to the first utterance of the
    dialog. The following dependency parse belongs to the first utterance, `Do you
    want to make a reservation?`:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，`one` 出现在句子的直接宾语位置，并且没有其他直接宾语。这意味着 `one` 应该是一个代词。为了解决 `one` 指代什么，我们将回顾对话的第一个话语。以下依存句法分析属于第一个话语，`Do
    you want to make a reservation?`：
- en: '![Figure 10.12 – Dependency parse of the example utterance'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.12 – 示例话语的依存句法分析'
- en: '](img/Figure_10_12.jpg)'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10_12](img/Figure_10_12.jpg)'
- en: Figure 10.12 – Dependency parse of the example utterance
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12 – 示例话语的依存句法分析
- en: 'If we look at *Figure 10.11*, we see that the sentence has a direct object,
    `a reservation`, so `one` should refer to `a reservation`. Then, we can arrange
    the resulting semantic parse as follows:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看*图10.11*，我们会看到句子有一个直接宾语，“一个预订”，因此“一个”应该指代“一个预订”。然后，我们可以将生成的语义解析安排如下：
- en: '[PRE57]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Replacing `one` with `a reservation` makes the sentence intent clearer. In our
    chatbot NLU, we only have two intents, but what if there are more intents, such
    as reservation cancellation, refunds, and so on? Then `I want to make one` can
    mean making a cancellation or getting a refund as well.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 将“一个”替换为“一个预订”可以使句子的意图更清晰。在我们的聊天机器人NLU中，我们只有两个意图，但如果有更多意图，比如预订取消、退款等，怎么办？那么“我想订一个”也可以意味着进行取消或获取退款。
- en: Therefore, we make anaphora resolution come before the intent recognition and
    feed the full sentence, where anaphora words are replaced with the phrases they
    refer to. This way, the intent classifier is fed with a sentence where the direct
    object is a noun phrase, not one of the words `one`, `same`, `it`, or `more`,
    which do not carry any meaning on their own.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将指代词消解放在意图识别之前，并输入完整的句子，其中指代词被它们所指的短语所替换。这样，意图分类器接收到的句子中，直接宾语是一个名词短语，而不是“一个”、“相同的”、“它”或“更多”等单词，这些单词本身没有任何意义。
- en: Now after extracting meaning (by extracting intent) statistically with Keras,
    in this section you learned ways of processing sentence syntax and semantics with
    special NLU techniques. You're ready to combine all the techniques you know and
    design your own chatbot NLU for your future career. This book started with linguistic
    concepts, continued with statistical applications, and in this chapter, we combined
    it all. You're ready to keep going. In all the NLU pipelines you'll design, always
    try to look at the problem from a different view and remember what you learned
    in this book.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在用Keras进行统计意义提取（通过提取意图）之后，在本节中，你学习了使用特殊NLU技术处理句子语法和语义的方法。你已经准备好结合你所知道的所有技术，为你的未来职业设计自己的聊天机器人NLU。这本书从语言概念开始，继续到统计应用，在本章中，我们将它们全部结合起来。你已经准备好继续前进。在所有你将设计的NLU管道中，始终尝试从不同的角度看待问题，并记住你在本书中学到的内容。
- en: Summary
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: That's it! You made it to the end of this exhaustive chapter and also to the
    end of this book!
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你已经到达了这一详尽章节的结尾，也到达了这本书的结尾！
- en: In this chapter, we designed an end-to-end chatbot NLU pipeline. As a first
    task, we explored our dataset. By doing this, we collected linguistic information
    about the utterances and understood the slot types and their corresponding values.
    Then, we performed a significant task of chatbot NLU, entity extraction. We extracted
    several types of entities such as city, date/time, and cuisine with the spaCy
    NER model as well as Matcher. Then, we performed another traditional chatbot NLU
    pipeline task – intent recognition. We trained a character-level LSTM model with
    TensorFlow and Keras.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们设计了一个端到端聊天机器人NLU管道。作为第一个任务，我们探索了我们的数据集。通过这样做，我们收集了关于话语的语言学信息，并理解了槽位类型及其对应的值。然后，我们执行了聊天机器人NLU的一个重要任务——实体提取。我们使用spaCy
    NER模型以及Matcher提取了多种类型的实体，如城市、日期/时间和菜系。然后，我们执行了另一个传统的聊天机器人NLU管道任务——意图识别。我们使用TensorFlow和Keras训练了一个字符级LSTM模型。
- en: In the last section, we dived into sentence-level and dialog-level semantics.
    We worked on sentence syntax by differentiating subjects from objects, then learned
    about sentence types and finally learned about the linguistic concept of anaphora
    resolution. We applied what we learned in the previous chapters, both linguistically
    and statistically, by combining several spaCy pipeline components such as NER,
    dependency parsers, and POS taggers.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们深入探讨了句子级和对话级语义。我们通过区分主语和宾语来处理句子语法，然后学习了句子类型，最后学习了指代词消解的语言学概念。我们通过结合几个spaCy管道组件（如NER、依存句法分析器和词性标注器）所学的知识，在语言和统计上应用了之前章节的内容。
- en: References
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Here are some references for this chapter:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这里为本章提供了一些参考文献：
- en: 'On voice assistant products:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 关于语音助手产品：
- en: 'Alexa developer blog: [https://developer.amazon.com/blogs/home/tag/Alexa](https://developer.amazon.com/blogs/home/tag/Alexa%20)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alexa 开发者博客：[https://developer.amazon.com/blogs/home/tag/Alexa](https://developer.amazon.com/blogs/home/tag/Alexa%20)
- en: 'Alexa science blog: [https://www.amazon.science/tag/alexa](https://www.amazon.science/tag/alexa%20)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alexa 科学博客：[https://www.amazon.science/tag/alexa](https://www.amazon.science/tag/alexa%20)
- en: 'Microsoft''s publication on chatbots: [https://academic.microsoft.com/search?q=chatbot](https://academic.microsoft.com/search?q=chatbot)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微软关于聊天机器人的出版物：[https://academic.microsoft.com/search?q=chatbot](https://academic.microsoft.com/search?q=chatbot)
- en: 'Google Assistant: [https://assistant.google.com/](https://assistant.google.com/%20)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌助手：[https://assistant.google.com/](https://assistant.google.com/%20)
- en: 'Keras layers and optimizers:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: Keras层和优化器：
- en: 'Keras layers: [https://keras.io/api/layers/](https://keras.io/api/layers/%20)'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras层：[https://keras.io/api/layers/](https://keras.io/api/layers/%20)
- en: 'Keras optimizers: [https://keras.io/api/optimizers/](https://keras.io/api/optimizers/%20)'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras优化器：[https://keras.io/api/optimizers/](https://keras.io/api/optimizers/%20)
- en: 'An overview of optimizers: [https://ruder.io/optimizing-gradient-descent/](https://ruder.io/optimizing-gradient-descent/%20)'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器概述：[https://ruder.io/optimizing-gradient-descent/](https://ruder.io/optimizing-gradient-descent/%20)
- en: 'Adam optimizer: [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980%20)'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adam优化器：[https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980%20)
- en: 'Datasets for conversational AI:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 对话式人工智能的数据集：
- en: 'Taskmaster from Google Research: [https://github.com/google-research-datasets/Taskmaster/tree/master/TM-1-2019](https://github.com/google-research-datasets/Taskmaster/tree/master/TM-1-2019%20)'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自谷歌研究团队的Taskmaster：[https://github.com/google-research-datasets/Taskmaster/tree/master/TM-1-2019](https://github.com/google-research-datasets/Taskmaster/tree/master/TM-1-2019%20)
- en: 'Simulated Dialogue dataset from Google Research: [https://github.com/google-research-datasets/simulated-dialogue](https://github.com/google-research-datasets/simulated-dialogue%20)'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自谷歌研究团队的模拟对话数据集：[https://github.com/google-research-datasets/simulated-dialogue](https://github.com/google-research-datasets/simulated-dialogue%20)
- en: 'Dialog Challenge dataset from Microsoft: [https://github.com/xiul-msr/e2e_dialog_challenge](https://github.com/xiul-msr/e2e_dialog_challenge%20)'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自微软的对话挑战数据集：[https://github.com/xiul-msr/e2e_dialog_challenge](https://github.com/xiul-msr/e2e_dialog_challenge%20)
- en: 'Dialog State Tracking Challenge dataset: [https://github.com/matthen/dstc](https://github.com/matthen/dstc)'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对话状态跟踪挑战数据集：[https://github.com/matthen/dstc](https://github.com/matthen/dstc)
