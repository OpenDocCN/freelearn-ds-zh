- en: Chapter 6. Developing a Sobel Edge Detection Filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the convolution Theory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding convolution in 1D
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding convolution in 2D
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenCL implementation of the Sobel edge filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding profiling in OpenCL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to take a look at how to develop a popular image
    processing algorithm known as edge detection. This problem happens to be a part
    of solving a more general problem in image segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Image segmentation is the process of partitioning a digital image into multiple
    segments (sets of pixels, also known as super pixels). The goal of segmentation
    is to simplify and/or change the representation of an image into something that
    is more meaningful and easier to analyze. Image segmentation is typically used
    to locate objects and boundaries (lines, curves, and so on) in images.
  prefs: []
  type: TYPE_NORMAL
- en: The Sobel operator is a discrete differentiation operator, computing an approximation
    of the gradient of the image density function. The Sobel operator is based on
    convolving the image with a small, separable, and an integer-value filter in both
    horizontal and vertical directions. Thus, it is relatively inexpensive in terms
    of computations.
  prefs: []
  type: TYPE_NORMAL
- en: Don't worry if you don't understand these notations right away, we are going
    to step through enough theory and math, and help you realize the application in
    OpenCL.
  prefs: []
  type: TYPE_NORMAL
- en: Briefly, the Sobel filtering is a three-step process. Two 3 x 3 filters are
    applied separately and independently on every pixel and the idea is to use these
    two filters to approximate the derivatives of x and y, respectively. Using the
    results of these filters, we can finally approximate the magnitude of the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: The gradient computed by running Sobel's edge detector through each pixel (which
    also uses its neighboring eight pixels) will inform us whether there are changes
    in the vertical and horizontal axes (where the neighboring pixels reside).
  prefs: []
  type: TYPE_NORMAL
- en: For those who are already familiar with the convolution theory, in general,
    may skip to the *How to do it* section of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the convolution theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the past, mathematicians developed calculus so that there's a systematic
    way to reason about how things change, and the convolution theory is really about
    measuring how these changes affect one another. At that time, the convolution
    integral was born.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the convolution theory](img/4520OT_06_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: And the ![Understanding the convolution theory](img/4520OT_06_23.jpg) operator
    is the convolution operator used in conventional math. An astute reader will notice
    immediately that we have replaced one function with the other, and the reason
    why this is done is because of the fact that the convolution operator is commutative,
    that is, the order of computation does not matter. The computation of the integral
    can be done in discrete form, and without loss of generality, we can replace the
    integral sign ![Understanding the convolution theory](img/4520OT_06_22.jpg) with
    the summation sign ![Understanding the convolution theory](img/4520OT_06_21.jpg),
    and with that, let's see the mathematical definition of convolution in discrete
    time domain.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Later we will walk through what the following equation tells us over a discrete
    time domain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/4520OT_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *x[n]* is an input signal, *h[n]* is an impulse response, and *y[n]* is
    the output. The asterisk (***) denotes convolution. Notice that we multiply the
    terms of *x[k]* by the terms of a time-shifted *h[n]* and add them up. The key
    to understanding convolution lies behind impulse response and impulse decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to understand the meaning of convolution, we are going to start from
    the concept of signal decomposition. The input signal can be broken down into
    additive components, and the system response of the input signal results in by
    adding the output of these components passed through the system.
  prefs: []
  type: TYPE_NORMAL
- en: The following section will illustrate on how convolution works in 1D, and once
    you're proficient in that, we will build on that concept and illustrate how in
    convolution works 2D and we'll see the Sobel edge detector in action!
  prefs: []
  type: TYPE_NORMAL
- en: Understanding convolution in 1D
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's imagine that a burst of energy (signal) have arrived into our system and
    it looks similar to the following diagram with *x[n] = {1,3,4,2,1}, for n = 0,1,2,3,4*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding convolution in 1D](img/4520OT_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: And let's assume that our impulse function has a non-zero value whenever **n**
    = **0** or **1**, while it'll have a zero value for all other values of **n**.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the preceding information, let''s work out what the output signal would
    be by quickly recalling the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/4520OT_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Following this equation faithfully, we realize that the output signal is amplified
    initially and quickly tapers off, and after solving this manually (yes, I mean
    evaluating the equation on a pencil and paper) we would see the following final
    output signal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/4520OT_06_18.jpg)![How to do it...](img/4520OT_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Looking at the preceding equation again, this time we rearrange them and remove
    all terms that evaluate to zero. Let''s try to see whether we can discover a pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/4520OT_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'And I believe you can see that each output value is computed from its previous
    two output values (taking into account the impulse function)! And now we may conclude,
    quite comfortably, that the general formula for computing the convolution in 1D
    is in fact the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/4520OT_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Finally, you should be aware that (by convention) any value that is not defined
    for any *x[i-k]* is automatically given the value zero. This seemingly small,
    subtle fact will play a role in our eventual understanding of the Sobel edge detection
    filter which we'll describe next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally for this section, let''s take a look at how a sequential convolution
    code in 1D might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Examining the code again, you will probably notice that we are iterating over
    the 1D array and the most interesting code would be in `statement 1`, as this
    is where the action really lies. Let's put that new knowledge aside and move on
    to extending this to a 2D space.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding convolution in 2D
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolution in 2D is actually an extension of the previously described *Understanding
    convolution in 1D* section, and we do so by computing the convolution in two dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The impulse function also exists in a 2D spatial domain, so let's call this
    function. *b[x,y]* has the value 1, where x and y are zero, and zero where x,y¹0\.
    The impulse function is also referred to as filter or kernel when it's being used
    in image processing.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the previous example as a guide, let's start thinking from the perspective
    of a signal which can be decomposed into the sum of its components and impulse
    functions, and their double summation accounts to the fact that this runs over
    both vertical and horizontal axes in our 2D space.
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/4520OT_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Next, I think it's very helpful if we use an example to illustrate how it works
    when we have two convolution kernels to represent the filters we like to apply
    on the elements in a 2D array. Let's give them names, **Sx** and **Sy**. The next
    thing is to try out how the equation would develop itself in a 2D setting, where
    the element we want to convolve is at *x[1,1]* and we make a note of its surrounding
    eight elements and then see what happens.
  prefs: []
  type: TYPE_NORMAL
- en: If you think about why we are choosing the surrounding eight elements, it's
    the only way we can measure how big a change is with respect to every other element.
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/4520OT_06_14.jpg)![How to do it…](img/4520OT_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s give it a go:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/4520OT_06_13.jpg)![How to do it…](img/4520OT_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This results in the summation of nine elements (including the element we're
    interested in), and this process is repeated for all elements in the 2D array.
    The following diagram illustrates how convolution in 2D works in a 2D space.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may wish to read Irwin Sobel's 1964 original doctoral thesis since he's
    the inventor, and this author had a good fortune of meeting the man himself.
  prefs: []
  type: TYPE_NORMAL
- en: What happens when you attempt to convolve around the elements that border the
    2D array or in image processing, are they referred to as edge pixels? If you use
    this formula for computation, you will notice that the results will be inaccurate,
    because those elements are undefined and hence they're in general discounted from
    the final computation. In general, you can imagine a 3 x 3 filtering operation
    being applied to each element of the 2D array and all such computations will result
    in a new value for that element in the output data array.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you may wonder what is being done to this output array? Remember that
    this array now contains values, which basically shows how big is the change detected
    in a particular element is. And when you obtain a bunch of them in the vicinity,
    then it usually tells you major color changes, that is, edges.
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/4520OT_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With this understanding, you can probably begin to appreciate why we took this
    effort to illustrate the theory behind a concept.
  prefs: []
  type: TYPE_NORMAL
- en: When you want to build non-trivial OpenCL applications for your customers, one
    of the things you have to deal with is learning how to interpret a problem and
    convert it to a solution. And what that means is mostly about formulating an algorithm
    (or picking existing algorithms to suit your case) and verifying that it works.
    Most of the problems you're likely to encounter are going to involve some sort
    of mathematical understanding and your ability to learn about it. You should treat
    this as an adventure!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we''ve armed ourselves with what convolution is in a 2D space, Let''s
    begin by taking a look at how convolution in 2D would work in regular C/C++ code
    with the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This implementation is probably the most direct for the purpose of understanding
    the concept, although it may not be the fastest (since it's not many-core aware).
    But it works, as there are conceptually two major loops where the two outer `fo`r
    loops are for iterating over the entire 2D array space, while the two inner `for`
    loops are for iterating the filter/kernel over the element, that is, convoluting
    and storing the final value into an appropriate output array.
  prefs: []
  type: TYPE_NORMAL
- en: Putting on our parallel algorithm developer hat now, we discover that `statement
    1` appears to be a nice target for work items to execute over. Next, let's take
    a look at how we can take what we've learnt and build the same program in OpenCL.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCL implementation of the Sobel edge filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you've been armed with how convolution actually works, you should be
    able to imagine how our algorithm might look like. Briefly, we will read an input
    image assuming that it's going to be in the Windows BMP format.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next we'll construct the necessary data structures for transporting this image
    file in the OpenCL device for convolution, and once that's done we'll read and
    write the data out to another image file, so that we can compare the two.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Optionally, you can choose to implement this using the `clCreateImage(...)`
    APIs provided by OpenCL, and we'll leave it as an exercise for the reader to make
    the attempt.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, you will be shown with an implementation from what
    is translated, what we have learnt so far. It won't be the most efficient algorithm,
    and that's really not our intention here. Rather, we want to show you how you
    can get this done quickly and we'll let you inject those optimizations which include
    the not withstanding, following data binning, data tiling, shared memory optimization,
    warp / wavefront-level programming, implementing 2D-convolution using fast fourier
    transformations, and so many other features.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A possible avenue from where I derived a lot of the latest techniques about
    solving convolution was by reading academic research papers published by AMD and
    NVIDIA, and also by visiting [gpgpu.org](http://gpgpu.org), [developer.amd.com](http://developer.amd.com),
    [developer.nvidia.com](http://developer.nvidia.com), and [developer.intel.com](http://developer.intel.com).
    Another good resource I can think of are books on image processing and computer
    vision from your favorite local bookstores. Also, books on processor and memory
    structure released by Intel are also good resources if you like.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We only show the code for the kernel found in `Ch6/sobelfilter/sobel_detector.cl`,
    since this is where our algorithm translation will reach its Xenith. And we''ve
    not shown the host code in `Ch6/sobelfilter/SobelFilter.c`, since we believe that
    you would be confident to know what typically resides in there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'An astute reader will probably figure out by reading the code, that the derived
    values for `Gx` and `Gy` should have been as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: But since we know their values will be zero, there is no need for us to include
    the computation inside it. Although we did, it's really a minor optimization.
    It shaved off some GPU processing cycles!
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, the compilation steps are similar to that in `Ch6/sobelfilter/SobelFilter.c`
    with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To execute the program, simply execute the executable file (`SobelFilter`) on
    the `Ch6/sobelfilter` directory, and an output image file named `OutputImage.bmp`
    would be presented (it's the output of reading in `InputImage.bmp` and conducting
    the convolution process against it).
  prefs: []
  type: TYPE_NORMAL
- en: The net effect is that the output contains an image that outlines the edges
    of the original input image, and you can even refer to the picture images in the
    *How it works…* section of this recipe to see how these two images are different
    from one another.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At first, we create a representation of a pixel to represent each of the channels
    in the RGBA fashion. That structure is given a simple name, `uchar4`, where it
    consists of four unsigned char data types which will correctly represent each
    color's range from [0..255] or [0x00..0xFF], since that's how each color's range
    is defined by convention.
  prefs: []
  type: TYPE_NORMAL
- en: We omit the description of the mechanism behind pulling the pixel information
    from the input image to how we construct the final in-memory representation of
    the image. Interested readers can search on the Internet regarding the Windows
    BMP format to understand how we parse the image data or read the source code in
    the `bmp.h` file via the `load` function, and we write out the image using the
    `write` function.
  prefs: []
  type: TYPE_NORMAL
- en: Skipping the OpenCL device memory allocation, since that by now is standard
    fare we arrived quickly at the portion where we look at how the kernel processes
    each pixel of the input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we do that, let''s quickly recall from the kernel launching code how
    many global work-items have been assigned and whether the work-group composition
    is like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`localThreads` is configured to have work-groups of sizes {256,1}, work-items
    processing a portion of the input 2D image data array.'
  prefs: []
  type: TYPE_NORMAL
- en: When the image is loaded into the device memory, the image is processed in blocks.
    Each block has a number of work-items or threads if you process the image. Each
    work-item proceeds the next to perform the convolution process on the center of
    the pixel and also on its eight neighbors. The resultant value generated by each
    work-item will be outputed as pixel value into the device memory. Pictorially,
    the following diagram illustrates what a typical work-item will perform.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You need to watch out and that is we actually used the data type conversion
    function, `convert_float4` to apply our unsigned char data values encapsulated
    within each pixel, which effectively widens the data type so that it doesn't overflow
    when the Sobel operator is applied on them.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, once we have the masked the values we need to compute the magnitude
    of this gradient and the standard way of computing that is to apply ![How it works…](img/4520OT_06_12a.jpg)
    where **Gx** = ![How it works…](img/4520OT_06_11.jpg) and **Gy** = ![How it works…](img/4520OT_06_10.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/4520OT_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Whether this algorithm works, the only way is to check it through an image.
    The following is the side-by-side comparison, where the first image is before
    the Sobel operator is applied and the second one is after it's being applied.
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/4520OT_06_06.jpg)![How it works…](img/4520OT_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: However, there is another nice optimization which we could have done, and it
    would have helped if we understood that a 3 X 3 convolution kernel (for example,
    the Sobel operator) is actually equivalent to the product of two vectors. This
    realization is behind the optimization algorithm also known as separable convolution.
  prefs: []
  type: TYPE_NORMAL
- en: Technically, a two-dimensional filter is considered to be separable if it can
    be expressed as an outer product of two vectors. Considering the Sobel operator
    here, we can actually write ![How it works…](img/4520OT_06_08.jpg) and ![How it
    works…](img/4520OT_06_09.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The superscript T is the transpose of a row vector, which is equivalent to the
    column-vector and vice versa. Note that convolution is itself associative, so
    it doesn't really matter in which way you multiply the vectors against the input
    image matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Why is this important? The main reason is because we actually save processing
    cycles by using this separable convolution kernel. Let's imagine we have a X-by-Y
    image and a convolution kernel of M-by-N. Using the original method, we would
    have conducted XYMN multiples and adds while using the separable convolution technique,
    we would have actually done XY (M + N) multiples and adds. Theoretically speaking,
    applying this to our 3-by-3 convolution kernel we would have increased our performance
    to 50 percent or 1.5 times and when we use a 9-by-9 convolution kernel, we would
    have increased our performance to 81 / 18 = 4.5 or 450 percent.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are going to talk about how you can profile your algorithms and their
    runtimes so that you can make your algorithms not only run faster, but also deepen
    your understanding of how the algorithm works and more often than not, help the
    developer develop a better intuition on how to make better use of the OpenCL device's
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding profiling in OpenCL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Profiling is a relatively simple operation from the perspective of an OpenCL
    developer, since it basically means that he/she wishes to measure how long a particular
    operation took. This is important because during any software development, users
    of the system would often specify the latencies which are considered acceptable,
    and as you develop bigger and more complex systems, profiling the application
    becomes important in helping you understand the bottlenecks of the application.
    The profiling we are going to take is a look done programmatically by the developer
    to explicitly measure the pockets of code. Of course, there is another class of
    profilers which profiles your OpenCL operations on a deeper level with various
    breakdowns on the running times measured and displayed, but that is out of the
    scope of the book. But we encourage readers to download the profilers from AMD
    and Intel to check them out.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While writing this book, AMD has made its OpenCL profiler and a generally available
    debugger named CodeXL found at [http://developer.amd.com/tools-and-sdks/heterogeneous-computing/codexl/](http://developer.amd.com/tools-and-sdks/heterogeneous-computing/codexl/).
    Intel has a similar package offered separately and you can refer to the following
    URL for more details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://software.intel.com/en-us/vcsource/tools/opencl-sdk-2013](http://software.intel.com/en-us/vcsource/tools/opencl-sdk-2013).
    As for NVIDIA GPGPUs, you can only use the APIs provided by OpenCL.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The two operations that OpenCL allows the developer to have such insight into
    their runtimes are data transfer operations and kernel execution operations; the
    times are all measured in nanoseconds.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since all devices cannot resolve to a nanosecond, it's important to determine
    what is the level of resolution, and you can know this by passing the `CL_DEVICE_PROFILING_TIMER_RESOLUTION`
    flag to `clGetDeviceInfo` for the appropriate device ID.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All you have to do is to pass the `CL_QUEUE_PROFILING_ENABLE` flag as part
    of the `properties` argument, when you create the command queue via `clCreateCommandQueue`.
    The API looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the profiling is enabled, the next thing you need to do is to inject OpenCL
    events into areas of the code, where you want to know how the runtimes fare. To
    achieve this, you need to create a `cl_event` variable for the regions of code
    you wish to monitor and associate this variable with one of the following APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data transfer operations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueue{Read|Write|Map}Buffer`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueue{Read|Write|Map}BufferRect`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueue{Read|Write|Map}Image`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueueUnmapMemObject`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueuCopyBuffer`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueueCopyBufferRect`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueueCopyImage`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueueCopyImageToBuffer`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueueCopyBufferToImage`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kernel operations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueueNDRangeKernel`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueueTask`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clEnqueueNativeTask`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The way to obtain the runtimes for these operations is to invoke the `clGetEventProfilingInfo`
    API, passing in one of these flags: `CL_PROFILING_COMMAND_QUEUED`, `CL_PROFILING_COMMAND_SUBMIT`,
    `CL_PROFILING_COMMAND_START`, or `CL_PROFILING_COMMAND_END`. The API looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: To obtain the time spent by the command in the queue, you invoke `clGetEventProfilingInfo`
    with `CL_PROFILING_COMMAND_SUBMIT` once, and at the end of the code region invoke
    `clGetEventProfilingInfo` with `CL_PROFILING_COMMAND_QUEUED` again to get the
    difference in time.
  prefs: []
  type: TYPE_NORMAL
- en: To obtain the duration that the command took to execute, invoke `clGetEventProfilingInfo`
    once with `CL_PROFILING_COMMAND_START` and invoke the same API with `CL_PROFILING_COMMAND_END`,
    from the difference in the runtimes you will obtain the value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a small code snippet which illustrates the basic mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
