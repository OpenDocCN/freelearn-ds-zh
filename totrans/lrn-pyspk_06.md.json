["```py\n    df = spark.createDataFrame(\n        [(12, 10, 3), (1, 4, 2)], \n        ['a', 'b', 'c']) \n    ```", "```py\n    ft.VectorAssembler(inputCols=['a', 'b', 'c'], \n            outputCol='features')\\\n        .transform(df) \\\n        .select('features')\\\n        .collect() \n    ```", "```py\n    [Row(features=DenseVector([12.0, 10.0, 3.0])), \n     Row(features=DenseVector([1.0, 4.0, 2.0]))]\n    ```", "```py\nimport pyspark.sql.types as typ\nlabels = [\n    ('INFANT_ALIVE_AT_REPORT', typ.IntegerType()),\n    ('BIRTH_PLACE', typ.StringType()),\n    ('MOTHER_AGE_YEARS', typ.IntegerType()),\n    ('FATHER_COMBINED_AGE', typ.IntegerType()),\n    ('CIG_BEFORE', typ.IntegerType()),\n    ('CIG_1_TRI', typ.IntegerType()),\n    ('CIG_2_TRI', typ.IntegerType()),\n    ('CIG_3_TRI', typ.IntegerType()),\n    ('MOTHER_HEIGHT_IN', typ.IntegerType()),\n    ('MOTHER_PRE_WEIGHT', typ.IntegerType()),\n    ('MOTHER_DELIVERY_WEIGHT', typ.IntegerType()),\n    ('MOTHER_WEIGHT_GAIN', typ.IntegerType()),\n    ('DIABETES_PRE', typ.IntegerType()),\n    ('DIABETES_GEST', typ.IntegerType()),\n    ('HYP_TENS_PRE', typ.IntegerType()),\n    ('HYP_TENS_GEST', typ.IntegerType()),\n    ('PREV_BIRTH_PRETERM', typ.IntegerType())\n]\nschema = typ.StructType([\n    typ.StructField(e[0], e[1], False) for e in labels\n])\nbirths = spark.read.csv('births_transformed.csv.gz', \n                        header=True, \n                        schema=schema)\n```", "```py\nimport pyspark.ml.feature as ft\n```", "```py\nbirths = births \\\n    .withColumn('BIRTH_PLACE_INT', births['BIRTH_PLACE'] \\\n    .cast(typ.IntegerType()))\n```", "```py\nencoder = ft.OneHotEncoder(\n    inputCol='BIRTH_PLACE_INT', \n    outputCol='BIRTH_PLACE_VEC')\n```", "```py\nfeaturesCreator = ft.VectorAssembler(\n    inputCols=[\n        col[0] \n        for col \n        in labels[2:]] + \\\n    [encoder.getOutputCol()], \n    outputCol='features'\n)\n```", "```py\nimport pyspark.ml.classification as cl\n```", "```py\nlogistic = cl.LogisticRegression(\n    maxIter=10, \n    regParam=0.01, \n    labelCol='INFANT_ALIVE_AT_REPORT')\n```", "```py\nfrom pyspark.ml import Pipeline\n```", "```py\npipeline = Pipeline(stages=[\n        encoder, \n        featuresCreator, \n        logistic\n    ])\n```", "```py\nbirths_train, births_test = births \\\n    .randomSplit([0.7, 0.3], seed=666)\n```", "```py\ntrain, test, val = births.\\\n    randomSplit([0.7, 0.2, 0.1], seed=666)\n```", "```py\nmodel = pipeline.fit(births_train)\ntest_model = model.transform(births_test)\n```", "```py\ntest_model.take(1)\n```", "```py\nimport pyspark.ml.evaluation as ev\n```", "```py\nevaluator = ev.BinaryClassificationEvaluator(\n    rawPredictionCol='probability', \n    labelCol='INFANT_ALIVE_AT_REPORT')\n```", "```py\nprint(evaluator.evaluate(test_model, \n    {evaluator.metricName: 'areaUnderROC'}))\nprint(evaluator.evaluate(test_model, \n   {evaluator.metricName: 'areaUnderPR'}))\n```", "```py\npipelinePath = './infant_oneHotEncoder_Logistic_Pipeline'\npipeline.write().overwrite().save(pipelinePath)\n```", "```py\nloadedPipeline = Pipeline.load(pipelinePath)\nloadedPipeline \\\n    .fit(births_train)\\\n    .transform(births_test)\\\n    .take(1)\n```", "```py\nfrom pyspark.ml import PipelineModel\n\nmodelPath = './infant_oneHotEncoder_Logistic_PipelineModel'\nmodel.write().overwrite().save(modelPath)\n\nloadedPipelineModel = PipelineModel.load(modelPath)\ntest_reloadedModel = loadedPipelineModel.transform(births_test)\n```", "```py\nimport pyspark.ml.tuning as tune\n```", "```py\nlogistic = cl.LogisticRegression(\n    labelCol='INFANT_ALIVE_AT_REPORT')\ngrid = tune.ParamGridBuilder() \\\n    .addGrid(logistic.maxIter,  \n             [2, 10, 50]) \\\n    .addGrid(logistic.regParam, \n             [0.01, 0.05, 0.3]) \\\n    .build()\n```", "```py\nevaluator = ev.BinaryClassificationEvaluator(\n    rawPredictionCol='probability', \n    labelCol='INFANT_ALIVE_AT_REPORT')\n```", "```py\ncv = tune.CrossValidator(\n    estimator=logistic, \n    estimatorParamMaps=grid, \n    evaluator=evaluator\n)\n```", "```py\npipeline = Pipeline(stages=[encoder ,featuresCreator])\ndata_transformer = pipeline.fit(births_train)\n```", "```py\ncvModel = cv.fit(data_transformer.transform(births_train))\n```", "```py\ndata_train = data_transformer \\\n    .transform(births_test)\nresults = cvModel.transform(data_train)\nprint(evaluator.evaluate(results, \n     {evaluator.metricName: 'areaUnderROC'}))\nprint(evaluator.evaluate(results, \n     {evaluator.metricName: 'areaUnderPR'}))\n```", "```py\nresults = [\n    (\n        [\n            {key.name: paramValue} \n            for key, paramValue \n            in zip(\n                params.keys(), \n                params.values())\n        ], metric\n    ) \n    for params, metric \n    in zip(\n        cvModel.getEstimatorParamMaps(), \n        cvModel.avgMetrics\n    )\n]\nsorted(results, \n       key=lambda el: el[1], \n       reverse=True)[0]\n```", "```py\nselector = ft.ChiSqSelector(\n    numTopFeatures=5, \n    featuresCol=featuresCreator.getOutputCol(), \n    outputCol='selectedFeatures',\n    labelCol='INFANT_ALIVE_AT_REPORT'\n)\n```", "```py\nlogistic = cl.LogisticRegression(\n    labelCol='INFANT_ALIVE_AT_REPORT',\n    featuresCol='selectedFeatures'\n)\npipeline = Pipeline(stages=[encoder, featuresCreator, selector])\ndata_transformer = pipeline.fit(births_train)\n```", "```py\ntvs = tune.TrainValidationSplit(\n    estimator=logistic, \n    estimatorParamMaps=grid, \n    evaluator=evaluator\n)\n```", "```py\ntvsModel = tvs.fit(\n    data_transformer \\\n        .transform(births_train)\n)\ndata_train = data_transformer \\\n    .transform(births_test)\nresults = tvsModel.transform(data_train)\nprint(evaluator.evaluate(results, \n     {evaluator.metricName: 'areaUnderROC'}))\nprint(evaluator.evaluate(results, \n     {evaluator.metricName: 'areaUnderPR'}))\n```", "```py\ntext_data = spark.createDataFrame([\n    ['''Machine learning can be applied to a wide variety \n        of data types, such as vectors, text, images, and \n        structured data. This API adopts the DataFrame from \n        Spark SQL in order to support a variety of data\n        types.'''],\n    (...)\n    ['''Columns in a DataFrame are named. The code examples \n        below use names such as \"text,\" \"features,\" and \n        \"label.\"''']\n], ['input'])\n```", "```py\ntokenizer = ft.RegexTokenizer(\n    inputCol='input', \n    outputCol='input_arr', \n    pattern='\\s+|[,.\\\"]')\n```", "```py\nstopwords = ft.StopWordsRemover(\n    inputCol=tokenizer.getOutputCol(), \n    outputCol='input_stop')\n```", "```py\nngram = ft.NGram(n=2, \n    inputCol=stopwords.getOutputCol(), \n    outputCol=\"nGrams\")\npipeline = Pipeline(stages=[tokenizer, stopwords, ngram])\n```", "```py\ndata_ngram = pipeline \\\n    .fit(text_data) \\\n    .transform(text_data)\ndata_ngram.select('nGrams').take(1)\n```", "```py\nimport numpy as np\nx = np.arange(0, 100)\nx = x / 100.0 * np.pi * 4\ny = x * np.sin(x / 1.764) + 20.1234\n```", "```py\nschema = typ.StructType([\n    typ.StructField('continuous_var', \n                    typ.DoubleType(), \n                    False\n   )\n])\ndata = spark.createDataFrame(\n    [[float(e), ] for e in y], \n    schema=schema)\n```", "```py\ndiscretizer = ft.QuantileDiscretizer(\n    numBuckets=5, \n    inputCol='continuous_var', \n    outputCol='discretized')\n```", "```py\ndata_discretized = discretizer.fit(data).transform(data)\n```", "```py\nvectorizer = ft.VectorAssembler(\n    inputCols=['continuous_var'], \n    outputCol= 'continuous_vec')\n```", "```py\nnormalizer = ft.StandardScaler(\n    inputCol=vectorizer.getOutputCol(), \n    outputCol='normalized', \n    withMean=True,\n    withStd=True\n)\npipeline = Pipeline(stages=[vectorizer, normalizer])\ndata_standardized = pipeline.fit(data).transform(data)\n```", "```py\nimport pyspark.sql.functions as func\nbirths = births.withColumn(\n    'INFANT_ALIVE_AT_REPORT', \n    func.col('INFANT_ALIVE_AT_REPORT').cast(typ.DoubleType())\n)\nbirths_train, births_test = births \\\n    .randomSplit([0.7, 0.3], seed=666)\n```", "```py\nclassifier = cl.RandomForestClassifier(\n    numTrees=5, \n    maxDepth=5, \n    labelCol='INFANT_ALIVE_AT_REPORT')\npipeline = Pipeline(\n    stages=[\n        encoder,\n        featuresCreator, \n        classifier])\nmodel = pipeline.fit(births_train)\ntest = model.transform(births_test)\n```", "```py\nevaluator = ev.BinaryClassificationEvaluator(\n    labelCol='INFANT_ALIVE_AT_REPORT')\nprint(evaluator.evaluate(test, \n    {evaluator.metricName: \"areaUnderROC\"}))\nprint(evaluator.evaluate(test, \n    {evaluator.metricName: \"areaUnderPR\"}))\n```", "```py\nclassifier = cl.DecisionTreeClassifier(\n    maxDepth=5, \n    labelCol='INFANT_ALIVE_AT_REPORT')\npipeline = Pipeline(stages=[\n    encoder,\n    featuresCreator, \n    classifier])\nmodel = pipeline.fit(births_train)\ntest = model.transform(births_test)\nevaluator = ev.BinaryClassificationEvaluator(\n    labelCol='INFANT_ALIVE_AT_REPORT')\nprint(evaluator.evaluate(test, \n    {evaluator.metricName: \"areaUnderROC\"}))\nprint(evaluator.evaluate(test, \n    {evaluator.metricName: \"areaUnderPR\"}))\n```", "```py\nimport pyspark.ml.clustering as clus\nkmeans = clus.KMeans(k = 5, \n    featuresCol='features')\npipeline = Pipeline(stages=[\n        assembler,\n        featuresCreator, \n        kmeans]\n)\nmodel = pipeline.fit(births_train)\n```", "```py\ntest = model.transform(births_test)\ntest \\\n    .groupBy('prediction') \\\n    .agg({\n        '*': 'count', \n        'MOTHER_HEIGHT_IN': 'avg'\n    }).collect()\n```", "```py\ntext_data = spark.createDataFrame([\n    ['''To make a computer do anything, you have to write a \n    computer program. To write a computer program, you have \n    to tell the computer, step by step, exactly what you want \n    it to do. The computer then \"executes\" the program, \n    following each step mechanically, to accomplish the end \n    goal. When you are telling the computer what to do, you \n    also get to choose how it's going to do it. That's where \n    computer algorithms come in. The algorithm is the basic \n    technique used to get the job done. Let's follow an \n    example to help get an understanding of the algorithm \n    concept.'''],\n    (...),\n    ['''Australia has over 500 national parks. Over 28 \n    million hectares of land is designated as national \n    parkland, accounting for almost four per cent of \n    Australia's land areas. In addition, a further six per \n    cent of Australia is protected and includes state \n    forests, nature parks and conservation reserves.National \n    parks are usually large areas of land that are protected \n    because they have unspoilt landscapes and a diverse \n    number of native plants and animals. This means that \n    commercial activities such as farming are prohibited and \n    human activity is strictly monitored.''']\n], ['documents'])\n```", "```py\ntokenizer = ft.RegexTokenizer(\n    inputCol='documents', \n    outputCol='input_arr', \n    pattern='\\s+|[,.\\\"]')\nstopwords = ft.StopWordsRemover(\n    inputCol=tokenizer.getOutputCol(), \n    outputCol='input_stop')\n```", "```py\nstringIndexer = ft.CountVectorizer(\n    inputCol=stopwords.getOutputCol(), \n    outputCol=\"input_indexed\")\ntokenized = stopwords \\\n    .transform(\n        tokenizer\\\n            .transform(text_data)\n    )\n\nstringIndexer \\\n    .fit(tokenized)\\\n    .transform(tokenized)\\\n    .select('input_indexed')\\\n    .take(2)\n```", "```py\nclustering = clus.LDA(k=2, \n    optimizer='online', \n    featuresCol=stringIndexer.getOutputCol())\n```", "```py\npipeline = ml.Pipeline(stages=[\n        tokenizer, \n        stopwords,\n        stringIndexer, \n        clustering]\n)\n```", "```py\ntopics = pipeline \\\n    .fit(text_data) \\\n    .transform(text_data)\ntopics.select('topicDistribution').collect()\n```", "```py\nfeatures = ['MOTHER_AGE_YEARS','MOTHER_HEIGHT_IN',\n            'MOTHER_PRE_WEIGHT','DIABETES_PRE',\n            'DIABETES_GEST','HYP_TENS_PRE', \n            'HYP_TENS_GEST', 'PREV_BIRTH_PRETERM',\n            'CIG_BEFORE','CIG_1_TRI', 'CIG_2_TRI', \n            'CIG_3_TRI'\n           ]\n```", "```py\nfeaturesCreator = ft.VectorAssembler(\n    inputCols=[col for col in features[1:]], \n    outputCol='features'\n)\nselector = ft.ChiSqSelector(\n    numTopFeatures=6, \n    outputCol=\"selectedFeatures\", \n    labelCol='MOTHER_WEIGHT_GAIN'\n)\n```", "```py\nimport pyspark.ml.regression as reg\nregressor = reg.GBTRegressor(\n    maxIter=15, \n    maxDepth=3,\n    labelCol='MOTHER_WEIGHT_GAIN')\n```", "```py\npipeline = Pipeline(stages=[\n        featuresCreator, \n        selector,\n        regressor])\nweightGain = pipeline.fit(births_train)\n```", "```py\nevaluator = ev.RegressionEvaluator(\n    predictionCol=\"prediction\", \n    labelCol='MOTHER_WEIGHT_GAIN')\nprint(evaluator.evaluate(\n     weightGain.transform(births_test), \n    {evaluator.metricName: 'r2'}))\n```"]