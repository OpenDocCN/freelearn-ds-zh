- en: 8 Loading Large Datasets beyond the Available RAM in Power BI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned how to read from and write to a CSV file,
    both with Python and in R. When it comes to reading a file, whether you use Power
    BI's standard data import feature or the techniques shown in the previous chapter,
    the main limitation on the file size is due to the amount of RAM available on
    the machine where Power BI Desktop is installed.
  prefs: []
  type: TYPE_NORMAL
- en: In a data enrichment phase, it may be necessary to extract information needed
    for ongoing analysis from very large files (terabytes in size). In these cases,
    it is almost always necessary to implement big data solutions to be able to handle
    such masses of data. Very often, however, it is necessary to import files that
    are slightly larger than the available RAM, in order to extract aggregate information
    and then persist it in a small table for reuse during processing. In such cases,
    it's not necessary to bother with demanding big data platforms, but you can take
    advantage of the flexibility provided by specific packages that implement distributed
    computing systems in both Python and R, without having to resort to Apache Spark-based
    backends.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: A typical analytic scenario using large datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing large datasets with Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing large datasets with R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter requires you to have a working internet connection and **Power
    BI Desktop** already installed on your machine. You must have properly configured
    the R and Python engines and IDEs as outlined in *Chapter 2*, *Configuring R with
    Power BI*, and *Chapter 3*, *Configuring Python with Power BI*.
  prefs: []
  type: TYPE_NORMAL
- en: A typical analytic scenario using large datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most frequent activities of a data scientist is to analyze a dataset
    of information relevant to a business scenario. The objective of the analysis
    is to be able to identify associations and relationships between variables, which
    help in some way to discover new measurable aspects of the business (insights)
    and can then be used to make it grow better. It may be the case that the available
    data may not be sufficient to determine strong associations between variables,
    because any additional variables may not be considered. In this case, attempting
    to obtain new data that is not generated by your business but enriches the context
    of your dataset (a **data augmentation** process) can improve the strength of
    the statistical associations between your variables. Being able to link, for example,
    weather forecast data to a dataset that reports the measurements of the water
    level of a dam certainly introduces significant variables to better interpret
    the phenomenon.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is in this context that you often find yourself having to extract information
    from CSV files downloaded from external data sources. For example, imagine that
    you have been assigned the task of analyzing what factors influenced the profitability
    of a chain of shoe stores located in major airports in the United States from
    1987 to 2012\. The first thing that comes to your mind is that maybe flight delays
    are somehow related to people staying at the airport. If you have to spend time
    at the airport, you definitely have more time to visit the various stores there
    and therefore the chance of you making a purchase increases. So, how do you find
    data on the average airline delay at each US airport for each day of the year?
    Fortunately, the *Research and Innovative Technology Administration (RITA)*, *Bureau
    of Transportation Statistics*, provides aggregated statistics ([http://bit.ly/airline-stats](http://bit.ly/airline-stats))
    and raw data containing flight arrival and departure details for all commercial
    flights within the US ([http://bit.ly/airline-stats-data](http://bit.ly/airline-stats-data)).
    A set of CSV files containing monthly airline data from 1987 to 2012 is already
    collected and zipped by Microsoft, and you can download it directly from this
    link: [http://bit.ly/AirOnTime87to12](http://bit.ly/AirOnTime87to12). If you would
    like more information about the fields in the files, please see the `AirOnTime87to12.dataset.description.txt`
    file.'
  prefs: []
  type: TYPE_NORMAL
- en: The compressed file in question is about 4 GB large and, once unzipped, contains
    many CSV files with detailed data of flights made in the US across months ranging
    from 1987 to 2012, with a total size of 30 GB! Your goal is to calculate the average
    daily flight delay for each origin airport and to save the resulting dataset in
    a CSV file. How do you import all that data in Power BI!? Let's see how to do
    this in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Import large datasets with Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *Chapter 3*, *Configuring Python with Power BI*, we suggested that you install
    some of the most commonly used data management packages in your environment, including
    NumPy, pandas, and scikit-learn. The biggest limitation of these packages is that
    *they cannot handle datasets larger than the RAM of the machine in which they
    are used*, thus they are not able to scale to more than one machine. To comply
    with this limitation, distributed systems based on **Spark**, which has become
    a dominant tool in the big data analysis landscape, are often used. However, the
    move to these systems forces developers to have to rethink already written code
    using an API called **PySpark**, born to use Spark objects with Python. This process
    is generally seen as causing delays in project delivery and causing frustration
    for developers, who master the libraries available for standard Python with much
    more confidence.
  prefs: []
  type: TYPE_NORMAL
- en: In response to the preceding issues, the community developed a new library for
    parallel computing in Python called **Dask** ([https://dask.org/](https://dask.org/)).
    This library provides transparent ways for the developer to scale pandas, scikit-learn,
    and NumPy workflows more natively, with minimal rewriting. Dask APIs are pretty
    much a copy of most of the APIs of those modules, making the developer's job easier.
  prefs: []
  type: TYPE_NORMAL
- en: One of the advantages of Dask is that *you don't need to set up a cluster of
    machines to be able to manipulate 100+ GB datasets*. You just need one of today's
    laptops with a multi-core CPU and 32 GB of RAM to handle them with ease. Therefore,
    thanks to Dask, you can perform analysis of moderately large datasets on your
    own laptop, without incurring the overhead typical of clusters, such as the use
    of Docker images on various nodes and complex debugging.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Evidently, even the Spark team realized the *inconvenient* points born from
    the use of PySpark by developers accustomed to developing with pandas as a data
    wrangling module. For this reason, they have introduced **Koalas** ([https://koalas.readthedocs.io](https://koalas.readthedocs.io)),
    which provides a pandas API on Apache Spark.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The fact remains that Dask has many advantages over Spark in using a distributed
    system on your laptop only. For example, Spark is based on a **Java Virtual Machine**
    (**JVM**) infrastructure, and therefore requires Java and other components to
    be installed, while Dask is written in pure Python. In addition, the use of Dask
    enables a faster transition from your laptop to a cluster on the cloud, which
    can be easily allocated, for example, thanks to Azure. This is made possible thanks
    to the **Dask Cloud Provider** package ([https://cloudprovider.dask.org/](https://cloudprovider.dask.org/)),
    which provides classes to create and manage temporary Dask clusters on various
    cloud platforms. Take a look at the references if you need to create a Dask cluster
    on Azure via Azure Spot Virtual Machines, or by leveraging Azure Machine Learning
    compute clusters (using, for example, NVIDIA RAPIDS for GPU-accelerated data science).
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to the topic at hand, let's then see how to install Dask on your
    laptop.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Dask on your laptop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You will install Dask on the `pbi_powerquery_env` environment, where the pandas
    and NumPy libraries are already installed. This time, it is not enough to simply
    run the `pip install dask` command, because this way you’ll install only core
    parts of Dask. The correct way for Dask users is to install all components. In
    order to display the graph of the execution plan of a Dask operation, a **Graphviz**
    module must also be installed. To do all this, proceed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Open your Anaconda prompt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Switch to your `pbi_powerquery_env` environment, entering this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Enter the following command to install all components of Dask:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Enter the following command to install all components of Graphviz:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You also need to install Graphviz executables in Windows:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [http://www.graphviz.org/download/](http://www.graphviz.org/download/),
    and then download and install the stable Windows install package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During the installation, choose to add Graphviz to the system path for the current
    user.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s explore at this point the structures made available by Dask that allow
    you to extend common interfaces, such as those of NumPy, pandas, and Python iterators,
    to handle objects larger than the available memory.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Dask DataFrame
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **Dask DataFrame** is part of the Dask *Big Data* collections that allow pandas,
    NumPy, and Python iterators to scale easily. In addition to Dask DataFrames, which
    are the counterpart of pandas DataFrames, **Dask Array** (which mimics NumPy),
    **Dask Bag** (which mimics iterators), and **Dask Delayed** (which mimics loops)
    are also part of the collections. However, we will focus on Dask DataFrames, which
    will allow us to achieve the analysis goal set at the beginning of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A Dask DataFrame is nothing more than a set of pandas DataFrames, which can
    reside on disk on a single machine or on multiple nodes in a cluster, allowing
    you to manage datasets larger than the RAM on your laptop. We assume that you
    have already unzipped the CSV files containing data on US flights from 1987 to
    2012, as mentioned at the beginning of this chapter, in the `D:\<your-path>\AirOnTimeCSV`
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you don't have a laptop with enough hardware resources (at least 16 GB of
    RAM), you should import a subset of CSV files first (such as 40–50 files) to test
    the scripts without having to wait excessively long execution times or incurring
    memory errors.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Then, you can create your Dask DataFrame very easily in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that the wildcard `*` character allows the capture of all CSV files contained
    in the folder that are in the form `airOTyyyymm.csv`, where `yyyy` indicates the
    year and `mm` the month number of the flight departure date. Moreover, the encoding
    of CSV files is declared as `latin-1`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Nowhere is it indicated that the downloaded CSV files had such encoding. By
    simply trying to import them without declaring it (and therefore assuming `utf-8`
    by default), loading returns the following strange error: `UnicodeDecodeError:
    ''utf-8'' codec can''t decode byte 0xe4 in position 4: invalid continuation byte`.
    Searching the web, it is easy to find that this kind of error is encoding-related
    and that `latin-1` is the correct one.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Also, it is a good idea to *specify only the columns of interest* via the `usecols`
    parameter in order to limit the columns to be read. This practice also guarantees
    to read only those columns that you are sure are not completely empty, thus avoiding
    reading errors due to the different inferred data type compared to the real one.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It may occur that some columns have a number of null values at the beginning
    and therefore Dask cannot impute the correct data type, as it uses a sample to
    do that. In this case, you should specifically declare the data type of those
    columns by using the `dtype` parameter.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now that the Dask DataFrame has been created, let's see how to use it to extract
    the information we need.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting information from a Dask DataFrame
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you have run the code to read all CSV files, you will have noticed that
    the operation took very little time. Come to think of it, it''s really too little
    time to read 30 GB of data. Could it be that the reading was not successful? The
    secret of most parallel computing frameworks is tied to this very feature: the
    read operation has not been physically performed but has been added to a possible
    queue of operations to be performed when you explicitly request to use the data.
    This concept is known as **lazy evaluation** or **delayed computation**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, your Dask DataFrame can be used in subsequent operations as if it already
    contains the data. In our case, as the average airline delay at each US airport
    for each day of the year is needed, consider using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are a little bit familiar with pandas DataFrame transformations, you
    will notice the use of the same methods for the Dask DataFrame. As with pandas,
    you have to use *double square brackets* to output a DataFrame; otherwise, you
    would get a series with a single pair of brackets (take a look here: [http://bit.ly/pandas-subset-df](http://bit.ly/pandas-subset-df)).
    Moreover, in order to use the indexes created by the `groupby()` method as columns
    in a DataFrame, you need to reset them in the `reset_index()` one (for more details,
    go here: [http://bit.ly/pandas-groupby](http://bit.ly/pandas-groupby)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Executing this piece of code also takes very little time. As you can imagine,
    the averaging operation has been queued after the data reading operation in the
    transformation queue, which in this case is assigned to the `mean_dep_delay_ddf`
    DataFrame. If you want to get a better idea of what the execution plan of the
    transformations queued so far is, you can create a graph to represent it. For
    simplicity, we will implement the graph using only one CSV file as input. These
    are the necessary steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a folder named `AirOnTimeCSVplot`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy only the first CSV file you unzipped earlier into the previous folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open a new Python script and run the following code on Visual Studio Code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `visualize()` method allows you to visualize the graph of the tasks estimated
    by the engine to realize the queued transformations, even before their execution.
    Specifically, the code will generate a PDF file in the same folder where the script
    you ran is located.
  prefs: []
  type: TYPE_NORMAL
- en: Starting at the bottom of *Figure 8.1*, which represents the contents of the
    newly generated PDF file, you can see that the single source CSV file is read
    by the `read-csv` function from two chunks split by the engine. The `dataframe-groupby-count-chunk`
    and `dataframe-groupby-sum-chunk` functions are applied to each chunk, since for
    each tuple defined by the keys of the grouping operation (`YEAR`, `MONTH`, `DAY_OF_MONTH`,
    and `ORIGIN`), we need to know the sum of the delay (`DEP_DELAY`) and the count
    of the occurrences to compute the average. After that, the results of the two
    `dataframe-groupby-sum-chunk` operations on the two chunks are aggregated by the
    `dataframe-groupby-sum-agg` function. Similarly, the `dataframe-groupby-count-agg`
    function aggregates the outputs of the two `dataframe-groupby-count-chunk` operations.
    Once the two DataFrames of sums and counts have been determined, the ratio between
    the two (that is, the mean) is calculated for each grouping using the `truediv`
    function. Finally, the `reset_index` function provides the desired DataFrame,
    the result of the distributed averaging operation.
  prefs: []
  type: TYPE_NORMAL
- en: If you think about it, the famous problem-solving strategy called **Divide and
    Conquer** (also known as **Divide et Impera**) has been adopted. It consists of
    dividing the original problem into smaller and generally simpler subproblems,
    each solved recursively. The solutions of the subproblems are then properly combined
    to obtain the solution of the original problem. If you've had any experience with
    the Hadoop world, the **MapReduce** paradigm follows the same philosophy, which
    was maintained and optimized later by Spark.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Computation underlying task graph](img/file216.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Computation underlying task graph
  prefs: []
  type: TYPE_NORMAL
- en: That said, let's get back to the initial script. We had defined the `mean_dep_delay_ddf`
    Dask DataFrame. All the transformations needed to get the desired result have
    been queued. How do you tell Dask to actually proceed with all the computations?
    You have to explicitly ask for the result via the `compute()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Pay attention to the fact that `compute()` returns *in-memory results*. It converts
    Dask DataFrames to pandas DataFrames, Dask arrays to NumPy arrays, and Dask bags
    to lists. It is a method that should only be invoked when you are certain that
    the result will fit comfortably in your machine's RAM. Otherwise, you can write
    your data on disk using specific methods, such as `to_textfiles()` or `to_parquet()`.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you instantiated a cluster, you could have decided to persist the calculated
    data not on disk but in memory, using the `persist()` method. The result would
    still have been a distributed Dask object but one that references pre-computed
    results distributed over the cluster's memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'To add some magic to your script, you can use the `ProgressBar()` object, which
    allows you to monitor the progress of your computations. Unfortunately, it happens
    that even if the bar reaches 100%, it still takes some time for the workers to
    finish processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – The progress bar on Visual Studio Code](img/file217.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – The progress bar on Visual Studio Code
  prefs: []
  type: TYPE_NORMAL
- en: 'So, don''t give up! Before running the following line of code, keep in mind
    that processing takes approximately 10 minutes on a machine with 32 GB of RAM
    and a processor with 6 cores. The script is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Your laptop will commit all available logical processors to parallelize computations,
    which you will see in the **Logical Processors** view in **Task Manager**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Parallel computations shown in Task Manager](img/file218.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Parallel computations shown in Task Manager
  prefs: []
  type: TYPE_NORMAL
- en: 'When processing is complete, your pandas DataFrame will be available in memory,
    and you can view some rows of it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – First 10 rows of the output pandas DataFrame](img/file219.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – First 10 rows of the output pandas DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: You can find the complete script to create a Dask DataFrame and extract information
    from it in the `01-load-large-dataset-in-python.py` file in the `Chapter08\Python`
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you were able to get the dataset of a few thousand rows on average
    flight delays for each airport of origin and for each day of the year by processing
    a CSV set as large as 30 GB!
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to write the contents of a Dask DataFrame to a CSV file, without
    going through the generation of a pandas DataFrame, you can invoke directly the
    `to_csv()` method, passing the path of the file to generate as a parameter, as
    the following example shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can well imagine, calling the `to_csv()` method triggers the actual execution
    of all queued transformations, just as it did with the `compute()` method, since
    you’re forcing Dask to read the DataFrame content in order to write it to disk.
    For this reason, if you need to generate the CSV file and also create the pandas
    DataFrame to be used later in your code, you should not call first `to_csv()`
    from the Dask DataFrame and then `compute()` to get the pandas DataFrame, because
    you would force the actual execution of the transformations pipeline twice. In
    this case, it is convenient to first generate the pandas DataFrame with `compute()`
    and then generate a CSV or an Excel file from it, as you learned in *Chapter 7*,
    *Logging Data from Power BI to External Sources*.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now try to apply what you have learned so far in Power BI.
  prefs: []
  type: TYPE_NORMAL
- en: Importing a large dataset in Power BI with Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You learned that Power BI can import data using a Python script directly and
    that the data must be organized in a pandas DataFrame in order to be used in the
    data model. Therefore, what you are going to do is develop a script that uses
    the objects illustrated in the previous section in order to instantiate a pandas
    DataFrame containing the data of flight delay averages in the US. From this DataFrame,
    you will then generate a CSV file. Here are the steps required to implement it
    in Power BI:'
  prefs: []
  type: TYPE_NORMAL
- en: Open Power BI Desktop and make sure it is referencing your Python `pbi_powerquery_env`
    environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Get data**, start entering `Python`, and then double-click on **Python
    script**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the content of the `02-load-large-dataset-in-power-bi.py` file into the
    `Chapter08/Python` folder and paste the content into the Python script editor.
    Remember to edit the path to the CSV files (`main_path`) and destination path
    of the CSV file to be generated, and then click **OK**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After about 11 minutes (using a 6-core laptop with 32 GB of RAM), you will
    see the **Navigator** window showing the aggregated data in the `mean_dep_delay_df`
    DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.5 – The mean_dep_delay_df DataFrame loaded in Power BI](img/file220.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 8.5 – The mean_dep_delay_df DataFrame loaded in Power BI
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select the DataFrame and click on **Load**. After that, a **Load** popup will
    appear, and it will remain active for about five more minutes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Final phase of loading data into Power BI](img/file221.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 8.6 – Final phase of loading data into Power BI
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When it disappears, your data is loaded into the data model.Moreover, the script
    also generated the CSV file containing your aggregated data. You’ll find the `mean_dep_delay_df.csv`
    file, sized about 60 MB, in your folder:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Content of the CSV file created from the Python script in Power
    BI](img/file222.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Content of the CSV file created from the Python script in Power
    BI
  prefs: []
  type: TYPE_NORMAL
- en: Impressive! You just imported 30 GB of data to your Power BI Desktop, with just
    a few lines in Python. Congratulations!
  prefs: []
  type: TYPE_NORMAL
- en: Did you know you can also do this in R? Let's see how.
  prefs: []
  type: TYPE_NORMAL
- en: Importing large datasets with R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The same scalability limitations illustrated for Python packages used to manipulate
    data also exist for R packages in the **Tidyverse** ecosystem. Even in R, it is
    not possible to use a dataset larger than the available RAM on the machine. The
    first solution that is adopted in these cases is also to switch to Spark-based
    distributed systems, which provide the **SparkR** language. It provides a distributed
    implementation of the DataFrame you are used to in R, supporting filtering, aggregation,
    and selection operations as you do with the `dplyr` package. For those of us who
    are fans of the Tidyverse world, RStudio actively develops the `sparklyr` package,
    which allows you to use all the functionality of `dplyr`, even for distributed
    DataFrames. However, adopting Spark-based systems to process CSVs that together
    take up little more than the RAM you have available on your machine may be overkill
    because of the overhead introduced by all the Java infrastructure needed to run
    them.
  prefs: []
  type: TYPE_NORMAL
- en: It is in these cases that adopting the `disk.frame` package ([https://github.com/xiaodaigh/disk.frame](https://github.com/xiaodaigh/disk.frame))
    is the winning approach. What `disk.frame` allows you to do is to divide a dataset
    larger than RAM into chunks and store each chunk in a separate file within a folder.
    **Chunk files** are serializations of DataFrames smaller than the initial one
    in the compressed `fst` format, introduced by the `fst` package ([https://github.com/fstpackage/fst](https://github.com/fstpackage/fst)).
    It is specifically designed to unleash the *speed and parallelism of solid-state
    disks* that are easily found in modern computers. DataFrames stored in the `fst`
    format have full random access, both column and row, and are therefore able to
    benefit from parallel computations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you want to benefit from the parallelism introduced by the `disk.frame` package,
    you must necessarily persist the source CSV files on an SSD disk. If you don't
    have it, and you use an HDD disk, your processing will take too long, and it won't
    be worthwhile.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can define the degree of parallelism with which `disk.frame` will work by
    indicating the number of parallel workers you want to use. For each worker, it
    will create an **R session** using the parallelism mechanism provided by the `future`
    package.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how to use `disk.frame` in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Installing disk.frame on your laptop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, you need to install the `disk.frame` package in your R engine:'
  prefs: []
  type: TYPE_NORMAL
- en: Open RStudio and make sure it is referencing your latest CRAN (Comprehensive
    R Archive Network) R engine (in our case, version 4.0.2).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the console prompt and enter this command: `install.packages("disk.frame")`.
    Make sure you install the latest version of the package (in our case, version
    0.5.0).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, you are ready to test it on RStudio.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a disk.frame instance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to create a `disk.frame` instance (a disk-based DataFrame), you must
    first define the number of parallel workers you want to initialize. Obviously,
    the higher the number of workers, the faster the `disk.frame` instance will be
    created. But you cannot initialize any large number of workers, because the number
    of parallel threads you can use depends on the processor of your machine. Also,
    you can define a maximum limit to the amount of data you can exchange from one
    worker to another. For this reason, at the beginning of the script that uses `disk.frame`,
    you will find the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `availableCores()` function calculates the number of logical processors
    available on your machine. Usually, it’s best practice to leave one out for computationally
    demanding tasks, so that the machine doesn't become unresponsive. The `setup_disk.frame()`
    function creates the cluster of workers needed for the computations. Furthermore,
    there is no limit to the amount of data that can be exchanged between workers
    (`Inf` stands for **infinite**).
  prefs: []
  type: TYPE_NORMAL
- en: We assume that you have already unzipped the CSV files containing data on US
    flights from 1987 to 2012, as mentioned at the beginning of this chapter, in the
    `D:\<your-path>\AirOnTimeCSV` folder.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you don't have a laptop with enough hardware resources, you should import
    a subset of CSV files first (such as 40–50 files) to test the scripts without
    having to wait excessively long execution times.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'You have to define a list of paths for each CSV file using the `list.files()`
    function, and then you can proceed with the creation of `disk.frame` using the
    `csv_to_disk.frame()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The creation of a `disk.frame` instance consists of two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The process creates as many chunks as there are CSV files for each allocated
    worker in temporary folders.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chunks are aggregated and persisted in the output folder (defined by the `outdir`
    parameter) associated with `disk.frame`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In short, `disk.frame` is a folder, preferably with a `.df` extension, that
    contains the aggregated chunks (`.fst` files) generated from the source files.
    As you've already seen with Dask, it is a good idea to *specify only the columns
    of interest* via the `select` parameter in order to limit the columns to be read.
    This practice also guarantees to read only those columns you are sure are not
    completely empty, thus avoiding reading errors due to the different inferred data
    type compared to the real one.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It may occur that some columns have a number of null values at the beginning
    and therefore `disk.frame` cannot impute the correct data type, as it uses a sample
    to do that. In this case, you should specifically declare the data type of those
    columns using the `colClasses` parameter.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note that once a value is assigned to the `create_dkf_exec_time` variable, the
    round brackets print it on the screen, all in one line. The creation time is about
    1 minute and 17 seconds on a machine with 6 cores and 32 GB, using 11 workers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the creation of `disk.frame` is complete, the multiple R sessions that
    make up the cluster could remain active. In order to force their disconnection,
    it is useful to invoke the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In this way, the machine's resources are completely released.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A correctly created `disk.frame` can always be referenced later using the `disk.frame()`
    function by passing it to the path of the `.df` folder, without having to create
    it again.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can find the complete script to create `disk.frame` in the `01-create-diskframe-in-r.R`
    file into the `Chapter08\R` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see how to get information from the newly created `disk.frame`.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting information from disk.frame
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we are interested in getting the average airline delay at each US airport
    for each day of the year, a grouping operation is required on the entire `disk.frame`.
    In order to optimize the execution time, in this case it is also necessary to
    instantiate a cluster of workers that will read in parallel the information from
    the newly created `disk.frame`. You can transform the `disk.frame` instance using
    the syntax you know from the `dplyr` package. Moreover, in order to optimize the
    machine resources, you will force the engine to read only the columns strictly
    necessary to the requested computation using the `srckeep()` function from the
    `disk.frame` package. In our case, it would be unnecessary to select the columns,
    since when creating the `disk.frame` instance we kept only those strictly necessary,
    but the following recommendation applies.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It’s good practice to always use the `srckeep()` function in aggregate data
    extraction scripts, because if the `disk.frame` instance had all the initial columns,
    the operation would cause a machine crash.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The following code should be used to extract aggregated data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Again, as already seen for Dask in Python, there are functions that collect
    all the queued transformations and trigger the execution of calculations. In the
    case of `disk.frame`, the function is `collect()`. The duration of this operation
    is about 20 minutes with a 32 GB machine with 6 cores, using 11 workers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The end result is a tibble containing the desired air delay averages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – First rows of the tibble containing the delay averages](img/file223.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – First rows of the tibble containing the delay averages
  prefs: []
  type: TYPE_NORMAL
- en: Also in this case, you were able to get the dataset of a few thousand rows on
    average flight delays by processing a CSV set as large as 30 GB!
  prefs: []
  type: TYPE_NORMAL
- en: You can find the complete script to extract aggregated data from `disk.frame`
    in the `02-extract-info-from-diskframe-in-r.R` file in the `Chapter08\R` folder.
  prefs: []
  type: TYPE_NORMAL
- en: The following common-sense observation applies here as well.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If the results of previous time-consuming processing are to be reused often,
    it is best to *persist them on disk in a reusable format*. In this way, it will
    be avoidedyou can avoid to executinge again all the onerous processing of the
    queued transformations.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A `disk.frame` instance doesn’t have direct methods, like a Dask DataFrame,
    that allow its contents to be written to disk downstream of all queued transformations.
    Since the result is a tibble, which for all intents and purposes is a DataFrame,
    you can write it to disk following the directions you learned in *Chapter 7*,
    *Logging Data from Power BI to External Sources*.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's see how you can apply what you've learned so far in Power BI.
  prefs: []
  type: TYPE_NORMAL
- en: Importing a large dataset in Power BI with R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The optimal solution to load a dataset larger than the available RAM with R
    via Power BI would be to be able to create the `disk.frame` instance directly
    via an R script in Power BI itself.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unfortunately, the operation of creating a `disk.frame` instance from various
    CSV files *triggers an error* in Power BI during the second phase of the process
    (the aggregation of various chunks into the `disk.frame` folder).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Therefore, in order to extract information from a `disk.frame` instance in
    Power BI, you must first create it in RStudio (or any other IDE or any automated
    script). Once the `disk.frame` instance has been created, it is then possible
    to use it in Power BI as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you have correctly created your `disk.frame` instance using RStudio
    by running the code contained in the `01-create-diskframe-in-r.R` file in the
    `Chapter08/R` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open Power BI Desktop and make sure it is referencing your latest CRAN R engine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Get data**, start entering `script`, and then double-click on **R
    script**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the content of the `03-extract-info-from-diskframe-in-power-bi.R` file
    into the `Chapter08\R` folder and paste it into the R script editor. Remember
    to edit the destination path of the CSV file to be generated, and then click **OK**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After about 30 minutes (using a 6-core laptop with 32 GB of RAM), you will
    see the **Navigator** window showing the aggregated data in the `mean_dep_delay_df`
    DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.9 – The mean_dep_delay_df DataFrame loaded in Power BI](img/file224.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 8.9 – The mean_dep_delay_df DataFrame loaded in Power BI
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select the DataFrame and click on **Load**. After that, a **Load** popup will
    appear, and it will remain active for about 10 more minutes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Final phase of loading data into Power BI](img/file225.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 8.10 – Final phase of loading data into Power BI
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When it disappears, your data is loaded into the data model.Moreover, the script
    also generated the CSV file containing your aggregated data. You’ll find the `mean_dep_delay_df.csv`
    file, sized about 60 MB, in your folder:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Content of the CSV file created from the R script in Power
    BI](img/file226.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – Content of the CSV file created from the R script in Power BI
  prefs: []
  type: TYPE_NORMAL
- en: Awesome! Would you ever have thought of importing 30 GB of data into your Power
    BI Desktop to extract information from it in R? Well, you just did it with just
    a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, you learned how to import datasets larger than the RAM available
    to your laptop into Python and R. You've also applied this knowledge to import
    a set of CSV files that, in total, take up more than the available RAM on your
    machine into Power BI Desktop.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to enrich your data with external information
    extracted from web services made available to users.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For additional reading, check out the following books and articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Create Dask clusters on Azure using Azure (Spot) VMs* ([https://cloudprovider.dask.org/en/latest/azure.html](https://cloudprovider.dask.org/en/latest/azure.html))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Accelerated Machine Learning at Scale with NVIDIA RAPIDS on Microsoft Azure
    ML with Dask* ([https://github.com/drabastomek/GTC/tree/master/SJ_2020/workshop](https://github.com/drabastomek/GTC/tree/master/SJ_2020/workshop))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
