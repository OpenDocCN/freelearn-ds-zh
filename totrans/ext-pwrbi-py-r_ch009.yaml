- en: 8 Loading Large Datasets beyond the Available RAM in Power BI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Power BI中加载超过可用RAM的大型数据集
- en: In the previous chapter, you learned how to read from and write to a CSV file,
    both with Python and in R. When it comes to reading a file, whether you use Power
    BI's standard data import feature or the techniques shown in the previous chapter,
    the main limitation on the file size is due to the amount of RAM available on
    the machine where Power BI Desktop is installed.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了如何使用Python和R从CSV文件中读取和写入数据。当涉及到读取文件时，无论你使用Power BI的标准数据导入功能还是上一章中展示的技术，文件大小的主要限制是由于Power
    BI Desktop安装的机器上可用的RAM量。
- en: In a data enrichment phase, it may be necessary to extract information needed
    for ongoing analysis from very large files (terabytes in size). In these cases,
    it is almost always necessary to implement big data solutions to be able to handle
    such masses of data. Very often, however, it is necessary to import files that
    are slightly larger than the available RAM, in order to extract aggregate information
    and then persist it in a small table for reuse during processing. In such cases,
    it's not necessary to bother with demanding big data platforms, but you can take
    advantage of the flexibility provided by specific packages that implement distributed
    computing systems in both Python and R, without having to resort to Apache Spark-based
    backends.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据丰富阶段，可能需要从非常大的文件（以TB为单位的大小）中提取用于持续分析所需的信息。在这些情况下，几乎总是需要实施大数据解决方案来处理如此大量的数据。然而，非常常见的情况是需要导入比可用RAM稍大的文件，以便提取汇总信息，然后将其持久化到一个小表中，以便在处理过程中重复使用。在这种情况下，没有必要烦恼于要求苛刻的大数据平台，但可以利用Python和R中实现分布式计算系统的特定包提供的灵活性，而无需求助于基于Apache
    Spark的后端。
- en: 'In this chapter, you will learn the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习以下主题：
- en: A typical analytic scenario using large datasets
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用大型数据集的典型分析场景
- en: Importing large datasets with Python
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python导入大型数据集
- en: Importing large datasets with R
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用R导入大型数据集
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter requires you to have a working internet connection and **Power
    BI Desktop** already installed on your machine. You must have properly configured
    the R and Python engines and IDEs as outlined in *Chapter 2*, *Configuring R with
    Power BI*, and *Chapter 3*, *Configuring Python with Power BI*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求你有一个工作的互联网连接，并且**Power BI Desktop**已经安装在你的机器上。你必须已经按照*第二章*、*配置Power BI中的R*和*第三章*、*配置Power
    BI中的Python*中概述的方式正确配置了R和Python引擎和IDE。
- en: A typical analytic scenario using large datasets
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用大型数据集的典型分析场景
- en: One of the most frequent activities of a data scientist is to analyze a dataset
    of information relevant to a business scenario. The objective of the analysis
    is to be able to identify associations and relationships between variables, which
    help in some way to discover new measurable aspects of the business (insights)
    and can then be used to make it grow better. It may be the case that the available
    data may not be sufficient to determine strong associations between variables,
    because any additional variables may not be considered. In this case, attempting
    to obtain new data that is not generated by your business but enriches the context
    of your dataset (a **data augmentation** process) can improve the strength of
    the statistical associations between your variables. Being able to link, for example,
    weather forecast data to a dataset that reports the measurements of the water
    level of a dam certainly introduces significant variables to better interpret
    the phenomenon.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家最频繁的活动之一是分析与业务场景相关的信息数据集。分析的目标是能够识别变量之间的关联和关系，这有助于以某种方式发现业务的新可衡量方面（见解），然后可以用来使业务更好地增长。可能的情况是，可用的数据可能不足以确定变量之间的强关联，因为任何额外的变量可能都没有被考虑。在这种情况下，尝试获取新的数据，这些数据不是由你的业务生成的，但可以丰富你的数据集的背景（一个**数据增强**过程），可以提高你变量之间统计关联的强度。例如，能够将天气预报数据与报告大坝水位测量的数据集联系起来，无疑引入了重要的变量，以更好地解释现象。
- en: 'It is in this context that you often find yourself having to extract information
    from CSV files downloaded from external data sources. For example, imagine that
    you have been assigned the task of analyzing what factors influenced the profitability
    of a chain of shoe stores located in major airports in the United States from
    1987 to 2012\. The first thing that comes to your mind is that maybe flight delays
    are somehow related to people staying at the airport. If you have to spend time
    at the airport, you definitely have more time to visit the various stores there
    and therefore the chance of you making a purchase increases. So, how do you find
    data on the average airline delay at each US airport for each day of the year?
    Fortunately, the *Research and Innovative Technology Administration (RITA)*, *Bureau
    of Transportation Statistics*, provides aggregated statistics ([http://bit.ly/airline-stats](http://bit.ly/airline-stats))
    and raw data containing flight arrival and departure details for all commercial
    flights within the US ([http://bit.ly/airline-stats-data](http://bit.ly/airline-stats-data)).
    A set of CSV files containing monthly airline data from 1987 to 2012 is already
    collected and zipped by Microsoft, and you can download it directly from this
    link: [http://bit.ly/AirOnTime87to12](http://bit.ly/AirOnTime87to12). If you would
    like more information about the fields in the files, please see the `AirOnTime87to12.dataset.description.txt`
    file.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你经常发现自己需要从外部数据源下载的 CSV 文件中提取信息。例如，假设你被分配了一个分析从 1987 年到 2012 年在美国主要机场的鞋店连锁店的盈利性受哪些因素影响的任务。首先出现在你脑海中的可能是，航班延误可能与人们留在机场有关。如果你必须花时间在机场，你肯定有更多的时间去参观那里的各种商店，因此你购买的机会也会增加。那么，你如何找到每个美国机场每年每一天的平均航空公司延误数据呢？幸运的是，*研究和创新技术管理局
    (RITA)*，*运输统计局*提供了包含美国所有商业航班到达和出发详细信息的汇总统计数据([http://bit.ly/airline-stats](http://bit.ly/airline-stats))和原始数据([http://bit.ly/airline-stats-data](http://bit.ly/airline-stats-data))。一套包含从
    1987 年到 2012 年每月航空公司数据的 CSV 文件已经由微软收集并压缩，你可以直接从以下链接下载：[http://bit.ly/AirOnTime87to12](http://bit.ly/AirOnTime87to12)。如果你想了解更多关于文件字段的信息，请参阅
    `AirOnTime87to12.dataset.description.txt` 文件。
- en: The compressed file in question is about 4 GB large and, once unzipped, contains
    many CSV files with detailed data of flights made in the US across months ranging
    from 1987 to 2012, with a total size of 30 GB! Your goal is to calculate the average
    daily flight delay for each origin airport and to save the resulting dataset in
    a CSV file. How do you import all that data in Power BI!? Let's see how to do
    this in Python.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 有关的问题压缩文件大约有 4 GB 大小，一旦解压，包含从 1987 年到 2012 年间在美国完成的许多 CSV 文件，详细记录了航班数据，总大小为
    30 GB！你的目标是计算每个出发机场的平均每日航班延误，并将结果数据集保存为 CSV 文件。你如何在 Power BI 中导入所有这些数据呢？让我们看看如何在
    Python 中完成这项任务。
- en: Import large datasets with Python
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Python 导入大型数据集
- en: In *Chapter 3*, *Configuring Python with Power BI*, we suggested that you install
    some of the most commonly used data management packages in your environment, including
    NumPy, pandas, and scikit-learn. The biggest limitation of these packages is that
    *they cannot handle datasets larger than the RAM of the machine in which they
    are used*, thus they are not able to scale to more than one machine. To comply
    with this limitation, distributed systems based on **Spark**, which has become
    a dominant tool in the big data analysis landscape, are often used. However, the
    move to these systems forces developers to have to rethink already written code
    using an API called **PySpark**, born to use Spark objects with Python. This process
    is generally seen as causing delays in project delivery and causing frustration
    for developers, who master the libraries available for standard Python with much
    more confidence.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第 3 章*，*配置 Power BI 中的 Python* 中，我们建议你在环境中安装一些最常用的数据管理包，包括 NumPy、pandas 和
    scikit-learn。这些包的最大限制是它们*无法处理大于它们所使用机器 RAM 的数据集*，因此它们无法扩展到多台机器。为了遵守这一限制，通常使用基于
    **Spark** 的分布式系统，Spark 已成为大数据分析领域的主导工具。然而，转向这些系统迫使开发者必须重新思考已经用 API 编写的代码，该 API
    称为 **PySpark**，它旨在使用 Python 对 Spark 对象进行操作。这个过程通常被视为导致项目交付延迟，并给开发者带来挫败感，因为开发者对标准
    Python 中可用的库有更大的信心。
- en: In response to the preceding issues, the community developed a new library for
    parallel computing in Python called **Dask** ([https://dask.org/](https://dask.org/)).
    This library provides transparent ways for the developer to scale pandas, scikit-learn,
    and NumPy workflows more natively, with minimal rewriting. Dask APIs are pretty
    much a copy of most of the APIs of those modules, making the developer's job easier.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 针对前面提到的问题，社区开发了一个名为 **Dask** 的新库，用于 Python 的并行计算（[https://dask.org/](https://dask.org/))。这个库为开发者提供了透明的方式来更原生地扩展
    pandas、scikit-learn 和 NumPy 工作流程，而无需进行大量的重写。Dask API 几乎是那些模块 API 的复制品，这使得开发者的工作更加容易。
- en: One of the advantages of Dask is that *you don't need to set up a cluster of
    machines to be able to manipulate 100+ GB datasets*. You just need one of today's
    laptops with a multi-core CPU and 32 GB of RAM to handle them with ease. Therefore,
    thanks to Dask, you can perform analysis of moderately large datasets on your
    own laptop, without incurring the overhead typical of clusters, such as the use
    of Docker images on various nodes and complex debugging.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 的一大优势是*你不需要设置一个由多台机器组成的集群来操作 100+ GB 的数据集*。你只需要一台配备多核 CPU 和 32 GB RAM 的笔记本电脑就可以轻松处理它们。因此，得益于
    Dask，你可以在自己的笔记本电脑上对中等规模的数据集进行分析，而无需承担集群的典型开销，例如在各个节点上使用 Docker 镜像和复杂的调试。
- en: '**Important Note**'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**重要提示**'
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Evidently, even the Spark team realized the *inconvenient* points born from
    the use of PySpark by developers accustomed to developing with pandas as a data
    wrangling module. For this reason, they have introduced **Koalas** ([https://koalas.readthedocs.io](https://koalas.readthedocs.io)),
    which provides a pandas API on Apache Spark.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 显然，即使是 Spark 团队也意识到了开发者使用 PySpark 时，习惯于以 pandas 作为数据处理模块进行开发所带来的不便。因此，他们推出了
    **Koalas**（[https://koalas.readthedocs.io](https://koalas.readthedocs.io)），它为
    Apache Spark 提供了 pandas API。
- en: The fact remains that Dask has many advantages over Spark in using a distributed
    system on your laptop only. For example, Spark is based on a **Java Virtual Machine**
    (**JVM**) infrastructure, and therefore requires Java and other components to
    be installed, while Dask is written in pure Python. In addition, the use of Dask
    enables a faster transition from your laptop to a cluster on the cloud, which
    can be easily allocated, for example, thanks to Azure. This is made possible thanks
    to the **Dask Cloud Provider** package ([https://cloudprovider.dask.org/](https://cloudprovider.dask.org/)),
    which provides classes to create and manage temporary Dask clusters on various
    cloud platforms. Take a look at the references if you need to create a Dask cluster
    on Azure via Azure Spot Virtual Machines, or by leveraging Azure Machine Learning
    compute clusters (using, for example, NVIDIA RAPIDS for GPU-accelerated data science).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 事实仍然是，Dask 在仅使用笔记本电脑上的分布式系统方面比 Spark 具有许多优势。例如，Spark 基于一个 **Java 虚拟机**（**JVM**）基础设施，因此需要安装
    Java 和其他组件，而 Dask 则是用纯 Python 编写的。此外，使用 Dask 可以更快地从笔记本电脑过渡到云端的集群，这可以通过 Azure 等平台轻松分配。这一切都得益于
    **Dask Cloud Provider** 包（[https://cloudprovider.dask.org/](https://cloudprovider.dask.org/)），它提供了在各个云平台上创建和管理临时
    Dask 集群的类。如果你需要通过 Azure Spot 虚拟机或在 Azure 机器学习计算集群（例如使用 NVIDIA RAPIDS 进行 GPU 加速的数据科学）中利用
    Azure 机器学习来创建 Dask 集群，请查看相关参考。
- en: Coming back to the topic at hand, let's then see how to install Dask on your
    laptop.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的话题，让我们看看如何在你的笔记本电脑上安装 Dask。
- en: Installing Dask on your laptop
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在你的笔记本电脑上安装 Dask
- en: 'You will install Dask on the `pbi_powerquery_env` environment, where the pandas
    and NumPy libraries are already installed. This time, it is not enough to simply
    run the `pip install dask` command, because this way you’ll install only core
    parts of Dask. The correct way for Dask users is to install all components. In
    order to display the graph of the execution plan of a Dask operation, a **Graphviz**
    module must also be installed. To do all this, proceed as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在已经安装了 pandas 和 NumPy 库的 `pbi_powerquery_env` 环境上安装 Dask。这次，仅仅运行 `pip install
    dask` 命令是不够的，因为这样你只会安装 Dask 的核心部分。对于 Dask 用户来说，正确的方式是安装所有组件。为了显示 Dask 操作执行计划的图形，还必须安装
    **Graphviz** 模块。为了完成所有这些，请按照以下步骤操作：
- en: Open your Anaconda prompt.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你的 Anaconda 提示符。
- en: 'Switch to your `pbi_powerquery_env` environment, entering this command:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到你的 `pbi_powerquery_env` 环境，输入以下命令：
- en: '[PRE0]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Enter the following command to install all components of Dask:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入以下命令来安装 Dask 的所有组件：
- en: '[PRE1]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Enter the following command to install all components of Graphviz:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入以下命令来安装 Graphviz 的所有组件：
- en: '[PRE2]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You also need to install Graphviz executables in Windows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要在Windows中安装Graphviz可执行文件：
- en: Go to [http://www.graphviz.org/download/](http://www.graphviz.org/download/),
    and then download and install the stable Windows install package.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问[http://www.graphviz.org/download/](http://www.graphviz.org/download/)，然后下载并安装稳定的Windows安装包。
- en: During the installation, choose to add Graphviz to the system path for the current
    user.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在安装过程中，选择将Graphviz添加到当前用户的系统路径中。
- en: Let’s explore at this point the structures made available by Dask that allow
    you to extend common interfaces, such as those of NumPy, pandas, and Python iterators,
    to handle objects larger than the available memory.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，让我们探讨Dask提供的结构，这些结构允许你扩展常见的接口，例如NumPy、pandas和Python迭代器，以处理比可用内存更大的对象。
- en: Creating a Dask DataFrame
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建Dask DataFrame
- en: A **Dask DataFrame** is part of the Dask *Big Data* collections that allow pandas,
    NumPy, and Python iterators to scale easily. In addition to Dask DataFrames, which
    are the counterpart of pandas DataFrames, **Dask Array** (which mimics NumPy),
    **Dask Bag** (which mimics iterators), and **Dask Delayed** (which mimics loops)
    are also part of the collections. However, we will focus on Dask DataFrames, which
    will allow us to achieve the analysis goal set at the beginning of the chapter.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**Dask DataFrame** 是Dask *大数据* 集合的一部分，允许pandas、NumPy和Python迭代器轻松扩展。除了Dask DataFrame，它是pandas
    DataFrame的对应物之外，**Dask Array**（模仿NumPy）、**Dask Bag**（模仿迭代器）和**Dask Delayed**（模仿循环）也是集合的一部分。然而，我们将专注于Dask
    DataFrame，这将使我们能够实现本章开头设定的分析目标。'
- en: A Dask DataFrame is nothing more than a set of pandas DataFrames, which can
    reside on disk on a single machine or on multiple nodes in a cluster, allowing
    you to manage datasets larger than the RAM on your laptop. We assume that you
    have already unzipped the CSV files containing data on US flights from 1987 to
    2012, as mentioned at the beginning of this chapter, in the `D:\<your-path>\AirOnTimeCSV`
    folder.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrame实际上不过是一组pandas DataFrame，它们可以驻留在单个机器的磁盘上，或者驻留在集群的多个节点上，允许你管理比笔记本电脑RAM更大的数据集。我们假设你已经解压了本章开头提到的包含1987年至2012年美国航班数据的CSV文件，并将其放置在`D:\<your-path>\AirOnTimeCSV`文件夹中。
- en: '**Important Note**'
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**重要提示**'
- en: ''
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you don't have a laptop with enough hardware resources (at least 16 GB of
    RAM), you should import a subset of CSV files first (such as 40–50 files) to test
    the scripts without having to wait excessively long execution times or incurring
    memory errors.
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你的笔记本电脑没有足够的硬件资源（至少16 GB的RAM），你应该首先导入CSV文件的一个子集（例如40-50个文件）来测试脚本，以免等待过长的执行时间或发生内存错误。
- en: 'Then, you can create your Dask DataFrame very easily in the following way:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以非常容易地以下述方式创建你的Dask DataFrame：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the wildcard `*` character allows the capture of all CSV files contained
    in the folder that are in the form `airOTyyyymm.csv`, where `yyyy` indicates the
    year and `mm` the month number of the flight departure date. Moreover, the encoding
    of CSV files is declared as `latin-1`.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到通配符`*`字符允许捕获文件夹中所有形式为`airOTyyyymm.csv`的CSV文件，其中`yyyy`表示年份，`mm`表示航班出发日期的月份。此外，CSV文件的编码声明为`latin-1`。
- en: '**Important Note**'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**重要提示**'
- en: ''
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Nowhere is it indicated that the downloaded CSV files had such encoding. By
    simply trying to import them without declaring it (and therefore assuming `utf-8`
    by default), loading returns the following strange error: `UnicodeDecodeError:
    ''utf-8'' codec can''t decode byte 0xe4 in position 4: invalid continuation byte`.
    Searching the web, it is easy to find that this kind of error is encoding-related
    and that `latin-1` is the correct one.'
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '下载的CSV文件没有指示其编码。简单地尝试导入它们而不声明（因此默认假设为`utf-8`），加载会返回以下奇怪的错误：`UnicodeDecodeError:
    ''utf-8'' codec can''t decode byte 0xe4 in position 4: invalid continuation byte`。在网上搜索，很容易发现这种错误与编码有关，而`latin-1`是正确的编码。'
- en: Also, it is a good idea to *specify only the columns of interest* via the `usecols`
    parameter in order to limit the columns to be read. This practice also guarantees
    to read only those columns that you are sure are not completely empty, thus avoiding
    reading errors due to the different inferred data type compared to the real one.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过`usecols`参数仅指定感兴趣的列是一个好主意，以限制要读取的列。这种做法还保证了只读取你确信不是完全空的列，从而避免由于与实际类型不同的推断数据类型而导致的读取错误。
- en: '**Important Note**'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**重要提示**'
- en: ''
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It may occur that some columns have a number of null values at the beginning
    and therefore Dask cannot impute the correct data type, as it uses a sample to
    do that. In this case, you should specifically declare the data type of those
    columns by using the `dtype` parameter.
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 可能会发生某些列在开始时具有许多空值，因此 Dask 无法推断正确的数据类型，因为它使用样本来做到这一点。在这种情况下，你应该通过使用 `dtype`
    参数特别声明这些列的数据类型。
- en: Now that the Dask DataFrame has been created, let's see how to use it to extract
    the information we need.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在Dask DataFrame已经创建，让我们看看如何使用它来提取我们所需的信息。
- en: Extracting information from a Dask DataFrame
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从 Dask DataFrame 中提取信息
- en: 'If you have run the code to read all CSV files, you will have noticed that
    the operation took very little time. Come to think of it, it''s really too little
    time to read 30 GB of data. Could it be that the reading was not successful? The
    secret of most parallel computing frameworks is tied to this very feature: the
    read operation has not been physically performed but has been added to a possible
    queue of operations to be performed when you explicitly request to use the data.
    This concept is known as **lazy evaluation** or **delayed computation**.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经运行了读取所有 CSV 文件的代码，你会注意到操作花费了非常少的时间。回想起来，这真的是读取 30 GB 数据所需的时间太少了。难道读取没有成功吗？大多数并行计算框架的秘密就与这个特性紧密相连：读取操作并没有真正执行，而是被添加到了一个可能的操作队列中，当你明确请求使用数据时，这些操作将被执行。这个概念被称为
    **延迟评估** 或 **延迟计算**。
- en: 'Then, your Dask DataFrame can be used in subsequent operations as if it already
    contains the data. In our case, as the average airline delay at each US airport
    for each day of the year is needed, consider using the following code:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你的 Dask DataFrame 可以在后续操作中使用，就像它已经包含数据一样。在我们的案例中，由于需要计算美国每个机场每年每一天的平均航空延误，考虑使用以下代码：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If you are a little bit familiar with pandas DataFrame transformations, you
    will notice the use of the same methods for the Dask DataFrame. As with pandas,
    you have to use *double square brackets* to output a DataFrame; otherwise, you
    would get a series with a single pair of brackets (take a look here: [http://bit.ly/pandas-subset-df](http://bit.ly/pandas-subset-df)).
    Moreover, in order to use the indexes created by the `groupby()` method as columns
    in a DataFrame, you need to reset them in the `reset_index()` one (for more details,
    go here: [http://bit.ly/pandas-groupby](http://bit.ly/pandas-groupby)).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你稍微熟悉 pandas DataFrame 的转换，你会注意到 Dask DataFrame 也使用了相同的方法。与 pandas 一样，你必须使用
    *双方括号* 来输出 DataFrame；否则，你会得到一个单对括号的一维序列（查看这里：[http://bit.ly/pandas-subset-df](http://bit.ly/pandas-subset-df)）。此外，为了将
    `groupby()` 方法创建的索引作为 DataFrame 的列使用，你需要在 `reset_index()` 方法中重置它们（更多详情，请参阅：[http://bit.ly/pandas-groupby](http://bit.ly/pandas-groupby)）。
- en: 'Executing this piece of code also takes very little time. As you can imagine,
    the averaging operation has been queued after the data reading operation in the
    transformation queue, which in this case is assigned to the `mean_dep_delay_ddf`
    DataFrame. If you want to get a better idea of what the execution plan of the
    transformations queued so far is, you can create a graph to represent it. For
    simplicity, we will implement the graph using only one CSV file as input. These
    are the necessary steps:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这段代码也花费了非常少的时间。正如你所想象的，平均操作已经在数据读取操作之后在转换队列中排队，在这个例子中，它被分配给了 `mean_dep_delay_ddf`
    DataFrame。如果你想更好地了解到目前为止转换队列的执行计划，你可以创建一个图来表示它。为了简单起见，我们将使用单个 CSV 文件作为输入来实现这个图。以下是必要的步骤：
- en: Create a folder named `AirOnTimeCSVplot`.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `AirOnTimeCSVplot` 的文件夹。
- en: Copy only the first CSV file you unzipped earlier into the previous folder.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只复制你之前解压缩的第一个 CSV 文件到之前的文件夹中。
- en: 'Open a new Python script and run the following code on Visual Studio Code:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Visual Studio Code 中打开一个新的 Python 脚本并运行以下代码：
- en: '[PRE5]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `visualize()` method allows you to visualize the graph of the tasks estimated
    by the engine to realize the queued transformations, even before their execution.
    Specifically, the code will generate a PDF file in the same folder where the script
    you ran is located.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`visualize()` 方法允许你可视化由引擎估计的任务图，以实现排队的转换，甚至在它们执行之前。具体来说，代码将在你运行的脚本所在的同一文件夹中生成一个
    PDF 文件。'
- en: Starting at the bottom of *Figure 8.1*, which represents the contents of the
    newly generated PDF file, you can see that the single source CSV file is read
    by the `read-csv` function from two chunks split by the engine. The `dataframe-groupby-count-chunk`
    and `dataframe-groupby-sum-chunk` functions are applied to each chunk, since for
    each tuple defined by the keys of the grouping operation (`YEAR`, `MONTH`, `DAY_OF_MONTH`,
    and `ORIGIN`), we need to know the sum of the delay (`DEP_DELAY`) and the count
    of the occurrences to compute the average. After that, the results of the two
    `dataframe-groupby-sum-chunk` operations on the two chunks are aggregated by the
    `dataframe-groupby-sum-agg` function. Similarly, the `dataframe-groupby-count-agg`
    function aggregates the outputs of the two `dataframe-groupby-count-chunk` operations.
    Once the two DataFrames of sums and counts have been determined, the ratio between
    the two (that is, the mean) is calculated for each grouping using the `truediv`
    function. Finally, the `reset_index` function provides the desired DataFrame,
    the result of the distributed averaging operation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图8.1*的底部开始，它表示新生成的PDF文件的内容，你可以看到单个源CSV文件被`read-csv`函数从由引擎分割的两个块中读取。`dataframe-groupby-count-chunk`和`dataframe-groupby-sum-chunk`函数应用于每个块，因为对于由分组操作的键定义的每个元组（`YEAR`，`MONTH`，`DAY_OF_MONTH`和`ORIGIN`），我们需要知道延迟总和（`DEP_DELAY`）和发生次数的总和来计算平均值。之后，`dataframe-groupby-sum-agg`函数对两个块上的两个`dataframe-groupby-sum-chunk`操作的结果进行聚合。同样，`dataframe-groupby-count-agg`函数聚合了两个`dataframe-groupby-count-chunk`操作的输出。一旦确定了总和和计数的数据框，就使用`truediv`函数计算每个分组之间的比率（即平均值）。最后，`reset_index`函数提供了所需的DataFrame，这是分布式平均操作的结果。
- en: If you think about it, the famous problem-solving strategy called **Divide and
    Conquer** (also known as **Divide et Impera**) has been adopted. It consists of
    dividing the original problem into smaller and generally simpler subproblems,
    each solved recursively. The solutions of the subproblems are then properly combined
    to obtain the solution of the original problem. If you've had any experience with
    the Hadoop world, the **MapReduce** paradigm follows the same philosophy, which
    was maintained and optimized later by Spark.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细思考，被称为**分而治之**（也称为**Divide et Impera**）的著名问题解决策略已经被采用。它包括将原始问题分解成更小且通常更简单的子问题，每个子问题递归解决。然后，将这些子问题的解决方案适当地组合起来，以获得原始问题的解决方案。如果你在Hadoop世界中有所经验，**MapReduce**范式遵循相同的哲学，后来Spark对其进行了维护和优化。
- en: '![Figure 8.1 – Computation underlying task graph](img/file216.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – 任务图背后的计算](img/file216.png)'
- en: Figure 8.1 – Computation underlying task graph
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 任务图背后的计算
- en: That said, let's get back to the initial script. We had defined the `mean_dep_delay_ddf`
    Dask DataFrame. All the transformations needed to get the desired result have
    been queued. How do you tell Dask to actually proceed with all the computations?
    You have to explicitly ask for the result via the `compute()` method.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们回到最初的脚本。我们定义了`mean_dep_delay_ddf` Dask DataFrame。获取所需结果所需的所有转换都已排队。你如何告诉Dask实际进行所有计算？你必须通过`compute()`方法显式请求结果。
- en: '**Important Note**'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**重要提示**'
- en: ''
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Pay attention to the fact that `compute()` returns *in-memory results*. It converts
    Dask DataFrames to pandas DataFrames, Dask arrays to NumPy arrays, and Dask bags
    to lists. It is a method that should only be invoked when you are certain that
    the result will fit comfortably in your machine's RAM. Otherwise, you can write
    your data on disk using specific methods, such as `to_textfiles()` or `to_parquet()`.
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意到`compute()`返回的是内存中的结果。它将Dask DataFrames转换为pandas DataFrames，Dask数组转换为NumPy数组，Dask
    bags转换为列表。这是一个只有在确定结果将舒适地适合你的机器的RAM时才应调用的方法。否则，你可以使用特定的方法，如`to_textfiles()`或`to_parquet()`，将你的数据写入磁盘。
- en: If you instantiated a cluster, you could have decided to persist the calculated
    data not on disk but in memory, using the `persist()` method. The result would
    still have been a distributed Dask object but one that references pre-computed
    results distributed over the cluster's memory.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你实例化了集群，你可以决定使用`persist()`方法将计算出的数据持久化到内存中，而不是磁盘上。结果仍然是一个分布式Dask对象，但它引用的是分布在集群内存中的预计算结果。
- en: 'To add some magic to your script, you can use the `ProgressBar()` object, which
    allows you to monitor the progress of your computations. Unfortunately, it happens
    that even if the bar reaches 100%, it still takes some time for the workers to
    finish processing:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给你的脚本增添一些魔法，你可以使用`ProgressBar()`对象，它允许你监控计算进度。不幸的是，即使进度条达到100%，工作者完成处理仍需要一些时间：
- en: '![Figure 8.2 – The progress bar on Visual Studio Code](img/file217.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 – Visual Studio Code上的进度条](img/file217.png)'
- en: Figure 8.2 – The progress bar on Visual Studio Code
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – Visual Studio Code上的进度条
- en: 'So, don''t give up! Before running the following line of code, keep in mind
    that processing takes approximately 10 minutes on a machine with 32 GB of RAM
    and a processor with 6 cores. The script is the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，不要放弃！在运行以下代码之前，请记住，在拥有32GB RAM和6核心处理器的机器上处理大约需要10分钟。脚本如下：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Your laptop will commit all available logical processors to parallelize computations,
    which you will see in the **Logical Processors** view in **Task Manager**:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你的笔记本电脑将把所有可用的逻辑处理器都用于并行计算，你将在**任务管理器**的**逻辑处理器**视图中看到这一点：
- en: '![Figure 8.3 – Parallel computations shown in Task Manager](img/file218.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3 – 任务管理器中显示的并行计算](img/file218.png)'
- en: Figure 8.3 – Parallel computations shown in Task Manager
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 任务管理器中显示的并行计算
- en: 'When processing is complete, your pandas DataFrame will be available in memory,
    and you can view some rows of it as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理完成后，你的pandas DataFrame将可用内存中，你可以如下查看它的几行：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output will be the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是以下内容：
- en: '![Figure 8.4 – First 10 rows of the output pandas DataFrame](img/file219.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 – 输出pandas DataFrame的前10行](img/file219.png)'
- en: Figure 8.4 – First 10 rows of the output pandas DataFrame
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 输出pandas DataFrame的前10行
- en: You can find the complete script to create a Dask DataFrame and extract information
    from it in the `01-load-large-dataset-in-python.py` file in the `Chapter08\Python`
    folder.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在`Chapter08\Python`文件夹中的`01-load-large-dataset-in-python.py`文件中找到创建Dask DataFrame并从中提取信息的完整脚本。
- en: Finally, you were able to get the dataset of a few thousand rows on average
    flight delays for each airport of origin and for each day of the year by processing
    a CSV set as large as 30 GB!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你能够通过处理高达30GB的CSV集来得到平均每家机场每天的航班延误数据集！
- en: 'If you want to write the contents of a Dask DataFrame to a CSV file, without
    going through the generation of a pandas DataFrame, you can invoke directly the
    `to_csv()` method, passing the path of the file to generate as a parameter, as
    the following example shows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要将Dask DataFrame的内容写入CSV文件，而不经过生成pandas DataFrame的过程，你可以直接调用`to_csv()`方法，将生成文件的路径作为参数传递，如下例所示：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can well imagine, calling the `to_csv()` method triggers the actual execution
    of all queued transformations, just as it did with the `compute()` method, since
    you’re forcing Dask to read the DataFrame content in order to write it to disk.
    For this reason, if you need to generate the CSV file and also create the pandas
    DataFrame to be used later in your code, you should not call first `to_csv()`
    from the Dask DataFrame and then `compute()` to get the pandas DataFrame, because
    you would force the actual execution of the transformations pipeline twice. In
    this case, it is convenient to first generate the pandas DataFrame with `compute()`
    and then generate a CSV or an Excel file from it, as you learned in *Chapter 7*,
    *Logging Data from Power BI to External Sources*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所想，调用`to_csv()`方法会触发所有排队转换的实际执行，就像调用`compute()`方法一样，因为你正在强制Dask读取DataFrame内容以便将其写入磁盘。因此，如果你需要生成CSV文件并创建稍后用于代码中的pandas
    DataFrame，你不应该首先从Dask DataFrame调用`to_csv()`，然后调用`compute()`来获取pandas DataFrame，因为你将强制转换管道的实际执行两次。在这种情况下，首先使用`compute()`生成pandas
    DataFrame，然后从它生成CSV或Excel文件，就像你在*第7章*，*从Power BI记录数据到外部源*中学到的那样，会更方便。
- en: Let's now try to apply what you have learned so far in Power BI.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来尝试应用你在Power BI中学到的知识。
- en: Importing a large dataset in Power BI with Python
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Python在Power BI中导入大型数据集
- en: 'You learned that Power BI can import data using a Python script directly and
    that the data must be organized in a pandas DataFrame in order to be used in the
    data model. Therefore, what you are going to do is develop a script that uses
    the objects illustrated in the previous section in order to instantiate a pandas
    DataFrame containing the data of flight delay averages in the US. From this DataFrame,
    you will then generate a CSV file. Here are the steps required to implement it
    in Power BI:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你了解到Power BI可以直接使用Python脚本导入数据，并且数据必须以pandas DataFrame的形式组织，才能在数据模型中使用。因此，你将要开发一个脚本，使用前一小节中展示的对象来实例化一个包含美国航班延误平均数据的pandas
    DataFrame。然后，从这个DataFrame中生成一个CSV文件。以下是实现它在Power BI中的步骤：
- en: Open Power BI Desktop and make sure it is referencing your Python `pbi_powerquery_env`
    environment.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Power BI桌面版，确保它正在引用你的Python `pbi_powerquery_env`环境。
- en: Click on **Get data**, start entering `Python`, and then double-click on **Python
    script**.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**获取数据**，开始输入`Python`，然后双击**Python脚本**。
- en: Copy the content of the `02-load-large-dataset-in-power-bi.py` file into the
    `Chapter08/Python` folder and paste the content into the Python script editor.
    Remember to edit the path to the CSV files (`main_path`) and destination path
    of the CSV file to be generated, and then click **OK**.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`02-load-large-dataset-in-power-bi.py`文件的内容复制到`Chapter08/Python`文件夹中，并将其内容粘贴到Python脚本编辑器中。记得编辑CSV文件路径（`main_path`）和要生成的CSV文件的目标路径，然后点击**确定**。
- en: 'After about 11 minutes (using a 6-core laptop with 32 GB of RAM), you will
    see the **Navigator** window showing the aggregated data in the `mean_dep_delay_df`
    DataFrame:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大约11分钟后（使用一个6核心的笔记本电脑，32 GB的RAM），你将看到**导航器**窗口显示`mean_dep_delay_df` DataFrame中的聚合数据：
- en: '![Figure 8.5 – The mean_dep_delay_df DataFrame loaded in Power BI](img/file220.png)'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图8.5 – 在Power BI中加载的mean_dep_delay_df DataFrame](img/file220.png)'
- en: Figure 8.5 – The mean_dep_delay_df DataFrame loaded in Power BI
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.5 – 在Power BI中加载的mean_dep_delay_df DataFrame
- en: 'Select the DataFrame and click on **Load**. After that, a **Load** popup will
    appear, and it will remain active for about five more minutes:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择DataFrame，然后点击**加载**。之后，将出现一个**加载**弹出窗口，并保持活跃大约五分钟：
- en: '![Figure 8.6 – Final phase of loading data into Power BI](img/file221.png)'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图8.6 – 将数据加载到Power BI的最终阶段](img/file221.png)'
- en: Figure 8.6 – Final phase of loading data into Power BI
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.6 – 将数据加载到Power BI的最终阶段
- en: 'When it disappears, your data is loaded into the data model.Moreover, the script
    also generated the CSV file containing your aggregated data. You’ll find the `mean_dep_delay_df.csv`
    file, sized about 60 MB, in your folder:'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当它消失时，你的数据已经加载到数据模型中。此外，脚本还生成了包含你的聚合数据的CSV文件。你将在文件夹中找到大约60 MB的`mean_dep_delay_df.csv`文件：
- en: '![Figure 8.7 – Content of the CSV file created from the Python script in Power
    BI](img/file222.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7 – 从Power BI中的Python脚本创建的CSV文件内容](img/file222.png)'
- en: Figure 8.7 – Content of the CSV file created from the Python script in Power
    BI
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – 从Power BI中的Python脚本创建的CSV文件内容
- en: Impressive! You just imported 30 GB of data to your Power BI Desktop, with just
    a few lines in Python. Congratulations!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！你只用几行Python代码就成功将30 GB的数据导入到Power BI桌面版中，恭喜你！
- en: Did you know you can also do this in R? Let's see how.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道你还可以用R来做这件事吗？让我们看看怎么做。
- en: Importing large datasets with R
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用R导入大型数据集
- en: The same scalability limitations illustrated for Python packages used to manipulate
    data also exist for R packages in the **Tidyverse** ecosystem. Even in R, it is
    not possible to use a dataset larger than the available RAM on the machine. The
    first solution that is adopted in these cases is also to switch to Spark-based
    distributed systems, which provide the **SparkR** language. It provides a distributed
    implementation of the DataFrame you are used to in R, supporting filtering, aggregation,
    and selection operations as you do with the `dplyr` package. For those of us who
    are fans of the Tidyverse world, RStudio actively develops the `sparklyr` package,
    which allows you to use all the functionality of `dplyr`, even for distributed
    DataFrames. However, adopting Spark-based systems to process CSVs that together
    take up little more than the RAM you have available on your machine may be overkill
    because of the overhead introduced by all the Java infrastructure needed to run
    them.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于用于操作数据的Python包所展示的相同可伸缩性限制，在**Tidyverse**生态系统中的R包也存在。即使在R中，也无法使用大于机器可用RAM的数据集。在这些情况下采用的第一种解决方案也是切换到基于Spark的分布式系统，这些系统提供了**SparkR**语言。它提供了R中常用的DataFrame的分布式实现，支持过滤、聚合和选择操作，就像使用`dplyr`包一样。对于我们这些Tidyverse世界的粉丝来说，RStudio积极开发`sparklyr`包，允许您使用`dplyr`的所有功能，即使是分布式DataFrame。然而，由于运行所需的所有Java基础设施带来的开销，采用基于Spark的系统来处理总共占用略多于机器上可用RAM的CSV文件可能有些过度。
- en: It is in these cases that adopting the `disk.frame` package ([https://github.com/xiaodaigh/disk.frame](https://github.com/xiaodaigh/disk.frame))
    is the winning approach. What `disk.frame` allows you to do is to divide a dataset
    larger than RAM into chunks and store each chunk in a separate file within a folder.
    **Chunk files** are serializations of DataFrames smaller than the initial one
    in the compressed `fst` format, introduced by the `fst` package ([https://github.com/fstpackage/fst](https://github.com/fstpackage/fst)).
    It is specifically designed to unleash the *speed and parallelism of solid-state
    disks* that are easily found in modern computers. DataFrames stored in the `fst`
    format have full random access, both column and row, and are therefore able to
    benefit from parallel computations.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，采用`disk.frame`包（[https://github.com/xiaodaigh/disk.frame](https://github.com/xiaodaigh/disk.frame)）是获胜的方法。`disk.frame`允许您将大于RAM的数据集分成块，并将每个块存储在文件夹中的单独文件中。**块文件**是小于初始DataFrame的压缩`fst`格式的序列化，由`fst`包（[https://github.com/fstpackage/fst](https://github.com/fstpackage/fst)）引入。它专门设计用来释放现代计算机中容易找到的固态硬盘的*速度和并行性*。存储在`fst`格式中的DataFrame具有完整的随机访问，包括列和行，因此能够从并行计算中受益。
- en: '**Important Note**'
  id: totrans-110
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**重要提示**'
- en: ''
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you want to benefit from the parallelism introduced by the `disk.frame` package,
    you must necessarily persist the source CSV files on an SSD disk. If you don't
    have it, and you use an HDD disk, your processing will take too long, and it won't
    be worthwhile.
  id: totrans-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果您想从`disk.frame`包引入的并行性中受益，您必须必然将源CSV文件持久化存储在SSD磁盘上。如果您没有SSD，并且使用HDD磁盘，您的处理将花费太长时间，而且不值得。
- en: You can define the degree of parallelism with which `disk.frame` will work by
    indicating the number of parallel workers you want to use. For each worker, it
    will create an **R session** using the parallelism mechanism provided by the `future`
    package.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过指定您想要使用的并行工作进程的数量来定义`disk.frame`将使用的并行度。对于每个工作进程，它将使用`future`包提供的并行机制创建一个**R会话**。
- en: Let's see how to use `disk.frame` in detail.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看如何使用`disk.frame`。
- en: Installing disk.frame on your laptop
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在您的笔记本电脑上安装disk.frame
- en: 'First, you need to install the `disk.frame` package in your R engine:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要在您的R引擎中安装`disk.frame`包：
- en: Open RStudio and make sure it is referencing your latest CRAN (Comprehensive
    R Archive Network) R engine (in our case, version 4.0.2).
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开RStudio并确保它引用的是您最新的CRAN（综合R档案网络）R引擎（在我们的例子中，版本4.0.2）。
- en: 'Click on the console prompt and enter this command: `install.packages("disk.frame")`.
    Make sure you install the latest version of the package (in our case, version
    0.5.0).'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击控制台提示并输入此命令：`install.packages("disk.frame")`。确保您安装了该包的最新版本（在我们的例子中，版本0.5.0）。
- en: At this point, you are ready to test it on RStudio.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经准备好在RStudio上测试它了。
- en: Creating a disk.frame instance
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建disk.frame实例
- en: 'In order to create a `disk.frame` instance (a disk-based DataFrame), you must
    first define the number of parallel workers you want to initialize. Obviously,
    the higher the number of workers, the faster the `disk.frame` instance will be
    created. But you cannot initialize any large number of workers, because the number
    of parallel threads you can use depends on the processor of your machine. Also,
    you can define a maximum limit to the amount of data you can exchange from one
    worker to another. For this reason, at the beginning of the script that uses `disk.frame`,
    you will find the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个 `disk.frame` 实例（基于磁盘的 DataFrame），您必须首先定义您想要初始化的并行工作器的数量。显然，工作器数量越多，`disk.frame`
    实例创建得越快。但您不能初始化任何大量工作器，因为您可以使用并行线程的数量取决于您的机器的处理器。此外，您可以定义从工作器到另一个工作器可以交换的数据量的最大限制。因此，在开始使用
    `disk.frame` 的脚本中，您会找到以下内容：
- en: '[PRE9]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `availableCores()` function calculates the number of logical processors
    available on your machine. Usually, it’s best practice to leave one out for computationally
    demanding tasks, so that the machine doesn't become unresponsive. The `setup_disk.frame()`
    function creates the cluster of workers needed for the computations. Furthermore,
    there is no limit to the amount of data that can be exchanged between workers
    (`Inf` stands for **infinite**).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`availableCores()` 函数计算您机器上可用的逻辑处理器数量。通常，对于计算密集型任务，最好留出一个，这样机器就不会变得无响应。`setup_disk.frame()`
    函数创建用于计算的工人集群。此外，工作器之间可以交换的数据量没有限制（`Inf` 代表 **无限**）。'
- en: We assume that you have already unzipped the CSV files containing data on US
    flights from 1987 to 2012, as mentioned at the beginning of this chapter, in the
    `D:\<your-path>\AirOnTimeCSV` folder.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设您已经解压了包含 1987 年至 2012 年美国航班数据的 CSV 文件，如本章开头所述，在 `D:\<your-path>\AirOnTimeCSV`
    文件夹中。
- en: '**Important Note**'
  id: totrans-125
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**重要提示**'
- en: ''
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you don't have a laptop with enough hardware resources, you should import
    a subset of CSV files first (such as 40–50 files) to test the scripts without
    having to wait excessively long execution times.
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果您没有足够的硬件资源的笔记本电脑，您应该首先导入 CSV 文件的一个子集（例如 40-50 个文件）来测试脚本，而无需等待过长的执行时间。
- en: 'You have to define a list of paths for each CSV file using the `list.files()`
    function, and then you can proceed with the creation of `disk.frame` using the
    `csv_to_disk.frame()` function:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须使用 `list.files()` 函数定义每个 CSV 文件的路径列表，然后您可以使用 `csv_to_disk.frame()` 函数继续创建
    `disk.frame`：
- en: '[PRE10]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The creation of a `disk.frame` instance consists of two steps:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 `disk.frame` 实例包括两个步骤：
- en: The process creates as many chunks as there are CSV files for each allocated
    worker in temporary folders.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该过程在每个分配给临时文件夹的每个工作器中创建与 CSV 文件数量相同的块。
- en: Chunks are aggregated and persisted in the output folder (defined by the `outdir`
    parameter) associated with `disk.frame`.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 块在输出文件夹（由 `outdir` 参数定义）中聚合并持久化，该文件夹与 `disk.frame` 关联。
- en: In short, `disk.frame` is a folder, preferably with a `.df` extension, that
    contains the aggregated chunks (`.fst` files) generated from the source files.
    As you've already seen with Dask, it is a good idea to *specify only the columns
    of interest* via the `select` parameter in order to limit the columns to be read.
    This practice also guarantees to read only those columns you are sure are not
    completely empty, thus avoiding reading errors due to the different inferred data
    type compared to the real one.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，`disk.frame` 是一个文件夹，最好带有 `.df` 扩展名，其中包含从源文件生成的聚合块（`.fst` 文件）。正如您已经通过 Dask
    看到的，通过 `select` 参数仅指定感兴趣的列是一个好主意，这样可以限制要读取的列。这种做法也保证了只读取您确信不是完全空的列，从而避免了由于与实际数据类型不同的推断数据类型而导致的读取错误。
- en: '**Important Note**'
  id: totrans-134
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**重要提示**'
- en: ''
  id: totrans-135
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It may occur that some columns have a number of null values at the beginning
    and therefore `disk.frame` cannot impute the correct data type, as it uses a sample
    to do that. In this case, you should specifically declare the data type of those
    columns using the `colClasses` parameter.
  id: totrans-136
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 可能会发生某些列在开头有大量空值的情况，因此 `disk.frame` 无法推断正确的数据类型，因为它使用样本来执行此操作。在这种情况下，您应该使用 `colClasses`
    参数特别声明这些列的数据类型。
- en: Note that once a value is assigned to the `create_dkf_exec_time` variable, the
    round brackets print it on the screen, all in one line. The creation time is about
    1 minute and 17 seconds on a machine with 6 cores and 32 GB, using 11 workers.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，一旦`create_dkf_exec_time`变量分配了值，圆括号就会将其打印在屏幕上，全部在一行中。在拥有6个核心和32GB的机器上，使用11个工作者的情况下，创建时间大约是1分17秒。
- en: 'Once the creation of `disk.frame` is complete, the multiple R sessions that
    make up the cluster could remain active. In order to force their disconnection,
    it is useful to invoke the following command:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦`disk.frame`创建完成，构成集群的多个R会话可以保持活跃。为了强制断开连接，可以使用以下命令：
- en: '[PRE11]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In this way, the machine's resources are completely released.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，机器资源就完全释放了。
- en: '**Important Note**'
  id: totrans-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**重要提示**'
- en: ''
  id: totrans-142
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A correctly created `disk.frame` can always be referenced later using the `disk.frame()`
    function by passing it to the path of the `.df` folder, without having to create
    it again.
  id: totrans-143
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 正确创建的`disk.frame`可以通过使用`disk.frame()`函数并传入`.df`文件夹的路径来在以后引用，无需再次创建。
- en: You can find the complete script to create `disk.frame` in the `01-create-diskframe-in-r.R`
    file into the `Chapter08\R` folder.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在`Chapter08\R`文件夹中的`01-create-diskframe-in-r.R`文件中找到创建`disk.frame`的完整脚本。
- en: Now let's see how to get information from the newly created `disk.frame`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何从新创建的`disk.frame`中获取信息。
- en: Extracting information from disk.frame
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从disk.frame提取信息
- en: As we are interested in getting the average airline delay at each US airport
    for each day of the year, a grouping operation is required on the entire `disk.frame`.
    In order to optimize the execution time, in this case it is also necessary to
    instantiate a cluster of workers that will read in parallel the information from
    the newly created `disk.frame`. You can transform the `disk.frame` instance using
    the syntax you know from the `dplyr` package. Moreover, in order to optimize the
    machine resources, you will force the engine to read only the columns strictly
    necessary to the requested computation using the `srckeep()` function from the
    `disk.frame` package. In our case, it would be unnecessary to select the columns,
    since when creating the `disk.frame` instance we kept only those strictly necessary,
    but the following recommendation applies.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们感兴趣的是获取美国每个机场每年每一天的平均航空公司延误，因此需要对整个`disk.frame`进行分组操作。为了优化执行时间，在这种情况下，还需要实例化一个工作者的集群，该集群将并行读取新创建的`disk.frame`中的信息。你可以使用从`dplyr`包中了解的语法来转换`disk.frame`实例。此外，为了优化机器资源，你将强制引擎只读取对请求的计算严格必要的列，使用`disk.frame`包中的`srckeep()`函数。在我们的案例中，选择列是不必要的，因为我们创建`disk.frame`实例时只保留了那些严格必要的列，但以下建议适用。
- en: '**Important Note**'
  id: totrans-148
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**重要提示**'
- en: ''
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It’s good practice to always use the `srckeep()` function in aggregate data
    extraction scripts, because if the `disk.frame` instance had all the initial columns,
    the operation would cause a machine crash.
  id: totrans-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在聚合数据提取脚本中始终使用`srckeep()`函数是一个好习惯，因为如果`disk.frame`实例包含所有初始列，该操作会导致机器崩溃。
- en: 'The following code should be used to extract aggregated data:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 应使用以下代码来提取聚合数据：
- en: '[PRE12]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Again, as already seen for Dask in Python, there are functions that collect
    all the queued transformations and trigger the execution of calculations. In the
    case of `disk.frame`, the function is `collect()`. The duration of this operation
    is about 20 minutes with a 32 GB machine with 6 cores, using 11 workers.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，正如在Python中的Dask所看到的，存在一些函数可以收集所有排队的转换并触发计算的执行。在`disk.frame`的情况下，该函数是`collect()`。这个操作的持续时间大约是20分钟，使用一个拥有6个核心的32GB机器，并使用11个工作者。
- en: 'The end result is a tibble containing the desired air delay averages:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果是包含所需空气延迟平均值的tibble：
- en: '![Figure 8.8 – First rows of the tibble containing the delay averages](img/file223.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图8.8 – 包含延迟平均值的tibble的前几行](img/file223.png)'
- en: Figure 8.8 – First rows of the tibble containing the delay averages
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – 包含延迟平均值的tibble的前几行
- en: Also in this case, you were able to get the dataset of a few thousand rows on
    average flight delays by processing a CSV set as large as 30 GB!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个情况下，你也能够通过处理高达30GB的CSV集来获取平均飞行延误的几千行数据集！
- en: You can find the complete script to extract aggregated data from `disk.frame`
    in the `02-extract-info-from-diskframe-in-r.R` file in the `Chapter08\R` folder.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在`Chapter08\R`文件夹中的`02-extract-info-from-diskframe-in-r.R`文件中找到从`disk.frame`提取聚合数据的完整脚本。
- en: The following common-sense observation applies here as well.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下常识性观察也适用于此。
- en: '**Important Note**'
  id: totrans-160
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**重要提示**'
- en: ''
  id: totrans-161
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If the results of previous time-consuming processing are to be reused often,
    it is best to *persist them on disk in a reusable format*. In this way, it will
    be avoidedyou can avoid to executinge again all the onerous processing of the
    queued transformations.
  id: totrans-162
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果之前耗时较长的处理结果需要经常重用，最好是将其以可重用格式 *持久化到磁盘上*。这样，你就可以避免再次执行所有繁重的排队转换处理。
- en: A `disk.frame` instance doesn’t have direct methods, like a Dask DataFrame,
    that allow its contents to be written to disk downstream of all queued transformations.
    Since the result is a tibble, which for all intents and purposes is a DataFrame,
    you can write it to disk following the directions you learned in *Chapter 7*,
    *Logging Data from Power BI to External Sources*.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`disk.frame` 实例没有像 Dask DataFrame 那样的直接方法，允许其内容在所有排队转换之后写入磁盘。由于结果是 tibble，从所有意义上说它是一个
    DataFrame，你可以按照你在 *第 7 章*，*从 Power BI 记录数据到外部来源*中学到的指示将其写入磁盘。'
- en: So, let's see how you can apply what you've learned so far in Power BI.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们看看你如何在 Power BI 中应用到目前为止学到的知识。
- en: Importing a large dataset in Power BI with R
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 R 在 Power BI 中导入大型数据集
- en: The optimal solution to load a dataset larger than the available RAM with R
    via Power BI would be to be able to create the `disk.frame` instance directly
    via an R script in Power BI itself.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 R 通过 Power BI 加载大于可用 RAM 的数据集的最佳解决方案是能够在 Power BI 本身通过 R 脚本直接创建 `disk.frame`
    实例。
- en: '**Important Note**'
  id: totrans-167
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**重要提示**'
- en: ''
  id: totrans-168
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unfortunately, the operation of creating a `disk.frame` instance from various
    CSV files *triggers an error* in Power BI during the second phase of the process
    (the aggregation of various chunks into the `disk.frame` folder).
  id: totrans-169
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不幸的是，从各种 CSV 文件创建 `disk.frame` 实例的操作在过程的第二阶段（将各种块聚合到 `disk.frame` 文件夹中）中在 Power
    BI 中会触发一个错误。
- en: 'Therefore, in order to extract information from a `disk.frame` instance in
    Power BI, you must first create it in RStudio (or any other IDE or any automated
    script). Once the `disk.frame` instance has been created, it is then possible
    to use it in Power BI as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了从 Power BI 中的 `disk.frame` 实例提取信息，你必须首先在 RStudio（或任何其他 IDE 或任何自动化脚本）中创建它。一旦创建了
    `disk.frame` 实例，就可以在 Power BI 中如下使用它：
- en: Make sure you have correctly created your `disk.frame` instance using RStudio
    by running the code contained in the `01-create-diskframe-in-r.R` file in the
    `Chapter08/R` folder.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保你已使用 RStudio 正确创建了 `disk.frame` 实例，方法是运行 `Chapter08/R` 文件夹中 `01-create-diskframe-in-r.R`
    文件中的代码。
- en: Open Power BI Desktop and make sure it is referencing your latest CRAN R engine.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 Power BI Desktop 并确保它引用的是你最新的 CRAN R 引擎。
- en: Click on **Get data**, start entering `script`, and then double-click on **R
    script**.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **获取数据**，开始输入 `script`，然后双击 **R 脚本**。
- en: Copy the content of the `03-extract-info-from-diskframe-in-power-bi.R` file
    into the `Chapter08\R` folder and paste it into the R script editor. Remember
    to edit the destination path of the CSV file to be generated, and then click **OK**.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `03-extract-info-from-diskframe-in-power-bi.R` 文件的内容复制到 `Chapter08\R` 文件夹，并将其粘贴到
    R 脚本编辑器中。请记住编辑要生成的 CSV 文件的目标路径，然后点击 **确定**。
- en: 'After about 30 minutes (using a 6-core laptop with 32 GB of RAM), you will
    see the **Navigator** window showing the aggregated data in the `mean_dep_delay_df`
    DataFrame:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大约 30 分钟后（使用具有 6 个核心和 32 GB RAM 的笔记本电脑），你将看到 **导航器** 窗口显示 `mean_dep_delay_df`
    DataFrame 中的聚合数据：
- en: '![Figure 8.9 – The mean_dep_delay_df DataFrame loaded in Power BI](img/file224.png)'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 8.9 – 在 Power BI 中加载的 mean_dep_delay_df DataFrame](img/file224.png)'
- en: Figure 8.9 – The mean_dep_delay_df DataFrame loaded in Power BI
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.9 – 在 Power BI 中加载的 mean_dep_delay_df DataFrame
- en: 'Select the DataFrame and click on **Load**. After that, a **Load** popup will
    appear, and it will remain active for about 10 more minutes:'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择 DataFrame 并点击 **加载**。之后，将出现一个 **加载** 弹出窗口，它将保持活跃约 10 分钟：
- en: '![Figure 8.10 – Final phase of loading data into Power BI](img/file225.png)'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 8.10 – 将数据加载到 Power BI 的最后阶段](img/file225.png)'
- en: Figure 8.10 – Final phase of loading data into Power BI
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.10 – 将数据加载到 Power BI 的最后阶段
- en: 'When it disappears, your data is loaded into the data model.Moreover, the script
    also generated the CSV file containing your aggregated data. You’ll find the `mean_dep_delay_df.csv`
    file, sized about 60 MB, in your folder:'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当它消失时，你的数据已加载到数据模型中。此外，脚本还生成了包含你的聚合数据的 CSV 文件。你将在你的文件夹中找到大小约为 60 MB 的 `mean_dep_delay_df.csv`
    文件：
- en: '![Figure 8.11 – Content of the CSV file created from the R script in Power
    BI](img/file226.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.11 – 从 Power BI 中的 R 脚本创建的 CSV 文件内容](img/file226.png)'
- en: Figure 8.11 – Content of the CSV file created from the R script in Power BI
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 – 从Power BI中的R脚本创建的CSV文件内容
- en: Awesome! Would you ever have thought of importing 30 GB of data into your Power
    BI Desktop to extract information from it in R? Well, you just did it with just
    a few lines of code.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！你曾经想过只用几行代码就将30 GB的数据导入Power BI Desktop中，从中提取信息吗？好吧，你刚刚就做到了。
- en: Summary
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to import datasets larger than the RAM available
    to your laptop into Python and R. You've also applied this knowledge to import
    a set of CSV files that, in total, take up more than the available RAM on your
    machine into Power BI Desktop.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何将比你的笔记本电脑可用的RAM更大的数据集导入Python和R。你还将这些知识应用于导入一组CSV文件，这些文件总共占用的空间超过了你机器上可用的RAM。
- en: In the next chapter, you will learn how to enrich your data with external information
    extracted from web services made available to users.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习如何使用用户可用的网络服务提取的外部信息来丰富你的数据。
- en: References
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'For additional reading, check out the following books and articles:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 对于额外的阅读，请参阅以下书籍和文章：
- en: '*Create Dask clusters on Azure using Azure (Spot) VMs* ([https://cloudprovider.dask.org/en/latest/azure.html](https://cloudprovider.dask.org/en/latest/azure.html))'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用Azure（Spot）VMs在Azure上创建Dask集群* ([https://cloudprovider.dask.org/en/latest/azure.html](https://cloudprovider.dask.org/en/latest/azure.html))'
- en: '*Accelerated Machine Learning at Scale with NVIDIA RAPIDS on Microsoft Azure
    ML with Dask* ([https://github.com/drabastomek/GTC/tree/master/SJ_2020/workshop](https://github.com/drabastomek/GTC/tree/master/SJ_2020/workshop))'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*在Microsoft Azure ML上使用Dask和NVIDIA RAPIDS进行大规模加速机器学习* ([https://github.com/drabastomek/GTC/tree/master/SJ_2020/workshop](https://github.com/drabastomek/GTC/tree/master/SJ_2020/workshop))'
