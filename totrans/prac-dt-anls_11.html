<html><head></head><body>
        

                            
                    Plotting, Visualization, and Storytelling
                
            
            
                
<p>This chapter will teach you how to visualize data by exploring additional chart options such as histograms, box plots, and scatter plots to advance your data literacy skills. Storytelling with data starts with understanding the relationships that exist within the numbers, so we will learn about distribution curves and how they apply to analysis. During this discovery phase of analysis of your data, you will learn how to identify outliers and patterns along with best practices in visualizing geographic data. We will wrap up this chapter by learning the difference between correlation versus causation.</p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>Explaining distribution analysis</li>
<li>Understanding outliers and trends</li>
<li>Geoanalytical techniques and tips</li>
<li>Finding patterns in data</li>
</ul>
<h1 id="uuid-d8b4f94f-6281-4fa2-aee3-8da8e74162e3">Technical requirements</h1>
<p>The GitHub repository of this book can be found at <a href="https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter09">https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter09</a>.</p>
<p>You can download and install the required software from <a href="https://www.anaconda.com/products/individual" target="_blank">https://www.anaconda.com/products/individual</a>.</p>
<p class="mce-root"/>
<h1 id="uuid-aeec570b-6980-4a91-bffd-f3f250bc46cc">Explaining distribution analysis</h1>
<p>I cannot recall a time in history where data, statistics, and science consumed daily lives as it does today. The news cycles are presenting a crisis as it unfolds in real time where changes to human behavior are happening and social norms are being redefined. As I'm writing this book, the concept of <strong>flattening the curve</strong> has gone mainstream and has become a globally understood concept because of the coronavirus (COVID-19) pandemic. You have probably seen something similar to what is shown in the following diagram, which was adapted from the <strong>Centers for Disease Control and Prevention</strong> (<strong>CDC</strong>). These types of visualizations are commonly used to communicate the importance of preventing the spread of a disease. The following visualization has two curves, one in yellow labeled <strong>No Intervention measures taken</strong> and the other in blue named "<strong>Flatten the Curve" using preventative measures</strong>. A dotted reference line labeled <strong>Healthcare capacity</strong> is available for relative comparison between the two curves. From a data literacy perspective, we can identify them as distribution curves shown side by side to measure the <strong>Daily # of Cases</strong> on the <em>y</em> axis with a common dimension of duration, which is labeled as <strong>Number of days since first Case</strong> on the <em>x</em> axis. A distribution curve is common in data analysis and visually represents the numeric data values of a single variable:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/e13ca00b-a21b-4c27-9b99-799c6a972751.png" style="width:70.67em;height:34.17em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>For reference, let's review the most well-known distribution curve, which is called a <strong>Gaussian</strong>, <strong>normal</strong>, or a <strong>bell</strong> curve because of the visual similarity to the shape of a physical bell. In a normal distribution, the values would be represented on the line with the highest point representing the mean or average of the sample or entire population of numeric values. As the line stretches away from the top in either direction, it shows how the numbers have variance from the mean. This spread or dispersion of the numbers from the average value is commonly known as the <strong>Standard Deviation</strong> (<strong>SD</strong>). In a perfect distribution, the values would fall within plus or minus one, two, or three standard deviations from the mean, which creates symmetry in the line shape.</p>
<p>Some interesting facts to understand about a normal distribution are the following:</p>
<ul>
<li>About 68% of the population of data values fall within plus or minus one standard deviation.</li>
<li>96% of the data falls without plus or minus two standard deviation.</li>
<li>99.7% of the data falls within plus or minus three standard deviation.</li>
<li>The calculated mean, median, and mode are all equal.</li>
<li>There is a 50/50% split between the data left and right of the median.</li>
</ul>
<p>Back to our example, in the first curve labeled <strong>No Intervention measures taken</strong>, the numbers actually double every few days, which creates a steep curve as it approaches the highest point. The second curve, which is identified as the <strong>"Flatten the Curve" using preventative measures</strong>, helps the consumer of this data to visualize the importance of stopping the spread of the virus because the height of the curve has been significantly reduced. I believe this chart became relevant to mass communications because of its simplicity of explaining a distribution curve without going into the statistics behind it. Even without showing the data behind the curves, anyone can visually understand the importance of this critical information.</p>
<p>At this point in time of writing this book, I do not know whether people around the world have successfully achieved the desired result of reducing the mortality rate of COVID-19. Furthermore, it is unknown at this time whether enough preventive measures such as social distancing are helping to <strong>flatten the curve</strong>. Many people all around the world have already suffered from the COVID-19 pandemic travesty. My heartfelt condolences go out to everyone who has suffered.</p>
<h2 id="uuid-052a785e-9cfe-44dc-b1e2-12d4440444d2">KYD</h2>
<p>To see how the COVID-19 data is distributed, I have provided a snapshot CSV file that is available in the GitHub repository for this book. To support our data analyst <strong>Know Your Data</strong> (<strong>KYD</strong>) mantra, I'll provide some additional information on how the data was collected and its format. The <kbd>COVID-19 Cases.csv</kbd> file was collected from authoritative open source COVID-19 sources. A GitHub repository maintained by the <strong>Center for Systems Science and Engineering</strong> (<strong>CSSE</strong>) at Johns Hopkins University is available in the <em>Further reading</em> section. The CDC has also been distributing COVID-19 data for the greater good. </p>
<p>A sample of the first few records in the CSV file will look similar to the following screenshot, which was retrieved from authoritative sources found in the <em>Further reading</em> section:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/d94977de-51f3-450a-b2f0-ea7529ef6fc1.png"/></p>
<p>This data source contains a daily snapshot of the COVID-19 cases by country. The key fields used in our analysis are as follows:</p>
<ul>
<li><kbd>Date</kbd>, which formatted as <kbd>M/D/YYYY</kbd>, is the date a positive COVID-19 case was identified.</li>
<li><kbd>Country_Region</kbd> is the country of origin where the COVID-19 cases are tracked.</li>
<li>The <kbd>Cases</kbd> field is the accumulated count of the number of COVID-19 cases by country and date.</li>
<li>The <kbd>Difference</kbd> field is the daily number of COVID-19 cases by country and date.</li>
<li><kbd>Case_Type</kbd> is the type of case that is assigned to each value and is either <kbd>Confirmed</kbd> or <kbd>Deaths</kbd>.</li>
</ul>
<p>From this data source, we can answer multiple questions about the data but will require some filtering to isolate records by <kbd>Country</kbd>, <kbd>Date</kbd>, and <kbd>Case_Type</kbd>.</p>
<h2 id="uuid-4d5e7cee-8122-4655-8608-6c0bea88977b">Shape of the curve</h2>
<p>Now that we have more information about the data, we can launch a new Jupyter Notebook for analysis to identify the shape of the curve.</p>
<p>Launch a new Jupyter Notebook and name it <kbd>ch_09_exercises</kbd>. To import the data from a CSV to a <kbd>pandas</kbd> DataFrame so we can create a histogram, we use the following steps:</p>
<ol>
<li>Import the following libraries by adding the following codes in your Jupyter Notebook and run the cell. Feel free to follow along by creating your own Notebook; I have also placed a copy in GitHub for reference:</li>
</ol>
<pre style="padding-left: 60px">In[]: import pandas as pd<br/>     import numpy as np<br/>      import matplotlib.pyplot as plt<br/>      %matplotlib inline</pre>
<p>These libraries should already be available using Anaconda. Refer to <a href="e0fe6eb2-8f38-41f7-9dea-2b177578fd3c.xhtml">Chapter 2</a><em>, Overview of Python and Installing Jupyter Notebook</em>, in case you need help with setting up your environment. <kbd>%matplotlib inline</kbd> is a magic command required to display the visual results inside your Jupyter Notebook after you run the cell.</p>
<ol start="2">
<li>Next, we create a new DataFrame by importing the CSV file:</li>
</ol>
<pre style="padding-left: 60px">In[]: covid_df = pd.read_csv("COVID-19 Cases.csv", header=0)</pre>
<p>Be sure you copied the <kbd>COVID-19 Cases.csv</kbd> file to the correct Jupyter folder directory to avoid errors with the connection.</p>
<ol start="3">
<li>To verify the DataFrame has loaded correctly, we can run the <kbd>head()</kbd> function to display the first few records:</li>
</ol>
<pre style="padding-left: 60px">In[]: covid_df.head()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">The output would look like the following screenshot where the source CSV file has been loaded into a DataFrame with a labeled header row with the index column to the left starting with a value of <kbd>0</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/6ab38e8d-4c31-4382-bb2f-0efa75c221bb.png"/></p>
<ol start="4">
<li>Next, we will isolate the data we want to focus our attention on by creating a new DataFrame from the source and applying a few filters against it. We want to isolate the records where all of the following conditions are true. First, the daily <kbd>Difference</kbd> count is greater than zero. Next, the <kbd>Case_Type</kbd> should be <kbd>Confirmed</kbd>. Finally, the <kbd>Country_Region</kbd> should be only <kbd>Italy</kbd>:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_results = covid_df[(covid_df.Difference &gt;0) &amp; (covid_df.Case_Type == 'Confirmed') &amp; (covid_df.Country_Region == 'Italy')]</pre>
<p>The new <kbd>df_results</kbd> DataFrame will not display results in Jupyter Notebook by default.</p>
<ol start="5">
<li>To see the results sorted, we run the following command:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_results.sort_values(by='Cases', ascending=False)</pre>
<p style="padding-left: 60px">The output should look like the following screenshot, where a new <kbd>df_results</kbd> DataFrame is displayed with the values sorted by <kbd>Cases</kbd> in descending order:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/2aa3a9f1-8a39-4271-84e9-3e8f1be9dcde.png"/></p>
<ol start="6">
<li>Now, we want to visually display the distribution of the values in the <kbd>Difference</kbd> column. We can pass an array of values into the default <kbd>hist()</kbd> plot using the following command:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_results.hist(column='Difference');</pre>
<p style="padding-left: 60px">The output would look like the following screenshot, where the array of values are displayed in a default histogram chart. The default settings bin the data values in equal sizes, which are displayed on the <em>x</em> axis. The frequency or count of the number of occurrences of the values falling inside of each bin are measured by the <em>y</em> axis:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/0c85d66d-0d1c-4b23-90b8-2bae06009535.png" style="width:25.92em;height:17.92em;"/></p>
<p style="padding-left: 60px">So, what does this histogram plot tell us about the data in the preceding chart at first glance? First, most of the data values are less than <strong>1,000</strong> since the highest bar is the first bar to the left. Second, we know the data is not a normal distribution based on the shape because we would have expected the most frequent results to be in the middle, closer to the mean of the data. How does this analysis apply to the COVID-19 data itself? This shape is good since the number of daily increases could be much larger.</p>
<p style="padding-left: 60px">To learn more details about the shape of this data, we can use the <kbd>describe()</kbd> function against this specific column in the DataFrame.</p>
<ol start="7">
<li>Use the <kbd>describe()</kbd> function against this DataFrame to see summary statistics. We can look at one column by explicitly passing it in the square brackets along with the column/field name in double quotes:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_results["Difference"].describe()</pre>
<p style="padding-left: 60px">The output would look like the following screenshot where summary statistics about the data in this specific field are displayed:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/dc966697-1b1a-4845-abac-eb5c4088ed2b.png"/></p>
<p>In the preceding screenshot, we have identified some key statistics to help to better understand the shape of this data. There are <kbd>33</kbd> values that are identified as <kbd>count</kbd> in the summary table with a <kbd>mean</kbd> of <kbd>1937.181818</kbd> and a <kbd>std</kbd> (standard deviation) of <kbd>2132.965299</kbd>. The range of those thirty-three values is from <kbd>1</kbd> to <kbd>6557</kbd>, which is identified by <kbd>min</kbd> and <kbd>max</kbd>. With that high of a standard deviation value, we know the numbers are pretty spread out.</p>
<p>The values will be displayed with a datatype of <kbd>float64</kbd> with a precision of six decimal places regardless of the source number value.</p>
<p>The 25%, 50%, and 75% labels return the respective percentiles for the series of values in this field. These values are also known as the <strong>Interquartile Range</strong> (<strong>IRQ</strong>) with the 50% or second quartile equal to the <strong>median</strong>. Having the data in quartiles creates equal bins or buckets for the data values to help us understand how the numeric values are distributed. If a majority of the values fall into one specific bucket, you know the data is not evenly distributed. With our example, we have a large gap between our mean and median with our data (1937 versus 778) so we can classify this data as skewed. Having a skew in our data helps to understand that the visual shape of the distribution curve or histogram is not symmetrical. To help you to remember the distribution types and skewness, I have summarized them in the following graph. When the mean is greater than the median, it would have a positive skew and when the opposite is true, a negative skew exists. As described at the top of each visual trend in the following diagram, the type of skew (negative or positive) is directly correlated with the mean and median values. When all of the mean, median, and mode values are equal, you have a symmetrical distribution:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/bbb7bd7d-79af-4d33-8459-7e8c5271f1ce.png" style="width:71.17em;height:28.50em;"/></p>
<p>From a data analysis perspective, having these key statistics defined helps us to understand the spread of the data values. Calculating the mean, median, and mode against your data values are collectively known as the measures of <strong>central tendency</strong>, which we introduced in <a href="9bdac090-8534-480e-8154-a854115c0b7a.xhtml">Chapter 8</a>, <em>Understanding Joins, Relationships, and Aggregates</em>. A very important and practical use of central tendency is in data science models. In predictive regression models that use historical data, the ability to calculate a prediction is based on finding a <em>best fit</em> to the distribution curve. If the data has a dramatic positive or negative skew with <em>long tails</em>, which is when values trail off by multiple standard deviations from the central tendency, the algorithm becomes less accurate. </p>
<p>So, now we understand the importance of calculating central tendency and how symmetrical data is visually represented as a normal distribution curve. A normal distribution, also known as the Gaussian distribution and the bell curve, occurs when the mean (average) is equal to the median (middle) and is equal to the mode (most frequent). I find adding a normal distribution line useful as a reference to compare against the actual data results in charts. This helps the consumer of the analysis to visually compare the ideal shape of the data versus the actual results. So, what causes data to skew or not fit into a normal distribution? As a data analyst, your job is to find out why and the first step is to isolate outliers that may exist in the data. We will discuss this in the next section by understanding outliers and trends.</p>
<h1 id="uuid-f62f829c-3d87-431d-a516-5c488e89a296">Understanding outliers and trends</h1>
<p>Finding outliers begins by looking at the distribution curve but requires additional techniques that we will walk through together. Additionally, don't underestimate the need for soft skills where you must reach out to others to better understand why an outlier exists in your data. An outlier is commonly known as one or more data values that are significantly different than the rest of the data. Spotting outliers in data is easy depending on the data visualization used, but in many cases, especially when data volumes are very large, they can be obscured when data is aggregated. If you recall from <a href="7282a629-c59a-4922-8422-e27ed44563db.xhtml">Chapter 7</a>, <em>Exploring Cleaning, Refining, and Blending Datasets</em>, we worked with hits created by a user for a website. A good example of obscuring outliers is when those user hits are aggregated by date. If a specific user has 1,000 hits per day when the average is 2, it would be difficult to identify that outlier user after the data was aggregated by week. So, what does an outlier look like visually in a series of data values? A good approach would be to use a box plot because it visually represents the data found in the <kbd>describe()</kbd> function:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/6b584e47-3c1e-4d5a-a84f-bdb6e535add2.png" style="width:38.58em;height:9.33em;"/></p>
<p>As you can see in the preceding diagram, the box isolates the quartiles of 25%, 50%, and 75%, and the min/max range of values is displayed at the most extreme vertical lines. The space between the box and the min/max lines is known as the whiskers of the box plot. If you see a plus symbol (+) displayed, they are known as fliers, which are outliers in this chart type.</p>
<p>A box plot can be displayed horizontally or vertically and can include multiple dimensions so you can compare the distribution between them.</p>
<p>Let's continue to analyze our existing dataset and see how it would be visualized using a box plot. Similar to the prior example, we will load all of the data from the source into a single DataFrame and then create a subset DataFrame using filters. We will continue using the <kbd>ch_09_exercises</kbd> Jupyter Notebook:</p>
<ol>
<li>Import the following libraries by adding the following command in your Jupyter Notebook and run the cell. Feel free to follow along by creating your own Notebook; I have placed a copy on GitHub for reference:</li>
</ol>
<pre style="padding-left: 60px">In[]: import pandas as pd<br/>      import numpy as np<br/>      import matplotlib.pyplot as plt<br/>      %matplotlib inline</pre>
<ol start="2">
<li>Create a new DataFrame by importing the CSV file:</li>
</ol>
<pre style="padding-left: 60px">In[]: covid_df = pd.read_csv("COVID-19 Cases.csv", header=0)</pre>
<ol start="3">
<li>To verify the DataFrame has loaded correctly, we can run the <kbd>head()</kbd> function to display the first few records:</li>
</ol>
<pre style="padding-left: 60px">In[]: covid_df.head()</pre>
<p style="padding-left: 60px">The output would look like the following screenshot where the source CSV file has been loaded into a DataFrame with a labeled header row with the index column to the left starting with a value of <kbd>0</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/aa8763d9-9494-4754-908f-59c488c66fa6.png"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">Similar to the prior exercise, we will isolate the data we want to focus attention on by creating a new DataFrame from the source and applying a few filters against it. We want to isolate the records where all of the following conditions are true. First, the daily <kbd>Difference</kbd> count is greater than zero. Next, <kbd>Case_Type</kbd> should be <kbd>Confirmed</kbd>. Finally, we use the pipe symbol, <kbd>|</kbd>, to create an <kbd>or</kbd> condition to allow for multiple <kbd>Country_Region</kbd>:</p>
<pre style="padding-left: 60px">In[]: df_results = covid_df[(covid_df.Difference &gt;0) &amp; (covid_df.Case_Type == 'Confirmed') &amp; ((covid_df.Country_Region == 'Italy') | (covid_df.Country_Region == 'Spain') | (covid_df.Country_Region == 'Germany'))]</pre>
<p>The new <kbd>df_results</kbd> DataFrame will not display results in Jupyter Notebook by default.</p>
<ol start="4">
<li>To see the results, we run the following command:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_results.head()</pre>
<p style="padding-left: 60px">The output should look like the following screenshot where a new <kbd>df_results</kbd> DataFrame is displayed:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/9b9caff9-64ea-462c-bcd7-87205e95e2d3.png"/></p>
<ol start="5">
<li>To display a box plot by <kbd>Country</kbd>, we use the following command. <kbd>boxplot()</kbd> has a few parameters such as <kbd>by=</kbd>, which allows us to group the data by <kbd>Country_Region</kbd>. We also include <kbd>column=</kbd> to isolate the values in the <kbd>Difference</kbd> field. Finally, we pass in <kbd>grid=False</kbd> to turn off the gridlines in the chart:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_results.boxplot(by='Country_Region', column=['Difference'], grid=False)</pre>
<p class="mce-root"/>
<p style="padding-left: 60px">The output would look like the following screenshot where a box plot will be displayed:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/3dace95a-85fc-46e7-9127-c2123f25a017.png" style="width:36.92em;height:23.08em;"/></p>
<p>So, having the data limited to only three countries allows us to narrow our analysis, and having the data presented side by side in the box plot chart, as shown in the preceding screenshot, allows us to visually compare the results. First, we notice a few box sizes, which, if you recall, are the quartiles of the data and are different sizes depending on <kbd>Country</kbd>. Germany has a smaller box that is closer to a square than a rectangle, which typically tells us the data spread is much tighter. Another observation we can identify in this chart is we have multiple plus symbols (+) displayed; these highlight outliers that exist in the countries of Germany and Spain.</p>
<p>Analyzing data by country and other attributes related to geography is a common requirement today for a data analyst. Next, we will explore best practices related to visually representing data in maps and spatial data, which is known as geoanalytics.</p>
<h1 id="uuid-57e76bac-e249-4c6e-85d9-8085cf7350ec">Geoanalytical techniques and tips</h1>
<p>For a data analyst, the concept of geoanalytics is a relatively new technique applied to spatial data to understand where data is geographically located. However, cartography, which is the study of maps, has been around for centuries and traditionally requires training, expertise, and niche software to provided insights from data by location. Today, there are multiple add-on modules and software available to create charts and visualizations that use maps to visualize data in exciting ways that provide a different perspective.</p>
<p class="mce-root"/>
<p>First, you need to understand the grain of the data you have available. Having precision of the exact latitude and longitude available in your source data is a luxury unless the source system was built to capture that information. For example, mobile app source data will commonly have this level of detail available because a smartphone can track your location. However, if we go back to our COVID-19 source data, the individual cases' lowest level of detail available is by <kbd>Province_State</kbd> so you lose the ability to display data below that level.</p>
<p>Next, if your source data does not include latitude and longitude values, you will have to add it, which sounds simple at first but usually has some challenges that you will have to overcome. When you profile the data, ensure it has conformity and consistency in specific fields such as <kbd>Country</kbd>, <kbd>City</kbd>, <kbd>Parcel</kbd>, or <kbd>Zip Code</kbd>. If the <kbd>Country</kbd> values exist, do they have <strong>International Organization for Standardization</strong> (<strong>ISO</strong>) codes so you can join the data to commonly available sources? If so, I have included a source a link to the World Bank data source in the <em>Further reading</em> section.</p>
<p>Finally, I find including a world or regional map as a good complement to, but not a replacement for, good data analysis solutions. In many cases, having a single map chart even with color gradients does not provide enough answers to common questions. For example, let's look at the following screenshot, which shows a global map of the COVID-19 outbreak found on the HealthMap site:</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img src="img/2711cc1d-13c0-4e46-9a2a-83c78b7411d4.png" style="width:75.08em;height:30.50em;"/></p>
<p>As displayed in the preceding screenshot, the legend at the bottom right of the chart provides insights on the outbreak by location using the color and size of the bubbles. If you look at this visualization as a static map, it leaves the audience asking more questions about the overlap of the bubbles with colors. For example, how do you distinguish between the details of closely clustered cities such as New York and Philadelphia? Also, the consumer of this geoanalytic chart might want to know whether the number of cases is accelerating per location, which is unclear.</p>
<p>However, these criticisms change once you visit the site, which allows the user to interact with the map.</p>
<p>When you use the HealthMap site created for the COVID-19 outbreak, you are offered a solution that provides the following features:</p>
<ul>
<li>Zoom capabilities to drill down to the lowest level of detail and zoom out to compare results across different locations</li>
<li>Mouse hover over the circles that provide aggregated counts by location along with the context of which location is selected or has the focus</li>
<li>A timeline feature that allows the user to see before and after results</li>
</ul>
<p>Once you appreciate having any or all of these features available in geoanalytics data, you expect it for any solution you are building as a data producer. If you are missing the data or tools to create it, I would recommend creating a complementary chart to support it. For example, in the following screenshot, we have a horizontal bar chart sorted in descending order by country:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/dde55948-937d-4639-809d-b60c97c7edfc.png" style="width:57.42em;height:33.92em;"/></p>
<p>The legend at the top of the chart indicates the measure used, which is titled <strong># of Cases by Country</strong>, to help the consumer of the data easily answer questions such as which country has the highest number of cases as of a specific date. When you bring together multiple charts that complement each other, they provide context to the consumer of the geographic data.</p>
<p>Be sure the common join key between the data sources behind these charts is consistent. If you select a country on the map, the bar chart should filter to match along with any date selections.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>Using multiple charts in tandem helps to tell a story with the information to build a connection with the audience. The consumers of data today are sophisticated so telling the same story over and over will not be effective. I recommend using time-honored techniques such as having a lesson or moral for the story to work well and you should feel empowered to adjust to include your own personal style.</p>
<p>I find creative inspiration from art and learning more about the masters of their craft. For example, the artist Pablo Picasso is well known for works created during his <strong>Blue Period</strong>, which defined a time in his life when all variations of the color blue were the primary color used commonly across different subjects he painted. This period lasted a few years and reflected his personal struggles living with depression and financial distress. In comparison, the COVID-19 pandemic is creating personal struggles for people all around the world. The high levels of mortality related to the COVID-19 data are causing global financial distress and numerous stories of personal loss. Picasso produced a staggering volume of work during his lifetime, with well over 100,000 pieces over a 70 plus year timeframe. Even during times of emotional distress, Picasso continued to find the strength to create new works of art and master his craft. I can relate to his struggles during this pandemic and am inspired to spend time on my data craft to help me through these trying times. </p>
<p>The power of storytelling with data becomes a critical skill to build trust with your audience so they can understand the information. Using data visualizations breaks down the technical barriers that can exist when working with data. As a person fluent in data literacy, you now have the additional skills required to create your own story using data. </p>
<p>Now that you understand the geographic techniques that are effective in data storytelling, let's focus our attention on identifying patterns within data.</p>
<h1 id="uuid-b548878d-3966-4c7b-90cc-f06490543385">Finding patterns in data</h1>
<p>Now that we have a better understanding of distribution curves and spotting outliers that can exist in your data, let's break down how to find patterns in your data. In my experience, as you work with more and more data, you will start to develop a <strong>sixth sense</strong> that will help you identify patterns faster, for example, the following diagram may appear like a random list of numbers where no distinguishing pattern is obvious, until you make some minor changes to make it easier to identify:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/4d9bd121-691c-43bd-b400-da9ae5fa6dbe.png" style="width:34.00em;height:4.08em;"/></p>
<p>Having the data sorted allows you to see groupings and clusters that exist within the data values. In this case, we have pairings of numbers that were not as evident until you sorted them together. With a quick sort, we can now see all of the numbers are duplicated, as in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/45b3b47a-30ab-47af-9b50-05c7e44d506b.png" style="width:32.00em;height:3.92em;"/></p>
<p>To hammer this point home, look at the following diagram where those same numbers from the preceding two diagrams are now colored by pairs, which creates a pattern to make it easier to visually identify:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/a027d950-2f62-49e2-86c7-9f5e0a72eec7.png" style="width:33.00em;height:3.75em;"/></p>
<p class="CDPAlignLeft CDPAlign">Data visualizations and charts will help you to identify these patterns as well, but some charts are better suited than others depending on the data. For example, to see patterns in the data over time, a line or bar chart will display the trends better and help to create recognizable patterns in your data. A good example of that can be seen in the following screenshot, which is a bar chart with a measure of <strong>Hits</strong> on the <em>y</em> axis and a dimension of date with the day of the week in the <kbd>dtype</kbd> format of <kbd>M/DD/YYYY-DDD</kbd> displayed on the <em>x </em>axis:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/f0de6865-a97a-4bac-8e99-7c71efc5d521.png"/></p>
<p>In the preceding screenshot, we have a trend chart that displays a pattern of web hit usage over each date. What becomes more evident when you look at this data sorted is the peaks and valleys that naturally appear every few days. Even without understanding all of the details behind it, having this pattern suggests our weekday data usage is higher than weekends, which I highlighted in a different color to make it more obvious.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>During this time of analysis and identifying patterns, you will find yourself coming to conclusions about the data. Doing this is natural and if based on business or domain expertise, is viable. If you find patterns in your data that apply to multiple dimensions or variables, you can identify a correlation between them. Correlations are common in data, especially when overlaying patterns over the same timeframe. A more formal definition is when one variable or series of values increase or decrease, a second variable will follow in parallel. A common example of a correlation between two values would be ice-cream store sales and the weather. If the weather has snow or heavy rain or is cold, ice-cream sales are typically lower. If the weather is warm and sunny, sales would be higher. Based on this information, you could say there is a <strong>positive</strong> correlation between the variables of sales and weather over the same period of time.</p>
<p>If the opposite relationship exists, where the inverse pattern between the two variables occurs, this would be considered a <strong>negative</strong> correlation.</p>
<p>To determine whether two variables are statically correlated, there is a concept called the <strong>correlation coefficient. </strong>This is a measurement that falls between <kbd>1</kbd> and <kbd>-1</kbd> and is denoted by <strong><em>r</em></strong>. If the value is <kbd>0</kbd>, there is no correlation between the two variables. The closer the values are to <kbd>1</kbd>, they have a positive correlation, which means when one variable's value changes, the other will trend in the same direction. The opposite is true when the values are closer to <kbd>-1</kbd>, where a negative correlation creates an inverse relationship between the two variables. So, can we visually see a correlation and pattern with data? A good approach would be to use a scatter plot.</p>
<p>Let's continue to analyze our existing dataset and see how it would be visualized using the scatter plot. Similar to the prior example, we will load all of the data from the source into a single DataFrame and then create a subset DataFrame using filters. In this example, we are going to create two subsets to allow for comparisons. We will continue using the <kbd>ch_09_exercises</kbd> Jupyter Notebook:</p>
<ol>
<li>Import the following libraries by adding the following command in your Jupyter Notebook and run the cell. Feel free to follow along by creating your own Notebook; I have placed a copy on GitHub for reference:</li>
</ol>
<pre style="padding-left: 60px">In[]: import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</pre>
<ol start="2">
<li>Create a new DataFrame by importing the CSV file:</li>
</ol>
<pre style="padding-left: 60px">In[]: covid_df = pd.read_csv("COVID-19 Cases.csv", header=0)</pre>
<p class="mce-root"/>
<ol start="3">
<li>We will now create two new DataFrames, which will be subsets from the original source. The advantage of naming them generically as <kbd>df_results_1</kbd> and <kbd>df_results_2</kbd> is that it allows you to adjust the filters such as <kbd>Country_Region</kbd> used in this one line without changing any other code in the additional steps:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_results_1 = covid_df[(covid_df.Case_Type == 'Confirmed') &amp; (covid_df.Country_Region == 'Germany')]</pre>
<ol start="4">
<li>Run the <kbd>head()</kbd> function to validate the results:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_results_1.head()</pre>
<p style="padding-left: 60px">The output will look like the following table where a new <kbd>df_results_1</kbd> DataFrame is displayed:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/ec152148-c135-4fc8-beb8-0348eecd31cc.png"/></p>
<ol start="5">
<li>We will load the second DataFrame that we will use to compare with the first using the following commands:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_results_2 = covid_df[(covid_df.Case_Type == 'Confirmed') &amp; (covid_df.Country_Region == 'Italy')]</pre>
<ol start="6">
<li>Run the <kbd>head()</kbd> function to validate the results:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_results_2.head()</pre>
<p style="padding-left: 60px">The output would look like the following table where a new <kbd>df_results_2</kbd> DataFrame is displayed:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/099f17f6-3dac-4264-9bfa-44d4cf3b1a09.png"/></p>
<ol start="7">
<li>Let's profile the data in each DataFrame to better understand it. We use the <kbd>describe()</kbd> function to better identify key statistics and how the data is distributed. First, we look at the contents of the first DataFrame:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_results_1["Cases"].describe()</pre>
<p style="padding-left: 60px">The output will look like the following screenshot where results are displayed:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/aaa6f682-d579-4693-ad08-a94e4a754998.png" style="width:20.33em;height:10.75em;"/></p>
<div><ol start="8">
<li>Then, we look at the contents of the second DataFrame:</li>
</ol>
</div>
<pre style="padding-left: 60px">In[]: df_results_2["Cases"].describe()</pre>
<p style="padding-left: 60px">The output will look like the following screenshot where results are displayed:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/d8439c3e-99e1-413a-99cd-26331e7e99f8.png" style="width:19.92em;height:11.00em;"/></p>
<p style="padding-left: 60px">Based on the results of the <kbd>describe()</kbd> function run against each DataFrame, we have a basis for comparison. First, we have the count, which is the same value of <kbd>61</kbd> values. This is important when creating a scatter plot since the size of the data is required to be the same. Another common value between the two data series is the minimum, which is at <kbd>0</kbd>. However, the maximum values are different, which are slightly larger than double (29,056 versus 63,927). Next, we have the mean, which is vastly different. The first results have a rounded mean value of <kbd>2,707.49</kbd> and the second is <kbd>8,241.77</kbd>. Finally, the standard deviation (std) is different as well so we know the shape and size of the distribution curves will be different.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">The question we want to answer is: are these values correlated? To confirm this visually, we continue by creating a scatter plot with a few simple commands. The scatter plot will have <em>x</em> and <em>y</em> axes and plot the values in a grid with dots representing where the values align to the axis.</p>
<ol start="9">
<li>We use the <kbd>plt.scatter()</kbd> function to create the visualization. It requires two parameters, which are the <em>x</em> and <em>y</em> axes values separated by a comma. We are passing the common series of values found in the <kbd>Cases</kbd> column from each DataFrame. We also include labels and a title to help the audience to understand the chart:</li>
</ol>
<pre style="padding-left: 60px">In[]: plt.scatter(df_results_1["Cases"], df_results_2["Cases"]);<br/>plt.title("# of Cases")<br/>plt.xlabel("Germany Cases")<br/>plt.ylabel("Italy Cases");</pre>
<p style="padding-left: 60px">The output would look like the following graph where results are displayed:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/925dbe77-c937-4018-b26e-6ac82b1db5e4.png" style="width:30.00em;height:19.92em;"/></p>
<p>The result of our scatter plot does have a correlation where the values closer to 0 on either axis cluster together, which are shown with overlapping blue dots on the chart. You can also observe a natural straight-line alignment between the values as the <strong># of Cases</strong> increase.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>With every correlation comes the inevitable expectation that one variable is dependent on the other and is the cause. Causation is when one variable directly impacts the direction of another. Cause and effect or root cause analysis are common analysis techniques. Identifying causation between only two variables is rare and requires deeper analysis to understand all of the factors that directly and indirectly impact the change. The first point to understand is that <em>correlation doesn't always equal causation</em>. A good example is our ice-cream store sales and the weather. We know that eating more ice-cream would never improve the weather but if you look purely at the data, you might accidentally come to that kind of conclusion if the data is highly correlated. Another point when analyzing data to determine whether correlation and causation are related is based on the sample size of the data. I would recommend being a <strong>data skeptic</strong> and question a causative conclusion if the population of data is incomplete or covers a small window of time. Finally, we have walked through the value of joining data for data analysis but that invites the opportunity to come to conclusions about data that didn't exist independently. Be sure to add assumptions and be transparent with the methods of how you joined the data so the correlations can be peer-reviewed. </p>
<h1 id="uuid-15ed172a-ecd4-49eb-b535-5b89225e0632" class="mce-root">Summary</h1>
<p>Congratulations, we have now learned some essential skills for making various plots that visualize the distribution of data. We discussed key statistics related to the central tendency of data by calculating the standard deviation, mean, median, and mode of a series of data values. We looked at normal distributions and how data values can be skewed positively or negatively. When data has symmetry, it becomes easier to work with some algorithms found in predictive analytics. We reviewed patterns and outliers that are common when working with datasets, along with how to use a box plot chart to visualize outliers.</p>
<p>We discussed best practices and tips for working with geospatial data, along with how it can be used to help to tell a story with data. Finally, we discussed the difference between correlation versus causation along with the importance of the correlation coefficient, so you can understand the relationships between two variables/series of data values.</p>
<p class="mce-root">In our next chapter, we will be switching to working with unstructured data sources and best practices when working with free text data.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<h1 id="uuid-44484f16-f119-460f-81eb-016d0893c5d5" class="mce-root">Further reading</h1>
<p>For more information on the relative topics of this chapter, you can visit the following links:</p>
<ul>
<li class="mce-root">Authoritative sources for COVID-19 data: <a href="https://github.com/CSSEGISandData/COVID-19">https://github.com/CSSEGISandData/COVID-19</a></li>
<li class="mce-root">Centers for Disease Control and Prevention COVID-19 data: <a href="https://www.cdc.gov/coronavirus/2019-ncov/">https://www.cdc.gov/coronavirus/2019-ncov/</a></li>
<li>Cheatsheets to help to create data visuals using Python: <a href="https://python-graph-gallery.com/cheat-sheets/">https://python-graph-gallery.com/cheat-sheets/</a></li>
<li>The World Bank ISO country data: <a href="https://datahelpdesk.worldbank.org/knowledgebase/articles/898590-country-api-queries">https://datahelpdesk.worldbank.org/knowledgebase/articles/898590-country-api-queries</a></li>
<li>Open source mapping software: <a href="https://www.openstreetmap.org/">https://www.openstreetmap.org/</a></li>
<li>HealthMap geoanalytics example of COVID-19: <a href="https://www.healthmap.org/covid-19/">https://www.healthmap.org/covid-19/</a></li>
</ul>


            

            
        
    </body></html>