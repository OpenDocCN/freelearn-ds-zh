- en: Understanding Joins, Relationships, and Aggregates
  prefs: []
  type: TYPE_NORMAL
- en: I'm really excited about this chapter because we are going to learn about the
    foundation of blending multiple datasets. This concept has been around for decades
    using SQL and other technologies including R, pandas, Excel, Cognos, and Qlikview.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to merge data is a powerful skill that applies across different
    technologies and helps you to answer complex questions such as how product sales
    can be impacted by weather forecasts. The data sources are mutually exclusive,
    but today, access to weather data can be added to your data model with a few joins
    based on geographic location and time of day. We will be covering how this can
    be done along with the different types of joins. Once exposed to this concept,
    you will learn what questions can be answered depending on the granularity of
    data available. For our weather and sales data example, the details become important
    to understand the level of analysis that can be done. If you wanted to know whether
    rain impacts sales, the more common fields available, such as date, day, and time,
    and geographic location tags, such as latitude and longitude, must be available
    in both sources for you to be accurate in your conclusions after joining the data
    together.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to construct high-quality datasets for further
    analysis. We will continue to advance your hands-on data literacy skills by learning
    how to work with join relationships and how to create aggregate data for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Foundations of join relationships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join types in action
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining data aggregation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary statistics and outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here's the GitHub repository of this book: [https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter08](https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter08).
  prefs: []
  type: TYPE_NORMAL
- en: You can download and install the required software from the following link: [https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual).
  prefs: []
  type: TYPE_NORMAL
- en: Foundations of join relationships
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For anyone familiar with SQL, the concept of joining data together is well understood.
    The ability to join one or more tables together for the purpose of analytics has
    remained relevant throughout my 20+ year career of working with data and I hope
    it continues to be relevant.
  prefs: []
  type: TYPE_NORMAL
- en: In prior chapters, we introduced the concept of data models and the need for
    primary and foreign key fields to define relationships. We will now elaborate
    on these concepts by explaining joins and the different types of joins that exist
    in SQL and DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Joining, in SQL, simply means merging two or more tables into a single dataset.
    The resulting size and shape of that single dataset will vary depending on the
    type of join that is used. Some key concepts you want to remember any time you
    are creating a join between datasets will be that the **common unique key** should
    always be used. Ideally, the key field functions as both the primary and foreign
    key but it can be derived using multiple fields to define a unique record for
    all rows of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will go through the following types of join relationships in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: One-to-one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many-to-many
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Left join
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right join
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inner join
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outer join
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With pandas DataFrames, the index in our examples has been the default or a
    single defined field because it has distinct values, but that is not always the
    case.
  prefs: []
  type: TYPE_NORMAL
- en: One-to-one relationships
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **one-to-one** relationship means the sources have common unique values by
    row and duplicates do not exist. A similar feature in Excel is the `vlookup` function
    with the exact match parameter enabled. When you use this Excel function, any
    matches to the source identifier return a distinct value from the target lookup.
    In SQL, one-to-one relationships ensure that the integrity between two tables
    is consistent. There are multiple reasons why these types of relationships are
    needed, but a good example is when a reference table that has the sales region
    is joined to a unique `Customer` table. In this example, a customer identifier
    (`ID`) field would exist in both tables and you would never have a sales region
    without a customer record and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: Many-to-one relationships
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **many-to-one relationship **means one of the sources can have duplicate rows
    but not both sources. You still need a unique key, index, or identifier between
    the sources. An example would be a lookup dimensional table joined to a fact table,
    which we covered in [Chapter 7](7282a629-c59a-4922-8422-e27ed44563db.xhtml), *Exploring
    Cleaning, Refining, and Blending Datasets*.
  prefs: []
  type: TYPE_NORMAL
- en: A transaction fact table will have duplicate records because a user hit for
    a website or a product sale is recorded by date/time for each occurrence. The
    result will generate millions of rows with duplicate records in the `userid` field.
    When the join uses a common field such as `userid` between the fact table and
    the second table, the second table must be unique in each row. This second table
    will have additional attributes about `userid`, such as `city`, `state`, and `zip
    code`, which will offer richer analysis options. Once you understand that this
    type of join relationship exists between two source tables, you can confidently
    join them together.
  prefs: []
  type: TYPE_NORMAL
- en: Many-to-many relationship
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A **many-to-many relationship **is when both sources have duplicate rows. Again,
    you should find a common unique key (one or more fields) or index between the
    sources. These types of joins are commonly referred to as *expensive* joins because
    the number of computing resources (memory and CPU) required will increase dramatically
    depending on the number of records and columns from both sources. A common example
    is a logical relationship between students and classes where many students can
    take many different classes. Conversely, many classes can have many different
    students. As a best practice, I would try to avoid direct many-to-many joins and
    use a bridge table to resolve them for analysis. For the students-to-classes example,
    you would need a roster table that marries together the unique list of one student
    per class, as in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6acde73e-e704-45c2-aac9-7e460f6a48ef.png)'
  prefs: []
  type: TYPE_IMG
- en: For any join, you should avoid a **Cartesian product**, which is where all possible
    combinations of rows and columns exist as a result of the join. There are some
    cases where this might be useful in your analysis but be cautious, especially
    with big data volumes.
  prefs: []
  type: TYPE_NORMAL
- en: A Cartesian product is when all possible combinations of rows and columns are
    combined. For our example, if we joined `tbl_students` and `tbl_classes` without
    including `tbl_roster`, you would end up with students assigned to classes they
    never signed up for. There are occasions where I have deliberately constructed
    a Cartesian product because it was needed for certain types of analysis or a chart.
    For example, if you have a student ranking scale from 1 to 10 but none of the
    students achieved all of the possible values, you could create a Cartesian join
    to fill in the gaps of missing values.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to concerns about exceeding memory and CPU utilization when working
    with many-to-many join types, a Cartesian product can easily consume all available
    memory, which could lead to a crash of your workstation or workspace.
  prefs: []
  type: TYPE_NORMAL
- en: Left join
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have covered the key concepts, I''m going to begin with a visual
    representation of one of the most common types of joins used in analysis, which
    is called a **left join**. Let''s start by looking at our source data for this
    example, which is represented in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12434f84-6fb4-41d4-a70d-70ed4704987e.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we have tables named `tbl_user_hits` and `tbl_user_geo`. The
    width and length of `tbl_user_hits` is two columns and three rows. In `tbl_user_geo`,
    which represents the user's geographical location, we have three columns and five
    rows. We have a common field named `userid` in both tables, which I highlighted
    and will use to join the data. These tables have a primary and foreign key many-to-one relationship
    because not all of the records from one table exist in the other.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we want to keep all of the records in `tbl_user_hits` and
    enrich the data by blending together matching attributes such as city and state
    where `userid` exists only for the records in the user hits table. The result
    is in the following screenshot, where the original source, `tbl_user_hits`, has
    the same number of rows and but now includes the columns from the `tbl_user_geo` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66e00ca9-6dc5-4a48-b67a-5b8c3e02551f.png)'
  prefs: []
  type: TYPE_IMG
- en: Why did the number of rows remain the same but the number of columns increase?
    A successful left join preserves the number of rows from the source and extends
    the number of columns.
  prefs: []
  type: TYPE_NORMAL
- en: The specific columns included in the join result can be defined but the default
    will include all of the columns.
  prefs: []
  type: TYPE_NORMAL
- en: Why are left joins common in analytics? That's because we are interested in
    adding more dimensional fields to answer more questions about the data that does
    not exist in a single table alone. Also, in your analysis, you typically don't
    want to include anything that doesn't match our source user hits. With this new
    join result, we can now answer questions such as which city has the highest number
    of users. Using a few date calculations, we can also provide monthly trends by
    state.
  prefs: []
  type: TYPE_NORMAL
- en: Right join
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we have a join type called a **right join**, which I find is less common.
    That is because there are not many use cases where the desire is to create gaps
    in the records of your merged data. A right join is where you want to preserve
    all of the columns and rows of the second table and fill in matching values from
    the first.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the concept, start by referring back to our prior source tables, `tbl_user_hits`, and `tbl_user_geo`.
    The result of a successful right join is shown in the following screenshot, where
    the join result is shown with five rows and four columns. The date field from
    `tbl_user_hits` has been combined with the `tbl_user_geo` source but missing values
    will appear as `null()` or `NaN`. Note that if `tbl_user_hits` had thousands or
    millions of rows, the join result would increase the original size of `tbl_user_geo`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6498169-9f5a-4efb-b2eb-8132350e11d4.png)'
  prefs: []
  type: TYPE_IMG
- en: One advantage of using the right join is that now you can identify which cities
    and states do not have any user hits. This could be useful information and could
    be used for marketing campaigns.
  prefs: []
  type: TYPE_NORMAL
- en: Inner join
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we have an **inner join**. This is when only the exact values from both
    tables are returned along with all of the columns. To demonstrate the effect,
    I have made some adjustments to our source tables, which are shown in the following
    screenshot. The table names are the same but now some of the prior records are
    removed from `tbl_user_geo`. This could be due to a regulation request or the
    `userid` rows could have been determined to be invalid, so now we can use an inner
    join to remove them from `tbl_user_hits`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b92bbd9-fb31-4be7-b59f-1315cea80175.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The join results are shown in the following screenshot, where only the matching
    values found in the `userid` key field are displayed along with all of the combined
    columns between both source tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d5915b31-4d17-4cd3-b1a5-4c702ce43841.png)'
  prefs: []
  type: TYPE_IMG
- en: The pandas function for performing joins is called `merge`and an inner join
    is the default option.
  prefs: []
  type: TYPE_NORMAL
- en: Outer join
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, we have an **outer join**, which provides a comprehensive list of
    all rows and columns from both sources. Unlike a **Cartesian**, where any possible
    combination of values is created, an outer join reflects the truth from both source
    tables. For our example, we will use the same source as the following screenshot,
    where records from `tbl_user_geo` were removed. Unlike an inner join, the outer
    join results provide you with the ability to see any missing records as null in
    SQL or `NaN` in Python/pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/07560fb3-1b9b-463e-82df-5da700bb27ad.png)'
  prefs: []
  type: TYPE_IMG
- en: While these concepts and common join types are not exhaustive, you now have
    a good foundational understanding of joining data sources together so we can move
    forward with walking through practical examples.
  prefs: []
  type: TYPE_NORMAL
- en: Join types in action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unfortunately, the SQLite database that we use does not support all of the join
    options (right and outer), so I will provide only two examples of a join (left
    and inner) using SQL in this walk-through. The good news is that the `pandas`
    library supports all join types using the `merge()` function, so we can recreate
    all of the examples already discussed. Feel free to walk through the following
    code; I have placed a copy of the Jupyter Notebook code on GitHub for reference.
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to copy any dependencies files into your working folder before walking
    through all of the steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin by launching a new Jupyter notebook and naming it `ch_08_exercises`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load a SQLite database connection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This library should already be available using Anaconda. Refer to [Chapter 2](e0fe6eb2-8f38-41f7-9dea-2b177578fd3c.xhtml), *Overview
    of Python and Installing Jupyter Notebook*, for help with setting up your environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to assign a connection to a variable named `conn` and point to
    the location of the database file, which is named `user_hits.db`. Since we already
    imported the `sqlite3` library in the prior `In[]`line, we can use this built-in
    function to communicate with the database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Be sure that you copied the `user_hits.db` file to the correct Jupyter folder
    directory to avoid errors with the connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To run a SQL statement and assign the results to a DataFrame, we have to run
    this one line of code. The `pandas` library includes a `read_sql_query` function
    to make it easier to communicate with databases using SQL. It requires a connection
    parameter that we named `conn` in the previous steps. We assign the results to
    a new DataFrame as `df_left_join` to make it easier to identify:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: SQL supports the concept of an alias for table names so you can shorten the
    syntax. In this example, `tbl_user_hits` has an alias of `u` and `tbl_user_geo`
    is `g`. This helps when explicitly calling field names that require a prefix of
    the table name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the results in a DataFrame, we can use all of the available
    `pandas` library commands against this data without going back to the database.
    To view the results, we can just run the `head()` command against this DataFrame
    using this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like the following screenshot, where the SQL results have
    been loaded into a DataFrame with a labeled header row with the index column to
    the left starting with a value of `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df65ebe0-1ab5-41f8-b7d9-d7077c60267f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The next join will be an inner and we will assign the results to a new DataFrame
    as `df_inner_join` to make it easier to identify:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the results in a DataFrame, we can use all of the available
    `pandas` library commands against this data without going back to the database.
    To view the results, we can just run the `head()` command against this DataFrame
    using this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The output will look like the following screenshot, where the SQL results have
    been loaded into a DataFrame with a labeled header row with the index column to
    the left starting with a value of `0:`
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca7e2492-9be7-4989-aee6-d8dcf457d106.png)'
  prefs: []
  type: TYPE_IMG
- en: We will continue the walk-through of the remaining exercise using the `merge()`
    function in pandas. I have a reference with more details about the function in
    the *Further reading* section, but using it is very straightforward. Once you
    have the two input tables stored as DataFrames, you can input them into the `merge()`
    function as parameters along with the join type you want to use, which is controlled
    using the `how` parameter. The default when you don't specify a parameter is an
    inner join, and it supports all of the SQL joins we have discussed, including
    left, right, and outer. The result of using the `merge()` function is to return
    a DataFrame with the source objects merged. Let's continue this walk-through exercise
    by loading our source SQL tables as DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new DataFrame called `df_user_hits`, which is a duplicate of the `tbl_user_hits`
    table, so we can use it later in the examples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To validate the results, you can run the `head()` function against this DataFrame
    using this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like the following screenshot, where the SQL results have
    been loaded into a DataFrame with a labeled header row with the index column to
    the left starting with a value of `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac631f3a-a94f-456f-ba70-eadd2e708bb5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Create a new DataFrame called `df_user_geo`, which is a duplicate of the `tbl_user_geo`
    table, so we can use it later in the examples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would look like the following screenshot, where the SQL results
    have been loaded into a DataFrame with a labeled header row with the index column
    to the left starting with a value of `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/454ba42c-acad-44f2-ba97-6c157bbff459.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s a best practice to close the database connection since we no longer need
    to run any SQL queries and have retrieved all of the data. You would run this
    command to close it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have all of the data loaded into pandas DataFrames, we can walk
    through the different join types by slightly modifying the parameters in the `merge()`
    function. The `left` and `right` parameters for all of the examples will be `df_user_hits`
    and `df_user_geo` respectively. The join fields are consistent, which is `userid`
    for both DataFrames. In this example, the source tables use the same common field
    name for their unique identifier, which is helpful.
  prefs: []
  type: TYPE_NORMAL
- en: The last parameter we will pass into the `merge()` function is named `how` which
    determines which type of join will be performed. We will start with my favorite,
    which is the left join.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new DataFrame with the results of the pandas `merge()` function that
    creates a left join between the two DataFrames. In the next line, you can include
    the new DataFrame name, which will output the results similar to using the `head()`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like the following screenshot, where the `merge()` results
    have been loaded into a new DataFrame named `df_left_join` with a labeled header
    row with the index column to the left starting with a value of `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7889a052-b564-40a4-9249-6b68798c6107.png)'
  prefs: []
  type: TYPE_IMG
- en: The expected result, only `userid` from `df_user_hits`, will be displayed.
  prefs: []
  type: TYPE_NORMAL
- en: Notice the difference between SQL and pandas, where the blank `null()` values
    are replaced with **NaN**, which stands for **Not a Number**.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will create a right join, which has a slight variation to our prior
    syntax.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new DataFrame with the results of the pandas `merge()` function that
    creates a right join between the two DataFrames. In the next line, you can include
    the new DataFrame name, which will output the results similar to using the `head()`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like the following screenshot, where the merge results
    have been loaded into a new DataFrame named `df_right_join` with a labeled header
    row with the index column to the left starting with a value of `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8785b52-6adf-48be-a8aa-c33c844a14f2.png)'
  prefs: []
  type: TYPE_IMG
- en: The expected result, only `userid` from `df_user_geo`, will be displayed.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to our SQL example, we can perform an inner join using the merge function
    by using the default, which excludes the `how` parameter when passing it to the
    `merge()` function.
  prefs: []
  type: TYPE_NORMAL
- en: You can always explicitly include the `how` parameter if you want to be sure
    to define the type of join used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new DataFrame with the results of the pandas `merge()` function that
    creates an inner join between the two DataFrames. In the next line, you can include
    the new DataFrame name, which will output the results similar to using the `head()`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like the following screenshot, where the merge results
    have been loaded into a new DataFrame named `df_inner_join` with a labeled header
    row with the index column to the left starting with a value of `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ebb1e35e-2d23-4da9-8583-8c06f83f8c8c.png)'
  prefs: []
  type: TYPE_IMG
- en: The expected result, only `userid` that exists in both DataFrames, will be displayed.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let's create an outer join using the `merge()` function by adding the
    `how` parameter and including the value of `outer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new DataFrame with the results of the pandas `merge()` function that
    creates an outer join between the two DataFrames. In the next line, you can include
    the new DataFrame name, which will output the results similar to using the `head()`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like the following screenshot, where the merge results
    have been loaded into a new DataFrame named `df_outer_join` with a labeled header
    row with the index column to the left starting with a value of `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d1dbcf8-85f4-4356-9fa9-a6d0b92b353a.png)'
  prefs: []
  type: TYPE_IMG
- en: The expected result, all `userid` instances that exist in either DataFrame,
    will be displayed.
  prefs: []
  type: TYPE_NORMAL
- en: Excellent, we have successfully recreated all of the join types discussed throughout
    this chapter so far. Whether you feel more comfortable using SQL or `pandas`,
    the ability to join datasets together is a powerful skill and significantly increases
    your data literacy acumen.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining data aggregation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data aggregation is part of your daily life and you may or may not even realize
    it. When you pull up a review of a restaurant that uses one to five stars or if
    you purchase an item on Amazon because it has thousands of customer ratings, both
    examples are data aggregates. A data aggregate can be defined as a summary typically
    based on a significantly larger detail. In SQL, an aggregation is when a `groupby`
    command is applied against one or more tables, which includes a statistical calculation
    such as sum, average, min, or max against one or more fields.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the granularity of data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The aggregation of calculations would be known as the measure. When you are
    grouping by one or more fields to get their distinct values, they are classified
    as dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, this should all sound very familiar because we introduced the concept of
    dimensions and measures in both [Chapter 5](bea8a62c-4469-47e3-a668-10fbb91815ea.xhtml),
    *Gathering and Loading Data in Python*, and [Chapter 6](6e8b782b-2dad-4b16-9bba-73cd644e9529.xhtml),
    *Visualizing and Working with Time Series Data*, because it''s the foundation
    for data modeling and visualizations. To reinforce the concept, let''s see how
    a table or DataFrame gets summarized visually. As you can see in the following
    screenshot, an input table with any number of rows and columns can be summarized
    and reduced by many different types of aggregations, such as by a user or by date:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a3cf2de-9d89-4e80-9b1f-ce54846e022c.png)'
  prefs: []
  type: TYPE_IMG
- en: So, why are aggregations needed and important to analytics? First, as you can
    see in the preceding screenshot, the shape and size are significantly reduced,
    which helps large datasets to be manageable for ease of consumption by humans
    or machines.
  prefs: []
  type: TYPE_NORMAL
- en: For humans, when data is aggregated, it becomes easy to view and understand
    because the person does not have to visually sift through thousands of rows and
    columns. For machines, the reduction of very large data sources in both shape
    and size helps to reduce the file size, memory footprint, and input/output to
    process the data. When you see the sizes of structured data in **gigabytes** (**GB**),
    the main factor impacting that size (either file, DataFrame, or database table)
    is the density of the data. The density of data is defined by each data point
    value within the rows and columns of the table or source file. When you aggregate
    data, the volume of data will be significantly reduced because one or more of
    the fields containing the distinct values are removed.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you have a high volume transaction table with millions of distinct
    `userid` values by timestamp, the daily number of records could be in the tens
    of millions if a user performs an action or event every few seconds. If your analysis
    requirement is to measure the average count of the number of users per day, you
    could create a daily snapshot table that has one row for each day. So, a simple
    aggregation reduced tens of millions of rows down to one per day! So, what's the
    catch? Well, first we lost all of the granularity of what actions each user was
    performing on each day. Another factor is the time to process and manage the aggregate
    table. Any data engineer will tell you that downtime and bugs occur, so they must
    keep the source table and any aggregate tables in sync and reinstate if the source
    changes.
  prefs: []
  type: TYPE_NORMAL
- en: To solve for any loss of granularity, you can create other aggregates based
    on different fields/dimensions, but that may create other problems if you add
    more attributes later after the aggregate table is created. For example, if you
    snapshot an aggregate table with average daily user counts by city and state for
    a year and then want the analysis by zip code, you would have to reprocess to
    backfill all of the history or have two different average granularities that change
    before and after a specific date.
  prefs: []
  type: TYPE_NORMAL
- en: Data aggregation in action
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, now that we have a better understanding of what aggregates exist and why,
    let's walk through how to create them in SQL and pandas. For this example, we
    will be working with a similar version of the user hits data named `tbl_user_geo_hits`.
    This table has the combined records from the sources we have been working with
    before, except we can now focus on the aggregation and `groupby` syntax. The SQL
    language can be complex and is robust enough to handle both joins and aggregation
    at the same time, but I find breaking down the process will make it easier to
    learn. Also, it is common to have persisted tables or views (database objects
    that behave like a table but are derived from one or more joins) available because
    of high data volumes and/or reporting.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin by launching a new Jupyter notebook and naming it `ch_08_sql_and_pandas_group_by`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load a SQLite database connection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This library should already be available using Anaconda. Refer to [Chapter 2](e0fe6eb2-8f38-41f7-9dea-2b177578fd3c.xhtml),* Overview
    of Python and Installing Jupyter Notebook*, for help with setting up your environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to assign a connection to a variable named `conn` and point to
    the location of the database file, which is named `user_hits.db`. Since we already
    imported the `sqlite3` library in the prior `In[]`line, we can use this built-in
    function to communicate with the database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Be sure that you copied the `user_hits.db` file to the correct Jupyter folder
    directory to avoid errors with the connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pandas` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To run a SQL statement and assign the results to a DataFrame, we have to run
    this one line of code. The `pandas` library includes a `read_sql_query` function
    to make it easier to communicate with databases using SQL. It requires a connection
    parameter that we named `conn` in the previous steps. We assign the results to
    a new DataFrame as `df_user_geo_hits` to make it easier to identify:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To validate the results, you can run the `head()` function against this DataFrame
    using this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like the following screenshot, where the merge results
    have been loaded into a new DataFrame named `df_user_geo_hits` with a labeled
    header row with the index column to the left starting with a value of `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/633c07ac-0826-4d8e-b36c-2a21e77f14b9.png)'
  prefs: []
  type: TYPE_IMG
- en: So, we have user hits available and have previewed the data by loading it into
    a DataFrame. The advantage of a group by feature is that it allows us to ask specific
    questions about the data and return answers with some slight adjustments to the
    dimensions and aggregation used in the SQL syntax. How many user hits occurred
    by city and state across all time? To answer this question, let's identify what
    dimensions and measures are required. The dimensions are `city` and `state` and
    the measure is an aggregation created by counting the frequency of the occurrence
    of the number of records, which is represented by the function of `count (*)`.
    Since we have all of this information that we need in a single table, no join
    is required.
  prefs: []
  type: TYPE_NORMAL
- en: If we only included the one dimension of `city`, then we would combine any duplicate
    `city` names and misrepresent the data. For example, the city of `Dover` exists
    in multiple states, such as Delaware and New Jersey. This is where the granularity
    of the data could be lost by not including the right fields in the group by aggregation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run a SQL statement and assign the results to a DataFrame, we have to run
    this one line of code. The `pandas` library includes a `read_sql_query()` function
    to make it easier to communicate with databases using SQL. It requires a connection
    parameter that we named `conn` in the previous steps. We assign the results to
    a new DataFrame as `df_groupby_SQL` to make it easier to identify:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To validate the results, you can run the `head()` function against this DataFrame
    using this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like the following screenshot, where the merge results
    have been loaded into a new DataFrame named `df_groupby_SQL` with a labeled header
    row with the index column to the left starting with a value of `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3121f316-28d2-47eb-8c6c-bf0f451b4a43.png)'
  prefs: []
  type: TYPE_IMG
- en: The SQL language supports shortcuts, so we use `group by 1, 2` to represent
    the two-dimensional fields of `city` and `state`. It also allows alias of field
    names, so `count(*) as hits` is used to make it easier to represent.
  prefs: []
  type: TYPE_NORMAL
- en: To recreate the SQL results using `pandas`, we can use the DataFrame we loaded
    and use the `groupby()` function with a few parameters. In our example, we pass
    the name of the columns we want to group by, which is both `city` and `state`.
    The measure will be similar to before by including `.count()`, and we include
    the field you want to perform the aggregation, which can be any field because
    we are counting the frequency. We use `userid` since the analysis is focused on
    the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run a SQL statement and assign the results to a DataFrame, we have to run
    this one line of code. The `pandas` library includes a `read_sql_query` function
    to make it easier to communicate with databases using SQL. It requires a connection
    parameter that we named `conn` in the previous steps. We assign the results to
    a new DataFrame as `df_groupby_city_state` to make it easier to identify:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To validate the results, you can run the `head()` function against this DataFrame
    using this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like the following screenshot, where the merge results
    have been loaded into a new DataFrame named `df_groupby_city_state`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6752132-adb0-4155-9c1e-aa53d368e346.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s a best practice to close the database connection since we no longer need
    to run any SQL queries and have retrieved all of the data. You would run this
    command to close it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: So, we have demonstrated the power of summarizing data by using an aggregation
    group in both `pandas` and SQL. Aggregation can be performed against a single
    table or a join between multiple tables. The common elements between using SQL
    or `pandas` are defining the dimension and measures, which are abstracted from
    the fields available in either a DataFrame or SQL object (table, join, or view).
    So far, we have only scratched the surface using one measure type, which has been
    count. There are more statistical functions available for data analysis. So, next,
    we will explore the differences between mean, median, and mode.
  prefs: []
  type: TYPE_NORMAL
- en: Summary statistics and outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We touched on the necessity of fundamental statistics when working with data
    in [Chapter 5](bea8a62c-4469-47e3-a668-10fbb91815ea.xhtml), *Gathering and Loading
    Data in Python*.Let's walk through the differences between mean, median, and mode
    in statistics as it applies to data analysis. The mean or average is when you
    sum the values of numeric values in a series divided by the count of those same
    numbers. The mean or average is a measure in analytics and is typically used to
    gauge performance over a period of time and define a comparison for each period
    of time.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you see average daily temperatures all the time in the news—how
    is that calculated? Depending on your geographic location, the weather will have
    the temperature recorded in specific increments, such as hours. The **National
    Oceanic and Atmospheric Administration** (**NOAA**), for example, uses stations
    and a scientific approach to calculate the minimum and maximum temperature values
    for each day and location. Those individual records are then used to create an
    average monthly temperature and 30-year averages by month.
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to understand the lineage of your data, especially when working with
    averages, because an average of an average will limit how you can work with the
    data and is most likely a misuse of the metric. Ideally, have the lowest level
    of detail so you can re-calculate an average based on any period of time desired.
  prefs: []
  type: TYPE_NORMAL
- en: For the example that we have been working with throughout this chapter, calculating
    the average or mean would provide a standard that we can use for comparison. So,
    if we want to know the average number of hits per day, all we have to do is count
    all of the records and divide that by the distinct count of the values of date.
    From the `tbl_user_geo_hits` data, the average would be 2.3 hits per day because
    you had seven records with three distinct days. We can now use this as a litmus
    test to measure when each day has significant increases or decreases when compared
    to that mean value.
  prefs: []
  type: TYPE_NORMAL
- en: The median, a central value across a series of values, can be determined by
    finding the middle value, where fifty percent of the values are greater than or
    less than that specific data value. It is common to have those values ordered
    first to make it easier to identify that middle value. Identifying the mean is
    good for measuring the central tendency, which helps you to identify how the data
    values are distributed and is not affected by outlier data points. We will explore
    the shape of distributions in [Chapter 9](e3570c4a-c8ad-483f-9f3f-3e113156e9c2.xhtml), *Plotting,
    Visualization, and Storytelling*<q>.</q>
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we have the mode, which is the value in a series that occurs the most
    frequently. The mode is less commonly used in analytics and analysis but is useful
    when identifying outliers in your data. What is an outlier? Mathematically, it
    could be defined as a value that is multiple standard deviations from the mean.
    Practically, it''s when a value or aggregated value is out of the normal pattern
    as compared to the rest of the data. A good way to visually identify outliers
    is to plot the values against a normal distribution or what is commonly known
    as a *bell curve*, which is represented in the following diagram as a dotted black
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/27778f95-265a-42de-8fcb-27ad2d17be2f.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in the preceding diagram, the distribution line follows a shape
    based on the mean and standard deviation. A true normal distribution would have values
    where the mean, median, and mode are all equal. Left and right of that peak at
    the top of the line are based on one, two, or three standard deviations. A value
    plus or minus three standard deviations would represent only 99.7% of the population
    of values.
  prefs: []
  type: TYPE_NORMAL
- en: In practical terms, we are saying less than one percent of all of the data would
    be toward the min and max values. Having your data in a normal distribution may
    or may not be relevant but having this understanding helps to identify the shape
    of your data values. In the preceding example, there are red dots that fall significantly
    below and above the line, which are considered outliers. Having this visual representation
    of the outliers as compared to the normal distribution will help you as a data
    analyst to have a conversation with others about the data. This is the power of
    data literacy, where a visual representation can spark dialog and help to answer
    questions about what is expected or normal and what is an exception or an outlier.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations, we have covered the fundamentals of joining and merging data
    in both SQL and Python using pandas DataFrames. Throughout the process, we discussed
    practical examples of which joins to use along with why you should use them against
    user hits data. Enriching our data by blending multiple data tables allows deeper
    analysis and the ability to answer many more questions about the original single
    data source. After learning about joins and the `merge()` function, we uncovered
    the advantages and disadvantages of data aggregation. We walked through practical
    examples of using the `groupby` feature in both SQL and DataFrames. We walked
    through the differences between statistical functions and mean, median, and mode,
    along with tips for finding outliers in your data by comparing results to a normal
    distribution bell curve.
  prefs: []
  type: TYPE_NORMAL
- en: In our next chapter, we will be heading back to using plot libraries and visualizing
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on the topics of this chapter, you can refer to the following
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Guide to using the merge function: [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NOAA weather data: [https://www.ncdc.noaa.gov/cdo-web/datatools/records](https://www.ncdc.noaa.gov/cdo-web/datatools/records)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SQL join types: [https://www.w3schools.com/sql/sql_join.asp](https://www.w3schools.com/sql/sql_join.asp)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Literacy Project – understanding aggregations: [http://qcc.qlik.com/mod/url/view.php?id=5268](https://thedataliteracyproject.org/learn)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
