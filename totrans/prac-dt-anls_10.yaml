- en: Understanding Joins, Relationships, and Aggregates
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 理解连接、关系和聚合
- en: I'm really excited about this chapter because we are going to learn about the
    foundation of blending multiple datasets. This concept has been around for decades
    using SQL and other technologies including R, pandas, Excel, Cognos, and Qlikview.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我对这一章非常兴奋，因为我们将要学习关于合并多个数据集基础的知识。这个概念已经存在了几十年，使用 SQL 和其他技术，包括 R、pandas、Excel、Cognos
    和 Qlikview。
- en: The ability to merge data is a powerful skill that applies across different
    technologies and helps you to answer complex questions such as how product sales
    can be impacted by weather forecasts. The data sources are mutually exclusive,
    but today, access to weather data can be added to your data model with a few joins
    based on geographic location and time of day. We will be covering how this can
    be done along with the different types of joins. Once exposed to this concept,
    you will learn what questions can be answered depending on the granularity of
    data available. For our weather and sales data example, the details become important
    to understand the level of analysis that can be done. If you wanted to know whether
    rain impacts sales, the more common fields available, such as date, day, and time,
    and geographic location tags, such as latitude and longitude, must be available
    in both sources for you to be accurate in your conclusions after joining the data
    together.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 合并数据的能力是一项强大的技能，它适用于不同的技术，并帮助你回答复杂的问题，例如产品销售如何受到天气预报的影响。数据源是互斥的，但今天，通过基于地理位置和时间的一些连接，可以轻松地将天气数据添加到你的数据模型中。我们将介绍如何做到这一点，以及不同类型的连接。一旦接触到这个概念，你将了解根据数据的粒度可以回答哪些问题。在我们的天气和销售数据示例中，细节变得非常重要，以便了解可以进行的分析水平。如果你想知道雨是否会影响销售，那么在合并数据后，你必须确保日期、星期和时间的更常见字段以及经纬度这样的地理位置标签在两个数据源中都可用，以便在得出结论后保持准确性。
- en: In this chapter, we will learn how to construct high-quality datasets for further
    analysis. We will continue to advance your hands-on data literacy skills by learning
    how to work with join relationships and how to create aggregate data for analysis.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何构建高质量的数据集以进行进一步分析。我们将通过学习如何处理连接关系以及如何创建用于分析的数据聚合来进一步提高你的动手数据素养技能。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Foundations of join relationships
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接关系的基础
- en: Join types in action
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接类型的应用
- en: Explaining data aggregation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释数据聚合
- en: Summary statistics and outliers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概述统计和异常值
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Here's the GitHub repository of this book: [https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter08](https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter08).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本书的 GitHub 仓库链接：[https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter08](https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter08)。
- en: You can download and install the required software from the following link: [https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从以下链接下载和安装所需的软件：[https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual)。
- en: Foundations of join relationships
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接关系的基础
- en: For anyone familiar with SQL, the concept of joining data together is well understood.
    The ability to join one or more tables together for the purpose of analytics has
    remained relevant throughout my 20+ year career of working with data and I hope
    it continues to be relevant.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于熟悉 SQL 的人来说，将数据合并在一起的概念已经非常清楚。将一个或多个表合并在一起进行数据分析的能力在我的20多年数据工作生涯中一直保持相关性，并且我希望它将继续保持相关性。
- en: In prior chapters, we introduced the concept of data models and the need for
    primary and foreign key fields to define relationships. We will now elaborate
    on these concepts by explaining joins and the different types of joins that exist
    in SQL and DataFrames.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们介绍了数据模型的概念以及定义关系时需要主键和外键字段的需求。现在，我们将通过解释连接以及 SQL 和 DataFrames 中存在的不同类型的连接来详细阐述这些概念。
- en: Joining, in SQL, simply means merging two or more tables into a single dataset.
    The resulting size and shape of that single dataset will vary depending on the
    type of join that is used. Some key concepts you want to remember any time you
    are creating a join between datasets will be that the **common unique key** should
    always be used. Ideally, the key field functions as both the primary and foreign
    key but it can be derived using multiple fields to define a unique record for
    all rows of data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQL中，连接简单地意味着将两个或多个表合并成一个单一的数据集。这个单一数据集的大小和形状将取决于所使用的连接类型。当你创建数据集之间的连接时，你想要记住的一些关键概念是应该始终使用**公共唯一键**。理想情况下，键字段同时作为主键和外键，但可以使用多个字段来定义所有数据行的唯一记录。
- en: 'We will go through the following types of join relationships in this section:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍以下类型的连接关系：
- en: One-to-one
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一对一
- en: Many-to-many
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多对多
- en: Left join
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左连接
- en: Right join
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右连接
- en: Inner join
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内连接
- en: Outer join
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外连接
- en: With pandas DataFrames, the index in our examples has been the default or a
    single defined field because it has distinct values, but that is not always the
    case.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，pandas DataFrame的索引是默认的或单个定义的字段，因为它们具有唯一的值，但这并不总是如此。
- en: One-to-one relationships
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一对一关系
- en: A **one-to-one** relationship means the sources have common unique values by
    row and duplicates do not exist. A similar feature in Excel is the `vlookup` function
    with the exact match parameter enabled. When you use this Excel function, any
    matches to the source identifier return a distinct value from the target lookup.
    In SQL, one-to-one relationships ensure that the integrity between two tables
    is consistent. There are multiple reasons why these types of relationships are
    needed, but a good example is when a reference table that has the sales region
    is joined to a unique `Customer` table. In this example, a customer identifier
    (`ID`) field would exist in both tables and you would never have a sales region
    without a customer record and vice versa.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**一对一**关系意味着源数据按行具有共同的唯一值，且不存在重复。在Excel中，具有启用精确匹配参数的`vlookup`函数具有类似的功能。当你使用这个Excel函数时，任何与源标识符匹配的都会返回一个与目标查找不同的值。在SQL中，一对一关系确保两个表之间的完整性保持一致。需要这些类型关系的原因有很多，但一个好的例子是将具有销售区域的参考表与唯一的`客户`表连接起来。在这个例子中，客户标识符（`ID`）字段将存在于两个表中，并且你永远不会在没有客户记录的情况下有销售区域，反之亦然。'
- en: Many-to-one relationships
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多对一关系
- en: A **many-to-one relationship **means one of the sources can have duplicate rows
    but not both sources. You still need a unique key, index, or identifier between
    the sources. An example would be a lookup dimensional table joined to a fact table,
    which we covered in [Chapter 7](7282a629-c59a-4922-8422-e27ed44563db.xhtml), *Exploring
    Cleaning, Refining, and Blending Datasets*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**多对一关系**意味着其中一个源可以具有重复的行，但不是两个源都可以。你仍然需要在源之间有一个唯一键、索引或标识符。一个例子是将我们已在[第7章](7282a629-c59a-4922-8422-e27ed44563db.xhtml)，*探索数据集的清理、精炼和混合*中讨论过的查找维度表与事实表连接起来。'
- en: A transaction fact table will have duplicate records because a user hit for
    a website or a product sale is recorded by date/time for each occurrence. The
    result will generate millions of rows with duplicate records in the `userid` field.
    When the join uses a common field such as `userid` between the fact table and
    the second table, the second table must be unique in each row. This second table
    will have additional attributes about `userid`, such as `city`, `state`, and `zip
    code`, which will offer richer analysis options. Once you understand that this
    type of join relationship exists between two source tables, you can confidently
    join them together.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个事务事实表将会有重复的记录，因为用户访问网站或产品销售的事件会按日期/时间记录，每个事件都会生成。结果将在`userid`字段中产生数百万行具有重复记录的行。当连接使用事实表和第二表之间的公共字段，如`userid`时，第二表中的每一行都必须是唯一的。这个第二表将包含关于`userid`的附加属性，例如`城市`、`州`和`邮编`，这将提供更丰富的分析选项。一旦你理解了两个源表之间存在这种类型的连接关系，你就可以自信地将它们连接起来。
- en: Many-to-many relationship
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多对多关系
- en: 'A **many-to-many relationship **is when both sources have duplicate rows. Again,
    you should find a common unique key (one or more fields) or index between the
    sources. These types of joins are commonly referred to as *expensive* joins because
    the number of computing resources (memory and CPU) required will increase dramatically
    depending on the number of records and columns from both sources. A common example
    is a logical relationship between students and classes where many students can
    take many different classes. Conversely, many classes can have many different
    students. As a best practice, I would try to avoid direct many-to-many joins and
    use a bridge table to resolve them for analysis. For the students-to-classes example,
    you would need a roster table that marries together the unique list of one student
    per class, as in the following screenshot:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**多对多关系**是指两个来源都有重复的行。同样，您应该在来源之间找到一个共同的唯一键（一个或多个字段）或索引。这些类型的连接通常被称为*昂贵的*连接，因为所需的计算资源（内存和CPU）将根据来源中的记录和列的数量显著增加。一个常见的例子是学生和班级之间的逻辑关系，其中许多学生可以参加许多不同的课程。相反，许多班级可以有不同数量的学生。作为一个最佳实践，我会尽量避免直接的多对多连接，并使用桥表来解决它们以进行分析。对于学生到班级的例子，您需要一个名单表，将每个班级中每个学生的唯一列表配对，如下面的截图所示：'
- en: '![](img/6acde73e-e704-45c2-aac9-7e460f6a48ef.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/6acde73e-e704-45c2-aac9-7e460f6a48ef.png)'
- en: For any join, you should avoid a **Cartesian product**, which is where all possible
    combinations of rows and columns exist as a result of the join. There are some
    cases where this might be useful in your analysis but be cautious, especially
    with big data volumes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何连接，你应该避免**笛卡尔积**，这是由于连接而产生的所有可能的行和列的组合。在某些情况下，这可能在您的分析中很有用，但请谨慎，尤其是在处理大量大数据时。
- en: A Cartesian product is when all possible combinations of rows and columns are
    combined. For our example, if we joined `tbl_students` and `tbl_classes` without
    including `tbl_roster`, you would end up with students assigned to classes they
    never signed up for. There are occasions where I have deliberately constructed
    a Cartesian product because it was needed for certain types of analysis or a chart.
    For example, if you have a student ranking scale from 1 to 10 but none of the
    students achieved all of the possible values, you could create a Cartesian join
    to fill in the gaps of missing values.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 笛卡尔积是指所有可能的行和列的组合。在我们的例子中，如果我们不包含 `tbl_roster` 而将 `tbl_students` 和 `tbl_classes`
    连接起来，您最终会得到被分配到他们从未注册的课程中的学生。有时我会故意构建一个笛卡尔积，因为它是进行某些类型分析或图表所需的。例如，如果您有一个从1到10的学生排名量表，但没有学生达到所有可能的值，您可以通过创建笛卡尔连接来填补缺失值的空白。
- en: Similar to concerns about exceeding memory and CPU utilization when working
    with many-to-many join types, a Cartesian product can easily consume all available
    memory, which could lead to a crash of your workstation or workspace.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 与处理多对多连接类型时对超出内存和CPU利用率的担忧类似，笛卡尔积可以轻易消耗所有可用内存，这可能导致您的工作站或工作空间崩溃。
- en: Left join
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 左连接
- en: 'Now that we have covered the key concepts, I''m going to begin with a visual
    representation of one of the most common types of joins used in analysis, which
    is called a **left join**. Let''s start by looking at our source data for this
    example, which is represented in the following screenshot:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了关键概念，我将开始用一个分析中最常见的连接类型之一的视觉表示来开始，这被称为**左连接**。让我们首先看看这个例子的源数据，如下面的截图所示：
- en: '![](img/12434f84-6fb4-41d4-a70d-70ed4704987e.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/12434f84-6fb4-41d4-a70d-70ed4704987e.png)'
- en: As you can see, we have tables named `tbl_user_hits` and `tbl_user_geo`. The
    width and length of `tbl_user_hits` is two columns and three rows. In `tbl_user_geo`,
    which represents the user's geographical location, we have three columns and five
    rows. We have a common field named `userid` in both tables, which I highlighted
    and will use to join the data. These tables have a primary and foreign key many-to-one relationship
    because not all of the records from one table exist in the other.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们有两个表名为 `tbl_user_hits` 和 `tbl_user_geo`。`tbl_user_hits` 的宽度是两列，长度是三行。在
    `tbl_user_geo` 表中，它代表用户的地理位置，我们有三列和五行。这两个表都有一个名为 `userid` 的公共字段，我已经将其突出显示，并会使用它来连接数据。这些表之间存在主键和外键的多对一关系，因为一个表中的所有记录并不都存在于另一个表中。
- en: 'For this example, we want to keep all of the records in `tbl_user_hits` and
    enrich the data by blending together matching attributes such as city and state
    where `userid` exists only for the records in the user hits table. The result
    is in the following screenshot, where the original source, `tbl_user_hits`, has
    the same number of rows and but now includes the columns from the `tbl_user_geo` table:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们希望保留`tbl_user_hits`表中的所有记录，并通过合并匹配的属性（如城市和州）来丰富数据，其中`userid`仅存在于用户点击表中的记录。结果如下面的截图所示，原始来源`tbl_user_hits`具有相同的行数，但现在包括了来自`tbl_user_geo`表的列：
- en: '![](img/66e00ca9-6dc5-4a48-b67a-5b8c3e02551f.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/66e00ca9-6dc5-4a48-b67a-5b8c3e02551f.png)'
- en: Why did the number of rows remain the same but the number of columns increase?
    A successful left join preserves the number of rows from the source and extends
    the number of columns.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么行数保持不变但列数增加？成功的左连接保留源中的行数并扩展列数。
- en: The specific columns included in the join result can be defined but the default
    will include all of the columns.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 可以定义包含在连接结果中的特定列，但默认情况下将包括所有列。
- en: Why are left joins common in analytics? That's because we are interested in
    adding more dimensional fields to answer more questions about the data that does
    not exist in a single table alone. Also, in your analysis, you typically don't
    want to include anything that doesn't match our source user hits. With this new
    join result, we can now answer questions such as which city has the highest number
    of users. Using a few date calculations, we can also provide monthly trends by
    state.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么左连接在分析中很常见？那是因为我们感兴趣的是添加更多的维度字段来回答更多关于数据的问题，而这些数据仅存在于单个表中。此外，在你的分析中，你通常不想包含任何与我们的源用户点击不匹配的内容。有了这个新的连接结果，我们现在可以回答诸如哪个城市有最多用户等问题。通过一些日期计算，我们还可以按州提供月度趋势。
- en: Right join
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 右连接
- en: Next, we have a join type called a **right join**, which I find is less common.
    That is because there are not many use cases where the desire is to create gaps
    in the records of your merged data. A right join is where you want to preserve
    all of the columns and rows of the second table and fill in matching values from
    the first.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有一个称为**右连接**的连接类型，我认为它不太常见。那是因为没有多少用例需要创建合并数据记录中的空白。右连接是你想要保留第二个表的所有列和行，并用第一个表中的匹配值填充。
- en: 'To understand the concept, start by referring back to our prior source tables, `tbl_user_hits`, and `tbl_user_geo`.
    The result of a successful right join is shown in the following screenshot, where
    the join result is shown with five rows and four columns. The date field from
    `tbl_user_hits` has been combined with the `tbl_user_geo` source but missing values
    will appear as `null()` or `NaN`. Note that if `tbl_user_hits` had thousands or
    millions of rows, the join result would increase the original size of `tbl_user_geo`:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个概念，首先回顾一下我们之前的源表`tbl_user_hits`和`tbl_user_geo`。成功的右连接结果如下面的截图所示，其中连接结果显示了五行和四列。`tbl_user_hits`的日期字段已与`tbl_user_geo`源合并，但缺失的值将显示为`null()`或`NaN`。请注意，如果`tbl_user_hits`有数千或数百万行，连接结果将增加`tbl_user_geo`的原始大小：
- en: '![](img/b6498169-9f5a-4efb-b2eb-8132350e11d4.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b6498169-9f5a-4efb-b2eb-8132350e11d4.png)'
- en: One advantage of using the right join is that now you can identify which cities
    and states do not have any user hits. This could be useful information and could
    be used for marketing campaigns.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用右连接的一个优点是现在你可以识别哪些城市和州没有任何用户点击。这可能是有用的信息，可用于营销活动。
- en: Inner join
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内连接
- en: 'Next, we have an **inner join**. This is when only the exact values from both
    tables are returned along with all of the columns. To demonstrate the effect,
    I have made some adjustments to our source tables, which are shown in the following
    screenshot. The table names are the same but now some of the prior records are
    removed from `tbl_user_geo`. This could be due to a regulation request or the
    `userid` rows could have been determined to be invalid, so now we can use an inner
    join to remove them from `tbl_user_hits`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有一个**内连接**。这是当两个表都返回精确值以及所有列时。为了演示效果，我对我们的源表做了一些调整，如下面的截图所示。表名相同，但现在`tbl_user_geo`中删除了一些先前记录。这可能是因为法规要求，或者`userid`行可能被确定为无效，因此现在我们可以使用内连接将它们从`tbl_user_hits`中删除：
- en: '![](img/0b92bbd9-fb31-4be7-b59f-1315cea80175.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0b92bbd9-fb31-4be7-b59f-1315cea80175.png)'
- en: 'The join results are shown in the following screenshot, where only the matching
    values found in the `userid` key field are displayed along with all of the combined
    columns between both source tables:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 连接结果在以下屏幕截图中显示，其中仅显示在`userid`键字段中找到的匹配值，以及两个源表之间的所有合并列：
- en: '![](img/d5915b31-4d17-4cd3-b1a5-4c702ce43841.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d5915b31-4d17-4cd3-b1a5-4c702ce43841.png)'
- en: The pandas function for performing joins is called `merge`and an inner join
    is the default option.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 执行连接的pandas函数称为`merge`，内连接是默认选项。
- en: Outer join
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外连接
- en: 'Finally, we have an **outer join**, which provides a comprehensive list of
    all rows and columns from both sources. Unlike a **Cartesian**, where any possible
    combination of values is created, an outer join reflects the truth from both source
    tables. For our example, we will use the same source as the following screenshot,
    where records from `tbl_user_geo` were removed. Unlike an inner join, the outer
    join results provide you with the ability to see any missing records as null in
    SQL or `NaN` in Python/pandas:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有一个**外连接**，它提供了来自两个源的所有行和列的完整列表。与**笛卡尔积**不同，笛卡尔积会创建任何可能的值组合，外连接反映了两个源表的真实情况。在我们的示例中，我们将使用以下屏幕截图中的相同源，其中已删除`tbl_user_geo`记录。与内连接不同，外连接结果允许你在SQL中看到任何缺失的记录作为null，或在Python/pandas中作为`NaN`：
- en: '![](img/07560fb3-1b9b-463e-82df-5da700bb27ad.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/07560fb3-1b9b-463e-82df-5da700bb27ad.png)'
- en: While these concepts and common join types are not exhaustive, you now have
    a good foundational understanding of joining data sources together so we can move
    forward with walking through practical examples.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些概念和常见的连接类型并不全面，但你现在对将数据源连接在一起有了很好的基础理解，因此我们可以继续查看实际示例。
- en: Join types in action
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态连接类型
- en: Unfortunately, the SQLite database that we use does not support all of the join
    options (right and outer), so I will provide only two examples of a join (left
    and inner) using SQL in this walk-through. The good news is that the `pandas`
    library supports all join types using the `merge()` function, so we can recreate
    all of the examples already discussed. Feel free to walk through the following
    code; I have placed a copy of the Jupyter Notebook code on GitHub for reference.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们使用的SQLite数据库不支持所有连接选项（右连接和外连接），因此我将只提供两个使用SQL的连接示例（左连接和内连接）。好消息是，`pandas`库使用`merge()`函数支持所有连接类型，因此我们可以重新创建所有已讨论的示例。请随意查看以下代码；我已经在GitHub上放置了Jupyter
    Notebook代码的副本以供参考。
- en: Be sure to copy any dependencies files into your working folder before walking
    through all of the steps.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看所有步骤之前，请确保将任何依赖文件复制到你的工作文件夹中。
- en: 'We will begin by launching a new Jupyter notebook and naming it `ch_08_exercises`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先启动一个新的Jupyter笔记本，并将其命名为`ch_08_exercises`：
- en: 'Load a SQLite database connection:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载SQLite数据库连接：
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This library should already be available using Anaconda. Refer to [Chapter 2](e0fe6eb2-8f38-41f7-9dea-2b177578fd3c.xhtml), *Overview
    of Python and Installing Jupyter Notebook*, for help with setting up your environment.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库应该已经通过Anaconda可用。有关设置环境的帮助，请参阅[第2章](e0fe6eb2-8f38-41f7-9dea-2b177578fd3c.xhtml)，*Python和Jupyter
    Notebook的概述*。
- en: 'Next, we need to assign a connection to a variable named `conn` and point to
    the location of the database file, which is named `user_hits.db`. Since we already
    imported the `sqlite3` library in the prior `In[]`line, we can use this built-in
    function to communicate with the database:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要将一个名为`conn`的连接分配给变量，并指向名为`user_hits.db`的数据库文件位置。由于我们在先前的`In[]`行中已经导入了`sqlite3`库，我们可以使用这个内置函数与数据库通信：
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Be sure that you copied the `user_hits.db` file to the correct Jupyter folder
    directory to avoid errors with the connection.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你已经将`user_hits.db`文件复制到正确的Jupyter文件夹目录中，以避免连接错误。
- en: 'Import the `pandas` library:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`库：
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To run a SQL statement and assign the results to a DataFrame, we have to run
    this one line of code. The `pandas` library includes a `read_sql_query` function
    to make it easier to communicate with databases using SQL. It requires a connection
    parameter that we named `conn` in the previous steps. We assign the results to
    a new DataFrame as `df_left_join` to make it easier to identify:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要运行SQL语句并将结果分配给DataFrame，我们必须运行这一行代码。`pandas`库包含一个`read_sql_query`函数，这使得使用SQL与数据库通信变得更容易。它需要一个连接参数，我们在前面的步骤中将其命名为`conn`。我们将结果分配给一个新的DataFrame作为`df_left_join`，以便更容易识别：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: SQL supports the concept of an alias for table names so you can shorten the
    syntax. In this example, `tbl_user_hits` has an alias of `u` and `tbl_user_geo`
    is `g`. This helps when explicitly calling field names that require a prefix of
    the table name.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 支持表名的别名概念，因此您可以缩短语法。在这个例子中，`tbl_user_hits` 的别名为 `u`，`tbl_user_geo` 为 `g`。这有助于在显式调用需要表名前缀的字段名时。
- en: 'Now that we have the results in a DataFrame, we can use all of the available
    `pandas` library commands against this data without going back to the database.
    To view the results, we can just run the `head()` command against this DataFrame
    using this code:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经将结果放入 DataFrame 中，我们可以使用所有可用的 `pandas` 库命令对此数据进行操作，而无需返回数据库。要查看结果，我们只需运行
    `head()` 命令对此 DataFrame 使用此代码：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output will look like the following screenshot, where the SQL results have
    been loaded into a DataFrame with a labeled header row with the index column to
    the left starting with a value of `0`:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中 SQL 结果已加载到具有标记标题行的 DataFrame 中，索引列位于左侧，起始值为 `0`：
- en: '![](img/df65ebe0-1ab5-41f8-b7d9-d7077c60267f.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/df65ebe0-1ab5-41f8-b7d9-d7077c60267f.png)'
- en: 'The next join will be an inner and we will assign the results to a new DataFrame
    as `df_inner_join` to make it easier to identify:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个连接将是一个内部连接，我们将结果分配给一个新的 DataFrame，命名为 `df_inner_join`，以便更容易识别：
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that we have the results in a DataFrame, we can use all of the available
    `pandas` library commands against this data without going back to the database.
    To view the results, we can just run the `head()` command against this DataFrame
    using this code:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经将结果放入 DataFrame 中，我们可以使用所有可用的 `pandas` 库命令对此数据进行操作，而无需返回数据库。要查看结果，我们只需运行
    `head()` 命令对此 DataFrame 使用此代码：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The output will look like the following screenshot, where the SQL results have
    been loaded into a DataFrame with a labeled header row with the index column to
    the left starting with a value of `0:`
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中 SQL 结果已加载到具有标记标题行的 DataFrame 中，索引列位于左侧，起始值为 `0`：
- en: '![](img/ca7e2492-9be7-4989-aee6-d8dcf457d106.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/ca7e2492-9be7-4989-aee6-d8dcf457d106.png)'
- en: We will continue the walk-through of the remaining exercise using the `merge()`
    function in pandas. I have a reference with more details about the function in
    the *Further reading* section, but using it is very straightforward. Once you
    have the two input tables stored as DataFrames, you can input them into the `merge()`
    function as parameters along with the join type you want to use, which is controlled
    using the `how` parameter. The default when you don't specify a parameter is an
    inner join, and it supports all of the SQL joins we have discussed, including
    left, right, and outer. The result of using the `merge()` function is to return
    a DataFrame with the source objects merged. Let's continue this walk-through exercise
    by loading our source SQL tables as DataFrames.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用 `pandas` 中的 `merge()` 函数进行剩余的练习。我在 *进一步阅读* 部分有一个关于该函数的详细信息参考，但使用它非常简单。一旦您将两个输入表存储为
    DataFrame，您可以将它们作为参数输入到 `merge()` 函数中，同时指定您想要使用的连接类型，这由 `how` 参数控制。当您不指定参数时，默认为内部连接，它支持我们讨论的所有
    SQL 连接，包括左连接、右连接和外连接。使用 `merge()` 函数的结果是返回一个包含源对象合并的 DataFrame。让我们通过将我们的源 SQL
    表作为 DataFrame 加载来继续这个练习。
- en: 'Create a new DataFrame called `df_user_hits`, which is a duplicate of the `tbl_user_hits`
    table, so we can use it later in the examples:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的 DataFrame，命名为 `df_user_hits`，它是 `tbl_user_hits` 表的副本，这样我们就可以在后面的示例中使用它：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To validate the results, you can run the `head()` function against this DataFrame
    using this code:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了验证结果，您可以使用此代码运行 `head()` 函数对此 DataFrame：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output will look like the following screenshot, where the SQL results have
    been loaded into a DataFrame with a labeled header row with the index column to
    the left starting with a value of `0`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中 SQL 结果已加载到具有标记标题行的 DataFrame 中，索引列位于左侧，起始值为 `0`：
- en: '![](img/ac631f3a-a94f-456f-ba70-eadd2e708bb5.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/ac631f3a-a94f-456f-ba70-eadd2e708bb5.png)'
- en: 'Create a new DataFrame called `df_user_geo`, which is a duplicate of the `tbl_user_geo`
    table, so we can use it later in the examples:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的 DataFrame，命名为 `df_user_geo`，它是 `tbl_user_geo` 表的副本，这样我们就可以在后面的示例中使用它：
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output would look like the following screenshot, where the SQL results
    have been loaded into a DataFrame with a labeled header row with the index column
    to the left starting with a value of `0`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中 SQL 结果已加载到具有标记标题行的 DataFrame 中，索引列位于左侧，起始值为 `0`：
- en: '![](img/454ba42c-acad-44f2-ba97-6c157bbff459.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/454ba42c-acad-44f2-ba97-6c157bbff459.png)'
- en: 'It''s a best practice to close the database connection since we no longer need
    to run any SQL queries and have retrieved all of the data. You would run this
    command to close it:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们不再需要运行任何SQL查询并已检索所有数据，关闭数据库连接是一种最佳实践。您将运行此命令来关闭它：
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that we have all of the data loaded into pandas DataFrames, we can walk
    through the different join types by slightly modifying the parameters in the `merge()`
    function. The `left` and `right` parameters for all of the examples will be `df_user_hits`
    and `df_user_geo` respectively. The join fields are consistent, which is `userid`
    for both DataFrames. In this example, the source tables use the same common field
    name for their unique identifier, which is helpful.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将所有数据加载到pandas DataFrame中，我们可以通过稍微修改`merge()`函数中的参数来遍历不同的连接类型。所有示例的`left`和`right`参数分别为`df_user_hits`和`df_user_geo`。连接字段是一致的，即两个DataFrame的`userid`。在这个例子中，源表使用相同的公共字段名作为它们的唯一标识符，这很有帮助。
- en: The last parameter we will pass into the `merge()` function is named `how` which
    determines which type of join will be performed. We will start with my favorite,
    which is the left join.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将传递给`merge()`函数的最后一个参数名为`how`，它决定了将执行哪种类型的连接。我们将从我最喜欢的开始，即左连接。
- en: 'Create a new DataFrame with the results of the pandas `merge()` function that
    creates a left join between the two DataFrames. In the next line, you can include
    the new DataFrame name, which will output the results similar to using the `head()`
    function:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用pandas的`merge()`函数创建两个DataFrame之间的左连接，创建一个新的DataFrame。在下一行中，您可以包含新的DataFrame名称，这将输出类似于使用`head()`函数的结果：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output will look like the following screenshot, where the `merge()` results
    have been loaded into a new DataFrame named `df_left_join` with a labeled header
    row with the index column to the left starting with a value of `0`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中`merge()`的结果已加载到名为`df_left_join`的新DataFrame中，带有标签的表头行，索引列位于左侧，起始值为`0`：
- en: '![](img/7889a052-b564-40a4-9249-6b68798c6107.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7889a052-b564-40a4-9249-6b68798c6107.png)'
- en: The expected result, only `userid` from `df_user_hits`, will be displayed.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 预期结果，仅显示来自`df_user_hits`的`userid`。
- en: Notice the difference between SQL and pandas, where the blank `null()` values
    are replaced with **NaN**, which stands for **Not a Number**.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 注意SQL和pandas之间的区别，其中空白`null()`值被**NaN**（代表**Not a Number**）所替换。
- en: Next, we will create a right join, which has a slight variation to our prior
    syntax.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个右连接，这与我们之前的语法略有不同。
- en: 'Create a new DataFrame with the results of the pandas `merge()` function that
    creates a right join between the two DataFrames. In the next line, you can include
    the new DataFrame name, which will output the results similar to using the `head()`
    function:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用pandas的`merge()`函数创建两个DataFrame之间的右连接，创建一个新的DataFrame。在下一行中，您可以包含新的DataFrame名称，这将输出类似于使用`head()`函数的结果：
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output will look like the following screenshot, where the merge results
    have been loaded into a new DataFrame named `df_right_join` with a labeled header
    row with the index column to the left starting with a value of `0`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中合并结果已加载到名为`df_right_join`的新DataFrame中，带有标签的表头行，索引列位于左侧，起始值为`0`：
- en: '![](img/b8785b52-6adf-48be-a8aa-c33c844a14f2.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b8785b52-6adf-48be-a8aa-c33c844a14f2.png)'
- en: The expected result, only `userid` from `df_user_geo`, will be displayed.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 预期结果，仅显示来自`df_user_geo`的`userid`。
- en: Similar to our SQL example, we can perform an inner join using the merge function
    by using the default, which excludes the `how` parameter when passing it to the
    `merge()` function.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们的SQL示例类似，我们可以通过使用默认值来执行内部连接，即在将`how`参数传递给`merge()`函数时省略它。
- en: You can always explicitly include the `how` parameter if you want to be sure
    to define the type of join used.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想确保定义所使用的连接类型，您始终可以显式包含`how`参数。
- en: 'Create a new DataFrame with the results of the pandas `merge()` function that
    creates an inner join between the two DataFrames. In the next line, you can include
    the new DataFrame name, which will output the results similar to using the `head()`
    function:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用pandas的`merge()`函数创建两个DataFrame之间的内部连接，创建一个新的DataFrame。在下一行中，您可以包含新的DataFrame名称，这将输出类似于使用`head()`函数的结果：
- en: '[PRE13]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output will look like the following screenshot, where the merge results
    have been loaded into a new DataFrame named `df_inner_join` with a labeled header
    row with the index column to the left starting with a value of `0`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中合并结果已加载到名为 `df_inner_join` 的新 DataFrame 中，带有标签的标题行，索引列从左侧开始，值为
    `0`：
- en: '![](img/ebb1e35e-2d23-4da9-8583-8c06f83f8c8c.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ebb1e35e-2d23-4da9-8583-8c06f83f8c8c.png)'
- en: The expected result, only `userid` that exists in both DataFrames, will be displayed.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 预期结果，仅显示存在于两个 DataFrame 中的 `userid`：
- en: Finally, let's create an outer join using the `merge()` function by adding the
    `how` parameter and including the value of `outer`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们通过添加 `how` 参数并包含 `outer` 值来使用 `merge()` 函数创建一个外连接：
- en: 'Create a new DataFrame with the results of the pandas `merge()` function that
    creates an outer join between the two DataFrames. In the next line, you can include
    the new DataFrame name, which will output the results similar to using the `head()`
    function:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的 DataFrame，其中包含 pandas `merge()` 函数创建的两个 DataFrame 之间的外连接的结果。在下一行，您可以包含新的
    DataFrame 名称，这将输出类似于使用 `head()` 函数的结果：
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output will look like the following screenshot, where the merge results
    have been loaded into a new DataFrame named `df_outer_join` with a labeled header
    row with the index column to the left starting with a value of `0`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中合并结果已加载到名为 `df_outer_join` 的新 DataFrame 中，带有标签的标题行，索引列从左侧开始，值为
    `0`：
- en: '![](img/0d1dbcf8-85f4-4356-9fa9-a6d0b92b353a.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0d1dbcf8-85f4-4356-9fa9-a6d0b92b353a.png)'
- en: The expected result, all `userid` instances that exist in either DataFrame,
    will be displayed.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 预期结果，将显示存在于任一 DataFrame 中的所有 `userid` 实例：
- en: Excellent, we have successfully recreated all of the join types discussed throughout
    this chapter so far. Whether you feel more comfortable using SQL or `pandas`,
    the ability to join datasets together is a powerful skill and significantly increases
    your data literacy acumen.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了，我们已经成功重新创建了本章迄今为止讨论的所有连接类型。无论您更习惯使用 SQL 还是 `pandas`，将数据集连接起来的能力是一项强大的技能，并且显著提高了您的数据素养。
- en: Explaining data aggregation
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释数据聚合
- en: Data aggregation is part of your daily life and you may or may not even realize
    it. When you pull up a review of a restaurant that uses one to five stars or if
    you purchase an item on Amazon because it has thousands of customer ratings, both
    examples are data aggregates. A data aggregate can be defined as a summary typically
    based on a significantly larger detail. In SQL, an aggregation is when a `groupby`
    command is applied against one or more tables, which includes a statistical calculation
    such as sum, average, min, or max against one or more fields.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 数据聚合是您日常生活中的一个部分，您可能甚至没有意识到它。当您查看使用一到五颗星的餐厅评论时，或者在亚马逊上购买一个有数千条客户评价的商品时，这两个例子都是数据聚合。数据聚合可以定义为基于显著更大的细节的摘要。在
    SQL 中，聚合是在一个或多个表上应用 `groupby` 命令时发生的，这包括对一个或多个字段进行的统计计算，如总和、平均值、最小值或最大值。
- en: Understanding the granularity of data
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解数据的粒度
- en: The aggregation of calculations would be known as the measure. When you are
    grouping by one or more fields to get their distinct values, they are classified
    as dimensions.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 计算的聚合被称为度量。当您按一个或多个字段分组以获取它们的唯一值时，它们被分类为维度。
- en: 'So, this should all sound very familiar because we introduced the concept of
    dimensions and measures in both [Chapter 5](bea8a62c-4469-47e3-a668-10fbb91815ea.xhtml),
    *Gathering and Loading Data in Python*, and [Chapter 6](6e8b782b-2dad-4b16-9bba-73cd644e9529.xhtml),
    *Visualizing and Working with Time Series Data*, because it''s the foundation
    for data modeling and visualizations. To reinforce the concept, let''s see how
    a table or DataFrame gets summarized visually. As you can see in the following
    screenshot, an input table with any number of rows and columns can be summarized
    and reduced by many different types of aggregations, such as by a user or by date:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这应该听起来都很熟悉，因为我们已经在 [第 5 章](bea8a62c-4469-47e3-a668-10fbb91815ea.xhtml)，*在
    Python 中收集和加载数据*，以及 [第 6 章](6e8b782b-2dad-4b16-9bba-73cd644e9529.xhtml)，*可视化和处理时间序列数据*
    中介绍了维度和度量的概念，因为它是数据建模和可视化的基础。为了加强这一概念，让我们看看表格或 DataFrame 如何通过视觉方式汇总。如图所示，任何行数和列数的输入表格都可以通过多种不同的聚合类型进行汇总和简化，例如按用户或按日期：
- en: '![](img/5a3cf2de-9d89-4e80-9b1f-ce54846e022c.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5a3cf2de-9d89-4e80-9b1f-ce54846e022c.png)'
- en: So, why are aggregations needed and important to analytics? First, as you can
    see in the preceding screenshot, the shape and size are significantly reduced,
    which helps large datasets to be manageable for ease of consumption by humans
    or machines.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '那么，为什么聚合对于分析来说是必需的和重要的呢？首先，正如你在前面的屏幕截图中所看到的，形状和大小显著减少，这有助于大型数据集便于人类或机器消费。 '
- en: For humans, when data is aggregated, it becomes easy to view and understand
    because the person does not have to visually sift through thousands of rows and
    columns. For machines, the reduction of very large data sources in both shape
    and size helps to reduce the file size, memory footprint, and input/output to
    process the data. When you see the sizes of structured data in **gigabytes** (**GB**),
    the main factor impacting that size (either file, DataFrame, or database table)
    is the density of the data. The density of data is defined by each data point
    value within the rows and columns of the table or source file. When you aggregate
    data, the volume of data will be significantly reduced because one or more of
    the fields containing the distinct values are removed.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人类来说，当数据聚合时，它变得易于查看和理解，因为人们不必视觉上筛选数千行和列。对于机器来说，在形状和大小上减少非常大的数据源有助于减少文件大小、内存占用和数据处理时的输入/输出。当你看到结构化数据的大小以**千兆字节**（**GB**）为单位时，影响该大小的主要因素（无论是文件、DataFrame还是数据库表）是数据的密度。数据的密度由表或源文件中的行和列中的每个数据点值定义。当你聚合数据时，数据量将显著减少，因为一个或多个包含唯一值的字段被移除。
- en: For example, if you have a high volume transaction table with millions of distinct
    `userid` values by timestamp, the daily number of records could be in the tens
    of millions if a user performs an action or event every few seconds. If your analysis
    requirement is to measure the average count of the number of users per day, you
    could create a daily snapshot table that has one row for each day. So, a simple
    aggregation reduced tens of millions of rows down to one per day! So, what's the
    catch? Well, first we lost all of the granularity of what actions each user was
    performing on each day. Another factor is the time to process and manage the aggregate
    table. Any data engineer will tell you that downtime and bugs occur, so they must
    keep the source table and any aggregate tables in sync and reinstate if the source
    changes.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你有一个高交易量的事务表，按时间戳有数百万个不同的`userid`值，如果用户每几秒钟执行一个动作或事件，每天的记录数可能达到数千万。如果你的分析需求是衡量每天用户的平均数量，你可以创建一个每天有一行的每日快照表。因此，简单的聚合将数千万行减少到每天一行！那么，问题在哪里呢？首先，我们失去了每个用户每天执行的动作的所有粒度。另一个因素是处理和管理聚合表的时间。任何数据工程师都会告诉你，停机时间和错误是会发生的，所以他们必须保持源表和任何聚合表同步，并在源表发生变化时恢复。
- en: To solve for any loss of granularity, you can create other aggregates based
    on different fields/dimensions, but that may create other problems if you add
    more attributes later after the aggregate table is created. For example, if you
    snapshot an aggregate table with average daily user counts by city and state for
    a year and then want the analysis by zip code, you would have to reprocess to
    backfill all of the history or have two different average granularities that change
    before and after a specific date.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决任何粒度的损失，你可以根据不同的字段/维度创建其他聚合，但如果你在聚合表创建后添加更多属性，可能会产生其他问题。例如，如果你对按城市和州平均每日用户计数进行一年的聚合表快照，然后想按邮政编码进行分析，你可能需要重新处理以回填所有历史数据，或者拥有两种不同的平均粒度，这些粒度在特定日期前后发生变化。
- en: Data aggregation in action
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据聚合的实际应用
- en: So, now that we have a better understanding of what aggregates exist and why,
    let's walk through how to create them in SQL and pandas. For this example, we
    will be working with a similar version of the user hits data named `tbl_user_geo_hits`.
    This table has the combined records from the sources we have been working with
    before, except we can now focus on the aggregation and `groupby` syntax. The SQL
    language can be complex and is robust enough to handle both joins and aggregation
    at the same time, but I find breaking down the process will make it easier to
    learn. Also, it is common to have persisted tables or views (database objects
    that behave like a table but are derived from one or more joins) available because
    of high data volumes and/or reporting.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们更好地理解了聚合的存在及其原因，让我们来了解一下如何在SQL和pandas中创建它们。在这个例子中，我们将使用名为`tbl_user_geo_hits`的用户点击数据的一个类似版本。这个表包含了我们之前一直在使用的来源的合并记录，但现在我们可以专注于聚合和`groupby`语法。SQL语言可能很复杂，但足够强大，可以同时处理连接和聚合，但我发现分解这个过程会使学习更容易。此外，由于数据量很大和/或需要报告，通常会有持久化的表或视图（类似于表但由一个或多个连接派生的数据库对象）可用。
- en: 'We will begin by launching a new Jupyter notebook and naming it `ch_08_sql_and_pandas_group_by`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先启动一个新的Jupyter笔记本，并将其命名为`ch_08_sql_and_pandas_group_by`：
- en: 'Load a SQLite database connection:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载SQLite数据库连接：
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This library should already be available using Anaconda. Refer to [Chapter 2](e0fe6eb2-8f38-41f7-9dea-2b177578fd3c.xhtml),* Overview
    of Python and Installing Jupyter Notebook*, for help with setting up your environment.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库应该已经通过Anaconda可用。有关设置环境的帮助，请参阅[第2章](e0fe6eb2-8f38-41f7-9dea-2b177578fd3c.xhtml)，*Python和Jupyter
    Notebook安装概述*。
- en: 'Next, we need to assign a connection to a variable named `conn` and point to
    the location of the database file, which is named `user_hits.db`. Since we already
    imported the `sqlite3` library in the prior `In[]`line, we can use this built-in
    function to communicate with the database:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要将一个连接分配给名为`conn`的变量，并指向数据库文件的位置，该文件名为`user_hits.db`。由于我们在之前的`In[]`行中已经导入了`sqlite3`库，我们可以使用这个内置函数与数据库进行通信：
- en: '[PRE16]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Be sure that you copied the `user_hits.db` file to the correct Jupyter folder
    directory to avoid errors with the connection.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您已将`user_hits.db`文件复制到正确的Jupyter文件夹目录中，以避免连接错误。
- en: 'Import the `pandas` library:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`库：
- en: '[PRE17]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To run a SQL statement and assign the results to a DataFrame, we have to run
    this one line of code. The `pandas` library includes a `read_sql_query` function
    to make it easier to communicate with databases using SQL. It requires a connection
    parameter that we named `conn` in the previous steps. We assign the results to
    a new DataFrame as `df_user_geo_hits` to make it easier to identify:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要运行SQL语句并将结果分配给DataFrame，我们必须运行这一行代码。`pandas`库包含一个`read_sql_query`函数，这使得使用SQL与数据库通信变得更加容易。它需要一个连接参数，我们在之前的步骤中将其命名为`conn`。我们将结果分配给一个新的DataFrame，命名为`df_user_geo_hits`，以便更容易识别：
- en: '[PRE18]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To validate the results, you can run the `head()` function against this DataFrame
    using this code:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了验证结果，你可以使用以下代码运行`head()`函数对此DataFrame进行操作：
- en: '[PRE19]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output will look like the following screenshot, where the merge results
    have been loaded into a new DataFrame named `df_user_geo_hits` with a labeled
    header row with the index column to the left starting with a value of `0`:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中合并结果已加载到名为`df_user_geo_hits`的新DataFrame中，带有标签的标题行，左侧的索引列从`0`开始：
- en: '![](img/633c07ac-0826-4d8e-b36c-2a21e77f14b9.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/633c07ac-0826-4d8e-b36c-2a21e77f14b9.png)'
- en: So, we have user hits available and have previewed the data by loading it into
    a DataFrame. The advantage of a group by feature is that it allows us to ask specific
    questions about the data and return answers with some slight adjustments to the
    dimensions and aggregation used in the SQL syntax. How many user hits occurred
    by city and state across all time? To answer this question, let's identify what
    dimensions and measures are required. The dimensions are `city` and `state` and
    the measure is an aggregation created by counting the frequency of the occurrence
    of the number of records, which is represented by the function of `count (*)`.
    Since we have all of this information that we need in a single table, no join
    is required.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有了用户点击量数据，并且通过将其加载到 DataFrame 中来预览了这些数据。使用按组功能的优势在于，它允许我们针对数据提出具体问题，并通过在
    SQL 语法中使用的维度和聚合上进行一些轻微调整来返回答案。在所有时间范围内，按城市和州发生了多少用户点击量？为了回答这个问题，让我们确定所需的维度和度量。维度是`城市`和`州`，度量是通过计算记录出现的频率的聚合，这由`count
    (*)`函数表示。由于我们需要的所有这些信息都在一个表中，因此不需要进行连接。
- en: If we only included the one dimension of `city`, then we would combine any duplicate
    `city` names and misrepresent the data. For example, the city of `Dover` exists
    in multiple states, such as Delaware and New Jersey. This is where the granularity
    of the data could be lost by not including the right fields in the group by aggregation.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只包括 `城市` 这一个维度，那么我们就会合并任何重复的 `城市` 名称，并错误地表示数据。例如，`多佛` 这个城市存在于多个州，如特拉华和新泽西州。这就是为什么如果不包括正确的字段在按组聚合中，数据可能会丢失粒度。
- en: 'To run a SQL statement and assign the results to a DataFrame, we have to run
    this one line of code. The `pandas` library includes a `read_sql_query()` function
    to make it easier to communicate with databases using SQL. It requires a connection
    parameter that we named `conn` in the previous steps. We assign the results to
    a new DataFrame as `df_groupby_SQL` to make it easier to identify:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要运行 SQL 语句并将结果分配给 DataFrame，我们必须运行这一行代码。`pandas` 库包含一个 `read_sql_query()` 函数，这使得使用
    SQL 与数据库通信变得更容易。它需要一个连接参数，我们在前面的步骤中将其命名为 `conn`。我们将结果分配给一个新的 DataFrame，命名为 `df_groupby_SQL`，以便更容易识别：
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To validate the results, you can run the `head()` function against this DataFrame
    using this code:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了验证结果，你可以使用以下代码对此 DataFrame 运行 `head()` 函数：
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output will look like the following screenshot, where the merge results
    have been loaded into a new DataFrame named `df_groupby_SQL` with a labeled header
    row with the index column to the left starting with a value of `0`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中合并结果已加载到名为 `df_groupby_SQL` 的新 DataFrame 中，带有标签的标题行从左侧的 `0` 值开始：
- en: '![](img/3121f316-28d2-47eb-8c6c-bf0f451b4a43.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3121f316-28d2-47eb-8c6c-bf0f451b4a43.png)'
- en: The SQL language supports shortcuts, so we use `group by 1, 2` to represent
    the two-dimensional fields of `city` and `state`. It also allows alias of field
    names, so `count(*) as hits` is used to make it easier to represent.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 语言支持快捷方式，因此我们使用 `group by 1, 2` 来表示 `城市` 和 `州` 的二维字段。它还允许字段名称的别名，因此使用 `count(*)
    as hits` 来使其更容易表示。
- en: To recreate the SQL results using `pandas`, we can use the DataFrame we loaded
    and use the `groupby()` function with a few parameters. In our example, we pass
    the name of the columns we want to group by, which is both `city` and `state`.
    The measure will be similar to before by including `.count()`, and we include
    the field you want to perform the aggregation, which can be any field because
    we are counting the frequency. We use `userid` since the analysis is focused on
    the user.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `pandas` 重新创建 SQL 结果，我们可以使用我们加载的 DataFrame，并使用 `groupby()` 函数和一些参数。在我们的例子中，我们传递我们想要按其分组列的名称，即
    `城市` 和 `州`。度量将与之前相似，通过包括 `.count()`，我们包括想要进行聚合的字段，由于我们是在计数频率，因此可以是任何字段。我们使用 `userid`，因为分析集中在用户上。
- en: 'To run a SQL statement and assign the results to a DataFrame, we have to run
    this one line of code. The `pandas` library includes a `read_sql_query` function
    to make it easier to communicate with databases using SQL. It requires a connection
    parameter that we named `conn` in the previous steps. We assign the results to
    a new DataFrame as `df_groupby_city_state` to make it easier to identify:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要运行 SQL 语句并将结果分配给 DataFrame，我们必须运行这一行代码。`pandas` 库包含一个 `read_sql_query` 函数，这使得使用
    SQL 与数据库通信变得更容易。它需要一个连接参数，我们在前面的步骤中将其命名为 `conn`。我们将结果分配给一个新的 DataFrame，命名为 `df_groupby_city_state`，以便更容易识别：
- en: '[PRE22]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To validate the results, you can run the `head()` function against this DataFrame
    using this code:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了验证结果，你可以使用以下代码对这个DataFrame运行`head()`函数：
- en: '[PRE23]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output will look like the following screenshot, where the merge results
    have been loaded into a new DataFrame named `df_groupby_city_state`:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图，其中合并结果已加载到名为`df_groupby_city_state`的新DataFrame中：
- en: '![](img/f6752132-adb0-4155-9c1e-aa53d368e346.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f6752132-adb0-4155-9c1e-aa53d368e346.png)'
- en: 'It''s a best practice to close the database connection since we no longer need
    to run any SQL queries and have retrieved all of the data. You would run this
    command to close it:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们不再需要运行任何SQL查询并已检索所有数据，关闭数据库连接是一个好习惯。你可以运行以下命令来关闭它：
- en: '[PRE24]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: So, we have demonstrated the power of summarizing data by using an aggregation
    group in both `pandas` and SQL. Aggregation can be performed against a single
    table or a join between multiple tables. The common elements between using SQL
    or `pandas` are defining the dimension and measures, which are abstracted from
    the fields available in either a DataFrame or SQL object (table, join, or view).
    So far, we have only scratched the surface using one measure type, which has been
    count. There are more statistical functions available for data analysis. So, next,
    we will explore the differences between mean, median, and mode.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经展示了通过在`pandas`和SQL中使用聚合组来总结数据的力量。聚合可以针对单个表或多个表之间的连接执行。使用SQL或`pandas`的共同之处在于定义维度和度量，这些是从DataFrame或SQL对象（表、连接或视图）中可用的字段抽象出来的。到目前为止，我们只使用了一种度量类型，即计数，来触及表面。还有更多用于数据分析的统计函数。因此，接下来，我们将探讨均值、中位数和众数之间的区别。
- en: Summary statistics and outliers
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率统计和异常值
- en: We touched on the necessity of fundamental statistics when working with data
    in [Chapter 5](bea8a62c-4469-47e3-a668-10fbb91815ea.xhtml), *Gathering and Loading
    Data in Python*.Let's walk through the differences between mean, median, and mode
    in statistics as it applies to data analysis. The mean or average is when you
    sum the values of numeric values in a series divided by the count of those same
    numbers. The mean or average is a measure in analytics and is typically used to
    gauge performance over a period of time and define a comparison for each period
    of time.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](bea8a62c-4469-47e3-a668-10fbb91815ea.xhtml)“在Python中收集和加载数据”中，我们讨论了处理数据时基本统计学的必要性。现在，让我们探讨统计学中均值、中位数和众数之间的区别，以及它们在数据分析中的应用。均值或平均值是指将一系列数值的总和除以这些数值的数量。均值或平均值是分析中的一个度量，通常用于衡量一段时间内的表现，并为每个时间段定义一个比较基准。
- en: For example, you see average daily temperatures all the time in the news—how
    is that calculated? Depending on your geographic location, the weather will have
    the temperature recorded in specific increments, such as hours. The **National
    Oceanic and Atmospheric Administration** (**NOAA**), for example, uses stations
    and a scientific approach to calculate the minimum and maximum temperature values
    for each day and location. Those individual records are then used to create an
    average monthly temperature and 30-year averages by month.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你经常在新闻中看到平均每日气温——它是如何计算的？根据你的地理位置，天气记录将使用特定的增量来记录温度，例如小时。例如，**国家海洋和大气管理局**（**NOAA**）使用站点和科学方法来计算每一天和每个地点的最小和最大温度值。然后，这些个别记录被用来创建平均月温度和按月计算的30年平均值。
- en: Be sure to understand the lineage of your data, especially when working with
    averages, because an average of an average will limit how you can work with the
    data and is most likely a misuse of the metric. Ideally, have the lowest level
    of detail so you can re-calculate an average based on any period of time desired.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你理解你的数据来源，尤其是在处理平均值时，因为平均值的平均值将限制你如何处理数据，并且很可能是指标误用。理想情况下，拥有最低级别的细节，这样你就可以根据所需的任何时间段重新计算平均值。
- en: For the example that we have been working with throughout this chapter, calculating
    the average or mean would provide a standard that we can use for comparison. So,
    if we want to know the average number of hits per day, all we have to do is count
    all of the records and divide that by the distinct count of the values of date.
    From the `tbl_user_geo_hits` data, the average would be 2.3 hits per day because
    you had seven records with three distinct days. We can now use this as a litmus
    test to measure when each day has significant increases or decreases when compared
    to that mean value.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们在本章中一直在使用的示例，计算平均值或均值将提供一个我们可以用于比较的标准。因此，如果我们想知道每天的平均点击次数，我们只需计算所有记录并除以日期值的唯一计数。从`tbl_user_geo_hits`数据中，平均值为每天2.3次点击，因为你共有七个记录，三天是不同的。现在我们可以用这个作为试金石来衡量每天与该平均值相比是否有显著的增加或减少。
- en: The median, a central value across a series of values, can be determined by
    finding the middle value, where fifty percent of the values are greater than or
    less than that specific data value. It is common to have those values ordered
    first to make it easier to identify that middle value. Identifying the mean is
    good for measuring the central tendency, which helps you to identify how the data
    values are distributed and is not affected by outlier data points. We will explore
    the shape of distributions in [Chapter 9](e3570c4a-c8ad-483f-9f3f-3e113156e9c2.xhtml), *Plotting,
    Visualization, and Storytelling*<q>.</q>
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 中位数，是一系列数值中的中心值，可以通过找到中间值来确定，其中50%的数值大于或小于该特定数据值。通常将这些值按顺序排列，以便更容易识别中间值。确定平均值对于测量中心趋势很有用，这有助于你识别数据值的分布情况，并且不受异常数据点的影响。我们将在第9章[绘图、可视化和讲故事](e3570c4a-c8ad-483f-9f3f-3e113156e9c2.xhtml)中探讨分布的形状。
- en: 'Finally, we have the mode, which is the value in a series that occurs the most
    frequently. The mode is less commonly used in analytics and analysis but is useful
    when identifying outliers in your data. What is an outlier? Mathematically, it
    could be defined as a value that is multiple standard deviations from the mean.
    Practically, it''s when a value or aggregated value is out of the normal pattern
    as compared to the rest of the data. A good way to visually identify outliers
    is to plot the values against a normal distribution or what is commonly known
    as a *bell curve*, which is represented in the following diagram as a dotted black
    line:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有众数，它是系列中出现频率最高的值。众数在分析和分析中不太常用，但在识别数据中的异常值时很有用。什么是异常值？从数学上讲，它可能被定义为与平均值多个标准差的数据值。实际上，它是指与数据中的其他值相比，某个值或汇总值偏离正常模式的情况。一种识别异常值的好方法是绘制值与正态分布或通常所说的*钟形曲线*的对比图，以下图中用虚线黑色线表示：
- en: '![](img/27778f95-265a-42de-8fcb-27ad2d17be2f.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/27778f95-265a-42de-8fcb-27ad2d17be2f.png)'
- en: As you can see in the preceding diagram, the distribution line follows a shape
    based on the mean and standard deviation. A true normal distribution would have values
    where the mean, median, and mode are all equal. Left and right of that peak at
    the top of the line are based on one, two, or three standard deviations. A value
    plus or minus three standard deviations would represent only 99.7% of the population
    of values.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，分布线遵循基于平均值和标准差形状。一个真正的正态分布将具有平均值、中位数和众数都相等的数据值。在曲线顶部的峰值左右，基于一个、两个或三个标准差。加减三个标准差的数据值将代表仅占所有数据值总人口的99.7%。
- en: In practical terms, we are saying less than one percent of all of the data would
    be toward the min and max values. Having your data in a normal distribution may
    or may not be relevant but having this understanding helps to identify the shape
    of your data values. In the preceding example, there are red dots that fall significantly
    below and above the line, which are considered outliers. Having this visual representation
    of the outliers as compared to the normal distribution will help you as a data
    analyst to have a conversation with others about the data. This is the power of
    data literacy, where a visual representation can spark dialog and help to answer
    questions about what is expected or normal and what is an exception or an outlier.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际的角度来说，我们说的是所有数据中不到百分之一的数据会接近最小值和最大值。你的数据是否呈正态分布可能并不重要，但了解这一点有助于识别数据值的形状。在先前的例子中，有一些红色点明显低于和高于线，这些被认为是异常值。与正态分布相比，这种异常值的视觉表示将帮助数据分析师与他人就数据进行交流。这是数据素养的力量，其中视觉表示可以激发对话，并帮助回答关于预期或正常情况以及异常或异常值的问题。
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Congratulations, we have covered the fundamentals of joining and merging data
    in both SQL and Python using pandas DataFrames. Throughout the process, we discussed
    practical examples of which joins to use along with why you should use them against
    user hits data. Enriching our data by blending multiple data tables allows deeper
    analysis and the ability to answer many more questions about the original single
    data source. After learning about joins and the `merge()` function, we uncovered
    the advantages and disadvantages of data aggregation. We walked through practical
    examples of using the `groupby` feature in both SQL and DataFrames. We walked
    through the differences between statistical functions and mean, median, and mode,
    along with tips for finding outliers in your data by comparing results to a normal
    distribution bell curve.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，我们已经覆盖了在 SQL 和 Python 中使用 pandas DataFrames 进行数据连接和合并的基础知识。在整个过程中，我们讨论了实际例子，说明了应该使用哪些连接类型，以及为什么应该针对用户点击数据使用它们。通过混合多个数据表来丰富我们的数据，可以让我们进行更深入的分析，并能够回答更多关于原始单一数据源的问题。在了解连接和
    `merge()` 函数之后，我们揭示了数据聚合的优缺点。我们通过实际例子介绍了在 SQL 和 DataFrames 中使用 `groupby` 功能。我们还探讨了统计函数与均值、中位数和众数之间的区别，以及通过将结果与正态分布的钟形曲线进行比较来寻找数据中异常值的技巧。
- en: In our next chapter, we will be heading back to using plot libraries and visualizing
    data.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将回到使用绘图库和可视化数据。
- en: Further reading
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information on the topics of this chapter, you can refer to the following
    links:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 关于本章主题的更多信息，您可以参考以下链接：
- en: 'Guide to using the merge function: [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用合并函数的指南：[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html)
- en: NOAA weather data: [https://www.ncdc.noaa.gov/cdo-web/datatools/records](https://www.ncdc.noaa.gov/cdo-web/datatools/records)
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NOAA 气象数据：[https://www.ncdc.noaa.gov/cdo-web/datatools/records](https://www.ncdc.noaa.gov/cdo-web/datatools/records)
- en: SQL join types: [https://www.w3schools.com/sql/sql_join.asp](https://www.w3schools.com/sql/sql_join.asp)
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQL 连接类型：[https://www.w3schools.com/sql/sql_join.asp](https://www.w3schools.com/sql/sql_join.asp)
- en: Data Literacy Project – understanding aggregations: [http://qcc.qlik.com/mod/url/view.php?id=5268](https://thedataliteracyproject.org/learn)
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据素养项目 - 理解聚合：[http://qcc.qlik.com/mod/url/view.php?id=5268](https://thedataliteracyproject.org/learn)
