<html><head></head><body>
		<div id="_idContainer210">
			<h1 id="_idParaDest-138"><em class="italic"><a id="_idTextAnchor140"/>Chapter 13</em>: Streaming Data with Apache Kafka</h1>
			<p>Apache Kafka opens up the world of real-time data streams. While there are fundamental differences in stream processing and batch processing, how you build data pipelines will be very similar. Understanding the differences between streaming data and batch processing will allow you to build data pipelines that take these differences into account.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Understanding logs</li>
				<li>Understanding how Kafka uses logs</li>
				<li>Building data pipelines with Kafka and NiFi</li>
				<li>Differentiating stream processing from batch processing</li>
				<li>Producing and consuming with Python</li>
			</ul>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor141"/>Understanding logs</h1>
			<p>If you have written code, you may <a id="_idIndexMarker645"/>be familiar with software logs. Software developers use logging to write output from applications to a text file to store different events that happen within the software. They then use these logs to help debug any issues that arise. In Python, you have probably implemented code similar to the following code:</p>
			<p class="source-code">import logging</p>
			<p class="source-code">logging.basicConfig(level=0,filename='python-log.log', filemode='w', format='%(levelname)s - %(message)s')</p>
			<p class="source-code">logging.debug('Attempted to divide by zero')</p>
			<p class="source-code">logging.warning('User left field blank in the form')</p>
			<p class="source-code">logging.error('Couldn't find specified file')</p>
			<p>The preceding code is a basic logging example that logs different levels – <strong class="source-inline">debug</strong>, <strong class="source-inline">warning</strong>, and <strong class="source-inline">error</strong> – to a file named <strong class="source-inline">python-log.log</strong>. The code will produce the following output:</p>
			<p class="source-code">DEBUG - Attempted to divide by zero</p>
			<p class="source-code">WARNING - User left field blank in the form</p>
			<p class="source-code">ERROR - Couldn't find specified file</p>
			<p>The messages are<a id="_idIndexMarker646"/> logged to the file in the order in which they occur. You do not, however, know the exact time the event happened. You can improve on this log by adding a timestamp, as shown in the following code:</p>
			<p class="source-code">logging.basicConfig(level=0,filename='python-log.log', filemode='w', format='%(asctime)s - %(levelname)s - %(message)s')</p>
			<p class="source-code">logging.info('Something happened')</p>
			<p class="source-code">logging.info('Something else happened, and it was bad')</p>
			<p>The results of the preceding code are shown in the following code block. Notice that now there is a timestamp. The log is ordered, as was the previous log. However, the exact time is known in this log:</p>
			<p class="source-code">2020-06-21 10:55:40,278 - INFO - Something happened</p>
			<p class="source-code">2020-06-21 10:55:40,278 - INFO - Something else happened, and it was bad</p>
			<p>In the preceding logs, you should notice that they follow a very specific format, or schema, which is defined in <strong class="source-inline">basicConfig</strong>. Another common log that you are probably familiar with is the <strong class="source-inline">web</strong> log.</p>
			<p>Web server logs are like software logs; they report events – usually requests – in chronological order, with a timestamp and the event. These logs follow a very specific format and have many tools available for parsing them. Databases also use logs internally to help in replication and to record modifications in a transaction. </p>
			<p>If applications, web servers, and databases all use logs, and they are all slightly different, what exactly is a log? </p>
			<p>A <strong class="bold">log</strong> is an ordered collection of events, or records, that is append only. </p>
			<p>That is all there is to it. Simple. To the<a id="_idIndexMarker647"/> point. And yet an extremely powerful tool in software development and data processing. The following diagram shows a sample log:</p>
			<div>
				<div id="_idContainer196" class="IMG---Figure">
					<img src="image/Figure_13.1_B15739.jpg" alt="Figure 13.1 – An example of a log&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.1 – An example of a log</p>
			<p>The preceding diagram shows individual records as blocks. The first record is on the left. Time is represented by the position of records in the log. The record to the right of another record is newer. So, record <strong class="bold">3</strong> is newer than record <strong class="bold">2</strong>. Records are not removed from the log but appended to the end. Record <strong class="bold">9</strong> is added to the far right of the log, as it is the newest record – until record 10 comes along. </p>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor142"/>Understanding how Kafka uses logs</h1>
			<p>Kafka maintains logs that <a id="_idIndexMarker648"/>are written to by producers and read by consumers. The following sections will explain<a id="_idIndexMarker649"/> topics, consumers, and producers.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor143"/>Topics</h2>
			<p>Apache Kafka uses logs<a id="_idIndexMarker650"/> to store data – records. Logs in Kafka are called <strong class="bold">topics</strong>. A topic is like <a id="_idIndexMarker651"/>a table in a database. In the previous chapter, you tested your Kafka cluster by creating a topic named <strong class="source-inline">dataengineering</strong>. The topic is saved to disk as a log file. Topics can be a single log, but usually they are scaled horizontally into partitions. Each partition is a log file that can be stored on another server. In a topic with partitions, the message order guarantee no longer applies to the topic, but only each partition. The following diagram shows a topic split into three partitions:</p>
			<div>
				<div id="_idContainer197" class="IMG---Figure">
					<img src="image/Figure_13.2_B15739.jpg" alt="Figure 13.2 – A Kafka topic with three partitions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.2 – A Kafka topic with three partitions</p>
			<p>The preceding topic – <strong class="bold">Transactions</strong> – has three partitions labeled <strong class="bold">P1</strong>, <strong class="bold">P2</strong>, and <strong class="bold">P3</strong>. Within each partition, the records are<a id="_idIndexMarker652"/> ordered, with the records to the left being older than the records to the right – the larger the number in the box, the newer the record. You will notice that the<a id="_idIndexMarker653"/> records have <strong class="bold">K:A</strong> in <strong class="bold">P1</strong> and <strong class="bold">K:B</strong> and <strong class="bold">K:C</strong> in <strong class="bold">P2</strong> and <strong class="bold">P3</strong>, respectively. Those are the keys associated with the records. By assigning a key, you guarantee that the records containing the same keys will go to the same partition. While the order of records in the topic may be out of order, each partition is in order.</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor144"/>Kafka producers and consumers</h2>
			<p>Kafka producers send data to<a id="_idIndexMarker654"/> a topic and a partition. Records can be sent round-robin to partitions or you can use a key to send data to specific partitions. When you send messages with a producer, you can do it in one of three ways:</p>
			<ul>
				<li><strong class="bold">Fire and Forget</strong>: You send a<a id="_idIndexMarker655"/> message and move on. You do not wait for an acknowledgment back from Kafka. In this method, records can get lost.</li>
				<li><strong class="bold">Synchronous</strong>: Send a message and <a id="_idIndexMarker656"/>wait for a response before moving on.</li>
				<li><strong class="bold">Asynchrous</strong>: Send a message<a id="_idIndexMarker657"/> and a callback. You move on once the message is sent, but will get a response at some point that you can handle.</li>
			</ul>
			<p>Producers are fairly straightforward – they<a id="_idIndexMarker658"/> send messages to a topic and partition, maybe request an acknowledgment, retry if a message fails – or not – and continue. Consumers, however, can be a little more complicated.</p>
			<p>Consumers read messages from a topic. Consumers run in a poll loop that runs indefinitely waiting for messages. Consumers can read from the beginning – they will start at the first message in the topic and read the entire history. Once caught up, the consumer will wait for new messages. </p>
			<p>If a consumer reads five messages, the offset is five. The offset is the position of the consumer in the topic. It is a bookmark for where the consumer left off. A consumer can always start reading a topic from the offset, or a specified offset – which is stored by Zookeeper. </p>
			<p>What happens when you have a consumer on the <strong class="source-inline">dataengineering</strong> topic, but the topic has three partitions and is writing<a id="_idIndexMarker659"/> records faster than you can read it? The following diagram shows a single consumer trying to consume three partitions:</p>
			<div>
				<div id="_idContainer198" class="IMG---Figure">
					<img src="image/Figure_13.3_B15739.jpg" alt="Figure 13.3 – Single consumer reading multiple partitions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.3 – Single consumer reading multiple partitions</p>
			<p>Using<a id="_idIndexMarker660"/> consumer groups, you can scale the reading of Kafka topics. In the preceding diagram, the consumer <strong class="bold">C1</strong> is in a consumer group, but is the only consumer. By adding additional consumers, the topics can be distributed. The following diagram shows what that looks like:</p>
			<div>
				<div id="_idContainer199" class="IMG---Figure">
					<img src="image/Figure_13.4_B15739.jpg" alt="Figure 13.4 – Two consumers in a consumer group consuming three partitions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 13.4 – Two consumers in a consumer group consuming three partitions</p>
			<p>The preceding diagram shows that there are still more partitions than consumers in the group, meaning that one<a id="_idIndexMarker661"/> consumer will be handling multiple partitions. You can <a id="_idIndexMarker662"/>add more consumers, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer200" class="IMG---Figure">
					<img src="image/Figure_13.5_B15739.jpg" alt="Figure 13.5 – More consumers than partitions leave one idle&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.5 – More consumers than partitions leave one idle</p>
			<p>In the preceding diagram, there are more consumers in the consumer group than partitions. When there are more consumers than partitions, consumers will sit idle. Therefore, it is not necessary to create more consumers than the number of partitions.</p>
			<p>You can, however, create more than one consumer group. The following diagram shows what this looks like:</p>
			<div>
				<div id="_idContainer201" class="IMG---Figure">
					<img src="image/Figure_13.6_B15739.jpg" alt="Figure 13.6 – Multiple consumer groups reading from a single topic&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.6 – Multiple consumer groups reading from a single topic</p>
			<p>Multiple consumer <a id="_idIndexMarker663"/>groups can read from the same partition. It is good practice to create a consumer group for every application that needs access to the topic. </p>
			<p>Now that you understand the<a id="_idIndexMarker664"/> basics of working with Kafka, the next section will show you how to build data pipelines using NiFi and Kafka.</p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor145"/>Building data pipelines with Kafka and NiFi</h1>
			<p>To build a data pipeline<a id="_idIndexMarker665"/> with Apache Kafka, you will need to create<a id="_idIndexMarker666"/> a producer since we do not have <a id="_idIndexMarker667"/>any production Kafka clusters to connect to. With the producer<a id="_idIndexMarker668"/> running, you can read the data like any other file or database.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor146"/>The Kafka producer</h2>
			<p>The Kafka producer <a id="_idIndexMarker669"/>will take advantage of the production data pipeline from <a href="B15739_11_ePub_AM.xhtml#_idTextAnchor118"><em class="italic">Chapter 11</em></a><em class="italic">, Project — Building a Production Data Pipeline</em>. The producer data pipeline will do little more than <a id="_idIndexMarker670"/>send the data to the Kafka topic. The following screenshot shows the completed producer data pipeline:</p>
			<div>
				<div id="_idContainer202" class="IMG---Figure">
					<img src="image/Figure_13.7_B15739.jpg" alt="Figure 13.7 – The NiFi data pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.7 – The NiFi data pipeline</p>
			<p>To create the data pipeline, perform the following steps:</p>
			<ol>
				<li>Open a terminal. You need to create the topic before you can send messages to it in NiFi. Enter the following command:<p class="source-code"><strong class="bold">bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 3 --topic users</strong></p><p>The preceding <a id="_idIndexMarker671"/>command is slightly different than<a id="_idIndexMarker672"/> the one used in the previous chapter to test the cluster. The difference is that the partitions flag is set to <strong class="source-inline">3</strong>. This will allow you to test using Consumer Groups in the next section.</p></li>
				<li>Drag an input port and connect it to the output from the <strong class="source-inline">ReadDataLake</strong> processor group, as shown in the following screenshot:<div id="_idContainer203" class="IMG---Figure"><img src="image/Figure_13.8_B15739.jpg" alt="Figure 13.8 – Connecting the input port to an output port&#13;&#10;"/></div><p class="figure-caption">Figure 13.8 – Connecting the input port to an output port</p></li>
				<li>Next, drag and drop the <strong class="source-inline">ControlRate</strong> processor to the canvas. The <strong class="source-inline">ControlRate</strong> processor will allow us to slow down the data flow with more control than just using <a id="_idIndexMarker673"/>backpressure in the queue. This <a id="_idIndexMarker674"/>will allow you to make it appear that data is streaming into the Kafka topic instead of just being there all at once. If you did write it all in one pass, once you had read it, the pipeline would stop until you added more data. </li>
				<li>To configure the <strong class="source-inline">ControlRate</strong> processor, set the <strong class="bold">Rate Control Criteria</strong> property to <strong class="source-inline">flowfile count</strong>. Set <strong class="bold">Maximum Rate</strong> to <strong class="source-inline">1</strong>. These two properties allow you to specify the amount of data passing through. Since you used the flowfile count, the maximum rate will be an integer of the number of flowfiles to let through. If you used the default option, you would set <strong class="bold">Maximum Rate</strong> to a file size on disk such as <strong class="source-inline">1 MB</strong>. Lastly, specify how frequently to allow the maximum rate through in the <strong class="bold">Time Duration</strong> property. I have left it at 1 minute. Every minute, one flowfile will be sent to the user's Kafka topic.</li>
				<li>To send the data to Kafka, drag and drop the <strong class="source-inline">PublishKafka_2_0</strong> processor to the canvas. There are multiple Kafka processors for different versions of Kafka. To configure the processor, you will set the <strong class="bold">Kafka Brokers</strong> property to <strong class="source-inline">localhost:9092</strong>, <strong class="source-inline">localhost:9093</strong>, and <strong class="source-inline">localhost:9094</strong>. A Kafka broker is a Kafka server. Since you are running a cluster, you will enter all of the IPs as a comma-separated string – just as you did in the command-line example in the previous chapter. Enter <strong class="bold">Topic Name</strong> as users and the <strong class="bold">Delivery Guarantee</strong> property to <strong class="bold">Guarantee Replication Delivery</strong>.</li>
			</ol>
			<p>You now have a Kafka producer configured in NiFi that will take the output from <strong class="source-inline">ReadDataLake</strong> and send each record to Kafka at one-minute intervals. To read the topic, you will create a NiFi data pipeline.</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor147"/>The Kafka consumer</h2>
			<p>As a data engineer, you may or<a id="_idIndexMarker675"/> may not need to set up the Kafka cluster and producers. However, as you learned at the beginning of this book, the<a id="_idIndexMarker676"/> role of the data engineer varies widely, and building the Kafka infrastructure could very well be part of your job. With Kafka receiving messages on a topic, it is time to read those messages. </p>
			<p>The completed data pipeline is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer204" class="IMG---Figure">
					<img src="image/Figure_13.9_B15739.jpg" alt="Figure 13.9 – Consuming a Kafka topic in NiFi&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.9 – Consuming a Kafka topic in NiFi</p>
			<p>To create the data pipeline, perform the following steps:</p>
			<ol>
				<li value="1">Drag and drop the <strong class="source-inline">ConsumeKafka_2_0</strong> processor to the canvas. To configure the processor, set the Kafka brokers to your cluster – <strong class="source-inline">localhost:9092</strong>, <strong class="source-inline">localhost:9093</strong>, and <strong class="source-inline">localhost:9094</strong>. Set <strong class="bold">Topic Name</strong> to <strong class="source-inline">users</strong> and <strong class="bold">Offset</strong> to <strong class="source-inline">Earliest</strong>. Lastly, set <strong class="bold">Group ID</strong> to <strong class="source-inline">NiFi Consumer</strong>. The <strong class="bold">Group ID </strong>property defines the consumer group that the consumer (processor) will be a part of. </li>
				<li>Next, I have added the <strong class="source-inline">ControlRate</strong> processor to the canvas. The <strong class="source-inline">ControlRate</strong> processor will slow down the reading of the records already on the topic. If the topic<a id="_idIndexMarker677"/> is not too large, you could just use backpressure on the queue so that once you have processed the historical data, the new records will move in real time. </li>
				<li>To configure the <strong class="source-inline">ControlRate</strong> processor, set <strong class="bold">Rate Control Criteria</strong> to <strong class="source-inline">flowfile count</strong>, <strong class="bold">Maximum Rate</strong> to <strong class="source-inline">1</strong>, and <strong class="bold">Time Duration</strong> to <strong class="source-inline">1</strong> minute. </li>
				<li>Add an output port<a id="_idIndexMarker678"/> to the canvas and name it. I have named it <strong class="source-inline">OutputKafkaConsumer</strong>. This will allow you to connect this processor group to others to complete a data pipeline.</li>
				<li>Start the processor group, and you will see records processed every minute. You are reading a Kafka topic with a single consumer in a consumer group. If you recall, when you created the topic, you set the number of partitions to three. Because there are multiple partitions, you can add more consumers to the group. To do that, you just need to add another <strong class="source-inline">ConsumeKafka_2_0</strong> processor and configure it.</li>
				<li>Drag another <strong class="source-inline">ConsumeKafka_2_0</strong> processor to the canvas. Configure it with the same Kafka brokers – <strong class="source-inline">localhost:9092</strong>, <strong class="source-inline">localhost:9093</strong>, and <strong class="source-inline">localhost:9094</strong> – and the same topic – <strong class="source-inline">users</strong>. Set <strong class="bold">Group ID</strong> to <strong class="source-inline">NiFi Consumer</strong>. </li>
				<li>Now that both processors have the same group ID, they will be members of the same consumer group. With two consumer and three partitions, one of the consumers will read two partitions and the other will read one. You can add another <strong class="source-inline">ConsumeKafka_2_0</strong> processor if the topic is streaming large amounts of data, but any more than three would sit idle.</li>
			</ol>
			<p>The new data pipeline is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer205" class="IMG---Figure">
					<img src="image/Figure_13.10_B15739.jpg" alt="Figure 13.10 – Consuming a Kafka with multiple consumers in a consumer group&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.10 – Consuming a Kafka with multiple consumers in a consumer group</p>
			<p>Running the processor group, you will start to see records flow through both <strong class="source-inline">ConsumerKafka_2_0</strong> processors. The configuration of the producer will determine which partition records are sent to and how many will flow through your consumer. Because of the default settings and the number of partitions to consumers, you will probably see two flowfiles processed by one consumer for every one flowfile processed by the other.</p>
			<p>Just as you can add more consumers to a consumer group, you can have more than one consumer group read a<a id="_idIndexMarker679"/> topic – the number of consumer groups is in no way related to the number of<a id="_idIndexMarker680"/> partitions.</p>
			<p>To add another consumer group to the data pipeline, drag and drop another <strong class="source-inline">ConsumeKafka_2_0</strong> processor. Set <strong class="bold">Kafka Brokers</strong> to <strong class="source-inline">localhost:9092</strong>, <strong class="source-inline">localhost:9093</strong>, and <strong class="source-inline">localhost:9094</strong>, and set <strong class="bold">Topic</strong> to <strong class="source-inline">users</strong>. The group ID is the name of the consumer group. Set it to anything other than <strong class="source-inline">NiFi Consumer</strong> – since this consumer group already exists. I have set <strong class="bold">Group ID</strong> to <strong class="source-inline">NiFi Consumer2</strong> – hardly creative or original, but it gets the job done. The data pipeline will now look like the following screenshot: <a id="_idIndexMarker681"/></p>
			<div>
				<div id="_idContainer206" class="IMG---Figure">
					<img src="image/Figure_13.11_B15739.jpg" alt="Figure 13.11 – Two consumer groups in NiFi&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.11 – Two consumer groups in NiFi</p>
			<p>In the preceding <a id="_idIndexMarker682"/>screenshot, you will notice that there is no <strong class="source-inline">ControlRate</strong> processor on the second consumer group. Once started, the processor will consume the entire history of the topic and send the records downstream. There were 17 records in the topic. The other queues are much smaller because the topic is being throttled.</p>
			<p>You can now connect the processor group to any other processor group to create a data pipeline that reads from Apache Kafka. In the following screenshot, I have connected the <strong class="source-inline">ReadKafka</strong> processor group to the production data pipeline from <a href="B15739_11_ePub_AM.xhtml#_idTextAnchor118"><em class="italic">Chapter 11</em></a><em class="italic">, Project – Building a Production Data Pipeline</em>:</p>
			<div>
				<div id="_idContainer207" class="IMG---Figure">
					<img src="image/Figure_13.12_B15739.jpg" alt="Figure 13.12 – The completed data pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.12 – The completed data pipeline</p>
			<p>Instead of reading the <a id="_idIndexMarker683"/>data from the data lake, the new data<a id="_idIndexMarker684"/> pipeline reads the data from the Kafka topic users. The records are sent to the staging processor group to continue the data pipeline. The end result is that the PostgreSQL production table will have all of the records from the Kafka topic. The reading of the data lake is now a Kafka producer.</p>
			<p>Creating producers and consumers in NiFi only requires the use of a single processor – <strong class="source-inline">PublishKafka</strong> or <strong class="source-inline">ConsumeKafka</strong>. The configuration is dependent on the Kafka cluster you will publish to or read from. In NiFi, Kafka is just another data input. How you process the data once received will be no different than if you ran a database query. There are some differences in the nature of the data that you must take into consideration, and the next section will discuss them.</p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor148"/>Differentiating stream processing from batch processing</h1>
			<p>While the processing tools <a id="_idIndexMarker685"/>don't change whether you are processing streams or batches, there are two things you should keep in mind while processing streams – <strong class="bold">unbounded</strong> and <strong class="bold">time</strong>.</p>
			<p>Data can be bounded <a id="_idIndexMarker686"/>or unbounded. Bounded data has an end, whereas<a id="_idIndexMarker687"/> unbounded data is constantly created and is possibly infinite. Bounded <a id="_idIndexMarker688"/>data is last year's sales of widgets. Unbounded data is a traffic sensor counting cars and recording their speeds on the highway. </p>
			<p>Why is this important in building data pipelines? Because with bounded data, you will know everything about the data. You can see it all at once. You can query it, put it in a staging environment, and then run Great Expectations on it to get a sense of the ranges, values, or other metrics to use in validation as you process your data. </p>
			<p>With unbounded data, it is streaming in and you don't know what the next piece of data will look like. This doesn't mean you can't validate it – you know that the speed of a car must be within a certain range and can't have the value <em class="italic">h</em> – it will be an integer between 0 and 200-ish.</p>
			<p>On bounded data, you can query the average or maximum of a field. On unbounded data, you will need to keep recalculating these values as data streams through the data pipeline. In the next chapter, you will learn about Apache Spark and how it can help in processing unbounded, or streaming, data.</p>
			<p>You may be thinking that yes, last year's sales numbers are bounded, but this year's are not. The year is not over, and the data is still streaming in. This brings up the second thing you should keep in mind when dealing with streams and that is time. Bounded data is complete over a time period or a window. And<a id="_idIndexMarker689"/> windowing is a method of making unbounded data bounded. </p>
			<p>There are three common windows – <strong class="bold">fixed</strong>, <strong class="bold">sliding</strong>, and <strong class="bold">session</strong>:</p>
			<ul>
				<li><strong class="bold">Fixed</strong> – Sometimes called <strong class="bold">tumbling windows, these</strong>re windows <a id="_idIndexMarker690"/>that covers a fixed time and records do not overlap. If you specify a one-minute window, the records will fall within each interval, as shown in the following diagram:</li>
			</ul>
			<div>
				<div id="_idContainer208" class="IMG---Figure">
					<img src="image/Figure_13.13_B15739.jpg" alt="Figure 13.13 – Data in fixed windows&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.13 – Data in fixed windows</p>
			<ul>
				<li><strong class="bold">Sliding</strong> – This is a<a id="_idIndexMarker691"/> window in which <a id="_idIndexMarker692"/>the window is defined, such as 1 minute, but the next window starts in less than the window length – say, every 30 <a id="_idIndexMarker693"/>seconds. This type of window will have duplicates and is good for rolling averages. A sliding window is shown in the following diagram:</li>
			</ul>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer209" class="IMG---Figure">
					<img src="image/Figure_13.14_B15739.jpg" alt="Figure 13.14 – Data in sliding windows&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.14 – Data in sliding windows</p>
			<p>The diagram shows two windows, one starting at 0 that is extended for 1 minute. The second window overlaps at 0:30 and extends for 1 minute until 1:30.</p>
			<ul>
				<li><strong class="bold">Session</strong> – Sessions will not have the<a id="_idIndexMarker694"/> same window of time but are events. For example, a user logs in to shop, their data is streamed for that login session, and the session is defined by some piece of data in the records, called a session token.</li>
			</ul>
			<p>When discussing windows and time, you must also consider what time to use – <strong class="bold">Event</strong>, <strong class="bold">Ingest</strong>, or <strong class="bold">Processing</strong>. The three different times could have different values and whichever you choose depends on your use case:</p>
			<ul>
				<li><strong class="bold">Event Time</strong> is when the event<a id="_idIndexMarker695"/> happens. This may be recorded in the record before it is sent to Kafka. For example, at 1:05, a car was recorded travelling 55 mph. </li>
				<li><strong class="bold">Ingest Time</strong> is the time that the <a id="_idIndexMarker696"/>data is recorded in a<a id="_idIndexMarker697"/> Kafka topic. The latency between the event and the recording could fluctuate depending on network latency. </li>
				<li><strong class="bold">Processing Time</strong> is the time in <a id="_idIndexMarker698"/>which you read the data from the Kafka topic and did something with it – such as processed it through your data pipeline and put it in the warehouse.</li>
			</ul>
			<p>By recognizing that you may be working with unbounded data, you will avoid problems in your data pipelines by not trying to analyze all the data at once, but by choosing an appropriate windowing and also by using the correct time for your use case. </p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor149"/>Producing and consuming with Python</h1>
			<p>You can create <a id="_idIndexMarker699"/>producers and consumers for Kafka using Python. There <a id="_idIndexMarker700"/>are multiple Kafka Python libraries – Kafka-Python, PyKafka, and Confluent Python Kafka. In this section, I <a id="_idIndexMarker701"/>will use Confluent Python Kafka, but if you <a id="_idIndexMarker702"/>want to use an open source, community-based library, you can use Kafka-Python. The principles and structure of the Python programs will be the same no matter which library you choose.</p>
			<p>To install the library, you can use <strong class="source-inline">pip</strong>. The following command will install it:</p>
			<p class="source-code">pip3 install confluent-kafka</p>
			<p>Once the library has finished installing, you can use it by importing it into your applications. The following sections will walk through writing a producer and consumer.</p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor150"/>Writing a Kafka producer in Python</h2>
			<p>To write a <a id="_idIndexMarker703"/>producer in Python, you will create a producer, send data, and listen for acknowledgements. In the previous examples, you used <strong class="source-inline">Faker</strong> to create fake data <a id="_idIndexMarker704"/>about people. You will use it again to generate the data in this example. To write the producer, perform the following steps:</p>
			<ol>
				<li value="1">Import the required libraries and create a faker:<p class="source-code"><strong class="bold">from confluent_kafka import Producer</strong></p><p class="source-code"><strong class="bold">from faker import Faker</strong></p><p class="source-code"><strong class="bold">import json</strong></p><p class="source-code"><strong class="bold">import time</strong></p><p class="source-code"><strong class="bold">fake=Faker()</strong></p></li>
				<li>Next, create the producer by specifying the IP addresses of your Kafka cluster:<p class="source-code"><strong class="bold">p=Producer({'bootstrap.servers':'localhost:9092,localhost:9093,localhost:9094'})</strong></p></li>
				<li>You can list the topics available to publish to as follows:<p class="source-code"><strong class="bold">p.list_topics().topics</strong></p></li>
				<li>There are different settings for acknowledgments and how you handle them, but for now, create a callback that will receive an error (<strong class="source-inline">err</strong>) and an acknowledgment (<strong class="source-inline">msg</strong>). In every call, only one of them will be true and have data. Using an <strong class="source-inline">if</strong> statement, check whether there is an error, otherwise, you can print the message:<p class="source-code"><strong class="bold">def receipt(err,msg):</strong></p><p class="source-code"><strong class="bold">    if err is not None:</strong></p><p class="source-code"><strong class="bold">        print('Error: {}'.format(err))</strong></p><p class="source-code"><strong class="bold">    else:</strong></p><p class="source-code"><strong class="bold">        print('{} : Message on topic {} on partition </strong></p><p class="source-code"><strong class="bold">        {} with value of {}'.format(time.strftime('%Y-</strong></p><p class="source-code"><strong class="bold">        %m-%d %H:%M:%S',time.localtime(msg.timestamp()</strong></p><p class="source-code"><strong class="bold">        [1]/1000)), msg.topic(), msg.partition(), </strong></p><p class="source-code"><strong class="bold">        msg.value().decode('utf-8')))</strong></p><p>The code that<a id="_idIndexMarker705"/> prints the message prints several pieces of data within the message object. The message has a timestamp in milliseconds. To<a id="_idIndexMarker706"/> convert it to seconds so that you can print the local datetime, divide by 1,000. It then print the topic that the message was published on – your producer can write to any topic, so if you are writing to multiple topics within a single producer, you will want to know which topic worked or had an error. It also prints the partition (<strong class="source-inline">0</strong>, <strong class="source-inline">1</strong>, <strong class="source-inline">2</strong>) and then the value of the message. The messages come back as bytes so you can decode them to <strong class="source-inline">utf-8</strong>.</p></li>
				<li>Next, create the producer loop. The code loops through a range creating a fake data object. The object is the same as in <a href="B15739_03_ePub_AM.xhtml#_idTextAnchor039"><em class="italic">Chapter 3</em></a><em class="italic">, Working with Files</em>. It then dumps the dictionary so that it can be sent to Kafka:<p class="source-code"><strong class="bold">for i in range(10):</strong></p><p class="source-code"><strong class="bold">    data={'name':fake.name(),'age':fake.random_</strong></p><p class="source-code"><strong class="bold">          int(min=18, max=80, step=1),'street':fake.</strong></p><p class="source-code"><strong class="bold">          street_address(),'city':fake.city(),</strong></p><p class="source-code"><strong class="bold">          'state':fake.state(),'zip':fake.zipcode()}</strong></p><p class="source-code"><strong class="bold">    m=json.dumps(data)</strong></p></li>
				<li>Before sending the data to Kafka, call <strong class="source-inline">poll()</strong> to get any acknowledgments for previous messages. Those will be sent to the callback (<strong class="source-inline">receipt</strong>). Now you can call <strong class="source-inline">produce()</strong> and pass the topic name, the data, and the function to send acknowledgments to:<p class="source-code">    <strong class="bold">p.poll(0)</strong></p><p class="source-code"><strong class="bold">    p.produce('users',m.encode('utf-8'),callback=receipt)</strong></p></li>
				<li>To finish, flush the producer. This will also get any existing acknowledgements and send them to <strong class="source-inline">receipt()</strong>:<p class="source-code"><strong class="bold">p.flush()</strong></p></li>
			</ol>
			<p>The results of the preceding code <a id="_idIndexMarker707"/>will be messages sent to the <strong class="source-inline">user</strong> topic <a id="_idIndexMarker708"/>on the Kafka cluster, and the terminal will print the acknowledgments, which will look like the following output:</p>
			<p class="source-code">2020-06-22 15:29:30 : Message on topic users on partition 1 with value of {'name': 'Willie Chambers', 'age': 66, 'street': '13647 Davis Neck Suite 480', 'city': 'Richardside', 'state': 'Nebraska', 'zip': '87109'}</p>
			<p>Now that you can send data to a Kafka topic, the next section will show you how to consume it.</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor151"/>Writing a Kafka consumer in Python</h2>
			<p>To create a <a id="_idIndexMarker709"/>consumer in Python, you create the consumer pointing to the Kafka cluster, select a topic to listen to, and then enter a loop that listens for <a id="_idIndexMarker710"/>new messages. The code that follows will walk you through how to write a Python consumer:</p>
			<ol>
				<li value="1">First, import the <strong class="source-inline">Consumer</strong> library and create the consumer. You will pass the IP addresses of your Kafka cluster, the consumer group name – this can be anything you want, but if you add multiple consumers to the group, they will need the same name, and Kafka will remember the offset where this consumer group stopped reading the topic – and lastly, you will pass the offset reset, or where you want to start reading:<p class="source-code"><strong class="bold">from confluent_kafka import Consumer</strong></p><p class="source-code"><strong class="bold">c=Consumer({'bootstrap.servers': 'localhost:9092,localhost:9093,localhost9093','group.id':'python-consumer','auto.offset.reset':'earliest'})</strong></p></li>
				<li>You can get a list of topics <a id="_idIndexMarker711"/>available to subscribe to as well as the number of partitions for a particular topic:<p class="source-code"><strong class="bold">c.list_topics().topics</strong></p><p class="source-code"><strong class="bold">t.topics['users'].partitions</strong></p></li>
				<li>Once you know which topic you want to consume, you can subscribe to it:<p class="source-code"><strong class="bold">c.subscribe(['users'])</strong></p></li>
				<li>To receive messages, create <a id="_idIndexMarker712"/>an infinite loop – if you want to listen forever. You can always start and stop using the offset to pick up where you left off. Call <strong class="source-inline">poll()</strong> to get the message. The result will be one of three things – nothing yet, an error, or a message. Using the <strong class="source-inline">if</strong> statements, check for a nothing, an error, or decode the message and do something with the data, which in this case is to print it. When you are done, close the connection:<p class="source-code"><strong class="bold">while True:</strong></p><p class="source-code"><strong class="bold">    msg=c.poll(1.0) #timeout</strong></p><p class="source-code"><strong class="bold">    if msg is None:</strong></p><p class="source-code"><strong class="bold">        continue</strong></p><p class="source-code"><strong class="bold">   </strong></p><p class="source-code"><strong class="bold">    if msg.error():</strong></p><p class="source-code"><strong class="bold">        print('Error: {}'.format(msg.error()))</strong></p><p class="source-code"><strong class="bold">        continue</strong></p><p class="source-code"><strong class="bold">    data=msg.value().decode('utf-8')</strong></p><p class="source-code"><strong class="bold">    print(data)</strong></p><p class="source-code"><strong class="bold">c.close()</strong></p></li>
			</ol>
			<p>The results will be several JSON objects scrolling through the terminal and will look like the following output:</p>
			<p class="source-code">{'name': 'Joseph Vaughn', 'age': 39, 'street': '978 Jordan Extensions Suite 684', 'city': 'Zunigamouth', 'state': 'Michigan', 'zip': '38090'}</p>
			<p>This is a basic <a id="_idIndexMarker713"/>example of consuming a topic with Python, but should<a id="_idIndexMarker714"/> give you an idea of the architecture and how to start building more complex consumers.</p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor152"/>Summary</h1>
			<p>In this chapter, you learned the basics of Apache Kafka – from what is a log and how Kafka uses it, to partitions, producers, and consumers. You learned how Apache NiFi can create producers and consumers with a single processor. The chapter took a quick detour to explain how streaming data is unbounded and how time and windowing work with streams. These are important considerations when working with streaming data and can result in errors if you assume you have all the data at one time. Lastly, you learned how to use Confluent Python Kafka to write basic producers and consumers in Python.</p>
			<p>Equipped with these skills, the next chapter will show you how to build a real-time data pipeline.</p>
		</div>
	</body></html>