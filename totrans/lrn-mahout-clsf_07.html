<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Learning Multilayer Perceptron Using Mahout"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Learning Multilayer Perceptron Using Mahout</h1></div></div></div><p>To understand a <span class="strong"><strong>Multilayer Perceptron</strong></span> (<span class="strong"><strong>MLP</strong></span>), we will first explore one more popular machine learning technique: <span class="strong"><strong>neural network</strong></span>. In this chapter, we will explore the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Neural network and neurons</li><li class="listitem" style="list-style-type: disc">MLP</li><li class="listitem" style="list-style-type: disc">Using Mahout for MLP implementation</li></ul></div><div class="section" title="Neural network and neurons"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec33"/>Neural network and neurons</h1></div></div></div><p>Neural network is an <a id="id262" class="indexterm"/>old algorithm, and it was developed with a goal in mind: to provide the computer with a brain. Neural network is inspired by the biological structure of the human brain where multiple neurons are connected and form columns and layers. A <a id="id263" class="indexterm"/>
<span class="strong"><strong>neuron</strong></span> is an electrically excitable cell that processes and transmits information through electrical and chemical signals. Perceptual input enters into the neural network through our sensory organs and is then further processed into higher levels. Let's understand how neurons work in our brain.</p><p>Neurons are computational units in the brain that collect the input from input nerves, which are called <a id="id264" class="indexterm"/>
<span class="strong"><strong>dendrites</strong></span>. They perform computation on these input messages and send the output using output <a id="id265" class="indexterm"/>nerves, which are called <span class="strong"><strong>axons</strong></span>. See the <a id="id266" class="indexterm"/>following figure (<a class="ulink" href="http://vv.carleton.ca/~neil/neural/neuron-a.html">http://vv.carleton.ca/~neil/neural/neuron-a.html</a>):</p><div class="mediaobject"><img src="graphics/4959OS_07_01.jpg" alt="Neural network and neurons"/></div><p>On the same lines, we develop a neural network in computers. We can represent a neuron in our algorithm <a id="id267" class="indexterm"/>as shown in the following figure:</p><div class="mediaobject"><img src="graphics/4959OS_07_02.jpg" alt="Neural network and neurons"/></div><p>Here, <span class="strong"><strong>x1</strong></span>, <span class="strong"><strong>x2</strong></span>, and <span class="strong"><strong>x3</strong></span> are the feature vectors, and they are assigned to a function <span class="strong"><strong>f</strong></span>, which will do the computation and provide the output. This activation function is usually chosen from the family of sigmoidal functions (as defined in <a class="link" href="ch03.html" title="Chapter 3. Learning Logistic Regression / SGD Using Mahout">Chapter 3</a>, <span class="emphasis"><em>Learning Logistic Regression / SGD Using Mahout</em></span>). In the case of classification problems, softmax activation functions are used. In classification problems, we want the output as <a id="id268" class="indexterm"/>the probabilities of target classes. So, it is desirable for the output to lie between 0 and 1 and the sum close to 1. Softmax function enforces these constraints. It is a generalization of the logistic function. More details on softmax function <a id="id269" class="indexterm"/>can be found at <a class="ulink" href="http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-12.html">http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-12.html</a>.</p></div></div>
<div class="section" title="Multilayer Perceptron"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec34"/>Multilayer Perceptron</h1></div></div></div><p>A neural <a id="id270" class="indexterm"/>network or artificial neural network generally refers to an MLP network. We defined neuron as an implementation in computers in the previous section. An MLP network consists of multiple layers of these neuron units. Let's understand a perceptron network of three layers, as shown in the next figure. The first layer of the MLP represents the input and has no other purpose than routing the input to every connected unit in a feed-forward fashion. The second layer <a id="id271" class="indexterm"/>is called hidden layers, and the last layer serves the <a id="id272" class="indexterm"/>special purpose of determining the output. The activation of neurons in the hidden layers can be defined as the sum of the weight of all the input. Neuron 1 in layer 2 is defined as follows:</p><p>Y12 = g(w110x0 +w111x1+w112x2+w113x3)</p><p>The first part where *x0 = 0* is called the bias and can be used as an offset, independent of the input. Neuron 2 in layer 2 is defined as follows:</p><p>Y22 = g(w120x0 +w121x1+w122x2+w123x3)</p><div class="mediaobject"><img src="graphics/4959OS_07_03.jpg" alt="Multilayer Perceptron"/></div><p>Neuron 3 in layer 2 is defined as follows:</p><p>Y32 = g (w130x0 +w131x1+w132x2+w133x3)</p><p>Here, g is a sigmoid function, as defined in <a class="link" href="ch03.html" title="Chapter 3. Learning Logistic Regression / SGD Using Mahout">Chapter 3</a>, <span class="emphasis"><em>Learning Logistic Regression / SGD Using Mahout</em></span>. The function is as follows:</p><p>g(z) = 1/1+e (-z)</p><p>In this MLP network output, from each input and hidden layers, neuron units are distributed to other nodes, and this is why this type of network is called a fully connected network. In this network, no values are fed back to the previous layer. (Feed forward is another strategy and is <a id="id273" class="indexterm"/>also known as back propagation. Details on this can be found at <a class="ulink" href="http://home.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html">http://home.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html</a>.)</p><p>An MLP <a id="id274" class="indexterm"/>network can have more than one hidden layer. To get the <a id="id275" class="indexterm"/>value of the weights so that we can get the predicted value as close as possible to the actual one is a training process of the MLP. To build an effective network, we consider a lot of items such as the number of hidden layers and neuron units in each layer, the cost function to minimize the error in predicted and actual values, and so on.</p><p>Now let's discuss two more important and problematic questions that arise when creating an MLP network:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How many hidden layers should one use for the network?</li><li class="listitem" style="list-style-type: disc">How many numbers of hidden units (neuron units) should one use in a hidden layer?</li></ul></div><p>Zero hidden layers are <a id="id276" class="indexterm"/>required to resolve linearly separable data. Assuming your data does require separation by a non-linear technique, always start with one hidden layer. Almost certainly, that's all you will need. If your data is separable using an MLP, then this MLP probably only needs a single hidden layer. In order to select the number of units in different layers, these are the guidelines:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Input layer</strong></span>: This <a id="id277" class="indexterm"/>refers to the number of explanatory <a id="id278" class="indexterm"/>variables in the model plus one for the bias node.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Output layer</strong></span>: In the <a id="id279" class="indexterm"/>case of classification, this refers to the number of target variables, and in the case of regression, this <a id="id280" class="indexterm"/>is obviously one.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Hidden layer</strong></span>: Start <a id="id281" class="indexterm"/>your network with one hidden layer and use the number of neuron units equivalent to the units in the input layer. The best way is to train several neural networks with different numbers of hidden layers and hidden neurons and measure the performance of these networks using cross-validation. You can stick with the number that <a id="id282" class="indexterm"/>yields the best-performing network. Problems that require two hidden layers are rarely encountered. However, neural networks that have more than one hidden layer can represent functions with any kind of shape. There is currently no theory to justify the use of neural networks with more than two hidden layers. In fact, for many practical problems, there is no reason to use any more than one hidden layer. A network with no hidden layer is only capable of representing linearly separable functions. Networks with one layer can approximate any function that contains a continuous mapping from one finite space to another, and networks with two hidden layers can represent an arbitrary decision boundary to arbitrary accuracy with rational activation functions and can approximate any smooth mapping to any accuracy (Chapter 5 of the book <span class="emphasis"><em>Introduction to Neural Networks for Java</em></span>).</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Number of neurons or hidden units</strong></span>: Use the number of neuron units equivalent to the <a id="id283" class="indexterm"/>units in the input layer. The number of hidden units should be less than twice the number of units in the input layer. Another rule to calculate this is <span class="emphasis"><em>(number of input units + number of output units)* 2/3</em></span>.</li></ul></div><p>Do the testing for generalization errors, training errors, bias, and variance. When a generalization error dips, then just before it begins to increase again, the numbers of nodes are usually found to be perfect at this point.</p><p>Now let's move on to the next section and explore how we can use Mahout for an MLP.</p></div>
<div class="section" title="MLP implementation in Mahout"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec35"/>MLP implementation in Mahout</h1></div></div></div><p>The MLP <a id="id284" class="indexterm"/>implementation is based on a more general neural network class. It is implemented to run on a single machine using Stochastic Gradient Descent, where the weights are updated using one data point at a time.</p><p>The number of <a id="id285" class="indexterm"/>layers and units per layer can be specified manually and determines the whole topology with each unit being fully connected to the previous layer. A bias unit is automatically added to the input of every layer. A bias unit is helpful for shifting the activation function to the left or right. It is like adding a coefficient to the linear function.</p><p>Currently, the logistic sigmoid is used as a squashing function in every hidden and output layer.</p><p>The command-line version does not perform iterations that lead to bad results on small datasets. Another restriction is that the CLI version of the MLP only supports classification, since the labels have to be given explicitly when executing the implementation in the command line.</p><p>A learned model can be stored and updated with new training instances using the <code class="literal">`--update`</code> flag. The output of the classification result is saved as a <code class="literal">.txt</code> file and only consists of the assigned labels. Apart from the command-line interface, it is possible to construct and compile more specialized neural networks using the API and interfaces in the <code class="literal">mrlegacy</code> package. (The core package is renamed as <code class="literal">mrlegacy</code>.)</p><p>In the command line, we use <code class="literal">TrainMultilayerPerceptron</code> and <code class="literal">RunMultilayerPerceptron</code> classes that are available in the <code class="literal">mrlegacy</code> package with three other classes: Neural <code class="literal">network.java</code>, <code class="literal">NeuralNetworkFunctions.java</code>, and <code class="literal">MultilayerPerceptron.java</code>. For this particular implementation, users can freely control the topology of the MLP, including the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The size of the input layer</li><li class="listitem" style="list-style-type: disc">The number of hidden layers</li><li class="listitem" style="list-style-type: disc">The size of each hidden layer</li><li class="listitem" style="list-style-type: disc">The size of the output layer</li><li class="listitem" style="list-style-type: disc">The cost function</li><li class="listitem" style="list-style-type: disc">The squashing function</li></ul></div><p>The model is trained in an online learning approach, where the weights of neurons in the MLP is updated <a id="id286" class="indexterm"/>and incremented using the backPropagation <a id="id287" class="indexterm"/>algorithm proposed by <span class="emphasis"><em>Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986), Learning representations by back-propagating errors. Nature, 323, 533-536</em></span>.</p></div>
<div class="section" title="Using Mahout for MLP"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec36"/>Using Mahout for MLP</h1></div></div></div><p>Mahout <a id="id288" class="indexterm"/>has implementation for an MLP network. The MLP implementation is currently located in the <code class="literal">Map-Reduce-Legacy</code> package. As with other classification <a id="id289" class="indexterm"/>algorithms, two separated classes are implemented to train and use this classifier. For training the classifier, the <code class="literal">org.apache.mahout.classifier.mlp.TrainMultilayerPerceptron</code> class, and for running the classifier, the <code class="literal">org.apache.mahout.classifier.mlp.RunMultilayerPerceptron</code> class is used. There are a number of parameters defined that are used with these classes, but we will discuss these parameters once we run our example on a dataset.</p><p>
<span class="strong"><strong>Dataset</strong></span>
</p><p>In this chapter, we <a id="id290" class="indexterm"/>will train an MLP to classify the iris dataset. The iris flower dataset contains data of three flower species, where each data point consists of four features. This dataset was introduced by Sir Ronald Fisher. It consists of 50 samples from each of three species of iris. These species are Iris setosa, Iris virginica, and Iris versicolor. Four features were measured from each sample:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Sepal length</li><li class="listitem" style="list-style-type: disc">Sepal width</li><li class="listitem" style="list-style-type: disc">Petal length</li><li class="listitem" style="list-style-type: disc">Petal width</li></ul></div><p>All measurements <a id="id291" class="indexterm"/>are in centimeters. You can download this dataset from <a class="ulink" href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/</a> and save it as a <code class="literal">.csv</code> file, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/4959OS_07_04.jpg" alt="Using Mahout for MLP"/></div><p>This <a id="id292" class="indexterm"/>dataset <a id="id293" class="indexterm"/>will look like the the following screenshot:</p><div class="mediaobject"><img src="graphics/4959OS_07_05.jpg" alt="Using Mahout for MLP"/></div><div class="section" title="Steps to use the MLP algorithm in Mahout"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec22"/>Steps to use the MLP algorithm in Mahout</h2></div></div></div><p>The steps to <a id="id294" class="indexterm"/>use the MLP algorithm in Mahout are as <a id="id295" class="indexterm"/>follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create the MLP model.<p>To create the MLP model, we will use the <code class="literal">TrainMultilayerPerceptron</code> class. Use the following command to generate the model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/mahout org.apache.mahout.classifier.mlp.TrainMultilayerPerceptron -i /tmp/irisdata.csv -labels Iris-setosa Iris-versicolor Iris-virginica -mo /tmp/model.model -ls 4 8 3 -l 0.2 -m 0.35 -r 0.0001</strong></span>
</pre></div><p>You can also run using the core jar: Mahout core jar (<code class="literal">xyz</code> stands for the version). If you have directly installed Mahout, it can be found under the <code class="literal">/usr/lib/mahout</code> folder. Execute the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Java –cp /usr/lib/mahout/ mahout-core-xyz-job.jar org.apache.mahout.classifier.mlp.TrainMultilayerPerceptron -i /tmp/irisdata.csv -labels Iris-setosa Iris-versicolor Iris-virginica -mo /user/hue/mlp/model.model -ls 4 8 3 -l 0.2 -m 0.35 -r 0.0001</strong></span>
</pre></div><p>The <code class="literal">TrainMultilayerPerceptron</code> class is used here and it takes different parameters. Also, <code class="literal">i</code> is the path for the input dataset. Here, we have <a id="id296" class="indexterm"/>put the dataset under the <code class="literal">/tmp</code> folder (local filesystem). Additionally, labels are defined in <a id="id297" class="indexterm"/>the dataset. Here we have the following labels:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">mo</code> is the output location for the created model.</li><li class="listitem" style="list-style-type: disc"><code class="literal">ls</code> is the number of units per layer, including input, hidden, and output layers. This parameter specifies the topology of the network. Here, we have <code class="literal">4</code> as the input feature, <code class="literal">8</code> for the hidden layer, and <code class="literal">3</code> for the output class number.</li><li class="listitem" style="list-style-type: disc"><code class="literal">l</code> is the learning rate that is used for weight updates. The default is 0.5. To approximate gradient descent, neural networks are trained with algorithms. Learning is possible either by batch or online methods. In batch training, weight changes are accumulated over an entire presentation of the training data (an epoch) before being applied, while online training updates weighs after the presentation of each training example (instance). More details can be found at <a class="ulink" href="http://axon.cs.byu.edu/papers/Wilson.nn03.batch.pdf">http://axon.cs.byu.edu/papers/Wilson.nn03.batch.pdf</a>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">m</code> is the momentum weight that is used for gradient descent. This must be in the range between 0–1.0.</li><li class="listitem" style="list-style-type: disc"><code class="literal">r</code> is the regularization value for the weight vector. This must be in the range between 0–0.1. It is used to prevent overfitting.</li></ul></div><div class="mediaobject"><img src="graphics/4959OS_07_06.jpg" alt="Steps to use the MLP algorithm in Mahout"/></div></li><li class="listitem">To test/run the MLP classification of the trained model, we can use the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/mahout org.apache.mahout.classifier.mlp.RunMultilayerPerceptron -i /tmp/irisdata.csv -cr 0 3 -mo /tmp/model.model -o /tmp/labelResult.txt</strong></span>
</pre></div><p>You can also run using the Mahout core jar (<code class="literal">xyz</code> stands for version). If you have directly installed Mahout, it can be found under the <code class="literal">/usr/lib/mahout</code> folder. Execute the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Java –cp /usr/lib/mahout/ mahout-core-xyz-job.jar org.apache.mahout.classifier.mlp.RunMultilayerPerceptron -i /tmp/irisdata.csv -cr 0 3 -mo /tmp/model.model -o /tmp/labelResult.txt</strong></span>
</pre></div><p>The <code class="literal">RunMultilayerPerceptron</code> class is employed here to use the model. This <a id="id298" class="indexterm"/>class also takes different <a id="id299" class="indexterm"/>parameters, which are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">i</code> indicates the input dataset location</li><li class="listitem" style="list-style-type: disc"><code class="literal">cr</code> is the range of columns to use from the input file, starting with 0 (that is, <code class="literal">`-cr 0 5`</code> for including the first six columns only)</li><li class="listitem" style="list-style-type: disc"><code class="literal">mo</code> is the location of the model built earlier</li><li class="listitem" style="list-style-type: disc"><code class="literal">o</code> is the path to store labeled results from running the model</li></ul></div><p> </p><div class="mediaobject"><img src="graphics/4959OS_07_07.jpg" alt="Steps to use the MLP algorithm in Mahout"/></div><p>
</p></li></ol></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec37"/>Summary</h1></div></div></div><p>In this chapter, we discussed one of the newly implemented algorithms in Mahout: MLP. We started our discussion by understanding neural networks and neuron units and continued our discussion further to understand the MLP network algorithm. We discussed how to choose different layer units. We then moved to Mahout and used the iris dataset to test and run an MLP algorithm implemented in Mahout. With this, we have finished our discussion on classification algorithms available in Apache Mahout.</p><p>Now we move on to the next chapter of this book where we will discuss the new changes coming up in the new Mahout release.</p></div></body></html>