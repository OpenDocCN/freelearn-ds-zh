["```py\npython3 -c \"import psycopg2; print(psycopg2.__version__)\"\n```", "```py\npip3 install psycopg2\n```", "```py\npip3 install psycopg2-binary\n```", "```py\n    import psycopg2 as db\n    ```", "```py\n    conn_string=\"dbname='dataengineering' host='localhost' user='postgres' password='postgres'\"\n    ```", "```py\n    conn=db.connect(conn_string)\n    ```", "```py\n    cur=conn.cursor()\n    ```", "```py\nquery = \"insert into users (id,name,street,city,zip) values({},'{}','{}','{}','{}')\".format(1,'Big Bird','Sesame Street','Fakeville','12345')\n```", "```py\ncur.mogrify(query)\n```", "```py\nquery2 = \"insert into users (id,name,street,city,zip) values(%s,%s,%s,%s,%s)\"\ndata=(1,'Big Bird','Sesame Street','Fakeville','12345')\n```", "```py\ncur.mogrify(query2,data)\n```", "```py\ncur.execute(query2,data)\n```", "```py\nconn.commit()\n```", "```py\n    import psycopg2 as db\n    from faker import Faker\n    ```", "```py\n    fake=Faker()\n    data=[]\n    i=2\n    ```", "```py\n    for r in range(1000):\n        data.append((i,fake.name(),fake.street_address(),\n                   fake.city(),fake.zipcode()))\n        i+=1\n    ```", "```py\n    data_for_db=tuple(data)\n    ```", "```py\n    conn_string=\"dbname='dataengineering' host='localhost' user='postgres' password='postgres'\"\n    conn=db.connect(conn_string)\n    cur=conn.cursor()\n    query = \"insert into users (id,name,street,city,zip) values(%s,%s,%s,%s,%s)\"\n    ```", "```py\n    print(cur.mogrify(query,data_for_db[1]))\n    ```", "```py\n    cur.executemany(query,data_for_db)\n    conn.commit()\n    ```", "```py\n    import psycopg2 as db\n    conn_string=\"dbname='dataengineering' host='localhost' user='postgres' password='postgres'\"\n    conn=db.connect(conn_string)\n    cur=conn.cursor()\n    ```", "```py\n    query = \"select * from users\"\n    cur.execute(query)\n    ```", "```py\n    for record in cur:\n        print(record)\n    ```", "```py\n    cur.fetchall()\n    cur.fetchmany(howmany)  # where howmany equals the number of records you want returned \n    cur.fetchone()\n    ```", "```py\n    data=cur.fetchone()\n    print(data[0])\n    ```", "```py\n    cur.rowcount\n    # 1001\n    ```", "```py\n    cur.rownumber\n    ```", "```py\n    conn=db.connect(conn_string)\n    cur=conn.cursor()\n    ```", "```py\n    f=open('fromdb.csv','w')\n    ```", "```py\n    cur.copy_to(f,'users',sep=',')\n    f.close()\n    ```", "```py\n    f=open('fromdb.csv','r')\n    f.read()\n    ```", "```py\n    import psycopg2 as db\n    import pandas as pd\n    conn_string=\"dbname='dataengineering' host='localhost' user='postgres' password='postgres'\"\n    conn=db.connect(conn_string)\n    ```", "```py\n    df=pd.read_sql(\"select * from users\", conn)\n    ```", "```py\n    df.to_json(orient='records')\n    ```", "```py\npip3 install elasticsearch\n```", "```py\nimport elasticsearch\nelasticsearch.__version__\n```", "```py\n(7.6.0)\n```", "```py\n    from elasticsearch import Elasticsearch\n    from faker import Faker\n    fake=Faker()\n    ```", "```py\n    es = Elasticsearch()\n    ```", "```py\n    es=Elasticsearch({'127.0.0.1'})\n    ```", "```py\ndoc={\"name\": fake.name(),\"street\": fake.street_address(), \"city\": fake.city(),\"zip\":fake.zipcode()}\nres=es.index(index=\"users\",doc_type=\"doc\",body=doc)\nprint(res['result']) #created\n```", "```py\n    from elasticsearch import helpers\n    ```", "```py\n    actions = [\n      {\n        \"_index\": \"users\",\n        \"_type\": \"doc\",\n        \"_source\": {\n    \t\"name\": fake.name(),\n    \t\"street\": fake.street_address(), \n    \t\"city\": fake.city(),\n    \t\"zip\":fake.zipcode()}\n      }\n      for x in range(998) # or for i,r in df.iterrows()\n    ]\n    ```", "```py\n    res = helpers.bulk(es, actions)\n    print(res['result'])\n    ```", "```py\n    from elasticsearch import Elasticsearch\n    es = Elasticsearch() \n    ```", "```py\n    doc={\"query\":{\"match_all\":{}}}\n    ```", "```py\n    res=es.search(index=\"users\",body=doc,size=10)\n    ```", "```py\n    print(res['hits']['hits'])\n    ```", "```py\n    for doc in res['hits']['hits']:\n        print(doc['_source'])\n    ```", "```py\nfrom pandas.io.json import json_normalize\ndf=json_normalize(res['hits']['hits'])\n```", "```py\ndoc={\"query\":{\"match\":{\"name\":\"Ronald Goodman\"}}}\nres=es.search(index=\"users\",body=doc,size=10)\nprint(res['hits']['hits'][0]['_source'])\n```", "```py\nres=es.search(index=\"users\",q=\"name:Ronald Goodman\",size=10)\nprint(res['hits']['hits'][0]['_source'])\n```", "```py\n# Get City Jamesberg - Returns Jamesberg and Lake Jamesberg\ndoc={\"query\":{\"match\":{\"city\":\"Jamesberg\"}}}\nres=es.search(index=\"users\",body=doc,size=10)\nprint(res['hits']['hits'])\n```", "```py\n[{'_index': 'users', '_type': 'doc', '_id': 'qDYoOHEBxMEH3Xr-PgMT', '_score': 6.929674, '_source': {'name': 'Tricia Mcmillan', 'street': '8077 Nancy #Mills Apt. 810', 'city': 'Jamesberg', 'zip': '63792'}}, {'_index': 'users', '_type': 'doc', '_id': 'pTYoOHEBxMEH3Xr-PgMT', '_score': 5.261652, '_source': {'name': 'Ryan Lowe', 'street': '740 Smith Pine Suite 065', 'city': 'Lake Jamesberg', 'zip': '38837'}}]\n```", "```py\n# Get Jamesberg and filter on zip so Lake Jamesberg is removed\ndoc={\"query\":{\"bool\":{\"must\":{\"match\":{\"city\":\"Jamesberg\"}},\n\"filter\":{\"term\":{\"zip\":\"63792\"}}}}}\nres=es.search(index=\"users\",body=doc,size=10)\nprint(res['hits']['hits'])\n```", "```py\n[{'_index': 'users', '_type': 'doc', '_id': 'qDYoOHEBxMEH3Xr-\nPgMT', '_score': 6.929674, '_source': {'name': 'Tricia \nMcmillan', 'street': '8077 Nancy #Mills Apt. 810', 'city': \n'Jamesberg', 'zip': '63792'}}]\n```", "```py\n    from elasticsearch import Elasticsearch\n    es = Elasticsearch() \n    ```", "```py\n    res = es.search(\n      index = 'users',\n      doc_type = 'doc',\n      scroll = '20m',\n      size = 500,\n      body = {\"query\":{\"match_all\":{}}}\n    )\n    ```", "```py\n    sid = res['_scroll_id']\n    size = res['hits']['total']['value']\n    ```", "```py\n    while (size > 0):\n        res = es.scroll(scroll_id = sid, scroll = '20m')\n    ```", "```py\n        sid = res['_scroll_id']\n        size = len(res['hits']['hits'])\n    ```", "```py\n        for doc in res['hits']['hits']:\n            print(doc['_source'])\n    ```", "```py\nimport datetime as dt\nfrom datetime import timedelta\nfrom airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.python_operator import PythonOperator\nimport pandas as pd\nimport psycopg2 as db\nfrom elasticsearch import Elasticsearch\n```", "```py\ndefault_args = {\n    'owner': 'paulcrickard',\n    'start_date': dt.datetime(2020, 4, 2),\n    'retries': 1,\n    'retry_delay': dt.timedelta(minutes=5),\n}\n```", "```py\nwith DAG('MyDBdag',\n         default_args=default_args,\n         schedule_interval=timedelta(minutes=5),      \n                           # '0 * * * *',\n         ) as dag:\n    getData = PythonOperator(task_id='QueryPostgreSQL',\n         python_callable=queryPostgresql)\n\n    insertData = PythonOperator\n    (task_id='InsertDataElasticsearch',\n         python_callable=insertElasticsearch)\ngetData >> insertData\n```", "```py\ndef queryPostgresql():\n    conn_string=\"dbname='dataengineering' host='localhost'\n    user='postgres' password='postgres'\"\n    conn=db.connect(conn_string)\n    df=pd.read_sql(\"select name,city from users\",conn)\n    df.to_csv('postgresqldata.csv')\n    print(\"-------Data Saved------\")\n```", "```py\ndef insertElasticsearch():\n    es = Elasticsearch() \n    df=pd.read_csv('postgresqldata.csv')\n    for i,r in df.iterrows():\n        doc=r.to_json()\n        res=es.index(index=\"frompostgresql\",\n                    doc_type=\"doc\",body=doc)\n        print(res)\t\n```", "```py\nairflow webserver\nairflow scheduler\n```", "```py\nselect name, city from users\n```"]