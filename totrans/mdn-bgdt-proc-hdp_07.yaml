- en: Large-Scale Data Processing Frameworks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模数据处理框架
- en: As the volume and complexity of data sources are increasing, deriving value
    out of data is also becoming increasingly difficult. Ever since Hadoop was made,
    it has built a massively scalable filesystem, HDFS. It has adopted the MapReduce
    concepts from functional programming to approach the large-scale data processing
    challenges. As technology is constantly evolving to overcome the challenges posed
    by data mining, enterprises are also finding ways to embrace these changes to
    stay ahead.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据源的数量和复杂性的增加，从数据中提取价值也变得越来越困难。自从Hadoop出现以来，它已经构建了一个可大规模扩展的文件系统HDFS。它采用了函数编程中的MapReduce概念来应对大规模数据处理挑战。随着技术不断进化以克服数据挖掘带来的挑战，企业也在寻找方法来拥抱这些变化，以保持领先。
- en: 'In this chapter, we will focus on these data processing solutions:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将关注以下数据处理解决方案：
- en: MapReduce
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MapReduce
- en: Apache Spark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark
- en: Spark SQL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL
- en: Spark Streaming
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: MapReduce
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MapReduce
- en: MapReduce is a concept that is borrowed from functional programming. The data
    processing is broken down into a map phase, where data preparation occurs, and
    a reduce phase, where the actual results are computed. The reason MapReduce has
    played an important role is the massive parallelism we can achieve as the data
    is sharded into multiple distributed servers. Without this advantage, MapReduce
    cannot really perform well.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce是从函数编程中借用的一个概念。数据处理被分解为映射阶段，其中进行数据准备，以及归约阶段，其中计算实际结果。MapReduce之所以扮演了重要角色，是因为我们可以通过将数据分片到多个分布式服务器来实现巨大的并行性。没有这个优势，MapReduce实际上无法很好地执行。
- en: 'Let''s take up a simple example to understand how MapReduce works in functional
    programming:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个简单的例子来了解MapReduce在函数编程中的工作原理：
- en: The input data is processed using a mapper function of our choice
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数据使用我们选择的映射函数进行处理
- en: The output from the mapper function should be in a state that is consumable
    by the reduce function
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 映射函数的输出应该处于归约函数可消费的状态
- en: The output from the mapper function is fed to the reduce function to generate
    the necessary results
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 映射函数的输出被送入归约函数以生成必要的结果
- en: 'Let''s understand these steps using a simple program. This program uses the
    following text (randomly created) as input:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的程序来理解这些步骤。该程序使用以下文本（随机创建的）作为输入：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The input consists of data with the following fields: **City Name**, **Product
    Name**, and **Item Price** on that day.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 输入由以下字段组成：**城市名称**、**产品名称**和当天的**项目价格**。
- en: We want to write a program that will show the total cost of all products in
    a given city. This can be done in many ways. But let's try to approach this using
    MapReduce and see how it works.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想编写一个程序来显示给定城市所有产品的总成本。这可以通过多种方式完成。但让我们尝试使用MapReduce来解决这个问题，看看它是如何工作的。
- en: 'The mapper program is like this:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 映射程序如下：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The reduce program is:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 归约程序如下：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We create a data pipeline using the UNIX terminal like this:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用UNIX终端创建一个数据管道，如下所示：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As we can see, the result is as expected. This is a very simple case of MapReduce.
    Let''s try to see what is happening:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，结果是预期的。这是一个非常简单的MapReduce案例。让我们尝试看看发生了什么：
- en: Each input line is processed by the `map.pl` program and prints the city and
    price
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个输入行都由`map.pl`程序处理并打印城市和价格
- en: The output from the `map.pl` program is fed to `reduce.pl`, which performs a
    `SUM()` operation for all records and categorizes them per city
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map.pl`程序的输出被送入`reduce.pl`，它对所有记录执行`SUM()`操作并将它们按城市分类'
- en: Let's shuffle the `input.txt` and see if we get the desired results.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们洗牌`input.txt`并看看我们是否得到期望的结果。
- en: 'Here is the modified `input.txt`:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是修改后的`input.txt`：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And the output from the MapReduce operation is:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce操作的输出如下：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'There is no difference because both the map and reduce operations are being
    performed independently in one go. There is no data parallelism here. The entire
    process can be visualized in this diagram:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 没有区别，因为映射和归约操作都是一次性独立执行的。这里没有数据并行性。整个过程可以在以下图中可视化：
- en: '![](img/01dc378d-5166-4980-ac5c-8b85fd387c32.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/01dc378d-5166-4980-ac5c-8b85fd387c32.png)'
- en: As we can see, there is one copy of the input data after the **Map Phase**,
    and the final output after **Reduce Phase** is what we are interested in.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在**映射阶段**之后有一个输入数据的副本，在**归约阶段**之后的最终输出是我们感兴趣的。
- en: Running a single-threaded process is useful and is needed when we don’t have
    to deal with massive amounts of data. When the input sizes are unbounded and cannot
    be fit into a single server, we need to start thinking of distributed/parallel
    algorithms to attack the problem at hand.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 运行单线程进程是有用的，并且在我们不需要处理大量数据时是必需的。当输入大小无界且无法适应单个服务器时，我们需要开始考虑分布式/并行算法来处理当前的问题。
- en: Hadoop MapReduce
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop MapReduce
- en: Apache MapReduce is a framework that makes it easier for us to run MapReduce
    operations on very large, distributed datasets. One of the advantages of Hadoop
    is a distributed file system that is rack-aware and scalable. The Hadoop job scheduler
    is intelligent enough to make sure that the computation happens on the nodes where
    the data is located. This is also a very important aspect as it reduces the amount
    of network IO.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Apache MapReduce 是一个框架，它使我们能够更容易地在非常大的分布式数据集上运行 MapReduce 操作。Hadoop 的一个优点是具有一个具有机架感知性和可扩展性的分布式文件系统。Hadoop
    作业调度器足够智能，可以确保计算发生在数据所在的节点上。这也是一个非常重要的方面，因为它减少了网络 I/O 的数量。
- en: 'Let''s see how the framework makes it easier to run massively parallel computations
    with the help of this diagram:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看框架如何通过这个图帮助我们在大量并行计算中更容易地运行：
- en: '![](img/4b616b8c-1ff0-4e08-b6e7-a197f494f0aa.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4b616b8c-1ff0-4e08-b6e7-a197f494f0aa.png)'
- en: This diagram looks a bit more complicated than the previous diagram, but most
    of the things are done by the Hadoop MapReduce framework itself for us. We still
    write the code for mapping and reducing our input data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图看起来比之前的图复杂一些，但大部分工作都是由 Hadoop MapReduce 框架为我们自己完成的。我们仍然编写代码来映射和归约我们的输入数据。
- en: 'Let''s see in detail what happens when we process our data with the Hadoop
    MapReduce framework from the preceding diagram:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看当我们使用前面的图中的 Hadoop MapReduce 框架处理我们的数据时会发生什么：
- en: Our input data is broken down into pieces
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将输入数据分解成块。
- en: Each piece of the data is fed to a mapper program
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个数据块都被喂给一个 mapper 程序。
- en: Outputs from all the mapper programs are collected, shuffled, and sorted
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有 mapper 程序的输出被收集、洗牌和排序。
- en: Each sorted piece is fed to the reducer program
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个排序好的数据块被喂给 reducer 程序。
- en: Outputs from all the reducers are combined to generate the output data
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有 reducer 的输出被组合起来生成输出数据。
- en: Streaming MapReduce
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式 MapReduce
- en: Streaming MapReduce is one of the features that is available in the Hadoop MapReduce
    framework, where we can use any of the external programs to act as Mapper and
    Reducer. As long as these programs can be executed by the target operating system,
    they are accepted to run the Map and Reduce tasks.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Streaming MapReduce 是 Hadoop MapReduce 框架中的一项功能，在这里我们可以使用任何外部程序作为 Mapper 和 Reducer。只要这些程序可以被目标操作系统执行，它们就可以被接受来运行
    Map 和 Reduce 任务。
- en: 'Here are a few things to keep in mind while writing these programs:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写这些程序时，以下是一些需要注意的事项：
- en: These programs should read the input from the `STDIN`
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些程序应该从 `STDIN` 读取输入。
- en: They should be able to process infinite amount of data (stream) or else they
    crash
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们应该能够处理无限量的数据（流）否则它们会崩溃。
- en: The memory requirements of these programs should be known well ahead of time
    before they are used in the streaming MapReduce, or else we might see unpredictable
    behavior
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用流式 MapReduce 之前，应该提前很好地了解这些程序的内存需求，否则我们可能会看到不可预测的行为。
- en: In the previous section, we have written simple Perl scripts to do mapping and
    reduction. In the current scenario also, we will use the same programs to understand
    how they perform our task.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，我们编写了简单的 Perl 脚本来执行映射和归约。在当前场景中，我们也将使用相同的程序来了解它们如何执行我们的任务。
- en: If you observe carefully, `map.pl` can process infinite amounts of data and
    will not have any memory overhead. But the `reduce.pl` program uses the Perl Hash
    data structure to perform the reduction operation. Here, we might face some memory
    pressure with real-world data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察，`map.pl` 可以处理无限量的数据，并且不会有任何内存开销。但是，`reduce.pl` 程序使用 Perl Hash 数据结构来执行归约操作。在这里，我们可能会遇到一些内存压力，尤其是在处理真实世界的数据时。
- en: 'In this exercise, we use randomized input data as shown here:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们使用随机输入数据，如下所示：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Later, we need to copy the mapper and reducer scripts to all the Hadoop nodes:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们需要将 mapper 和 reducer 脚本复制到所有 Hadoop 节点上：
- en: We are using the same Hadoop cluster that's built as part of [Chapter 10](220c9e01-7416-4692-8de7-02f6b4373ac5.xhtml), *Production
    Hadoop Cluster Deployment* for this exercise. If you remember, the nodes are master,
    `node-1`, `node-2`, and `node-3`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的是作为[第10章](220c9e01-7416-4692-8de7-02f6b4373ac5.xhtml)，*生产Hadoop集群部署*一部分构建的同一个Hadoop集群。如果您还记得，节点是master，`node-1`，`node-2`和`node-3`。
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this step, we are copying the input to the `hadoop /tmp/ directory`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们正在将输入复制到`hadoop /tmp/`目录。
- en: Please use a sensible directory in your production environments as per your
    enterprise standards. Here the `/tmp` directory is used for illustration purposes
    only.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请根据您的企业标准在您的生产环境中使用一个合理的目录。在这里，`/tmp`目录仅用于说明目的。
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In this step, we are using the Hadoop streaming MapReduce framework to use
    our scripts for performing the computation:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们使用Hadoop流式MapReduce框架来使用我们的脚本来执行计算：
- en: The contents of the `map.pl` and `reduce.pl` are exactly the same as we have
    used in the previous examples.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`map.pl`和`reduce.pl`的内容与我们之前使用的例子完全相同。'
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is stored in HDFS, which we can view like this:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 输出存储在HDFS中，我们可以这样查看：
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If we observe carefully, the results match exactly with our traditional program.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仔细观察，结果与我们的传统程序完全一致。
- en: Java MapReduce
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Java MapReduce
- en: In the previous section, we have seen how to use any arbitrary programming language
    to run a MapReduce operation on Hadoop. But in most practical scenarios, it's
    good if we leverage the libraries provided by the Hadoop MapReduce infrastructure
    as they are powerful and take care of many requirements for us.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了如何使用任何任意编程语言在Hadoop上运行MapReduce操作。但在大多数实际场景中，如果我们利用Hadoop MapReduce基础设施提供的库，那会更好，因为它们功能强大，并为我们处理许多需求。
- en: Let's try to write a simple Java program using the MapReduce libraries and see
    whether we can generate the same output as in the previous exercises. In this
    example, we will use the official MapReduce implementation from the official docs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试编写一个简单的Java程序，使用MapReduce库，看看我们是否能生成与之前练习相同的输出。在这个例子中，我们将使用官方文档中的官方MapReduce实现。
- en: Documents at: [https://hadoop.apache.org/docs/r2.8.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html](https://hadoop.apache.org/docs/r2.8.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 文档位于：[https://hadoop.apache.org/docs/r2.8.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html](https://hadoop.apache.org/docs/r2.8.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)
- en: Since our input is very different from the example, and we also want to find
    the Total price of all products in a given city, we have to change the mapper
    program as per our CSV `input.txt` file. The reduce function is the same as the
    one in the official documents where our mapper function generates a `<City, Price>`
    pair. This can easily be consumed by the existing implementation.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的输入与示例非常不同，而且我们还想找到给定城市中所有产品的总价，我们必须根据我们的CSV `input.txt`文件更改mapper程序。reduce函数与官方文档中相同，其中我们的mapper函数生成一个`<City,
    Price>`对。这可以很容易地被现有实现消费。
- en: 'We have called our program `TotalPrice.java`. Let''s see how our source code
    looks:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的程序命名为`TotalPrice.java`。让我们看看我们的源代码是什么样的：
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once we have the source code, we need to compile it to create a **Java Archive**
    (**JAR**) file. It’s done in the following manner:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了源代码，我们需要编译它来创建一个**Java归档**（**JAR**）文件。这可以通过以下方式完成：
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Once we have the JAR file created, we can use the Hadoop command to submit
    the job to process the `input.txt`, and produce the output in the `/tmp/output-12`
    directory:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了JAR文件，我们可以使用Hadoop命令提交作业来处理`input.txt`，并在`/tmp/output-12`目录中生成输出：
- en: As in the case of streaming MapReduce, we need not copy the source to all the
    Hadoop servers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 就像流式MapReduce的情况一样，我们不需要将源代码复制到所有的Hadoop服务器上。
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This run should go through fine and will produce the output files in the `/tmp/output-12`
    directory. We can see the contents of the output using this command:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这次运行应该会顺利，并将在`/tmp/output-12`目录中生成输出文件。我们可以使用以下命令查看输出内容：
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This exactly matches with the previous runs as well.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这与之前的运行完全一致。
- en: As we can see, the Hadoop Mapreduce framework has taken all the necessary steps
    to make sure that the entire pipeline progress is kept within its control, giving
    us the desired result.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，Hadoop Mapreduce框架已经采取了所有必要的步骤来确保整个管道进度保持在它的控制之下，从而给我们带来期望的结果。
- en: Even though we have used a very simple dataset for our computation, Hadoop Mapreduce
    makes sure that, regardless of the size of data we are dealing with, the same
    program we have written before yields the results we are looking for. This makes
    it a very powerful architecture for batch jobs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们使用了非常简单的数据集进行计算，但Hadoop Mapreduce确保，无论我们处理的数据大小如何，我们之前编写的相同程序都能得到我们想要的结果。这使得它成为一个非常适合批量作业的强大架构。
- en: Summary
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: So far, we have seen that Hadoop Mapreduce is a powerful framework that offers
    both streaming and batch modes of operation to process vast amounts of data with
    very simple instructions. Even though Mapreduce was originally the choice of computation
    framework in Hadoop, it has failed to meet the ever-changing demands of the market,
    and new architectures were developed to address those concerns. We will learn
    about one such framework called **Apache Spark** in the next section.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到Hadoop Mapreduce是一个强大的框架，它提供了流式和批量操作模式，可以用非常简单的指令处理大量数据。尽管Mapreduce最初是Hadoop的计算框架选择，但它未能满足市场不断变化的需求，因此开发了新的架构来解决这些问题。在下一节中，我们将学习一个名为**Apache
    Spark**的框架。
- en: Apache Spark 2
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 2
- en: Apache Spark is a general-purpose cluster computing system. It's very well suited
    for large-scale data processing. It performs 100 times better than Hadoop when
    run completely in-memory and 10 times better when run entirely from disk. It has
    a sophisticated directed acyclic graph execution engine that supports an acyclic
    data flow model.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个通用的集群计算系统。它非常适合大规模数据处理。当完全在内存中运行时，它的性能比Hadoop高100倍，当完全从磁盘运行时，性能高10倍。它拥有复杂的定向无环图执行引擎，支持无环数据流模型。
- en: Apache Spark has first-class support for writing programs in Java, Scala, Python,
    and R programming languages to cater to a wider audience. It offers more than
    80 different operators to build parallel applications without worrying about the
    underlying infrastructure.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark为Java、Scala、Python和R编程语言提供了第一类支持，以适应更广泛的受众。它提供了超过80种不同的算子来构建并行应用程序，而无需担心底层基础设施。
- en: Apache Spark has libraries catering to **Structured Query Language**, known
    as Spark **SQL**; this supports writing queries in programs using ANSI SQL. It
    also has support for computing streaming data, which is very much needed in today's
    real-time data processing requirements such as powering dashboards for interactive
    user experience systems. Apache Spark also has **machine learning libraries**
    such as **Mlib**, which caters to running scientific programs. Then it has support
    for writing programs for data that follows graph data structures, known as **GraphX**.
    This makes it a really powerful framework that supports most advanced ways of
    computing.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark拥有针对**结构化查询语言**（Structured Query Language，简称Spark SQL）的库，支持在程序中使用ANSI
    SQL编写查询。它还支持计算流数据，这在当今实时数据处理需求中非常必要，例如为交互式用户体验系统提供仪表板。Apache Spark还拥有**机器学习库**，如**Mlib**，用于运行科学程序。然后它还支持编写遵循图数据结构的程序，称为**GraphX**。这使得它成为一个真正强大的框架，支持大多数先进的计算方式。
- en: Apache Spark runs not only on the Hadoop platform but also on a variety of systems,
    such as Apache Mesos, Kubernetes, Standalone, or the Cloud. This makes it a perfect
    choice for today's enterprise to chose the way it wants to leverage the power
    of this system.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark不仅运行在Hadoop平台上，还运行在各种系统上，如Apache Mesos、Kubernetes、Standalone或云。这使得它成为当今企业选择利用该系统力量的完美选择。
- en: In the coming sections, we will learn more about Spark and its ecosystem. We
    are using Spark 2.2.0 for this exercise.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将学习更多关于Spark及其生态系统的内容。我们在这个练习中使用Spark 2.2.0版本。
- en: Installing Spark using Ambari
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Ambari安装Spark
- en: From the previous chapter, we have an existing Ambari installation that is running.
    We will leverage the same installation to add Spark support. Let's see how we
    can accomplish this.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们已经有一个正在运行的Ambari安装。我们将利用相同的安装来添加Spark支持。让我们看看我们如何实现这一点。
- en: Service selection in Ambari Admin
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ambari Admin中的服务选择
- en: 'Once we log in to the Ambari Admin interface, we see the main cluster that
    is created. On this page, we click on the Actions button on the left-hand-side
    menu. It shows a screen as follows. From this menu, we click on the Add Service
    option:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们登录到Ambari Admin界面，我们会看到创建的主集群。在这个页面上，我们点击左侧菜单中的操作按钮。屏幕显示如下。从该菜单中，我们点击添加服务选项：
- en: '![](img/8100d26e-c7bb-41c3-bdad-fe6a1cf66d35.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8100d26e-c7bb-41c3-bdad-fe6a1cf66d35.png)'
- en: Add Service Wizard
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加服务向导
- en: 'Once we click on the Add Service menu item, we are shown a Wizard, where we
    have to select Spark 2 from the list of all supported services in Ambari. The
    screen looks like this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们点击“添加服务”菜单项，我们将看到一个向导，我们必须从Ambari支持的所有服务列表中选择Spark 2。屏幕看起来像这样：
- en: '![](img/c122cc80-ffbe-4ae3-9d88-cb240afc89f1.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c122cc80-ffbe-4ae3-9d88-cb240afc89f1.png)'
- en: Click on the Next button when the service selection is complete.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务选择完成后，点击“下一步”按钮。
- en: Server placement
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务器放置
- en: 'Once the Spark 2 service is selected, other dependent services are also automatically
    selected for us and we are given a choice to select the placement of the master
    servers. I have left the default selection as is:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选择了Spark 2服务，其他依赖服务也会自动为我们选择，并允许我们选择主服务器的放置位置。我已将默认选择保持不变：
- en: '![](img/0ded1e01-9c31-48ea-b1dd-194d6846a4f5.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0ded1e01-9c31-48ea-b1dd-194d6846a4f5.png)'
- en: Click on the Next button when the changes look good.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当更改看起来不错时，点击“下一步”按钮。
- en: Clients and Slaves selection
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户端和从节点选择
- en: 'In this step, we are given a choice to select the list of nodes that act as
    clients for the masters we have selected in the previous step. We can also select
    the list of servers on which we can install the client utilities. Make the selection
    as per your choice:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在此步骤中，我们可以选择作为之前步骤中选定的主服务器客户端的节点列表。我们还可以选择可以安装客户端工具的服务器列表。根据您的选择进行选择：
- en: '![](img/681a2f28-675b-4600-b27f-5b4ce2c35997.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/681a2f28-675b-4600-b27f-5b4ce2c35997.png)'
- en: Click on the Next button when the changes are done.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在更改完成后，点击“下一步”按钮。
- en: Service customization
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务定制
- en: Since Hive is also getting installed as part of the Spark 2 selection, we are
    given a choice to customize the details of the Hive datasource. I have created
    the database on the master node with the username as `hive`, password as `hive`,
    and the database also as `hive`. Please choose a strong password while making
    changes in production.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Hive也是作为Spark 2选择的一部分进行安装的，因此我们有机会自定义Hive数据源的具体细节。我已经在主节点上创建了数据库，用户名为`hive`，密码为`hive`，数据库也命名为`hive`。在更改生产环境中的设置时，请选择一个强大的密码。
- en: 'The customization screen looks like this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 定制屏幕看起来像这样：
- en: '![](img/e4fe5fb6-ff5d-4579-b454-50761a0b5c74.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e4fe5fb6-ff5d-4579-b454-50761a0b5c74.png)'
- en: Click on Next once the changes are done correctly.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当更改正确完成后，点击“下一步”。
- en: Software deployment
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 软件部署
- en: 'In this screen, we are shown a summary of the selections we have made so far.
    Click on Deploy to start deploying the Spark 2 software on the selected servers.
    We can always cancel the wizard and start over again in this step if we feel that
    we have missed any customization:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在此屏幕上，我们展示了我们迄今为止所做的选择摘要。点击“部署”以开始在所选服务器上部署Spark 2软件。如果我们觉得我们错过了任何定制，我们可以在这一步取消向导并重新开始：
- en: '![](img/631201aa-2cf2-464f-b74c-f862277555ba.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/631201aa-2cf2-464f-b74c-f862277555ba.png)'
- en: Spark installation progress
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark安装进度
- en: 'In this step, we are shown the progress of Spark software installation and
    its other dependencies. Once everything is deployed, we are shown a summary of
    any warnings and errors. As we can see from the following screen, there are some
    warnings encountered during the installation, which indicates that we need to
    restart a few services once the wizard is complete. Don''t worry its pretty normal
    to see these errors. We will correct these in the coming steps to have a successfully
    running Spark system:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在此步骤中，我们展示了Spark软件安装及其其他依赖项的进度。一旦一切部署完成，我们将显示任何警告和错误的摘要。正如我们从以下屏幕中可以看到的，在安装过程中遇到了一些警告，这表明在向导完成后我们需要重新启动一些服务。不要担心，看到这些错误是很正常的。我们将在接下来的步骤中纠正这些错误，以确保Spark系统成功运行：
- en: '![](img/e8bcda2c-e9b8-4274-ae3d-20f4788442cd.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e8bcda2c-e9b8-4274-ae3d-20f4788442cd.png)'
- en: Clicking on Complete finishes the wizard.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“完成”以完成向导。
- en: Service restarts and cleanup
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务重启和清理
- en: 'Since there were warnings during the installation process, we have to restart
    all the affected components. The restart process is shown in this screen:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在安装过程中出现了警告，我们必须重新启动所有受影响的组件。重启过程显示在此屏幕上：
- en: '![](img/bcbcd61c-c0d3-467d-bc39-02fce56366a6.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bcbcd61c-c0d3-467d-bc39-02fce56366a6.png)'
- en: Once we give a confirmation, all the associated services will be restarted and
    we will have a successfully running system.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确认，所有相关的服务都将重新启动，我们将拥有一个成功运行的系统。
- en: This finishes the installation of Spark 2 on an existing Hadoop cluster managed
    by Ambari. We will now learn more about various data structures and libraries
    in Spark in the coming sections.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了在由Ambari管理的现有Hadoop集群上安装Spark 2的过程。在接下来的章节中，我们将学习更多关于Spark中的各种数据结构和库。
- en: Apache Spark data structures
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark数据结构
- en: 'Even though Mapreduce provides a powerful way to process large amounts of data,
    it is restricted due to several drawbacks:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Mapreduce提供了一种处理大量数据的有力方式，但它由于几个缺点而受到限制：
- en: Lack of support for variety of operators
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏对各种运算符的支持
- en: Real-time data processing
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时数据处理
- en: Caching the results of data for faster iterations
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓存数据结果以加快迭代速度
- en: This is to name a few. Since Apache Spark was built from the ground up, it has
    approached the big data computation problem in a very generic way and has provided
    the developers with data structures that makes it easier to represent any type
    of data and use those to compute in a better way.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是列举了一小部分。由于Apache Spark是从底层构建的，它以非常通用的方式处理大数据计算问题，并为开发者提供了数据结构，使得表示任何类型的数据和使用这些数据结构进行更好的计算变得更加容易。
- en: RDDs, DataFrames and datasets
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDDs, DataFrames and datasets
- en: At the core of Apache Spark are distributed datasets called **RDD**, also known
    as **Resilient Distributed Datasets**. These are immutable datasets that are present
    in the cluster, which are highly available and fault tolerant. The elements in
    the RDD can be operated in parallel, giving a lot of power to the Spark cluster.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark的核心是称为RDD的分布式数据集，也称为**弹性分布式数据集**。这些是不可变的、存在于集群中的数据集，具有高度可用性和容错性。RDD中的元素可以并行操作，为Spark集群提供了很大的能力。
- en: Since the data is already present in storage systems, such as HDFS, RDBMS, S3,
    and so on, RDDs can easily be created from these external datasources. The API
    also provides us with the power to create RDDs from existing in-memory data elements.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据已经存在于存储系统中，如HDFS、RDBMS、S3等，因此可以轻松地从这些外部数据源创建RDD。API还为我们提供了从现有的内存数据元素创建RDD的能力。
- en: These RDDs do not have any pre-defined structure. So, they can assume any form
    and, by leveraging the different operators in the Spark library, we can write
    powerful programs that give us necessary results without worrying too much about
    the data complexities.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这些RDD没有预定义的结构。因此，它们可以采取任何形式，通过利用Spark库中的不同操作符，我们可以编写强大的程序，提供必要的结果，而不必过多担心数据复杂性。
- en: In order to cater to the RDBMS needs, DataFrames come into play where a DataFrame
    can be compared with a table in a relational database system. As we know, tables
    have rows and columns and the structure of the data is known ahead of time. By
    knowing the structure of the data, several optimizations can be performed during
    data processing.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足RDBMS的需求，DataFrame应运而生，其中DataFrame可以与关系数据库系统中的表进行比较。正如我们所知，表有行和列，数据结构在事先是已知的。通过了解数据结构，可以在数据处理过程中执行多种优化。
- en: Spark datasets are somewhat similar to the DataFrames. But they extend the functionality
    of the DataFrames by supporting semi-structured data objects with native language
    objects (Java and Scala). DataFrames are an immutable collection of objects with
    semantics of a relational schema. Since we are dealing with semi-structured data
    and native language objects, there is an encoder/decoder system that takes care
    of automatically converting between the types.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Spark数据集与DataFrame有些相似。但它们通过支持使用本地语言对象（Java和Scala）的半结构化数据对象来扩展DataFrame的功能。DataFrame是一个不可变的对象集合，具有关系模式的语义。由于我们处理的是半结构化数据和本地语言对象，因此存在一个编码/解码系统，负责在类型之间自动转换。
- en: 'Here is a quick comparison chart:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个快速比较表：
- en: '| **Feature** | **RDDs** | **DataFrame** | **Dataset** |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| **特性** | **RDD** | **DataFrame** | **Dataset** |'
- en: '| Data type | Unstructured data | Structured data | Semi-structured data |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 数据类型 | 非结构化数据 | 结构化数据 | 半结构化数据 |'
- en: '| Schema requirement | Completely free form | Strict datatypes  | Loosely coupled
    |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 模式要求 | 完全自由形式 | 严格数据类型  | 松散耦合 |'
- en: '| Optimization provided by Spark | Not needed as data is unstructured | Leverages
    optimizations as datatypes are known | Inferred datatypes provide some level of
    optimization |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Spark提供的优化 | 对于非结构化数据不需要 | 利用已知的数据类型进行优化 | 推断的数据类型提供一定程度的优化 |'
- en: '| High level expressions/filters | Difficult as the data form is complex in
    nature | Can leverage these as we know the data we are dealing with | Can leverage
    here too |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 高级表达式/过滤器 | 数据形式复杂，难以处理 | 我们知道我们处理的数据，因此可以利用这些 |'
- en: Apache Spark programming
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark编程
- en: Apache Spark has very good programming language support. It provides first-class
    support for Java, Scala, Python, and R programming languages. Even though the
    data structures and operators that are available with the programming languages
    are similar in nature, we have to use programming-language-specific constructs
    to achieve the desired logic. Throughout this chapter, we will use Python as the
    programming language of choice. However, Spark itself is agnostic to these programming
    languages and produces the same results regardless of the programming language
    used.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark提供了非常好的编程语言支持。它为Java、Scala、Python和R编程语言提供了第一级支持。尽管编程语言中可用的数据结构和运算符在本质上相似，但我们必须使用特定于编程语言的构造来达到所需的逻辑。在本章中，我们将使用Python作为首选的编程语言。然而，Spark本身对这些编程语言是中立的，并且使用任何编程语言都会产生相同的结果。
- en: 'Apache Spark with Python can be used in two different ways. The first way is
    to launch the `pyspark` interactive shell, which helps us run Python instructions.
    The experience is similar to the Python shell utility. Another way is to write
    standalone programs that can be invoked using the spark-submit command. In order
    to use standalone Spark programs, we have to understand the basic structure of
    a Spark program:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python的Apache Spark可以有两种不同的方式。第一种方式是启动`pyspark`交互式外壳，它帮助我们运行Python指令。体验类似于Python外壳实用程序。另一种方式是编写可以由spark-submit命令调用的独立程序。为了使用独立的Spark程序，我们必须了解Spark程序的基本结构：
- en: '![](img/54fcd556-77c8-4ce0-a7d1-3f698c9163a2.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/54fcd556-77c8-4ce0-a7d1-3f698c9163a2.png)'
- en: 'The typical anatomy of a spark program consists of a main function that executes
    different operators on the RDDs to generate the desired result. There is support
    for more than 80 different types of operators in the Spark library. At a high
    level, we can classify these operators into two types: transformations and actions.
    Transformation operators convert data from one form to another. Actions generate
    the result from the data. In order to optimize the resources in the cluster for
    performance reasons, Apache Spark actually executes the programs in checkpoints.
    Each checkpoint is arrived at only when there is a action operator. This is one
    important thing to remember, especially if you are new to programming with Spark.
    Even the most advanced programmers sometimes get confused about why they don''t
    see the desired result as they did not use any action operator on the data.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Spark程序的一般结构包括一个主函数，该函数在RDD上执行不同的运算符以生成所需的结果。Spark库支持超过80种不同类型的运算符。从高层次上讲，我们可以将这些运算符分为两种类型：转换和动作。转换运算符将数据从一种形式转换为另一种形式。动作运算符从数据生成结果。为了优化集群中的资源以实现性能，Apache
    Spark实际上在检查点中执行程序。只有当有动作运算符时，才会到达检查点。这是需要记住的一个重要事项，尤其是如果你是Spark编程的新手。即使是经验最丰富的程序员有时也会对为什么他们没有看到预期的结果而感到困惑，因为他们没有在数据上使用任何动作运算符。
- en: Coming back to the preceding diagram, we have a driver program that has main
    routine which performs several actions/transformations on the data thats stored
    in a filesystem like HDFS and gives us the desired result. We are aware that RDDs
    are the basic parallel datastore in the Spark programming language. Spark is intelligent
    enough to create these RDDs from the seed storage like HDFS and once they are
    created, it can cache the RDDs in Memory and also make these RDDs highly available
    by making them fault-tolerant. Even if the copy of the RDD goes offline due to
    a node crash, future access on the same RDDs will quickly be generated from the
    computation from which it was originally generated.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 回到前面的图表，我们有一个驱动程序，它有一个主程序，该程序对存储在类似HDFS的文件系统中的数据进行几个操作/转换，并给出我们期望的结果。我们知道RDD是Spark编程语言中的基本并行数据存储。Spark足够智能，可以从种子存储（如HDFS）创建这些RDD，一旦创建，它可以在内存中缓存RDD，并通过使它们容错来提高这些RDD的可用性。即使由于节点崩溃，RDD的副本离线，对相同RDD的后续访问也将快速从原始生成的计算中生成。
- en: Sample data for analysis
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析的样本数据
- en: In order to understand the programming API of spark, we should have a sample
    dataset on which we can perform some operations to gain confidence. In order to
    generate this dataset, we will import the sample table from the employees database
    from the previous chapter.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解Spark的编程API，我们应该有一个样本数据集，我们可以对其进行一些操作以获得信心。为了生成这个数据集，我们将从上一章的员工数据库中导入样本表。
- en: 'These are the instructions we follow to generate this dataset:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们遵循的指令来生成这个数据集：
- en: 'Log in to the server and switch to Hive user:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 登录到服务器并切换到Hive用户：
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This will put us in a remote shell, where we can dump the table from the MySQL
    database:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使我们进入一个远程shell，在那里我们可以从MySQL数据库中导出表：
- en: '[PRE16]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we should copy the file to Hadoop using the following command:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们应该使用以下命令将文件复制到Hadoop：
- en: '[PRE17]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now, the data preparation is complete as we have successfully copied it to HDFS.
    We can start using this data with Spark.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据准备已经完成，因为我们已经成功将其复制到HDFS。我们可以开始使用Spark来使用这些数据。
- en: Interactive data analysis with pyspark
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Interactive data analysis with pyspark
- en: Apache Spark distribution comes with an interactive shell called **pyspark**.
    Since we are dealing with interpreted programming languages like Python, we can
    write interactive programs while learning.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark发行版附带一个名为**pyspark**的交互式shell。由于我们处理的是像Python这样的解释型编程语言，我们可以在学习的同时编写交互式程序。
- en: 'If you remember, we have installed Spark with Apache Ambari. So we have to
    follow the standard directory locations of Apache Ambari to access the Spark-related
    binaries:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，我们使用Apache Ambari安装了Spark。因此，我们必须遵循Apache Ambari的标准目录位置来访问与Spark相关的二进制文件：
- en: '[PRE18]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The preceding steps launch the interactive Spark shell.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的步骤启动了交互式Spark shell。
- en: 'As a first step in understanding Spark''s data structures, we will load the
    `employees.csv` file from the HDFS and count the total number of lines in the
    file using these instructions:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解Spark数据结构的第一个步骤中，我们将从HDFS加载`employees.csv`文件，并使用以下说明来计算文件中的总行数：
- en: '[PRE19]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As we can see, the count matches with the previous load operation on the Unix
    shell.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，计数与之前在Unix shell上的加载操作相匹配。
- en: 'Now, let''s try to load the first five records from the file and try to see
    the schema of the data structure object:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试从文件中加载前五条记录并尝试查看数据结构对象的模式：
- en: '[PRE20]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As we can see, even though we have a CSV (tab separated file), Spark has read
    the file as a normal text file separated by newlines and the schema contains only
    one value, which is of string datatype.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，即使我们有CSV（制表符分隔的文件），Spark也将文件读取为以换行符分隔的普通文本文件，其模式中只包含一个值，即字符串数据类型。
- en: 'In this mode of operation, where we treat each record as a line, we can perform
    only a few types of operations, such as counting all occurrences of a given name:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种操作模式下，我们将每条记录视为一行，我们只能执行几种类型的操作，例如计算给定名称的所有出现次数：
- en: '[PRE21]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This mode of operations is somewhat similar to log processing. But the true
    power of Spark comes from the power of treating the data as a table with rows
    and columns, also known as **DataFrames**:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这种操作模式与日志处理有些相似。但Spark的真正力量来自于将数据视为具有行和列的表格，也称为**DataFrames**：
- en: '[PRE22]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now, we can see that Spark has automatically converted the input CSV text into
    a DataFrame. But all the fields are treated as strings.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到Spark已经自动将输入CSV文本转换为DataFrame。但所有字段都被视为字符串。
- en: 'Let''s try to use the schema inference feature of spark to automatically find
    the datatype of the fields:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用Spark的架构推断功能自动找到字段的类型：
- en: '[PRE23]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now we can see that all the fields have a proper datatype that is closest to
    the MySQL table definition.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到所有字段都有一个合适的数据类型，这与MySQL表定义最接近。
- en: 'We can apply simple actions on the data to see the results. Let''s try to find
    the total male records:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在数据上应用简单的操作来查看结果。让我们尝试找到总男性记录数：
- en: '[PRE24]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Also, try to find the male records that have more than $100K of pay:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尝试找到工资超过$100K的男性记录：
- en: '[PRE25]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: It's so simple, right? There are many more operators that are available for
    exploration in the official Spark documentation.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 难道很简单吗？官方Spark文档中还有许多其他可用于探索的操作符。
- en: Standalone application with Spark
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Standalone application with Spark
- en: In the previous section, we have seen how to use the interactive shell `pyspark`
    to learn the Spark Python API. In this section, we will write a simple Python
    program that we will run on the Spark cluster. In real-world scenarios, this is
    how we run our applications on the Spark cluster.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to do this, we will write a program called `MyFirstApp.py` with the
    following contents:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In order to run this program on the Spark cluster, we have to use the spark-submit
    command, which does the needful in terms of scheduling and coordinating the complete
    application life cycle:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As expected, those are the total number of records in our input file (excluding
    the header line).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Spark streaming application
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the powerful features of spark is building applications that process
    real-time streaming data and produce real-time results. In order to understand
    this more, we will write a simple application that tries to find duplicate messages
    in an input stream and prints all the unique messages.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: This kind of application is helpful when we are dealing with an unreliable stream
    of data and we want to submit only the data that is unique.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'The source code for this application is given here:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In this application, we connect to a remote service on port `5000`, which emits
    the messages at its own page. The program summarizes the result of operation every
    5 seconds as defined in the `StreamingContext` parameter.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s start a simple TCP server using the UNIX netcat command (`nc`)
    and a simple loop:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'After this, submit our program to the spark cluster:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'After the program starts, we see the following output:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We see that every word has exactly 5 as the count, which is expected as we are
    printing it five times in the Unix command loop.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'We can understand this with the help of this diagram:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39f75dd6-29a5-4c8e-96cb-ad8fe707d20d.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: '**INPUT STREAM** produces a continuous stream of data, which is consumed in
    real time by the **Spark Program**. After that, the results are printed by eliminating
    the duplicates'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: If we see this in chronological order, the data from time zero to time five
    seconds (**T0** - **T5**) is processed and results are available in **T5** time.
    Same thing for all other time slots.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: In this simple example, we have just learned the basics of how Spark Streaming
    can be used to build real-time applications.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL application
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When writing applications using Spark, developers have the option to use SQL
    on structured data to get the desired results. An example makes this easier for
    us to understand how to do this:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In this example, we build a DataFrame from `employees.csv` and then create a
    view in memory called **employees**. Later, we can use ANSI SQL to write and execute
    queries to generate the necessary results.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are interested in finding the top paid employees, the results are
    shown as expected:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As we can see, the simplified API provided by Apache Spark makes it easier to
    write SQL Queries on top of CSV data (without the need for an RDBMS) to get what
    we are looking for.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，Apache Spark提供的简化API使得在CSV数据上编写SQL查询（无需RDBMS）变得更加容易，以获取我们想要的结果。
- en: Summary
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you looked at the basic concepts of large-scale data processing
    frameworks and also learned that one of the powerful features of spark is building
    applications that process real-time streaming data and produce real-time results.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了大规模数据处理框架的基本概念，并且还了解到Spark的一个强大功能是构建处理实时流数据并产生实时结果的应用程序。
- en: In the next few chapters, we will discuss how to build real-time data search
    pipelines with Elasticsearch stack.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几章中，我们将讨论如何使用Elasticsearch堆栈构建实时数据搜索管道。
