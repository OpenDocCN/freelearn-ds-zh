- en: Large-Scale Data Processing Frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the volume and complexity of data sources are increasing, deriving value
    out of data is also becoming increasingly difficult. Ever since Hadoop was made,
    it has built a massively scalable filesystem, HDFS. It has adopted the MapReduce
    concepts from functional programming to approach the large-scale data processing
    challenges. As technology is constantly evolving to overcome the challenges posed
    by data mining, enterprises are also finding ways to embrace these changes to
    stay ahead.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will focus on these data processing solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark Streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MapReduce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MapReduce is a concept that is borrowed from functional programming. The data
    processing is broken down into a map phase, where data preparation occurs, and
    a reduce phase, where the actual results are computed. The reason MapReduce has
    played an important role is the massive parallelism we can achieve as the data
    is sharded into multiple distributed servers. Without this advantage, MapReduce
    cannot really perform well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take up a simple example to understand how MapReduce works in functional
    programming:'
  prefs: []
  type: TYPE_NORMAL
- en: The input data is processed using a mapper function of our choice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output from the mapper function should be in a state that is consumable
    by the reduce function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output from the mapper function is fed to the reduce function to generate
    the necessary results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s understand these steps using a simple program. This program uses the
    following text (randomly created) as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The input consists of data with the following fields: **City Name**, **Product
    Name**, and **Item Price** on that day.
  prefs: []
  type: TYPE_NORMAL
- en: We want to write a program that will show the total cost of all products in
    a given city. This can be done in many ways. But let's try to approach this using
    MapReduce and see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mapper program is like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The reduce program is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a data pipeline using the UNIX terminal like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the result is as expected. This is a very simple case of MapReduce.
    Let''s try to see what is happening:'
  prefs: []
  type: TYPE_NORMAL
- en: Each input line is processed by the `map.pl` program and prints the city and
    price
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output from the `map.pl` program is fed to `reduce.pl`, which performs a
    `SUM()` operation for all records and categorizes them per city
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's shuffle the `input.txt` and see if we get the desired results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the modified `input.txt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'And the output from the MapReduce operation is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'There is no difference because both the map and reduce operations are being
    performed independently in one go. There is no data parallelism here. The entire
    process can be visualized in this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01dc378d-5166-4980-ac5c-8b85fd387c32.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, there is one copy of the input data after the **Map Phase**,
    and the final output after **Reduce Phase** is what we are interested in.
  prefs: []
  type: TYPE_NORMAL
- en: Running a single-threaded process is useful and is needed when we don’t have
    to deal with massive amounts of data. When the input sizes are unbounded and cannot
    be fit into a single server, we need to start thinking of distributed/parallel
    algorithms to attack the problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop MapReduce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache MapReduce is a framework that makes it easier for us to run MapReduce
    operations on very large, distributed datasets. One of the advantages of Hadoop
    is a distributed file system that is rack-aware and scalable. The Hadoop job scheduler
    is intelligent enough to make sure that the computation happens on the nodes where
    the data is located. This is also a very important aspect as it reduces the amount
    of network IO.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how the framework makes it easier to run massively parallel computations
    with the help of this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b616b8c-1ff0-4e08-b6e7-a197f494f0aa.png)'
  prefs: []
  type: TYPE_IMG
- en: This diagram looks a bit more complicated than the previous diagram, but most
    of the things are done by the Hadoop MapReduce framework itself for us. We still
    write the code for mapping and reducing our input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see in detail what happens when we process our data with the Hadoop
    MapReduce framework from the preceding diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: Our input data is broken down into pieces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each piece of the data is fed to a mapper program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outputs from all the mapper programs are collected, shuffled, and sorted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each sorted piece is fed to the reducer program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outputs from all the reducers are combined to generate the output data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming MapReduce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Streaming MapReduce is one of the features that is available in the Hadoop MapReduce
    framework, where we can use any of the external programs to act as Mapper and
    Reducer. As long as these programs can be executed by the target operating system,
    they are accepted to run the Map and Reduce tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few things to keep in mind while writing these programs:'
  prefs: []
  type: TYPE_NORMAL
- en: These programs should read the input from the `STDIN`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They should be able to process infinite amount of data (stream) or else they
    crash
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The memory requirements of these programs should be known well ahead of time
    before they are used in the streaming MapReduce, or else we might see unpredictable
    behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous section, we have written simple Perl scripts to do mapping and
    reduction. In the current scenario also, we will use the same programs to understand
    how they perform our task.
  prefs: []
  type: TYPE_NORMAL
- en: If you observe carefully, `map.pl` can process infinite amounts of data and
    will not have any memory overhead. But the `reduce.pl` program uses the Perl Hash
    data structure to perform the reduction operation. Here, we might face some memory
    pressure with real-world data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this exercise, we use randomized input data as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Later, we need to copy the mapper and reducer scripts to all the Hadoop nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: We are using the same Hadoop cluster that's built as part of [Chapter 10](220c9e01-7416-4692-8de7-02f6b4373ac5.xhtml), *Production
    Hadoop Cluster Deployment* for this exercise. If you remember, the nodes are master,
    `node-1`, `node-2`, and `node-3`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this step, we are copying the input to the `hadoop /tmp/ directory`.
  prefs: []
  type: TYPE_NORMAL
- en: Please use a sensible directory in your production environments as per your
    enterprise standards. Here the `/tmp` directory is used for illustration purposes
    only.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step, we are using the Hadoop streaming MapReduce framework to use
    our scripts for performing the computation:'
  prefs: []
  type: TYPE_NORMAL
- en: The contents of the `map.pl` and `reduce.pl` are exactly the same as we have
    used in the previous examples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is stored in HDFS, which we can view like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If we observe carefully, the results match exactly with our traditional program.
  prefs: []
  type: TYPE_NORMAL
- en: Java MapReduce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we have seen how to use any arbitrary programming language
    to run a MapReduce operation on Hadoop. But in most practical scenarios, it's
    good if we leverage the libraries provided by the Hadoop MapReduce infrastructure
    as they are powerful and take care of many requirements for us.
  prefs: []
  type: TYPE_NORMAL
- en: Let's try to write a simple Java program using the MapReduce libraries and see
    whether we can generate the same output as in the previous exercises. In this
    example, we will use the official MapReduce implementation from the official docs.
  prefs: []
  type: TYPE_NORMAL
- en: Documents at: [https://hadoop.apache.org/docs/r2.8.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html](https://hadoop.apache.org/docs/r2.8.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)
  prefs: []
  type: TYPE_NORMAL
- en: Since our input is very different from the example, and we also want to find
    the Total price of all products in a given city, we have to change the mapper
    program as per our CSV `input.txt` file. The reduce function is the same as the
    one in the official documents where our mapper function generates a `<City, Price>`
    pair. This can easily be consumed by the existing implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have called our program `TotalPrice.java`. Let''s see how our source code
    looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the source code, we need to compile it to create a **Java Archive**
    (**JAR**) file. It’s done in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the JAR file created, we can use the Hadoop command to submit
    the job to process the `input.txt`, and produce the output in the `/tmp/output-12`
    directory:'
  prefs: []
  type: TYPE_NORMAL
- en: As in the case of streaming MapReduce, we need not copy the source to all the
    Hadoop servers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This run should go through fine and will produce the output files in the `/tmp/output-12`
    directory. We can see the contents of the output using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This exactly matches with the previous runs as well.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the Hadoop Mapreduce framework has taken all the necessary steps
    to make sure that the entire pipeline progress is kept within its control, giving
    us the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: Even though we have used a very simple dataset for our computation, Hadoop Mapreduce
    makes sure that, regardless of the size of data we are dealing with, the same
    program we have written before yields the results we are looking for. This makes
    it a very powerful architecture for batch jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen that Hadoop Mapreduce is a powerful framework that offers
    both streaming and batch modes of operation to process vast amounts of data with
    very simple instructions. Even though Mapreduce was originally the choice of computation
    framework in Hadoop, it has failed to meet the ever-changing demands of the market,
    and new architectures were developed to address those concerns. We will learn
    about one such framework called **Apache Spark** in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is a general-purpose cluster computing system. It's very well suited
    for large-scale data processing. It performs 100 times better than Hadoop when
    run completely in-memory and 10 times better when run entirely from disk. It has
    a sophisticated directed acyclic graph execution engine that supports an acyclic
    data flow model.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark has first-class support for writing programs in Java, Scala, Python,
    and R programming languages to cater to a wider audience. It offers more than
    80 different operators to build parallel applications without worrying about the
    underlying infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark has libraries catering to **Structured Query Language**, known
    as Spark **SQL**; this supports writing queries in programs using ANSI SQL. It
    also has support for computing streaming data, which is very much needed in today's
    real-time data processing requirements such as powering dashboards for interactive
    user experience systems. Apache Spark also has **machine learning libraries**
    such as **Mlib**, which caters to running scientific programs. Then it has support
    for writing programs for data that follows graph data structures, known as **GraphX**.
    This makes it a really powerful framework that supports most advanced ways of
    computing.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark runs not only on the Hadoop platform but also on a variety of systems,
    such as Apache Mesos, Kubernetes, Standalone, or the Cloud. This makes it a perfect
    choice for today's enterprise to chose the way it wants to leverage the power
    of this system.
  prefs: []
  type: TYPE_NORMAL
- en: In the coming sections, we will learn more about Spark and its ecosystem. We
    are using Spark 2.2.0 for this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Spark using Ambari
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the previous chapter, we have an existing Ambari installation that is running.
    We will leverage the same installation to add Spark support. Let's see how we
    can accomplish this.
  prefs: []
  type: TYPE_NORMAL
- en: Service selection in Ambari Admin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we log in to the Ambari Admin interface, we see the main cluster that
    is created. On this page, we click on the Actions button on the left-hand-side
    menu. It shows a screen as follows. From this menu, we click on the Add Service
    option:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8100d26e-c7bb-41c3-bdad-fe6a1cf66d35.png)'
  prefs: []
  type: TYPE_IMG
- en: Add Service Wizard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we click on the Add Service menu item, we are shown a Wizard, where we
    have to select Spark 2 from the list of all supported services in Ambari. The
    screen looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c122cc80-ffbe-4ae3-9d88-cb240afc89f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Click on the Next button when the service selection is complete.
  prefs: []
  type: TYPE_NORMAL
- en: Server placement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the Spark 2 service is selected, other dependent services are also automatically
    selected for us and we are given a choice to select the placement of the master
    servers. I have left the default selection as is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ded1e01-9c31-48ea-b1dd-194d6846a4f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Click on the Next button when the changes look good.
  prefs: []
  type: TYPE_NORMAL
- en: Clients and Slaves selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this step, we are given a choice to select the list of nodes that act as
    clients for the masters we have selected in the previous step. We can also select
    the list of servers on which we can install the client utilities. Make the selection
    as per your choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/681a2f28-675b-4600-b27f-5b4ce2c35997.png)'
  prefs: []
  type: TYPE_IMG
- en: Click on the Next button when the changes are done.
  prefs: []
  type: TYPE_NORMAL
- en: Service customization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since Hive is also getting installed as part of the Spark 2 selection, we are
    given a choice to customize the details of the Hive datasource. I have created
    the database on the master node with the username as `hive`, password as `hive`,
    and the database also as `hive`. Please choose a strong password while making
    changes in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'The customization screen looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4fe5fb6-ff5d-4579-b454-50761a0b5c74.png)'
  prefs: []
  type: TYPE_IMG
- en: Click on Next once the changes are done correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Software deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this screen, we are shown a summary of the selections we have made so far.
    Click on Deploy to start deploying the Spark 2 software on the selected servers.
    We can always cancel the wizard and start over again in this step if we feel that
    we have missed any customization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/631201aa-2cf2-464f-b74c-f862277555ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Spark installation progress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this step, we are shown the progress of Spark software installation and
    its other dependencies. Once everything is deployed, we are shown a summary of
    any warnings and errors. As we can see from the following screen, there are some
    warnings encountered during the installation, which indicates that we need to
    restart a few services once the wizard is complete. Don''t worry its pretty normal
    to see these errors. We will correct these in the coming steps to have a successfully
    running Spark system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8bcda2c-e9b8-4274-ae3d-20f4788442cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Clicking on Complete finishes the wizard.
  prefs: []
  type: TYPE_NORMAL
- en: Service restarts and cleanup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since there were warnings during the installation process, we have to restart
    all the affected components. The restart process is shown in this screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bcbcd61c-c0d3-467d-bc39-02fce56366a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Once we give a confirmation, all the associated services will be restarted and
    we will have a successfully running system.
  prefs: []
  type: TYPE_NORMAL
- en: This finishes the installation of Spark 2 on an existing Hadoop cluster managed
    by Ambari. We will now learn more about various data structures and libraries
    in Spark in the coming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even though Mapreduce provides a powerful way to process large amounts of data,
    it is restricted due to several drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: Lack of support for variety of operators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caching the results of data for faster iterations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is to name a few. Since Apache Spark was built from the ground up, it has
    approached the big data computation problem in a very generic way and has provided
    the developers with data structures that makes it easier to represent any type
    of data and use those to compute in a better way.
  prefs: []
  type: TYPE_NORMAL
- en: RDDs, DataFrames and datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the core of Apache Spark are distributed datasets called **RDD**, also known
    as **Resilient Distributed Datasets**. These are immutable datasets that are present
    in the cluster, which are highly available and fault tolerant. The elements in
    the RDD can be operated in parallel, giving a lot of power to the Spark cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Since the data is already present in storage systems, such as HDFS, RDBMS, S3,
    and so on, RDDs can easily be created from these external datasources. The API
    also provides us with the power to create RDDs from existing in-memory data elements.
  prefs: []
  type: TYPE_NORMAL
- en: These RDDs do not have any pre-defined structure. So, they can assume any form
    and, by leveraging the different operators in the Spark library, we can write
    powerful programs that give us necessary results without worrying too much about
    the data complexities.
  prefs: []
  type: TYPE_NORMAL
- en: In order to cater to the RDBMS needs, DataFrames come into play where a DataFrame
    can be compared with a table in a relational database system. As we know, tables
    have rows and columns and the structure of the data is known ahead of time. By
    knowing the structure of the data, several optimizations can be performed during
    data processing.
  prefs: []
  type: TYPE_NORMAL
- en: Spark datasets are somewhat similar to the DataFrames. But they extend the functionality
    of the DataFrames by supporting semi-structured data objects with native language
    objects (Java and Scala). DataFrames are an immutable collection of objects with
    semantics of a relational schema. Since we are dealing with semi-structured data
    and native language objects, there is an encoder/decoder system that takes care
    of automatically converting between the types.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a quick comparison chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Feature** | **RDDs** | **DataFrame** | **Dataset** |'
  prefs: []
  type: TYPE_TB
- en: '| Data type | Unstructured data | Structured data | Semi-structured data |'
  prefs: []
  type: TYPE_TB
- en: '| Schema requirement | Completely free form | Strict datatypes  | Loosely coupled
    |'
  prefs: []
  type: TYPE_TB
- en: '| Optimization provided by Spark | Not needed as data is unstructured | Leverages
    optimizations as datatypes are known | Inferred datatypes provide some level of
    optimization |'
  prefs: []
  type: TYPE_TB
- en: '| High level expressions/filters | Difficult as the data form is complex in
    nature | Can leverage these as we know the data we are dealing with | Can leverage
    here too |'
  prefs: []
  type: TYPE_TB
- en: Apache Spark programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark has very good programming language support. It provides first-class
    support for Java, Scala, Python, and R programming languages. Even though the
    data structures and operators that are available with the programming languages
    are similar in nature, we have to use programming-language-specific constructs
    to achieve the desired logic. Throughout this chapter, we will use Python as the
    programming language of choice. However, Spark itself is agnostic to these programming
    languages and produces the same results regardless of the programming language
    used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Spark with Python can be used in two different ways. The first way is
    to launch the `pyspark` interactive shell, which helps us run Python instructions.
    The experience is similar to the Python shell utility. Another way is to write
    standalone programs that can be invoked using the spark-submit command. In order
    to use standalone Spark programs, we have to understand the basic structure of
    a Spark program:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54fcd556-77c8-4ce0-a7d1-3f698c9163a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The typical anatomy of a spark program consists of a main function that executes
    different operators on the RDDs to generate the desired result. There is support
    for more than 80 different types of operators in the Spark library. At a high
    level, we can classify these operators into two types: transformations and actions.
    Transformation operators convert data from one form to another. Actions generate
    the result from the data. In order to optimize the resources in the cluster for
    performance reasons, Apache Spark actually executes the programs in checkpoints.
    Each checkpoint is arrived at only when there is a action operator. This is one
    important thing to remember, especially if you are new to programming with Spark.
    Even the most advanced programmers sometimes get confused about why they don''t
    see the desired result as they did not use any action operator on the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to the preceding diagram, we have a driver program that has main
    routine which performs several actions/transformations on the data thats stored
    in a filesystem like HDFS and gives us the desired result. We are aware that RDDs
    are the basic parallel datastore in the Spark programming language. Spark is intelligent
    enough to create these RDDs from the seed storage like HDFS and once they are
    created, it can cache the RDDs in Memory and also make these RDDs highly available
    by making them fault-tolerant. Even if the copy of the RDD goes offline due to
    a node crash, future access on the same RDDs will quickly be generated from the
    computation from which it was originally generated.
  prefs: []
  type: TYPE_NORMAL
- en: Sample data for analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to understand the programming API of spark, we should have a sample
    dataset on which we can perform some operations to gain confidence. In order to
    generate this dataset, we will import the sample table from the employees database
    from the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the instructions we follow to generate this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in to the server and switch to Hive user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This will put us in a remote shell, where we can dump the table from the MySQL
    database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we should copy the file to Hadoop using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, the data preparation is complete as we have successfully copied it to HDFS.
    We can start using this data with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Interactive data analysis with pyspark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark distribution comes with an interactive shell called **pyspark**.
    Since we are dealing with interpreted programming languages like Python, we can
    write interactive programs while learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you remember, we have installed Spark with Apache Ambari. So we have to
    follow the standard directory locations of Apache Ambari to access the Spark-related
    binaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The preceding steps launch the interactive Spark shell.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step in understanding Spark''s data structures, we will load the
    `employees.csv` file from the HDFS and count the total number of lines in the
    file using these instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the count matches with the previous load operation on the Unix
    shell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s try to load the first five records from the file and try to see
    the schema of the data structure object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, even though we have a CSV (tab separated file), Spark has read
    the file as a normal text file separated by newlines and the schema contains only
    one value, which is of string datatype.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this mode of operation, where we treat each record as a line, we can perform
    only a few types of operations, such as counting all occurrences of a given name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This mode of operations is somewhat similar to log processing. But the true
    power of Spark comes from the power of treating the data as a table with rows
    and columns, also known as **DataFrames**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can see that Spark has automatically converted the input CSV text into
    a DataFrame. But all the fields are treated as strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to use the schema inference feature of spark to automatically find
    the datatype of the fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now we can see that all the fields have a proper datatype that is closest to
    the MySQL table definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply simple actions on the data to see the results. Let''s try to find
    the total male records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, try to find the male records that have more than $100K of pay:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: It's so simple, right? There are many more operators that are available for
    exploration in the official Spark documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Standalone application with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we have seen how to use the interactive shell `pyspark`
    to learn the Spark Python API. In this section, we will write a simple Python
    program that we will run on the Spark cluster. In real-world scenarios, this is
    how we run our applications on the Spark cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to do this, we will write a program called `MyFirstApp.py` with the
    following contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to run this program on the Spark cluster, we have to use the spark-submit
    command, which does the needful in terms of scheduling and coordinating the complete
    application life cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: As expected, those are the total number of records in our input file (excluding
    the header line).
  prefs: []
  type: TYPE_NORMAL
- en: Spark streaming application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the powerful features of spark is building applications that process
    real-time streaming data and produce real-time results. In order to understand
    this more, we will write a simple application that tries to find duplicate messages
    in an input stream and prints all the unique messages.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of application is helpful when we are dealing with an unreliable stream
    of data and we want to submit only the data that is unique.
  prefs: []
  type: TYPE_NORMAL
- en: 'The source code for this application is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In this application, we connect to a remote service on port `5000`, which emits
    the messages at its own page. The program summarizes the result of operation every
    5 seconds as defined in the `StreamingContext` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s start a simple TCP server using the UNIX netcat command (`nc`)
    and a simple loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, submit our program to the spark cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'After the program starts, we see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We see that every word has exactly 5 as the count, which is expected as we are
    printing it five times in the Unix command loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can understand this with the help of this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39f75dd6-29a5-4c8e-96cb-ad8fe707d20d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**INPUT STREAM** produces a continuous stream of data, which is consumed in
    real time by the **Spark Program**. After that, the results are printed by eliminating
    the duplicates'
  prefs: []
  type: TYPE_NORMAL
- en: If we see this in chronological order, the data from time zero to time five
    seconds (**T0** - **T5**) is processed and results are available in **T5** time.
    Same thing for all other time slots.
  prefs: []
  type: TYPE_NORMAL
- en: In this simple example, we have just learned the basics of how Spark Streaming
    can be used to build real-time applications.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When writing applications using Spark, developers have the option to use SQL
    on structured data to get the desired results. An example makes this easier for
    us to understand how to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we build a DataFrame from `employees.csv` and then create a
    view in memory called **employees**. Later, we can use ANSI SQL to write and execute
    queries to generate the necessary results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are interested in finding the top paid employees, the results are
    shown as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the simplified API provided by Apache Spark makes it easier to
    write SQL Queries on top of CSV data (without the need for an RDBMS) to get what
    we are looking for.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you looked at the basic concepts of large-scale data processing
    frameworks and also learned that one of the powerful features of spark is building
    applications that process real-time streaming data and produce real-time results.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few chapters, we will discuss how to build real-time data search
    pipelines with Elasticsearch stack.
  prefs: []
  type: TYPE_NORMAL
