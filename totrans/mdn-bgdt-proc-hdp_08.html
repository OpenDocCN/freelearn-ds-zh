<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building Enterprise Search Platform</h1>
                </header>
            
            <article>
                
<p>After learning data ingestions and data persistence approaches, let's learn about searching the data. In this chapter, we will learn about the following important things:</p>
<ul>
<li>Data search techniques</li>
<li>Building real-time search engines.</li>
<li>Searching real-time, full-text data</li>
<li>Data indexing techniques</li>
<li>Building a real-time data search pipeline</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The data search concept</h1>
                </header>
            
            <article>
                
<p>In our everyday life, we always keep on searching something. In the morning, we search for a toothbrush, newspaper, search stock prices, bus schedule, office bag, and so on. The list goes on and on. This search activity stops when we go to bed at the end of the day. We use a lot of tools and techniques to search these things to minimize the actual search time. We use Google to search most of the things such as news, stock prices, bus schedule, and anything and everything we need. To search a particular page of a book, we use the book's index. So, the point is that search is a very important activity of our life. There are two important concepts can be surfaced out of this, that is, search tool and search time. Just think of a situation where you want to know about a particular stock price of a company and it takes a few minutes to load that page. You will definitely get very annoyed. It is because the Search Time in this case is not acceptable to you. So then the question is, <em>How to reduce this search time?</em> We will learn that in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The need for an enterprise search engine</h1>
                </header>
            
            <article>
                
<p>Just like we all need a tool to search our own things, every company also needs a search engine to build so that internal and external entities can find what they want.</p>
<p>For example, an employee has to search for his/her PTO balance, paystub of a particular month, and so on. The HR department may search for employees who are in finance group or so. In an e-commerce company, a product catalog is the most searchable object. It is a very sensitive object because it directly impacts the revenue of the company. If a customer wants to buy a pair of shoes, the first thing he/she can do is search the company product catalog. If the search time is more than a few seconds, the customer may lose interest in the product. It may also be possible that the same customer goes to another website to buy a pair of shoes, resulting in a loss of revenue.</p>
<p>It appears that even with all the tech and data in the world, we can't do much without two crucial components:</p>
<ul>
<li>Data search</li>
<li>Data index</li>
</ul>
<p>Companies such as Google, Amazon, and Apple have changed the word's expectations of search. We all expect them to search anything, anytime, and using any tool such as website, mobile, and voice-activated tools like Google Echo, Alexa, and HomePad. We expect these tools to answer all our questions, from <em>How's the weather today?</em> to give me a list of gas stations near me.</p>
<p>As these expectations are growing, the need to index more and more data is also growing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tools for building an enterprise search engine</h1>
                </header>
            
            <article>
                
<p>The following are some popular tools/products/technologies available:</p>
<ul>
<li>Apache Lucene</li>
<li>Elasticsearch</li>
<li>Apache Solr</li>
<li>Custom (in-house) search engine</li>
</ul>
<div class="packt_infobox">In this chapter, I will focus on Elasticsearch in detail. I will discuss Apache Solr on a conceptual level only.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Elasticsearch</h1>
                </header>
            
            <article>
                
<p>Elasticsearch is an open source search engine. It is based on Apache Lucene. It is distributed and supports multi-tenant capability. It uses schema-free JSON documents and has a built-in, HTTP-based web interface. It also supports analytical RESTful query workloads. It is a Java-based database server. Its main protocol is HTTP/JSON.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why Elasticsearch?</h1>
                </header>
            
            <article>
                
<p>Elasticsearch is the most popular data indexing tool as of today. It is because of its the following features:</p>
<ul>
<li>It is <strong>fast</strong>. Data is indexed at a real-time speed.</li>
<li>It is <strong>scalable</strong>. It scales horizontally.</li>
<li>It is <strong>flexible</strong>. It supports any data format, structured, semi-structured, or unstructured.</li>
<li>It is <strong>distributed</strong>. If one node fails, the cluster is still available for business.</li>
<li>It supports data search query in any language: Java, Python Ruby, C#, and so on.</li>
<li>It has a <strong>Hadoop connector,</strong> which facilitates smooth communication between Elasticsearch and Hadoop.</li>
<li>It supports robust data <strong>aggregation</strong> on huge datasets to find trends and patterns.</li>
<li>The <strong>Elastic stack</strong> (Beats, Logstash, Elasticsearch, and Kibana) and X-Pack offers out-of-the-box support for data ingestion, data indexing, data visualization, data security, and monitoring.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"> Elasticsearch components</h1>
                </header>
            
            <article>
                
<p>Before we take a deep dive, let's understand a few important components of Elasticsearch.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Index</h1>
                </header>
            
            <article>
                
<p>Elasticsearch index is a collection of JSON documents. Elasticsearch is a data store that may contain multiple indices. Each index may be divided into one or many types. A type is a group of similar documents. A type may contain multiple documents. In terms of database analogy, an index is a database and each of its types is a table. Each JSON document is a row in that table.</p>
<div class="packt_infobox">Indices created in Elasticsearch 6.0.0 or later may only contain a single mapping type.</div>
<p>Mapping types will be completely removed in Elasticsearch 7.0.0.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Document</h1>
                </header>
            
            <article>
                
<p>Document in Elasticsearch means a JSON document. It is a basic unit of data to be stored in an index. An index comprises multiple documents. In the RDBMS world, a document is nothing but a row in a table. For example, a customer document may look like the following:</p>
<pre>{<br/>"name": "Sam Taylor",<br/>"birthdate": "1995-08-11",<br/>"address":<br/>{<br/>"street": "155 rabbit Street",<br/>"city": "San Francisco",<br/>"state": "ca",<br/>"postalCode": "94107"<br/>},<br/>"contactPhone":<br/>[<br/>{<br/>"type": "home",<br/>"number": "510-415-8929"<br/>},<br/>{<br/>"type": "cell",<br/>"number": "408-171-8187"<br/>}<br/>]<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mapping</h1>
                </header>
            
            <article>
                
<p>Mapping is schema definition of an index. Just like a database, we have to define a data structure of a table. We have to create a table, its columns, and column data types. In Elasticsearch, we have define a structure of an index during its creation. We may have to define which field can be indexed, searchable, and storable.</p>
<p>The good news is that, Elasticsearch supports <strong>dynamic mapping</strong>. It means that mapping is not mandatory at index creation time. An index can be created without mapping. When a document is sent to Elasticsearch for indexing, Elasticsearch automatically defines the data structure of each field and make each field a searchable field.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cluster</h1>
                </header>
            
            <article>
                
<p>Elasticsearch is a collection of nodes (servers). Each node may store part of the data in index and provides federated indexing and search capabilities across all nodes. Each cluster has a unique name, <kbd>elasticsearch</kbd>, by default. A cluster is divided into multiple types of nodes, namely Master Node and Data Node. But an Elasticsearch cluster can be created using just one node having both Master and Data nodes installed on the same node:</p>
<ul>
<li><strong>Master node</strong>: This controls the entire cluster. There can be more than one master node in a cluster <span>(</span><span>three are </span><span>recommended)</span>. Its main function is index creation or deletion and allocation of shards (partitions) to data nodes.</li>
<li><strong>Data node</strong>: This stores the actual index data in shards. They support all data-related operations such as aggregations, index search, and so on.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Type</h1>
                </header>
            
            <article>
                
<p>Documents are divided into various logical types for example, order document, product document, customer document, and so on. Instead of creating three separate order, product, and customer indices, a single index can be logically divided into order, product and customer types. In RDBMS analogy, a type is nothing but a Table in a database. So, a type is a logical partition of an index.</p>
<div class="packt_tip">Type is deprecated in Elasticsearch version 6.0.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to index documents in Elasticsearch?</h1>
                </header>
            
            <article>
                
<p>Let's learn how Elasticsearch <span>actually</span><span> </span><span>works by indexing these three sample documents. While learning this, we will touch upon a few important functions/concepts of Elasticsearch.</span></p>
<p>These are the three sample JSON documents to be indexed:</p>
<pre>{<br/>"name": "Angela Martin",<br/>"birthdate": "1997-11-02",<br/>"street": "63542 Times Square",<br/>"city": "New York",<br/>"state": "NY",<br/>"zip": "10036",<br/>"homePhone": "212-415-8929",<br/>"cellPhone": "212-171-8187"<br/>} ,<br/>{<br/>"name": "Sam Taylor",<br/>"birthdate": "1995-08-11",<br/>"street": "155 rabbit Street",<br/>"city": "San Francisco",<br/>"state": "ca",<br/>"zip": "94107",<br/>"homePhone": "510-415-8929",<br/>"cellPhone": "408-171-8187"<br/>} ,<br/>{<br/>"name": "Dan Lee",<br/>"birthdate": "1970-01-25",<br/>"street": "76336 1st Street",<br/>"city": "Los Angeles",<br/>"state": "ca",<br/>"zip": "90010",<br/>"homePhone": "323-892-5363",<br/>"cellPhone": "213-978-1320"<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Elasticsearch installation</h1>
                </header>
            
            <article>
                
<p>First things first. Let's install Elasticsearch.</p>
<p>Please do the following steps to install Elasticsearch on your server. It is assumed that you are installing Elasticsearch using CentOS 7 on your server.</p>
<p>What minimum Hardware is required?</p>
<ul>
<li><strong>RAM</strong>: 4 GB</li>
<li><strong>CPU</strong>: 2</li>
</ul>
<p>Which JDK needs to be installed? We need JDK 8. If you don't have JDK 8 installed on your server, do the following steps to install JDK 8:</p>
<ol>
<li>Change to home folder:</li>
</ol>
<pre style="padding-left: 60px"><strong> $ cd ~</strong></pre>
<ol start="2">
<li>Download JDK RPM:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ wget --no-cookies --no-check-certificate --header "Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u73-b02/jdk-8u73-linux-x64.rpm</strong></pre>
<ol start="3">
<li>Install RMP using YUM (it is assumed that you have <kbd>sudo</kbd> access):</li>
</ol>
<pre style="padding-left: 60px"><strong>$ sudo yum -y localinstall jdk-8u73-linux-x64.rpm</strong></pre>
<p>Since, we have installed JDK 8 on our server successfully, let's start installing Elasticsearch.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installation of Elasticsearch</h1>
                </header>
            
            <article>
                
<p>For detailed installation steps, please refer to the following URL:</p>
<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/rpm.html" target="_blank"><strong>https://www.elastic.co/guide/en/elasticsearch/reference/current/rpm.html</strong></a></p>
<ol>
<li>The RPM for Elasticsearch v6.2.3 can be downloaded from the website and installed as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.1.2.rpm</strong><br/><strong>$ sudo rpm --install elasticsearch-6.1.2.rpm</strong></pre>
<ol start="2">
<li>To configure Elasticsearch to start automatically when the system boots up, run the following commands.</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo /bin/systemctl daemon-reload</strong><br/><strong>sudo /bin/systemctl enable elasticsearch.service</strong></pre>
<ol start="3">
<li>Elasticsearch can be started and stopped as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo systemctl start elasticsearch.service</strong><br/><strong>sudo systemctl stop elasticsearch.service</strong></pre>
<p>The main configuration file is located in the config folder called <kbd>elasticsearch.yml</kbd>.</p>
<p>Let's do the following initial config changes in <kbd>elasticsearch.yml</kbd>. Find and replace the following parameters:</p>
<pre><strong>cluster.name: my-elaticsearch</strong><br/><strong>path.data: /opt/data</strong><br/><strong>path.logs: /opt/logs</strong><br/><strong>network.host: 0.0.0.0</strong><br/><strong>http.port: 9200</strong></pre>
<p>Now start Elasticsearch:</p>
<pre><strong>sudo systemctl start elasticsearch.service</strong></pre>
<p>Check whether Elasticsearch is running using the following URL:</p>
<pre><strong>http://localhost:9200</strong></pre>
<p>We will get the following response:</p>
<pre><strong>// 20180320161034</strong><br/><strong>// http://localhost:9200/</strong><br/><strong>{</strong><br/><strong>    "name": "o7NVnfX",</strong><br/><strong>"cluster_name": "my-elasticsearch",</strong><br/><strong>"cluster_uuid": "jmB-_FEuTb6N_OFokwxF1A",</strong><br/><strong>"version": {</strong><br/><strong>"number": "6.1.2",</strong><br/><strong>"build_hash": "5b1fea5",</strong><br/><strong>"build_date": "2017-01-10T02:35:59.208Z",</strong><br/><strong>"build_snapshot": false,</strong><br/><strong>"lucene_version": "7.1.0",</strong><br/><strong>"minimum_wire_compatibility_version": "5.6.0",</strong><br/><strong>"minimum_index_compatibility_version": "5.0.0"</strong><br/><strong>},</strong><br/><strong>"tagline": "You Know, for Search"</strong><br/><strong>}</strong></pre>
<p>Now, our Elasticsearch is working fine. Let's create an index to store our documents.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Create index</h1>
                </header>
            
            <article>
                
<p>We will use the following <kbd>curl</kbd> command to create our first index named <kbd>my_index</kbd>:</p>
<pre>curl -XPUT 'localhost:9200/my_index?pretty' -H 'Content-Type: application/json' -d'<br/>{<br/>"settings" : {<br/>"index" : {<br/>"number_of_shards" : 2,<br/>"number_of_replicas" : 1<br/>}<br/>}<br/>}<br/>'</pre>
<p>We will get this response:</p>
<pre><strong>{</strong><br/><strong>"acknowledged" : true,</strong><br/><strong>"shards_acknowledged" : true,</strong><br/><strong>"index" : "my_index"</strong><br/><strong>}</strong></pre>
<p>In the index creation URL, we used settings, shards, and replica. Let's understand what is meant by shard and replica.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Primary shard</h1>
                </header>
            
            <article>
                
<p>We have created index with three shards. It means Elasticsearch will divide index into three partitions. Each partition is called a <strong>shard</strong>. Each shard is a full-fledged, independent Lucene index. The basic idea is that Elasticsearch will store each shard on a separate data node to increase the scalability. We have to mention how many shards we want at the time of index creation. Then, Elasticsearch will take care of it automatically. During document search, Elasticsearch will aggregate all documents from all available shards to consolidate the results so as to fulfill a user search request. It is totally transparent to the user. So the concept is that index can be divided into multiple shards and each shard can be hosted on each data node. The placement of shards will be taken care of by Elasticsearch itself. If we don't specify the number of shards in the index creation URL, Elasticsearch will create five shards per index by default.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Replica shard</h1>
                </header>
            
            <article>
                
<p>We have created index with one replica. It means Elasticsearch will create one copy (replica) of each shard and place each replica on separate data node other than the shard from which it is copied. So, now there are two shards, primary shard (the original shard) and replica shard (the copy of the primary shard). During a high volume of search activity, Elasticsearch can provide query results either from primary shards or from replica shards placed on different data nodes. This is how Elasticsearch increases the query throughput because each search query may go to different data nodes.</p>
<p>In the summary, both, primary shards and replica shards provide horizontal scalability and throughput. It scales out your search volume/throughput since searches can be executed on all replicas in parallel.</p>
<p>Elasticsearch is a distributed data store. It means data can be divided into multiple data nodes. For example, assume if we have just one data node and we keep on ingesting and indexing documents on the same data node, it may possible that after reaching out the hardware capacity of that node, we will not be to ingest documents. Hence, in order to accommodate more documents, we have to add another data node to the existing Elasticsearch cluster. If we add another data node, Elasticsearch will re-balance the shards to the newly created data node. So now, user search queries can be accommodated to both the data nodes. If we created one replica shard, then two replicas per shard will be created and placed on these two data nodes. Now, if one of the data nodes goes down, then still user search queries will be executed using just one data node.</p>
<p>This picture shows how user search queries are executed from both the data nodes:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b8d46b2c-601b-444e-90d9-3b2912de3bd5.png" style="width:32.08em;height:29.17em;"/></div>
<p>The following picture shows that even if data nodes <strong>A</strong> goes down, still, user queries are executed from data node <strong>B</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/29ff9261-0355-4bbb-8702-cc2758b6827d.png" style="width:32.83em;height:30.25em;"/></div>
<p>Let's verify the newly created index:</p>
<pre><strong>curl -XGET 'localhost:9200/_cat/indices?v&amp;amp;amp;amp;pretty'</strong></pre>
<p>We will get the following response:</p>
<pre><strong>health status index uuid pri rep docs.count docs.deleted store.size pri.store.size</strong><br/><strong>yellow open my_index 2MXqDHedSUqoV8Zyo0l-Lw 5 1 1 0 6.9kb 6.9kb</strong></pre>
<p>Let's understand the response:</p>
<ul>
<li><strong>Health:</strong> This means the overall cluster health is yellow. There are three statuses: green, yellow, and red. The status <kbd>Green</kbd>" means the cluster is fully functional and everything looks good. The status "Yellow" means cluster is fully available but some of the replicas are not allocated yet. In our example, since we are using just one node and 5 shards and 1 replica each, Elasticsearch will not allocate all the replicas of all the shards on just one data node. The cluster status "Red" means cluster is partially available and some datasets are not available. The reason may be that the data node is down or something else.</li>
<li><strong>Status</strong>: <kbd>Open</kbd>. It means the cluster is open for business.</li>
<li><strong>Index</strong> : Index name. In our example, the index name is <kbd>my_index</kbd>.</li>
<li><strong>Uuid</strong> : This is unique index ID.</li>
<li><strong>Pri</strong> : Number of primary shards.</li>
<li><strong>Rep</strong> : Number of replica shards.</li>
<li><strong>docs.count</strong> : Total number of documents in an index.</li>
<li><strong>docs.deleted</strong> : Total number of documents deleted so far from an index.</li>
<li><strong>store.size</strong> : The store size taken by primary and replica shards.</li>
<li><strong>pri.store.size</strong> : The store size taken only by primary shards.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ingest documents into index</h1>
                </header>
            
            <article>
                
<p>The following <kbd>curl</kbd> command can be used to ingest a single document in the <kbd>my_index</kbd> index:</p>
<pre>curl -X PUT 'localhost:9200/my_index/customer/1' -H 'Content-Type: application/json' -d '<br/>{<br/>"name": "Angela Martin",<br/>"birthdate": "1997-11-02",<br/>"street": "63542 Times Square",<br/>"city": "New York",<br/>"state": "NY",<br/>"zip": "10036",<br/>"homePhone": "212-415-8929",<br/>"cellPhone": "212-171-8187"<br/>}'</pre>
<p>In the previous command, we use a type called <kbd>customer</kbd>, which is a logical partition of an index. In a RDBMS analogy, a type is like a table in Elasticsearch.</p>
<p>Also, we used the number <kbd>1</kbd> after the type customer. It is an ID of a customer. If we omit it, then Elasticsearch will generate an arbitrary ID for the document.</p>
<p>We have multiple documents to be inserted into the <kbd>my_index</kbd> index. Inserting documents one by one in the command line is very tedious and time consuming. Hence, we can include all the documents in a file and do a bulk insert into <kbd>my_index</kbd>.</p>
<p>Create a <kbd>sample.json</kbd> file and include all the three documents:</p>
<pre>{"index":{"_id":"1"}}<br/><br/>{"name": "Sam Taylor","birthdate": "1995-08-11","address":{"street": "155 rabbit Street","city": "San Francisco","state": "CA","zip": "94107"},"contactPhone":[{"type": "home","number": "510-415-8929"},{"type": "cell","number": "408-171-8187"}]}<br/><br/>{"index":{"_id":"2"}}<br/>{"name": "Dan Lee","birthdate": "1970-01-25","address":{"street": "76336 1st Street","city": "Los Angeles","state": "CA","zip": "90010"},"contactPhone":[{"type": "home","number": "323-892-5363"},{"type": "cell","number": "213-978-1320"}]}<br/><br/>{"index":{"_id":"3"}}<br/><br/>{"name": "Angela Martin","birthdate": "1997-11-02","address":{"street": "63542 Times Square","city": "New York","state": "NY","zip": "10036"},"contactPhone":[{"type": "home","number": "212-415-8929"},{"type": "cell","number": "212-171-8187"}]}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bulk Insert</h1>
                </header>
            
            <article>
                
<p>Let's ingest all the documents in the file <kbd>sample.json</kbd> at once using the following command:</p>
<pre><strong>curl -H 'Content-Type: application/json' -XPUT 'localhost:9200/my_index/customer/_bulk?pretty&amp;amp;amp;amp;refresh' --data-binary "@sample.json"</strong></pre>
<p>Let's verify all the records using our favorite browser. It will show all the three records:</p>
<pre><strong>http://localhost:9200/my_index/_search</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Document search</h1>
                </header>
            
            <article>
                
<p>Since we have documents in our <kbd>my_index</kbd> index, we can search these documents:</p>
<p>Find out a document where <kbd>city = " Los Angeles?</kbd> and query is as follows:</p>
<pre>curl -XGET 'http://localhost:9200/my_index2/_search?pretty' -H 'Content-Type: application/json' -d' {<br/>"query": {<br/>"match": {<br/>"city": "Los Angeles" }<br/>}<br/>}'</pre>
<p>Response:</p>
<pre>{<br/>"took" : 3,<br/>"timed_out" : false,<br/>"_shards" : {<br/>"total" : 3,"successful" : 3,<br/>"skipped" : 0,<br/>"failed" : 0<br/>},<br/>"hits" : {<br/>"total" : 1,<br/>"max_score" : 1.3862944,<br/>"hits" : [<br/>{<br/>"_index" : "my_index",<br/>"_type" : "customer",<br/>"_id" : "3",<br/>"_score" : 1.3862944,<br/>"_source" : {<br/>"name" : "Dan Lee",<br/>"birthdate" : "1970-01-25",<br/>"street" : "76336 1st Street",<br/><strong>"city" : "Los Angeles",<br/></strong>"state" : "ca",<br/>"postalCode" : "90010",<br/>"homePhone" : "323-892-5363",<br/>"cellPhone" : "213-978-1320"<br/>}<br/>}<br/>]<br/>}<br/>}</pre>
<p>If we analyze the response, we can see that the source section gives a back the document we were looking for. The document is in the index <kbd>my_index</kbd>, <kbd>"_type" : "customer"</kbd>, <kbd>"_id" : "3"</kbd>. Elasticsearch searches all <kbd>three _shards</kbd> successfully.</p>
<p>Under the <kbd>hits</kbd> section, there is a field called <kbd>_score</kbd>. Elasticsearch calculates the relevance frequency of each field within a document and stores it in index. It is called the weight of the document. This weight is calculated based on four important factors: term frequency, inverse frequency, document frequency, and field length frequency. This brings up another question, <em>How does Elasticsearch index a document?</em></p>
<p>For example, we have the following four documents to index in Elasticsearch:</p>
<ul>
<li>I love Elasticsearch</li>
<li>Elasticsearch is a document store</li>
<li>HBase is key value data store</li>
<li>I love HBase</li>
</ul>
<table class="table">
<tbody>
<tr>
<td>
<p><strong>Term</strong></p>
</td>
<td>
<p><strong>Frequency</strong></p>
</td>
<td>
<p><strong>Document No.</strong></p>
</td>
</tr>
<tr>
<td>
<p>a</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>2</p>
</td>
</tr>
<tr>
<td>
<p>index</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>2</p>
</td>
</tr>
<tr>
<td>
<p>Elasticsearch</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>1,2</p>
</td>
</tr>
<tr>
<td>
<p>HBase</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p>I</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>1,4</p>
</td>
</tr>
<tr>
<td>
<p>is</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>2,3</p>
</td>
</tr>
<tr>
<td>
<p>Key</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>3</p>
</td>
</tr>
<tr>
<td>
<p>love</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>1,4</p>
</td>
</tr>
<tr>
<td>
<p>store</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>2,3</p>
</td>
</tr>
<tr>
<td>
<p>value</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>3</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>When we ingest three documents in Elasticsearch, an Inverted Index is created, like the following.</p>
<p>Now, if we want to query term Elasticsearch, then only two documents need to be searched: 1 and 2. If we run another query to find <em>love Elasticsearch</em>, then three documents need to be searched (documents 1,2, and 4) before sending the results from only the first document.</p>
<p>Also, there is one more important concept we need to understand.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Meta fields</h1>
                </header>
            
            <article>
                
<p>When we ingest a document into index, Elasticsearch adds a few meta fields to each index document. The following is the list of meta fields with reference to our sample <kbd>my_index</kbd>:</p>
<ul>
<li><kbd>_index</kbd>: Name of the index. <kbd>my_index</kbd>.</li>
<li><kbd>_type</kbd>: Mapping type. "customer" (deprecated in version 6.0).</li>
<li><kbd>_uid</kbd>: <kbd>_type + _id</kbd> (deprecated in version 6.0).</li>
<li><kbd>_id</kbd>: <kbd>document_id</kbd> (1).</li>
<li><kbd>_all</kbd>: This concatenates all the fields of an index into a searchable string (deprecated in version 6.0).</li>
<li><kbd>_ttl</kbd>: Life a document before it can be automatically deleted.</li>
<li><kbd>_timestamp</kbd>: Provides a timestamp for a document.</li>
<li><kbd>_source</kbd>: This is an actual document, which is automatically indexed by default.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mapping</h1>
                </header>
            
            <article>
                
<p>In RDBMS analogy, mapping means defining a table schema. We always define a table structure, that is, column data types. In Elasticsearch, we also need to define the data type for each field. But then comes another question. Why did <span>we </span>not define it before when we ingested three documents into the <kbd>my_index</kbd> index? The answer is simple. Elasticsearch doesn't care. It is claimed that <em>Elasticsearch is a schema-less data model</em>.</p>
<p>If we don't define a mapping, Elasticsearch dynamically creates a mapping for us by defining all fields as text. Elasticsearch is intelligent enough to find out date fields to assign the <kbd>date</kbd> data type to them.</p>
<p>Let's find the existing dynamic mapping of index <kbd>my_index</kbd>:</p>
<pre><strong>curl -XGET 'localhost:9200/my_index2/_mapping/?pretty'</strong></pre>
<p>Response:</p>
<pre>{<br/>"my_index" : {<br/>"mappings" : {<br/>customer" : {<br/>"properties" : {<br/>"<strong>birthdate</strong>" : {<br/>"type" : "<strong>date</strong>"<br/>},<br/>"<strong>cellPhone</strong>" : {<br/>"type" : "<strong>text</strong>",<br/>"fields" : {<br/>"keyword" : {<br/>"type" : "keyword",<br/>"ignore_above" : 256<br/>}<br/>}<br/>},<br/>"<strong>city</strong>" : {<br/>"type" : "<strong>text</strong>",<br/>"fields" : {<br/>"keyword" : {<br/>"type" : "keyword",<br/>"ignore_above" : 256<br/>}<br/>}<br/>},<br/>"<strong>homePhone</strong>" : {<br/>"type" : "<strong>text</strong>",<br/>"fields" : {<br/>"keyword" : {<br/>"type" : "keyword",<br/>"ignore_above" : 256<br/>}<br/>}<br/>},<br/>"<strong>name</strong>" : {<br/>"type" : "<strong>text</strong>",<br/>"fields" : {<br/>"keyword" : {<br/>type" : "keyword",<br/>"ignore_above" : 256<br/>}<br/>}<br/>},<br/>"<strong>postalCode</strong>" : {<br/>"type" : "<strong>text</strong>",<br/>"fields" : {<br/>"keyword" : {<br/>"type" : "keyword",<br/>"ignore_above" : 256<br/>}<br/>}<br/>},<br/>"<strong>state</strong>" : {<br/>"type" : "<strong>text</strong>",<br/>"fields" : {<br/>"keyword" : {<br/>"type" : "keyword",<br/>"ignore_above" : 256<br/>}<br/>}<br/>},<br/><strong>"street" : {</strong><span><br/></span>"type" : "<strong>text</strong>",<br/>"fields" : {<br/>"keyword" : {<br/>"type" : "keyword",<br/>"ignore_above" : 256<br/>}<br/>}<br/>}<br/>}<br/>}<br/>}<br/>}<br/>}</pre>
<p>Elasticsearch supports two mapping types as follows:</p>
<ul>
<li>Static mapping</li>
<li>Dynamic mapping</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Static mapping</h1>
                </header>
            
            <article>
                
<p>In static mapping, we always know our data and we define the appropriate data type for each field. Static mapping has to be defined at the time of index creation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dynamic mapping</h1>
                </header>
            
            <article>
                
<p>We have already used dynamic mapping for our documents in our example. Basically, we did not define any data type for any field. But when we ingested documents using <kbd>_Bulk</kbd> load, Elasticsearch transparently defined <kbd>text</kbd> and <kbd>date</kbd> data types appropriately for each field. Elasticsearch intelligently found our <kbd>Birthdate</kbd> as a date field and assigned the <kbd>date</kbd> data type to it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Elasticsearch-supported data types</h1>
                </header>
            
            <article>
                
<p>The following spreadsheet summarizes the available data types in Elasticsearch:</p>
<table class="table">
<tbody>
<tr>
<td>
<p><strong>Common</strong></p>
</td>
<td>
<p><strong>Complex</strong></p>
</td>
<td>
<p><strong>Geo</strong></p>
</td>
<td>
<p><strong>Specialized</strong></p>
</td>
</tr>
<tr>
<td>
<p>String</p>
</td>
<td>
<p>Array</p>
</td>
<td>
<p>Geo_Point</p>
</td>
<td>
<p><kbd>ip</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Keyword</p>
</td>
<td>
<p>Object (single Json)</p>
</td>
<td>
<p>Geo_Shape</p>
</td>
<td>
<p><kbd>completion</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Date</p>
</td>
<td>
<p>Nested (Json array)</p>
</td>
<td/>
<td>
<p><kbd>token_count</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Long</p>
</td>
<td/>
<td/>
<td>
<p><kbd>join</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Short</p>
</td>
<td/>
<td/>
<td>
<p><kbd>percolator</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Byte</p>
</td>
<td/>
<td/>
<td>
<p><kbd>murmur3</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Double</p>
</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>
<p>Float</p>
</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>
<p>Boolean</p>
</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>
<p>Binary</p>
</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>
<p>Integer_range</p>
</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>
<p>Float_range</p>
</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>
<p>Long_range</p>
</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>
<p>Double_range</p>
</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>
<p>Date_range</p>
</td>
<td/>
<td/>
<td/>
</tr>
</tbody>
</table>
<p>Most of the data types need no explanation. But the following are a few explanations for specific data types:</p>
<ul>
<li><strong>Geo-Point</strong>:<strong> </strong>You can define latitude and longitude points here</li>
<li><strong>Geo-Shape</strong>:<strong> </strong>This is for defining shapes</li>
<li><strong>Completion</strong>:<strong> </strong>This data type is for defining auto completion of words.</li>
<li><strong>Join</strong>: To define parent/child relationships</li>
<li><strong>Percolator</strong>:<strong> </strong>This is for query-dsl</li>
<li><strong>Murmur3</strong>:<strong> </strong>During index time, it is for calculations hash value and store it into index </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mapping example</h1>
                </header>
            
            <article>
                
<p>Let's re-create another index, <kbd>second_index</kbd>, which is similar to our <kbd>first_index</kbd> with static mapping, where we will define the data type of each field separately:</p>
<pre>curl -XPUT localhost:9200/second_index -d '{<br/>"mappings": {<br/>"customer": {<br/>"_source": {<br/>"enabled": false<br/>},<br/>"properties": {<br/>"name": {"type": "string", "store": true},<br/>"birthdate": {"type": "string"},<br/>"street": {"type": "string"},<br/>"city": {"type": "date"},<br/>"state": {"type": "string", "index": "no", "store": true}<br/>"zip": {"type": "string", "index": "no", "store": true}}<br/>}<br/>}<br/>}</pre>
<p>Let's understand the preceding mapping. We disable the <kbd>_source</kbd> field for the customer type. It means, we get rid of the default behavior, where Elasticsearch stores and indexes the document by default. Now, since we have disabled it, we will deal with each and every field separately to decide whether that field should be indexed stored or both.</p>
<p>So, in the preceding example, we want to store only three fields, <kbd>name</kbd>, <kbd>state</kbd> and <kbd>zip</kbd>. Also, we don't want to index the <kbd>state</kbd> and <kbd>zip</kbd> fields. It means <kbd>state</kbd> and <kbd>zip</kbd> fields are not searchable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzer</h1>
                </header>
            
            <article>
                
<p>We have already learned about an inverted index. We know that Elasticsearch stores a document into an inverted index. This transformation is known as analysis. This is required for a successful response of the index search query.</p>
<p>Also, many of the times, we need to use some kind of transformation before sending that document to Elasticsearch index. We may need to change the document to lowercase, stripping off HTML tags if any from the document, remove white space between two words, tokenize the fields based on delimiters, and so on.</p>
<p>Elasticsearch offers the following built-in analyzers:</p>
<ul>
<li><strong>Standard analyzer</strong>:<strong> </strong>It is a default analyzer. This uses standard tokenizer to divide text. It normalizes tokens, lowercases tokens, and also removes unwanted tokens.</li>
<li><strong>Simple analyzer</strong>:<strong> </strong>This analyzer is composed of lowercase tokenizer.</li>
<li><strong>Whitespace analyzer</strong>: This uses the whitespace tokenizer to divide text at spaces.</li>
<li><strong>Language analyzers</strong>:<strong> </strong>Elasticsearch provides many language-specific analyzers such as English, and so on.</li>
<li><strong>Fingerprint analyzer</strong>:<strong> </strong>The fingerprint analyzer is a specialist analyzer. It creates a fingerprint, which can be used for duplicate detection.</li>
<li><strong>Pattern analyzer</strong>:<strong> </strong>The pattern analyzer uses a regular expression to split the text into terms.</li>
<li><strong>Stop analyzer</strong>:<strong> </strong>This uses letter tokenizer to divide text. It removes stop words from token streams. for example, all stop words like a, an, the, is and so on.</li>
<li><strong>Keyword analyzer</strong>:<strong> </strong>This analyzer tokenizes an entire stream as a single token. It can be used for zip code.</li>
<li><strong>Character filter</strong>: Prepare a string before it is tokenize. Example: remove html tags.</li>
<li><strong>Tokenizer</strong><span>: MUST have a single tokenizer. It's used to break up the string into individual terms or </span>tokens.</li>
<li><strong>Token filter</strong>: <span>Change, add or remove tokens. Stemmer is a token filter, it is used to get base of w</span>ord, for example: learned, learning =&gt; learn</li>
</ul>
<p>Example of atandard analyzer:</p>
<pre><strong>curl -XPOST 'localhost:9200/_analyze?pretty' -H 'Content-Type: application/json' -d'</strong><br/><strong>{</strong><br/><strong>"analyzer": "standard",</strong><br/><strong>"text": " 1. Today it's a Sunny-day, very Bright."</strong><br/><strong>}'</strong></pre>
<p>Response:</p>
<pre><strong>[today, it's , a, sunny, day, very, bright ]</strong></pre>
<p>Example of simple analyzer:</p>
<pre class="mce-root">curl -XPOST 'localhost:9200/_analyze?pretty' -H 'Content-Type: application/json' -d'<br/>{<br/>"analyzer": "simple",<br/>"text": " 1. Today it's a Sunny-day, very Bright."<br/>}'</pre>
<p>Response:</p>
<pre>[today, it's , a, sunny, day, very, bright ]</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Elasticsearch stack components</h1>
                </header>
            
            <article>
                
<p>The Elasticsearch stack consists of following</p>
<ul>
<li>Beats</li>
<li>Logstash</li>
<li>Elasticsearch</li>
<li>Kibana</li>
</ul>
<p>Let's study them in brief.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Beats</h1>
                </header>
            
            <article>
                
<p>Please refer to the following URL to know more about beats: <a href="https://www.elastic.co/products/beats">https://www.elastic.co/products/beats</a>.</p>
<p>Beats are lightweight data shippers. Beats are installed on to servers as agents. Their main function is collect the data and send it to either Logstash or Elasticsearch. We can configure beats to send data to Kafka topics also.</p>
<p>There are multiple beats. Each beat is meant for collecting specific datasets and metrics. The following are various types of Beats:</p>
<ul>
<li><strong>Filebeat</strong>:<strong> </strong>For collection of log files. They simplify the collection, parsing, and visualization of common log formats down to a single command. Filebeat comes with internal modules (auditd, Apache, nginx, system, and MySQL).</li>
<li><strong>Metricbeat</strong>:<strong> </strong><span>For collection of metrics. They collect metrics from any systems and services, for example, memory, COU, and disk. Metricbeat is a lightweight way to send system and service statistics.</span></li>
<li><strong>Packetbeat</strong>: This is for collection of network data. Packetbeat is a lightweight network packet analyzer that sends data to Logstash or Elasticsearch.</li>
<li><strong>Winlogbeat</strong>:<strong> </strong>For collection of Windows event data. Winlogbeat live-streams Windows event logs to Elasticsearch and Logstash.</li>
<li><strong>Auditbeat</strong>:<strong> </strong>For collection of audit data. Auditbeat collects audit framework data.</li>
<li><strong>Heartbeat</strong>:<strong> </strong>For collection of uptime monitoring data. Heartbeat ships this information and response time to Elasticsearch.</li>
</ul>
<p>Installation of Filebeat:</p>
<pre><strong>$wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.1.2-x86_64.rpm</strong><br/><strong>$ sudo rpm --install filebeat-6.1.2-x86_64.rpm</strong><br/><strong>sudo /bin/systemctl daemon-reload</strong><br/><strong>sudo /bin/systemctl enable filebeat.service</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logstash</h1>
                </header>
            
            <article>
                
<p>Logstash is a lightweight, open source data processing pipeline. It allows collecting data from a wide variety of sources, transforming it on the fly, and sending it to any desired destination.</p>
<p>It is most often used as a data pipeline for Elasticsearch, a popular analytics and search engine. Logstash is a popular choice for loading data into Elasticsearch because of its tight integration, powerful log processing capabilities, and over 200 prebuilt open source plugins that can help you get your data indexed the way you want it.</p>
<p>The following is a structure of <kbd>Logstash.conf</kbd>:</p>
<pre>input {<br/>...<br/>}<br/>filter {<br/>...<br/>}<br/>output {<br/>..<br/>}</pre>
<p>Installation of Logstash:</p>
<pre><strong>$ wget https://artifacts.elastic.co/downloads/logstash/logstash-6.1.2.rpm</strong><br/><strong>$ sudo rpm --install logstash-6.1.2.rpm</strong><br/><strong>$ sudo /bin/systemctl daemon-reload</strong><br/><strong>$ sudo systemctl start logstash.service</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kibana</h1>
                </header>
            
            <article>
                
<p>Kibana is an open-source data visualization and exploration tool used for log and time series analytics, application monitoring, and operational intelligence use cases. Kibana offers tight integration with Elasticsearch, a popular analytics and search engine, which makes Kibana the default choice for visualizing data stored in Elasticsearch. Kibana is also popular due to its powerful and easy-to-use features such as histograms, line graphs, pie charts, heat maps, and built-in geospatial support<strong>.</strong></p>
<p>Installation of Kibana:</p>
<pre><strong>$wget https://artifacts.elastic.co/downloads/kibana/kibana-6.1.2-x86_64.rpm</strong><br/><strong>$ sudo rpm --install kibana-6.1.2-x86_64.rpm</strong><br/><strong>sudo /bin/systemctl daemon-reload</strong><br/><strong>sudo /bin/systemctl enable kibana.service</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Use case</h1>
                </header>
            
            <article>
                
<p>Let's assume that we have an application deployed on an application server. That application is logging on to an access log. Then how <span>can </span>we analyze this access log using a dashboard? We would like to create a real-time visualization of the following info:</p>
<ul>
<li>Number of various response codes</li>
<li>Total number of responses</li>
<li>List of IPs</li>
</ul>
<p>Proposed technology stack:</p>
<ul>
<li><strong>Filebeat</strong>: To read access log and write to Kafka topic</li>
<li><strong>Kafka:</strong> Message queues and o buffer message</li>
<li><strong>Logstash:</strong> To pull messages from Kafka and write to Elasticsearch index</li>
<li><strong>Elasticsearch</strong>: For indexing messages</li>
<li><strong>Kibana</strong>: Dashboard visualization</li>
</ul>
<p>In order to solve this problem, we install filebeat on Appserver. Filebeat will read each line from the access log and write to the kafka topic in real time. Messages will be buffered in Kafka. Logstash will pull messages from the Kafka topic and write to Elasticsearch.</p>
<p>Kibana will create real-time streaming dashboard by reading messages from Elasticsearch index. The following is the architecture of our use case:</p>
<div class="CDPAlignCenter CDPAlign"><strong><img src="assets/70bfafda-6da2-425d-bcbd-39bfe676906d.png"/></strong></div>
<p>Here is the step-by-step code sample, <kbd>Acccss.log</kbd>:</p>
<pre>127.0.0.1 - - [21/Mar/2017:13:52:29 -0400] "GET /web-portal/performance/js/common-functions.js HTTP/1.1" 200 3558<br/>127.0.0.1 - - [21/Mar/2017:13:52:30 -0400] "GET /web-portal/performance/js/sitespeed-functions.js HTTP/1.1" 200 13068<br/>127.0.0.1 - - [21/Mar/2017:13:52:34 -0400] "GET /web-portal/img/app2-icon-dark.png HTTP/1.1" 200 4939<br/>127.0.0.1 - - [21/Mar/2017:13:52:43 -0400] "GET /web-search-service/service/performanceTest/release/list HTTP/1.1" 200 186<br/>127.0.0.1 - - [21/Mar/2017:13:52:44 -0400] "GET /web-portal/performance/fonts/opan-sans/OpenSans-Light-webfont.woff HTTP/1.1" 200 22248<br/>127.0.0.1 - - [21/Mar/2017:13:52:44 -0400] "GET /web-portal/performance/img/icon/tile-actions.png HTTP/1.1" 200 100<br/>127.0.0.1 - - [21/Mar/2017:13:52:44 -0400] "GET /web-portal/performance/fonts/fontawesome/fontawesome-webfont.woff?v=4.0.3 HTTP/1.1" 200 44432</pre>
<p>The following is the complete <kbd>Filebeat.ymal</kbd>:</p>
<p>In the Kafka output section, we have mentioned Kafka broker details. <kbd>output.kafka</kbd>:</p>
<pre># initial brokers for reading cluster metadata<br/>hosts: ["localhost:6667"]</pre>
<p>The following is the complete <kbd>Filebeat.ymal</kbd>:</p>
<pre>###################### Filebeat Configuration Example #########################<br/># This file is an example configuration file highlighting only the most common<br/># options. The filebeat.reference.yml file from the same directory contains all the<br/># supported options with more comments. You can use it as a reference.<br/>#<br/># You can find the full configuration reference here:<br/># https://www.elastic.co/guide/en/beats/filebeat/index.html<br/># For more available modules and options, please see the filebeat.reference.yml sample<br/># configuration file.<br/>#======================== Filebeat prospectors========================<br/>filebeat.prospectors:<br/># Each - is a prospector. Most options can be set at the prospector level, so<br/># you can use different prospectors for various configurations.<br/># Below are the prospector specific configurations.<br/>- type: log<br/># Change to true to enable this prospector configuration.<br/>enabled: true<br/># Paths that should be crawled and fetched. Glob based paths.<br/>paths:<br/>- /var/log/myapp/*.log<br/>#- c:programdataelasticsearchlogs*<br/>#json.keys_under_root: true<br/>#json.add_error_key: true<br/># Exclude lines. A list of regular expressions to match. It drops the lines that are<br/># matching any regular expression from the list.<br/>#exclude_lines: ['^DBG']<br/># Include lines. A list of regular expressions to match. It exports the lines that are<br/># matching any regular expression from the list.<br/>#include_lines: ['^ERR', '^WARN']<br/># Exclude files. A list of regular expressions to match. Filebeat drops the files that<br/># are matching any regular expression from the list. By default, no files are dropped.<br/>#exclude_files: ['.gz$']<br/># Optional additional fields. These fields can be freely picked<br/># to add additional information to the crawled log files for filtering<br/>#fields:<br/># level: debug<br/># review: 1<br/>fields:<br/>app: myapp<br/>env: dev<br/>dc: gce<br/>### Multiline options<br/># Mutiline can be used for log messages spanning multiple lines. This is common<br/># for Java Stack Traces or C-Line Continuation<br/># The regexp Pattern that has to be matched. The example pattern matches all lines starting with [#multiline.pattern: ^[<br/># Defines if the pattern set under pattern should be negated or not. Default is false.<br/>#multiline.negate: false<br/># Match can be set to "after" or "before". It is used to define if lines should be append to a pattern<br/># that was (not) matched before or after or as long as a pattern is not matched based on negate.<br/># Note: After is the equivalent to previous and before is the equivalent to to next in Logstash<br/>#multiline.match: after<br/>#============================= Filebeat modules ===============================<br/>filebeat.config.modules:<br/># Glob pattern for configuration loading<br/>path: ${path.config}/modules.d/*.yml<br/># Set to true to enable config reloading<br/>reload.enabled: false<br/># Period on which files under path should be checked for changes<br/>#reload.period: 10s<br/>#==================== Elasticsearch template setting ==========================<br/>setup.template.settings:<br/>index.number_of_shards: 3<br/>#index.codec: best_compression<br/>#_source.enabled: false<br/>#================================ General =====================================<br/># The name of the shipper that publishes the network data. It can be used to group<br/># all the transactions sent by a single shipper in the web interface.<br/>#name:<br/># The tags of the shipper are included in their own field with each<br/># transaction published.<br/>#tags: ["service-X", "web-tier"]<br/># Optional fields that you can specify to add additional information to the<br/># output.<br/>#fields:<br/># env: staging<br/>#============================== Dashboards =====================================<br/># These settings control loading the sample dashboards to the Kibana index. Loading<br/># the dashboards is disabled by default and can be enabled either by setting the<br/># options here, or by using the `-setup` CLI flag or the `setup` command.<br/>#setup.dashboards.enabled: false<br/># The URL from where to download the dashboards archive. By default this URL<br/># has a value which is computed based on the Beat name and version. For released<br/># versions, this URL points to the dashboard archive on the artifacts.elastic.co<br/># website.<br/>#setup.dashboards.url:<br/>#============================== Kibana =====================================<br/># Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API.<br/># This requires a Kibana endpoint configuration.<br/>setup.kibana:<br/># Kibana Host<br/># Scheme and port can be left out and will be set to the default (http and 5601)<br/># In case you specify and additional path, the scheme is required: http://localhost:5601/path<br/># IPv6 addresses should always be defined as: https://[2001:db8::1]:5601<br/>#host: "localhost:5601"<br/>#============================= Elastic Cloud ==================================<br/># These settings simplify using filebeat with the Elastic Cloud (https://cloud.elastic.co/).<br/># The cloud.id setting overwrites the `output.elasticsearch.hosts` and<br/># `setup.kibana.host` options.<br/># You can find the `cloud.id` in the Elastic Cloud web UI.<br/>#cloud.id:<br/># The cloud.auth setting overwrites the `output.elasticsearch.username` and<br/># `output.elasticsearch.password` settings. The format is `&lt;user&gt;:&lt;pass&gt;`.<br/>#cloud.auth:<br/>#================================ Outputs =====================================<br/># Configure what output to use when sending the data collected by the beat.<br/>#-----------------------------------Kafka Output-------------------------------<br/><strong>output.kafka:<br/></strong><strong># initial brokers for reading cluster metadata<br/></strong><strong>hosts: ["localhost:6667"]<br/></strong># message topic selection + partitioning<br/>topic: logs-topic<br/>partition.round_robin:<br/>reachable_only: false<br/>required_acks: 1<br/>compression: gzip<br/>max_message_bytes: 1000000<br/>#-------------------------- Elasticsearch output ------------------------------<br/>#output.elasticsearch:<br/># Array of hosts to connect to.<br/>#hosts: ["localhost:9200"]<br/># Optional protocol and basic auth credentials.<br/>#protocol: "https"<br/>#username: "elastic"<br/>#password: "changeme"<br/>#----------------------------- Logstash output --------------------------------#output.logstash:<br/># The Logstash hosts<br/>#hosts: ["localhost:5044"]<br/># Optional SSL. By default is off.<br/># List of root certificates for HTTPS server verifications<br/>#ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]<br/># Certificate for SSL client authentication<br/>#ssl.certificate: "/etc/pki/client/cert.pem"<br/># Client Certificate Key<br/>#ssl.key: "/etc/pki/client/cert.key"<br/>#================================ Logging =====================================<br/># Sets log level. The default log level is info.<br/># Available log levels are: error, warning, info, debug<br/>logging.level: debug<br/># At debug level, you can selectively enable logging only for some components.<br/># To enable all selectors use ["*"]. Examples of other selectors are "beat",<br/># "publish", "service".<br/>#logging.selectors: ["*"]<br/>#============================== Xpack Monitoring ===============================<br/># filebeat can export internal metrics to a central Elasticsearch monitoring<br/># cluster. This requires xpack monitoring to be enabled in Elasticsearch. The<br/># reporting is disabled by default.<br/># Set to true to enable the monitoring reporter.<br/>#xpack.monitoring.enabled: false<br/># Uncomment to send the metrics to Elasticsearch. Most settings from the<br/># Elasticsearch output are accepted here as well. Any setting that is not set is<br/># automatically inherited from the Elasticsearch output configuration, so if you<br/># have the Elasticsearch output configured, you can simply uncomment the<br/># the following line.<br/>#xpack.monitoring.elasticsearch:</pre>
<p>We have to create a <kbd>logs-topic</kbd> <span>topic </span>in Kafka before we start ingesting messages into it. It is assumed that we have already installed Kafka on the server. Please refer to <a href="b17d8b55-1716-471b-aa8c-daf6d590172e.xhtml">Chapter 2</a>, <em>Hadoop Life Cycle Management</em> to read more about Kafka.</p>
<p>Create logs-topic:</p>
<pre>bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic logs-topic</pre>
<p>The following is the <kbd>Logstash.conf</kbd> (to read messages from Kafka and push them to Elasticseach):</p>
<pre>input<br/>{<br/>kafka<br/>{<br/>bootstrap_servers =&gt; "127.0.0.1:6667"<br/>group_id =&gt; "logstash_logs"<br/>topics =&gt; ["logs-topic"]<br/>consumer_threads =&gt; 1<br/>type =&gt; "kafka_logs"<br/>}<br/>}<br/>filter {<br/>if [type] == "kafka_logs"<br/>{<br/>json {<br/>source =&gt; "message"<br/>}<br/>grok {<br/>match =&gt; { "message" =&gt; "%{IP:ip} - - [%{GREEDYDATA:log_timestamp}] %{GREEDYDATA:middle} %{NUMBER:status} %{NUMBER:bytes}" }<br/>}<br/>mutate {<br/>add_field =&gt; {<br/>"App" =&gt; "%{[fields][app]}"<br/>}<br/>}<br/>}<br/>}<br/>output {<br/>if [App] == "myapp"<br/>{<br/>elasticsearch<br/>{<br/>action =&gt; "index"<br/>codec =&gt; "plain"<br/>hosts =&gt; ["http://127.0.0.1:9200"]<br/>index =&gt; "log_index-%{+YYYY-MM-dd}"<br/>}<br/>}<br/>}</pre>
<p>In the Kafka section, we've mentioned the following things:</p>
<pre>Kafka bootstrap_servers =&gt; "127.0.0.1:6667"<br/>Kafka topics =&gt; ["logs-topic"]</pre>
<p>In the filter section<strong>,</strong> we are converting each message into JSON format. After that, we are parsing each message and dividing it into multiple fields such as <kbd>ip</kbd>, <kbd>timestamp</kbd>, and <kbd>status</kbd>. Also, we add the application name <kbd>myapp</kbd> field to each message.</p>
<p>In the output section, we are writing each message to Elasticsearch. The index name is <kbd>log_index-YYYY-MM-dd</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you looked at the basic concepts and components of an Elasticsearch cluster.</p>
<p>After this, we discussed how Elasticsearch indexes a document using inverted index. We also discussed mapping and analysis techniques. We learned how we can denormalize an event before ingesting into Elasticsearch. We discussed how Elasticsearch uses horizontal scalability and throughput. After learning about Elasticstack components such as Beats, Logstash, and Kibana, we handled a live use case, where we demonstrated how access log events can be ingested into Kafka using Filebeat. We developed a code to pull messages from Kafka and ingest into Elasticsearch using Logstash. At the end, we learned data visualization using Kibana.</p>
<p><span>In the next chapter, we will see how to build analytics to design data visualization solutions that drive business decisions.</span></p>


            </article>

            
        </section>
    </body></html>