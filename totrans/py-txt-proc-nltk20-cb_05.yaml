- en: Chapter 5. Extracting Chunks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 提取块
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: Chunking and chinking with regular expressions
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正则表达式进行块处理和切分
- en: Merging and splitting chunks with regular expressions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正则表达式合并和拆分块
- en: Expanding and removing chunks with regular expressions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正则表达式扩展和删除块
- en: Partial parsing with regular expressions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正则表达式进行部分解析
- en: Training a tagger-based chunker
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练基于标签器的块提取器
- en: Classification-based chunking
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于分类的块处理
- en: Extracting named entities
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取命名实体
- en: Extracting proper noun chunks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取专有名词块
- en: Extracting location chunks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取地点块
- en: Training a named entity chunker
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练命名实体块提取器
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: '**Chunk extraction** or **partial parsing** is the process of extracting short
    phrases from a part-of-speech tagged sentence. This is different than full parsing,
    in that we are interested in standalone **chunks** or **phrases** instead of full
    parse trees. The idea is that meaningful phrases can be extracted from a sentence
    by simply looking for particular patterns of part-of-speech tags.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**块提取**或**部分解析**是从词性标注句子中提取短语的进程。这与完整解析不同，因为我们感兴趣的是独立的**块**或**短语**，而不是完整的解析树。想法是通过简单地寻找特定的词性标签模式，可以从句子中提取有意义的短语。'
- en: As in [Chapter 4](ch04.html "Chapter 4. Part-of-Speech Tagging"), *Part-of-Speech
    Tagging*, we will be using the **Penn Treebank corpus** for basic training and
    testing chunk extraction. We will also be using the CoNLL 2000 corpus as it has
    a simpler and more flexible format that supports multiple chunk types (refer to
    the *Creating a chunked phrase corpus* recipe in [Chapter 3](ch03.html "Chapter 3. Creating
    Custom Corpora"), *Creating Custom Corpora* for more details on the `conll2000`
    corpus and IOB tags).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如同[第4章](ch04.html "第4章。词性标注")中所述的*词性标注*，我们将使用**宾州树库语料库**进行基本训练和测试块提取。我们还将使用
    CoNLL 2000 语料库，因为它具有更简单、更灵活的格式，支持多种块类型（有关 `conll2000` 语料库和 IOB 标记的更多详细信息，请参阅[第3章](ch03.html
    "第3章。创建自定义语料库")中的*创建块短语语料库*配方）。
- en: Chunking and chinking with regular expressions
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用正则表达式进行块处理和切分
- en: Using modified regular expressions, we can define **chunk patterns**. These
    are patterns of part-of-speech tags that define what kinds of words make up a
    chunk. We can also define patterns for what kinds of words should not be in a
    chunk. These unchunked words are known as **chinks**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用修改后的正则表达式，我们可以定义**块模式**。这些是定义构成块单词类型的词性标签模式。我们还可以定义不应在块中的单词类型的模式。这些未块化的单词被称为**切分**。
- en: A `ChunkRule` specifies what to include in a chunk, while a `ChinkRule` specifies
    what to exclude from a chunk. In other words, **chunking** creates chunks, while
    **chinking** breaks up those chunks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`ChunkRule` 指定要包含在块中的内容，而 `ChinkRule` 指定要从块中排除的内容。换句话说，**块处理**创建块，而**切分**则将这些块拆分。'
- en: Getting ready
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We first need to know how to define chunk patterns. These are modified regular
    expressions designed to match sequences of part-of-speech tags. An individual
    tag is specified by surrounding angle brackets, such as `<NN>` to match a noun
    tag. Multiple tags can then be combined, as in `<DT><NN>` to match a determiner
    followed by a noun. Regular expression syntax can be used within the angle brackets
    to match individual tag patterns, so you can do `<NN.*>` to match all nouns including
    `NN` and `NNS`. You can also use regular expression syntax outside of the angle
    brackets to match patterns of tags. `<DT>?<NN.*>+` will match an optional determiner
    followed by one or more nouns. The chunk patterns are converted internally to
    regular expressions using the `tag_pattern2re_pattern()` function:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要知道如何定义块模式。这些是经过修改的正则表达式，旨在匹配词性标签序列。单个标签通过括号指定，例如 `<NN>` 以匹配名词标签。然后可以将多个标签组合起来，例如
    `<DT><NN>` 以匹配一个限定词后跟一个名词。在括号内可以使用正则表达式语法来匹配单个标签模式，因此您可以使用 `<NN.*>` 来匹配包括 `NN`
    和 `NNS` 在内的所有名词。您还可以在括号外使用正则表达式语法来匹配标签的模式。`<DT>?<NN.*>+` 将匹配一个可选的限定词后跟一个或多个名词。块模式通过
    `tag_pattern2re_pattern()` 函数内部转换为正则表达式：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You don't have to use this function to do chunking, but it might be useful or
    interesting to see how your chunk patterns convert to regular expressions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 您不必使用此功能进行块处理，但查看您的块模式如何转换为正则表达式可能是有用或有趣的。
- en: How to do it...
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The pattern for specifying a chunk is to use surrounding curly braces, such
    as `{<DT><NN>}`. To specify a chink, you flip the braces, as in `}<VB>{`. These
    rules can be combined into a **grammar** for a particular phrase type. Here''s
    a grammar for noun-phrases that combines both a chunk and a chink pattern, along
    with the result of parsing the sentence "The book has many chapters":'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The grammar tells the `RegexpParser` that there are two rules for parsing `NP`
    chunks. The first chunk pattern says that a chunk starts with a *determiner* followed
    by any kind of *noun*. Then any number of other words is allowed, until a final
    noun is found. The second pattern says that verbs should be *chinked*, thus separating
    any large chunks that contain a verb. The result is a tree with two noun-phrase
    chunks: "the book" and "many chapters".'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tagged sentences are always parsed into a `Tree` (found in the `nltk.tree` module).
    The top node of the `Tree` is '`S`', which stands for *sentence*. Any chunks found
    will be subtrees whose nodes will refer to the chunk type. In this case, the chunk
    type is '`NP`' for *noun-phrase*. Trees can be drawn calling the `draw()` method,
    as in `t.draw()`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s what happens, step-by-step:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: The sentence is converted into a flat `Tree`, as shown in the following figure:![How
    it works...](img/3609_05_01.jpg)
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `Tree` is used to create a `ChunkString`.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`RegexpParser` parses the grammar to create a `NP RegexpChunkParser` with the
    given rules.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A `ChunkRule` is created and applied to the `ChunkString`, which matches the
    entire sentence into a chunk, as shown in the following figure:![How it works...](img/3609_05_02.jpg)
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A `ChinkRule` is created and applied to the same `ChunkString`, which splits
    the big chunk into two smaller chunks with a verb between them, as shown in the
    following figure:![How it works...](img/3609_05_03.jpg)
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `ChunkString` is converted back to a `Tree`, now with two **NP** chunk subtrees,
    as shown in the following figure:![How it works...](img/3609_05_04.jpg)
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can do this yourself using the classes in `nltk.chunk.regexp`. `ChunkRule`
    and `ChinkRule` are both subclasses of `RegexpChunkRule` and require two arguments:
    the pattern, and a description of the rule. `ChunkString` is an object that starts
    with a flat tree, which is then modified by each rule when it is passed in to
    the rule''s `apply()` method. A `ChunkString` is converted back to a `Tree` with
    the `to_chunkstruct()` method. Here''s the code to demonstrate it:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The preceding tree diagrams can be drawn at each step by calling `cs.to_chunkstruct().draw()`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will notice that the subtrees from the `ChunkString` are tagged as `'CHUNK'`
    and not `'NP'`. That's because the previous rules are phrase agnostic; they create
    chunks without needing to know what kind of chunks they are.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Internally, the `RegexpParser` creates a `RegexpChunkParser` for each chunk
    phrase type. So if you are only chunking `NP` phrases, there will only be one
    `RegexpChunkParser`. The `RegexpChunkParser` gets all the rules for the specific
    chunk type, and handles applying the rules in order and converting the `'CHUNK'`
    trees to the specific chunk type, such as `'NP'`.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，`RegexpParser`为每个词块短语类型创建一个`RegexpChunkParser`。所以如果你只对`NP`短语进行词块处理，就只有一个`RegexpChunkParser`。`RegexpChunkParser`获取特定词块类型的所有规则，并按顺序应用这些规则，将`'CHUNK'`树转换为特定词块类型，如`'NP'`。
- en: Here's some code to illustrate the usage of `RegexpChunkParser`. We pass the
    previous two rules into the `RegexpChunkParser`, and then parse the same sentence
    tree we created before. The resulting tree is just like what we got from applying
    both rules in order, except `'CHUNK'` has been replaced with `'NP'` in the two
    subtrees. This is because `RegexpChunkParser` defaults to `chunk_node='NP'`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些代码示例，说明了`RegexpChunkParser`的用法。我们将前两个规则传递给`RegexpChunkParser`，然后解析之前创建的相同句子树。得到的树与按顺序应用两个规则得到的树相同，只是两个子树中的`'CHUNK'`已被替换为`'NP'`。这是因为`RegexpChunkParser`默认的`chunk_node`是`'NP'`。
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Different chunk types
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不同的词块类型
- en: If you wanted to parse a different chunk type, then you could pass that in as
    `chunk_node` to `RegexpChunkParser`. Here's the same code we have just seen, but
    instead of `'NP'` subtrees, we will call them `'CP'` for *custom phrase*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要解析不同的词块类型，可以将它作为`chunk_node`传递给`RegexpChunkParser`。以下是刚刚看到的相同代码，但我们将`'NP'`子树改为`'CP'`（自定义短语）：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`RegexpParser` does this internally when you specify multiple phrase types.
    This will be covered in *Partial parsing with regular expressions*.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当你指定多个短语类型时，`RegexpParser`会内部执行此操作。这将在*使用正则表达式进行部分解析*中介绍。
- en: Alternative patterns
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 替代模式
- en: 'The same parsing results can be obtained by using two chunk patterns in the
    grammar, and discarding the chink pattern:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在语法中使用两个词块模式并丢弃`chink`模式，可以得到相同的解析结果：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In fact, you could reduce the two chunk patterns into a single pattern.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，你可以将两个词块模式合并成一个模式。
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: How you create and combine patterns is really up to you. Pattern creation is
    a process of trial and error, and entirely depends on what your data looks like
    and which patterns are easiest to express.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何创建和组合模式完全取决于你。模式创建是一个试错的过程，完全取决于你的数据看起来像什么，以及哪些模式最容易表达。
- en: Chunk rule with context
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带有上下文的词块规则
- en: You can also create chunk rules with a surrounding tag context. For example,
    if your pattern is `<DT>{<NN>}`, which will be parsed into a `ChunkRuleWithContext`.
    Any time there's a tag on either side of the curly braces, you will get a `ChunkRuleWithContext`
    instead of a `ChunkRule`. This can allow you to be more specific about when to
    parse particular kinds of chunks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以创建带有周围标签上下文的词块规则。例如，如果你的模式是`<DT>{<NN>}`，它将被解析为`ChunkRuleWithContext`。任何在花括号两侧有标签的情况下，你都会得到一个`ChunkRuleWithContext`而不是`ChunkRule`。这可以让你更具体地确定何时解析特定类型的词块。
- en: 'Here''s an example of using `ChunkWithContext` directly. It takes four arguments:
    the left context, the pattern to chunk, the right context, and a description:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个直接使用`ChunkWithContext`的示例。它接受四个参数：左侧上下文、词块模式、右侧上下文和描述：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This example only chunks nouns that follow a determiner, therefore ignoring
    the noun that follows an adjective. Here''s how it would look using the `RegexpParser`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子中只对跟在限定词后面的名词进行词块处理，因此忽略了跟在形容词后面的名词。以下是使用`RegexpParser`的示例：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: See also
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: In the next recipe, we will cover merging and splitting chunks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个菜谱中，我们将介绍合并和拆分词块。
- en: Merging and splitting chunks with regular expressions
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用正则表达式合并和拆分词块
- en: In t his recipe, we will cover two more rules for chunking. A `MergeRule` can
    merge two chunks together based on the end of the first chunk and the beginning
    of the second chunk. A `SplitRule` will split a chunk into two based on the specified
    split pattern.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将介绍两个额外的词块规则。`MergeRule`可以根据第一个词块的末尾和第二个词块的开头合并两个词块。`SplitRule`将根据指定的拆分模式将一个词块拆分为两个。
- en: How to do it...
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: A `Sp` `litRule` is specified with two opposing curly braces surrounded by a
    pattern on either side. To split a chunk after a noun, you would do `<NN.*>}{<.*>`.
    A `Merg` `eRule` is specified by flipping the curly braces, and will join chunks
    where the end of the first chunk matches the left pattern, and the beginning of
    the next chunk matches the right pattern. To merge two chunks where the first
    ends with a noun and the second begins with a noun, you would use `<NN.*>{}<NN.*>`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`Sp` `litRule`通过在两侧的模式中包围两个对立的大括号来指定。要在名词之后分割一个块，你会这样做`<NN.*>}{<.*>`。一个`Merg`
    `eRule`通过翻转大括号来指定，它将连接第一个块的末尾与左模式匹配，下一个块的开始与右模式匹配的块。要合并两个块，第一个以名词结束，第二个以名词开始，你会使用`<NN.*>{}<NN.*>`。
- en: Note
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The order of rules is very important and re-ordering can affect the results.
    The `RegexpParser` applies the rules one at a time from top to bottom, so each
    rule will be applied to the `ChunkString` resulting from the previous rule.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 规则的顺序非常重要，重新排序可能会影响结果。`RegexpParser`从上到下逐个应用规则，因此每个规则都会应用到前一个规则产生的`ChunkString`。
- en: 'Here ''s an example of splitting and merging, starting with the sentence tree
    as shown next:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个分割和合并的例子，从下面的句子树开始：
- en: '![How to do it...](img/3609_05_05.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/3609_05_05.jpg)'
- en: The whole sentence is chunked, as shown in the following diagram:![How to do
    it...](img/3609_05_06.jpg)
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 整个句子被分割，如下面的图所示：![如何做...](img/3609_05_06.jpg)
- en: The chunk is split into multiple chunks after every noun, as shown in the following
    tree:![How to do it...](img/3609_05_07.jpg)
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个名词之后，块被分割成多个块，如下面的树所示：![如何做...](img/3609_05_07.jpg)
- en: Each chunk with a determiner is split into separate chunks, creating four chunks
    where there were three:![How to do it...](img/3609_05_08.jpg)
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个带有限定词的块都被分割成单独的块，创建了四个块，而之前是三个：![如何做...](img/3609_05_08.jpg)
- en: Chunks ending with a noun are merged with the next chunk if it begins with a
    noun, reducing the four chunks back down to three, as shown in the following diagram:![How
    to do it...](img/3609_05_09.jpg)
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以名词结尾的块如果下一个块以名词开始，则与下一个块合并，将四个块减少到三个，如下面的图所示：![如何做...](img/3609_05_09.jpg)
- en: 'Using the `RegexpParser`, the code looks like this:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`RegexpParser`，代码看起来像这样：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And the final tree of `NP` chunks is shown in the following diagram:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`NP`块的最后树如下所示：'
- en: '![How to do it...](img/3609_05_10.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/3609_05_10.jpg)'
- en: How it works...
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The `MergeRule` and `SplitRule` classes take three arguments: the left pattern,
    right pattern, and a description. The `RegexpParser` takes care of splitting the
    original patterns on the curly braces to get the left and right sides, but you
    can also create these manually. Here''s a step-by-step walkthrough of how the
    original sentence is modified by applying each rule:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`MergeRule`和`SplitRule`类接受三个参数：左模式、右模式和描述。`RegexpParser`负责在花括号上分割原始模式以获取左右两侧，但你也可以手动创建这些。以下是如何通过应用每个规则逐步修改原始句子的说明：'
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: There's more...
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The parsing of the rules and splitting of left and right patterns is done in
    the static `parse()` method of the `RegexpChunkRule` superclass. This is called
    by the `RegexpParser` to get the list of rules to pass in to the `RegexpChunkParser`.
    Here are some examples of parsing the patterns used before:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 规则的解析以及左右模式的分割是在`RegexpChunkRule`超类的静态`parse()`方法中完成的。这是由`RegexpParser`调用来获取传递给`RegexpChunkParser`的规则列表的。以下是一些解析之前使用的模式的示例：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Rule descriptions
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 规则描述
- en: 'Descriptions for each rule can be specified with a comment string after the
    rule (a comment string must start with `#`). If no comment string is found, the
    rule''s description will be empty. Here''s an example:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在规则后面的注释字符串（注释字符串必须以`#`开头）来指定每个规则的描述。如果没有找到注释字符串，则规则的描述将为空。以下是一个示例：
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Comment string descriptions can also be used within grammar strings that are
    passed to `RegexpParser`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在传递给`RegexpParser`的语法字符串中也可以使用注释字符串描述。
- en: See also
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The previous recipe goes over how to use `ChunkRule` and how rules are passed
    in to `RegexpChunkParser`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的配方介绍了如何使用`ChunkRule`以及如何将规则传递给`RegexpChunkParser`。
- en: Expanding and removing chunks with regular expressions
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用正则表达式扩展和删除块
- en: 'There are three `RegexpChunkRule` subclasses that are not supported by `RegexpChunkRule.parse()`
    and therefore must be created manually if you want to use them. These rules are:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个`RegexpChunkRule`子类不被`RegexpChunkRule.parse()`支持，因此如果你想要使用它们，必须手动创建。这些规则是：
- en: '`ExpandLeftRule`: Adds unchunked (chink) words to the left of a chunk to the
    chunk.'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ExpandLeftRule`：将未解块的（词）添加到块的左侧。'
- en: '`ExpandRightRule`: Adds unchunked (chink) words to the right of a chunk to
    the chunk.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ExpandRightRule`：将未解块的（词）添加到块的右侧。'
- en: '`UnChunkRule`: Unchunk any matching chunk.'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`UnChunkRule`：解块任何匹配的块。'
- en: How to do it...
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: '`ExpandLeft` `Rule` and `ExpandRightRule` both take two patterns along with
    a description as arguments. For `ExpandLeftRule`, the first pattern is the chink
    we want to add to the beginning of the chunk, while the right pattern will match
    the beginning of the chunk we want to expand. With `ExpandRightRule`, the left
    pattern should match the end of the chunk we want to expand, and the right pattern
    matches the chink we want to add to the end of the chunk. The idea is similar
    to the `MergeRule`, but in this case we are merging chink words instead of other
    chunks.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExpandLeft` `Rule` 和 `ExpandRightRule` 都接受两个模式以及一个描述作为参数。对于 `ExpandLeftRule`，第一个模式是我们想要添加到块开头的词，而右侧的模式将匹配我们想要扩展的块的开头。对于
    `ExpandRightRule`，左侧的模式应该匹配我们想要扩展的块的末尾，而右侧的模式匹配我们想要添加到块末尾的词。这个想法与 `MergeRule`
    类似，但在这个情况下，我们是在合并词而不是其他块。'
- en: '`UnChunkRule` is the opposite of `ChunkRule`. Any chunk that exactly matches
    the `UnChunkRule` pattern will be unchunked, and become a chink. Here''s some
    code demonstrating usage with the `RegexpChunkParser`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`UnChunkRule` 是 `ChunkRule` 的反义词。任何与 `UnChunkRule` 模式完全匹配的块将被解块，并成为词。以下是一些使用
    `RegexpChunkParser` 的代码示例：'
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You will notice the end result is a flat sentence, which is exactly what we
    started with. That's because the final `UnChunkRule` undid the chunk created by
    the previous rules. Read on to see the step-by-step procedure of what happened.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到最终结果是平面的句子，这正是我们开始时的样子。那是因为最终的 `UnChunkRule` 取消了之前规则创建的块。继续阅读以了解发生了什么一步一步的过程。
- en: How it works...
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The preceding rules were applied in the following order, starting with the
    sentence tree shown below:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下规则按以下顺序应用，从下面的句子树开始：
- en: '![How it works...](img/3609_05_11.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/3609_05_11.jpg)'
- en: Make single nouns into a chunk, as shown in the following diagram:![How it works...](img/3609_05_12.jpg)
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将单个名词组合成一个块，如下所示：![工作原理...](img/3609_05_12.jpg)
- en: Expand left determiners into chunks that begin with a noun, as shown in the
    following diagram:![How it works...](img/3609_05_13.jpg)
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将左侧的限定词扩展到以名词开头的块中，如下所示：![工作原理...](img/3609_05_13.jpg)
- en: Expand right plural nouns into chunks that end with a noun, chunking the whole
    sentence as shown in the following diagram:![How it works...](img/3609_05_14.jpg)
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将右侧的复数名词扩展到以名词结尾的块中，如下所示：![工作原理...](img/3609_05_14.jpg)
- en: Unchunk every chunk that is a determiner + noun + plural noun, resulting in
    the original sentence tree, as shown in the following diagram:![How it works...](img/3609_05_15.jpg)
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个由限定词 + 名词 + 复数名词组成的块进行解块，从而得到原始句子树，如下所示：![工作原理...](img/3609_05_15.jpg)
- en: 'Here''s the code showing each step:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是展示每一步的代码：
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: There's more...
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'I n practice, you can probably get away with only using the previous four rules:
    `ChunkRule`, `ChinkRule`, `MergeRule`, and `SplitRule`. But if you do need very
    fine-grained control over chunk parsing and removing, now you know how to do it
    with the expansion and unchunk rules.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，你可能只需要使用前面提到的四个规则：`ChunkRule`、`ChinkRule`、`MergeRule` 和 `SplitRule`。但如果你确实需要非常精细地控制块解析和移除，现在你知道如何使用扩展和解块规则来做到这一点。
- en: See also
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The previous two recipes covered the more common chunk rules that are supported
    by `RegexpChunkRule.parse()` and `RegexpParser`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个配方涵盖了 `RegexpChunkRule.parse()` 和 `RegexpParser` 支持的更常见的块规则。
- en: Partial parsing with regular expressions
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用正则表达式进行部分解析
- en: S o far, we have only been parsing noun-phrases. But `RegexpParser` supports
    grammar with multiple phrase types, such as *verb-phrases* and *prepositional-phrases*.
    We can put the rules we have learned to use and define a grammar that can be evaluated
    against the `conll2000` corpus, which has `NP`, `VP`, and `PP` phrases.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只解析了名词短语。但 `RegexpParser` 支持具有多种短语类型的语法，例如 *动词短语* 和 *介词短语*。我们可以将学到的规则应用到定义一个语法中，该语法可以与
    `conll2000` 语料库进行评估，该语料库包含 `NP`、`VP` 和 `PP` 短语。
- en: How to do it...
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: We will define a grammar to parse three phrase types. For noun-phrases, we have
    a `ChunkRule` that looks for an optional determiner followed by one or more nouns.
    We then have a `MergeRule` for adding an adjective to the front of a noun chunk.
    For prepositional-phrases, we simply chunk any `IN` word, such as "in" or "on".
    For verb-phrases, we chunk an optional modal word (such as "should") followed
    by a verb.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个语法来解析三种短语类型。对于名词短语，我们有一个`ChunkRule`，它寻找一个可选的限定词后跟一个或多个名词。然后我们有一个`MergeRule`，用于在名词块的前面添加一个形容词。对于介词短语，我们简单地分块任何`IN`词，例如“in”或“on”。对于动词短语，我们分块一个可选的情态词（例如“should”）后跟一个动词。
- en: Note
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Each grammar rule is followed by a `#` comment. This comment is passed in to
    each rule as the description. Comments are optional, but they can be helpful notes
    for understanding what the rule does, and will be included in trace output.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 每个语法规则后面都跟着一个`#`注释。这个注释被传递给每个规则作为描述。注释是可选的，但它们可以是理解规则做什么的有用注释，并且将包含在追踪输出中。
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: When we call `evaluate()` on the `chunker`, we give it a list of chunked sentences
    and get back a `ChunkScore` object, which can give us the accuracy of the `chunker`,
    along with a number of other metrics.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在`chunker`上调用`evaluate()`方法时，我们给它一个分块句子的列表，并返回一个`ChunkScore`对象，该对象可以给我们`chunker`的准确度，以及许多其他指标。
- en: How it works...
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'T he `RegexpParser` parses the grammar string into sets of rules, one set of
    rules for each phrase type. These rules are used to create a `RegexpChunkParser`.
    The rules are parsed using `RegexpChunkRule.parse()`, which returns one of the
    five subclasses: `ChunkRule`, `ChinkRule`, `MergeRule`, `SplitRule`, or `ChunkRuleWithContext`.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`RegexpParser`将语法字符串解析成一系列规则，每个短语类型有一组规则。这些规则被用来创建一个`RegexpChunkParser`。规则是通过`RegexpChunkRule.parse()`解析的，它返回五个子类之一：`ChunkRule`、`ChinkRule`、`MergeRule`、`SplitRule`或`ChunkRuleWithContext`。'
- en: Now that the grammar has been translated into sets of rules, these rules are
    used to parse a tagged sentence into a `Tree` structure. `RegexpParser` inherits
    from `ChunkParserI`, which provides a `parse()` method to parse the tagged words.
    Whenever a part of the tagged tokens match a chunk rule, a subtree is constructed
    so that the tagged tokens become the leaves of a `Tree` whose node string is the
    chunk tag. `ChunkParserI` also provides the `evaluate()` method, which compares
    the given chunked sentences to the output of the `parse()` method to construct
    and return a `ChunkScore` object.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在语法已经被翻译成一系列规则，这些规则被用来将标记句子解析成`Tree`结构。`RegexpParser`从`ChunkParserI`继承，它提供了一个`parse()`方法来解析标记的词。每当标记的标记的一部分与分块规则匹配时，就会构建一个子树，使得标记的标记成为`Tree`的叶子，其节点字符串是分块标签。`ChunkParserI`还提供了一个`evaluate()`方法，该方法将给定的分块句子与`parse()`方法的输出进行比较，以构建并返回一个`ChunkScore`对象。
- en: There's more...
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: You can also evaluate this `chunker` on the `treebank_chunk` corpus.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在`treebank_chunk`语料库上评估这个`chunker`。
- en: '[PRE16]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `treebank_chunk` corpus is a special version of the `treebank` corpus that
    provides a `chunked_sents()` method. The regular `treebank` corpus cannot provide
    that method due to its file format.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`treebank_chunk`语料库是`treebank`语料库的一个特殊版本，它提供了一个`chunked_sents()`方法。由于文件格式，常规的`treebank`语料库无法提供该方法。'
- en: ChunkScore metrics
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分块评分指标
- en: '`ChunkScore` provides a few other metrics besides accuracy. Of the chunks the
    `chunker` was able to guess, precision tells you how many were correct. Recall
    tells you how well the `chunker` did at finding correct chunks, compared to how
    many total chunks there were.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`ChunkScore`除了准确度之外还提供了一些其他指标。对于`chunker`能够猜测的分块，精确度告诉你有多少是正确的。召回率告诉你`chunker`在找到正确分块方面的表现如何，与总共有多少分块相比。'
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You can also get lists of chunks that were missed by the `chunker`, chunks that
    were incorrectly found, correct chunks, and guessed chunks. These can be useful
    to figure out how to improve your chunk grammar.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以获取`chunker`遗漏的分块列表，错误找到的分块，正确的分块和猜测的分块。这些可以帮助你了解如何改进你的分块语法。
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As you can see by the number of incorrect chunks, and by comparing `guessed()`
    and `correct()`, our chunker guessed that there were more chunks that actually
    existed. And it also missed a good number of correct chunks.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，通过错误分块的数量，以及比较`guessed()`和`correct()`，我们的分块器猜测实际上存在的分块更多。它还遗漏了大量正确的分块。
- en: Looping and tracing
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 循环和追踪
- en: If you want to apply the chunk rules in your grammar more than once, you pass
    `loop=2` into `RegexpParser` at initialization. The default is `loop=1`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在语法中多次应用分块规则，你可以在初始化`RegexpParser`时传递`loop=2`。默认是`loop=1`。
- en: To watch an internal trace of the chunking process, pass `trace=1` into `RegexpParser`.
    To get even more output, pass in `trace=2`. This will give you a printout of what
    the chunker is doing as it is doing it. Rule comments/descriptions will be included
    in the trace output, giving you a good idea of which rule is applied when.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 要观察分块过程的内部跟踪，请将 `trace=1` 传递给 `RegexpParser`。要获取更多输出，请传递 `trace=2`。这将给出分块器正在执行的操作的打印输出。规则注释/描述将包含在跟踪输出中，这可以帮助你了解何时应用了哪个规则。
- en: See also
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: If coming up with regular expression chunk patterns seems like too much work,
    then read the next recipes where we will cover how to train a chunker based on
    a corpus of chunked sentences.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果觉得提出正则表达式块模式看起来工作量太大，那么请阅读下一部分内容，我们将介绍如何基于分块句子的语料库训练一个分块器。
- en: Training a tagger-based chunker
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于标签器的分块器
- en: Training a chunker can be a great alternative to manually specifying regular
    expression chunk patterns. Instead of a painstaking process of trial and error
    to get the exact right patterns, we can use existing corpus data to train chunkers
    much like we did in [Chapter 4](ch04.html "Chapter 4. Part-of-Speech Tagging"),
    *Part-of-Speech Tagging*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 训练分块器可以是一个手动指定正则表达式块模式的绝佳替代方案。而不是通过繁琐的试错过程来获取确切的正确模式，我们可以使用现有的语料库数据来训练分块器，就像我们在
    [第 4 章](ch04.html "第 4 章。词性标注") 中做的那样，*词性标注*。
- en: How to do it...
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'As with the part-of-speech tagging, we will use the treebank corpus data for
    training. But this time we will use the `treebank_chunk` corpus, which is specifically
    formatted to produce chunked sentences in the form of trees. These `chunked_sents()`
    will be used by a `TagChunker` class to train a tagger-based chunker. The `TagChunker`
    uses a helper function `conll_tag_chunks()` to extract a list of `(pos, iob)`
    tuples from a list of `Tree`. These `(pos, iob)` tuples are then used to train
    a tagger in the same way `(word, pos)` tuples were used in [Chapter 4](ch04.html
    "Chapter 4. Part-of-Speech Tagging"), *Part-of-Speech Tagging* to train part-of-speech
    taggers. But instead of learning part-of-speech tags for words, we are learning
    IOB tags for part-of-speech tags. Here''s the code from `chunkers.py`:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 与词性标注一样，我们将使用树库语料库数据进行训练。但这次我们将使用 `treebank_chunk` 语料库，该语料库专门格式化以生成以树的形式呈现的分块句子。这些
    `chunked_sents()` 将由 `TagChunker` 类用于训练基于标签器的分块器。`TagChunker` 使用辅助函数 `conll_tag_chunks()`
    从 `Tree` 列表中提取 `(pos, iob)` 元组列表。然后，这些 `(pos, iob)` 元组将用于以与 [第 4 章](ch04.html
    "第 4 章。词性标注") 中 *词性标注* 相同的方式训练标签器。但与学习单词的词性标签不同，我们正在学习词性标签的 IOB 标签。以下是 `chunkers.py`
    中的代码：
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Once we have our trained `TagChunker`, we can then evaluate the `ChunkScore`
    the same way we did for the `RegexpParser` in the previous recipes.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了训练好的 `TagChunker`，我们就可以像在之前的食谱中对 `RegexpParser` 进行评估一样评估 `ChunkScore`。
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Pretty darn accurate! Training a chunker is clearly a great alternative to manually
    specified grammars and regular expressions.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 非常准确！训练分块器显然是手动指定语法和正则表达式的绝佳替代方案。
- en: How it works...
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Recall from the *Creating a chunked phrase corpus* recipe in [Chapter 3](ch03.html
    "Chapter 3. Creating Custom Corpora"), *Creating Custom Corpora* that the `conll2000`
    corpus defines chunks using IOB tags, which specify the type of chunk and where
    it begins and ends. We can train a part-of-speech tagger on these IOB tag patterns,
    and then use that to power a `ChunkerI` subclass. But first we need to transform
    a `Tree` that you would get from the `chunked_sents()` method of a corpus into
    a format usable by a part-of-speech tagger. This is what `conll_tag_chunks()`
    does. It uses `nltk.chunk.tree2conlltags()` to convert a sentence `Tree` into
    a list of 3-tuples of the form `(word, pos, iob)` where `pos` is the part-of-speech
    tag and `iob` is an IOB tag, such as `B-NP` to mark the beginning of a noun-phrase,
    or `I-NP` to mark that the word is inside the noun-phrase. The reverse of this
    method is `nltk.chunk.conlltags2tree()`. Here''s some code to demonstrate these
    `nltk.chunk` functions:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 从[第3章](ch03.html "第3章。创建自定义语料库")中的*创建分块短语语料库*配方回忆起，在*创建自定义语料库*中，`conll2000`语料库使用IOB标签定义分块，这些标签指定了分块的类型以及它的开始和结束位置。我们可以在这些IOB标签模式上训练一个词性标注器，然后使用它来驱动`ChunkerI`子类。但首先，我们需要将从一个语料库的`chunked_sents()`方法得到的`Tree`转换成一个词性标注器可用的格式。这就是`conll_tag_chunks()`所做的事情。它使用`nltk.chunk.tree2conlltags()`将一个句子`Tree`转换成一个形式为`(word,
    pos, iob)`的3元组列表，其中`pos`是词性标签，`iob`是一个IOB标签，例如`B-NP`用来标记名词短语的开始，或者`I-NP`用来标记该词位于名词短语内部。这个方法的逆操作是`nltk.chunk.conlltags2tree()`。以下是一些演示这些`nltk.chunk`函数的代码：
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The next step is to convert these 3-tuples into 2-tuples that the tagger can
    recognize. Because the `RegexpParser` uses part-of-speech tags for chunk patterns,
    we will do that here too and use part-of-speech tags as if they were words to
    tag. By simply dropping the `word` from 3-tuple `(word, pos, iob)`, the `conll_tag_chunks()`
    function returns a list of 2-tuples of the form `(pos, iob)`. When given the preceding
    example `Tree` in a list, the results are in a format we can feed to a tagger.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将这些3元组转换为标签分类器可以识别的2元组。因为`RegexpParser`使用词性标签作为分块模式，所以我们在这里也会这样做，并将词性标签作为单词来标注。通过简单地从3元组`(word,
    pos, iob)`中删除`word`，`conll_tag_chunks()`函数返回一个形式为`(pos, iob)`的2元组列表。当给定的示例`Tree`作为一个列表提供时，结果就是我们能够喂给标签分类器的格式。
- en: '[PRE22]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The final step is a subclass of `ChunkParserI` called `TagChunker`. It trains
    on a list of chunk trees using an internal tagger. This internal tagger is composed
    of a `UnigramTagger` and a `BigramTagger` in a backoff chain, using the `backoff_tagger()`
    method created in the *Training and combining Ngram taggers* recipe in [Chapter
    4](ch04.html "Chapter 4. Part-of-Speech Tagging"), *Part-of-Speech Tagging*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是`ChunkParserI`的一个子类，称为`TagChunker`。它使用一个内部标签分类器在一系列分块树上进行训练。这个内部标签分类器由一个`UnigramTagger`和一个`BigramTagger`组成，它们在一个回退链中使用，使用的是[第4章](ch04.html
    "第4章。词性标注")中*训练和组合Ngram标签分类器*配方中创建的`backoff_tagger()`方法。
- en: Finally, `ChunkerI` subclasses must implement a `parse()` method that expects
    a part-of-speech tagged sentence. We unzip that sentence into a list of words
    and part-of-speech tags. The tags are then tagged by the tagger to get IOB tags,
    which are then re-combined with the words and part-of-speech tags to create 3-tuples
    we can pass to `nltk.chunk.conlltags2tree()` to return a final `Tree`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`ChunkerI`子类必须实现一个`parse()`方法，该方法期望一个词性标注过的句子。我们将该句子解包成一个单词和词性标签的列表。然后，标签由标签分类器进行标注以获得IOB标签，这些标签随后与单词和词性标签重新组合，以创建我们可以传递给`nltk.chunk.conlltags2tree()`的3元组，从而返回一个最终的`Tree`。
- en: There's more...
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Since we have been talking about the `conll` IOB tags, let us see how the `TagChunker`
    does on the `conll2000` corpus:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们一直在讨论`conll` IOB 标签，让我们看看`TagChunker`在`conll2000`语料库上的表现：
- en: '[PRE23]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Not quite as good as on `treebank_chunk`, but `conll2000` is a much larger corpus,
    so it's not too surprising.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 不如`treebank_chunk`好，但`conll2000`是一个更大的语料库，所以这并不太令人惊讶。
- en: Using different taggers
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用不同的标签分类器
- en: 'If you want to use different tagger classes with the `TagChunker`, you can
    pass them in as `tagger_classes`. For example, here''s the `TagChunker` using
    just a `UnigramTagger`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要使用不同的标签分类器与`TagChunker`一起使用，你可以将它们作为`tagger_classes`传入。例如，这里是一个仅使用`UnigramTagger`的`TagChunker`：
- en: '[PRE24]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `tagger_classes` will be passed directly into the `backoff_tagger()` function,
    which means they must be subclasses of `SequentialBackoffTagger`. In testing,
    the default of `tagger_classes=[UnigramTagger, BigramTagger]` produces the best
    results.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`tagger_classes`将直接传递到`backoff_tagger()`函数中，这意味着它们必须是`SequentialBackoffTagger`的子类。在测试中，默认的`tagger_classes=[UnigramTagger,
    BigramTagger]`产生了最佳结果。'
- en: See also
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The *Training and combining Ngram taggers* recipe in [Chapter 4](ch04.html "Chapter 4. Part-of-Speech
    Tagging"), *Part-of-Speech Tagging* covers backoff tagging with a `UnigramTagger`
    and `BigramTagger`. `ChunkScore` metrics returned by the `evaluate()` method of
    a chunker were explained in the previous recipe.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章 [“词性标注”](ch04.html "第4章。词性标注") 的 *“训练和组合Ngram标注器”* 菜谱中，涵盖了使用 `UnigramTagger`
    和 `BigramTagger` 的回退标注。在之前的菜谱中解释了由短语结构切分器的 `evaluate()` 方法返回的 `ChunkScore` 指标。
- en: Classification-based chunking
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于分类的短语结构切分
- en: U nlike most part-of-speech taggers, the `ClassifierBasedTagger` learns from
    features. That means we can create a `ClassifierChunker` that can learn from both
    the words and part-of-speech tags, instead of only the part-of-speech tags as
    the `TagChunker` does.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数词性标注器不同，`ClassifierBasedTagger` 从特征中学习。这意味着我们可以创建一个 `ClassifierChunker`，它可以同时从单词和词性标注中学习，而不仅仅是像
    `TagChunker` 那样只从词性标注中学习。
- en: How to do it...
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'For the `ClassifierChunker`, we don''t want to discard the words from the training
    sentences, as we did in the previous recipe. Instead, to remain compatible with
    the 2-tuple `(word, pos)` format required for training a `ClassiferBasedTagger`,
    we convert the `(word, pos, iob)` 3-tuples from `nltk.chunk.tree2conlltags()`
    into `((word, pos), iob)` 2-tuples using the `chunk_trees2train_chunks()` function.
    This code can be found in `chunkers.py`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `ClassifierChunker`，我们不想像在之前的菜谱中那样丢弃训练句子中的单词。相反，为了保持与训练 `ClassiferBasedTagger`
    所需的2元组 `(word, pos)` 格式兼容，我们使用 `chunk_trees2train_chunks()` 函数将 `nltk.chunk.tree2conlltags()`
    中的 `(word, pos, iob)` 3元组转换为 `((word, pos), iob)` 2元组。此代码可在 `chunkers.py` 中找到：
- en: '[PRE25]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Next, we need a feature detector function to pass into `ClassifierBasedTagger`.
    Our default feature detector function, `prev_next_pos_iob()`, knows that the list
    of `tokens` is really a list of `(word, pos)` tuples, and can use that to return
    a feature set suitable for a classifier. To give the classifier as much information
    as we can, this feature set contains the current, previous and next word, and
    part-of-speech tag, along with the previous IOB tag.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个特征检测函数传递给 `ClassifierBasedTagger`。我们的默认特征检测函数 `prev_next_pos_iob()`
    知道 `tokens` 列表实际上是 `(word, pos)` 元组的列表，并且可以使用它来返回适合分类器的特征集。为了给分类器尽可能多的信息，这个特征集包含当前、前一个和下一个单词以及词性标注，以及前一个IOB标注。
- en: '[PRE26]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: N ow we can define the `ClassifierChunker`, which uses an internal `ClassifierBasedTagger`
    with features extracted using `prev_next_pos_iob()`, and training sentences from
    `chunk_trees2train_chunks()`. As a subclass of `ChunkerParserI`, it implements
    the `parse()` method, which converts the `((w, t), c)` tuples produced by the
    internal tagger into a `Tree` using `nltk.chunk.conlltags2tree()`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义 `ClassifierChunker`，它使用内部 `ClassifierBasedTagger`，并使用 `prev_next_pos_iob()`
    提取的特征和来自 `chunk_trees2train_chunks()` 的训练句子。作为 `ChunkerParserI` 的子类，它实现了 `parse()`
    方法，该方法使用 `nltk.chunk.conlltags2tree()` 将内部标注器产生的 `((w, t), c)` 元组转换为 `Tree`。
- en: '[PRE27]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Using the same `train_chunks` and `test_chunks` from the `treebank_chunk` corpus
    in the previous recipe, we can evaluate this code from `chunkers.py`:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与之前菜谱中 `treebank_chunk` 语料库相同的 `train_chunks` 和 `test_chunks`，我们可以评估 `chunkers.py`
    中的此代码：
- en: '[PRE28]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'C ompared to the `TagChunker`, all the scores have gone up a bit. Let us see
    how it does on `conll2000`:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `TagChunker` 相比，所有分数都有所上升。让我们看看它在 `conll2000` 上的表现：
- en: '[PRE29]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This is much improved over the `TagChunker`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这比 `TagChunker` 有很大改进。
- en: How it works...
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Like the `TagChunker` in the previous recipe, we are training a part-of-speech
    tagger for IOB tagging. But in this case, we want to include the word as a feature
    to power a classifier. By creating nested 2-tuples of the form `((word, pos),
    iob)`, we can pass the word through the tagger into our feature detector function.
    `chunk_trees2train_chunks()` produces these nested 2-tuples, and `prev_next_pos_iob()`
    is aware of them and uses each element as a feature. The following features are
    extracted:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前菜谱中的 `TagChunker` 一样，我们正在训练一个用于IOB标注的词性标注器。但在这个情况下，我们希望包括单词作为特征来驱动分类器。通过创建形式为
    `((word, pos), iob)` 的嵌套2元组，我们可以将单词通过标注器传递到特征检测函数。`chunk_trees2train_chunks()`
    生成这些嵌套2元组，`prev_next_pos_iob()` 了解它们并使用每个元素作为特征。以下特征被提取：
- en: The current word and part-of-speech tag
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前单词和词性标注
- en: The previous word, part-of-speech tag, and IOB tag
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前一个单词、词性标注和IOB标注
- en: The next word and part-of-speech tag
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个单词和词性标注
- en: 'The arguments to `prev_next_pos_iob()` look the same as the `feature_detector()`
    method of the `ClassifierBasedTagger`: `tokens`, `index`, and `history`. But this
    time, `tokens` will be a list of `(word, pos)` 2-tuples, and `history` will be
    a list of IOB tags. The special feature values `''<START>''` and `''<END>''` are
    used if there are no previous or next tokens.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`prev_next_pos_iob()`的参数看起来与`ClassifierBasedTagger`的`feature_detector()`方法的参数相同：`tokens`、`index`和`history`。但这次，`tokens`将是一个包含`(word,
    pos)`二元组的列表，而`history`将是一个IOB标签的列表。如果没有前一个或下一个标记，将使用特殊特征值`''<START>''`和`''<END>''`。'
- en: T he `ClassifierChunker` uses an internal `ClassifierBasedTagger` and `prev_next_pos_iob()`
    as its default `feature_detector`. The results from the tagger, which are in the
    same nested 2-tuple form, are then reformatted into 3-tuples to return a final
    `Tree` using `nltk.chunk.conlltags2tree()`.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClassifierChunker`使用内部的`ClassifierBasedTagger`和`prev_next_pos_iob()`作为其默认的`feature_detector`。然后，将标签器的结果（以相同的嵌套二元组形式）重新格式化为三元组，使用`nltk.chunk.conlltags2tree()`返回一个最终的`Tree`。'
- en: There's more...
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多...
- en: You can use your own feature detector function by passing it in to the `ClassifierChunker`
    as `feature_detector`. The `tokens` will contain a list of `(word, tag)` tuples,
    and `history` will be a list of the previous IOB tags found.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过将自定义的特征检测函数传递给`ClassifierChunker`作为`feature_detector`来使用自己的特征检测函数。`tokens`将包含一个`(word,
    tag)`元组的列表，而`history`将包含之前找到的IOB标签的列表。
- en: Using a different classifier builder
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用不同的分类器构建器
- en: 'The `ClassifierBasedTagger` defaults to using `NaiveBayesClassifier.train`
    as its `classifier_builder`. But you can use any classifier you want by overriding
    the `classifier_builder` keyword argument. Here''s an example using `MaxentClassifier.train`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClassifierBasedTagger`默认使用`NaiveBayesClassifier.train`作为其`classifier_builder`。但您可以通过覆盖`classifier_builder`关键字参数来使用任何您想要的分类器。以下是一个使用`MaxentClassifier.train`的示例：'
- en: '[PRE30]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Instead of using `MaxentClassifier.train` directly, it has been wrapped in a
    `lambda` so that its output is quiet (`trace=0`) and it finishes in a reasonable
    amount of time. As you can see, the scores are slightly different compared to
    using the `NaiveBayesClassifier`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 不是直接使用`MaxentClassifier.train`，它被包装在一个`lambda`中，以便其输出是安静的（`trace=0`）并且能够在合理的时间内完成。如您所见，与使用`NaiveBayesClassifier`相比，分数略有不同。
- en: See also
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The previous recipe, *Training a tagger-based chunker*, introduced the idea
    of using a part-of-speech tagger for training a chunker. The *Classifier-based
    tagging* recipe in [Chapter 4](ch04.html "Chapter 4. Part-of-Speech Tagging"),
    *Part-of-Speech Tagging* describes `ClassifierBasedPOSTagger`, which is a subclass
    of `ClassifierBasedTagger`. In [Chapter 7](ch07.html "Chapter 7. Text Classification"),
    *Text Classification*, we will cover classification in detail.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的配方*基于标签器的训练*介绍了使用词性标注器来训练标签器的想法。第4章*词性标注*中的*基于分类器的标签*配方描述了`ClassifierBasedPOSTagger`，它是`ClassifierBasedTagger`的子类。在第7章*文本分类*中，我们将详细讨论分类。
- en: Extracting named entities
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取命名实体
- en: '**Named entity recognition** is a specific kind of chunk extraction that uses
    **entity tags** instead of, or in addition to, chunk tags. Common entity tags
    include `PERSON`, `ORGANIZATION`, and `LOCATION`. Part-of-speech tagged sentences
    are parsed into chunk trees as with normal chunking, but the nodes of the trees
    can be entity tags instead of chunk phrase tags.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**命名实体识别**是一种特定的块提取，它使用**实体标签**而不是，或者除了，块标签。常见的实体标签包括`PERSON`、`ORGANIZATION`和`LOCATION`。词性标注句子与正常块提取一样被解析成块树，但树的节点可以是实体标签而不是块短语标签。'
- en: How to do it...
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'NLTK comes with a pre-trained named entity chunker. This chunker has been trained
    on data from the ACE program, a **NIST** (**National Institute of Standards and
    Technology**) sponsored program for **Automatic Content Extraction**, which you
    can read more about here: [http://www.itl.nist.gov/iad/894.01/tests/ace/](http://www.itl.nist.gov/iad/894.01/tests/ace/).
    Unfortunately, this data is not included in the NLTK corpora, but the trained
    chunker is. This chunker can be used through the `ne_chunk()` method in the `nltk.chunk`
    module. `ne_chunk()` will chunk a single sentence into a `Tree`. The following
    is an example using `ne_chunk()` on the first tagged sentence of the `treebank_chunk`
    corpus:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK附带了一个预训练的命名实体分块器。这个分块器已经在ACE程序的数据上进行了训练，ACE程序是由**NIST**（**国家标准与技术研究院**）赞助的**自动内容提取**项目，你可以在这里了解更多信息：[http://www.itl.nist.gov/iad/894.01/tests/ace/](http://www.itl.nist.gov/iad/894.01/tests/ace/)。不幸的是，这些数据不包括在NLTK语料库中，但训练好的分块器是包含在内的。这个分块器可以通过`nltk.chunk`模块中的`ne_chunk()`方法使用。`ne_chunk()`将单个句子分块成一个`Tree`。以下是一个使用`ne_chunk()`对`treebank_chunk`语料库中第一个标记句子的示例：
- en: '[PRE31]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You can see two entity tags are found: `PERSON` and `ORGANIZATION`. Each of
    these subtrees contain a list of the words that are recognized as a `PERSON` or
    `ORGANIZATION`. To extract these named entities, we can write a simple helper
    method that will get the leaves of all the subtrees we are interested in.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到找到了两个实体标签：`PERSON`和`ORGANIZATION`。这些子树中的每一个都包含一个被识别为`PERSON`或`ORGANIZATION`的单词列表。为了提取这些命名实体，我们可以编写一个简单的辅助方法，该方法将获取我们感兴趣的子树的所有叶子节点。
- en: '[PRE32]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Then we can call this method to get all the `PERSON` or `ORGANIZATION` leaves
    from a tree.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以调用此方法从树中获取所有`PERSON`或`ORGANIZATION`叶子节点。
- en: '[PRE33]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: You may notice that the chunker has mistakenly separated "Vinken" into its own
    `ORGANIZATION Tree` instead of including it with the `PERSON Tree` containing
    "Pierre". Such is the case with statistical natural language processing—you can't
    always expect perfection.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，分块器错误地将“Vinken”分成了它自己的`ORGANIZATION Tree`，而不是将其包含在包含“Pierre”的`PERSON
    Tree`中。这是统计自然语言处理的情况——你不能总是期望完美。
- en: How it works...
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The pre-trained named entity chunker is much like any other chunker, and in
    fact uses a `MaxentClassifier` powered `ClassifierBasedTagger` to determine IOB
    tags. But instead of `B-NP` and `I-NP` IOB tags, it uses `B-PERSON`, `I-PERSON`,
    `B-ORGANIZATION`, `I-ORGANIZATION`, and more. It also uses the `O` tag to mark
    words that are not part of a named entity (and thus outside the named entity subtrees).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的命名实体分块器与其他分块器类似，实际上它使用了一个由`MaxentClassifier`驱动的`ClassifierBasedTagger`来确定IOB标签。但它不使用`B-NP`和`I-NP`
    IOB标签，而是使用`B-PERSON`、`I-PERSON`、`B-ORGANIZATION`、`I-ORGANIZATION`等标签。它还使用`O`标签来标记不属于命名实体的单词（因此位于命名实体子树之外）。
- en: There's more...
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'To process multiple sentences at a time, you can use `batch_ne_chunk()`. Here''s
    an example where we process the first 10 sentences from `treebank_chunk.tagged_sents()`
    and get the `ORGANIZATION sub_leaves()`:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要同时处理多个句子，可以使用`batch_ne_chunk()`。以下是一个示例，我们处理了`treebank_chunk.tagged_sents()`的前10个句子，并获取了`ORGANIZATION
    sub_leaves()`：
- en: '[PRE34]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: You can see there are a couple of multi-word `ORGANIZATION` chunks, such as
    "New England Journal". There are also a few sentences that have no `ORGANIZATION`
    chunks, as indicated by the empty lists `[]`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到有几个多词的`ORGANIZATION`分块，例如“New England Journal”。还有一些句子没有`ORGANIZATION`分块，如空列表`[]`所示。
- en: Binary named entity extraction
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 二进制命名实体提取
- en: 'If you don''t care about the particular kind of named entity to extract, you
    can pass `binary=True` into `ne_chunk()` or `batch_ne_chunk()`. Now, all named
    entities will be tagged with `NE`:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不在乎要提取的特定类型的命名实体，可以将`binary=True`传递给`ne_chunk()`或`batch_ne_chunk()`。现在，所有命名实体都将被标记为`NE`：
- en: '[PRE35]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: If we get the `sub_leaves()`, we can see that "Pierre Vinken" is correctly combined
    into a single named entity.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们获取`sub_leaves()`，我们可以看到“Pierre Vinken”被正确地组合成一个单一的命名实体。
- en: '[PRE36]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: See also
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: In the next recipe, we will create our own simple named entity chunker.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个菜谱中，我们将创建自己的简单命名实体分块器。
- en: Extracting proper noun chunks
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取专有名词分块
- en: A simple way to do named entity extraction is to chunk all proper nouns (tagged
    with `NNP`). We can tag these chunks as `NAME`, since the definition of a proper
    noun is the name of a person, place, or thing.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 提取命名实体的简单方法是将所有专有名词（标记为`NNP`）进行分块。我们可以将这些分块标记为`NAME`，因为专有名词的定义是人的名字、地点或事物的名称。
- en: How to do it...
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: Using the `RegexpParser`, we can create a very simple grammar that combines
    all proper nouns into a `NAME` chunk. Then we can test this on the first tagged
    sentence of `treebank_chunk` to compare the results to the previous recipe.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`RegexpParser`，我们可以创建一个非常简单的语法，将所有专有名词组合成一个`NAME`片段。然后我们可以在`treebank_chunk`的第一个标记句子上测试这个语法，以比较之前的结果。
- en: '[PRE37]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Although we get "Nov." as a `NAME` chunk, this isn't a wrong result, as "Nov."
    is the name of a month.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们得到了“Nov.”作为一个`NAME`片段，但这不是一个错误的结果，因为“Nov.”是月份的名称。
- en: How it works...
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `NAME` chunker is a simple usage of the `RegexpParser`, covered in *Chunking
    and chinking with regular expressions*, *Merging and splitting chunks with regular
    expressions*, and *Partial parsing with regular expressions* recipes of this chapter.
    All sequences of `NNP` tagged words are combined into `NAME` chunks.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`NAME`片段器是`RegexpParser`的一个简单用法，在本章的*使用正则表达式进行分块和切分*、*使用正则表达式合并和拆分片段*和*使用正则表达式进行部分解析*食谱中进行了介绍。所有标记为`NNP`的词序列都被组合成`NAME`片段。'
- en: There's more...
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'If we wanted to be sure to only chunk the names of people, then we can build
    a `PersonChunker` that uses the `names` corpus for chunking. This class can be
    found in `chunkers.py`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要确保只对人的名字进行分块，那么我们可以构建一个使用`names`语料库进行分块的`PersonChunker`。这个类可以在`chunkers.py`中找到：
- en: '[PRE38]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The `PersonChunker` iterates over the tagged sentence, checking if each word
    is in its `names_set` (constructed from the `names` corpus). If the current word
    is in the `names_set`, then it uses either the `B-PERSON` or `I-PERSON` IOB tags,
    depending on whether the previous word was also in the `names_set`. Any word that''s
    not in the `names_set` gets the `O` IOB tag. When complete, the list of IOB tags
    is converted to a `Tree` using `nltk.chunk.conlltags2tree()`. Using it on the
    same tagged sentence as before, we get the following result:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`PersonChunker`遍历标记句子，检查每个词是否在其`names_set`（从`names`语料库构建）中。如果当前词在`names_set`中，那么它使用`B-PERSON`或`I-PERSON`
    IOB标签，具体取决于前一个词是否也在`names_set`中。不在`names_set`中的任何词都得到`O` IOB标签。完成后，使用`nltk.chunk.conlltags2tree()`将IOB标签列表转换为`Tree`。使用它对之前的标记句子进行操作，我们得到以下结果：'
- en: '[PRE39]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We no longer get "Nov.", but we have also lost "Vinken", as it is not found
    in the `names` corpus. This recipe highlights some of the difficulties of chunk
    extraction and natural language processing in general:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不再得到“Nov.”，但我们也失去了“Vinken”，因为它没有在`names`语料库中找到。这个食谱突出了片段提取和自然语言处理一般的一些困难。
- en: If you use general patterns, you will get general results
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用通用模式，你会得到通用结果
- en: If you are looking for specific results, you must use specific data
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你正在寻找特定的结果，你必须使用特定的数据
- en: If your specific data is incomplete, your results will be incomplete too
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的具体数据不完整，你的结果也会不完整
- en: See also
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The previous recipe defines the `sub_leaves()` method used to show the found
    chunks. In the next recipe, we will cover how to find `LOCATION` chunks based
    on the `gazetteers` corpus.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的食谱定义了`sub_leaves()`方法，用于显示找到的片段。在下一个食谱中，我们将介绍如何根据`gazetteers`语料库找到`LOCATION`片段。
- en: Extracting location chunks
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取位置片段
- en: 'To identify location chunks, we can make a different kind of `ChunkParserI`
    subclass that uses the `gazetteers` corpus to identify location words. `gazetteers`
    is a `WordListCorpusReader` that contains the following location words:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 要识别位置片段，我们可以创建一个不同的`ChunkParserI`子类，该子类使用`gazetteers`语料库来识别位置词。`gazetteers`是一个包含以下位置词的`WordListCorpusReader`：
- en: Country names
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 国家名称
- en: U.S. states and abbreviations
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 美国州份和缩写
- en: Major U.S. cities
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要美国城市
- en: Canadian provinces
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加拿大省份
- en: Mexican states
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 墨西哥州份
- en: How to do it...
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: The `LocationChunker`, found in `chunkers.py`, iterates over a tagged sentence
    looking for words that are found in the `gazetteers` corpus. When it finds one
    or more location words, it creates a `LOCATION` chunk using IOB tags. The helper
    method `iob_locations()` is where the IOB `LOCATION` tags are produced, and the
    `parse()` method converts these IOB tags to a `Tree`.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`LocationChunker`，位于`chunkers.py`中，遍历一个标记句子，寻找在`gazetteers`语料库中找到的词。当它找到一个或多个位置词时，它使用IOB标签创建一个`LOCATION`片段。辅助方法`iob_locations()`是产生IOB
    `LOCATION`标签的地方，而`parse()`方法将这些IOB标签转换为`Tree`。'
- en: '[PRE40]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can use the `LocationChunker` to parse the following sentence into two locations,
    "San Francisco, CA is cold compared to San Jose, CA":'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`LocationChunker`将以下句子解析为两个位置：“与旧金山，CA相比，圣何塞，CA很冷”。
- en: '[PRE41]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: And the result is that we get two `LOCATION` chunks, just as expected.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是我们得到了两个`LOCATION`片段，正如预期的那样。
- en: How it works...
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `LocationChunker` starts by constructing a `set` of all locations in the
    `gazetteers` corpus. Then it finds the maximum number of words in a single location
    string, so it knows how many words it must look ahead when parsing a tagged sentence.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`LocationChunker` 首先构建 `gazetteers` 语料库中所有地点的 `set`。然后它找到单个地点字符串中的单词最大数量，这样它就知道在解析标记句子时必须向前查看多少个单词。'
- en: The `parse()` method calls a helper method `iob_locations()`, which generates
    3-tuples of the form `(word, pos, iob)` where `iob` is either `O` if the word
    is not a location, or `B-LOCATION` or `I-LOCATION` for `LOCATION` chunks. `iob_locations()`
    finds location chunks by looking at the current word and the next words to check
    if the combined word is in the locations `set`. Multiple location words that are
    next to each other are then put into the same `LOCATION` chunk, such as in the
    preceding example with "San Francisco" and "CA".
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`parse()` 方法调用一个辅助方法 `iob_locations()`，该方法生成形式为 `(word, pos, iob)` 的 3-元组，其中
    `iob` 是 `O`（如果单词不是地点），或者对于 `LOCATION` 分块是 `B-LOCATION` 或 `I-LOCATION`。`iob_locations()`
    通过查看当前单词和下一个单词来查找地点分块，以检查组合单词是否在地点 `set` 中。相邻的多个地点单词然后被放入同一个 `LOCATION` 分块中，例如在先前的例子中，“San
    Francisco” 和 “CA”。'
- en: Like in the previous recipe, it's simpler and more convenient to construct a
    list of `(word, pos, iob)` tuples to pass in to `nltk.chunk.conlltags2tree()`
    to return a `Tree`. The alternative is to construct a `Tree` manually, but that
    requires keeping track of children, subtrees, and where you currently are in the
    `Tree`.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的配方类似，构建一个 `(word, pos, iob)` 元组的列表传递给 `nltk.chunk.conlltags2tree()` 以返回一个
    `Tree` 更简单、更方便。另一种方法是手动构建一个 `Tree`，但这需要跟踪子节点、子树以及你在 `Tree` 中的当前位置。
- en: There's more...
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: One of the nice aspects of this `LocationChunker` is that it doesn't care about
    the part-of-speech tags. As long as the location words are found in the locations
    set, any part-of-speech tag will do.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 `LocationChunker` 的一个优点是它不关心词性标记。只要地点单词在地点 `set` 中找到，任何词性标记都可以。
- en: See also
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考内容
- en: In the next recipe, we will cover how to train a named entity chunker using
    the `ieer` corpus.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个配方中，我们将介绍如何使用 `ieer` 语料库训练一个命名实体分块器。
- en: Training a named entity chunker
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个命名实体分块器
- en: You can train your own named entity chunker using the `ieer` corpus, which stands
    for **Information Extraction—Entity Recognition** (**ieer**). It takes a bit of
    extra work though, because the `ieer` corpus has chunk trees, but no part-of-speech
    tags for words.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `ieer` 语料库训练自己的命名实体分块器，`ieer` 代表 **信息提取—实体识别**（**ieer**）。但这需要一些额外的工作，因为
    `ieer` 语料库有分块树，但没有单词的词性标记。
- en: How to do it...
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: Using the `ieertree2conlltags()` and `ieer_chunked_sents()` functions in `chunkers.py`,
    we can create named entity chunk trees from the `ieer` corpus to train the `ClassifierChunker`
    created in *Classification-based chunking* recipe of this chapter.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `chunkers.py` 中的 `ieertree2conlltags()` 和 `ieer_chunked_sents()` 函数，我们可以从
    `ieer` 语料库创建命名实体分块树，以训练本章“基于分类的分块”配方中创建的 `ClassifierChunker`。
- en: '[PRE42]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We will use 80 out of 94 sentences for training, and the rest for testing. Then
    we can see how it does on the first sentence of the `treebank_chunk` corpus.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 94 句中的 80 句进行训练，其余的用于测试。然后我们可以看看它在 `treebank_chunk` 语料库的第一句话上的表现如何。
- en: '[PRE43]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'So it found a correct `DURATION` and `DATE`, but tagged "Pierre Vinken" as
    a `LOCATION`. Let us see how it scores against the rest of `ieer` chunk trees:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它找到了正确的 `DURATION` 和 `DATE`，但将“Pierre Vinken”标记为 `LOCATION`。让我们看看它与其他 `ieer`
    分块树相比的得分如何：
- en: '[PRE44]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Accuracy is pretty good, but precision and recall are very low. That means lots
    of false negatives and false positives.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率相当不错，但精确率和召回率非常低。这意味着有很多误判和误报。
- en: How it works...
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The truth is, we are not working with ideal training data. The `ieer` trees
    generated by `ieer_chunked_sents()` are not entirely accurate. First, there are
    no explicit sentence breaks, so each document is a single tree. Second, the words
    are not explicitly tagged, so we have to guess using `nltk.tag.pos_tag()`.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们并不是在理想训练数据上工作。由 `ieer_chunked_sents()` 生成的 `ieer` 树并不完全准确。首先，没有明确的句子分隔，所以每个文档都是一个单独的树。其次，单词没有明确的标记，所以我们不得不使用
    `nltk.tag.pos_tag()` 来猜测。
- en: The `ieer` corpus provides a `parsed_docs()` method that returns a list of documents
    with a `text` attribute. This `text` attribute is a document `Tree` that is converted
    to a list of 3-tuples of the form `(word, pos, iob)`. To get these final 3-tuples,
    we must first flatten the `Tree` using `tree.pos()`, which returns a list of 2-tuples
    of the form `(word, entity)`, where entity is either the entity tag or the top
    tag of the tree. Any words whose entity is the top tag are outside the named entity
    chunks and get the IOB tag `O`. All words that have unique entity tags are either
    the beginning of or inside a named entity chunk. Once we have all the IOB tags,
    then we can get the part-of-speech tags of all the words and join the words, part-of-speech
    tags, and IOB tags into 3-tuples using `itertools.izip()`.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`ieer`语料库提供了一个`parsed_docs()`方法，该方法返回一个具有`text`属性的文档列表。这个`text`属性是一个文档`Tree`，它被转换为形式为`(word,
    pos, iob)`的3元组列表。为了获得这些最终的3元组，我们必须首先使用`tree.pos()`来展平`Tree`，它返回形式为`(word, entity)`的2元组列表，其中`entity`是实体标签或树的顶层标签。任何实体标签为顶层标签的单词都位于命名实体块之外，并得到IOB标签`O`。所有具有唯一实体标签的单词要么是命名实体块的开始，要么在命名实体块内部。一旦我们有了所有的IOB标签，我们就可以获取所有单词的词性标签，并使用`itertools.izip()`将单词、词性标签和IOB标签组合成3元组。'
- en: There's more...
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Despite the non-ideal training data, the `ieer` corpus provides a good place
    to start for training a named entity chunker. The data comes from the *New York
    Times* and *AP Newswire* reports. Each doc from `ieer.parsed_docs()` also contains
    a headline attribute that is a `Tree`.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管训练数据并不理想，但`ieer`语料库为训练一个命名实体分块器提供了一个良好的起点。数据来自《纽约时报》和《美联社新闻稿》。`ieer.parsed_docs()`中的每个文档也包含一个标题属性，该属性是一个`Tree`。
- en: '[PRE45]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: See also
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The *Extracting named entities* recipe in this chapter, covers the pre-trained
    named entity chunker that comes included with NLTK.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的*提取命名实体*配方涵盖了NLTK附带预训练的命名实体分块器。
