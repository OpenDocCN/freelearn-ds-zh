<html><head></head><body>
		<div id="_idContainer019">
			<h1 id="_idParaDest-53" class="chapter-number"><a id="_idTextAnchor053"/>3</h1>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor054"/>Spark Architecture and Transformations</h1>
			<p>Spark approaches data processing differently than traditional tools and technologies. To understand Spark’s unique approach, we will have to understand its basic architecture. A deep dive into Spark’s architecture and its components will give you an idea of how Spark achieves its ground-breaking processing speeds for big <span class="No-Break">data analytics.</span></p>
			<p>In this chapter, you will learn about the following <span class="No-Break">broader topics:</span></p>
			<ul>
				<li>Spark architecture and <span class="No-Break">execution hierarchy</span></li>
				<li>Different <span class="No-Break">Spark components</span></li>
				<li>The roles of the Spark driver and <span class="No-Break">Spark executor</span></li>
				<li>Different deployment modes <span class="No-Break">in Spark</span></li>
				<li>Transformations and actions as <span class="No-Break">Spark operations</span></li>
			</ul>
			<p>By the end of this chapter, you will have valuable insights into Spark’s inner workings and know how to apply this knowledge effectively for your <span class="No-Break">certification test.</span></p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor055"/>Spark architecture</h1>
			<p>In the previous chapters, we discussed that Apache Spark is an open source, distributed computing <a id="_idIndexMarker080"/>framework designed for big data processing and analytics. Its architecture is built to handle various workloads efficiently, offering speed, scalability, and fault tolerance. Understanding the architecture of Spark is crucial for comprehending its capabilities in processing large volumes <span class="No-Break">of data.</span></p>
			<p>The components of Spark architecture work in collaboration to process data efficiently. The following major components <span class="No-Break">are involved:</span></p>
			<ul>
				<li><span class="No-Break">Spark driver</span></li>
				<li><span class="No-Break">SparkContext</span></li>
				<li><span class="No-Break">Cluster manager</span></li>
				<li><span class="No-Break">Worker node</span></li>
				<li><span class="No-Break">Spark executor</span></li>
				<li><span class="No-Break">Task</span></li>
			</ul>
			<p>Before we talk <a id="_idIndexMarker081"/>about any of these components, it’s important to understand their execution hierarchy to know how each component interacts when a Spark <span class="No-Break">program starts.</span></p>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor056"/>Execution hierarchy</h1>
			<p>Let’s look at <a id="_idIndexMarker082"/>the execution flow of a Spark application with the help of the architecture depicted in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B19176_03_01.jpg" alt="Figure 3.1: Spark architecture"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1: Spark architecture</p>
			<p>These steps outline the flow from submitting a Spark job to freeing up resources when the job <span class="No-Break">is completed:</span></p>
			<ol>
				<li>Spark executions start with a user submitting a <strong class="source-inline">spark-submit</strong> request to the Spark engine. This will create a Spark application. Once an action is performed, it will result in a <strong class="bold">job</strong> <span class="No-Break">being created.</span></li>
				<li>This request will initiate communication with the cluster manager. In turn, the cluster manager initializes the Spark driver to execute the <strong class="source-inline">main()</strong> method of the Spark application. To execute this method, <strong class="source-inline">SparkSession</strong> <span class="No-Break">is created.</span></li>
				<li>The driver starts communicating with the cluster manager and asks for resources <a id="_idIndexMarker083"/>to start planning <span class="No-Break">for execution.</span></li>
				<li>The cluster manager then starts the executors, which can communicate with the <span class="No-Break">driver directly.</span></li>
				<li>The driver <a id="_idIndexMarker084"/>creates a logical plan, known as a <strong class="bold">directed acyclic graph</strong> (<strong class="bold">DAG</strong>), and physical plan for execution based on the total number of tasks required to <span class="No-Break">be executed.</span></li>
				<li>The driver also divides data to be run on each executor, along <span class="No-Break">with tasks.</span></li>
				<li>Once each task finishes running, the driver gets <span class="No-Break">the results.</span></li>
				<li>When the program finishes running, the <strong class="source-inline">main()</strong> method exits and Spark frees all executors and <span class="No-Break">driver resources.</span></li>
			</ol>
			<p>Now that you understand the execution hierarchy, let’s discuss each of Spark’s components <span class="No-Break">in detail.</span></p>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor057"/>Spark components</h1>
			<p>Let’s dive <a id="_idIndexMarker085"/>into the inner workings of each Spark component to understand how each of them plays a crucial role in empowering efficient distributed <span class="No-Break">data processing.</span></p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor058"/>Spark driver</h2>
			<p>The Spark driver is the core of the intelligent and efficient computations in Spark. Spark follows an <a id="_idIndexMarker086"/>architecture that is commonly known as <a id="_idIndexMarker087"/>the <strong class="bold">master-worker architecture</strong> in network topology. Consider the Spark driver as a master and Spark executors as slaves. The driver has <a id="_idIndexMarker088"/>control and knowledge of all the executors at any given time. It is the responsibility of the driver to know how many executors are present and if any executor has failed so that it can fall back on its alternative. The Spark driver also maintains communication with executors all the time. The driver runs on the master node of a machine or cluster. When a Spark application starts running, the driver keeps up with all the required information that is needed to run the <span class="No-Break">application successfully.</span></p>
			<p>As shown in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.1</em>, the driver node contains <strong class="source-inline">SparkSession</strong>, which is the entry point of the Spark application. Previously, this was known as the <strong class="source-inline">SparkContext</strong> object, but in Spark 2.0, <strong class="source-inline">SparkSession</strong> handles all contexts to start execution. The application’s main <a id="_idIndexMarker089"/>method runs on the driver to coordinate the whole application. It runs on its own <strong class="bold">Java Virtual Machine</strong> (<strong class="bold">JVM</strong>). Spark driver can run as an independent process or it can run on one of the worker nodes, depending on <span class="No-Break">the architecture.</span></p>
			<p>The Spark driver is responsible for dividing the application into smaller entities for execution. These <a id="_idIndexMarker090"/>entities are known as <strong class="bold">tasks</strong>. You will learn more about tasks in the upcoming sections of this chapter. The Spark driver also decides what data the executor will work on and what tasks are run on which executor. These tasks are scheduled to run on the executor nodes with the help of the cluster manager. This information that is driven by the driver enables fault tolerance. Since the driver has all the information about the number of available workers and the tasks that are running on each of them alongside data in case a worker fails, that task can be reassigned to a different cluster. Even if a task is taking too long to run, it can be assigned to another executor if that gets free. In that case, whichever executor returns the task earlier would prevail. The Spark <a id="_idIndexMarker091"/>driver also maintains metadata about the <strong class="bold">Resilient Distributed Dataset</strong> (<strong class="bold">RDD</strong>) and <span class="No-Break">its partitions.</span></p>
			<p>It is the responsibility of the Spark driver to design the complete execution map. It determines which tasks run on which executors, as well as how the data is distributed across these executors. This is done by creating RDDs internally. Based on this distribution of data, the operations that are required are determined, such as transformations and actions that are defined in the program. A DAG is created based on these decisions. The Spark driver optimizes the logical plan (DAG) and finds the best possible execution strategy for the DAG, in addition to determining the most optimal location for the execution of a particular task. These executions are done in parallel. The executors simply follow these commands without doing any optimization on <span class="No-Break">their end.</span></p>
			<p>For performance considerations, it is optimal to have the Spark driver work close to the executor. This reduces the latency by a great deal. This means that there would be less delay in the <a id="_idIndexMarker092"/>response time of the processes. Another point to note here is that this is true for the data as well. The executor reading <a id="_idIndexMarker093"/>the data close to it would have better performance than otherwise. Ideally, the <a id="_idIndexMarker094"/>driver and worker nodes should be run in the same <strong class="bold">local area network</strong> (<strong class="bold">LAN</strong>) for the <span class="No-Break">best performance.</span></p>
			<p>The Spark driver also creates a web UI for the execution details. This UI is very helpful in determining the performance of the application. In cases where troubleshooting is required and some bottlenecks need to be identified in the Spark process, this UI is <span class="No-Break">very helpful.</span></p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor059"/>SparkSession</h2>
			<p><strong class="source-inline">SparkSession</strong> is the <a id="_idIndexMarker095"/>main point of entry and interaction with Spark. As discussed <a id="_idIndexMarker096"/>earlier, in the previous versions of Spark, <strong class="source-inline">SparkContext</strong> used to play this role, but in Spark 2.0, <strong class="source-inline">SparkSession</strong> can be created for this purpose. The Spark driver creates a <strong class="source-inline">SparkSession</strong> object to interact with the cluster manager and get resource allocation <span class="No-Break">through it.</span></p>
			<p>In the lifetime of the application, <strong class="source-inline">SparkSession</strong> is also used to interact with all the underlying Spark APIs. We talked about different Spark APIs in <a href="B19176_02.xhtml#_idTextAnchor030"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> namely, SparkSQL, Spark Streaming, MLlib, and GraphX. All of these APIs use <strong class="source-inline">SparkSession</strong> from its core to interact with the <span class="No-Break">Spark application.</span></p>
			<p><strong class="source-inline">SparkSession</strong> keeps track of Spark executors throughout the <span class="No-Break">application’s execution.</span></p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor060"/>Cluster manager</h2>
			<p>Spark is a distributed framework, which requires it to have access to computing resources. This <a id="_idIndexMarker097"/>access is governed and controlled by a process known as the cluster manager. It is the responsibility of the cluster manager to <a id="_idIndexMarker098"/>allocate computing resources for the Spark application when the application execution starts. These resources become available at the request <a id="_idIndexMarker099"/>of the application master. In the Apache Spark ecosystem, the <strong class="bold">application master</strong> plays a crucial role in managing and coordinating the execution of Spark applications within a distributed cluster environment. It’s an essential component that’s responsible for negotiating resources, scheduling tasks, and monitoring the <span class="No-Break">application’s execution.</span></p>
			<p>Once the resources are available, the driver is made aware of those resources. It’s the responsibility of the driver to manage these resources based on tasks that need to be executed by the Spark application. Once the application has finished execution, these resources are released back to the <span class="No-Break">cluster manager.</span></p>
			<p>Applications have their dedicated executor processes that parallelize how tasks are run. The advantage is that each application is independent of the other and runs on its own schedule. Data also becomes independent for each of these applications, so data sharing can only take place by writing data to disk so that it can be shared <span class="No-Break">across applications.</span></p>
			<h3>Cluster modes</h3>
			<p>Cluster modes define how Spark applications utilize cluster resources, manage task execution, and interact with cluster managers for <span class="No-Break">resource allocation.</span></p>
			<p>If there is <a id="_idIndexMarker100"/>more than one user sharing resources on the <a id="_idIndexMarker101"/>cluster, be it Spark applications or other applications that need cluster resources, they have to be managed based on different modes. There are two types of modes available for cluster managers – standalone client mode and cluster mode. The following table highlights some of the differences between <span class="No-Break">the two:</span></p>
			<table id="table001-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Client Mode</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Cluster Mode</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>In client mode, the driver program runs on the machine where the Spark application <span class="No-Break">is submitted.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>In cluster mode, the driver program runs within the cluster, on one of the <span class="No-Break">worker nodes.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>The driver program is responsible for orchestrating the execution of the Spark application, including creating <strong class="source-inline">SparkContext</strong> and <span class="No-Break">coordinating tasks.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>The cluster manager is responsible for launching the driver program and allocating resources <span class="No-Break">for execution.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>The client machine interacts directly with the cluster manager to request resources and launch executors on <span class="No-Break">worker nodes.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Once the driver program is launched, it coordinates with the cluster manager to request resources and distribute tasks to <span class="No-Break">worker nodes.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>It may not be suitable for production deployments with <span class="No-Break">large-scale applications.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>It is commonly used for production deployments as it allows for better resource utilization and scalability. It also ensures <span class="No-Break">fault tolerance.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.1: Client mode versus cluster mode</p>
			<p>Now, we will <a id="_idIndexMarker102"/>talk about different deployment modes and their corresponding <a id="_idIndexMarker103"/>managers <span class="No-Break">in Spark:</span></p>
			<ul>
				<li><strong class="bold">Built-in standalone mode</strong> (<strong class="bold">Spark’s native manager</strong>): A simple cluster manager <a id="_idIndexMarker104"/>bundled with Spark that’s suitable for small to medium-scale deployments without <span class="No-Break">external dependencies.</span></li>
				<li><strong class="bold">Apache YARN</strong> (<strong class="bold">Hadoop’s resource manager</strong>): Integrated with Spark, YARN <a id="_idIndexMarker105"/>enables Spark applications to share Hadoop’s cluster <span class="No-Break">resources efficiently.</span></li>
				<li><strong class="bold">Apache Mesos</strong> (<strong class="bold">resource sharing platform</strong>): Mesos offers efficient resource <a id="_idIndexMarker106"/>sharing across multiple applications, allowing Spark to run alongside <span class="No-Break">other frameworks.</span></li>
			</ul>
			<p>We will talk more about deployment modes later in <span class="No-Break">this chapter.</span></p>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor061"/>Spark executors</h2>
			<p>Spark executors are the processes that run on the worker node and execute tasks sent by the driver. The data is stored in memory primarily but can also be written to disk storage closest to them. Driver launches the executors based on the DAG that Spark generates for <a id="_idIndexMarker107"/>its execution. Once the tasks have finished executing, executors send the results back to <span class="No-Break">the driver.</span></p>
			<p>Since the driver <a id="_idIndexMarker108"/>is the main controller of the Spark application, if an executor fails or takes too long to execute a task, the driver can choose to send that task over to other available executors. This ensures reliability and fault tolerance in Spark. We will read more about this later in <span class="No-Break">this chapter.</span></p>
			<p>It is the responsibility of the executor to read data from external sources that are needed to run the tasks. It can also write its partitioned data to the disk as needed. All processing for a task is done by <span class="No-Break">the executor.</span></p>
			<p>The key functions <a id="_idIndexMarker109"/>of an executor are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Task execution</strong>: Executors run tasks assigned by the Spark application, processing data stored in RDDs <span class="No-Break">or DataFrames</span></li>
				<li><strong class="bold">Resource allocation</strong>: Each Spark application has a set of executors allocated by the cluster manager for managing resources such as CPU cores <span class="No-Break">and memory</span></li>
			</ul>
			<p>In Apache Spark, the concepts of job, stage, and task form the fundamental building blocks of its distributed computing framework. Understanding these components is essential to grasp the core workings of Spark’s parallel processing and task execution. See <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.2</em> to understand the relationship between these concepts while we discuss them <span class="No-Break">in detail:</span></p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B19176_03_02.jpg" alt="Figure 3.2: Interaction between jobs, stages, and tasks"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2: Interaction between jobs, stages, and tasks</p>
			<p>Let’s take a <span class="No-Break">closer look:</span></p>
			<ul>
				<li><strong class="bold">Job</strong>: The Spark <a id="_idIndexMarker110"/>application will initiate multiple jobs when the application starts running. These jobs can be executed in parallel, wherein each job can consist of multiple tasks. A job gets initiated when a Spark action is called (such as <strong class="source-inline">collect</strong>). We will learn more about actions later. When an action (such as <strong class="source-inline">collect</strong> or <strong class="source-inline">count</strong>) is invoked on a dataset, it triggers the execution of one or <span class="No-Break">more jobs.</span><p class="list-inset">A job consists of several stages, each containing tasks that execute a set of transformations on <span class="No-Break">data partitions.</span></p></li>
				<li><strong class="bold">Stage</strong>: Each job <a id="_idIndexMarker111"/>is divided into stages that may depend on other stages. Stages act as transformation boundaries – they are created at the boundaries of wide transformations that require data shuffling across partitions. If a stage is dependent on outputs from a previous stage, then this stage would not begin execution until the previous dependent stages have <span class="No-Break">finished execution.</span><p class="list-inset">Each stage is <a id="_idIndexMarker112"/>divided into a set of tasks to be executed on the cluster nodes, processing data <span class="No-Break">in parallel.</span></p></li>
				<li><strong class="bold">Task</strong>: A task is the smallest unit of execution in Spark. It is the smallest object compiled and <a id="_idIndexMarker113"/>run by Spark to perform a group of operations. It is executed on a Spark executor. Tasks are essentially a series of operations such as filter, groupBy, <span class="No-Break">and others.</span><p class="list-inset">Tasks run in parallel across executors. They can be run on multiple nodes and are independent of each other. This is done with the help of slots. Each task processes a portion of the data partition. Occasionally, a group of these tasks has to finish execution to begin the next <span class="No-Break">task’s execution.</span></p></li>
			</ul>
			<p>Now that we understand these concepts, let’s see why they are significant <span class="No-Break">in Spark:</span></p>
			<ul>
				<li><strong class="bold">Parallel processing</strong>: Executors, jobs, stages, and tasks collaborate to enable parallel <a id="_idIndexMarker114"/>execution of computations, optimizing performance by leveraging <span class="No-Break">distributed computing</span></li>
				<li><strong class="bold">Task granularity and efficiency</strong>: Tasks divide computations into smaller units, facilitating <a id="_idIndexMarker115"/>efficient resource utilization and parallelism <a id="_idIndexMarker116"/>across <span class="No-Break">cluster nodes</span></li>
			</ul>
			<p>Next, we will move on to discuss a significant concept that enhances efficiency <span class="No-Break">in computation.</span></p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor062"/>Partitioning in Spark</h1>
			<p>In Apache Spark, partitioning is a critical concept that’s used to divide data across multiple nodes <a id="_idIndexMarker117"/>in a cluster for parallel processing. Partitioning improves data locality, enhances performance, and enables efficient computation by distributing data in a structured manner. Spark supports both static and dynamic partitioning strategies to organize data across the <span class="No-Break">cluster nodes:</span></p>
			<ul>
				<li><strong class="bold">Static partitioning of resources</strong>: Static partitioning is available on all cluster managers. With static partitioning, maximum resources are allocated to each application and these resources remain dedicated to these applications during <span class="No-Break">their lifetime.</span></li>
				<li><strong class="bold">Dynamic sharing of resources</strong>: Dynamic partitioning is only available on Mesos. When dynamically sharing resources, the Spark application gets fixed and independent memory allocation, such as static partitioning. The major difference is that when the tasks are not being run by an application, these cores can be used by other applications <span class="No-Break">as well.</span></li>
			</ul>
			<p>Let’s discuss <a id="_idIndexMarker118"/>why partitioning <span class="No-Break">is significant:</span></p>
			<ul>
				<li><strong class="bold">Performance optimization</strong>: Effective partitioning strategies, whether static or dynamic, significantly impact Spark’s performance by improving data locality and reducing <span class="No-Break">data shuffle</span></li>
				<li><strong class="bold">Adaptability and flexibility</strong>: Dynamic partitioning provides adaptability to varying data sizes or distribution patterns without <span class="No-Break">manual intervention</span></li>
				<li><strong class="bold">Control and predictability</strong>: Static partitioning offers control and predictability over data distribution, which can be advantageous in specific <span class="No-Break">use cases</span></li>
			</ul>
			<p>In summary, partitioning strategies – whether static or dynamic – in Spark play a crucial role in optimizing data distribution across cluster nodes, improving performance, and ensuring efficient parallel processing <span class="No-Break">of data.</span></p>
			<p>Apache Spark offers different cluster and deployment modes to run applications across distributed computing environments. We’ll take a look at them in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor063"/>Deployment modes</h1>
			<p>There are <a id="_idIndexMarker119"/>different deployment modes available in Spark. These deployment modes define how Spark applications are launched, executed, and managed in diverse computing infrastructures. Based on these different deployment modes, it gets decided where the Spark driver, executor, and cluster manager <span class="No-Break">will run.</span></p>
			<p>The different deployment modes that are available in Spark are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Local</strong>: In local mode, the Spark driver and executor run on a single JVM and the cluster manager runs on the same host as the driver <span class="No-Break">and executor.</span></li>
				<li><strong class="bold">Standalone</strong>: In standalone mode, the driver can run on any node of the cluster and the executor will launch its own independent JVM. The cluster manager can remain on any of the hosts in <span class="No-Break">the cluster.</span></li>
				<li><strong class="bold">YARN (client)</strong>: In this mode, the Spark driver runs on the client and YARN’s resource manager allocates containers for executors <span class="No-Break">on NodeManagers.</span></li>
				<li><strong class="bold">YARN (cluster)</strong>: In this mode, the Spark driver runs with the YARN application master while YARN’s resource manager allocates containers for executors <span class="No-Break">on NodeManagers.</span></li>
				<li><strong class="bold">Kubernetes</strong>: In this mode, the driver runs in Kubernetes pods. Executors have their <span class="No-Break">own pods.</span></li>
			</ul>
			<p>Let’s look at <a id="_idIndexMarker120"/>some points of significance regarding the different <span class="No-Break">deployment modes:</span></p>
			<ul>
				<li><strong class="bold">Resource utilization</strong>: Different deployment modes optimize resource utilization by determining where the driver program runs and how resources are allocated between the client and <span class="No-Break">the cluster.</span></li>
				<li><strong class="bold">Accessibility and control</strong>: Client mode offers easy accessibility to driver logs and outputs, facilitating development and debugging, while cluster mode utilizes cluster resources more efficiently for <span class="No-Break">production workloads.</span></li>
				<li><strong class="bold">Integration with container orchestration</strong>: Kubernetes deployment mode enables seamless integration with containerized environments, leveraging Kubernetes’ orchestration capabilities for efficient <span class="No-Break">resource management.</span></li>
			</ul>
			<p>There are some considerations to keep in mind while choosing <span class="No-Break">deployment modes:</span></p>
			<ul>
				<li><strong class="bold">Development versus production</strong>: Client mode is suitable for development and debugging, while cluster mode is ideal for <span class="No-Break">production workloads</span></li>
				<li><strong class="bold">Resource management</strong>: Evaluate the allocation of resources between client and cluster nodes based on the <span class="No-Break">application’s requirements</span></li>
				<li><strong class="bold">Containerization needs</strong>: Consider Kubernetes deployment for containerized environments, leveraging Kubernetes features for efficient <span class="No-Break">container management</span></li>
			</ul>
			<p>In summary, deployment modes in Apache Spark provide flexibility in how Spark applications <a id="_idIndexMarker121"/>are launched and executed, catering to different development, production, and containerized <span class="No-Break">deployment scenarios.</span></p>
			<p>Next, we will look at RDDs, which serve as foundational data abstractions in Apache Spark, enabling distributed processing, fault tolerance, and flexibility in handling large-scale data operations. While RDDs continue to be a fundamental concept, Spark’s DataFrame and Dataset APIs offer advancements in structured data processing and <span class="No-Break">performance optimization.</span></p>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor064"/>RDDs</h1>
			<p>Apache Spark’s RDD stands as a foundational abstraction that underpins the distributed computing <a id="_idIndexMarker122"/>capabilities within the Spark framework. RDDs serve as the core data structure in Spark, enabling fault-tolerant and parallel operations on large-scale distributed datasets and they are immutable. This means that they cannot be changed over time. For any operations, a new RDD has to be generated from the existing RDD. When a new RDD originates from the original RDD, the new RDD has a pointer to the RDD it is generated from. This is the way Spark documents the lineage for all the transformations <a id="_idIndexMarker123"/>taking place on an RDD. This lineage enables <strong class="bold">lazy evaluation</strong> in Spark, which generates DAGs for <span class="No-Break">different operations.</span></p>
			<p>This immutability and lineage gives Spark the ability to reproduce any DataFrame in case of failure and it makes fault-tolerant by design. Since RDD is the lowest level of abstraction in Spark, all other datasets built on top of RDDs share these properties. The high-level DataFrame API is built on top of the low-level RDD API as well, so DataFrames also share the <span class="No-Break">same properties.</span></p>
			<p>RDDs are also partitioned by Spark and each partition is distributed to multiple nodes in <span class="No-Break">the cluster.</span></p>
			<p>Here are <a id="_idIndexMarker124"/>some of the key characteristics of <span class="No-Break">Spark RDDs:</span></p>
			<ul>
				<li><strong class="bold">Immutable nature</strong>: RDDs are immutable, ensuring that once created, they cannot be altered, allowing for a lineage <span class="No-Break">of transformations.</span></li>
				<li><strong class="bold">Resilience through lineage</strong>: RDDs store lineage information, enabling reconstruction of lost partitions in case of failures. Spark is designed to be fault-tolerant. Therefore, if an executor on a worker node fails while calculating an RDD, that RDD can be recomputed by another executor using the lineage that Spark <span class="No-Break">has created.</span></li>
				<li><strong class="bold">Partitioned data</strong>: RDDs divide data into partitions, distributed across multiple nodes in a cluster for <span class="No-Break">parallel processing.</span></li>
				<li><strong class="bold">Parallel execution</strong>: Spark executes operations on RDDs in parallel across distributed partitions, <span class="No-Break">enhancing performance.</span></li>
			</ul>
			<p>Let’s discuss some more characteristics <span class="No-Break">in detail.</span></p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor065"/>Lazy computation</h2>
			<p>RDDs support lazy evaluation, deferring execution of transformations until an action is invoked. The way Spark achieves its efficiency in processing and fault tolerance is through lazy <a id="_idIndexMarker125"/>evaluation. Code execution in Spark is delayed. Unless an action is called an operation, Spark does not start code <a id="_idIndexMarker126"/>execution. This helps Spark achieve optimization as well. For all the transformations and actions, Spark keeps track of the steps in the code that need to be executed by creating a DAG for these operations. Because Spark creates the query plan before execution, it can make smart decisions about the hierarchy of execution as well. To achieve this, one <a id="_idIndexMarker127"/>of the features Spark uses is called <span class="No-Break"><strong class="bold">predicate pushdown</strong></span><span class="No-Break">.</span></p>
			<p>Predicate pushdown means that Spark can prioritize the operations to make them the most efficient. One example can be a filter operation. A filter operation would generally reduce the amount of data that the subsequent operations have to work with if the filter operation can be applied before other transformations. This is exactly how Spark operates. It will execute filters as early in the process as possible, thus making the next operations <span class="No-Break">more performant.</span></p>
			<p>This also implies <a id="_idIndexMarker128"/>that Spark jobs would fail only at execution time. Since Spark uses lazy evaluation, until an action is called, the code <a id="_idIndexMarker129"/>is not executed and certain errors can be missed. To catch these errors, Spark code would need to have an action for execution and hence <span class="No-Break">error handling.</span></p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor066"/>Transformations</h2>
			<p>Transformations create new RDDs by applying functions to existing RDDs (for example, <strong class="source-inline">map</strong>, <strong class="source-inline">filter</strong>, and <strong class="source-inline">reduce</strong>). Transformations are operations that do not result in any code execution. These statements result in Spark creating a DAG for execution. Once that DAG is <a id="_idIndexMarker130"/>created, Spark would need an action operation in the end to run the code. Due to this, when certain developers try <a id="_idIndexMarker131"/>to time the code from Spark, they see that certain operations’ runtime is very fast. The reason could be that the code is only comprised of transformations until that point. Since no action is present, the code doesn’t run. To accurately measure the runtime of each operation, actions have to be called to force Spark to execute <span class="No-Break">those statements.</span></p>
			<p>Here are <a id="_idIndexMarker132"/>some of the operations that can be classified <span class="No-Break">as transformations:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">orderBy()</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">groupBy()</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">filter()</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">select()</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">join()</strong></span></li>
			</ul>
			<p>When these commands are executed, they are evaluated lazily. This means all these operations on DataFrames result in a new DataFrame, but they are not executed until an action is followed by them. This would return a DataFrame or RDD when it is triggered by <span class="No-Break">an action.</span></p>
			<h3>Actions and computation execution</h3>
			<p>Actions (for example, <strong class="source-inline">collect</strong>, <strong class="source-inline">count</strong>, and <strong class="source-inline">saveAsTextFile</strong>) prompt the execution of <a id="_idIndexMarker133"/>transformations on RDDs. Execution is triggered by <a id="_idIndexMarker134"/>actions only, not by transformations. When an action is called, this is when Spark starts execution on the DAG it created during the analysis phase of <a id="_idIndexMarker135"/>code. With the DAG created, Spark creates multiple <a id="_idIndexMarker136"/>query plans based on its internal optimizations. Then, it executes the plan that is the most efficient and cost-effective. We will discuss query plans later in <span class="No-Break">this book.</span></p>
			<p>Here are <a id="_idIndexMarker137"/>some of the operations that can be classified <span class="No-Break">as actions:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">show()</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">take()</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">count()</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">collect()</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">save()</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">foreach()</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">first()</strong></span></li>
			</ul>
			<p>All of these operations would result in Spark triggering code execution and thus operations <span class="No-Break">are run.</span></p>
			<p>Let’s take a look at the following code to understand these <span class="No-Break">concepts better:</span></p>
			<pre class="source-code">
# Python
&gt;&gt;&gt; df = spark.read.text("{path_to_data_file}")
&gt;&gt;&gt; names_df = df.select(col("firstname"),col("lastname"))
&gt;&gt;&gt; names_df.show()</pre>			<p>In the preceding code, until line 2, nothing would be executed. On line 3, an action is triggered and thus it triggers the whole code execution. Therefore, if you give the wrong data path in line 1 or the wrong column names in line 2, Spark will not detect this until it runs line 3. This is a different paradigm than most other programming paradigms. This is what we call lazy evaluation <span class="No-Break">in Spark.</span></p>
			<p>Actions bring about c<a id="_idIndexMarker138"/>omputation and collect results to be sent to the <span class="No-Break">driver program.</span></p>
			<p>Now that we’ve covered the basics of transformations and actions in Spark, let’s move on to understanding the two types of transformations <span class="No-Break">it offers.</span></p>
			<h3>Types of transformations</h3>
			<p>Apache Spark’s <a id="_idIndexMarker139"/>transformations are broadly categorized into narrow and wide transformations, each serving distinct purposes in the context of distributed <span class="No-Break">data processing.</span></p>
			<h4>Narrow transformations</h4>
			<p>Narrow transformations, also known as local transformations, operate on individual partitions of data without shuffling or redistributing data across partitions. These transformations <a id="_idIndexMarker140"/>enable Spark to process data <a id="_idIndexMarker141"/>within a single partition independently. In narrow transformations, Spark will work with a single input partition and a single output partition. This means that these types of transformations would result in an operation that can be performed on a single partition. The data doesn’t have to be taken from multiple partitions or written back to multiple partitions. This results in operations that don’t <span class="No-Break">require shuffle.</span></p>
			<p>Here are <a id="_idIndexMarker142"/>some of <span class="No-Break">their characteristics:</span></p>
			<ul>
				<li><strong class="bold">Partition-level operation</strong>: Narrow transformations process data at the partition level, performing computations within <span class="No-Break">each partition</span></li>
				<li><strong class="bold">Independence and local processing</strong>: They do not require data movement or communication across partitions, allowing parallel execution <span class="No-Break">within partitions</span></li>
				<li><strong class="bold">Examples</strong>: Operations such as <strong class="source-inline">map</strong>, <strong class="source-inline">filter</strong>, and <strong class="source-inline">flatMap</strong> are typical examples of <span class="No-Break">narrow transformations</span></li>
			</ul>
			<p>Now, let’s look at <span class="No-Break">their significance:</span></p>
			<ul>
				<li><strong class="bold">Efficiency and speed</strong>: Narrow transformations are efficient as they involve local processing within partitions, reducing <span class="No-Break">communication overhead</span></li>
				<li><strong class="bold">Parallelism</strong>: They facilitate maximum parallelism by operating on partitions independently, <span class="No-Break">optimizing performance</span></li>
			</ul>
			<h4>Wide transformations</h4>
			<p>Wide transformations, also termed global or shuffle-dependent transformations, involve operations <a id="_idIndexMarker143"/>that require data shuffling and redistribution across partitions. These transformations involve dependencies between partitions, necessitating data exchange. With wide transformations, Spark will use the data present on multiple <a id="_idIndexMarker144"/>partitions and it could also write back the results to multiple partitions. These transformations would force a shuffle operation, so they are also referred to as <span class="No-Break">shuffle transformations.</span></p>
			<p>Wide transformations are complex operations. They would need to write the results out in between operations if needed and they also have to aggregate data across different machines in <span class="No-Break">certain cases.</span></p>
			<p>Here are <a id="_idIndexMarker145"/>some of <span class="No-Break">their characteristics:</span></p>
			<ul>
				<li><strong class="bold">Data shuffling</strong>: Wide transformations reorganize data across partitions by reshuffling or aggregating data from <span class="No-Break">multiple partitions</span></li>
				<li><strong class="bold">Dependency on multiple partitions</strong>: They depend on data from various partitions, leading to the exchange and reorganization of data across <span class="No-Break">the cluster</span></li>
				<li><strong class="bold">Examples</strong>: Operations such as <strong class="source-inline">groupBy</strong>, <strong class="source-inline">join</strong>, and <strong class="source-inline">sortByKey</strong> are typical examples of <span class="No-Break">wide transformations</span></li>
			</ul>
			<p>Now, let’s look at <span class="No-Break">their significance:</span></p>
			<ul>
				<li><strong class="bold">Network and disk overhead</strong>: Wide transformations introduce network and disk overhead due to data shuffling, <span class="No-Break">impacting performance</span></li>
				<li><strong class="bold">Stage boundary creation</strong>: They define stage boundaries within a Spark job, resulting in distinct stages during <span class="No-Break">job execution</span></li>
			</ul>
			<p>The following are the differences between narrow and <span class="No-Break">wide transformations:</span></p>
			<ul>
				<li><strong class="bold">Data movement</strong>: Narrow transformations process data within partitions locally, minimizing data movement, while wide transformations involve data shuffling <a id="_idIndexMarker146"/>and movement <span class="No-Break">across partitions</span></li>
				<li><strong class="bold">Performance impact</strong>: Narrow transformations typically offer higher performance <a id="_idIndexMarker147"/>due to reduced data movement, whereas wide transformations involve additional overhead due to <span class="No-Break">data shuffling</span></li>
				<li><strong class="bold">Parallelism scope</strong>: Narrow transformations enable maximum parallelism within partitions, while wide transformations might limit parallelism due to dependency on <span class="No-Break">multiple partitions</span></li>
			</ul>
			<p>In Apache Spark, understanding the distinction between narrow and wide transformations is crucial. Narrow transformations excel in local processing within partitions, optimizing performance, while wide transformations, although necessary for certain operations, introduce overhead due to data shuffling and global reorganization <span class="No-Break">across partitions.</span></p>
			<p>Let’s look <a id="_idIndexMarker148"/>at the significance of <span class="No-Break">Spark RDDs:</span></p>
			<ul>
				<li><strong class="bold">Distributed data processing</strong>: RDDs enable distributed processing of large-scale data across a cluster of machines, promoting parallelism <span class="No-Break">and scalability</span></li>
				<li><strong class="bold">Fault tolerance and reliability</strong>: Their immutability and lineage-based recovery ensure fault tolerance and reliability in <span class="No-Break">distributed environments</span></li>
				<li><strong class="bold">Flexibility in operations</strong>: RDDs support a wide array of transformations and actions, allowing diverse data manipulations and <span class="No-Break">processing operations</span></li>
			</ul>
			<h3>Evolution and alternatives</h3>
			<p>While RDDs <a id="_idIndexMarker149"/>remain fundamental, Spark’s DataFrame and Dataset APIs offer optimized, higher-level abstractions suitable for structured data processing <span class="No-Break">and optimization.</span></p>
			<p>Spark RDDs serve as the bedrock of distributed data processing within the Apache Spark framework, providing immutability, fault tolerance, and the foundational structure for performing parallel operations on distributed datasets. Although RDDs are fundamental, Spark’s DataFrame <a id="_idIndexMarker150"/>and Dataset APIs offer advancements in performance and structured data processing, catering to various use cases and preferences within the <span class="No-Break">Spark ecosystem.</span></p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor067"/>Summary</h1>
			<p>In this chapter, we learned about Spark’s architecture and its inner workings. This exploration of Spark’s distributed computing landscape covered different Spark components, such as the Spark driver and <strong class="source-inline">SparkSession</strong>. We also talked about the different types of cluster managers available in Spark. Then, we touched on different types of partitioning regarding Spark and its <span class="No-Break">deployment modes.</span></p>
			<p>Next, we discussed Spark executors, jobs, stages, and tasks and highlighted the differences between them before learning about RDDs and their transformation types, learning more about narrow and <span class="No-Break">wide transformations.</span></p>
			<p>These concepts form the foundation for harnessing Spark’s immense capabilities in distributed data processing <span class="No-Break">and analytics.</span></p>
			<p>In the next chapter, we will discuss Spark DataFrames and their <span class="No-Break">corresponding operations.</span></p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor068"/>Sample questions</h1>
			<p><span class="No-Break"><strong class="bold">Question 1:</strong></span></p>
			<p>What’s true about Spark’s <span class="No-Break">execution hierarchy?</span></p>
			<ol class="margin-left">
				<li class="Alphabets">In Spark’s execution hierarchy, a job may reach multiple <span class="No-Break">stage boundaries.</span></li>
				<li class="Alphabets">In Spark’s execution hierarchy, manifests are one layer <span class="No-Break">above jobs.</span></li>
				<li class="Alphabets"> In Spark’s execution hierarchy, a stage comprises <span class="No-Break">multiple jobs.</span></li>
				<li class="Alphabets">In Spark’s execution hierarchy, executors are the <span class="No-Break">smallest unit.</span></li>
				<li class="Alphabets">In Spark’s execution hierarchy, tasks are one layer <span class="No-Break">above slots.</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 2:</strong></span></p>
			<p>What do <span class="No-Break">executors do?</span></p>
			<ol class="margin-left">
				<li class="Alphabets">Executors host the Spark driver on a <span class="No-Break">worker-node basis.</span></li>
				<li class="Alphabets">Executors are responsible for carrying out work that they get assigned by <span class="No-Break">the driver.</span></li>
				<li class="Alphabets">After the start of the Spark application, executors are launched on a <span class="No-Break">per-task basis.</span></li>
				<li class="Alphabets">Executors are located in slots inside <span class="No-Break">worker nodes.</span></li>
				<li class="Alphabets">The executors’ storage is ephemeral and as such it defers the task of caching data directly to the worker <span class="No-Break">node thread.</span></li>
			</ol>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor069"/>Answers</h2>
			<ol>
				<li>A</li>
				<li>B</li>
			</ol>
		</div>
	

		<div id="_idContainer020" class="Content">
			<h1 id="_idParaDest-70" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor070"/>Part 3: Spark Operations</h1>
			<p>In this part, we will cover Spark DataFrames and their operations, emphasizing their role in structured data processing and analytics. This will include DataFrame creation, manipulation, and various operations such as filtering, aggregations, joins, and groupings, demonstrated through illustrative examples. Then, we will discuss advanced operations and optimization techniques, including broadcast variables, accumulators, and custom partitioning. This part also talks about performance optimization strategies, highlighting the significance of adaptive query execution and offering practical tips for enhancing Spark job performance. Furthermore, we will explore SQL queries in Spark, focusing on its SQL-like querying capabilities and interoperability with the DataFrame API. Examples will illustrate complex data manipulations and analytics through SQL queries <span class="No-Break">in Spark.</span></p>
			<p>This part has the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B19176_04.xhtml#_idTextAnchor071"><em class="italic">Chapter 4</em></a>, <em class="italic">Spark DataFrames and their Operations</em></li>
				<li><a href="B19176_05.xhtml#_idTextAnchor115"><em class="italic">Chapter 5</em></a>, <em class="italic">Advanced Operations and Optimizations in Spark</em></li>
				<li><a href="B19176_06.xhtml#_idTextAnchor164"><em class="italic">Chapter 6</em></a><em class="italic">,</em> <em class="italic">SQL Queries in Spark</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer021">
			</div>
		</div>
		<div>
			<div id="_idContainer022" class="Basic-Graphics-Frame">
			</div>
		</div>
	</body></html>