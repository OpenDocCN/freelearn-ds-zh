<html><head></head><body>
		<div><h1 id="_idParaDest-53" class="chapter-number"><a id="_idTextAnchor053"/>3</h1>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor054"/>Spark Architecture and Transformations</h1>
			<p>Spark approaches data processing differently than traditional tools and technologies. To understand Spark’s unique approach, we will have to understand its basic architecture. A deep dive into Spark’s architecture and its components will give you an idea of how Spark achieves its ground-breaking processing speeds for big data analytics.</p>
			<p>In this chapter, you will learn about the following broader topics:</p>
			<ul>
				<li>Spark architecture and execution hierarchy</li>
				<li>Different Spark components</li>
				<li>The roles of the Spark driver and Spark executor</li>
				<li>Different deployment modes in Spark</li>
				<li>Transformations and actions as Spark operations</li>
			</ul>
			<p>By the end of this chapter, you will have valuable insights into Spark’s inner workings and know how to apply this knowledge effectively for your certification test.</p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor055"/>Spark architecture</h1>
			<p>In the previous chapters, we discussed that Apache Spark is an open source, distributed computing <a id="_idIndexMarker080"/>framework designed for big data processing and analytics. Its architecture is built to handle various workloads efficiently, offering speed, scalability, and fault tolerance. Understanding the architecture of Spark is crucial for comprehending its capabilities in processing large volumes of data.</p>
			<p>The components of Spark architecture work in collaboration to process data efficiently. The following major components are involved:</p>
			<ul>
				<li>Spark driver</li>
				<li>SparkContext</li>
				<li>Cluster manager</li>
				<li>Worker node</li>
				<li>Spark executor</li>
				<li>Task</li>
			</ul>
			<p>Before we talk <a id="_idIndexMarker081"/>about any of these components, it’s important to understand their execution hierarchy to know how each component interacts when a Spark program starts.</p>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor056"/>Execution hierarchy</h1>
			<p>Let’s look at <a id="_idIndexMarker082"/>the execution flow of a Spark application with the help of the architecture depicted in <em class="italic">Figure 3</em><em class="italic">.1</em>:</p>
			<div><div><img src="img/B19176_03_01.jpg" alt="Figure 3.1: Spark architecture"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1: Spark architecture</p>
			<p>These steps outline the flow from submitting a Spark job to freeing up resources when the job is completed:</p>
			<ol>
				<li>Spark executions start with a user submitting a <code>spark-submit</code> request to the Spark engine. This will create a Spark application. Once an action is performed, it will result in a <strong class="bold">job</strong> being created.</li>
				<li>This request will initiate communication with the cluster manager. In turn, the cluster manager initializes the Spark driver to execute the <code>main()</code> method of the Spark application. To execute this method, <code>SparkSession</code> is created.</li>
				<li>The driver starts communicating with the cluster manager and asks for resources <a id="_idIndexMarker083"/>to start planning for execution.</li>
				<li>The cluster manager then starts the executors, which can communicate with the driver directly.</li>
				<li>The driver <a id="_idIndexMarker084"/>creates a logical plan, known as a <strong class="bold">directed acyclic graph</strong> (<strong class="bold">DAG</strong>), and physical plan for execution based on the total number of tasks required to be executed.</li>
				<li>The driver also divides data to be run on each executor, along with tasks.</li>
				<li>Once each task finishes running, the driver gets the results.</li>
				<li>When the program finishes running, the <code>main()</code> method exits and Spark frees all executors and driver resources.</li>
			</ol>
			<p>Now that you understand the execution hierarchy, let’s discuss each of Spark’s components in detail.</p>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor057"/>Spark components</h1>
			<p>Let’s dive <a id="_idIndexMarker085"/>into the inner workings of each Spark component to understand how each of them plays a crucial role in empowering efficient distributed data processing.</p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor058"/>Spark driver</h2>
			<p>The Spark driver is the core of the intelligent and efficient computations in Spark. Spark follows an <a id="_idIndexMarker086"/>architecture that is commonly known as <a id="_idIndexMarker087"/>the <strong class="bold">master-worker architecture</strong> in network topology. Consider the Spark driver as a master and Spark executors as slaves. The driver has <a id="_idIndexMarker088"/>control and knowledge of all the executors at any given time. It is the responsibility of the driver to know how many executors are present and if any executor has failed so that it can fall back on its alternative. The Spark driver also maintains communication with executors all the time. The driver runs on the master node of a machine or cluster. When a Spark application starts running, the driver keeps up with all the required information that is needed to run the application successfully.</p>
			<p>As shown in <em class="italic">Figure 3</em><em class="italic">.1</em>, the driver node contains <code>SparkSession</code>, which is the entry point of the Spark application. Previously, this was known as the <code>SparkContext</code> object, but in Spark 2.0, <code>SparkSession</code> handles all contexts to start execution. The application’s main <a id="_idIndexMarker089"/>method runs on the driver to coordinate the whole application. It runs on its own <strong class="bold">Java Virtual Machine</strong> (<strong class="bold">JVM</strong>). Spark driver can run as an independent process or it can run on one of the worker nodes, depending on the architecture.</p>
			<p>The Spark driver is responsible for dividing the application into smaller entities for execution. These <a id="_idIndexMarker090"/>entities are known as <strong class="bold">tasks</strong>. You will learn more about tasks in the upcoming sections of this chapter. The Spark driver also decides what data the executor will work on and what tasks are run on which executor. These tasks are scheduled to run on the executor nodes with the help of the cluster manager. This information that is driven by the driver enables fault tolerance. Since the driver has all the information about the number of available workers and the tasks that are running on each of them alongside data in case a worker fails, that task can be reassigned to a different cluster. Even if a task is taking too long to run, it can be assigned to another executor if that gets free. In that case, whichever executor returns the task earlier would prevail. The Spark <a id="_idIndexMarker091"/>driver also maintains metadata about the <strong class="bold">Resilient Distributed Dataset</strong> (<strong class="bold">RDD</strong>) and its partitions.</p>
			<p>It is the responsibility of the Spark driver to design the complete execution map. It determines which tasks run on which executors, as well as how the data is distributed across these executors. This is done by creating RDDs internally. Based on this distribution of data, the operations that are required are determined, such as transformations and actions that are defined in the program. A DAG is created based on these decisions. The Spark driver optimizes the logical plan (DAG) and finds the best possible execution strategy for the DAG, in addition to determining the most optimal location for the execution of a particular task. These executions are done in parallel. The executors simply follow these commands without doing any optimization on their end.</p>
			<p>For performance considerations, it is optimal to have the Spark driver work close to the executor. This reduces the latency by a great deal. This means that there would be less delay in the <a id="_idIndexMarker092"/>response time of the processes. Another point to note here is that this is true for the data as well. The executor reading <a id="_idIndexMarker093"/>the data close to it would have better performance than otherwise. Ideally, the <a id="_idIndexMarker094"/>driver and worker nodes should be run in the same <strong class="bold">local area network</strong> (<strong class="bold">LAN</strong>) for the best performance.</p>
			<p>The Spark driver also creates a web UI for the execution details. This UI is very helpful in determining the performance of the application. In cases where troubleshooting is required and some bottlenecks need to be identified in the Spark process, this UI is very helpful.</p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor059"/>SparkSession</h2>
			<p><code>SparkSession</code> is the <a id="_idIndexMarker095"/>main point of entry and interaction with Spark. As discussed <a id="_idIndexMarker096"/>earlier, in the previous versions of Spark, <code>SparkContext</code> used to play this role, but in Spark 2.0, <code>SparkSession</code> can be created for this purpose. The Spark driver creates a <code>SparkSession</code> object to interact with the cluster manager and get resource allocation through it.</p>
			<p>In the lifetime of the application, <code>SparkSession</code> is also used to interact with all the underlying Spark APIs. We talked about different Spark APIs in <a href="B19176_02.xhtml#_idTextAnchor030"><em class="italic">Chapter 2</em></a> namely, SparkSQL, Spark Streaming, MLlib, and GraphX. All of these APIs use <code>SparkSession</code> from its core to interact with the Spark application.</p>
			<p><code>SparkSession</code> keeps track of Spark executors throughout the application’s execution.</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor060"/>Cluster manager</h2>
			<p>Spark is a distributed framework, which requires it to have access to computing resources. This <a id="_idIndexMarker097"/>access is governed and controlled by a process known as the cluster manager. It is the responsibility of the cluster manager to <a id="_idIndexMarker098"/>allocate computing resources for the Spark application when the application execution starts. These resources become available at the request <a id="_idIndexMarker099"/>of the application master. In the Apache Spark ecosystem, the <strong class="bold">application master</strong> plays a crucial role in managing and coordinating the execution of Spark applications within a distributed cluster environment. It’s an essential component that’s responsible for negotiating resources, scheduling tasks, and monitoring the application’s execution.</p>
			<p>Once the resources are available, the driver is made aware of those resources. It’s the responsibility of the driver to manage these resources based on tasks that need to be executed by the Spark application. Once the application has finished execution, these resources are released back to the cluster manager.</p>
			<p>Applications have their dedicated executor processes that parallelize how tasks are run. The advantage is that each application is independent of the other and runs on its own schedule. Data also becomes independent for each of these applications, so data sharing can only take place by writing data to disk so that it can be shared across applications.</p>
			<h3>Cluster modes</h3>
			<p>Cluster modes define how Spark applications utilize cluster resources, manage task execution, and interact with cluster managers for resource allocation.</p>
			<p>If there is <a id="_idIndexMarker100"/>more than one user sharing resources on the <a id="_idIndexMarker101"/>cluster, be it Spark applications or other applications that need cluster resources, they have to be managed based on different modes. There are two types of modes available for cluster managers – standalone client mode and cluster mode. The following table highlights some of the differences between the two:</p>
			<table id="table001-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Client Mode</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Cluster Mode</strong></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>In client mode, the driver program runs on the machine where the Spark application is submitted.</p>
						</td>
						<td class="No-Table-Style">
							<p>In cluster mode, the driver program runs within the cluster, on one of the worker nodes.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>The driver program is responsible for orchestrating the execution of the Spark application, including creating <code>SparkContext</code> and coordinating tasks.</p>
						</td>
						<td class="No-Table-Style">
							<p>The cluster manager is responsible for launching the driver program and allocating resources for execution.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>The client machine interacts directly with the cluster manager to request resources and launch executors on worker nodes.</p>
						</td>
						<td class="No-Table-Style">
							<p>Once the driver program is launched, it coordinates with the cluster manager to request resources and distribute tasks to worker nodes.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>It may not be suitable for production deployments with large-scale applications.</p>
						</td>
						<td class="No-Table-Style">
							<p>It is commonly used for production deployments as it allows for better resource utilization and scalability. It also ensures fault tolerance.</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.1: Client mode versus cluster mode</p>
			<p>Now, we will <a id="_idIndexMarker102"/>talk about different deployment modes and their corresponding <a id="_idIndexMarker103"/>managers in Spark:</p>
			<ul>
				<li><strong class="bold">Built-in standalone mode</strong> (<strong class="bold">Spark’s native manager</strong>): A simple cluster manager <a id="_idIndexMarker104"/>bundled with Spark that’s suitable for small to medium-scale deployments without external dependencies.</li>
				<li><strong class="bold">Apache YARN</strong> (<strong class="bold">Hadoop’s resource manager</strong>): Integrated with Spark, YARN <a id="_idIndexMarker105"/>enables Spark applications to share Hadoop’s cluster resources efficiently.</li>
				<li><strong class="bold">Apache Mesos</strong> (<strong class="bold">resource sharing platform</strong>): Mesos offers efficient resource <a id="_idIndexMarker106"/>sharing across multiple applications, allowing Spark to run alongside other frameworks.</li>
			</ul>
			<p>We will talk more about deployment modes later in this chapter.</p>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor061"/>Spark executors</h2>
			<p>Spark executors are the processes that run on the worker node and execute tasks sent by the driver. The data is stored in memory primarily but can also be written to disk storage closest to them. Driver launches the executors based on the DAG that Spark generates for <a id="_idIndexMarker107"/>its execution. Once the tasks have finished executing, executors send the results back to the driver.</p>
			<p>Since the driver <a id="_idIndexMarker108"/>is the main controller of the Spark application, if an executor fails or takes too long to execute a task, the driver can choose to send that task over to other available executors. This ensures reliability and fault tolerance in Spark. We will read more about this later in this chapter.</p>
			<p>It is the responsibility of the executor to read data from external sources that are needed to run the tasks. It can also write its partitioned data to the disk as needed. All processing for a task is done by the executor.</p>
			<p>The key functions <a id="_idIndexMarker109"/>of an executor are as follows:</p>
			<ul>
				<li><strong class="bold">Task execution</strong>: Executors run tasks assigned by the Spark application, processing data stored in RDDs or DataFrames</li>
				<li><strong class="bold">Resource allocation</strong>: Each Spark application has a set of executors allocated by the cluster manager for managing resources such as CPU cores and memory</li>
			</ul>
			<p>In Apache Spark, the concepts of job, stage, and task form the fundamental building blocks of its distributed computing framework. Understanding these components is essential to grasp the core workings of Spark’s parallel processing and task execution. See <em class="italic">Figure 3</em><em class="italic">.2</em> to understand the relationship between these concepts while we discuss them in detail:</p>
			<div><div><img src="img/B19176_03_02.jpg" alt="Figure 3.2: Interaction between jobs, stages, and tasks"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2: Interaction between jobs, stages, and tasks</p>
			<p>Let’s take a closer look:</p>
			<ul>
				<li><code>collect</code>). We will learn more about actions later. When an action (such as <code>collect</code> or <code>count</code>) is invoked on a dataset, it triggers the execution of one or more jobs.<p class="list-inset">A job consists of several stages, each containing tasks that execute a set of transformations on data partitions.</p></li>
				<li><strong class="bold">Stage</strong>: Each job <a id="_idIndexMarker111"/>is divided into stages that may depend on other stages. Stages act as transformation boundaries – they are created at the boundaries of wide transformations that require data shuffling across partitions. If a stage is dependent on outputs from a previous stage, then this stage would not begin execution until the previous dependent stages have finished execution.<p class="list-inset">Each stage is <a id="_idIndexMarker112"/>divided into a set of tasks to be executed on the cluster nodes, processing data in parallel.</p></li>
				<li><strong class="bold">Task</strong>: A task is the smallest unit of execution in Spark. It is the smallest object compiled and <a id="_idIndexMarker113"/>run by Spark to perform a group of operations. It is executed on a Spark executor. Tasks are essentially a series of operations such as filter, groupBy, and others.<p class="list-inset">Tasks run in parallel across executors. They can be run on multiple nodes and are independent of each other. This is done with the help of slots. Each task processes a portion of the data partition. Occasionally, a group of these tasks has to finish execution to begin the next task’s execution.</p></li>
			</ul>
			<p>Now that we understand these concepts, let’s see why they are significant in Spark:</p>
			<ul>
				<li><strong class="bold">Parallel processing</strong>: Executors, jobs, stages, and tasks collaborate to enable parallel <a id="_idIndexMarker114"/>execution of computations, optimizing performance by leveraging distributed computing</li>
				<li><strong class="bold">Task granularity and efficiency</strong>: Tasks divide computations into smaller units, facilitating <a id="_idIndexMarker115"/>efficient resource utilization and parallelism <a id="_idIndexMarker116"/>across cluster nodes</li>
			</ul>
			<p>Next, we will move on to discuss a significant concept that enhances efficiency in computation.</p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor062"/>Partitioning in Spark</h1>
			<p>In Apache Spark, partitioning is a critical concept that’s used to divide data across multiple nodes <a id="_idIndexMarker117"/>in a cluster for parallel processing. Partitioning improves data locality, enhances performance, and enables efficient computation by distributing data in a structured manner. Spark supports both static and dynamic partitioning strategies to organize data across the cluster nodes:</p>
			<ul>
				<li><strong class="bold">Static partitioning of resources</strong>: Static partitioning is available on all cluster managers. With static partitioning, maximum resources are allocated to each application and these resources remain dedicated to these applications during their lifetime.</li>
				<li><strong class="bold">Dynamic sharing of resources</strong>: Dynamic partitioning is only available on Mesos. When dynamically sharing resources, the Spark application gets fixed and independent memory allocation, such as static partitioning. The major difference is that when the tasks are not being run by an application, these cores can be used by other applications as well.</li>
			</ul>
			<p>Let’s discuss <a id="_idIndexMarker118"/>why partitioning is significant:</p>
			<ul>
				<li><strong class="bold">Performance optimization</strong>: Effective partitioning strategies, whether static or dynamic, significantly impact Spark’s performance by improving data locality and reducing data shuffle</li>
				<li><strong class="bold">Adaptability and flexibility</strong>: Dynamic partitioning provides adaptability to varying data sizes or distribution patterns without manual intervention</li>
				<li><strong class="bold">Control and predictability</strong>: Static partitioning offers control and predictability over data distribution, which can be advantageous in specific use cases</li>
			</ul>
			<p>In summary, partitioning strategies – whether static or dynamic – in Spark play a crucial role in optimizing data distribution across cluster nodes, improving performance, and ensuring efficient parallel processing of data.</p>
			<p>Apache Spark offers different cluster and deployment modes to run applications across distributed computing environments. We’ll take a look at them in the next section.</p>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor063"/>Deployment modes</h1>
			<p>There are <a id="_idIndexMarker119"/>different deployment modes available in Spark. These deployment modes define how Spark applications are launched, executed, and managed in diverse computing infrastructures. Based on these different deployment modes, it gets decided where the Spark driver, executor, and cluster manager will run.</p>
			<p>The different deployment modes that are available in Spark are as follows:</p>
			<ul>
				<li><strong class="bold">Local</strong>: In local mode, the Spark driver and executor run on a single JVM and the cluster manager runs on the same host as the driver and executor.</li>
				<li><strong class="bold">Standalone</strong>: In standalone mode, the driver can run on any node of the cluster and the executor will launch its own independent JVM. The cluster manager can remain on any of the hosts in the cluster.</li>
				<li><strong class="bold">YARN (client)</strong>: In this mode, the Spark driver runs on the client and YARN’s resource manager allocates containers for executors on NodeManagers.</li>
				<li><strong class="bold">YARN (cluster)</strong>: In this mode, the Spark driver runs with the YARN application master while YARN’s resource manager allocates containers for executors on NodeManagers.</li>
				<li><strong class="bold">Kubernetes</strong>: In this mode, the driver runs in Kubernetes pods. Executors have their own pods.</li>
			</ul>
			<p>Let’s look at <a id="_idIndexMarker120"/>some points of significance regarding the different deployment modes:</p>
			<ul>
				<li><strong class="bold">Resource utilization</strong>: Different deployment modes optimize resource utilization by determining where the driver program runs and how resources are allocated between the client and the cluster.</li>
				<li><strong class="bold">Accessibility and control</strong>: Client mode offers easy accessibility to driver logs and outputs, facilitating development and debugging, while cluster mode utilizes cluster resources more efficiently for production workloads.</li>
				<li><strong class="bold">Integration with container orchestration</strong>: Kubernetes deployment mode enables seamless integration with containerized environments, leveraging Kubernetes’ orchestration capabilities for efficient resource management.</li>
			</ul>
			<p>There are some considerations to keep in mind while choosing deployment modes:</p>
			<ul>
				<li><strong class="bold">Development versus production</strong>: Client mode is suitable for development and debugging, while cluster mode is ideal for production workloads</li>
				<li><strong class="bold">Resource management</strong>: Evaluate the allocation of resources between client and cluster nodes based on the application’s requirements</li>
				<li><strong class="bold">Containerization needs</strong>: Consider Kubernetes deployment for containerized environments, leveraging Kubernetes features for efficient container management</li>
			</ul>
			<p>In summary, deployment modes in Apache Spark provide flexibility in how Spark applications <a id="_idIndexMarker121"/>are launched and executed, catering to different development, production, and containerized deployment scenarios.</p>
			<p>Next, we will look at RDDs, which serve as foundational data abstractions in Apache Spark, enabling distributed processing, fault tolerance, and flexibility in handling large-scale data operations. While RDDs continue to be a fundamental concept, Spark’s DataFrame and Dataset APIs offer advancements in structured data processing and performance optimization.</p>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor064"/>RDDs</h1>
			<p>Apache Spark’s RDD stands as a foundational abstraction that underpins the distributed computing <a id="_idIndexMarker122"/>capabilities within the Spark framework. RDDs serve as the core data structure in Spark, enabling fault-tolerant and parallel operations on large-scale distributed datasets and they are immutable. This means that they cannot be changed over time. For any operations, a new RDD has to be generated from the existing RDD. When a new RDD originates from the original RDD, the new RDD has a pointer to the RDD it is generated from. This is the way Spark documents the lineage for all the transformations <a id="_idIndexMarker123"/>taking place on an RDD. This lineage enables <strong class="bold">lazy evaluation</strong> in Spark, which generates DAGs for different operations.</p>
			<p>This immutability and lineage gives Spark the ability to reproduce any DataFrame in case of failure and it makes fault-tolerant by design. Since RDD is the lowest level of abstraction in Spark, all other datasets built on top of RDDs share these properties. The high-level DataFrame API is built on top of the low-level RDD API as well, so DataFrames also share the same properties.</p>
			<p>RDDs are also partitioned by Spark and each partition is distributed to multiple nodes in the cluster.</p>
			<p>Here are <a id="_idIndexMarker124"/>some of the key characteristics of Spark RDDs:</p>
			<ul>
				<li><strong class="bold">Immutable nature</strong>: RDDs are immutable, ensuring that once created, they cannot be altered, allowing for a lineage of transformations.</li>
				<li><strong class="bold">Resilience through lineage</strong>: RDDs store lineage information, enabling reconstruction of lost partitions in case of failures. Spark is designed to be fault-tolerant. Therefore, if an executor on a worker node fails while calculating an RDD, that RDD can be recomputed by another executor using the lineage that Spark has created.</li>
				<li><strong class="bold">Partitioned data</strong>: RDDs divide data into partitions, distributed across multiple nodes in a cluster for parallel processing.</li>
				<li><strong class="bold">Parallel execution</strong>: Spark executes operations on RDDs in parallel across distributed partitions, enhancing performance.</li>
			</ul>
			<p>Let’s discuss some more characteristics in detail.</p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor065"/>Lazy computation</h2>
			<p>RDDs support lazy evaluation, deferring execution of transformations until an action is invoked. The way Spark achieves its efficiency in processing and fault tolerance is through lazy <a id="_idIndexMarker125"/>evaluation. Code execution in Spark is delayed. Unless an action is called an operation, Spark does not start code <a id="_idIndexMarker126"/>execution. This helps Spark achieve optimization as well. For all the transformations and actions, Spark keeps track of the steps in the code that need to be executed by creating a DAG for these operations. Because Spark creates the query plan before execution, it can make smart decisions about the hierarchy of execution as well. To achieve this, one <a id="_idIndexMarker127"/>of the features Spark uses is called <strong class="bold">predicate pushdown</strong>.</p>
			<p>Predicate pushdown means that Spark can prioritize the operations to make them the most efficient. One example can be a filter operation. A filter operation would generally reduce the amount of data that the subsequent operations have to work with if the filter operation can be applied before other transformations. This is exactly how Spark operates. It will execute filters as early in the process as possible, thus making the next operations more performant.</p>
			<p>This also implies <a id="_idIndexMarker128"/>that Spark jobs would fail only at execution time. Since Spark uses lazy evaluation, until an action is called, the code <a id="_idIndexMarker129"/>is not executed and certain errors can be missed. To catch these errors, Spark code would need to have an action for execution and hence error handling.</p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor066"/>Transformations</h2>
			<p>Transformations create new RDDs by applying functions to existing RDDs (for example, <code>map</code>, <code>filter</code>, and <code>reduce</code>). Transformations are operations that do not result in any code execution. These statements result in Spark creating a DAG for execution. Once that DAG is <a id="_idIndexMarker130"/>created, Spark would need an action operation in the end to run the code. Due to this, when certain developers try <a id="_idIndexMarker131"/>to time the code from Spark, they see that certain operations’ runtime is very fast. The reason could be that the code is only comprised of transformations until that point. Since no action is present, the code doesn’t run. To accurately measure the runtime of each operation, actions have to be called to force Spark to execute those statements.</p>
			<p>Here are <a id="_idIndexMarker132"/>some of the operations that can be classified as transformations:</p>
			<ul>
				<li><code>orderBy()</code></li>
				<li><code>groupBy()</code></li>
				<li><code>filter()</code></li>
				<li><code>select()</code></li>
				<li><code>join()</code></li>
			</ul>
			<p>When these commands are executed, they are evaluated lazily. This means all these operations on DataFrames result in a new DataFrame, but they are not executed until an action is followed by them. This would return a DataFrame or RDD when it is triggered by an action.</p>
			<h3>Actions and computation execution</h3>
			<p>Actions (for example, <code>collect</code>, <code>count</code>, and <code>saveAsTextFile</code>) prompt the execution of <a id="_idIndexMarker133"/>transformations on RDDs. Execution is triggered by <a id="_idIndexMarker134"/>actions only, not by transformations. When an action is called, this is when Spark starts execution on the DAG it created during the analysis phase of <a id="_idIndexMarker135"/>code. With the DAG created, Spark creates multiple <a id="_idIndexMarker136"/>query plans based on its internal optimizations. Then, it executes the plan that is the most efficient and cost-effective. We will discuss query plans later in this book.</p>
			<p>Here are <a id="_idIndexMarker137"/>some of the operations that can be classified as actions:</p>
			<ul>
				<li><code>show()</code></li>
				<li><code>take()</code></li>
				<li><code>count()</code></li>
				<li><code>collect()</code></li>
				<li><code>save()</code></li>
				<li><code>foreach()</code></li>
				<li><code>first()</code></li>
			</ul>
			<p>All of these operations would result in Spark triggering code execution and thus operations are run.</p>
			<p>Let’s take a look at the following code to understand these concepts better:</p>
			<pre class="source-code">
# Python
&gt;&gt;&gt; df = spark.read.text("{path_to_data_file}")
&gt;&gt;&gt; names_df = df.select(col("firstname"),col("lastname"))
&gt;&gt;&gt; names_df.show()</pre>			<p>In the preceding code, until line 2, nothing would be executed. On line 3, an action is triggered and thus it triggers the whole code execution. Therefore, if you give the wrong data path in line 1 or the wrong column names in line 2, Spark will not detect this until it runs line 3. This is a different paradigm than most other programming paradigms. This is what we call lazy evaluation in Spark.</p>
			<p>Actions bring about c<a id="_idIndexMarker138"/>omputation and collect results to be sent to the driver program.</p>
			<p>Now that we’ve covered the basics of transformations and actions in Spark, let’s move on to understanding the two types of transformations it offers.</p>
			<h3>Types of transformations</h3>
			<p>Apache Spark’s <a id="_idIndexMarker139"/>transformations are broadly categorized into narrow and wide transformations, each serving distinct purposes in the context of distributed data processing.</p>
			<h4>Narrow transformations</h4>
			<p>Narrow transformations, also known as local transformations, operate on individual partitions of data without shuffling or redistributing data across partitions. These transformations <a id="_idIndexMarker140"/>enable Spark to process data <a id="_idIndexMarker141"/>within a single partition independently. In narrow transformations, Spark will work with a single input partition and a single output partition. This means that these types of transformations would result in an operation that can be performed on a single partition. The data doesn’t have to be taken from multiple partitions or written back to multiple partitions. This results in operations that don’t require shuffle.</p>
			<p>Here are <a id="_idIndexMarker142"/>some of their characteristics:</p>
			<ul>
				<li><strong class="bold">Partition-level operation</strong>: Narrow transformations process data at the partition level, performing computations within each partition</li>
				<li><strong class="bold">Independence and local processing</strong>: They do not require data movement or communication across partitions, allowing parallel execution within partitions</li>
				<li><code>map</code>, <code>filter</code>, and <code>flatMap</code> are typical examples of narrow transformations</li>
			</ul>
			<p>Now, let’s look at their significance:</p>
			<ul>
				<li><strong class="bold">Efficiency and speed</strong>: Narrow transformations are efficient as they involve local processing within partitions, reducing communication overhead</li>
				<li><strong class="bold">Parallelism</strong>: They facilitate maximum parallelism by operating on partitions independently, optimizing performance</li>
			</ul>
			<h4>Wide transformations</h4>
			<p>Wide transformations, also termed global or shuffle-dependent transformations, involve operations <a id="_idIndexMarker143"/>that require data shuffling and redistribution across partitions. These transformations involve dependencies between partitions, necessitating data exchange. With wide transformations, Spark will use the data present on multiple <a id="_idIndexMarker144"/>partitions and it could also write back the results to multiple partitions. These transformations would force a shuffle operation, so they are also referred to as shuffle transformations.</p>
			<p>Wide transformations are complex operations. They would need to write the results out in between operations if needed and they also have to aggregate data across different machines in certain cases.</p>
			<p>Here are <a id="_idIndexMarker145"/>some of their characteristics:</p>
			<ul>
				<li><strong class="bold">Data shuffling</strong>: Wide transformations reorganize data across partitions by reshuffling or aggregating data from multiple partitions</li>
				<li><strong class="bold">Dependency on multiple partitions</strong>: They depend on data from various partitions, leading to the exchange and reorganization of data across the cluster</li>
				<li><code>groupBy</code>, <code>join</code>, and <code>sortByKey</code> are typical examples of wide transformations</li>
			</ul>
			<p>Now, let’s look at their significance:</p>
			<ul>
				<li><strong class="bold">Network and disk overhead</strong>: Wide transformations introduce network and disk overhead due to data shuffling, impacting performance</li>
				<li><strong class="bold">Stage boundary creation</strong>: They define stage boundaries within a Spark job, resulting in distinct stages during job execution</li>
			</ul>
			<p>The following are the differences between narrow and wide transformations:</p>
			<ul>
				<li><strong class="bold">Data movement</strong>: Narrow transformations process data within partitions locally, minimizing data movement, while wide transformations involve data shuffling <a id="_idIndexMarker146"/>and movement across partitions</li>
				<li><strong class="bold">Performance impact</strong>: Narrow transformations typically offer higher performance <a id="_idIndexMarker147"/>due to reduced data movement, whereas wide transformations involve additional overhead due to data shuffling</li>
				<li><strong class="bold">Parallelism scope</strong>: Narrow transformations enable maximum parallelism within partitions, while wide transformations might limit parallelism due to dependency on multiple partitions</li>
			</ul>
			<p>In Apache Spark, understanding the distinction between narrow and wide transformations is crucial. Narrow transformations excel in local processing within partitions, optimizing performance, while wide transformations, although necessary for certain operations, introduce overhead due to data shuffling and global reorganization across partitions.</p>
			<p>Let’s look <a id="_idIndexMarker148"/>at the significance of Spark RDDs:</p>
			<ul>
				<li><strong class="bold">Distributed data processing</strong>: RDDs enable distributed processing of large-scale data across a cluster of machines, promoting parallelism and scalability</li>
				<li><strong class="bold">Fault tolerance and reliability</strong>: Their immutability and lineage-based recovery ensure fault tolerance and reliability in distributed environments</li>
				<li><strong class="bold">Flexibility in operations</strong>: RDDs support a wide array of transformations and actions, allowing diverse data manipulations and processing operations</li>
			</ul>
			<h3>Evolution and alternatives</h3>
			<p>While RDDs <a id="_idIndexMarker149"/>remain fundamental, Spark’s DataFrame and Dataset APIs offer optimized, higher-level abstractions suitable for structured data processing and optimization.</p>
			<p>Spark RDDs serve as the bedrock of distributed data processing within the Apache Spark framework, providing immutability, fault tolerance, and the foundational structure for performing parallel operations on distributed datasets. Although RDDs are fundamental, Spark’s DataFrame <a id="_idIndexMarker150"/>and Dataset APIs offer advancements in performance and structured data processing, catering to various use cases and preferences within the Spark ecosystem.</p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor067"/>Summary</h1>
			<p>In this chapter, we learned about Spark’s architecture and its inner workings. This exploration of Spark’s distributed computing landscape covered different Spark components, such as the Spark driver and <code>SparkSession</code>. We also talked about the different types of cluster managers available in Spark. Then, we touched on different types of partitioning regarding Spark and its deployment modes.</p>
			<p>Next, we discussed Spark executors, jobs, stages, and tasks and highlighted the differences between them before learning about RDDs and their transformation types, learning more about narrow and wide transformations.</p>
			<p>These concepts form the foundation for harnessing Spark’s immense capabilities in distributed data processing and analytics.</p>
			<p>In the next chapter, we will discuss Spark DataFrames and their corresponding operations.</p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor068"/>Sample questions</h1>
			<p><strong class="bold">Question 1:</strong></p>
			<p>What’s true about Spark’s execution hierarchy?</p>
			<ol class="margin-left">
				<li class="Alphabets">In Spark’s execution hierarchy, a job may reach multiple stage boundaries.</li>
				<li class="Alphabets">In Spark’s execution hierarchy, manifests are one layer above jobs.</li>
				<li class="Alphabets"> In Spark’s execution hierarchy, a stage comprises multiple jobs.</li>
				<li class="Alphabets">In Spark’s execution hierarchy, executors are the smallest unit.</li>
				<li class="Alphabets">In Spark’s execution hierarchy, tasks are one layer above slots.</li>
			</ol>
			<p><strong class="bold">Question 2:</strong></p>
			<p>What do executors do?</p>
			<ol class="margin-left">
				<li class="Alphabets">Executors host the Spark driver on a worker-node basis.</li>
				<li class="Alphabets">Executors are responsible for carrying out work that they get assigned by the driver.</li>
				<li class="Alphabets">After the start of the Spark application, executors are launched on a per-task basis.</li>
				<li class="Alphabets">Executors are located in slots inside worker nodes.</li>
				<li class="Alphabets">The executors’ storage is ephemeral and as such it defers the task of caching data directly to the worker node thread.</li>
			</ol>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor069"/>Answers</h2>
			<ol>
				<li>A</li>
				<li>B</li>
			</ol>
		</div>
	

		<div><h1 id="_idParaDest-70" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor070"/>Part 3: Spark Operations</h1>
			<p>In this part, we will cover Spark DataFrames and their operations, emphasizing their role in structured data processing and analytics. This will include DataFrame creation, manipulation, and various operations such as filtering, aggregations, joins, and groupings, demonstrated through illustrative examples. Then, we will discuss advanced operations and optimization techniques, including broadcast variables, accumulators, and custom partitioning. This part also talks about performance optimization strategies, highlighting the significance of adaptive query execution and offering practical tips for enhancing Spark job performance. Furthermore, we will explore SQL queries in Spark, focusing on its SQL-like querying capabilities and interoperability with the DataFrame API. Examples will illustrate complex data manipulations and analytics through SQL queries in Spark.</p>
			<p>This part has the following chapters:</p>
			<ul>
				<li><a href="B19176_04.xhtml#_idTextAnchor071"><em class="italic">Chapter 4</em></a>, <em class="italic">Spark DataFrames and their Operations</em></li>
				<li><a href="B19176_05.xhtml#_idTextAnchor115"><em class="italic">Chapter 5</em></a>, <em class="italic">Advanced Operations and Optimizations in Spark</em></li>
				<li><a href="B19176_06.xhtml#_idTextAnchor164"><em class="italic">Chapter 6</em></a><em class="italic">,</em> <em class="italic">SQL Queries in Spark</em></li>
			</ul>
		</div>
		<div><div></div>
		</div>
		<div><div></div>
		</div>
	</body></html>