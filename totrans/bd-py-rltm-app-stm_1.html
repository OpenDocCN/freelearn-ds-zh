<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 1. Getting Acquainted with Storm"><div class="titlepage"><div><div><h1 class="title"><a id="ch01"/>Chapter 1. Getting Acquainted with Storm</h1></div></div></div><p>In this chapter, you will get acquainted with the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">An overview of Storm</li><li class="listitem" style="list-style-type: disc">The "before Storm" era and key features of Storm</li><li class="listitem" style="list-style-type: disc">Storm cluster modes</li><li class="listitem" style="list-style-type: disc">Storm installation</li><li class="listitem" style="list-style-type: disc">Starting various daemons</li><li class="listitem" style="list-style-type: disc">Playing with Storm configurations</li></ul></div><p>Over the complete course of the chapter, you will learn why Storm is creating a buzz in the industry and why it is relevant in present-day scenarios. What is this real-time computation? We will also explain the different types of Storm's cluster modes, the installation, and the approach to configuration.</p><div class="section" title="Overview of Storm"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec08"/>Overview of Storm</h1></div></div></div><p>Storm is a<a id="id0" class="indexterm"/> distributed, fault-tolerant, and highly scalable platform for processing streaming data in a real-time manner. It became an Apache top-level project in September 2014, and was previously an Apache Incubator project since September 2013.</p><p>Real-time processing on a massive scale has become a requirement of businesses. Apache Storm provides the capability to process data (a.k.a tuples or stream) as and when it arrives in a real-time manner with distributed computing options. The ability to add more machines to the Storm cluster makes Storm scalable. Then, the third most important thing that comes with storm is fault tolerance. If the storm program (also known as topology) is equipped with reliable spout, it can reprocess the failed tuples lost due to machine failure and also give fault tolerance. It is based on XOR magic, which will be explained in <a class="link" href="ch02.html" title="Chapter 2. The Storm Anatomy">Chapter 2</a>, <span class="emphasis"><em>The Storm Anatomy</em></span>.</p><p>Storm was originally created by Nathan Marz and his team at BackType. The project was made open source after<a id="id1" class="indexterm"/> it was acquired by Twitter. Interestingly, Storm received a tag as Real Time Hadoop.</p><p>Storm is best suited for many<a id="id2" class="indexterm"/> real-time use cases. A few of its interesting use cases are explained here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>ETL pipeline</strong></span>: ETL stands for <span class="strong"><strong>Extraction</strong></span>, <span class="strong"><strong>Transformation</strong></span>, and <span class="strong"><strong>Load</strong></span>. It is a very common use case <a id="id3" class="indexterm"/>of Storm. Data can be extracted or read from any source. Here, the data can be complex XML, a JDBC result set row, or simply a few key-value<a id="id4" class="indexterm"/> records. Data (also known as tuples in Storm) can be enriched on the fly with more information, transformed into the required storage format, and stored in a NoSQL/RDBMS data store. All of these things can be achieved at a very high throughput in a real-time manner with simple storm programs. Using the Storm ETL pipeline, you can ingest into a big data warehouse at high speed.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Trending topic analysis</strong></span>: Twitter uses<a id="id5" class="indexterm"/> such use cases to know the trending topics within a given time frame or at present. There are numerous use cases, and finding the top trends in a real-time manner is required. Storm can fit well in such use cases. You can also perform running aggregation of values with the help of any database.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Regulatory check engine</strong></span>: Real-time <a id="id6" class="indexterm"/>event data can pass through a business-specific regulatory algorithm, which can perform a compliance check in a real-time manner. Banks use these for trade data checks in real time.</li></ul></div><p>Storm can ideally fit into any use case where there is a need to process data in a fast and reliable manner, at a rate of more than 10,000 messages processing per second, as soon as data arrives. Actually, 10,000+ is a small number. Twitter is able to process millions of tweets per second on a large cluster. It depends on how well the Storm topology is written, how well it is tuned, and the cluster size.</p><p>Storm program (a.k.a topologies) are designed to run 24x7 and will not stop until someone stops them explicitly.</p><p>Storm is written using both Clojure as well as Java. Clojure is a Lisp, functional programming language that runs on JVM and is best for concurrency and parallel programming. Storm leverages the mature Java library, which was built over the last 10 years. All of these can be found inside the <code class="literal">storm</code>/<code class="literal">lib</code> folder.</p><div class="section" title="Before the Storm era"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec07"/>Before the Storm era</h2></div></div></div><p>Before Storm became popular, real-time or near-real-time processing problems were solved using intermediate brokers and with the help of message queues. Listener or worker processes run <a id="id7" class="indexterm"/>using the Python or Java languages. For parallel processing, code was dependent on the threading model supplied using the programming language itself. Many times, the old style of working did not utilize CPU and memory very well. In some cases, mainframes were used as well, but they also became outdated over time. Distributed computing was not so easy. There were either many intermediate outputs or hops in this old style of working. There was no way to perform a fail replay automatically. Storm addressed all of these pain areas very well. It is one of the best real-time computation frameworks available for use.</p></div><div class="section" title="Key features of Storm"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec08"/>Key features of Storm</h2></div></div></div><p>Here are Storm's key features; they<a id="id8" class="indexterm"/> address the aforementioned problems:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Simple to program</strong></span>: It's easy to learn the Storm framework. You can write code in the programming language of your choice and can also use the existing libraries of that programming language. There is no compromise.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Storm already supports most programming languages</strong></span>: However, even if something is not supported, it can be done by supplying code and configuration using the JSON <a id="id9" class="indexterm"/>protocol defined in the Storm <span class="strong"><strong>Data Specification Language</strong></span> (<span class="strong"><strong>DSL</strong></span>).</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Horizontal scalability or distributed computing is possible</strong></span>: Computation can be multiplied by adding more machines to the Storm cluster without stopping running programs, also known as topologies.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Fault tolerant</strong></span>: Storm <a id="id10" class="indexterm"/>manages worker and machine-level failure. Heartbeats of each process are tracked to manage different types of failure, such as task failure on one machine or an entire machine's failure.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Guaranteed message processing</strong></span>: There is a provision of performing auto and explicit ACK within storm processes on messages (tuples). If ACK is not received, storm can do a reply of a message.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Free, open source, and lots of open source community support</strong></span>: Being an Apache project, Storm has free distribution and modifying rights without any worry about the legal aspect. Storm gets a lot of attention from the open source community and is attracting a large number of good developers to contribute to the code.</li></ul></div></div><div class="section" title="Storm cluster modes"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec09"/>Storm cluster modes</h2></div></div></div><p>The Storm cluster can be <a id="id11" class="indexterm"/>set up in four flavors based on the requirement. If <a id="id12" class="indexterm"/>you want to set up a large cluster, go for distributed installation. If you want to learn Storm, then go for a single machine installation. If you want to connect to an existing Storm cluster, use client mode. Finally, if you want to perform development on an IDE, simply unzip the <code class="literal">storm</code> TAR and point to all dependencies of the <code class="literal">storm</code> library. At the initial learning<a id="id13" class="indexterm"/> phase, a single-machine storm installation is actually what you need.</p><div class="section" title="Developer mode"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec01"/>Developer mode</h3></div></div></div><p>A developer can download storm from the distribution site, unzip it somewhere in <code class="literal">$HOME</code>, and simply submit the Storm<a id="id14" class="indexterm"/> topology as local mode. Once the topology is successfully tested locally, it can be submitted to run over the cluster.</p></div><div class="section" title="Single-machine Storm cluster"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec02"/>Single-machine Storm cluster</h3></div></div></div><p>This flavor is best for students<a id="id15" class="indexterm"/> and medium-scale computation. Here, everything runs <a id="id16" class="indexterm"/>on a single machine, including <span class="strong"><strong>Zookeeper</strong></span>, <span class="strong"><strong>Nimbus</strong></span>, and <span class="strong"><strong>Supervisor</strong></span>. <code class="literal">Storm/bin</code> is used to run all commands. Also, no<a id="id17" class="indexterm"/> extra Storm client is<a id="id18" class="indexterm"/> required. You can do everything from the same machine. This case is well demonstrated in the following figure:</p><div class="mediaobject"><img src="images/B03471_01_01.jpg" alt="Single-machine Storm cluster"/></div></div><div class="section" title="Multimachine Storm cluster"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec03"/>Multimachine Storm cluster</h3></div></div></div><p>This option is required <a id="id19" class="indexterm"/>when you have a large-scale computation requirement. It is a horizontal scaling option. The following figure explains this case in detail. In this figure, we have five physical machines, and to increase fault tolerance in the systems, we are running Zookeeper on two machines. As shown in the diagram, <span class="strong"><strong>Machine 1</strong></span> and <span class="strong"><strong>Machine 2</strong></span> are a group of Zookeeper machines; one of them is the leader at any point of time, and when it dies, the other becomes the leader. <span class="strong"><strong>Nimbus</strong></span> is a lightweight process, so it can run on either machine, 1 or 2. We also have <span class="strong"><strong>Machine 3</strong></span>, <span class="strong"><strong>Machine 4</strong></span>, and <span class="strong"><strong>Machine 5</strong></span> dedicated for performing actual processing. Each one of these machines (3, 4, and 5) requires a supervisor daemon to run over there. Machines 3, 4, and 5 <a id="id20" class="indexterm"/>should know where the Nimbus/Zookeeper daemon is running and that entry should be present in their <code class="literal">storm.yaml</code>.</p><div class="mediaobject"><img src="images/B03471_01_02.jpg" alt="Multimachine Storm cluster"/></div><p>So, each physical machine (3, 4, and 5) runs one supervisor daemon, and each machine's <code class="literal">storm.yaml</code> points to the IP address of the machine where Nimbus is running (this can be 1 or 2). All Supervisor machines must add the Zookeeper IP addresses (1 and 2) to <code class="literal">storm.yaml</code>. The Storm UI daemon should run on the Nimbus machine (this can be 1 or 2).</p></div><div class="section" title="The Storm client"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec04"/>The Storm client</h3></div></div></div><p>The Storm client is required only <a id="id21" class="indexterm"/>when you have a Storm cluster of multiple machines. To start the client, unzip the Storm distribution and add the Nimbus IP address to the <code class="literal">storm.yaml</code> file. The Storm client can be used to submit Storm topologies and check the status of running topologies from command-line options. Storm versions older than 0.9 should put the <code class="literal">yaml</code> file inside <code class="literal">$STORM_HOME/.storm/storm.yaml</code> (not required for newer versions).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note02"/>Note</h3><p>The <code class="literal">jps</code> command is a very<a id="id22" class="indexterm"/> useful Unix command for seeing the Java process ID of Zookeeper, Nimbus, and Supervisor. The <code class="literal">kill -9 &lt;pid&gt;</code> option can stop a running process. The <code class="literal">jps</code> command will work only when <code class="literal">JAVA_HOME</code> is set in the <code class="literal">PATH</code> environment <a id="id23" class="indexterm"/>variable.</p></div></div></div></div><div class="section" title="Prerequisites for a Storm installation"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec10"/>Prerequisites for a Storm installation</h2></div></div></div><p>Installing Java and <a id="id24" class="indexterm"/>Python is easy. Let's assume our Linux machine is ready<a id="id25" class="indexterm"/> with Java and Python:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A Linux machine (Storm version 0.9 and later can also run on Windows machines)</li><li class="listitem" style="list-style-type: disc">Java 6 (<code class="literal">set export PATH=$PATH:$JAVA_HOME/bin</code>)</li><li class="listitem" style="list-style-type: disc">Python 2.6 (required to run Storm daemons and management commands)</li></ul></div><p>We will be making lots of changes in the storm configuration file (that is, <code class="literal">storm.yaml</code>), which is actually present under <code class="literal">$STORM_HOME/config</code>. First, we start the Zookeeper process, which carries out coordination between Nimbus and the Supervisors. Then, we start the Nimbus master daemon, which distributes code in the Storm cluster. Next, the Supervisor daemon listens for work assigned (by Nimbus) to the node it runs on and starts and stops the worker processes as necessary.</p><p>ZeroMQ/JZMQ and Netty are inter-JVM communication libraries that permit two machines or two JVMs to send and receive process data (tuples) between each other. JZMQ is a Java binding of ZeroMQ. The latest versions of Storm (0.9+) have now been moved to Netty. If you download an old version of Storm, installing ZeroMQ and JZMQ is required. In this book, we will be considering only the latest versions of Storm, so you don't really require ZeroMQ/JZMQ.</p><div class="section" title="Zookeeper installation"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec05"/>Zookeeper installation</h3></div></div></div><p>Zookeeper is a coordinator<a id="id26" class="indexterm"/> for the Storm cluster. The interaction between Nimbus and worker nodes is done through Zookeeper. The installation of Zookeeper is well <a id="id27" class="indexterm"/>explained on the official website at <a class="ulink" href="http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html#sc_InstallingSingleMode">http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html#sc_InstallingSingleMode</a>.</p><p>The setup can<a id="id28" class="indexterm"/> be downloaded from:</p><p>
<a class="ulink" href="https://archive.apache.org/dist/zookeeper/zookeeper-3.3.5/zookeeper-3.3.5.tar.gz">https://archive.apache.org/dist/zookeeper/zookeeper-3.3.5/zookeeper-3.3.5.tar.gz</a>. After downloading, edit the <code class="literal">zoo.cfg</code> file.</p><p>The following are the<a id="id29" class="indexterm"/> Zookeeper commands that are used:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Starting the <code class="literal">zookeeper</code> process:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>../zookeeper/bin/./zkServer.sh start</strong></span></pre></div></li><li class="listitem" style="list-style-type: disc">Checking the running status of the <code class="literal">zookeeper</code> service:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>../zookeeper/bin/./zkServer.sh status</strong></span></pre></div></li><li class="listitem" style="list-style-type: disc">Stopping the<a id="id30" class="indexterm"/> <code class="literal">zookeeper</code> service:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>../zookeeper/bin/./zkServer.sh stop</strong></span></pre></div></li></ul></div><p>Alternatively, use <code class="literal">jps</code> to find <code class="literal">&lt;pid&gt;</code> and then use <code class="literal">kill -9 &lt;pid&gt;</code> to kill the processes.</p></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Storm installation"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec09"/>Storm installation</h1></div></div></div><p>Storm can be installed<a id="id31" class="indexterm"/> in either of these two ways:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Fetch a Storm<a id="id32" class="indexterm"/> release from this location using Git:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://github.com/nathanmarz/storm.git">https://github.com/nathanmarz/storm.git</a></li></ul></div></li><li class="listitem">Download<a id="id33" class="indexterm"/> directly from the following link: <a class="ulink" href="https://storm.apache.org/downloads.html">https://storm.apache.org/downloads.html</a></li></ol></div><p>Storm configurations can be done using <code class="literal">storm.yaml</code>, which is present in the <code class="literal">conf</code> folder.</p><p>The following are the configurations for a single-machine Storm cluster installation.</p><p>Port <code class="literal"># 2181</code> is the default port of Zookeeper. To add more than one <code class="literal">zookeeper</code>, keep entry – separated:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>storm.zookeeper.servers:</strong></span>
<span class="strong"><strong>     - "localhost"</strong></span>

<span class="strong"><strong># you must change 2181 to another value if zookeeper running on another port.</strong></span>
<span class="strong"><strong>storm.zookeeper.port: 2181</strong></span>
<span class="strong"><strong># In single machine mode nimbus run locally so we are keeping it localhost.</strong></span>
<span class="strong"><strong># In distributed mode change localhost to machine name where nimbus daemon is running.</strong></span>
<span class="strong"><strong>nimbus.host: "localhost"</strong></span>
<span class="strong"><strong># Here storm will generate logs of workers, nimbus and supervisor.</strong></span>
<span class="strong"><strong>storm.local.dir: "/var/stormtmp"</strong></span>
<span class="strong"><strong>java.library.path: "/usr/local/lib"</strong></span>
<span class="strong"><strong># Allocating 4 ports for workers. More numbers can also be added.</strong></span>
<span class="strong"><strong>supervisor.slots.ports:</strong></span>
<span class="strong"><strong>     - 6700</strong></span>
<span class="strong"><strong>     - 6701</strong></span>
<span class="strong"><strong>     - 6702</strong></span>
<span class="strong"><strong>     - 6703</strong></span>
<span class="strong"><strong># Memory is allocated to each worker. In below case we are allocating 768 mb per worker.worker.childopts: "-Xmx768m"</strong></span>
<span class="strong"><strong># Memory to nimbus daemon- Here we are giving 512 mb to nimbus.</strong></span>
<span class="strong"><strong>nimbus.childopts: "-Xmx512m"</strong></span>
<span class="strong"><strong># Memory to supervisor daemon- Here we are giving 256 mb to supervisor.</strong></span></pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note03"/>Note</h3><p>Notice <code class="literal">supervisor.childopts: "-Xmx256m"</code>. In this setting, we reserved four supervisor ports, which means that a maximum of four worker processes can run on this machine.</p></div></div><p>
<code class="literal">storm.local.dir</code>: This directory<a id="id34" class="indexterm"/> location should be cleaned if there is a problem with starting Nimbus and Supervisor. In the case of running a topology on the local IDE on a Windows machine, <code class="literal">C:\Users\&lt;User-Name&gt;\AppData\Local\Temp</code> should be cleaned.</p><div class="section" title="Enabling native (Netty only) dependency"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec11"/>Enabling native (Netty only) dependency</h2></div></div></div><p>Netty enables inter JVM communication and <a id="id35" class="indexterm"/>it is very simple to <a id="id36" class="indexterm"/>use.</p><div class="section" title="Netty configuration"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec06"/>Netty configuration</h3></div></div></div><p>You don't really need to install<a id="id37" class="indexterm"/> anything extra for Netty. This is because it's a pure Java-based communication library. All new versions of Storm support Netty. </p><p>Add the following lines to your <code class="literal">storm.yaml</code> file. Configure and adjust the values to best suit your use case:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>storm.messaging.transport: "backtype.storm.messaging.netty.Context"</strong></span>
<span class="strong"><strong>storm.messaging.netty.server_worker_threads: 1</strong></span>
<span class="strong"><strong>storm.messaging.netty.client_worker_threads: 1</strong></span>
<span class="strong"><strong>storm.messaging.netty.buffer_size: 5242880</strong></span>
<span class="strong"><strong>storm.messaging.netty.max_retries: 100</strong></span>
<span class="strong"><strong>storm.messaging.netty.max_wait_ms: 1000</strong></span>
<span class="strong"><strong>storm.messaging.netty.min_wait_ms: 100</strong></span></pre></div></div><div class="section" title="Starting daemons"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec07"/>Starting daemons</h3></div></div></div><p>Storm daemons are the <a id="id38" class="indexterm"/>processes that are needed to pre-run before you submit your program to the cluster. When you run a topology program on a local IDE, these daemons auto-start on predefined ports, but over the cluster, they must run at all times:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Start the master daemon, <code class="literal">nimbus</code>. Go to the <code class="literal">bin</code> directory of the Storm installation and execute the following command (assuming that <code class="literal">zookeeper</code> is running):<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>   ./storm nimbus</strong></span>
<span class="strong"><strong>     Alternatively, to run in the background, use the same command with nohup, like this:</strong></span>
<span class="strong"><strong>    Run in background</strong></span>
<span class="strong"><strong>    nohup ./storm nimbus &amp;</strong></span></pre></div></li><li class="listitem">Now we have to start the <code class="literal">supervisor</code> daemon. Go to the <code class="literal">bin</code> directory of the Storm installation and execute this command:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>  ./storm supervisor</strong></span></pre></div><p>To run in the background, use the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>         nohup ./storm  supervisor &amp;</strong></span></pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note04"/>Note</h3><p>If Nimbus or the<a id="id39" class="indexterm"/> Supervisors restart, the running topologies are unaffected as both are stateless.</p></div></div></li><li class="listitem">Let's start the <code class="literal">storm</code> UI. The Storm UI is an optional process. It helps us to see the Storm statistics of a running topology. You can see how many executors and workers are assigned to a particular topology. The command needed to run the storm UI is as follows:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>       ./storm ui</strong></span></pre></div><p>Alternatively, to run in the background, use this line with <code class="literal">nohup</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>       nohup ./storm ui &amp;</strong></span></pre></div><p>To access the Storm UI, visit <code class="literal">http://localhost:8080</code>.</p></li><li class="listitem">We will now start <code class="literal">storm logviewer</code>. Storm UI is another optional process for seeing the log from the browser. You can also see the <code class="literal">storm</code> log using the command-line option in the <code class="literal">$STORM_HOME/logs</code> folder. To start logviewer, use this command:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>         ./storm logviewer</strong></span></pre></div><p>To run in the background, use the following line with <code class="literal">nohup</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>         nohup ./storm logviewer &amp;</strong></span></pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note05"/>Note</h3><p>To access Storm's log, visit <code class="literal">http://localhost:8000log viewer</code> daemon should run on each machine. Another way to access the log of <code class="literal">&lt;machine name&gt;</code> for worker port <code class="literal">6700</code> is given here:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&lt;Machine name&gt;:8000/log?file=worker-6700.log</strong></span></pre></div></div></div></li><li class="listitem">DRPC daemon: DRPC is<a id="id40" class="indexterm"/> another optional service. <span class="strong"><strong>DRPC</strong></span> stands for<a id="id41" class="indexterm"/> <span class="strong"><strong>Distributed Remote Procedure Call</strong></span>. You will require the DRPC daemon if you want to supply to the storm topology an argument externally through the DRPC client. Note that an argument can be supplied only once, and the DRPC client can wait for long until storm topology does the processing and the return. DRPC is not a popular option to use in projects, as firstly, it is blocking to the client, and secondly, you can supply only one argument at a time. DRPC is not supported by Python and Petrel.</li></ol></div><p>Summarizing, the steps for starting processes are as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, all the Zookeeper daemons.</li><li class="listitem">Nimbus daemons.</li><li class="listitem">Supervisor daemon on one or more machine.</li><li class="listitem">The UI daemon where Nimbus is running (optional).</li><li class="listitem">The Logviewer daemon (optional).</li><li class="listitem">Submitting the topology.</li></ol></div><p>You can restart the <code class="literal">nimbus</code> daemon anytime without any impact on existing processes or topologies. You can restart the supervisor daemon and can also add more supervisor machines to the Storm cluster anytime.</p><p>To submit <code class="literal">jar</code> to the Storm cluster, go to the <code class="literal">bin</code> directory of the Storm installation and execute the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>./storm jar &lt;path-to-topology-jar&gt; &lt;class-with-the-main&gt; &lt;arg1&gt; … &lt;argN&gt;</strong></span></pre></div></div></div><div class="section" title="Playing with optional configurations"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec12"/>Playing with optional configurations</h2></div></div></div><p>All the previous<a id="id42" class="indexterm"/> settings are required to start the cluster, but there are many other settings that are optional<a id="id43" class="indexterm"/> and can be tuned based on the topology's requirement. A prefix can help find the nature of a configuration. The complete list of <a id="id44" class="indexterm"/>default <code class="literal">yaml</code> configuration is available at <a class="ulink" href="https://github.com/apache/storm/blob/master/conf/defaults.yaml">https://github.com/apache/storm/blob/master/conf/defaults.yaml</a>.</p><p>Configurations can be identified by how the prefix starts. For example, all UI configurations start with <code class="literal">ui*</code>.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"><p>Nature of the configuration</p></th><th style="text-align: left" valign="bottom"><p>Prefix to look into</p></th></tr></thead><tbody><tr><td style="text-align: left" valign="top"><p>General</p></td><td style="text-align: left" valign="top"><p>
<code class="literal">storm.*</code></p></td></tr><tr><td style="text-align: left" valign="top"><p>Nimbus</p></td><td style="text-align: left" valign="top"><p>
<code class="literal">nimbus.*</code></p></td></tr><tr><td style="text-align: left" valign="top"><p>UI</p></td><td style="text-align: left" valign="top"><p>
<code class="literal">ui.*</code></p></td></tr><tr><td style="text-align: left" valign="top"><p>Log viewer</p></td><td style="text-align: left" valign="top"><p>
<code class="literal">logviewer.*</code></p></td></tr><tr><td style="text-align: left" valign="top"><p>DRPC</p></td><td style="text-align: left" valign="top"><p>
<code class="literal">drpc.*</code></p></td></tr><tr><td style="text-align: left" valign="top"><p>Supervisor</p></td><td style="text-align: left" valign="top"><p>
<code class="literal">supervisor.*</code></p></td></tr><tr><td style="text-align: left" valign="top"><p>Topology</p></td><td style="text-align: left" valign="top"><p>
<code class="literal">topology.*</code></p></td></tr></tbody></table></div><p>All of these optional configurations can be added to <code class="literal">STORM_HOME/conf/storm.yaml</code> for any change other than the default values. All settings that start with <code class="literal">topology.*</code> can either be set programmatically from the topology or from <code class="literal">storm.yaml</code>. All other settings can be set only from the <code class="literal">storm.yaml</code> file. For example, the following table shows three different ways to <a id="id45" class="indexterm"/>play with these parameters. However, all of these three do the same thing:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"><p>/conf/storm.yaml</p></th><th style="text-align: left" valign="bottom"><p>Topology builder</p></th><th style="text-align: left" valign="bottom"><p>Custom yaml</p></th></tr></thead><tbody><tr><td style="text-align: left" valign="top"><p>Changing <code class="literal">storm.yaml</code></p>
<p>(impacts all the topologies of the cluster)</p></td><td style="text-align: left" valign="top"><p>Changing the topology builder while writing code</p>
<p>(impacts only the current topology)</p></td><td style="text-align: left" valign="top"><p>Supplying <code class="literal">topology.yaml</code> as a command-line option</p>
<p>(impacts only the current topology)</p></td></tr><tr><td style="text-align: left" valign="top"><p>
<code class="literal">topology.workers: 1</code></p></td><td style="text-align: left" valign="top"><p>
<code class="literal">conf.setNumberOfWorker(1);</code></p>
<p>This is supplied through Python code</p></td><td style="text-align: left" valign="top"><p>Create <code class="literal">topology.yaml</code> with the entry made into it similar to <code class="literal">storm.yaml</code>, and supply it when running the topology</p>
<p>Python:</p>
<p>
<code class="literal">petrel submit --config topology.yaml</code></p></td></tr></tbody></table></div><p>Any configuration change in <code class="literal">storm.yaml</code> will affect all running topologies, but when using the <code class="literal">conf.setXXX</code> option in <a id="id46" class="indexterm"/>code, different topologies can overwrite that option, what is best suited for each of them.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec10"/>Summary</h1></div></div></div><p>Here comes the conclusion of the first chapter. This chapter gave an overview of how applications were developed before Storm came into existence. A brief knowledge of what real-time computations are and how Storm, as a programming framework, is becoming so popular was also acquired as we went through the chapter and approached the conclusion. This chapter taught you to perform Storm configurations. It also gave you details about the daemons of Storm, Storm clusters, and their step up. In the next chapter, we will be exploring the details of Storm's anatomy.</p></div></div>
</body></html>