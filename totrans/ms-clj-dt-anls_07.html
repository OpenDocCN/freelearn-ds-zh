<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Null Hypothesis Tests – Analyzing Crime Data</h1></div></div></div><p>Getting started with data analysis can be so easy. We just plug numbers into a function or library and retrieve the results. But sometimes, it's easy to forget that we have to pay attention to how the data and experiments are constructed and how the questions are framed. Much of the reliability of statistics comes from following good practices and developed processes for framing and executing the tests and experiments.</p><p>Of course, there's a lot to setting up statistical experiments and following best practices in gathering data and applying statistical tests. We won't be able to do more than cursorily glance at this topic. Hopefully, either it will serve as a reminder of things you already know or it will outline what you need to know and point you in the right direction to learn more.</p><p>Over the course of this chapter, we'll move back and forth between looking at the problem we're tackling and seeing what null hypothesis testing is, how it can help us, and how we can apply it.</p><p>In this chapter, we will cover the following topics:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Introducing confirmatory data analysis</li><li class="listitem" style="list-style-type: disc">Understanding null hypothesis testing</li><li class="listitem" style="list-style-type: disc">Understanding crime</li><li class="listitem" style="list-style-type: disc">Getting the data</li><li class="listitem" style="list-style-type: disc">Transforming the data</li><li class="listitem" style="list-style-type: disc">Conducting the experiment</li><li class="listitem" style="list-style-type: disc">Interpreting the results</li></ul></div><p>So without any further delay, let's learn about the techniques and the problems we'll address with these methods in this chapter.</p><div><div><div><div><h1 class="title"><a id="ch07lvl1sec49"/>Introducing confirmatory data analysis</h1></div></div></div><p>Oftentimes, data analysis<a id="id461" class="indexterm"/> seems like a menu of analyses applied to problems, but lacking an overall structure. Of course, this isn't the case, but it seems that way to programmers without a strong background in statistics.</p><p>Frameworks such as <strong>confirmatory data analysis</strong><a id="id462" class="indexterm"/> and <strong>null hypothesis testing</strong><a id="id463" class="indexterm"/> provide the structure that may be missing. Generally, when you begin working with data, you start by generating some summary statistics that highlight some of the basic characteristics of the data. Afterwards, you probably generate some graphs that further elucidate the essential qualities of the data. This all falls into the realm of<a id="id464" class="indexterm"/> <strong>exploratory data analysis</strong>.</p><p>However, as the exploration wraps up, you'll probably start to think of some theories about the data that you'd like to test. You'll generate some hypotheses, and you'll need to test whether they're true or not. And based on those tests, you'll further refine your knowledge of the data, what's in it, and what it means.</p><p>This more formal stage of data analysis represents confirmatory data analysis. At this stage, you're concerned with using reliable tests that match your data, and you're trying to determine how representative your sample is. You are minimizing error and trying to get a <a id="id465" class="indexterm"/>
<strong>pvalue</strong>—the probability that a result so extreme could have happened by chance—that means that the results are statistically significant.</p><p>But what does all this mean, exactly? How do we go about conceptualizing, planning, and executing these tests?</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec50"/>Understanding null hypothesis testing</h1></div></div></div><p>One common way of<a id="id466" class="indexterm"/> structuring and processing these tests is to use null hypothesis testing. This represents a <strong>frequentist</strong> approach<a id="id467" class="indexterm"/> to statistical inference. This draws inferences based upon the frequencies or proportions in the data, paying attention to confidence intervals and error rates. Another approach is<a id="id468" class="indexterm"/> Bayesian inference, which focuses on degrees of belief, but we won't go into that in this chapter.</p><p>Frequentist inference<a id="id469" class="indexterm"/> has been very successful. Its use is assumed in many fields, such as the social sciences and biology. Its techniques are widely implemented in many libraries and software packages, and it's relatively easy to start using it. It's the approach we'll use in this chapter.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec40"/>Understanding the process</h2></div></div></div><p>To use the <a id="id470" class="indexterm"/>null hypothesis process, we should understand what we'll be doing at each step of the way. The following is the basic process that we'll work through in this chapter:</p><div><ol class="orderedlist arabic"><li class="listitem">Formulate an initial hypothesis.</li><li class="listitem">State the null (<em>H<sub>0</sub></em>) and alternative (<em>H<sub>1</sub></em>) hypotheses.</li><li class="listitem">Identify the statistical assumptions in the sample.</li><li class="listitem">Determine which tests (<em>T</em>) are appropriate.</li><li class="listitem">Select the significance level (<em>a</em>), such as <em>p&lt;0.05</em> or <em>p&lt;0.01</em>.</li><li class="listitem">Determine the critical region, that is, the region of the distribution in which the null hypothesis will be rejected.</li><li class="listitem">Calculate the test statistic and the probability of the observation under the null hypothesis (<em>p</em>).</li><li class="listitem">Either reject the null hypothesis or fail to reject it.</li></ol></div><p>We'll go into these step-by-step, and we'll walk through this process twice to get a good feel for how it works. Most of this is pretty simple, really.</p><div><div><div><div><h3 class="title"><a id="ch07lvl3sec14"/>Formulating an initial hypothesis</h3></div></div></div><p>Before we can start <a id="id471" class="indexterm"/>testing a theory about our data, we need to have something to test. This is generally something that might be true or false, and we want to determine which of the two it is. Some examples of initial hypotheses might be height correlating to diet, speed limit correlating to accident mortality, or a Super Bowl win for an old American Football League team (AFC division) correlating to a declining stock market (the so-called Super Bowl indicator).</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec15"/>Stating the null and alternative hypotheses</h3></div></div></div><p>Now we have to reformulate the initial hypothesis into the statistical phrases that we'll use more directly the rest of the time. This is a useful point that helps to clarify the rest of the process.</p><p>In this case, the <a id="id472" class="indexterm"/>null hypothesis is the control, or what we're trying to disprove. It's the opposite of the alternative hypothesis, which is what we want to prove.</p><p>For example, in the last example from the previous section, the Super Bowl indicator, the re-cast hypotheses might be as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Null hypothesis</strong>: Who wins the Super Bowl has no effect upon the stock market.</li><li class="listitem" style="list-style-type: disc"><strong>Alternative hypothesis</strong>: When <a id="id473" class="indexterm"/>an AFC division team wins the Super Bowl, the stock market will decline; when an NFC division team wins the Super Bowl, the stock market will be up.</li></ul></div><p>For the rest of the process, <a id="id474" class="indexterm"/>we will concern ourselves with rejecting the null hypothesis. That can only happen when we've determined two things: first, that the data we have supports the alternative hypothesis, and second, that this is very unlikely to be a mistake; that is, the results we see probably are not a sample that misrepresents the underlying population.</p><p>This is going to keep coming up, so let's unpack it a little.</p><p>You're interested in making an observation about a population—all men; all women; all people; all statisticians; or past, present, and future stock market trends—but obviously you can't make an observation for every person or aspect in the population. So instead, you select a sample. It should be random. The question then becomes: does the sample accurately represent the population? Say you're interested in people's heights. How close is the sample's average height to the population's average height?</p><p>Let's assume that what we're interested in falls on a normal distribution, as height generally does. What would this look like? For the following chart, I generated some random height data. The blue bars (appearing as dark gray in physical books) represent the histogram for the population, and the red bars (appearing as light gray in physical books) are the histogram for the sample.</p><div><img src="img/4139OS_07_01.jpg" alt="Stating the null and alternative hypotheses"/></div><p>We can see from <a id="id475" class="indexterm"/>the preceding graph that the distributions are similar, but certainly <a id="id476" class="indexterm"/>not the same. And in fact, the mean for the population is 6.01, while the mean for the sample is 5.94. They're not too far apart in this case, but some samples would be much further off.</p><p>It has been proven theoretically that the difference between the population mean and the possible sample means will fall on a normal distribution. The following is the plot for the difference in the means from 500 sets of samples drawn from the same population:</p><div><img src="img/4139OS_07_02.jpg" alt="Stating the null and alternative hypotheses"/></div><p>This histogram <a id="id477" class="indexterm"/>makes it clear that large differences between the <a id="id478" class="indexterm"/>population mean and the sample mean are unlikely, and the larger the difference, the more improbable it is. This is important for several reasons. First, if we know the distribution of the differences of means, it allows us to set constraints on results. If we are working with sample data, we know that the same values for the population will fall within a set bound.</p><p>Also, if we know the distribution of differences, then we know if our results are significant. This means that we can reject the null hypothesis that the averages are the same. Any two sample means should fall within the same boundaries. Large differences between any two sets of sample means are similarly improbable.</p><p>For example, one sample would be the control data, and one would be the test data. If the difference between the two samples is large enough to be improbable, then we can infer that the test behavior produced a significant difference (assuming the rest of the experiment is well designed and other things aren't complicating the experiment). If it's unlikely enough, then we say that it's significant, and we reject the null hypothesis.</p><p>Depending on what we're testing, we may be interested in results that are on the left-hand side of the graph, the right-hand side, or either. That is, the test statistic for the alternative hypothesis may be significantly less than, significantly greater than, or equal to the null hypothesis. We express this in notation using one of the following three forms. (These use the character mu, <em>μ</em>, using the sample mean as the test statistic.) In each of these notations, the first line states the null hypothesis, and the second states the alternative hypothesis. For instance, the first pair in the following notation says that the null hypothesis is that the test sample's mean should be greater than or equal to the control sample's mean, and the alternative hypothesis is that the test sample's mean should be less than the control sample's mean:</p><div><img src="img/4139OS_07_03.jpg" alt="Stating the null and alternative hypotheses"/></div><p>We've taken our<a id="id479" class="indexterm"/> time to understand this more thoroughly because it's fundamental to the<a id="id480" class="indexterm"/> rest of the process. However, if you don't understand it at this point, throughout the rest of the chapter, we'll keep going over this. By the end, you should have a good understanding of this graph of sample mean differences and what it implies.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec16"/>Determining appropriate tests</h3></div></div></div><p>Another aspect of your<a id="id481" class="indexterm"/> data that you'll need to pay attention to is the shape of the data. This can often be easily visualized using a histogram. For example, the following screenshot shows a normal distribution and two distributions that are skewed:</p><div><img src="img/4139OS_07_04.jpg" alt="Determining appropriate tests"/></div><p>The red curve is <a id="id482" class="indexterm"/>skewed left (appearing as dark gray), and the yellow curve is skewed right (appearing as white). The blue curve (appearing as light gray) is a normal distribution with no skew.</p><p>Many statistical tests are designed for normal data, and they won't give good results for skewed data. For example, t-test and regression analysis both give good results only for normally distributed data.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec17"/>Selecting the significance level</h3></div></div></div><p>Next, we need to select the<a id="id483" class="indexterm"/> significance level that we want to achieve for our test. This is the level of certainty that we'll need to have before we can reject the null hypothesis. More to the point, this is the maximum chance that the results could be an outlying sample from the population, which would cause you to incorrectly reject the null hypothesis.</p><p>Often, the significance level, usually given as the p-value, is given as <em>p&lt;0.05</em> or <em>p&lt;0.01</em>. This means that the results have a less than 5 percent or 1 percent chance of being caused by a sample with an outlying mean.</p><p>If we look at the graph of sample mean differences given earlier, we can see that we're looking at differences of about 2.4 inches to be significant. In other words, based on this population, the average difference in height would need to be more than 2 inches for it to be considered statistically significant.</p><p>Say we wanted to see<a id="id484" class="indexterm"/> if men and women were on average, of different heights. If the average height difference were only 1 inch, that could likely be the result of the samples that we picked. However, if the average height difference were 2.4 inches or more, that would be unlikely to have come from the sample.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec18"/>Determining the critical region</h3></div></div></div><p>Now we have <a id="id485" class="indexterm"/>determined two important pieces of information: we've expressed our null and alternative hypotheses, and we've decided on a needed level of significance. We can use these two to determine the critical region for the test results, that is, the region for which we can reject the null hypothesis.</p><p>Remember that our hypotheses can take one of three forms. The following conditions determine where our critical region is:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">For the alternative hypothesis that the two samples' means are not equal, we'll perform a two-tailed test.</li><li class="listitem" style="list-style-type: disc">For the alternative hypothesis that the test sample's mean is less than the control sample's, we'll perform a one-tailed test with the critical region on the left-hand side of the graph.</li><li class="listitem" style="list-style-type: disc">And for the alternative hypothesis that the test sample's mean is greater than the control sample's, we'll perform a one-tailed test with the critical region on the right-hand side of the graph.</li></ul></div><p>The following hypothetical graph highlights the part of the curve in which the critical regions occur. The curve represents the distribution of the test statistic for the sample, and the shaded parts will be the areas that the critical region(s) might come from.</p><div><img src="img/4139OS_07_05.jpg" alt="Determining the critical region"/></div><p>The exact <a id="id486" class="indexterm"/>size of the critical regions is determined by the <em>p</em> value that we decided upon. In all cases, the area of the critical regions is the <em>p</em> percentage of the entire curve. That is, if we've decided that we're trying for <em>p&lt;0.05</em>, and the area under the whole curve is 100, the area in the critical region will be 5.</p><p>If we are performing a two-tailed test, then that area will be divided into two, so in the example we just outlined, each side will have an area of 2.5. However, for one-tailed tests, the entire critical region will fall on one side.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec19"/>Calculating the test statistics and its probability</h3></div></div></div><p>Now we have to<a id="id487" class="indexterm"/> calculate the <a id="id488" class="indexterm"/>test statistic. Depending on the nature of the data, the sample, and on what you're trying to answer, this could involve comparing means, a student's t-test, X<sup>2</sup> test, or any number of other tests.</p><p>These tests will give you a number, but interpreting it directly is often not helpful. Instead, you then need to calculate the value of <em>p</em> for that test's distribution. If you're doing things by hand, this can involve either looking up the value in tables or if you're using a software program, this is often done for you and returned as a part of the results.</p><p>We'll use Incanter in several sections later in this chapter, starting with calculating the test statistic and its probability. Its functions generally return both the test value and the <em>p</em> value.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec20"/>Deciding whether to reject the null hypothesis or not</h3></div></div></div><p>Now we can find the value of <em>p</em> in <a id="id489" class="indexterm"/>relation to the critical regions and determine whether we can reject the null hypothesis or not.</p><p>For instance, say that we've decided that the level of significance that we want to achieve is <em>p&lt;0.05</em> and the actual value of <em>p</em> is <em>0.001</em>. This will allow us to reject the null hypothesis.</p><p>However, if the value of <em>p</em> is <em>0.055</em>, we would fail to reject the null hypothesis. We would have to assume that the alternative hypothesis is incorrect, at least until more information is available.</p></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec41"/>Flipping coins</h2></div></div></div><p>Now that we've been <a id="id490" class="indexterm"/>over the process of null hypothesis<a id="id491" class="indexterm"/> testing, let's walk through the process one more time with an example. This should be simple and straightforward enough that we can focus on the process, and not on the test itself.</p><p>For that purpose, we'll test whether a dice is loaded or not. If it is balanced, then the expected probability of any given side should be 1/6, or about 16 percent. However, if the die is loaded, then the probability for rolling one side should be greater than 16 percent, and the probabilities for rolling the other sides would be less than 16 percent.</p><p>Of course, generally this isn't something that you would worry about. But before you agree to play craps with the dice that your friend 3D printed, you may want to test them.</p><p>For this test, I've rolled one die 1,000 times. The following is the table of how many times each side came up:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Side</p>
</th><th style="text-align: left" valign="bottom">
<p>Frequency</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>157</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>151</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>175</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>187</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>143</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>187</p>
</td></tr></tbody></table></div><p>So we can see<a id="id492" class="indexterm"/> that the frequencies are relatively close, within a<a id="id493" class="indexterm"/> range of 44, but they aren't exactly the same. This is what we'd expect. The question is whether they're different enough that we can say with some certainty that the die is loaded.</p><div><div><div><div><h3 class="title"><a id="ch07lvl3sec21"/>Formulating an initial hypothesis</h3></div></div></div><p>So we suspect that our test die <a id="id494" class="indexterm"/>is fair, but we don't know that. We'll frame our hypothesis this way: on any roll, all sides have an equal chance of appearing.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec22"/>Stating the null and alternative hypotheses</h3></div></div></div><p>Our initial hypothesis can act as our null <a id="id495" class="indexterm"/>hypothesis. And in this<a id="id496" class="indexterm"/> case, we expect to fail to reject it. Let's state both hypotheses explicitly:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><em>H<sub>0</sub></em>: All sides have an equal chance of appearing on any roll.</li><li class="listitem" style="list-style-type: disc"><em>H<sub>1</sub></em>: One side has a greater chance of appearing on any roll.</li></ul></div><p>In this case, we let <em>H<sub>0</sub></em> be such that the two sides are equal because we want there to be more latitude in what counts as fair, and we want to enforce a high burden of proof before we declare a die loaded.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec23"/>Identifying the statistical assumptions in the sample</h3></div></div></div><p>For our sample, we'll <a id="id497" class="indexterm"/>roll the die in question 1,000 times. We'll assume that each roll is identical: that it's being done with approximately the same arm and hand movements, and that the die is landing on a flat surface. We'll also assume that before being thrown, the die is being shaken enough to be appropriately random.</p><p>This way, no biases are introduced because of the mechanics of how the die is being thrown.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec24"/>Determining appropriate tests</h3></div></div></div><p>For this, we'll use a<a id="id498" class="indexterm"/> Pearson's Χ<sup>2</sup> goodness-of-fit test. This is used to test whether an observed frequency distribution matches a theoretical distribution. It works by calculating a normalized sum of squared deviations. We're trying to test whether some observations match an expected distribution, so this test is a great fit.</p><p>We'll see exactly <a id="id499" class="indexterm"/>how to apply this test in a minute.</p><div><div><div><div><h4 class="title"><a id="ch07lvl4sec06"/>Selecting the significance level</h4></div></div></div><p>Proving that a die is <a id="id500" class="indexterm"/>loaded does require a higher burden of proof than assuming that it's fair, but we don't want the bar to be too high. Because of that, we'll use <em>p&lt;0.05</em> for this.</p></div><div><div><div><div><h4 class="title"><a id="ch07lvl4sec07"/>Determining the critical region</h4></div></div></div><p>The output of the Χ<sup>2</sup> test<a id="id501" class="indexterm"/> fits an Χ<sup>2</sup> distribution, not a normal distribution, so the graph won't look the same. Also, Χ<sup>2</sup> tests are intrinsically one sided. When the number is too far out on the right, then it indicates that the data fits the theoretical values poorly. A value to the left on the Χ<sup>2</sup> distribution just indicates that the fit is very good, which isn't really a problem.</p><p>The following is a graph comparing the normal distribution, centered on 50, with the X<sup>2</sup> distribution, with 3 degrees of freedom:</p><div><img src="img/4139OS_07_06.jpg" alt="Determining the critical region"/></div><p>Either way, the statistics<a id="id502" class="indexterm"/> library that we're going to use (Incanter) will take care of this for us.</p></div><div><div><div><div><h4 class="title"><a id="ch07lvl4sec08"/>Calculating the test statistic and its probability</h4></div></div></div><p>So let's fire up the <a id="id503" class="indexterm"/>Leiningen REPL and see what<a id="id504" class="indexterm"/> we can do. For this project, we're going to use the following <code class="literal">project.clj</code> file:</p><div><pre class="programlisting">(defproject nullh "0.1.0-snapshot"
  :dependencies [[org.clojure/clojure "1.5.1"]
                 [enlive "1.1.4"]
                 [http.async.client "0.5.2"]
                 [org.clojure/data.csv "0.1.2"]
                 [org.clojure/data.json "0.2.3"]
                 [me.raynes/fs "1.4.5"]
                 [incanter "1.5.4"]
                 [geocoder-clj "0.2.2"]
                 [geo-clj "0.3.5"]
                 [congomongo "0.4.1"]
                 [org.apache.poi/poi-ooxml "3.9"]]
  :profiles {:dev {:dependencies
                   [[org.clojure/tools.namespace "0.2.4"]]
                   :source-paths ["dev"]}})</pre></div><p>First, we'll load Incanter, then we'll create a matrix containing our data, and finally we'll run an Χ<sup>2</sup> test over it with the following code:</p><div><pre class="programlisting">
<strong>user=&gt; (require '[incanter.core :as i] '[incanter.stats :as s])</strong>
<strong>nil</strong>
<strong>user=&gt; (def table (i/matrix [157 151 175 187 143 187]))</strong>
<strong>#'user/table</strong>
<strong>user=&gt; (def r (s/chisq-test :table table))</strong>
<strong>#'user/r</strong>
<strong>user=&gt; (pprint (select-keys r [:p-value :df :X-sq]))</strong>
<strong>{:X-sq 10.771999999999998, :df 5, :p-value 0.05609271590058857}</strong>
</pre></div><p>Let's look at <a id="id505" class="indexterm"/>this code in more <a id="id506" class="indexterm"/>detail:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The function <code class="literal">incanter.stats/chisq-test</code> returns a lot of information, including its own input. So, before displaying it at the end, I filtered out most of the data and only returned the three keys that we're particularly interested in. The following are those keys and the values that they returned.<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">:X-sq</code>: This is the Χ<sup>2</sup> statistic. Higher values of this indicate that the data does not fit their expected values.</li><li class="listitem" style="list-style-type: disc"><code class="literal">:df</code>: This is the degrees of freedom. This represents the number of parameters that are free to vary. For nominal data (data without natural ordering), such as rolls of dice, this is the number of values that the data can take, minus one. In this case, since it's a six-sided die, the degree of freedom is five.</li><li class="listitem" style="list-style-type: disc"><code class="literal">:p-value</code>: This is the value of <em>p</em> that we've been talking about. This is the probability that we'd see these results from the Χ<sup>2</sup> test if the null hypothesis were true.</li></ul></div></li></ul></div><p>Now that we have these numbers, how do we apply them to our hypotheses?</p></div><div><div><div><div><h4 class="title"><a id="ch07lvl4sec09"/>Deciding whether to reject the null hypothesis or not</h4></div></div></div><p>In this case, since <em>p&gt;0.05</em>, we fail to reject the null hypothesis. We can't really rule it out, but we don't have enough evidence to support it either. In this case, we can assume that the die is fair.</p><p>Hopefully, this example <a id="id507" class="indexterm"/>gives you a better understanding of the null hypothesis testing process and how it works. With that under our belts, let's turn our attention to a bigger, more meaningful problem than the fairness of imaginary dice.</p></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec51"/>Understanding burglary rates</h1></div></div></div><p>Understanding crime seems like a<a id="id508" class="indexterm"/> universal problem. Earlier, societies grappled with the problem of evil in the universe from a theological perspective; today, sociologists and criminologists construct theories and study society using a variety of tools and techniques. However the problem is cast, the aim is to better understand why some people violate social norms in ways that are often violent and harmful to those around them and even themselves. By better understanding this problem, ultimately we'd like to be able to create social programs and government policies that minimize the damage and create a safer and hopefully more just society for all involved.</p><p>Of course, as data scientists and programmers engaging in data analysis, we're inclined to approach this problem as a data problem. That's what we'll do in the rest of this chapter. We'll gather some crime and economic data and look for a tie between the two. In the course of our analysis, we'll explore the data, tentatively suggest a hypothesis, and test it against the data.</p><p>We'll look at crime data from the United Nations and see what relationships it has with data from the World Bank data site.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec42"/>Getting the data</h2></div></div></div><p>In order to<a id="id509" class="indexterm"/> get the <a id="id510" class="indexterm"/>data, perform the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">First, we need to download the data.</li><li class="listitem">For the crime data, we'll go to the website of the<a id="id511" class="indexterm"/> United Nations Office on Drugs and Crime (<a class="ulink" href="http://www.unodc.org/">http://www.unodc.org/</a>). It publishes crime data for countries around the world over a number of years. Their data page, <a class="ulink" href="http://www.unodc.org/unodc/en/data-and-analysis/statistics/data.html">http://www.unodc.org/unodc/en/data-and-analysis/statistics/data.html</a>, has links to Excel files for a number of different categories of crime in the section of the page labeled <strong>Statistics on crime</strong>.</li><li class="listitem">You should download each of these and save them to the directory <code class="literal">unodc-data</code>. You can extract the data from these in a minute. First, you can get the data that we want to correlate to the crime data.</li><li class="listitem">We'll get this<a id="id512" class="indexterm"/> data from the World Bank's data site (<a class="ulink" href="http://data.worldbank.org/">http://data.worldbank.org/</a>). Navigating the site is a little complicated, and in my <a id="id513" class="indexterm"/>experience it changes regularly. For the moment, at least, this seems to be the easiest way to get the data:<div><ol class="orderedlist arabic"><li class="listitem">Visit the <strong>Indicators</strong> page at <a class="ulink" href="http://data.worldbank.org/indicator">http://data.worldbank.org/indicator</a>.</li><li class="listitem">In the search box, enter <code class="literal">land area</code> and select <strong>Land area (sq. km)</strong>, as shown in the following screenshot:</li></ol></div><div><img src="img/4139OS_07_07.jpg" alt="Getting the data"/></div></li><li class="listitem">Then hit the <strong>Go</strong> button.</li><li class="listitem">On the next page, you'll be given the option to download the dataset in a number of formats. Choose <strong>CSV</strong>.</li><li class="listitem">Download the data and unzip it into a directory named <code class="literal">ag.lnd</code>, based on the indicator codes that the World Bank uses. (You can use a different directory name, but you'll need to modify the directions that follow.)<div><img src="img/4139OS_07_08.jpg" alt="Getting the data"/></div></li></ol></div><p>We'll also want <a id="id514" class="indexterm"/>some economic data. To get that, perform the<a id="id515" class="indexterm"/> following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Go back to the <strong>Indicators</strong> page.</li><li class="listitem">Search for <strong>GNI per capita</strong> (it's the default selection for the search box).</li><li class="listitem">From the filtered results, select <strong>GNI per capita, Atlas method (current US$)</strong>.</li><li class="listitem">Click on <strong>Go</strong>.</li><li class="listitem">Download the data as CSV again.</li><li class="listitem">Unzip the data into a directory named <code class="literal">ny.gnp</code>.</li></ol></div><p>At this point, you should have a directory with several subdirectories containing data files. The structure should look something like the following screenshot:</p><div><img src="img/4139OS_07_09.jpg" alt="Getting the data"/></div><p>Some of the data is ready to go, but before we use it, we need to extract the data from the Excel files. Let's turn our attention there.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec43"/>Parsing the Excel files</h2></div></div></div><p>Before we can extract the data<a id="id516" class="indexterm"/> from the Excel files, we need to find out what our input for this will be. If we open up one of the Excel files, in this case <code class="literal">CTS_Assault.xls</code>, we'll see something similar to the following screenshot:</p><div><img src="img/4139OS_07_10.jpg" alt="Parsing the Excel files"/></div><p>Let's list out some of the features of the sheets that we'll need to take into account:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">There are about thirteen rows of headers, most of which are hidden in the preceding screenshot.</li><li class="listitem" style="list-style-type: disc">Again, not shown in the preceding screenshot, but some of the files have more than one tab of data.</li><li class="listitem" style="list-style-type: disc">There are some hidden columns between columns A and D.</li><li class="listitem" style="list-style-type: disc">The subregion isn't listed on each row, so we'll need some way to carry this over.</li><li class="listitem" style="list-style-type: disc">All the years for each crime and country combination are listed on one row. We'll probably want to pivot that so that there's a column for the crime, one for the country, one for the year, and one for the data value.</li><li class="listitem" style="list-style-type: disc">There is a lot of missing data. We can filter that out.</li></ul></div><p>To get into the Excel files, we'll use the Apache POI project<a id="id517" class="indexterm"/> (<a class="ulink" href="http://poi.apache.org/">http://poi.apache.org/</a>). This library provides access to file formats of Microsoft Office's suites.</p><p>We'll use this library to extract the data from the Excel files in several stages, as follows:</p><div><ol class="orderedlist arabic"><li class="listitem">Pull raw data rows out of the Excel files</li><li class="listitem">Populate a tree of data that groups the data hierarchically by region, subregion, and country</li><li class="listitem">Flatten the hierarchically arranged data back into a sequence of maps containing all the data for each row</li><li class="listitem">Wrap all of this in one easy-to-use function</li></ol></div><p>Let's follow the <a id="id518" class="indexterm"/>preceding steps for the rest of this section, and in the end we'll add a controller function that pulls it all together.</p><p>We'll keep all of this code in a single module. The following namespace declaration for this will include all the dependencies that we'll need. For the fully specified <code class="literal">project.clj</code> file that includes all of these, refer to the code download for this chapter. I named the project <code class="literal">nullh</code>, so the file that I'm working with here is named <code class="literal">src/nullh/unodc.clj</code>.</p><div><pre class="programlisting">(ns nullh.unodc
  (:require [clojure.java.io :as io]
            [clojure.string :as str]
            [me.raynes.fs :as fs]
            [clojure.data.json :as json]
            [nullh.utils :as u])
  (:import
    [java.io FileInputStream]
    [org.apache.poi.ss.usermodel
     Cell CellStyle DataFormat Font RichTextString Row Sheet]
    [org.apache.poi.hssf.usermodel HSSFWorkbook]))</pre></div><p>Now we can start populating this namespace.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec44"/>Pulling out raw data</h2></div></div></div><p>For the first<a id="id519" class="indexterm"/> stage of the <a id="id520" class="indexterm"/>process, in which we read the data into a series of raw data rows, we'll use a couple of record types, as shown in the following code. The first, <code class="literal">sheet-data</code>, associates the title of the worksheet with the data in it. The second, <code class="literal">xl-row</code>, simply stores the data in each row's cells into named fields.</p><div><pre class="programlisting">(defrecord sheet-data [sheet-name sheet-rows])
(defrecord xl-row
  [sheet region sub-region country
   count-2003 count-2004 count-2005 count-2006 count-2007
   count-2008 count-2009 count-2010 count-2011
   rate-2003 rate-2004 rate-2005 rate-2006 rate-2007
   rate-2008 rate-2009 rate-2010 rate-2011])</pre></div><p>As we interact with the worksheet's data and API, we'll use a number of utilities that makes access to the worksheet objects more like working with native Clojure objects. The following are some of those utilities:</p><div><pre class="programlisting">(defn sheets [workbook]
  (-&gt;&gt; workbook
    (.getNumberOfSheets)
    (range)
    (map #(.getSheetAt workbook %))))
(defn rows [sheet]
  (-&gt;&gt; sheet
    (.getPhysicalNumberOfRows)
    (range)
    (map #(.getRow sheet %))
    (remove nil?)))
(defn cells [row]
  (-&gt;&gt; row
    (.getPhysicalNumberOfRows)
    (range)
    (map #(.getCell row %))))</pre></div><p>We'll spend a lot<a id="id521" class="indexterm"/> of time accessing cells' values. We'll want to make a<a id="id522" class="indexterm"/> simpler, more Clojure-like wrapper around the Java library's API for accessing them. How we do this will depend on the cell's type, and we can use <em>multimethods</em> to handle dispatching for it, as shown in the following code:</p><div><pre class="programlisting">(defn cell-type [cell]
  (if (nil? cell)
    nil
    (let [cell-types {Cell/CELL_TYPE_BLANK   :blank
                      Cell/CELL_TYPE_BOOLEAN :boolean
                      Cell/CELL_TYPE_ERROR   :error
                      Cell/CELL_TYPE_FORMULA :formula
                      Cell/CELL_TYPE_NUMERIC :numeric
                      Cell/CELL_TYPE_STRING  :string}]
      (cell-types (.getCellType cell)))))
(defmulti cell-value cell-type)
(defmethod cell-value :blank   [_] nil)
(defmethod cell-value :boolean [c] (.getBooleanCellValue c))
(defmethod cell-value :error   [c] (.getErrorCellValue   c))
(defmethod cell-value :formula [c] (.getErrorCellValue   c))
(defmethod cell-value :numeric [c] (.getNumericCellValue c))
(defmethod cell-value :string  [c] (.getStringCellValue  c))
(defmethod cell-value :default [c] nil)</pre></div><p>Now, with these methods in place, we can easily read the data into a sequence of data rows. First, we'll need to open the workbook file with the following code:</p><div><pre class="programlisting">(defn open-file [filename]
  (with-open [s (io/input-stream filename)]
    (HSSFWorkbook. s)))</pre></div><p>And we can take each sheet and read it into a <code class="literal">sheet-data</code> record with the following code:</p><div><pre class="programlisting">(defn get-sheet-data [sheet]
  (-&gt;sheet-data (.getSheetName sheet) (rows sheet)))</pre></div><p>The rows <a id="id523" class="indexterm"/>themselves<a id="id524" class="indexterm"/> will need to go through a number of transformations, all without touching the sheet name field. To facilitate this, we'll define a higher order function that maps a function over the rows field, as follows:</p><div><pre class="programlisting">(defn on-rows [sheet f]
  (assoc sheet :sheet-rows (f (:sheet-rows sheet))))</pre></div><p>The first row transformation will involve skipping the header rows for each sheet, as shown in the following code:</p><div><pre class="programlisting">(defn first-cell-empty? [cells]
  (empty? (cell-value (first cells))))
(defn skip-headers [sheet]
  (on-rows sheet (fn [r]
                   (-&gt;&gt; r
                     (drop-while #(first-cell-empty? (cells %)))
                     (drop 1)
                     (take-while #(not (first-cell-empty? %)))))))</pre></div><p>Now we can take the sequence of <code class="literal">sheet-data</code> records and flatten them by adding the sheet name onto the row data as follows:</p><div><pre class="programlisting">(defn row-values [sheet-name row]
  (conj (mapv cell-value (cells row)) sheet-name))
(defn sheet-data-&gt;seq [sheet]
  (map #(row-values (:sheet-name sheet) %) (:sheet-rows sheet)))</pre></div><p>We do need to take each row and clean it up by rearranging the field order, making sure it has exactly the right number of fields with the help of the following code:</p><div><pre class="programlisting">(defn clean-row [row]
  (u/pad-vec 22
             (concat (list (last row) (first row))
                     (take 11 (drop 3 row))
                     (drop 15 row))))</pre></div><p>Now that we've <a id="id525" class="indexterm"/>hardened our data a little, we can take the Clojure <a id="id526" class="indexterm"/>vectors and populate the <code class="literal">xl-row</code> records with them as follows:</p><div><pre class="programlisting">(defn seq-&gt;xl-row [coll] (apply -&gt;xl-row coll))</pre></div><p>Finally, we have a fairly clean sequence of row data.</p><div><div><div><div><h3 class="title"><a id="ch07lvl3sec25"/>Growing a data tree</h3></div></div></div><p>Unfortunately, we haven't<a id="id527" class="indexterm"/> yet dealt with some problems, such as the subregion not being populated in every row. Let's take care of that now.</p><p>We'll tackle that problem by changing the sequence of records into a hierarchical tree of data. The tree is represented by a number of record types as shown in the following code:</p><div><pre class="programlisting">(defrecord region [region-name sub-regions])
(defrecord sub-region [sub-region-name countries])
(defrecord country [country-name counts rates sheet])
(defrecord yearly-data
   [year-2003 year-2004 year-2005 year-2006 year-2007 year-2008
    year-2009 year-2010 year-2011])</pre></div><p>To build the tree, we'll have a number of functions. Each takes a group of data that will go into one tree or subtree. It populates that part of the tree and returns it.</p><p>The first of these functions is <code class="literal">xl-rows-&gt;regions</code>. It takes a sequence of <code class="literal">xl-rows</code>, groups them by region, and constructs a tree of <code class="literal">region</code> records for it as shown in the following code:</p><div><pre class="programlisting">(defn xl-rows-&gt;regions [coll]
  (-&gt;&gt; coll
    (group-by :region)
    (map #(-&gt;region
            (first %) (xl-rows-&gt;sub-regions (second %))))))</pre></div><p>The most complicated part of building this tree is dealing with the missing subregions. We'll use three functions to deal with that. The first, <code class="literal">conj-into</code>, conjugates onto a value in a map, or adds a new vector containing the data if there's no data for that key. The second, <code class="literal">fold-sub-region</code>, folds each row into a map based on either the subregion referred to in the row, or the last specified subregion. Finally, <code class="literal">xl-rows-&gt;sub-regions</code> takes a sequence of rows from one region, divides them into subregions, and creates the <code class="literal">sub-region</code> records for them, as shown in the following code:</p><div><pre class="programlisting">(defn conj-into [m k v]

  (if (contains? m k)
    (assoc m k (conj (get m k) v))
    (assoc m k [v])))
(defn fold-sub-region [state row]
  (let [[current accum] state]
    (if (str/blank? (:sub-region row))
      [current
       (conj-into accum current (assoc row :sub-region current))]
      (let [new-sub-region (:sub-region row)]
        [new-sub-region
         (conj-into accum new-sub-region row)]))))
(defn xl-rows-&gt;sub-regions [coll]
  (-&gt;&gt; coll
    (reduce fold-sub-region [nil {}])
    second
    (map #(-&gt;sub-region
            (first %) (xl-rows-&gt;countries (second %))))))</pre></div><p>Now that we have the <a id="id528" class="indexterm"/>subregions identified, we can build a tree for each country. For that, we'll pull the count data and the rate data into their own structures and put it all together into a <code class="literal">country</code> record with the following code:</p><div><pre class="programlisting">(defn xl-rows-&gt;countries [coll]
  (-&gt;&gt; coll
    (group-by :country)
    (map #(let [[country-name [row &amp; _]] %]
            (-&gt;country country-name
                       (xl-row-&gt;counts row)
                       (xl-row-&gt;rates row)
                       (:sheet row))))))</pre></div><p>The counts and rates are represented by the same record type, so we'll use a shared function to pull the fields from the row that populate the fields in the type as shown in the following code:</p><div><pre class="programlisting">(defn xl-row-&gt;yearly [coll fields]
  (apply -&gt;yearly-data (map #(get coll %) fields)))
(defn xl-row-&gt;counts [coll]
  (xl-row-&gt;yearly
    coll
    [:count-2003 :count-2004 :count-2005 :count-2006 :count-2007
     :count-2008 :count-2009 :count-2010 :count-2011]))
(defn xl-row-&gt;rates [coll]
  (xl-row-&gt;yearly
    coll
    [:rate-2003 :rate-2004 :rate-2005 :rate-2006 :rate-2007
     :rate-2008 :rate-2009 :rate-2010 :rate-2011]))</pre></div><p>These functions all <a id="id529" class="indexterm"/>build the hierarchy of data that's stored in the worksheets.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec26"/>Cutting down the data tree</h3></div></div></div><p>We reverse the process to <a id="id530" class="indexterm"/>flatten the data again. In the process, this implicitly populates the missing subregions into all of the rows. Let's see how this works.</p><p>To begin with, we take a sequence of regions and convert each one into a sequence of <code class="literal">xl-row</code> records, as shown in the following code:</p><div><pre class="programlisting">(defn region-&gt;xl-rows [tree nil-row]
  (let [region-row (assoc nil-row :region (:region-name tree))]
    (mapcat #(sub-regions-&gt;xl-rows % region-row)
            (:sub-regions tree))))
(defn regions-&gt;xl-rows [region-coll]
  (let [nil-row (seq-&gt;xl-row (repeat 22 nil))]
    (mapcat #(region-&gt;xl-rows % nil-row) region-coll)))</pre></div><p>Just as before, this work will be delegated to other functions; in this case, <code class="literal">sub-regions-&gt;xl-rows</code>, which again delegates to <code class="literal">country-&gt;xl-rows</code>. The second function in the following code is a little long (and so I've omitted some lines from it), but both are conceptually simple:</p><div><pre class="programlisting">(defn country-&gt;xl-rows [tree sub-region-row]
  (let [counts (:counts tree), rates (:rates tree)]
    (assoc sub-region-row
           :sheet (:sheet tree)
           :country (:country-name tree)
           :count-2003 (:year-2003 counts)
           :count-2004 (:year-2004 counts)
           ;; ...
           :rate-2003 (:year-2003 rates)
           :rate-2004 (:year-2004 rates)
           ;; ...
           )))
(defn sub-regions-&gt;xl-rows [tree region-row]
  (let [sub-region-row (assoc region-row :sub-region
                              (:sub-region-name tree))]
    (map #(country-&gt;xl-rows % sub-region-row) (:countries tree))))</pre></div><p>At this point, we have a sequence of data rows with the missing subregions supplied. But we're still not done.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec27"/>Putting it all together</h3></div></div></div><p>We'll provide several levels of<a id="id531" class="indexterm"/> function to make this easier. First, one that ties together everything that we've seen so far. It takes a filename and returns a sequence of <code class="literal">xl-row</code> records as follows:</p><div><pre class="programlisting">(defn read-sheets [filename]
  (-&gt;&gt; filename
    (open-file)
    (sheets)
    (map get-sheet-data)
    (map skip-headers)
    (map (fn [s] (on-rows s #(remove empty? %))))
    (mapcat sheet-data-&gt;seq)
    (map clean-row)
    (map seq-&gt;xl-row)
    (xl-rows-&gt;regions)
    (regions-&gt;xl-rows)))</pre></div><p>That's it. We have our data read in. It's been processed a little, but it's still pretty raw. The following is an example row:</p><div><pre class="programlisting">{:sheet "CTS 2012 Domestic Burglary",
 :region "Africa",
 :sub-region "Middle Africa",
 :country "Sao Tome and Principe",
 :count-2003 nil,
 :count-2004 nil,
 :count-2005 nil,
 :count-2006 2.0,
 :count-2007 0.0,
 :count-2008 2.0,
 :count-2009 5.0,
 :count-2010 16.0,
 :count-2011 20.0,
 :rate-2003 nil,
 :rate-2004 nil,
 :rate-2005 nil,
 :rate-2006 1.290572368845583,
 :rate-2007 0.0,
 :rate-2008 1.2511573205214825,
 :rate-2009 3.0766390794695875,
 :rate-2010 9.673694202434143,
 :rate-2011 11.867604998635224}</pre></div><p>We still need to clean it up a little and pivot the data to put each data value into its own row. Instead of having one row with <code class="literal">:count-2003</code>, <code class="literal">:count-2004</code>, and so on, we'll have many rows, each with <code class="literal">:count</code> and <code class="literal">:year</code>.</p><p>Let's turn our attention there next.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec28"/>Transforming the data</h3></div></div></div><p>So far, we've only <a id="id532" class="indexterm"/>lightly cleaned part of our data. We haven't even looked at the data that we want to correlate the crime data with. Also, the shape of the data is awkward for the analyses that we want to conduct, so we'll need to pivot it the way we described earlier. We'll see more about this in a minute.</p><p>For this stage of processing, we want to put all of the code into a new file. We'll name this file <code class="literal">src/nullh/data.clj</code>, and the namespace declaration for it looks as follows:</p><div><pre class="programlisting">(ns nullh.data
  (:require [incanter.core :as i]
            [incanter.io :as iio]
            [clojure.set :as set]
            [clojure.string :as str]
            [clojure.data.csv :as csv]
            [clojure.data.json :as json]
            [clojure.java.io :as io]
            [me.raynes.fs :as fs]
            [nullh.unodc :as unodc]
            [nullh.utils :as u]))</pre></div><p>We'll now start working with Incanter datasets. We haven't used Incanter much so far in this book, and that's a little unusual, because Incanter is one of the go-to libraries for working with numbers and statistics in Clojure. It's powerful and flexible, and it makes working with data easy.</p><p>Let's take the data that we read from the Excel files and import it into an Incanter dataset. We need to read the data into one long sequence, pull out  the keys for the data fields, and then create the dataset as follows:</p><div><pre class="programlisting">(defn read-cts-data [dirname]
  (let [input (mapcat unodc/read-sheets (u/ls dirname))
        cols (keys (first input))]
    (i/dataset cols (doall (map #(map second %) input)))))</pre></div><p>Now we can read the data that we downloaded from the World Bank into another dataset. Both data files have roughly the same fields, so we can use the same function for both. Unfortunately, we need to load the CSV ourselves, because Incanter's introspection doesn't quite give us the results that we want. Because of this, we'll also include a few functions for converting the data into doubles as we read it in, and we'll define those columns that the data contains, as follows:</p><div><pre class="programlisting">(def headers [:country-name :country-code :indicator-name
              :indicator-code :1961 :1962 :1963 :1964 :1965 :1966
              :1967 :1968 :1969 :1970 :1971 :1972 :1973 :1974
              :1975 :1976 :1977 :1978 :1979 :1980 :1981 :1982
              :1983 :1984 :1985 :1986 :1987 :1988 :1989 :1990
              :1991 :1992 :1993 :1994 :1995 :1996 :1997 :1998
              :1999 :2000 :2001 :2002 :2003 :2004 :2005 :2006
              :2007 :2008 :2009 :2010 :2011 :2012 :2013])
(defn -&gt;double [x] (if (str/blank? x) nil (Double/parseDouble x)))
(defn coerce-row [row]
  (let [[x y] (split-at 4 row)]
    (concat x (map -&gt;double y))))
(defn read-indicator-data [filename]
  (with-open [f (io/reader filename)]
    (-&gt;&gt; f
      csv/read-csv
      (drop 3)
      (map coerce-row)
      doall
      (i/dataset headers))))</pre></div><p>We can<a id="id533" class="indexterm"/> use the <code class="literal">read-indicator-data</code> function to load data from the two World Bank indicators that we downloaded earlier.</p><p>Now we want to put all the data from UNODC together with either of the World Bank datasets. As we do that, we'll also pivot the data tables so that instead of one column for each year, there's one column containing the year and one containing the value for that year. At the same time, we'll remove rows with missing data and aggregate the counts for all of the crimes for a country for each year.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec29"/>Joining the data sources</h3></div></div></div><p>Bringing the two data sources <a id="id534" class="indexterm"/>together is relatively simple and can be done with the following code:</p><div><pre class="programlisting">(defn join-all [indicator cts]
  (i/$join [:country-name :country] indicator cts))</pre></div><p>Basically, we just let Incanter join the two data structures on the fields by matching the World Bank data's <code class="literal">:country-name</code> field with the UNODC data's <code class="literal">:country</code> field.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec30"/>Pivoting the data</h3></div></div></div><p>Now that the data has been<a id="id535" class="indexterm"/> joined, we can pivot it. In the end, we want to have the following fields on every row:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">region</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">subregion</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">country</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">country-code</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">indicator</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">indicator-code</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">crime</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">year</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">count</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">rate</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">indicator-value</code></li></ul></div><p>As you can see, some of these fields are from the UNODC data and some are from the World Bank data.</p><p>We'll do this translation on a sequence of maps instead of the dataset. We'll get started with the following code:</p><div><pre class="programlisting">(defn pivot-map [m]
  (let [years [2003 2004 2005 2006 2007 2008 2009 2010 2011]]
    (map #(pivot-year m %) years)))
(defn pivot-data [map-seq] (mapcat pivot-map map-seq))</pre></div><p>First, we use <code class="literal">-&gt;maps</code> to convert the<a id="id536" class="indexterm"/> dataset to a sequence of maps. Then, pass the processing off to <code class="literal">pivot-map</code>. This function pivots the data for each year.</p><p>We pivot the data for each year separately. We do this by repeatedly transforming the data map for a row. This is a great example of how Clojure's immutability makes things easier. We don't have to worry about copying the map or clobbering any data. We can just modify the original data multiple times, saving the result of each transformation process as a separate, new data row.</p><p>The process itself is fairly simple. First, we use the year to create keywords for the fields that we are interested in. Next, we select the rows that we want to keep from the original data map. Then we rename a few to make them clearer. And finally, we add the year to the output map as follows:</p><div><pre class="programlisting">(defn pivot-year [m year]
  (let [count-key (keyword (str "count-" year))
        rate-key (keyword (str "rate-" year))
        year-key (keyword (str year))]
    (-&gt; m
      (select-keys [:region :sub-region :country :country-code
                    :indicator :indicator-code
                    :sheet count-key rate-key year-key])
      (set/rename-keys {:sheet :crime,
                        count-key :count,
                        rate-key :rate,
                        year-key :indicator-value})
      (assoc :year year))))</pre></div><p>That's it. This should make<a id="id537" class="indexterm"/> the data easier to work with. We can do some more transformations on the data and clean it up a bit further.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec31"/>Filtering the missing data</h3></div></div></div><p>First, there are a lot of <a id="id538" class="indexterm"/>holes in the data, and we don't want to have to worry about that. So if a row is missing any of the three data fields (<code class="literal">:count</code>, <code class="literal">:rate</code>,or <code class="literal">:indicator-value</code>), let's get rid of it with the following code:</p><div><pre class="programlisting">(defn remove-missing [coll]
  (let [fields [:count :rate :indicator-value]
        has-missing (fn [r] (some nil? (map r fields)))]
    (remove has-missing coll)))</pre></div><p>We just check whether any of these fields has a <code class="literal">nil</code> value. If any of them do, we remove that row.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec32"/>Putting it all together</h3></div></div></div><p>Let's make a wrapper function<a id="id539" class="indexterm"/> around this process. That'll help us stay consistent and make the library easier to use. This loads the data from UNODC and one of the World Bank datasets. It joins, pivots, and removes the missing rows before returning an Incanter dataset, as shown in the following code:</p><div><pre class="programlisting">(defn -&gt;maps [dset]
  (let [col-names (i/col-names dset)]
    (map #(zipmap col-names %) (i/to-list dset))))
(defn load-join-pivot [cts-dir data-file]
  (let [cts (read-cts-data cts-dir)
        indicator-data (read-indicator-data data-file)]
    (-&gt;&gt; (join-all indicator-data cts)
      -&gt;maps
      pivot-data
      remove-missing
      i/to-dataset)))</pre></div><p>Let's use these functions to load up one of the datasets as follows:</p><div><pre class="programlisting">(def d (d/load-join-pivot
         "unodc-data"
         "ag.lnd/ag.lnd.totl.k2_Indicator_en_csv_v2.csv"))</pre></div><p>At this point, the data is in decent shape—actually, as good as this data is probably going to get (more about that near the end of this chapter). So let's see what's in the data and what it has to tell us.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec52"/>Exploring the data</h1></div></div></div><p>Let's explore a little<a id="id540" class="indexterm"/> and try to get a feel for the data. First, let's try to get some <a id="id541" class="indexterm"/>summary statistics for the various datasets. Afterward, we'll generate some graphs to get a more intuitive sense for what's in the data and how they're related.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec45"/>Generating summary statistics</h2></div></div></div><p>Incanter makes generating <a id="id542" class="indexterm"/>summary statistics easy. You can pass a dataset to the <code class="literal">incanter.stats/summary</code> function. It returns a sequence of maps. Each map represents the summary data for each column in the original dataset. This includes whether the data is numeric or not. For nominal data, it returns some sample items and their counts. For numeric data, it returns the mean, median, minimum, and maximum.</p><div><div><div><div><h3 class="title"><a id="ch07lvl3sec33"/>Summarizing UNODC crime data</h3></div></div></div><p>If we load the data and<a id="id543" class="indexterm"/> filter it for the crime of "burglary", we can get the summary statistics for those fields as follows:</p><div><pre class="programlisting">(s/summary
  (i/$where {:crime {:$eq "CTS 2012 Burglary"}} by-ag-lnd))</pre></div><p>And if we pick apart the data structures that it outputs, the following are the summary statistics for the primary data fields:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Column</p>
</th><th style="text-align: left" valign="bottom">
<p>Minimum</p>
</th><th style="text-align: left" valign="bottom">
<p>Maximum</p>
</th><th style="text-align: left" valign="bottom">
<p>Mean</p>
</th><th style="text-align: left" valign="bottom">
<p>Median</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<strong>Rate</strong>
</p>
</td><td style="text-align: left" valign="top">
<p>0.1</p>
</td><td style="text-align: left" valign="top">
<p>1939.23</p>
</td><td style="text-align: left" valign="top">
<p>376.4</p>
</td><td style="text-align: left" valign="top">
<p>292.67</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<strong>Count</strong>
</p>
</td><td style="text-align: left" valign="top">
<p>11</p>
</td><td style="text-align: left" valign="top">
<p>443010</p>
</td><td style="text-align: left" valign="top">
<p>60380</p>
</td><td style="text-align: left" valign="top">
<p>17184</p>
</td></tr></tbody></table></div><p>So, from the preceding table, we see that both fields have wide variance and are skewed somewhat, based on the differences between the means and the medians. These two having similar distributions is to be expected, since the rate is derived from the count.</p><p>Charts and graphs can also help to understand our data better. Incanter makes generating charts quite simple. Let's see how to do that.</p><p>First, we'll load the data and pivot it, since that will make it easier to pull the data out of the graph. For this example, we'll load the UNODC crime data joined to the World Bank land area data as follows:</p><div><pre class="programlisting">(def by-ag-lnd
  (d/load-join-pivot
    "unodc-data"
    "ag.lnd/ag.lnd.totl.k2_Indicator_en_csv_v2.csv"))</pre></div><p>Next, we'll filter the dataset to contain only the burglary data as shown in the following code:</p><div><pre class="programlisting">(def burglary
  (i/$where {:crime {:$eq "CTS 2012 Burglary"}} by-ag-lnd))</pre></div><p>Finally, we use the <code class="literal">incanter.charts/histogram</code> function to create the graph, and the <code class="literal">incanter.core/view</code> function to display it to the screen with the following code:</p><div><pre class="programlisting">(def h
  (c/histogram (i/sel burglary :cols :count)
                :nbins 30
                :title "Burglary Counts"
                :x-label "Burglaries"))
(i/view h)</pre></div><p>The following is the <a id="id544" class="indexterm"/>histogram of the <code class="literal">:count</code> field:</p><div><img src="img/4139OS_07_11.jpg" alt="Summarizing UNODC crime data"/></div><p>From this graph, we can see that <a id="id545" class="indexterm"/>this data does not follow a normal distribution. How does the other data correspond?</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec34"/>Summarizing World Bank land area and GNI data</h3></div></div></div><p>We can use the<a id="id546" class="indexterm"/> same <a id="id547" class="indexterm"/>function, <code class="literal">incanter.stats/summary</code>, to generate the same statistics for the land area data that is given in the following table:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Column</p>
</th><th style="text-align: left" valign="bottom">
<p>Minimum</p>
</th><th style="text-align: left" valign="bottom">
<p>Maximum</p>
</th><th style="text-align: left" valign="bottom">
<p>Mean</p>
</th><th style="text-align: left" valign="bottom">
<p>Median</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<strong>Land area</strong>
</p>
</td><td style="text-align: left" valign="top">
<p>300</p>
</td><td style="text-align: left" valign="top">
<p>16381390</p>
</td><td style="text-align: left" valign="top">
<p>822324</p>
</td><td style="text-align: left" valign="top">
<p>100250</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<strong>GNI</strong>
</p>
</td><td style="text-align: left" valign="top">
<p>240</p>
</td><td style="text-align: left" valign="top">
<p>88500</p>
</td><td style="text-align: left" valign="top">
<p>17170</p>
</td><td style="text-align: left" valign="top">
<p>8140</p>
</td></tr></tbody></table></div><p>The World Bank land <a id="id548" class="indexterm"/>area data has a distribution that is similar to the crime data. Smaller, less wealthy countries are, of course, more numerous. The distribution of the land area values is given as follows:</p><div><img src="img/4139OS_07_12.jpg" alt="Summarizing World Bank land area and GNI data"/></div><p>The following is the <a id="id549" class="indexterm"/>distribution of the GNI values:</p><div><img src="img/4139OS_07_13.jpg" alt="Summarizing World Bank land area and GNI data"/></div><p>This gives us some feel for the<a id="id550" class="indexterm"/> data. All of these follow an exponential distribution, as we can see in the next graph:</p><div><img src="img/4139OS_07_14.jpg" alt="Summarizing World Bank land area and GNI data"/></div><p>This makes it clear that all graphs with exponential distribution start with a steep drop and quickly flatten out into a near-flat line.</p></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec46"/>Generating more charts and graphs</h2></div></div></div><p>Some more charts can<a id="id551" class="indexterm"/> help us begin to understand the relationship <a id="id552" class="indexterm"/>between some of these variables. We'll write a function to plot any crime against the World Bank indicator data joined into the current dataset.</p><p>First, however, we'll need a utility function to filter the data rows by the crime. This is a data-oriented function, so we'll store it in <code class="literal">nullh.data</code>, as shown in the following code:</p><div><pre class="programlisting">(defn by-crime [dset crime-label]
  (i/$where {:crime {:$eq crime-label}} dset))</pre></div><p>The next function, <code class="literal">plot-crime</code>, pulls out the data points and then passes everything to the <code class="literal">incanter.charts/scatter-plot</code> function to generate the graph:</p><div><pre class="programlisting">(defn plot-crime [dset indicator-label crime-label]
  (let [x (i/sel dset :cols :indicator-value)
        y (i/sel dset :cols :rate)
        title (str indicator-label " and " crime-label)]
    (c/scatter-plot x y
                    :title title
                    :x-label indicator-label
                    :y-label crime-label)))</pre></div><p>This makes it easy to get a quick, visual comparison of data about different types of crimes and how they relate to the World Bank indicator data.</p><p>For example, the<a id="id553" class="indexterm"/> following code shows us how the burglary ("CTS 2012 Burglary") relates<a id="id554" class="indexterm"/> to the land area data (the <code class="literal">plot-crime</code> function is in the <code class="literal">nullh.charts</code> namespace, which is aliased as <code class="literal">n-ch</code>):</p><div><pre class="programlisting">(def by-ag-lnd
  (d/load-join-pivot
    "unodc-data"
    "ag.lnd/ag.lnd.totl.k2_Indicator_en_csv_v2.csv"))
(def ag-plot
  (n-ch/plot-crime (d/by-crime by-ag-lnd "CTS 2012 Burglary")
                   "Land Area" "Burglary"))
(i/view ag-plot)</pre></div><p>The preceding code produces the following graph:</p><div><img src="img/4139OS_07_15.jpg" alt="Generating more charts and graphs"/></div><p>This data appears to have some strange artifacts. Look at the line of data points where the land area is around 9,000,000, stretching from about 500 burglaries per year to almost 1,000 burglaries. What is that about?</p><p>Well, when we think <a id="id555" class="indexterm"/>about it, the land area of a country rarely changes, but if a country has burglary data for several years, we'll have the land area represented <a id="id556" class="indexterm"/>those many times. We could simplify the data by getting the average of the data.</p><p>In order to do this, we aggregate all of the year data for each country. To do that, we'll use the following function:</p><div><pre class="programlisting">(defn aggregate-years-by-country
  ([dset] (aggregate-years-by-country dset :mean))
  ([dset by]
   (let [data-cols [:count :rate :indicator-value]]
     (-&gt;&gt; [:count :rate :indicator-value]
       (map #(i/$rollup by % :country dset))
       (reduce #(i/$join [:country :country] %1 %2))))))</pre></div><p>The preceding code uses the <code class="literal">incanter.core/$rollup</code> function to get each country's average for each data column. It then uses <code class="literal">reduce</code> and <code class="literal">incanter.core/$join</code> to fold the data back into one dataset.</p><p>When we graph aggregated data, we get the following graph:</p><div><img src="img/4139OS_07_16.jpg" alt="Generating more charts and graphs"/></div><p>This makes it clearer that there is probably no relationship between these two variables.</p><p>The following graph compares the burglary data to the GNI per capita. Since that indicator doesn't typically vary much over the time span represented in the data (China and a few other countries not withstanding), we have again aggregated each country's data.</p><div><img src="img/4139OS_07_17.jpg" alt="Generating more charts and graphs"/></div><p>This data appears to <a id="id557" class="indexterm"/>possibly have a correlation between these two <a id="id558" class="indexterm"/>variables, although it may not be very strong. This is something we can test.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec53"/>Conducting the experiment</h1></div></div></div><p>Now we're <a id="id559" class="indexterm"/>ready to<a id="id560" class="indexterm"/> frame and perform the experiment. Let's walk through the steps to do that one more time.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec47"/>Formulating an initial hypothesis</h2></div></div></div><p>In this case, our <a id="id561" class="indexterm"/>hypothesis is that <em>there is a relationship between the per capita gross national income and the rate of burglaries</em>. We could go further and make the hypothesis stronger by specifying that higher GNI correlates to a higher burglary rate, somewhat counter-intuitively.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec48"/>Stating the null and alternative hypotheses</h2></div></div></div><p>Given that statement of our <a id="id562" class="indexterm"/>working<a id="id563" class="indexterm"/> hypothesis, we can now formulate the null and alternative hypotheses.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><em>H<sub>0</sub></em>: There is no relationship between the per capita gross national income and a country's burglary rate.</li><li class="listitem" style="list-style-type: disc"><em>H<sub>1</sub></em>: There is a relationship between the per capita gross national income and the country's burglary rate.</li></ul></div><p>These statements will now guide us through the rest of the process.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec49"/>Identifying the statistical assumptions in the sample</h2></div></div></div><p>There are a number of <a id="id564" class="indexterm"/>assumptions in this data that we need to be aware of. First, since the crime data comes from multiple sources, there's going to be very little consistency in it.</p><p>To start with, the very definitions of these crimes may vary widely between different countries. Also, data collection procedures and practices will make the reliability of those numbers difficult.</p><p>The World Bank data is perhaps more consistent—things like land area can be measured and validated externally—but GNI can be reliant upon the own country's reporting, and that may often be inflated, as countries attempt to make themselves look more important and influential.</p><p>Moreover, there are also a lot of holes in the data. Because we haven't normalized the country names, there are no observations for the United States. It's listed as "United States" in one dataset and as "United States of America" in the other. And while this single instance would be simple to correct, we really have to do a more thorough audit of the country names.</p><p>So while there's nothing systematic that we need to take into account, there are several problems with the data that we need to keep in mind. We'll revisit these closer to the end of this chapter.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec50"/>Determining which tests are appropriate</h2></div></div></div><p>Now we have to <a id="id565" class="indexterm"/>determine which tests to run. Some tests are appropriate to different types of data and to different distributions of data. For example, nominal and numeric data require very different analyses.</p><p>If the relationship were known to be linear, we could use Pearson's correlation coefficient. Unfortunately, the relationship in our data appears to be more complicated than that.</p><p>In this case, our data is continuous numeric data. And we're interested in the relationship between two variables, but neither is truly independent, because we're not really sure exactly how the <em>sampling</em> was done, based on the description of the assumptions given earlier.</p><p>Because of all these<a id="id566" class="indexterm"/> factors, we'll use Spearman's rank correlation.</p><p>How did I pick this? It's fairly simple, but just complicated enough that we will not go into the details here.</p><p>The main point is that which statistical test you use is highly dependent on the nature of your data. Much of this knowledge comes from learning and experience, but once you've determined your data, a good statistical textbook or any of a number of online flowcharts can help you pick the right test.</p><p>But what is Spearman's rank correlation? Let's take a minute and find out.</p><div><div><div><div><h3 class="title"><a id="ch07lvl3sec35"/>Understanding Spearman's rank correlation coefficient</h3></div></div></div><p>Spearman's rank correlation coefficient <a id="id567" class="indexterm"/>measures the association between two variables. It is particularly useful when only the rank of the data is known, but it can also be useful in other situations. For instance, it isn't thrown off by outliers, because it only looks at the rank.</p><p>The formula for this statistic is as follows:</p><div><img src="img/4139OS_07_18.jpg" alt="Understanding Spearman's rank correlation coefficient"/></div><p>The value of <em>n</em> is the size of the sample. The value of <em>d</em> is each observation's difference in rank for the two variables. For example, in the data we've been looking at, Denmark ranks first for burglary (interesting), but third for per capita GNI. So Spearman's rank correlation would look at <em>3 – 1 = 2</em>.</p><p>A coefficient of <em>0</em> means there is no relationship between the two variables, and a coefficient of <em>-1</em> or <em>+1</em> means that the two variables are perfectly related. That is, the data can be perfectly described using a <a id="id568" class="indexterm"/>
<strong>monotonic</strong> function: a function from one variable to the other that preserves the order of the items. The function doesn't have to be linear. In fact, it could easily describe a curve. But it does capture the data.</p><p>The coefficient doesn't give<a id="id569" class="indexterm"/> us statistical significance (the <em>p</em> value), however. To get that, we just need to know that the Spearman's rank correlation coefficient is distributed approximately normally, when <em>n ≥ 10</em>. It has a mean of <em>0</em> and a standard deviation given as follows:</p><div><img src="img/4139OS_07_19.jpg" alt="Understanding Spearman's rank correlation coefficient"/></div><p>With these formulae, we can compute the <em>z</em> score of coefficient for our test. The <em>z</em> score is the distance of a data point from the mean, measured in standard deviations. The <em>p</em> value is closely related to the <em>z</em> score. So if we know the <em>z</em> score, we also know the <em>p</em> value.</p></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec51"/>Selecting the significance level</h2></div></div></div><p>Now we need to <a id="id570" class="indexterm"/>select how high of a bar we need the significance to rise to. The target <em>p</em> value is known as the <em>α</em> value. In general, <em>α = 0.05</em> is commonly used, although if you want to be extra careful, <em>α = 0.01</em> is also normal.</p><p>For this test, we'll just use <em>α = 0.05</em>.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec52"/>Determining the critical region</h2></div></div></div><p>We'll accept any kind of <a id="id571" class="indexterm"/>relationship for rejecting the null hypothesis, so this will be a two-tailed test. That means that the critical region will come from both sides of the curve, with their areas being 0.05 divided equally for 0.025 on each side.</p><p>This corresponds to a <em>z</em> score of <em>z &lt; -1.96</em> or <em>z &gt; 1.96</em>.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec53"/>Calculating the test statistic and its probability</h2></div></div></div><p>We can use<a id="id572" class="indexterm"/> Incanter's<a id="id573" class="indexterm"/> function, <code class="literal">incanter.stats/spearmans-rho</code>, to calculate the Spearman's coefficient. However, it doesn't only calculate the <em>z</em> score. We can easily create the following function that wraps all of these calculations. We'll put this into <code class="literal">src/nullh/stats.clj</code>. We'll name the function <code class="literal">spearmans</code>.</p><div><pre class="programlisting">(defn spearmans
  ([col-a col-b] (spearmans col-a col-b i/$data))
  ([col-a col-b dataset]
   (let [rho (s/spearmans-rho
               (i/sel dataset :cols col-a)
               (i/sel dataset :cols col-b))
         n (i/nrow dataset)
         mu 0.0
         sigma (Math/sqrt (/ 1.0 (- n 1.0)))
         z (/ (- rho mu) sigma)]
     {:rho rho, :n n, :mu mu, :sigma sigma, :z z})))</pre></div><p>Now, we can run this on the dataset. Let's start from the beginning and load the datasets from the disk with the following commands:</p><div><pre class="programlisting">
<strong>user=&gt; (def by-ny-gnp</strong>
<strong>         (d/load-join-pivot</strong>
<strong>           "unodc-data"</strong>
<strong>           "ny.gnp/ny.gnp.pcap.cd_Indicator_en_csv_v2.csv"))</strong>
<strong>#'user/by-ny-gnp</strong>
<strong>user=&gt; (def burglary (d/by-crime by-ny-gnp "CTS 2012 Burglary"))</strong>
<strong>#'user/burglary</strong>
<strong>user=&gt; (pprint (n-stat/spearmans :indicator-value :rate burglary))</strong>
<strong>{:rho 0.6938241467993876,</strong>
<strong> :n 537,</strong>
<strong> :mu 0.0,</strong>
<strong> :sigma 0.04319342127906801,</strong>
<strong> :z 16.063190325134588}</strong>
</pre></div><p>The preceding commands allowed us to see the process from front to back, and we can take the output and consider how the test went.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec54"/>Deciding whether to reject the null hypothesis or not</h2></div></div></div><p>The final <em>z</em>-score was 16.03. Going by <a id="id574" class="indexterm"/>the book, a <em>z</em>-score this high is usually not even included on the charts. This would be a significant result, which would allow us to reject the null hypothesis. So, from this we can conclude that there is a relationship between the per capita GNI and the burglary rate.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec54"/>Interpreting the results</h1></div></div></div><p>Of course, the results don't tell<a id="id575" class="indexterm"/> us a whole lot. For one, we have to remember that <a id="id576" class="indexterm"/>just because there's a relationship, that doesn't imply causality. Moreover, because the result is so significant, we should probably be skeptical about the results and whether they're caused by some artifact in the data or the procedures.</p><p>We've already talked about the problems in the data, and some of them may be at fault. Particularly, some of the data is missing because of normalization problems, which may change the results. Another possibility is that industrialized nations keep better records, so they would appear to have more burglaries.</p></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec55"/>Summary</h1></div></div></div><p>So, in this chapter, we learned how null hypothesis testing can help us structure our analyses. Having a well thought out and standard procedure also ensures that we are thorough in our analysis. For example, in this chapter, we were forced to confront the ugly truths about the data we were working with, and that gave us insights into the results that we achieved later.</p><p>In the next chapter, we'll actually get a chance to use these techniques again, when we look at conducting A/B testing on websites.</p></div></body></html>