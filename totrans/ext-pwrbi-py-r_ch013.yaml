- en: '12 Adding Statistics Insights: Outliers and Missing Values'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an effort to extend the data enrichment possibilities in Power BI through
    statistical functions, we will explore some methodologies to detect univariate
    and multivariate outliers in your dataset. In addition, advanced methodologies
    to impute possible missing values in datasets and time-series will be explored.
    Knowledge of these techniques is critical for the experienced analyst because
    Power BI does not provide useful tools for this purpose by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What outliers are and how to deal with them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing outlier detection algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What missing values are and how to deal with them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diagnosing missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing missing value imputation algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter requires you to have a working internet connection and **Power
    BI Desktop** installed on your machine. You must have properly configured the
    R and Python engines and IDEs as outlined in *Chapter 2, Configuring R With Power
    BI*, and *Chapter 3, Configuring Python with Power BI*.
  prefs: []
  type: TYPE_NORMAL
- en: What outliers are and how to deal with them
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generally, outliers are defined as those observations that lie at an *abnormal
    distance* from other observations in a data sample. In other words, they are *uncommon
    values* in a dataset. The abnormal distance we're talking about obviously doesn't
    have a fixed measurement but is strictly dependent on the dataset you're analyzing.
    Simply put, it will be the analyst who decides the distance beyond which to consider
    other abnormal distances based on their experience and functional knowledge of
    the business reality represented by the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It makes sense to talk about outliers for numeric variables or for numeric variables
    grouped by elements of categorical variables. It makes no sense to talk about
    outliers for categorical variables only.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'But why is there so much focus on managing outliers? The answer is that very
    often they cause undesirable macroscopic effects on some statistical operations.
    The most striking example is that of a linear correlation in the presence of an
    outlier in an "uncomfortable" position and the same calculated by eliminating
    the outlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – A simple scatterplot](img/file304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 – A simple scatterplot
  prefs: []
  type: TYPE_NORMAL
- en: 'As is evident in *Figure 12.1* and from what you learned from *Chapter 11,
    Adding Statistics Insights: Associations*, Pearson''s correlation *r* suffers
    greatly from outliers.'
  prefs: []
  type: TYPE_NORMAL
- en: But then, is it always sufficient to remove outliers *apriori* in order to solve
    any problems you might find downstream in your analysis? As you can imagine, the
    answer is “no,” because it all depends on the type of outlier you are dealing
    with.
  prefs: []
  type: TYPE_NORMAL
- en: The causes of outliers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before considering any action to be applied to the outliers of a variable,
    it is necessary to consider what might have generated them. Once the cause is
    established, it may be immediate to fix the outliers. Here is a possible categorization
    of the causes of outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data entry errors**: There may be an analyst collecting the data who made
    mistakes compiling it. If the analyst is collecting the birth dates of a group
    of people, it may be, for example, that instead of writing 1977, they write 177\.
    If the dates that they have collected belong to the range from 1900 to 2100, it
    is quite easy to correct the outlier that has been created due to the entry error.
    Other times, it is not possible to recover the correct value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intentional outliers**: Very often, the introduction of "errors" is intentional
    by the individuals to whom the measurements apply. For example, adolescents typically
    do not accurately report the amount of alcohol they consume.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data processing errors**: Data transformation processes that are usually
    applied to analytics solutions can introduce unintended errors, which in turn
    can give rise to possible outliers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sampling errors**: Sometimes, the data on which you perform your analysis
    must be sampled from a much larger dataset. It may be in this case that the analyst
    does not select a subset of data representing the entire population of data. For
    example, you need to measure the height of athletes, and, by mistake, you include
    some basketball players in your dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural outliers**: So-called “natural” outliers exist because they are part
    of the nature of business and are not the result of any kind of error. For example,
    it''s pretty much a given that shopping malls sell more products at Christmas
    time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the nature of specific outliers is identified, it is certainly easier to
    try to correct them as much as possible. How do we proceed? There are a few common
    ways to correct outliers that can be considered.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with outliers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The most widely used approaches to deal with outliers are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dropping them**: The analyst concludes that eliminating the outliers altogether
    guarantees better results in the final analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Capping them**: It is common to use the strategy of assigning a fixed extreme
    value (cap) to all those observations that exceed it (in absolute value) when
    it is certain that all extreme observations behave in the same way as those with
    the cap value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Assigning a new value**: In this case, outliers are eliminated by replacing
    them with null values, and these null values are imputed using one of the simplest
    techniques: the replacement of null values with a fixed value that could be, for
    example, the mean or median of the variable in question. You''ll see more complex
    imputation strategies in the next sections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transforming the data**: When the analyst is dealing with natural outliers,
    very often the histogram of the variable''s distribution takes on a skewed shape.
    Right-skewed distributions are very common, and if they were used as they appeared,
    many statistical tests that assume a normal distribution would give incorrect
    results. In this case, it is often used to transform the variable by applying
    a monotonic function, which in some way "straightens out" the imbalance (this
    is the case of the `log()` function, for example). Once transformed, the new variable
    satisfies the requirements of the tests and can therefore be analyzed without
    errors. Once the results have been obtained from the transformed variable, they
    must be transformed again by the inverse function of the one used at the beginning
    (if `log()` was used, then the inverse is `exp()`) in order to have values that
    are consistent with the business variable under analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you know the most common ways of dealing with outliers, you need to
    figure out how to identify them.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are different methods used to detect outliers depending on whether you
    are analyzing one variable at a time (**univariate analysis**) or multiple variables
    at once (**multivariate analysis**). In the univariate case, the analysis is fairly
    straightforward. The multivariate case, however, is more complex. Let's examine
    them in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Univariate outliers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the most direct and widely used ways to identify outliers for a single
    variable is to make use of boxplots, which you learned about in *Chapter 11, Adding
    Statistics Insights: Associations*. Some of the key points of a boxplot are the
    **interquartile range** (**IQR**), defined as the distance from the **first quartile**
    (**Q1**) to the **third quartile** (**Q3**), the **lower whisker** (Q1 - 1.5 x
    IQR), and the **upper whisker** (Q3 + 1.5 x IQR):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Boxplot’s main characteristics](img/file305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 – Boxplot’s main characteristics
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, all observations that are before the lower whisker and after the
    upper whisker are identified as outliers. This one is also known as **Tukey’s
    method**.
  prefs: []
  type: TYPE_NORMAL
- en: Identification becomes more complicated when dealing with more than one variable.
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate outliers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Identifying outliers when you are handling more than one variable (**multivariate
    outliers**) is not always straightforward. It depends on the number of variables
    in play and their data type.
  prefs: []
  type: TYPE_NORMAL
- en: Numeric variable and categorical variable
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As long as you need to analyze how a numerical variable is distributed among
    the different elements of a categorical variable, it is still feasible with the
    tools seen so far. In fact, just plot a boxplot for the values of the numeric
    variable grouped by each element of the categorical variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Numeric versus categorical variables](img/file306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 – Numeric versus categorical variables
  prefs: []
  type: TYPE_NORMAL
- en: In fact, it may be that the numerical variable does not present any outliers
    but reveals some when it is broken down by the elements of the categorical variable.
  prefs: []
  type: TYPE_NORMAL
- en: All numeric variables
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Generally, the inexperienced analyst tends to simplify the determination of
    outliers in multidimensional cases when there are only numeric variables.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'One might assume that an observation that is extreme in any variable is also
    a multivariate outlier, and this is often true. However, the opposite is not true:
    when variables are correlated, one can have a multivariate outlier that is not
    a univariate outlier in any variable.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'When dealing with only numeric variables, it is still possible to use algorithms
    that measure the distance from the center of the distribution. Let''s take the
    case of two numerical variables, which allows us to visualize the outliers using
    a scatterplot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Scatterplot of two numeric variables](img/file307.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 – Scatterplot of two numeric variables
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 12.4*, we have also added in the margin the two boxplots
    for every single variable under analysis to verify that for each of them there
    are no outliers, except for the one detected at the bottom. You can also see that
    there is one outlier that is clearly different from all other observations but
    is not detected as an outlier by the two boxplots.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine fixing a hypothetical center of the distribution and defining a distance
    from the center (Euclidean distance) above which the observations are to be considered
    outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – Euclidean distance from the center of the distribution](img/file308.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.5 – Euclidean distance from the center of the distribution
  prefs: []
  type: TYPE_NORMAL
- en: The above rule defines a circle centered at the center of the distribution.
    Considering a circle with the radius you see in *Figure 12.5*, you are going to
    identify several outliers (perhaps false positives?) that were not identified
    by looking at the boxplots alone, but the outlier you failed to identify before
    remains unidentified in this case as well. As you can well understand, the problem
    is that the distribution has an ellipsoidal shape that is distributed along the
    main diagonal of the Cartesian plane. Using a circle is certainly ill-suited to
    fit a distribution of a different shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if there was a distance that also considered the shape of the distribution?
    This is precisely the case with the **Mahalanobis distance**. This new distance
    differs from the others because it considers the **covariance** between the two
    variables. Covariance and Pearson’s correlation are two quantities associated
    with very similar concepts, so in some cases they are interchangeable (take a
    look at the references). The fact that the Mahalanobis distance accounts for the
    correlation between the two variables is evident in *Figure 12.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.6 – Mahalanobis distance from the center of the distribution](img/file309.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.6 – Mahalanobis distance from the center of the distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'For the curious, this is the formula to calculate it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.7 – Mahalanobis distance formula](img/file310.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.7 – Mahalanobis distance formula
  prefs: []
  type: TYPE_NORMAL
- en: 'is a multivariate observation, is the multivariate mean of all observations,
    and *S* is the covariance matrix. The fact that the Mahalanobis distance depends
    on the mean of all observations (a very unstable measure, very sensitive to outliers)
    and the covariance matrix makes you realize that the same limitations encountered
    for the Pearson’s coefficient apply, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Possible outliers at inconvenient locations could greatly affect the multivariate
    center defined by the mean of all observations. If the center is not well calculated,
    it is very likely that the resulting application of the Mahalanobis distance will
    identify erroneous outliers. This is easily solved by computing the center via
    a median-based formula, which is much more robust to the presence of extreme observations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If extreme outliers are present, the covariance matrix may also be negatively
    affected. This problem is also solved by adopting a robust version of the covariance
    matrix using the **Minimum Covariance Determinant** (**MCD**). This method, in
    addition to providing a robust covariance matrix, also returns a robust estimate
    of the center of the observations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is very likely that the usage of the Mahalanobis distance will return false
    outliers in case of *skewed*, *nonlinear*, or *heteroscedastic* distributions.
    These are the cases in which it is necessary to resort to transformations of the
    variables involved, as far as possible, before applying distance calculations.
    The goal of the transformations is to obtain distributions that are as normal
    as possible and to make the associations between the various variables as linear
    as possible. In these cases, **Box-Cox transformations** or **Yeo-Johnson transformations**
    are used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The identification of outliers becomes much more complicated when dealing with
    mixed variables (numerical and categorical) in numbers greater than two. It is
    necessary to use different data science techniques (feature engineering techniques
    for categorical variables, handling of unbalanced datasets, and so on) and to
    apply specific machine learning anomaly detection algorithms. For this reason,
    cases of this type are out of scope.
  prefs: []
  type: TYPE_NORMAL
- en: Once the outliers (both univariate and multivariate) are identified, it is up
    to the analyst to decide which method to adopt to try to fix it, if possible.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now see how to implement outlier detection algorithms according to what
    you learned in the previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing outlier detection algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing you'll do is implement what you've just studied in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing outlier detection in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will use the *Wine Quality* dataset created by Paulo Cortez
    et al. ([https://archive.ics.uci.edu/ml/datasets/wine+quality](https://archive.ics.uci.edu/ml/datasets/wine+quality))
    to show how to detect outliers in Python. The dataset contains as many observations
    as the different types of red wine, each described by the organoleptic properties
    measured by the variables, except for the `quality` one, which provides a measure
    of the quality of the product using a discrete grade scale from 1 to 10.
  prefs: []
  type: TYPE_NORMAL
- en: You'll find the code used in this section in the `01-detect-outliers-in-python.py`
    file into the `Chapter12\Python` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have loaded the data from the `winequality-red.csv` file directly
    from the web into the `df` variable, let''s start by examining the `sulphates`
    variable. Let''s check if it contains any outliers by displaying its boxplot,
    which was obtained through a wrapper function that we have defined in the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.8 – Sulphates boxplot](img/file311.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.8 – Sulphates boxplot
  prefs: []
  type: TYPE_NORMAL
- en: 'Apparently there are plenty of values after 1.0\. To be able to locate them
    in the dataset, we created a function that accepts a dataframe as input and the
    name of the numeric column to be considered, and as output returns the dataframe
    with the addition of a column of Boolean values, containing `True` when the value
    of the column is an outlier, and `False` otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have identified the outliers of the initial distribution of the `sulphates`
    variable, we can draw its boxplot by removing the outliers to see what changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting boxplot is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.9 – Sulphates boxplot once outliers were removed](img/file312.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.9 – Sulphates boxplot once outliers were removed
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, some outliers are still visible. This is due to the fact that
    removing the outliers from the initial distribution caused the distribution to
    change (its statistical properties changed). So, what you see in *Figure 12.9*
    are the outliers of the new distribution that was created.
  prefs: []
  type: TYPE_NORMAL
- en: 'As already explained, it is up to the analyst to figure out if the outliers
    identified can be corrected in some way, eliminated, or left where they are. Suppose
    in this case that the outliers in the second distribution are natural outliers.
    Let''s try to break the new distribution down to the individual values of the
    `quality` variable and draw a boxplot for each of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.10 – Sulphates boxplots for each quality value](img/file313.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.10 – Sulphates boxplots for each quality value
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 12.10*, the distribution of sulphates for wines that received
    a grade of 5 has several outliers. This could induce the analyst to try to understand
    how much the presence of sulphates affects the final rating given by users to
    the wine, paying particular attention to the case of a wine considered of average
    quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'If, on the other hand, we wanted to identify multivariate outliers for all
    numerical variables in the dataset, excluding the quality variable, we need to
    change our approach by trying to apply the Mahalanobis distance, as you learned
    in the previous section. We assume that the elimination of outliers for each individual
    variable has been validated. So, let’s now try to figure out if multivariate outliers
    are present for the numeric variables in the `df_no_outliers’` dataframe. First,
    however, it is necessary to check whether the distributions of the variables under
    analysis are skewed. Therefore, we try to draw a histogram for each of the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.11 – Histograms of all the wine quality variables without outliers](img/file314.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.11 – Histograms of all the wine quality variables without outliers
  prefs: []
  type: TYPE_NORMAL
- en: It is evident that some of them are extremely right-skewed (*residual sugar*,
    *chlorides*, *total sulfur dioxide*, etc.), therefore it is necessary to try to
    apply transformations that attempt to “normalize” the single distributions. Generally,
    *Box-Cox transformations* are applied. But since in this case some values of the
    distributions are not positive, it is not possible to apply them. It is therefore
    necessary to use other transformations that have the same objective, named *Yeo-Johnson*.
    For more details on these transformations, check out the references.
  prefs: []
  type: TYPE_NORMAL
- en: 'For convenience, we created a wrapper function that transforms a pandas dataframe
    of only numeric variables by applying Yeo-Johnson transformations and also returns
    the corresponding lambda values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, once you''ve transformed the dataframe into an object, you can try drawing
    histograms of the distributions of the transformed variables to see if the skewness
    has been smoothed out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the plot you get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.12 – Histograms of all the wine quality variables transformed](img/file315.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.12 – Histograms of all the wine quality variables transformed
  prefs: []
  type: TYPE_NORMAL
- en: It is quite evident that now the distributions look more like the "bells" of
    normal distributions. You can now calculate Mahalanobis distances with the surety
    that it will detect outliers with fewer errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The identification of outliers is done using a robust estimator of covariance,
    the *MCD*. Since the squared Mahalanobis distance behaves as a Chi-Squared distribution
    (see the references), we can calculate the threshold value above which to consider
    an observation an outlier thanks to this distribution, passing the desired cutoff
    value to its *percent point function* (`ppf()`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have determined the threshold value, you can add two columns to the
    dataframe: a column that identifies whether the observation (the row) is an outlier
    according to Mahalanobis and a column that reports the probability that an observation
    is not an outlier by chance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You''ll see a dataframe chunk like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.13 – Outliers information shown in the dataframe](img/file316.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.13 – Outliers information shown in the dataframe
  prefs: []
  type: TYPE_NORMAL
- en: Wow! With a minimum of statistics knowledge, you were able to identify multivariate
    outliers of numeric variables in Python.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to get the same results with R. Let's see how.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing outlier detection in R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You''ll find the code used in this section in the `01-detect-outliers-in-r.R`
    file in the `Chapter12\R` folder. In order to run it properly, you need to install
    new packages:'
  prefs: []
  type: TYPE_NORMAL
- en: Open RStudio and make sure it is referencing your latest CRAN R (version 4.0.2
    in our case).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the **Console** window and enter this command: `install.packages(''robust'')`.
    Then press *Enter*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter this command: `install.packages(''recipes'')`. Then press *Enter*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once you have loaded the data from the `winequality-red.csv` file directly
    from the web into the `df` variable, you’ll draw the boxplot of the `sulphates`
    variable using the `boxPlot()` wrapper function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.14 – Boxplot of the sulphates variable](img/file317.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.14 – Boxplot of the sulphates variable
  prefs: []
  type: TYPE_NORMAL
- en: 'Since there are many outliers visible in *Figure 12.14*, they are identified
    using the `add_is_outlier_IQR()` function, which adds an identifier column to
    the dataframe. As the name indicates, the function determines the outliers based
    on the interquartile range. At this point, the boxplot of the same variable is
    drawn again, this time eliminating the previously identified outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.15 – Boxplot of the sulphates variable after removing outliers](img/file318.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.15 – Boxplot of the sulphates variable after removing outliers
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming you now want to identify multivariate outliers, it is worthwhile to
    first look at the histograms of the individual variables to see if significant
    skewness is present. The histograms are drawn using the following `dataframeHist()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'A special feature of this function is the use of the `pivot_longer()` function
    of the `tidyr` package on all columns to verticalize their names in the new `name`
    column, to which correspond the initial values in the new `value` column. The
    result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.16 – Multiple histograms for each numeric variable](img/file319.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.16 – Multiple histograms for each numeric variable
  prefs: []
  type: TYPE_NORMAL
- en: Since the skewness is obvious, you can apply Yeo-Johnson transformations thanks
    to the `yeo_johnson_transf()` wrapper function we created for you. The peculiarity
    of this function is that it makes use of a ready-made step in the `recipes` package,
    which facilitates the whole pre-processing phase. To learn more about the use
    of `recipes`, take a look at the references.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you learned in the previous section, the Yeo-Johnson transformations solve
    the skewness problem quite well in this case. Therefore, it is possible to try
    applying Mahalanobis distance to detect outliers via the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, given a cutoff value associated with the statistical significance
    with which we want to determine outliers, you can obtain the corresponding threshold
    value above which to consider an observation an outlier. Once the threshold is
    calculated, it is trivial to create an indicator column for the outliers. It is
    also possible to add a column indicating the probability with which an observation
    can be considered an outlier not by chance thanks to the `pchisq()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The final result partially presents this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.17 – Final tibble containing the multivariate outliers’ information](img/file320.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.17 – Final tibble containing the multivariate outliers’ information
  prefs: []
  type: TYPE_NORMAL
- en: Way to go! You were able to identify multivariate outliers in R as well.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, it is trivial to apply the Python and R code seen so far to Power
    BI.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing outlier detection in Power BI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Power BI has tools that allow you to view outliers graphically and then analyze
    them by hovering. One of these was introduced in the November 2020 release and
    is the **Anomaly Detection** feature. The other one is the **Outliers Detection**
    custom visual. Let''s see what the main differences are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Anomaly Detection** is available directly in Power BI once you enable it
    as a preview feature (at the moment, it is in preview). It is *only supported
    for line chart visuals* containing *time-series data* in the Axis field.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outliers Detection** is an open source R custom visual, and it has to be
    installed separately ([https://bit.ly/power-bi-outliers-detection](https://bit.ly/power-bi-outliers-detection)).
    There are five different implemented methods to detect outliers, and it works
    well for univariate and bivariate datasets. Multivariate datasets are to be avoided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you may have noticed, both of these tools are Power BI visuals.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All transformations performed within Python or R visuals modify the dataframe
    that will later be the object of visualization, but the changes cannot be persisted
    in the data model in any way.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is precisely for this reason that we have decided to illustrate some methods
    for detecting outliers that can be applied in Power Query using Python or R. This
    way, you can identify observations that are outliers by simply filtering your
    data model tables appropriately. Due to the simplicity of the code, in this case
    we will also implement the correlation coefficients in both Python and R in one
    project.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, make sure that Power BI Desktop references the correct versions of Python
    and R in the **Options**. Then follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on **Get Data**, search for `web`, select **Web**, and click on **Connect**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter [https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv](https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv)
    into the URL textbox and click **OK**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You’ll see a preview of the data. Then click **Transform Data**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Transform** on the ribbon and then **Run Python script**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the script you can find in the `02-detect-outliers-in-power-bi-with-python
    copy.py` file in the `Chapter12\Python` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are only interested in the data in `dataset`. So, click on its **Table**
    value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You'll see a preview of the dataset that also has the new columns to identify
    outliers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Home** on the ribbon and then click **Close & Apply**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1 to 3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Transform** on the ribbon and then **Run R script**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the script you can find in the `02-detect-outliers-in-power-bi-with-r.R`
    file in the `Chapter12\R` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are only interested in the `data` tibble. So, click on its **Table** value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You’ll see the preview of the dataset that also has the new columns to identify
    outliers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Home** on the ribbon and then click **Close & Apply**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazing! You have just identified outliers of a numeric dataset in Power BI
    with both Python and R!
  prefs: []
  type: TYPE_NORMAL
- en: What missing values are and how to deal with them
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data describing real-world phenomena often has a lot of missing data. Lack of
    data is a fact that cannot be overlooked, especially if the analyst wants to do
    an advanced study of the dataset to understand how much the variables in it are
    correlated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The consequences of mishandling missing values can be many:'
  prefs: []
  type: TYPE_NORMAL
- en: The *statistical power* of variables with missing values is diminished, especially
    when a substantial number of values is missing for a single variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *representativeness of the dataset* subject to missing values may also be
    diminished, and thus the dataset in question may not correctly represent the substantive
    characteristics of the set of all observations of a phenomenon.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any statistical estimates may not converge to whole population values, thus
    *generating bias*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results of the analysis conducted may not be correct.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But let's see what the causes could be that generate missing values in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The causes of missing values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There can be many causes for a lack of values, determined by intentional or
    unintentional behaviors. Here is a non-exhaustive list:'
  prefs: []
  type: TYPE_NORMAL
- en: Corruption of data due to errors in writing, reading, or transmitting it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replacing outliers that excessively skew the dataset with null values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refusing to answer a question on a survey
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lack of knowledge of the issues asked in a survey question
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, all of these causes can be summarized into *four types of cases*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Missing Completely at Random** (**MCAR**): The causes that generate the null
    values are totally independent both of the hypothetical values they would have
    if valorized (Y) and of the values of the other variables in the dataset (X).
    They just depend on external variables (Z). The advantage of data that is MCAR
    from a statistical perspective is that the dataset consisting of only complete
    values for both variables X and Y is an *unbiased sample* of the entire population.
    Unfortunately, MCAR cases are rarely found in real-world data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing At Random** (**MAR**): The missing data of a partially incomplete
    variable (Y) is related to some other variables in the dataset that do not have
    null values (X), but not to the values of the incomplete variable itself (Y).
    The dataset consisting of only complete values for both variables X and Y in the
    MAR case constitutes a *biased sample* of the entire population because it will
    surely miss all those values of X on which the null values of Y depend, resulting
    in a dataset that is not representative of the entire phenomenon. MAR is a more
    realistic assumption than MCAR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing Not at Random due to external variables Z** (**MNAR Z**): The missing
    data of a partially incomplete variable (Y) depends on variables not included
    in the dataset (external variables). For example, given a dataset without the
    variable “sex,” there may be observations for which the age value is zero. It
    could be possible that the respondents not providing this information are mostly
    women, since they stereotypically do not want to reveal their age. Therefore,
    eliminating observations that have non-null values for the “age” variable would
    generate a *biased dataset*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing Not at Random due to missing values Y** (**MNAR Y**): The missing
    data of a partially incomplete variable (Y) depends on the hypothetical values
    they would have if valorized. For example, it is well known that adolescents tend
    to never disclose the fact that they consume alcohol. Therefore, if we remove
    from the dataset those observations for which the value for alcohol consumption
    is null, implicitly we risk removing from the dataset most of the observations
    pertaining to adolescents, thus obtaining a *biased dataset*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also statistical tests that allow you to understand if the distribution
    of missing data is MCAR (look at the references). But, as already mentioned, cases
    of MCAR are so rare that it is better to assume that the distribution of missing
    values of a dataset under consideration is either MAR or MNAR.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the type of missing data distribution, specific strategies can
    be employed to sanitize the missing values.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first thing to do, when possible, is to understand together with the referent
    of the data the reason for the missing values in the dataset, and whether it is
    possible to recover them. Unfortunately, most of the time it is not possible to
    recover missing data from the source and therefore different strategies must be
    adopted, depending on the case.
  prefs: []
  type: TYPE_NORMAL
- en: Easy imputation by hand
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There may be cases of variables that are *obvious to impute by hand*. For example,
    in correspondence of the “blue” value of the variable “color” you will notice
    that the variable “weight” always takes the value of 2.4, except in a few cases
    where it is null. In those cases, it is easy to impute the missing values of the
    variable “weight” in relation to the “blue” color with the value 2.4.
  prefs: []
  type: TYPE_NORMAL
- en: Discarding data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The first solution to the missing values that comes to the analyst''s mind
    is surely to eliminate the problem at the source, that is, to eliminate missing
    values. There are several ways to eliminate them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listwise** or **Complete-Case Analysis** (**CCA**) **deletion**: This method
    involves *deleting any observation (row) that contains at least one missing data
    element in any variable*. It is often applied when the number of missing values
    is low, and the number of observations is sufficiently large. As you have seen
    in the classification of the four types of missing data, the only case in which
    adopting this solution doesn’t result in a biased dataset is the MCAR, a very
    rare case among datasets describing real-world phenomena. Listwise deletion is
    therefore not a good strategy when you are not faced with a case of MCAR with
    a sufficiently high number of observations in the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pairwise** or **Available-Case Analysis** (**ACA**) **deletion**: Depending
    on the variables considered in a statistical analysis, this method *eliminates
    only those observations (rows) that have null values for the only variables involved*.
    Null values present in variables that are not involved in the analysis are not
    a reason to eliminate observations. Again, adopting this method does not generate
    a biased dataset only if the case under analysis is MCAR. The most obvious disadvantage
    of this method is that if you need to compare different analyses, you cannot apply
    it because the number of observations in the sample varies as the variables involved
    in the different analyses vary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variable deletion**: This method considers *removing the entire variable
    from the analysis* under study (and not from the dataset a priori!) when the proportion
    of missing values ranges is 60% and above. It makes sense to eliminate a variable
    if, after careful study, it is concluded that it does not contain important information
    for the analysis at hand. Otherwise, it is always preferable to try a method of
    imputation. Generally, the elimination of a variable is always the last option
    and should only be considered if the final analysis actually benefits from it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the analyst still has to heal the problem of missing values, even after
    trying to apply these elimination techniques, they must then resort to imputation
    techniques. Let's look at the most commonly used methods.
  prefs: []
  type: TYPE_NORMAL
- en: Mean, median, and mode imputation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is an intuitively attractive method, also known as **single imputation**,
    for which you fill missing values with predefined values. Simplicity is unfortunately
    countered by some not negligible issues.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the most common substitution of null values is with the **mean** value
    of the variable's distribution resulting from ignoring missing values. The motivation
    behind this choice is that *the mean is a reasonable estimate of an observation
    drawn at random from a normal distribution*. However, if the distribution in question
    is skewed, the analyst runs the risk of making severely biased estimates even
    if the dataset’s missing value distribution is MCAR.
  prefs: []
  type: TYPE_NORMAL
- en: The skewness problem can be solved by using the **median** of the variable.
    However, the fact remains that the common problem in single imputation is replacing
    a missing value with a single value and then treating it as if it were a true
    value. As a result, single imputation ignores uncertainty and almost always underestimates
    variance (remember that variance is synonymous with information; a variable with
    0 variance is a constant value variable that usually does not enrich statistical
    analyses).
  prefs: []
  type: TYPE_NORMAL
- en: '**Mode** (the value that is repeated most often) imputation is often used with
    categorical data represented as numbers. Even this method, when used without having
    strong theoretical grounds, introduces bias, so much so that sometimes analysts
    prefer to create a new category specifically for missing values.'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple imputation is often preferable to single imputation as it overcomes
    the problems of underestimating variance by considering both within-imputation
    and between-imputation variance. Let's see what this is all about.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple imputation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It is thanks to Donald B. Rubin that in 1987 a methodology to deal with the
    problem of underestimation of variance in the case of single imputation was made
    public. This methodology goes by the name of **multiple imputation** and consists
    of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Imputation**: This step is very similar to the single imputation step, except
    that this time, values are extracted *m* times from a distribution for each missing
    value. The result of this operation is a set of *m* imputed datasets, for which
    all observed values are always the same, with different imputed values depending
    on the uncertainty of the respective distributions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Analysis**: You use all *m* imputed datasets for the statistical analysis
    you need to do. The result of this step is a set of *m* results (or analyses)
    obtained by applying the analysis in question to each of the *m* imputed datasets.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pooling**: The *m* results are combined in order to obtain unbiased estimates
    with the correct statistical properties. The *m* estimates of each missing value
    are pooled in order to have an estimated variance that combines the usual sampling
    variance (**within-imputation variance**) and the extra variance caused by missing
    data (**between-imputation variance**).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The whole process can be summarized by *Figure 12.18*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.18 – Multiple imputation process](img/file321.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.18 – Multiple imputation process
  prefs: []
  type: TYPE_NORMAL
- en: Multiple imputation can be used in cases where the data is MCAR, MAR, and even
    when the data is MNAR if there are enough auxiliary variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common implementations of multiple imputation are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multivariate Imputation by Chained Equations** (**MICE**): This imputes missing
    values focusing on one variable at a time. Once the focus is on one variable,
    MICE uses all other variables in the dataset (or an appropriately chosen subset
    of those variables) to predict missing values in that variable. Predictions of
    missing values are based on linear regression models for numerical variables and
    logistic regression models for categorical variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amelia II**: This is named after Amelia Mary Earhart, an American aviation
    pioneer who, during an attempt to become the first woman to complete a global
    circumnavigation flight in 1937, disappeared over the central Pacific Ocean. Amelia
    II combines a bootstrapping-based algorithm and an **Expectation–Maximization**
    (**EM**) algorithm, making it fast and reliable. It also works very well for time-series
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recently, multiple imputation has been implemented using deep learning algorithms
    as well. In particular, the **Multiple Imputation with Denoising Autoencoders**
    (**MIDAS**) algorithm offers significant advantages in terms of accuracy and efficiency
    over other multiple imputation strategies, particularly when applied to large
    datasets with complex features.
  prefs: []
  type: TYPE_NORMAL
- en: Univariate time-series imputation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The problem of missing data afflicts not only multivariate tabular datasets,
    but also time-series datasets. For example, sensors that constantly collect data
    about a phenomenon could stop working at any time, generating holes in the series.
    Often, the analyst is faced with a time-series that has missing values and must
    somehow impute these values because the processes to which to submit the series
    do not handle null values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The constraint of the consequentiality of events given by the temporal dimension
    forces the analyst to use specific imputation methods for time-series. Let''s
    look at the most commonly used methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Last Observation Carried Forward (LOCF), Next Observation Carried Backward
    (NOCB)**: In the LOCF method, the last observed (that is, non-null) measure of
    the variable in question is used for all subsequent missing values. The only condition
    in which LOCF is unbiased is when the missing data is completely random, and the
    data used as the basis for LOCF imputation has exactly the same distribution as
    the unknown missing data. Since it can never be proven that these distributions
    are exactly the same, all analyses that make use of LOCF are suspect and will
    almost certainly generate biased results.The NOCB method is a similar approach
    to LOCF, but works in the opposite direction, taking the first (non-null) observation
    after the missing value and replacing it with the missing value. It obviously
    has the same limitations as LOCF.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exponentially Weighted Moving Average (EMWA)**: In general, the moving average
    is commonly used in time-series to smooth out fluctuations due to short-term effects
    and to highlight long-term trends or cycles. EWMA is designed such that older
    observations are given lower weights. The weights decrease exponentially as the
    observation gets older (hence the name “exponentially weighted”). Missing values
    are imputed using the values of the resulting “smoothed” time-series.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpolation**: The interpolation technique is one of the most widely used
    techniques to impute missing data from a time-series. The basic idea is to use
    a simple function (such as a linear function, a polynomial function, or a spline
    function) that fits with the non-zero points near the missing value, then interpolate
    the value for the missing observation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seasonally Decomposed Imputation**: If the time-series under analysis has
    seasonality, this method could give very good results. The procedure adopted is
    to remove the seasonal component from the time-series, perform the imputation
    on the seasonally adjusted series, and then add the seasonal component back.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also algorithms to impute missing values for multivariate time-series.
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate time-series imputation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This topic is beyond the scope of this chapter, but we simply wanted to specify
    that the *Amelia II* algorithm we discussed earlier is also used to impute missing
    values in a multivariate time-series, whereas it is not suitable for imputation
    on univariate time-series.
  prefs: []
  type: TYPE_NORMAL
- en: In order to figure out whether to impute missing values, we must first identify
    them in the dataset. Let's see how to do that.
  prefs: []
  type: TYPE_NORMAL
- en: Diagnosing missing values in R and Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before thinking about imputing missing values in a dataset, we must first know
    the extent to which the missing values affect each individual variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code used in this section in the `Chapter12\R\03-diagnose-missing-values-in-r.R`
    and `Chapter12\Python\03-diagnose-missing-values-in-python.py` files. In order
    to properly run the code and the code of the following sections, you need to install
    the requisite R and Python packages as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the Anaconda prompt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the `conda activate pbi_powerquery_env` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the `pip install missingno` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the `pip install upsetplot` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, open RStudio and make sure it is referencing your latest CRAN R (version
    4.0.2 in our case).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Console** window and enter `install.packages('naniar')`. Then
    press *Enter*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter `install.packages('imputeTS')`. Then press *Enter*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter `install.packages('forecast')`. Then press *Enter*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter `install.packages('ggpubr')`. Then press *Enter*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter `install.packages('missForest')`. Then press *Enter*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter `install.packages('mice')`. Then press *Enter*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter `install.packages('miceadds')`. Then press *Enter*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's see at this point what features will come in handy when you face the analysis
    of missing values in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The R package `naniar` provides the `vis_miss()` function, which displays in
    a single image the missing values of the whole dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.19 – Plot of missing values in the entire dataset](img/file322.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.19 – Plot of missing values in the entire dataset
  prefs: []
  type: TYPE_NORMAL
- en: You can draw similar graphs in Python thanks to the `missingno` library ([https://github.com/ResidentMario/missingno](https://github.com/ResidentMario/missingno)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Knowing only the percentage value of the number of missing values compared
    to the total number of values of the variable under consideration can be limiting.
    That''s why it''s often useful to also know the details for each column via the
    `miss_var_summary()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.20 – Missing values summary](img/file323.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.20 – Missing values summary
  prefs: []
  type: TYPE_NORMAL
- en: We developed a similar function in Python in the code you can find in the repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'It would be interesting to be able to visualize combinations of missing values
    and missing intersections between variables. The R package `naniar` ([https://github.com/njtierney/naniar](https://github.com/njtierney/naniar))
    allows you to do just this kind of analysis thanks to the `gg_miss_upset()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.21 – UpSet plot of dataset’s missing values](img/file324.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.21 – UpSet plot of dataset’s missing values
  prefs: []
  type: TYPE_NORMAL
- en: To achieve the same plot in Python, the process is a bit more complicated. You
    must first use the `upsetplot` module ([https://github.com/jnothman/UpSetPlot](https://github.com/jnothman/UpSetPlot)).
    The problem lies in providing the `UpSet()` function exposed by this package with
    an input dataframe in the required format. For this reason, we made the helper
    function `upsetplot_miss()` that you will find in the code to easily create the
    upset plot of missing values in Python as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'In case you need to get an idea of the missing values in a time-series, the
    `imputeTS` R package provides the `ggplot_na_distribution()` function, which shows
    very clearly the holes in the time-series:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.22 – Detecting missing values in a time-series](img/file325.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.22 – Detecting missing values in a time-series
  prefs: []
  type: TYPE_NORMAL
- en: 'If, on the other hand, you need to get more complete details about the statistics
    of missing values in a time-series, the `statsNA()` function is for you:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.23 – Statistics of missing values in a time-series](img/file326.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.23 – Statistics of missing values in a time-series
  prefs: []
  type: TYPE_NORMAL
- en: Once you have carefully studied the distributions of the missing values of each
    variable and their intersections, you can decide which variables to leave in the
    dataset and which to submit to the various imputation strategies. Let's see how
    to do imputation in R and Python.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing missing value imputation algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From here on, all missing value analysis will be done in R because very statistically
    specialized and simple-to-use packages that do not exist in the Python ecosystem
    have been developed for this language.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we need to calculate the Pearson correlation coefficient between the
    two numerical variables, `Age` and `Fare`, of the Titanic disaster dataset. Let's
    first consider the case where missing values are eliminated.
  prefs: []
  type: TYPE_NORMAL
- en: Removing missing values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The impact of applying listwise and pairwise deletion techniques is evident
    in the calculation of Pearson''s correlation between numerical variables in the
    Titanic dataset. Let''s load the data and select only numeric features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If you now calculate the correlation matrix for the two techniques separately,
    you will notice the differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.24 – Correlation matrix calculated using listwise and pairwise
    deletion](img/file327.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.24 – Correlation matrix calculated using listwise and pairwise deletion
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how to impute missing values in the case of a tabular dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Imputing tabular data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can find the code used in this section in the `Chapter12\R\04-handle-tabular-missing-values-in-r.R`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Again, starting with the Titanic disaster dataset, the first thing you need
    to do is remove the `Name` and `Ticket` columns because they have a high number
    of distinct values.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is important to eliminate categorical variables that have a high number of
    distinct values because otherwise, the MICE algorithm would fail due to the excessive
    RAM required. Generally, variables with high cardinality are not useful for the
    imputation of null values of other variables. There are cases in which the information
    contained in these variables could be fundamental for the imputation (for example,
    zip codes). In this case, it is necessary to use transformations that reduce the
    cardinality without losing the information contained in the variables. For further
    information, take a look at the references.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Since the missing values in the `Cabin` column represent more than 70% of all
    values, we decided to remove that as well. After that, the categorical variables
    `Survived`, `Sex`, and `Embarked` are transformed as factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, it is possible to calculate the Pearson correlation for each
    pair of numerical variables by applying the pooling technique provided by Rubin
    in multiple imputations. The `miceadds` package exposes wrapper functions that
    simplify this operation for the most common statistical analysis given the result
    of the `mice()` function as a parameter. In our case, the function of interest
    is `micombine.cor()`, and we use it in our `corr_impute_missing_values()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'It is easy therefore to obtain the aforementioned correlations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.25 – Statistical inference for correlations for multiple imputed
    datasets](img/file328.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.25 – Statistical inference for correlations for multiple imputed datasets
  prefs: []
  type: TYPE_NORMAL
- en: Without going into too much detail about the other fields, the correlation coefficient
    between the variables is given by the `r` column. Since the *r* coefficient is
    the result of a process of inference, the `lower95` and `upper95` columns define
    the upper and lower 95% confidence interval bounds.
  prefs: []
  type: TYPE_NORMAL
- en: '**Important Note**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'If you get an error such as **Error in matchindex(yhatobs, yhatmis, donors)
    : function ''Rcpp_precious_remove'' not provided by package ''Rcpp''**, it is
    likely that you are running a recent version of a package compiled with an earlier
    version of `Rcpp`. Updating `Rcpp` with the `install.packages(''Rcpp'')` command
    should fix it.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sometimes, the goal of your analysis is not to get results from statistical
    functions, but to simply fill in the holes left by missing values because the
    dataset in question must then be used to train a machine learning algorithm that
    does not admit null values. The latest versions of scikit-learn (currently in
    the experimental phase) expose the impute module with its `SimpleImputer`, `KNNImputer`,
    and `IterativeImputer` methods. In this way, it is possible to impute the missing
    values of a dataset through machine learning algorithms (k-nearest neighbors;
    linear regression) among other more naïve methods (substitutions with fixed values,
    mean, median, or mode), and also to have an average score of how the algorithm
    performs in general (cross-validated mean squared error). You’ll see an example
    of one of these methods in *Chapter 13, Using Machine Learning without Premium
    or Embedded Capacity*.
  prefs: []
  type: TYPE_NORMAL
- en: If, on the other hand, you need to impute missing values from a univariate time-series,
    how would you proceed? Let's see it.
  prefs: []
  type: TYPE_NORMAL
- en: Imputing time-series data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can find the code used in this section in the `Chapter12\R\05-handle-time-series-missing-values-in-r.R`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'We consider a time-series of the average number of aircraft passengers per
    month. Let''s duplicate it and eliminate from it 10% of the values randomly and,
    in addition, let''s eliminate a couple of duplicated values by hand. Then, we’ll
    merge the two time-series into a single tibble:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The result can be seen in *Figure 12.22*. The `imputeTS` package exposes convenient
    functions that implement the missing value imputations already described in the
    *Univariate time-series imputation* section. Once the values are imputed using
    the different algorithms and parameters, it is possible to calculate the accuracy
    because you also know the complete time-series. We use the `accuracy()` function
    exposed by the `forecast` package to calculate the final accuracy using various
    metrics, such as *mean absolute error* and *root mean squared error*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.24 – Error metrics for imputed values in a time-series](img/file329.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.24 – Error metrics for imputed values in a time-series
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Seasonally Decomposed Imputation** (**seadec**) strategy seems to be
    the best one. Here’s the plot of missing values according to this strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.25 – Representation of imputed values in the time-series](img/file330.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.25 – Representation of imputed values in the time-series
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at how to use what we've learned so far about missing values
    in Power BI.
  prefs: []
  type: TYPE_NORMAL
- en: Imputing missing values in Power BI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have delved into the theory and techniques of imputing missing values, whether
    you are dealing with a tabular dataset or a time-series, precisely because in
    Power BI there is no way to adopt them through native tools, except for the naïve
    solution of replacing them with a default value (such as fixed value, mean, or
    median). In fact, when the business analyst finds themselves needing to fill in
    the gaps in the data, they often ask for the help of a data scientist or someone
    with the statistical knowledge to tackle the problem. Now that you’ve studied
    this chapter, you’re able to tackle it on your own!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply what we did in the previous sections for tabular and time-series
    data in Power BI:'
  prefs: []
  type: TYPE_NORMAL
- en: Open Power BI Desktop and make sure it is referencing the latest engine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Get data**, search for `web`, and double-click on the **Web** connector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter the following URL and click **OK**: [http://bit.ly/titanic-dataset-csv](http://bit.ly/titanic-dataset-csv).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next import screen, click **Transform Data**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the **Transform** tab, click on **Run R script**, copy the script in the
    `06-impute-tabular-missing-values-in-power-bi-with-r.R` file in the `Chapter12\R`
    folder, paste it into the editor, and then click **OK**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You may be asked to configure the privacy levels of the R script and the CSV
    file. In this case, select both the **Organizational** and **Public** level.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are only interested in the data in `corr_tbl`. So, click on its **Table**
    value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As a result, you’ll see the table containing the correlation coefficients calculated
    using MICE and the pooling method provided by the multivariate imputation technique:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.26 – Correlation table calculated with the multivariate imputation
    technique](img/file331.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 12.26 – Correlation table calculated with the multivariate imputation
    technique
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Go to the **Home** tab and click **Close & Apply**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Get data** and double-click on the **Text/CSV** connector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the `air.csv` file you can find in the `Chapter12` folder and click **Open**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next import screen, click **Transform Data**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Power BI automatically interprets the `date` text field as a date field, and
    therefore applies a **Changed Type** operation from Text to Date. In order to
    correctly process dates within an R script with the `lubridate` package, you must
    delete the **Changed Type** step by clicking on the red cross before inserting
    the R script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.27 – Delete the Changed Type step](img/file332.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 12.27 – Delete the Changed Type step
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Go to the **Transform** tab, click on **Run R script**, copy the script in the
    `07-impute-time-series-missing-values-in-power-bi-with-r.R` file in the `Chapter12\R`
    folder, paste it into the editor, and then click **OK**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You may be asked to configure the privacy levels of the R script and the CSV
    file. In this case, select both the **Organizational** and **Public** level.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As a result, you’ll see the table containing the original time-series (the
    `value` column) and other time-series obtained through different imputation algorithms
    (each in a different column):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.28 – Table with imputed time-series](img/file333.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 12.28 – Table with imputed time-series
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Go to the **Home** tab and click **Close & Apply**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Impressive! You managed to apply the most complex missing value imputation algorithms
    to a tabular dataset and a time-series in Power BI with minimal effort. Congratulations!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, you learned what outliers are, what causes them generally,
    and how they are treated. You also learned how to identify them based on the number
    of variables involved and their given type, both in Python and in R.
  prefs: []
  type: TYPE_NORMAL
- en: Another important topic you covered was how to impute missing values in tabular
    and time-series datasets. You learned how to diagnose them and impute them with
    R.
  prefs: []
  type: TYPE_NORMAL
- en: After that, you implemented the value imputation algorithms in Power BI.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will see how to use machine learning algorithms in
    Power BI without the need for Premium or Embedded capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For additional reading, check out the following books and articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Add Marginal Plot to ggplot2 Scatterplot Using ggExtra Package in R* ([https://statisticsglobe.com/ggplot2-graphic-with-marginal-plot-in-r](https://statisticsglobe.com/ggplot2-graphic-with-marginal-plot-in-r))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*5 Things You Should Know About Covariance* ([https://towardsdatascience.com/5-things-you-should-know-about-covariance-26b12a0516f1](https://towardsdatascience.com/5-things-you-should-know-about-covariance-26b12a0516f1))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Mahalanobis Distance and its Limitations* ([https://rpubs.com/jjsuarestra99/mahalanobis](https://rpubs.com/jjsuarestra99/mahalanobis))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Box-Cox Transformation Explained* ([https://towardsdatascience.com/box-cox-transformation-explained-51d745e34203](https://towardsdatascience.com/box-cox-transformation-explained-51d745e34203))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*How to Use Power Transforms for Machine Learning* ([https://machinelearningmastery.com/power-transforms-with-scikit-learn/](https://machinelearningmastery.com/power-transforms-with-scikit-learn/))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*The Relationship between the Mahalanobis Distance and the Chi-Squared Distribution*
    ([https://markusthill.github.io/mahalanbis-chi-squared/](https://markusthill.github.io/mahalanbis-chi-squared/))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Using the recipes package for easy pre-processing* ([http://www.rebeccabarter.com/blog/2019-06-06_pre_processing/](http://www.rebeccabarter.com/blog/2019-06-06_pre_processing/))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Anomaly detection* ([https://docs.microsoft.com/en-us/power-bi/visuals/power-bi-visualization-anomaly-detection](https://docs.microsoft.com/en-us/power-bi/visuals/power-bi-visualization-anomaly-detection))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Missing data: mechanisms, methods, and messages* ([http://www.i-deel.org/uploads/5/2/4/1/52416001/chapter_4.pdf](http://www.i-deel.org/uploads/5/2/4/1/52416001/chapter_4.pdf))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Multiple imputation by chained equations: what is it and how does it work?*
    ([https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Amelia II: A Program for Missing Data* ([https://www.jstatsoft.org/article/view/v045i07](https://www.jstatsoft.org/article/view/v045i07))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*All about Categorical Variable Encoding* ([https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
