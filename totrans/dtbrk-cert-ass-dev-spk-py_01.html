<html><head></head><body>
		<div><h1 id="_idParaDest-16" class="chapter-number"><a id="_idTextAnchor016"/>1</h1>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor017"/>Overview of the Certification Guide and Exam</h1>
			<p>Preparing for any task initially involves comprehending the problem at hand thoroughly and, subsequently, devising a strategy to tackle the challenge. Creating a step-by-step methodology for addressing each aspect of the challenge is an effective approach within this planning phase. This method enables smaller tasks to be handled individually, aiding in a systematic progression through the challenges without the need to feel overwhelmed.</p>
			<p>This chapter intends to demonstrate this step-by-step approach to working through your Spark certification exam. In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Overview of the certification exam</li>
				<li>Different types of questions to expect in the exam</li>
				<li>Overview of the rest of the chapters in this book</li>
			</ul>
			<p>We’ll start by providing an overview of the certification exam.</p>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor018"/>Overview of the certification exam</h1>
			<p>The exam <a id="_idIndexMarker000"/>consists of <strong class="bold">60 questions</strong>. The time you’re given to attempt these questions is <strong class="bold">120 minutes</strong>. This gives you about <strong class="bold">2 minutes </strong><strong class="bold">per question</strong>.</p>
			<p>To pass the exam, you need to have a <strong class="bold">score of 70%</strong>, which means that you need to <strong class="bold">answer 42 questions correctly</strong> out of 60 for you to pass.</p>
			<p>If you are well prepared, this time should be enough for you to answer the questions and also review them before the time finishes.</p>
			<p>Next, we will see how the questions are distributed throughout the exam.</p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor019"/>Distribution of questions</h2>
			<p>Exam questions <a id="_idIndexMarker001"/>are distributed into the following broad categories. The following table provides a breakdown of questions based on different categories:</p>
			<table id="table001-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Topic</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Percentage </strong><strong class="bold">of Exam</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Number </strong><strong class="bold">of Questions</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Spark Architecture: Understanding of Concepts</p>
						</td>
						<td class="No-Table-Style">
							<p>17%</p>
						</td>
						<td class="No-Table-Style">
							<p>10</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Spark Architecture: Understanding of Applications</p>
						</td>
						<td class="No-Table-Style">
							<p>11%</p>
						</td>
						<td class="No-Table-Style">
							<p>7</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Spark DataFrame API Applications</p>
						</td>
						<td class="No-Table-Style">
							<p>72%</p>
						</td>
						<td class="No-Table-Style">
							<p>43</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 1.1: Exam breakdown</p>
			<p>Looking at this distribution, you would want to focus on the Spark DataFrame API a lot more in your exam preparation since this section covers around 72% of the exam (about 43 questions). If you can answer these questions correctly, passing the exam will become easier.</p>
			<p>But this doesn’t mean that you shouldn’t focus on the Spark architecture areas. Spark architecture questions have varied difficulty, and they can sometimes be confusing. At the same time, they allow you to score easy points as architecture questions are generally straightforward.</p>
			<p>Let’s look at some of the other resources available that can help you prepare for this exam.</p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor020"/>Resources to prepare for the exam</h2>
			<p>When you start planning to take the certification exam, the first thing you must do is master Spark <a id="_idIndexMarker002"/>concepts. This book will help you with these concepts. Once you’ve done this, it would be useful to do mock exams. There are two mock exams available in this book for you to take advantage of.</p>
			<p>In addition, Databricks provides a practice exam, which is very useful for exam preparation. You can find it here: <a href="https://files.training.databricks.com/assessments/practice-exams/PracticeExam-DCADAS3-Python.pdf">https://files.training.databricks.com/assessments/practice-exams/PracticeExam-DCADAS3-Python.pdf</a>.</p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor021"/>Resources available during the exam</h2>
			<p>During the <a id="_idIndexMarker003"/>exam, you will be given access to the Spark documentation. This is done via <strong class="bold">Webassessor</strong> and its interface is a little different than <a id="_idIndexMarker004"/>the regular Spark documentation you’ll find on the internet. It would be good for you to familiarize yourself with this interface. You can find the interface at <a href="https://www.webassessor.com/zz/DATABRICKS/Python_v2.html">https://www.webassessor.com/zz/DATABRICKS/Python_v2.html</a>. I recommend going through it and trying to find different packages and functions of Spark via this documentation to make yourself comfortable navigating it during the exam.</p>
			<p>Next, we will look at how we can register for the exam.</p>
			<h1 id="_idParaDest-22"><a id="_idTextAnchor022"/>Registering for your exam</h1>
			<p>Databricks <a id="_idIndexMarker005"/>is the company that has prepared these exams and certifications. Here is the link to register for the exam: <a href="https://www.databricks.com/learn/certification/apache-spark-developer-associate">https://www.databricks.com/learn/certification/apache-spark-developer-associate</a>.</p>
			<p>Next, we will look at some of the prerequisites for the exam.</p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor023"/>Prerequisites for the exam</h2>
			<p>Some prerequisites <a id="_idIndexMarker006"/>are needed before you can take the exam so that you can be successful in passing the certification. Some of the major ones are as follows:</p>
			<ul>
				<li>Grasp the fundamentals of Spark architecture, encompassing the principles of Adaptive Query Execution.</li>
				<li>Utilize the Spark DataFrame API proficiently for various data manipulation tasks, such as the following:<ul><li>Performing column operations, such as selection, renaming, and manipulation</li><li>Executing row operations, including filtering, dropping, sorting, and aggregating data</li><li>Conducting DataFrame-related tasks, such as joining, reading, writing, and implementing partitioning strategies</li><li>Demonstrating <a id="_idIndexMarker007"/>proficiency in working with <strong class="bold">user-defined functions</strong> (<strong class="bold">UDFs</strong>) and Spark SQL functions</li></ul></li>
				<li>While not explicitly tested, a functional understanding of either Python or Scala is expected. The examination is available in both programming languages.</li>
			</ul>
			<p>Hopefully, by the <a id="_idIndexMarker008"/>end of this book, you will be able to fully grasp all these concepts and have done enough practice on your own to be prepared for the exam with full confidence.</p>
			<p>Now, let’s discuss what to expect during the online proctored exam.</p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor024"/>Online proctored exam</h2>
			<p>The Spark certification exam is an online proctored exam. What this means is that you will be <a id="_idIndexMarker009"/>taking the exam from the comfort of your home, but someone will be proctoring the exam online. I encourage you to understand the procedures and rules of the proctored exam in advance. This will save you a lot of trouble and anxiety at the time of the exam.</p>
			<p>To give you an overview, throughout the exam session, the following procedures will be in place:</p>
			<ul>
				<li>Webcam monitoring will be conducted by a Webassessor proctor to ensure exam integrity</li>
				<li>You will need to present a valid form of identification with a photo</li>
				<li>You will need to conduct the exam alone</li>
				<li>Your desk needs to be decluttered and there should be no other electronic devices in the room except the laptop that you’ll need for the exam</li>
				<li>There should not be any posters or charts on the walls of the room that may aid you in the exam</li>
				<li>The proctor will be listening to you during the exam as well, so you’ll want to make sure that you’re sitting in a quiet and comfortable environment</li>
				<li>It is recommended to <strong class="bold">not</strong> use your work laptop for this exam as it requires software to be installed and your antivirus and firewall to be disabled</li>
			</ul>
			<p>The <a id="_idIndexMarker010"/>proctor’s responsibilities are as follows:</p>
			<ul>
				<li>Overseeing your exam session to maintain exam integrity</li>
				<li>Addressing any queries related to the exam delivery process</li>
				<li>Offering technical assistance if needed</li>
				<li>It’s important to note that the proctor will not offer any form of assistance regarding the exam content</li>
			</ul>
			<p>I recommend that you take sufficient time before the exam to set up the environment where you’ll be taking the exam. This will ensure a smooth online exam procedure where you can focus on the questions and not worry about anything else.</p>
			<p>Now, let’s talk about the different types of questions that may appear in the exam.</p>
			<h1 id="_idParaDest-25"><a id="_idTextAnchor025"/>Types of questions</h1>
			<p>There are <a id="_idIndexMarker011"/>different categories of questions that you will find in the exam. They can be broadly divided into theoretical and code questions. We will look at both categories and their respective subcategories in this section.</p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor026"/>Theoretical questions</h2>
			<p>Theoretical questions are the questions where you will be asked about the conceptual understanding <a id="_idIndexMarker012"/>of certain topics. Theoretical questions <a id="_idIndexMarker013"/>can be subdivided further into different categories. Let’s look at some of these categories, along with example questions taken from previous exams that fall into them.</p>
			<h3>Explanation questions</h3>
			<p>Explanation <a id="_idIndexMarker014"/>questions are ones where you need to define and explain something. It can also include how something works and what it does. Let’s look at an example.</p>
			<p>Which of the following describes a worker node?</p>
			<ol class="margin-left">
				<li class="Alphabets">Worker nodes are the nodes of a cluster that perform computations.</li>
				<li class="Alphabets">Worker nodes are synonymous with executors.</li>
				<li class="Alphabets">Worker nodes always have a one-to-one relationship with executors.</li>
				<li class="Alphabets">Worker nodes are the most granular level of execution in the Spark execution hierarchy.</li>
				<li class="Alphabets">Worker nodes are the coarsest level of execution in the Spark execution hierarchy.</li>
			</ol>
			<h3>Connection questions</h3>
			<p>Connections <a id="_idIndexMarker015"/>questions are such questions where you need to define how different things are related to each other or how they differ from each other. Let’s look at an example to demonstrate this.</p>
			<p>Which of the following describes the relationship between worker nodes and executors?</p>
			<ol class="margin-left">
				<li class="Alphabets">An <a id="_idIndexMarker016"/>executor is a <strong class="bold">Java Virtual Machine</strong> (<strong class="bold">JVM</strong>) running on a worker node.</li>
				<li class="Alphabets">A worker node is a JVM running on an executor.</li>
				<li class="Alphabets">There are always more worker nodes than executors.</li>
				<li class="Alphabets">There are <a id="_idIndexMarker017"/>always the same number of executors and worker nodes.</li>
				<li class="Alphabets">Executors and worker nodes are not related.</li>
			</ol>
			<h3>Scenario question</h3>
			<p>Scenario <a id="_idIndexMarker018"/>questions involve defining how things work in different if-else scenarios – for example, “If ______ occurs, then _____ happens.” Moreover, it also includes questions where a statement is incorrect about a scenario. Let’s look at an example to demonstrate this.</p>
			<p>If Spark is running in cluster mode, which of the following statements about nodes is incorrect?</p>
			<ol class="margin-left">
				<li class="Alphabets">There is a single worker node that contains the Spark driver and the executors.</li>
				<li class="Alphabets">The Spark driver runs in its own non-worker node without any executors.</li>
				<li class="Alphabets">Each executor is a running JVM inside a worker node.</li>
				<li class="Alphabets">There is always more than one node.</li>
				<li class="Alphabets">There might be more executors than total nodes or more total nodes than executors.</li>
			</ol>
			<h3>Categorization questions</h3>
			<p>Categorization <a id="_idIndexMarker019"/>questions are such questions where you need to describe categories that something belongs to. Let’s look at an example to demonstrate this.</p>
			<p>Which of the following statements accurately describes stages?</p>
			<ol class="margin-left">
				<li class="Alphabets">Tasks within a stage can be simultaneously executed by multiple machines.</li>
				<li class="Alphabets">Various stages within a job can run concurrently.</li>
				<li class="Alphabets">Stages comprise one or more jobs.</li>
				<li class="Alphabets">Stages <a id="_idIndexMarker020"/>temporarily store transactions before committing them through actions.</li>
			</ol>
			<h3>Configuration questions</h3>
			<p>Configuration questions are such questions where you need to outline how things will behave <a id="_idIndexMarker021"/>based on different cluster configurations. Let’s look at an example to demonstrate this.</p>
			<p>Which of the following statements accurately describes Spark’s cluster execution mode?</p>
			<ol class="margin-left">
				<li class="Alphabets">Cluster mode runs executor processes on gateway nodes.</li>
				<li class="Alphabets">Cluster mode involves the driver being hosted on a gateway machine.</li>
				<li class="Alphabets">In cluster mode, the Spark driver and the cluster manager are not co-located.</li>
				<li class="Alphabets">The driver in cluster mode is located on a worker node.</li>
			</ol>
			<p>Next, we’ll look at the code-based questions and their subcategories.</p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor027"/>Code-based questions</h2>
			<p>The next category is code-based questions. A large number of Spark API-based questions <a id="_idIndexMarker022"/>lie in this category. Code-based questions are the questions where you will be given a code snippet, and you will be asked questions about it. Code-based <a id="_idIndexMarker023"/>questions can be subdivided further into different categories. Let’s look at some of these categories, along with example questions taken from previous exams that fall into these different subcategories.</p>
			<h3>Function identification questions</h3>
			<p>Function <a id="_idIndexMarker024"/>identification questions are such questions where you need to define which function does something. It is important to know the different functions that are available in Spark for data manipulation, along with their syntax. Let’s look at an example to demonstrate this.</p>
			<p>Which of <a id="_idIndexMarker025"/>the following code blocks returns a copy of the <code>df</code> DataFrame, where the <code>column</code> salary has been renamed <code>employeeSalary</code>?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.withColumn(["salary", "employeeSalary"])</code></li>
				<li class="Alphabets"><code>df.withColumnRenamed("salary").alias("employeeSalary ")</code></li>
				<li class="Alphabets"><code>df.withColumnRenamed("salary", " </code><code>employeeSalary ")</code></li>
				<li class="Alphabets"><code>df.withColumn("salary", " </code><code>employeeSalary ")</code></li>
			</ol>
			<h3>Fill-in-the-blank questions</h3>
			<p>Fill-in-the-blank questions are such questions where you need to complete the code block <a id="_idIndexMarker026"/>by filling in the blanks. Let’s look at an example to demonstrate this.</p>
			<p>The following code block should return a DataFrame with the <code>employeeId</code>, <code>salary</code>, <code>bonus</code>, and <code>department</code> columns from the <code>transactionsDf</code> DataFrame. Choose the answer that correctly fills the blanks to accomplish this.</p>
			<pre class="source-code">
df.__1__(__2__)</pre>			<ol class="margin-left">
				<li class="Alphabets"><ol><li class="lower-roman"><code>drop</code></li><li class="lower-roman"><code>"employeeId", "salary", "</code><code>bonus", "department"</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>filter</code></li><li class="lower-roman"><code>"employeeId, salary, </code><code>bonus, department"</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>select</code></li><li class="lower-roman"><code>["employeeId", "salary", "</code><code>bonus", "department"]</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>select</code></li><li class="lower-roman"><code>col(["employeeId", "salary", "</code><code>bonus", "department"])</code></li></ol></li>
			</ol>
			<h3>Order-lines-of-code questions</h3>
			<p>Order-lines-of-code questions are such questions where you need to place the lines of code <a id="_idIndexMarker027"/>in a certain order so that you can execute an operation correctly. Let’s look at an example to demonstrate this.</p>
			<p>Which of the following code blocks creates a DataFrame that shows the mean of the <code>salary</code> column of the <code>salaryDf</code> DataFrame based on the <code>department</code> and <code>state</code> columns, where <code>age</code> is greater than 35?</p>
			<ol>
				<li class="lower-roman"><code>salaryDf.filter(col("age") &gt; </code><code>35)</code></li>
				<li class="lower-roman"><code>.</code><code>filter(col("employeeID")</code></li>
				<li class="lower-roman"><code>.</code><code>filter(col("employeeID").isNotNull())</code></li>
				<li class="lower-roman"><code>.</code><code>groupBy("department")</code></li>
				<li class="lower-roman"><code>.</code><code>groupBy("department", "state")</code></li>
				<li class="lower-roman"><code>.</code><code>agg(avg("salary").alias("mean_salary"))</code></li>
				<li class="lower-roman"><code>.</code><code>agg(average("salary").alias("mean_salary"))</code></li>
			</ol>
			<ol class="margin-left">
				<li class="Alphabets">i, ii, v, vi</li>
				<li class="Alphabets">i, iii, v, vi</li>
				<li class="Alphabets">i, iii, vi, vii</li>
				<li class="Alphabets">i, ii, iv, vi</li>
			</ol>
			<h1 id="_idParaDest-28"><a id="_idTextAnchor028"/>Summary</h1>
			<p>This chapter provided an overview of the certification exam. At this point, you know what to expect in the exam and how to best prepare for it. To do so, we covered different types of questions that you will encounter.</p>
			<p>Going forward, each chapter of this book will equip you with practical knowledge and hands-on examples so that you can harness the power of Apache Spark for various data processing and analytics tasks.</p>
		</div>
	

		<div><h1 id="_idParaDest-29" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor029"/>Part 2: Introducing Spark</h1>
			<p>This part will offer you a comprehensive understanding of Spark’s capabilities and operational principles. It will cover what Spark is, why it’s important, and some of the applications Spark is most useful in. It will tell you about the different types of users who can benefit from Spark. It will also cover the basics of Spark architecture and how applications are navigated through in Spark. It will detail narrow and wide Spark transformations and discuss lazy evaluations in Spark. It’s important to have this understanding because Spark works differently than other traditional frameworks.</p>
			<p>This part has the following chapters:</p>
			<ul>
				<li><a href="B19176_02.xhtml#_idTextAnchor030"><em class="italic">Chapter 2</em></a><em class="italic">, Understanding Apache Spark and Its Applications</em></li>
				<li><a href="B19176_03.xhtml#_idTextAnchor053"><em class="italic">Chapter 3</em></a><em class="italic">, Spark Architecture and Transformations</em></li>
			</ul>
		</div>
		<div><div></div>
		</div>
		<div><div></div>
		</div>
	</body></html>