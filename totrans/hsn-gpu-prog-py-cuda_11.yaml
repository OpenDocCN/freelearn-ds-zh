- en: Performance Optimization in CUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this penultimate chapter, we will cover some fairly advanced CUDA features
    that we can use for low-level performance optimizations. We will start by learning
    about dynamic parallelism, which allows kernels to launch and manage other kernels
    on the GPU, and see how we can use this to implement quicksort directly on the
    GPU. We will learn about vectorized memory access, which can be used to increase
    memory access speedups when reading from the GPU's global memory. We will then
    look at how we can use CUDA atomic operations, which are thread-safe functions
    that can operate on shared data without thread synchronization or *mutex* locks.
    We will learn about Warps, which are fundamental blocks of 32 or fewer threads,
    in which threads can read or write to each other's variables directly, and then
    make a brief foray into the world of PTX Assembly. We'll do this by directly writing
    some basic PTX Assembly inline within our CUDA-C code, which itself will be inline
    in our Python code! Finally, we will bring all of these little low-level tweaks
    together into one final example, where we will apply them to make a blazingly
    fast summation kernel, and compare this to PyCUDA's sum.
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning outcomes for this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic parallelism in CUDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing quicksort on the GPU with dynamic parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using vectorized types to speed up device memory accesses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using thread-safe CUDA atomic operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic PTX Assembly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying all of these concepts to write a performance-optimized summation kernel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we will take a look at **dynamic parallelism**, a feature in CUDA that
    allows a kernel to launch and manage other kernels without any interaction or
    input on behalf of the host. This also makes many of the host-side CUDA-C features
    that are normally available also available on the GPU, such as device memory allocation/deallocation,
    device-to-device memory copies, context-wide synchronizations, and streams.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with a very simple example. We will create a small kernel over *N*
    threads that will print a short message to the terminal from each thread, which
    will then recursively launch another kernel over *N - 1* threads. This process
    will continue until *N* reaches 1\. (Of course, beyond illustrating how dynamic
    parallelism works, this example would be pretty pointless.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the `import` statements in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we have to import `DynamicSourceModule` rather than the usual `SourceModule`!
    This is due to the fact that the dynamic parallelism feature requires particular
    configuration details to be set by the compiler. Otherwise, this will look and
    act like a usual `SourceModule` operation. Now we can continue writing the kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The most important thing here to note is this: we must be careful that we have
    only a single thread launch the next iteration of kernels with a single thread
    with a well-placed `if` statement that checks the `threadIdx` and `blockIdx` values.
    If we don''t do this, then each thread will launch far more kernel instances than
    necessary at every depth iteration. Also, notice how we could just launch the
    kernel in a normal way with the usual CUDA-C triple-bracket notation—we don''t
    have to use any obscure or low-level commands to make use of dynamic parallelism.'
  prefs: []
  type: TYPE_NORMAL
- en: When using the CUDA dynamic parallelism feature, always be careful to avoid
    unnecessary kernel launches. This can be done by having a designated thread launch
    the next iteration of kernels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s finish this up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can run the preceding code, which will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5147fd83-75b9-4dd4-8e5f-07a8cf013436.png)'
  prefs: []
  type: TYPE_IMG
- en: This example can also be found in the `dynamic_hello.py` file under the directory
    in this book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Quicksort with dynamic parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's look at a slightly more interesting and utilitarian application of
    dynamic parallelism—the **Quicksort Algorithm**. This is actually a well-suited
    algorithm for parallelization, as we will see.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with a brief review. Quicksort is a recursive and in-place sorting
    algorithm that has an average and best case performance of *O(N log N)*, and worst-case
    performance of *O(N²)*. Quicksort is performed by choosing an arbitrary point
    called a *pivot* in an unsorted array, and then partitioning the array into a
    left array (which contains all points less than the pivot), a right array (which
    contains all points equal to or greater than the pivot), with the pivot in-between
    the two arrays. If one or both of the arrays now has a length greater than 1,
    then we recursively call quicksort again on one or both of the sub-arrays, with
    the pivot point now in its final position.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quicksort can be implemented in a single line in pure Python using functional
    programming:'
  prefs: []
  type: TYPE_NORMAL
- en: '`qsort = lambda xs : [] if xs == [] else qsort(filter(lambda x: x < xs[-1]
    , xs[0:-1])) + [xs[-1]] + qsort(filter(lambda x: x >= xs[-1] , xs[0:-1]))`'
  prefs: []
  type: TYPE_NORMAL
- en: We can see where parallelism will come into play by the fact that quicksort
    is recursively called on both the right and left arrays—we can see how this will
    start with one thread operating on an initial large array, but by the time the
    arrays get very small, there should be many threads working on them. Here, we
    will actually accomplish this by launching all of the kernels over one *single
    thread each*!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get going, and start with the import statements. (We will ensure that
    we import the `shuffle` function from the standard random module for the example
    that we will go over later.):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we''ll write our quicksort kernel. We''ll write a `device` function for
    the partitioning step, which will take an integer pointer, the lowest point of
    the subarray to partition, and the highest point of the subarray. This function
    will also use the highest point of this subarray as the pivot. Ultimately, after
    this function is done, it will return the final resting place of the pivot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can write the kernel that implements this partition function into a
    parallel quicksort. We''ll have to use the CUDA-C conventions for streams, which
    we haven''t seen so far: to launch a kernel *k* in a stream *s* in CUDA-C, we
    use `k<<<grid, block, sharedMemBytesPerBlock, s>>>(...)`. By using two streams
    here, we can be sure that they are launched in parallel. (Considering that we
    won''t be using shared memory, we''ll set the third launch parameter to "0".)
    The creation and destruction of the stream objects should be self-explanatory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s randomly shuffle a list of 100 integers and have our kernel sort
    this for us. Notice how we launch the kernel over a single thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This program is also available in the `dynamic_quicksort.py` file in this book's
    GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Vectorized data types and memory access
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now look at CUDA's Vectorized Data Types. These are *vectorized* versions
    of the standard datatypes, such as int or double, in that they can store multiple
    values. There are *vectorized* versions of the 32-bit types of up to size 4 (for
    example, `int2`, `int3`, `int4`, and `float4`), while 64-bit variables can only
    be vectorized to be twice their original size (for example, `double2` and `long2`).
    For a size 4 vectorized variable, we access each individual element using the
    C "struct" notation for the members `x`, `y`, `z`, and `w`, while we use `x`,`y`,
    and `z` for a 3-member variable and just `x` and `y` for a 2-member variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'These may seem pointless right now, but these datatypes can be used to improve
    the performance of loading arrays from the global memory. Now, let''s do a small
    test to see how we can load some int4 variables from an array of integers, and
    double2s from an array of doubles—we will have to use the CUDA `reinterpret_cast`
    operator to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice how we have to use the `dereference` operator `*` to set the vectorized
    variables, and how we have to jump to the next address by reference (`&ints[4]`,
    `&doubles[2]`) to load the second `int4` and `double2` by using the reference
    operator `&` on the array:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b0e1417-e3c7-4896-9672-279ced733a2f.png)'
  prefs: []
  type: TYPE_IMG
- en: This example is also available in the `vectorized_memory.py` file in this book's
    GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Thread-safe atomic operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now learn about **atomic operations** in CUDA. Atomic operations are
    very simple, thread-safe operations that output to a single global array element
    or shared memory variable, which would normally lead to race conditions otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Let's think of one example. Suppose that we have a kernel, and we set a local
    variable called `x` across all threads at some point. We then want to find the
    maximum value over all *x*s, and then set this value to the shared variable we
    declare with `__shared__ int x_largest`. We can do this by just calling `atomicMax(&x_largest,
    x)` over every thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a brief example of atomic operations. We will write a small
    program for two experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting a variable to 0 and then adding 1 to this for each thread
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the maximum thread ID value across all threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s start out by setting the `tid` integer to the global thread ID as usual,
    and then set the global `add_out` variable to 0\. In the past, we would do this
    by having a single thread alter the variable using an `if` statement, but now
    we can use `atomicExch(add_out, 0)` across all threads. Let''s do the imports
    and write our kernel up to this point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'It should be noted that while Atomics are indeed thread-safe, they by no means
    guarantee that all threads will access them at the same time, and they may be
    executed at different times by different threads. This can be problematic here,
    since we will be modifying `add_out` in the next step. This might lead to `add_out`
    being reset after it''s already been partially modified by some of the threads.
    Let''s do a block-synchronization to guard against this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use `atomicAdd` to add `1` to `add_out` for each thread, which will
    give us the total number of threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s check what the maximum value of `tid` is for all threads by using
    `atomicMax`. We can then close off our CUDA kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now add the test code; let''s try launching this over 1 block of 100
    threads. We only need two variables here, so we will have to allocate some `gpuarray`
    objects of only size 1\. We will then print the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are prepared to run this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0799721-50a9-45e2-a9d2-6c3f8d097bcb.png)'
  prefs: []
  type: TYPE_IMG
- en: This example is also available as the `atomic.py` file in this book's GitHub
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: Warp shuffling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now look at what is known as **warp shuffling**. This is a feature in
    CUDA that allows threads that exist within the same CUDA Warp concurrently to
    communicate by directly reading and writing to each other's registers (that is,
    their local stack-space variables), without the use of *shared* variables or global
    device memory. Warp shuffling is actually much faster and easier to use than the
    other two options. This almost sounds too good to be true, so there must be a
    *catch—*indeed, the *catch* is that this only works between threads that exist
    on the same CUDA Warp, which limits shuffling operations to groups of threads
    of size 32 or less. Another catch is that we can only use datatypes that are 32
    bits or less. This means that we can't shuffle 64-bit *long long* integers or
    *double* floating point values across a Warp.
  prefs: []
  type: TYPE_NORMAL
- en: Only 32-bit (or smaller) datatypes can be used with CUDA Warp shuffling! This
    means that while we can use integers, floats, and chars, we cannot use doubles
    or *long long* integers!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s briefly review CUDA Warps before we move on to any coding. (You might
    wish to review the section entitled *The warp lockstep property* in [Chapter 6](6d1c808f-1dc2-4454-b0b8-d0a36bc3c908.xhtml),
    *Debugging and Profiling Your CUDA Code*, before we continue.) A CUDA **Warp**
    is the minimal execution unit in CUDA that consists of 32 threads or less, that
    runs on exactly 32 GPU cores. Just as a Grid consists of blocks, blocks similarly
    consist of one or more Warps, depending on the number of threads the Block uses
    – if a Block consists of 32 threads, then it will use one Warp, and if it uses
    96 threads, it will consist of three Warps. Even if a Warp is of a size less than
    32, it is also considered a full Warp: this means that a Block with only one single
    thread will use 32 cores. This also implies that a block of 33 threads will consist
    of two Warps and 31 cores.'
  prefs: []
  type: TYPE_NORMAL
- en: To remember what we looked at in [Chapter 6](6d1c808f-1dc2-4454-b0b8-d0a36bc3c908.xhtml),
    *Debugging and Profiling Your CUDA Code*, a Warp has what is known as the **Lockstep
    Property**. This means that every thread in a warp will iterate through every
    instruction, perfectly in parallel with every other thread in the Warp. That is
    to say, every thread in a single Warp will step through the same exact instructions
    simultaneously, *ignoring* any instructions that are not applicable to a particular
    thread – this is why any divergence among threads within a single Warp is to be
    avoided as much as possible. NVIDIA calls this execution model **Single Instruction
    Multiple Thread**, or **SIMT**. By now, you should understand why we have tried
    to always use Blocks of 32 threads consistently throughout the text!
  prefs: []
  type: TYPE_NORMAL
- en: We need to learn one more term before we get going—a **lane** in a Warp is a
    unique identifier for a particular thread within the warp, which will be between
    0 and 31\. Sometimes, this is also called the **Lane ID**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with a simple example: we will use the `__shfl_xor` command to
    swap the values of a particular variable between all even and odd numbered Lanes
    (threads) within our warp. This is actually very quick and easy to do, so let''s
    write our kernel and take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Everything here is familiar to us except `__shfl_xor` . This is how an individual
    CUDA thread sees this: this function takes the value of `temp` as an input from
    the current thread. It performs an `XOR` operation on the binary Lane ID of the
    current thread with `1`, which will be either its left neighbor (if the least
    significant digit of this thread''s Lane is "1" in binary), or its right neighbor
    (if the least significant digit is "0" in binary). It then sends the current thread''s
    `temp` value to its neighbor, while retrieving the neighbor''s temp value, which
    is `__shfl_xor`. This will be returned as output right back into `temp`. We then
    set the value in the output array, which will swap our input array values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s write the rest of the test code and then check the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0f8d38e-aae3-4dee-8d83-ac869ba587bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's do one more warp-shuffling example before we move on—we will implement
    an operation to sum a single local variable over all of the threads in a Warp.
    Let's recall the Naive Parallel Sum algorithm from [Chapter 4](5a5f4317-50c7-4ce6-9d04-ac3be4c6d28b.xhtml),
    *Kernels, Threads, Blocks, and Grids*, which is very fast but makes the *naive*
    assumption that we have as many processors as we do pieces of data—this is one
    of the few cases in life where we actually will, assuming that we're working with
    an array of size 32 or less. We will use the `__shfl_down` function to implement
    this in a single warp. `__shfl_down` takes the thread variable in the first parameter
    and works by *shifting* a variable between threads by the certain number of steps
    indicated in the second parameter, while the third parameter will indicate the
    total size of the Warp.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement this right now. Again, if you aren''t familiar with the Naive
    Parallel Sum or don''t remember why this should work, please review [Chapter 4](5a5f4317-50c7-4ce6-9d04-ac3be4c6d28b.xhtml),
    *Kernels, Threads, Blocks, and Grids*. We will implement a straight-up sum with
    `__shfl_down`, and then run this on an array that includes the integers 0 through
    31\. We will then compare this against NumPy''s own `sum` function to ensure correctness:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ed86d23-a9fb-4770-add7-d80587b7aa01.png)'
  prefs: []
  type: TYPE_IMG
- en: The examples in this section are also available as the `shfl_sum.py` and `shfl_xor.py`
    files under the `Chapter11` directory in this book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Inline PTX assembly
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now scratch the surface of writing PTX (Parallel Thread eXecution) Assembly
    language, which is a kind of a pseudo-assembly language that works across all
    Nvidia GPUs, which is, in turn, compiled by a Just-In-Time (JIT) compiler to the
    specific GPU's actual machine code. While this obviously isn't intended for day-to-day
    usage, it will let us work at an even a lower level than C if necessary. One particular
    use case is that you can easily disassemble a CUDA binary file (a host-side executable/library
    or a CUDA .cubin binary) and inspect its PTX code if no source code is otherwise
    available. This can be done with the `cuobjdump.exe -ptx cuda_binary` command
    in both Windows and Linux.
  prefs: []
  type: TYPE_NORMAL
- en: 'As stated previously, we will only cover some of the basic usages of PTX from
    within CUDA-C, which has a particular syntax and usage which is similar to that
    of using the inline host-side assembly language in GCC. Let''s get going with
    our code—we will do the imports and start writing our GPU code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We will do several mini-experiments here by writing the code into separate
    device functions. Let''s start with a simple function that sets an input variable
    to zero. (We can use the C++ pass-by-reference operator `&` in CUDA, which we
    will use in the `device` function.):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s break this down before we move on. `asm`, of course, will indicate to
    the `nvcc` compiler that we are going to be using assembly, so we will have to
    put that code into quotes so that it can be handled properly. The `mov` instruction
    just copies a constant or other value, and inputs this into a **register**. (A
    register is the most fundamental type of on-chip storage unit that a GPU or CPU
    uses to store or manipulate values; this is how most *local* variables are used
    in CUDA.) The `.s32` part of `mov.s32` indicates that we are working with a signed,
    32-bit integer variable—PTX Assembly doesn''t have *types* for data in the sense
    of C, so we have to be careful to use the correct particular operations. `%0`
    tells `nvcc` to use the register corresponding to the `0th` argument of the string
    here, and we separate this from the next *input* to `mov` with a comma, which
    is the constant `0`. We then end the line of assembly with a semicolon, like we
    would in C, and close off this string of assembly code with a quote. We''ll have
    to then use a colon (not a comma!) to indicate the variables we want to use in
    our code. The `"=r"` means two things: the `=` will indicate to `nvcc` that the
    register will be written to as an output, while the `r` indicates that this should
    be handled as a 32-bit integer datatype. We then put the variable we want to be
    handled by the assembler in parentheses, and then close off the `asm`, just like
    we would with any C function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All of that exposition to set the value of a single variable to 0! Now, let''s
    make a small device function that will add two floating-point numbers for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Let's stop and notice a few things. First, of course, we are using `add.f32`
    to indicate that we want to add two 32-bit floating point values together. We
    also use `"=f"` to indicate that we will be writing to a register, and `f` to
    indicate that we will be only reading from it. Also, notice how we use a colon
    to separate the `write` registers from the `only read` registers for `nvcc`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at one more simple example before we continue, that is, a function
    akin to the `++` operator in C that increments an integer by `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: First, notice that we use the "0th" parameter as both the output and the first
    input. Next, notice that we are using `+r` rather than `=r`—the `+` tells `nvcc`
    that this register will be read from *and* written to in this instruction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we won''t be getting any fancier than this, as even writing a simple `if`
    statement in assembly language is fairly involved. However, let''s look at some
    more examples that will come in useful when using CUDA Warps. Let''s start with
    a small function that will give us the lane ID of the current thread; this is
    particularly useful, and actually far more straightforward than doing this with
    CUDA-C, since the lane ID is actually stored in a special register called `%laneid`
    that we can''t access in pure C. (Notice how we use two `%` symbols in the code,
    which will indicate to `nvcc` to directly use the `%` in the assembly code for
    the `%laneid` reference rather than interpret this as an argument to the `asm`
    command.):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s write two more functions that will be useful for dealing with CUDA
    Warps. Remember, you can only pass a 32-bit variable across a Warp using a shuffle
    command. This means that to pass a 64-bit variable over a warp, we have to split
    this into two 32-bit variables, shuffle both of those to another thread individually,
    and then re-combine these 32-bit values back into the original 64-bit variable.
    We can use the `mov.b64` command for the case of splitting a 64-bit double into
    two 32-bit integers—notice how we have to use `d` to indicate a 64-bit floating-point
    double:'
  prefs: []
  type: TYPE_NORMAL
- en: Notice our use of `volatile` in the following code, which will ensure that these
    commands are executed exactly as written after they are compiled. We do this because
    sometimes a compiler will make its own optimizations to or around inline assembly
    code, but for particularly delicate operations such as this, we want this done
    exactly as written.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s write a simple kernel that will test all of the PTX assembly device
    functions we wrote. We will then launch it over one single thread so that we can
    check everything:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now run the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bebb9f8f-a21c-4b06-b3e9-3686b8c96b41.png)'
  prefs: []
  type: TYPE_IMG
- en: This example is also available as the `ptx_assembly.py` file under the `Chapter11`
    directory in this book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Performance-optimized array sum
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the final example of this book, we will now make a standard array summation
    kernel for a given array of doubles, except this time we will use every trick
    that we've learned in this chapter to make it as fast as possible. We will check
    the output of our summing kernel against NumPy's `sum` function, and then we will
    run some tests with the standard Python `timeit` function to compare how our function
    compares to PyCUDA's own `sum` function for `gpuarray` objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started by importing all of the necessary libraries, and then start
    with a `laneid` function, similar to the one we used in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Let's note a few things—notice that we put a new inline statement in the declaration
    of our device function. This will effectively make our function into a macro,
    which will shave off a little time from calling and branching to a device function
    when we call this from the kernel. Also, notice that we set the `id` variable
    by reference instead of returning a value—in this case, there may actually be
    two integer registers that should be used, and there should be an additional copy
    command. This guarantees that this won't happen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write the other device functions in a similar fashion. We will need
    to have two more device functions so that we can split and combine a 64-bit double
    into two 32-bit variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start writing the kernel. We will take in an array of doubles called
    input, and then output the entire sum to `out`, which should be initialized to
    `0`. We will start by getting the lane ID for the current thread and loading two
    values from global memory into the current thread with vectorized memory loading:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s sum these values from the double2 `vals` variable into a new double
    variable, `sum_val`, which will keep track of all the summations across this thread.
    We will create two 32-bit integers, `s1` and `s2`, that we will use for splitting
    this value and sharing it with Warp Shuffling, and then create a `temp` variable
    for reconstructed values we receive from other threads in this Warp:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s use a Naive Parallel sum again across the warp, which will be the
    same as summing 32-bit integers across a Warp, except we will be using our `split64`
    and `combine64` PTX functions on `sum_val` and `temp` for each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we are done, let''s have the `0th` thread of every single warp add
    their end value to `out` using the thread-safe `atomicAdd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now write our test code with `timeit` operations to measure the average
    time of our kernel and PyCUDA''s sum over 20 iterations of both on an array of
    10000*2*32 doubles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run this from IPython. Make sure that you have run both `gpuarray.sum`
    and `sum_ker` beforehand to ensure that we aren''t timing any compilation by `nvcc`
    as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1e04079-149b-4da4-9524-6bc4ef455108.png)'
  prefs: []
  type: TYPE_IMG
- en: So, while summing is normally pretty boring, we can be excited by the fact that
    our clever use of hardware tricks can speed up such a bland and trivial algorithm
    quite a bit.
  prefs: []
  type: TYPE_NORMAL
- en: This example is available as the `performance_sum_ker.py` file under the `Chapter11`
    directory in this book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter by learning about dynamic parallelism, which is a paradigm
    that allows us to launch and manage kernels directly on the GPU from other kernels.
    We saw how we can use this to implement a quicksort algorithm on the GPU directly.
    We then learned about vectorized datatypes in CUDA, and saw how we can use these
    to speed up memory reads from global device memory. We then learned about CUDA
    Warps, which are small units of 32 threads or less on the GPU, and we saw how
    threads within a single Warp can directly read and write to each other's registers
    using Warp Shuffling. We then looked at how we can write a few basic operations
    in PTX assembly, including import operations such as determining the lane ID and
    splitting a 64-bit variable into two 32-bit variables. Finally, we ended this
    chapter by writing a new performance-optimized summation kernel that is used for
    arrays of doubles, applying almost most of the tricks we've learned in this chapter.
    We saw that this is actually faster than the standard PyCUDA sum on double arrays
    with a length of an order of 500,000.
  prefs: []
  type: TYPE_NORMAL
- en: We have gotten through all of the technical chapters of this book! You should
    be proud of yourself, since you are now surely a skilled GPU programmer with many
    tricks up your sleeve. We will now embark upon the final chapter, where we will
    take a brief tour of a few of the different paths you can take to apply and extend
    your GPU programming knowledge from here.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the atomic operations example, try changing the grid size from 1 to 2 before
    the kernel is launched while leaving the total block size at 100\. If this gives
    you the wrong output for `add_out` (anything other than 200), then why is it wrong,
    considering that `atomicExch` is thread-safe?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the atomic operations example, try removing `__syncthreads`, and then run
    the kernel over the original parameters of grid size 1 and block size 100\. If
    this gives you the wrong output for `add_out` (anything other than 100), then
    why is it wrong, considering that `atomicExch` is thread-safe?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we not have to use `__syncthreads` to synchronize over a block of size
    32 or less?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We saw that `sum_ker` is around five times faster than PyCUDA's sum operation
    for random-valued arrays of length 640,000 (`10000*2*32`). If you try adding a
    zero to the end of this number (that is, multiply it by 10), you'll notice that
    the performance drops to the point where `sum_ker` is only about 1.5 times as
    fast as PyCUDA's sum. If you add another zero to the end of that number, you'll
    notice that `sum_ker` is only 75% as fast as PyCUDA's sum. Why do you think this
    is the case? How can we improve `sum_ker` to be faster on larger arrays?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Which algorithm performs more addition operations (counting both calls to the
    C + operator and atomicSum as a single operation): `sum_ker` or PyCUDA''s `sum`?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
