- en: 'Chapter 4: Rule-Based Matching'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章：基于规则的匹配
- en: '**Rule-based information extraction** is indispensable for any NLP pipeline.
    Certain types of entities, such as times, dates, and telephone numbers have distinct
    formats that can be recognized by a set of rules, without having to train statistical
    models.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于规则的实体提取**对于任何 NLP 管道都是必不可少的。某些类型的实体，如时间、日期和电话号码，具有独特的格式，可以通过一组规则识别，而无需训练统计模型。'
- en: In this chapter, you will learn how to quickly extract information from the
    text by matching patterns and phrases. You will use `Matcher` objects. You will
    continue with fine-graining statistical models with rule-based matching to lift
    statistical models to better accuracies.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何通过匹配模式和短语快速从文本中提取信息。你将使用 `Matcher` 对象。你将继续使用基于规则的匹配来细化统计模型，以提高准确性。
- en: By the end of this chapter, you will know a vital part of information extraction.
    You will be able to extract entities of specific formats, as well as entities
    specific to your domain.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解信息提取的关键部分。你将能够提取特定格式的实体，以及特定领域的实体。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下主要主题：
- en: Token-based matching
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于标记的匹配
- en: PhraseMatcher
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短语匹配器
- en: EntityRuler
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实体规则器
- en: Combining spaCy models and matchers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合 spaCy 模型和匹配器
- en: Token-based matching
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于标记的匹配
- en: So far, we've explored the sophisticated linguistic concepts that require statistical
    models and their usages with spaCy. Some NLU tasks can be solved in tricky ways
    without the help of any statistical model. One of those ways is **regex**, which
    we use to match a predefined set of patterns to our text.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了需要统计模型和其用法的复杂语言概念，以及使用 spaCy 的应用。某些 NLU 任务可以在没有统计模型帮助的情况下以巧妙的方式解决。其中一种方式是
    **正则表达式**，我们用它来将预定义的模式与我们的文本进行匹配。
- en: A regex (a regular expression) is a sequence of characters that specifies a
    search pattern. A regex describes a set of strings that follows the specified
    pattern. A regex can include letters, digits, and characters with special meanings,
    such as *?*, *.*, and ***. Python's built-in library provides great support to
    define and match regular expressions. There's another Python 3 library called
    regex that aims wants to replace **re** in the future.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式（正则表达式）是一系列字符，它指定了一个搜索模式。正则表达式描述了一组遵循指定模式的字符串。正则表达式可以包括字母、数字以及具有特殊意义的字符，例如
    *?*, *.*, 和 ***。Python 的内置库提供了强大的支持来定义和匹配正则表达式。还有一个名为 regex 的 Python 3 库，其目标是未来取代
    **re**。
- en: Readers who are actively developing NLP applications with Python have definitely
    come across regex code and, even better, have written regex themselves.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正在积极使用 Python 开发 NLP 应用的读者肯定遇到过正则表达式代码，甚至更好地，他们自己编写过正则表达式。
- en: 'What does a regex look like, then? The following regex matches the following
    strings:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，正则表达式看起来是什么样子的呢？以下正则表达式匹配以下字符串：
- en: Barack Obama
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 巴拉克·奥巴马
- en: Barack Obama
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 巴拉克·奥巴马
- en: Barack Hussein Obama
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 巴拉克·侯赛因·奥巴马
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This pattern can be read as: the string `Barack` can be followed optionally
    by the string `Hussein` (the `?` character in regex means optional, that is, `0`
    or `1` occurrence) and should be followed by the string `Obama`. The inter-word
    spaces can be a single space character, a tab, or any other whitespace character
    (`\s` matches all sorts of whitespace characters, including the newline character).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模式可以读作：字符串 `Barack` 可以可选地后跟字符串 `Hussein`（正则表达式中的 `?` 字符表示可选的，即 `0` 或 `1` 次出现）并且应该后跟字符串
    `Obama`。单词间的空格可以是一个空格字符、一个制表符或任何其他空白字符（`\s` 匹配所有类型的空白字符，包括换行符）。
- en: 'It''s not very readable, even for such a short and uncomplicated pattern, is
    it? That is the downside of regex, it is the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对于如此简短且简单的模式，它也不太易读，对吧？这就是正则表达式的缺点，它是以下：
- en: Difficult to read
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 难以阅读
- en: Difficult to debug
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 难以调试
- en: Error prone with space, punctuation, and number characters
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容易出错，尤其是在空格、标点符号和数字字符方面
- en: 'For these reasons, many software engineers don''t like to work with regex in
    their production code. spaCy provides a very clean, readable, production-level,
    and maintainable alternative: the `Matcher` class. The `Matcher` class can match
    our predefined rules to the sequence of tokens in `Doc` and `Span` objects; moreover,
    the rules can refer to the token or its linguistic attributes (more on this subject
    later in this section).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，许多软件工程师不喜欢在生产代码中使用正则表达式。spaCy提供了一个非常干净、可读、生产级和可维护的替代方案：`Matcher`类。`Matcher`类可以将我们预定义的规则与`Doc`和`Span`对象中的标记序列进行匹配；此外，规则可以引用标记或其语言属性（关于这个主题，在本节稍后部分会详细介绍）。
- en: 'Let''s start with a basic example of how to call the `Matcher` class:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从如何调用`Matcher`类的基本例子开始：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It looks complicated, but don''t be intimidated, we''ll go over the lines one
    by one:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来很复杂，但不要感到害怕，我们会逐行讲解：
- en: We imported `spacy` in the first line; this should be familiar.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一行，我们导入了`spacy`；这应该是熟悉的。
- en: On the second line, we imported the `Matcher` class in order to use it in the
    rest of the code.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二行，我们导入了`Matcher`类，以便在代码的其余部分中使用它。
- en: On the next lines, we created the `nlp` object as usual and created the `doc`
    object with our example sentence.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在接下来的几行中，我们像往常一样创建了`nlp`对象，并使用我们的示例句子创建了`doc`对象。
- en: 'Now, pay attention: a `matcher` object needs to be initialized with a `Vocabulary`
    object, so on line 5 we initialize our `matcher` object with the language model''s
    vocabulary (this is the usual way to do it).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，请注意：一个`matcher`对象需要用`Vocabulary`对象进行初始化，因此在第5行，我们用语言模型词汇表初始化了我们的`matcher`对象（这是通常的做法）。
- en: What comes next is to define the pattern we want to match. Here, we define *pattern*
    as a list where every list item enclosed in a bracelet represents one token object.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来要做的是定义我们想要匹配的模式。在这里，我们将`pattern`定义为列表，其中每个括号内的列表项代表一个标记对象。
- en: 'You can read the pattern list in the preceding code snippet as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以按照以下方式阅读前面代码片段中的模式列表：
- en: A token whose lowered text is `good`
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个文本降低后的内容为`good`的标记
- en: A token whose lowered text is `morning`
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个文本降低后的内容为`morning`的标记
- en: A token that is punctuation (that is, the `IS_PUNCT` feature is `True`)
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个标点符号标记（即，`IS_PUNCT`特征为`True`）
- en: Then, we need to introduce this pattern to the `matcher`; this is what the `matcher.add()`
    line does. On line 7, we introduced our pattern to the `matcher` object and named
    this rule `morningGreeting`. Finally, we can do the matching operation on line
    8 by calling `matcher` on the `doc`. After that, we examine the result we get.
    A match result is a list of triplets in the form `(match id, start position, end
    position)`. On the final line, we iterate over the result list and print the result
    match's start position, end position, and text.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要将这个模式引入到`matcher`中；这正是`matcher.add()`行所做的事情。在第7行，我们将我们的模式引入到`matcher`对象中，并将其命名为`morningGreeting`。最后，我们可以在第8行通过调用`matcher`在`doc`上执行匹配操作。之后，我们检查得到的结果。一个匹配结果是形式为`(match
    id, start position, end position)`的三元组列表。在最后一行，我们遍历结果列表并打印匹配结果的起始位置、结束位置和文本。
- en: As you might have noticed, the whitespace between `Good` and `morning` didn't
    matter at all. Indeed, we could have put two whitespaces in between, written down
    `Good morning`, and the result would be identical. Why? Because `Matcher` matches
    the tokens and the token attributes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经注意到的，`Good`和`morning`之间的空白根本无关紧要。实际上，我们可以在它们之间放两个空格，写下`Good morning`，结果将是相同的。为什么？因为`Matcher`匹配标记和标记属性。
- en: 'A pattern always refers to a continuous sequence of token objects, and every
    item in bracelets corresponds to one token object. Let''s go back to the pattern
    in the preceding code snippet:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模式始终指的是一个连续的标记对象序列，并且每个花括号中的项对应一个标记对象。让我们回到前面代码片段中的模式：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We see that the result is always a three-token match.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，结果始终是三个标记的匹配。
- en: 'Can we add more than one pattern? The answer is yes. Let''s see it with an
    example and also see an example of `match_id` as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否添加多个模式？答案是肯定的。让我们通过一个例子来看一下，同时也会看到一个`match_id`的例子如下：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This time we did things a bit differently:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们做了一些不同的处理：
- en: On line 8, we defined a second pattern, again matching three tokens, but this
    time `evening` instead of `morning`.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第8行，我们定义了第二个模式，同样匹配三个标记，但这次是`evening`而不是`morning`。
- en: 'On the next line, we added it to the `matcher`. At this point, `matcher` contains
    2 patterns: `morningGreeting` and `eveningGreeting`.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在下一行，我们将它添加到了`matcher`中。此时，`matcher`包含了2个模式：`morningGreeting`和`eveningGreeting`。
- en: Again, we called the `matcher` on our sentence and examined the result. This
    time the results list has two items, `Good morning`, and `good evening!`, corresponding
    to two different patterns, `morningGreeting` and `eveningGreeting`.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 再次，我们在我们的句子上调用 `matcher` 并检查结果。这次结果列表有两个项目，`Good morning` 和 `good evening!`，对应于两个不同的模式，`morningGreeting`
    和 `eveningGreeting`。
- en: 'In the preceding code example, `pattern1` and `pattern2` differ only by one
    token: `evening/morning`. Instead of writing two patterns, can we say `evening`
    or `morning`? We can do that as well. Here are the attributes that Matcher recognizes:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，`pattern1` 和 `pattern2` 只有一个标记不同：`evening/morning`。我们是否可以说 `evening`
    或 `morning`？我们也可以这样做。以下是 Matcher 识别的属性：
- en: '![Figure 4.1 – Token attributes for Matcher'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.1 – Matcher 的标记属性'
- en: '](img/B16570_4_01.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_4_01.jpg)'
- en: Figure 4.1 – Token attributes for Matcher
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – Matcher 的标记属性
- en: 'Let''s go over the attributes one by one with some examples. We used `LOWER`
    in the preceding examples; it means the *lowercase form of the token text*. `ORTH`
    and `TEXT` are similar to `LOWER`: they mean an exact match of the token text,
    including the case. Here''s an example:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个通过一些示例来回顾这些属性。在前面的例子中，我们使用了 `LOWER`；这意味着标记文本的小写形式。`ORTH` 和 `TEXT` 与 `LOWER`
    类似：它们意味着标记文本的精确匹配，包括大小写。以下是一个示例：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding code will match `BIll`, but not `bill`. `LENGTH` is used for
    specifying the token length. The following code finds all tokens of length `1`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将匹配 `BIll`，但不会匹配 `bill`。`LENGTH` 用于指定标记长度。以下代码查找所有长度为 `1` 的标记：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The next block of token attributes is `IS_ALPHA`, `IS_ASCII`, and `IS_DIGIT`.
    These features are handy for finding number tokens and *ordinary* words (which
    do not include any interesting characters). The following pattern matches a sequence
    of two tokens, a number followed by an ordinary word:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个标记属性块是 `IS_ALPHA`, `IS_ASCII`, 和 `IS_DIGIT`。这些特性对于查找数字标记和 *普通* 单词（不包括任何有趣的字符）很有用。以下模式匹配两个标记的序列，一个数字后面跟一个普通单词：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the preceding code segment, `2 o'clock` didn't match the pattern because
    `o'clock` contains an apostrophe, which is not an alphabetic character (alphabetic
    characters are digits, letters, and the underscore character). `2 apples` matched
    because the token `apples` consists of letters.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码段中，`2 o'clock` 没有匹配到模式，因为 `o'clock` 包含一个撇号，这不是一个字母字符（字母字符是数字、字母和下划线字符）。`2
    apples` 匹配，因为标记 `apples` 由字母组成。
- en: '`IS_LOWER`, `IS_UPPER`, and `IS_TITLE` are useful attributes for recognizing
    the token''s casing. `IS_UPPER` is `True` if the token is all uppercase letters
    and `IS_TITLE` is `True` if the token starts with a capital letter. `IS_LOWER`
    is `True` if the token is all lowercase letters. Imagine we want to find emphasized
    words in a text; one way is to look for the tokens with all uppercase letters.
    The uppercase tokens usually have significant weights in sentiment analysis models.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`IS_LOWER`, `IS_UPPER`, 和 `IS_TITLE` 是用于识别标记大小写的有用属性。如果标记全部为大写字母，则 `IS_UPPER`
    为 `True`；如果标记以大写字母开头，则 `IS_TITLE` 为 `True`。如果标记全部为小写字母，则 `IS_LOWER` 为 `True`。想象一下，我们想要在文本中找到强调的单词；一种方法就是寻找全部为大写字母的标记。大写标记在情感分析模型中通常具有显著的重要性。'
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`IS_PUNCT`, `IS_SPACE`, and `IS_STOP` are usually used in patterns that include
    some helper tokens and correspond to punctuation, space, and `IS_SENT_START` is
    another useful attribute; it matches sentence start tokens. Here''s a pattern
    for sentences that start with *can* and the second word has a capitalized first
    letter:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`IS_PUNCT`, `IS_SPACE`, 和 `IS_STOP` 通常用于包含一些辅助标记的模式中，分别对应于标点符号、空格，而 `IS_SENT_START`
    是另一个有用的属性；它匹配句子开头的标记。以下是一个以 *can* 开头且第二个单词首字母大写的句子模式：'
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here, we did a different thing: we put two attributes into one brace. In this
    example, the first item in `pattern` means that a token that is the first token
    of the sentence and whose lowered text is *can*. We can add as many attributes
    as we like. For instance, `{"IS_SENT_START": False, "IS_TITLE": True, "LOWER":
    "bill"}` is a completely valid attribute dictionary, and it describes a token
    that is capitalized, not the first token of sentence, and has the text `bill`.
    So, it is the set of `Bill` instances that does not appear as the first word of
    a sentence.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们做了不同的事情：我们将两个属性放入一个花括号中。在这个例子中，`pattern` 中的第一个项目表示一个标记是句子的第一个标记，并且其小写文本为
    *can*。我们可以添加任意多的属性。例如，`{"IS_SENT_START": False, "IS_TITLE": True, "LOWER": "bill"}`
    是一个完全有效的属性字典，它描述了一个首字母大写、不是句子第一个标记且文本为 `bill` 的标记。因此，它是那些不作为句子第一个单词出现的 `Bill`
    实例的集合。'
- en: '`LIKE_NUM`, `LIKE_URL`, and `LIKE_EMAIL` are attributes that are related to
    token shape again; remember, we saw them in [*Chapter 3*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)*,
    Linguistic Features*. These attributes match tokens that look like numbers, URLs,
    and emails.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`LIKE_NUM`、`LIKE_URL` 和 `LIKE_EMAIL` 是与标记形状相关的属性；记住，我们在 [*第 3 章*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)*，语言特征*
    中看到了它们。这些属性匹配看起来像数字、URL 和电子邮件的标记。'
- en: Though the preceding code looks short and simple, the shape attributes can be
    lifesavers in NLU applications. Most of the time you need nothing other than clever
    combinations of shape and linguistic attributes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前面的代码看起来简短且简单，但在 NLU 应用中，形状属性可以成为救命稻草。大多数时候，你需要的只是形状和语言属性的巧妙组合。
- en: 'After seeing the shape attributes, let''s see the `POS`, `TAG`, `DEP`, `LEMMA`,
    and `SHAPE` linguistic attributes. You saw these token attributes in the previous
    chapter; now we''ll use them in token matching. The following code snippet spots
    sentences that start with an auxiliary verb:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在看到形状属性之后，让我们看看 `POS`、`TAG`、`DEP`、`LEMMA` 和 `SHAPE` 语言属性。你在上一章中看到了这些标记属性；现在我们将使用它们进行标记匹配。以下代码片段查找以助动词开头的句子：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You may recall from [*Chapter 3*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)*,
    Linguistic Features*, that `MD` is the tag for modal and auxiliary verbs. The
    preceding code snippet is a standard way of finding yes/no question sentences.
    In such cases, we usually look for sentences that start with a modal or an auxiliary
    verb.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得从 [*第 3 章*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)*，语言特征* 中，`MD`
    是情态动词和助动词的标签。前面的代码片段是查找是/否疑问句的标准方法。在这种情况下，我们通常寻找以情态动词或助动词开头的句子。
- en: Pro tip
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Don''t be afraid to work with `TEXT`/`LEMMA` with `POS`/`TAG`. For instance,
    the word *match* is *to go together* when it''s a verb or it can be a *fire starter
    tool* when it''s a noun. In this case, we make the distinction as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 不要害怕与 `TEXT`/`LEMMA` 和 `POS`/`TAG` 一起工作。例如，当 `match` 是动词时，它与 `to go together`
    是一致的，或者当它是名词时，它可以是一个 *fire starter tool*。在这种情况下，我们按照以下方式区分：
- en: '`{"LEMMA": "match", "POS": "VERB"}` and'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`{"LEMMA": "match", "POS": "VERB"}` 以及'
- en: '`{"LEMMA": "match", "POS": "NOUN".`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`{"LEMMA": "match", "POS": "NOUN".`'
- en: Similarly, you can combine other linguistic features with token shape attributes
    to make sure that you extract only the pattern you mean to.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，你可以将其他语言特征与标记形状属性结合起来，以确保你只提取你想要的模式。
- en: We'll see more examples of combining linguistic features with the `Matcher`
    class in the upcoming sections. Now, we'll explore more Matcher features.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将看到更多将语言特征与 `Matcher` 类结合的示例。现在，我们将探索更多匹配器功能。
- en: Extended syntax support
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展语法支持
- en: 'Matcher allows patterns to be more expressive by allowing some operators inside
    the curly brackets. These operators are for extended comparison and look similar
    to Python''s `in`, `not in`, and comparison operators. Here''s the list of the
    operators:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配器允许在花括号内使用一些运算符，从而使模式更加丰富。这些运算符用于扩展比较，类似于 Python 的 `in`、`not in` 和比较运算符。以下是运算符列表：
- en: '![Fig 4.2 – List of rich comparison operators'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![Fig 4.2 – 丰富的比较运算符列表](Fig 4.2 – List of rich comparison operators)'
- en: '](img/B16570_4_02.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_4_02.jpg](img/B16570_4_02.jpg)'
- en: Figure 4.2 – List of rich comparison operators
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 丰富的比较运算符列表
- en: 'In our very first example, we matched `good evening` and `good morning` with
    two different patterns. Now, we can match `good morning`/`evening` with one pattern
    with the help of `IN` as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一个例子中，我们使用两个不同的模式匹配了 `good evening` 和 `good morning`。现在，我们可以通过使用 `IN` 来匹配
    `good morning`/`evening`，使用一个模式如下：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Comparison operators usually go together with the `LENGTH` attribute. Here''s
    an example of finding long tokens:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 比较运算符通常与 `LENGTH` 属性一起使用。以下是一个查找长标记的示例：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: They were fun words to process! Now, we'll move onto another very practical
    feature of Matcher patterns, regex-like operators.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些词处理起来很有趣！现在，我们将继续探讨匹配器模式的另一个非常实用的功能，即类似正则表达式的运算符。
- en: Regex-like operators
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类似正则表达式的运算符
- en: 'At the beginning of the chapter, we pointed out that spaCy''s `Matcher` class
    offers a cleaner and more readable equivalent to regex operations, indeed much
    cleaner and much more readable. The most common regex operations are optional
    match (`?`), match at least once (`+`), and match 0 or more times (`*`). spaCy''s
    Matcher also offers these operators by using the following syntax:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我们指出 spaCy 的 `Matcher` 类提供了对正则表达式操作的更干净、更易读的等效方法，确实更加干净和易读。最常见的正则表达式操作是可选匹配（`?`）、至少匹配一次（`+`）和匹配
    0 或多次（`*`）。spaCy 的 Matcher 也通过以下语法提供这些运算符：
- en: '![Fig 4.3 – OP key description'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![Fig 4.3 – OP key description](Fig 4.3 – OP key description)'
- en: '](img/B16570_4_03.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_4_03.jpg](img/B16570_4_03.jpg)'
- en: Figure 4.3 – OP key description
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – OP 键描述
- en: 'The very first example regex of this chapter was matching Barack Obama''s first
    name, with the middle name being optional. The regex was as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第一个正则表达式示例是匹配巴拉克·奥巴马的名字，中间名是可选的。正则表达式如下：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `?` operator after `Hussein` means the pattern in the brackets is optional,
    hence this regex matches both `Barack Obama` and `Barack Hussein Obama`. We use
    the `?` operator in a Matcher pattern as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`?` 操作符在 `Hussein` 之后意味着括号中的模式是可选的，因此这个正则表达式匹配了 `Barack Obama` 和 `Barack Hussein
    Obama`。我们在匹配器模式中使用 `?` 操作符如下：'
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here, by using the `"OP": "?"` in the second list item, we made this token
    optional. The `matcher` picked `Barack Obama` in the first doc object and `Barack
    Hussein Obama` in the second one as a result.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，通过在第二个列表项中使用 `"OP": "?"`，我们使这个标记可选。`matcher` 选择了第一个文档对象中的 `Barack Obama`
    和第二个文档对象中的 `Barack Hussein Obama`。'
- en: 'We previously pointed that the `+` and `*` operators have the same meaning
    as their regex counterparts. `+` means the token should occur at least once and
    `*` means the token can occur 0 or more times. Let''s see some examples:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前指出，`+` 和 `*` 操作符与它们的正则表达式对应物具有相同的意思。`+` 表示标记应该至少出现一次，而 `*` 表示标记可以出现 0 次或多次。让我们看一些例子：
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here''s what happened:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是发生了什么：
- en: In the pattern, the first token reads as *any one of hello, hi, hallo should
    occur 1 or more times* and the second token is punctuation.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模式中，第一个标记表示 *hello、hi、hallo 应该出现 1 次或多次*，第二个标记是标点符号。
- en: The third `doc` object does not match at all; there's no greeting word.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个 `doc` 对象完全不匹配；没有问候词。
- en: The second `doc` object matches `hello,`.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个 `doc` 对象匹配 `hello,`。
- en: When we come to the results of the first `doc` objects' matches, we see that
    there are not one, but three distinct matches. This is completely normal because
    there are indeed three sequences matching the pattern. If you have a closer look
    at the match results, all of them match the pattern we created, because `hello`,
    `hello hello`, and `hello hello hello` all match the `(hello)+` pattern.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们来看第一个 `doc` 对象匹配的结果时，我们看到不仅有 1 个，而是有 3 个不同的匹配。这是完全正常的，因为确实有三个序列与模式匹配。如果你仔细查看匹配结果，所有这些都与我们创建的模式匹配，因为
    `hello`、`hello hello` 和 `hello hello hello` 都与 `(hello)+` 模式匹配。
- en: 'Let''s do the same pattern with `*` and see what happens this time:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 `*` 做相同的模式，看看这次会发生什么：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the first `doc` object''s matches, there are two extra items: `""` and `?`.
    The `"*"` operator matches `0` or more, so our `(hello)*punct_character` pattern
    grabs `""` and `?`. The same applies to the second and third documents: punctuation
    marks alone without any greeting word are picked. This is not what you want in
    your NLP applications, probably.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个 `doc` 对象的匹配中，有两个额外的项：`""` 和 `?`。`"*"` 操作符匹配 `0` 或更多，所以我们的 `(hello)*punct_character`
    模式抓取了 `""` 和 `?`。同样适用于第二个和第三个文档：单独的标点符号而没有任何问候词被选中。这可能在你的 NLP 应用程序中不是你想要的。
- en: The preceding example is a good example that we should be careful of while creating
    our patterns; sometimes, we get unwanted matches. For this reason, we usually
    consider using `IS_SENT_START` and take care with `"*"` operator.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 上述例子是一个很好的例子，我们在创建我们的模式时应该小心；有时，我们得到不想要的匹配。因此，我们通常考虑使用 `IS_SENT_START` 并注意 `"*"`
    操作符。
- en: 'The spaCy Matcher class also accepts a very special pattern, a **wildcard**
    token pattern. A wildcard token will match any token. We usually use it for the
    words we want to pick independent from their text or attributes or for the words
    we ignore. Let''s see an example:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 匹配器类还接受一个非常特殊的模式，即通配符标记模式。通配符标记将匹配任何标记。我们通常用它来选择独立于其文本或属性或我们忽略的单词。让我们看一个例子：
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here, we wanted to capture the first names in the sentence. We achieved it
    by parsing out token sequences in the form *name is/was/be firstname*. The first
    token pattern, `LOWER:` `"name"`, matches the tokens whose lowered text is `name`.
    The second token pattern, `LEMMA: "be"`, matches the `is`, `was`, and `be` tokens.
    The third token is the wildcard token, `{}`, which means *any* token. We pick
    up any token that comes after *name is/was/be* with this pattern.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们想要捕获句子中的名字。我们通过解析形式为 *name is/was/be firstname* 的标记序列来实现。第一个标记模式，`LOWER:`
    `"name"`，匹配文本小写为 `name` 的标记。第二个标记模式，`LEMMA: "be"`，匹配 `is`、`was` 和 `be` 标记。第三个标记是通配符标记
    `{}`，它表示 *任何* 标记。我们使用这个模式拾取任何在 *name is/was/be* 之后出现的标记。'
- en: 'We also use a wildcard token when we want to ignore a token. Let''s make an
    example together:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要忽略一个标记时，我们也会使用通配符标记。让我们一起做一个例子：
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It's just the opposite of the previous example. Here, we wanted to pick up *forward
    email* sequences, and we allowed that one token to come between `forward` and
    `email`. Here, the semantically important part is the forwarding an email action;
    whose email is it doesn't matter much.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这与前面的例子正好相反。在这里，我们想要提取 *forward email* 序列，并且我们允许一个标记在 `forward` 和 `email` 之间。在这里，语义上重要的部分是转发电子邮件的动作；它是谁的电子邮件并不那么重要。
- en: We have mentioned regex quite a lot in this chapter so far, so now it's time
    to see how spaCy's Matcher class makes use of regex syntax.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经在本章中多次提到了正则表达式，因此现在是时候看看 spaCy 的 Matcher 类如何使用正则表达式语法了。
- en: Regex support
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则表达式支持
- en: 'When we match individual tokens, usually we want to allow some variations,
    such as common typos, UK/US English character differences, and so on. Regex is
    very handy for this task and spaCy Matcher offers full support for token-level
    regex matching. Let''s explore how we can use regex for our applications:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们匹配单个标记时，通常我们想要允许一些变化，例如常见的拼写错误、UK/US 英语字符差异等等。正则表达式非常适合这项任务，而 spaCy Matcher
    提供了对标记级正则表达式匹配的全面支持。让我们探索我们如何使用正则表达式为我们自己的应用服务：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, our second token pattern is `[Tt]ravell?ed`, which means the token can
    be capitalized or not. Also, there's an optional `l` after the first `l`. Allowing
    twin vowels and *ise/ize* alteration is a standard way of dealing with British
    and American English variations.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的第二个标记模式是 `[Tt]ravell?ed`，这意味着标记可以是首字母大写也可以不是。此外，在第一个 `l` 后面有一个可选的 `l`。允许双元音和
    *ise/ize* 变化是处理英国和美式英语变体的标准方式。
- en: Another way of using regex is using it not only with text, but also with `POS`
    tags. What does the following code segment do?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正则表达式的一种另一种方法是不仅与文本一起使用，还与 `POS` 标签一起使用。以下代码段做了什么？
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We have extracted all the finite verbs (you can think of a finite verb as a
    non-modal verb). How did we do it? Our token pattern includes the regex `^V`,
    which means all fine-grained POS tags that start with `V`: `VB`, `VGD`, `VBG`,
    `VBN`, `VBP`, and `VBZ`. Then we extracted tokens with verbal POS tags.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提取了所有的不定式动词（你可以将不定式动词视为非情态动词）。我们是如何做到这一点的？我们的标记模式包括正则表达式 `^V`，这意味着所有以 `V`
    开头的细粒度 POS 标签：`VB`、`VGD`、`VBG`、`VBN`、`VBP` 和 `VBZ`。然后我们提取了具有动词 POS 标签的标记。
- en: Looks tricky! Occasionally we use some tricks in NLU applications; while going
    through the examples of this book, you'll pick them up too. We encourage you to
    go over our examples and then try some example sentences of yours.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来很复杂！在 NLU 应用中，我们偶尔会使用一些技巧；在阅读本书的示例时，你也会学会它们。我们鼓励你回顾我们的示例，然后尝试一些你自己的示例句子。
- en: Matcher online demo
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 匹配器在线演示
- en: 'In the whole matching business, we occasionally see the match results visually.
    Regex offers *regex101* ([https://regex101.com/](https://regex101.com/)), an online
    tool for checking if your regex pattern is working correctly (surprises with regex
    always happen). The following figure shows an example pattern and checking it
    against a text:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个匹配过程中，我们偶尔会看到匹配结果的可视化。正则表达式提供了 *regex101* ([https://regex101.com/](https://regex101.com/))，这是一个在线工具，用于检查你的正则表达式模式是否正确工作（正则表达式总是会有惊喜）。以下图显示了示例模式和对其进行的文本检查：
- en: '![Fig 4.4 – An example regex match and pattern explanations'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.4 – 一个正则表达式匹配示例和模式解释'
- en: '](img/B16570_4_04.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_4_04.jpg)'
- en: Figure 4.4 – An example regex match and pattern explanations
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – 一个正则表达式匹配示例和模式解释
- en: The explanations on the right side are quite detailed and illuminating. This
    is a tool used not only by NLP learners/beginners, but also professionals (regex
    can be quite difficult to read sometimes).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的解释相当详细且具有启发性。这是一个不仅被 NLP 学习者/初学者，而且被专业人士使用的工具（正则表达式有时可能很难阅读）。
- en: spaCy Matcher offers a similar tool on its online demo page ([https://explosion.ai/demos/matcher](https://explosion.ai/demos/matcher)).
    We can create patterns and test them against the text we want, interactively.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 匹配器在其在线演示页面([https://explosion.ai/demos/matcher](https://explosion.ai/demos/matcher))上提供了一个类似的工具。我们可以创建模式并交互式地测试它们与想要测试的文本。
- en: 'In the following screenshot, we can see a match example. On the right side
    we can select the attributes, values, and operators (such as +, *, !, and ?).
    After making this selection, the demo outputs the corresponding pattern string
    on the right side, below the checkboxes. On the left side, we first choose the
    spaCy language model we want (in this example, English core small), then see the
    results:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下屏幕截图中，我们可以看到一个匹配示例。在右侧，我们可以选择属性、值和运算符（如 +、*、! 和 ?）。在做出此选择后，演示在复选框下方右侧输出相应的模式字符串。在左侧，我们首先选择我们想要的
    spaCy 语言模型（在这个例子中，是英语核心小型），然后查看结果：
- en: '![Fig 4.5 – spaCy Matcher online demo'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.5 – spaCy 匹配器在线演示'
- en: '](img/B16570_4_05.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_4_05.jpg)'
- en: Figure 4.5 – spaCy Matcher online demo
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – spaCy 匹配器在线演示
- en: Just like regex101, spaCy's Matcher demo helps you to see why your pattern matched
    or didn't match.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 regex101 一样，spaCy 的 Matcher 演示可以帮助你看到为什么你的模式匹配或未匹配。
- en: PhraseMatcher
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PhraseMatcher
- en: While processing financial, medical, or legal text, often we have long lists
    and dictionaries and we want to scan the text against our lists. As we saw in
    the previous section, Matcher patterns are quite handcrafted; we coded each token
    individually. If you have a long list of phrases, Matcher is not very handy. It's
    not possible to code all the terms one by one.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理金融、医疗或法律文本时，我们经常有长长的列表和字典，并希望将文本与我们的列表进行扫描。正如我们在前节中看到的，Matcher 模式相当是手工制作的；我们单独为每个标记编写代码。如果你有一长串短语列表，Matcher
    就不太方便了。不可能一个接一个地编写所有术语。
- en: 'spaCy offers a solution for comparing text against long dictionaries – the
    `PhraseMatcher` class. The `PhraseMatcher` class helps us match long dictionaries.
    Let''s get started with an example:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 提供了一种将文本与长字典进行比较的解决方案 – `PhraseMatcher` 类。`PhraseMatcher` 类帮助我们匹配长字典。让我们从一个例子开始：
- en: '[PRE20]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here''s what we did:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所做的是：
- en: First, we imported `spacy`, then we imported the `PhraseMatcher` class.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们导入了 `spacy`，然后导入了 `PhraseMatcher` 类。
- en: After the imports, we created a `Language` object, `nlp`, and initialized a
    `PhraseMatcher` object, `matcher`, with its vocabulary.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在导入之后，我们创建了一个 `Language` 对象，`nlp`，并初始化了一个 `PhraseMatcher` 对象，`matcher`，并为其提供了词汇表。
- en: The next two lines are where we created the pattern list.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来的两行是我们创建模式列表的地方。
- en: On line 6, we called `nlp.make_doc()` on the terms one by one to create the
    patterns.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第 6 行，我们对每个术语调用了 `nlp.make_doc()` 来创建模式。
- en: '`make_doc()` creates a Doc from every term, and it''s quite efficient in terms
    of processing because instead of the whole pipeline, it only calls the `Tokenizer`.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`make_doc()` 从每个术语创建一个 Doc，在处理方面非常高效，因为它只调用 `Tokenizer` 而不是整个管道。'
- en: 'The rest of the code is similar to what we did with Matcher: we iterated over
    the resulting spans.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其余的代码与我们在 Matcher 中的操作类似：我们遍历了生成的跨度。
- en: 'This way, we match the pattern by their exact text values. What if we want
    to match them with other attributes? Here''s an example of matching by the `LOWER`
    attribute:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们通过它们的精确文本值来匹配模式。如果我们想通过其他属性来匹配它们呢？这里是一个通过 `LOWER` 属性匹配的例子：
- en: '[PRE21]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: On line 1, while creating a `PhraseMatcher` instance, we passed an additional
    argument, `attr=LOWER`. This way, the `PhraseMatcher` used the `token.lower` attribute
    during the match. Notice that the terms are uppercase and the matches are lowercase.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 1 行，在创建 `PhraseMatcher` 实例时，我们传递了一个额外的参数，`attr=LOWER`。这样，`PhraseMatcher`
    在匹配时使用了 `token.lower` 属性。注意，术语是大写的，而匹配结果是小写的。
- en: 'Another possible usage of PhraseMatcher is matching the `SHAPE` attribute.
    This matching strategy can be used on system logs, where IP numbers, dates, and
    other numerical values occur a lot. The good thing here is that you do not need
    to worry how the numbers are tokenized, you just leave it to `PhraseMatcher`.
    Let''s see an example:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: PhraseMatcher 的另一个可能用途是匹配 `SHAPE` 属性。这种匹配策略可以用于系统日志，其中 IP 地址、日期和其他数值经常出现。这里的好事是，你不需要担心数值是如何分词的，你只需将其留给
    `PhraseMatcher`。让我们看一个例子：
- en: '[PRE22]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: That's it! We matched the tokens and phrases successfully; what's left is named
    entities. Named entity extraction is an essential component of any NLP system
    and most of the pipelines you'll design will include an **named entity recognition**
    (**NER**) component. The next section is devoted to rule-based named entity extraction.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们成功匹配了标记和短语；剩下的是命名实体。命名实体提取是任何 NLP 系统的一个基本组成部分，你将设计的多数管道都将包括一个 **命名实体识别**（**NER**）组件。下一节将专门介绍基于规则的命名实体提取。
- en: EntityRuler
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: EntityRuler
- en: 'While covering Matcher, we saw that we can extract named entities with Matcher
    by using the `ENT_TYPE` attribute. We recall from the previous chapter that `ENT_TYPE`
    is a linguistic attribute that refers to the entity type of the token, such as
    person, place, or organization. Let''s see an example:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍 Matcher 时，我们看到了可以通过使用 `ENT_TYPE` 属性使用 Matcher 提取命名实体。我们回忆起前一章中提到的 `ENT_TYPE`
    是一个语言属性，它指的是标记的实体类型，例如人、地点或组织。让我们看一个例子：
- en: '[PRE23]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Again, we created a `Matcher` object called `matcher` and called it on the
    `Doc` object, `doc`. The result is two tokens, `Bill` and `Gates`; Matcher always
    matches at the token level. We got `Bill` and `Gates`, instead of the full entity,
    `Bill Gates`. If you want to get the full entity rather than the individual tokens,
    you can do this:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们创建了一个名为 `matcher` 的 `Matcher` 对象，并在 `Doc` 对象 `doc` 上调用它。结果是两个标记，`Bill`
    和 `Gates`；匹配器总是在标记级别进行匹配。我们得到了 `Bill` 和 `Gates`，而不是完整的实体 `Bill Gates`。如果你想要得到完整的实体而不是单个标记，你可以这样做：
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Usually, we match two or more entities together, or with other linguistic attributes
    to extract information. Here''s an example of how we can understand the action
    in the sentence and which person in the sentence committed this action:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们会将两个或多个实体组合在一起，或者与其他语言属性一起提取信息。以下是一个例子，说明我们如何理解句子中的动作以及句子中哪个人物执行了这个动作：
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We noticed that the Matcher returns two matches here; usually, we loop through
    the results and pick the longest match.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到匹配器在这里返回了两个匹配项；通常，我们会遍历结果并选择最长的匹配项。
- en: In the preceding examples, we matched the entities that the spaCy statistical
    model already extracted. What if we have domain-specific entities that we want
    to match? For instance, our dataset consists of wiki pages about ancient Greek
    philosophers. The philosopher names are naturally in Greek and don't follow English
    statistical patterns; it's expected that a tagger trained on English text would
    fail to extract the entity name occasionally. In these situations, we'd like spaCy
    to tell our entities and combine them with the statistical rules.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们匹配了 spaCy 统计模型已经提取的实体。如果我们有特定领域的实体想要匹配怎么办？例如，我们的数据集由关于古希腊哲学家的维基页面组成。哲学家的名字自然是希腊语，并不遵循英语统计模式；预计在用英语文本训练的标记器中，偶尔会失败地提取实体名称。在这些情况下，我们希望
    spaCy 识别我们的实体并将它们与统计规则结合起来。
- en: spaCy's `EntityRuler` is the component that allows us to add rules on top of
    the statistical model and creates an even more powerful **NER** model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 的 `EntityRuler` 是一个组件，它允许我们在统计模型之上添加规则，从而创建一个更强大的 **NER** 模型。
- en: '`EntityRuler` is not a matcher, it''s a pipeline component that we can add
    to our pipeline via `nlp.add_pipe`. When it finds a match, the match is appended
    to `doc.ents` and `ent_type` will be the label we pass in the pattern. Let''s
    see it in action:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`EntityRuler` 不是一个匹配器，它是一个可以通过 `nlp.add_pipe` 添加到我们管道中的管道组件。当它找到匹配项时，匹配项会被追加到
    `doc.ents` 中，而 `ent_type` 将会是我们在模式中传递的标签。让我们看看它是如何工作的：'
- en: '[PRE26]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: That's it, really easy, yet powerful! We added our own entity with just a couple
    of lines.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，真的非常简单，但也很强大！我们只用了几行代码就添加了自己的实体。
- en: The `Matcher` class and `EntityRuler` are exciting and powerful features of
    the spaCy library, as we saw from the examples. Now, we move onto an exclusive
    section of quick and very handy recipes.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`Matcher` 类和 `EntityRuler` 是 spaCy 库中令人兴奋且强大的功能，正如我们从示例中看到的。现在，我们将进入一个专门的部分，介绍一些快速且非常实用的技巧。'
- en: Combining spaCy models and matchers
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合 spaCy 模型和匹配器
- en: In this section, we'll go through some recipes that will guide you through the
    entity extraction types you'll encounter in your NLP career. All the examples
    are ready-to-use and real-world recipes. Let's start with number-formatted entities.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些技巧，这些技巧将指导你了解你在 NLP 职业生涯中会遇到的各种实体提取类型。所有示例都是现成的、真实世界的技巧。让我们从数字格式化的实体开始。
- en: Extracting IBAN and account numbers
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取 IBAN 和账户号码
- en: IBAN and account numbers are two important entity types that occur in finance
    and banking frequently. We'll learn how to parse them out.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: IBAN 和账户号码是金融和银行业中经常出现的两种重要实体类型。我们将学习如何解析它们。
- en: 'An IBAN is an international number format for bank account numbers. It has
    the format of a two-digit country code followed by numbers. Here are some IBANs
    from different countries:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: IBAN 是一种国际银行账户号码格式。它由两位数字的国家代码后跟数字组成。以下是一些来自不同国家的 IBAN：
- en: '![](img/B16570_4_06.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16570_4_06.jpg)'
- en: 'Figure 4.6 – IBAN formats from different countries (source: Wikipedia)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – 来自不同国家的 IBAN 格式（来源：维基百科）
- en: 'How can we create a pattern for an IBAN? Obviously, in all cases, we start
    with two capital letters, followed by two digits. Then any number of digits can
    follow. We can express the country code and the next two digits as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何为 IBAN 创建一个模式？显然，在所有情况下，我们从一个大写字母开始，后面跟着两个数字。然后可以跟任意数量的数字。我们可以将国家代码和接下来的两个数字表示如下：
- en: '[PRE27]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here, `XX` corresponds to two capital letters and `dd` is two digits. Then
    `XXdd` pattern matches the first block of the IBAN perfectly. How about the rest
    of the digit blocks? For the rest of the blocks, we need to match a block of 1-4
    digits. The regex `\d{1,4}` means a token consisting of 1 to 4 digits. This pattern
    will match a digit block:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`XX`对应于两个大写字母，`dd`是两个数字。然后`XXdd`模式完美地匹配IBAN的第一个块。那么其他数字块呢？对于其他块，我们需要匹配一个1到4位的数字块。正则表达式`\d{1,4}`表示由1到4位数字组成的标记。这个模式将匹配一个数字块：
- en: '[PRE28]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We have a number of these blocks, so the pattern to match the digit blocks
    of an IBAN is as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有几个这样的块，所以匹配IBAN数字块的模式如下：
- en: '[PRE29]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, we combine the first block with the rest of the blocks. Let''s see the
    code and the matches:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将第一个块与其他块结合起来。让我们看看代码和匹配结果：
- en: '[PRE30]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You can always follow a similar strategy when parsing numeric entities: first,
    divide the entity into some meaningful parts/blocks, then try to determine the
    shape or the length of the individual blocks.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在解析数字实体时，你可以始终遵循类似的策略：首先，将实体分成一些有意义的部分/块，然后尝试确定各个块的形式或长度。
- en: 'We successfully parsed IBANs, now we can parse the account numbers. Parsing
    the account numbers is a bit trickier; account numbers are just plain numbers
    and don''t have a special shape to help us differentiate them from usual numbers.
    What do we do, then? We can make a context lookup in this case; we can look around
    the number token and see if we can find *account number* or *account num* around
    the number token. This pattern should do the trick:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功解析了IBAN，现在我们可以解析账户号码。解析账户号码有点棘手；账户号码只是普通的数字，没有特殊的形式帮助我们区分它们和普通数字。那么我们该怎么办呢？在这种情况下，我们可以进行上下文查找；我们可以查看数字标记周围，看看我们是否可以在数字标记周围找到*account
    number*或*account num*。这个模式应该可以解决问题：
- en: '[PRE31]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We used a wildcard here: `{}` means any token. We allowed one token to go in
    between *number* and *account number*; this can be *is*, *was*, and so on. Let''s
    see the code:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用了一个通配符：`{}`表示任何标记。我们允许一个标记在*number*和*account number*之间；这可以是*is*，*was*等等。让我们看看代码：
- en: '[PRE32]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: If you want, you can include a possessive pronoun such as *my*, *your*, or *his*
    in the match, depending on the application's needs.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想，你可以在匹配中包含一个所有格代词，如*my*，*your*或*his*，具体取决于应用程序的需求。
- en: That's it for banking numbers. Now we'll extract another type of common numeric
    entity, phone numbers.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 银行号码就到这里。现在我们将提取另一种常见的数字实体，电话号码。
- en: Extracting phone numbers
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取电话号码
- en: Phone numbers can have very different formats depending on the country, and
    matching phone numbers is often a tricky business. The best strategy here is to
    be specific about the country phone number format you want to parse. If there
    are several countries, you can add corresponding individual patterns to the matcher.
    If you have too many countries, then you can relax some conditions and go for
    a more general pattern (we'll see how to do that).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 电话号码的格式可能因国家而异，匹配电话号码通常是一项棘手的工作。这里最好的策略是明确你想要解析的国家电话号码格式。如果有几个国家，你可以在匹配器中添加相应的单个模式。如果你有太多的国家，那么你可以放宽一些条件，采用更通用的模式（我们将看到如何做到这一点）。
- en: 'Let''s start with the US phone number format. A US number is written as *(541)
    754-3010* domestically or *+1 (541) 754-3010* internationally. We can form our
    pattern with an optional *+1*, then a three-digit area code, then two blocks of
    numbers separated with an optional *-*.  Here is the pattern:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从美国的电话号码格式开始。美国国内电话号码写作*(541) 754-3010*，国际电话号码写作*+1 (541) 754-3010*。我们可以用可选的*+1*来形成我们的模式，然后是一个三位数的区号，然后是两个用可选的*-*分隔的数字块。  以下是模式：
- en: '[PRE33]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let''s see an example:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个例子：
- en: '[PRE34]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: How about we make the pattern more general to apply to other countries as well?
    In this case, we can start with a 1 to 3-digit country code, followed by some
    digit blocks. It will match a broader set of numbers, so it's better to be careful
    not to match other numeric entities in your text.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否可以将模式做得更通用，以便也适用于其他国家呢？在这种情况下，我们可以从一个1到3位的国家代码开始，后面跟着一些数字块。这将匹配更广泛的数字集合，因此最好小心不要匹配你文本中的其他数字实体。
- en: We'll move onto textual entities from numeric entities. Now we'll process social
    media text and extract different types of entities that can occur in social media
    text.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将转到文本实体，从数字实体开始。现在我们将处理社交媒体文本，并提取社交媒体文本中可能出现的不同类型的实体。
- en: Extracting mentions
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取提及
- en: 'Imagine analyzing a dataset of social media posts about companies and products
    and your task is to find out which companies are mentioned in which ways. The
    dataset will contain this sort of sentence:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下分析一个关于公司和产品的社交媒体帖子数据集，你的任务是找出哪些公司在以何种方式被提及。数据集将包含这种类型的句子：
- en: '[PRE35]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'What we''re looking for is most probably patterns of the *BusinessName is/was/be
    adverb* adjective* form. The following pattern would work:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要找的可能是最常见的模式，即 *BusinessName is/was/be adverb* 形式的模式。以下模式将有效：
- en: '[PRE36]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Here, we look for an organization type entity, followed by a *is/was/be*, then
    optional adverbs, and finally an adjective.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们寻找一个组织类型实体，然后是 *is/was/be*，然后是可选的副词，最后是一个形容词。
- en: 'What if you want to extract a specific business, let''s say the company *ACME*?
    All you have to do is replace the first token with the specific company name:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想提取一个特定的企业，比如说公司 *ACME*，你只需要将第一个标记替换为特定的公司名称：
- en: '[PRE37]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: That's it, easy peasy! After extracting the social media mentions, the next
    thing to do is to extract the hashtags and the emojis.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，简单易行！在提取社交媒体提及之后，接下来要做的事情是提取哈希标签和表情符号。
- en: Hashtag and emoji extraction
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 哈希标签和表情符号提取
- en: 'Processing social media text is a hot topic and has some challenges. Social
    media text includes two sorts of unusual token types: hashtags and emojis. Both
    token types have a huge impact on the text meaning. The hashtag refers to the
    subject/object of the sentence, usually, and emojis can assign the sentiment of
    the sentence by themselves.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 处理社交媒体文本是一个热门话题，并且有一些挑战。社交媒体文本包括两种不寻常的标记类型：哈希标签和表情符号。这两种标记类型都对文本意义有巨大影响。哈希标签通常指代句子的主题/宾语，而表情符号可以自己赋予句子的情感。
- en: 'A hashtag consists of a `#` character at the beginning, then followed by a
    word of `ASCII` characters, with no inter-word spaces. Some examples are *#MySpace*,
    *#MondayMotivation* and so on. The spaCy tokenizer tokenizes these words into
    two tokens:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希标签由一个位于开头的 `#` 字符组成，然后是 `ASCII` 字符的单词，没有单词间的空格。一些例子包括 *#MySpace*、*#MondayMotivation*
    等等。spaCy 分词器将这些单词分词为两个标记：
- en: '[PRE38]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'As a result, our pattern needs to match two tokens, the `#` character and the
    rest. The following pattern will match a hashtag easily:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的模式需要匹配两个标记，即 `#` 字符和其余部分。以下模式可以轻松匹配哈希标签：
- en: '[PRE39]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The following code extracts a hashtag:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码提取了一个哈希标签：
- en: '[PRE40]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'How about an emoji? An emoji is usually coded with lists according to their
    sentiment value, such as positive, negative, happy, sad, and so on. Here, we''ll
    separate emojis into two classes, positive and negative. The following code spots
    the selected emoji in the text:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 那表情符号呢？表情符号通常根据它们的情感值编码为列表，例如积极、消极、快乐、悲伤等等。在这里，我们将表情符号分为两类，积极和消极。以下代码在文本中找到了选定的表情符号：
- en: '[PRE41]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Et voilà, the emoji ![](img/B16570_4_emoj15.png) is extracted happily! We'll
    make use of emojis in sentiment analysis chapter as well.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '哈哈，表情符号 ![表情](img/B16570_4_emoj15.png) 欢快地被提取了！我们将在情感分析章节中也使用表情符号。 '
- en: Now, let's extract some entities. We'll start with the common procedure of expanding
    named entities.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们提取一些实体。我们将从扩展命名实体的常见程序开始。
- en: Expanding named entities
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展命名实体
- en: Often, we would like to expand a named entity's span to the left or to the right.
    Imagine you want to extract `PERSON` type named entities with titles so that you
    can deduce the gender or profession easily. spaCy's `NER` class already extracts
    person names, so how about the titles?
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 经常，我们希望将命名实体的范围向左或向右扩展。想象一下，你想要提取带有头衔的 `PERSON` 类型命名实体，这样你可以轻松地推断性别或职业。spaCy
    的 `NER` 类已经提取了人名，那么头衔呢？
- en: '[PRE42]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'As you see, the word `Ms.` is not included in the named entity because it''s
    not a part of the person''s name. A quick solution is to make a new entity type
    called `TITLE`:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，单词 `Ms.` 没有包含在命名实体中，因为它不是人名的组成部分。一个快速的解决方案是创建一个新的实体类型，称为 `TITLE`：
- en: '[PRE43]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This is a quick and very handy recipe. You'll come across parsing titles a lot
    if you process wiki text or financial text.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个快速且非常实用的方法。如果你处理维基文本或财经文本，你会经常遇到解析标题的情况。
- en: In our next and final example, we'll combine POS attributes, dependency labels,
    and named entities.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的下一个也是最后一个例子中，我们将结合 POS 属性、依存标签和命名实体。
- en: Combining linguistic features and named entities
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合语言特征和命名实体
- en: While charging meaning to a sentence, we evaluate word semantics by considering
    the contexts they occur in. Matching the words individually usually does not help
    us understand the full meaning. In most NLU tasks we have to combine linguistic
    features.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在为句子赋予意义时，我们通过考虑它们出现的上下文来评估词义。单独匹配单词通常不能帮助我们理解完整的意义。在大多数自然语言理解任务中，我们必须结合语言特征。
- en: Imagine you're parsing professional biographies and make a work history of the
    subjects. You want to extract person names, the cities they have lived in, and
    the city they're currently working in.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你正在解析专业传记，并为主题制作一份工作历史。你希望提取人名、他们曾经居住的城市以及他们目前工作的城市。
- en: 'Obviously we''ll look for the word *live*; however, the POS tags hold the key
    here: whether it''s the present tense or the past tense. In order to determine
    which city/place, we''ll use syntactic information that is given by the dependency
    labels.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们会寻找单词 *live*；然而，这里的关键在于 POS 标签：它是现在时还是过去时。为了确定哪个城市/地点，我们将使用由依存标签提供的句法信息。
- en: 'Let''s examine the following example:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查以下示例：
- en: '[PRE44]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Here is a visual representation of the preceding example:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是前述示例的视觉表示：
- en: '![Fig 4.7 – Example parse, the entity ”Einstein” being subject of the sentence'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.7 – 示例解析，实体 ”Einstein” 是句子的主题](img/B16570_4_07.jpg)'
- en: '](img/B16570_4_07.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.7 – 示例解析，实体 "Einstein" 是句子的主题](img/B16570_4_07.jpg)'
- en: Figure 4.7 – Example parse, the entity "Einstein" being subject of the sentence
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – 示例解析，句子中的实体 "Einstein" 是主题
- en: 'Here, `lived` is the main verb of the sentence, hence the root of the sentence.
    `Einstein` is the subject of the sentence, at the same time the person entity
    who `lived`. As we can see, the `Einstein` token''s head is `lived`. There''s
    also a place entity in the sentence, `Zurich`. If we follow the arcs from `lived`,
    we reach `Zurich` via a prepositional attachment. Finally, to determine the verb''s
    tense, we can examine the POS tag. Let''s see it in the following code:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`lived` 是句子的主要动词，因此是句子的根。`Einstein` 是句子的主语，同时也是居住的人实体。正如我们所见，`Einstein`
    标记的头部是 `lived`。句子中还有一个地点实体，`Zurich`。如果我们跟随从 `lived` 出发的弧线，我们通过介词附加到达 `Zurich`。最后，为了确定动词的时态，我们可以检查
    POS 标签。让我们在下面的代码中看看：
- en: '[PRE45]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Here, we combined POS tag information, dependency labels (hence syntactic information
    of the sentence), and named entities. It may not be easy for you to grasp it at
    first sight, but you'll get there by practicing.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们结合了 POS 标签信息、依存标签（因此句子的句法信息）和命名实体。一开始你可能觉得这不容易理解，但通过练习你会掌握的。
- en: Summary
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter introduced you to a very handy and powerful feature of spaCy, spaCy's
    matcher classes. You learned how to do rule-based matching with linguistic and
    token-level features. You learned about the `Matcher` class, spaCy's rule-based
    matcher. We explored the `Matcher` class by using it with different token features,
    such as shape, lemma, text, and entity type.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向你介绍了 spaCy 的一个非常实用且强大的功能——spaCy 的匹配器类。你学习了如何使用语言和标记级特征进行基于规则的匹配。你了解了 `Matcher`
    类，spaCy 的基于规则的匹配器。我们通过使用不同的标记特征（如形状、词元、文本和实体类型）来探索 `Matcher` 类。
- en: Then, you learned about `EntityRuler`, another lifesaving class that you can
    achieve a lot with. You learned how to extract named entities with the `EntityRuler`
    class.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你学习了关于 `EntityRuler` 的内容，这是另一个非常有用的类，你可以用它做很多事情。你学习了如何使用 `EntityRuler` 类提取命名实体。
- en: Finally, we put together what you've learned in this chapter and your previous
    knowledge and combined linguistic features with rule-based matching with several
    examples. You learned how to extract patterns, entities of specific formats, and
    entities specific to your domain.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将本章所学内容与你的先前知识相结合，通过几个示例将语言特征与基于规则的匹配相结合。你学习了如何提取模式、特定格式的实体以及特定领域的实体。
- en: With this chapter, you completed the linguistic features. In the next chapter,
    we'll dive into the world of statistical semantics via a very important concept
    – **word vectors**. You'll discover the power of statistics in representing words,
    phrases, and sentences. Let's discover the world of semantics together!
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章，你完成了语言特征的介绍。在下一章，我们将通过一个非常重要的概念——**词向量**——深入统计语义的世界。你将发现统计在表示单词、短语和句子方面的力量。让我们共同探索语义的世界吧！
