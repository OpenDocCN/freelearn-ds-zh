- en: 'Chapter 4: Rule-Based Matching'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Rule-based information extraction** is indispensable for any NLP pipeline.
    Certain types of entities, such as times, dates, and telephone numbers have distinct
    formats that can be recognized by a set of rules, without having to train statistical
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to quickly extract information from the
    text by matching patterns and phrases. You will use `Matcher` objects. You will
    continue with fine-graining statistical models with rule-based matching to lift
    statistical models to better accuracies.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will know a vital part of information extraction.
    You will be able to extract entities of specific formats, as well as entities
    specific to your domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Token-based matching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PhraseMatcher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EntityRuler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining spaCy models and matchers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Token-based matching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've explored the sophisticated linguistic concepts that require statistical
    models and their usages with spaCy. Some NLU tasks can be solved in tricky ways
    without the help of any statistical model. One of those ways is **regex**, which
    we use to match a predefined set of patterns to our text.
  prefs: []
  type: TYPE_NORMAL
- en: A regex (a regular expression) is a sequence of characters that specifies a
    search pattern. A regex describes a set of strings that follows the specified
    pattern. A regex can include letters, digits, and characters with special meanings,
    such as *?*, *.*, and ***. Python's built-in library provides great support to
    define and match regular expressions. There's another Python 3 library called
    regex that aims wants to replace **re** in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Readers who are actively developing NLP applications with Python have definitely
    come across regex code and, even better, have written regex themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'What does a regex look like, then? The following regex matches the following
    strings:'
  prefs: []
  type: TYPE_NORMAL
- en: Barack Obama
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barack Obama
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barack Hussein Obama
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This pattern can be read as: the string `Barack` can be followed optionally
    by the string `Hussein` (the `?` character in regex means optional, that is, `0`
    or `1` occurrence) and should be followed by the string `Obama`. The inter-word
    spaces can be a single space character, a tab, or any other whitespace character
    (`\s` matches all sorts of whitespace characters, including the newline character).'
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s not very readable, even for such a short and uncomplicated pattern, is
    it? That is the downside of regex, it is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Difficult to read
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difficult to debug
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error prone with space, punctuation, and number characters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For these reasons, many software engineers don''t like to work with regex in
    their production code. spaCy provides a very clean, readable, production-level,
    and maintainable alternative: the `Matcher` class. The `Matcher` class can match
    our predefined rules to the sequence of tokens in `Doc` and `Span` objects; moreover,
    the rules can refer to the token or its linguistic attributes (more on this subject
    later in this section).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with a basic example of how to call the `Matcher` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It looks complicated, but don''t be intimidated, we''ll go over the lines one
    by one:'
  prefs: []
  type: TYPE_NORMAL
- en: We imported `spacy` in the first line; this should be familiar.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the second line, we imported the `Matcher` class in order to use it in the
    rest of the code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the next lines, we created the `nlp` object as usual and created the `doc`
    object with our example sentence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, pay attention: a `matcher` object needs to be initialized with a `Vocabulary`
    object, so on line 5 we initialize our `matcher` object with the language model''s
    vocabulary (this is the usual way to do it).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What comes next is to define the pattern we want to match. Here, we define *pattern*
    as a list where every list item enclosed in a bracelet represents one token object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can read the pattern list in the preceding code snippet as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A token whose lowered text is `good`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A token whose lowered text is `morning`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A token that is punctuation (that is, the `IS_PUNCT` feature is `True`)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we need to introduce this pattern to the `matcher`; this is what the `matcher.add()`
    line does. On line 7, we introduced our pattern to the `matcher` object and named
    this rule `morningGreeting`. Finally, we can do the matching operation on line
    8 by calling `matcher` on the `doc`. After that, we examine the result we get.
    A match result is a list of triplets in the form `(match id, start position, end
    position)`. On the final line, we iterate over the result list and print the result
    match's start position, end position, and text.
  prefs: []
  type: TYPE_NORMAL
- en: As you might have noticed, the whitespace between `Good` and `morning` didn't
    matter at all. Indeed, we could have put two whitespaces in between, written down
    `Good morning`, and the result would be identical. Why? Because `Matcher` matches
    the tokens and the token attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'A pattern always refers to a continuous sequence of token objects, and every
    item in bracelets corresponds to one token object. Let''s go back to the pattern
    in the preceding code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We see that the result is always a three-token match.
  prefs: []
  type: TYPE_NORMAL
- en: 'Can we add more than one pattern? The answer is yes. Let''s see it with an
    example and also see an example of `match_id` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This time we did things a bit differently:'
  prefs: []
  type: TYPE_NORMAL
- en: On line 8, we defined a second pattern, again matching three tokens, but this
    time `evening` instead of `morning`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the next line, we added it to the `matcher`. At this point, `matcher` contains
    2 patterns: `morningGreeting` and `eveningGreeting`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, we called the `matcher` on our sentence and examined the result. This
    time the results list has two items, `Good morning`, and `good evening!`, corresponding
    to two different patterns, `morningGreeting` and `eveningGreeting`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the preceding code example, `pattern1` and `pattern2` differ only by one
    token: `evening/morning`. Instead of writing two patterns, can we say `evening`
    or `morning`? We can do that as well. Here are the attributes that Matcher recognizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Token attributes for Matcher'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_4_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – Token attributes for Matcher
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go over the attributes one by one with some examples. We used `LOWER`
    in the preceding examples; it means the *lowercase form of the token text*. `ORTH`
    and `TEXT` are similar to `LOWER`: they mean an exact match of the token text,
    including the case. Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will match `BIll`, but not `bill`. `LENGTH` is used for
    specifying the token length. The following code finds all tokens of length `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The next block of token attributes is `IS_ALPHA`, `IS_ASCII`, and `IS_DIGIT`.
    These features are handy for finding number tokens and *ordinary* words (which
    do not include any interesting characters). The following pattern matches a sequence
    of two tokens, a number followed by an ordinary word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code segment, `2 o'clock` didn't match the pattern because
    `o'clock` contains an apostrophe, which is not an alphabetic character (alphabetic
    characters are digits, letters, and the underscore character). `2 apples` matched
    because the token `apples` consists of letters.
  prefs: []
  type: TYPE_NORMAL
- en: '`IS_LOWER`, `IS_UPPER`, and `IS_TITLE` are useful attributes for recognizing
    the token''s casing. `IS_UPPER` is `True` if the token is all uppercase letters
    and `IS_TITLE` is `True` if the token starts with a capital letter. `IS_LOWER`
    is `True` if the token is all lowercase letters. Imagine we want to find emphasized
    words in a text; one way is to look for the tokens with all uppercase letters.
    The uppercase tokens usually have significant weights in sentiment analysis models.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`IS_PUNCT`, `IS_SPACE`, and `IS_STOP` are usually used in patterns that include
    some helper tokens and correspond to punctuation, space, and `IS_SENT_START` is
    another useful attribute; it matches sentence start tokens. Here''s a pattern
    for sentences that start with *can* and the second word has a capitalized first
    letter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we did a different thing: we put two attributes into one brace. In this
    example, the first item in `pattern` means that a token that is the first token
    of the sentence and whose lowered text is *can*. We can add as many attributes
    as we like. For instance, `{"IS_SENT_START": False, "IS_TITLE": True, "LOWER":
    "bill"}` is a completely valid attribute dictionary, and it describes a token
    that is capitalized, not the first token of sentence, and has the text `bill`.
    So, it is the set of `Bill` instances that does not appear as the first word of
    a sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: '`LIKE_NUM`, `LIKE_URL`, and `LIKE_EMAIL` are attributes that are related to
    token shape again; remember, we saw them in [*Chapter 3*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)*,
    Linguistic Features*. These attributes match tokens that look like numbers, URLs,
    and emails.'
  prefs: []
  type: TYPE_NORMAL
- en: Though the preceding code looks short and simple, the shape attributes can be
    lifesavers in NLU applications. Most of the time you need nothing other than clever
    combinations of shape and linguistic attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'After seeing the shape attributes, let''s see the `POS`, `TAG`, `DEP`, `LEMMA`,
    and `SHAPE` linguistic attributes. You saw these token attributes in the previous
    chapter; now we''ll use them in token matching. The following code snippet spots
    sentences that start with an auxiliary verb:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You may recall from [*Chapter 3*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)*,
    Linguistic Features*, that `MD` is the tag for modal and auxiliary verbs. The
    preceding code snippet is a standard way of finding yes/no question sentences.
    In such cases, we usually look for sentences that start with a modal or an auxiliary
    verb.
  prefs: []
  type: TYPE_NORMAL
- en: Pro tip
  prefs: []
  type: TYPE_NORMAL
- en: 'Don''t be afraid to work with `TEXT`/`LEMMA` with `POS`/`TAG`. For instance,
    the word *match* is *to go together* when it''s a verb or it can be a *fire starter
    tool* when it''s a noun. In this case, we make the distinction as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`{"LEMMA": "match", "POS": "VERB"}` and'
  prefs: []
  type: TYPE_NORMAL
- en: '`{"LEMMA": "match", "POS": "NOUN".`'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, you can combine other linguistic features with token shape attributes
    to make sure that you extract only the pattern you mean to.
  prefs: []
  type: TYPE_NORMAL
- en: We'll see more examples of combining linguistic features with the `Matcher`
    class in the upcoming sections. Now, we'll explore more Matcher features.
  prefs: []
  type: TYPE_NORMAL
- en: Extended syntax support
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Matcher allows patterns to be more expressive by allowing some operators inside
    the curly brackets. These operators are for extended comparison and look similar
    to Python''s `in`, `not in`, and comparison operators. Here''s the list of the
    operators:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 4.2 – List of rich comparison operators'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_4_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – List of rich comparison operators
  prefs: []
  type: TYPE_NORMAL
- en: 'In our very first example, we matched `good evening` and `good morning` with
    two different patterns. Now, we can match `good morning`/`evening` with one pattern
    with the help of `IN` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Comparison operators usually go together with the `LENGTH` attribute. Here''s
    an example of finding long tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: They were fun words to process! Now, we'll move onto another very practical
    feature of Matcher patterns, regex-like operators.
  prefs: []
  type: TYPE_NORMAL
- en: Regex-like operators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the beginning of the chapter, we pointed out that spaCy''s `Matcher` class
    offers a cleaner and more readable equivalent to regex operations, indeed much
    cleaner and much more readable. The most common regex operations are optional
    match (`?`), match at least once (`+`), and match 0 or more times (`*`). spaCy''s
    Matcher also offers these operators by using the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 4.3 – OP key description'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_4_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – OP key description
  prefs: []
  type: TYPE_NORMAL
- en: 'The very first example regex of this chapter was matching Barack Obama''s first
    name, with the middle name being optional. The regex was as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `?` operator after `Hussein` means the pattern in the brackets is optional,
    hence this regex matches both `Barack Obama` and `Barack Hussein Obama`. We use
    the `?` operator in a Matcher pattern as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, by using the `"OP": "?"` in the second list item, we made this token
    optional. The `matcher` picked `Barack Obama` in the first doc object and `Barack
    Hussein Obama` in the second one as a result.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We previously pointed that the `+` and `*` operators have the same meaning
    as their regex counterparts. `+` means the token should occur at least once and
    `*` means the token can occur 0 or more times. Let''s see some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what happened:'
  prefs: []
  type: TYPE_NORMAL
- en: In the pattern, the first token reads as *any one of hello, hi, hallo should
    occur 1 or more times* and the second token is punctuation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third `doc` object does not match at all; there's no greeting word.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second `doc` object matches `hello,`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we come to the results of the first `doc` objects' matches, we see that
    there are not one, but three distinct matches. This is completely normal because
    there are indeed three sequences matching the pattern. If you have a closer look
    at the match results, all of them match the pattern we created, because `hello`,
    `hello hello`, and `hello hello hello` all match the `(hello)+` pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do the same pattern with `*` and see what happens this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first `doc` object''s matches, there are two extra items: `""` and `?`.
    The `"*"` operator matches `0` or more, so our `(hello)*punct_character` pattern
    grabs `""` and `?`. The same applies to the second and third documents: punctuation
    marks alone without any greeting word are picked. This is not what you want in
    your NLP applications, probably.'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding example is a good example that we should be careful of while creating
    our patterns; sometimes, we get unwanted matches. For this reason, we usually
    consider using `IS_SENT_START` and take care with `"*"` operator.
  prefs: []
  type: TYPE_NORMAL
- en: 'The spaCy Matcher class also accepts a very special pattern, a **wildcard**
    token pattern. A wildcard token will match any token. We usually use it for the
    words we want to pick independent from their text or attributes or for the words
    we ignore. Let''s see an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we wanted to capture the first names in the sentence. We achieved it
    by parsing out token sequences in the form *name is/was/be firstname*. The first
    token pattern, `LOWER:` `"name"`, matches the tokens whose lowered text is `name`.
    The second token pattern, `LEMMA: "be"`, matches the `is`, `was`, and `be` tokens.
    The third token is the wildcard token, `{}`, which means *any* token. We pick
    up any token that comes after *name is/was/be* with this pattern.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also use a wildcard token when we want to ignore a token. Let''s make an
    example together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: It's just the opposite of the previous example. Here, we wanted to pick up *forward
    email* sequences, and we allowed that one token to come between `forward` and
    `email`. Here, the semantically important part is the forwarding an email action;
    whose email is it doesn't matter much.
  prefs: []
  type: TYPE_NORMAL
- en: We have mentioned regex quite a lot in this chapter so far, so now it's time
    to see how spaCy's Matcher class makes use of regex syntax.
  prefs: []
  type: TYPE_NORMAL
- en: Regex support
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we match individual tokens, usually we want to allow some variations,
    such as common typos, UK/US English character differences, and so on. Regex is
    very handy for this task and spaCy Matcher offers full support for token-level
    regex matching. Let''s explore how we can use regex for our applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Here, our second token pattern is `[Tt]ravell?ed`, which means the token can
    be capitalized or not. Also, there's an optional `l` after the first `l`. Allowing
    twin vowels and *ise/ize* alteration is a standard way of dealing with British
    and American English variations.
  prefs: []
  type: TYPE_NORMAL
- en: Another way of using regex is using it not only with text, but also with `POS`
    tags. What does the following code segment do?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We have extracted all the finite verbs (you can think of a finite verb as a
    non-modal verb). How did we do it? Our token pattern includes the regex `^V`,
    which means all fine-grained POS tags that start with `V`: `VB`, `VGD`, `VBG`,
    `VBN`, `VBP`, and `VBZ`. Then we extracted tokens with verbal POS tags.'
  prefs: []
  type: TYPE_NORMAL
- en: Looks tricky! Occasionally we use some tricks in NLU applications; while going
    through the examples of this book, you'll pick them up too. We encourage you to
    go over our examples and then try some example sentences of yours.
  prefs: []
  type: TYPE_NORMAL
- en: Matcher online demo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the whole matching business, we occasionally see the match results visually.
    Regex offers *regex101* ([https://regex101.com/](https://regex101.com/)), an online
    tool for checking if your regex pattern is working correctly (surprises with regex
    always happen). The following figure shows an example pattern and checking it
    against a text:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 4.4 – An example regex match and pattern explanations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_4_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 – An example regex match and pattern explanations
  prefs: []
  type: TYPE_NORMAL
- en: The explanations on the right side are quite detailed and illuminating. This
    is a tool used not only by NLP learners/beginners, but also professionals (regex
    can be quite difficult to read sometimes).
  prefs: []
  type: TYPE_NORMAL
- en: spaCy Matcher offers a similar tool on its online demo page ([https://explosion.ai/demos/matcher](https://explosion.ai/demos/matcher)).
    We can create patterns and test them against the text we want, interactively.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we can see a match example. On the right side
    we can select the attributes, values, and operators (such as +, *, !, and ?).
    After making this selection, the demo outputs the corresponding pattern string
    on the right side, below the checkboxes. On the left side, we first choose the
    spaCy language model we want (in this example, English core small), then see the
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 4.5 – spaCy Matcher online demo'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_4_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 – spaCy Matcher online demo
  prefs: []
  type: TYPE_NORMAL
- en: Just like regex101, spaCy's Matcher demo helps you to see why your pattern matched
    or didn't match.
  prefs: []
  type: TYPE_NORMAL
- en: PhraseMatcher
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While processing financial, medical, or legal text, often we have long lists
    and dictionaries and we want to scan the text against our lists. As we saw in
    the previous section, Matcher patterns are quite handcrafted; we coded each token
    individually. If you have a long list of phrases, Matcher is not very handy. It's
    not possible to code all the terms one by one.
  prefs: []
  type: TYPE_NORMAL
- en: 'spaCy offers a solution for comparing text against long dictionaries – the
    `PhraseMatcher` class. The `PhraseMatcher` class helps us match long dictionaries.
    Let''s get started with an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what we did:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we imported `spacy`, then we imported the `PhraseMatcher` class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the imports, we created a `Language` object, `nlp`, and initialized a
    `PhraseMatcher` object, `matcher`, with its vocabulary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next two lines are where we created the pattern list.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On line 6, we called `nlp.make_doc()` on the terms one by one to create the
    patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`make_doc()` creates a Doc from every term, and it''s quite efficient in terms
    of processing because instead of the whole pipeline, it only calls the `Tokenizer`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The rest of the code is similar to what we did with Matcher: we iterated over
    the resulting spans.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This way, we match the pattern by their exact text values. What if we want
    to match them with other attributes? Here''s an example of matching by the `LOWER`
    attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: On line 1, while creating a `PhraseMatcher` instance, we passed an additional
    argument, `attr=LOWER`. This way, the `PhraseMatcher` used the `token.lower` attribute
    during the match. Notice that the terms are uppercase and the matches are lowercase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another possible usage of PhraseMatcher is matching the `SHAPE` attribute.
    This matching strategy can be used on system logs, where IP numbers, dates, and
    other numerical values occur a lot. The good thing here is that you do not need
    to worry how the numbers are tokenized, you just leave it to `PhraseMatcher`.
    Let''s see an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: That's it! We matched the tokens and phrases successfully; what's left is named
    entities. Named entity extraction is an essential component of any NLP system
    and most of the pipelines you'll design will include an **named entity recognition**
    (**NER**) component. The next section is devoted to rule-based named entity extraction.
  prefs: []
  type: TYPE_NORMAL
- en: EntityRuler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While covering Matcher, we saw that we can extract named entities with Matcher
    by using the `ENT_TYPE` attribute. We recall from the previous chapter that `ENT_TYPE`
    is a linguistic attribute that refers to the entity type of the token, such as
    person, place, or organization. Let''s see an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we created a `Matcher` object called `matcher` and called it on the
    `Doc` object, `doc`. The result is two tokens, `Bill` and `Gates`; Matcher always
    matches at the token level. We got `Bill` and `Gates`, instead of the full entity,
    `Bill Gates`. If you want to get the full entity rather than the individual tokens,
    you can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Usually, we match two or more entities together, or with other linguistic attributes
    to extract information. Here''s an example of how we can understand the action
    in the sentence and which person in the sentence committed this action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We noticed that the Matcher returns two matches here; usually, we loop through
    the results and pick the longest match.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding examples, we matched the entities that the spaCy statistical
    model already extracted. What if we have domain-specific entities that we want
    to match? For instance, our dataset consists of wiki pages about ancient Greek
    philosophers. The philosopher names are naturally in Greek and don't follow English
    statistical patterns; it's expected that a tagger trained on English text would
    fail to extract the entity name occasionally. In these situations, we'd like spaCy
    to tell our entities and combine them with the statistical rules.
  prefs: []
  type: TYPE_NORMAL
- en: spaCy's `EntityRuler` is the component that allows us to add rules on top of
    the statistical model and creates an even more powerful **NER** model.
  prefs: []
  type: TYPE_NORMAL
- en: '`EntityRuler` is not a matcher, it''s a pipeline component that we can add
    to our pipeline via `nlp.add_pipe`. When it finds a match, the match is appended
    to `doc.ents` and `ent_type` will be the label we pass in the pattern. Let''s
    see it in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: That's it, really easy, yet powerful! We added our own entity with just a couple
    of lines.
  prefs: []
  type: TYPE_NORMAL
- en: The `Matcher` class and `EntityRuler` are exciting and powerful features of
    the spaCy library, as we saw from the examples. Now, we move onto an exclusive
    section of quick and very handy recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Combining spaCy models and matchers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll go through some recipes that will guide you through the
    entity extraction types you'll encounter in your NLP career. All the examples
    are ready-to-use and real-world recipes. Let's start with number-formatted entities.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting IBAN and account numbers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IBAN and account numbers are two important entity types that occur in finance
    and banking frequently. We'll learn how to parse them out.
  prefs: []
  type: TYPE_NORMAL
- en: 'An IBAN is an international number format for bank account numbers. It has
    the format of a two-digit country code followed by numbers. Here are some IBANs
    from different countries:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16570_4_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6 – IBAN formats from different countries (source: Wikipedia)'
  prefs: []
  type: TYPE_NORMAL
- en: 'How can we create a pattern for an IBAN? Obviously, in all cases, we start
    with two capital letters, followed by two digits. Then any number of digits can
    follow. We can express the country code and the next two digits as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `XX` corresponds to two capital letters and `dd` is two digits. Then
    `XXdd` pattern matches the first block of the IBAN perfectly. How about the rest
    of the digit blocks? For the rest of the blocks, we need to match a block of 1-4
    digits. The regex `\d{1,4}` means a token consisting of 1 to 4 digits. This pattern
    will match a digit block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We have a number of these blocks, so the pattern to match the digit blocks
    of an IBAN is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we combine the first block with the rest of the blocks. Let''s see the
    code and the matches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'You can always follow a similar strategy when parsing numeric entities: first,
    divide the entity into some meaningful parts/blocks, then try to determine the
    shape or the length of the individual blocks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We successfully parsed IBANs, now we can parse the account numbers. Parsing
    the account numbers is a bit trickier; account numbers are just plain numbers
    and don''t have a special shape to help us differentiate them from usual numbers.
    What do we do, then? We can make a context lookup in this case; we can look around
    the number token and see if we can find *account number* or *account num* around
    the number token. This pattern should do the trick:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We used a wildcard here: `{}` means any token. We allowed one token to go in
    between *number* and *account number*; this can be *is*, *was*, and so on. Let''s
    see the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: If you want, you can include a possessive pronoun such as *my*, *your*, or *his*
    in the match, depending on the application's needs.
  prefs: []
  type: TYPE_NORMAL
- en: That's it for banking numbers. Now we'll extract another type of common numeric
    entity, phone numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting phone numbers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Phone numbers can have very different formats depending on the country, and
    matching phone numbers is often a tricky business. The best strategy here is to
    be specific about the country phone number format you want to parse. If there
    are several countries, you can add corresponding individual patterns to the matcher.
    If you have too many countries, then you can relax some conditions and go for
    a more general pattern (we'll see how to do that).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the US phone number format. A US number is written as *(541)
    754-3010* domestically or *+1 (541) 754-3010* internationally. We can form our
    pattern with an optional *+1*, then a three-digit area code, then two blocks of
    numbers separated with an optional *-*.  Here is the pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: How about we make the pattern more general to apply to other countries as well?
    In this case, we can start with a 1 to 3-digit country code, followed by some
    digit blocks. It will match a broader set of numbers, so it's better to be careful
    not to match other numeric entities in your text.
  prefs: []
  type: TYPE_NORMAL
- en: We'll move onto textual entities from numeric entities. Now we'll process social
    media text and extract different types of entities that can occur in social media
    text.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting mentions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Imagine analyzing a dataset of social media posts about companies and products
    and your task is to find out which companies are mentioned in which ways. The
    dataset will contain this sort of sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'What we''re looking for is most probably patterns of the *BusinessName is/was/be
    adverb* adjective* form. The following pattern would work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Here, we look for an organization type entity, followed by a *is/was/be*, then
    optional adverbs, and finally an adjective.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if you want to extract a specific business, let''s say the company *ACME*?
    All you have to do is replace the first token with the specific company name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: That's it, easy peasy! After extracting the social media mentions, the next
    thing to do is to extract the hashtags and the emojis.
  prefs: []
  type: TYPE_NORMAL
- en: Hashtag and emoji extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Processing social media text is a hot topic and has some challenges. Social
    media text includes two sorts of unusual token types: hashtags and emojis. Both
    token types have a huge impact on the text meaning. The hashtag refers to the
    subject/object of the sentence, usually, and emojis can assign the sentiment of
    the sentence by themselves.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A hashtag consists of a `#` character at the beginning, then followed by a
    word of `ASCII` characters, with no inter-word spaces. Some examples are *#MySpace*,
    *#MondayMotivation* and so on. The spaCy tokenizer tokenizes these words into
    two tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, our pattern needs to match two tokens, the `#` character and the
    rest. The following pattern will match a hashtag easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code extracts a hashtag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'How about an emoji? An emoji is usually coded with lists according to their
    sentiment value, such as positive, negative, happy, sad, and so on. Here, we''ll
    separate emojis into two classes, positive and negative. The following code spots
    the selected emoji in the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Et voilà, the emoji ![](img/B16570_4_emoj15.png) is extracted happily! We'll
    make use of emojis in sentiment analysis chapter as well.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's extract some entities. We'll start with the common procedure of expanding
    named entities.
  prefs: []
  type: TYPE_NORMAL
- en: Expanding named entities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often, we would like to expand a named entity's span to the left or to the right.
    Imagine you want to extract `PERSON` type named entities with titles so that you
    can deduce the gender or profession easily. spaCy's `NER` class already extracts
    person names, so how about the titles?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'As you see, the word `Ms.` is not included in the named entity because it''s
    not a part of the person''s name. A quick solution is to make a new entity type
    called `TITLE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This is a quick and very handy recipe. You'll come across parsing titles a lot
    if you process wiki text or financial text.
  prefs: []
  type: TYPE_NORMAL
- en: In our next and final example, we'll combine POS attributes, dependency labels,
    and named entities.
  prefs: []
  type: TYPE_NORMAL
- en: Combining linguistic features and named entities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While charging meaning to a sentence, we evaluate word semantics by considering
    the contexts they occur in. Matching the words individually usually does not help
    us understand the full meaning. In most NLU tasks we have to combine linguistic
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you're parsing professional biographies and make a work history of the
    subjects. You want to extract person names, the cities they have lived in, and
    the city they're currently working in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously we''ll look for the word *live*; however, the POS tags hold the key
    here: whether it''s the present tense or the past tense. In order to determine
    which city/place, we''ll use syntactic information that is given by the dependency
    labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s examine the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a visual representation of the preceding example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 4.7 – Example parse, the entity ”Einstein” being subject of the sentence'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_4_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 – Example parse, the entity "Einstein" being subject of the sentence
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, `lived` is the main verb of the sentence, hence the root of the sentence.
    `Einstein` is the subject of the sentence, at the same time the person entity
    who `lived`. As we can see, the `Einstein` token''s head is `lived`. There''s
    also a place entity in the sentence, `Zurich`. If we follow the arcs from `lived`,
    we reach `Zurich` via a prepositional attachment. Finally, to determine the verb''s
    tense, we can examine the POS tag. Let''s see it in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Here, we combined POS tag information, dependency labels (hence syntactic information
    of the sentence), and named entities. It may not be easy for you to grasp it at
    first sight, but you'll get there by practicing.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced you to a very handy and powerful feature of spaCy, spaCy's
    matcher classes. You learned how to do rule-based matching with linguistic and
    token-level features. You learned about the `Matcher` class, spaCy's rule-based
    matcher. We explored the `Matcher` class by using it with different token features,
    such as shape, lemma, text, and entity type.
  prefs: []
  type: TYPE_NORMAL
- en: Then, you learned about `EntityRuler`, another lifesaving class that you can
    achieve a lot with. You learned how to extract named entities with the `EntityRuler`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we put together what you've learned in this chapter and your previous
    knowledge and combined linguistic features with rule-based matching with several
    examples. You learned how to extract patterns, entities of specific formats, and
    entities specific to your domain.
  prefs: []
  type: TYPE_NORMAL
- en: With this chapter, you completed the linguistic features. In the next chapter,
    we'll dive into the world of statistical semantics via a very important concept
    – **word vectors**. You'll discover the power of statistics in representing words,
    phrases, and sentences. Let's discover the world of semantics together!
  prefs: []
  type: TYPE_NORMAL
