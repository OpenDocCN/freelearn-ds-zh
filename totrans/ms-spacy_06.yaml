- en: 'Chapter 4: Rule-Based Matching'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章：基于规则的匹配
- en: '**Rule-based information extraction** is indispensable for any NLP pipeline.
    Certain types of entities, such as times, dates, and telephone numbers have distinct
    formats that can be recognized by a set of rules, without having to train statistical
    models.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于规则的实体提取**对于任何 NLP 管道都是必不可少的。某些类型的实体，如时间、日期和电话号码，具有独特的格式，可以通过一组规则识别，而无需训练统计模型。'
- en: In this chapter, you will learn how to quickly extract information from the
    text by matching patterns and phrases. You will use `Matcher` objects. You will
    continue with fine-graining statistical models with rule-based matching to lift
    statistical models to better accuracies.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何通过匹配模式和短语快速从文本中提取信息。你将使用 `Matcher` 对象。你将继续使用基于规则的匹配来细化统计模型，以提高准确性。
- en: By the end of this chapter, you will know a vital part of information extraction.
    You will be able to extract entities of specific formats, as well as entities
    specific to your domain.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解信息提取的关键部分。你将能够提取特定格式的实体，以及特定领域的实体。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下主要主题：
- en: Token-based matching
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于标记的匹配
- en: PhraseMatcher
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短语匹配器
- en: EntityRuler
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实体规则器
- en: Combining spaCy models and matchers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合 spaCy 模型和匹配器
- en: Token-based matching
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于标记的匹配
- en: So far, we've explored the sophisticated linguistic concepts that require statistical
    models and their usages with spaCy. Some NLU tasks can be solved in tricky ways
    without the help of any statistical model. One of those ways is **regex**, which
    we use to match a predefined set of patterns to our text.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了需要统计模型和其用法的复杂语言概念，以及使用 spaCy 的应用。某些 NLU 任务可以在没有统计模型帮助的情况下以巧妙的方式解决。其中一种方式是
    **正则表达式**，我们用它来将预定义的模式与我们的文本进行匹配。
- en: A regex (a regular expression) is a sequence of characters that specifies a
    search pattern. A regex describes a set of strings that follows the specified
    pattern. A regex can include letters, digits, and characters with special meanings,
    such as *?*, *.*, and ***. Python's built-in library provides great support to
    define and match regular expressions. There's another Python 3 library called
    regex that aims wants to replace **re** in the future.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式（正则表达式）是一系列字符，它指定了一个搜索模式。正则表达式描述了一组遵循指定模式的字符串。正则表达式可以包括字母、数字以及具有特殊意义的字符，例如
    *?*, *.*, 和 ***。Python 的内置库提供了强大的支持来定义和匹配正则表达式。还有一个名为 regex 的 Python 3 库，其目标是未来取代
    **re**。
- en: Readers who are actively developing NLP applications with Python have definitely
    come across regex code and, even better, have written regex themselves.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正在积极使用 Python 开发 NLP 应用的读者肯定遇到过正则表达式代码，甚至更好地，他们自己编写过正则表达式。
- en: 'What does a regex look like, then? The following regex matches the following
    strings:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，正则表达式看起来是什么样子的呢？以下正则表达式匹配以下字符串：
- en: Barack Obama
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 巴拉克·奥巴马
- en: Barack Obama
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 巴拉克·奥巴马
- en: Barack Hussein Obama
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 巴拉克·侯赛因·奥巴马
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This pattern can be read as: the string `Barack` can be followed optionally
    by the string `Hussein` (the `?` character in regex means optional, that is, `0`
    or `1` occurrence) and should be followed by the string `Obama`. The inter-word
    spaces can be a single space character, a tab, or any other whitespace character
    (`\s` matches all sorts of whitespace characters, including the newline character).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模式可以读作：字符串 `Barack` 可以可选地后跟字符串 `Hussein`（正则表达式中的 `?` 字符表示可选的，即 `0` 或 `1` 次出现）并且应该后跟字符串
    `Obama`。单词间的空格可以是一个空格字符、一个制表符或任何其他空白字符（`\s` 匹配所有类型的空白字符，包括换行符）。
- en: 'It''s not very readable, even for such a short and uncomplicated pattern, is
    it? That is the downside of regex, it is the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对于如此简短且简单的模式，它也不太易读，对吧？这就是正则表达式的缺点，它是以下：
- en: Difficult to read
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 难以阅读
- en: Difficult to debug
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 难以调试
- en: Error prone with space, punctuation, and number characters
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容易出错，尤其是在空格、标点符号和数字字符方面
- en: 'For these reasons, many software engineers don''t like to work with regex in
    their production code. spaCy provides a very clean, readable, production-level,
    and maintainable alternative: the `Matcher` class. The `Matcher` class can match
    our predefined rules to the sequence of tokens in `Doc` and `Span` objects; moreover,
    the rules can refer to the token or its linguistic attributes (more on this subject
    later in this section).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with a basic example of how to call the `Matcher` class:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It looks complicated, but don''t be intimidated, we''ll go over the lines one
    by one:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: We imported `spacy` in the first line; this should be familiar.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the second line, we imported the `Matcher` class in order to use it in the
    rest of the code.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the next lines, we created the `nlp` object as usual and created the `doc`
    object with our example sentence.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, pay attention: a `matcher` object needs to be initialized with a `Vocabulary`
    object, so on line 5 we initialize our `matcher` object with the language model''s
    vocabulary (this is the usual way to do it).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What comes next is to define the pattern we want to match. Here, we define *pattern*
    as a list where every list item enclosed in a bracelet represents one token object.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can read the pattern list in the preceding code snippet as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: A token whose lowered text is `good`
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A token whose lowered text is `morning`
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A token that is punctuation (that is, the `IS_PUNCT` feature is `True`)
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we need to introduce this pattern to the `matcher`; this is what the `matcher.add()`
    line does. On line 7, we introduced our pattern to the `matcher` object and named
    this rule `morningGreeting`. Finally, we can do the matching operation on line
    8 by calling `matcher` on the `doc`. After that, we examine the result we get.
    A match result is a list of triplets in the form `(match id, start position, end
    position)`. On the final line, we iterate over the result list and print the result
    match's start position, end position, and text.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: As you might have noticed, the whitespace between `Good` and `morning` didn't
    matter at all. Indeed, we could have put two whitespaces in between, written down
    `Good morning`, and the result would be identical. Why? Because `Matcher` matches
    the tokens and the token attributes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'A pattern always refers to a continuous sequence of token objects, and every
    item in bracelets corresponds to one token object. Let''s go back to the pattern
    in the preceding code snippet:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We see that the result is always a three-token match.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'Can we add more than one pattern? The answer is yes. Let''s see it with an
    example and also see an example of `match_id` as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This time we did things a bit differently:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: On line 8, we defined a second pattern, again matching three tokens, but this
    time `evening` instead of `morning`.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the next line, we added it to the `matcher`. At this point, `matcher` contains
    2 patterns: `morningGreeting` and `eveningGreeting`.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, we called the `matcher` on our sentence and examined the result. This
    time the results list has two items, `Good morning`, and `good evening!`, corresponding
    to two different patterns, `morningGreeting` and `eveningGreeting`.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the preceding code example, `pattern1` and `pattern2` differ only by one
    token: `evening/morning`. Instead of writing two patterns, can we say `evening`
    or `morning`? We can do that as well. Here are the attributes that Matcher recognizes:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Token attributes for Matcher'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_4_01.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – Token attributes for Matcher
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go over the attributes one by one with some examples. We used `LOWER`
    in the preceding examples; it means the *lowercase form of the token text*. `ORTH`
    and `TEXT` are similar to `LOWER`: they mean an exact match of the token text,
    including the case. Here''s an example:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding code will match `BIll`, but not `bill`. `LENGTH` is used for
    specifying the token length. The following code finds all tokens of length `1`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The next block of token attributes is `IS_ALPHA`, `IS_ASCII`, and `IS_DIGIT`.
    These features are handy for finding number tokens and *ordinary* words (which
    do not include any interesting characters). The following pattern matches a sequence
    of two tokens, a number followed by an ordinary word:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the preceding code segment, `2 o'clock` didn't match the pattern because
    `o'clock` contains an apostrophe, which is not an alphabetic character (alphabetic
    characters are digits, letters, and the underscore character). `2 apples` matched
    because the token `apples` consists of letters.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '`IS_LOWER`, `IS_UPPER`, and `IS_TITLE` are useful attributes for recognizing
    the token''s casing. `IS_UPPER` is `True` if the token is all uppercase letters
    and `IS_TITLE` is `True` if the token starts with a capital letter. `IS_LOWER`
    is `True` if the token is all lowercase letters. Imagine we want to find emphasized
    words in a text; one way is to look for the tokens with all uppercase letters.
    The uppercase tokens usually have significant weights in sentiment analysis models.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`IS_PUNCT`, `IS_SPACE`, and `IS_STOP` are usually used in patterns that include
    some helper tokens and correspond to punctuation, space, and `IS_SENT_START` is
    another useful attribute; it matches sentence start tokens. Here''s a pattern
    for sentences that start with *can* and the second word has a capitalized first
    letter:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here, we did a different thing: we put two attributes into one brace. In this
    example, the first item in `pattern` means that a token that is the first token
    of the sentence and whose lowered text is *can*. We can add as many attributes
    as we like. For instance, `{"IS_SENT_START": False, "IS_TITLE": True, "LOWER":
    "bill"}` is a completely valid attribute dictionary, and it describes a token
    that is capitalized, not the first token of sentence, and has the text `bill`.
    So, it is the set of `Bill` instances that does not appear as the first word of
    a sentence.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '`LIKE_NUM`, `LIKE_URL`, and `LIKE_EMAIL` are attributes that are related to
    token shape again; remember, we saw them in [*Chapter 3*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)*,
    Linguistic Features*. These attributes match tokens that look like numbers, URLs,
    and emails.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Though the preceding code looks short and simple, the shape attributes can be
    lifesavers in NLU applications. Most of the time you need nothing other than clever
    combinations of shape and linguistic attributes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'After seeing the shape attributes, let''s see the `POS`, `TAG`, `DEP`, `LEMMA`,
    and `SHAPE` linguistic attributes. You saw these token attributes in the previous
    chapter; now we''ll use them in token matching. The following code snippet spots
    sentences that start with an auxiliary verb:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You may recall from [*Chapter 3*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)*,
    Linguistic Features*, that `MD` is the tag for modal and auxiliary verbs. The
    preceding code snippet is a standard way of finding yes/no question sentences.
    In such cases, we usually look for sentences that start with a modal or an auxiliary
    verb.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Pro tip
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'Don''t be afraid to work with `TEXT`/`LEMMA` with `POS`/`TAG`. For instance,
    the word *match* is *to go together* when it''s a verb or it can be a *fire starter
    tool* when it''s a noun. In this case, we make the distinction as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '`{"LEMMA": "match", "POS": "VERB"}` and'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '`{"LEMMA": "match", "POS": "NOUN".`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, you can combine other linguistic features with token shape attributes
    to make sure that you extract only the pattern you mean to.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: We'll see more examples of combining linguistic features with the `Matcher`
    class in the upcoming sections. Now, we'll explore more Matcher features.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Extended syntax support
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Matcher allows patterns to be more expressive by allowing some operators inside
    the curly brackets. These operators are for extended comparison and look similar
    to Python''s `in`, `not in`, and comparison operators. Here''s the list of the
    operators:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 4.2 – List of rich comparison operators'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_4_02.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – List of rich comparison operators
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'In our very first example, we matched `good evening` and `good morning` with
    two different patterns. Now, we can match `good morning`/`evening` with one pattern
    with the help of `IN` as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Comparison operators usually go together with the `LENGTH` attribute. Here''s
    an example of finding long tokens:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: They were fun words to process! Now, we'll move onto another very practical
    feature of Matcher patterns, regex-like operators.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Regex-like operators
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the beginning of the chapter, we pointed out that spaCy''s `Matcher` class
    offers a cleaner and more readable equivalent to regex operations, indeed much
    cleaner and much more readable. The most common regex operations are optional
    match (`?`), match at least once (`+`), and match 0 or more times (`*`). spaCy''s
    Matcher also offers these operators by using the following syntax:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 4.3 – OP key description'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_4_03.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – OP key description
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – OP 键描述
- en: 'The very first example regex of this chapter was matching Barack Obama''s first
    name, with the middle name being optional. The regex was as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第一个正则表达式示例是匹配巴拉克·奥巴马的名字，中间名是可选的。正则表达式如下：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `?` operator after `Hussein` means the pattern in the brackets is optional,
    hence this regex matches both `Barack Obama` and `Barack Hussein Obama`. We use
    the `?` operator in a Matcher pattern as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`?` 操作符在 `Hussein` 之后意味着括号中的模式是可选的，因此这个正则表达式匹配了 `Barack Obama` 和 `Barack Hussein
    Obama`。我们在匹配器模式中使用 `?` 操作符如下：'
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here, by using the `"OP": "?"` in the second list item, we made this token
    optional. The `matcher` picked `Barack Obama` in the first doc object and `Barack
    Hussein Obama` in the second one as a result.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，通过在第二个列表项中使用 `"OP": "?"`，我们使这个标记可选。`matcher` 选择了第一个文档对象中的 `Barack Obama`
    和第二个文档对象中的 `Barack Hussein Obama`。'
- en: 'We previously pointed that the `+` and `*` operators have the same meaning
    as their regex counterparts. `+` means the token should occur at least once and
    `*` means the token can occur 0 or more times. Let''s see some examples:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前指出，`+` 和 `*` 操作符与它们的正则表达式对应物具有相同的意思。`+` 表示标记应该至少出现一次，而 `*` 表示标记可以出现 0 次或多次。让我们看一些例子：
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here''s what happened:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是发生了什么：
- en: In the pattern, the first token reads as *any one of hello, hi, hallo should
    occur 1 or more times* and the second token is punctuation.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模式中，第一个标记表示 *hello、hi、hallo 应该出现 1 次或多次*，第二个标记是标点符号。
- en: The third `doc` object does not match at all; there's no greeting word.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个 `doc` 对象完全不匹配；没有问候词。
- en: The second `doc` object matches `hello,`.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个 `doc` 对象匹配 `hello,`。
- en: When we come to the results of the first `doc` objects' matches, we see that
    there are not one, but three distinct matches. This is completely normal because
    there are indeed three sequences matching the pattern. If you have a closer look
    at the match results, all of them match the pattern we created, because `hello`,
    `hello hello`, and `hello hello hello` all match the `(hello)+` pattern.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们来看第一个 `doc` 对象匹配的结果时，我们看到不仅有 1 个，而是有 3 个不同的匹配。这是完全正常的，因为确实有三个序列与模式匹配。如果你仔细查看匹配结果，所有这些都与我们创建的模式匹配，因为
    `hello`、`hello hello` 和 `hello hello hello` 都与 `(hello)+` 模式匹配。
- en: 'Let''s do the same pattern with `*` and see what happens this time:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 `*` 做相同的模式，看看这次会发生什么：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the first `doc` object''s matches, there are two extra items: `""` and `?`.
    The `"*"` operator matches `0` or more, so our `(hello)*punct_character` pattern
    grabs `""` and `?`. The same applies to the second and third documents: punctuation
    marks alone without any greeting word are picked. This is not what you want in
    your NLP applications, probably.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个 `doc` 对象的匹配中，有两个额外的项：`""` 和 `?`。`"*"` 操作符匹配 `0` 或更多，所以我们的 `(hello)*punct_character`
    模式抓取了 `""` 和 `?`。同样适用于第二个和第三个文档：单独的标点符号而没有任何问候词被选中。这可能在你的 NLP 应用程序中不是你想要的。
- en: The preceding example is a good example that we should be careful of while creating
    our patterns; sometimes, we get unwanted matches. For this reason, we usually
    consider using `IS_SENT_START` and take care with `"*"` operator.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 上述例子是一个很好的例子，我们在创建我们的模式时应该小心；有时，我们得到不想要的匹配。因此，我们通常考虑使用 `IS_SENT_START` 并注意 `"*"`
    操作符。
- en: 'The spaCy Matcher class also accepts a very special pattern, a **wildcard**
    token pattern. A wildcard token will match any token. We usually use it for the
    words we want to pick independent from their text or attributes or for the words
    we ignore. Let''s see an example:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 匹配器类还接受一个非常特殊的模式，即通配符标记模式。通配符标记将匹配任何标记。我们通常用它来选择独立于其文本或属性或我们忽略的单词。让我们看一个例子：
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here, we wanted to capture the first names in the sentence. We achieved it
    by parsing out token sequences in the form *name is/was/be firstname*. The first
    token pattern, `LOWER:` `"name"`, matches the tokens whose lowered text is `name`.
    The second token pattern, `LEMMA: "be"`, matches the `is`, `was`, and `be` tokens.
    The third token is the wildcard token, `{}`, which means *any* token. We pick
    up any token that comes after *name is/was/be* with this pattern.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们想要捕获句子中的名字。我们通过解析形式为 *name is/was/be firstname* 的标记序列来实现。第一个标记模式，`LOWER:`
    `"name"`，匹配文本小写为 `name` 的标记。第二个标记模式，`LEMMA: "be"`，匹配 `is`、`was` 和 `be` 标记。第三个标记是通配符标记
    `{}`，它表示 *任何* 标记。我们使用这个模式拾取任何在 *name is/was/be* 之后出现的标记。'
- en: 'We also use a wildcard token when we want to ignore a token. Let''s make an
    example together:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要忽略一个标记时，我们也会使用通配符标记。让我们一起做一个例子：
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It's just the opposite of the previous example. Here, we wanted to pick up *forward
    email* sequences, and we allowed that one token to come between `forward` and
    `email`. Here, the semantically important part is the forwarding an email action;
    whose email is it doesn't matter much.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: We have mentioned regex quite a lot in this chapter so far, so now it's time
    to see how spaCy's Matcher class makes use of regex syntax.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Regex support
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we match individual tokens, usually we want to allow some variations,
    such as common typos, UK/US English character differences, and so on. Regex is
    very handy for this task and spaCy Matcher offers full support for token-level
    regex matching. Let''s explore how we can use regex for our applications:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, our second token pattern is `[Tt]ravell?ed`, which means the token can
    be capitalized or not. Also, there's an optional `l` after the first `l`. Allowing
    twin vowels and *ise/ize* alteration is a standard way of dealing with British
    and American English variations.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Another way of using regex is using it not only with text, but also with `POS`
    tags. What does the following code segment do?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We have extracted all the finite verbs (you can think of a finite verb as a
    non-modal verb). How did we do it? Our token pattern includes the regex `^V`,
    which means all fine-grained POS tags that start with `V`: `VB`, `VGD`, `VBG`,
    `VBN`, `VBP`, and `VBZ`. Then we extracted tokens with verbal POS tags.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Looks tricky! Occasionally we use some tricks in NLU applications; while going
    through the examples of this book, you'll pick them up too. We encourage you to
    go over our examples and then try some example sentences of yours.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Matcher online demo
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the whole matching business, we occasionally see the match results visually.
    Regex offers *regex101* ([https://regex101.com/](https://regex101.com/)), an online
    tool for checking if your regex pattern is working correctly (surprises with regex
    always happen). The following figure shows an example pattern and checking it
    against a text:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 4.4 – An example regex match and pattern explanations'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_4_04.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 – An example regex match and pattern explanations
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: The explanations on the right side are quite detailed and illuminating. This
    is a tool used not only by NLP learners/beginners, but also professionals (regex
    can be quite difficult to read sometimes).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: spaCy Matcher offers a similar tool on its online demo page ([https://explosion.ai/demos/matcher](https://explosion.ai/demos/matcher)).
    We can create patterns and test them against the text we want, interactively.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we can see a match example. On the right side
    we can select the attributes, values, and operators (such as +, *, !, and ?).
    After making this selection, the demo outputs the corresponding pattern string
    on the right side, below the checkboxes. On the left side, we first choose the
    spaCy language model we want (in this example, English core small), then see the
    results:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 4.5 – spaCy Matcher online demo'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.5 – spaCy 匹配器在线演示'
- en: '](img/B16570_4_05.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_4_05.jpg)'
- en: Figure 4.5 – spaCy Matcher online demo
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – spaCy 匹配器在线演示
- en: Just like regex101, spaCy's Matcher demo helps you to see why your pattern matched
    or didn't match.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 regex101 一样，spaCy 的 Matcher 演示可以帮助你看到为什么你的模式匹配或未匹配。
- en: PhraseMatcher
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PhraseMatcher
- en: While processing financial, medical, or legal text, often we have long lists
    and dictionaries and we want to scan the text against our lists. As we saw in
    the previous section, Matcher patterns are quite handcrafted; we coded each token
    individually. If you have a long list of phrases, Matcher is not very handy. It's
    not possible to code all the terms one by one.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理金融、医疗或法律文本时，我们经常有长长的列表和字典，并希望将文本与我们的列表进行扫描。正如我们在前节中看到的，Matcher 模式相当是手工制作的；我们单独为每个标记编写代码。如果你有一长串短语列表，Matcher
    就不太方便了。不可能一个接一个地编写所有术语。
- en: 'spaCy offers a solution for comparing text against long dictionaries – the
    `PhraseMatcher` class. The `PhraseMatcher` class helps us match long dictionaries.
    Let''s get started with an example:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 提供了一种将文本与长字典进行比较的解决方案 – `PhraseMatcher` 类。`PhraseMatcher` 类帮助我们匹配长字典。让我们从一个例子开始：
- en: '[PRE20]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here''s what we did:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所做的是：
- en: First, we imported `spacy`, then we imported the `PhraseMatcher` class.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们导入了 `spacy`，然后导入了 `PhraseMatcher` 类。
- en: After the imports, we created a `Language` object, `nlp`, and initialized a
    `PhraseMatcher` object, `matcher`, with its vocabulary.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在导入之后，我们创建了一个 `Language` 对象，`nlp`，并初始化了一个 `PhraseMatcher` 对象，`matcher`，并为其提供了词汇表。
- en: The next two lines are where we created the pattern list.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来的两行是我们创建模式列表的地方。
- en: On line 6, we called `nlp.make_doc()` on the terms one by one to create the
    patterns.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第 6 行，我们对每个术语调用了 `nlp.make_doc()` 来创建模式。
- en: '`make_doc()` creates a Doc from every term, and it''s quite efficient in terms
    of processing because instead of the whole pipeline, it only calls the `Tokenizer`.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`make_doc()` 从每个术语创建一个 Doc，在处理方面非常高效，因为它只调用 `Tokenizer` 而不是整个管道。'
- en: 'The rest of the code is similar to what we did with Matcher: we iterated over
    the resulting spans.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其余的代码与我们在 Matcher 中的操作类似：我们遍历了生成的跨度。
- en: 'This way, we match the pattern by their exact text values. What if we want
    to match them with other attributes? Here''s an example of matching by the `LOWER`
    attribute:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们通过它们的精确文本值来匹配模式。如果我们想通过其他属性来匹配它们呢？这里是一个通过 `LOWER` 属性匹配的例子：
- en: '[PRE21]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: On line 1, while creating a `PhraseMatcher` instance, we passed an additional
    argument, `attr=LOWER`. This way, the `PhraseMatcher` used the `token.lower` attribute
    during the match. Notice that the terms are uppercase and the matches are lowercase.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 1 行，在创建 `PhraseMatcher` 实例时，我们传递了一个额外的参数，`attr=LOWER`。这样，`PhraseMatcher`
    在匹配时使用了 `token.lower` 属性。注意，术语是大写的，而匹配结果是小写的。
- en: 'Another possible usage of PhraseMatcher is matching the `SHAPE` attribute.
    This matching strategy can be used on system logs, where IP numbers, dates, and
    other numerical values occur a lot. The good thing here is that you do not need
    to worry how the numbers are tokenized, you just leave it to `PhraseMatcher`.
    Let''s see an example:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: PhraseMatcher 的另一个可能用途是匹配 `SHAPE` 属性。这种匹配策略可以用于系统日志，其中 IP 地址、日期和其他数值经常出现。这里的好事是，你不需要担心数值是如何分词的，你只需将其留给
    `PhraseMatcher`。让我们看一个例子：
- en: '[PRE22]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: That's it! We matched the tokens and phrases successfully; what's left is named
    entities. Named entity extraction is an essential component of any NLP system
    and most of the pipelines you'll design will include an **named entity recognition**
    (**NER**) component. The next section is devoted to rule-based named entity extraction.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们成功匹配了标记和短语；剩下的是命名实体。命名实体提取是任何 NLP 系统的一个基本组成部分，你将设计的多数管道都将包括一个 **命名实体识别**（**NER**）组件。下一节将专门介绍基于规则的命名实体提取。
- en: EntityRuler
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: EntityRuler
- en: 'While covering Matcher, we saw that we can extract named entities with Matcher
    by using the `ENT_TYPE` attribute. We recall from the previous chapter that `ENT_TYPE`
    is a linguistic attribute that refers to the entity type of the token, such as
    person, place, or organization. Let''s see an example:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍 Matcher 时，我们看到了可以通过使用 `ENT_TYPE` 属性使用 Matcher 提取命名实体。我们回忆起前一章中提到的 `ENT_TYPE`
    是一个语言属性，它指的是标记的实体类型，例如人、地点或组织。让我们看一个例子：
- en: '[PRE23]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Again, we created a `Matcher` object called `matcher` and called it on the
    `Doc` object, `doc`. The result is two tokens, `Bill` and `Gates`; Matcher always
    matches at the token level. We got `Bill` and `Gates`, instead of the full entity,
    `Bill Gates`. If you want to get the full entity rather than the individual tokens,
    you can do this:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Usually, we match two or more entities together, or with other linguistic attributes
    to extract information. Here''s an example of how we can understand the action
    in the sentence and which person in the sentence committed this action:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We noticed that the Matcher returns two matches here; usually, we loop through
    the results and pick the longest match.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding examples, we matched the entities that the spaCy statistical
    model already extracted. What if we have domain-specific entities that we want
    to match? For instance, our dataset consists of wiki pages about ancient Greek
    philosophers. The philosopher names are naturally in Greek and don't follow English
    statistical patterns; it's expected that a tagger trained on English text would
    fail to extract the entity name occasionally. In these situations, we'd like spaCy
    to tell our entities and combine them with the statistical rules.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: spaCy's `EntityRuler` is the component that allows us to add rules on top of
    the statistical model and creates an even more powerful **NER** model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '`EntityRuler` is not a matcher, it''s a pipeline component that we can add
    to our pipeline via `nlp.add_pipe`. When it finds a match, the match is appended
    to `doc.ents` and `ent_type` will be the label we pass in the pattern. Let''s
    see it in action:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: That's it, really easy, yet powerful! We added our own entity with just a couple
    of lines.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: The `Matcher` class and `EntityRuler` are exciting and powerful features of
    the spaCy library, as we saw from the examples. Now, we move onto an exclusive
    section of quick and very handy recipes.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Combining spaCy models and matchers
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll go through some recipes that will guide you through the
    entity extraction types you'll encounter in your NLP career. All the examples
    are ready-to-use and real-world recipes. Let's start with number-formatted entities.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Extracting IBAN and account numbers
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IBAN and account numbers are two important entity types that occur in finance
    and banking frequently. We'll learn how to parse them out.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'An IBAN is an international number format for bank account numbers. It has
    the format of a two-digit country code followed by numbers. Here are some IBANs
    from different countries:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16570_4_06.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6 – IBAN formats from different countries (source: Wikipedia)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'How can we create a pattern for an IBAN? Obviously, in all cases, we start
    with two capital letters, followed by two digits. Then any number of digits can
    follow. We can express the country code and the next two digits as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here, `XX` corresponds to two capital letters and `dd` is two digits. Then
    `XXdd` pattern matches the first block of the IBAN perfectly. How about the rest
    of the digit blocks? For the rest of the blocks, we need to match a block of 1-4
    digits. The regex `\d{1,4}` means a token consisting of 1 to 4 digits. This pattern
    will match a digit block:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`XX`对应于两个大写字母，`dd`是两个数字。然后`XXdd`模式完美地匹配IBAN的第一个块。那么其他数字块呢？对于其他块，我们需要匹配一个1到4位的数字块。正则表达式`\d{1,4}`表示由1到4位数字组成的标记。这个模式将匹配一个数字块：
- en: '[PRE28]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We have a number of these blocks, so the pattern to match the digit blocks
    of an IBAN is as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有几个这样的块，所以匹配IBAN数字块的模式如下：
- en: '[PRE29]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, we combine the first block with the rest of the blocks. Let''s see the
    code and the matches:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将第一个块与其他块结合起来。让我们看看代码和匹配结果：
- en: '[PRE30]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You can always follow a similar strategy when parsing numeric entities: first,
    divide the entity into some meaningful parts/blocks, then try to determine the
    shape or the length of the individual blocks.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在解析数字实体时，你可以始终遵循类似的策略：首先，将实体分成一些有意义的部分/块，然后尝试确定各个块的形式或长度。
- en: 'We successfully parsed IBANs, now we can parse the account numbers. Parsing
    the account numbers is a bit trickier; account numbers are just plain numbers
    and don''t have a special shape to help us differentiate them from usual numbers.
    What do we do, then? We can make a context lookup in this case; we can look around
    the number token and see if we can find *account number* or *account num* around
    the number token. This pattern should do the trick:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功解析了IBAN，现在我们可以解析账户号码。解析账户号码有点棘手；账户号码只是普通的数字，没有特殊的形式帮助我们区分它们和普通数字。那么我们该怎么办呢？在这种情况下，我们可以进行上下文查找；我们可以查看数字标记周围，看看我们是否可以在数字标记周围找到*account
    number*或*account num*。这个模式应该可以解决问题：
- en: '[PRE31]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We used a wildcard here: `{}` means any token. We allowed one token to go in
    between *number* and *account number*; this can be *is*, *was*, and so on. Let''s
    see the code:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用了一个通配符：`{}`表示任何标记。我们允许一个标记在*number*和*account number*之间；这可以是*is*，*was*等等。让我们看看代码：
- en: '[PRE32]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: If you want, you can include a possessive pronoun such as *my*, *your*, or *his*
    in the match, depending on the application's needs.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想，你可以在匹配中包含一个所有格代词，如*my*，*your*或*his*，具体取决于应用程序的需求。
- en: That's it for banking numbers. Now we'll extract another type of common numeric
    entity, phone numbers.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 银行号码就到这里。现在我们将提取另一种常见的数字实体，电话号码。
- en: Extracting phone numbers
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取电话号码
- en: Phone numbers can have very different formats depending on the country, and
    matching phone numbers is often a tricky business. The best strategy here is to
    be specific about the country phone number format you want to parse. If there
    are several countries, you can add corresponding individual patterns to the matcher.
    If you have too many countries, then you can relax some conditions and go for
    a more general pattern (we'll see how to do that).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 电话号码的格式可能因国家而异，匹配电话号码通常是一项棘手的工作。这里最好的策略是明确你想要解析的国家电话号码格式。如果有几个国家，你可以在匹配器中添加相应的单个模式。如果你有太多的国家，那么你可以放宽一些条件，采用更通用的模式（我们将看到如何做到这一点）。
- en: 'Let''s start with the US phone number format. A US number is written as *(541)
    754-3010* domestically or *+1 (541) 754-3010* internationally. We can form our
    pattern with an optional *+1*, then a three-digit area code, then two blocks of
    numbers separated with an optional *-*.  Here is the pattern:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从美国的电话号码格式开始。美国国内电话号码写作*(541) 754-3010*，国际电话号码写作*+1 (541) 754-3010*。我们可以用可选的*+1*来形成我们的模式，然后是一个三位数的区号，然后是两个用可选的*-*分隔的数字块。  以下是模式：
- en: '[PRE33]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let''s see an example:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个例子：
- en: '[PRE34]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: How about we make the pattern more general to apply to other countries as well?
    In this case, we can start with a 1 to 3-digit country code, followed by some
    digit blocks. It will match a broader set of numbers, so it's better to be careful
    not to match other numeric entities in your text.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否可以将模式做得更通用，以便也适用于其他国家呢？在这种情况下，我们可以从一个1到3位的国家代码开始，后面跟着一些数字块。这将匹配更广泛的数字集合，因此最好小心不要匹配你文本中的其他数字实体。
- en: We'll move onto textual entities from numeric entities. Now we'll process social
    media text and extract different types of entities that can occur in social media
    text.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将转到文本实体，从数字实体开始。现在我们将处理社交媒体文本，并提取社交媒体文本中可能出现的不同类型的实体。
- en: Extracting mentions
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取提及
- en: 'Imagine analyzing a dataset of social media posts about companies and products
    and your task is to find out which companies are mentioned in which ways. The
    dataset will contain this sort of sentence:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'What we''re looking for is most probably patterns of the *BusinessName is/was/be
    adverb* adjective* form. The following pattern would work:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Here, we look for an organization type entity, followed by a *is/was/be*, then
    optional adverbs, and finally an adjective.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'What if you want to extract a specific business, let''s say the company *ACME*?
    All you have to do is replace the first token with the specific company name:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: That's it, easy peasy! After extracting the social media mentions, the next
    thing to do is to extract the hashtags and the emojis.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Hashtag and emoji extraction
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Processing social media text is a hot topic and has some challenges. Social
    media text includes two sorts of unusual token types: hashtags and emojis. Both
    token types have a huge impact on the text meaning. The hashtag refers to the
    subject/object of the sentence, usually, and emojis can assign the sentiment of
    the sentence by themselves.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'A hashtag consists of a `#` character at the beginning, then followed by a
    word of `ASCII` characters, with no inter-word spaces. Some examples are *#MySpace*,
    *#MondayMotivation* and so on. The spaCy tokenizer tokenizes these words into
    two tokens:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'As a result, our pattern needs to match two tokens, the `#` character and the
    rest. The following pattern will match a hashtag easily:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The following code extracts a hashtag:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'How about an emoji? An emoji is usually coded with lists according to their
    sentiment value, such as positive, negative, happy, sad, and so on. Here, we''ll
    separate emojis into two classes, positive and negative. The following code spots
    the selected emoji in the text:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Et voilà, the emoji ![](img/B16570_4_emoj15.png) is extracted happily! We'll
    make use of emojis in sentiment analysis chapter as well.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's extract some entities. We'll start with the common procedure of expanding
    named entities.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Expanding named entities
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often, we would like to expand a named entity's span to the left or to the right.
    Imagine you want to extract `PERSON` type named entities with titles so that you
    can deduce the gender or profession easily. spaCy's `NER` class already extracts
    person names, so how about the titles?
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'As you see, the word `Ms.` is not included in the named entity because it''s
    not a part of the person''s name. A quick solution is to make a new entity type
    called `TITLE`:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This is a quick and very handy recipe. You'll come across parsing titles a lot
    if you process wiki text or financial text.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: In our next and final example, we'll combine POS attributes, dependency labels,
    and named entities.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Combining linguistic features and named entities
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While charging meaning to a sentence, we evaluate word semantics by considering
    the contexts they occur in. Matching the words individually usually does not help
    us understand the full meaning. In most NLU tasks we have to combine linguistic
    features.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you're parsing professional biographies and make a work history of the
    subjects. You want to extract person names, the cities they have lived in, and
    the city they're currently working in.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously we''ll look for the word *live*; however, the POS tags hold the key
    here: whether it''s the present tense or the past tense. In order to determine
    which city/place, we''ll use syntactic information that is given by the dependency
    labels.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s examine the following example:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Here is a visual representation of the preceding example:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 4.7 – Example parse, the entity ”Einstein” being subject of the sentence'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_4_07.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 – Example parse, the entity "Einstein" being subject of the sentence
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, `lived` is the main verb of the sentence, hence the root of the sentence.
    `Einstein` is the subject of the sentence, at the same time the person entity
    who `lived`. As we can see, the `Einstein` token''s head is `lived`. There''s
    also a place entity in the sentence, `Zurich`. If we follow the arcs from `lived`,
    we reach `Zurich` via a prepositional attachment. Finally, to determine the verb''s
    tense, we can examine the POS tag. Let''s see it in the following code:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Here, we combined POS tag information, dependency labels (hence syntactic information
    of the sentence), and named entities. It may not be easy for you to grasp it at
    first sight, but you'll get there by practicing.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced you to a very handy and powerful feature of spaCy, spaCy's
    matcher classes. You learned how to do rule-based matching with linguistic and
    token-level features. You learned about the `Matcher` class, spaCy's rule-based
    matcher. We explored the `Matcher` class by using it with different token features,
    such as shape, lemma, text, and entity type.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Then, you learned about `EntityRuler`, another lifesaving class that you can
    achieve a lot with. You learned how to extract named entities with the `EntityRuler`
    class.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we put together what you've learned in this chapter and your previous
    knowledge and combined linguistic features with rule-based matching with several
    examples. You learned how to extract patterns, entities of specific formats, and
    entities specific to your domain.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: With this chapter, you completed the linguistic features. In the next chapter,
    we'll dive into the world of statistical semantics via a very important concept
    – **word vectors**. You'll discover the power of statistics in representing words,
    phrases, and sentences. Let's discover the world of semantics together!
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
