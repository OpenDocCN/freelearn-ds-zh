<html><head></head><body>
		<div id="_idContainer016">
			<h1 id="_idParaDest-30" class="chapter-number"><a id="_idTextAnchor030"/>2</h1>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor031"/>Understanding Apache Spark and Its Applications</h1>
			<p>With the advent of machine learning and data science, the world is seeing a paradigm shift. A tremendous amount of data is being collected every second, and it’s hard for computing power to keep up with this pace of rapid data growth. To make use of all this data, Spark has become a de facto standard for big data processing. Migrating data processing to Spark is not only a question of saving resources that will allow you to focus on your business; it’s also a means of modernizing your workloads to leverage the capabilities of Spark and the modern technology stack to create new <span class="No-Break">business opportunities.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>What is <span class="No-Break">Apache Spark?</span></li>
				<li>Why choose <span class="No-Break">Apache Spark?</span></li>
				<li>Different components <span class="No-Break">of Spark</span></li>
				<li>What are the Spark <span class="No-Break">use cases?</span></li>
				<li>Who are the <span class="No-Break">Spark users?</span></li>
			</ul>
			<h1 id="_idParaDest-32"><a id="_idTextAnchor032"/>What is Apache Spark?</h1>
			<p>Apache Spark is an open-source big data framework that is used for multiple big data applications. The strength of Spark lies in its superior parallel processing capabilities that makes it a leader in <span class="No-Break">its domain.</span></p>
			<p>According to its website (<a href="https://spark.apache.org/">https://spark.apache.org/</a>), “<em class="italic">The most widely-used engine for </em><span class="No-Break"><em class="italic">scalable computing.</em></span><span class="No-Break">”</span></p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor033"/>The history of Apache Spark</h2>
			<p>Apache Spark started as a research project at the UC Berkeley AMPLab in 2009 and moved to an open source license in 2010. Later, in 2013, it came under the Apache Software Foundation (<a href="https://spark.apache.org/">https://spark.apache.org/</a>). It gained popularity after 2013, and today, it serves as a backbone for a large number of big data products across various Fortune 500 companies and has thousands of developers actively working <span class="No-Break">on it.</span></p>
			<p>Spark came into being because of limitations in the Hadoop MapReduce framework. MapReduce’s main premise was to read data from disk, distribute that data for parallel processing, apply map functions to the data, and then reduce those functions and save them back to disk. This back-and-forth reading and saving to disk becomes time-consuming and costly <span class="No-Break">very quickly.</span></p>
			<p>To overcome this limitation, Spark introduced the concept of in-memory computation. On top of that, Spark has several capabilities that came as a result of different research initiatives. You will read more about them in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor034"/>Understanding Spark differentiators</h2>
			<p>Spark’s foundation lies in its major capabilities such as in-memory computation, lazy evaluation, fault tolerance, and support for multiple languages such as Python, SQL, Scala, and R. We will discuss each one of them in detail in the <span class="No-Break">following section.</span></p>
			<p>Let’s start with <span class="No-Break">in-memory computation.</span></p>
			<h3>In-memory computation</h3>
			<p>The first major differentiator technology that Spark’s foundation is built on is that it utilizes in-memory computations. Remember when we discussed Hadoop MapReduce technology? One of its major limitations is to write back to disk at each step. Spark saw this as an opportunity for improvement and introduced the concept of in-memory computation. The main idea is that the data remains in memory as long as it is worked on. If we can work with the size of data that can be stored in the memory at once, we can eliminate the need to write to disk at each step. As a result, the complete computation cycle can be done in memory if we can work with all computations on that amount of data. Now, the thing to note here is that with the advent of big data, it’s hard to contain all the data in memory. Even if we look at heavyweight servers and clusters in the cloud computing world, memory remains finite. This is where Spark’s internal framework of parallel processing comes into play. Spark framework utilizes the underlying hardware resources in the most efficient manner. It distributes the computations across multiple cores and utilizes the hardware capabilities to <span class="No-Break">the maximum.</span></p>
			<p>This tremendously reduces the computation time, since the overhead of writing to disk and reading it back for the subsequent step is minimized as long as the data can be fit in the memory of <span class="No-Break">Spark compute.</span></p>
			<h3>Lazy evaluation</h3>
			<p>Generally, when we work with programming frameworks, the backend compilers look at each statement and execute it. While this works great for programming paradigms, with big data and parallel processing, we need to shift to a look-ahead kind of model. Spark is well known for its parallel processing capabilities. To achieve even better performance, Spark doesn’t execute code as it reads it, but once the code is there and we submit a Spark statement to execute, the first step is that Spark builds a logical map of the queries. Once that map is built, then it plans what the best path of execution is. You will read more about its intricacies in the Spark architecture chapters. Once the plan is established, only then will the execution begin. Once the execution begins, even then, Spark holds off executing all statements until it hits an “action” statement. There are two types of statements <span class="No-Break">in Spark:</span></p>
			<ul>
				<li><span class="No-Break">Transformations</span></li>
				<li><span class="No-Break">Actions</span></li>
			</ul>
			<p>You will learn more about the different types of Spark statements in detail in <a href="B19176_03.xhtml#_idTextAnchor053"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, where we discuss Spark architecture. Here are a few advantages of <span class="No-Break">lazy evaluation:</span></p>
			<ul>
				<li><span class="No-Break">Efficiency</span></li>
				<li><span class="No-Break">Code manageability</span></li>
				<li>Query and <span class="No-Break">resource optimization</span></li>
				<li><span class="No-Break">Reduced complexities</span></li>
			</ul>
			<h3>Resilient datasets/fault tolerance</h3>
			<p>Spark’s foundation is built on <strong class="bold">resilient distributed datasets</strong> (<strong class="bold">RDDs</strong>). It is an immutable distributed collection of objects that represent a set of records. RDDs are distributed across a number of servers, and they are computed in parallel across multiple cluster nodes. RDDs can be generated with code. When we read data from an external storage location into Spark, RDDs hold that data. This data can be shared across multiple clusters and can be computed in parallel, thus giving Spark a very efficient way of running computations on RDD data. RDDs are loaded in memory for processing; therefore, loading to and from memory computations is not required, <span class="No-Break">unlike Hadoop.</span></p>
			<p>RDDs are fault-tolerant. This means that if there are failures, RDDs have the ability to self-recover. Spark achieves that by distributing these RDDs to different worker nodes while keeping in view what task is performed by which worker node. This handling of worker nodes is done by the Spark driver. We will discuss this in detail in <span class="No-Break">upcoming chapters.</span></p>
			<p>RDDs give a lot of power to Spark in terms of resilience and fault-tolerance. This capability, along with other features, makes Spark the tool of choice for any <span class="No-Break">production-grade applications.</span></p>
			<h3>Multiple language support</h3>
			<p>Spark supports multiple languages for development such as Java, R, Scala, and Python. This gives users the flexibility to use any language of choice to build applications <span class="No-Break">in Spark.</span></p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor035"/>The components of Spark</h2>
			<p>Let’s talk about the different components Spark has. As you can see in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.1</em>, Spark Core is the backbone of operations in Spark and spans across all the other components that Spark has. Other components that we’re going to discuss in this section are Spark SQL, Spark Streaming, Spark MLlib, <span class="No-Break">and GraphX.</span></p>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/B19176_02_01.jpg" alt="Figure 2.1: Spark components"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1: Spark components</p>
			<p>Let’s look at the first component <span class="No-Break">of Spark.</span></p>
			<h3>Spark Core</h3>
			<p>Spark Core is central to all the other components of Spark. It provides functionalities and core features for all the different components. Spark SQL, Spark Streaming, Spark MLlib, and GraphX all make use of Spark Core as their base. All the functionality and features of Spark are controlled by Spark Core. It provides in-memory computing capabilities to deliver speed, a generalized execution model to support a wide variety of applications, and Java, Scala, and Python APIs for ease <span class="No-Break">of development.</span></p>
			<p>In all of these different components, you can write queries in supported languages. Spark will then convert these queries to <strong class="bold">directed acyclic graphs</strong> (<strong class="bold">DAGs</strong>), and Spark Core has the responsibility of <span class="No-Break">executing them.</span></p>
			<p>The key responsibilities of Spark Core are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Interacting with <span class="No-Break">storage systems</span></li>
				<li><span class="No-Break">Memory management</span></li>
				<li><span class="No-Break">Task distribution</span></li>
				<li><span class="No-Break">Task scheduling</span></li>
				<li><span class="No-Break">Task monitoring</span></li>
				<li><span class="No-Break">In-memory computation</span></li>
				<li><span class="No-Break">Fault tolerance</span></li>
				<li><span class="No-Break">Optimization</span></li>
			</ul>
			<p>Spark Core contains an API for RDDs which are an integral part of Spark. It also provides different APIs to interact and work with RDDs. All the components of Spark work with underlying RDDs for data manipulation and processing. RDDs make it possible for Spark to have a lineage for data, since they are immutable. This means that every time an operation is run on an RDD that requires changes in it, Spark will create a new RDD for it. Hence, it maintains the lineage information of RDDs and their <span class="No-Break">corresponding operations.</span></p>
			<h3>Spark SQL</h3>
			<p>SQL is the most popular language for database and data warehouse applications. Analysts use this language for all their exploratory data analysis on relational databases and their counterparts in traditional data warehouses. Spark SQL adds this advantage to the Spark ecosystem. Spark SQL is used to query structured data in SQL using the <span class="No-Break">DataFrame API.</span></p>
			<p>As its name represents, Spark SQL gives SQL support to Spark. This means we can query the data present in RDDs and other external sources, such as Parquet files. This is a powerful capability of Spark, since it gives developers the flexibility to use a relational table structure on top of RDDs and other file formats and write SQL queries on top of it. This also adds the capabilities of using SQL where necessary and unifies it with analytics applications and use cases, thus providing unification of <span class="No-Break">the platforms.</span></p>
			<p>With Spark SQL, developers are able to do the following <span class="No-Break">with ease:</span></p>
			<ul>
				<li>They can read data from different file formats and different sources into RDDs <span class="No-Break">and DataFrames</span></li>
				<li>They can run SQL queries on top of the data present in DataFrames, thus giving flexibility to the developers to use programming languages or SQL to <span class="No-Break">process data</span></li>
				<li>Once they’re done with the processing of the data, they have the capability to write RDDs and DataFrames to <span class="No-Break">external sources</span></li>
			</ul>
			<p>Spark SQL consists of a cost-based optimizer that optimizes queries, keeping in view the resources; it also has the capability to generate code for these optimizations, which makes these queries very fast and efficient. To support even faster query times, it can scale to multiple nodes with the help of Spark Core and also provides features such as fault tolerance and resiliency. This is known as the Catalyst optimizer. We will read more about it in <a href="B19176_05.xhtml#_idTextAnchor115"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><span class="No-Break">.</span></p>
			<p>The most noticeable features of Sparks SQL are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>It provides an engine for high-level <span class="No-Break">structured APIs</span></li>
				<li>Reads/writes data to and from a large number of file formats such as Avro, Delta, <strong class="bold">Comma-Separated Values</strong> (<strong class="bold">CSV</strong>), <span class="No-Break">and Parquet</span></li>
				<li>Provides <strong class="bold">Open Database Connectivity</strong> (<strong class="bold">ODBC</strong>)/<strong class="bold">Java Database Connectivity</strong> (<strong class="bold">JDBC</strong>) connectors to <strong class="bold">business intelligence</strong> (<strong class="bold">BI</strong>) tools such as PowerBI and Tableau, as well as popular <strong class="bold">relational </strong><span class="No-Break"><strong class="bold">databases</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">RDBMs</strong></span><span class="No-Break">)</span></li>
				<li>Provides a way to query structured data in files as tables <span class="No-Break">and views</span></li>
				<li>It supports ANSI SQL:2003-compliant commands <span class="No-Break">and HiveQL</span></li>
			</ul>
			<p>Now that we have covered SparkSQL, let’s discuss the Spark <span class="No-Break">Streaming component.</span></p>
			<h3>Spark Streaming</h3>
			<p>We have talked about the rapid growth of data in today’s times. If we were to divide this data into groups, there are two types of datasets in practice, batch <span class="No-Break">and streaming:</span></p>
			<ul>
				<li><strong class="bold">Batch data</strong> is when there’s a chunk of data present that you have to ingest and then transform all at once. Think of when you want to get a sales report of all the sales in a month. You would have the monthly data available as a batch and process it all <span class="No-Break">at once.</span></li>
				<li><strong class="bold">Streaming data</strong> is when you need output of that data in real time. To serve this requirement, you would have to ingest and process that data in real time. This means every data point can be ingested as a single data element, and we would not wait for it to be ingested after a block of data is collected. Think of when self-driving cars need to make decisions in real time based on the data they collect. All the data needs to be ingested and processed in real time for the car to make effective decisions in a <span class="No-Break">given moment.</span></li>
			</ul>
			<p>There are a large number of industries generating streaming data. To make use of this data, you need real-time ingestion, processing, and management of this data. It has become essential for organizations to use streaming data as it arrives for real-time analytics and other use cases. This gives them an edge over their competitors, as this allows them to make decisions in <span class="No-Break">real time.</span></p>
			<p>Spark Streaming enables organizations to make use of streaming data. One of the most important factors of Spark Streaming is its ease of use alongside batch data processing. You can combine batch and stream data within one framework and use it to augment your analytics applications. Spark Streaming also inherits Spark Core’s features of resilience and fault tolerance, giving it a dominant position in the industry. It integrates with a large number of streaming data sources such as HDFS, Kafka, <span class="No-Break">and Flume.</span></p>
			<p>The beauty of Spark Streaming is that batch data can be processed as streams to take advantage of built-in paradigms of streaming data and look-back capabilities. There are certain factors that need to be taken into consideration when we work with real-time data. When we work with real-time data streams, there’s a chance that some of the data may get missed due to system hiccups or failures altogether. Spark Streaming takes care of this in a seamless way. To cater to these requirements, it has a built-in mechanism called <strong class="bold">checkpoints</strong>. The purpose of these checkpoints is to keep track of the incoming data, knowing what was processed downstream and which data is still left to be processed in the next cycle. We will learn more about this in <a href="B19176_07.xhtml#_idTextAnchor183"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> when we discuss Spark Streaming in <span class="No-Break">more detail.</span></p>
			<p>This makes Spark resilient to failures. If there are any failures, you need minimal work to reprocess old data. You can also define mechanisms and algorithms for missing data or late processed data. This gives a lot of flexibility to the data pipelines and makes them easier to maintain in large <span class="No-Break">production environments.</span></p>
			<h3>Spark MLlib</h3>
			<p>Spark provides a framework for distributed and scalable machine learning. It distributes the computations across different nodes, thus resulting in better performance for model training. It also distributes hyperparameter tuning. You will learn more about hyperparameter tuning in <a href="B19176_08.xhtml#_idTextAnchor220"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, where we talk about machine learning. Because Spark can scale to large datasets, it is the framework of choice for machine learning production pipelines. When you build products, execution and computation speed matter a lot. Spark gives you the ability to work with large amounts of data and build state-of-the-art machine learning models that can run very efficiently. Instead of working with models that take days to train, Spark reduces that time to hours. In addition, working with more data results in better-performing models in <span class="No-Break">most cases.</span></p>
			<p>Most of the commonly used machine learning algorithms are part of Spark’s libraries. There are two machine learning packages available <span class="No-Break">in Spark:</span></p>
			<ul>
				<li><span class="No-Break">Spark MLlib</span></li>
				<li><span class="No-Break">Spark ML</span></li>
			</ul>
			<p>The major difference between these two is the type of data they work with. Spark MLlib is built on top of RDDs while Spark ML works with DataFrames. Spark MLlib is the older library and has now entered maintenance mode. The more up-to-date library is Spark ML. You should also note that Spark ML is not the official name of the library itself, but it is commonly used to refer to the DataFrame-based API in Spark. The official name is still Spark MLlib. However, it’s important to know <span class="No-Break">the differences.</span></p>
			<p>Spark MLlib contains the most commonly used machine learning libraries for <strong class="bold">classification</strong>, <strong class="bold">regression</strong>, <strong class="bold">clustering</strong>, and <strong class="bold">recommendation systems</strong>. It also has some support for frequent pattern <span class="No-Break">mining algorithms.</span></p>
			<p>When there is a need to serve these models to millions and billions of users, Spark is also helpful. You can distribute and parallelize both data processing (<strong class="bold">Extract, Transform, Load</strong> (<strong class="bold">ETL</strong>)) and model scoring <span class="No-Break">with Spark.</span></p>
			<h3>GraphX</h3>
			<p>GraphX is Spark’s API for graphs and graph-parallel computation. GraphX extends Spark’s RDD to work with graphs and allows you to run parallel computations with graph objects. This speeds up the <span class="No-Break">computations significantly.</span></p>
			<p>Here’s a network graph that represents what a graph <span class="No-Break">looks like.</span></p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B19176_02_02.jpg" alt="Figure 2.2: A network graph"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2: A network graph</p>
			<p>A graph is an object with vertices and edges. Properties are attached to each vertex and edge. There are primary graph operations that Spark supports, such as <strong class="source-inline">subgraph</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">joinVertices</strong></span><span class="No-Break">.</span></p>
			<p>The main premise is that you can use GraphX for exploratory analysis and ETL and transform and join graphs with RDDs efficiently. There are two types of operator— <strong class="source-inline">Graph</strong> and <strong class="source-inline">GraphOps</strong>. On top of that, graph aggregation operators are also available. Spark also includes a number of graph algorithms that are used in common use cases. Some of the most popular algorithms are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><span class="No-Break">PageRank</span></li>
				<li><span class="No-Break">Connected components</span></li>
				<li><span class="No-Break">Label propagation</span></li>
				<li><span class="No-Break">SVD++</span></li>
				<li>Strongly <span class="No-Break">connected components</span></li>
				<li><span class="No-Break">Triangle count</span></li>
			</ul>
			<p>Now, let’s discuss why we want to use Spark in our applications and what some of the features it <span class="No-Break">provides are.</span></p>
			<h1 id="_idParaDest-36"><a id="_idTextAnchor036"/>Why choose Apache Spark?</h1>
			<p>In this section, we will discuss the applications of Apache<a id="_idIndexMarker028"/> Spark and its features, such as speed, reusability, in-memory computations, and how Spark is a <span class="No-Break">unified platform.</span></p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor037"/>Speed</h2>
			<p>Apache Spark is one of the fastest processing frameworks for data available today. It beats Hadoop MapReduce by a large margin. The main reason is its in-memory computation capabilities and lazy evaluation. We will learn more about this when we discuss Spark architecture in the <span class="No-Break">next chapter.</span></p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor038"/>Reusability</h2>
			<p>Reusability is a very important consideration for large organizations making use of modern platforms. Spark can join batch and stream data seamlessly. Moreover, you can augment datasets with historical data to serve your use cases better. This gives a large historical view of data to run queries or build modern <span class="No-Break">analytical systems.</span></p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor039"/>In-memory computation</h2>
			<p>With in-memory computation, all the overhead of reading and writing to disks is eliminated. The data is cached, and at each step, the required data is already present in memory. At the end of the processing, results are aggregated and sent back to the driver for <span class="No-Break">further steps.</span></p>
			<p>All of this is facilitated by the process of DAG creation that Spark performs inherently. Before execution, Spark creates a DAG of the necessary steps and prioritizes them based on its internal algorithms. We will learn more about this in the next chapter. These capabilities support in-memory computation, resulting in fast <span class="No-Break">processing speeds.</span></p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor040"/>A unified platform</h2>
			<p>Spark provides a unified platform for data engineering, data science, machine learning, analytics, streaming, and graph processing. All of these components are integrated with Spark Core. The core engine is very high-speed and generalizes the commonly needed tasks for its other components. This gives Spark an advantage over other platforms because of the unification of its different components. These components can work in conjunction with each other, providing a unified experience for software applications. In modern applications, this unification makes it easy to use, and different parts of the application can make use of the core capabilities of these components without compromising <a id="_idIndexMarker029"/><span class="No-Break">on features.</span></p>
			<p>Now that you understand the benefits of using Spark, let’s talk about the different use cases of Spark in <span class="No-Break">the industry.</span></p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor041"/>What are the Spark use cases?</h1>
			<p>In this section, we will learn about how Spark is used in the industry. There are various use cases of Spark prevalent today, some of which include big data processing, machine learning applications, near-real-time and real-time streaming, and using <span class="No-Break">graph analytics.</span></p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor042"/>Big data processing</h2>
			<p>One of the most popular<a id="_idIndexMarker030"/> use cases for Spark is big data processing. You might be wondering what big data is, so let’s take a look at the components that mark data as <span class="No-Break">big data.</span></p>
			<p>The first component<a id="_idIndexMarker031"/> of big data is the <strong class="bold">volume of data</strong>. By volume, we mean that the data is very large in size, often amounting to terabytes, petabytes, and beyond in some cases. Organizations have collected a large amount of data over the years. This data can be used for analysis. However, the first step in this activity is to process these large amounts of data. Also, it’s only recently that computing power has grown to now be able to process such vast volumes <span class="No-Break">of data.</span></p>
			<p>The second component<a id="_idIndexMarker032"/> of big data is the <strong class="bold">velocity of data</strong>. The velocity of data refers to the speed of its generation, ingestion, and distribution. This means that the speed with which this data is generated has increased manifold in recent years. Take, for example, data generated by your smart appliance that sends data every second to a server. In this process, the server also needs to keep up with the ingestion of this data, and then distributing this across different sources might be the <span class="No-Break">next step.</span></p>
			<p>The third component<a id="_idIndexMarker033"/> of big data is the <strong class="bold">variety of data</strong>. The variety of data refers to the different sources that generate the data. It also refers to different types of data that are generated. Gone are the days when data was only generated in structured formats that could be saved as tables in databases. Currently, data can be structured, semi-structured, or unstructured. The systems now have to work with all these different data types, and tools should be able to manipulate these different data types. Think of images that need to be processed or audio and video files that can be analyzed with <span class="No-Break">advanced analytics.</span></p>
			<p>Some other components can be added to the original three Vs as well, such as veracity and value. However, these components are out of the scope of <span class="No-Break">our discussion.</span></p>
			<p>Big data is too large for regular machines to process it. That’s why it’s called big data. Big data that<a id="_idIndexMarker034"/> is high-volume, high-velocity, and high-variety needs to be processed with advanced analytical tools such as Spark, which can distribute the workload across different machines or clusters and does processing in parallel to make use of all the resources available on a machine. So, instead of using only a single machine and loading all the data into one node, Spark gives us the ability to divide the data up into different parts and process them in parallel and across different machines. This massively speeds up the whole process and makes use of all the <span class="No-Break">available resources.</span></p>
			<p>For all of the aforementioned reasons, Spark is one of the most widely used big data processing technologies. Large organizations make use of Spark to analyze and manipulate their big data stacks. Spark<a id="_idIndexMarker035"/> serves as a backbone for big data processing in complex analytical <span class="No-Break">use cases.</span></p>
			<p>Some examples of big data use cases<a id="_idIndexMarker036"/> are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Business intelligence for reporting <span class="No-Break">and dashboarding</span></li>
				<li>Data warehousing for <span class="No-Break">complex applications</span></li>
				<li>Operational analytics for <span class="No-Break">application monitoring</span></li>
			</ul>
			<p>It is important to note here that working with Spark requires a mindset shift from single-node processing to big data-processing paradigms. You now have to start thinking about how to best utilize and optimize the use of large clusters for processing and what some of the best practices around parallel <span class="No-Break">processing are.</span></p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor043"/>Machine learning applications</h2>
			<p>As data grows, so does the need<a id="_idIndexMarker037"/> for machine learning<a id="_idIndexMarker038"/> models to make use of more and more data. The general understanding in the machine learning community today is that the more data is provided to the models, the better the models will be. This resulted in the need for massive amounts of data to be given to a model for predictive analytics. When we deal with massive amounts of data, the challenges for training machine learning models become more complex than data processing. The reason is that machine learning models crunch data and run statistical estimations to come to a minimum error point. To get to that minimum error, the model must do complex mathematical operations such as matrix multiplication. These computations require large amounts of data to be available in memory and then computations to run on it. This serves as the case for parallel processing in <span class="No-Break">machine learning.</span></p>
			<p>Machine learning adds an element of prediction to products. Instead of reacting to the changes that have already taken place, we can proactively look for ways to improve our products and services based on historical data and trends. Every aspect of an organization can make use of machine learning for predictive analytics. Machine learning can be applied to a number of industries, from hospitals to retail stores to manufacturing organizations. All of us have encountered some kind of machine learning algorithms when we do tasks on the internet, such as online buying and selling, browsing and searching for websites, and using social media platforms. Machine learning has become a major part of our lives knowingly <span class="No-Break">or unknowingly.</span></p>
			<p>Although there’s a large number<a id="_idIndexMarker039"/> of use cases that an organization can make use of in terms of machine learning, I’m highlighting only a <span class="No-Break">few here:</span></p>
			<ul>
				<li><span class="No-Break">Personalized shopping</span></li>
				<li>Website searches <span class="No-Break">and ranking</span></li>
				<li>Fraud detection for banking <span class="No-Break">and insurance</span></li>
				<li>Customer <span class="No-Break">sentiment analysis</span></li>
				<li><span class="No-Break">Customer segmentation</span></li>
				<li><span class="No-Break">Recommendation engines</span></li>
				<li><span class="No-Break">Price optimization</span></li>
				<li>Predictive maintenance <span class="No-Break">and support</span></li>
				<li>Text and <span class="No-Break">video analytics</span></li>
				<li><span class="No-Break">Customer/patient 360</span></li>
			</ul>
			<p>Let’s move on to cover real-time <span class="No-Break">streaming next.</span></p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor044"/>Real-time streaming</h2>
			<p>Real-time streaming<a id="_idIndexMarker040"/> is one of the use cases where Spark<a id="_idIndexMarker041"/> really shines. There are very few competing frameworks that offer the flexibility that Spark <span class="No-Break">Streaming has.</span></p>
			<p>Spark Streaming provides a mechanism to ingest data from multiple streaming data sources, such as Kafka and Amazon Kinesis. Once the data is ingested, it can be processed in real time with very efficient Spark <span class="No-Break">Streaming processing.</span></p>
			<p>There are a large number of real-time use cases that can make use of Spark Streaming. Some of them are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><span class="No-Break">Self-driving cars</span></li>
				<li>Real-time reporting <span class="No-Break">and analysis</span></li>
				<li>Providing updates on stock <span class="No-Break">market data</span></li>
				<li>Internet of Things (IoT) data ingestion <span class="No-Break">and processing</span></li>
				<li>Real-time news <span class="No-Break">data processing</span></li>
				<li>Real-time analytics for optimization of inventory <span class="No-Break">and operations</span></li>
				<li>Real-time fraud detection systems for <span class="No-Break">credit cards</span></li>
				<li>Real-time <span class="No-Break">event detection</span></li>
				<li><span class="No-Break">Real-time recommendations</span></li>
			</ul>
			<p>Large global organizations make use of Spark Streaming to process billion and trillions of data rows in real time. We see some of this in action in our everyday life. Your credit card blocking a transaction while you’re out shopping is one such example of real-time fraud detection in action. Netflix and YouTube use real-time interactions, with the video platforms recommending users what to <span class="No-Break">watch next.</span></p>
			<p>As we move to a world of every device sending data back to its server for analysis, there’s an increased need for streaming and real-time analysis. One of the main advantages of using Spark Streaming for this kind of data is the built-in capabilities it has, for look-back and late processing of data. We discussed the usefulness of this approach earlier as well, and<a id="_idIndexMarker042"/> a lot of manual pipeline processing<a id="_idIndexMarker043"/> work is removed due to these capabilities. We will learn more about this when we discuss Spark Streaming in <a href="B19176_07.xhtml#_idTextAnchor183"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor045"/>Graph analytics</h2>
			<p>Graph analytics provides<a id="_idIndexMarker044"/> a unique way of looking at data<a id="_idIndexMarker045"/> by analyzing relationships between different entities. The vertices of a graph represent the entities and the edges of a graph represent the relationship between two entities. Think of your social network on Facebook or Instagram. You represent one entity, and the people you are connected to represent another entity. The relationship (connection) between you and your friends is the edge. Similarly, your interests on your social media could all be different edges. Then, there can be a location category, for which all people who belong to one location would have an edge (a relationship) with that location, and so on. Therefore, connections can be made with any different type of entities. The more connected you are, the higher the chance that you are connected to like-minded people or interests. This is one method of measuring relationships between different entities. There are several uses for these kinds of graphs. The beauty of Spark is the distributed processing of these graphs to find these relationships very quickly. There can be millions and billions of connections for billions of entities. Spark has the capability to distribute these workloads and compute complex algorithms <span class="No-Break">very fast.</span></p>
			<p>The following are some use cases<a id="_idIndexMarker046"/> of <span class="No-Break">graph analytics:</span></p>
			<ul>
				<li>Social <span class="No-Break">network analysis</span></li>
				<li><span class="No-Break">Fraud detection</span></li>
				<li>Page ranking based <span class="No-Break">on relevance</span></li>
				<li><span class="No-Break">Weather prediction</span></li>
				<li>Search <span class="No-Break">engine optimization</span></li>
				<li>Supply <span class="No-Break">chain analysis</span></li>
				<li>Finding influencers on <span class="No-Break">social media</span></li>
				<li>Money laundering<a id="_idIndexMarker047"/> and <span class="No-Break">fraud detection</span></li>
			</ul>
			<p>With a growing number of use cases<a id="_idIndexMarker048"/> of graph analytics, this proves to be a critical use case in the industry today where we need to analyze networks of relationships <span class="No-Break">among entities.</span></p>
			<p>In the next section, we’re going to discuss who the Spark users are and what their typical role is within <span class="No-Break">an organization.</span></p>
			<h1 id="_idParaDest-46"><a id="_idTextAnchor046"/>Who are the Spark users?</h1>
			<p>As the world moves toward<a id="_idIndexMarker049"/> data-driven decision-making approaches, the role of data and the different types of users who can leverage it for critical business decisions has become paramount. There are different types of users in data who can leverage Spark for different purposes. I will introduce some of those different users in this section. This is not an exhaustive list, but it should give you an idea of the different roles that exist in data-driven organizations today. However, as the industry grows, many more new roles are coming up that are similar to the ones present in the following sections, although each may have its own <span class="No-Break">separate role.</span></p>
			<p>We’ll start with the role of <span class="No-Break">data analysts.</span></p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor047"/>Data analysts</h2>
			<p>The more traditional role<a id="_idIndexMarker050"/> in data today<a id="_idIndexMarker051"/> is a data analyst. The data analyst is typically the first-tier role in data. What this means is that data analysts are at the core of decision making in organizations. This role spans across different business units in an organization, and oftentimes, data analysts have to interact with multiple business stakeholders to put across their requirements. This requires knowledge of the business domain as well as its processes. When an analyst has an understanding of the business and its goals, only then can they perform their duties best. Moreover, a lot of times, the requirement is to make current processes more efficient, which results in a better bottom line for the business. Therefore, having an understanding of not just the business goals but also how it all works together is one of the main requirements for <span class="No-Break">this role.</span></p>
			<p>A typical job role for a data analyst<a id="_idIndexMarker052"/> may look <span class="No-Break">as follows:</span></p>
			<ol>
				<li>When data analysts are given a project in an organization, the first step in the project is to gather requirements from multiple stakeholders. Let’s work with an example here. Say you joined an organization as a data analyst. This organization makes and sells computer hardware. You are given the task of reporting on the revenue each month for the last 10 years. The first step for you would be to gather all requirements. It is possible that some stakeholders want to know how many units of certain products are sold each month, while others may want to know whether the revenues are consistently growing or not. Remember, the end users of your reports might work in different business units of <span class="No-Break">the organization.</span></li>
				<li>Once you have all the requirements<a id="_idIndexMarker053"/> gathered from all the concerned stakeholders, then you move on to the next step, which is to look for the relevant data sources to answer the questions that you are tasked with. You may need to talk with database administrators in the organization or platform architects to know where the different data sources reside that have relevant information for you <span class="No-Break">to extract.</span></li>
				<li>Once you have all the relevant sources, then you want to connect with those sources programmatically (in most cases) and clean and join some data together to come up with relevant statistics, based on your requirements. This is where Spark would help you connect to these different data sources and also read and manipulate the data most efficiently. You also want to slice and dice the data based on your business requirements. Once the data is clean and statistics are generated, you want to generate some reports based on these statistics. There are different tools in the market to generate reports, such as Qlik and Tableau, that you can work with. Once the reports are generated, you may want to share your results with the stakeholders. You could present your results to them or share the reports with them, depending on what the preferred medium is. This will help stakeholders make informed business-critical decisions that are data-driven <span class="No-Break">in nature.</span></li>
			</ol>
			<p>Collaboration across different roles also plays an important role for data analysts. Since organizations have been collecting data for a long time, the most important thing is working with all the data that has been collected over the years and making sense of it, helping businesses with critical decision making. Helping with data-driven decision making is the key to being<a id="_idIndexMarker054"/> a successful <span class="No-Break">data analyst.</span></p>
			<p>Here’s a summary of the steps<a id="_idIndexMarker055"/> taken in a project, as discussed in the <span class="No-Break">previous paragraphs:</span></p>
			<ol>
				<li>Gather requirements <span class="No-Break">from stakeholders.</span></li>
				<li>Identify the relevant <span class="No-Break">data sources.</span></li>
				<li>Collaborate with subject matter <span class="No-Break">experts (SMEs).</span></li>
				<li>Slice and <span class="No-Break">dice data.</span></li>
				<li><span class="No-Break">Generate reports.</span></li>
				<li>Share <span class="No-Break">the results.</span></li>
			</ol>
			<p>Let’s look at data engineers next. This role is gaining a lot of traction in the <span class="No-Break">industry today.</span></p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor048"/>Data engineers</h2>
			<p>The next role<a id="_idIndexMarker056"/> that is getting<a id="_idIndexMarker057"/> more and more prevalent in the industry is a data engineer. This is a relatively new role but has gained immense popularity in recent times. The reason for this is that data is growing at tremendous levels. We have more data being generated per second now than in a whole month a few years ago. Working with all this data requires specialized skills. The data can no longer be contained in the modest memory of most computers, so we have to make use of the massive scale of cloud computing to serve this purpose. As data needs are becoming a lot more complex, we need complex architectures to process and use this data for business decision making. This is where the role of the data engineer comes into play. The main job of the data engineer is to prepare data for ingestion for different purposes. The downstream systems that leverage this prepared data could be dashboards that run reports based on this data, or it could be a predictive analytics solution that works with advanced machine learning algorithms to make proactive decisions based on <span class="No-Break">the data.</span></p>
			<p>More broadly, data engineers are responsible for creating, maintaining, optimizing, and monitoring data pipelines that serve different use cases<a id="_idIndexMarker058"/> in an organization. These pipelines are typically known as Extract, Transform, Load (ETL) pipelines. The major differentiator is the sheer scale of data that data engineers have to work with. When there are downstream needs for data for BI reporting, advanced analytics, and/or machine learning, that is where data pipelines come into play for <span class="No-Break">large projects.</span></p>
			<p>A typical job role for a data engineer<a id="_idIndexMarker059"/> in an organization may look <a id="_idIndexMarker060"/>as follows. When data engineers are given a task to create a data pipeline for a project, the first thing they need to consider is the overall architecture of an application. There might be data architects in some organizations to help with some of the architecture requirements, but that might not always be the case. So, a data engineer would ask questions such as <span class="No-Break">the following:</span></p>
			<ul>
				<li>What are the different sources <span class="No-Break">of data?</span></li>
				<li>What is the size of <span class="No-Break">the data?</span></li>
				<li>Where does the data <span class="No-Break">reside today?</span></li>
				<li>Do we need to migrate the data between <span class="No-Break">different tools?</span></li>
				<li>How do we connect to <span class="No-Break">the data?</span></li>
				<li>What kind of transformations are required for <span class="No-Break">the data?</span></li>
				<li>How often does the data <span class="No-Break">get updated?</span></li>
				<li>Should we expect a schema change in the <span class="No-Break">new data?</span></li>
				<li>How do we monitor the pipelines if there <span class="No-Break">are failures?</span></li>
				<li>Do we need to create a notification system <span class="No-Break">for failures?</span></li>
				<li>Do we need to add a retry mechanism <span class="No-Break">for failures?</span></li>
				<li>What is the timeout strategy <span class="No-Break">for failures?</span></li>
				<li>How do we run back-dated pipelines if there <span class="No-Break">are failures?</span></li>
				<li>How do we deal with <span class="No-Break">bad data?</span></li>
				<li>What strategy we should follow – ETL <span class="No-Break">or ELT?</span></li>
				<li>How can we save the costs <span class="No-Break">of computation?</span></li>
			</ul>
			<p>Once they have answers to these questions, then they start working on a resilient architecture to build data pipelines. Once those pipelines are run and tested, the next step is to maintain these pipelines and make the processing more efficient and visible for failure detection. The goal is to build these pipelines so that once everything is run, the end state of data is consistent for different downstream use cases. Too often, data engineers have to collaborate with data analysts and data scientists to come up with correct data<a id="_idIndexMarker061"/> transformation<a id="_idIndexMarker062"/> requirements, based on the required <span class="No-Break">use cases.</span></p>
			<p>Let’s talk about data scientists now, a job that has been advertised as “<em class="italic">the sexiest job of the 21st century</em>” on <span class="No-Break">multiple forums.</span></p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor049"/>Data scientists</h2>
			<p>Traditionally, data<a id="_idIndexMarker063"/> has been used<a id="_idIndexMarker064"/> for decision making based on what has happened in the past. This means that organizations have been reactive, based on the data. Now, there’s been a paradigm shift in advanced and predictive analytics. This means instead of being reactive, organizations can be proactive in their decision making. They achieve this with the help of all the data that is available to organizations now. To make effective use of this data, data scientists play a major part. They take analytics to the next level, where instead of just looking at what has happened in the past, they have sophisticated machine learning algorithms to predict what could take place in the future as well. All this is based on the huge amounts of data that is available <span class="No-Break">to them.</span></p>
			<p>A typical job role for a data scientist in an organization may look <span class="No-Break">as follows.</span></p>
			<p>The data scientist is given a problem to solve or a question to answer. The first task is to see what kind of data is available to them that would help them answer this question. They would create a few hypotheses to test with the given data. If the results are positive and the data is able to answer some of the problem statements, then they move on to experimenting with the data and seek ways to more effectively answer the questions at hand. For this purpose, they would join different datasets together, and they would also transform the data to make it ready for some machine learning algorithms to consume. At this stage, they would also need to decide what kind of machine learning problem they aim <span class="No-Break">to solve.</span></p>
			<p>There are three major types of machine learning techniques<a id="_idIndexMarker065"/> that they <span class="No-Break">can use:</span></p>
			<ul>
				<li><span class="No-Break">Regression</span></li>
				<li><span class="No-Break">Classification</span></li>
				<li><span class="No-Break">Clustering</span></li>
			</ul>
			<p>Based on the technique decided and data transformations, they would then move to prototype with a few machine learning algorithms to create a baseline model. A baseline model is a very basic model that serves to answer the original question. Based on this baseline model, other models can be created that would be able to answer the question better. In some cases, some predefined rules can also serve as a baseline model. What this means is that the business might already be operating on some predefined rules that can serve as a baseline to compare the machine learning model. Once the initial prototyping is done, then the data scientist moves on to more advanced optimizations in terms of models. They can work with different hyperparameters of the model or experiment with different data transformations and sample sizes. All of this can be done in Spark or other tools and languages, depending on their preference. Spark has the edge to run these algorithms in a parallel fashion, making the whole process very efficient. Once the data scientist is happy with the model results based on different metrics, they would then move that model to a production environment where these models can be served to customers solving specific problems. At this point, they would hand over these models to machine<a id="_idIndexMarker066"/> learning engineers to start incorporating<a id="_idIndexMarker067"/> them into <span class="No-Break">the pipelines.</span></p>
			<p>Here’s a summary of the steps taken in a project, as discussed in the <span class="No-Break">previous paragraph:</span></p>
			<ol>
				<li>Create and test <span class="No-Break">a hypothesis.</span></li>
				<li>Transform <span class="No-Break">the data.</span></li>
				<li>Decide on a machine <span class="No-Break">learning algorithm.</span></li>
				<li>Prototype with different machine <span class="No-Break">learning models.</span></li>
				<li>Create a <span class="No-Break">baseline model.</span></li>
				<li>Tune <span class="No-Break">the model.</span></li>
				<li>Tune <span class="No-Break">the data.</span></li>
				<li>Transition<a id="_idIndexMarker068"/> models<a id="_idIndexMarker069"/> <span class="No-Break">to production.</span></li>
			</ol>
			<p>Let’s discuss the role of machine learning <span class="No-Break">engineers next.</span></p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor050"/>Machine learning engineers</h2>
			<p>Like data engineers, machine learning<a id="_idIndexMarker070"/> engineers also build<a id="_idIndexMarker071"/> pipelines, but these pipelines are primarily built for machine learning model deployment. Machine learning engineers typically take prototyped models created by data scientists and build machine learning pipelines around them. We will discuss what machine learning pipelines are and what some of the questions that need to be answered to build these <span class="No-Break">pipelines are.</span></p>
			<p>Machine learning models are built to solve complex problems and provide advanced analytic methods to serve a business. After prototyping, these models need to run in the production environments of the organizations and be deployed to serve customers. For deployment, there are several considerations<a id="_idIndexMarker072"/> that need to be taken <span class="No-Break">into account:</span></p>
			<ul>
				<li>How much data is there for <span class="No-Break">model training?</span></li>
				<li>How many customers do we plan to <span class="No-Break">serve concurrently?</span></li>
				<li>How often do we need to retrain <span class="No-Break">the models?</span></li>
				<li>How often do we expect the data <span class="No-Break">to change?</span></li>
				<li>How do we scale the pipeline up and down based <span class="No-Break">on demand?</span></li>
				<li>How do we monitor failures in <span class="No-Break">model training?</span></li>
				<li>Do we need notifications <span class="No-Break">for failures?</span></li>
				<li>Do we need to add a retry mechanism <span class="No-Break">for failures?</span></li>
				<li>What is the timeout strategy <span class="No-Break">for failures?</span></li>
				<li>How do we measure model performance <span class="No-Break">in production?</span></li>
				<li>How do we tackle <span class="No-Break">data drift?</span></li>
				<li>How do we tackle<a id="_idIndexMarker073"/> <span class="No-Break">model drift?</span></li>
			</ul>
			<p>Once these questions are answered, the next step is to build a pipeline around these models. The main purpose of the pipeline would be such that when new data comes in, the pre-trained models are able to answer questions based on <span class="No-Break">new datasets.</span></p>
			<p>Let’s use an example to better<a id="_idIndexMarker074"/> understand these pipelines. We’ll continue with the first example of an organization selling <span class="No-Break">computer hardware:</span></p>
			<ol>
				<li>Suppose the organization wants to build a recommender system on its website that recommends to users which products <span class="No-Break">to buy.</span></li>
				<li>The data scientists have built a prototype model that works well with test data. Now, they want to deploy it <span class="No-Break">to production.</span></li>
				<li>To deploy this model, the machine learning engineers would have to see how they can incorporate this model on <span class="No-Break">the website.</span></li>
				<li>They would start by getting the data ingested from the website to get the <span class="No-Break">user information.</span></li>
				<li>Once they have the information, they pass it through the data pipeline to clean and join <span class="No-Break">the data.</span></li>
				<li>They might also want to add some precomputed features to the model, such as the time of the year, to get a better idea of whether it’s a holiday season and some special deals are <span class="No-Break">going on.</span></li>
				<li>Then, they would need a REST API endpoint to get the latest recommendations for each user on <span class="No-Break">the website.</span></li>
				<li>After that, the website needs to be connected to the REST endpoint to serve the <span class="No-Break">actual customers.</span></li>
				<li>Once these models are deployed on live systems (the website, in our example), there needs to be a monitoring system<a id="_idIndexMarker075"/> for any errors and changes in either<a id="_idIndexMarker076"/> the model<a id="_idIndexMarker077"/> or the data. This is known as <strong class="bold">model drift</strong> and <strong class="bold">data </strong><span class="No-Break"><strong class="bold">drift</strong></span><span class="No-Break">, respectively.</span></li>
			</ol>
			<h3>Data drift</h3>
			<p>Data may change over time. In our example, people’s preferences may change with time, or with seasonality, data may be different. For example, during a holiday season, people’s preferences might slightly change because they are looking to get presents for their friends and family, so recommending relevant products based on these preferences is of paramount importance for a business. Monitoring these trends and changes in the data would result<a id="_idIndexMarker078"/> in better models over time and would ultimately benefit <span class="No-Break">the business.</span></p>
			<h3>Model drift</h3>
			<p>Similar to data drift, we also <a id="_idIndexMarker079"/>have the concept of model drift. This means that the model changes over time, and the old model that was initially built is not the most performant in terms of recommending items to website visitors. With changing data, the model also needs to be updated from time to time. To get a sense of when a model needs to be updated, we need to have monitoring in place for models as well. This monitoring would constantly compare old model results with the new data and see whether model performance is degrading. If that’s the case, it’s time to update <span class="No-Break">the model.</span></p>
			<p>This whole life cycle of model deployment is typically the responsibility of machine learning engineers. Note that the process would slightly vary for different problems, but the overall idea remains <span class="No-Break">the same.</span></p>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor051"/>Summary</h1>
			<p>In this chapter, we learned about the basics of Apache Spark and why Spark is becoming a lot more prevalent in the industry for big data applications. We also learned about the different components of Spark and how these components are helpful in terms of application development. Then, we discussed the different roles that are present in the industry today and who can make use of Spark’s capabilities. Finally, we discussed the modern-day uses of Spark in different industry <span class="No-Break">use cases.</span></p>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor052"/>Sample questions</h1>
			<p>Although these questions are not part of the Spark certification, it’s good to answer these to assess your understanding of the basics <span class="No-Break">of Spark:</span></p>
			<ol>
				<li>What are the core components <span class="No-Break">of Spark?</span></li>
				<li>When do we want to use <span class="No-Break">Spark Streaming?</span></li>
				<li>What is a look-back mechanism in <span class="No-Break">Spark Streaming?</span></li>
				<li>What are some good use cases <span class="No-Break">for Spark?</span></li>
				<li>Which roles in an organization should <span class="No-Break">use Spark?</span></li>
			</ol>
		</div>
	</body></html>