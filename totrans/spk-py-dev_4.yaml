- en: Chapter 4. Learning from Data Using Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章 使用Spark从数据中学习
- en: As we have laid the foundation for data to be harvested in the previous chapter,
    we are now ready to learn from the data. Machine learning is about drawing insights
    from data. Our objective is to give an overview of the Spark **MLlib** (short
    for **Machine Learning library**) and apply the appropriate algorithms to our
    dataset in order to derive insights. From the Twitter dataset, we will be applying
    an unsupervised clustering algorithm in order to distinguish between Apache Spark-relevant
    tweets versus the rest. We have as initial input a mixed bag of tweets. We first
    need to preprocess the data in order to extract the relevant features, then apply
    the machine learning algorithm to our dataset, and finally evaluate the results
    and the performance of our model.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们已经为数据收集奠定了基础，现在我们准备好从数据中学习。机器学习是从数据中提取洞察力。我们的目标是概述Spark **MLlib**（简称**机器学习库**），并将适当的算法应用于我们的数据集以获得洞察力。从Twitter数据集中，我们将应用无监督聚类算法来区分Apache
    Spark相关的推文与其他推文。我们的初始输入是一堆混合的推文。我们首先需要预处理数据以提取相关特征，然后应用机器学习算法到我们的数据集，最后评估结果和模型的表现。
- en: 'In this chapter, we will cover the following points:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Providing an overview of the Spark MLlib module with its algorithms and the
    typical machine learning workflow.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概述Spark MLlib模块及其算法和典型的机器学习工作流程。
- en: Preprocessing the Twitter harvested dataset to extract the relevant features,
    applying an unsupervised clustering algorithm to identify *Apache Spark*-relevant
    tweets. Then, evaluating the model and the results obtained.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对从Twitter收集的数据集进行预处理以提取相关特征，应用无监督聚类算法来识别*Apache Spark*相关的推文。然后，评估模型和获得的结果。
- en: Describing the Spark machine learning pipeline.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述Spark机器学习管道。
- en: Contextualizing Spark MLlib in the app architecture
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在应用架构中定位Spark MLlib
- en: Let's first contextualize the focus of this chapter on data-intensive app architecture.
    We will concentrate our attention on the analytics layer and more precisely machine
    learning. This will serve as a foundation for streaming apps as we want to apply
    the learning from the batch processing of data as inference rules for the streaming
    analysis.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先明确本章的重点是数据密集型应用架构。我们将集中关注分析层，更具体地说，是机器学习。这将为流式应用奠定基础，因为我们希望将数据批处理中的学习应用于流式分析的推理规则。
- en: The following diagram sets the context of the chapter's focus, highlighting
    the machine learning module within the analytics layer while using tools for exploratory
    data analysis, Spark SQL, and Pandas.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表设置了本章重点的背景，突出了分析层中的机器学习模块，同时使用探索性数据分析、Spark SQL和Pandas等工具。
- en: '![Contextualizing Spark MLlib in the app architecture](img/B03986_04_01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![在应用架构中定位Spark MLlib](img/B03986_04_01.jpg)'
- en: Classifying Spark MLlib algorithms
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类Spark MLlib算法
- en: Spark MLlib is a rapidly evolving module of Spark with new algorithms added
    with each release of Spark.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib是Spark的一个快速发展的模块，随着Spark的每次发布都会添加新的算法。
- en: 'The following diagram provides a high-level overview of Spark MLlib algorithms
    grouped in the traditional broad machine learning techniques and following the
    categorical or continuous nature of the data:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表提供了Spark MLlib算法的高级概述，这些算法按照传统的广泛机器学习技术和数据的分类或连续性进行分组：
- en: '![Classifying Spark MLlib algorithms](img/B03986_04_02.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![分类Spark MLlib算法](img/B03986_04_02.jpg)'
- en: We categorize the Spark MLlib algorithms in two columns, categorical or continuous,
    depending on the type of data. We distinguish between data that is categorical
    or more qualitative in nature versus continuous data, which is quantitative in
    nature. An example of qualitative data is predicting the weather; given the atmospheric
    pressure, the temperature, and the presence and type of clouds, the weather will
    be sunny, dry, rainy, or overcast. These are discrete values. On the other hand,
    let's say we want to predict house prices, given the location, square meterage,
    and the number of beds; the real estate value can be predicted using linear regression.
    In this case, we are talking about continuous or quantitative values.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据数据类型将Spark MLlib算法分为两列，分类或连续。我们区分数据是分类的或更具有定性性质的数据与连续数据，后者具有定量性质。定性数据的例子是预测天气；给定大气压力、温度以及云的存在和类型，天气将是晴朗、干燥、雨天或阴天。这些都是离散值。另一方面，假设我们想要预测房价，给定位置、面积和床的数量；可以使用线性回归预测房地产价值。在这种情况下，我们谈论的是连续或定量值。
- en: The horizontal grouping reflects the types of machine learning method used.
    Unsupervised versus supervised machine learning techniques are dependent on whether
    the training data is labeled. In an unsupervised learning challenge, no labels
    are given to the learning algorithm. The goal is to find the hidden structure
    in its input. In the case of supervised learning, the data is labeled. The focus
    is on making predictions using regression if the data is continuous or classification
    if the data is categorical.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 水平分组反映了所使用的机器学习方法的类型。无监督与监督机器学习技术取决于训练数据是否标记。在无监督学习挑战中，不会向学习算法提供标签。目标是找到其输入中的隐藏结构。在监督学习的情况下，数据是标记的。重点是使用回归进行预测，如果数据是连续的，或者使用分类，如果数据是分类的。
- en: An important category of machine learning is recommender systems, which leverage
    collaborative filtering techniques. The Amazon web store and Netflix have very
    powerful recommender systems powering their recommendations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的一个重要类别是推荐系统，它利用协同过滤技术。亚马逊网络商店和Netflix拥有非常强大的推荐系统，为他们的推荐提供支持。
- en: '**Stochastic Gradient Descent** is one of the machine learning optimization
    techniques that is well suited for Spark distributed computation.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机梯度下降**是适用于Spark分布式计算的一种机器学习优化技术。'
- en: For processing large amounts of text, Spark offers crucial libraries for feature
    extraction and transformation such as **TF-IDF** (short for **Term Frequency –
    Inverse Document Frequency**), Word2Vec, standard scaler, and normalizer.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于处理大量文本，Spark提供了关键库用于特征提取和转换，如**TF-IDF**（代表**词频-逆文档频率**），Word2Vec，标准缩放器和归一化器。
- en: Supervised and unsupervised learning
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习
- en: We delve more deeply here in to the traditional machine learning algorithms
    offered by Spark MLlib. We distinguish between supervised and unsupervised learning
    depending on whether the data is labeled. We distinguish between categorical or
    continuous depending on whether the data is discrete or continuous.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里更深入地探讨Spark MLlib提供的传统机器学习算法。我们根据数据是否标记来区分监督学习和无监督学习。我们根据数据是否离散或连续来区分分类或连续。
- en: 'The following diagram explains the Spark MLlib supervised and unsupervised
    machine learning algorithms and preprocessing techniques:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表解释了Spark MLlib的监督学习和无监督机器学习算法以及预处理技术：
- en: '![Supervised and unsupervised learning](img/B03986_04_03.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![监督学习和无监督学习](img/B03986_04_03.jpg)'
- en: 'The following supervised and unsupervised MLlib algorithms and preprocessing
    techniques are currently available in Spark:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中目前可用的以下监督学习和无监督学习算法以及预处理技术：
- en: '**Clustering**: This is an unsupervised machine learning technique where the
    data is not labeled. The aim is to extract structure from the data:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：这是一种无监督机器学习技术，其中数据没有标记。目标是提取数据中的结构：'
- en: '**K-Means**: This partitions the data in K distinct clusters'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K-Means**：这种算法将数据划分为K个不同的簇'
- en: '**Gaussian Mixture**: Clusters are assigned based on the maximum posterior
    probability of the component'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高斯混合**：簇是根据组件的最大后验概率分配的'
- en: '**Power Iteration Clustering (PIC)**: This groups vertices of a graph based
    on pairwise edge similarities'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幂迭代聚类（PIC）**：这种算法根据成对边的相似性对图的顶点进行分组'
- en: '**Latent Dirichlet Allocation** (**LDA**): This is used to group collections
    of text documents into topics'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）：这用于将文本文档集合分组到主题中。'
- en: '**Streaming K-Means**: This means clusters dynamically streaming data using
    a windowing function on the incoming data'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流 K-Means**：这意味着使用输入数据的窗口函数动态地流式传输数据的聚类。'
- en: '**Dimensionality Reduction**: This aims to reduce the number of features under
    consideration. Essentially, this reduces noise in the data and focuses on the
    key features:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：这旨在减少考虑的特征数量。本质上，这减少了数据中的噪声并关注关键特征：'
- en: '**Singular Value Decomposition** (**SVD**): This breaks the matrix that contains
    the data into simpler meaningful pieces. It factorizes the initial matrix into
    three matrices.'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奇异值分解**（**SVD**）：这将包含数据的矩阵分解成更简单、更有意义的部分。它将初始矩阵分解成三个矩阵。'
- en: '**Principal Component Analysis** (**PCA**): This approximates a high dimensional
    dataset with a low dimensional sub space.'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）：这通过低维子空间近似高维数据集。'
- en: '**Regression and Classification**: Regression predicts output values using
    labeled training data, while Classification groups the results into classes. Classification
    has dependent variables that are categorical or unordered whilst Regression has
    dependent variables that are continuous and ordered:'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归和分类**：回归通过使用标记的训练数据来预测输出值，而分类将结果分组到类别中。分类的因变量是分类的或无序的，而回归的因变量是连续的且有顺序的：'
- en: '**Linear Regression Models** (linear regression, logistic regression, and support
    vector machines): Linear regression algorithms can be expressed as convex optimization
    problems that aim to minimize an objective function based on a vector of weight
    variables. The objective function controls the complexity of the model through
    the regularized part of the function and the error of the model through the loss
    part of the function.'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性回归模型**（线性回归、逻辑回归和支持向量机）：线性回归算法可以表示为凸优化问题，该问题旨在基于权重变量的向量最小化目标函数。目标函数通过函数的正则化部分控制模型的复杂性，通过函数的损失部分控制模型的误差。'
- en: '**Naive Bayes**: This makes predictions based on the conditional probability
    distribution of a label given an observation. It assumes that features are mutually
    independent of each other.'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯**：这基于给定观察的标签的条件概率分布进行预测。它假设特征之间相互独立。'
- en: '**Decision Trees**: This performs recursive binary partitioning of the feature
    space. The information gain at the tree node level is maximized in order to determine
    the best split for the partition.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决策树**：这执行特征空间的递归二分分区。为了确定最佳的分割，树节点级别的信息增益被最大化。'
- en: '**Ensembles of trees** (Random Forests and Gradient-Boosted Trees): Tree ensemble
    algorithms combine base decision tree models in order to build a performant model.
    They are intuitive and very successful for classification and regression tasks.'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树集成**（随机森林和梯度提升树）：树集成算法通过组合基础决策树模型来构建一个性能良好的模型。它们直观且在分类和回归任务中非常成功。'
- en: '**Isotonic Regression**: This minimizes the mean squared error between given
    data and observed responses.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**等距回归**：这最小化给定数据和观察到的响应之间的均方误差。'
- en: Additional learning algorithms
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外的学习算法
- en: 'Spark MLlib offers more algorithms than the supervised and unsupervised learning
    ones. We have broadly three more additional types of machine learning methods:
    recommender systems, optimization algorithms, and feature extraction.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib 提供的算法比监督学习和无监督学习算法更多。我们广泛地有三种额外的机器学习类型：推荐系统、优化算法和特征提取。
- en: '![Additional learning algorithms](img/B03986_04_04.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![额外的学习算法](img/B03986_04_04.jpg)'
- en: 'The following additional MLlib algorithms are currently available in Spark:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在 Spark 中当前可用的其他 MLlib 算法：
- en: '**Collaborative filtering**: This is the basis for recommender systems. It
    creates a user-item association matrix and aims to fill the gaps. Based on other
    users and items along with their ratings, it recommends an item that the target
    user has no ratings for. In distributed computing, one of the most successful
    algorithms is **ALS** (short for **Alternating Least Square**):'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协同过滤**：这是推荐系统的基础。它创建一个用户-项目关联矩阵并试图填补空白。基于其他用户和项目及其评分，它推荐一个目标用户没有评分的项目。在分布式计算中，最成功的算法之一是
    **ALS**（代表 **交替最小二乘**）：'
- en: '**Alternating Least Squares**: This matrix factorization technique incorporates
    implicit feedback, temporal effects, and confidence levels. It decomposes the
    large user item matrix into a lower dimensional user and item factors. It minimizes
    a quadratic loss function by fixing alternatively its factors.'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交替最小二乘法**：这种矩阵分解技术结合了隐式反馈、时间效应和置信水平。它将大的用户-项目矩阵分解为低维度的用户和项目因子。通过交替固定其因子来最小化二次损失函数。'
- en: '**Feature extraction and transformation**: These are essential techniques for
    large text document processing. It includes the following techniques:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取和转换**：这些是大型文本文档处理的基本技术。它包括以下技术：'
- en: '**Term Frequency**: Search engines use TF-IDF to score and rank document relevance
    in a vast corpus. It is also used in machine learning to determine the importance
    of a word in a document or corpus. Term frequency statistically determines the
    weight of a term relative to its frequency in the corpus. Term frequency on its
    own can be misleading as it overemphasizes words such as *the*, *of*, or *and*
    that give little information. Inverse Document Frequency provides the specificity
    or the measure of the amount of information, whether the term is rare or common
    across all documents in the corpus.'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词频**：搜索引擎使用 TF-IDF 对大量语料库中的文档相关性进行评分和排名。它也用于机器学习中确定文档或语料库中单词的重要性。词频统计上确定了术语相对于其在语料库中的频率的权重。仅词频本身可能会误导，因为它过分强调了如
    *the*、*of* 或 *and* 这样的单词，这些单词提供的信息很少。逆文档频率提供了特异性或信息量的度量，无论术语在语料库的所有文档中是稀有还是常见。'
- en: '**Word2Vec**: This includes two models, **Skip-Gram** and **Continuous Bag
    of Word**. The Skip-Gram predicts neighboring words given a word, based on sliding
    windows of words, while Continuous Bag of Words predicts the current word given
    the neighboring words.'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Word2Vec**：这包括两个模型，**Skip-Gram** 和 **连续词袋模型**。Skip-Gram 根据滑动窗口中的单词预测给定单词的邻近单词，而连续词袋模型根据邻近单词预测当前单词。'
- en: '**Standard Scaler**: As part of preprocessing, the dataset must often be standardized
    by mean removal and variance scaling. We compute the mean and standard deviation
    on the training data and apply the same transformation to the test data.'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准缩放器**：作为预处理的一部分，数据集通常需要通过均值移除和方差缩放进行标准化。我们在训练数据上计算均值和标准差，并将相同的转换应用于测试数据。'
- en: '**Normalizer**: We scale the samples to have unit norm. It is useful for quadratic
    forms such as the dot product or kernel methods.'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归一化器**：我们将样本缩放到具有单位范数。对于二次形式，如点积或核方法，它很有用。'
- en: '**Feature selection**: This reduces the dimensionality of the vector space
    by selecting the most relevant features for the model.'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择**：这通过选择模型中最相关的特征来降低向量空间的维度。'
- en: '**Chi-Square Selector**: This is a statistical method to measure the independence
    of two events.'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卡方选择器**：这是一种统计方法，用于衡量两个事件之间的独立性。'
- en: '**Optimization**: These specific Spark MLlib optimization algorithms focus
    on various techniques of gradient descent. Spark provides very efficient implementation
    of gradient descent on a distributed cluster of machines. It looks for the local
    minima by iteratively going down the steepest descent. It is compute-intensive
    as it iterates through all the data available:'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化**：这些特定的 Spark MLlib 优化算法专注于梯度下降的各种技术。Spark 在分布式机器集群上提供了非常高效的梯度下降实现。它通过迭代地沿着最陡下降方向寻找局部最小值。由于它迭代通过所有可用的数据，因此它计算密集型：'
- en: '**Stochastic Gradient Descent**: We minimize an objective function that is
    the sum of differentiable functions. Stochastic Gradient Descent uses only a sample
    of the training data in order to update a parameter in a particular iteration.
    It is used for large-scale and sparse machine learning problems such as text classification.'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降**：我们最小化一个目标函数，该函数是不同可微函数的总和。随机梯度下降在特定迭代中仅使用训练数据的一个样本来更新参数。它用于大规模和稀疏机器学习问题，如文本分类。'
- en: '**Limited-memory BFGS** (**L-BFGS**): As the name says, L-BFGS uses limited
    memory and suits the distributed optimization algorithm implementation of Spark
    MLlib.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限内存 BFGS**（**L-BFGS**）：正如其名，L-BFGS 使用有限的内存，适合 Spark MLlib 的分布式优化算法实现。'
- en: Spark MLlib data types
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark MLlib 数据类型
- en: 'MLlib supports four essential data types: **local vector**, **labeled point**,
    **local matrix**, and **distributed matrix**. These data types are widely used
    in Spark MLlib algorithms:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 支持四种基本数据类型：**本地向量**、**标记点**、**本地矩阵**和**分布式矩阵**。这些数据类型在 Spark MLlib 算法中得到了广泛的应用：
- en: '**Local vector**: This resides in a single machine. It can be dense or sparse:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地向量**: 这位于单个机器上。它可以密集或稀疏：'
- en: Dense vector is a traditional array of doubles. An example of dense vector is
    `[5.0, 0.0, 1.0, 7.0]`.
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密集向量是一个传统的双精度数组。一个密集向量的例子是 `[5.0, 0.0, 1.0, 7.0]`。
- en: Sparse vector uses integer indices and double values. So the sparse representation
    of the vector `[5.0, 0.0, 1.0, 7.0]` would be `(4, [0, 2, 3], [5.0, 1.0, 7.0])`,
    where represent the dimension of the vector.
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏向量使用整数索引和双精度值。因此，向量 `[5.0, 0.0, 1.0, 7.0]` 的稀疏表示将是 `(4, [0, 2, 3], [5.0, 1.0,
    7.0])`，其中 `4` 表示向量的维度。
- en: 'Here''s an example of local vector in PySpark:'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里是PySpark中本地向量的示例：
- en: '[PRE0]'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Labeled point**. A labeled point is a dense or sparse vector with a label
    used in supervised learning. In the case of binary labels, 0.0 represents the
    negative label whilst 1.0 represents the positive value.'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记点**: 标记点是一个密集或稀疏向量，带有用于监督学习的标签。在二进制标签的情况下，0.0表示负标签，而1.0表示正值。'
- en: 'Here''s an example of a labeled point in PySpark:'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里是PySpark中一个标记点的示例：
- en: '[PRE1]'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Local Matrix**: This local matrix resides in a single machine with integer-type
    indices and values of type double.'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地矩阵**: 这个本地矩阵位于单个机器上，具有整数类型的索引和双精度类型的值。'
- en: 'Here''s an example of a local matrix in PySpark:'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里是PySpark中一个本地矩阵的示例：
- en: '[PRE2]'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Distributed Matrix**: Leveraging the distributed mature of the RDD, distributed
    matrices can be shared in a cluster of machines. We distinguish four distributed
    matrix types: `RowMatrix`, `IndexedRowMatrix`, `CoordinateMatrix`, and `BlockMatrix`:'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式矩阵**: 利用RDD的分布式特性，分布式矩阵可以在机器集群中共享。我们区分四种分布式矩阵类型：`RowMatrix`、`IndexedRowMatrix`、`CoordinateMatrix`和`BlockMatrix`：'
- en: '`RowMatrix`: This takes an RDD of vectors and creates a distributed matrix
    of rows with meaningless indices, called `RowMatrix`, from the RDD of vectors.'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RowMatrix`: 这需要一个向量的RDD，并从向量的RDD创建一个无意义的索引的行分布式矩阵，称为`RowMatrix`。'
- en: '`IndexedRowMatrix`: In this case, row indices are meaningful. First, we create
    an RDD of indexed rows using the class `IndexedRow` and then create an `IndexedRowMatrix`.'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IndexedRowMatrix`: 在这种情况下，行索引是有意义的。首先，我们使用`IndexedRow`类创建一个索引行的RDD，然后创建一个`IndexedRowMatrix`。'
- en: '`CoordinateMatrix`: This is useful to represent very large and very sparse
    matrices. `CoordinateMatrix` is created from RDDs of the `MatrixEntry` points,
    represented by a tuple of type (long, long, or float)'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`坐标矩阵`: 这对于表示非常大且非常稀疏的矩阵非常有用。`坐标矩阵`是由`MatrixEntry`点的RDD创建的，这些点由一个类型为（long，long或float）的元组表示。'
- en: '`BlockMatrix`: These are created from RDDs of sub-matrix blocks, where a sub-matrix
    block is `((blockRowIndex, blockColIndex), sub-matrix)`.'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`块矩阵`: 这些矩阵是由子矩阵块RDD创建的，其中子矩阵块是 `((块行索引, 块列索引), 子矩阵)`。'
- en: Machine learning workflows and data flows
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习工作流程和数据流
- en: Beyond algorithms, machine learning is also about processes. We will discuss
    the typical workflows and data flows of supervised and unsupervised machine learning.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 除了算法之外，机器学习还涉及过程。我们将讨论监督式和未监督式机器学习的典型工作流程和数据流。
- en: Supervised machine learning workflows
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督式机器学习工作流程
- en: In supervised machine learning, the input training dataset is labeled. One of
    the key data practices is to split input data into training and test sets, and
    validate the mode accordingly.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督式机器学习中，输入训练数据集是标记的。关键数据实践之一是将输入数据分为训练集和测试集，并相应地验证模型。
- en: 'We typically go through a six-step process flow in supervised learning:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督式学习中，我们通常经历六个步骤的过程流：
- en: '**Collect the data**: This step essentially ties in with the previous chapter
    and ensures we collect the right data with the right volume and granularity in
    order to enable the machine learning algorithm to provide reliable answers.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收集数据**: 这一步实际上与上一章紧密相关，并确保我们收集到正确数量和粒度的数据，以便使机器学习算法能够提供可靠的答案。'
- en: '**Preprocess the data**: This step is about checking the data quality by sampling,
    filling in the missing values if any, scaling and normalizing the data. We also
    define the feature extraction process. Typically, in the case of large text-based
    datasets, we apply tokenization, stop words removal, stemming, and TF-IDF.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理数据**: 这一步是通过对数据进行抽样、填补缺失值（如果有）、缩放和归一化来检查数据质量。我们还定义了特征提取过程。通常，在大型基于文本的数据集的情况下，我们应用分词、去除停用词、词干提取和TF-IDF。'
- en: In the case of supervised learning, we separate the input data into a training
    and test set. We can also implement various strategies of sampling and splitting
    the dataset for cross-validation purposes.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在监督学习的情况下，我们将输入数据分为训练集和测试集。我们还可以实施各种采样和分割数据集的策略，用于交叉验证。
- en: '**Ready the data**: In this step, we get the data in the format or data type
    expected by the algorithms. In the case of Spark MLlib, this includes local vector,
    dense or sparse vectors, labeled points, local matrix, distributed matrix with
    row matrix, indexed row matrix, coordinate matrix, and block matrix.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准备数据**：在这一步，我们获取算法期望的格式或数据类型的数据。在Spark MLlib的情况下，这包括本地向量、密集或稀疏向量、标记点、本地矩阵、带有行矩阵的分布式矩阵、索引行矩阵、坐标矩阵和块矩阵。'
- en: '**Model**: In this step, we apply the algorithms that are suitable for the
    problem at hand and get the results for evaluation of the most suitable algorithm
    in the evaluate step. We might have multiple algorithms suitable for the problem;
    their respective performance will be scored in the evaluate step to select the
    best preforming ones. We can implement an ensemble or combination of models in
    order to reach the best results.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：在这一步，我们应用适合当前问题的算法，并获取评估步骤中最适合算法的结果。我们可能有多种适合问题的算法；它们各自的表现将在评估步骤中评分，以选择最佳表现者。我们可以实现集成或模型组合，以达到最佳结果。'
- en: '**Optimize**: We may need to run a grid search for the optimal parameters of
    certain algorithms. These parameters are determined during training, and fine-tuned
    during the testing and production phase.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化**：我们可能需要运行网格搜索以确定某些算法的最佳参数。这些参数在训练期间确定，并在测试和生产阶段进行微调。'
- en: '**Evaluate**: We ultimately score the models and select the best one in terms
    of accuracy, performance, reliability, and scalability. We move the best performing
    model to test with the held out test data in order to ascertain the prediction
    accuracy of our model. Once satisfied with the fine-tuned model, we move it to
    production to process live data.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估**：我们最终对模型进行评分，并选择在准确性、性能、可靠性和可扩展性方面最佳的模型。我们将最佳性能的模型移动到测试中，以测试保留的测试数据，以确定我们模型的预测准确性。一旦对微调后的模型满意，我们将将其移动到生产中处理实时数据。'
- en: 'The supervised machine learning workflow and dataflow are represented in the
    following diagram:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习工作流程和数据流如下图所示：
- en: '![Supervised machine learning workflows](img/B03986_04_05.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![监督机器学习工作流程](img/B03986_04_05.jpg)'
- en: Unsupervised machine learning workflows
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督机器学习工作流程
- en: As opposed to supervised learning, our initial data is not labeled in the case
    of unsupervised learning, which is most often the case in real life. We will extract
    the structure from the data by using clustering or dimensionality reduction algorithms.
    In the unsupervised learning case, we do not split the data into training and
    test, as we cannot make any prediction because the data is not labeled. We will
    train the data along six steps similar to those in supervised learning. Once the
    model is trained, we will evaluate the results and fine-tune the model and then
    release it for production.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习不同，在无监督学习中，我们的初始数据没有标记，这在现实生活中是最常见的情况。我们将通过使用聚类或降维算法从数据中提取结构。在无监督学习的情况下，我们不会将数据分为训练集和测试集，因为我们无法进行任何预测，因为数据没有标记。我们将按照与监督学习类似的六个步骤训练数据。一旦模型训练完成，我们将评估结果并微调模型，然后将其发布到生产环境中。
- en: Unsupervised learning can be a preliminary step to supervised learning. Namely,
    we look at reducing the dimensionality of the data prior to attacking the learning
    phase.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习可以是监督学习的一个初步步骤。也就是说，我们在攻击学习阶段之前查看减少数据的维度。
- en: 'The unsupervised machine learning workflows and dataflow are represented as
    follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督机器学习的工作流程和数据流如下所示：
- en: '![Unsupervised machine learning workflows](img/B03986_04_06.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![无监督机器学习工作流程](img/B03986_04_06.jpg)'
- en: Clustering the Twitter dataset
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对Twitter数据集进行聚类
- en: 'Let''s first get a feel for the data extracted from Twitter and get an understanding
    of the data structure in order to prepare and run it through the K-Means clustering
    algorithms. Our plan of attack uses the process and dataflow depicted earlier
    for unsupervised learning. The steps are as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先了解一下从Twitter提取的数据，并了解数据结构，以便准备并运行它通过K-Means聚类算法。我们的攻击计划使用之前用于无监督学习的过程和数据流。步骤如下：
- en: Combine all tweet files into a single dataframe.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有推文文件合并到一个单独的数据框中。
- en: Parse the tweets, remove stop words, extract emoticons, extract URL, and finally
    normalize the words (for example, mapping them to lowercase and removing punctuation
    and numbers).
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析推文，删除停用词，提取表情符号，提取 URL，最后规范化单词（例如，将它们映射为小写并删除标点符号和数字）。
- en: 'Feature extraction includes the following:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征提取包括以下内容：
- en: '**Tokenization**: This breaks down the parsed tweet text into individual words
    or tokens'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词**：这会将解析的推文文本分解成单个单词或标记'
- en: '**TF-IDF**: This applies the TF-IDF algorithm to create feature vectors from
    the tokenized tweet texts'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TF-IDF**：这将对分词后的推文文本应用 TF-IDF 算法以创建特征向量'
- en: '**Hash TF-IDF**: This applies a hashing function to the token vectors'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**哈希 TF-IDF**：这将对标记向量应用哈希函数'
- en: Run the K-Means clustering algorithm.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 K-Means 聚类算法。
- en: 'Evaluate the results of the K-Means clustering:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估 K-Means 聚类结果：
- en: Identify tweet membership to clusters
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别推文所属的聚类
- en: Perform dimensionality reduction to two dimensions with the Multi-Dimensional
    Scaling or the Principal Component Analysis algorithm
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多维缩放或多维主成分分析算法将维度降低到二维
- en: Plot the clusters
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制聚类
- en: 'Pipeline:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管道：
- en: Fine-tune the number of relevant clusters K
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调相关聚类数 K
- en: Measure the model cost
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量模型成本
- en: Select the optimal model
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择最佳模型
- en: Applying Scikit-Learn on the Twitter dataset
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Twitter 数据集上应用 Scikit-Learn
- en: Python's own Scikit-Learn machine learning library is one of the most reliable,
    intuitive, and robust tools around. Let's run through a preprocessing and unsupervised
    learning using Pandas and Scikit-Learn. It is often beneficial to explore a sample
    of the data using Scikit-Learn before spinning off clusters with Spark MLlib.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Python 自带的 Scikit-Learn 机器学习库是最可靠、直观和健壮的工具之一。让我们通过 Pandas 和 Scikit-Learn 来运行预处理和无监督学习。在用
    Spark MLlib 分离聚类之前，使用 Scikit-Learn 探索数据样本通常是有益的。
- en: 'We have a mixed bag of 7,540 tweets. It contains tweets related to Apache Spark,
    Python, the upcoming presidential election with Hillary Clinton and Donald Trump
    as protagonists, and some tweets related to fashion and music with Lady Gaga and
    Justin Bieber. We are running the K-Means clustering algorithm using Python Scikit-Learn
    on the Twitter dataset harvested. We first load the sample data into a Pandas
    dataframe:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有 7,540 条推文的混合体。它包含与 Apache Spark、Python、即将到来的总统选举（希拉里·克林顿和唐纳德·特朗普为主角）相关的推文，以及一些与时尚和音乐（Lady
    Gaga 和贾斯汀·比伯）相关的推文。我们正在使用 Python Scikit-Learn 在收集的 Twitter 数据集上运行 K-Means 聚类算法。我们首先将样本数据加载到
    Pandas 数据框中：
- en: '[PRE3]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We first perform a feature extraction from the tweets'' text. We apply a sparse
    vectorizer to the dataset using a TF-IDF vectorizer with 10,000 features and English
    stop words:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从推文的文本中执行特征提取。我们使用 TF-IDF 向量化器并带有 10,000 个特征和英语停用词的稀疏向量器对数据集进行应用：
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As the dataset is now broken into a 7540 sample with vectors of 6,638 features,
    we are ready to feed this sparse matrix to the K-Means clustering algorithm. We
    will choose seven clusters and 100 maximum iterations initially:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集现在已分解为 7,540 个样本和 6,638 个特征向量，我们可以将这个稀疏矩阵输入到 K-Means 聚类算法中。我们最初将选择七个聚类和
    100 次最大迭代：
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The K-Means clustering algorithm converged after 18 iterations. We see in the
    following results the seven clusters with their respective key words. Clusters
    `0` and `6` are about music and fashion with Justin Bieber and Lady Gaga-related
    tweets. Clusters `1` and `5` are related to the U.S.A. presidential elections
    with Donald Trump-and Hilary Clinton-related tweets. Clusters `2` and `3` are
    the ones of interest to us as they are about Apache Spark and Python. Cluster
    `4` contains Thailand-related tweets:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means 聚类算法在 18 次迭代后收敛。在以下结果中，我们可以看到七个聚类及其相应的关键词。聚类 `0` 和 `6` 是关于音乐和时尚的，与贾斯汀·比伯和
    Lady Gaga 相关的推文。聚类 `1` 和 `5` 与美国总统选举相关，与唐纳德·特朗普和希拉里·克林顿相关的推文。聚类 `2` 和 `3` 是我们感兴趣的，因为它们是关于
    Apache Spark 和 Python 的。聚类 `4` 包含与泰国相关的推文：
- en: '[PRE6]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We will visualize the results by plotting the cluster. We have 7,540 samples
    with 6,638 features. It will be impossible to visualize that many dimensions.
    We will use the **Multi-Dimensional Scaling** (**MDS**) algorithm to bring down
    the multidimensional features of the clusters into two tractable dimensions to
    be able to picture them:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过绘制聚类来可视化结果。我们有 7,540 个样本和 6,638 个特征。可视化这么多维度是不可能的。我们将使用 **多维缩放（MDS**）算法将聚类的多维特征降低到两个可处理的维度，以便能够描绘它们：
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here''s a plot of Cluster `2`, *Big Data* and *Spark*, represented by blue
    dots along with Cluster `3`, *Spark* and *Python*, represented by red dots, and
    some sample tweets related to the respective clusters:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是簇`2`、*大数据*和*Spark*的图表，用蓝色点表示，以及簇`3`、*Spark*和*Python*的图表，用红色点表示，还有一些与相应簇相关的样本推文：
- en: '![Applying Scikit-Learn on the Twitter dataset](img/B03986_04_07.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![在Twitter数据集上应用Scikit-Learn](img/B03986_04_07.jpg)'
- en: We have gained some good insights into the data with the exploration and processing
    done with Scikit-Learn. We will now focus our attention on Spark MLlib and take
    it for a ride on the Twitter dataset.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用Scikit-Learn进行探索和处理，对数据有了一些好的见解。现在，我们将把注意力转向Spark MLlib，并在Twitter数据集上试驾一番。
- en: Preprocessing the dataset
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理数据集
- en: 'Now, we will focus on feature extraction and engineering in order to ready
    the data for the clustering algorithm run. We instantiate the Spark Context and
    read the Twitter dataset into a Spark dataframe. We will then successively tokenize
    the tweet text data, apply a hashing Term frequency algorithm to the tokens, and
    finally apply the Inverse Document Frequency algorithm and rescale the data. The
    code is as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将专注于特征提取和工程，以便为聚类算法运行准备数据。我们实例化Spark上下文，并将Twitter数据集读入Spark数据框。然后我们将依次对推文文本数据进行分词，对标记应用哈希词频算法，并最终应用逆文档频率算法并重新缩放数据。代码如下：
- en: '[PRE8]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Running the clustering algorithm
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行聚类算法
- en: 'We will use the K-Means algorithm against the Twitter dataset. As an unlabeled
    and shuffled bag of tweets, we want to see if the *Apache Spark* tweets are grouped
    in a single cluster. From the previous steps, the TF-IDF sparse vector of features
    is converted into an RDD that will be the input to the Spark MLlib program. We
    initialize the K-Means model with 5 clusters, 10 iterations of 10 runs:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用K-Means算法对Twitter数据集进行处理。作为一个未标记且打乱的推文集合，我们想看看*Apache Spark*的推文是否被分在单个簇中。在前面的步骤中，特征TF-IDF稀疏向量被转换成一个RDD，它将成为Spark
    MLlib程序的输入。我们用5个簇、10次迭代和10次运行初始化K-Means模型：
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Evaluating the model and the results
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型和结果
- en: 'One way to fine-tune the clustering algorithm is by varying the number of clusters
    and verifying the output. Let''s check the clusters and get a feel for the clustering
    results so far:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 调整聚类算法的一种方法是通过改变簇的数量并验证输出。让我们检查簇并感受一下到目前为止的聚类结果：
- en: '[PRE10]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We map the `5` clusters with some sample tweets. Cluster `0` is about Spark.
    Cluster `1` is about Python. Cluster `2` is about Lady Gaga. Cluster `3` is about
    Thailand's Phuket News. Cluster `4` is about Donald Trump.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`5`个簇与一些样本推文进行映射。簇`0`是关于Spark的。簇`1`是关于Python的。簇`2`是关于Lady Gaga的。簇`3`是关于泰国普吉岛新闻的。簇`4`是关于唐纳德·特朗普的。
- en: Building machine learning pipelines
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建机器学习管道
- en: We want to compose the feature extraction, preparatory activities, training,
    testing, and prediction activities while optimizing the best tuning parameter
    to get the best performing model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望在优化最佳调整参数以获得最佳性能模型的同时，组合特征提取、准备活动、训练、测试和预测活动。
- en: 'The following tweet captures perfectly in five lines of code a powerful machine
    learning Pipeline implemented in Spark MLlib:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下推文完美地用五行代码实现了在Spark MLlib中实现的强大机器学习管道：
- en: '![Building machine learning pipelines](img/B03986_04_08.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![构建机器学习管道](img/B03986_04_08.jpg)'
- en: The Spark ML pipeline is inspired by Python's Scikit-Learn and creates a succinct,
    declarative statement of the successive transformations to the data in order to
    quickly deliver a tunable model.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML管道灵感来源于Python的Scikit-Learn，它创建了一个简洁的、声明性的语句，用于对数据进行连续的转换，以便快速交付可调整的模型。
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we got an overview of Spark MLlib's ever-expanding library
    of algorithms Spark MLlib. We discussed supervised and unsupervised learning,
    recommender systems, optimization, and feature extraction algorithms. We then
    put the harvested data from Twitter into the machine learning process, algorithms,
    and evaluation to derive insights from the data. We put the Twitter-harvested
    dataset through a Python Scikit-Learn and Spark MLlib K-means clustering in order
    to segregate the tweets relevant to *Apache Spark*. We also evaluated the performance
    of the model.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们概述了Spark MLlib不断扩大的算法库Spark MLlib。我们讨论了监督学习和无监督学习、推荐系统、优化和特征提取算法。然后我们将从Twitter收集的数据放入机器学习过程、算法和评估中，以从数据中提取见解。我们将Twitter收集的数据集通过Python
    Scikit-Learn和Spark MLlib K-means聚类进行分离，以隔离与*Apache Spark*相关的推文。我们还评估了模型的性能。
- en: This gets us ready for the next chapter, which will cover Streaming Analytics
    using Spark. Let's jump right in.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们准备进入下一章打下了基础，下一章将涵盖使用Spark的流式分析。让我们直接进入正题。
