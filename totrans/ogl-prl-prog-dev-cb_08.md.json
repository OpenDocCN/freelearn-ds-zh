["```py\n// num_rows  – number of rows in matrix\n// data      – the array that stores the non-zero values\n// indices   – the array that stores the column indices for zero, non-\n//              zero values in the matrix\n// num_cols  – the number of columns.\n// vec       - the dense vector\n// y         - the output \nvoid spmv_ell_cpu(const int num_rows,\n                  const int num_cols,\n                  const int * indices;\n                  const float * data,\n                  const float * vec, float * out) {\nfor( int row = 0; row < num_rows, row++) {\n    float temp = 0;\n      // row-major order\n    for(int n = 0; n < num_cols; n++) {\n        int col = indices[num_cols * row + n];\n        float value = data[num_cols * row + n];\n        if (value != 0 && col != 0)\n            temp += value * vec[col];\n    }\n    out[row] += temp;\n}\n}\n```", "```py\n// num_rows  – number of rows in matrix\n// data      – the array that stores the non-zero values\n// indices   – the array that stores the column indices for zero, non-\n//              zero values in the matrix\n// num_cols  – the number of columns.\n// vec       - the dense vector\n// y         - the output \n__kernel void\nspmv_ell_gpu(__global const int num_rows,\n             __global const int num_cols,\n             __global const int * indices;\n             __global const float * data,\n             __global const float * vec, float * out) {\n     int row = get_global_id(0);\n     if (row < num_rows) {\n    float temp = 0;\n\t    // row-major order\n    for(int n = 0; n < num_cols; n++) {\n        int col = indices[num_cols * row + n];\n        float value = data[num_cols * row + n];\n        if (value != 0 && col != 0)\n            temp += value * vec[col];\n    }\n    out[row] += temp;\n}\n}\n```", "```py\n// data – the 1-D array containing non-zero values\n// vec – our dense vector\n// cols – column indices indicating where non-zero values are\n// rowLengths – the maximum length of non-zeros in each row\n// dim – dimension of our square matrix\n// out – the 1-D array which our output array will be \n__kernel void\nspmv_ellpackr_kernel(__global const float * restrict data,\n                     __global const float * restrict vec\n                     __global const int * restrict cols,\n                     __global const int * restrict rowLengths,\n                     const int dim, \n                     __global float * restrict out) {\n    int t = get_global_id(0);\n\n    if (t < dim)\n    {\n        float result = 0.0;\n        int max = rowLengths[t];\n        for (int i = 0; i < max; i++) {\n            int ind = i * dim + t;\n            result += data [ind] * vec[cols[ind]];\n        }\n        out[t] = result;\n    }\n}\n```", "```py\n// data – the 1-D array containing non-zero values\n// vec – our dense vector\n// cols – column indices indicating where non-zero values are\n// rowLengths – the maximum length of non-zeros in each row\n// dim – dimension of our square matrix\n// out – the 1-D array which our output array will be \n#define VECTOR_SIZE 32 // NVIDIA = 32, AMD = 64\n__kernel void\nspmv_ellpackr_vector_kernel(__global const float * restrict val,\n                            __global const float * restrict vec,\n                            __global const int * restrict cols,\n                            __global const int * restrict rowLengths,\n                            const int dim,\n                            __global float * restrict out) {\n\n    // Thread ID in block\n    int t = get_local_id(0);\n    // Thread id within warp/wavefront\n    int id = t & (VECTOR_SIZE-1);\n    // one warp/wavefront per row\n    int threadsPerBlock = get_local_size(0) / VECTOR_SIZE;\n    int row = (get_group_id(0) * threadsPerBlock) + (t / VECTOR_SIZE);\n\n    __local float volatile partialSums[128];\n\n    if (row < dim) {\n        float result = 0.0;\n        int max = ceil(rowLengths[row]/VECTOR_SIZE);\n        // the kernel is vectorized here where simultaneous threads\n        // access data in an adjacent fashion, improves memory\n        // coalescence and increase device bandwidth\n        for (int i = 0; i < max; i ++) {\n            int ind = i * (dim * VECTOR_SIZE) + row * VECTOR_SIZE + id;\n            result += val[ind] * vec[cols[ind]];\n        }\n        partialSums[t] = sum;\n        barrier(CLK_LOCAL_MEM_FENCE);\n\n        // Reduce partial sums\n        // Needs to be modified if there is a change in vector length\n        if (id < 16) partialSums[t] += partialSums[t +16];\n        barrier(CLK_LOCAL_MEM_FENCE);\n        if (id <  8) partialSums[t] += partialSums[t + 8];\n        barrier(CLK_LOCAL_MEM_FENCE);\n        if (id <  4) partialSums[t] += partialSums[t + 4];\n        barrier(CLK_LOCAL_MEM_FENCE);\n        if (id <  2) partialSums[t] += partialSums[t + 2];\n        barrier(CLK_LOCAL_MEM_FENCE);\n        if (id <  1) partialSums[t] += partialSums[t + 1];\n        barrier(CLK_LOCAL_MEM_FENCE);\n\n        // Write result\n        if (tid == 0)\n        {\n            out[row] = partialSums[tid];\n        }\n\n    }\n}\n```", "```py\n// num_rows – number of rows in matrix\n// ptr – the array that stores the offset to the i-th row in ptr[i]\n// indices – the array that stores the column indices for non-zero\n//           values in the matrix\n// x       - the dense vector\n// y       - the output \nvoid spmv_csr_cpu(const int num_rows,\n                  const int * ptr;\n                  const int * indices;\n                  const float * data,\n                  const float * vec, float * out) {\nfor( int row = 0; row < num_rows, row++) {\n    float temp = 0;\n    int start_row = ptr[row];\n    int end_row = ptr[row+1];\n    for(int jj = start_row; jj < end_row; jj++)\n        temp += data[jj] * vec [indices[jj]];\n    out[row] += temp;\n}\n}\n```", "```py\n// M – the matrix with dimensions 'height' x 'width'\n// V – the dense vector of length 'width'\n// W – the output\nvoid matvec_cpu(const float* M, const float* V, int width, int height, float* W)\n{\n    for (int i = 0; i < height; ++i) {\n        double sum = 0;\n        for (int j = 0; j < width; ++j) {\n            double a = M[i * width + j];\n            double b = V[j];\n            sum += a * b;\n        }\n        W[i] = (float)sum;\n    }\n}\n```", "```py\n__kernel void \nspmv_csr_scalar_kernel( __global const float * restrict val,\n                        __global const float * restrict vec,\n                        __global const int * restrict cols,\n                        __global const int * restrict ptr,\n                        const int dim, __global float * restrict out){\n    int row = get_global_id(0);\n\n    if (row < dim) {\n        float temp=0;\n        int start = ptr[row];\n        int end = ptr[row+1];\n        for (int j = start; j < end; j++) {\n            int col = cols[j];\n            temp += val[j] * vec[col];\n        }\n        out[row] = temp;\n    }\n}\n```", "```py\n#define VECTOR_SIZE 32 \n// Nvidia is 32 threads per warp, ATI is 64 per wavefront\n__kernel void\nspmv_csr_vector_kernel(__global const float * restrict val,\n                       __global const float * restrict vec,\n                       __global const int * restrict cols,\n                       __global const int * restrict ptr,\n                       const int dim, __global float * restrict out){\n    int tid = get_local_id(0);\n    int id = tid & (VECTOR_SIZE-1);\n    // One row per warp\n    int threadsPerBlock = get_local_size(0) / VECTOR_SIZE;\n    int row = (get_group_id(0) * threadsPerBlock) + (tid / VECTOR_SIZE);\n\n    __local volatile float partialSums[128];\n    partialSums[t] = 0;\n\n    if (row < dim)\n    {\n        int vecStart = ptr[row];\n        int vecEnd   = ptr[row+1];\n        float sum = 0;\n        for (int j = vecStart + id; j < vecEnd; j += VECTOR_SIZE) {\n            int col = cols[j];\n            sum += val[j] * vec[col];\n        }\n        partialSums[tid] = sum;\n        barrier(CLK_LOCAL_MEM_FENCE);\n\n        // Reduce partial sums\n        // Needs to be modified if there is a change in vector length\n        if (id < 16) partialSums[tid] += partialSums[t +16];\n        barrier(CLK_LOCAL_MEM_FENCE);\n        if (id <  8) partialSums[tid] += partialSums[tid + 8];\n        barrier(CLK_LOCAL_MEM_FENCE);\n        if (id <  4) partialSums[tid] += partialSums[tid + 4];\n        barrier(CLK_LOCAL_MEM_FENCE);\n        if (id <  2) partialSums[tid] += partialSums[tid + 2];\n        barrier(CLK_LOCAL_MEM_FENCE);\n        if (id <  1) partialSums[tid] += partialSums[tid + 1];\n        barrier(CLK_LOCAL_MEM_FENCE);\n\n        // Write result\n        if (id == 0)\n        {\n            out[row] = partialSums[tid];\n        }\n    }\n}\n```", "```py\ngcc -std=c99 -Wall -DUNIX -g -DDEBUG -arch i386 -o SpMV -framework OpenCL\n\n```", "```py\nPassed!\n\n```", "```py\n__local volatile float partialSums[128];\n```", "```py\nif (id < 16) partialSums[tid] += partialSums[t +16];\nbarrier(CLK_LOCAL_MEM_FENCE);\nif (id <  8) partialSums[tid] += partialSums[tid + 8];\nbarrier(CLK_LOCAL_MEM_FENCE);\nif (id <  4) partialSums[tid] += partialSums[tid + 4];\nbarrier(CLK_LOCAL_MEM_FENCE);\nif (id <  2) partialSums[tid] += partialSums[tid + 2];\nbarrier(CLK_LOCAL_MEM_FENCE);\nif (id <  1) partialSums[tid] += partialSums[tid + 1];\nbarrier(CLK_LOCAL_MEM_FENCE);\n\n// Write result\nif (id == 0) {\n    out[row] = partialSums[tid];\n}\n```", "```py\nif (id < 16) partialSums[tid] += partialSums[t +16];\nbarrier(CLK_LOCAL_MEM_FENCE);\n```", "```py\nif (id < 8) partialSums[tid] += partialSums[t +8];\nbarrier(CLK_LOCAL_MEM_FENCE);\n```", "```py\n#define VEXCL_SHOW_KERNELS \n// define this macro before VexCL header inclusion to view output   \n// kernels\n\n#include <vexcl/vexcl.hpp>\ntypedef double real;\n#include <iostream>\n#include <vector>\n#include <cstdlib>\n\nvoid gpuConjugateGradient(const std::vector<size_t> &row,\n                          const std::vector<size_t> &col,\n                          const std::vector<real> &val,\n                          const std::vector<real> &rhs,\n                          std::vector<real> &x) {\n    /*\n     Initialize the OpenCL context\n     */\n    vex::Context oclCtx(vex::Filter::Type(CL_DEVICE_TYPE_GPU) &&\n                        vex::Filter::DoublePrecision);\n\n    size_t n = x.size();\n    vex::SpMat<real> A(oclCtx, n, n, row.data(), col.data(), val.data());\n    vex::vector<real> f(oclCtx, rhs);\n    vex::vector<real> u(oclCtx, x);\n    vex::vector<real> r(oclCtx, n);\n    vex::vector<real> p(oclCtx, n);\n    vex::vector<real> q(oclCtx, n);\n\n    vex::Reductor<real,vex::MAX> max(oclCtx);\n    vex::Reductor<real,vex::SUM> sum(oclCtx);\n\n    /*\n     Solve the equation Au = f with the \"conjugate gradient\" method\n     See http://en.wikipedia.org/wiki/Conjugate_gradient_method\n     */\n    float rho1, rho2;\n    r = f - A * u;\n\n    for(uint iter = 0; max(fabs(r)) > 1e-8 && iter < n; iter++) {\n        rho1 = sum(r * r);\n        if(iter == 0 ) {\n          p = r;\n        } else {\n          float beta = rho1 / rho2;\n          p = r + beta * p;\n        }\n\n        q = A * p;\n\n        float alpha = rho1 / sum(p * q);\n        u += alpha * p;\n        r -= alpha * q;\n        rho2 = rho1;\n    }\n\n    using namespace vex;\n    vex::copy(u, x); // copy the result back out to the host vector\n}\n```"]