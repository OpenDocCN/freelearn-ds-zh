<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Building an E-mail Classification System Using Apache Mahout</h1></div></div></div><p>In this chapter, we will create a classifier system using Mahout. In order to build this system, we will cover the following topics:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Getting the dataset</li><li class="listitem" style="list-style-type: disc">Preparation of the dataset</li><li class="listitem" style="list-style-type: disc">Preparing the model</li><li class="listitem" style="list-style-type: disc">Training the model</li></ul></div><p>In this chapter, we will <a id="id337" class="indexterm"/>target the creation of two different classifiers. The first one will be an easy one because you can both create and test it on a pseudo-distributed Hadoop installation. For the second classifier, I will provide you with all the details, so you can run it using your fully distributed Hadoop installation. I will count the second one as a hands-on exercise for the readers of this book.</p><p>First of all, let's understand the problem statement for the first use case. Nowadays, in most of the e-mail systems, we see that e-mails are classified as spam or not spam. E-mails that are not spam are delivered directly into our inbox but spam e-mails are stored in a folder called <code class="literal">Spam</code>. Usually, based on a certain pattern such as message subject, sender's e-mail address, or certain keywords in the message body, we categorize an incoming e-mail as spam. We will create a classifier using Mahout, which will classify an e-mail into spam or not spam. We will use SpamAssassin, an Apache open source project dataset for this task.</p><p>For the second use case, we will create a classifier, which can predict a group of incoming e-mails. As an open source project, there are lots of projects under the Apache software foundation, such as Apache Mahout, Apache Hadoop, Apache Solr, and so on. We will take the <strong>Apache </strong>
<a id="id338" class="indexterm"/>
<strong>Software Foundation</strong> (<strong>ASF</strong>) e-mail dataset and using this, we will create and train our model so that our model can predict a new incoming e-mail. So, based on certain features, we will be able to predict which group a new incoming e-mail belongs to.</p><p>In Mahout's classification problem, we will have to identify a pattern in the dataset to help us predict the group of a new e-mail. We already have a dataset, which is separated by project names. We will use the ASF public e-mail archives dataset for this use case.</p><p>Now, let's consider our first use case: spam e-mail detection classifier.</p><div><div><div><div><h1 class="title"><a id="ch09lvl1sec42"/>Spam e-mail dataset</h1></div></div></div><p>As I <a id="id339" class="indexterm"/>mentioned, we will be using the Apache SpamAssassin <a id="id340" class="indexterm"/>projects dataset. Apache SpamAssassin is an open source spam filter. Download <code class="literal">20021010_easy_ham.tar</code> and <code class="literal">20021010_spam.tar</code> from <a class="ulink" href="http://spamassassin.apache.org/publiccorpus/">http://spamassassin.apache.org/publiccorpus/</a>, as shown in the following screenshot:</p><div><img src="img/4959OS_09_01.jpg" alt="Spam e-mail dataset"/></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec43"/>Creating the model using the Assassin dataset</h1></div></div></div><p>We can <a id="id341" class="indexterm"/>create the model with the help of the <a id="id342" class="indexterm"/>following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Create a folder under <code class="literal">tmp</code> with the name <code class="literal">dataset</code>, and then click on the folder and unzip the datasets using the following command:<div><pre class="programlisting">
<strong>mkdir /tmp/assassin/dataset</strong>
<strong>tar –xvf  /tmp/assassin/ 20021010_easy_ham.tar.bz2 </strong>
<strong>tar –xvf /tmp/assassin/ 20021010_spam.tar.bz2</strong>
</pre></div><p>This will create two folders under the <code class="literal">dataset</code> folder, <code class="literal">easy _ham</code> and <code class="literal">spam</code>, as shown in the following screenshot:</p><div><img src="img/4959OS_09_02.jpg" alt="Creating the model using the Assassin dataset"/></div></li><li class="listitem">Create a folder in <code class="literal">Hdfs</code> and move this dataset into Hadoop:<div><pre class="programlisting">
<strong>hadoop fs  -mkdir /user/hue/assassin/</strong>
<strong>hadoop fs –put /tmp/assassin/dataset  /user/hue/assassin </strong>
<strong>tar –xvf /tmp/assassin/ 20021010_spam.tar.bz2</strong>
</pre></div><p>Now <a id="id343" class="indexterm"/>our data preparation is done. We have downloaded the data and moved this data into <code class="literal">hdfs</code>. Let's move on to the next step.</p></li><li class="listitem">Convert this <a id="id344" class="indexterm"/>data into sequence files so that we can process it using Hadoop:<div><pre class="programlisting">
<strong>bin/mahout seqdirectory –i /user/hue/assassin/dataset –o /user/hue/assassinseq-out</strong>
</pre></div><div><img src="img/4959OS_09_03.jpg" alt="Creating the model using the Assassin dataset"/></div></li><li class="listitem">Convert the <code class="literal">sequence</code> file into sparse vector (Mahout algorithms accept input in <a id="id345" class="indexterm"/>vector format, which is why we <a id="id346" class="indexterm"/>are converting the <code class="literal">sequence</code> file into sparse vector) by using the following command:<div><pre class="programlisting">
<strong>bin/mahout seq2sparse -i /user/hue/assassinseq-out/part-m-00000 -o /user/hue/assassinvec -lnorm -nv -wt tfidf</strong>
</pre></div><div><img src="img/4959OS_09_04.jpg" alt="Creating the model using the Assassin dataset"/></div><p>The command in the preceding screenshot is explained as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">lnorm</code>: This command is used for output vector to be log normalized.</li><li class="listitem" style="list-style-type: disc"><code class="literal">nv</code>: This command is used for named vector.</li><li class="listitem" style="list-style-type: disc"><code class="literal">wt</code>: This command is used to identify the kind of weight to use. Here we use <code class="literal">tf-idf</code>.</li></ul></div></li><li class="listitem">Split the <a id="id347" class="indexterm"/>set of vectors for training and <a id="id348" class="indexterm"/>testing the model, as follows:<div><pre class="programlisting">
<strong>bin/mahout split -i /user/hue/assassinvec/tfidf-vectors --trainingOutput /user/hue/assassindatatrain --testOutput /user/hue/assassindatatest --randomSelectionPct 20 --overwrite --sequenceFiles -xm sequential</strong>
</pre></div><p>The preceding command can be explained as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">randomSelectionPct</code> parameter divides the percentage of data into test and training datasets. In this case, it's 80 percent for test and 20 percent for training.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">xm</code> parameter specifies what portion of the <code class="literal">tf (tf-idf)</code> vectors is to be used expressed in times the standard deviation.</li><li class="listitem" style="list-style-type: disc">The sigma symbol specifies the document frequencies of these vectors. It can be used to remove really high frequency terms. It is expressed as a double value. A good value to be specified is 3.0. If <a id="id349" class="indexterm"/>the value is less <a id="id350" class="indexterm"/>than <code class="literal">0</code>, no vectors will be filtered out.</li></ul></div><div><img src="img/4959OS_09_05.jpg" alt="Creating the model using the Assassin dataset"/></div></li><li class="listitem">Now, train the model using the following command:<div><pre class="programlisting">
<strong>bin/mahout trainnb -i /user/hue/assassindatatrain -el -o /user/hue/prodmodel -li /user/hue/prodlabelindex -ow -c</strong>
</pre></div><div><img src="img/4959OS_09_06.jpg" alt="Creating the model using the Assassin dataset"/></div></li><li class="listitem">Now, test <a id="id351" class="indexterm"/>the model using the following <a id="id352" class="indexterm"/>command:<div><pre class="programlisting">
<strong>bin/mahout testnb -i /user/hue/assassindatatest -m /user/hue/prodmodel/ -l  /user/hue/prodlabelindex -ow -o /user/hue/prodresults</strong>
</pre></div><div><img src="img/4959OS_09_07.jpg" alt="Creating the model using the Assassin dataset"/></div></li></ol></div><p>You can see from the results that the output is displayed on the console. As per the matrix, the system has correctly classified 99.53 percent of the instances given.</p><p>We can use this created model to classify new documents. To do this, we can either use a Java program or create a servlet that can be deployed on our server.</p><p>Let's take an <a id="id353" class="indexterm"/>example of a Java program in continuation of <a id="id354" class="indexterm"/>this exercise.</p></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec44"/>Program to use a classifier model</h1></div></div></div><p>We will <a id="id355" class="indexterm"/>create a Java program that will use our model to classify new e-mails. This program will take model, labelindex, dictionary-file, document frequency, and text file as input and will generate a score for the categories. The <a id="id356" class="indexterm"/>category will be decided based on the higher scores.</p><p>Let's have a look at this program step by step:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">.jar</code> files required to make a compilation of this program are as follows:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Hadoop-core-x.y.x.jar</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Mahout-core-xyz.jar</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Mahout-integration-xyz.jar</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Mahout-math-xyz.jar</code></li></ul></div></li><li class="listitem" style="list-style-type: disc">The <code class="literal">import</code> statements are listed as follows. We are discussing this because there are <a id="id357" class="indexterm"/>lots of changes in the <a id="id358" class="indexterm"/>Mahout releases and people usually find it difficult to get the correct classes.<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">import java.io.BufferedReader;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import java.io.FileReader;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import java.io.StringReader;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import java.util.HashMap;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import java.util.Map;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import org.apache.hadoop.conf.Configuration;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import org.apache.hadoop.fs.Path;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import org.apache.lucene.analysis.Analyzer;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import org.apache.lucene.analysis.TokenStream;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import org.apache.lucene.analysis.standard.StandardAnalyzer;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import org.apache.lucene.util.Version;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import org.apache.mahout.classifier.naivebayes.BayesUtils;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import org.apache.mahout.classifier.naivebayes.NaiveBayesModel;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import org.apache.mahout.classifier.naivebayes.StandardNaiveBayesClassifier;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import org.apache.mahout.common.Pair;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import org.apache.mahout.math.RandomAccessSparseVector;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import org.apache.mahout.math.Vector;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import org.apache.mahout.math.Vector.Element;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import org.apache.mahout.vectorizer.TFIDF;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import org.apache.hadoop.io.*;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import com.google.common.collect.ConcurrentHashMultiset;</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">import com.google.common.collect.Multiset;</code></li></ul></div></li><li class="listitem" style="list-style-type: disc">The supporting <a id="id359" class="indexterm"/>methods to read the dictionary are as follows:<div><pre class="programlisting">public static Map&lt;String, Integer&gt; readDictionary(Configuration conf, Path dictionaryPath) {
  Map&lt;String, Integer&gt; dictionary = new HashMap&lt;String, Integer&gt;();
  for (Pair&lt;Text, IntWritable&gt; pair : new SequenceFileIterable&lt;Text, IntWritable&gt;(dictionaryPath, true, conf)) {
    dictionary.put(pair.getFirst().toString(), pair.getSecond().get());
  }
  return dictionary;
}</pre></div></li><li class="listitem" style="list-style-type: disc">The <a id="id360" class="indexterm"/>supporting methods to read the document frequency are as follows:<div><pre class="programlisting">public static Map&lt;Integer, Long&gt; readDocumentFrequency(Configuration conf, Path documentFrequencyPath) {
  Map&lt;Integer, Long&gt; documentFrequency = new HashMap&lt;Integer, Long&gt;();
  for (Pair&lt;IntWritable, LongWritable&gt; pair : new SequenceFileIterable&lt;IntWritable, LongWritable&gt;(documentFrequencyPath, true, conf)) {
    documentFrequency.put(pair.getFirst().get(), pair.getSecond().get());
  }
  return documentFrequency;
}</pre></div></li><li class="listitem" style="list-style-type: disc">The first part of the <code class="literal">main</code> method is used to perform the following actions:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Getting the input</li><li class="listitem" style="list-style-type: disc">Loading the model</li><li class="listitem" style="list-style-type: disc">Initializing <code class="literal">StandardNaiveBayesClassifier</code> using our created model</li><li class="listitem" style="list-style-type: disc">Reading <code class="literal">labelindex</code>, document frequency, and dictionary created while creating the vector from the dataset<p>The following code can be used for the preceding actions:</p><div><pre class="programlisting">public static void main(String[] args) throws Exception {
  if (args.length &lt; 5) {
    System.out.println("Arguments: [model] [labelindex] [dictionary] [documentfrequency] [new file] ");
    return;
  }
  String modelPath = args[0];
  String labelIndexPath = args[1];
  String dictionaryPath = args[2];
  String documentFrequencyPath = args[3];
  String newDataPath = args[4];
  Configuration configuration = new Configuration(); // model is a matrix (wordId, labelId) =&gt; probability score
  NaiveBayesModel model = NaiveBayesModel.materialize(new Path(modelPath), configuration); 
  StandardNaiveBayesClassifier classifier = new StandardNaiveBayesClassifier(model); 
  // labels is a map label =&gt; classId
  Map&lt;Integer, String&gt; labels = BayesUtils.readLabelIndex(configuration, new Path(labelIndexPath));
  Map&lt;String, Integer&gt; dictionary = readDictionary(configuration, new Path(dictionaryPath));
  Map&lt;Integer, Long&gt; documentFrequency = readDocumentFrequency(configuration, new Path(documentFrequencyPath));</pre></div></li></ul></div></li><li class="listitem" style="list-style-type: disc">The <a id="id361" class="indexterm"/>second part of the <code class="literal">main</code> <a id="id362" class="indexterm"/>method is used to extract words from the e-mail:<div><pre class="programlisting">Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_CURRENT);

int labelCount = labels.size();
int documentCount = documentFrequency.get(-1).intValue();

System.out.println("Number of labels: " + labelCount);
System.out.println("Number of documents in training set: " + documentCount);
BufferedReader reader = new BufferedReader(new FileReader(newDataPath));
while(true) {
  String line = reader.readLine();
  if (line == null) {
    break;
  }

  ConcurrentHashMultiset&lt;Object&gt; words = ConcurrentHashMultiset.create(); 
  // extract words from mail
  TokenStream ts = analyzer.tokenStream("text", new StringReader(line));
  CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
  ts.reset();
  int wordCount = 0;
  while (ts.incrementToken()) {
    if (termAtt.length() &gt; 0) {
      String word = ts.getAttribute(CharTermAttribute.class).toString();
      Integer wordId = dictionary.get(word);
      // if the word is not in the dictionary, skip it
      if (wordId != null) {
        words.add(word);
        wordCount++;
      }
    }
  }
  ts.close();</pre></div></li><li class="listitem" style="list-style-type: disc">The <a id="id363" class="indexterm"/>third part of the <code class="literal">main</code> method is <a id="id364" class="indexterm"/>used to create vector of the <code class="literal">id</code> word and the <code class="literal">tf-idf</code> weights:<div><pre class="programlisting">Vector vector = new RandomAccessSparseVector(10000);
TFIDF tfidf = new TFIDF();
for (Multiset.Entry entry:words.entrySet()) {
  String word =  (String)entry.getElement();
  int count = entry.getCount();
  Integer wordId = dictionary.get(word);
  Long freq = documentFrequency.get(wordId);
  double tfIdfValue = tfidf.calculate(count, freq.intValue(), wordCount, documentCount);
  vector.setQuick(wordId, tfIdfValue);
}</pre></div></li><li class="listitem" style="list-style-type: disc">In the fourth part of the <code class="literal">main</code> method, with <code class="literal">classifier</code>, we get the score for each label and assign the e-mail to the higher scored label:<div><pre class="programlisting">  Vector resultVector = classifier.classifyFull(vector);
    double bestScore = -Double.MAX_VALUE;
    int bestCategoryId = -1;          
    for(int i=0 ;i&lt;resultVector.size();i++) {
      Element e1  = resultVector.getElement(i);
      int categoryId = e1.index();
      double score = e1.get();
      if (score &gt; bestScore) {
        bestScore = score;
        bestCategoryId = categoryId;
      }
      System.out.print("  " + labels.get(categoryId) + ": " + score);
    }
    System.out.println(" =&gt; " + labels.get(bestCategoryId));
  }
}</pre></div></li></ul></div><p>Now, put all <a id="id365" class="indexterm"/>these codes under one class and create <a id="id366" class="indexterm"/>the <code class="literal">.jar</code> file of this class. We will use this <code class="literal">.jar</code> file to test our new e-mails.</p></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec45"/>Testing the program</h1></div></div></div><p>To test the <a id="id367" class="indexterm"/>program, perform the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Create a folder named <code class="literal">assassinmodeltest</code> in the local directory, as follows:<div><pre class="programlisting">
<strong>mkdir /tmp/assassinmodeltest</strong>
</pre></div></li><li class="listitem">To use this model, get the following files from <code class="literal">hdfs</code> to <code class="literal">/tmp/assassinmodeltest</code>:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">For the earlier created model, use the following command:<div><pre class="programlisting">
<strong>hadoop fs –get /user/hue/prodmodel /tmp/assassinmodeltest</strong>
</pre></div></li><li class="listitem" style="list-style-type: disc">For <code class="literal">labelindex</code>, use the following command:<div><pre class="programlisting">
<strong>hadoop fs –get /user/hue/prodlabelindex  /tmp/assassinmodeltest</strong>
</pre></div></li><li class="listitem" style="list-style-type: disc">For <code class="literal">df-counts</code> from the <code class="literal">assassinvec</code> folder (change the name of the <code class="literal">part-00000</code> file to <code class="literal">df-count</code>), use the following commands:<div><pre class="programlisting">
<strong>hadoop fs –get /user/hue/assassinvec/df-count  /tmp/assassinmodeltest</strong>
<strong>dictionary.file-0 from the same assassinvec folder</strong>
<strong>hadoop fs –get /user/hue/assassinvec/dictionary.file-0  /tmp/assassinmodeltest</strong>
</pre></div></li></ul></div></li><li class="listitem">Under <code class="literal">/tmp/assassinmodeltest</code>, create a file with the message shown in the following screenshot:<div><img src="img/4959OS_09_08.jpg" alt="Testing the program"/></div></li><li class="listitem">Now, run the <a id="id368" class="indexterm"/>program using the following command:<div><pre class="programlisting">
<strong>Java –cp /tmp/assassinmodeltest/spamclassifier.jar:/usr/lib/mahout/* com.packt.spamfilter.TestClassifier /tmp/assassinmodeltest /tmp/assassinmodeltest/prodlabelindex /tmp/assassinmodeltest/dictionary.file-0 /tmp/assassinmodeltest/df-count /tmp/assassinmodeltest/testemail</strong>
</pre></div><div><img src="img/4959OS_09_09.jpg" alt="Testing the program"/></div></li><li class="listitem">Now, update the <code class="literal">test</code> e-mail file with the message shown in the following screenshot:<div><img src="img/4959OS_09_10.jpg" alt="Testing the program"/></div></li><li class="listitem">Run the <a id="id369" class="indexterm"/>program again using the same command as given in step 4 and view the result as follows:<div><img src="img/4959OS_09_11.jpg" alt="Testing the program"/></div></li></ol></div><p>Now, we have a program ready that can use our classifier model and predict the unknown items. Let's move on to our second use case.</p></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec46"/>Second use case as an exercise</h1></div></div></div><p>As discussed at the start of this chapter, we will now work on a second use case, where we will predict the category of a new e-mail.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec25"/>The ASF e-mail dataset</h2></div></div></div><p>The Apache Software <a id="id370" class="indexterm"/>Foundation e-mail dataset is partitioned by <a id="id371" class="indexterm"/>project. This e-mail dataset can be found at <a class="ulink" href="http://aws.amazon.com/datasets/7791434387204566">http://aws.amazon.com/datasets/7791434387204566</a>.</p><div><img src="img/4959OS_09_12.jpg" alt="The ASF e-mail dataset"/></div><p>A smaller dataset <a id="id372" class="indexterm"/>can be found at <a class="ulink" href="http://files.grantingersoll.com/ibm.tar.gz">http://files.grantingersoll.com/ibm.tar.gz</a>. (Refer to <a class="ulink" href="http://lucidworks.com/blog/scaling-mahout/">http://lucidworks.com/blog/scaling-mahout/</a>). Use this data to perform the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Move this <a id="id373" class="indexterm"/>data to the folder of your choice (<code class="literal">/tmp/asfmail</code>) and unzip the folder:<div><pre class="programlisting">
<strong>mkdir /tmp/asfmail</strong>
<strong>tar –xvf  ibm.tar</strong>
</pre></div></li><li class="listitem">Move the dataset to <code class="literal">hdfs</code>:<div><pre class="programlisting">
<strong>hadoop fs -put /tmp/asfmail/ibm/content /user/hue/asfmail</strong>
</pre></div></li><li class="listitem">Convert the <code class="literal">mbox</code> files into Hadoop's <code class="literal">SequenceFile</code> format using Mahout's <code class="literal">SequenceFilesFromMailArchives</code> as follows:<div><pre class="programlisting">
<strong>mahout  org.apache.mahout.text.SequenceFilesFromMailArchives --charset "UTF-8" --body --subject --input /user/hue/asfmail/content --output /user/hue/asfmailout</strong>
</pre></div><div><img src="img/4959OS_09_13.jpg" alt="The ASF e-mail dataset"/></div></li><li class="listitem">Convert the <a id="id374" class="indexterm"/><code class="literal">sequence</code> file into sparse vector:<div><pre class="programlisting">
<strong>mahout  seq2sparse --input /user/hue/asfmailout --output /user/hue/asfmailseqsp --norm 2 --weight TFIDF --namedVector --maxDFPercent 90 --minSupport 2 --analyzerName org.apache.mahout.text.MailArchivesClusteringAnalyzer</strong>
</pre></div><div><img src="img/4959OS_09_14.jpg" alt="The ASF e-mail dataset"/></div></li><li class="listitem">Modify the labels:<div><pre class="programlisting">
<strong>mahout  org.apache.mahout.classifier.email.PrepEmailDriver --input /user/hue/asfmailseqsp --output /user/hue/asfmailseqsplabel --maxItemsPerLabel 1000</strong>
</pre></div></li></ol></div><p>Now, the next three <a id="id375" class="indexterm"/>steps are similar to the ones we performed earlier:</p><div><ol class="orderedlist arabic"><li class="listitem">Split the dataset into <code class="literal">training</code> and <code class="literal">test</code> datasets using the following command:<div><pre class="programlisting">
<strong>mahout  split --input /user/hue/asfmailseqsplabel --trainingOutput /user/hue/asfmailtrain --testOutput /user/hue/asfmailtest  --randomSelectionPct 20 --overwrite --sequenceFiles</strong>
</pre></div></li><li class="listitem">Train the model using the <code class="literal">training</code> dataset as follows:<div><pre class="programlisting">
<strong>mahout trainnb -i /user/hue/asfmailtrain -o /user/hue/asfmailmodel -extractLabels --labelIndex /user/hue/asfmaillabels</strong>
</pre></div></li><li class="listitem">Test the model using the <code class="literal">test</code> dataset:<div><pre class="programlisting">
<strong>mahout testnb -i /user/hue/asfmailtest -m /user/hue/asfmailmodel --labelIndex /user/hue/asfmaillabels</strong>
</pre></div></li></ol></div><p>As you may have noticed, all the steps are exactly identical to the ones we performed earlier. Hereby, I leave this topic as an exercise for you to create your own classifier system using this model. You can use hints as provided for the spam filter classifier. We now move our discussion to tuning our classifier. Let's take a brief overview of the best practices in this area.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec47"/>Classifiers tuning</h1></div></div></div><p>We already discussed classifiers' evaluation techniques in <a class="link" href="ch01.html" title="Chapter 1. Classification in Data Analysis">Chapter 1</a>, <em>Classification in Data Analysis</em>. Just as a reminder, we evaluate our model using techniques such as confusion matrix, entropy matrix, area under curve, and so on.</p><p>From the explanatory <a id="id376" class="indexterm"/>variables, we create the feature vector. To check how a particular model is working, these feature vectors need to be investigated. In Mahout, there is a class available for this, <code class="literal">ModelDissector</code>. It takes the following three inputs:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Features</strong>: This class <a id="id377" class="indexterm"/>takes a feature vector to use (destructively)</li><li class="listitem" style="list-style-type: disc"><strong>TraceDictionary</strong>: This <a id="id378" class="indexterm"/>class takes a trace dictionary containing variables and the locations in the feature vector that are affected by them</li><li class="listitem" style="list-style-type: disc"><strong>Learner</strong>: This class <a id="id379" class="indexterm"/>takes the model that we are probing to find weights on features</li></ul></div><p>
<code class="literal">ModelDissector</code> tweaks the feature vector and observes how the model output changes. By taking an average of the number of examples, we can determine the effect of different explanatory variables.</p><p>
<code class="literal">ModelDissector</code> has a summary method, which returns the most important features with their weights, most important category, and the top few categories that they affect.</p><p>The output of <code class="literal">ModelDissector</code> is helpful in troubleshooting problems in a wrongly created model.</p><p>More details for the <a id="id380" class="indexterm"/>code can be found at <a class="ulink" href="https://github.com/apache/mahout/blob/master/mrlegacy/src/main/java/org/apache/mahout/classifier/sgd/ModelDissector.java">https://github.com/apache/mahout/blob/master/mrlegacy/src/main/java/org/apache/mahout/classifier/sgd/ModelDissector.java</a>.</p><p>While improving the output of the classifier, one should take care with two commonly occurring problems: target leak, and broken feature extraction.</p><p>If the model is showing <a id="id381" class="indexterm"/>results that are too good to be true or an output beyond expectations, we could have a problem with target leak. This error comes once information from the target variable is included in the explanatory variables, which are used to train the classifier. In this instance, the classifier will work too well for the <code class="literal">test</code> dataset.</p><p>On the other hand, broken feature extraction occurs when feature extraction is broken. This type of classifier shows the opposite result from the target leak classifiers. Here, the model provides results poorer than expected.</p><p>To tune the classifier, we can use new explanatory variables, transformations of explanatory variables, and can also eliminate some of the variables. We should also try different learning algorithms to create the model and choose an algorithm, which is good in performance, training time, and speed.</p><p>More details on tuning can be found in Chapter 16, <em>Deploying a classifier</em> in the book <em>Mahout in Action</em>.</p></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec48"/>Summary</h1></div></div></div><p>In this chapter, we discussed creating our own production ready classifier model. We took up two use cases here, one for an e-mail spam filter and the other for classifying the e-mail as per the projects. We used datasets for Apache SpamAssassin for the e-mail filter and ASF for the e-mail classifier.</p><p>We also saw how to increase the performance of your model.</p><p>So you are now ready to implement classifiers using Apache Mahout for your own real world use cases. Happy learning!</p></div></body></html>