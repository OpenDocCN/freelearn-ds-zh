<html><head></head><body><div><div><h1 id="_idParaDest-155"><a id="_idTextAnchor158"/>Chapter 9: spaCy and Transformers</h1>
			<p>In this chapter, you will learn about the latest hot topic in NLP, transformers, and how to use them with TensorFlow and spaCy. </p>
			<p>First, you will learn about transformers and transfer learning. Second, you'll learn about the architecture details of the commonly used Transformer architecture – <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>). You'll also learn how <strong class="bold">BERT Tokenizer</strong> and <strong class="bold">WordPiece</strong> algorithms work. Then you will learn how to quickly get started with pre-trained transformer models of the <strong class="bold">HuggingFace</strong> library. Next, you'll practice how to fine-tune HuggingFace Transformers with TensorFlow and Keras. Finally, you'll learn how <em class="italic">spaCy v3.0</em> integrates transformer models as pre-trained pipelines. </p>
			<p>By the end of this chapter, you will be completing the statistical NLP topics of this book. You will add your knowledge of transformers to the knowledge of Keras and TensorFlow that you acquired in <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a>, <em class="italic">Text Classification with spaCy</em>. You'll be able to build state-of-the-art NLP models with just a few lines of code with the power of Transformer models and transfer learning. </p>
			<p>In this chapter, we're going to cover the following main topics: </p>
			<ul>
				<li>Transformers and transfer learning</li>
				<li>Understanding BERT</li>
				<li>Transformers and TensorFlow</li>
				<li>Transformers and spaCy</li>
			</ul>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor159"/>Technical requirements</h1>
			<p>In this chapter, we'll use the <code>transformers</code> and <code>tensorflow</code> Python libraries along with <code>spaCy</code>. You can install these libraries via <code>pip</code>:</p>
			<pre>pip install transformers
pip install "tensorflow&gt;=2.0.0"</pre>
			<p>The chapter code can be found at the book's GitHub repository: <a href="https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter09">https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter09</a>.</p>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor160"/>Transformers and transfer learning</h1>
			<p>A milestone in NLP happened in 2017 with the release of the research paper <em class="italic">Attention Is All You Need</em>, by Vaswani et al. (<a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>), which introduced a <a id="_idIndexMarker587"/>brand-new machine learning idea and architecture – transformers. Transformers in NLP is a fresh idea that aims to solve sequential modeling tasks <a id="_idIndexMarker588"/>and targets some problems introduced by <strong class="bold">long short-term memory</strong> (<strong class="bold">LSTM</strong>) architecture (recall LSTM architecture from <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a>, <em class="italic">Text Classification with spaCy</em>). Here's how the paper explains how transformers work:</p>
			<p class="author-quote">"The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution."</p>
			<p>Transduction in this context means transforming input words to output words by transforming <a id="_idIndexMarker589"/>input words and sentences into vectors. Typically, a transformer is trained on a huge corpus such as Wiki or news. Then, in our downstream tasks, we use these vectors as they carry information regarding the word semantics, sentence structure, and sentence semantics (we'll see how to use the vectors precisely in our code in the <em class="italic">Transformers and TensorFlow</em> section).</p>
			<p>We already explored the idea of pre-trained word vectors in <a href="B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087"><em class="italic">Chapter 5</em></a>, <em class="italic">Working with Word Vectors and Semantic Similarity</em>. Word vectors such as Glove and FastText vectors are already trained on the Wikipedia corpus and we used them directly for our semantic similarity calculations. In this way, we imported information about word semantics from the Wiki corpus into our semantic similarity calculations. Importing knowledge from pre-trained word vectors or pre-trained statistical models is called <strong class="bold">transfer learning</strong>.</p>
			<p>Transformers offer thousands of pre-trained models to perform NLP tasks, such as text classification, text summarization, question answering, machine translation, and natural language generation in more than 100 languages. Transformers aim to make state-of-the-art NLP accessible to everyone. </p>
			<p>The following screenshot shows a list of the Transformer models provided by HuggingFace (we'll learn about HuggingFace Transformers in the <em class="italic">HuggingFace Transformers</em> section). Each model is named with a combination of the architecture name (<em class="italic">Bert, DistilBert</em>, and so on), possibly the language code (<em class="italic">en</em>, <em class="italic">de</em>, <em class="italic">multilingual</em>, and similar, given on the left side of the following screenshot), and information regarding whether the model is cased or uncased (the model distinguishes between uppercase and lowercase characters). </p>
			<p>Also, on the <a id="_idIndexMarker590"/>left-hand side of <em class="italic">Figure 9.1</em>, we see the task names. Each model <a id="_idIndexMarker591"/>is labeled with a task name. We select a model that is suitable for our task, such as text classification or machine translation:</p>
			<div><div><img src="img/B16570_9_1.jpg" alt="Figure 9.1 – A list of HuggingFace Transformers, taken from the HuggingFace website&#13;&#10;" width="1050" height="799"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – A list of HuggingFace Transformers, taken from the HuggingFace website</p>
			<p>To understand what's great about transformers, we'll first revisit LSTM architecture from <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a>, <em class="italic">Text Classification with spaCy</em>. In the previous chapter, we already stepped into the statistical modeling world with Keras and LSTM architecture. LSTMs are great for modeling text; however, they have some shortcomings too:</p>
			<ul>
				<li>LSTM architecture sometimes has difficulties with learning long text. Statistical dependencies in a long text can be difficult to represent by an LSTM because, as the time steps pass, LSTM can forget some of the words that were processed at earlier time steps.</li>
				<li>The nature of LSTMs is sequential. We process one word at each time step. Obviously, parallelizing the learning process is not possible; we have to process sequentially. Not allowing parallelization creates a performance bottleneck.</li>
			</ul>
			<p>Transformers <a id="_idIndexMarker592"/>address these problems by not using recurrent layers at all. If we <a id="_idIndexMarker593"/>have a look at the following, the architecture looks completely different from an LSTM architecture. Transformer architecture <a id="_idIndexMarker594"/>consists of two parts – an input encoder (called the <strong class="bold">Encoder</strong>) block on the left, and <a id="_idIndexMarker595"/>the output decoder (called the <strong class="bold">Decoder</strong>) block on the right. The following diagram is taken from this paper and exhibits the transformer architecture:</p>
			<div><div><img src="img/B16570_9_2.jpg" alt="Figure 9.2 – Transformer architecture from the paper entitled &quot;Attention is All You Need&quot;&#13;&#10;" width="360" height="516"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – Transformer architecture from the paper entitled "Attention is All You Need"</p>
			<p>The preceding <a id="_idIndexMarker596"/>architecture is invented for a machine translation <a id="_idIndexMarker597"/>task; hence, the input is a sequence of words from the source language, and the output is a sequence of words in the target language. The encoder generates a vector representation of the input words and passes them to the decoder (the word vector transfer is represented by the arrow from the encoder block in the direction of the decoder block). The decoder takes these input word vectors, transforms the output words into word vectors, and finally generates the probability of each output word (labeled in <em class="italic">Figure 9.2</em> as <strong class="bold">Output Probabilities</strong>).</p>
			<p>Inside the encoder and decoder blocks, we see feedforward layers, which are basically a dense layer we used in <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a>, <em class="italic">Text Classification with spaCy</em>. The innovation transformers bring lies in the <strong class="bold">Multi-Head Attention</strong> block. This block creates a dense representation <a id="_idIndexMarker598"/>for each word by using a self-attention mechanism. The <strong class="bold">Self-attention</strong> mechanism <a id="_idIndexMarker599"/>relates each word in the input sentence to the other words <a id="_idIndexMarker600"/>in the input sentence. The word embedding of each word is calculated by taking a weighted average of the other words' embeddings. This way, the importance of each word in the input sentence is calculated, so the architecture focuses its <em class="italic">attention</em> on each input word in turn. </p>
			<p>The following diagram is taken from the original paper and illustrates self-attention. The diagram illustrates how the input words on the left-hand side attend the input word "<strong class="bold">it</strong>" on the right-hand side. Darker colors mean more relevance, hence the words "<strong class="bold">the animal</strong>" are more related to "<strong class="bold">it</strong>" rather than the other words in this sentence. What does this mean? This means that the transformer can resolve what the pronoun "<strong class="bold">it</strong>" refers to precisely in this sentence, which is the phrase "<strong class="bold">the animal</strong>." This is a great accomplishment of transformers; they can resolve many semantic dependencies in a given sentence: </p>
			<div><div><img src="img/B16570_9_3.jpg" alt="Figure 9.3 – Illustration of the self-attention mechanism&#13;&#10;" width="389" height="399"/>
				</div>
			</div>
			<p class="figure-caption">  </p>
			<p class="figure-caption">Figure 9.3 – Illustration of the self-attention mechanism</p>
			<p>If you want <a id="_idIndexMarker601"/>to learn about the details of the Transformer architecture, you can visit http://jalammar.github.io/illustrated-transformer/. This talk on YouTube also explains the self-attention mechanism and transformers for all levels of NLP developers: <a href="https://www.youtube.com/watch?v=rBCqOTEfxvg">https://www.youtube.com/watch?v=rBCqOTEfxvg</a>. </p>
			<p>We have <a id="_idIndexMarker602"/>already seen that there is a variety of transformer architectures and, depending <a id="_idIndexMarker603"/>on the task, we use different types of transformers for different tasks, such as text classification and machine translation. In the rest of this chapter, we'll work with a very popular transformer architecture – BERT. Let's see the BERT architecture and how to use it in our NLP applications in the next section.</p>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor161"/>Understanding BERT</h1>
			<p>In this <a id="_idIndexMarker604"/>section, we'll explore the most influential and commonly used Transformer model, BERT. BERT is introduced in Google's research paper here: <a href="https://arxiv.org/pdf/1810.04805.pdf">https://arxiv.org/pdf/1810.04805.pdf</a>. </p>
			<p>What does BERT do exactly? To understand what BERT outputs, let's dissect the name:</p>
			<ul>
				<li><strong class="bold">Bidirectional</strong>: Training <a id="_idIndexMarker605"/>on the text data is bi-directional, which means each input sentence is processed from left to right as well as from right to left. </li>
				<li><strong class="bold">Encoder</strong>: An <a id="_idIndexMarker606"/>encoder encodes the input sentence.</li>
				<li><strong class="bold">Representations</strong>: A <a id="_idIndexMarker607"/>representation is a word vector.</li>
				<li><strong class="bold">Transformers</strong>: The <a id="_idIndexMarker608"/>architecture is transformer-based.</li>
			</ul>
			<p>BERT is essentially a trained transformer encoder stack. Input into BERT is a sentence, and the output is a sequence of word vectors. The word vectors are contextual, which means that a word vector is assigned to a word based on the input sentence. In short, BERT outputs <strong class="bold">contextual word representations</strong>.</p>
			<p>We have already <a id="_idIndexMarker609"/>seen a number of issues that transformers aim to solve in the previous section. Another problem that transformers address concerns word vectors. In <a href="B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087"><em class="italic">Chapter 5</em></a><em class="italic">, Working with Word Vectors and Semantic Similarity</em>, we saw that word vectors are context-free; the word vector for a word is <em class="italic">always</em> the same independent of the sentence it is used in. The following diagram explains this problem:</p>
			<div><div><img src="img/B16570_9_4.jpg" alt="Figure 9.4 – Word vector for the word &quot;bank&quot; &#13;&#10;" width="915" height="274"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4 – Word vector for the word "bank" </p>
			<p>Here, even though the word <strong class="bold">bank</strong> has two completely different meanings in these two sentences, the word vectors are the same, because Glove and FastText are <strong class="bold">static</strong>. Each word has only one vector and vectors are saved to a file following training. Then, we download these pre-trained vectors and load them into our application. </p>
			<p>On <a id="_idIndexMarker610"/>the contrary, BERT word vectors are <em class="italic">dynamic</em>. BERT can generate different word vectors for the same word depending on the input sentence. The following diagram shows the word vectors generated by BERT, in contrast to the word vector in <em class="italic">Figure 9.4</em>:</p>
			<p class="figure-caption">  </p>
			<div><div><img src="img/B16570_9_5.jpg" alt="Figure 9.5 – Two distinct word vectors generated by BERT for the same word, &quot;bank,&quot; in two different contexts&#13;&#10;" width="899" height="245"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5 – Two distinct word vectors generated by BERT for the same word, "bank," in two different contexts</p>
			<p>How does BERT generate these word vectors? In the next section, we'll explore the details of the BERT architecture. </p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor162"/>BERT architecture</h2>
			<p>As has <a id="_idIndexMarker611"/>already been remarked in the previous section, BERT is a transformer encoder stack, which means that several encoder layers are stacked on top of each other. The first layer initializes the word vectors randomly, and then each encoder layer transforms the output of the previous encoder layer. The paper introduces two model sizes for BERT: BERT Base and BERT Large. The following diagram shows the BERT architecture:</p>
			<div><div><img src="img/B16570_9_6.jpg" alt="Figure 9.6 – BERT Base and Large architectures, having 12 and 24 encoder layers, respectively&#13;&#10;" width="655" height="618"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6 – BERT Base and Large architectures, having 12 and 24 encoder layers, respectively</p>
			<p>Both BERT models have a huge number of encoder layers. BERT Base has 12 encoder layers and BERT Large has 24 encoder layers. The dimensions of the resulting word vectors are different too; BERT Base generates word vectors of size 768 and BERT Large generates word vectors of size 1024.</p>
			<p>As we <a id="_idIndexMarker612"/>remarked in the previous section, BERT outputs word vectors for each input word. The following diagram exhibits a high-level overview of BERT inputs and outputs (discard the CLS token for now; you'll learn about it in the <em class="italic">BERT input format</em> section):</p>
			<div><div><img src="img/B16570_9_7.jpg" alt="Figure 9.7 – BERT model input word and output word vectors&#13;&#10;" width="1075" height="1014"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – BERT model input word and output word vectors</p>
			<p>In the <a id="_idIndexMarker613"/>preceding diagram, we can see a high-level overview of BERT inputs and outputs. Indeed, BERT input has to be in a special format and includes some special tokens, such as CLS, in <em class="italic">Figure 9.7</em>. In the next section, you'll learn about the details of the BERT input format.</p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor163"/>BERT input format</h2>
			<p>We have covered BERT architecture, so let's now understand how to generate the output <a id="_idIndexMarker614"/>vectors using BERT. For this purpose, we'll get to know the BERT input data format. The BERT input format can represent a single sentence, as well as a pair of sentences (for tasks such as question answering and semantic similarity, we input two sentences to the model) in a single sequence of tokens.</p>
			<p>BERT works <a id="_idIndexMarker615"/>with a class of <code>[CLS]</code>, <code>[SEP]</code>, and <code>[PAD]</code>:</p>
			<ul>
				<li>The first special token of BERT is <code>[CLS]</code>. The first token of every input sequence has to be <code>[CLS]</code>. We use this token in classification tasks as an aggregate of the input sentence. We ignore this token in non-classification tasks. </li>
				<li><code>[SEP]</code> means a <code>[CLS] sentence [SEP]</code>, and for two sentences, the input looks like <code>[CLS] sentence1 [SEP] sentence2 [SEP]</code>.</li>
				<li><code>[PAD]</code> is a <a id="_idIndexMarker618"/>special token meaning <strong class="bold">padding</strong>. Recall from the previous chapter that we use padding values to make sentences in our dataset of equal length. BERT receives sentences of a fixed length; hence, we pad the short sentences before feeding them to BERT. The maximum length of tokens we can feed to BERT is <strong class="bold">512</strong>.</li>
			</ul>
			<p>How about tokenizing the words? Recall from the previous section that we fed a sentence to our Keras model one word at a time. We tokenized our input sentences into words using the spaCy tokenizer. BERT works slightly differently, BERT uses WordPiece tokenization. A "word piece" is literally a piece of a word. The WordPiece algorithm breaks words down into several subwords. The idea is to break down complex/long tokens into simpler tokens. For example, the word <code>playing</code> is tokenized as <code>play</code> and <code>##ing</code>. A <code>##</code> character is placed before every word piece to indicate that this token is not a word from the language's vocabulary but that it's a word piece.</p>
			<p>Let's take a look at some more examples:</p>
			<pre>playing  play, ##ing
played   play, ##ed
going    go, ##ing
vocabulary = [play,go, ##ing, ##ed]</pre>
			<p>This way, we represent the language vocabulary more compactly as WordPiece groups common <a id="_idIndexMarker619"/>subwords. WordPiece tokenization creates wonders on rare/unseen words, as these words are broken down into their subwords.</p>
			<p>After tokenizing the input sentence and adding the special tokens, each token is converted to its ID. After that, as a final step, we feed the sequence of token IDs to BERT.</p>
			<p>To summarize, this is how we transform a sentence into BERT input format:</p>
			<div><div><img src="img/B16570_9_8.jpg" alt="Figure 9.8 – Transforming an input sentence into BERT input format&#13;&#10;" width="845" height="672"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.8 – Transforming an input sentence into BERT input format</p>
			<p>BERT Tokenizer has different methods for performing all the tasks described previously, but it also <a id="_idIndexMarker620"/>has an encoding method that combines these steps into a single step. We'll see how to use BERT Tokenizer in detail in the <em class="italic">Transformers and TensorFlow</em> section. Before that, we'll learn about the algorithms that are used to train BERT.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor164"/>How is BERT trained?</h2>
			<p>BERT is <a id="_idIndexMarker621"/>trained on a large unlabeled Wiki corpus and a huge book corpus. Creators of BERT stated the following in Google Research's BERT GitHub repository, <a href="https://github.com/google-research/bert">https://github.com/google-research/bert</a>, as follows:</p>
			<p class="author-quote">"We then train a large model (12-layer to 24-layer Transformer) on a large corpus (Wikipedia + BookCorpus) for a long time (1M update steps), and that's BERT."</p>
			<p>BERT is <a id="_idIndexMarker622"/>trained with two training <a id="_idIndexMarker623"/>methods, <strong class="bold">masked language model</strong> (<strong class="bold">MLM</strong>) and <strong class="bold">next sentence prediction</strong> (<strong class="bold">NSP</strong>). Let's first go over <a id="_idIndexMarker624"/>the details of masked language modeling.</p>
			<p>Language modeling is the task of predicting the next token given the sequence of previous tokens. For example, given the sequence of words <em class="italic">Yesterday I visited</em>, a language model can predict the next token as one of the tokens <em class="italic">church</em>, <em class="italic">hospital</em>, <em class="italic">school</em>, and so on. Masked language modeling is a bit different. In this approach, we mask a percentage of the tokens randomly by replacing them with a <code>[MASK]</code> token and expect MLM to predict the masked words. </p>
			<p>The <a id="_idIndexMarker625"/>masked language model in BERT is implemented as follows: First, 15 of the input tokens are chosen at random. Then, the following happens: </p>
			<ol>
				<li>80% of the tokens chosen are replaced with <code>[MASK]</code>.</li>
				<li>20% of the tokens chosen are replaced with another token from the vocabulary.</li>
				<li>The remaining 10% are left unchanged.</li>
			</ol>
			<p>A training example sentence to LMM looks like the following: </p>
			<pre>[CLS] Yesterday I [MASK] my friend at [MASK] house [SEP] </pre>
			<p>Next, we will look into the details of the other algorithm, NSP.</p>
			<p>As the name suggests, NSP is the task of predicting the next sentence given an input sentence. In this approach, we feed two sentences to BERT and expect BERT to predict the order of the sentences, more specifically, if the second sentence is the sentence following the first sentence.</p>
			<p>Let's make an example input to NSP. We'll feed two sentences separated by the <code>[SEP]</code> token as input:</p>
			<pre>[CLS] A man robbed a [MASK] yesterday [MASK] 8 o'clock [SEP] He [MASK] the bank with 6 million dollars [SEP]
Label = IsNext</pre>
			<p>In this example, the second sentence can follow the first sentence; hence, the predicted label is <code>IsNext</code>. How about this example:</p>
			<pre>[CLS] Rabbits like to [MASK] carrots and [MASK] leaves [SEP] [MASK] Schwarzenegger is elected as the governor of [MASK] [SEP]
Label= NotNext</pre>
			<p>This <a id="_idIndexMarker626"/>example pair of sentences generate the <code>NotNext</code> label, as obviously they are not contextually or semantically related.</p>
			<p>That's it! We have learned about BERT architecture; we also learned the details of BERT input data format and how BERT is trained. Now, we're ready to dive into TensorFlow code. In the next section, we'll see how to apply what we learned so far in our TensorFlow code. </p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor165"/>Transformers and TensorFlow</h1>
			<p>In this section, we'll dive into transformers code with TensorFlow. Pre-trained transformer <a id="_idIndexMarker627"/>models are provided to the developer community as open <a id="_idIndexMarker628"/>source by many organizations, including Google (<a href="https://github.com/google-research/bert">https://github.com/google-research/bert</a>), Facebook (<a href="https://github.com/pytorch/fairseq/blob/master/examples/language_model/README.md">https://github.com/pytorch/fairseq/blob/master/examples/language_model/README.md</a>), and HuggingFace (<a href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a>). All the listed organizations offer pre-trained models and nice interfaces to integrate transformers into our Python code. The interfaces are compatible with either PyTorch or Tensorflow or both.</p>
			<p>Throughout this chapter, we'll be using HuggingFace's pre-trained transformers and their TensorFlow interface to the transformer models. HuggingFace is an AI company with a focus on NLP and quite devoted to open source. In the next section, we'll take a closer look at what is available in HuggingFace Transformers. </p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor166"/>HuggingFace Transformers</h2>
			<p>In the first section, we'll discover HuggingFace's pre-trained models, the TensorFlow interface for <a id="_idIndexMarker629"/>using these models, and HuggingFace model conventions in general. We saw in <em class="italic">Figure 9.1</em> that HuggingFace offers different sorts of models. Each model is dedicated to a task such as text classification, question answering, and sequence-to-sequence modeling. </p>
			<p>The following diagram is taken from the HuggingFace documentation and shows details of the distilbert-base-uncased-distilled-squad model. In the documentation, first, the task is tagged (upper-left corner of the diagram; the <em class="italic">Question Answering</em> tag), followed by supported deep learning libraries (PyTorch, TensorFlow, TFLite, TFSavedModel for this model), the dataset it trained on (squad, in this instance), the model language (<em class="italic">en</em> for English), and the license and base model's name (DistilBERT in this case). </p>
			<p>Some models are trained with similar algorithms, and so belong to the same model family. By way of an example, the DistilBERT family includes many models, such as distilbert-base-uncased and distilbert-multilingual-cased. Each model name also includes some information, such as casing (the model recognizes uppercase/lowercase differences) or the model language, such as <code>en</code>, <code>de</code>, or <code>multilingual</code>:</p>
			<div><div><img src="img/B16570_9_9.jpg" alt="Figure 9.9 – Documentation of the distilbert-base-uncased-distilled-squad model&#13;&#10;" width="520" height="262"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.9 – Documentation of the distilbert-base-uncased-distilled-squad model</p>
			<p>We already <a id="_idIndexMarker630"/>explored BERT in detail in the previous section. HuggingFace documentation provides information about each model family and an individual model's API in detail. <em class="italic">Figure 9.10</em> shows a list of available models and a list of BERT model architecture variations:</p>
			<div><div><img src="img/B16570_9_10.jpg" alt="Figure 9.10 – List of the available models on the left-hand side, with a list of the BERT model variations on the right-hand side&#13;&#10;" width="323" height="280"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.10 – List of the available models on the left-hand side, with a list of the BERT model variations on the right-hand side</p>
			<p>The BERT model has many variations for a variety of tasks, such as text classification, question <a id="_idIndexMarker631"/>answering, and next sentence prediction. Each of these models is obtained by placing some extra layers on top of the BERT output. Recall from the previous section that the BERT output is a sequence of word vectors for each word of the input sentence. For example, the BERTForSequenceClassification model is obtained by placing a dense layer (we covered dense layers in the previous chapter) on top of the BERT word vectors. </p>
			<p>In the rest of this chapter, we'll explore how to use some of these architectures for our tasks as well as how to use BERT word vectors with Keras. Before all of these tasks, we will start with the basic task of tokenization to prepare our input sentences. Let's see the tokenizer code in the next section.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor167"/>Using the BERT tokenizer</h2>
			<p>In the <em class="italic">Understanding BERT</em> section, we already saw that BERT uses the WordPiece algorithm <a id="_idIndexMarker632"/>for tokenization. Every input word is broken down into subwords. Let's see how to prepare our input data with the HuggingFace library.</p>
			<p>The following lines exhibit the basic usage of the tokenizer:</p>
			<pre>from transformers import BertTokenizer
btokenizer =\
BertTokenizer.from_pretrained('bert-base-uncased')
tokens = btokenizer.tokenize(sentence)
tokens
['he', 'lived', 'characteristic', '##ally', 'idle', 'and', 'romantic', '.']
ids = btokenizer.convert_tokens_to_ids(tokens)
ids
[2002, 2973, 8281, 3973, 18373, 1998, 6298, 1012]</pre>
			<p>Here are <a id="_idIndexMarker633"/>the steps we followed in the preceding code block:</p>
			<ol>
				<li value="1">First, we imported <code>BertTokenizer</code>. Different models have different tokenizers; for instance, XLNet model's tokenizer is called <code>XLNetTokenizer</code>. </li>
				<li>Second, we called the <code>from_pretrained</code> method on the tokenizer object and provided the model's name. Note that we don't need to download the pre-trained bert-base-uncased model manually; this method downloads the model by itself. </li>
				<li>Then, we called the <code>tokenize</code> method. <code>tokenize</code> basically tokenizes the sentence by breaking all the words down into subwords. </li>
				<li>We print tokens to examine the subwords. The words "he," "lived," "idle," and so on exist in Tokenizer's vocabulary, and so are kept as they are. "Characteristically" is a rare word, so does not exist in Tokenizer's vocabulary. Then, Tokenizer splits this word into the subwords "characteristic" and "##ally." Notice that "##ally" starts with the characters "##" to emphasize the fact that this is a piece of word.</li>
				<li>Next, we convert tokens to their token IDs by calling <code>convert_tokens_to_ids</code>.</li>
			</ol>
			<p>How about <code>[CLS]</code> and <code>[SEP]</code> tokens? In the previous section, we already saw that we have to add these two special tokens to the beginning and end of the input sentence. For the preceding code, we need to involve one more step and add our special tokens manually. Can we do all of these preprocessing steps in a single step, perhaps? The answer is yes; BERT provides a method called <code>encode</code> that does the following:</p>
			<ul>
				<li>Adds <code>CLS</code> and <code>SEP</code> tokens to the input sentence</li>
				<li>Tokenizes the sentence by breaking the tokens down into subwords</li>
				<li>Converts the tokens to their token IDs</li>
			</ul>
			<p>We call <a id="_idIndexMarker634"/>the <code>encode</code> method directly on the input sentence as follows: </p>
			<pre>from transformers import BertTokenizer
btokenizer =\
BertTokenizer.from_pretrained('bert-base-uncased')
sentence = "He lived characteristically idle and romantic."
ids = btokenizer.encode(sentence)
ids
[101, 2002, 2973, 8281, 3973, 18373, 1998, 6298, 1012, 102]</pre>
			<p>This code segment outputs the token IDs in just a single step, instead of calling <code>tokenize</code> and <code>convert_tokens_to_ids</code> one after the other. The result is a Python list. </p>
			<p>How about padding the sentence? We already saw in the previous section that all the input sentences in a dataset should be of equal length because BERT cannot process variable-length sentences. Hence, we need to pad the short sentences to the length of the longest sentence available in the dataset. Also, if we want to use a TensorFlow tensor instead of a plain list, we need to write some conversion code. The HuggingFace library provides <code>encode_plus</code> to make our life easier and combine all these steps into one method as follows:</p>
			<pre>from transformers import BertTokenizer
btokenizer =\
BertTokenizer.from_pretrained('bert-base-uncased')
sentence = "He lived characteristically idle and romantic."
encoded = btokenizer.encode_plus(
        text=sentence,
        add_special_tokens=True,
        max_length=12,
        pad_to_max_length=True,
        return_tensors="tf"
) 
token_ids = encoded["input_ids"]
print(token_ids)
tf.Tensor([[  101  2002  2973  8281  3973 18373  1998  6298  1012   102     0     0]], shape=(1, 12), dtype=int32)</pre>
			<p>Here, we called <code>encode_plus</code> on our input sentence directly. Our sentence is now padded to <a id="_idIndexMarker635"/>a length of 12 (the two <code>0</code> IDs at the end of the sequence are the pad tokens), while the special tokens <code>[CLS]</code> and <code>[SEP]</code> are also added to the sentence. The output is directly a TensorFlow tensor, including the token IDs. </p>
			<p>The <code>encode_plus</code> method <a id="_idIndexMarker636"/>takes the following parameters: </p>
			<ul>
				<li><code>text</code>: Input sentence.</li>
				<li><code>add_special_tokens</code>: Add <code>CLS</code> and <code>SEP</code> tokens.</li>
				<li><code>max_length</code>: The maximum length you want your sentence to be. If the sentence is shorter than <code>max_length</code> tokens, we want to pad the sentence.</li>
				<li><code>pad_to_max_length</code>: We feed <code>True</code> if we want to pad the sentence, otherwise <code>False</code>.</li>
				<li><code>return_tensors</code>: We pass this parameter if we want the output to be a tensor, otherwise the output is a Python list. The available options are <code>tf</code> and <code>pt</code> for TensorFlow and PyTorch, respectively.</li>
			</ul>
			<p>As we can see, BERT Tokenizer provides several methods on input sentences. Preparing data is not so straightforward, but you'll get used to it by practicing. We always encourage you to try out the code examples with your own text. </p>
			<p>Now, we're <a id="_idIndexMarker637"/>ready to process the transformed input sentences. Let's go ahead and provide our input sentences to the BERT model to obtain BERT word vectors.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Always check the name of the tokenizer class that you should use with your transformer. A list of models and their corresponding tokenizers is available at <a href="https://huggingface.co/transformers/">https://huggingface.co/transformers/</a>.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor168"/>Obtaining BERT word vectors</h2>
			<p>In this section, we'll examine the output of the BERT model. As we stated in the <em class="italic">Understanding BERT</em> section, the output of the BERT model is a sequence of word vectors, one vector <a id="_idIndexMarker638"/>per input word. BERT has a special output format and, in this section, we'll examine BERT outputs in detail.</p>
			<p>Let's see the code first:</p>
			<pre>from transformers import BertTokenizer, TFBertModel
btokenizer =\
 BertTokenizer.from_pretrained('bert-base-uncased')
bmodel = TFBertModel.from_pretrained("bert-base-uncased")
sentence = "He was idle."
encoded = btokenizer.encode_plus(
        text=sentence,
        add_special_tokens=True,
        max_length=10,
        pad_to_max_length=True,
        return_attention_mask=True,
        return_tensors="tf"
)
inputs = encoded["input_ids"] 
outputs = bmodel(inputs)</pre>
			<p>This code is very similar to the code from the previous section. Here, we also imported <code>TFBertModel</code>. After that, we initialized out BERT model with the pre-trained model, <code>bert-base-uncased</code>. Then, we transformed our input sentence to BERT input format with <code>encode_plus</code> and captured the result, <code>tf.tensor</code>, in the input variable. We fed our sentence to <a id="_idIndexMarker639"/>the BERT model and captured this output with the <code>outputs</code> variable. What's inside the <code>outputs</code> variable then?</p>
			<p>The output of the BERT model is a tuple of two elements. Let's print the shapes of the output pair:</p>
			<pre>outputs[0].shape
(1, 10, 768)
outputs[1].shape
(1, 768)</pre>
			<p>The first element of the output is the shape <code>(batch size, sequence length, hidden size)</code>. We fed only one sentence, hence the batch size here is 1 (the batch size is how many sentences we feed to the model at once). The sequence length here is 10 because we fed <code>max_length=10</code> to the tokenizer and padded our sentence to a length of 10. <code>hidden_size</code> is a parameter of BERT. In the <em class="italic">BERT Architecture</em> section, we already remarked that BERT's hidden layer size is 768, and so produces word vectors with a dimension of 768. So, the first output element contains 768-dimensional vectors per word, hence it contains 10 words x 768-dimensional vectors. </p>
			<p>The second output is only one vector of 768-dimension. This vector is basically the word embedding of the <code>[CLS]</code> token. Recall from the <em class="italic">BERT input format</em> section that the <code>[CLS]</code> token is an aggregate of the whole sentence. You can think of <code>[CLS]</code> token's embedding as the pooled version of embeddings of all the words in the sentence. The shape of the <a id="_idIndexMarker640"/>second element of the output tuple is always (<code>batch size,</code> <code>hidden_size</code>). Basically, we collect the <code>[CLS]</code> token's embedding per input sentence.</p>
			<p>Great! We extracted the BERT embeddings. Next, we'll use these embeddings to train our text classification model with TensorFlow and tf.keras. </p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor169"/>Using BERT for text classification </h2>
			<p>In this section, we'll train a binary text classifier with BERT and tf.keras. We'll reuse some of <a id="_idIndexMarker641"/>the code from the previous chapter, but this time the code will be much <a id="_idIndexMarker642"/>shorter because we'll replace the embedding and LSTM layers with BERT. The complete code is available at the GitHub repository: <a href="https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter09/BERT_spam.ipynb">https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter09/BERT_spam.ipynb</a>.ipynb. In this section, we'll skip the data preparation. We used the SMS Spam Collection dataset from Kaggle. You can find the dataset under the <code>data/</code> directory at the GitHub repository, too. </p>
			<p>Let's get started by importing the BERT models and tokenizer:</p>
			<pre>from transformers import BertTokenizer, TFBertModel
bert_tokenizer =\
BertTokenizer.from_pretrained("bert-base-uncased")
bmodel = TFBertModel.from_pretrained("bert-base-uncased")</pre>
			<p>We have imported the <code>BertTokenizer</code> tokenizer and the BERT model, <code>TFBertModel</code>. We initialized both the tokenizer and the BERT model with the pre-trained bert-base-uncased model. Notice that the model's name starts with TF – the names of all of the HuggingFace pre-trained models for TensorFlow start with TF. Please pay attention to this detail when you want to play with other transformer models in the future. </p>
			<p>We'll also import Keras layers and functions, together with <code>numpy</code>:</p>
			<pre>import numpy as np
import tensorflow
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.models import Model</pre>
			<p>Now, we're ready to process the input data with <code>BertTokenizer</code>:</p>
			<pre>input_ids=[]
 
for sent in sentences:
   bert_inp=bert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =64,pad_to_max_length = True,return_attention_mask = True)
   input_ids.append(bert_inp['input_ids'])
input_ids=np.asarray(input_ids)
labels=np.array(labels)</pre>
			<p>As we saw in the <em class="italic">Using the BERT tokenizer</em> section, this code segment will generate token <a id="_idIndexMarker643"/>IDs for each input sentence of the dataset and append them to a list. Labels are the list of class labels and consist of 0 and 1s. Then we convert the Python lists, <code>input_ids</code>, and labels to <code>numpy</code> arrays to feed them to our Keras model.</p>
			<p>Finally, we <a id="_idIndexMarker644"/>define our Keras model by means of the following lines:</p>
			<pre>inputs = Input(shape=(64,), dtype="int32")
bert = bmodel(inputs)
bert = bert[1]
outputs = Dense(units=1, activation="sigmoid")(bert)
model = Model(inputs, outputs)</pre>
			<p>That's it! We defined our BERT-based text classifier in only five lines of code! Let's dissect the code:</p>
			<ol>
				<li value="1">First, we defined the input layer, which inputs the sentences to our model. The shape is <code>(64,)</code> because each input sentence is 64 tokens in length. We padded each sentence to the length of 64 tokens when we called the <code>encode_plus</code> method.</li>
				<li>Next, we fed the input sentences to the BERT model.</li>
				<li>At the third line, we extracted the second output of the BERT output. Recall from the previous section that the BERT model's output is a tuple. The first element of the output tuple is a sequence of word vectors, and the second element <a id="_idIndexMarker645"/>is a single vector that represents the whole sentence, called <code>bert[1]</code> extracts the pooled output vector; this is a vector of shape (1, 768).</li>
				<li>Next, we squashed the pooled output vector to a vector of shape 1 by a sigmoid function, which is the class label. </li>
				<li>We defined our Keras model with the inputs and outputs.</li>
			</ol>
			<p>Here, the BERT model takes only one line but can transfer the enormous knowledge of the Wiki <a id="_idIndexMarker646"/>corpus to your model. At the end of the training, this model obtains an accuracy of <code>0.96</code>. We usually fit the model for one epoch due to fact that BERT overfits easily even on a moderate size corpus. </p>
			<p>The rest <a id="_idIndexMarker647"/>of the code handles compiling and fitting the Keras model. Note that BERT has huge memory requirements also. You can see how much RAM is required from Google Research's GitHub link: <a href="https://github.com/google-research/bert#out-of-memory-issues">https://github.com/google-research/bert#out-of-memory-issues</a>. </p>
			<p>If you have trouble running this section's code on your machine, you can use <strong class="bold">Google Colab</strong>, which provides a Jupyter notebook environment through your browser. You can start <a id="_idIndexMarker648"/>using Google Colab immediately through <a href="https://colab.research.google.com/notebooks/intro.ipynb">https://colab.research.google.com/notebooks/intro.ipynb</a>. Our training code runs on Google Colab for around 1.5 hours, while bigger datasets can take more time even though it's just one epoch.</p>
			<p>In this section, we learned how to train a Keras model with BERT from scratch. Now, we'll switch to an easier task. We'll explore how to use a pre-trained transformer pipeline. Let's move on to the next section for the details.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor170"/>Using Transformer pipelines</h2>
			<p>The HuggingFace Transformers library provides pipelines to help developers benefit from transformer <a id="_idIndexMarker649"/>code immediately without any custom training. A <strong class="bold">pipeline</strong> is a tokenizer <a id="_idIndexMarker650"/>and a pre-trained model combined.</p>
			<p>HuggingFace provides a variety of models for a variety of NLP tasks. Here are some tasks that <a id="_idIndexMarker651"/>HuggingFace pipelines offer:</p>
			<ul>
				<li><strong class="bold">Sentiment analysis</strong></li>
				<li><strong class="bold">Question answering</strong></li>
				<li><strong class="bold">NER</strong></li>
				<li><strong class="bold">Text summarization</strong></li>
				<li><strong class="bold">Translation</strong></li>
			</ul>
			<p>You can <a id="_idIndexMarker652"/>see the full list of tasks at the Huggingface documentation: <a href="https://huggingface.co/transformers/task_summary.html">https://huggingface.co/transformers/task_summary.html</a>. In this section, we'll explore the pipelines for sentiment analysis and question answering (the use of pipelines with other tasks is similar).  </p>
			<p>Let's go through some examples. We'll start with sentiment analysis:</p>
			<pre>from transformers import pipeline
nlp = pipeline("sentiment-analysis")
 
sent1 = "I hate you so much right now."
sent2 = "I love fresh air and exercising."
result1 = nlp(sent1)
result2 = nlp(sent2)</pre>
			<p>In the preceding code snippet, we took the following steps:</p>
			<ol>
				<li value="1">First, we imported the pipeline function from the <code>transformers</code> library. This function creates pipeline objects with the task name given as a parameter. Hence, we created our sentiment analysis pipeline object, <code>nlp</code>, by calling this function on the second line.</li>
				<li>Next, we define two example sentences with negative and positive sentiment, respectively. </li>
				<li>Then we feed these sentences to the pipeline object, <code>nlp</code>. </li>
			</ol>
			<p>Here is <a id="_idIndexMarker653"/>the output:</p>
			<pre>result1
[{'label': 'NEGATIVE', 'score': 0.9984998}]
result2
[{'label': 'POSITIVE', 'score': 0.99987185}]</pre>
			<p>This worked great! Next, we'll play with question answering. Let's see the code:</p>
			<pre>from transformers import pipeline
nlp = pipeline("question-answering")
 
res = nlp({
    'question': 'What is the name of this book?',
    'context': "I'll publish my new book Mastering spaCy soon."
})
print(res)
{'score': 0.0007240351873990664, 'start': 25, 'end': 40, 'answer': 'Mastering spaCy'}</pre>
			<p>Again, we imported the pipeline function and used it to create a pipeline object, <code>nlp</code>. In <em class="italic">question-answering</em> tasks, we need to provide a context (the same background information for the model to work on) to the model as well as our question. We asked the model about the name of this book after giving the information that our new publication will be out soon. The answer is <code>Mastering spaCy</code>; the transformer worked wonders on this pair! We encourage you to try out your own examples.</p>
			<p>We have <a id="_idIndexMarker654"/>completed our exploration of HuggingFace transformers. Now, we will move on to our final section of this chapter and see what spaCy offers us as regards transformers. </p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor171"/>Transformers and spaCy</h1>
			<p>spaCy v3.0 was released with great new features and components. The most exciting new feature <a id="_idIndexMarker655"/>is undoubtedly <strong class="bold">transformer-based pipelines</strong>. The new <a id="_idIndexMarker656"/>transformer-based pipelines bring spaCy's accuracy to the state of the art. Integrating transformers into the spaCy NLP pipeline introduced <a id="_idIndexMarker657"/>one more pipeline component called <strong class="bold">Transformer</strong>. This component allows us to use all HuggingFace models with spaCy pipelines. If we recall from <a href="B16570_02_Final_JM_ePub.xhtml#_idTextAnchor037"><em class="italic">Chapter 2</em></a>, <em class="italic">Core Operations with spaCy</em>, this is what the spaCy NLP pipeline looks like without transformers:</p>
			<div><div><img src="img/B16570_9_11.jpg" alt=" Figure 9.11 – Vector-based spaCy pipeline components&#13;&#10;" width="473" height="93"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 9.11 – Vector-based spaCy pipeline components</p>
			<p>With the release of v3.0, v2 style spaCy models are still supported and transformer-based models are introduced. A transformer-based pipeline component looks like the following:</p>
			<div><div><img src="img/B16570_9_12.jpg" alt="Figure 9.12 – Transformed-based spaCy pipeline components&#13;&#10;" width="1404" height="168"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.12 – Transformed-based spaCy pipeline components</p>
			<p>For each <a id="_idIndexMarker658"/>supported language, transformer-based models and v2 style models <a id="_idIndexMarker659"/>are listed under the Models page of the documentation (English for an example: <a href="https://spacy.io/models/en">https://spacy.io/models/en</a>). Transformer-based models can have different size and pipeline components, just like v2 style models. Also, each model has corpus and genre information as well, just like the v2 style models. Here is an example of an English transformer-based language model from the Models page:</p>
			<div><div><img src="img/B16570_9_13.jpg" alt="Figure 9.13 – spaCy English transformer-based language models &#13;&#10;" width="529" height="373"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.13 – spaCy English transformer-based language models </p>
			<p>As we see from the preceding screenshot, the first pipeline component is a transformer and the rest <a id="_idIndexMarker660"/>of the pipeline components are the ones we already covered in <a href="B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055"><em class="italic">Chapter 3</em></a>, <em class="italic">Linguistic Features</em>. The transformer component generates the word representations <a id="_idIndexMarker661"/>and deals with the WordPiece algorithm to tokenize words into subwords. The word vectors are fed to the rest of the pipeline.</p>
			<p>Downloading, loading, and using transformer-based models are identical to v2 style models. Currently, English has two pre-trained transformer-based models, <code>en_core_web_trf</code> and <code>en_core_web_lg</code>. Let's get started by downloading the <code>en_core_web_trf</code> model:</p>
			<pre>python3 -m spacy download en_core_web_trf</pre>
			<p>This should produce output similar to the following:</p>
			<pre>Collecting en-core-web-trf==3.0.0
  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.0.0/en_core_web_trf-3.0.0-py3-none-any.whl (459.7 MB)
     |████████████████████████████████| 459.7 MB 40 kB/s 
Requirement already satisfied: spacy&lt;3.1.0,&gt;=3.0.0 in /usr/local/lib/python3.6/dist-packages (from en-core-web-trf==3.0.0) (3.0.5)</pre>
			<p>Once the model download is complete, the following output should be generated:</p>
			<pre>Successfully installed en-core-web-trf-3.0.0 spacy-alignments-0.8.3 spacy-transformers-1.0.2 tokenizers-0.10.2 transformers-4.5.1
<img src="img/01.png" alt="" width="36" height="30"/> Download and installation successful
You can now load the package via spacy.load('en_core_web_trf')</pre>
			<p>Loading a <a id="_idIndexMarker662"/>transformer-based model is identical to what we do for v2 style models, too:</p>
			<pre>import spacy
nlp = spacy.load("en_core_web_trf")</pre>
			<p>After loading <a id="_idIndexMarker663"/>our model and initializing the pipeline, we can use this model in the same way we used v2 style models:</p>
			<pre>doc = nlp("I visited my friend Betty at her house.")
doc.ents
(Betty,)
for word in doc:
    print(word.pos_, word.lemma_)
... 
PRON I
VERB visit
PRON my
NOUN friend
PROPN Betty
ADP at
PRON her
NOUN house
PUNCT .</pre>
			<p>So far so good, but what's new then? Let's examine some features that come from the transformer <a id="_idIndexMarker664"/>component. We can access the features related to the transformer <a id="_idIndexMarker665"/>component via <code>doc._.trf_data.trf_data</code>, which contains the word pieces, input <code>ids</code>, and vectors that are generated by the transformer. Let's examine the features one by one:</p>
			<pre>doc = nlp("It went there unwillingly.")
doc._.trf_data.wordpieces
WordpieceBatch(strings=[['&lt;s&gt;', 'It', 'Gwent', 'Gthere', 'Gunw', 'ill', 'ingly', '.', '&lt;/s&gt;']], input_ids=array([[    0,   243,   439,    89, 10963,  1873,  7790,     4,     2]]), attention_mask=array([[1, 1, 1, 1, 1, 1, 1, 1, 1]]), lengths=[9], token_type_ids=None)</pre>
			<p>In the preceding output, we see five elements: word pieces, input IDs, attention masks, lengths, and token type IDs. Word pieces are the subwords that are generated by the WordPiece algorithm. The word pieces of this sentence are as follows:</p>
			<pre>&lt;s&gt;
It
Gwent
Gthere
Gunw
Ill
ingly
.
&lt;/s&gt;</pre>
			<p>Here, <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> are special tokens and are used in the sentence at the beginning and end. The word <code>unwillingly</code> is divided into three subwords – <code>unw</code>, <code>ill</code>, and <code>ingly</code>. The <code>G</code> character is used to mark the word boundaries. The tokens without <code>G</code> are subwords, such as <code>ill</code> and <code>ingly</code> in the preceding word piece list (with the exception of the first word in the sentence, the first word of the sentence is marked by <code>&lt;s&gt;</code>). </p>
			<p>Next, we have to take a look at <code>input_ids.</code> Input IDs have the same meaning as the input IDs we introduced in the <em class="italic">Using the BERT tokenizer</em> section. These are basically the subword IDs assigned by the transformer's tokenizer. </p>
			<p>The attention <a id="_idIndexMarker666"/>mask is a list of 0s and 1s for pointing the transformer <a id="_idIndexMarker667"/>to those tokens it should pay attention to. <code>0</code> corresponds to <code>PAD</code> tokens, while all the other tokens should have a corresponding <code>1</code>. </p>
			<p><code>lengths</code> is the length of the sentence after breaking it down into subwords. Here, it's <code>9</code> obviously, but notice that <code>len(doc)</code> outputs <code>5</code>, while spaCy always operates on linguistic words. </p>
			<p><code>token_type_ids</code> are used by transformer tokenizers to mark the sentence boundaries for two sentence input tasks, such as question and answering. Here, we provide only one text, hence this feature is not applicable.</p>
			<p>We can see that the token vectors generated by the transformer, <code>doc._.trf_data.tensors</code>, contain the output of the transformer, a sequence of word vectors per word, and the pooled output vector (we introduced these concepts in the <em class="italic">Obtaining BERT word vectors</em> section. If you need to refresh your memory, please refer to this section): </p>
			<pre>doc._.trf_data.tensors[0].shape
(1, 9, 768)
doc._.trf_data.tensors[1].shape
(1, 768)</pre>
			<p>The first element of the tuple is the vectors for the tokens. Each vector is <code>768</code>-dimensional; hence <code>9</code> words produce <code>9</code> x <code>768</code>-dimensional vectors. The second element of the tuple is the pooled output vector, which is an aggregate representation for the input sentence, and so is of the shape <code>1</code>x<code>768</code>.</p>
			<p>This concludes our exploration of spaCy transformer-based pipelines. Once again, we saw that spaCy <a id="_idIndexMarker668"/>provides user-friendly API and packaging, even for complicated <a id="_idIndexMarker669"/>models such as transformers. Transformer integration is yet another great reason to use spaCy for NLP. </p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor172"/>Summary</h1>
			<p>You have completed an exhaustive chapter about a very hot topic in NLP. Congratulations! In this chapter, you started by learning what sort of models transformers are and what transfer learning is. Then, you learned about the commonly used Transformer architecture, BERT. You learned the architecture details and the specific input format, as well as the BERT Tokenizer and WordPiece algorithm. </p>
			<p>Next, you became familiar with BERT code by using the popular HuggingFace Transformers library. You practiced fine-tuning BERT on a custom dataset for a sentiment analysis task with TensorFlow and Keras. You also practiced using pre-trained HuggingFace pipelines for a variety of NLP tasks, such as text classification and question answering. Finally, you explored the spaCy and Transformers integration of the new spaCy release, spaCy v3.0.</p>
			<p>By the end of this chapter, you had completed the statistical NLP sections of this book. Now you're ready to put everything you learned together to build a modern NLP pipeline. Let's move on to the next chapter and see how we use our new statistical skills!</p>
		</div>
	</div></body></html>