<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 11. Packaging Spark Applications"><div class="titlepage"><div><div><h1 class="title"><a id="ch11"/>Chapter 11. Packaging Spark Applications</h1></div></div></div><p>So far we have been working with a very convenient way of developing code in Spark - the Jupyter notebooks. Such an approach is great when you want to develop a proof of concept and document what you do along the way.</p><p>However, Jupyter notebooks will not work if you need to schedule a job, so it runs every hour. Also, it is fairly hard to package your application as it is not easy to split your script into logical chunks with well-defined APIs - everything sits in a single notebook.</p><p>In this chapter, we will learn how to write your scripts in a reusable form of modules and submit jobs to Spark programmatically.</p><p>Before you begin, however, you might want to check out the <span class="emphasis"><em>Bonus Chapter 2, Free Spark Cloud Offering</em></span> where we provide instructions on how to subscribe and use either Databricks' Community Edition or Microsoft's HDInsight<a id="id702" class="indexterm"/> Spark offerings; the instructions on how to do so can be found here: <a class="ulink" href="https://www.packtpub.com/sites/default/files/downloads/FreeSparkCloudOffering.pdf">https://www.packtpub.com/sites/default/files/downloads/FreeSparkCloudOffering.pdf</a>.</p><p>In this chapter you will learn:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">What the <code class="literal">spark-submit</code> command is</li><li class="listitem" style="list-style-type: disc">How to package and deploy your app programmatically</li><li class="listitem" style="list-style-type: disc">How to modularize your Python code and submit it along with PySpark script</li></ul></div><div class="section" title="The spark-submit command"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec73"/>The spark-submit command</h1></div></div></div><p>The entry<a id="id703" class="indexterm"/> point for submitting jobs to Spark (be it locally or on a cluster) is the <code class="literal">spark-submit</code> script. The script, however, allows you not only to submit the jobs (although that is its main purpose), but also kill jobs or check their status.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note64"/>Note</h3><p>Under the hood, the <code class="literal">spark-submit</code> command passes the call to the <code class="literal">spark-class</code> script that, in turn, starts a launcher Java application. For those interested, you can<a id="id704" class="indexterm"/> check the GitHub repository for Spark: <a class="ulink" href="https://github.com/apache/spark/blob/master/bin/spark-submit">https://github.com/apache/spark/blob/master/bin/sparksubmit</a>t.</p></div></div><p>The <code class="literal">spark-submit</code> command provides a unified API for deploying apps on a variety of Spark supported cluster managers (such as Mesos or Yarn), thus relieving you from configuring your<a id="id705" class="indexterm"/> application for each of them separately.</p><p>On the general level, the syntax looks as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>spark-submit [options] &lt;python file&gt; [app arguments]</strong></span>
</pre></div><p>We will go through the list of all the options soon. The <code class="literal">app arguments</code> are the parameters you want to pass to your application.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note65"/>Note</h3><p>You can either parse the parameters from the command line yourself using <code class="literal">sys.argv</code> (after <code class="literal">import sys</code>) or you can utilize the <code class="literal">argparse </code>module for Python.</p></div></div><div class="section" title="Command line parameters"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec104"/>Command line parameters</h2></div></div></div><p>You can pass a host of different parameters for Spark engine when using <code class="literal">spark-submit</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note66"/>Note</h3><p>In what follows we will cover only the parameters specific for Python (as <code class="literal">spark-submit</code> can also be used to submit applications written in Scala or Java and packaged as <code class="literal">.jar</code> files).</p></div></div><p>We will now<a id="id706" class="indexterm"/> go through the parameters one-by-one so you have a good overview of what you can do from the command line:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">--master</code>: Parameter<a id="id707" class="indexterm"/> used to set the URL of the master (head) node. Allowed syntax is:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">local</code>: Used for executing your code on your local machine. If you pass <code class="literal">local</code>, Spark will then run in a single thread (without leveraging any parallelism). On a multi-core machine you can specify either, the exact number of cores for Spark to use by stating <code class="literal">local[n]</code> where <code class="literal">n</code> is the number of cores to use, or run Spark spinning as many threads as there are cores on the machine using <code class="literal">local[*]</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">spark://host:port</code>: It is a URL and a port for the Spark standalone cluster (that does not run any job scheduler such as Mesos or Yarn).</li><li class="listitem" style="list-style-type: disc"><code class="literal">mesos://host:port</code>: It is a URL and a port for the Spark cluster deployed over Mesos.</li><li class="listitem" style="list-style-type: disc"><code class="literal">yarn</code>: Used to submit jobs from a head node that runs Yarn as the workload balancer.</li></ul></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">--deploy-mode</code>: Parameter<a id="id708" class="indexterm"/> that allows you to decide whether to launch the Spark driver process locally (using <code class="literal">client</code>) or on one of the worker machines inside the cluster (using the <code class="literal">cluster</code> option). The default for this parameter is <code class="literal">client</code>. Here's an excerpt from Spark's documentation that explains the differences with more specificity (source: <a class="ulink" href="http://bit.ly/2hTtDVE">http://bit.ly/2hTtDVE</a>):<div class="blockquote"><blockquote class="blockquote"><p>A common deployment strategy is to submit your application from [a screen session on] a gateway machine that is physically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster). In this setup, client mode is appropriate. In client mode, the driver is launched directly within the spark-submit process which acts as a client to the cluster. The input and output of the application is attached to the console. Thus, this mode is especially suitable for applications that involve the REPL (e.g. Spark shell).</p><p>Alternatively, if your application is submitted from a machine far from the worker machines (e.g. locally on your laptop), it is common to use cluster mode to minimize network latency between the drivers and the executors. Currently, standalone mode does not support cluster mode for Python applications.</p></blockquote></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">--name</code>: Name<a id="id709" class="indexterm"/> of your application. Note that if you specified the name of your app programmatically when creating <code class="literal">SparkSession</code> (we will get to that in the next section) then the parameter from the command line will be overridden. We will explain the precedence of parameters shortly when discussing the <code class="literal">--conf</code> parameter.</li><li class="listitem" style="list-style-type: disc"><code class="literal">--py-files</code>: Comma-delimited<a id="id710" class="indexterm"/> list of <code class="literal">.py</code>, <code class="literal">.egg</code> or <code class="literal">.zip</code> files to include for Python apps. These files will be delivered to each executor for use. Later in this chapter we will show you how to package your code into a module.</li><li class="listitem" style="list-style-type: disc"> <code class="literal">--files</code>: Command<a id="id711" class="indexterm"/> gives a comma-delimited list of files that will also be delivered to each executor to use.</li><li class="listitem" style="list-style-type: disc"><code class="literal">--conf</code>: Parameter <a id="id712" class="indexterm"/>to change a configuration of your app dynamically from the command line. The syntax is <code class="literal">&lt;Spark property&gt;=&lt;value for the property&gt;</code>. For example, you can pass <code class="literal">--conf spark.local.dir=/home/SparkTemp/</code> or <code class="literal">--conf spark.app.name=learningPySpark</code>; the latter would be an equivalent of submitting the <code class="literal">--name</code> property as explained previously.<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note67"/>Note</h3><p>Spark uses the configuration parameters from three places: the parameters from the <code class="literal">SparkConf</code> you specify when creating <code class="literal">SparkContext</code> within your app take the highest precedence, then any parameter that you pass to the <code class="literal">spark-submit</code> script from the command line, and lastly, any parameter that is specified in the <code class="literal">conf/spark-defaults.conf</code> file.</p></div></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">--properties-file</code>: File with a configuration. It should have the same set of<a id="id713" class="indexterm"/> properties as the <code class="literal">conf/spark-defaults.conf</code> file as it will be read instead of it.</li><li class="listitem" style="list-style-type: disc"><code class="literal">--driver-memory</code>: Parameter that specifies how much memory to allocate for<a id="id714" class="indexterm"/> the application on the driver. Allowed values have a syntax similar to the 1,000M, 2G. The default is 1,024M.</li><li class="listitem" style="list-style-type: disc"><code class="literal">--executor-memory</code>: Parameter that specifies how much memory to allocate for<a id="id715" class="indexterm"/> the application on each of the executors. The default is 1G.</li><li class="listitem" style="list-style-type: disc"><code class="literal">--help</code>: Shows<a id="id716" class="indexterm"/> the help message and exits.</li><li class="listitem" style="list-style-type: disc"><code class="literal">--verbose</code>: Prints<a id="id717" class="indexterm"/> additional debug information when running your app.</li><li class="listitem" style="list-style-type: disc"><code class="literal">--version</code>: Prints<a id="id718" class="indexterm"/> the version of Spark.</li></ul></div><p>In a Spark standalone with <code class="literal">cluster</code> deploy mode only, or on a cluster deployed over Yarn, you can use the <code class="literal">--driver-cores </code>that allows specifying the number of cores for the driver (default is 1). In a Spark standalone or Mesos with <code class="literal">cluster</code> deploy mode only you also have the opportunity to use either of these:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">--supervise</code>: <a id="id719" class="indexterm"/>Parameter that, if specified, will restart the driver if it is lost or fails. This also can be set in Yarn by setting the <code class="literal">--deploy-mode</code> to <code class="literal">cluster</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">--kill</code>: Will finish<a id="id720" class="indexterm"/> the process given its <code class="literal">submission_id</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">--status</code>: If this<a id="id721" class="indexterm"/> command is specified, it will request the status of the specified app</li></ul></div><p>In a Spark standalone and Mesos only (with the <code class="literal">client</code> deploy mode) you can also specify the <code class="literal">--total-executor-cores</code>, a parameter that will request the number of cores specified for all executors (not each). On the other hand, in a Spark standalone and YARN, only the <code class="literal">--executor-cores </code>parameter specifies the number of cores per executor (defaults to 1 in YARN mode, or to all available cores on the worker in standalone mode).</p><p>In addition, when submitting to a YARN cluster you can specify:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">--queue</code>: This parameter<a id="id722" class="indexterm"/> specifies a queue on YARN to submit the job to (default is <code class="literal">default</code>)</li><li class="listitem" style="list-style-type: disc"><code class="literal">--num-executors</code>: Parameter that specifies how many executor machines to<a id="id723" class="indexterm"/> request for the job. If dynamic allocation is enabled, the initial number of executors will be at least the number specified.</li></ul></div><p>Now that we have discussed all the parameters it is time to put it into practice.</p></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Deploying the app programmatically"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec74"/>Deploying the app programmatically</h1></div></div></div><p>Unlike the<a id="id724" class="indexterm"/> Jupyter notebooks, when you use the <code class="literal">spark-submit</code> command, you need to prepare the <code class="literal">SparkSession</code> yourself and configure it so your application runs properly.</p><p>In this section, we will learn how to create and configure the <code class="literal">SparkSession</code> as well as how to use<a id="id725" class="indexterm"/> modules external to Spark.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note68"/>Note</h3><p>If you have not created your free account with either Databricks or Microsoft (or any other provider of Spark) do not worry - we will be still using your local machine as this is easier to get us started. However, if you decide to take your application to the cloud it will literally only require changing the <code class="literal">--master</code> parameter when you submit the job.</p></div></div><div class="section" title="Configuring your SparkSession"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec105"/>Configuring your SparkSession</h2></div></div></div><p>The main<a id="id726" class="indexterm"/> difference between using Jupyter and<a id="id727" class="indexterm"/> submitting jobs programmatically is the fact that you have to create your Spark context (and Hive, if you plan to use HiveQL), whereas when running Spark with Jupyter the contexts are automatically started for you.</p><p>In this section, we will develop a simple app that will use public data from Uber with trips made in the NYC area in June 2016; we downloaded the dataset from <a class="ulink" href="https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-06.csv">https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-06.csv</a> (beware as it is an almost 3GB file). The original dataset contains 11 million trips, but for our example we retrieved only 3.3 million and selected only a subset of all available columns.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note69"/>Note</h3><p>The transformed dataset can be downloaded from <a class="ulink" href="http://www.tomdrabas.com/data/LearningPySpark/uber_data_nyc_2016-06_3m_partitioned.csv.zip">http://www.tomdrabas.com/data/LearningPySpark/uber_data_nyc_2016-06_3m_partitioned.csv.zip</a>. Download the file and unzip it to the <code class="literal">Chapter13</code> folder from GitHub. The file might look strange as it is actually a directory containing four files inside that, when read by Spark, will form one dataset.</p></div></div><p>So, let's get to it!</p></div><div class="section" title="Creating SparkSession"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec106"/>Creating SparkSession</h2></div></div></div><p>Things with<a id="id728" class="indexterm"/> Spark 2.0 have become slightly<a id="id729" class="indexterm"/> simpler than with previous versions when it comes to creating <code class="literal">SparkContext</code>. In fact, instead of creating a <code class="literal">SparkContext</code> explicitly, Spark currently uses <code class="literal">SparkSession</code> to expose higher-level functionality. Here's how you do it:</p><div class="informalexample"><pre class="programlisting">from pyspark.sql import SparkSession

spark = SparkSession \
        .builder \
        .appName('CalculatingGeoDistances') \
        .getOrCreate()

print('Session created')</pre></div><p>The preceding<a id="id730" class="indexterm"/> code is all that you need!</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip45"/>Tip</h3><p>If you want to use RDD API you still can. However, you do not need to create a <code class="literal">SparkContext</code> anymore as <code class="literal">SparkSession</code> starts one under the hood. To get the access you can simply call (borrowing from the preceding example): <code class="literal">sc = spark.SparkContext</code>.</p></div></div><p>In this example, we first create the <code class="literal">SparkSession</code> object and call its <code class="literal">.builder</code> internal class. The <code class="literal">.appName(...)</code> method allows us to give our application a name, and the <code class="literal">.getOrCreate()</code> method either creates or retrieves an already created <code class="literal">SparkSession</code>. It is a good convention to give your application a meaningful name as it helps to (1) find your application on a cluster and (2) creates less confusion for everyone.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note70"/>Note</h3><p>Under the hood, the Spark session creates a <code class="literal">SparkContext</code> object. When you call <code class="literal">.stop()</code> on <code class="literal">SparkSession</code> it actually terminates the <code class="literal">SparkContext</code> within.</p></div></div></div><div class="section" title="Modularizing code"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec107"/>Modularizing code</h2></div></div></div><p>Building your<a id="id731" class="indexterm"/> code in such a way so it can be reused later is always a good thing. The same can be done with Spark - you can modularize your methods and then reuse them at a later point. It also aids readability of your code and its maintainability.</p><p>In this example, we will build a module that would do some calculations on our dataset: It will compute the <span class="emphasis"><em>as-the-crow-flies</em></span> distance (in miles) between the pickup and drop-off locations (using the Haversine formula), and also will convert the calculated distance from miles into kilometers.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note71"/>Note</h3><p>More on<a id="id732" class="indexterm"/> the Haversine formula can be found here: <a class="ulink" href="http://www.movable-type.co.uk/scripts/latlong.html">http://www.movable-type.co.uk/scripts/latlong.html</a>.</p></div></div><p>So, first, we will<a id="id733" class="indexterm"/> build a module.</p><div class="section" title="Structure of the module"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl3sec20"/>Structure of the module</h3></div></div></div><p>We put the<a id="id734" class="indexterm"/> code for our extraneous methods inside the <code class="literal">additionalCode</code> folder.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip46"/>Tip</h3><p>Check out the GitHub repository for this book if you have not done so already <a class="ulink" href="https://github.com/drabastomek/learningPySpark/tree/master/Chapter11">https://github.com/drabastomek/learningPySpark/tree/master/Chapter11</a>.</p></div></div><p>The tree for the folder looks as follows:</p><div class="mediaobject"><img src="images/B05793_11_01.jpg" alt="Structure of the module"/></div><p>As you can see, it has a structure of a somewhat normal Python package: At the top we have the <code class="literal">setup.py</code> file so we can package up our module, and then inside we have our code.</p><p>The <code class="literal">setup.py </code>file in our case looks as follows:</p><div class="informalexample"><pre class="programlisting">from setuptools import setup

setup(
    name='PySparkUtilities',
    version='0.1dev',
    packages=['utilities', 'utilities/converters'],
    license='''
        Creative Commons 
        Attribution-Noncommercial-Share Alike license''',
    long_description='''
        An example of how to package code for PySpark'''
)</pre></div><p>We will not delve<a id="id735" class="indexterm"/> into details here on the structure (which on its own is fairly self-explanatory): You can read more about how to define <code class="literal">setup.py</code> files for<a id="id736" class="indexterm"/> other projects here <a class="ulink" href="https://pythonhosted.org/an_example_pypi_project/setuptools.html">https://pythonhosted.org/an_example_pypi_project/setuptools.html</a>.</p><p>The <code class="literal">__init__.py</code> file in the utilities folder has the following code:</p><div class="informalexample"><pre class="programlisting">from .geoCalc import geoCalc
__all__ = ['geoCalc','converters']</pre></div><p>It effectively exposes the <code class="literal">geoCalc.py</code> and <code class="literal">converters</code> (more on these shortly).</p></div><div class="section" title="Calculating the distance between two points"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl3sec21"/>Calculating the distance between two points</h3></div></div></div><p>The first<a id="id737" class="indexterm"/> method we mentioned uses the Haversine formula to calculate the direct distance between any two points on a map (Cartesian coordinates). The code that does this lives in the <code class="literal">geoCalc.py</code> file of the module.</p><p>The <code class="literal">calculateDistance(...)</code> is a static method of the <code class="literal">geoCalc</code> class. It takes two geo-points, expressed as either a tuple or a list with two elements (latitude and longitude, in that order), and uses the Haversine formula to calculate the distance. The Earth's radius necessary to calculate the distance is expressed in miles so the distance calculated will also be in miles.</p></div><div class="section" title="Converting distance units"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl3sec22"/>Converting distance units</h3></div></div></div><p>We build<a id="id738" class="indexterm"/> the utilities package so it can be more universal. As a part of the package we expose methods to convert between various units of measurement.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note72"/>Note</h3><p>At this time we limit it to the distance only, but the functionality can be further extended to other domains such as area, volume, or temperature.</p></div></div><p>For ease of use, any class<a id="id739" class="indexterm"/> implemented as a <code class="literal">converter</code> should expose the same interface. That is why it is advised that such a class derives from our <code class="literal">BaseConverter</code> class (see <code class="literal">base.py</code>):</p><div class="informalexample"><pre class="programlisting">from abc import ABCMeta, abstractmethod

class BaseConverter(metaclass=ABCMeta):
    @staticmethod
    @abstractmethod
    def convert(f, t):
        raise NotImplementedError</pre></div><p>It is a purely abstract class that cannot be instantiated: Its sole purpose is to force the derived classes to implement the <code class="literal">convert(...)</code> method. See the <code class="literal">distance.py</code> file for details of the implementation. The code should be self-explanatory for someone proficient in Python so we will not be going through it step-by-step here.</p></div><div class="section" title="Building an egg"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl3sec23"/>Building an egg</h3></div></div></div><p>Now that we<a id="id740" class="indexterm"/> have all our code in place we can package it. The documentation for PySpark states that you can pass <code class="literal">.py</code> files (using the <code class="literal">--py-files</code> switch) to the <code class="literal">spark-submit</code> script separated by commas. However, it is much more convenient to package our module into a <code class="literal">.zip</code> or an <code class="literal">.egg</code>. This is when the <code class="literal">setup.py</code> file comes handy - all you have to do is to call this inside the <code class="literal">additionalCode</code> folder:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python setup.py bdist_egg</strong></span>
</pre></div><p>If all goes well you should see three additional folders: <code class="literal">PySparkUtilities.egg-info</code>, <code class="literal">build</code>, and <code class="literal">dist</code> - we are interested in the file that sits in the <code class="literal">dist</code> folder: The <code class="literal">PySparkUtilities-0.1.dev0-py3.5.egg</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip47"/>Tip</h3><p>After running the preceding command, you might find that the name of your <code class="literal">.egg</code> file is slightly different as you might have a different Python version. You can still use it in your Spark jobs, but you will have to adapt the <code class="literal">spark-submit</code> command to reflect the name of your <code class="literal">.egg</code> file.</p></div></div></div><div class="section" title="User defined functions in Spark"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl3sec24"/>User defined functions in Spark</h3></div></div></div><p>In order to<a id="id741" class="indexterm"/> do operations on <code class="literal">DataFrame</code>s in PySpark you have two options: Use built-in functions to work with data (most of the time it will be sufficient to achieve what you need and it is recommended as<a id="id742" class="indexterm"/> the code is more performant) or create your own user-defined functions.</p><p>To define a UDF you have to wrap the Python function within the <code class="literal">.udf(...)</code> method and define its return value type. This is how we do it in our script (check the <code class="literal">calculatingGeoDistance.py</code> file):</p><div class="informalexample"><pre class="programlisting">import utilities.geoCalc as geo
from utilities.converters import metricImperial

getDistance = func.udf(
    lambda lat1, long1, lat2, long2: 
        geo.calculateDistance(
            (lat1, long1),
            (lat2, long2)
        )
    )

convertMiles = func.udf(lambda m: 
    metricImperial.convert(str(m) + ' mile', 'km'))</pre></div><p>We can then use such functions to calculate the distance and convert it to miles:</p><div class="informalexample"><pre class="programlisting">uber = uber.withColumn(
    'miles', 
        getDistance(
            func.col('pickup_latitude'),
            func.col('pickup_longitude'), 
            func.col('dropoff_latitude'), 
            func.col('dropoff_longitude')
        )
    )

uber = uber.withColumn(
    'kilometers', 
    convertMiles(func.col('miles')))</pre></div><p>Using the <code class="literal">.withColumn(...)</code> method we create additional columns with the values of interest to us.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note73"/>Note</h3><p>A word of caution needs to be stated here. If you use the PySpark built-in functions, even though you call them Python objects, underneath that call is translated and executed as Scala code. If, however, you write your own methods in Python, it is not translated into Scala and, hence, has to be executed on the driver. This causes<a id="id743" class="indexterm"/> a significant performance hit. Check out this answer from Stack Overflow for more details: <a class="ulink" href="http://stackoverflow.com/questions/32464122/spark-performance-for-scala-vs-python">http://stackoverflow.com/questions/32464122/spark-performance-for-scala-vs-python</a>.</p></div></div><p>Let's now<a id="id744" class="indexterm"/> put all the puzzles together and finally submit our job.</p></div></div><div class="section" title="Submitting a job"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec108"/>Submitting a job</h2></div></div></div><p>In your CLI type the following (we assume you keep the structure of the folders unchanged from<a id="id745" class="indexterm"/> how it is structured on GitHub):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./launch_spark_submit.sh \</strong></span>
<span class="strong"><strong>--master local[4] \</strong></span>
<span class="strong"><strong>--py-files additionalCode/dist/PySparkUtilities-0.1.dev0-py3.5.egg \</strong></span>
<span class="strong"><strong>calculatingGeoDistance.py</strong></span>
</pre></div><p>We owe you some explanation for the <code class="literal">launch_spark_submit.sh</code> shell script. In Bonus <a class="link" href="ch01.html" title="Chapter 1. Understanding Spark">Chapter 1</a>, <span class="emphasis"><em>Installing Spark</em></span>, we configured our Spark instance to run Jupyter (by setting the <code class="literal">PYSPARK_DRIVER_PYTHON</code> system variable to <code class="literal">jupyter</code>). If you were to simply use <code class="literal">spark-submit</code> on a machine configured in such a way, you would most likely get some variation of the following error:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>jupyter: 'calculatingGeoDistance.py' is not a Jupyter command</strong></span>
</pre></div><p>Thus, before running the <code class="literal">spark-submit</code> command we first have to unset the variable and then run the code. This would quickly become extremely tiring so we automated it with the <code class="literal">launch_spark_submit.sh</code> script:</p><div class="informalexample"><pre class="programlisting">#!/bin/bash

unset PYSPARK_DRIVER_PYTHON
spark-submit $*
export PYSPARK_DRIVER_PYTHON=jupyter</pre></div><p>As you can see, this is nothing more than a wrapper around the <code class="literal">spark-submit </code>command.</p><p>If all goes well, you will see the following <span class="emphasis"><em>stream of consciousness</em></span> appearing in your CLI:</p><div class="mediaobject"><img src="images/B05793_11_02.jpg" alt="Submitting a job"/></div><p>There's a host<a id="id746" class="indexterm"/> of useful things that you can get from reading the output:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Current version of Spark: 2.1.0</li><li class="listitem" style="list-style-type: disc">Spark UI (what will be useful to track the progress of your job) is started successfully on <code class="literal">http://localhost:4040</code></li><li class="listitem" style="list-style-type: disc">Our <code class="literal">.egg</code> file was added successfully to the execution</li><li class="listitem" style="list-style-type: disc">The <code class="literal">uber_data_nyc_2016-06_3m_partitioned.csv</code> was read successfully</li><li class="listitem" style="list-style-type: disc">Each start and stop of jobs and tasks are listed</li></ul></div><p>Once the job finishes, you will see something similar to the following:</p><div class="mediaobject"><img src="images/B05793_11_03.jpg" alt="Submitting a job"/></div><p>From the preceding screenshot, we can read that the distances are reported correctly. You can also see<a id="id747" class="indexterm"/> that the Spark UI process has now been stopped and all the clean up jobs have been performed.</p></div><div class="section" title="Monitoring execution"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec109"/>Monitoring execution</h2></div></div></div><p>When you use the <code class="literal">spark-submit </code>command, Spark launches a local server that allows you to track<a id="id748" class="indexterm"/> the execution of the job. Here's what the window looks like:</p><div class="mediaobject"><img src="images/B05793_11_04.jpg" alt="Monitoring execution"/></div><p>At the top you can switch between the <span class="strong"><strong>Jobs</strong></span> or <span class="strong"><strong>Stages</strong></span> view; the <span class="strong"><strong>Jobs</strong></span> view allows you to track the distinct<a id="id749" class="indexterm"/> jobs that are executed to complete the whole script, while the <span class="strong"><strong>Stages</strong></span> view allows you to track all the stages that are executed.</p><p>You can also peak inside each stage execution profile and track each task execution by clicking on the link of the stage. In the following screenshot, you can see the execution profile for Stage 3 with four tasks running:</p><div class="mediaobject"><img src="images/B05793_11_05.jpg" alt="Monitoring execution"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip48"/>Tip</h3><p>In a cluster setup instead of <span class="strong"><strong>driver/localhost</strong></span> you would see the driver number and host's IP address.</p></div></div><p>Inside a job or a stage, you can click on the DAG Visualization to see how your job or stage gets executed (the following chart on the left shows the <span class="strong"><strong>Job</strong></span> view, while the one on the right shows the <span class="strong"><strong>Stage</strong></span> view):</p><div class="mediaobject"><img src="images/B05793_11_06.jpg" alt="Monitoring execution"/></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Databricks Jobs"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec75"/>Databricks Jobs</h1></div></div></div><p>If you are using the Databricks product, an easy way to go from development from your Databricks notebooks<a id="id750" class="indexterm"/> to production is to use the Databricks Jobs feature. It will allow you to:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Schedule your Databricks notebook to run on an existing or new cluster</li><li class="listitem" style="list-style-type: disc">Schedule at your desired frequency (from minutes to months)</li><li class="listitem" style="list-style-type: disc">Schedule time out and retries for your job</li><li class="listitem" style="list-style-type: disc">Be alerted when the job starts, completes, and/or errors out</li><li class="listitem" style="list-style-type: disc">View historical job runs as well as review the history of the individual notebook job runs</li></ul></div><p>This capability greatly simplifies the scheduling and production workflow of your job submissions. Note that you will need to upgrade your Databricks subscription (from Community edition) to use this feature.</p><p>To use this feature, go to the Databricks <span class="strong"><strong>Jobs</strong></span> menu and click on <span class="strong"><strong>Create Job</strong></span>. From here, fill out the job name<a id="id751" class="indexterm"/> and then choose the notebook that you want to turn into a job, as shown in the following screenshot:</p><div class="mediaobject"><img src="images/B05793_11_07.jpg" alt="Databricks Jobs"/></div><p>Once you have chosen your notebook, you can also choose whether to use an existing cluster that is running or have the job scheduler launch a <span class="strong"><strong>New Cluster</strong></span> specifically for this job, as shown in the following screenshot:</p><div class="mediaobject"><img src="images/B05793_11_08.jpg" alt="Databricks Jobs"/></div><p>Once you<a id="id752" class="indexterm"/> have chosen your notebook and cluster; you can set the schedule, alerts, timeout, and retries. Once you have completed setting up your job, it should look<a id="id753" class="indexterm"/> something similar to the <span class="strong"><strong>Population vs. Price Linear Regression Job</strong></span>, as noted in the following screenshot:</p><div class="mediaobject"><img src="images/B05793_11_09.jpg" alt="Databricks Jobs"/></div><p>You can test the job by clicking on the <span class="strong"><strong>Run Now</strong></span> link under <span class="strong"><strong>Active runs</strong></span> to test your job.</p><p>As noted in the <span class="strong"><strong>Meetup Streaming RSVPs</strong></span> Job, you can view the history of your completed runs; as shown<a id="id754" class="indexterm"/> in the screenshot, for this notebook there are <span class="strong"><strong>50</strong></span> completed job runs:</p><div class="mediaobject"><img src="images/B05793_11_10.jpg" alt="Databricks Jobs"/></div><p>By clicking<a id="id755" class="indexterm"/> on the job run (in this case, <span class="strong"><strong>Run 50</strong></span>), you can see the results of that job run. Not only can you view the start time, duration, and status, but also the results for that specific job:</p><div class="mediaobject"><img src="images/B05793_11_11.jpg" alt="Databricks Jobs"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note74"/>Note</h3><p>
<span class="strong"><strong>REST Job Server</strong></span>
</p><p>A popular way to run jobs is to also use REST APIs. If you are using Databricks, you can run your jobs using the Databricks REST APIs. If you prefer to manage your own job server, a popular open source REST Job Server is <code class="literal">spark-jobserver</code> - a RESTful interface for submitting and managing Apache Spark jobs, jars, and job contexts. The project recently (at the time of writing) was updated so it can handle PySpark jobs.</p><p>For more information, please refer to <a class="ulink" href="https://github.com/spark-jobserver/spark-jobserver">https://github.com/spark-jobserver/spark-jobserver</a>.</p></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec76"/>Summary</h1></div></div></div><p>In this chapter, we walked you through the steps on how to submit applications written in Python to Spark from the command line. The selection of the <code class="literal">spark-submit </code>parameters has been discussed. We also showed you how you can package your Python code and submit it alongside your PySpark script. Furthermore, we showed you how you can track the execution of your job.</p><p>In addition, we also provided a quick overview of how to run Databricks notebooks using the Databricks Jobs feature. This feature simplifies the transition from development to production, allowing you to take your notebook and execute it as an end-to-end workflow.</p><p>This brings us to the end of this book. We hope you enjoyed the journey, and that the material contained herein will help you start working with Spark using Python. Good luck!</p></div></div>
</body></html>