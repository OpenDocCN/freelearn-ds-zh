<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-149"><a id="_idTextAnchor160"/>7</h1>
<h1 id="_idParaDest-150"><a id="_idTextAnchor161"/>Using iSAX to Approximate MPdist</h1>
<p>So far in this book, we have seen the use of iSAX for searching subsequences and joining iSAX indexes based on SAX representations but no other applications of it.</p>
<p>In this chapter, we are going to use iSAX indexes to approximately calculate the <strong class="bold">Matrix Profile</strong> vectors as well as the <strong class="bold">MPdist</strong> distance between two time series – we are still going to use iSAX for searching and joining, but the end results are going to be more sophisticated. The idea that governs this chapter is the perception that <em class="italic">terminal nodes in an iSAX index group have similar subsequences</em> from a SAX representation perspective – this is what we are trying to take advantage of for our approximate computations.</p>
<p>In this chapter, we are going to cover the following main topics:</p>
<ul>
<li>Understanding the Matrix Profile</li>
<li>Computing the Matrix Profile using iSAX</li>
<li>Understanding MPdist</li>
<li>Calculating MPdist using iSAX</li>
<li>Implementing the MPdist calculation in Python</li>
<li>Using the Python code</li>
</ul>
<h1 id="_idParaDest-151"><a id="_idTextAnchor162"/>Technical requirements</h1>
<p>The GitHub repository for the book is at <a href="https://github.com/PacktPublishing/Time-Series-Indexing">https://github.com/PacktPublishing/Time-Series-Indexing</a>. The code for each chapter is in its own directory. Therefore, the code for <a href="B14769_07.xhtml#_idTextAnchor160"><em class="italic">Chapter 7</em></a> can be found in the <code>ch07</code> folder of the GitHub repository.</p>
<h1 id="_idParaDest-152"><a id="_idTextAnchor163"/>Understanding the Matrix Profile</h1>
<p>Time series are <a id="_idIndexMarker442"/>everywhere, and there are many tasks that we might need to perform on large time series including similarity search, outlier detection, classification, and clustering. Dealing directly with a large time series is very time-consuming and is going to slow down the process. Most of the aforementioned tasks are based on the computation of the nearest neighbor of subsequences using a given sliding window size. This is where the <strong class="bold">Matrix Profile</strong> comes into play because it helps you perform the previous tasks once you have computed them.</p>
<p>We already saw the Matrix Profile in <a href="B14769_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, but in this section, we are going to discuss it in more detail in order to understand better the reason that it is so slow to compute.</p>
<p>Various research papers exist that present and extend the Matrix Profile, including the following:</p>
<ul>
<li><em class="italic">Matrix Profile I: All Pairs Similarity Joins for Time Series: A Unifying View That Includes Motifs, Discords and Shapelets</em>, written by Chin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding, Hoang Anh Dau, Diego Furtado Silva, Abdullah Mueen, and Eamonn J. Keogh (<a href="https://ieeexplore.ieee.org/document/7837992">https://ieeexplore.ieee.org/document/7837992</a>)</li>
<li><em class="italic">Matrix Profile II: Exploiting a Novel Algorithm and GPUs to Break the One Hundred Million Barrier for Time Series Motifs and Joins</em>, written by Yan Zhu, Zachary Zimmerman, Nader Shakibay Senobari, Chin-Chia Michael Yeh, Gareth Funning, Abdullah Mueen, Philip Brisk, and Eamonn Keogh (<a href="https://ieeexplore.ieee.org/abstract/document/7837898">https://ieeexplore.ieee.org/abstract/document/7837898</a>)</li>
<li><em class="italic">Matrix profile goes MAD: variable-length motif and discord discovery in data series</em>, written by Michele Linardi, Yan Zhu, Themis Palpanas, and Eamonn J. Keogh (<a href="https://doi.org/10.1007/s10618-020-00685-w">https://doi.org/10.1007/s10618-020-00685-w</a>)</li>
</ul>
<p class="callout-heading">About normalization</p>
<p class="callout">As it happens with the SAX representation, all Euclidean distances that are going to be computed in this chapter use normalized subsequences.</p>
<p>The next subsection shows what the Matrix Profile computation returns.</p>
<h2 id="_idParaDest-153"><a id="_idTextAnchor164"/>What does the Matrix Profile compute?</h2>
<p>In this subsection, we <a id="_idIndexMarker443"/>are going to explain what the Matrix Profile calculates. Imagine having a time series and a sliding window size that is smaller than the time series length. The Matrix Profile computes <em class="italic">two vectors</em>.</p>
<p>The first vector contains the <em class="italic">Euclidean distances of the nearest neighbor</em> of each subsequence. The value at index <code>0</code> is the Euclidean distance of the nearest neighbor of the subsequence that begins at index <code>0</code>, and so on.</p>
<p>In the second vector, the value at each place of the vector is the index of the subsequence that is the nearest neighbor and corresponds to the Euclidean distance stored in the previous vector. So, if the value at index <code>0</code> is <code>123</code>, this means that the nearest neighbor of the subsequence that begins at index <code>0</code> in the original time series is the subsequence that begins at index <code>123</code> in the original time series. The first vector is going to contain that Euclidean distance value.</p>
<p>It is very important to understand that when computing the Matrix Profile for a time series using a self-join – that is, by looking for the nearest neighbor at the subsequences of the same time series – we need to <em class="italic">exclude the subsequences that are close</em> to the subsequence that we are examining. This is required because subsequences that share many elements in the same order tend to have smaller Euclidean distances by default. However, when dealing with a subsequence that is from another time series, we do not need to exclude any subsequences from the calculations.</p>
<p>A naïve implementation of the computation of the Matrix Profile vectors is to get the first subsequence, compare it to all other subsequences (excluding the subsequences that are close), find its nearest neighbor, and put the Euclidean distance and the index of the nearest neighbor at index <code>0</code> of the two vectors. Then, do the same for all other subsequences. Although this works for smaller time series, it is not very efficient as its algorithmic complexity is O(n 2). This means that for a time series with 10,000 subsequences, we need to perform 10,000 times 10,000 computations (100,000,000). We are going to implement that algorithm to understand how slow it can be in real life.</p>
<p>The authors of the original Matrix Profile paper created a clever technique that involves <strong class="bold">Fast Fourier</strong> transforms <a id="_idIndexMarker444"/>that compute the Matrix Profile vectors with a viable complexity – the name of the algorithm is <strong class="bold">Mueen’s Algorithm for Similarity Search</strong> (<strong class="bold">MASS</strong>). If <a id="_idIndexMarker445"/>you want to learn more about the details of the MASS algorithm and the ideas behind the Matrix Profile, you should read the <em class="italic">Matrix Profile I: All Pairs Similarity Joins for Time Series: A Unifying View That Includes Motifs, Discords and Shapelets</em> paper (<a href="https://ieeexplore.ieee.org/document/7837992">https://ieeexplore.ieee.org/document/7837992</a>).</p>
<p>The next section presents an implementation of the naïve algorithm for computing the Matrix Profile <a id="_idIndexMarker446"/>vectors. The naivete of the algorithm lies in its complexity, not its accuracy.</p>
<h2 id="_idParaDest-154"><a id="_idTextAnchor165"/>Manually computing the exact Matrix Profile</h2>
<p>In this subsection, we<a id="_idIndexMarker447"/> are going to manually compute the exact Matrix Profile to show how slow the process can be, especially when working with large time series. We are using the word <em class="italic">exact</em> to differentiate this from the approximate Matrix Profile computation that we are going to implement in the <em class="italic">Computing Matrix Profile using iSAX</em> section of this chapter.</p>
<p>The last Python statements in the <code>main()</code> function of <code>mp.py</code> are the following:</p>
<pre class="source-code">
    dist, index = mp(ta, windowSize)
    print(dist)
    print(index)</pre>
<p>The first statement runs the <code>mp()</code> function, which returns two values, both of them being lists (vectors), which are the two Matrix Profile vectors.</p>
<p>The implementation of the <code>mp()</code> function is where we compute the two vectors and is presented in two parts. The first part comes with the following code:</p>
<pre class="source-code">
def mp(ts, window):
    l = len(ts) - window + 1
    dist = [None] * l
    index = [None] * l
    for i1 in range(l):
        t1 = ts[i1:i1+window]
        min = None
        minIndex = 0
        <strong class="bold">exclusionMin</strong> = i1 - window // 4
        if exclusionMin &lt; 0:
            exclusionMin = 0
        <strong class="bold">exclusionMax</strong> = i1 + window // 4
        if exclusionMax &gt; l-1:
            exclusionMax = l-1</pre>
<p>In the previous code, we iterate over all the subsequences of the given time series. For each such subsequence, we define the indexes of the exclusion zone as specified in the <em class="italic">Matrix Profile I: All Pairs Similarity Joins for Time Series: A Unifying View That Includes Motifs, Discords and </em><em class="italic">Shapelets</em> paper.</p>
<p>For a sliding window size of <code>16</code>, the exclusion zone is <code>4</code> elements (<code>16 // 4</code>) on the left and <code>4</code> elements (<code>16 // 4</code>) on the <a id="_idIndexMarker448"/>right side of the subsequence.</p>
<p>The second part of <code>mp()</code> is as follows:</p>
<pre class="source-code">
        for i2 in range(l):
            # Exclusion zone
            if i2 &gt;= exclusionMin and i2 &lt;= exclusionMax:
                continue
            t2 = ts[i2:i2+window]
            temp = round(euclidean(t1, t2), 3)
            if min == None:
                min = temp
                minIndex = i2
            elif min &gt; temp:
                min = temp
                minIndex = i2
        dist[i1] = min
        index[i1] = minIndex
    return dist, index</pre>
<p>In this part of the section, we compare each subsequence from the first part of the code with all the subsequences of the time series while taking into account the exclusion zone.</p>
<p>The bad thing about <code>mp()</code> is that it contains two <code>for</code> loops, which makes its computational complexity <em class="italic">O(n</em>2<em class="italic">)</em>.</p>
<p>The output of <code>mp.py</code> when working with the <code>ts.gz</code> time series from <a href="B14769_06.xhtml#_idTextAnchor145"><em class="italic">Chapter 6</em></a> (which is found in the <code>ch06</code> directory of the GitHub repository of the book) is similar to the following for a sliding window of <code>16</code> – we are going to use the output to test the correctness of <a id="_idIndexMarker449"/>our implementation by comparing it to the original Matrix Profile algorithm and its output:</p>
<pre class="source-code">
$ ./mp.py ../ch06/ts.gz -w 16
TS: ../ch06/ts.gz Sliding Window size: 16
[3.294, 3.111, 3.321, 3.535, 3.285, 3.373, 3.332, 3.693, 4.066, 4.065, 3.898, 3.484, 3.372, 3.1, 3.047, 3.299, 3.056, 3.361, 3.766, 3.759, 3.871, 3.884, 3.619, 3.035, 2.358, 3.012, 3.052, 3.136, 3.161, 3.219, 3.309, 3.526, 3.386, 3.973, 4.207, 4.101, 4.249, 4.498, 4.492, 4.255, 4.241, 3.285, 3.517, 3.494, 3.257, 3.316, 3.526, 4.183, 4.011, 3.294, 3.111, 3.321, 3.535, 3.1, 3.047, 3.332, 3.035, 2.358, 3.012, 3.052, 3.136, 3.161, 3.219, 3.201, 3.187, 3.017, 2.676, 2.763, 2.959, 3.952, 3.865, 3.678, 3.687, 3.201, 3.187, 3.017, 2.676, 2.763, 2.959, 3.316, 3.526, 3.899, 3.651, 3.664, 3.885]
[49, 50, 51, 52, 53, 54, 55, 56, 57, 46, 65, 27, 28, 53, 54, 74, 75, 76, 77, 59, 60, 61, 55, 56, 57, 58, 59, 60, 61, 62, 14, 15, 16, 66, 71, 68, 69, 56, 20, 63, 26, 75, 66, 67, 78, 79, 80, 81, 82, 0, 1, 2, 3, 13, 14, 6, 23, 24, 25, 26, 27, 28, 29, 73, 74, 75, 76, 77, 78, 79, 80, 61, 62, 63, 64, 65, 66, 67, 68, 45, 46, 62, 63, 64, 65]
--- 0.36465 seconds ---</pre>
<p>Naively thinking, the subsequences with the smallest and largest Euclidean distances can be considered outliers as they differ from all other subsequences – this is an example of the use of the Matrix Profile for anomaly detection.</p>
<p>Using a sliding <a id="_idIndexMarker450"/>window size of <code>32</code>, <code>mp.py</code> produces the following kind of output:</p>
<pre class="source-code">
$ ./mp.py ../ch06/ts.gz -w 32
TS: ../ch06/ts.gz Sliding Window size: 32
[4.976, 5.131, 5.38, 5.485, 5.636, 5.75, 5.87, 6.076, 6.502, 6.705, 6.552, 6.145, 6.279, 6.599, 6.766, 6.667, 6.577, 6.429, 6.358, 6.358, 5.978, 5.804, 5.588, 5.092, 4.976, 5.01, 5.35, 5.456, 6.036, 6.082, 6.258, 6.513, 6.556, 6.553, 6.672, 6.745, 6.767, 6.777, 7.018, 7.12, 6.564, 6.203, 6.291, 6.118, 6.048, 5.869, 6.142, 6.431, 6.646, 4.976, 5.131, 5.38, 5.485, 5.636, 5.75, 5.588, 5.092, 4.976, 5.01, 5.35, 5.456, 6.036, 6.082, 6.258, 6.513, 6.556, 6.598, 6.518, 6.473]
[49, 50, 51, 52, 53, 54, 55, 56, 24, 58, 24, 25, 26, 27, 63, 64, 65, 55, 56, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 62, 63, 0, 65, 67, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 17, 57, 59]
--- 0.22118 seconds ---</pre>
<p>Lastly, using a sliding window size of <code>64</code>, the produced output is the following:</p>
<pre class="source-code">
$ ./mp.py ../ch06/ts.gz -w 64
TS: ../ch06/ts.gz Sliding Window size: 64
[10.529, 10.406, 10.475, 10.377, 10.702, 10.869, 10.793, 10.827, 10.743, 11.14, 10.865, 10.819, 10.876, 10.808, 10.802, 10.73, 10.713, 10.67, 11.288, 11.296, 11.113, 11.202, 11.196, 11.121, 11.033, 11.145, 11.228, 11.125, 11.108, 10.865, 10.819, 10.671, 10.702, 10.529, 10.406, 10.475, 10.377]
[33, 34, 35, 36, 32, 33, 34, 35, 36, 28, 29, 30, 31, 32, 33, 34, 35, 36, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 0, 10, 11, 3, 4, 0, 1, 2, 3]
--- 0.03179 seconds ---</pre>
<p>The reason for having a smaller output here is that the bigger the sliding window size, the fewer the number of subsequences that are created from a time series.</p>
<p>Now, let us experiment with a time series with 25,000 elements that was created as follows:</p>
<pre class="source-code">
$ ../ch01/synthetic_data.py 25000 -5 5 &gt; 25k
$ gzip 25k</pre>
<p>The results for <code>25k.gz</code> with the same sliding window sizes as before are as follows (only the times are shown – the rest of the output is omitted for brevity):</p>
<pre class="source-code">
$ ./mp.py -w 16 25k.gz
--- 43707.95353 seconds ---
$ ./mp.py -w 32 25k.gz
--- 44162.44419 seconds ---
$ ./mp.py -w 64 25k.gz
--- 45113.62417 seconds ---</pre>
<p>At this point, we<a id="_idIndexMarker451"/> should be aware of the fact that computing the Matrix Profile vectors can be really slow as it took <code>mp.py</code> 45,113 seconds to compute the Matrix Profile in the last run.</p>
<p>Can you think of the reason that even a small increase in the sliding window size also increases the overall times? The answer is that the bigger the sliding window size, the bigger the subsequence length, and therefore, the more time it takes to compute the Euclidean distance between two subsequences. Here is the time it takes to compute the Matrix Profile vectors for a sliding window size of <code>2048</code>:</p>
<pre class="source-code">
$ ./mp.py -w 2048 25k.gz
--- 46271.63763 seconds ---</pre>
<p>Have in mind that <em class="italic">the MASS algorithm does not have such an issue</em> as it computes the Euclidean distances in its own clever way. As a result, its performance depends on the time series length only.</p>
<p>Now, let us present a Python script that computes the exact Matrix Profile using the MASS algorithm with the help of the <code>stumpy</code> Python package. We are using the <code>realMP.py</code> script for <a id="_idIndexMarker452"/>computing the Matrix Profile vectors, which has the following implementation:</p>
<pre class="source-code">
#!/usr/bin/env python
import pandas as pd
import argparse
import time
import stumpy
import numpy as np
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("-w", "--window", dest = "window",
        default = "16", help="Sliding Window", type=int)
    parser.add_argument("TS")
    args = parser.parse_args()
    windowSize = args.window
    inputTS = args.TS
    print("TS:", inputTS, "Sliding Window size:",
        windowSize)
    start_time = time.time()
    ts = pd.read_csv(inputTS, names=['values'],
        compression='gzip')
    ts_numpy = ts.to_numpy()
    ta = ts_numpy.reshape(len(ts_numpy))
    realMP = <strong class="bold">stumpy.stump</strong>(ta, windowSize)
    realDistances = realMP[:,0]
    realIndexes = realMP[:,1]
    print("--- %.5f seconds ---" % (time.time() –
        start_time))
    print(realDistances)
    print(realIndexes)
if __name__ == '__main__':
    main()</pre>
<p>The return value of <code>stumpy.stump()</code> is a multi-dimensional array. The first column (<code>[:,0]</code>) is the<a id="_idIndexMarker453"/> vector of distances, and the second column (<code>[:,1]</code>) is the vector of indexes. In the previous code, we print both these vectors, which is not very handy when dealing with large time series – comment out these two <code>print()</code> statements if you want.</p>
<p>In order to verify the correctness of <code>mp.py</code>, we present the output of <code>realMP.py</code> for the <code>ts.gz</code> time series and a sliding window size of <code>64</code>:</p>
<pre class="source-code">
$ ./realMP.py ../ch06/ts.gz -w 64
TS: ../ch06/ts.gz Sliding Window size: 64
--- 11.31371 seconds ---
[10.5292 10.40594 10.47460 10.3770 10.7024 10.8689 10.7928
 10.8274 10.74260 11.140 10.864 10.818 10.8757 10.8078
 10.8017 10.7296 10.7129 10.6704
 11.2882 11.2963 11.1125 11.2019 11.19556 11.1206
 11.0330 11.14458 11.22779 11.12475
 11.10825 10.864619 10.8186 10.6714
 10.7024 10.52926 10.40594 10.4746 10.3770]
[33 34 35 36 32 33 34 35 36 28 29 30 31 32 33 34 35 36 0 1 2 3 4 5 6 7 8 8 0 10 11 3 4 0 1 2 3]</pre>
<p>Now that we are sure about the correctness of <code>mp.py</code>, let us experiment with the <code>25k.gz</code> time series to see how much time it takes to compute the exact Matrix Profile vectors.</p>
<p>The time it takes <code>realMP.py</code> and the <code>stumpy.stump()</code> function to compute the Matrix Profile<a id="_idIndexMarker454"/> vectors <em class="italic">on a single CPU core</em> for the <code>25k.gz</code> time series is the following:</p>
<pre class="source-code">
$ taskset --cpu-list 0 ./realMP.py 25k.gz -w 1024
TS: 25k.gz Sliding Window size: 1024
--- 11.19547 seconds ---
[42.41325061659 42.4212959655 42.45021115618 ...
 42.64248908665 42.64380072599 42.6591584368]
[10218 10219 10220 ... 7240 7241 20243]</pre>
<p>The time it takes <code>realMP.py</code> to compute the Matrix Profile vectors for the <code>25k.gz</code> time series on an Intel i7 with 8 CPU cores is the following:</p>
<pre class="source-code">
$ ./realMP.py 25k.gz -w 1024
TS: 25k.gz Sliding Window size: 1024
--- 9.68259 seconds ---
[42.41325061659 42.4212959655 42.45021115618 ...
 42.64248908665 42.64380072599 42.6591584368]
[10218 10219 10220 ... 7240 7241 20243]</pre>
<p>Moreover, the time it takes <code>realMP.py</code> and the <code>stumpy.stump()</code> function to compute the Matrix Profile vectors <em class="italic">on a single CPU core</em> for the <code>ch06/100k.gz</code> time series and sliding window size of <code>1024</code> is the following:</p>
<pre class="source-code">
$ taskset --cpu-list 0 ./realMP.py ../ch06/100k.gz -w 1024
TS: ../ch06/100k.gz Sliding Window size: 1024
--- 44.45451 seconds ---
[42.0661718111 42.044733861 42.050637591 ...
 42.252694931 42.225343182 42.2147590858]
[51861 51862 51863 ... 13502 13503 13504]</pre>
<p>Lastly, let us try <code>realMP.py</code> on a <em class="italic">single CPU core</em> on the <code>500k.gz</code> time series from <a href="B14769_04.xhtml#_idTextAnchor102"><em class="italic">Chapter 4</em></a>:</p>
<pre class="source-code">
$ taskset --cpu-list 0 ./realMP.py ../ch04/500k.gz -w 1024
TS: ../ch04/500k.gz Sliding Window size: 1024
--- 1229.49608 seconds ---
[41.691930926 41.689248432 41.642429848 ...
 41.712625718 41.6520521157 41.636642904]
[446724 446725 446726 ... 260568 260569 260570]</pre>
<p>The conclusion from the previous output is that computing the Matrix Profile gets slower as the length of the time series gets bigger, which is the main reason for thinking about an approximate computation of it. What we lose in accuracy, we gain in time. We cannot have <a id="_idIndexMarker455"/>everything!</p>
<p>The next section explains the technique that we are going to use to approximately compute the Matrix Profile vectors with the help of iSAX.</p>
<h1 id="_idParaDest-155"><a id="_idTextAnchor166"/>Computing the Matrix Profile using iSAX</h1>
<p>First of all, let <a id="_idIndexMarker456"/>us<a id="_idIndexMarker457"/> make something clear: we are going to present <em class="italic">an approximate method</em>. If you want to calculate the exact Matrix Profile, then you should use an implementation that uses the original algorithm.</p>
<p>The idea behind the used technique is the following: <em class="italic">it is more likely that the nearest neighbor of a subsequence is going to be found in the subsequences stored in the same terminal node as the subsequence under examination</em>. Therefore, we do not need to check all the subsequences of the time series, just a small subset of them.</p>
<p>The next subsection discusses and resolves an issue that might come up in our calculations, which<a id="_idIndexMarker458"/> is <a id="_idIndexMarker459"/>what are we going to do if we cannot find a proper match for a subsequence in a terminal node.</p>
<h2 id="_idParaDest-156"><a id="_idTextAnchor167"/>What happens if there is not a valid match?</h2>
<p>In this subsection, we<a id="_idIndexMarker460"/> are going to clarify the problematic cases of the process. There exist two conditions that might end up in an undesired situation:</p>
<ul>
<li>A terminal node contains a single subsequence only</li>
<li>For a given subsequence, all the remaining subsequences of the terminal node are in the exclusion zone</li>
</ul>
<p>In both cases, we are not going to be able to find the approximate nearest neighbor of a subsequence. Can we resolve these issues?</p>
<p>There exist multiple answers to that question, including doing nothing or choosing a different subsequence and using that to compute the Euclidean distance of the nearest neighbor. We are going with the latter solution, but instead of randomly choosing a subsequence, we are going for the subsequence that is next to the left side of the exclusion zone. If there is no space on the left side of the exclusion zone, we are going to choose the subsequence that is next to the right side of the exclusion zone. As these two conditions cannot happen at the same time, we are good!</p>
<p>The next subsection discusses how to compute the error of the approximate Matrix Profile vector of distances compared to the real Matrix Profile vector of distances.</p>
<h2 id="_idParaDest-157"><a id="_idTextAnchor168"/>Calculating the error</h2>
<p>As explained <a id="_idIndexMarker461"/>earlier, we are computing an approximate Matrix Profile vector. In such cases, we need a way to compute how far we are from the real values. There exist various ways to compute an error value between two quantities. As a Matrix Profile is a list of values, we need to find a way to compute an error value that supports a list of values, not single values only.</p>
<p>The most common way is to find the Euclidean distance between the approximate vector and the exact vector. However, this does not always tell the whole truth. A good alternative would be to use the <strong class="bold">Root Mean Square </strong><strong class="bold">Error</strong> (<strong class="bold">RMSE</strong>).</p>
<p>The formula for the RMSE is a little complex at first. It is presented in <em class="italic">Figure 7</em><em class="italic">.1</em>:</p>
<div><div><img alt="Figure 7.1 – The RMSE formula" height="165" src="img/Figure_7.1_B14769.jpg" width="993"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – The RMSE formula</p>
<p>In practice, this means that we find the difference between the actual value and the approximate one and we square that. We do that for all the pairs and then add all these values – this is the purpose of the big Greek Sigma letter. After that, we divide by the number of the pairs. Lastly, we find the square root of that last value and we are done. If you are not good at mathematics, bear in mind that you do not need to remember that formula – we are going to implement it in Python in a while.</p>
<p>The desired property that the RMSE has is that it takes into account the number of elements that we compare. Put simply, the RMSE takes the <em class="italic">average</em>, whereas the Euclidean distance takes the <em class="italic">sum</em>. In our case, using the average error looks more appropriate.</p>
<p>As an example, the Euclidean distance between <code>(0, 0, 0, 2, 2)</code> and <code>(2, 1, 0, 0, 0)</code> is equal to <code>3.6055</code>. On the other hand, the RMSE of these two vectors is equal to <code>1.61245</code>.</p>
<p>With all that in mind, we are ready to present our approximate implementation.</p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor169"/>Approximate Matrix Profile implementation</h2>
<p>In this subsection, we<a id="_idIndexMarker462"/> present the Python script that approximately computes the Matrix Profile vectors.</p>
<p>The important code for <code>apprMP.py</code> can be found in <code>approximateMP()</code>, which is presented in four parts. The first part of the function is the following:</p>
<pre class="source-code">
def approximateMP(ts_numpy):
    ISAX = isax.iSAX()
    length = len(ts_numpy)
    windowSize = variables.slidingWindowSize
    segments = variables.segments
    # Split sequence into subsequences
    for i in range(length - windowSize + 1):
        ts = ts_numpy[i:i+windowSize]
        ts_node = isax.TS(ts, segments)
        ts_node.index = i
        ISAX.insert(ts_node)
    vDist = [None] * (length - windowSize + 1)
    vIndex = [None] * (length - windowSize + 1)
    nSubsequences = length - windowSize + 1</pre>
<p>The previous code splits the time series into subsequences and creates the iSAX index. It also initializes the <code>vDist</code> and <code>vIndex</code> variables, for keeping the list of distances and the list of indexes, respectively.</p>
<p>The second part of <code>approximateMP()</code> is the following:</p>
<pre class="source-code">
for k in ISAX.ht:
        t = ISAX.ht[k]
        if t.terminalNode == False:
            continue
        # I is the index of the subsequence
        # in the terminal node
        for i in range(t.nTimeSeries()):
            # This is the REAL index of the subsequence
            # in the time series
            idx = t.children[i].index
            # This is the subsequence that we are examining
            currentTS = t.children[i].ts
            exclusionMin = idx–- windowSize // 4
            if exclusionMin &lt; 0:
                exclusionMin = 0
            exclusionMax = idx + windowSize // 4
            if exclusionMax &gt; nSubsequences-1:
                exclusionMax = nSubsequences-1
            min = None
            minIndex = 0</pre>
<p>In the previous code, we<a id="_idIndexMarker463"/> take each node of the iSAX index and determine whether it is a terminal node or not – we are only interested in terminal nodes. If we are dealing with a terminal node, we process each subsequence stored there. First, we define the indexes of the exclusion zone making sure that the minimum value of the left side of the exclusion zone is <code>0</code> – this is the index of the first element of the time series – and the maximum value of the right side of the exclusion zone is not bigger than the length of the time series minus 1.</p>
<p>The third part<a id="_idIndexMarker464"/> of it is the following:</p>
<pre class="source-code">
           for sub in range(t.nTimeSeries()):
                # This is the REAL index of the subsequence
                # we are examining in the time series
                currentIdx = t.children[sub].index
                if currentIdx &gt;= exclusionMin and currentIdx &lt;= exclusionMax:
                    continue
                temp = round(<strong class="bold">tools.euclidean(currentTS,</strong>
                    <strong class="bold">t.children[sub].ts</strong>), 3)
                if min == None:
                    min = temp
                    minIndex = currentIdx
                elif min &gt; temp:
                    min = temp
                    minIndex = currentIdx</pre>
<p>We compare each subsequence of the selected terminal node with the rest of the subsequences it contains because we expect that there is a high probability of the nearest neighbor being in the same node.</p>
<p>Then, we make sure that the index of the subsequence that is going to be compared with the initial subsequence is not in the exclusion zone. If we find such a subsequence, we compute the Euclidean distance and keep the relevant index value. From all these subsequences that are outside the exclusion zone and are located in the terminal node, we keep the minimum Euclidean distance and the related index.</p>
<p>We do that for all the subsequences in all the terminal nodes of the iSAX index.</p>
<p>The last part<a id="_idIndexMarker465"/> of the <code>approximateMP()</code> function is the following:</p>
<pre class="source-code">
            # Pick left limit first, then the right limit
            if min == None:
                if exclusionMin-1 &gt; 0:
                    randomSub = ts_numpy[exclusionMin-
                        1:exclusionMin+windowSize-1]
                    vDist[idx] = round(tools.euclidean(
                        currentTS, randomSub), 3)
                    vIndex[idx] = exclusionMin - 1
                else:
                    randomSub = ts_numpy[exclusionMax+
                        1:exclusionMax+windowSize+1]
                    vDist[idx] = round(tools.euclidean(
                        currentTS, randomSub), 3)
                    vIndex[idx] = exclusionMax + 1
            else:
                vDist[idx] = min
                vIndex[idx] = minIndex
    return vIndex, vDist</pre>
<p>If, at this point, we do not have a valid Euclidean distance value (<code>None</code>), we compare the initial subsequence with the subsequence next to the left side of the exclusion zone, if it exists – this means if the left side of the exclusion zone is not <code>0</code>. Otherwise, we compare it with the subsequence next to the right side of the exclusion zone. We put the relevant index and Euclidean distance into the <code>vIndex</code> and <code>vDist</code> variables, respectively. However, if we already have an index and Euclidean distance from earlier, we use these <a id="_idIndexMarker466"/>values.</p>
<p>The next subsection compares the accuracy of our approximate technique when using different iSAX parameters.</p>
<h2 id="_idParaDest-159"><a id="_idTextAnchor170"/>Comparing the accuracy of two different parameter sets</h2>
<p>In this<a id="_idIndexMarker467"/> subsection, we are going to compute the approximate Matrix Profile vector of a single time series using two different sets of iSAX parameters and check the accuracy of the results using the RMSE.</p>
<p>To make things simpler, we have created a Python script that computes the two approximate Matrix Profile vectors of Euclidean distances, as well as the exact Matrix Profile vectors, and calculates the RMSE – the name of the script is <code>rmse.py</code>. We are not going to present the entire Python code of <code>rmse.py</code>, just the important Python statements, starting from the function that computes the RMSE:</p>
<pre class="source-code">
def RMSE(realV, approximateV):
    diffrnce = np.subtract(realV, approximateV)
    sqre_err = np.square(diffrnce)
    rslt_meansqre_err = sqre_err.mean()
    error = math.sqrt(rslt_meansqre_err)
    return error</pre>
<p>The previous code implements the computation of the RMSE value according to the formula presented in <em class="italic">Figure 7</em><em class="italic">.1</em>.</p>
<p>The remaining relevant Python code is located in the <code>main()</code> function:</p>
<pre class="source-code">
    # Real Matrix Profile
    TSreshape = ts_numpy.reshape(len(ts_numpy))
    realMP = stumpy.stump(TSreshape, windowSize)
    realDistances = realMP[:,0]
    # Approximate Matrix Profile
    _, vDist = approximateMP(ts_numpy)
    rmseError = RMSE(realDistances, vDist)
    print("Error =", rmseError)</pre>
<p>First, we compute <a id="_idIndexMarker468"/>the real Matrix Profile vector with <code>stumpy.stump()</code>, and then we compute the approximate Matrix Profile vector with the Euclidean distances using <code>approximateMP()</code>. After that, we call the <code>RMSE()</code> function and get the numeric result, which we print on the screen.</p>
<p>So, let us run <code>rmse.py</code> and see what we get:</p>
<pre class="source-code">
$ ./rmse.py -w 32 -s 4 -t 500 -c 16 25k.gz
Max Cardinality: 16 Segments: 4 Sliding Window: 32 Threshold: 500 Default Promotion: False
Error = 10.70823863253679</pre>
<p>Now, let us use <code>rmse.py</code> another time, but this time, with different iSAX parameters, as follows:</p>
<pre class="source-code">
$ ./rmse.py -w 32 -s 4 -t 1500 -c 16 25k.gz
Max Cardinality: 16 Segments: 4 Sliding Window: 32 Threshold: 1500 Default Promotion: False
Error = 9.996114543048341</pre>
<p>What do the previous results tell us? First, the results tell us that our approximate technique does not leave any subsequence without a Euclidean distance. If there was such a case, then <code>rmse.py</code> would have generated an error message like the following:</p>
<pre class="source-code">
TypeError: unsupported operand type(s) for -: 'float' and 'NoneType'</pre>
<p>As, in the initialization of <code>vDist</code>, all its elements are set equal to <code>None</code>, the previous error means that the value of at least one of the elements was not reset. Therefore, it is still equal to <code>None</code> and our code fails to subtract a floating point value, calculated by <code>stumpy.stump()</code>, from <code>None</code>.</p>
<p>Apart from that, the results tell us that bigger threshold values produce more accurate results, which makes perfect sense, as there are more subsequences in each terminal node. However, this makes the computation of the approximate Matrix Profile slower. As a general rule, the closer the number of subsequences at each terminal node is to the threshold value, the better the accuracy – we do not want terminal nodes with a small number of subsequences stored in them.</p>
<p>Now that we<a id="_idIndexMarker469"/> know about the Matrix Profile, let us discuss MPdist, how it is computed, and the role of the Matrix Profile in this computation.</p>
<h1 id="_idParaDest-160"><a id="_idTextAnchor171"/>Understanding MPdist</h1>
<p>Now that we know <a id="_idIndexMarker470"/>about the Matrix Profile, we are ready to learn about MPdist and how the Matrix Profile is used in the calculation of MPdist. The paper that defines the MPdist distance is <em class="italic">Matrix Profile XII: MPdist: A Novel Time Series Distance Measure to Allow Data Mining in More Challenging Scenarios</em>, written by S. Gharghabi, S. Imani, A. Bagnall, A. Darvishzadeh, and E. Keogh (<a href="https://ieeexplore.ieee.org/abstract/document/8594928">https://ieeexplore.ieee.org/abstract/document/8594928</a>).</p>
<p>The intuition behind MPdist is that two time series can be considered similar if they <em class="italic">have similar patterns throughout their duration</em>. Such patterns are extracted in the form of subsequences using a sliding window. This is illustrated in <em class="italic">Figure 7</em><em class="italic">.2</em>:</p>
<div><div><img alt="Figure 7.2 – Grouping time series" height="569" src="img/Figure_7.2_B14769.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Grouping time series</p>
<p>In <em class="italic">Figure 7</em><em class="italic">.2</em>, we see that MPdist <em class="italic">(c)</em> understands the similarity between time series that follow the same pattern better, whereas Euclidean distance <em class="italic">(b)</em> compares time series based on time, and therefore groups the presented time series differently. In my opinion, the grouping that is based on MPdist is more accurate.</p>
<p>The advantages of MPdist (according to the people that created it) are that the MPdist distance measure tries to be more flexible than most available distance measures, including the Euclidean distance, and it takes into account similarities that may not take place at the same time. Additionally, MPdist can compare time series of different sizes –Euclidean distance <a id="_idIndexMarker471"/>cannot do that – and requires just a single parameter (the sliding window size) to operate.</p>
<p>The next subsection discusses the way MPdist is computed.</p>
<h2 id="_idParaDest-161"><a id="_idTextAnchor172"/>How to compute MPdist</h2>
<p>In this subsection, we<a id="_idIndexMarker472"/> are going to discuss the way the real MPdist is computed in order to better understand the complexity of the process.</p>
<p>The computation of MPdist is based on the Matrix Profile. First, we are given two time series, A and B, and a sliding window size. Then, for each subsequence of the first time series, we find its nearest neighbor in the second time series, and we put the related Euclidean distance into a list of values. We do that for all the subsequences of the first time series. This is also <a id="_idIndexMarker473"/>called the <strong class="bold">AB join</strong>. Then, we do the same but for the second time series – this is <a id="_idIndexMarker474"/>called the <strong class="bold">BA join</strong>. So, in the end, we calculated<a id="_idIndexMarker475"/> the <strong class="bold">ABBA join</strong> and we have a list of Euclidean distances that we sort from the smallest to the biggest. From that list, we get the Euclidean distance found at the index value that is equal to <em class="italic">5% of the length of the list</em> – the authors of MPdist decided to use the Euclidean distance at that index as the MPdist value.</p>
<p>For both the AB join and BA join, the authors of MPdist use the MASS algorithm to compute the nearest neighbor of each subsequence, in order to avoid the inefficient algorithmic complexity of O(n 2).</p>
<p>In the next<a id="_idIndexMarker476"/> subsection, we will create a Python script that manually computes the MPdist distance between two time series.</p>
<h2 id="_idParaDest-162"><a id="_idTextAnchor173"/>Manually computing MPdist</h2>
<p>In this subsection, we<a id="_idIndexMarker477"/> are going to show how to manually compute the MPdist value between two time series. The idea behind the implementation is based on the code found in <code>mp.py</code> – however, fundamental differences exist as the Matrix Profile returns a vector of values instead of a single value.</p>
<p>The logic code of <code>mpdist.py</code> is implemented in two functions, named <code>mpdist()</code> and <code>JOIN()</code>. <code>mpdist()</code> is implemented as follows:</p>
<pre class="source-code">
def mpdist(ts1, ts2, window):
    L_AB = JOIN(ts1, ts2, window)
    L_BA = JOIN(ts2, ts1, window)
    JABBA = L_AB + L_BA
    JABBA.sort()
    index = int(0.05 * (len(JABBA) + 2 * window)) + 1
    return JABBA[index]</pre>
<p>The previous code uses the <code>JOIN()</code> function to compute <code>AB Join</code> and <code>BA Join</code>. Then, it concatenates the numeric results, which are all Euclidean distances, and sorts them. Based on the length of the concatenation, it computes <code>index</code>, which is used for selecting a value from the <code>JABBA</code> array.</p>
<p><code>JOIN()</code> is implemented as follows:</p>
<pre class="source-code">
def JOIN(ts1, ts2, window):
    LIST = []
    l1 = len(ts1) - window + 1
    l2 = len(ts2) - window + 1
    for i1 in range(l1):
        t1 = ts1[i1:i1+window]
        min = round(euclidean(t1, ts2[0:window]), 4)
        for i2 in range(1, l2):
            t2 = ts2[i2:i2+window]
            temp = round(euclidean(t1, t2), 4)
            if min &gt; temp:
                min = temp
        LIST.append(min)
    return LIST</pre>
<p>This is where the<a id="_idIndexMarker478"/> join is implemented. For every subsequence in the <code>ts1</code> time series, we find the nearest neighbor in the <code>ts2</code> time series – there is no need for an exclusion zone in this case.</p>
<p>The bad thing about <code>mpdist.py</code> is that it contains two <code>for</code> loops, which makes its computational complexity O(n 2) – this is no surprise, as MPdist is based on the Matrix Profile. Therefore, the previous technique is viable for small time series only. In general, <strong class="bold">brute-force algorithms</strong> usually <a id="_idIndexMarker479"/>do not work well for large amounts of data.</p>
<p>At this point, we are going to create two time series with 10,000 elements each:</p>
<pre class="source-code">
$ ../ch01/synthetic_data.py 10000 -5 5 &gt; 10k1
$ ../ch01/synthetic_data.py 10000 -5 5 &gt; 10k2
$ gzip 10k1; gzip 10k2</pre>
<p>The output of <code>mpdist.py</code> when working with <code>10k1.gz</code> and <code>10k2.gz</code> and a sliding window size of <code>128</code> is as follows:</p>
<pre class="source-code">
$ ./mpdist.py 10k1.gz 10k2.gz -w 128
--- 12026.64167 seconds ---
MPdist: 12.5796</pre>
<p>It took <code>mpdist.py</code> approximately 12,026 seconds to compute MPdist.</p>
<p>The output of <code>mpdist.py</code> when working with <code>10k1.gz</code> and <code>10k2.gz</code> and a sliding window size of <code>2048</code> is the following:</p>
<pre class="source-code">
$ ./mpdist.py 10k1.gz 10k2.gz -w 2048
--- 9154.55179 seconds ---
MPdist: 60.7277</pre>
<p>Why do you think<a id="_idIndexMarker480"/> the calculation of the <code>2048</code> sliding window ran faster than the same calculation for the sliding window size of <code>128</code>? It most likely has to do with the fact that the <code>2048</code> sliding window needs fewer iterations (1,920 times 1,920, which is equal to 3,686,400) due to the larger sliding window size, which also compensates for the cost of computing Euclidean distances for larger subsequences in the <code>2048</code> sliding window case.</p>
<p>Let us now see how much time it takes the MASS algorithm to compute MPdist.</p>
<p>The time it takes the <code>stumpy.mpdist()</code> function to compute the previous MPdist distances on a single CPU core is the following – we are using the <code>mpdistance.py</code> script from <a href="B14769_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>:</p>
<pre class="source-code">
$ taskset --cpu-list 0 ../ch01/mpdistance.py 10k1.gz 10k2.gz 128
TS1: 10k1.gz TS2: 10k2.gz Window Size: 128
--- 10.28342 seconds ---
MPdist: 12.5790
$ taskset --cpu-list 0 ../ch01/mpdistance.py 10k1.gz 10k2.gz 2048
TS1: 10k1.gz TS2: 10k2.gz Window Size: 2048
--- 10.03479 seconds ---
MPdist: 60.7277</pre>
<p>So, it takes the <code>stumpy.mpdist()</code> function about 10 seconds.</p>
<p>The time it takes the <code>stumpy.mpdist()</code> function to compute the previous MPdist distances on four CPU cores is the following:</p>
<pre class="source-code">
$ taskset --cpu-list 0,1,2,3 ../ch01/mpdistance.py 10k1.gz 10k2.gz 128
TS1: 10k1.gz TS2: 10k2.gz Window Size: 128
--- 9.42861 seconds ---
MPdist: 12.5790
$ taskset --cpu-list 0,1,2,3 ../ch01/mpdistance.py 10k1.gz 10k2.gz 2048
TS1: 10k1.gz TS2: 10k2.gz Window Size: 2048
--- 9.33578 seconds ---
MPdist: 60.7277</pre>
<p>Why are the times <a id="_idIndexMarker481"/>almost the same when using a single CPU core? The answer is that with small time series, <code>stumpy.mpdist()</code> <em class="italic">does not have enough time</em> to use all CPU cores.</p>
<p>Lastly, the time it takes the <code>stumpy.mpdist()</code> function to compute the two MPdist distances on eight CPU cores is the following:</p>
<pre class="source-code">
$ ../ch01/mpdistance.py 10k1.gz 10k2.gz 128
TS1: 10k1.gz TS2: 10k2.gz Window Size: 128
--- 9.54642 seconds ---
MPdist: 12.5790
$ ../ch01/mpdistance.py 10k1.gz 10k2.gz 2048
TS1: 10k1.gz TS2: 10k2.gz Window Size: 2048
--- 9.33648 seconds ---
MPdist: 60.7277</pre>
<p>Why are the times the same when using four CPU cores? As before, for very small time series, the number of CPU cores used does not make any difference to the computation time as there is<a id="_idIndexMarker482"/> not enough time to use them.</p>
<p>We are now ready to use the existing knowledge to approximately compute MPdist with the help of iSAX.</p>
<h1 id="_idParaDest-163"><a id="_idTextAnchor174"/>Calculating MPdist using iSAX</h1>
<p>In this section, we <a id="_idIndexMarker483"/>are<a id="_idIndexMarker484"/> going to discuss our views and ideas regarding using iSAX indexes to <em class="italic">approximately </em><em class="italic">compute MPdist</em>.</p>
<p>We know that iSAX keeps together subsequences with the same SAX representation. As before, our feeling is that it is more likely to find the nearest neighbor of a subsequence from a given time series in the subsequences with the same SAX representation from another time series.</p>
<p>The next section is about putting our thoughts into practice.</p>
<h1 id="_idParaDest-164"><a id="_idTextAnchor175"/>Implementing the MPdist calculation in Python</h1>
<p>In this section, we<a id="_idIndexMarker485"/> will discuss two ways to approximately<a id="_idIndexMarker486"/> compute MPdist with the help of iSAX.</p>
<p>The first way is much simpler than the second one and is slightly based on the approximate calculation of the Matrix Profile. We take each subsequence from the first time series, and we match it with a terminal node with the same SAX representation from the iSAX index of the second time series in order to get the approximate nearest neighbor – if a subsequence does not have a match <em class="italic">based on its SAX representation</em>, we ignore that subsequence. So, in this case, we do not join iSAX indexes, which makes the process much slower – our experiments are going to show how much slower this technique is.</p>
<p>For the second<a id="_idIndexMarker487"/> way, we<a id="_idIndexMarker488"/> just use <a id="_idIndexMarker489"/>the <strong class="bold">similarity join</strong> of two iSAX indexes, which we first saw in <a href="B14769_05.xhtml#_idTextAnchor124"><em class="italic">Chapter 5</em></a>.</p>
<p>The next subsection shows the implementation of the first technique.</p>
<h2 id="_idParaDest-165"><a id="_idTextAnchor176"/>Using the approximate Matrix Profile way</h2>
<p>Although we do<a id="_idIndexMarker490"/> not return any Matrix Profile vectors, this technique looks like computing the Matrix Profile because <em class="italic">we examine subsequences one by one</em> and not in groups, and return their Euclidean distance with the approximate nearest neighbor. In this technique, <em class="italic">there is no exclusion zone</em> in the computation because we are comparing subsequences from two different time series.</p>
<p>The important code within <code>apprMPdist.py</code> is the following – we assume that we have already generated the two iSAX indexes:</p>
<pre class="source-code">
    # We search iSAX2 for the NN of the
    # subsequences from TS1
    for idx in range(0, len(ts1)-windowSize+1):
        currentQuery = ts1[idx:idx+windowSize]
        t = NN(i2, currentQuery)
        if t != None:
            ED.append(t)
    # We search iSAX1 for the NN of the
    # subsequences from TS2
    for idx in range(0, len(ts2)-windowSize+1):
        currentQuery = ts2[idx:idx+windowSize]
        t = NN(i1, currentQuery)
        if t != None:
            ED.append(t)
    ED.sort()
    idx = int(0.05 * ( len(ED) + 2 * windowSize)) + 1
    print("Approximate MPdist:", round(ED[idx], 3))</pre>
<p>For each subsequence of the first time series, search the iSAX index of the second time series for the approximate nearest neighbor using the <code>NN()</code> function. Then, do the same for the subsequences of the second time series and the iSAX index of the first time series.</p>
<p>What is interesting is the implementation of the <code>NN()</code> function, used in the previous code. We are going<a id="_idIndexMarker491"/> to present <code>NN()</code> in three parts. The first part is the following:</p>
<pre class="source-code">
def NN(ISAX, q):
    ED = None
    segments = variables.segments
    threshold = variables.threshold
    # Create TS Node
    qTS = isax.TS(q, segments)
    segs = [1] * segments
    # If the relevant child of root is not there
    # we have a miss
    lower_cardinality = tools.lowerCardinality(segs, qTS)
    lower_cardinality_str = ""
    for i in lower_cardinality:
        lower_cardinality_str=lower_cardinality_str+"_"+i
    lower_cardinality_str = lower_cardinality_str[1:len(
        lower_cardinality_str)]
    if ISAX.ht.get(lower_cardinality_str) == None:
        return None</pre>
<p>In the previous<a id="_idIndexMarker492"/> code, we try to find an iSAX node with the same SAX representation as the subsequence we are examining – we begin with the children of the root node of the iSAX. If such a child of the root node cannot be found, then we have a miss and we ignore that particular subsequence. As the final list of Euclidean distances is large (this depends on the lengths of the time series), missing some subsequences has no real effect on the end result.</p>
<p>The second part of <code>NN()</code> is the following:</p>
<pre class="source-code">
    # Otherwise, we have a hit
    n = ISAX.ht.get(lower_cardinality_str)
    while n.terminalNode == False:
        left = n.left
        right = n.right
        leftSegs = left.word.split('_')
        # Promote
        tempCard = tools.promote(qTS, leftSegs)
        if tempCard == left.word:
            n = left
        elif tempCard == right.word:
            n = right</pre>
<p>In the previous code, we try to locate the iSAX node with the desired SAX representation by traversing the iSAX index.</p>
<p>The last part<a id="_idIndexMarker493"/> of <code>NN()</code> is the following:</p>
<pre class="source-code">
    # Iterate over the subsequences of the terminal node
    for i in range(0, threshold):
        child = n.children[i]
        if type(child) == isax.TS:
            distance = <strong class="bold">tools.euclidean</strong>(normalize(child.ts),
                normalize(qTS.ts))
            if ED == None:
                ED = distance
            if ED &gt; distance:
                ED = distance
        else:
            break
    return ED</pre>
<p>After locating the desired terminal node, we compare its subsequences with the given subsequence and return the minimum Euclidean distance found. The main program puts all these minimum Euclidean distances into a list.</p>
<p>Now, let us discuss the second technique, which joins two iSAX indexes.</p>
<h2 id="_idParaDest-166"><a id="_idTextAnchor177"/>Using the join of two iSAX indexes</h2>
<p>The second <a id="_idIndexMarker494"/>way is much faster <a id="_idIndexMarker495"/>than the first one. In this way, we <em class="italic">join the two iSAX indexes</em> based on the technique from <a href="B14769_05.xhtml#_idTextAnchor124"><em class="italic">Chapter 5</em></a>, and we get the list of Euclidean distances. From that list, we choose a value to be the approximate MPdist.</p>
<p class="callout-heading">What happens if there is not a match among iSAX nodes?</p>
<p class="callout">In some rare cases that depend on the time series data and the iSAX parameters, some nodes from one iSAX might end up not having a match in the other iSAX, and vice versa. In our case, we <em class="italic">ignore those nodes</em>, which means that we end up having a smaller-than-expected list of Euclidean distances.</p>
<p>The important code within <code>joinMPdist.py</code> is the following – we assume that we have already generated the two iSAX indexes:</p>
<pre class="source-code">
    # Join the two iSAX indexes
    Join(i1, i2)
    variables.ED.sort()
    print("variables.ED length:", len(variables.ED))
    # Index
    idx = int(0.05*(len(variables.ED) + 2*windowSize))+1
    print("Approximate MPdist:", variables.ED[idx])</pre>
<p>The previous code uses the <code>Join()</code> function from <code>isax.iSAXjoin</code>, which we implemented and saw in <a href="B14769_05.xhtml#_idTextAnchor124"><em class="italic">Chapter 5</em></a>. We have already seen the join of two iSAX indexes. However, this<a id="_idIndexMarker496"/> is the first time that we actually use the results of that join for something.</p>
<p>We are now going to start using the existing implementations and see their performance.</p>
<h1 id="_idParaDest-167"><a id="_idTextAnchor178"/>Using the Python code</h1>
<p>In this section, we are <a id="_idIndexMarker497"/>going to use the Python scripts that we have created.</p>
<p>Running <code>apprMPdist.py</code> using the two time series with 10,000 elements each that we created earlier in this chapter generates the following kind of output:</p>
<pre class="source-code">
$ ./apprMPdist.py 10k1.gz 10k2.gz -s 3 -c 64 -t 500 -w 120
Max Cardinality: 64 Segments: 3 Sliding Window: 120 Threshold: 500 Default Promotion: False
MPdist: 351.27 seconds
Approximate MPdist: 12.603</pre>
<p>Using a bigger sliding window size generates the following output:</p>
<pre class="source-code">
$ ./apprMPdist.py 10k1.gz 10k2.gz -s 3 -c 64 -t 500 -w 300
Max Cardinality: 64 Segments: 3 Sliding Window: 300 Threshold: 500 Default Promotion: False
MPdist: 384.74 seconds
Approximate MPdist: 21.757</pre>
<p>So, bigger sliding window sizes require more time. As before, this is because calculating Euclidean distances for bigger sliding window sizes is slower.</p>
<p>Executing <code>joinMPdist.py</code> produces the following output:</p>
<pre class="source-code">
$ ./joinMPdist.py 10k1.gz 10k2.gz -s 3 -c 64 -t 500 -w 120
Max Cardinality: 64 Segments: 3 Sliding Window: 120 Threshold: 500 Default Promotion: False
MPdist: 37.70 seconds
variables.ED length: 17605
Approximate MPdist: 12.60282</pre>
<p>As before, using a bigger sliding window produces the following output:</p>
<pre class="source-code">
$ ./joinMPdist.py 10k1.gz 10k2.gz -s 3 -c 64 -t 500 -w 300
Max Cardinality: 64 Segments: 3 Sliding Window: 300 Threshold: 500 Default Promotion: False
MPdist: 31.24 seconds
variables.ED length: 13972
Approximate MPdist: 21.76263</pre>
<p>It looks like <code>joinMPdist.py</code> is a lot faster than <code>apprMPdist.py</code>, which makes perfect sense as it is <em class="italic">using two iSAX indexes at the same time</em> to construct the list of Euclidean<a id="_idIndexMarker498"/> distances. Put simply, the running of <code>joinMPdist.py</code> requires fewer computations.</p>
<p>The next subsection compares the accuracy and the speed of the two methods when working with larger time series.</p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor179"/>Comparing the accuracy and the speed of the methods</h2>
<p>Both methods are <a id="_idIndexMarker499"/>far from perfect. However, in this subsection, we are going to compare their accuracy and speed in relation to the MPdist implementation found in the <code>stumpy</code> Python package.</p>
<p>We want to test our code on bigger time series, as this is where our technique might be faster than the exact MPdist function of <code>stumpy</code>. In this case, we are going to use two time series with around 500,000 elements each – we already created such time series in <a href="B14769_05.xhtml#_idTextAnchor124"><em class="italic">Chapter 5</em></a>.</p>
<p>For <code>apprMPdist.py</code>, the results for sliding window sizes of <code>120</code>, <code>600</code>, and <code>1200</code> are as follows:</p>
<pre class="source-code">
$ ./apprMPdist.py ../ch05/500k.gz ../ch05/506k.gz -s 6 -c 64 -t 500 -w 120
Max Cardinality: 32 Segments: 6 Sliding Window: 120 Threshold: 500 Default Promotion: False
MPdist: 19329.64 seconds
Approximate MPdist: 12.405
$ ./apprMPdist.py ../ch05/500k.gz ../ch05/506k.gz -s 6 -c 64 -t 500 -w 600
Max Cardinality: 64 Segments: 6 Sliding Window: 600 Threshold: 500 Default Promotion: False
MPdist: 21219.60 seconds
Approximate MPdist: 31.871
$ ./apprMPdist.py ../ch05/500k.gz ../ch05/506k.gz -s 6 -c 64 -t 500 -w 1200
Max Cardinality: 64 Segments: 6 Sliding Window: 1200 Threshold: 500 Default Promotion: False
MPdist: 23120.07 seconds
Approximate MPdist: 46.279</pre>
<p>For the <code>joinMPdist.py</code> script, the output for sliding window sizes of <code>120</code>, <code>600</code>, and <code>1200</code> is the <a id="_idIndexMarker500"/>following:</p>
<pre class="source-code">
$ ./joinMPdist.py ../ch05/500k.gz ../ch05/506k.gz -s 6 -c 64 -t 500 -w 120
Max Cardinality: 64 Segments: 6 Sliding Window: 120 Threshold: 500 Default Promotion: False
MPdist: 2595.92 seconds
variables.ED length: 910854
Approximate MPdist: 12.40684
$ ./joinMPdist.py ../ch05/500k.gz ../ch05/506k.gz -s 6 -c 64 -t 500 -w 600;
Max Cardinality: 64 Segments: 6 Sliding Window: 600 Threshold: 500 Default Promotion: False
MPdist: 2270.72 seconds
variables.ED length: 798022
Approximate MPdist: 31.88064
$ ./joinMPdist.py ../ch05/500k.gz ../ch05/506k.gz -s 6 -c 64 -t 500 -w 1200
Max Cardinality: 64 Segments: 6 Sliding Window: 1200 Threshold: 500 Default Promotion: False
MPdist: 2145.76 seconds
variables.ED length: 674777
Approximate MPdist: 46.29538</pre>
<p>The results of <code>joinMPdist.py</code> are really promising when working with larger time series. Although it looks like the bigger the sliding window size, the faster the technique, this is not completely true because as the sliding window gets bigger, we have more nodes without a match, and therefore, the list of values gets smaller, which means that we compute fewer Euclidean distances as the sliding window increases. This is not always the case, as this depends on the time series data.</p>
<p>Lastly, the result<a id="_idIndexMarker501"/> from the <code>stumpy</code> Python package when running on a single CPU core is as follows:</p>
<pre class="source-code">
$ taskset --cpu-list 0 ../ch01/mpdistance.py ../ch05/500k.gz ../ch05/506k.gz 120
TS1: ../ch05/500k.gz TS2: ../ch05/506k.gz Window Size: 120
500000 506218
--- 4052.73237 seconds ---
MPdist: 11.4175
$ taskset --cpu-list 0 ../ch01/mpdistance.py ../ch05/500k.gz ../ch05/506k.gz 600
TS1: ../ch05/500k.gz TS2: ../ch05/506k.gz Window Size: 600
500000 506218
--- 4042.52154 seconds ---
MPdist: 30.7796
$ taskset --cpu-list 0 ../ch01/mpdistance.py ../ch05/500k.gz ../ch05/506k.gz 1200
TS1: ../ch05/500k.gz TS2: ../ch05/506k.gz Window Size: 1200
500000 506218
--- 4045.72392 seconds ---
MPdist: 45.1887</pre>
<p><em class="italic">Figure 7</em><em class="italic">.3</em> shows the accuracy of the approximate methods, which are named <strong class="bold">Search</strong> and <strong class="bold">Join</strong>, compared to the real MPdist value, which is named <strong class="bold">Real</strong>, for the three sliding window sizes used.</p>
<div><div><img alt="Figure 7.3 – Comparing the accuracy of the approximate methods to the real MPdist" height="544" src="img/Figure_7.3_B14769.jpg" width="913"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Comparing the accuracy of the approximate methods to the real MPdist</p>
<p>What does the output of <em class="italic">Figure 7</em><em class="italic">.3</em> tell us? First of all, the approximate methods performed pretty well<a id="_idIndexMarker502"/> because the approximate values are really close to the real MPdist values. So, at least for our example time series, the approximate techniques are very accurate.</p>
<p>Similarly, <em class="italic">Figure 7</em><em class="italic">.4</em> compares the times of the approximate methods to the time of the <code>stumpy</code> computation when running on a single CPU core for the three sliding window sizes used – the presented times for the approximate methods <em class="italic">do not include the time it takes to create the two </em><em class="italic">iSAX indexes</em>.</p>
<div><div><img alt="Figure 7.4 – Comparing the times of the approximate methods to the real MPdist" height="548" src="img/Figure_7.4_B14769.jpg" width="915"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Comparing the times of the approximate methods to the real MPdist</p>
<p>What does the output of <em class="italic">Figure 7</em><em class="italic">.4</em> tell us? The first technique is really slow and should not be used – that is the purpose of experimentation: to find out what works well and what does not. On the other hand, the performance of the second approximate technique is<a id="_idIndexMarker503"/> very good. Additionally, <em class="italic">Figure 7</em><em class="italic">.4</em> shows that the <code>stumpy</code> computation takes the same time regardless of the sliding window size – this is a good and desirable feature of the MASS algorithm.</p>
<h1 id="_idParaDest-169"><a id="_idTextAnchor180"/>Summary</h1>
<p>Although the main purpose of iSAX is to help us search for subsequences by indexing them, there are other ways to use an iSAX index.</p>
<p>In this chapter, we presented a way to approximately compute the Matrix Profile vectors and two ways to approximately compute the MPdist distance between two time series. All these techniques use iSAX indexes.</p>
<p>We presented two ways to approximately compute MPdist. Out of the two methods, the one that joins two iSAX indexes is much more efficient than the other – so the use of an iSAX index by itself does not guarantee efficiency; we have to use an iSAX index the right way to get better results.</p>
<p>There is a small chapter left to finish this book, which is about the next steps you can follow if you are really into time series and databases.</p>
<h1 id="_idParaDest-170"><a id="_idTextAnchor181"/>Useful links</h1>
<ul>
<li>The <code>stumpy</code> Python package: <a href="https://pypi.org/project/stumpy/">https://pypi.org/project/stumpy/</a></li>
<li>The <code>numba</code> Python package: <a href="https://pypi.org/project/numba/">https://pypi.org/project/numba/</a></li>
<li>The RMSE: <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation">https://en.wikipedia.org/wiki/Root-mean-square_deviation</a></li>
<li>The UCR Matrix Profile page: <a href="https://www.cs.ucr.edu/~eamonn/MatrixProfile.xhtml">https://www.cs.ucr.edu/~eamonn/MatrixProfile.xhtml</a></li>
<li>SAX home page: <a href="https://www.cs.ucr.edu/~eamonn/SAX.htm">https://www.cs.ucr.edu/~eamonn/SAX.htm</a></li>
</ul>
<h1 id="_idParaDest-171"><a id="_idTextAnchor182"/>Exercises</h1>
<p>Try to do the following exercises:</p>
<ul>
<li>Try to use <code>mp.py</code> with a time series with 50,000 elements and see how much time it takes to complete for sliding window sizes of <code>16</code>, <code>2048</code>, and <code>4096</code>.</li>
<li>Try to use <code>mp.py</code> with a time series with 65,000 elements and see how much time it takes to complete.</li>
<li>Experiment with the exclusion zone limits of <code>mp.py</code> and see what you get.</li>
<li>Use <code>realMP.py</code> and <code>stumpy.stump()</code> to compute the Matrix Profile vectors for a time series with 200,000 elements – create that time series if you do not have one.</li>
<li>Use <code>realMP.py</code> and <code>stumpy.stump()</code> to compute the Matrix Profile vectors for a time series with 500,000 elements. Now, consider that a time series with 500,000 elements is on the small side!</li>
<li>Try <code>realMP.py</code> on the <code>2M.gz</code> time series from <a href="B14769_04.xhtml#_idTextAnchor102"><em class="italic">Chapter 4</em></a> using a single CPU code. As you can see, <code>realMP.py</code> starts getting really slow with larger time series. Now, consider that a time series with 2,000,000 elements is not big.</li>
<li>We can make <code>mp.py</code> a little faster by storing the normalized versions of the subsequences and using the normalized versions when calculating the Euclidean distances, instead of computing the normalized versions inside the <code>euclidean()</code> function every time we call <code>euclidean()</code>. Try to implement that functionality.</li>
<li>Similarly, we can make <code>mpdist.py</code> faster by storing the normalized versions of the subsequences and using them for the Euclidean distance computations.</li>
<li>Create an image similar to <em class="italic">Figure 7</em><em class="italic">.4</em> but for larger time series. Begin with time series with 1,000,000 elements and see what you get.</li>
</ul>
</div>
</div></body></html>