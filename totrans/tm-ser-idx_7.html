<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer052">
<h1 class="chapter-number" id="_idParaDest-149"><a id="_idTextAnchor160"/>7</h1>
<h1 id="_idParaDest-150"><a id="_idTextAnchor161"/>Using iSAX to Approximate MPdist</h1>
<p>So far in this book, we have seen the use of iSAX for searching subsequences and joining iSAX indexes based on SAX representations but no other applications <span class="No-Break">of it.</span></p>
<p>In this chapter, we are going to use iSAX indexes to approximately calculate the <strong class="bold">Matrix Profile</strong> vectors as well as the <strong class="bold">MPdist</strong> distance between two time series – we are still going to use iSAX for searching and joining, but the end results are going to be more sophisticated. The idea that governs this chapter is the perception that <em class="italic">terminal nodes in an iSAX index group have similar subsequences</em> from a SAX representation perspective – this is what we are trying to take advantage of for our <span class="No-Break">approximate computations.</span></p>
<p>In this chapter, we are going to cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>Understanding the <span class="No-Break">Matrix Profile</span></li>
<li>Computing the Matrix Profile <span class="No-Break">using iSAX</span></li>
<li><span class="No-Break">Understanding MPdist</span></li>
<li>Calculating MPdist <span class="No-Break">using iSAX</span></li>
<li>Implementing the MPdist calculation <span class="No-Break">in Python</span></li>
<li>Using the <span class="No-Break">Python code</span></li>
</ul>
<h1 id="_idParaDest-151"><a id="_idTextAnchor162"/>Technical requirements</h1>
<p>The GitHub repository for the book is at <a href="https://github.com/PacktPublishing/Time-Series-Indexing">https://github.com/PacktPublishing/Time-Series-Indexing</a>. The code for each chapter is in its own directory. Therefore, the code for <a href="B14769_07.xhtml#_idTextAnchor160"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> can be found in the <strong class="source-inline">ch07</strong> folder of the <span class="No-Break">GitHub repository.</span></p>
<h1 id="_idParaDest-152"><a id="_idTextAnchor163"/>Understanding the Matrix Profile</h1>
<p>Time series are <a id="_idIndexMarker442"/>everywhere, and there are many tasks that we might need to perform on large time series including similarity search, outlier detection, classification, and clustering. Dealing directly with a large time series is very time-consuming and is going to slow down the process. Most of the aforementioned tasks are based on the computation of the nearest neighbor of subsequences using a given sliding window size. This is where the <strong class="bold">Matrix Profile</strong> comes into play because it helps you perform the previous tasks once you have <span class="No-Break">computed them.</span></p>
<p>We already saw the Matrix Profile in <a href="B14769_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, but in this section, we are going to discuss it in more detail in order to understand better the reason that it is so slow <span class="No-Break">to compute.</span></p>
<p>Various research papers exist that present and extend the Matrix Profile, including <span class="No-Break">the following:</span></p>
<ul>
<li><em class="italic">Matrix Profile I: All Pairs Similarity Joins for Time Series: A Unifying View That Includes Motifs, Discords and Shapelets</em>, written by Chin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding, Hoang Anh Dau, Diego Furtado Silva, Abdullah Mueen, and Eamonn J. <span class="No-Break">Keogh (</span><a href="https://ieeexplore.ieee.org/document/7837992"><span class="No-Break">https://ieeexplore.ieee.org/document/7837992</span></a><span class="No-Break">)</span></li>
<li><em class="italic">Matrix Profile II: Exploiting a Novel Algorithm and GPUs to Break the One Hundred Million Barrier for Time Series Motifs and Joins</em>, written by Yan Zhu, Zachary Zimmerman, Nader Shakibay Senobari, Chin-Chia Michael Yeh, Gareth Funning, Abdullah Mueen, Philip Brisk, and Eamonn <span class="No-Break">Keogh (</span><a href="https://ieeexplore.ieee.org/abstract/document/7837898"><span class="No-Break">https://ieeexplore.ieee.org/abstract/document/7837898</span></a><span class="No-Break">)</span></li>
<li><em class="italic">Matrix profile goes MAD: variable-length motif and discord discovery in data series</em>, written by Michele Linardi, Yan Zhu, Themis Palpanas, and Eamonn J. <span class="No-Break">Keogh (</span><a href="https://doi.org/10.1007/s10618-020-00685-w"><span class="No-Break">https://doi.org/10.1007/s10618-020-00685-w</span></a><span class="No-Break">)</span></li>
</ul>
<p class="callout-heading">About normalization</p>
<p class="callout">As it happens with the SAX representation, all Euclidean distances that are going to be computed in this chapter use <span class="No-Break">normalized subsequences.</span></p>
<p>The next subsection shows what the Matrix Profile <span class="No-Break">computation returns.</span></p>
<h2 id="_idParaDest-153"><a id="_idTextAnchor164"/>What does the Matrix Profile compute?</h2>
<p>In this subsection, we <a id="_idIndexMarker443"/>are going to explain what the Matrix Profile calculates. Imagine having a time series and a sliding window size that is smaller than the time series length. The Matrix Profile computes <span class="No-Break"><em class="italic">two vectors</em></span><span class="No-Break">.</span></p>
<p>The first vector contains the <em class="italic">Euclidean distances of the nearest neighbor</em> of each subsequence. The value at index <strong class="source-inline">0</strong> is the Euclidean distance of the nearest neighbor of the subsequence that begins at index <strong class="source-inline">0</strong>, and <span class="No-Break">so on.</span></p>
<p>In the second vector, the value at each place of the vector is the index of the subsequence that is the nearest neighbor and corresponds to the Euclidean distance stored in the previous vector. So, if the value at index <strong class="source-inline">0</strong> is <strong class="source-inline">123</strong>, this means that the nearest neighbor of the subsequence that begins at index <strong class="source-inline">0</strong> in the original time series is the subsequence that begins at index <strong class="source-inline">123</strong> in the original time series. The first vector is going to contain that Euclidean <span class="No-Break">distance value.</span></p>
<p>It is very important to understand that when computing the Matrix Profile for a time series using a self-join – that is, by looking for the nearest neighbor at the subsequences of the same time series – we need to <em class="italic">exclude the subsequences that are close</em> to the subsequence that we are examining. This is required because subsequences that share many elements in the same order tend to have smaller Euclidean distances by default. However, when dealing with a subsequence that is from another time series, we do not need to exclude any subsequences from <span class="No-Break">the calculations.</span></p>
<p>A naïve implementation of the computation of the Matrix Profile vectors is to get the first subsequence, compare it to all other subsequences (excluding the subsequences that are close), find its nearest neighbor, and put the Euclidean distance and the index of the nearest neighbor at index <strong class="source-inline">0</strong> of the two vectors. Then, do the same for all other subsequences. Although this works for smaller time series, it is not very efficient as its algorithmic complexity is <span class="_-----MathTools-_Math_Variable_v-normal">O</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base">)</span>. This means that for a time series with 10,000 subsequences, we need to perform 10,000 times 10,000 computations (100,000,000). We are going to implement that algorithm to understand how slow it can be in <span class="No-Break">real life.</span></p>
<p>The authors of the original Matrix Profile paper created a clever technique that involves <strong class="bold">Fast Fourier</strong> transforms <a id="_idIndexMarker444"/>that compute the Matrix Profile vectors with a viable complexity – the name of the algorithm is <strong class="bold">Mueen’s Algorithm for Similarity Search</strong> (<strong class="bold">MASS</strong>). If <a id="_idIndexMarker445"/>you want to learn more about the details of the MASS algorithm and the ideas behind the Matrix Profile, you should read the <em class="italic">Matrix Profile I: All Pairs Similarity Joins for Time Series: A Unifying View That Includes Motifs, Discords and Shapelets</em> <span class="No-Break">paper (</span><a href="https://ieeexplore.ieee.org/document/7837992"><span class="No-Break">https://ieeexplore.ieee.org/document/7837992</span></a><span class="No-Break">).</span></p>
<p>The next section presents an implementation of the naïve algorithm for computing the Matrix Profile <a id="_idIndexMarker446"/>vectors. The naivete of the algorithm lies in its complexity, not <span class="No-Break">its accuracy.</span></p>
<h2 id="_idParaDest-154"><a id="_idTextAnchor165"/>Manually computing the exact Matrix Profile</h2>
<p>In this subsection, we<a id="_idIndexMarker447"/> are going to manually compute the exact Matrix Profile to show how slow the process can be, especially when working with large time series. We are using the word <em class="italic">exact</em> to differentiate this from the approximate Matrix Profile computation that we are going to implement in the <em class="italic">Computing Matrix Profile using iSAX</em> section of <span class="No-Break">this chapter.</span></p>
<p>The last Python statements in the <strong class="source-inline">main()</strong> function of <strong class="source-inline">mp.py</strong> are <span class="No-Break">the following:</span></p>
<pre class="source-code">
    dist, index = mp(ta, windowSize)
    print(dist)
    print(index)</pre>
<p>The first statement runs the <strong class="source-inline">mp()</strong> function, which returns two values, both of them being lists (vectors), which are the two Matrix <span class="No-Break">Profile vectors.</span></p>
<p>The implementation of the <strong class="source-inline">mp()</strong> function is where we compute the two vectors and is presented in two parts. The first part comes with the <span class="No-Break">following code:</span></p>
<pre class="source-code">
def mp(ts, window):
    l = len(ts) - window + 1
    dist = [None] * l
    index = [None] * l
    for i1 in range(l):
        t1 = ts[i1:i1+window]
        min = None
        minIndex = 0
        <strong class="bold">exclusionMin</strong> = i1 - window // 4
        if exclusionMin &lt; 0:
            exclusionMin = 0
        <strong class="bold">exclusionMax</strong> = i1 + window // 4
        if exclusionMax &gt; l-1:
            exclusionMax = l-1</pre>
<p>In the previous code, we iterate over all the subsequences of the given time series. For each such subsequence, we define the indexes of the exclusion zone as specified in the <em class="italic">Matrix Profile I: All Pairs Similarity Joins for Time Series: A Unifying View That Includes Motifs, Discords and </em><span class="No-Break"><em class="italic">Shapelets</em></span><span class="No-Break"> paper.</span></p>
<p>For a sliding window size of <strong class="source-inline">16</strong>, the exclusion zone is <strong class="source-inline">4</strong> elements (<strong class="source-inline">16 // 4</strong>) on the left and <strong class="source-inline">4</strong> elements (<strong class="source-inline">16 // 4</strong>) on the <a id="_idIndexMarker448"/>right side of <span class="No-Break">the subsequence.</span></p>
<p>The second part of <strong class="source-inline">mp()</strong> is <span class="No-Break">as follows:</span></p>
<pre class="source-code">
        for i2 in range(l):
            # Exclusion zone
            if i2 &gt;= exclusionMin and i2 &lt;= exclusionMax:
                continue
            t2 = ts[i2:i2+window]
            temp = round(euclidean(t1, t2), 3)
            if min == None:
                min = temp
                minIndex = i2
            elif min &gt; temp:
                min = temp
                minIndex = i2
        dist[i1] = min
        index[i1] = minIndex
    return dist, index</pre>
<p>In this part of the section, we compare each subsequence from the first part of the code with all the subsequences of the time series while taking into account the <span class="No-Break">exclusion zone.</span></p>
<p>The bad thing about <strong class="source-inline">mp()</strong> is that it contains two <strong class="source-inline">for</strong> loops, which makes its computational <span class="No-Break">complexity </span><span class="No-Break"><em class="italic">O(n</em></span><span class="No-Break"><span class="superscript">2</span></span><span class="No-Break"><em class="italic">)</em></span><span class="No-Break">.</span></p>
<p>The output of <strong class="source-inline">mp.py</strong> when working with the <strong class="source-inline">ts.gz</strong> time series from <a href="B14769_06.xhtml#_idTextAnchor145"><span class="No-Break"><em class="italic">Chapter 6</em></span></a> (which is found in the <strong class="source-inline">ch06</strong> directory of the GitHub repository of the book) is similar to the following for a sliding window of <strong class="source-inline">16</strong> – we are going to use the output to test the correctness of <a id="_idIndexMarker449"/>our implementation by comparing it to the original Matrix Profile algorithm and <span class="No-Break">its output:</span></p>
<pre class="source-code">
$ ./mp.py ../ch06/ts.gz -w 16
TS: ../ch06/ts.gz Sliding Window size: 16
[3.294, 3.111, 3.321, 3.535, 3.285, 3.373, 3.332, 3.693, 4.066, 4.065, 3.898, 3.484, 3.372, 3.1, 3.047, 3.299, 3.056, 3.361, 3.766, 3.759, 3.871, 3.884, 3.619, 3.035, 2.358, 3.012, 3.052, 3.136, 3.161, 3.219, 3.309, 3.526, 3.386, 3.973, 4.207, 4.101, 4.249, 4.498, 4.492, 4.255, 4.241, 3.285, 3.517, 3.494, 3.257, 3.316, 3.526, 4.183, 4.011, 3.294, 3.111, 3.321, 3.535, 3.1, 3.047, 3.332, 3.035, 2.358, 3.012, 3.052, 3.136, 3.161, 3.219, 3.201, 3.187, 3.017, 2.676, 2.763, 2.959, 3.952, 3.865, 3.678, 3.687, 3.201, 3.187, 3.017, 2.676, 2.763, 2.959, 3.316, 3.526, 3.899, 3.651, 3.664, 3.885]
[49, 50, 51, 52, 53, 54, 55, 56, 57, 46, 65, 27, 28, 53, 54, 74, 75, 76, 77, 59, 60, 61, 55, 56, 57, 58, 59, 60, 61, 62, 14, 15, 16, 66, 71, 68, 69, 56, 20, 63, 26, 75, 66, 67, 78, 79, 80, 81, 82, 0, 1, 2, 3, 13, 14, 6, 23, 24, 25, 26, 27, 28, 29, 73, 74, 75, 76, 77, 78, 79, 80, 61, 62, 63, 64, 65, 66, 67, 68, 45, 46, 62, 63, 64, 65]
--- 0.36465 seconds ---</pre>
<p>Naively thinking, the subsequences with the smallest and largest Euclidean distances can be considered outliers as they differ from all other subsequences – this is an example of the use of the Matrix Profile for <span class="No-Break">anomaly detection.</span></p>
<p>Using a sliding <a id="_idIndexMarker450"/>window size of <strong class="source-inline">32</strong>, <strong class="source-inline">mp.py</strong> produces the following kind <span class="No-Break">of output:</span></p>
<pre class="source-code">
$ ./mp.py ../ch06/ts.gz -w 32
TS: ../ch06/ts.gz Sliding Window size: 32
[4.976, 5.131, 5.38, 5.485, 5.636, 5.75, 5.87, 6.076, 6.502, 6.705, 6.552, 6.145, 6.279, 6.599, 6.766, 6.667, 6.577, 6.429, 6.358, 6.358, 5.978, 5.804, 5.588, 5.092, 4.976, 5.01, 5.35, 5.456, 6.036, 6.082, 6.258, 6.513, 6.556, 6.553, 6.672, 6.745, 6.767, 6.777, 7.018, 7.12, 6.564, 6.203, 6.291, 6.118, 6.048, 5.869, 6.142, 6.431, 6.646, 4.976, 5.131, 5.38, 5.485, 5.636, 5.75, 5.588, 5.092, 4.976, 5.01, 5.35, 5.456, 6.036, 6.082, 6.258, 6.513, 6.556, 6.598, 6.518, 6.473]
[49, 50, 51, 52, 53, 54, 55, 56, 24, 58, 24, 25, 26, 27, 63, 64, 65, 55, 56, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 62, 63, 0, 65, 67, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 17, 57, 59]
--- 0.22118 seconds ---</pre>
<p>Lastly, using a sliding window size of <strong class="source-inline">64</strong>, the produced output is <span class="No-Break">the following:</span></p>
<pre class="source-code">
$ ./mp.py ../ch06/ts.gz -w 64
TS: ../ch06/ts.gz Sliding Window size: 64
[10.529, 10.406, 10.475, 10.377, 10.702, 10.869, 10.793, 10.827, 10.743, 11.14, 10.865, 10.819, 10.876, 10.808, 10.802, 10.73, 10.713, 10.67, 11.288, 11.296, 11.113, 11.202, 11.196, 11.121, 11.033, 11.145, 11.228, 11.125, 11.108, 10.865, 10.819, 10.671, 10.702, 10.529, 10.406, 10.475, 10.377]
[33, 34, 35, 36, 32, 33, 34, 35, 36, 28, 29, 30, 31, 32, 33, 34, 35, 36, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 0, 10, 11, 3, 4, 0, 1, 2, 3]
--- 0.03179 seconds ---</pre>
<p>The reason for having a smaller output here is that the bigger the sliding window size, the fewer the number of subsequences that are created from a <span class="No-Break">time series.</span></p>
<p>Now, let us experiment with a time series with 25,000 elements that was created <span class="No-Break">as follows:</span></p>
<pre class="source-code">
$ ../ch01/synthetic_data.py 25000 -5 5 &gt; 25k
$ gzip 25k</pre>
<p>The results for <strong class="source-inline">25k.gz</strong> with the same sliding window sizes as before are as follows (only the times are shown – the rest of the output is omitted <span class="No-Break">for brevity):</span></p>
<pre class="source-code">
$ ./mp.py -w 16 25k.gz
--- 43707.95353 seconds ---
$ ./mp.py -w 32 25k.gz
--- 44162.44419 seconds ---
$ ./mp.py -w 64 25k.gz
--- 45113.62417 seconds ---</pre>
<p>At this point, we<a id="_idIndexMarker451"/> should be aware of the fact that computing the Matrix Profile vectors can be really slow as it took <strong class="source-inline">mp.py</strong> 45,113 seconds to compute the Matrix Profile in the <span class="No-Break">last run.</span></p>
<p>Can you think of the reason that even a small increase in the sliding window size also increases the overall times? The answer is that the bigger the sliding window size, the bigger the subsequence length, and therefore, the more time it takes to compute the Euclidean distance between two subsequences. Here is the time it takes to compute the Matrix Profile vectors for a sliding window size <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">2048</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
$ ./mp.py -w 2048 25k.gz
--- 46271.63763 seconds ---</pre>
<p>Have in mind that <em class="italic">the MASS algorithm does not have such an issue</em> as it computes the Euclidean distances in its own clever way. As a result, its performance depends on the time series <span class="No-Break">length only.</span></p>
<p>Now, let us present a Python script that computes the exact Matrix Profile using the MASS algorithm with the help of the <strong class="source-inline">stumpy</strong> Python package. We are using the <strong class="source-inline">realMP.py</strong> script for <a id="_idIndexMarker452"/>computing the Matrix Profile vectors, which has the <span class="No-Break">following implementation:</span></p>
<pre class="source-code">
#!/usr/bin/env python
import pandas as pd
import argparse
import time
import stumpy
import numpy as np
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("-w", "--window", dest = "window",
        default = "16", help="Sliding Window", type=int)
    parser.add_argument("TS")
    args = parser.parse_args()
    windowSize = args.window
    inputTS = args.TS
    print("TS:", inputTS, "Sliding Window size:",
        windowSize)
    start_time = time.time()
    ts = pd.read_csv(inputTS, names=['values'],
        compression='gzip')
    ts_numpy = ts.to_numpy()
    ta = ts_numpy.reshape(len(ts_numpy))
    realMP = <strong class="bold">stumpy.stump</strong>(ta, windowSize)
    realDistances = realMP[:,0]
    realIndexes = realMP[:,1]
    print("--- %.5f seconds ---" % (time.time() –
        start_time))
    print(realDistances)
    print(realIndexes)
if __name__ == '__main__':
    main()</pre>
<p>The return value of <strong class="source-inline">stumpy.stump()</strong> is a multi-dimensional array. The first column (<strong class="source-inline">[:,0]</strong>) is the<a id="_idIndexMarker453"/> vector of distances, and the second column (<strong class="source-inline">[:,1]</strong>) is the vector of indexes. In the previous code, we print both these vectors, which is not very handy when dealing with large time series – comment out these two <strong class="source-inline">print()</strong> statements if <span class="No-Break">you want.</span></p>
<p>In order to verify the correctness of <strong class="source-inline">mp.py</strong>, we present the output of <strong class="source-inline">realMP.py</strong> for the <strong class="source-inline">ts.gz</strong> time series and a sliding window size <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">64</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
$ ./realMP.py ../ch06/ts.gz -w 64
TS: ../ch06/ts.gz Sliding Window size: 64
--- 11.31371 seconds ---
[10.5292 10.40594 10.47460 10.3770 10.7024 10.8689 10.7928
 10.8274 10.74260 11.140 10.864 10.818 10.8757 10.8078
 10.8017 10.7296 10.7129 10.6704
 11.2882 11.2963 11.1125 11.2019 11.19556 11.1206
 11.0330 11.14458 11.22779 11.12475
 11.10825 10.864619 10.8186 10.6714
 10.7024 10.52926 10.40594 10.4746 10.3770]
[33 34 35 36 32 33 34 35 36 28 29 30 31 32 33 34 35 36 0 1 2 3 4 5 6 7 8 8 0 10 11 3 4 0 1 2 3]</pre>
<p>Now that we are sure about the correctness of <strong class="source-inline">mp.py</strong>, let us experiment with the <strong class="source-inline">25k.gz</strong> time series to see how much time it takes to compute the exact Matrix <span class="No-Break">Profile vectors.</span></p>
<p>The time it takes <strong class="source-inline">realMP.py</strong> and the <strong class="source-inline">stumpy.stump()</strong> function to compute the Matrix Profile<a id="_idIndexMarker454"/> vectors <em class="italic">on a single CPU core</em> for the <strong class="source-inline">25k.gz</strong> time series is <span class="No-Break">the following:</span></p>
<pre class="source-code">
$ taskset --cpu-list 0 ./realMP.py 25k.gz -w 1024
TS: 25k.gz Sliding Window size: 1024
--- 11.19547 seconds ---
[42.41325061659 42.4212959655 42.45021115618 ...
 42.64248908665 42.64380072599 42.6591584368]
[10218 10219 10220 ... 7240 7241 20243]</pre>
<p>The time it takes <strong class="source-inline">realMP.py</strong> to compute the Matrix Profile vectors for the <strong class="source-inline">25k.gz</strong> time series on an Intel i7 with 8 CPU cores is <span class="No-Break">the following:</span></p>
<pre class="source-code">
$ ./realMP.py 25k.gz -w 1024
TS: 25k.gz Sliding Window size: 1024
--- 9.68259 seconds ---
[42.41325061659 42.4212959655 42.45021115618 ...
 42.64248908665 42.64380072599 42.6591584368]
[10218 10219 10220 ... 7240 7241 20243]</pre>
<p>Moreover, the time it takes <strong class="source-inline">realMP.py</strong> and the <strong class="source-inline">stumpy.stump()</strong> function to compute the Matrix Profile vectors <em class="italic">on a single CPU core</em> for the <strong class="source-inline">ch06/100k.gz</strong> time series and sliding window size of <strong class="source-inline">1024</strong> is <span class="No-Break">the following:</span></p>
<pre class="source-code">
$ taskset --cpu-list 0 ./realMP.py ../ch06/100k.gz -w 1024
TS: ../ch06/100k.gz Sliding Window size: 1024
--- 44.45451 seconds ---
[42.0661718111 42.044733861 42.050637591 ...
 42.252694931 42.225343182 42.2147590858]
[51861 51862 51863 ... 13502 13503 13504]</pre>
<p>Lastly, let us try <strong class="source-inline">realMP.py</strong> on a <em class="italic">single CPU core</em> on the <strong class="source-inline">500k.gz</strong> time series from <a href="B14769_04.xhtml#_idTextAnchor102"><span class="No-Break"><em class="italic">Chapter 4</em></span></a><span class="No-Break">:</span></p>
<pre class="source-code">
$ taskset --cpu-list 0 ./realMP.py ../ch04/500k.gz -w 1024
TS: ../ch04/500k.gz Sliding Window size: 1024
--- 1229.49608 seconds ---
[41.691930926 41.689248432 41.642429848 ...
 41.712625718 41.6520521157 41.636642904]
[446724 446725 446726 ... 260568 260569 260570]</pre>
<p>The conclusion from the previous output is that computing the Matrix Profile gets slower as the length of the time series gets bigger, which is the main reason for thinking about an approximate computation of it. What we lose in accuracy, we gain in time. We cannot <span class="No-Break">have </span><span class="No-Break"><a id="_idIndexMarker455"/></span><span class="No-Break">everything!</span></p>
<p>The next section explains the technique that we are going to use to approximately compute the Matrix Profile vectors with the help <span class="No-Break">of iSAX.</span></p>
<h1 id="_idParaDest-155"><a id="_idTextAnchor166"/>Computing the Matrix Profile using iSAX</h1>
<p>First of all, let <a id="_idIndexMarker456"/>us<a id="_idIndexMarker457"/> make something clear: we are going to present <em class="italic">an approximate method</em>. If you want to calculate the exact Matrix Profile, then you should use an implementation that uses the <span class="No-Break">original algorithm.</span></p>
<p>The idea behind the used technique is the following: <em class="italic">it is more likely that the nearest neighbor of a subsequence is going to be found in the subsequences stored in the same terminal node as the subsequence under examination</em>. Therefore, we do not need to check all the subsequences of the time series, just a small subset <span class="No-Break">of them.</span></p>
<p>The next subsection discusses and resolves an issue that might come up in our calculations, which<a id="_idIndexMarker458"/> is <a id="_idIndexMarker459"/>what are we going to do if we cannot find a proper match for a subsequence in a <span class="No-Break">terminal node.</span></p>
<h2 id="_idParaDest-156"><a id="_idTextAnchor167"/>What happens if there is not a valid match?</h2>
<p>In this subsection, we<a id="_idIndexMarker460"/> are going to clarify the problematic cases of the process. There exist two conditions that might end up in an <span class="No-Break">undesired situation:</span></p>
<ul>
<li>A terminal node contains a single <span class="No-Break">subsequence only</span></li>
<li>For a given subsequence, all the remaining subsequences of the terminal node are in the <span class="No-Break">exclusion zone</span></li>
</ul>
<p>In both cases, we are not going to be able to find the approximate nearest neighbor of a subsequence. Can we resolve <span class="No-Break">these issues?</span></p>
<p>There exist multiple answers to that question, including doing nothing or choosing a different subsequence and using that to compute the Euclidean distance of the nearest neighbor. We are going with the latter solution, but instead of randomly choosing a subsequence, we are going for the subsequence that is next to the left side of the exclusion zone. If there is no space on the left side of the exclusion zone, we are going to choose the subsequence that is next to the right side of the exclusion zone. As these two conditions cannot happen at the same time, we <span class="No-Break">are good!</span></p>
<p>The next subsection discusses how to compute the error of the approximate Matrix Profile vector of distances compared to the real Matrix Profile vector <span class="No-Break">of distances.</span></p>
<h2 id="_idParaDest-157"><a id="_idTextAnchor168"/>Calculating the error</h2>
<p>As explained <a id="_idIndexMarker461"/>earlier, we are computing an approximate Matrix Profile vector. In such cases, we need a way to compute how far we are from the real values. There exist various ways to compute an error value between two quantities. As a Matrix Profile is a list of values, we need to find a way to compute an error value that supports a list of values, not single <span class="No-Break">values only.</span></p>
<p>The most common way is to find the Euclidean distance between the approximate vector and the exact vector. However, this does not always tell the whole truth. A good alternative would be to use the <strong class="bold">Root Mean Square </strong><span class="No-Break"><strong class="bold">Error</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">RMSE</strong></span><span class="No-Break">).</span></p>
<p>The formula for the RMSE is a little complex at first. It is presented in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer048">
<img alt="Figure 7.1 – The RMSE formula" height="165" src="image/Figure_7.1_B14769.jpg" width="993"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – The RMSE formula</p>
<p>In practice, this means that we find the difference between the actual value and the approximate one and we square that. We do that for all the pairs and then add all these values – this is the purpose of the big Greek Sigma letter. After that, we divide by the number of the pairs. Lastly, we find the square root of that last value and we are done. If you are not good at mathematics, bear in mind that you do not need to remember that formula – we are going to implement it in Python in <span class="No-Break">a while.</span></p>
<p>The desired property that the RMSE has is that it takes into account the number of elements that we compare. Put simply, the RMSE takes the <em class="italic">average</em>, whereas the Euclidean distance takes the <em class="italic">sum</em>. In our case, using the average error looks <span class="No-Break">more appropriate.</span></p>
<p>As an example, the Euclidean distance between <strong class="source-inline">(0, 0, 0, 2, 2)</strong> and <strong class="source-inline">(2, 1, 0, 0, 0)</strong> is equal to <strong class="source-inline">3.6055</strong>. On the other hand, the RMSE of these two vectors is equal <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">1.61245</strong></span><span class="No-Break">.</span></p>
<p>With all that in mind, we are ready to present our <span class="No-Break">approximate implementation.</span></p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor169"/>Approximate Matrix Profile implementation</h2>
<p>In this subsection, we<a id="_idIndexMarker462"/> present the Python script that approximately computes the Matrix <span class="No-Break">Profile vectors.</span></p>
<p>The important code for <strong class="source-inline">apprMP.py</strong> can be found in <strong class="source-inline">approximateMP()</strong>, which is presented in four parts. The first part of the function is <span class="No-Break">the following:</span></p>
<pre class="source-code">
def approximateMP(ts_numpy):
    ISAX = isax.iSAX()
    length = len(ts_numpy)
    windowSize = variables.slidingWindowSize
    segments = variables.segments
    # Split sequence into subsequences
    for i in range(length - windowSize + 1):
        ts = ts_numpy[i:i+windowSize]
        ts_node = isax.TS(ts, segments)
        ts_node.index = i
        ISAX.insert(ts_node)
    vDist = [None] * (length - windowSize + 1)
    vIndex = [None] * (length - windowSize + 1)
    nSubsequences = length - windowSize + 1</pre>
<p>The previous code splits the time series into subsequences and creates the iSAX index. It also initializes the <strong class="source-inline">vDist</strong> and <strong class="source-inline">vIndex</strong> variables, for keeping the list of distances and the list of <span class="No-Break">indexes, respectively.</span></p>
<p>The second part of <strong class="source-inline">approximateMP()</strong> is <span class="No-Break">the following:</span></p>
<pre class="source-code">
for k in ISAX.ht:
        t = ISAX.ht[k]
        if t.terminalNode == False:
            continue
        # I is the index of the subsequence
        # in the terminal node
        for i in range(t.nTimeSeries()):
            # This is the REAL index of the subsequence
            # in the time series
            idx = t.children[i].index
            # This is the subsequence that we are examining
            currentTS = t.children[i].ts
            exclusionMin = idx–- windowSize // 4
            if exclusionMin &lt; 0:
                exclusionMin = 0
            exclusionMax = idx + windowSize // 4
            if exclusionMax &gt; nSubsequences-1:
                exclusionMax = nSubsequences-1
            min = None
            minIndex = 0</pre>
<p>In the previous code, we<a id="_idIndexMarker463"/> take each node of the iSAX index and determine whether it is a terminal node or not – we are only interested in terminal nodes. If we are dealing with a terminal node, we process each subsequence stored there. First, we define the indexes of the exclusion zone making sure that the minimum value of the left side of the exclusion zone is <strong class="source-inline">0</strong> – this is the index of the first element of the time series – and the maximum value of the right side of the exclusion zone is not bigger than the length of the time series <span class="No-Break">minus 1.</span></p>
<p>The third part<a id="_idIndexMarker464"/> of it is <span class="No-Break">the following:</span></p>
<pre class="source-code">
           for sub in range(t.nTimeSeries()):
                # This is the REAL index of the subsequence
                # we are examining in the time series
                currentIdx = t.children[sub].index
                if currentIdx &gt;= exclusionMin and currentIdx &lt;= exclusionMax:
                    continue
                temp = round(<strong class="bold">tools.euclidean(currentTS,</strong>
                    <strong class="bold">t.children[sub].ts</strong>), 3)
                if min == None:
                    min = temp
                    minIndex = currentIdx
                elif min &gt; temp:
                    min = temp
                    minIndex = currentIdx</pre>
<p>We compare each subsequence of the selected terminal node with the rest of the subsequences it contains because we expect that there is a high probability of the nearest neighbor being in the <span class="No-Break">same node.</span></p>
<p>Then, we make sure that the index of the subsequence that is going to be compared with the initial subsequence is not in the exclusion zone. If we find such a subsequence, we compute the Euclidean distance and keep the relevant index value. From all these subsequences that are outside the exclusion zone and are located in the terminal node, we keep the minimum Euclidean distance and the <span class="No-Break">related index.</span></p>
<p>We do that for all the subsequences in all the terminal nodes of the <span class="No-Break">iSAX index.</span></p>
<p>The last part<a id="_idIndexMarker465"/> of the <strong class="source-inline">approximateMP()</strong> function is <span class="No-Break">the following:</span></p>
<pre class="source-code">
            # Pick left limit first, then the right limit
            if min == None:
                if exclusionMin-1 &gt; 0:
                    randomSub = ts_numpy[exclusionMin-
                        1:exclusionMin+windowSize-1]
                    vDist[idx] = round(tools.euclidean(
                        currentTS, randomSub), 3)
                    vIndex[idx] = exclusionMin - 1
                else:
                    randomSub = ts_numpy[exclusionMax+
                        1:exclusionMax+windowSize+1]
                    vDist[idx] = round(tools.euclidean(
                        currentTS, randomSub), 3)
                    vIndex[idx] = exclusionMax + 1
            else:
                vDist[idx] = min
                vIndex[idx] = minIndex
    return vIndex, vDist</pre>
<p>If, at this point, we do not have a valid Euclidean distance value (<strong class="source-inline">None</strong>), we compare the initial subsequence with the subsequence next to the left side of the exclusion zone, if it exists – this means if the left side of the exclusion zone is not <strong class="source-inline">0</strong>. Otherwise, we compare it with the subsequence next to the right side of the exclusion zone. We put the relevant index and Euclidean distance into the <strong class="source-inline">vIndex</strong> and <strong class="source-inline">vDist</strong> variables, respectively. However, if we already have an index and Euclidean distance from earlier, we use <span class="No-Break">these </span><span class="No-Break"><a id="_idIndexMarker466"/></span><span class="No-Break">values.</span></p>
<p>The next subsection compares the accuracy of our approximate technique when using different <span class="No-Break">iSAX parameters.</span></p>
<h2 id="_idParaDest-159"><a id="_idTextAnchor170"/>Comparing the accuracy of two different parameter sets</h2>
<p>In this<a id="_idIndexMarker467"/> subsection, we are going to compute the approximate Matrix Profile vector of a single time series using two different sets of iSAX parameters and check the accuracy of the results using <span class="No-Break">the RMSE.</span></p>
<p>To make things simpler, we have created a Python script that computes the two approximate Matrix Profile vectors of Euclidean distances, as well as the exact Matrix Profile vectors, and calculates the RMSE – the name of the script is <strong class="source-inline">rmse.py</strong>. We are not going to present the entire Python code of <strong class="source-inline">rmse.py</strong>, just the important Python statements, starting from the function that computes <span class="No-Break">the RMSE:</span></p>
<pre class="source-code">
def RMSE(realV, approximateV):
    diffrnce = np.subtract(realV, approximateV)
    sqre_err = np.square(diffrnce)
    rslt_meansqre_err = sqre_err.mean()
    error = math.sqrt(rslt_meansqre_err)
    return error</pre>
<p>The previous code implements the computation of the RMSE value according to the formula presented in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></p>
<p>The remaining relevant Python code is located in the <span class="No-Break"><strong class="source-inline">main()</strong></span><span class="No-Break"> function:</span></p>
<pre class="source-code">
    # Real Matrix Profile
    TSreshape = ts_numpy.reshape(len(ts_numpy))
    realMP = stumpy.stump(TSreshape, windowSize)
    realDistances = realMP[:,0]
    # Approximate Matrix Profile
    _, vDist = approximateMP(ts_numpy)
    rmseError = RMSE(realDistances, vDist)
    print("Error =", rmseError)</pre>
<p>First, we compute <a id="_idIndexMarker468"/>the real Matrix Profile vector with <strong class="source-inline">stumpy.stump()</strong>, and then we compute the approximate Matrix Profile vector with the Euclidean distances using <strong class="source-inline">approximateMP()</strong>. After that, we call the <strong class="source-inline">RMSE()</strong> function and get the numeric result, which we print on <span class="No-Break">the screen.</span></p>
<p>So, let us run <strong class="source-inline">rmse.py</strong> and see what <span class="No-Break">we get:</span></p>
<pre class="source-code">
$ ./rmse.py -w 32 -s 4 -t 500 -c 16 25k.gz
Max Cardinality: 16 Segments: 4 Sliding Window: 32 Threshold: 500 Default Promotion: False
Error = 10.70823863253679</pre>
<p>Now, let us use <strong class="source-inline">rmse.py</strong> another time, but this time, with different iSAX parameters, <span class="No-Break">as follows:</span></p>
<pre class="source-code">
$ ./rmse.py -w 32 -s 4 -t 1500 -c 16 25k.gz
Max Cardinality: 16 Segments: 4 Sliding Window: 32 Threshold: 1500 Default Promotion: False
Error = 9.996114543048341</pre>
<p>What do the previous results tell us? First, the results tell us that our approximate technique does not leave any subsequence without a Euclidean distance. If there was such a case, then <strong class="source-inline">rmse.py</strong> would have generated an error message like <span class="No-Break">the following:</span></p>
<pre class="source-code">
TypeError: unsupported operand type(s) for -: 'float' and 'NoneType'</pre>
<p>As, in the initialization of <strong class="source-inline">vDist</strong>, all its elements are set equal to <strong class="source-inline">None</strong>, the previous error means that the value of at least one of the elements was not reset. Therefore, it is still equal to <strong class="source-inline">None</strong> and our code fails to subtract a floating point value, calculated by <strong class="source-inline">stumpy.stump()</strong>, <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">None</strong></span><span class="No-Break">.</span></p>
<p>Apart from that, the results tell us that bigger threshold values produce more accurate results, which makes perfect sense, as there are more subsequences in each terminal node. However, this makes the computation of the approximate Matrix Profile slower. As a general rule, the closer the number of subsequences at each terminal node is to the threshold value, the better the accuracy – we do not want terminal nodes with a small number of subsequences stored <span class="No-Break">in them.</span></p>
<p>Now that we<a id="_idIndexMarker469"/> know about the Matrix Profile, let us discuss MPdist, how it is computed, and the role of the Matrix Profile in <span class="No-Break">this computation.</span></p>
<h1 id="_idParaDest-160"><a id="_idTextAnchor171"/>Understanding MPdist</h1>
<p>Now that we know <a id="_idIndexMarker470"/>about the Matrix Profile, we are ready to learn about MPdist and how the Matrix Profile is used in the calculation of MPdist. The paper that defines the MPdist distance is <em class="italic">Matrix Profile XII: MPdist: A Novel Time Series Distance Measure to Allow Data Mining in More Challenging Scenarios</em>, written by S. Gharghabi, S. Imani, A. Bagnall, A. Darvishzadeh, and E. <span class="No-Break">Keogh (</span><a href="https://ieeexplore.ieee.org/abstract/document/8594928"><span class="No-Break">https://ieeexplore.ieee.org/abstract/document/8594928</span></a><span class="No-Break">).</span></p>
<p>The intuition behind MPdist is that two time series can be considered similar if they <em class="italic">have similar patterns throughout their duration</em>. Such patterns are extracted in the form of subsequences using a sliding window. This is illustrated in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer049">
<img alt="Figure 7.2 – Grouping time series" height="569" src="image/Figure_7.2_B14769.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Grouping time series</p>
<p>In <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.2</em>, we see that MPdist <em class="italic">(c)</em> understands the similarity between time series that follow the same pattern better, whereas Euclidean distance <em class="italic">(b)</em> compares time series based on time, and therefore groups the presented time series differently. In my opinion, the grouping that is based on MPdist is <span class="No-Break">more accurate.</span></p>
<p>The advantages of MPdist (according to the people that created it) are that the MPdist distance measure tries to be more flexible than most available distance measures, including the Euclidean distance, and it takes into account similarities that may not take place at the same time. Additionally, MPdist can compare time series of different sizes –Euclidean distance <a id="_idIndexMarker471"/>cannot do that – and requires just a single parameter (the sliding window size) <span class="No-Break">to operate.</span></p>
<p>The next subsection discusses the way MPdist <span class="No-Break">is computed.</span></p>
<h2 id="_idParaDest-161"><a id="_idTextAnchor172"/>How to compute MPdist</h2>
<p>In this subsection, we<a id="_idIndexMarker472"/> are going to discuss the way the real MPdist is computed in order to better understand the complexity of <span class="No-Break">the process.</span></p>
<p>The computation of MPdist is based on the Matrix Profile. First, we are given two time series, A and B, and a sliding window size. Then, for each subsequence of the first time series, we find its nearest neighbor in the second time series, and we put the related Euclidean distance into a list of values. We do that for all the subsequences of the first time series. This is also <a id="_idIndexMarker473"/>called the <strong class="bold">AB join</strong>. Then, we do the same but for the second time series – this is <a id="_idIndexMarker474"/>called the <strong class="bold">BA join</strong>. So, in the end, we calculated<a id="_idIndexMarker475"/> the <strong class="bold">ABBA join</strong> and we have a list of Euclidean distances that we sort from the smallest to the biggest. From that list, we get the Euclidean distance found at the index value that is equal to <em class="italic">5% of the length of the list</em> – the authors of MPdist decided to use the Euclidean distance at that index as the <span class="No-Break">MPdist value.</span></p>
<p>For both the AB join and BA join, the authors of MPdist use the MASS algorithm to compute the nearest neighbor of each subsequence, in order to avoid the inefficient algorithmic complexity of <span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">O</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">n</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break">.</span></p>
<p>In the next<a id="_idIndexMarker476"/> subsection, we will create a Python script that manually computes the MPdist distance between two <span class="No-Break">time series.</span></p>
<h2 id="_idParaDest-162"><a id="_idTextAnchor173"/>Manually computing MPdist</h2>
<p>In this subsection, we<a id="_idIndexMarker477"/> are going to show how to manually compute the MPdist value between two time series. The idea behind the implementation is based on the code found in <strong class="source-inline">mp.py</strong> – however, fundamental differences exist as the Matrix Profile returns a vector of values instead of a <span class="No-Break">single value.</span></p>
<p>The logic code of <strong class="source-inline">mpdist.py</strong> is implemented in two functions, named <strong class="source-inline">mpdist()</strong> and <strong class="source-inline">JOIN()</strong>. <strong class="source-inline">mpdist()</strong> is implemented <span class="No-Break">as follows:</span></p>
<pre class="source-code">
def mpdist(ts1, ts2, window):
    L_AB = JOIN(ts1, ts2, window)
    L_BA = JOIN(ts2, ts1, window)
    JABBA = L_AB + L_BA
    JABBA.sort()
    index = int(0.05 * (len(JABBA) + 2 * window)) + 1
    return JABBA[index]</pre>
<p>The previous code uses the <strong class="source-inline">JOIN()</strong> function to compute <strong class="source-inline">AB Join</strong> and <strong class="source-inline">BA Join</strong>. Then, it concatenates the numeric results, which are all Euclidean distances, and sorts them. Based on the length of the concatenation, it computes <strong class="source-inline">index</strong>, which is used for selecting a value from the <span class="No-Break"><strong class="source-inline">JABBA</strong></span><span class="No-Break"> array.</span></p>
<p><strong class="source-inline">JOIN()</strong> is implemented <span class="No-Break">as follows:</span></p>
<pre class="source-code">
def JOIN(ts1, ts2, window):
    LIST = []
    l1 = len(ts1) - window + 1
    l2 = len(ts2) - window + 1
    for i1 in range(l1):
        t1 = ts1[i1:i1+window]
        min = round(euclidean(t1, ts2[0:window]), 4)
        for i2 in range(1, l2):
            t2 = ts2[i2:i2+window]
            temp = round(euclidean(t1, t2), 4)
            if min &gt; temp:
                min = temp
        LIST.append(min)
    return LIST</pre>
<p>This is where the<a id="_idIndexMarker478"/> join is implemented. For every subsequence in the <strong class="source-inline">ts1</strong> time series, we find the nearest neighbor in the <strong class="source-inline">ts2</strong> time series – there is no need for an exclusion zone in <span class="No-Break">this case.</span></p>
<p>The bad thing about <strong class="source-inline">mpdist.py</strong> is that it contains two <strong class="source-inline">for</strong> loops, which makes its computational complexity <span class="_-----MathTools-_Math_Variable_v-normal">O</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base">)</span> – this is no surprise, as MPdist is based on the Matrix Profile. Therefore, the previous technique is viable for small time series only. In general, <strong class="bold">brute-force algorithms</strong> usually <a id="_idIndexMarker479"/>do not work well for large amounts <span class="No-Break">of data.</span></p>
<p>At this point, we are going to create two time series with 10,000 <span class="No-Break">elements each:</span></p>
<pre class="source-code">
$ ../ch01/synthetic_data.py 10000 -5 5 &gt; 10k1
$ ../ch01/synthetic_data.py 10000 -5 5 &gt; 10k2
$ gzip 10k1; gzip 10k2</pre>
<p>The output of <strong class="source-inline">mpdist.py</strong> when working with <strong class="source-inline">10k1.gz</strong> and <strong class="source-inline">10k2.gz</strong> and a sliding window size of <strong class="source-inline">128</strong> is <span class="No-Break">as follows:</span></p>
<pre class="source-code">
$ ./mpdist.py 10k1.gz 10k2.gz -w 128
--- 12026.64167 seconds ---
MPdist: 12.5796</pre>
<p>It took <strong class="source-inline">mpdist.py</strong> approximately 12,026 seconds to <span class="No-Break">compute MPdist.</span></p>
<p>The output of <strong class="source-inline">mpdist.py</strong> when working with <strong class="source-inline">10k1.gz</strong> and <strong class="source-inline">10k2.gz</strong> and a sliding window size of <strong class="source-inline">2048</strong> is <span class="No-Break">the following:</span></p>
<pre class="source-code">
$ ./mpdist.py 10k1.gz 10k2.gz -w 2048
--- 9154.55179 seconds ---
MPdist: 60.7277</pre>
<p>Why do you think<a id="_idIndexMarker480"/> the calculation of the <strong class="source-inline">2048</strong> sliding window ran faster than the same calculation for the sliding window size of <strong class="source-inline">128</strong>? It most likely has to do with the fact that the <strong class="source-inline">2048</strong> sliding window needs fewer iterations (1,920 times 1,920, which is equal to 3,686,400) due to the larger sliding window size, which also compensates for the cost of computing Euclidean distances for larger subsequences in the <strong class="source-inline">2048</strong> sliding <span class="No-Break">window case.</span></p>
<p>Let us now see how much time it takes the MASS algorithm to <span class="No-Break">compute MPdist.</span></p>
<p>The time it takes the <strong class="source-inline">stumpy.mpdist()</strong> function to compute the previous MPdist distances on a single CPU core is the following – we are using the <strong class="source-inline">mpdistance.py</strong> script from <a href="B14769_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a><span class="No-Break">:</span></p>
<pre class="source-code">
$ taskset --cpu-list 0 ../ch01/mpdistance.py 10k1.gz 10k2.gz 128
TS1: 10k1.gz TS2: 10k2.gz Window Size: 128
--- 10.28342 seconds ---
MPdist: 12.5790
$ taskset --cpu-list 0 ../ch01/mpdistance.py 10k1.gz 10k2.gz 2048
TS1: 10k1.gz TS2: 10k2.gz Window Size: 2048
--- 10.03479 seconds ---
MPdist: 60.7277</pre>
<p>So, it takes the <strong class="source-inline">stumpy.mpdist()</strong> function about <span class="No-Break">10 seconds.</span></p>
<p>The time it takes the <strong class="source-inline">stumpy.mpdist()</strong> function to compute the previous MPdist distances on four CPU cores is <span class="No-Break">the following:</span></p>
<pre class="source-code">
$ taskset --cpu-list 0,1,2,3 ../ch01/mpdistance.py 10k1.gz 10k2.gz 128
TS1: 10k1.gz TS2: 10k2.gz Window Size: 128
--- 9.42861 seconds ---
MPdist: 12.5790
$ taskset --cpu-list 0,1,2,3 ../ch01/mpdistance.py 10k1.gz 10k2.gz 2048
TS1: 10k1.gz TS2: 10k2.gz Window Size: 2048
--- 9.33578 seconds ---
MPdist: 60.7277</pre>
<p>Why are the times <a id="_idIndexMarker481"/>almost the same when using a single CPU core? The answer is that with small time series, <strong class="source-inline">stumpy.mpdist()</strong> <em class="italic">does not have enough time</em> to use all <span class="No-Break">CPU cores.</span></p>
<p>Lastly, the time it takes the <strong class="source-inline">stumpy.mpdist()</strong> function to compute the two MPdist distances on eight CPU cores is <span class="No-Break">the following:</span></p>
<pre class="source-code">
$ ../ch01/mpdistance.py 10k1.gz 10k2.gz 128
TS1: 10k1.gz TS2: 10k2.gz Window Size: 128
--- 9.54642 seconds ---
MPdist: 12.5790
$ ../ch01/mpdistance.py 10k1.gz 10k2.gz 2048
TS1: 10k1.gz TS2: 10k2.gz Window Size: 2048
--- 9.33648 seconds ---
MPdist: 60.7277</pre>
<p>Why are the times the same when using four CPU cores? As before, for very small time series, the number of CPU cores used does not make any difference to the computation time as there is<a id="_idIndexMarker482"/> not enough time to <span class="No-Break">use them.</span></p>
<p>We are now ready to use the existing knowledge to approximately compute MPdist with the help <span class="No-Break">of iSAX.</span></p>
<h1 id="_idParaDest-163"><a id="_idTextAnchor174"/>Calculating MPdist using iSAX</h1>
<p>In this section, we <a id="_idIndexMarker483"/>are<a id="_idIndexMarker484"/> going to discuss our views and ideas regarding using iSAX indexes to <em class="italic">approximately </em><span class="No-Break"><em class="italic">compute MPdist</em></span><span class="No-Break">.</span></p>
<p>We know that iSAX keeps together subsequences with the same SAX representation. As before, our feeling is that it is more likely to find the nearest neighbor of a subsequence from a given time series in the subsequences with the same SAX representation from another <span class="No-Break">time series.</span></p>
<p>The next section is about putting our thoughts <span class="No-Break">into practice.</span></p>
<h1 id="_idParaDest-164"><a id="_idTextAnchor175"/>Implementing the MPdist calculation in Python</h1>
<p>In this section, we<a id="_idIndexMarker485"/> will discuss two ways to approximately<a id="_idIndexMarker486"/> compute MPdist with the help <span class="No-Break">of iSAX.</span></p>
<p>The first way is much simpler than the second one and is slightly based on the approximate calculation of the Matrix Profile. We take each subsequence from the first time series, and we match it with a terminal node with the same SAX representation from the iSAX index of the second time series in order to get the approximate nearest neighbor – if a subsequence does not have a match <em class="italic">based on its SAX representation</em>, we ignore that subsequence. So, in this case, we do not join iSAX indexes, which makes the process much slower – our experiments are going to show how much slower this <span class="No-Break">technique is.</span></p>
<p>For the second<a id="_idIndexMarker487"/> way, we<a id="_idIndexMarker488"/> just use <a id="_idIndexMarker489"/>the <strong class="bold">similarity join</strong> of two iSAX indexes, which we first saw in <a href="B14769_05.xhtml#_idTextAnchor124"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><span class="No-Break">.</span></p>
<p>The next subsection shows the implementation of the <span class="No-Break">first technique.</span></p>
<h2 id="_idParaDest-165"><a id="_idTextAnchor176"/>Using the approximate Matrix Profile way</h2>
<p>Although we do<a id="_idIndexMarker490"/> not return any Matrix Profile vectors, this technique looks like computing the Matrix Profile because <em class="italic">we examine subsequences one by one</em> and not in groups, and return their Euclidean distance with the approximate nearest neighbor. In this technique, <em class="italic">there is no exclusion zone</em> in the computation because we are comparing subsequences from two different <span class="No-Break">time series.</span></p>
<p>The important code within <strong class="source-inline">apprMPdist.py</strong> is the following – we assume that we have already generated the two <span class="No-Break">iSAX indexes:</span></p>
<pre class="source-code">
    # We search iSAX2 for the NN of the
    # subsequences from TS1
    for idx in range(0, len(ts1)-windowSize+1):
        currentQuery = ts1[idx:idx+windowSize]
        t = NN(i2, currentQuery)
        if t != None:
            ED.append(t)
    # We search iSAX1 for the NN of the
    # subsequences from TS2
    for idx in range(0, len(ts2)-windowSize+1):
        currentQuery = ts2[idx:idx+windowSize]
        t = NN(i1, currentQuery)
        if t != None:
            ED.append(t)
    ED.sort()
    idx = int(0.05 * ( len(ED) + 2 * windowSize)) + 1
    print("Approximate MPdist:", round(ED[idx], 3))</pre>
<p>For each subsequence of the first time series, search the iSAX index of the second time series for the approximate nearest neighbor using the <strong class="source-inline">NN()</strong> function. Then, do the same for the subsequences of the second time series and the iSAX index of the first <span class="No-Break">time series.</span></p>
<p>What is interesting is the implementation of the <strong class="source-inline">NN()</strong> function, used in the previous code. We are going<a id="_idIndexMarker491"/> to present <strong class="source-inline">NN()</strong> in three parts. The first part is <span class="No-Break">the following:</span></p>
<pre class="source-code">
def NN(ISAX, q):
    ED = None
    segments = variables.segments
    threshold = variables.threshold
    # Create TS Node
    qTS = isax.TS(q, segments)
    segs = [1] * segments
    # If the relevant child of root is not there
    # we have a miss
    lower_cardinality = tools.lowerCardinality(segs, qTS)
    lower_cardinality_str = ""
    for i in lower_cardinality:
        lower_cardinality_str=lower_cardinality_str+"_"+i
    lower_cardinality_str = lower_cardinality_str[1:len(
        lower_cardinality_str)]
    if ISAX.ht.get(lower_cardinality_str) == None:
        return None</pre>
<p>In the previous<a id="_idIndexMarker492"/> code, we try to find an iSAX node with the same SAX representation as the subsequence we are examining – we begin with the children of the root node of the iSAX. If such a child of the root node cannot be found, then we have a miss and we ignore that particular subsequence. As the final list of Euclidean distances is large (this depends on the lengths of the time series), missing some subsequences has no real effect on the <span class="No-Break">end result.</span></p>
<p>The second part of <strong class="source-inline">NN()</strong> is <span class="No-Break">the following:</span></p>
<pre class="source-code">
    # Otherwise, we have a hit
    n = ISAX.ht.get(lower_cardinality_str)
    while n.terminalNode == False:
        left = n.left
        right = n.right
        leftSegs = left.word.split('_')
        # Promote
        tempCard = tools.promote(qTS, leftSegs)
        if tempCard == left.word:
            n = left
        elif tempCard == right.word:
            n = right</pre>
<p>In the previous code, we try to locate the iSAX node with the desired SAX representation by traversing the <span class="No-Break">iSAX index.</span></p>
<p>The last part<a id="_idIndexMarker493"/> of <strong class="source-inline">NN()</strong> is <span class="No-Break">the following:</span></p>
<pre class="source-code">
    # Iterate over the subsequences of the terminal node
    for i in range(0, threshold):
        child = n.children[i]
        if type(child) == isax.TS:
            distance = <strong class="bold">tools.euclidean</strong>(normalize(child.ts),
                normalize(qTS.ts))
            if ED == None:
                ED = distance
            if ED &gt; distance:
                ED = distance
        else:
            break
    return ED</pre>
<p>After locating the desired terminal node, we compare its subsequences with the given subsequence and return the minimum Euclidean distance found. The main program puts all these minimum Euclidean distances into <span class="No-Break">a list.</span></p>
<p>Now, let us discuss the second technique, which joins two <span class="No-Break">iSAX indexes.</span></p>
<h2 id="_idParaDest-166"><a id="_idTextAnchor177"/>Using the join of two iSAX indexes</h2>
<p>The second <a id="_idIndexMarker494"/>way is much faster <a id="_idIndexMarker495"/>than the first one. In this way, we <em class="italic">join the two iSAX indexes</em> based on the technique from <a href="B14769_05.xhtml#_idTextAnchor124"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, and we get the list of Euclidean distances. From that list, we choose a value to be the <span class="No-Break">approximate MPdist.</span></p>
<p class="callout-heading">What happens if there is not a match among iSAX nodes?</p>
<p class="callout">In some rare cases that depend on the time series data and the iSAX parameters, some nodes from one iSAX might end up not having a match in the other iSAX, and vice versa. In our case, we <em class="italic">ignore those nodes</em>, which means that we end up having a smaller-than-expected list of <span class="No-Break">Euclidean distances.</span></p>
<p>The important code within <strong class="source-inline">joinMPdist.py</strong> is the following – we assume that we have already generated the two <span class="No-Break">iSAX indexes:</span></p>
<pre class="source-code">
    # Join the two iSAX indexes
    Join(i1, i2)
    variables.ED.sort()
    print("variables.ED length:", len(variables.ED))
    # Index
    idx = int(0.05*(len(variables.ED) + 2*windowSize))+1
    print("Approximate MPdist:", variables.ED[idx])</pre>
<p>The previous code uses the <strong class="source-inline">Join()</strong> function from <strong class="source-inline">isax.iSAXjoin</strong>, which we implemented and saw in <a href="B14769_05.xhtml#_idTextAnchor124"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>. We have already seen the join of two iSAX indexes. However, this<a id="_idIndexMarker496"/> is the first time that we actually use the results of that join <span class="No-Break">for something.</span></p>
<p>We are now going to start using the existing implementations and see <span class="No-Break">their performance.</span></p>
<h1 id="_idParaDest-167"><a id="_idTextAnchor178"/>Using the Python code</h1>
<p>In this section, we are <a id="_idIndexMarker497"/>going to use the Python scripts that we <span class="No-Break">have created.</span></p>
<p>Running <strong class="source-inline">apprMPdist.py</strong> using the two time series with 10,000 elements each that we created earlier in this chapter generates the following kind <span class="No-Break">of output:</span></p>
<pre class="source-code">
$ ./apprMPdist.py 10k1.gz 10k2.gz -s 3 -c 64 -t 500 -w 120
Max Cardinality: 64 Segments: 3 Sliding Window: 120 Threshold: 500 Default Promotion: False
MPdist: 351.27 seconds
Approximate MPdist: 12.603</pre>
<p>Using a bigger sliding window size generates the <span class="No-Break">following output:</span></p>
<pre class="source-code">
$ ./apprMPdist.py 10k1.gz 10k2.gz -s 3 -c 64 -t 500 -w 300
Max Cardinality: 64 Segments: 3 Sliding Window: 300 Threshold: 500 Default Promotion: False
MPdist: 384.74 seconds
Approximate MPdist: 21.757</pre>
<p>So, bigger sliding window sizes require more time. As before, this is because calculating Euclidean distances for bigger sliding window sizes <span class="No-Break">is slower.</span></p>
<p>Executing <strong class="source-inline">joinMPdist.py</strong> produces the <span class="No-Break">following output:</span></p>
<pre class="source-code">
$ ./joinMPdist.py 10k1.gz 10k2.gz -s 3 -c 64 -t 500 -w 120
Max Cardinality: 64 Segments: 3 Sliding Window: 120 Threshold: 500 Default Promotion: False
MPdist: 37.70 seconds
variables.ED length: 17605
Approximate MPdist: 12.60282</pre>
<p>As before, using a bigger sliding window produces the <span class="No-Break">following output:</span></p>
<pre class="source-code">
$ ./joinMPdist.py 10k1.gz 10k2.gz -s 3 -c 64 -t 500 -w 300
Max Cardinality: 64 Segments: 3 Sliding Window: 300 Threshold: 500 Default Promotion: False
MPdist: 31.24 seconds
variables.ED length: 13972
Approximate MPdist: 21.76263</pre>
<p>It looks like <strong class="source-inline">joinMPdist.py</strong> is a lot faster than <strong class="source-inline">apprMPdist.py</strong>, which makes perfect sense as it is <em class="italic">using two iSAX indexes at the same time</em> to construct the list of Euclidean<a id="_idIndexMarker498"/> distances. Put simply, the running of <strong class="source-inline">joinMPdist.py</strong> requires <span class="No-Break">fewer computations.</span></p>
<p>The next subsection compares the accuracy and the speed of the two methods when working with larger <span class="No-Break">time series.</span></p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor179"/>Comparing the accuracy and the speed of the methods</h2>
<p>Both methods are <a id="_idIndexMarker499"/>far from perfect. However, in this subsection, we are going to compare their accuracy and speed in relation to the MPdist implementation found in the <strong class="source-inline">stumpy</strong> <span class="No-Break">Python package.</span></p>
<p>We want to test our code on bigger time series, as this is where our technique might be faster than the exact MPdist function of <strong class="source-inline">stumpy</strong>. In this case, we are going to use two time series with around 500,000 elements each – we already created such time series in <a href="B14769_05.xhtml#_idTextAnchor124"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><span class="No-Break">.</span></p>
<p>For <strong class="source-inline">apprMPdist.py</strong>, the results for sliding window sizes of <strong class="source-inline">120</strong>, <strong class="source-inline">600</strong>, and <strong class="source-inline">1200</strong> are <span class="No-Break">as follows:</span></p>
<pre class="source-code">
$ ./apprMPdist.py ../ch05/500k.gz ../ch05/506k.gz -s 6 -c 64 -t 500 -w 120
Max Cardinality: 32 Segments: 6 Sliding Window: 120 Threshold: 500 Default Promotion: False
MPdist: 19329.64 seconds
Approximate MPdist: 12.405
$ ./apprMPdist.py ../ch05/500k.gz ../ch05/506k.gz -s 6 -c 64 -t 500 -w 600
Max Cardinality: 64 Segments: 6 Sliding Window: 600 Threshold: 500 Default Promotion: False
MPdist: 21219.60 seconds
Approximate MPdist: 31.871
$ ./apprMPdist.py ../ch05/500k.gz ../ch05/506k.gz -s 6 -c 64 -t 500 -w 1200
Max Cardinality: 64 Segments: 6 Sliding Window: 1200 Threshold: 500 Default Promotion: False
MPdist: 23120.07 seconds
Approximate MPdist: 46.279</pre>
<p>For the <strong class="source-inline">joinMPdist.py</strong> script, the output for sliding window sizes of <strong class="source-inline">120</strong>, <strong class="source-inline">600</strong>, and <strong class="source-inline">1200</strong> is <span class="No-Break">the </span><span class="No-Break"><a id="_idIndexMarker500"/></span><span class="No-Break">following:</span></p>
<pre class="source-code">
$ ./joinMPdist.py ../ch05/500k.gz ../ch05/506k.gz -s 6 -c 64 -t 500 -w 120
Max Cardinality: 64 Segments: 6 Sliding Window: 120 Threshold: 500 Default Promotion: False
MPdist: 2595.92 seconds
variables.ED length: 910854
Approximate MPdist: 12.40684
$ ./joinMPdist.py ../ch05/500k.gz ../ch05/506k.gz -s 6 -c 64 -t 500 -w 600;
Max Cardinality: 64 Segments: 6 Sliding Window: 600 Threshold: 500 Default Promotion: False
MPdist: 2270.72 seconds
variables.ED length: 798022
Approximate MPdist: 31.88064
$ ./joinMPdist.py ../ch05/500k.gz ../ch05/506k.gz -s 6 -c 64 -t 500 -w 1200
Max Cardinality: 64 Segments: 6 Sliding Window: 1200 Threshold: 500 Default Promotion: False
MPdist: 2145.76 seconds
variables.ED length: 674777
Approximate MPdist: 46.29538</pre>
<p>The results of <strong class="source-inline">joinMPdist.py</strong> are really promising when working with larger time series. Although it looks like the bigger the sliding window size, the faster the technique, this is not completely true because as the sliding window gets bigger, we have more nodes without a match, and therefore, the list of values gets smaller, which means that we compute fewer Euclidean distances as the sliding window increases. This is not always the case, as this depends on the time <span class="No-Break">series data.</span></p>
<p>Lastly, the result<a id="_idIndexMarker501"/> from the <strong class="source-inline">stumpy</strong> Python package when running on a single CPU core is <span class="No-Break">as follows:</span></p>
<pre class="source-code">
$ taskset --cpu-list 0 ../ch01/mpdistance.py ../ch05/500k.gz ../ch05/506k.gz 120
TS1: ../ch05/500k.gz TS2: ../ch05/506k.gz Window Size: 120
500000 506218
--- 4052.73237 seconds ---
MPdist: 11.4175
$ taskset --cpu-list 0 ../ch01/mpdistance.py ../ch05/500k.gz ../ch05/506k.gz 600
TS1: ../ch05/500k.gz TS2: ../ch05/506k.gz Window Size: 600
500000 506218
--- 4042.52154 seconds ---
MPdist: 30.7796
$ taskset --cpu-list 0 ../ch01/mpdistance.py ../ch05/500k.gz ../ch05/506k.gz 1200
TS1: ../ch05/500k.gz TS2: ../ch05/506k.gz Window Size: 1200
500000 506218
--- 4045.72392 seconds ---
MPdist: 45.1887</pre>
<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.3</em> shows the accuracy of the approximate methods, which are named <strong class="bold">Search</strong> and <strong class="bold">Join</strong>, compared to the real MPdist value, which is named <strong class="bold">Real</strong>, for the three sliding window <span class="No-Break">sizes used.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer050">
<img alt="Figure 7.3 – Comparing the accuracy of the approximate methods to the real MPdist" height="544" src="image/Figure_7.3_B14769.jpg" width="913"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Comparing the accuracy of the approximate methods to the real MPdist</p>
<p>What does the output of <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.3</em> tell us? First of all, the approximate methods performed pretty well<a id="_idIndexMarker502"/> because the approximate values are really close to the real MPdist values. So, at least for our example time series, the approximate techniques are <span class="No-Break">very accurate.</span></p>
<p>Similarly, <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.4</em> compares the times of the approximate methods to the time of the <strong class="source-inline">stumpy</strong> computation when running on a single CPU core for the three sliding window sizes used – the presented times for the approximate methods <em class="italic">do not include the time it takes to create the two </em><span class="No-Break"><em class="italic">iSAX indexes</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer051">
<img alt="Figure 7.4 – Comparing the times of the approximate methods to the real MPdist" height="548" src="image/Figure_7.4_B14769.jpg" width="915"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Comparing the times of the approximate methods to the real MPdist</p>
<p>What does the output of <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.4</em> tell us? The first technique is really slow and should not be used – that is the purpose of experimentation: to find out what works well and what does not. On the other hand, the performance of the second approximate technique is<a id="_idIndexMarker503"/> very good. Additionally, <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.4</em> shows that the <strong class="source-inline">stumpy</strong> computation takes the same time regardless of the sliding window size – this is a good and desirable feature of the <span class="No-Break">MASS algorithm.</span></p>
<h1 id="_idParaDest-169"><a id="_idTextAnchor180"/>Summary</h1>
<p>Although the main purpose of iSAX is to help us search for subsequences by indexing them, there are other ways to use an <span class="No-Break">iSAX index.</span></p>
<p>In this chapter, we presented a way to approximately compute the Matrix Profile vectors and two ways to approximately compute the MPdist distance between two time series. All these techniques use <span class="No-Break">iSAX indexes.</span></p>
<p>We presented two ways to approximately compute MPdist. Out of the two methods, the one that joins two iSAX indexes is much more efficient than the other – so the use of an iSAX index by itself does not guarantee efficiency; we have to use an iSAX index the right way to get <span class="No-Break">better results.</span></p>
<p>There is a small chapter left to finish this book, which is about the next steps you can follow if you are really into time series <span class="No-Break">and databases.</span></p>
<h1 id="_idParaDest-170"><a id="_idTextAnchor181"/>Useful links</h1>
<ul>
<li>The <strong class="source-inline">stumpy</strong> Python <span class="No-Break">package: </span><a href="https://pypi.org/project/stumpy/"><span class="No-Break">https://pypi.org/project/stumpy/</span></a></li>
<li>The <strong class="source-inline">numba</strong> Python <span class="No-Break">package: </span><a href="https://pypi.org/project/numba/"><span class="No-Break">https://pypi.org/project/numba/</span></a></li>
<li>The <span class="No-Break">RMSE: </span><a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation"><span class="No-Break">https://en.wikipedia.org/wiki/Root-mean-square_deviation</span></a></li>
<li>The UCR Matrix Profile <span class="No-Break">page: </span><a href="https://www.cs.ucr.edu/~eamonn/MatrixProfile.xhtml"><span class="No-Break">https://www.cs.ucr.edu/~eamonn/MatrixProfile.xhtml</span></a></li>
<li>SAX home <span class="No-Break">page: </span><a href="https://www.cs.ucr.edu/~eamonn/SAX.htm"><span class="No-Break">https://www.cs.ucr.edu/~eamonn/SAX.htm</span></a></li>
</ul>
<h1 id="_idParaDest-171"><a id="_idTextAnchor182"/>Exercises</h1>
<p>Try to do the <span class="No-Break">following exercises:</span></p>
<ul>
<li>Try to use <strong class="source-inline">mp.py</strong> with a time series with 50,000 elements and see how much time it takes to complete for sliding window sizes of <strong class="source-inline">16</strong>, <strong class="source-inline">2048</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">4096</strong></span><span class="No-Break">.</span></li>
<li>Try to use <strong class="source-inline">mp.py</strong> with a time series with 65,000 elements and see how much time it takes <span class="No-Break">to complete.</span></li>
<li>Experiment with the exclusion zone limits of <strong class="source-inline">mp.py</strong> and see what <span class="No-Break">you get.</span></li>
<li>Use <strong class="source-inline">realMP.py</strong> and <strong class="source-inline">stumpy.stump()</strong> to compute the Matrix Profile vectors for a time series with 200,000 elements – create that time series if you do not <span class="No-Break">have one.</span></li>
<li>Use <strong class="source-inline">realMP.py</strong> and <strong class="source-inline">stumpy.stump()</strong> to compute the Matrix Profile vectors for a time series with 500,000 elements. Now, consider that a time series with 500,000 elements is on the <span class="No-Break">small side!</span></li>
<li>Try <strong class="source-inline">realMP.py</strong> on the <strong class="source-inline">2M.gz</strong> time series from <a href="B14769_04.xhtml#_idTextAnchor102"><span class="No-Break"><em class="italic">Chapter 4</em></span></a> using a single CPU code. As you can see, <strong class="source-inline">realMP.py</strong> starts getting really slow with larger time series. Now, consider that a time series with 2,000,000 elements is <span class="No-Break">not big.</span></li>
<li>We can make <strong class="source-inline">mp.py</strong> a little faster by storing the normalized versions of the subsequences and using the normalized versions when calculating the Euclidean distances, instead of computing the normalized versions inside the <strong class="source-inline">euclidean()</strong> function every time we call <strong class="source-inline">euclidean()</strong>. Try to implement <span class="No-Break">that functionality.</span></li>
<li>Similarly, we can make <strong class="source-inline">mpdist.py</strong> faster by storing the normalized versions of the subsequences and using them for the Euclidean <span class="No-Break">distance computations.</span></li>
<li>Create an image similar to <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.4</em> but for larger time series. Begin with time series with 1,000,000 elements and see what <span class="No-Break">you get.</span></li>
</ul>
</div>
</div></body></html>