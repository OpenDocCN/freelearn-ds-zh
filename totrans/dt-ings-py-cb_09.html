<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-312"><a id="_idTextAnchor319"/>9</h1>
<h1 id="_idParaDest-313"><a id="_idTextAnchor320"/>Putting Everything Together with Airﬂow</h1>
<p>So far, we have covered the different aspects and steps of data ingestion. We have seen how to configure and ingest structured and unstructured data, what analytical data is, and how to improve logs for more insightful monitoring and error handling. Now is the time to group all this information to create something similar to a real-world project.</p>
<p>From now on, in the following chapters, we will use Apache Airflow, an open source platform that allows us to create, schedule, and monitor workflows. Let’s start our journey by configuring and understanding the fundamental concepts of Apache Airflow and how powerful this tool is.</p>
<p>In this chapter, you will learn about the following topics:</p>
<ul>
<li>Configuring Airflow</li>
<li>Creating DAGs</li>
<li>Creating custom operators</li>
<li>Conﬁguring sensors</li>
<li>Creating connectors in Airﬂow</li>
<li>Creating parallel ingest tasks</li>
<li>Deﬁning ingest-dependent DAGs</li>
</ul>
<p>By the end of this chapter, you will have learned about the most important components of Airﬂow and how to conﬁgure them, including how to solve related issues in this process.</p>
<h1 id="_idParaDest-314"><a id="_idTextAnchor321"/>Technical requirements</h1>
<p>You can find the code for this chapter in the GitHub repository here: <a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook</a></p>
<h2 id="_idParaDest-315"><a id="_idTextAnchor322"/>Installing Airflow</h2>
<p>This chapter<a id="_idIndexMarker600"/> requires that Airflow is installed on your local machine. You can install it directly on your <strong class="bold">Operating System</strong> (<strong class="bold">OS</strong>) or using a Docker image. For more information regarding this, refer to the <em class="italic">Configuring Docker for Airflow</em> recipe in <a href="B19453_01.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a>.</p>
<h1 id="_idParaDest-316"><a id="_idTextAnchor323"/>Configuring Airflow</h1>
<p><strong class="bold">Apache Airflow</strong> has many<a id="_idIndexMarker601"/> capabilities and a quick setup, which helps us start designing our workflows as code. Some additional configurations might be required as we progress with the workflows and into data processing. Gladly, Airflow has a dedicated file for inserting other arrangements without changing anything within its core.</p>
<p>In this recipe, we will learn more about the <code>airflow.conf</code> file, how to use it, and other valuable configurations required to execute the other recipes in this chapter. We will also cover where to find this file and how other environment variables work with this tool. Understanding these concepts in practice helps us to identify potential improvements or solve problems.</p>
<h2 id="_idParaDest-317"><a id="_idTextAnchor324"/>Getting ready</h2>
<p>Before moving on to the code, ensure your Airflow runs correctly. You can do that by checking the Airflow UI at this link: <code>http://localhost:8080</code>.</p>
<p>If you are using a Docker container (as I am) to host your Airflow application, you can check its status on the terminal with the following command:</p>
<pre class="source-code">
$ docker ps</pre>
<p>This is the output:</p>
<div><div><img alt="Figure 9.1 – Airflow Docker containers running" height="131" src="img/Figure_9.01_B19453.jpg" width="1407"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Airflow Docker containers running</p>
<p>Or, you can check the container status on Docker Desktop, as in the following screenshot:</p>
<div><div><img alt="Figure 9.2 – Docker Desktop view of Airflow containers running" height="455" src="img/Figure_9.02_B19453.jpg" width="1051"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Docker Desktop view of Airflow containers running</p>
<h2 id="_idParaDest-318"><a id="_idTextAnchor325"/>How to do it…</h2>
<p>Here are the steps to<a id="_idIndexMarker602"/> perform this recipe:</p>
<ol>
<li>Let’s start by installing the MongoDB additional provider for Airflow. If you are using the <code>docker-compose.yaml</code> file, open it and add <code>apache-airflow-providers-mongo</code> inside <code>_PIP_ADDITIONAL_REQUIREMENTS</code>. Your code will look like this:</li>
</ol>
<div><div><img alt="Figure 9.3 – The docker-compose.yaml file in the environment variables section" height="483" src="img/Figure_9.03_B19453.jpg" width="945"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – The docker-compose.yaml file in the environment variables section</p>
<p>If you are hosting <a id="_idIndexMarker603"/>Airflow directly on your machine, you can do the same<a id="_idIndexMarker604"/> installation using <strong class="bold">PyPi</strong>: <a href="https://pypi.org/project/apache-airflow-providers-mongo/">https://pypi.org/project/apache-airflow-providers-mongo/</a>.</p>
<ol>
<li value="2">Next, we will create a folder called <code>files_to_test</code>, and inside it, create two more folders: <code>output_files</code> and <code>sensors_files</code>. You don’t need to worry about its usage yet since it will be used later in this chapter. Your Airflow folder structure should look like this:</li>
</ol>
<div><div><img alt="Figure 9.4 – Airflow local directory folder structure" height="149" src="img/Figure_9.04_B19453.jpg" width="218"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – Airflow local directory folder structure</p>
<ol>
<li value="3">Now, let’s mount the volumes of our Docker image. You can skip this part if you are not using Docker to host <a id="_idIndexMarker605"/>Airflow.</li>
</ol>
<p>In your <code>docker-compose.yaml</code> file, under the <code>volume</code> parameter, add the following:</p>
<pre class="source-code">
<strong class="bold">- ./config/airflow.cfg:/usr/local/airflow/airflow.cfg</strong>
<strong class="bold">- ./files_to_test:/opt/airflow/files_to_test</strong></pre>
<p>Your final <code>volumes</code> section will look like this:</p>
<div><div><img alt="Figure 9.5 – docker-compose.yaml volumes" height="138" src="img/Figure_9.05_B19453.jpg" width="539"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – docker-compose.yaml volumes</p>
<p>Stop and restart your container so these changes can be propagated.</p>
<ol>
<li value="4">Finally, we will fix a bug in the <code>docker-compose.yaml</code> file. This official fix for this bug is within the Airflow official documentation and therefore wasn’t included in the Docker image. You can see the complete issue and the solution here: <a href="https://github.com/apache/airflow/discussions/24809">https://github.com/apache/airflow/discussions/24809</a>.</li>
</ol>
<p>To fix the bug, go to the <code>airflow-init</code> section of the <code>docker-compose</code> file and insert <code>_PIP_ADDITIONAL_REQUIREMENTS: ''</code> inside the <code>environment</code> parameter. Your code will look like this:</p>
<div><div><img alt="Figure 9.6 – The docker-compose.yaml environment variables section with PIP_ADDITIONAL_REQUIREMENTS" height="188" src="img/Figure_9.06_B19453.jpg" width="677"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – The docker-compose.yaml environment variables section with PIP_ADDITIONAL_REQUIREMENTS</p>
<p>This action will fix the following issue registered on GitHub: <a href="https://github.com/apache/airflow/pull/23517">https://github.com/apache/airflow/pull/23517</a>.</p>
<h2 id="_idParaDest-319"><a id="_idTextAnchor326"/>How it works…</h2>
<p>The configuration presented here<a id="_idIndexMarker606"/> is simple. However, it guarantees the application will keep working through the chapter recipes.</p>
<p>Let’s start with the package we installed in <em class="italic">step 1</em>. Like other frameworks or platforms, Airflow has its <em class="italic">batteries</em> included, which means it already comes with various packages. But, as its popularity started to increase, it started to require other types of connections or operators, which the open source community took care of.</p>
<p>You can find a list of released packages that can be installed on Airflow here: <a href="https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.xhtml">https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.xhtml</a>.</p>
<p>Before jumping into other code explanations, let’s understand the <code>volume</code> section inside the <code>docker-compose.yaml</code> file. This configuration allows Airflow to see which folders reflect the same respective ones inside the Docker container without the necessity to upload code using a Docker command every time. In other words, we can synchronously add<a id="_idIndexMarker607"/> our <strong class="bold">Directed Acyclic Graph</strong> (<strong class="bold">DAG</strong>) files and new operators and see some logs, among other things, and this will be reflected inside the container.</p>
<p>Next, we declared the Docker mount volume configurations for two parts: the new folder we created (<code>files_to_test</code>) and the <code>airflow.cfg</code> file. The first one will allow Airflow to replicate the <code>files_to_test</code> local folder inside the container, so we can use it to use files in a more simplified way. Otherwise, if we try to use it without the mounting volume, the following error will appear when trying to retrieve any file:</p>
<div><div><img alt="Figure 9.7 – Error in Airflow when the folder is not referred to in the container volume" height="180" src="img/Figure_9.07_B19453.jpg" width="917"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – Error in Airflow when the folder is not referred to in the container volume</p>
<p>Although we will not use the <code>airflow.cfg</code> file, for now, it is a good practice to know how to access this file and what it is used for. This file contains the Airflow configurations and can be edited to include more. Usually, sensitive data is stored inside it to prevent other people from <a id="_idIndexMarker608"/>having improper access since, by default, the content of the <code>airflow.cfg</code> file cannot be accessed in the UI.</p>
<p class="callout-heading">Note</p>
<p class="callout">Be very cautious when changing or handling the <code>airflow.cfg</code> file. This file contains all the required configurations and other relevant settings to make Airflow work. We will explore more about this in <a href="B19453_10.xhtml#_idTextAnchor364"><em class="italic">Chapter 10</em></a><em class="italic">.</em></p>
<h2 id="_idParaDest-320"><a id="_idTextAnchor327"/>See also</h2>
<p>For more information about the Docker image, see the documentation page here: <a href="https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.xhtml">https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.xhtml</a>.</p>
<h1 id="_idParaDest-321"><a id="_idTextAnchor328"/>Creating DAGs</h1>
<p>The core concept of Airflow is based on DAGs, which collect, group, and organize tasks to be executed in a specific<a id="_idIndexMarker609"/> order. A DAG is also responsible for managing the dependencies between its tasks. Simply put, it is not concerned about what a task is doing but just <em class="italic">how</em> to execute it. Typically, a DAG starts at a scheduled time, but we can also define dependencies between other DAGs so that they will start based on their execution statuses.</p>
<p>We will create our first DAG in this recipe and set it to run based on a specific schedule. With this first step, we enter into practically designing our first workflow.</p>
<h2 id="_idParaDest-322"><a id="_idTextAnchor329"/>Getting ready</h2>
<p>Please refer to the <em class="italic">Getting ready</em> section in the <em class="italic">Configuring Airflow</em> recipe for this recipe since we will handle it with the same technology.</p>
<p>Also, let’s create a directory called <code>ids_ingest</code> inside our <code>dags</code> folder. Inside the <code>ids_ingest</code> folder, we will create two files: <code>__init__.py</code> and <code>ids_ingest_dag.py</code>. The final structure will look as follows:</p>
<div><div><img alt="Figure 9.8 – Airflow’s local directory structure with the ids_ingest DAG" height="251" src="img/Figure_9.08_B19453.jpg" width="281"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.8 – Airflow’s local directory structure with the ids_ingest DAG</p>
<h2 id="_idParaDest-323"><a id="_idTextAnchor330"/>How to do it…</h2>
<p>In this exercise, we will write a<a id="_idIndexMarker610"/> DAG that retrieves the IDs of the <code>github_events.json</code> file. Open <code>ids_ingest_dag.py</code>, and let’s add the content to write our first DAG:</p>
<ol>
<li>Let’s start by importing the libraries we will use in this script. I like to separate the imports from the Airflow library and Python’s library as a good practice:<pre class="source-code">
from airflow import DAG
from airflow.settings import AIRFLOW_HOME
from airflow.operators.bash import BashOperator
from airflow.operators.python_operator import PythonOperator
import json
from datetime import datetime, timedelta</pre></li>
<li>Then, we will define <code>default_args</code> for our DAG, as you can see here:<pre class="source-code">
# Define default arguments
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 3, 22),
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}</pre></li>
<li>Now, we will create a <a id="_idIndexMarker611"/>Python function that receives the JSON file and returns the IDs inside it. Since it is a small function, we can create it inside the DAG’s file:<pre class="source-code">
def get_ids_from_json(filename_json):
    with open (filename_json, 'r') as f:
        git = json.loads(f.read())
    print([item['id'] for item in git])</pre></li>
<li>Next, we will instantiate our DAG object, and inside it, we will define two operators: a <code>BashOperator</code> instance to show a console message and <code>PythonOperator</code> to execute the function we just created, as you can see here:<pre class="source-code">
# Instantiate a DAG object
with DAG(
    dag_id='simple_ids_ingest',
    default_args=default_args,
    schedule_interval=timedelta(days=1),
) as dag:
    first_task = BashOperator(
            task_id="first_task",
            bash_command="echo $AIRFLOW_HOME",
        )
    filename_json = f"{AIRFLOW_HOME}/files_to_test/github_events.json"
    get_id_from_json = PythonOperator(
        task_id="get_id_from_json",
        python_callable=get_ids_from_json,
        op_args=[filename_json]
    )</pre></li>
</ol>
<p>Make sure you<a id="_idIndexMarker612"/> save the file before jumping to the next step.</p>
<ol>
<li value="5">Now, head over to the Airflow UI. Although <a id="_idIndexMarker613"/>plenty of DAG examples are provided by the Airflow team, you should look for a DAG called <code>simple_ids_ingest</code>. You will notice the DAG is not enabled. Click on the toggle button to enable it, and you should have something like the following:</li>
</ol>
<div><div><img alt="Figure 9.9 – The Airflow DAG enabled on the UI" height="68" src="img/Figure_9.09_B19453.jpg" width="905"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.9 – The Airflow DAG enabled on the UI</p>
<ol>
<li value="6">As soon as you enable it, the DAG will start running. Click on the DAG name to be redirected to the DAG’s page, as you can see in the following screenshot:</li>
</ol>
<div><div><img alt="Figure 9.10 – DAG Grid page view" height="933" src="img/Figure_9.10_B19453.jpg" width="1304"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.10 – DAG Grid page view</p>
<p>If everything is well <a id="_idIndexMarker614"/>configured, your page should look like this:</p>
<div><div><img alt="Figure 9.11 – DAG running successfully in Graph page view" height="405" src="img/Figure_9.11_B19453.jpg" width="1446"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.11 – DAG running successfully in Graph page view</p>
<ol>
<li value="7">Then, click on the <code>get_id_from_json</code> task. A small window will show up as follows:</li>
</ol>
<div><div><img alt="Figure 9.12 – Task options" height="650" src="img/Figure_9.12_B19453.jpg" width="649"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.12 – Task options</p>
<ol>
<li value="8">Then, click the <strong class="bold">Log</strong> button. You will be <a id="_idIndexMarker615"/>redirected to a new page with the logs for this task, as seen here:</li>
</ol>
<div><div><img alt="Figure 9.13 – Task logs in the Airflow UI" height="617" src="img/Figure_9.13_B19453.jpg" width="1465"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.13 – Task logs in the Airflow UI</p>
<p>As we can see in the preceding<a id="_idIndexMarker616"/> screenshot, our task successfully finished and returned the IDs as we expected. You can see the results in the <code>INFO</code> log under the <code>AIRFLOW_CTX_DAG_RUN</code> message.</p>
<h2 id="_idParaDest-324"><a id="_idTextAnchor331"/>How it works…</h2>
<p>We created our first DAG with a few lines to retrieve and show a list of IDs from a JSON file. Now, let’s understand how it works.</p>
<p>To start with, we created our files under the <code>dags</code> directory. It happens because, by default, Airflow will understand everything inside of it as a DAG file. The folder we created inside of it was just for organization purposes, and Airflow will ignore it. Along with the <code>ids_ingest_dag.py</code> file, we also made an <code>__init__.py</code> file. This file internally tells Airflow to look inside this folder. As a result, you will see the following structure:</p>
<div><div><img alt="Figure 9.14 – Airflow local directory structure with the ids_ingest DAG" height="249" src="img/Figure_9.14_B19453.jpg" width="287"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.14 – Airflow local directory structure with the ids_ingest DAG</p>
<p class="callout-heading">Note</p>
<p class="callout">As you might be wondering, it is possible to change this configuration, but I don’t recommend this at all since other internal packages might depend on it. Do it only in the case of extreme necessity.</p>
<p>Now, let’s take a look at <a id="_idIndexMarker617"/>our instantiated DAG:</p>
<pre class="source-code">
with DAG(
    dag_id='simple_ids_ingest',
    default_args=default_args,
    schedule_interval=timedelta(days=1),
) as dag:</pre>
<p>As you can observe, creating a DAG is simple, and its parameters are spontaneous. <code>dag_id</code> is crucial and must be unique; otherwise, it can create confusion and merge with other DAGs. The <code>default_args</code> we declared in <em class="italic">step 2</em> will guide the DAG, telling when it needs to be executed, its user owner, the number of retries in case of a failure, and other valuable parameters. After the <code>as dag</code> declaration, we inserted the bash and Python operators, and they must be indented to be understood as the DAG’s tasks.</p>
<p>Finally, to set our workflow, we declared the following line:</p>
<pre class="source-code">
first_task &gt;&gt; get_id_from_json</pre>
<p>As we might guess, it sets the order of which task should be executed first.</p>
<h2 id="_idParaDest-325"><a id="_idTextAnchor332"/>There's more…</h2>
<p>We saw how easy it is to<a id="_idIndexMarker618"/> create a task to execute a Python function and a bash command. By default, Airflow comes with some handy operators to be used daily within a data ingestion pipeline. For more information, you can refer to the official documentation page here: <a href="https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.xhtml">https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.xhtml</a>.</p>
<h3>Tasks, operators, XCom, and others</h3>
<p>Airflow DAGs are a powerful way to<a id="_idIndexMarker619"/> group and execute operations. Besides the task and operators we saw here, DAGs support other types of workloads and <a id="_idIndexMarker620"/>communication across<a id="_idIndexMarker621"/> other tasks or DAGs. Unfortunately, since that is not the main subject of this book, we will not cover those concepts in detail, but I highly recommend reading the official documentation here: <a href="https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/index.xhtml">https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/index.xhtml</a>.</p>
<h3>Error handling</h3>
<p>If you encounter any errors while<a id="_idIndexMarker622"/> building this DAG, you can use the instructions from <em class="italic">step 7</em> and <em class="italic">step 8</em> to debug it. You can see a preview here of how the tasks look when an error occurs:</p>
<div><div><img alt="Figure 9.15 – DAG Graph page view showing a running error" height="495" src="img/Figure_9.15_B19453.jpg" width="1480"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.15 – DAG Graph page view showing a running error</p>
<h2 id="_idParaDest-326"><a id="_idTextAnchor333"/>See also</h2>
<p>You can find the code for the Airflow example DAGs on their official GitHub page here: <a href="https://github.com/apache/airflow/tree/main/airflow/example_dags">https://github.com/apache/airflow/tree/main/airflow/example_dags</a>.</p>
<h1 id="_idParaDest-327"><a id="_idTextAnchor334"/>Creating custom operators</h1>
<p>As seen in the previous recipe, <em class="italic">Creating DAGs</em>, it is nearly impossible to create a DAG without instantiating a task or, in other <a id="_idIndexMarker623"/>words, defining an operator. Operators are responsible for holding the logic required to process data in the pipeline.</p>
<p>We also know that Airflow already has predefined operators, allowing dozens of ways to ingest and process data. Now, it is time to put into practice how to create custom operators. Custom operators allow us to apply specific logic to a related project or data pipeline.</p>
<p>You will learn how to create a simple customized operator in this recipe. Although it is very basic, you will be able to apply the foundations of this technique to different scenarios.</p>
<p>In this recipe, we will create a custom operator to connect to and retrieve data from the HolidayAPI, the same as we saw previously, in <a href="B19453_02.xhtml#_idTextAnchor064"><em class="italic">Chapter 2</em></a>.</p>
<h2 id="_idParaDest-328"><a id="_idTextAnchor335"/>Getting ready</h2>
<p>Please, refer to the <em class="italic">Getting ready</em> section in the <em class="italic">Configuring Airflow</em> recipe for this recipe since we will handle it with the same technology.</p>
<p>We also need to add an environment variable to store our API secret. To do so, select the <strong class="bold">Variable</strong> item under the <strong class="bold">Admin</strong> menu in the Airflow UI, and you will be redirected to the desired page. Now, click the <strong class="bold">+</strong> button to add a new variable, as follows:</p>
<div><div><img alt="Figure 9.16 – The Variable page in the Airflow UI" height="346" src="img/Figure_9.16_B19453.jpg" width="898"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.16 – The Variable page in the Airflow UI</p>
<p>On the <code>SECRET_HOLIDAY_API</code> under the <strong class="bold">Key</strong> field and your API secret under the <strong class="bold">Value</strong> field. Use the <a id="_idIndexMarker624"/>same values you used to execute the <em class="italic">Retrieving data using API authentication</em> recipe in <a href="B19453_02.xhtml#_idTextAnchor064"><em class="italic">Chapter 2</em></a>. Save it and you will be redirected to the <strong class="bold">Variables</strong> page, as shown in the following screenshot:</p>
<div><div><img alt="Figure 9.17 – The Airflow UI with a new variable to store the HolidayAPI secret" height="324" src="img/Figure_9.17_B19453.jpg" width="778"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.17 – The Airflow UI with a new variable to store the HolidayAPI secret</p>
<p>Now, we are ready to create our custom operator.</p>
<h2 id="_idParaDest-329"><a id="_idTextAnchor336"/>How to do it…</h2>
<p>The code we will use to create the custom operator is the same one we saw in <a href="B19453_02.xhtml#_idTextAnchor064"><em class="italic">Chapter 2</em></a>, in the <em class="italic">Retrieving data using API authentication</em> recipe, with some alterations to fit Airflow’s requirements. Here are the steps for it:</p>
<ol>
<li>Let’s start by creating the structure inside the <code>plugins</code> folder. Since we want to make a custom operator, we need to create a folder called <code>operators</code>, where we will put a Python file called <code>holiday_api_plugin.py</code>. Your file tree should look like this:</li>
</ol>
<div><div><img alt="Figure 9.18 – Airflow’s local directory structure for the plugins folder" height="122" src="img/Figure_9.18_B19453.jpg" width="285"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.18 – Airflow’s local directory structure for the plugins folder</p>
<ol>
<li value="2">We will create some code inside <code>holiday_api_plugin.py</code>, starting with the library imports and<a id="_idIndexMarker625"/> declaring a global variable for where our file output needs to be placed:<pre class="source-code">
from airflow.settings import AIRFLOW_HOME
from airflow.models.baseoperator import BaseOperator
import requests
import json
file_output_path = f"{AIRFLOW_HOME}/files_to_test/output_files/"</pre></li>
<li>Then, we need to create a Python class, declare its constructors, and finally insert the exact code from <a href="B19453_02.xhtml#_idTextAnchor064"><em class="italic">Chapter 2</em></a> inside a function called <code>execute</code>:<pre class="source-code">
class HolidayAPIIngestOperator(BaseOperator):
    def __init__(self, filename, secret_key, country, year, **kwargs):
        super().__init__(**kwargs)
        self.filename = filename
        self.secret_key = secret_key
        self.country = country
        self.year = year
    def execute(self, context):
        params = { 'key': self.secret_key,
                'country': self.country,
                'year': self.year
        }
        url = "https://holidayapi.com/v1/holidays?"
        output_file = file_output_path + self.filename
        try:
            req = requests.get(url, params=params)
            print(req.json())
            with open(output_file, "w") as f:
                json.dump(req.json(), f)
            return "Holidays downloaded successfully"
        except Exception as e:
            raise e</pre></li>
</ol>
<p>Save the file and our <a id="_idIndexMarker626"/>operator is ready. Now, we need to create the DAG to execute it.</p>
<ol>
<li value="4">Using the same logic as in the <em class="italic">Creating DAGs</em> recipe, we will create a file called <code>holiday_ingest_dag.py</code>. Your new DAG directory tree should look like this:</li>
</ol>
<div><div><img alt="Figure 9.19 – Airflow’s directory structure for the holiday_ingest DAG folder" height="93" src="img/Figure_9.19_B19453.jpg" width="275"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.19 – Airflow’s directory structure for the holiday_ingest DAG folder</p>
<ol>
<li value="5">Now, let’s insert our <a id="_idIndexMarker627"/>DAG code inside the <code>holiday_ingest_dag.py</code> file and save it:<pre class="source-code">
from airflow import DAG
# Other imports
from operators.holiday_api_plugin import HolidayAPIIngestOperator
# Define default arguments
# Instantiate a DAG object
with DAG(
    dag_id='holiday_ingest',
    default_args=default_args,
    schedule_interval=timedelta(days=1),
) as dag:
    filename_json = f"holiday_brazil.json"
    task = HolidayAPIIngestOperator(
        task_id="holiday_api_ingestion",
        filename=filename_json,
        secret_key=Variable.get("SECRET_HOLIDAY_API"),
        country="BR",
        year=2022
    )
task</pre></li>
</ol>
<p>For the full code, refer to the GitHub repository here: <a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_custom_operators">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_custom_operators</a>.</p>
<ol>
<li value="6">Then, go to the<a id="_idIndexMarker628"/> Airflow UI, look for the <code>holiday_ingest</code> DAG, and enable it. It will look like the following figure:</li>
</ol>
<div><div><img alt="Figure 9.20 – The holiday_ingest DAG enabled in the Airflow UI" height="56" src="img/Figure_9.20_B19453.jpg" width="1496"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.20 – The holiday_ingest DAG enabled in the Airflow UI</p>
<p>Your job will start to run immediately.</p>
<ol>
<li value="7">Now, let’s find the task logs by following the same steps from the <em class="italic">Creating DAGs</em> recipe, but now clicking on the <code>holiday_api_ingestion</code> task. Your log page should look like the following figure:</li>
</ol>
<div><div><img alt="Figure 9.21 – holiday_api_ingestion task logs" height="542" src="img/Figure_9.21_B19453.jpg" width="1462"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.21 – holiday_api_ingestion task logs</p>
<ol>
<li value="8">Finally, let’s see whether the output file was created successfully. Go to the <code>files_to_test</code> folder, click on the <code>output_files</code> folder, and if everything was successfully configured, a file called <code>holiday_brazil.json</code> will be inside it. See the following figure for reference:</li>
</ol>
<div><div><img alt="Figure 9.22 – holiday_brazil.json inside the output_files screenshot" height="76" src="img/Figure_9.22_B19453.jpg" width="176"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.22 – holiday_brazil.json inside the output_files screenshot</p>
<p>The beginning of the <a id="_idIndexMarker629"/>output file should look like this:</p>
<div><div><img alt="Figure 9.23 – The first lines of holiday_brazil.json" height="458" src="img/Figure_9.23_B19453.jpg" width="560"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.23 – The first lines of holiday_brazil.json</p>
<h2 id="_idParaDest-330"><a id="_idTextAnchor337"/>How it works…</h2>
<p>As you can see, a custom Airflow operator is an isolated class with a unique purpose. Usually, custom operators are created with the intention to also be used by other teams or DAGs, which avoids code redundancy or duplication. Now, let’s understand how it works.</p>
<p>We started the recipe by creating the file to host the new operator inside the <code>plugin</code> folder. We do this because, internally, Airflow understands that everything inside of it is custom code. Since we wanted to only create an operator, we put it inside a folder with the same name. However, it is also possible to <a id="_idIndexMarker630"/>create another resource called <strong class="bold">Hooks</strong>. You can learn more about creating hooks in Airflow here: <a href="https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.xhtml">https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.xhtml</a>.</p>
<p>Now, heading to the<a id="_idIndexMarker631"/> operator code, we declare our code to ingest the HolidayAPI inside a class, as you can see here:</p>
<pre class="source-code">
class HolidayAPIIngestOperator(BaseOperator):
    def __init__(self, filename, secret_key, country, year, **kwargs):
        super().__init__(**kwargs)
        self.filename = filename
        self.secret_key = secret_key
        self.country = country
        self.year = year</pre>
<p>We did this to extend Airflow’s <code>BaseOperator</code> so that we could customize it and insert new constructors. <code>filename</code>, <code>secret_key</code>, <code>country</code>, and <code>year</code> are the parameters we need to execute the API ingest.</p>
<p>Then, we declared the <code>execute</code> function to ingest data from the API. The context is an Airflow parameter that allows the function to read configuration values:</p>
<pre class="source-code">
def execute(self, context):</pre>
<p>Then, our final step was to create a DAG to execute the operator we made. The code is like the previous DAG we created in the <em class="italic">Creating DAGs</em> recipe, only with a few new items. The first item was the new <code>import</code> instances, as you can see here:</p>
<pre class="source-code">
from airflow.models import Variable
from operators.holiday_api_plugin import HolidayAPIIngestOperator</pre>
<p>The first <code>import</code> statement allows us to use the value of <code>SECRET_HOLIDAY_API</code> we inserted using the UI, and the second imports our custom operator. Observe that we only used the <code>operators.holiday_api_plugin</code> path. Due to Airflow’s internal configuration, it understands that the code inside an <code>operators</code> folder (inside the <code>plugins</code> folder) is an operator.</p>
<p>Now we can instantiate<a id="_idIndexMarker632"/> the custom operator like any other built-in operator in Airflow by passing the required parameters, as you can see in the code here:</p>
<pre class="source-code">
task = HolidayAPIIngestOperator(
        task_id="holiday_api_ingestion",
        filename=filename_json,
        secret_key=Variable.get("SECRET_HOLIDAY_API"),
        country="BR",
        year=2022)</pre>
<h2 id="_idParaDest-331"><a id="_idTextAnchor338"/>There's more…</h2>
<p>If an entire project has the same form of authentication for retrieving data from a specific API or database, creating custom operators or hooks is a valuable way to avoid code duplication.</p>
<p>However, before jumping excitedly into creating your plugin, remember that Airflow’s community already provides many operators. For example, if you use AWS in your daily work, you don’t need to worry about creating a new operator to connect with AWS Glue since that already has been done and approved by the Apache community. See the documentation here: <a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/glue.xhtml">https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/glue.xhtml</a>.</p>
<p>You can see the<a id="_idIndexMarker633"/> complete list of AWS operators here: <a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/index.xhtml">https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/index.xhtml</a>.</p>
<h2 id="_idParaDest-332"><a id="_idTextAnchor339"/>See also</h2>
<p>For more custom operator examples, see <em class="italic">Virajdatt Kohir’s</em> blog here: <a href="https://kvirajdatt.medium.com/airflow-writing-custom-operators-and-publishing-them-as-a-package-part-2-3f4603899ec2">https://kvirajdatt.medium.com/airflow-writing-custom-operators-and-publishing-them-as-a-package-part-2-3f4603899ec2</a>.</p>
<h1 id="_idParaDest-333"><a id="_idTextAnchor340"/>Conﬁguring sensors</h1>
<p>Under the operator’s <a id="_idIndexMarker634"/>umbrella, we have sensors. Sensors are designed to wait to execute a task until something happens. For example, a sensor triggers a pipeline (or task) when a file lands in an <code>HDFS</code> folder, as shown here: <a href="https://airflow.apache.org/docs/apache-airflow-providers-apache-hdfs/stable/_api/airflow/providers/apache/hdfs/sensors/hdfs/index.xhtml">https://airflow.apache.org/docs/apache-airflow-providers-apache-hdfs/stable/_api/airflow/providers/apache/hdfs/sensors/hdfs/index.xhtml</a>. As you might be wondering, there are also sensors for specific schedules or time deltas.</p>
<p>Sensors are a fundamental part of creating an automated and event-driven pipeline. In this recipe, we will configure a <code>weekday</code> sensor, which executes our data pipeline on a specific day of the week.</p>
<h2 id="_idParaDest-334"><a id="_idTextAnchor341"/>Getting ready</h2>
<p>Refer to the <em class="italic">Getting ready</em> section in the <em class="italic">Configuring Airflow</em> recipe for this recipe since we will handle it with the same technology.</p>
<p>Besides that, let’s put a JSON file to the following path inside the Airflow folder: <code>files_to_test/sensors_files/.</code></p>
<p>In my case, I will use the <code>github_events.json</code> file, but you can use any of your preferences.</p>
<h2 id="_idParaDest-335"><a id="_idTextAnchor342"/>How to do it…</h2>
<p>Here are the steps to perform this recipe:</p>
<ol>
<li>Let’s start our DAG script by importing the required libraries, defining <code>default_args</code>, and instantiating our DAG, as you can see here:<pre class="source-code">
from airflow import DAG
from airflow.settings import AIRFLOW_HOME
from airflow.operators.bash import BashOperator
from airflow.sensors.weekday import DayOfWeekSensor
from airflow.utils.weekday import WeekDay
from datetime import datetime, timedelta
default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 3, 22),
    'retry_delay': timedelta(minutes=5)
}
# Instantiate a DAG object
with DAG(
    dag_id='sensors_move_file',
    default_args=default_args,
    schedule_interval="@once",
) as dag:</pre></li>
<li>Now, let’s define our<a id="_idIndexMarker635"/> first task using <code>DayOfWeekSensor</code>. See the code here:<pre class="source-code">
    move_file_on_saturdays = DayOfWeekSensor(
        task_id="move_file_on_saturdays",
        timeout=120,
        soft_fail=True,
        week_day=WeekDay.SATURDAY
    )</pre></li>
</ol>
<p>I suggest setting the day of the week as a parameter while doing this exercise to ensure no confusion. For example, if you want it to be executed on a Monday, set <code>week_day</code> to <code>WeekDay.MONDAY</code>, and so on.</p>
<ol>
<li value="3">Then, we will define another task using <code>BashOperator</code>. This task will execute the command to move a JSON file from <code>files_to_test/sensors_files/</code> to <code>files_to_test/output_files/</code>. Your code should look like this:<pre class="source-code">
    move_file_task = BashOperator(
            task_id="move_file_task",
            bash_command="mv $AIRFLOW_HOME/files_to_test/sensors_files/*.json $AIRFLOW_HOME/files_to_test/output_files/",
        )</pre></li>
<li>Then, we will define the <a id="_idIndexMarker636"/>execution workflow of our DAG, as you can see here:<pre class="source-code">
move_file_on_saturdays.set_downstream(move_file_task)</pre></li>
</ol>
<p>The <code>.set_downstream()</code> function will work similarly to the double arrows (<code>&gt;&gt;</code>) we already used to define the workflow. You can read more about this here: <a href="https://airflow.apache.org/docs/apache-airflow/1.10.3/concepts.xhtml?highlight=trigger#bitshift-composition">https://airflow.apache.org/docs/apache-airflow/1.10.3/concepts.xhtml?highlight=trigger#bitshift-composition</a>.</p>
<ol>
<li value="5">As seen in the previous two recipes of this chapter, now we will enable our <code>sensors_move_file</code> DAG, which will start immediately. If you set the weekday as the same day on which you are executing this exercise, your DAG <strong class="bold">Graph</strong> view will look like this, indicating success:</li>
</ol>
<div><div><img alt="Figure 9.24 – sensors_move_file tasks showing a success status" height="70" src="img/Figure_9.24_B19453.jpg" width="390"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.24 – sensors_move_file tasks showing a success status</p>
<ol>
<li value="6">Now, let’s see whether our file was moved to the directories. As described in the <em class="italic">Getting ready</em> section, I put a JSON file called <code>github_events.json</code> inside the <code>sensor_files</code> folder. Now, it will be inside <code>output_files</code>, as you can see here:</li>
</ol>
<div><div><img alt="Figure 9.25 – github_events.json inside the output_files folder" height="104" src="img/Figure_9.25_B19453.jpg" width="177"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.25 – github_events.json inside the output_files folder</p>
<p>This indicates our sensor executed as expected!</p>
<h2 id="_idParaDest-336"><a id="_idTextAnchor343"/>How it works…</h2>
<p>Sensors are valuable operators <a id="_idIndexMarker637"/>that execute an action based on a state. They can be triggered when a file lands in a directory, during the day, when an external task finishes, and so on. Here, we approached an example using a day of the week commonly used in data teams to change files from an ingested folder to a cold storage folder.</p>
<p>Sensors count with an internal method called <code>poke</code>, which will check a resource’s status until the criteria are met. If you look at the <code>move_file_on_saturday</code> log, you will see something like this:</p>
<pre class="source-code">
[2023-03-25, 00:19:03 UTC] {weekday.py:83} INFO - Poking until weekday is in WeekDay.SATURDAY, Today is SATURDAY
[2023-03-25, 00:19:03 UTC] {base.py:301} INFO - Success criteria met. Exiting.
[2023-03-25, 00:19:03 UTC] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=sensors_move_file, task_id=move_file_on_saturdays, execution_date=20230324T234623, start_date=20230325T001903, end_date=20230325T001903
[2023-03-25, 00:19:03 UTC] {local_task_job.py:156} INFO - Task exited with return code 0</pre>
<p>Looking at the following code, we did not define a <code>reschedule</code> parameter, so the job will stop until we manually trigger it again:</p>
<pre class="source-code">
    move_file_on_saturdays = DayOfWeekSensor(
        task_id="move_file_on_saturdays",
        timeout=120,
        soft_fail=True,
        week_day=WeekDay.SATURDAY
    )</pre>
<p>Other parameters we <a id="_idIndexMarker638"/>defined were <code>timeout</code>, which indicates the time in seconds before it fails or stops retrying, and <code>soft_fail</code>, which marks the task as <code>SKIPPED</code> in the case of failure.</p>
<p>You can see other allowed parameters here: <a href="https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/base/index.xhtml">https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/base/index.xhtml</a>.</p>
<p>And, of course, like the rest of the operators, we can create our custom sensor by extending the <code>BaseSensorOperator</code> class from Airflow. The main challenge here is that to be considered a sensor, it needs to overwrite the <code>poke</code> parameter without creating a recursing or non-ending function.</p>
<h2 id="_idParaDest-337"><a id="_idTextAnchor344"/>See also</h2>
<p>You can see a list of the default Airflow sensors on the official documentation page here: <a href="https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/index.xhtml">https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/index.xhtml</a>.</p>
<h1 id="_idParaDest-338"><a id="_idTextAnchor345"/>Creating connectors in Airﬂow</h1>
<p>Having DAGs and operators without <a id="_idIndexMarker639"/>connecting to any external source is useless. Of course, there are many<a id="_idIndexMarker640"/> ways to ingest files, even from other DAGs or task results. Still, data ingestion usually involves using external sources such as APIs or databases as the first step of a data pipeline.</p>
<p>To make this happen, in this recipe, we will understand how to create a connector in Airflow to connect to a sample database.</p>
<h2 id="_idParaDest-339"><a id="_idTextAnchor346"/>Getting ready</h2>
<p>Refer to the <em class="italic">Getting ready</em> section of the <em class="italic">Configuring Airflow</em> recipe for this recipe since we will handle it with the same<a id="_idIndexMarker641"/> technology.</p>
<p>This exercise will also require the<a id="_idIndexMarker642"/> MongoDB local database to be up and running. Ensure you have configured it as seen in <a href="B19453_01.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a> and have at least one database and collection. You can use the instructions from the <em class="italic">Connecting to a NoSQL database (MongoDB)</em> recipe in <a href="B19453_05.xhtml#_idTextAnchor161"><em class="italic">Chapter 5</em></a>.</p>
<h2 id="_idParaDest-340"><a id="_idTextAnchor347"/>How to do it…</h2>
<p>Here are the steps to perform this recipe:</p>
<ol>
<li>Let’s start by opening the Airflow UI. On the top menu, select the <strong class="bold">Admin</strong> button and then <strong class="bold">Connections</strong>, and you will be redirected to the <strong class="bold">Connections</strong> page. Since we haven’t configured anything yet, this page will be empty, as you can see in the following screenshot:</li>
</ol>
<div><div><img alt="Figure 9.26 – The Connections page in the Airflow UI" height="319" src="img/Figure_9.26_B19453.jpg" width="826"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.26 – The Connections page in the Airflow UI</p>
<ol>
<li value="2">Then, click the <strong class="bold">+</strong> button to be redirected to the <strong class="bold">Add Connection</strong> page. Under the <strong class="bold">Connection Type</strong> field, search for and select <strong class="bold">MongoDB</strong>. Insert your connection values under the respective fields, as shown in the following screenshot:</li>
</ol>
<div><div><img alt="Figure 9.27 – Creating a new connector in the Airflow UI" height="930" src="img/Figure_9.27_B19453.jpg" width="1004"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.27 – Creating a new connector in the Airflow UI</p>
<p>Click on the <strong class="bold">Save</strong> button, and you <a id="_idIndexMarker643"/>should have something similar to this on the <strong class="bold">Connection</strong> page:</p>
<div><div><img alt="Figure 9.28 – The MongoDB connector created in the Airflow UI" height="85" src="img/Figure_9.28_B19453.jpg" width="1228"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.28 – The MongoDB connector created in the Airflow UI</p>
<ol>
<li value="3">Let’s create our new DAG using the same<a id="_idIndexMarker644"/> folder and file tree structure that we saw in the <em class="italic">Creating DAGs</em> recipe. I will call the <code>mongodb_check_conn_dag.py</code> DAG file.</li>
<li>Inside the DAG file, let’s start by importing and declaring the required libraries and variables, as you can see here:<pre class="source-code">
from airflow import DAG
from airflow.settings import AIRFLOW_HOME
from airflow.providers.mongo.hooks.mongo import MongoHook
from airflow.operators.python import PythonOperator
import os
import json
from datetime import datetime, timedelta
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 3, 22),
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}</pre></li>
<li>Now, we will create a function<a id="_idIndexMarker645"/> to connect with MongoDB<a id="_idIndexMarker646"/> locally and print <code>collection</code> <code>reviews</code> from the <code>db_airbnb</code> database, as you can see here:<pre class="source-code">
def get_mongo_collection():
    hook = MongoHook(conn_id ='mongodb')
    client = hook.get_conn()
    print(client)
    print( hook.get_collection(mongo_collection="reviews", mongo_db="db_airbnb"))</pre></li>
<li>Then, let’s proceed with the DAG object:<pre class="source-code">
# Instantiate a DAG object
with DAG(
    dag_id='mongodb_check_conn',
    default_args=default_args,
    schedule_interval=timedelta(days=1),
) as dag:</pre></li>
<li>Finally, let’s use <code>PythonOperator</code> to call <a id="_idIndexMarker647"/>our <code>get_mongo_collection</code> function defined in <em class="italic">step 5</em>:<pre class="source-code">
    mongo_task = PythonOperator(
        task_id='mongo_task',
        python_callable=get_mongo_collection
    )</pre></li>
</ol>
<p>Don’t forget to put the name<a id="_idIndexMarker648"/> of your task in the indentation of the DAG, as follows:</p>
<pre class="source-code">
mongo_task</pre>
<ol>
<li value="8">Heading to the Airflow UI, let’s enable the DAG and wait for it to be executed. After finishing successfully, your <code>mongodb_task</code> log should look like this:</li>
</ol>
<div><div><img alt="Figure 9.29 – mongodb_task logs" height="555" src="img/Figure_9.29_B19453.jpg" width="1434"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.29 – mongodb_task logs</p>
<p>As you can see, we connected and retrieved the <code>Collection</code> object from MongoDB.</p>
<h2 id="_idParaDest-341"><a id="_idTextAnchor348"/>How it works…</h2>
<p>Creating a connection in <a id="_idIndexMarker649"/>Airflow is straightforward, as demonstrated here using the UI. It is also possible to create connections programmatically using the <code>Connection</code> class.</p>
<p>After we set our MongoDB<a id="_idIndexMarker650"/> connection parameters, we needed to create a form to access it, and we did so using a hook. A <strong class="bold">hook</strong> is a high-level interface<a id="_idIndexMarker651"/> that allows connections to external sources without the need to be preoccupied with low-level code or special libraries.</p>
<p>Remember that we configured an external package in the <em class="italic">Configuring Airflow</em> recipe? It was a provider to allow an easy connection with MongoDB:</p>
<pre class="source-code">
from airflow.providers.mongo.hooks.mongo import MongoHook</pre>
<p>Inside the <code>get_mongo_collection</code> function, we instantiated <code>MongoHook</code> and passed the same connection ID name set in the <code>Connection</code> page, as you can see here:</p>
<pre class="source-code">
hook = MongoHook(conn_id ='mongodb')</pre>
<p>With that instance, we can call the methods of the <code>MongoHook</code> class and even pass other parameters to configure the connection. See the documentation for this class here: <a href="https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/hooks/mongo/index.xhtml">https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/hooks/mongo/index.xhtml</a>.</p>
<h2 id="_idParaDest-342"><a id="_idTextAnchor349"/>There's more…</h2>
<p>You can also use the <code>airflow.cfg</code> file to set the connection strings or any other environment variable. It is a good practice to store sensitive information here, such as credentials, since they will not be shown in the UI. It is also possible to encrypt these values with additional configuration.</p>
<p>For more information, see the documentation here: <a href="https://airflow.apache.org/docs/apache-airflow/stable/howto/set-config.xhtml">https://airflow.apache.org/docs/apache-airflow/stable/howto/set-config.xhtml</a>.</p>
<h2 id="_idParaDest-343"><a id="_idTextAnchor350"/>See also</h2>
<ul>
<li>You can learn about the MongoDB provider on the Airflow official documentation page here: <a href="https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/index.xhtml">https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/index.xhtml</a></li>
<li>If you are interested in reading more about connections, see this link: https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.xhtml#custom-connection-types</li>
</ul>
<h1 id="_idParaDest-344"><a id="_idTextAnchor351"/>Creating parallel ingest tasks</h1>
<p>When working with data, we<a id="_idIndexMarker652"/> hardly ever just perform one ingestion at a time, and a real-world project involves many ingestions happening simultaneously, often in parallel. We know scheduling two or more DAGs to run alongside each other is possible, but what about tasks inside one DAG?</p>
<p>This recipe will illustrate how to create parallel task execution in Airflow.</p>
<h2 id="_idParaDest-345"><a id="_idTextAnchor352"/>Getting ready</h2>
<p>Please refer to the <em class="italic">Getting ready</em> section of the <em class="italic">Configuring Airflow</em> recipe for this recipe since we will handle it with the same technology.</p>
<p>To avoid redundancy in this exercise, we won’t explicitly include the imports and main DAG configuration. Instead, the focus is on organizing the operator’s workflow. You can use the same logic to create your DAG structure as in the <em class="italic">Creating </em><em class="italic">DAGs</em> recipe.</p>
<p>For the complete Python file used here, go to the GitHub page here: <a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_parallel_ingest_tasks">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_parallel_ingest_tasks</a>.</p>
<h2 id="_idParaDest-346"><a id="_idTextAnchor353"/>How to do it…</h2>
<p>For this exercise, my DAG will be called <code>parallel_tasks_dag</code>. Now, let’s try it:</p>
<ol>
<li>Let’s start by creating five <code>BashOperator</code> instances, as you can see here:<pre class="source-code">
# (DAG configuration above)
    t_0 = BashOperator(
            task_id="t_0",
            bash_command="echo 'This tasks will be executed first'",
        )
    t_1 = BashOperator(
            task_id="t_1",
            bash_command="echo 'This tasks no1 will be executed in parallel with t_2 and t_3'",
        )
    t_2 = BashOperator(
            task_id="t_2",
            bash_command="echo 'This tasks no2 will be executed in parallel with t_1 and t_3'",
        )
    t_3 = BashOperator(
            task_id="t_3",
            bash_command="echo 'This tasks no3 will be executed in parallel with t_1 and t_2'",
        )
    t_final = BashOperator(
        task_id="t_final",
        bash_command="echo 'Finished all tasks in parallel'",
    )</pre></li>
<li>The idea is for three <a id="_idIndexMarker653"/>of them to be executed in parallel so they will be inside square brackets. The first and last tasks will have the same workflow declared as we saw in the <em class="italic">Creating DAGs</em> recipe, using the <code>&gt;&gt;</code> character. The final flow structure will look like this:<pre class="source-code">
t_0 &gt;&gt; [t_1, t_2, t_3] &gt;&gt; t_final</pre></li>
<li>Finally, enable your DAG, and let’s see what it looks like on the DAG <strong class="bold">Graph</strong> page. It would be best if you had something like the following figure:</li>
</ol>
<div><div><img alt="Figure 9.30 – parallel_tasks_dag tasks in Airflow" height="277" src="img/Figure_9.30_B19453.jpg" width="260"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.30 – parallel_tasks_dag tasks in Airflow</p>
<p>As you can observe, the tasks inside the square brackets are displayed in parallel and will start after <code>t_0</code> finishes its work.</p>
<h2 id="_idParaDest-347"><a id="_idTextAnchor354"/>How it works…</h2>
<p>Although creating parallel tasks inside a DAG is simple, this type of workflow arrangement is advantageous when working with data.</p>
<p>Consider an example of data ingestion: we need to guarantee we have ingested all the desired endpoints before moving out to the next pipeline phase. See the following figure as a reference:</p>
<div><div><img alt="Figure 9.31 – Example of parallel execution" height="462" src="img/Figure_9.31_B19453.jpg" width="1012"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.31 – Example of parallel execution</p>
<p>The parallel execution will only move to the final task when all the parallel ones finish successfully. With this, we guarantee<a id="_idIndexMarker654"/> the data pipeline will not ingest only a small portion of the data but all the required data.</p>
<p>Back to our exercise, we can simulate this <a id="_idIndexMarker655"/>behavior, creating an <code>t_2</code> by removing one of the simple quotation marks. In the following figure, you can see what the DAG graph will look like:</p>
<div><div><img alt="Figure 9.32 – Airflow’s parallel_tasks_dag with an error t_2 task" height="283" src="img/Figure_9.32_B19453.jpg" width="256"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.32 – Airflow’s parallel_tasks_dag with an error t_2 task</p>
<p>The <code>t_final</code> task will retry executing until we fix <code>t_2</code> or the number of retries reaches its limit.</p>
<p>However, avoiding many parallel tasks is a good practice, mainly if you have limited infrastructure resources to handle them. There are many ways to create dependency on external tasks or DAGs, and we can use them to make more efficient pipelines.</p>
<h2 id="_idParaDest-348"><a id="_idTextAnchor355"/>There's more…</h2>
<p>Along with the concept of task<a id="_idIndexMarker656"/> parallelism, we have <code>BranchOperator</code>. <code>BranchOperator</code> executes one or more tasks simultaneously based on a criteria match. Let’s illustrate this with the following figure:</p>
<div><div><img alt="Figure 9.33 – Branching task diagram example" height="404" src="img/Figure_9.33_B19453.jpg" width="1218"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.33 – Branching task diagram example</p>
<p>Based on the day-of-the-week criteria, the <code>day_of_the_week_branch</code> task will trigger a specific task assigned for that day.</p>
<p>If you want to know more about it, <em class="italic">Analytics Vidhya</em> has a good blog post about it, which you can read here: <a href="https://www.analyticsvidhya.com/blog/2023/01/data-engineering-101-branchpythonoperator-in-apache-airflow/">https://www.analyticsvidhya.com/blog/2023/01/data-engineering-101-branchpythonoperator-in-apache-airflow/</a>.</p>
<h2 id="_idParaDest-349"><a id="_idTextAnchor356"/>See also</h2>
<ul>
<li>BetterDataScience has a good blog post about parallel tasks in Airflow. You can find it here: <a href="https://betterdatascience.com/apache-airflow-run-tasks-in-parallel/">https://betterdatascience.com/apache-airflow-run-tasks-in-parallel/</a>.</li>
<li>You can read more about Airflow task parallelism here:<a href="https://hevodata.com/learn/airflow-parallelism/"> https://hevodata.com/learn/airflow-parallelism/</a>.</li>
</ul>
<h1 id="_idParaDest-350"><a id="_idTextAnchor357"/>Deﬁning ingest-dependent DAGs</h1>
<p>In the data world, considerable <a id="_idIndexMarker657"/>discussion exists about how to organize Airflow DAGs. The approach I generally use is to create a DAG for a specific pipeline based on the business logic or final destination. Nevertheless, sometimes, to proceed with a task inside a DAG, we depend on another DAG to finish the process and get the output.</p>
<p>In this recipe, we will create two DAGs, where the first depends on the result of the second to be successful. Otherwise, it will not be completed. To assist us, we will use the <code>ExternalTaskSensor</code> operator.</p>
<h2 id="_idParaDest-351"><a id="_idTextAnchor358"/>Getting ready</h2>
<p>Please refer to the <em class="italic">Getting ready</em> section of the <em class="italic">Configuring Airflow</em> recipe for this recipe since we will handle it with the same technology.</p>
<p>This recipe depends on the <code>holiday_ingest</code> DAG, created in the <em class="italic">Creating custom operators</em> recipe, so ensure you have that.</p>
<p>We will not explicitly cite the imports and main DAG configuration to prevent redundancy and repetition in this exercise. The aim here is how to organize the operator’s workflow. You can use the same logic to create your DAG structure as in the <em class="italic">Creating </em><em class="italic">DAGs</em> recipe.</p>
<p>For the complete Python file used here, go to the GitHub page here:</p>
<p><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/de%EF%AC%81ning_dependent_ingests_DAGs">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/de%EF%AC%81ning_dependent_ingests_DAGs</a></p>
<h2 id="_idParaDest-352"><a id="_idTextAnchor359"/>How to do it…</h2>
<p>For this exercise, let’s create a DAG triggered when <code>holiday_ingest</code> finishes successfully, and returns all the holiday dates in the console output. My DAG will be called <code>external_sensor_dag</code>, but feel free to provide any other ID name. Just ensure it is unique and therefore will not impair other DAGs:</p>
<ol>
<li>Along with the default imports we have, let’s add the following:<pre class="source-code">
from airflow.sensors.external_task import ExternalTaskSensor</pre></li>
<li>Now, we will insert a Python function to return the holiday dates in the <code>holiday_brazil.json</code> file, which is the output of the <code>holiday_ingest</code> DAG:<pre class="source-code">
def get_holiday_dates(filename_json):
    with open (filename_json, 'r') as f:
        json_hol = json.load(f)
        holidays = json_hol["holidays"]
    print([item['date'] for item in holidays])</pre></li>
<li>Then, we will make the two <a id="_idIndexMarker658"/>operators of the DAG and define the workflow:<pre class="source-code">
    wait_holiday_api_ingest = ExternalTaskSensor(
        task_id='wait_holiday_api_ingest',
        external_dag_id='holiday_ingest',
        external_task_id='holiday_api_ingestion',
        allowed_states=["success"],
        execution_delta = timedelta(minutes=1),
        timeout=600,
    )
    filename_json = f"{AIRFLOW_HOME}/files_to_test/output_files/holiday_brazil.json"
    date_tasks = PythonOperator(
        task_id='date_tasks',
        python_callable=get_holiday_dates,
        op_args=[filename_json]
    )
wait_holiday_api_ingest &gt;&gt; date_tasks</pre></li>
</ol>
<p>Save it and enable this DAG in the Airflow UI. Once enabled, you will notice the <code>wait_holiday_api_ingest</code> task will be in the <code>RUNNING</code> state and will not proceed to the other task, as follows:</p>
<div><div><img alt="Figure 9.34 – The wait_holiday_api_ingest task in the running state" height="66" src="img/Figure_9.34_B19453.jpg" width="355"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.34 – The wait_holiday_api_ingest task in the running state</p>
<p>You will also notice<a id="_idIndexMarker659"/> the log for this task looks like the following:</p>
<pre class="source-code">
<strong class="bold">[2023-03-26, 20:50:23 UTC] {external_task.py:166} INFO - Poking for tasks ['holiday_api_ingestion'] in dag holiday_ingest on 2023-03-24T23:50:00+00:00 ...</strong>
<strong class="bold">[2023-03-26, 20:51:23 UTC] {external_task.py:166} INFO - Poking for tasks ['holiday_api_ingestion'] in dag holiday_ingest on 2023-03-24T23:50:00+00:00 ...</strong></pre>
<ol>
<li value="4">Now, we will enable and run <code>holiday_ingest</code> (if it is not enabled yet).</li>
<li>Then, go back to <code>external_sensor_dag</code>, and you will see it finished successfully, as shown in the following figure:</li>
</ol>
<div><div><img alt="Figure 9.35 – external_sensor_dag showing success" height="70" src="img/Figure_9.35_B19453.jpg" width="357"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.35 – external_sensor_dag showing success</p>
<p>If we examine the<a id="_idIndexMarker660"/> logs of <code>date_tasks</code>, you will see the following output on the console:</p>
<pre class="source-code">
<strong class="bold">[2023-03-25, 16:39:31 UTC] {logging_mixin.py:115} INFO - ['2022-01-01', '2022-02-28', '2022-03-01', '2022-03-02', '2022-03-20', '2022-04-15', '2022-04-17', '2022-04-21', '2022-05-01', '2022-05-08', '2022-06-16', '2022-06-21', '2022-08-14', '2022-09-07', '2022-09-23', '2022-10-12', '2022-11-02', '2022-11-15', '2022-12-21', '2022-12-24', '2022-12-25', '2022-12-31']</strong></pre>
<p>Here is the complete log for reference:</p>
<div><div><img alt="Figure 9.36 – date_tasks logs in the Airflow UI" height="686" src="img/Figure_9.36_B19453.jpg" width="1444"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.36 – date_tasks logs in the Airflow UI</p>
<p>Now, let’s understand how it works in the next section.</p>
<h2 id="_idParaDest-353"><a id="_idTextAnchor360"/>How it works…</h2>
<p>Let’s start by taking a<a id="_idIndexMarker661"/> look at our <code>wait_holiday_api_ingest</code> task:</p>
<pre class="source-code">
wait_holiday_api_ingest = ExternalTaskSensor(
        task_id='wait_holiday_api_ingest',
        external_dag_id='holiday_ingest',
        external_task_id='holiday_api_ingestion',
        allowed_states=["success"],
        execution_delta = timedelta(minutes=1),
        timeout=300,
    )</pre>
<p><code>ExternalTaskSensor</code> is a sensor type that will only execute if another task outside its DAG finishes with a specific status defined on the <code>allowed_states</code> parameter. The default value for this parameter is <code>SUCCESS</code>.</p>
<p>The sensor will search for a specific DAG and task in Airflow using the <code>external_dag_id</code> and <code>external_task_id</code> parameters, which we have defined as <code>holiday_ingest</code> and <code>holiday_api_ingestion</code>, respectively. Finally, <code>execution_delta</code> will determine the time interval at which to poke the external DAG again.</p>
<p>Once it finishes, the DAG will remain in the <code>SUCCESS</code> state unless we define a different behavior in the default arguments. If we clear its status, it will return to the <code>RUNNING</code> mode until the sensor criteria are met again.</p>
<h2 id="_idParaDest-354"><a id="_idTextAnchor361"/>There's more…</h2>
<p>We know Airflow is a powerful tool, but it is not immune from occasional failures. Internally, Airflow has its routes to reach internal and external DAGs, which can occasionally fail. For example, one of these errors might be a DAG not being found, which can happen due to various reasons such as misconfiguration or connectivity issues. You can see a screenshot of one of these errors here:</p>
<div><div><img alt="Figure 9.37 – Occasional 403 error log in an Airflow task" height="219" src="img/Figure_9.37_B19453.jpg" width="1445"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.37 – Occasional 403 error log in an Airflow task</p>
<p>Looking closely, we<a id="_idIndexMarker662"/> can observe that for several seconds, one of the Airflow workers lost permission to access or retrieve information from another worker. If this happens to you, disable your DAG and enable it again.</p>
<h3>XCom</h3>
<p>In this exercise, we used an output file to perform an action, but we can also use the output of a task without requiring it to write a file somewhere. Instead, we can use the <strong class="bold">XCom</strong> (short for <strong class="bold">cross-communications</strong>) mechanism<a id="_idIndexMarker663"/> to help us with it.</p>
<p>To use XCom across tasks, we can simply use <em class="italic">push</em> and <em class="italic">pull</em> the values using the <code>xcom_push</code> and <code>xcom_pull</code> methods inside the required tasks. Behind the scenes, Airflow stores those values temporarily in one of its databases, making it easier to access them again.</p>
<p>To check your stored XComs in the Airflow UI, click on the <strong class="bold">Admin</strong> button and select <strong class="bold">XCom</strong>.</p>
<p class="callout-heading">Note</p>
<p class="callout">In production environments, XComs might have a purge routine. Check with the Airflows administrators if you need to keep a value for longer.</p>
<p>You can read more about XComs on the official documentation page here: <a href="https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.xhtml">https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.xhtml</a>.</p>
<h2 id="_idParaDest-355"><a id="_idTextAnchor362"/>See also</h2>
<p>You can learn more about this<a id="_idIndexMarker664"/> operator on the Airflow official documentation page: <a href="https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/external_task_sensor.xhtml">https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/external_task_sensor.xhtml</a>.</p>
<h1 id="_idParaDest-356"><a id="_idTextAnchor363"/>Further reading</h1>
<ul>
<li><a href="https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.xhtml">https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.xhtml</a></li>
<li><a href="https://airflow.apache.org/docs/apache-airflow/2.2.4/best-practices.xhtml">https://airflow.apache.org/docs/apache-airflow/2.2.4/best-practices.xhtml</a></li>
<li><a href="https://www.qubole.com/tech-blog/apache-airflow-tutorial-dags-tasks-operators-sensors-hooks-xcom">https://www.qubole.com/tech-blog/apache-airflow-tutorial-dags-tasks-operators-sensors-hooks-xcom</a></li>
<li><a href="https://python.plainenglish.io/apache-airflow-how-to-correctly-setup-custom-plugins-2f80fe5e3dbe">https://python.plainenglish.io/apache-airflow-how-to-correctly-setup-custom-plugins-2f80fe5e3dbe</a></li>
<li><a href="https://copyprogramming.com/howto/airflow-how-to-mount-airflow-cfg-in-docker-container">https://copyprogramming.com/howto/airflow-how-to-mount-airflow-cfg-in-docker-container</a></li>
</ul>
</div>
</div></body></html>