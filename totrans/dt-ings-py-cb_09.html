<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer258">
<h1 class="chapter-number" id="_idParaDest-312"><a id="_idTextAnchor319"/>9</h1>
<h1 id="_idParaDest-313"><a id="_idTextAnchor320"/>Putting Everything Together with Airﬂow</h1>
<p>So far, we have covered the different aspects and steps of data ingestion. We have seen how to configure and ingest structured and unstructured data, what analytical data is, and how to improve logs for more insightful monitoring and error handling. Now is the time to group all this information to create something similar to a <span class="No-Break">real-world project.</span></p>
<p>From now on, in the following chapters, we will use Apache Airflow, an open source platform that allows us to create, schedule, and monitor workflows. Let’s start our journey by configuring and understanding the fundamental concepts of Apache Airflow and how powerful this <span class="No-Break">tool is.</span></p>
<p>In this chapter, you will learn about the <span class="No-Break">following topics:</span></p>
<ul>
<li><span class="No-Break">Configuring Airflow</span></li>
<li><span class="No-Break">Creating DAGs</span></li>
<li>Creating <span class="No-Break">custom operators</span></li>
<li><span class="No-Break">Conﬁguring sensors</span></li>
<li>Creating connectors <span class="No-Break">in Airﬂow</span></li>
<li>Creating parallel <span class="No-Break">ingest tasks</span></li>
<li>Deﬁning <span class="No-Break">ingest-dependent DAGs</span></li>
</ul>
<p>By the end of this chapter, you will have learned about the most important components of Airﬂow and how to conﬁgure them, including how to solve related issues in <span class="No-Break">this process.</span></p>
<h1 id="_idParaDest-314"><a id="_idTextAnchor321"/>Technical requirements</h1>
<p>You can find the code for this chapter in the GitHub repository <span class="No-Break">here: </span><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook"><span class="No-Break">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook</span></a></p>
<h2 id="_idParaDest-315"><a id="_idTextAnchor322"/>Installing Airflow</h2>
<p>This chapter<a id="_idIndexMarker600"/> requires that Airflow is installed on your local machine. You can install it directly on your <strong class="bold">Operating System</strong> (<strong class="bold">OS</strong>) or using a Docker image. For more information regarding this, refer to the <em class="italic">Configuring Docker for Airflow</em> recipe in <a href="B19453_01.xhtml#_idTextAnchor022"><span class="No-Break"><em class="italic">Chapter 1</em></span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-316"><a id="_idTextAnchor323"/>Configuring Airflow</h1>
<p><strong class="bold">Apache Airflow</strong> has many<a id="_idIndexMarker601"/> capabilities and a quick setup, which helps us start designing our workflows as code. Some additional configurations might be required as we progress with the workflows and into data processing. Gladly, Airflow has a dedicated file for inserting other arrangements without changing anything within <span class="No-Break">its core.</span></p>
<p>In this recipe, we will learn more about the <strong class="source-inline">airflow.conf</strong> file, how to use it, and other valuable configurations required to execute the other recipes in this chapter. We will also cover where to find this file and how other environment variables work with this tool. Understanding these concepts in practice helps us to identify potential improvements or <span class="No-Break">solve problems.</span></p>
<h2 id="_idParaDest-317"><a id="_idTextAnchor324"/>Getting ready</h2>
<p>Before moving on to the code, ensure your Airflow runs correctly. You can do that by checking the Airflow UI at this <span class="No-Break">link: </span><span class="No-Break"><strong class="source-inline">http://localhost:8080</strong></span><span class="No-Break">.</span></p>
<p>If you are using a Docker container (as I am) to host your Airflow application, you can check its status on the terminal with the <span class="No-Break">following command:</span></p>
<pre class="source-code">
$ docker ps</pre>
<p>This is <span class="No-Break">the output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer221">
<img alt="Figure 9.1 – Airflow Docker containers running" height="131" src="image/Figure_9.01_B19453.jpg" width="1407"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Airflow Docker containers running</p>
<p>Or, you can check the container status on Docker Desktop, as in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer222">
<img alt="Figure 9.2 – Docker Desktop view of Airflow containers running" height="455" src="image/Figure_9.02_B19453.jpg" width="1051"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Docker Desktop view of Airflow containers running</p>
<h2 id="_idParaDest-318"><a id="_idTextAnchor325"/>How to do it…</h2>
<p>Here are the steps to<a id="_idIndexMarker602"/> perform <span class="No-Break">this recipe:</span></p>
<ol>
<li>Let’s start by installing the MongoDB additional provider for Airflow. If you are using the <strong class="source-inline">docker-compose.yaml</strong> file, open it and add <strong class="source-inline">apache-airflow-providers-mongo</strong> inside <strong class="source-inline">_PIP_ADDITIONAL_REQUIREMENTS</strong>. Your code will look <span class="No-Break">like this:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer223">
<img alt="Figure 9.3 – The docker-compose.yaml file in the environment variables section" height="483" src="image/Figure_9.03_B19453.jpg" width="945"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – The docker-compose.yaml file in the environment variables section</p>
<p>If you are hosting <a id="_idIndexMarker603"/>Airflow directly on your machine, you can do the same<a id="_idIndexMarker604"/> installation using <span class="No-Break"><strong class="bold">PyPi</strong></span><span class="No-Break">: </span><a href="https://pypi.org/project/apache-airflow-providers-mongo/"><span class="No-Break">https://pypi.org/project/apache-airflow-providers-mongo/</span></a><span class="No-Break">.</span></p>
<ol>
<li value="2">Next, we will create a folder called <strong class="source-inline">files_to_test</strong>, and inside it, create two more folders: <strong class="source-inline">output_files</strong> and <strong class="source-inline">sensors_files</strong>. You don’t need to worry about its usage yet since it will be used later in this chapter. Your Airflow folder structure should look <span class="No-Break">like this:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer224">
<img alt="Figure 9.4 – Airflow local directory folder structure" height="149" src="image/Figure_9.04_B19453.jpg" width="218"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – Airflow local directory folder structure</p>
<ol>
<li value="3">Now, let’s mount the volumes of our Docker image. You can skip this part if you are not using Docker to <span class="No-Break">host </span><span class="No-Break"><a id="_idIndexMarker605"/></span><span class="No-Break">Airflow.</span></li>
</ol>
<p>In your <strong class="source-inline">docker-compose.yaml</strong> file, under the <strong class="source-inline">volume</strong> parameter, add <span class="No-Break">the following:</span></p>
<pre class="source-code">
<strong class="bold">- ./config/airflow.cfg:/usr/local/airflow/airflow.cfg</strong>
<strong class="bold">- ./files_to_test:/opt/airflow/files_to_test</strong></pre>
<p>Your final <strong class="source-inline">volumes</strong> section will look <span class="No-Break">like this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer225">
<img alt="Figure 9.5 – docker-compose.yaml volumes" height="138" src="image/Figure_9.05_B19453.jpg" width="539"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – docker-compose.yaml volumes</p>
<p>Stop and restart your container so these changes can <span class="No-Break">be propagated.</span></p>
<ol>
<li value="4">Finally, we will fix a bug in the <strong class="source-inline">docker-compose.yaml</strong> file. This official fix for this bug is within the Airflow official documentation and therefore wasn’t included in the Docker image. You can see the complete issue and the solution <span class="No-Break">here: </span><a href="https://github.com/apache/airflow/discussions/24809"><span class="No-Break">https://github.com/apache/airflow/discussions/24809</span></a><span class="No-Break">.</span></li>
</ol>
<p>To fix the bug, go to the <strong class="source-inline">airflow-init</strong> section of the <strong class="source-inline">docker-compose</strong> file and insert <strong class="source-inline">_PIP_ADDITIONAL_REQUIREMENTS: ''</strong> inside the <strong class="source-inline">environment</strong> parameter. Your code will look <span class="No-Break">like this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer226">
<img alt="Figure 9.6 – The docker-compose.yaml environment variables section with PIP_ADDITIONAL_REQUIREMENTS" height="188" src="image/Figure_9.06_B19453.jpg" width="677"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – The docker-compose.yaml environment variables section with PIP_ADDITIONAL_REQUIREMENTS</p>
<p>This action will fix the following issue registered on <span class="No-Break">GitHub: </span><a href="https://github.com/apache/airflow/pull/23517"><span class="No-Break">https://github.com/apache/airflow/pull/23517</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-319"><a id="_idTextAnchor326"/>How it works…</h2>
<p>The configuration presented here<a id="_idIndexMarker606"/> is simple. However, it guarantees the application will keep working through the <span class="No-Break">chapter recipes.</span></p>
<p>Let’s start with the package we installed in <em class="italic">step 1</em>. Like other frameworks or platforms, Airflow has its <em class="italic">batteries</em> included, which means it already comes with various packages. But, as its popularity started to increase, it started to require other types of connections or operators, which the open source community took <span class="No-Break">care of.</span></p>
<p>You can find a list of released packages that can be installed on Airflow <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.xhtml</span></a><span class="No-Break">.</span></p>
<p>Before jumping into other code explanations, let’s understand the <strong class="source-inline">volume</strong> section inside the <strong class="source-inline">docker-compose.yaml</strong> file. This configuration allows Airflow to see which folders reflect the same respective ones inside the Docker container without the necessity to upload code using a Docker command every time. In other words, we can synchronously add<a id="_idIndexMarker607"/> our <strong class="bold">Directed Acyclic Graph</strong> (<strong class="bold">DAG</strong>) files and new operators and see some logs, among other things, and this will be reflected inside <span class="No-Break">the container.</span></p>
<p>Next, we declared the Docker mount volume configurations for two parts: the new folder we created (<strong class="source-inline">files_to_test</strong>) and the <strong class="source-inline">airflow.cfg</strong> file. The first one will allow Airflow to replicate the <strong class="source-inline">files_to_test</strong> local folder inside the container, so we can use it to use files in a more simplified way. Otherwise, if we try to use it without the mounting volume, the following error will appear when trying to retrieve <span class="No-Break">any file:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer227">
<img alt="Figure 9.7 – Error in Airflow when the folder is not referred to in the container volume" height="180" src="image/Figure_9.07_B19453.jpg" width="917"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – Error in Airflow when the folder is not referred to in the container volume</p>
<p>Although we will not use the <strong class="source-inline">airflow.cfg</strong> file, for now, it is a good practice to know how to access this file and what it is used for. This file contains the Airflow configurations and can be edited to include more. Usually, sensitive data is stored inside it to prevent other people from <a id="_idIndexMarker608"/>having improper access since, by default, the content of the <strong class="source-inline">airflow.cfg</strong> file cannot be accessed in <span class="No-Break">the UI.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Be very cautious when changing or handling the <strong class="source-inline">airflow.cfg</strong> file. This file contains all the required configurations and other relevant settings to make Airflow work. We will explore more about this in <a href="B19453_10.xhtml#_idTextAnchor364"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><span class="No-Break"><em class="italic">.</em></span></p>
<h2 id="_idParaDest-320"><a id="_idTextAnchor327"/>See also</h2>
<p>For more information about the Docker image, see the documentation page <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.xhtml</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-321"><a id="_idTextAnchor328"/>Creating DAGs</h1>
<p>The core concept of Airflow is based on DAGs, which collect, group, and organize tasks to be executed in a specific<a id="_idIndexMarker609"/> order. A DAG is also responsible for managing the dependencies between its tasks. Simply put, it is not concerned about what a task is doing but just <em class="italic">how</em> to execute it. Typically, a DAG starts at a scheduled time, but we can also define dependencies between other DAGs so that they will start based on their <span class="No-Break">execution statuses.</span></p>
<p>We will create our first DAG in this recipe and set it to run based on a specific schedule. With this first step, we enter into practically designing our <span class="No-Break">first workflow.</span></p>
<h2 id="_idParaDest-322"><a id="_idTextAnchor329"/>Getting ready</h2>
<p>Please refer to the <em class="italic">Getting ready</em> section in the <em class="italic">Configuring Airflow</em> recipe for this recipe since we will handle it with the <span class="No-Break">same technology.</span></p>
<p>Also, let’s create a directory called <strong class="source-inline">ids_ingest</strong> inside our <strong class="source-inline">dags</strong> folder. Inside the <strong class="source-inline">ids_ingest</strong> folder, we will create two files: <strong class="source-inline">__init__.py</strong> and <strong class="source-inline">ids_ingest_dag.py</strong>. The final structure will look <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer228">
<img alt="Figure 9.8 – Airflow’s local directory structure with the ids_ingest DAG" height="251" src="image/Figure_9.08_B19453.jpg" width="281"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.8 – Airflow’s local directory structure with the ids_ingest DAG</p>
<h2 id="_idParaDest-323"><a id="_idTextAnchor330"/>How to do it…</h2>
<p>In this exercise, we will write a<a id="_idIndexMarker610"/> DAG that retrieves the IDs of the <strong class="source-inline">github_events.json</strong> file. Open <strong class="source-inline">ids_ingest_dag.py</strong>, and let’s add the content to write our <span class="No-Break">first DAG:</span></p>
<ol>
<li>Let’s start by importing the libraries we will use in this script. I like to separate the imports from the Airflow library and Python’s library as a <span class="No-Break">good practice:</span><pre class="source-code">
from airflow import DAG
from airflow.settings import AIRFLOW_HOME
from airflow.operators.bash import BashOperator
from airflow.operators.python_operator import PythonOperator
import json
from datetime import datetime, timedelta</pre></li>
<li>Then, we will define <strong class="source-inline">default_args</strong> for our DAG, as you can <span class="No-Break">see here:</span><pre class="source-code">
# Define default arguments
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 3, 22),
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}</pre></li>
<li>Now, we will create a <a id="_idIndexMarker611"/>Python function that receives the JSON file and returns the IDs inside it. Since it is a small function, we can create it inside the <span class="No-Break">DAG’s file:</span><pre class="source-code">
def get_ids_from_json(filename_json):
    with open (filename_json, 'r') as f:
        git = json.loads(f.read())
    print([item['id'] for item in git])</pre></li>
<li>Next, we will instantiate our DAG object, and inside it, we will define two operators: a <strong class="source-inline">BashOperator</strong> instance to show a console message and <strong class="source-inline">PythonOperator</strong> to execute the function we just created, as you can <span class="No-Break">see here:</span><pre class="source-code">
# Instantiate a DAG object
with DAG(
    dag_id='simple_ids_ingest',
    default_args=default_args,
    schedule_interval=timedelta(days=1),
) as dag:
    first_task = BashOperator(
            task_id="first_task",
            bash_command="echo $AIRFLOW_HOME",
        )
    filename_json = f"{AIRFLOW_HOME}/files_to_test/github_events.json"
    get_id_from_json = PythonOperator(
        task_id="get_id_from_json",
        python_callable=get_ids_from_json,
        op_args=[filename_json]
    )</pre></li>
</ol>
<p>Make sure you<a id="_idIndexMarker612"/> save the file before jumping to the <span class="No-Break">next step.</span></p>
<ol>
<li value="5">Now, head over to the Airflow UI. Although <a id="_idIndexMarker613"/>plenty of DAG examples are provided by the Airflow team, you should look for a DAG called <strong class="source-inline">simple_ids_ingest</strong>. You will notice the DAG is not enabled. Click on the toggle button to enable it, and you should have something like <span class="No-Break">the following:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer229">
<img alt="Figure 9.9 – The Airflow DAG enabled on the UI" height="68" src="image/Figure_9.09_B19453.jpg" width="905"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.9 – The Airflow DAG enabled on the UI</p>
<ol>
<li value="6">As soon as you enable it, the DAG will start running. Click on the DAG name to be redirected to the DAG’s page, as you can see in the <span class="No-Break">following screenshot:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer230">
<img alt="Figure 9.10 – DAG Grid page view" height="933" src="image/Figure_9.10_B19453.jpg" width="1304"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.10 – DAG Grid page view</p>
<p>If everything is well <a id="_idIndexMarker614"/>configured, your page should look <span class="No-Break">like this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer231">
<img alt="Figure 9.11 – DAG running successfully in Graph page view" height="405" src="image/Figure_9.11_B19453.jpg" width="1446"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.11 – DAG running successfully in Graph page view</p>
<ol>
<li value="7">Then, click on the <strong class="bold">Graph</strong> item in the top menu and click on the <strong class="source-inline">get_id_from_json</strong> task. A small window will show up <span class="No-Break">as follows:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer232">
<img alt="Figure 9.12 – Task options" height="650" src="image/Figure_9.12_B19453.jpg" width="649"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.12 – Task options</p>
<ol>
<li value="8">Then, click the <strong class="bold">Log</strong> button. You will be <a id="_idIndexMarker615"/>redirected to a new page with the logs for this task, as <span class="No-Break">seen here:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer233">
<img alt="Figure 9.13 – Task logs in the Airflow UI" height="617" src="image/Figure_9.13_B19453.jpg" width="1465"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.13 – Task logs in the Airflow UI</p>
<p>As we can see in the preceding<a id="_idIndexMarker616"/> screenshot, our task successfully finished and returned the IDs as we expected. You can see the results in the <strong class="source-inline">INFO</strong> log under the <span class="No-Break"><strong class="source-inline">AIRFLOW_CTX_DAG_RUN</strong></span><span class="No-Break"> message.</span></p>
<h2 id="_idParaDest-324"><a id="_idTextAnchor331"/>How it works…</h2>
<p>We created our first DAG with a few lines to retrieve and show a list of IDs from a JSON file. Now, let’s understand how <span class="No-Break">it works.</span></p>
<p>To start with, we created our files under the <strong class="source-inline">dags</strong> directory. It happens because, by default, Airflow will understand everything inside of it as a DAG file. The folder we created inside of it was just for organization purposes, and Airflow will ignore it. Along with the <strong class="source-inline">ids_ingest_dag.py</strong> file, we also made an <strong class="source-inline">__init__.py</strong> file. This file internally tells Airflow to look inside this folder. As a result, you will see the <span class="No-Break">following structure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer234">
<img alt="Figure 9.14 – Airflow local directory structure with the ids_ingest DAG" height="249" src="image/Figure_9.14_B19453.jpg" width="287"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.14 – Airflow local directory structure with the ids_ingest DAG</p>
<p class="callout-heading">Note</p>
<p class="callout">As you might be wondering, it is possible to change this configuration, but I don’t recommend this at all since other internal packages might depend on it. Do it only in the case of <span class="No-Break">extreme necessity.</span></p>
<p>Now, let’s take a look at <a id="_idIndexMarker617"/>our <span class="No-Break">instantiated DAG:</span></p>
<pre class="source-code">
with DAG(
    dag_id='simple_ids_ingest',
    default_args=default_args,
    schedule_interval=timedelta(days=1),
) as dag:</pre>
<p>As you can observe, creating a DAG is simple, and its parameters are spontaneous. <strong class="source-inline">dag_id</strong> is crucial and must be unique; otherwise, it can create confusion and merge with other DAGs. The <strong class="source-inline">default_args</strong> we declared in <em class="italic">step 2</em> will guide the DAG, telling when it needs to be executed, its user owner, the number of retries in case of a failure, and other valuable parameters. After the <strong class="source-inline">as dag</strong> declaration, we inserted the bash and Python operators, and they must be indented to be understood as the <span class="No-Break">DAG’s tasks.</span></p>
<p>Finally, to set our workflow, we declared the <span class="No-Break">following line:</span></p>
<pre class="source-code">
first_task &gt;&gt; get_id_from_json</pre>
<p>As we might guess, it sets the order of which task should be <span class="No-Break">executed first.</span></p>
<h2 id="_idParaDest-325"><a id="_idTextAnchor332"/>There's more…</h2>
<p>We saw how easy it is to<a id="_idIndexMarker618"/> create a task to execute a Python function and a bash command. By default, Airflow comes with some handy operators to be used daily within a data ingestion pipeline. For more information, you can refer to the official documentation page <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.xhtml</span></a><span class="No-Break">.</span></p>
<h3>Tasks, operators, XCom, and others</h3>
<p>Airflow DAGs are a powerful way to<a id="_idIndexMarker619"/> group and execute operations. Besides the task and operators we saw here, DAGs support other types of workloads and <a id="_idIndexMarker620"/>communication across<a id="_idIndexMarker621"/> other tasks or DAGs. Unfortunately, since that is not the main subject of this book, we will not cover those concepts in detail, but I highly recommend reading the official documentation <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/index.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/index.xhtml</span></a><span class="No-Break">.</span></p>
<h3>Error handling</h3>
<p>If you encounter any errors while<a id="_idIndexMarker622"/> building this DAG, you can use the instructions from <em class="italic">step 7</em> and <em class="italic">step 8</em> to debug it. You can see a preview here of how the tasks look when an <span class="No-Break">error occurs:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer235">
<img alt="Figure 9.15 – DAG Graph page view showing a running error" height="495" src="image/Figure_9.15_B19453.jpg" width="1480"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.15 – DAG Graph page view showing a running error</p>
<h2 id="_idParaDest-326"><a id="_idTextAnchor333"/>See also</h2>
<p>You can find the code for the Airflow example DAGs on their official GitHub page <span class="No-Break">here: </span><a href="https://github.com/apache/airflow/tree/main/airflow/example_dags"><span class="No-Break">https://github.com/apache/airflow/tree/main/airflow/example_dags</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-327"><a id="_idTextAnchor334"/>Creating custom operators</h1>
<p>As seen in the previous recipe, <em class="italic">Creating DAGs</em>, it is nearly impossible to create a DAG without instantiating a task or, in other <a id="_idIndexMarker623"/>words, defining an operator. Operators are responsible for holding the logic required to process data in <span class="No-Break">the pipeline.</span></p>
<p>We also know that Airflow already has predefined operators, allowing dozens of ways to ingest and process data. Now, it is time to put into practice how to create custom operators. Custom operators allow us to apply specific logic to a related project or <span class="No-Break">data pipeline.</span></p>
<p>You will learn how to create a simple customized operator in this recipe. Although it is very basic, you will be able to apply the foundations of this technique to <span class="No-Break">different scenarios.</span></p>
<p>In this recipe, we will create a custom operator to connect to and retrieve data from the HolidayAPI, the same as we saw previously, in <a href="B19453_02.xhtml#_idTextAnchor064"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-328"><a id="_idTextAnchor335"/>Getting ready</h2>
<p>Please, refer to the <em class="italic">Getting ready</em> section in the <em class="italic">Configuring Airflow</em> recipe for this recipe since we will handle it with the <span class="No-Break">same technology.</span></p>
<p>We also need to add an environment variable to store our API secret. To do so, select the <strong class="bold">Variable</strong> item under the <strong class="bold">Admin</strong> menu in the Airflow UI, and you will be redirected to the desired page. Now, click the <strong class="bold">+</strong> button to add a new variable, <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer236">
<img alt="Figure 9.16 – The Variable page in the Airflow UI" height="346" src="image/Figure_9.16_B19453.jpg" width="898"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.16 – The Variable page in the Airflow UI</p>
<p>On the <strong class="bold">List Variable</strong> page, insert <strong class="source-inline">SECRET_HOLIDAY_API</strong> under the <strong class="bold">Key</strong> field and your API secret under the <strong class="bold">Value</strong> field. Use the <a id="_idIndexMarker624"/>same values you used to execute the <em class="italic">Retrieving data using API authentication</em> recipe in <a href="B19453_02.xhtml#_idTextAnchor064"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>. Save it and you will be redirected to the <strong class="bold">Variables</strong> page, as shown in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer237">
<img alt="Figure 9.17 – The Airflow UI with a new variable to store the HolidayAPI secret" height="324" src="image/Figure_9.17_B19453.jpg" width="778"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.17 – The Airflow UI with a new variable to store the HolidayAPI secret</p>
<p>Now, we are ready to create our <span class="No-Break">custom operator.</span></p>
<h2 id="_idParaDest-329"><a id="_idTextAnchor336"/>How to do it…</h2>
<p>The code we will use to create the custom operator is the same one we saw in <a href="B19453_02.xhtml#_idTextAnchor064"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, in the <em class="italic">Retrieving data using API authentication</em> recipe, with some alterations to fit Airflow’s requirements. Here are the steps <span class="No-Break">for it:</span></p>
<ol>
<li>Let’s start by creating the structure inside the <strong class="source-inline">plugins</strong> folder. Since we want to make a custom operator, we need to create a folder called <strong class="source-inline">operators</strong>, where we will put a Python file called <strong class="source-inline">holiday_api_plugin.py</strong>. Your file tree should look <span class="No-Break">like this:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer238">
<img alt="Figure 9.18 – Airflow’s local directory structure for the plugins folder" height="122" src="image/Figure_9.18_B19453.jpg" width="285"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.18 – Airflow’s local directory structure for the plugins folder</p>
<ol>
<li value="2">We will create some code inside <strong class="source-inline">holiday_api_plugin.py</strong>, starting with the library imports and<a id="_idIndexMarker625"/> declaring a global variable for where our file output needs to <span class="No-Break">be placed:</span><pre class="source-code">
from airflow.settings import AIRFLOW_HOME
from airflow.models.baseoperator import BaseOperator
import requests
import json
file_output_path = f"{AIRFLOW_HOME}/files_to_test/output_files/"</pre></li>
<li>Then, we need to create a Python class, declare its constructors, and finally insert the exact code from <a href="B19453_02.xhtml#_idTextAnchor064"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> inside a function <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">execute</strong></span><span class="No-Break">:</span><pre class="source-code">
class HolidayAPIIngestOperator(BaseOperator):
    def __init__(self, filename, secret_key, country, year, **kwargs):
        super().__init__(**kwargs)
        self.filename = filename
        self.secret_key = secret_key
        self.country = country
        self.year = year
    def execute(self, context):
        params = { 'key': self.secret_key,
                'country': self.country,
                'year': self.year
        }
        url = "https://holidayapi.com/v1/holidays?"
        output_file = file_output_path + self.filename
        try:
            req = requests.get(url, params=params)
            print(req.json())
            with open(output_file, "w") as f:
                json.dump(req.json(), f)
            return "Holidays downloaded successfully"
        except Exception as e:
            raise e</pre></li>
</ol>
<p>Save the file and our <a id="_idIndexMarker626"/>operator is ready. Now, we need to create the DAG to <span class="No-Break">execute it.</span></p>
<ol>
<li value="4">Using the same logic as in the <em class="italic">Creating DAGs</em> recipe, we will create a file called <strong class="source-inline">holiday_ingest_dag.py</strong>. Your new DAG directory tree should look <span class="No-Break">like this:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer239">
<img alt="Figure 9.19 – Airflow’s directory structure for the holiday_ingest DAG folder" height="93" src="image/Figure_9.19_B19453.jpg" width="275"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.19 – Airflow’s directory structure for the holiday_ingest DAG folder</p>
<ol>
<li value="5">Now, let’s insert our <a id="_idIndexMarker627"/>DAG code inside the <strong class="source-inline">holiday_ingest_dag.py</strong> file and <span class="No-Break">save it:</span><pre class="source-code">
from airflow import DAG
# Other imports
from operators.holiday_api_plugin import HolidayAPIIngestOperator
# Define default arguments
# Instantiate a DAG object
with DAG(
    dag_id='holiday_ingest',
    default_args=default_args,
    schedule_interval=timedelta(days=1),
) as dag:
    filename_json = f"holiday_brazil.json"
    task = HolidayAPIIngestOperator(
        task_id="holiday_api_ingestion",
        filename=filename_json,
        secret_key=Variable.get("SECRET_HOLIDAY_API"),
        country="BR",
        year=2022
    )
task</pre></li>
</ol>
<p>For the full code, refer to the GitHub repository <span class="No-Break">here: </span><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_custom_operators"><span class="No-Break">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_custom_operators</span></a><span class="No-Break">.</span></p>
<ol>
<li value="6">Then, go to the<a id="_idIndexMarker628"/> Airflow UI, look for the <strong class="source-inline">holiday_ingest</strong> DAG, and enable it. It will look like the <span class="No-Break">following figure:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer240">
<img alt="Figure 9.20 – The holiday_ingest DAG enabled in the Airflow UI" height="56" src="image/Figure_9.20_B19453.jpg" width="1496"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.20 – The holiday_ingest DAG enabled in the Airflow UI</p>
<p>Your job will start to <span class="No-Break">run immediately.</span></p>
<ol>
<li value="7">Now, let’s find the task logs by following the same steps from the <em class="italic">Creating DAGs</em> recipe, but now clicking on the <strong class="source-inline">holiday_api_ingestion</strong> task. Your log page should look like the <span class="No-Break">following figure:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer241">
<img alt="Figure 9.21 – holiday_api_ingestion task logs" height="542" src="image/Figure_9.21_B19453.jpg" width="1462"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.21 – holiday_api_ingestion task logs</p>
<ol>
<li value="8">Finally, let’s see whether the output file was created successfully. Go to the <strong class="source-inline">files_to_test</strong> folder, click on the <strong class="source-inline">output_files</strong> folder, and if everything was successfully configured, a file called <strong class="source-inline">holiday_brazil.json</strong> will be inside it. See the following figure <span class="No-Break">for reference:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer242">
<img alt="Figure 9.22 – holiday_brazil.json inside the output_files screenshot" height="76" src="image/Figure_9.22_B19453.jpg" width="176"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.22 – holiday_brazil.json inside the output_files screenshot</p>
<p>The beginning of the <a id="_idIndexMarker629"/>output file should look <span class="No-Break">like this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer243">
<img alt="Figure 9.23 – The first lines of holiday_brazil.json" height="458" src="image/Figure_9.23_B19453.jpg" width="560"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.23 – The first lines of holiday_brazil.json</p>
<h2 id="_idParaDest-330"><a id="_idTextAnchor337"/>How it works…</h2>
<p>As you can see, a custom Airflow operator is an isolated class with a unique purpose. Usually, custom operators are created with the intention to also be used by other teams or DAGs, which avoids code redundancy or duplication. Now, let’s understand how <span class="No-Break">it works.</span></p>
<p>We started the recipe by creating the file to host the new operator inside the <strong class="source-inline">plugin</strong> folder. We do this because, internally, Airflow understands that everything inside of it is custom code. Since we wanted to only create an operator, we put it inside a folder with the same name. However, it is also possible to <a id="_idIndexMarker630"/>create another resource called <strong class="bold">Hooks</strong>. You can learn more about creating hooks in Airflow <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.xhtml</span></a><span class="No-Break">.</span></p>
<p>Now, heading to the<a id="_idIndexMarker631"/> operator code, we declare our code to ingest the HolidayAPI inside a class, as you can <span class="No-Break">see here:</span></p>
<pre class="source-code">
class HolidayAPIIngestOperator(BaseOperator):
    def __init__(self, filename, secret_key, country, year, **kwargs):
        super().__init__(**kwargs)
        self.filename = filename
        self.secret_key = secret_key
        self.country = country
        self.year = year</pre>
<p>We did this to extend Airflow’s <strong class="source-inline">BaseOperator</strong> so that we could customize it and insert new constructors. <strong class="source-inline">filename</strong>, <strong class="source-inline">secret_key</strong>, <strong class="source-inline">country</strong>, and <strong class="source-inline">year</strong> are the parameters we need to execute the <span class="No-Break">API ingest.</span></p>
<p>Then, we declared the <strong class="source-inline">execute</strong> function to ingest data from the API. The context is an Airflow parameter that allows the function to read <span class="No-Break">configuration values:</span></p>
<pre class="source-code">
def execute(self, context):</pre>
<p>Then, our final step was to create a DAG to execute the operator we made. The code is like the previous DAG we created in the <em class="italic">Creating DAGs</em> recipe, only with a few new items. The first item was the new <strong class="source-inline">import</strong> instances, as you can <span class="No-Break">see here:</span></p>
<pre class="source-code">
from airflow.models import Variable
from operators.holiday_api_plugin import HolidayAPIIngestOperator</pre>
<p>The first <strong class="source-inline">import</strong> statement allows us to use the value of <strong class="source-inline">SECRET_HOLIDAY_API</strong> we inserted using the UI, and the second imports our custom operator. Observe that we only used the <strong class="source-inline">operators.holiday_api_plugin</strong> path. Due to Airflow’s internal configuration, it understands that the code inside an <strong class="source-inline">operators</strong> folder (inside the <strong class="source-inline">plugins</strong> folder) is <span class="No-Break">an operator.</span></p>
<p>Now we can instantiate<a id="_idIndexMarker632"/> the custom operator like any other built-in operator in Airflow by passing the required parameters, as you can see in the <span class="No-Break">code here:</span></p>
<pre class="source-code">
task = HolidayAPIIngestOperator(
        task_id="holiday_api_ingestion",
        filename=filename_json,
        secret_key=Variable.get("SECRET_HOLIDAY_API"),
        country="BR",
        year=2022)</pre>
<h2 id="_idParaDest-331"><a id="_idTextAnchor338"/>There's more…</h2>
<p>If an entire project has the same form of authentication for retrieving data from a specific API or database, creating custom operators or hooks is a valuable way to avoid <span class="No-Break">code duplication.</span></p>
<p>However, before jumping excitedly into creating your plugin, remember that Airflow’s community already provides many operators. For example, if you use AWS in your daily work, you don’t need to worry about creating a new operator to connect with AWS Glue since that already has been done and approved by the Apache community. See the documentation <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/glue.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/glue.xhtml</span></a><span class="No-Break">.</span></p>
<p>You can see the<a id="_idIndexMarker633"/> complete list of AWS operators <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/index.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/index.xhtml</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-332"><a id="_idTextAnchor339"/>See also</h2>
<p>For more custom operator examples, see <em class="italic">Virajdatt Kohir’s</em> blog <span class="No-Break">here: </span><a href="https://kvirajdatt.medium.com/airflow-writing-custom-operators-and-publishing-them-as-a-package-part-2-3f4603899ec2"><span class="No-Break">https://kvirajdatt.medium.com/airflow-writing-custom-operators-and-publishing-them-as-a-package-part-2-3f4603899ec2</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-333"><a id="_idTextAnchor340"/>Conﬁguring sensors</h1>
<p>Under the operator’s <a id="_idIndexMarker634"/>umbrella, we have sensors. Sensors are designed to wait to execute a task until something happens. For example, a sensor triggers a pipeline (or task) when a file lands in an <strong class="source-inline">HDFS</strong> folder, as shown here: <a href="https://airflow.apache.org/docs/apache-airflow-providers-apache-hdfs/stable/_api/airflow/providers/apache/hdfs/sensors/hdfs/index.xhtml">https://airflow.apache.org/docs/apache-airflow-providers-apache-hdfs/stable/_api/airflow/providers/apache/hdfs/sensors/hdfs/index.xhtml</a>. As you might be wondering, there are also sensors for specific schedules or <span class="No-Break">time deltas.</span></p>
<p>Sensors are a fundamental part of creating an automated and event-driven pipeline. In this recipe, we will configure a <strong class="source-inline">weekday</strong> sensor, which executes our data pipeline on a specific day of <span class="No-Break">the week.</span></p>
<h2 id="_idParaDest-334"><a id="_idTextAnchor341"/>Getting ready</h2>
<p>Refer to the <em class="italic">Getting ready</em> section in the <em class="italic">Configuring Airflow</em> recipe for this recipe since we will handle it with the <span class="No-Break">same technology.</span></p>
<p>Besides that, let’s put a JSON file to the following path inside the Airflow <span class="No-Break">folder: </span><span class="No-Break"><strong class="source-inline">files_to_test/sensors_files/.</strong></span></p>
<p>In my case, I will use the <strong class="source-inline">github_events.json</strong> file, but you can use any of <span class="No-Break">your preferences.</span></p>
<h2 id="_idParaDest-335"><a id="_idTextAnchor342"/>How to do it…</h2>
<p>Here are the steps to perform <span class="No-Break">this recipe:</span></p>
<ol>
<li>Let’s start our DAG script by importing the required libraries, defining <strong class="source-inline">default_args</strong>, and instantiating our DAG, as you can <span class="No-Break">see here:</span><pre class="source-code">
from airflow import DAG
from airflow.settings import AIRFLOW_HOME
from airflow.operators.bash import BashOperator
from airflow.sensors.weekday import DayOfWeekSensor
from airflow.utils.weekday import WeekDay
from datetime import datetime, timedelta
default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 3, 22),
    'retry_delay': timedelta(minutes=5)
}
# Instantiate a DAG object
with DAG(
    dag_id='sensors_move_file',
    default_args=default_args,
    schedule_interval="@once",
) as dag:</pre></li>
<li>Now, let’s define our<a id="_idIndexMarker635"/> first task using <strong class="source-inline">DayOfWeekSensor</strong>. See the <span class="No-Break">code here:</span><pre class="source-code">
    move_file_on_saturdays = DayOfWeekSensor(
        task_id="move_file_on_saturdays",
        timeout=120,
        soft_fail=True,
        week_day=WeekDay.SATURDAY
    )</pre></li>
</ol>
<p>I suggest setting the day of the week as a parameter while doing this exercise to ensure no confusion. For example, if you want it to be executed on a Monday, set <strong class="source-inline">week_day</strong> to <strong class="source-inline">WeekDay.MONDAY</strong>, and <span class="No-Break">so on.</span></p>
<ol>
<li value="3">Then, we will define another task using <strong class="source-inline">BashOperator</strong>. This task will execute the command to move a JSON file from <strong class="source-inline">files_to_test/sensors_files/</strong> to <strong class="source-inline">files_to_test/output_files/</strong>. Your code should look <span class="No-Break">like this:</span><pre class="source-code">
    move_file_task = BashOperator(
            task_id="move_file_task",
            bash_command="mv $AIRFLOW_HOME/files_to_test/sensors_files/*.json $AIRFLOW_HOME/files_to_test/output_files/",
        )</pre></li>
<li>Then, we will define the <a id="_idIndexMarker636"/>execution workflow of our DAG, as you can <span class="No-Break">see here:</span><pre class="source-code">
move_file_on_saturdays.set_downstream(move_file_task)</pre></li>
</ol>
<p>The <strong class="source-inline">.set_downstream()</strong> function will work similarly to the double arrows (<strong class="source-inline">&gt;&gt;</strong>) we already used to define the workflow. You can read more about this <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow/1.10.3/concepts.xhtml?highlight=trigger#bitshift-composition"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/1.10.3/concepts.xhtml?highlight=trigger#bitshift-composition</span></a><span class="No-Break">.</span></p>
<ol>
<li value="5">As seen in the previous two recipes of this chapter, now we will enable our <strong class="source-inline">sensors_move_file</strong> DAG, which will start immediately. If you set the weekday as the same day on which you are executing this exercise, your DAG <strong class="bold">Graph</strong> view will look like this, <span class="No-Break">indicating success:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer244">
<img alt="Figure 9.24 – sensors_move_file tasks showing a success status" height="70" src="image/Figure_9.24_B19453.jpg" width="390"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.24 – sensors_move_file tasks showing a success status</p>
<ol>
<li value="6">Now, let’s see whether our file was moved to the directories. As described in the <em class="italic">Getting ready</em> section, I put a JSON file called <strong class="source-inline">github_events.json</strong> inside the <strong class="source-inline">sensor_files</strong> folder. Now, it will be inside <strong class="source-inline">output_files</strong>, as you can <span class="No-Break">see here:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer245">
<img alt="Figure 9.25 – github_events.json inside the output_files folder" height="104" src="image/Figure_9.25_B19453.jpg" width="177"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.25 – github_events.json inside the output_files folder</p>
<p>This indicates our sensor executed <span class="No-Break">as expected!</span></p>
<h2 id="_idParaDest-336"><a id="_idTextAnchor343"/>How it works…</h2>
<p>Sensors are valuable operators <a id="_idIndexMarker637"/>that execute an action based on a state. They can be triggered when a file lands in a directory, during the day, when an external task finishes, and so on. Here, we approached an example using a day of the week commonly used in data teams to change files from an ingested folder to a cold <span class="No-Break">storage folder.</span></p>
<p>Sensors count with an internal method called <strong class="source-inline">poke</strong>, which will check a resource’s status until the criteria are met. If you look at the <strong class="source-inline">move_file_on_saturday</strong> log, you will see something <span class="No-Break">like this:</span></p>
<pre class="source-code">
[2023-03-25, 00:19:03 UTC] {weekday.py:83} INFO - Poking until weekday is in WeekDay.SATURDAY, Today is SATURDAY
[2023-03-25, 00:19:03 UTC] {base.py:301} INFO - Success criteria met. Exiting.
[2023-03-25, 00:19:03 UTC] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=sensors_move_file, task_id=move_file_on_saturdays, execution_date=20230324T234623, start_date=20230325T001903, end_date=20230325T001903
[2023-03-25, 00:19:03 UTC] {local_task_job.py:156} INFO - Task exited with return code 0</pre>
<p>Looking at the following code, we did not define a <strong class="source-inline">reschedule</strong> parameter, so the job will stop until we manually trigger <span class="No-Break">it again:</span></p>
<pre class="source-code">
    move_file_on_saturdays = DayOfWeekSensor(
        task_id="move_file_on_saturdays",
        timeout=120,
        soft_fail=True,
        week_day=WeekDay.SATURDAY
    )</pre>
<p>Other parameters we <a id="_idIndexMarker638"/>defined were <strong class="source-inline">timeout</strong>, which indicates the time in seconds before it fails or stops retrying, and <strong class="source-inline">soft_fail</strong>, which marks the task as <strong class="source-inline">SKIPPED</strong> in the case <span class="No-Break">of failure.</span></p>
<p>You can see other allowed parameters <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/base/index.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/base/index.xhtml</span></a><span class="No-Break">.</span></p>
<p>And, of course, like the rest of the operators, we can create our custom sensor by extending the <strong class="source-inline">BaseSensorOperator</strong> class from Airflow. The main challenge here is that to be considered a sensor, it needs to overwrite the <strong class="source-inline">poke</strong> parameter without creating a recursing or <span class="No-Break">non-ending function.</span></p>
<h2 id="_idParaDest-337"><a id="_idTextAnchor344"/>See also</h2>
<p>You can see a list of the default Airflow sensors on the official documentation page <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/index.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/index.xhtml</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-338"><a id="_idTextAnchor345"/>Creating connectors in Airﬂow</h1>
<p>Having DAGs and operators without <a id="_idIndexMarker639"/>connecting to any external source is useless. Of course, there are many<a id="_idIndexMarker640"/> ways to ingest files, even from other DAGs or task results. Still, data ingestion usually involves using external sources such as APIs or databases as the first step of a <span class="No-Break">data pipeline.</span></p>
<p>To make this happen, in this recipe, we will understand how to create a connector in Airflow to connect to a <span class="No-Break">sample database.</span></p>
<h2 id="_idParaDest-339"><a id="_idTextAnchor346"/>Getting ready</h2>
<p>Refer to the <em class="italic">Getting ready</em> section of the <em class="italic">Configuring Airflow</em> recipe for this recipe since we will handle it with the <span class="No-Break">same</span><span class="No-Break"><a id="_idIndexMarker641"/></span><span class="No-Break"> technology.</span></p>
<p>This exercise will also require the<a id="_idIndexMarker642"/> MongoDB local database to be up and running. Ensure you have configured it as seen in <a href="B19453_01.xhtml#_idTextAnchor022"><span class="No-Break"><em class="italic">Chapter 1</em></span></a> and have at least one database and collection. You can use the instructions from the <em class="italic">Connecting to a NoSQL database (MongoDB)</em> recipe in <a href="B19453_05.xhtml#_idTextAnchor161"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-340"><a id="_idTextAnchor347"/>How to do it…</h2>
<p>Here are the steps to perform <span class="No-Break">this recipe:</span></p>
<ol>
<li>Let’s start by opening the Airflow UI. On the top menu, select the <strong class="bold">Admin</strong> button and then <strong class="bold">Connections</strong>, and you will be redirected to the <strong class="bold">Connections</strong> page. Since we haven’t configured anything yet, this page will be empty, as you can see in the <span class="No-Break">following screenshot:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer246">
<img alt="Figure 9.26 – The Connections page in the Airflow UI" height="319" src="image/Figure_9.26_B19453.jpg" width="826"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.26 – The Connections page in the Airflow UI</p>
<ol>
<li value="2">Then, click the <strong class="bold">+</strong> button to be redirected to the <strong class="bold">Add Connection</strong> page. Under the <strong class="bold">Connection Type</strong> field, search for and select <strong class="bold">MongoDB</strong>. Insert your connection values under the respective fields, as shown in the <span class="No-Break">following screenshot:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer247">
<img alt="Figure 9.27 – Creating a new connector in the Airflow UI" height="930" src="image/Figure_9.27_B19453.jpg" width="1004"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.27 – Creating a new connector in the Airflow UI</p>
<p>Click on the <strong class="bold">Save</strong> button, and you <a id="_idIndexMarker643"/>should have something similar to this on the <span class="No-Break"><strong class="bold">Connection</strong></span><span class="No-Break"> page:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer248">
<img alt="Figure 9.28 – The MongoDB connector created in the Airflow UI" height="85" src="image/Figure_9.28_B19453.jpg" width="1228"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.28 – The MongoDB connector created in the Airflow UI</p>
<ol>
<li value="3">Let’s create our new DAG using the same<a id="_idIndexMarker644"/> folder and file tree structure that we saw in the <em class="italic">Creating DAGs</em> recipe. I will call the <strong class="source-inline">mongodb_check_conn_dag.py</strong> <span class="No-Break">DAG file.</span></li>
<li>Inside the DAG file, let’s start by importing and declaring the required libraries and variables, as you can <span class="No-Break">see here:</span><pre class="source-code">
from airflow import DAG
from airflow.settings import AIRFLOW_HOME
from airflow.providers.mongo.hooks.mongo import MongoHook
from airflow.operators.python import PythonOperator
import os
import json
from datetime import datetime, timedelta
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 3, 22),
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}</pre></li>
<li>Now, we will create a function<a id="_idIndexMarker645"/> to connect with MongoDB<a id="_idIndexMarker646"/> locally and print <strong class="source-inline">collection</strong> <strong class="source-inline">reviews</strong> from the <strong class="source-inline">db_airbnb</strong> database, as you can <span class="No-Break">see here:</span><pre class="source-code">
def get_mongo_collection():
    hook = MongoHook(conn_id ='mongodb')
    client = hook.get_conn()
    print(client)
    print( hook.get_collection(mongo_collection="reviews", mongo_db="db_airbnb"))</pre></li>
<li>Then, let’s proceed with the <span class="No-Break">DAG object:</span><pre class="source-code">
# Instantiate a DAG object
with DAG(
    dag_id='mongodb_check_conn',
    default_args=default_args,
    schedule_interval=timedelta(days=1),
) as dag:</pre></li>
<li>Finally, let’s use <strong class="source-inline">PythonOperator</strong> to call <a id="_idIndexMarker647"/>our <strong class="source-inline">get_mongo_collection</strong> function defined in <span class="No-Break"><em class="italic">step 5</em></span><span class="No-Break">:</span><pre class="source-code">
    mongo_task = PythonOperator(
        task_id='mongo_task',
        python_callable=get_mongo_collection
    )</pre></li>
</ol>
<p>Don’t forget to put the name<a id="_idIndexMarker648"/> of your task in the indentation of the DAG, <span class="No-Break">as follows:</span></p>
<pre class="source-code">
mongo_task</pre>
<ol>
<li value="8">Heading to the Airflow UI, let’s enable the DAG and wait for it to be executed. After finishing successfully, your <strong class="source-inline">mongodb_task</strong> log should look <span class="No-Break">like this:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer249">
<img alt="Figure 9.29 – mongodb_task logs" height="555" src="image/Figure_9.29_B19453.jpg" width="1434"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.29 – mongodb_task logs</p>
<p>As you can see, we connected and retrieved the <strong class="source-inline">Collection</strong> object <span class="No-Break">from MongoDB.</span></p>
<h2 id="_idParaDest-341"><a id="_idTextAnchor348"/>How it works…</h2>
<p>Creating a connection in <a id="_idIndexMarker649"/>Airflow is straightforward, as demonstrated here using the UI. It is also possible to create connections programmatically using the <span class="No-Break"><strong class="source-inline">Connection</strong></span><span class="No-Break"> class.</span></p>
<p>After we set our MongoDB<a id="_idIndexMarker650"/> connection parameters, we needed to create a form to access it, and we did so using a hook. A <strong class="bold">hook</strong> is a high-level interface<a id="_idIndexMarker651"/> that allows connections to external sources without the need to be preoccupied with low-level code or <span class="No-Break">special libraries.</span></p>
<p>Remember that we configured an external package in the <em class="italic">Configuring Airflow</em> recipe? It was a provider to allow an easy connection <span class="No-Break">with MongoDB:</span></p>
<pre class="source-code">
from airflow.providers.mongo.hooks.mongo import MongoHook</pre>
<p>Inside the <strong class="source-inline">get_mongo_collection</strong> function, we instantiated <strong class="source-inline">MongoHook</strong> and passed the same connection ID name set in the <strong class="source-inline">Connection</strong> page, as you can <span class="No-Break">see here:</span></p>
<pre class="source-code">
hook = MongoHook(conn_id ='mongodb')</pre>
<p>With that instance, we can call the methods of the <strong class="source-inline">MongoHook</strong> class and even pass other parameters to configure the connection. See the documentation for this class <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/hooks/mongo/index.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/hooks/mongo/index.xhtml</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-342"><a id="_idTextAnchor349"/>There's more…</h2>
<p>You can also use the <strong class="source-inline">airflow.cfg</strong> file to set the connection strings or any other environment variable. It is a good practice to store sensitive information here, such as credentials, since they will not be shown in the UI. It is also possible to encrypt these values with <span class="No-Break">additional configuration.</span></p>
<p>For more information, see the documentation <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow/stable/howto/set-config.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/howto/set-config.xhtml</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-343"><a id="_idTextAnchor350"/>See also</h2>
<ul>
<li>You can learn about the MongoDB provider on the Airflow official documentation page <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/index.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/index.xhtml</span></a></li>
<li>If you are interested in reading more about connections, see this <span class="No-Break">link: </span><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.xhtml#custom-connection-types</span></li>
</ul>
<h1 id="_idParaDest-344"><a id="_idTextAnchor351"/>Creating parallel ingest tasks</h1>
<p>When working with data, we<a id="_idIndexMarker652"/> hardly ever just perform one ingestion at a time, and a real-world project involves many ingestions happening simultaneously, often in parallel. We know scheduling two or more DAGs to run alongside each other is possible, but what about tasks inside <span class="No-Break">one DAG?</span></p>
<p>This recipe will illustrate how to create parallel task execution <span class="No-Break">in Airflow.</span></p>
<h2 id="_idParaDest-345"><a id="_idTextAnchor352"/>Getting ready</h2>
<p>Please refer to the <em class="italic">Getting ready</em> section of the <em class="italic">Configuring Airflow</em> recipe for this recipe since we will handle it with the <span class="No-Break">same technology.</span></p>
<p>To avoid redundancy in this exercise, we won’t explicitly include the imports and main DAG configuration. Instead, the focus is on organizing the operator’s workflow. You can use the same logic to create your DAG structure as in the <em class="italic">Creating </em><span class="No-Break"><em class="italic">DAGs</em></span><span class="No-Break"> recipe.</span></p>
<p>For the complete Python file used here, go to the GitHub page <span class="No-Break">here: </span><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_parallel_ingest_tasks"><span class="No-Break">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_parallel_ingest_tasks</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-346"><a id="_idTextAnchor353"/>How to do it…</h2>
<p>For this exercise, my DAG will be called <strong class="source-inline">parallel_tasks_dag</strong>. Now, let’s <span class="No-Break">try it:</span></p>
<ol>
<li>Let’s start by creating five <strong class="source-inline">BashOperator</strong> instances, as you can <span class="No-Break">see here:</span><pre class="source-code">
# (DAG configuration above)
    t_0 = BashOperator(
            task_id="t_0",
            bash_command="echo 'This tasks will be executed first'",
        )
    t_1 = BashOperator(
            task_id="t_1",
            bash_command="echo 'This tasks no1 will be executed in parallel with t_2 and t_3'",
        )
    t_2 = BashOperator(
            task_id="t_2",
            bash_command="echo 'This tasks no2 will be executed in parallel with t_1 and t_3'",
        )
    t_3 = BashOperator(
            task_id="t_3",
            bash_command="echo 'This tasks no3 will be executed in parallel with t_1 and t_2'",
        )
    t_final = BashOperator(
        task_id="t_final",
        bash_command="echo 'Finished all tasks in parallel'",
    )</pre></li>
<li>The idea is for three <a id="_idIndexMarker653"/>of them to be executed in parallel so they will be inside square brackets. The first and last tasks will have the same workflow declared as we saw in the <em class="italic">Creating DAGs</em> recipe, using the <strong class="source-inline">&gt;&gt;</strong> character. The final flow structure will look <span class="No-Break">like this:</span><pre class="source-code">
t_0 &gt;&gt; [t_1, t_2, t_3] &gt;&gt; t_final</pre></li>
<li>Finally, enable your DAG, and let’s see what it looks like on the DAG <strong class="bold">Graph</strong> page. It would be best if you had something like the <span class="No-Break">following figure:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer250">
<img alt="Figure 9.30 – parallel_tasks_dag tasks in Airflow" height="277" src="image/Figure_9.30_B19453.jpg" width="260"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.30 – parallel_tasks_dag tasks in Airflow</p>
<p>As you can observe, the tasks inside the square brackets are displayed in parallel and will start after <strong class="source-inline">t_0</strong> finishes <span class="No-Break">its work.</span></p>
<h2 id="_idParaDest-347"><a id="_idTextAnchor354"/>How it works…</h2>
<p>Although creating parallel tasks inside a DAG is simple, this type of workflow arrangement is advantageous when working <span class="No-Break">with data.</span></p>
<p>Consider an example of data ingestion: we need to guarantee we have ingested all the desired endpoints before moving out to the next pipeline phase. See the following figure as <span class="No-Break">a reference:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer251">
<img alt="Figure 9.31 – Example of parallel execution" height="462" src="image/Figure_9.31_B19453.jpg" width="1012"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.31 – Example of parallel execution</p>
<p>The parallel execution will only move to the final task when all the parallel ones finish successfully. With this, we guarantee<a id="_idIndexMarker654"/> the data pipeline will not ingest only a small portion of the data but all the <span class="No-Break">required data.</span></p>
<p>Back to our exercise, we can simulate this <a id="_idIndexMarker655"/>behavior, creating an <strong class="bold">End-of-Line</strong> (<strong class="bold">EOL</strong>) error in <strong class="source-inline">t_2</strong> by removing one of the simple quotation marks. In the following figure, you can see what the DAG graph will <span class="No-Break">look like:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer252">
<img alt="Figure 9.32 – Airflow’s parallel_tasks_dag with an error t_2 task" height="283" src="image/Figure_9.32_B19453.jpg" width="256"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.32 – Airflow’s parallel_tasks_dag with an error t_2 task</p>
<p>The <strong class="source-inline">t_final</strong> task will retry executing until we fix <strong class="source-inline">t_2</strong> or the number of retries reaches <span class="No-Break">its limit.</span></p>
<p>However, avoiding many parallel tasks is a good practice, mainly if you have limited infrastructure resources to handle them. There are many ways to create dependency on external tasks or DAGs, and we can use them to make more <span class="No-Break">efficient pipelines.</span></p>
<h2 id="_idParaDest-348"><a id="_idTextAnchor355"/>There's more…</h2>
<p>Along with the concept of task<a id="_idIndexMarker656"/> parallelism, we have <strong class="source-inline">BranchOperator</strong>. <strong class="source-inline">BranchOperator</strong> executes one or more tasks simultaneously based on a criteria match. Let’s illustrate this with the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer253">
<img alt="Figure 9.33 – Branching task diagram example" height="404" src="image/Figure_9.33_B19453.jpg" width="1218"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.33 – Branching task diagram example</p>
<p>Based on the day-of-the-week criteria, the <strong class="source-inline">day_of_the_week_branch</strong> task will trigger a specific task assigned for <span class="No-Break">that day.</span></p>
<p>If you want to know more about it, <em class="italic">Analytics Vidhya</em> has a good blog post about it, which you can read <span class="No-Break">here: </span><a href="https://www.analyticsvidhya.com/blog/2023/01/data-engineering-101-branchpythonoperator-in-apache-airflow/"><span class="No-Break">https://www.analyticsvidhya.com/blog/2023/01/data-engineering-101-branchpythonoperator-in-apache-airflow/</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-349"><a id="_idTextAnchor356"/>See also</h2>
<ul>
<li>BetterDataScience has a good blog post about parallel tasks in Airflow. You can find it <span class="No-Break">here: </span><a href="https://betterdatascience.com/apache-airflow-run-tasks-in-parallel/"><span class="No-Break">https://betterdatascience.com/apache-airflow-run-tasks-in-parallel/</span></a><span class="No-Break">.</span></li>
<li>You can read more about Airflow task parallelism <span class="No-Break">here:</span><a href="https://hevodata.com/learn/airflow-parallelism/"><span class="No-Break"> </span><span class="No-Break">https://hevodata.com/learn/airflow-parallelism/</span></a><span class="No-Break">.</span></li>
</ul>
<h1 id="_idParaDest-350"><a id="_idTextAnchor357"/>Deﬁning ingest-dependent DAGs</h1>
<p>In the data world, considerable <a id="_idIndexMarker657"/>discussion exists about how to organize Airflow DAGs. The approach I generally use is to create a DAG for a specific pipeline based on the business logic or final destination. Nevertheless, sometimes, to proceed with a task inside a DAG, we depend on another DAG to finish the process and get <span class="No-Break">the output.</span></p>
<p>In this recipe, we will create two DAGs, where the first depends on the result of the second to be successful. Otherwise, it will not be completed. To assist us, we will use the <span class="No-Break"><strong class="source-inline">ExternalTaskSensor</strong></span><span class="No-Break"> operator.</span></p>
<h2 id="_idParaDest-351"><a id="_idTextAnchor358"/>Getting ready</h2>
<p>Please refer to the <em class="italic">Getting ready</em> section of the <em class="italic">Configuring Airflow</em> recipe for this recipe since we will handle it with the <span class="No-Break">same technology.</span></p>
<p>This recipe depends on the <strong class="source-inline">holiday_ingest</strong> DAG, created in the <em class="italic">Creating custom operators</em> recipe, so ensure you <span class="No-Break">have that.</span></p>
<p>We will not explicitly cite the imports and main DAG configuration to prevent redundancy and repetition in this exercise. The aim here is how to organize the operator’s workflow. You can use the same logic to create your DAG structure as in the <em class="italic">Creating </em><span class="No-Break"><em class="italic">DAGs</em></span><span class="No-Break"> recipe.</span></p>
<p>For the complete Python file used here, go to the GitHub <span class="No-Break">page here:</span></p>
<p><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/de%EF%AC%81ning_dependent_ingests_DAGs"><span class="No-Break">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/de%EF%AC%81ning_dependent_ingests_DAGs</span></a></p>
<h2 id="_idParaDest-352"><a id="_idTextAnchor359"/>How to do it…</h2>
<p>For this exercise, let’s create a DAG triggered when <strong class="source-inline">holiday_ingest</strong> finishes successfully, and returns all the holiday dates in the console output. My DAG will be called <strong class="source-inline">external_sensor_dag</strong>, but feel free to provide any other ID name. Just ensure it is unique and therefore will not impair <span class="No-Break">other DAGs:</span></p>
<ol>
<li>Along with the default imports we have, let’s add <span class="No-Break">the following:</span><pre class="source-code">
from airflow.sensors.external_task import ExternalTaskSensor</pre></li>
<li>Now, we will insert a Python function to return the holiday dates in the <strong class="source-inline">holiday_brazil.json</strong> file, which is the output of the <span class="No-Break"><strong class="source-inline">holiday_ingest</strong></span><span class="No-Break"> DAG:</span><pre class="source-code">
def get_holiday_dates(filename_json):
    with open (filename_json, 'r') as f:
        json_hol = json.load(f)
        holidays = json_hol["holidays"]
    print([item['date'] for item in holidays])</pre></li>
<li>Then, we will make the two <a id="_idIndexMarker658"/>operators of the DAG and define <span class="No-Break">the workflow:</span><pre class="source-code">
    wait_holiday_api_ingest = ExternalTaskSensor(
        task_id='wait_holiday_api_ingest',
        external_dag_id='holiday_ingest',
        external_task_id='holiday_api_ingestion',
        allowed_states=["success"],
        execution_delta = timedelta(minutes=1),
        timeout=600,
    )
    filename_json = f"{AIRFLOW_HOME}/files_to_test/output_files/holiday_brazil.json"
    date_tasks = PythonOperator(
        task_id='date_tasks',
        python_callable=get_holiday_dates,
        op_args=[filename_json]
    )
wait_holiday_api_ingest &gt;&gt; date_tasks</pre></li>
</ol>
<p>Save it and enable this DAG in the Airflow UI. Once enabled, you will notice the <strong class="source-inline">wait_holiday_api_ingest</strong> task will be in the <strong class="source-inline">RUNNING</strong> state and will not proceed to the other task, <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer254">
<img alt="Figure 9.34 – The wait_holiday_api_ingest task in the running state" height="66" src="image/Figure_9.34_B19453.jpg" width="355"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.34 – The wait_holiday_api_ingest task in the running state</p>
<p>You will also notice<a id="_idIndexMarker659"/> the log for this task looks like <span class="No-Break">the following:</span></p>
<pre class="source-code">
<strong class="bold">[2023-03-26, 20:50:23 UTC] {external_task.py:166} INFO - Poking for tasks ['holiday_api_ingestion'] in dag holiday_ingest on 2023-03-24T23:50:00+00:00 ...</strong>
<strong class="bold">[2023-03-26, 20:51:23 UTC] {external_task.py:166} INFO - Poking for tasks ['holiday_api_ingestion'] in dag holiday_ingest on 2023-03-24T23:50:00+00:00 ...</strong></pre>
<ol>
<li value="4">Now, we will enable and run <strong class="source-inline">holiday_ingest</strong> (if it is not <span class="No-Break">enabled yet).</span></li>
<li>Then, go back to <strong class="source-inline">external_sensor_dag</strong>, and you will see it finished successfully, as shown in the <span class="No-Break">following figure:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer255">
<img alt="Figure 9.35 – external_sensor_dag showing success" height="70" src="image/Figure_9.35_B19453.jpg" width="357"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.35 – external_sensor_dag showing success</p>
<p>If we examine the<a id="_idIndexMarker660"/> logs of <strong class="source-inline">date_tasks</strong>, you will see the following output on <span class="No-Break">the console:</span></p>
<pre class="source-code">
<strong class="bold">[2023-03-25, 16:39:31 UTC] {logging_mixin.py:115} INFO - ['2022-01-01', '2022-02-28', '2022-03-01', '2022-03-02', '2022-03-20', '2022-04-15', '2022-04-17', '2022-04-21', '2022-05-01', '2022-05-08', '2022-06-16', '2022-06-21', '2022-08-14', '2022-09-07', '2022-09-23', '2022-10-12', '2022-11-02', '2022-11-15', '2022-12-21', '2022-12-24', '2022-12-25', '2022-12-31']</strong></pre>
<p>Here is the complete log <span class="No-Break">for reference:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer256">
<img alt="Figure 9.36 – date_tasks logs in the Airflow UI" height="686" src="image/Figure_9.36_B19453.jpg" width="1444"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.36 – date_tasks logs in the Airflow UI</p>
<p>Now, let’s understand how it works in the <span class="No-Break">next section.</span></p>
<h2 id="_idParaDest-353"><a id="_idTextAnchor360"/>How it works…</h2>
<p>Let’s start by taking a<a id="_idIndexMarker661"/> look at our <span class="No-Break"><strong class="source-inline">wait_holiday_api_ingest</strong></span><span class="No-Break"> task:</span></p>
<pre class="source-code">
wait_holiday_api_ingest = ExternalTaskSensor(
        task_id='wait_holiday_api_ingest',
        external_dag_id='holiday_ingest',
        external_task_id='holiday_api_ingestion',
        allowed_states=["success"],
        execution_delta = timedelta(minutes=1),
        timeout=300,
    )</pre>
<p><strong class="source-inline">ExternalTaskSensor</strong> is a sensor type that will only execute if another task outside its DAG finishes with a specific status defined on the <strong class="source-inline">allowed_states</strong> parameter. The default value for this parameter <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">SUCCESS</strong></span><span class="No-Break">.</span></p>
<p>The sensor will search for a specific DAG and task in Airflow using the <strong class="source-inline">external_dag_id</strong> and <strong class="source-inline">external_task_id</strong> parameters, which we have defined as <strong class="source-inline">holiday_ingest</strong> and <strong class="source-inline">holiday_api_ingestion</strong>, respectively. Finally, <strong class="source-inline">execution_delta</strong> will determine the time interval at which to poke the external <span class="No-Break">DAG again.</span></p>
<p>Once it finishes, the DAG will remain in the <strong class="source-inline">SUCCESS</strong> state unless we define a different behavior in the default arguments. If we clear its status, it will return to the <strong class="source-inline">RUNNING</strong> mode until the sensor criteria are <span class="No-Break">met again.</span></p>
<h2 id="_idParaDest-354"><a id="_idTextAnchor361"/>There's more…</h2>
<p>We know Airflow is a powerful tool, but it is not immune from occasional failures. Internally, Airflow has its routes to reach internal and external DAGs, which can occasionally fail. For example, one of these errors might be a DAG not being found, which can happen due to various reasons such as misconfiguration or connectivity issues. You can see a screenshot of one of these <span class="No-Break">errors here:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer257">
<img alt="Figure 9.37 – Occasional 403 error log in an Airflow task" height="219" src="image/Figure_9.37_B19453.jpg" width="1445"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.37 – Occasional 403 error log in an Airflow task</p>
<p>Looking closely, we<a id="_idIndexMarker662"/> can observe that for several seconds, one of the Airflow workers lost permission to access or retrieve information from another worker. If this happens to you, disable your DAG and enable <span class="No-Break">it again.</span></p>
<h3>XCom</h3>
<p>In this exercise, we used an output file to perform an action, but we can also use the output of a task without requiring it to write a file somewhere. Instead, we can use the <strong class="bold">XCom</strong> (short for <strong class="bold">cross-communications</strong>) mechanism<a id="_idIndexMarker663"/> to help us <span class="No-Break">with it.</span></p>
<p>To use XCom across tasks, we can simply use <em class="italic">push</em> and <em class="italic">pull</em> the values using the <strong class="source-inline">xcom_push</strong> and <strong class="source-inline">xcom_pull</strong> methods inside the required tasks. Behind the scenes, Airflow stores those values temporarily in one of its databases, making it easier to access <span class="No-Break">them again.</span></p>
<p>To check your stored XComs in the Airflow UI, click on the <strong class="bold">Admin</strong> button and <span class="No-Break">select </span><span class="No-Break"><strong class="bold">XCom</strong></span><span class="No-Break">.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">In production environments, XComs might have a purge routine. Check with the Airflows administrators if you need to keep a value <span class="No-Break">for longer.</span></p>
<p>You can read more about XComs on the official documentation page <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.xhtml</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-355"><a id="_idTextAnchor362"/>See also</h2>
<p>You can learn more about this<a id="_idIndexMarker664"/> operator on the Airflow official documentation <span class="No-Break">page: </span><a href="https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/external_task_sensor.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/external_task_sensor.xhtml</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-356"><a id="_idTextAnchor363"/>Further reading</h1>
<ul>
<li><a href="https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.xhtml</span></a></li>
<li><a href="https://airflow.apache.org/docs/apache-airflow/2.2.4/best-practices.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/2.2.4/best-practices.xhtml</span></a></li>
<li><a href="https://www.qubole.com/tech-blog/apache-airflow-tutorial-dags-tasks-operators-sensors-hooks-xcom"><span class="No-Break">https://www.qubole.com/tech-blog/apache-airflow-tutorial-dags-tasks-operators-sensors-hooks-xcom</span></a></li>
<li><a href="https://python.plainenglish.io/apache-airflow-how-to-correctly-setup-custom-plugins-2f80fe5e3dbe"><span class="No-Break">https://python.plainenglish.io/apache-airflow-how-to-correctly-setup-custom-plugins-2f80fe5e3dbe</span></a></li>
<li><a href="https://copyprogramming.com/howto/airflow-how-to-mount-airflow-cfg-in-docker-container"><span class="No-Break">https://copyprogramming.com/howto/airflow-how-to-mount-airflow-cfg-in-docker-container</span></a></li>
</ul>
</div>
</div></body></html>