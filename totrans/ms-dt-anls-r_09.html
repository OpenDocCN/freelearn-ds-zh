<html><head></head><body><div class="chapter" title="Chapter&#xA0;9.&#xA0;From Big to Small Data"><div class="titlepage"><div><div><h1 class="title"><a id="ch09"/>Chapter 9. From Big to Small Data</h1></div></div></div><p>Now that we have some cleansed data ready for analysis, let's first see how we can find our way around the high number of variables in our dataset. This chapter will introduce some statistical techniques to<a class="indexterm" id="id626"/> reduce the number of variables by dimension reduction and feature extraction, such as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Principal</strong></span><a class="indexterm" id="id627"/><span class="strong"><strong> Component Analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>)</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Factor</strong></span><a class="indexterm" id="id628"/><span class="strong"><strong> Analysis</strong></span> (<span class="strong"><strong>FA</strong></span>)</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Multidimensional Scaling</strong></span> (<span class="strong"><strong>MDS</strong></span>) and a<a class="indexterm" id="id629"/> few other techniques</li></ul></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note59"/>Note</h3><p>Most dimension reduction<a class="indexterm" id="id630"/> methods require that two or more numeric variables in the dataset are highly associated or correlated, so the columns in our matrix are not totally independent of each other. In such a situation, the goal of dimension reduction is to decrease the number of columns in the dataset to the actual matrix rank; or, in other words, the number of variables can be decreased whilst most of the information content can be retained. In linear algebra, the matrix rank refers to the dimensions of the vector space generated by the matrix—or, in simpler terms, the number of independent columns and rows in a quadratic matrix. Probably it's easier to understand rank by a quick example: imagine a dataset on students where we know the gender, the age, and the date of birth of respondents. This data is redundant as the age can be computed (via a linear transformation) from the date of birth. Similarly, the year variable is static (without any variability) in the <code class="literal">hflights</code> dataset, and the elapsed time can be also computed by the departure and arrival times.</p></div></div><p>These transformations basically concentrate on the common variance identified among the variables and exclude the remaining total (unique) variance. This results in a dataset with fewer columns, which is probably easier to maintain and process, but at the cost of some information loss and the creation of artificial variables, which are usually harder to comprehend compared to the original columns.</p><p>In the case of perfect dependence, all but one of the perfectly correlated variables can be omitted, as the rest provide no additional information about the dataset. Although it does not happen often, in most cases it's still totally acceptable to keep only one or a few components extracted from a set of questions, for example in a survey for further analysis.</p><div class="section" title="Adequacy tests"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec62"/>Adequacy tests</h1></div></div></div><p>The first thing you <a class="indexterm" id="id631"/>want to do, when thinking about reducing the number of dimensions or looking for latent variables in the dataset with multivariate statistical analysis, is to check whether the variables are correlated and the data is normally distributed.</p><div class="section" title="Normality"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec54"/>Normality</h2></div></div></div><p>The latter is often not a <a class="indexterm" id="id632"/>strict requirement. For example, the results of a PCA can be still valid and interpreted if we do not have multivariate normality; on<a class="indexterm" id="id633"/> the other hand, maximum likelihood factor <a class="indexterm" id="id634"/>analysis does have this strong assumption.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip12"/>Tip</h3><p>You should always use the appropriate methods to achieve your data analysis goals, based on the characteristics of your data.</p></div></div><p>Anyway, you can use (for example) <code class="literal">qqplot</code> to do a pair-wise comparison of variables, and <code class="literal">qqnorm</code> to do univariate normality tests of your variables. First, let's demonstrate this with a subset of <code class="literal">hflights</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; library(hlfights)</strong></span>
<span class="strong"><strong>&gt; JFK &lt;- hflights[which(hflights$Dest == 'JFK'),</strong></span>
<span class="strong"><strong>+                 c('TaxiIn', 'TaxiOut')]</strong></span>
</pre></div><p>So we filter our dataset to only those flights heading to the John F. Kennedy International Airport and we are interested in only two variables describing how long the taxiing in and out times were in minutes. The preceding command with the traditional <code class="literal">[</code> indexing can be refactored with <code class="literal">subset</code> for much more readable source code:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; JFK &lt;- subset(hflights, Dest == 'JFK', select = c(TaxiIn, TaxiOut))</strong></span>
</pre></div><p>Please note that now there's no need to quote variable names or refer to the <code class="literal">data.frame</code> name inside the <code class="literal">subset</code> call. For more details on this, please see <a class="link" href="ch03.html" title="Chapter 3. Filtering and Summarizing Data">Chapter 3</a>, <span class="emphasis"><em>Filtering and Summarizing Data</em></span>. And now let's see how the values of these two columns are distributed:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; par(mfrow = c(1, 2))</strong></span>
<span class="strong"><strong>&gt; qqnorm(JFK$TaxiIn, ylab = 'TaxiIn')</strong></span>
<span class="strong"><strong>&gt; qqline(JFK$TaxiIn)</strong></span>
<span class="strong"><strong>&gt; qqnorm(JFK$TaxiOut, ylab = 'TaxiOut')</strong></span>
<span class="strong"><strong>&gt; qqline(JFK$TaxiOut)</strong></span>
</pre></div><div class="mediaobject"><img alt="Normality" src="graphics/2028OS_09_01.jpg"/></div><p>To render the<a class="indexterm" id="id635"/> preceding plot, we created a new graphical device (with <code class="literal">par</code> to hold two plots in a row), then called <code class="literal">qqnorm</code>, to show the quantiles of the <a class="indexterm" id="id636"/>empirical variables against the normal distribution, and also added a line for the latter with <code class="literal">qqline</code> for easier comparison. If the data was scaled previously, <code class="literal">qqline</code> would render a 45-degree line.</p><p>Checking the QQ-plots <a class="indexterm" id="id637"/>suggest that the data does not fit the normal distribution very well, which can be also verified by an analytical test such as the Shapiro-Wilk normality test:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; shapiro.test(JFK$TaxiIn)</strong></span>

<span class="strong"><strong>  Shapiro-Wilk normality test</strong></span>

<span class="strong"><strong>data:  JFK$TaxiIn</strong></span>
<span class="strong"><strong>W = 0.8387, p-value &lt; 2.2e-16</strong></span>
</pre></div><p>The <code class="literal">p-value</code> is really small, so the null hypothesis (stating that the data is normally distributed) is rejected. But how can we test normality for a bunch of variables without and beyond separate statistical tests?</p></div><div class="section" title="Multivariate normality"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec55"/>Multivariate normality</h2></div></div></div><p>Similar statistical tests exist for multiple variables as well; these methods provide different ways to check if the data<a class="indexterm" id="id638"/> fits the multivariate normal distribution. To this end, we will use the <code class="literal">MVN</code> package, but similar methods can be also found in the <code class="literal">mvnormtest</code> package<a class="indexterm" id="id639"/>. The latter includes the multivariate version of the previously discussed <a class="indexterm" id="id640"/>Shapiro-Wilk test as well.</p><p>But Mardia's test is <a class="indexterm" id="id641"/>more often<a class="indexterm" id="id642"/> used to check multivariate normality and, even better, it does not limit the sample size to below 5,000. After loading the <code class="literal">MVN</code> package<a class="indexterm" id="id643"/>, calling the appropriate R function is pretty straightforward with a very intuitive interpretation—after getting rid of the missing values in our dataset:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; JFK &lt;- na.omit(JFK)</strong></span>
<span class="strong"><strong>&gt; library(MVN)</strong></span>
<span class="strong"><strong>&gt; mardiaTest(JFK)</strong></span>
<span class="strong"><strong>   Mardia's Multivariate Normality Test </strong></span>
<span class="strong"><strong>--------------------------------------- </strong></span>
<span class="strong"><strong>   data : JFK </strong></span>

<span class="strong"><strong>   g1p            : 20.84452 </strong></span>
<span class="strong"><strong>   chi.skew       : 2351.957 </strong></span>
<span class="strong"><strong>   p.value.skew   : 0 </strong></span>

<span class="strong"><strong>   g2p            : 46.33207 </strong></span>
<span class="strong"><strong>   z.kurtosis     : 124.6713 </strong></span>
<span class="strong"><strong>   p.value.kurt   : 0 </strong></span>

<span class="strong"><strong>   chi.small.skew : 2369.368 </strong></span>
<span class="strong"><strong>   p.value.small  : 0 </strong></span>

<span class="strong"><strong>   Result          : Data is not multivariate normal. </strong></span>
<span class="strong"><strong>---------------------------------------</strong></span>
</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip13"/>Tip</h3><p>For more details on handling and filtering missing values, please see <a class="link" href="ch08.html" title="Chapter 8. Polishing Data">Chapter 8</a>, <span class="emphasis"><em>Polishing Data</em></span>.</p></div></div><p>Out of the three p values, the third one refers to cases when the sample size is extremely small (&lt;20), so now we only concentrate on the first two values, both below 0.05. This means that the data does not seem to be multivariate normal. Unfortunately, Mardia's<a class="indexterm" id="id644"/> test fails to perform well in some cases, so more robust methods might be more appropriate to use.</p><p>The <code class="literal">MVN</code> package <a class="indexterm" id="id645"/>can run the Henze-Zirkler's and Royston's Multivariate Normality Test as well. Both return user-friendly and easy to interpret results:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; hzTest(JFK)</strong></span>
<span class="strong"><strong>  Henze-Zirkler's Multivariate Normality Test </strong></span>
<span class="strong"><strong>--------------------------------------------- </strong></span>
<span class="strong"><strong>  data : JFK </strong></span>

<span class="strong"><strong>  HZ      : 42.26252 </strong></span>
<span class="strong"><strong>  p-value : 0 </strong></span>

<span class="strong"><strong>  Result  : Data is not multivariate normal. </strong></span>
<span class="strong"><strong>--------------------------------------------- </strong></span>

<span class="strong"><strong>&gt; roystonTest(JFK)</strong></span>
<span class="strong"><strong>  Royston's Multivariate Normality Test </strong></span>
<span class="strong"><strong>--------------------------------------------- </strong></span>
<span class="strong"><strong>  data : JFK </strong></span>

<span class="strong"><strong>  H       : 264.1686 </strong></span>
<span class="strong"><strong>  p-value : 4.330916e-58 </strong></span>

<span class="strong"><strong>  Result  : Data is not multivariate normal. </strong></span>
<span class="strong"><strong>---------------------------------------------</strong></span>
</pre></div><p>A more visual method to test <a class="indexterm" id="id646"/>multivariate normality is to render similar QQ plots to those we used before. But, instead of comparing only one <a class="indexterm" id="id647"/>variable with the theoretical normal distribution, let's first compute the squared Mahalanobis distance between our variables, which should follow a chi-square distribution with the degrees of freedom being the number of our variables. The<a class="indexterm" id="id648"/> <code class="literal">MVN</code> package can automatically compute all the required values and render those with any of the preceding normality test R functions; just set the <code class="literal">qqplot</code> argument to be <code class="literal">TRUE</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; mvt &lt;- roystonTest(JFK, qqplot = TRUE)</strong></span>
</pre></div><div class="mediaobject"><img alt="Multivariate normality" src="graphics/2028OS_09_02.jpg"/></div><p>If the dataset was <a class="indexterm" id="id649"/>normally distributed, the points shown in the preceding graphs should fit the straight line. Other alternative graphical methods can<a class="indexterm" id="id650"/> produce more visual and user-friendly plots with the previously created <code class="literal">mvt</code> R object. The <code class="literal">MVN</code> package ships the <code class="literal">mvnPlot</code> function, which can render perspective and contour plots for two variables and thus provides a nice way to test bivariate normality:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; par(mfrow = c(1, 2))</strong></span>
<span class="strong"><strong>&gt; mvnPlot(mvt, type = "contour", default = TRUE)</strong></span>
<span class="strong"><strong>&gt; mvnPlot(mvt, type = "persp", default = TRUE)</strong></span>
</pre></div><div class="mediaobject"><img alt="Multivariate normality" src="graphics/2028OS_09_03.jpg"/></div><p>On the right plot, you can see the empirical distribution of the two variables on a perspective plot, where most cases can be found in the bottom-left corner. This means that most flights had only relatively short<a class="indexterm" id="id651"/> <span class="strong"><strong>TaxiIn</strong></span> and <span class="strong"><strong>TaxiOut</strong></span> times, which suggests a rather heavy-tailed <a class="indexterm" id="id652"/>distribution. The left plot shows a similar<a class="indexterm" id="id653"/> image, but from a bird's eye view: the contour lines<a class="indexterm" id="id654"/> represent a cross-section of the right-hand side 3D graph. Multivariate normal distribution looks more central, something like a 2-dimensional bell curve:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; set.seed(42)</strong></span>
<span class="strong"><strong>&gt; mvt &lt;- roystonTest(MASS::mvrnorm(100, mu = c(0, 0),</strong></span>
<span class="strong"><strong>+          Sigma = matrix(c(10, 3, 3, 2), 2)))</strong></span>
<span class="strong"><strong>&gt; mvnPlot(mvt, type = "contour", default = TRUE)</strong></span>
<span class="strong"><strong>&gt; mvnPlot(mvt, type = "persp", default = TRUE)</strong></span>
</pre></div><div class="mediaobject"><img alt="Multivariate normality" src="graphics/2028OS_09_04.jpg"/></div><p>See <a class="link" href="ch13.html" title="Chapter 13. Data Around Us">Chapter 13</a>, <span class="emphasis"><em>Data Around Us</em></span> on how to create similar contour maps on spatial data.</p></div><div class="section" title="Dependence of variables"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec56"/>Dependence of variables</h2></div></div></div><p>Besides<a class="indexterm" id="id655"/> normality, relatively high correlation coefficients are desired when applying <a class="indexterm" id="id656"/>dimension reduction methods. The reason is that, if there is no statistical relationship between the variables, for <a class="indexterm" id="id657"/>example, PCA will return the exact same values without much transformation.</p><p>To this end, let's see how the numerical variables of the <code class="literal">hflights</code> dataset are correlated (the output, being a large matrix, is suppressed this time):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; hflights_numeric &lt;- hflights[, which(sapply(hflights, is.numeric))]</strong></span>
<span class="strong"><strong>&gt; cor(hflights_numeric, use = "pairwise.complete.obs")</strong></span>
</pre></div><p>In the preceding example, we have created a new R object to hold only the numeric columns of the original <code class="literal">hflights</code> data frame, leaving out five character vectors. Then, we run <code class="literal">cor</code> with pair-wise deletion of missing values, which returns a matrix with 16 columns and 16 rows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; str(cor(hflights_numeric, use = "pairwise.complete.obs"))</strong></span>
<span class="strong"><strong> num [1:16, 1:16] NA NA NA NA NA NA NA NA NA NA ...</strong></span>
<span class="strong"><strong> - attr(*, "dimnames")=List of 2</strong></span>
<span class="strong"><strong> ..$ : chr [1:16] "Year" "Month" "DayofMonth" "DayOfWeek" ...</strong></span>
<span class="strong"><strong> ..$ : chr [1:16] "Year" "Month" "DayofMonth" "DayOfWeek" ...</strong></span>
</pre></div><p>The number of missing values in the resulting correlation matrix seems to be very high. This is because <code class="literal">Year</code> was 2011 in all cases, thus resulting in a standard variation of zero. It's wise to exclude <code class="literal">Year</code> along with the non-numeric variables from the dataset—by not only filtering for numeric values, but also checking the variance:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; hflights_numeric &lt;- hflights[,which(</strong></span>
<span class="strong"><strong>+     sapply(hflights, function(x)</strong></span>
<span class="strong"><strong>+         is.numeric(x) &amp;&amp; var(x, na.rm = TRUE) != 0))]</strong></span>
</pre></div><p>Now the number of missing values is a lot lower:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; table(is.na(cor(hflights_numeric, use = "pairwise.complete.obs")))</strong></span>
<span class="strong"><strong>FALSE  TRUE </strong></span>
<span class="strong"><strong>  209    16</strong></span>
</pre></div><p>Can you guess why we still have some missing values here despite the pair-wise deletion of missing values? Well, running the preceding command results in a rather informative warning, but we will get back to this question later:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>Warning message:</strong></span>
<span class="strong"><strong>In cor(hflights_numeric, use = "pairwise.complete.obs") :</strong></span>
<span class="strong"><strong>  the</strong></span><span class="strong"><strong> standard deviation is zero</strong></span>
</pre></div><p>Let's now proceed with analyzing the actual numbers in the 15x15 correlation matrix, which would be way too large to print in this book. To this end, we did not show the result of the original <code class="literal">cor</code> command shown previously, but instead, let's rather visualize those 225 numbers with the graphical capabilities of the<a class="indexterm" id="id658"/> <code class="literal">ellipse</code> package:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; library(ellipse)</strong></span>
<span class="strong"><strong>&gt; plotcorr(cor(hflights_numeric, use = "pairwise.complete.obs"))</strong></span>
</pre></div><div class="mediaobject"><img alt="Dependence of variables" src="graphics/2028OS_09_05.jpg"/></div><p>Now we see the <a class="indexterm" id="id659"/>values of the correlation matrix represented by ellipses, where:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A perfect circle <a class="indexterm" id="id660"/>stands for the correlation coefficient of zero</li><li class="listitem" style="list-style-type: disc">Ellipses with a smaller area reflect the relatively large distance of the correlation coefficient from zero</li><li class="listitem" style="list-style-type: disc">The tangent represents the negative/positive sign of the coefficient</li></ul></div><p>To help you with analyzing the preceding results, let's render a similar plot with a few artificially generated numbers that are easier to interpret:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; plotcorr(cor(data.frame(</strong></span>
<span class="strong"><strong>+     1:10,</strong></span>
<span class="strong"><strong>+     1:10 + runif(10),</strong></span>
<span class="strong"><strong>+     1:10 + runif(10) * 5,</strong></span>
<span class="strong"><strong>+     runif(10),</strong></span>
<span class="strong"><strong>+     10:1,</strong></span>
<span class="strong"><strong>+     check.names = FALSE)))</strong></span>
</pre></div><div class="mediaobject"><img alt="Dependence of variables" src="graphics/2028OS_09_06.jpg"/></div><p>Similar plots on<a class="indexterm" id="id661"/> the correlation matrix can be created with the<a class="indexterm" id="id662"/> <code class="literal">corrgram</code> package.</p><p>But let's get back to<a class="indexterm" id="id663"/> the <code class="literal">hflights</code> dataset! On the previous diagram, some narrow ellipses are rendered for the time-related variables, which show a relatively high correlation coefficient, and even the <code class="literal">Month</code> variable seems to be slightly associated with the <code class="literal">FlightNum</code> function:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; cor(hflights$FlightNum, hflights$Month)</strong></span>
<span class="strong"><strong>[1] 0.2057641</strong></span>
</pre></div><p>On the other hand, the plot shows perfect circles in most cases, which stand for a correlation coefficient around zero. This suggests that most variables are not correlated at all, so computing the principal components of the original dataset would not be very helpful due to the low<a class="indexterm" id="id664"/> proportion of common variance.</p></div><div class="section" title="KMO and Barlett's test"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec57"/>KMO and Barlett's test</h2></div></div></div><p>We can verify this assumption on low communalities by a number of statistical tests; for example, the SAS and <a class="indexterm" id="id665"/>SPSS folks tend to use KMO or Bartlett's test to see if the data<a class="indexterm" id="id666"/> is suitable for PCA. Both algorithms are available in R as well via, for example, via the <a class="indexterm" id="id667"/>
<code class="literal">psych</code> package:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; library(psych)</strong></span>
<span class="strong"><strong>&gt; KMO(cor(hflights_numeric, use = "pairwise.complete.obs"))</strong></span>
<span class="strong"><strong>Error in solve.default(r) : </strong></span>
<span class="strong"><strong> system is computationally singular: reciprocal condition number = 0</strong></span>
<span class="strong"><strong>In addition: Warning message:</strong></span>
<span class="strong"><strong>In cor(hflights_numeric, use = "pairwise.complete.obs") :</strong></span>
<span class="strong"><strong> the standard deviation is zero</strong></span>
<span class="strong"><strong>matrix is not invertible, image not found</strong></span>
<span class="strong"><strong>Kaiser-Meyer-Olkin factor adequacy</strong></span>
<span class="strong"><strong>Call: KMO(r = cor(hflights_numeric, use = "pairwise.complete.obs"))</strong></span>
<span class="strong"><strong>Overall MSA = NA</strong></span>
<span class="strong"><strong>MSA for each item = </strong></span>
<span class="strong"><strong>      Month    DayofMonth     DayOfWeek </strong></span>
<span class="strong"><strong>       0.5        0.5        0.5 </strong></span>
<span class="strong"><strong>     DepTime      ArrTime     FlightNum </strong></span>
<span class="strong"><strong>       0.5        NA        0.5 </strong></span>
<span class="strong"><strong>ActualElapsedTime      AirTime     ArrDelay </strong></span>
<span class="strong"><strong>        NA        NA        NA </strong></span>
<span class="strong"><strong>     DepDelay     Distance      TaxiIn </strong></span>
<span class="strong"><strong>       0.5        0.5        NA </strong></span>
<span class="strong"><strong>     TaxiOut     Cancelled     Diverted </strong></span>
<span class="strong"><strong>       0.5        NA        NA</strong></span>
</pre></div><p>Unfortunately, the <code class="literal">Overall MSA</code> (<span class="emphasis"><em>Measure of Sampling Adequacy</em></span>, representing the average correlations between the variables) is not available in the preceding output due to the previously identified missing values of the correlation matrix. Let's pick a pair of variables where the correlation coefficient was <code class="literal">NA</code> for further analysis! Such a pair can be easily identified from the previous plot; no circle or ellipse was drawn for missing values, for example, for <code class="literal">Cancelled</code> and <code class="literal">AirTime</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; cor(hflights_numeric[, c('Cancelled', 'AirTime')])</strong></span>
<span class="strong"><strong>          Cancelled AirTime</strong></span>
<span class="strong"><strong>Cancelled         1      NA</strong></span>
<span class="strong"><strong>AirTime          NA       1</strong></span>
</pre></div><p>This can be explained by the fact, that if a flight is cancelled, then the time spent in the air does not vary much; furthermore, this data is not available:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; cancelled &lt;- which(hflights_numeric$Cancelled == 1)</strong></span>
<span class="strong"><strong>&gt; table(hflights_numeric$AirTime[cancelled], exclude = NULL)</strong></span>
<span class="strong"><strong>&lt;NA&gt; </strong></span>
<span class="strong"><strong>2973</strong></span>
</pre></div><p>So we get missing values when calling <code class="literal">cor</code> due to these <code class="literal">NA</code>; similarly, we also get <code class="literal">NA</code> when calling <code class="literal">cor</code> with pair-wise deletion, as only the non-cancelled flights remain in the dataset, resulting<a class="indexterm" id="id668"/> in zero variance for the <code class="literal">Cancelled</code> variable:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; table(hflights_numeric$Cancelled)</strong></span>
<span class="strong"><strong>     0      1 </strong></span>
<span class="strong"><strong>224523   2973</strong></span>
</pre></div><p>This suggests removing the <code class="literal">Cancelled</code> variable from the dataset before we run the previously discussed<a class="indexterm" id="id669"/> assumption tests, as the information stored in that variable is redundantly available in other columns of the dataset as well. Or, in other words, the <code class="literal">Cancelled</code> column can be computed by a linear transformation of the other columns, which can be left out from further analysis:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; hflights_numeric &lt;- subset(hflights_numeric, select = -Cancelled)</strong></span>
</pre></div><p>And let's see if we still have any missing values in the correlation matrix:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; which(is.na(cor(hflights_numeric, use = "pairwise.complete.obs")),</strong></span>
<span class="strong"><strong>+   arr.ind = TRUE)</strong></span>
<span class="strong"><strong>                  row col</strong></span>
<span class="strong"><strong>Diverted           14   7</strong></span>
<span class="strong"><strong>Diverted           14   8</strong></span>
<span class="strong"><strong>Diverted           14   9</strong></span>
<span class="strong"><strong>ActualElapsedTime   7  14</strong></span>
<span class="strong"><strong>AirTime             8  14</strong></span>
<span class="strong"><strong>ArrDelay            9  14</strong></span>
</pre></div><p>It seems that the <code class="literal">Diverted</code> column is responsible for a similar situation, and the other three variables were not available when the flight was diverted. After another subset, we are now ready to call KMO on a full correlation matrix:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; hflights_numeric &lt;- subset(hflights_numeric, select = -Diverted)</strong></span>
<span class="strong"><strong>&gt; KMO(cor(hflights_numeric[, -c(14)], use = "pairwise.complete.obs"))</strong></span>
<span class="strong"><strong>Kaiser-Meyer-Olkin factor adequacy</strong></span>
<span class="strong"><strong>Call: KMO(r = cor(hflights_numeric[, -c(14)], use = "pairwise.complete.obs"))</strong></span>
<span class="strong"><strong>Overall MSA =  0.36</strong></span>
<span class="strong"><strong>MSA for each item = </strong></span>
<span class="strong"><strong>            Month        DayofMonth         DayOfWeek </strong></span>
<span class="strong"><strong>             0.42              0.37              0.35 </strong></span>
<span class="strong"><strong>          DepTime           ArrTime         FlightNum </strong></span>
<span class="strong"><strong>             0.51              0.49              0.74 </strong></span>
<span class="strong"><strong>ActualElapsedTime           AirTime          ArrDelay </strong></span>
<span class="strong"><strong>             0.40              0.40              0.39 </strong></span>
<span class="strong"><strong>         DepDelay          Distance            TaxiIn </strong></span>
<span class="strong"><strong>             0.38              0.67              0.06 </strong></span>
<span class="strong"><strong>          TaxiOut </strong></span>
<span class="strong"><strong>             0.06</strong></span>
</pre></div><p>The <code class="literal">Overall MSA</code>, or the so called<a class="indexterm" id="id670"/> <span class="strong"><strong>Kaiser-Meyer-Olkin</strong></span> (<span class="strong"><strong>KMO</strong></span>) index, is a number between 0 and 1; this value suggests whether the partial correlations of the variables are small enough to continue with data reduction methods. A general rating system or rule of a thumb for KMO<a class="indexterm" id="id671"/> can be found in the following table, as suggested by Kaiser:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Value</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>KMO &lt; 0.5</p>
</td><td style="text-align: left" valign="top">
<p>Unacceptable</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.5 &lt; KMO &lt; 0.6</p>
</td><td style="text-align: left" valign="top">
<p>Miserable</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.6 &lt; KMO &lt; 0.7</p>
</td><td style="text-align: left" valign="top">
<p>Mediocre</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.7 &lt; KMO &lt; 0.8</p>
</td><td style="text-align: left" valign="top">
<p>Middling</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.8 &lt; KMO &lt; 0.9</p>
</td><td style="text-align: left" valign="top">
<p>Meritorious</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>KMO &gt; 0.9</p>
</td><td style="text-align: left" valign="top">
<p>Marvelous</p>
</td></tr></tbody></table></div><p>The KMO index being below 0.5 is considered unacceptable, which basically means that the partial correlation computed from the correlation matrix suggests that the variables are not correlated enough for a meaningful dimension reduction or latent variable model.</p><p>Although leaving out <a class="indexterm" id="id672"/>some variables with the lowest MSA would improve the <code class="literal">Overall MSA</code>, and we could build some appropriate models in the following pages, for instructional purposes we won't spend any more time on data transformation for the time being, and we will use the <code class="literal">mtcars</code> dataset, which was introduced in <a class="link" href="ch03.html" title="Chapter 3. Filtering and Summarizing Data">Chapter 3</a>, <span class="emphasis"><em>Filtering and Summarizing Data</em></span>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; KMO(mtcars)</strong></span>
<span class="strong"><strong>Kaiser-Meyer-Olkin factor adequacy</strong></span>
<span class="strong"><strong>Call: KMO(r = mtcars)</strong></span>
<span class="strong"><strong>Overall MSA =  0.83</strong></span>
<span class="strong"><strong>MSA for each item = </strong></span>
<span class="strong"><strong> mpg  cyl disp   hp drat   wt qsec   vs   am gear carb </strong></span>
<span class="strong"><strong>0.93 0.90 0.76 0.84 0.95 0.74 0.74 0.91 0.88 0.85 0.62</strong></span>
</pre></div><p>It seems that the <code class="literal">mtcars</code> database is a great choice for multivariate statistical analysis. This can be also verified by the so-called Bartlett test, which suggests whether the correlation matrix is similar to an identity matrix. Or, in other words, if there is a statistical relationship between the variables. On the other hand, if the correlation matrix has only zeros except for the diagonal, then the variables are independent from each other; thus it would not make much sense to think of multivariate methods. The <a class="indexterm" id="id673"/>
<code class="literal">psych</code> package provides an easy-to-use function<a class="indexterm" id="id674"/> to compute Bartlett's test as well:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; cortest.bartlett(cor(mtcars))</strong></span>
<span class="strong"><strong>$chisq</strong></span>
<span class="strong"><strong>[1] 1454.985</strong></span>

<span class="strong"><strong>$p.value</strong></span>
<span class="strong"><strong>[1] 3.884209e-268</strong></span>

<span class="strong"><strong>$df</strong></span>
<span class="strong"><strong>[1] </strong></span>
<span class="strong"><strong>55</strong></span>
</pre></div><p>The very low <code class="literal">p-value</code> suggests that we reject the null-hypothesis of the Bartlett test. This means that the correlation matrix differs from the identity matrix, so the correlation coeffiecients between the variables seem to be closer to 1 than 0. This is in sync with the high KMO value.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note60"/>Note</h3><p>Before focusing on the <a class="indexterm" id="id675"/>actual statistical methods, please be advised that, although the preceding assumptions make sense in most cases and should be followed as a rule of a thumb, KMO and Bartlett's tests are not always required. High communality is important for factor analysis and other latent models, while for example PCA is a mathematical transformation that will work with even low KMO values.</p></div></div></div></div></div>
<div class="section" title="Principal Component Analysis"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec63"/>Principal Component Analysis</h1></div></div></div><p>Finding the really important fields<a class="indexterm" id="id676"/> in databases with a huge number of variables may prove to be a challenging task for the data scientist. This is where <span class="strong"><strong>Principal Component Analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>) comes into the picture: to find the core<a class="indexterm" id="id677"/> components of data. It was invented more than 100 years ago by Karl Pearson, and it has been widely used in diverse fields since then.</p><p>The objective of PCA is to interpret the data in a more meaningful structure with the help of <a class="indexterm" id="id678"/>orthogonal transformations. This linear transformation is intended to reveal the internal structure of the dataset with an arbitrarily designed new basis in the vector space, which best explains the variance of the data. In plain English, this simply means that we compute new variables from the original data, where these new variables include the variance of the original variables in decreasing order.</p><p>This can be either done by<a class="indexterm" id="id679"/> eigendecomposition of the covariance, correlation matrix (the so-called <a class="indexterm" id="id680"/>R-mode PCA), or singular value <a class="indexterm" id="id681"/>decomposition (the so-called Q-mode PCA) of the dataset. Each method has great advantages, such as computation performance, memory requirements, or simply avoiding the prior standardization of<a class="indexterm" id="id682"/> the data before passing it to PCA when using a correlation matrix in eigendecomposition.</p><p>Either way, PCA can successfully ship a lower-dimensional image of the data, where the uncorrelated principal components are the linear combinations of the original variables. And this informative overview can be a great help to the analyst when identifying the underlying structure of the variables; thus the technique is very often used<a class="indexterm" id="id683"/> for exploratory data analysis.</p><p>PCA results in the exact <a class="indexterm" id="id684"/>same number of extracted components as the original variables. The first component includes most of the common variance, so it has the highest importance in describing the original dataset, while the last component often only includes some unique information from only one original variable. Based on this, we would usually only keep the first few components of PCA for further analysis, but we will also see some use cases where we will concentrate on the extracted unique variance.</p><div class="section" title="PCA algorithms"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec58"/>PCA algorithms</h2></div></div></div><p>R provides a variety of functions to run PCA. Although it's possible to compute the components manually by<a class="indexterm" id="id685"/> <code class="literal">eigen</code> or <code class="literal">svd</code> as<a class="indexterm" id="id686"/> R-mode or<a class="indexterm" id="id687"/> Q-mode PCA, we will focus on the higher level functions for the sake of simplicity. Relying on my stats-teacher background, I think that sometimes it's more efficient to concentrate on how to run an analysis and interpreting the results rather than spending way too much time with the linear algebra background—especially with given time/page limits.</p><p>R-mode PCA can be<a class="indexterm" id="id688"/> conducted by <code class="literal">princomp</code> or <code class="literal">principal</code> from<a class="indexterm" id="id689"/> the <code class="literal">psych</code> package, while the more preferred Q-mode PCA<a class="indexterm" id="id690"/> can be called by <code class="literal">prcomp</code>. Now let's focus on the latter and see what the components of <code class="literal">mtcars</code> look like:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; prcomp(mtcars, scale = TRUE)</strong></span>
<span class="strong"><strong>Standard deviations:</strong></span>
<span class="strong"><strong> [1] 2.57068 1.62803 0.79196 0.51923 0.47271 0.46000 0.36778 0.35057</strong></span>
<span class="strong"><strong> [9] 0.27757 0.22811 0.14847</strong></span>

<span class="strong"><strong>Rotation:</strong></span>
<span class="strong"><strong>           PC1       PC2       PC3        PC4       PC5       PC6</strong></span>
<span class="strong"><strong>mpg   -0.36253  0.016124 -0.225744 -0.0225403  0.102845 -0.108797</strong></span>
<span class="strong"><strong>cyl    0.37392  0.043744 -0.175311 -0.0025918  0.058484  0.168554</strong></span>
<span class="strong"><strong>disp   0.36819 -0.049324 -0.061484  0.2566079  0.393995 -0.336165</strong></span>
<span class="strong"><strong>hp     0.33006  0.248784  0.140015 -0.0676762  0.540047  0.071436</strong></span>
<span class="strong"><strong>drat  -0.29415  0.274694  0.161189  0.8548287  0.077327  0.244497</strong></span>
<span class="strong"><strong>wt     0.34610 -0.143038  0.341819  0.2458993 -0.075029 -0.464940</strong></span>
<span class="strong"><strong>qsec  -0.20046 -0.463375  0.403169  0.0680765 -0.164666 -0.330480</strong></span>
<span class="strong"><strong>vs    -0.30651 -0.231647  0.428815 -0.2148486  0.599540  0.194017</strong></span>
<span class="strong"><strong>am    -0.23494  0.429418 -0.205767 -0.0304629  0.089781 -0.570817</strong></span>
<span class="strong"><strong>gear  -0.20692  0.462349  0.289780 -0.2646905  0.048330 -0.243563</strong></span>
<span class="strong"><strong>carb   0.21402  0.413571  0.528545 -0.1267892 -0.361319  0.183522</strong></span>
<span class="strong"><strong>           PC7        PC8       PC9      PC10      PC11</strong></span>
<span class="strong"><strong>mpg   0.367724 -0.7540914  0.235702  0.139285 -0.1248956</strong></span>
<span class="strong"><strong>cyl   0.057278 -0.2308249  0.054035 -0.846419 -0.1406954</strong></span>
<span class="strong"><strong>disp  0.214303  0.0011421  0.198428  0.049380  0.6606065</strong></span>
<span class="strong"><strong>hp   -0.001496 -0.2223584 -0.575830  0.247824 -0.2564921</strong></span>
<span class="strong"><strong>drat  0.021120  0.0321935 -0.046901 -0.101494 -0.0395302</strong></span>
<span class="strong"><strong>wt   -0.020668 -0.0085719  0.359498  0.094394 -0.5674487</strong></span>
<span class="strong"><strong>qsec  0.050011 -0.2318400 -0.528377 -0.270673  0.1813618</strong></span>
<span class="strong"><strong>vs   -0.265781  0.0259351  0.358583 -0.159039  0.0084146</strong></span>
<span class="strong"><strong>am   -0.587305 -0.0597470 -0.047404 -0.177785  0.0298235</strong></span>
<span class="strong"><strong>gear  0.605098  0.3361502 -0.001735 -0.213825 -0.0535071</strong></span>
<span class="strong"><strong>carb -0.174603 -0.3956291  0.170641  0.072260  0.3195947</strong></span>
</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note61"/>Note</h3><p>Please note that we have called <code class="literal">prcomp</code> with <code class="literal">scale</code> set to <code class="literal">TRUE</code>, which is <code class="literal">FALSE</code> by default due to being backward-compatible with the S language. But in general, scaling is highly recommended. Using the scaling option is equivalent to running PCA on a dataset after scaling it previously, such as: <code class="literal">prcomp(scale(mtcars))</code>, which results in data with unit variance.</p></div></div><p>First, <code class="literal">prcomp</code> returned the <a class="indexterm" id="id691"/>standard deviations of the principal components, which shows how much information was preserved by the 11 components. The standard deviation of the first component is a lot larger than any other subsequent value, which explains more than 60 percent of the variance:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; summary(prcomp(mtcars, scale = TRUE))</strong></span>
<span class="strong"><strong>Importance of components:</strong></span>
<span class="strong"><strong>                         PC1   PC2   PC3    PC4    PC5    PC6    PC7</strong></span>
<span class="strong"><strong>Standard deviation     2.571 1.628 0.792 0.5192 0.4727 0.4600 0.3678</strong></span>
<span class="strong"><strong>Proportion of Variance 0.601 0.241 0.057 0.0245 0.0203 0.0192 0.0123</strong></span>
<span class="strong"><strong>Cumulative Proportion  0.601 0.842 0.899 0.9232 0.9436 0.9628 0.9751</strong></span>
<span class="strong"><strong>                          PC8   PC9    PC10  PC11</strong></span>
<span class="strong"><strong>Standard deviation     0.3506 0.278 0.22811 0.148</strong></span>
<span class="strong"><strong>Proportion of Variance 0.0112 0.007 0.00473 0.002</strong></span>
<span class="strong"><strong>Cumulative Proportion  0.9863 0.993 0.99800 1.000</strong></span>
</pre></div><p>Besides the first component, only the second one has a higher standard deviation than 1, which means that only the first two components include at least as much information as the original variables did. Or, in other words: only the first two variables have a higher eigenvalue than<a class="indexterm" id="id692"/> one. The eigenvalue can be computed by the square of the standard deviation of the principal<a class="indexterm" id="id693"/> components, summing up to the number of original variables as expected:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; sum(prcomp(scale(mtcars))$sdev^2)</strong></span>
<span class="strong"><strong>[1] 11</strong></span>
</pre></div></div><div class="section" title="Determining the number of components"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec59"/>Determining the number of components</h2></div></div></div><p>PCA algorithms always<a class="indexterm" id="id694"/> compute the same number of principal components as the number of variables in the original dataset. The importance of the component decreases from the first one to the last one.</p><p>As a rule of a thumb, we can simply keep all those components with higher standard deviation than 1. This means that we keep those components, which explains at least as much variance as the original variables do:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; prcomp(scale(mtcars))$sdev^2</strong></span>
<span class="strong"><strong> [1] 6.608400 2.650468 0.627197 0.269597 0.223451 0.211596 0.135262</strong></span>
<span class="strong"><strong> [8] 0.122901 0.077047 0.052035 0.022044</strong></span>
</pre></div><p>So the preceding summary suggests keeping only two components out of the 11, which explains almost 85 percent of the variance:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; (6.6 + 2.65) / 11</strong></span>
<span class="strong"><strong>[1] 0.8409091</strong></span>
</pre></div><p>An alternative and great visualization tool to help us determine the optimal number of component is scree plot. Fortunately, there are at least two great functions in the <code class="literal">psych</code> package we can use here: the <code class="literal">scree</code> and the <code class="literal">VSS.scree</code> functions:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; VSS.scree(cor(mtcars))</strong></span>
</pre></div><div class="mediaobject"><img alt="Determining the number of components" src="graphics/2028OS_09_08.jpg"/></div><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; scree(cor(mtcars))</strong></span>
</pre></div><div class="mediaobject"><img alt="Determining the number of components" src="graphics/2028OS_09_09.jpg"/></div><p>The only difference<a class="indexterm" id="id695"/> between the preceding two plots is that <code class="literal">scree</code> also shows<a class="indexterm" id="id696"/> the eigenvalues of a factor analysis besides PCA. Read more about this in the next section of this chapter.</p><p>As can be seen, <code class="literal">VSS.scree</code> provides a visual overview on the eigenvalues of the principal components, and it also highlights the critical value at 1 by a horizontal line. This is usually referred to as the<a class="indexterm" id="id697"/> Kaiser criterion.</p><p>Besides this rule of a thumb, as discussed previously one can also rely on the so-called Elbow-rule, which<a class="indexterm" id="id698"/> simply suggests that the line-plot represents an arm and the optimal number of components is the point where this arm's elbow can be found. So we have to look for the point from where the curve becomes less steep. This sharp break is probably at 3 in this case instead of 2, as we have found with the Kaiser criterion.</p><p>And besides Cattell's original <a class="indexterm" id="id699"/>scree test, we can also compare the previously described <code class="literal">scree</code> of the components with a bit of a randomized data to identify the optimal number of components to keep:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; fa.parallel(mtcars)</strong></span>
</pre></div><div class="mediaobject"><img alt="Determining the number of components" src="graphics/2028OS_09_09.jpg"/></div><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>Parallel analysis suggests that the number of factors = 2 </strong></span>
<span class="strong"><strong>and the number of components =  2</strong></span>
</pre></div><p>Now we have verified the optimal number of principal components to keep for further analysis with a variety of statistical tools, and we can work with only two variables instead of 11 after all, which<a class="indexterm" id="id700"/> is great! But what do these artificially created variables actually mean?</p></div><div class="section" title="Interpreting components"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec60"/>Interpreting components</h2></div></div></div><p>The only problem <a class="indexterm" id="id701"/>with reducing the dimension of our data is that it can be very frustrating to find out what our newly created, highly compressed, and transformed data actually is. Now we have <code class="literal">PC1</code> and <code class="literal">PC2</code> for our 32 cars:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; pc &lt;- prcomp(mtcars, scale = TRUE)</strong></span>
<span class="strong"><strong>&gt; head(pc$x[, 1:2])</strong></span>
<span class="strong"><strong>                        PC1      PC2</strong></span>
<span class="strong"><strong>Mazda RX4         -0.646863  1.70811</strong></span>
<span class="strong"><strong>Mazda RX4 Wag     -0.619483  1.52562</strong></span>
<span class="strong"><strong>Datsun 710        -2.735624 -0.14415</strong></span>
<span class="strong"><strong>Hornet 4 Drive    -0.306861 -2.32580</strong></span>
<span class="strong"><strong>Hornet Sportabout  1.943393 -0.74252</strong></span>
<span class="strong"><strong>Valiant           -0.055253 -2.74212</strong></span>
</pre></div><p>These values were computed by multiplying the original dataset with the identified weights, so-called loadings (<code class="literal">rotation</code>) or the component matrix. This is a standard linear transformation:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; head(scale(mtcars) %*% pc$rotation[, 1:2])</strong></span>
<span class="strong"><strong>                        PC1      PC2</strong></span>
<span class="strong"><strong>Mazda RX4         -0.646863  1.70811</strong></span>
<span class="strong"><strong>Mazda RX4 Wag     -0.619483  1.52562</strong></span>
<span class="strong"><strong>Datsun 710        -2.735624 -0.14415</strong></span>
<span class="strong"><strong>Hornet 4 Drive    -0.306861 -2.32580</strong></span>
<span class="strong"><strong>Hornet Sportabout  1.943393 -0.74252</strong></span>
<span class="strong"><strong>Valiant           -0.055253 -2.74212</strong></span>
</pre></div><p>Both variables are scaled with the mean being zero and the standard deviation as described previously:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; summary(pc$x[, 1:2])</strong></span>
<span class="strong"><strong>      PC1              PC2        </strong></span>
<span class="strong"><strong> Min.   :-4.187   Min.   :-2.742  </strong></span>
<span class="strong"><strong> 1st Qu.:-2.284   1st Qu.:-0.826  </strong></span>
<span class="strong"><strong> Median :-0.181   Median :-0.305  </strong></span>
<span class="strong"><strong> Mean   : 0.000   Mean   : 0.000  </strong></span>
<span class="strong"><strong> 3rd Qu.: 2.166   3rd Qu.: 0.672  </strong></span>
<span class="strong"><strong> Max.   : 3.892   Max.   : 4.311  </strong></span>
<span class="strong"><strong>&gt; apply(pc$x[, 1:2], 2, sd)</strong></span>
<span class="strong"><strong>   PC1    PC2 </strong></span>
<span class="strong"><strong>2.5707 1.6280 </strong></span>
<span class="strong"><strong>&gt; pc$sdev[1:2]</strong></span>
<span class="strong"><strong>[1] 2.5707 1.6280</strong></span>
</pre></div><p>All scores computed by PCA are scaled, because it always returns the values transformed to a new coordinate<a class="indexterm" id="id702"/> system with an orthogonal basis, which means that the components are not correlated and scaled:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; round(cor(pc$x))</strong></span>
<span class="strong"><strong>     PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11</strong></span>
<span class="strong"><strong>PC1    1   0   0   0   0   0   0   0   0    0    0</strong></span>
<span class="strong"><strong>PC2    0   1   0   0   0   0   0   0   0    0    0</strong></span>
<span class="strong"><strong>PC3    0   0   1   0   0   0   0   0   0    0    0</strong></span>
<span class="strong"><strong>PC4    0   0   0   1   0   0   0   0   0    0    0</strong></span>
<span class="strong"><strong>PC5    0   0   0   0   1   0   0   0   0    0    0</strong></span>
<span class="strong"><strong>PC6    0   0   0   0   0   1   0   0   0    0    0</strong></span>
<span class="strong"><strong>PC7    0   0   0   0   0   0   1   0   0    0    0</strong></span>
<span class="strong"><strong>PC8    0   0   0   0   0   0   0   1   0    0    0</strong></span>
<span class="strong"><strong>PC9    0   0   0   0   0   0   0   0   1    0    0</strong></span>
<span class="strong"><strong>PC10   0   0   0   0   0   0   0   0   0    1    0</strong></span>
<span class="strong"><strong>PC11   0   0   0   0   </strong></span>
<span class="strong"><strong>0   0   0   0   0    0    1</strong></span>
</pre></div><p>To see what the principal components actually mean, it's really helpful to check the loadings matrix, as we have seen before:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; pc$rotation[, 1:2]</strong></span>
<span class="strong"><strong>          PC1       PC2</strong></span>
<span class="strong"><strong>mpg  -0.36253  0.016124</strong></span>
<span class="strong"><strong>cyl   0.37392  0.043744</strong></span>
<span class="strong"><strong>disp  0.36819 -0.049324</strong></span>
<span class="strong"><strong>hp    0.33006  0.248784</strong></span>
<span class="strong"><strong>drat -0.29415  0.274694</strong></span>
<span class="strong"><strong>wt    0.34610 -0.143038</strong></span>
<span class="strong"><strong>qsec -0.20046 -0.463375</strong></span>
<span class="strong"><strong>vs   -0.30651 -0.231647</strong></span>
<span class="strong"><strong>am   -0.23494  0.429418</strong></span>
<span class="strong"><strong>gear -0.20692  0.462349</strong></span>
<span class="strong"><strong>carb  0.21402  0.413571</strong></span>
</pre></div><p>Probably this analytical table might be more meaningful in some visual way, for example as a <code class="literal">biplot</code>, which shows not only the original variables but also the observations (black labels) on the same plot with the new coordinate system based on the principal components (red labels):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; biplot(pc, cex = c(0.8, 1.2))</strong></span>
<span class="strong"><strong>&gt; abline(h = 0, v = 0, lty = 'dashed')</strong></span>
</pre></div><div class="mediaobject"><img alt="Interpreting components" src="graphics/2028OS_09_10.jpg"/></div><p>We can conclude<a class="indexterm" id="id703"/> that <code class="literal">PC1</code> includes information mostly from the number of cylinders (<code class="literal">cyl</code>), displacement (<code class="literal">disp</code>), weight (<code class="literal">wt</code>), and gas consumption (<code class="literal">mpg</code>), although the latter looks likely to decrease the value of <code class="literal">PC1</code>. This was found by checking the highest and lowest values on the <code class="literal">PC1</code> axis. Similarly, we find that <code class="literal">PC2</code> is constructed by speed-up (<code class="literal">qsec</code>), number of gears (<code class="literal">gear</code>), carburetors (<code class="literal">carb</code>), and the transmission type (<code class="literal">am</code>).</p><p>To verify this, we can easily compute the correlation coefficient between the original variables and the principal components:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; cor(mtcars, pc$x[, 1:2])</strong></span>
<span class="strong"><strong>          PC1       PC2</strong></span>
<span class="strong"><strong>mpg  -0.93195  0.026251</strong></span>
<span class="strong"><strong>cyl   0.96122  0.071216</strong></span>
<span class="strong"><strong>disp  0.94649 -0.080301</strong></span>
<span class="strong"><strong>hp    0.84847  0.405027</strong></span>
<span class="strong"><strong>drat -0.75617  0.447209</strong></span>
<span class="strong"><strong>wt    0.88972 -0.232870</strong></span>
<span class="strong"><strong>qsec -0.51531 -0.754386</strong></span>
<span class="strong"><strong>vs   -0.78794 -0.377127</strong></span>
<span class="strong"><strong>am   -0.60396  0.699103</strong></span>
<span class="strong"><strong>gear -0.53192  0.752715</strong></span>
<span class="strong"><strong>carb  0.55017  0.673304</strong></span>
</pre></div><p>Does this make sense? How would you name <code class="literal">PC1</code> and <code class="literal">PC2</code>? The number of cylinders and displacement<a class="indexterm" id="id704"/> seem like engine parameters, while the weight is probably rather influenced by the body of the car. Gas consumption should be affected by both specs. The other component's variables deal with suspension, but we also have speed there, not to mention the bunch of mediocre correlation coefficients in the preceding matrix. Now what?</p></div><div class="section" title="Rotation methods"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec61"/>Rotation methods</h2></div></div></div><p>Based on the fact that<a class="indexterm" id="id705"/> rotation methods are done in a subspace, rotation is always suboptimal compared to the previously discussed PCA. This means that the new axes after rotation will explain less variance than the original components.</p><p>On the other hand, rotation simplifies the structure of the components and thus makes it a lot easier to understand and interpret the results; thus, these methods are often used in practice.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note62"/>Note</h3><p>Rotation methods can be (and are) usually applied to both PCA and FA (more on this later). Orthogonal methods are preferred.</p></div></div><p>There are two main types of rotation:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Orthogonal, where the new axes are orthogonal to each other. There is no correlation between the components/factors.</li><li class="listitem" style="list-style-type: disc">Oblique, where the new axes are not necessarily orthogonal to each other; thus there might be some correlation between the variables.</li></ul></div><p>Varimax rotation is one of the most popular rotation methods. It was developed by Kaiser in 1958 and has been popular ever since. It is often used because the method maximizes the variance of the loadings matrix, resulting in more interpretable scores:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; varimax(pc$rotation[, 1:2])</strong></span>
<span class="strong"><strong>$loadings</strong></span>
<span class="strong"><strong>     PC1    PC2   </strong></span>
<span class="strong"><strong>mpg  -0.286 -0.223</strong></span>
<span class="strong"><strong>cyl   0.256  0.276</strong></span>
<span class="strong"><strong>disp  0.312  0.201</strong></span>
<span class="strong"><strong>hp           0.403</strong></span>
<span class="strong"><strong>drat -0.402       </strong></span>
<span class="strong"><strong>wt    0.356  0.116</strong></span>
<span class="strong"><strong>qsec  0.148 -0.483</strong></span>
<span class="strong"><strong>vs          -0.375</strong></span>
<span class="strong"><strong>am   -0.457  0.174</strong></span>
<span class="strong"><strong>gear -0.458  0.217</strong></span>
<span class="strong"><strong>carb -0.106  0.454</strong></span>

<span class="strong"><strong>                 PC1   PC2</strong></span>
<span class="strong"><strong>SS loadings    1.000 1.000</strong></span>
<span class="strong"><strong>Proportion Var 0.091 0.091</strong></span>
<span class="strong"><strong>Cumulative Var 0.091 0.182</strong></span>

<span class="strong"><strong>$rotmat</strong></span>
<span class="strong"><strong>         [,1]    [,2]</strong></span>
<span class="strong"><strong>[1,]  0.76067 0.64914</strong></span>
<span class="strong"><strong>[2,] -0.64914 0.76067</strong></span>
</pre></div><p>Now the first component seems to be mostly affected (negatively dominated) by the transmission type, number <a class="indexterm" id="id706"/>of gears, and rear axle ratio, while the second one is affected by speed-up, horsepower, and the number of carburetors. This suggests naming <code class="literal">PC2</code> as <code class="literal">power</code>, while <code class="literal">PC1</code> instead refers to <code class="literal">transmission</code>. Let's see those 32 automobiles in this new coordinate system:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; pcv &lt;- varimax(pc$rotation[, 1:2])$loadings</strong></span>
<span class="strong"><strong>&gt; plot(scale(mtcars) %*% pcv, type = 'n',</strong></span>
<span class="strong"><strong>+     xlab = 'Transmission', ylab = 'Power')</strong></span>
<span class="strong"><strong>&gt; text(scale(mtcars) %*% pcv, labels = rownames(mtcars))</strong></span>
</pre></div><div class="mediaobject"><img alt="Rotation methods" src="graphics/2028OS_09_11.jpg"/></div><p>Based on the preceding plot, every data scientist should pick a car from the upper left quarter to go with the top rated models, right? Those cars have great power based on the <span class="emphasis"><em>y</em></span> axis and good transmission systems, as shown on the <span class="emphasis"><em>x</em></span> axis—do not forget about the transmission being negatively correlated with the original variables. But let's see some other rotation methods and the advantages of those as well!</p><p>Quartimax rotation<a class="indexterm" id="id707"/> is an orthogonal<a class="indexterm" id="id708"/> method, as well, and minimizes the number of components needed to explain each variable. This often results in a general component and additional smaller components. When a compromise between Varimax and Quartimax rotation methods is needed, you might opt for<a class="indexterm" id="id709"/> Equimax rotation.</p><p>Oblique rotation methods include <a class="indexterm" id="id710"/>Oblimin and <a class="indexterm" id="id711"/>Promax, which are not available in the base stats or even the highly used <a class="indexterm" id="id712"/>
<code class="literal">psych</code> package. Instead, we can load the <a class="indexterm" id="id713"/>
<code class="literal">GPArotation</code> package, which provides a wide range of rotation methods for PCA and FA as well. For demonstration purposes, let's see how Promax rotation works, which is a lot faster compared to, for<a class="indexterm" id="id714"/> example, Oblimin:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; library(GPArotation)</strong></span>
<span class="strong"><strong>&gt; promax(pc$rotation[, 1:2])</strong></span>
<span class="strong"><strong>$loadings</strong></span>

<span class="strong"><strong>Loadings:</strong></span>
<span class="strong"><strong>     PC1    PC2   </strong></span>
<span class="strong"><strong>mpg  -0.252 -0.199</strong></span>
<span class="strong"><strong>cyl   0.211  0.258</strong></span>
<span class="strong"><strong>disp  0.282  0.174</strong></span>
<span class="strong"><strong>hp           0.408</strong></span>
<span class="strong"><strong>drat -0.416       </strong></span>
<span class="strong"><strong>wt    0.344       </strong></span>
<span class="strong"><strong>qsec  0.243 -0.517</strong></span>
<span class="strong"><strong>vs          -0.380</strong></span>
<span class="strong"><strong>am   -0.502  0.232</strong></span>
<span class="strong"><strong>gear -0.510  0.276</strong></span>
<span class="strong"><strong>carb -0.194  0.482</strong></span>

<span class="strong"><strong>                 PC1   PC2</strong></span>
<span class="strong"><strong>SS loadings    1.088 1.088</strong></span>
<span class="strong"><strong>Proportion Var 0.099 0.099</strong></span>
<span class="strong"><strong>Cumulative Var 0.099 0.198</strong></span>

<span class="strong"><strong>$rotmat</strong></span>
<span class="strong"><strong>         [,1]    [,2]</strong></span>
<span class="strong"><strong>[1,]  0.65862 0.58828</strong></span>
<span class="strong"><strong>[2,] -0.80871 0.86123</strong></span>

<span class="strong"><strong>&gt; cor(promax(pc$rotation[, 1:2])$loadings)</strong></span>
<span class="strong"><strong>         PC1      PC2</strong></span>
<span class="strong"><strong>PC1  1.00000 -0.23999</strong></span>
<span class="strong"><strong>PC2 -0.23999  1.00000</strong></span>
</pre></div><p>The result of the last command supports the view that oblique rotation methods generate scores that might be correlated, unlike when running an orthogonal rotation.</p></div><div class="section" title="Outlier-detection with PCA"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec62"/>Outlier-detection with PCA</h2></div></div></div><p>PCA can be used for a variety of goals besides exploratory data analysis. For example, we can use PCA to generate eigenfaces, compress images, classify observations, or to detect outliers in a multidimensional space via image filtering. Now, we will construct a simplified model discussed in a related research post published on R-bloggers in 2012: <a class="ulink" href="http://www.r-bloggers.com/finding-a-pin-in-a-haystack-pca-image-filtering">http://www.r-bloggers.com/finding-a-pin-in-a-haystack-pca-image-filtering</a>.</p><p>The challenge described in the post was to detect a foreign metal object in the sand photographed by the Curiosity Rover on the Mars. The image can be found at the official NASA website at <a class="ulink" href="http://www.nasa.gov/images/content/694811main_pia16225-43_full.jpg">http://www.nasa.gov/images/content/694811main_pia16225-43_full.jpg</a>, for which I've created a shortened URL for future use: <a class="ulink" href="http://bit.ly/nasa-img">http://bit.ly/nasa-img</a>.</p><p>In the following image, you can see a strange metal object highlighted in the sand in a black circle, just to make sure you know what we are looking for. The image found at the preceding URL does not have this highlight:</p><div class="mediaobject"><img alt="Outlier-detection with PCA" src="graphics/2028OS_09_12.jpg"/></div><p>And now let's use some statistical methods to identify that object without (much) human intervention! First, we need to download the image from the Internet and load it into R. The <code class="literal">jpeg</code> package will be really helpful here:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;</strong></span>
<span class="strong"><strong> library(jpeg)</strong></span>
<span class="strong"><strong>&gt; t &lt;- tempfile()</strong></span>
<span class="strong"><strong>&gt; download.file('http://bit.ly/nasa-img', t)</strong></span>
<span class="strong"><strong>trying URL 'http://bit.ly/nasa-img'</strong></span>
<span class="strong"><strong>Content type 'image/jpeg' length 853981 bytes (833 Kb)</strong></span>
<span class="strong"><strong>opened URL</strong></span>
<span class="strong"><strong>==================================================</strong></span>
<span class="strong"><strong>downloaded 833 Kb</strong></span>

<span class="strong"><strong>&gt;</strong></span>
<span class="strong"><strong> img &lt;- readJPEG(t)</strong></span>
<span class="strong"><strong>&gt; str(img)</strong></span>
<span class="strong"><strong> num [1:1009, 1:1345, 1:3] 0.431 0.42 0.463 0.486 0.49 ...</strong></span>
</pre></div><p>The <code class="literal">readJPEG</code> function returns the RGB values of every pixel in the picture, resulting in a three dimensional array where the first dimension is the row, the second is the column, and the third dimension includes the three color values.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note63"/>Note</h3><p>RGB is an additive color model that can reproduce a wide variety of colors by mixing red, green, and blue by given intensities and optional transparency. This color model is highly used in computer science.</p></div></div><p>As PCA requires a matrix as an input, we have to convert this 3-dimensional array to a 2-dimensional dataset. To this end, let's not bother with the order of pixels for the time being, as we can reconstruct that later, but let's simply list the RGB values of all pixels, one after the other:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; h &lt;- dim(img)[1]</strong></span>
<span class="strong"><strong>&gt; w &lt;- dim(img)[2]</strong></span>
<span class="strong"><strong>&gt; m &lt;- matrix(img, h*w)</strong></span>
<span class="strong"><strong>&gt; str(m)</strong></span>
<span class="strong"><strong> num [1:1357105, 1:3] 0.431 0.42 0.463 0.486 0.49 ...</strong></span>
</pre></div><p>In a nutshell, we saved the original height of the image (in pixels) in variable <code class="literal">h</code>, saved the width in <code class="literal">w</code>, and then converted the 3D array to a matrix with 1,357,105 rows. And, after four lines of data loading and three lines of data transformation, we can call the actual, rather simplified statistical method at last:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; pca &lt;- prcomp(m)</strong></span>
</pre></div><p>As we've seen before, data scientists do indeed deal with data preparation most of the time, while the actual data analysis can be done easily, right?</p><p>The extracted components seems to perform pretty well; the first component explains more than 96 percent of the variance:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; summary(pca)</strong></span>
<span class="strong"><strong>Importance of components:</strong></span>
<span class="strong"><strong>                         PC1    PC2     PC3</strong></span>
<span class="strong"><strong>Standard deviation     0.277 0.0518 0.00765</strong></span>
<span class="strong"><strong>Proportion of Variance 0.965 0.0338 0.00074</strong></span>
<span class="strong"><strong>Cumulative Proportion  0.965 0.9993 1.00000</strong></span>
</pre></div><p>Previously, interpreting RGB values was pretty straightforward, but what do these components mean?</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;</strong></span><span class="strong"><strong> pca$rotation</strong></span>
<span class="strong"><strong>          PC1      PC2      PC3</strong></span>
<span class="strong"><strong>[1,] -0.62188  0.71514  0.31911</strong></span>
<span class="strong"><strong>[2,] -0.57409 -0.13919 -0.80687</strong></span>
<span class="strong"><strong>[3,] -0.53261 -0.68498  0.49712</strong></span>
</pre></div><p>It seems that the first component is rather mixed with all three colors, the second component misses the green color, while the third component includes almost only green. Why not visualize that instead of trying to imagine how these artificial values look? To this end, let's extract the color intensities from the preceding component/loading matrix by the following quick helper function:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;</strong></span><span class="strong"><strong> extractColors &lt;- function(x)</strong></span>
<span class="strong"><strong>+     rgb(x[1], x[2], x[3])</strong></span>
</pre></div><p>Calling this on the absolute values of the component matrix results in the hex-color codes that describe the principal components:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; (colors &lt;- apply(abs(pca$rotation), 2, extractColors))</strong></span>
<span class="strong"><strong>      PC1       PC2       PC3 </strong></span>
<span class="strong"><strong>"#9F9288" "#B623AF" "#51CE7F"</strong></span>
</pre></div><p>These color codes can be easily rendered—for example, on a pie chart, where the area of the pies represents the explained variance of the principal components:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; pie(pca$sdev, col = colors, labels = colors)</strong></span>
</pre></div><div class="mediaobject"><img alt="Outlier-detection with PCA" src="graphics/2028OS_09_13.jpg"/></div><p>Now we no longer have red, green, or blue intensities or actual colors in the computed scores stored in <code class="literal">pca$x</code>; rather, the principal components describe each pixel with the visualized colors shown previously. And, as previously discussed, the third component stands for a greenish color, the second one misses green (resulting in a purple color), while the first component includes a rather high value from all RGB colors resulting in a tawny color, which is not surprising at all knowing that the photo was taken in the desert of Mars.</p><p>Now we can render the original image with monochrome colors to show the intensity of the principal components. The following few lines of code produce two modified photos of the Curiosity Rover and its environment based on <code class="literal">PC1</code> and <code class="literal">PC2</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; par(mfrow = c(1, 2), mar = rep(0, 4))</strong></span>
<span class="strong"><strong>&gt; image(matrix(pca$x[, 1], h), col = gray.colors(100))</strong></span>
<span class="strong"><strong>&gt; image(matrix(pca$x[, 2], h), col = gray.colors(100), yaxt = 'n')</strong></span>
</pre></div><div class="mediaobject"><img alt="Outlier-detection with PCA" src="graphics/2028OS_09_14.jpg"/></div><p>Although the image was rotated by 90 degrees in some of the linear transformations, it's pretty clear that the first image was not really helpful in finding the foreign metal object in the sand. As a matter of fact, this image represents the noise in the desert area, as <code class="literal">PC1</code> included sand-like color intensities, so this component is useful for describing the variety of tawny colors.</p><p>On the other hand, the second component highlights the metal object in the sand very well! All surrounding pixels are dim, due to the low ratio of purple color in normal sand, while the anomalous object is rather dark.</p><p>I really like this piece of R code and the simplified example: although they're still basic enough to follow, they also demonstrate the power of R and how standard data analytic methods can be used to harvest information from raw data.</p></div></div>
<div class="section" title="Factor analysis"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec64"/>Factor analysis</h1></div></div></div><p>Although the literature on confirmatory <span class="strong"><strong>factor analysis</strong></span> (<span class="strong"><strong>FA</strong></span>) is really impressive and is being highly used in, for example, social sciences, we will only focus on exploratory FA, where our goal is to identify some unknown, not observed variables based on other empirical data.</p><p>The latent variable model of FA was first introduced in 1904 by Spearman for one factor, and then Thurstone generalized the model for more than one factor in 1947. This statistical model assumes that the manifest variables available in the dataset are the results of latent variables that were not observed but can be tracked based on the observed data.</p><p>FA can deal with continuous (numeric) variables, and the model states that each observed variable is the sum of some unknown, latent factors.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note64"/>Note</h3><p>Please note the that normality, KMO, and Bartlett's tests are a lot more important to check before doing FA compared to PCA; the latter is a rather descriptive method while, in FA, we are actually building a model.</p></div></div><p>The most used exploratory FA method is maximum-likelihood FA, which is also available in the <code class="literal">factanal</code> function in the already installed <code class="literal">stats</code> package. Other factoring methods are made available by the <code class="literal">fa</code> functions in the <code class="literal">psych</code> package—for example, <span class="strong"><strong>ordinary least squares</strong></span> (<span class="strong"><strong>OLS</strong></span>), <span class="strong"><strong>weighted least squares</strong></span> (<span class="strong"><strong>WLS</strong></span>), <span class="strong"><strong>generalized weighted least squares</strong></span> (<span class="strong"><strong>GLS</strong></span>), or principal factor solution. These functions take raw data or the covariance matrix as input.</p><p>For demonstration purposes, let's see how the default factoring method performs on a subset of <code class="literal">mtcars</code>. Let's extract all performance-related variables except for displacement, which is probably accountable for all the other relevant metrics:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; m &lt;- subset(mtcars, select = c(mpg, cyl, hp, carb))</strong></span>
</pre></div><p>Now simply call and save the results of <code class="literal">fa</code> on the preceding <code class="literal">data.frame</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; (f </strong></span><span class="strong"><strong>&lt;- fa(m))</strong></span>
<span class="strong"><strong>Factor Analysis using method =  minres</strong></span>
<span class="strong"><strong>Call: fa(r = m)</strong></span>
<span class="strong"><strong>Standardized loadings (pattern matrix) based upon correlation matrix</strong></span>
<span class="strong"><strong>       MR1   h2   u2 com</strong></span>
<span class="strong"><strong>mpg  -0.87 0.77 0.23   1</strong></span>
<span class="strong"><strong>cyl   0.91 0.83 0.17   1</strong></span>
<span class="strong"><strong>hp    0.92 0.85 0.15   1</strong></span>
<span class="strong"><strong>carb  0.69 0.48 0.52   1</strong></span>

<span class="strong"><strong>                MR1</strong></span>
<span class="strong"><strong>SS loadings    2.93</strong></span>
<span class="strong"><strong>Proportion Var 0.73</strong></span>

<span class="strong"><strong>Mean item complexity =  1</strong></span>
<span class="strong"><strong>Test of the hypothesis that 1 factor is sufficient.</strong></span>

<span class="strong"><strong>The degrees of freedom for the null model are  6 </strong></span>
<span class="strong"><strong>and the objective function was  3.44 with Chi Square of  99.21</strong></span>
<span class="strong"><strong>The degrees of freedom for the model are 2</strong></span>
<span class="strong"><strong>and the objective function was  0.42 </strong></span>

<span class="strong"><strong>The root mean square of the residuals (RMSR) is  0.07 </strong></span>
<span class="strong"><strong>The df corrected root mean square of the residuals is  0.12 </strong></span>

<span class="strong"><strong>The harmonic number of observations is  32</strong></span>
<span class="strong"><strong>with the empirical chi square  1.92  with prob &lt;  0.38 </strong></span>
<span class="strong"><strong>The total number of observations was  32</strong></span>
<span class="strong"><strong>with MLE Chi Square =  11.78  with prob &lt;  0.0028 </strong></span>

<span class="strong"><strong>Tucker Lewis Index of factoring reliability =  0.677</strong></span>
<span class="strong"><strong>RMSEA index =  0.42</strong></span>
<span class="strong"><strong>and the 90 % confidence intervals are  0.196 0.619</strong></span>
<span class="strong"><strong>BIC =  4.84</strong></span>
<span class="strong"><strong>Fit based upon off diagonal values = 0.99</strong></span>
<span class="strong"><strong>Measures of factor score adequacy             </strong></span>
<span class="strong"><strong>                                                MR1</strong></span>
<span class="strong"><strong>Correlation of scores with factors             0.97</strong></span>
<span class="strong"><strong>Multiple R square of scores with factors       0.94</strong></span>
<span class="strong"><strong>Minimum correlation of possible factor scores  0.87</strong></span>
</pre></div><p>Well, this is a rather impressive amount of information with a bunch of details! <code class="literal">MR1</code> stands for the first extracted factor named after the default factoring method (Minimal Residuals or OLS). Since there is only one factor included in the model, rotation of factors is not an option. There is a test or hypothesis to check whether the numbers of factors are sufficient, and some coefficients represent a really great model fit.</p><p>The results can be summarized on the following plot:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; fa.</strong></span><span class="strong"><strong>diagram(f)</strong></span>
</pre></div><div class="mediaobject"><img alt="Factor analysis" src="graphics/2028OS_09_15.jpg"/></div><p>Here we see the high correlation coefficients between the latent and the observed variables, and the direction of the arrows suggests that the factor has an effect on the values found in our empirical dataset. Guess the relationship between this factor and the displacement of the car engines!</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; cor(f$scores, mtcars$disp)</strong></span>
<span class="strong"><strong>0.87595</strong></span>
</pre></div><p>Well, this seems like a good match.</p></div>
<div class="section" title="Principal Component Analysis versus Factor Analysis"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec65"/>Principal Component Analysis versus Factor Analysis</h1></div></div></div><p>Unfortunately, principal components are often confused with factors, and the two terms and related methods are sometimes used as synonyms, although the mathematical background and goals of the two methods are really different.</p><p>PCA is used to reduce the number of variables by creating principal components that then can be used in further projects instead of the original variables. This means that we try to extract the essence of the dataset in the means of artificially created variables, which best describe the variance of the data:</p><div class="mediaobject"><img alt="Principal Component Analysis versus Factor Analysis" src="graphics/2028OS_09_19.jpg"/></div><p>FA is the other way around, as it tries to identify unknown, latent variables to explain the original data. In plain English, we use the manifest variables from our empirical dataset to guess the internal structure of the data:</p><div class="mediaobject"><img alt="Principal Component Analysis versus Factor Analysis" src="graphics/2028OS_09_20.jpg"/></div></div>
<div class="section" title="Multidimensional Scaling"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec66"/>Multidimensional Scaling</h1></div></div></div><p>
<span class="strong"><strong>Multidimensional Scaling</strong></span> (<span class="strong"><strong>MDS</strong></span>) is a multivariate technique that was first used in geography. The main goal of MDS is to plot multivariate data points in two dimensions, thus revealing the structure of the dataset by visualizing the relative distance of the observations. MDA is<a class="indexterm" id="id715"/> used in diverse fields such as attitude study in psychology, sociology, and market research.</p><p>While the <code class="literal">MASS</code> package<a class="indexterm" id="id716"/> provides non-metric MDS via the <code class="literal">isoMDS</code> function, we will concentrate on the classical metric MDS, which is available in the <code class="literal">cmdscale</code> function offered by the <code class="literal">stats</code> package. Both types of MDS take a distance matrix as the main argument and can be created from any numeric tabular data by the <code class="literal">dist</code> function.</p><p>But before we explore more complex examples, let's see what MDS can offer us while working with an already existing distance matrix, such as the built-in <code class="literal">eurodist</code> dataset:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; as.matrix(eurodist)[1:5, 1:5]</strong></span>
<span class="strong"><strong>          Athens Barcelona Brussels Calais Cherbourg</strong></span>
<span class="strong"><strong>Athens         0      3313     2963   3175      3339</strong></span>
<span class="strong"><strong>Barcelona   3313         0     1318   1326      1294</strong></span>
<span class="strong"><strong>Brussels    2963      1318        0    204       583</strong></span>
<span class="strong"><strong>Calais      3175      1326      204      0       460</strong></span>
<span class="strong"><strong>Cherbourg   3339      1294      583    460         0</strong></span>
</pre></div><p>The preceding values represents the travel distance between 21 European cities in kilometers, although only the first 5-5 values were shown. Running classical MDS is fairly easy:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; (mds &lt;- cmdscale(eurodist))</strong></span>
<span class="strong"><strong>                      [,1]      [,2]</strong></span>
<span class="strong"><strong>Athens           2290.2747  1798.803</strong></span>
<span class="strong"><strong>Barcelona        -825.3828   546.811</strong></span>
<span class="strong"><strong>Brussels           59.1833  -367.081</strong></span>
<span class="strong"><strong>Calais            -82.8460  -429.915</strong></span>
<span class="strong"><strong>Cherbourg        -352.4994  -290.908</strong></span>
<span class="strong"><strong>Cologne           293.6896  -405.312</strong></span>
<span class="strong"><strong>Copenhagen        681.9315 -1108.645</strong></span>
<span class="strong"><strong>Geneva             -9.4234   240.406</strong></span>
<span class="strong"><strong>Gibraltar       -2048.4491   642.459</strong></span>
<span class="strong"><strong>Hamburg           561.1090  -773.369</strong></span>
<span class="strong"><strong>Hook of Holland   164.9218  -549.367</strong></span>
<span class="strong"><strong>Lisbon          -1935.0408    49.125</strong></span>
<span class="strong"><strong>Lyons            -226.4232   187.088</strong></span>
<span class="strong"><strong>Madrid          -1423.3537   305.875</strong></span>
<span class="strong"><strong>Marseilles       -299.4987   388.807</strong></span>
<span class="strong"><strong>Milan             260.8780   416.674</strong></span>
<span class="strong"><strong>Munich            587.6757    81.182</strong></span>
<span class="strong"><strong>Paris            -156.8363  -211.139</strong></span>
<span class="strong"><strong>Rome              709.4133  1109.367</strong></span>
<span class="strong"><strong>Stockholm         839.4459 -1836.791</strong></span>
<span class="strong"><strong>Vienna            911.2305   205.930</strong></span>
</pre></div><p>These scores<a class="indexterm" id="id717"/> are very similar to two principal components, such as running <code class="literal">prcomp(eurodist)$x[, 1:2]</code>. As a matter of fact, PCA can be considered as the most basic MDS solution.</p><p>Anyway, we have just transformed the 21-dimensional space into 2 dimensions, which can be plotted very easily (unlike the previous matrix with 21 rows and 21 columns):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; plot(mds)</strong></span>
</pre></div><div class="mediaobject"><img alt="Multidimensional Scaling" src="graphics/2028OS_09_16.jpg"/></div><p>Does this ring a<a class="indexterm" id="id718"/> bell? If not, please feel free to see the following image, where the following two lines of code also show the city names instead of the anonymous points:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; plot(mds, type = 'n')</strong></span>
<span class="strong"><strong>&gt; text(mds[, 1], mds[, 2], labels(eurodist))</strong></span>
</pre></div><div class="mediaobject"><img alt="Multidimensional Scaling" src="graphics/2028OS_09_17.jpg"/></div><p>Although the <span class="emphasis"><em>y</em></span> axis is flipped, which you can fix by multiplying the second argument of text by -1, we have just rendered a European map of cities from the distance matrix—without any further <a class="indexterm" id="id719"/>geographical data. I find this rather impressive.</p><p>Please find more data visualization tricks and methods in <a class="link" href="ch13.html" title="Chapter 13. Data Around Us">Chapter 13</a>, <span class="emphasis"><em>Data Around Us</em></span>.</p><p>Now let's see how to apply MDS on non-geographic data that was not prepared with a view to its being a distance matrix. Let's get back to the <code class="literal">mtcars</code> dataset:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; mds &lt;- cmdscale(dist(mtcars))</strong></span>
<span class="strong"><strong>&gt; plot(mds, type = 'n')</strong></span>
<span class="strong"><strong>&gt; text(mds[, 1], mds[, 2], rownames(mds))</strong></span>
</pre></div><div class="mediaobject"><img alt="Multidimensional Scaling" src="graphics/2028OS_09_18.jpg"/></div><p>The plot shows the 32 cars of the original dataset scattered in a two-dimensional space. The distance between the elements was computed by MDS, which took into account all the 11 original <a class="indexterm" id="id720"/>variables, and it's very easy to identify the similar and very different car types. We will cover these topics in more details in the next chapter, <a class="link" href="ch10.html" title="Chapter 10. Classification and Clustering">Chapter 10</a>, <span class="emphasis"><em>Classification and Clustering</em></span>.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec67"/>Summary</h1></div></div></div><p>In this chapter, we covered a number of ways to deal with multivariate data to reduce the number of available dimensions in the means of artificially computed continuous variables and to identify underlying, latent, and similarly numeric variables. On the other hand, sometimes it's rather difficult to describe reality with numbers and we should rather think in categories.</p><p>The next chapter will introduce new methods to define data types (clusters) and will also demonstrate how to classify elements with the help of available training data.</p></div></body></html>