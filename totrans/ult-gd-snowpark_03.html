<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-29"><a id="_idTextAnchor028" class="calibre6 pcalibre1 pcalibre"/>2</h1>
<h1 id="_idParaDest-30" class="calibre5"><a id="_idTextAnchor029" class="calibre6 pcalibre1 pcalibre"/>Establishing a Foundation with Snowpark</h1>
<p class="calibre3"><a id="_idTextAnchor030" class="calibre6 pcalibre1 pcalibre"/>In the previous chapter, you learned the basics of Snowpark, its benefits, and how it allows developers to develop complex data applications using Python. This chapter will focus on establishing a solid foundation with Snowpark. Here, you will learn how to configure and operate Snowpark, select a coding style and structure, and explore Snowpark’s fundamentals in depth. This will help you acquire practical knowledge and skills to work efficiently with Snowpark, including setting up the environment, structuring the code, and utilizing it for different workloads.</p>
<p class="calibre3">In this chapter, we’re going to cover the following main topics:</p>
<ul class="calibre15">
<li class="calibre14">Configuring the Snowpark development environment</li>
<li class="calibre14">Operating with Snowpark</li>
<li class="calibre14">Establishing a project structure for Snowpark</li>
</ul>
<h1 id="_idParaDest-31" class="calibre5"><a id="_idTextAnchor031" class="calibre6 pcalibre1 pcalibre"/>Technical requirements</h1>
<p class="calibre3">For this chapter, you’ll require an active Snowflake account and Python installed with Anaconda configured locally. You can refer to the following documentation for installation instructions:</p>
<ul class="calibre15">
<li class="calibre14">You can sign up for a Snowflake Trial account at <a href="https://signup.snowflake.com/" class="calibre6 pcalibre1 pcalibre">https://signup.snowflake.com/</a></li>
<li class="calibre14">To configure Anaconda, follow the guide at  <a href="https://code.visualstudio.com/docs/python/python-tutorial" class="calibre6 pcalibre1 pcalibre">https://conda.io/projects/conda/en/latest/user-guide/getting-started.html</a>.</li>
<li class="calibre14">In addition, to install and set up Python for VS Code, follow the guide at <a href="https://code.visualstudio.com/docs/python/python-tutorial" class="calibre6 pcalibre1 pcalibre">https://code.visualstudio.com/docs/python/python-tutorial</a></li>
<li class="calibre14">To learn how to operate a Jupyter Notebook in VS Code, go to <a href="https://code.visualstudio.com/docs/datascience/jupyter-notebooks" class="calibre6 pcalibre1 pcalibre">https://code.visualstudio.com/docs/datascience/jupyter-notebooks</a></li>
</ul>
<p class="calibre3">The supporting materials for this chapter are available in this book’s GitHub repository at <a href="https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark" class="calibre6 pcalibre1 pcalibre">https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark</a>.</p>
<h1 id="_idParaDest-32" class="calibre5"><a id="_idTextAnchor032" class="calibre6 pcalibre1 pcalibre"/>Configuring the Snowpark development environment</h1>
<p class="calibre3">The first <a id="_idIndexMarker028" class="calibre6 pcalibre1 pcalibre"/>step in developing Snowpark is to set up the Snowpark development environment. Developers have much flexibility regarding <a id="_idIndexMarker029" class="calibre6 pcalibre1 pcalibre"/>which <strong class="bold">integrated development environments</strong> (<strong class="bold">IDEs</strong>) they can use to get started with Snowpark; the only thing they need to do is install the Snowpark client <strong class="bold">application programmable interface</strong> (<strong class="bold">API</strong>) and <a id="_idIndexMarker030" class="calibre6 pcalibre1 pcalibre"/>connect to their Snowflake account. The development environment can be a local Python environment that contains your favorite IDE or the new Snowflake Python worksheets in Snowsight. This section will cover setting up a Snowpark Python worksheet and the local development environment.</p>
<h2 id="_idParaDest-33" class="calibre7"><a id="_idTextAnchor033" class="calibre6 pcalibre1 pcalibre"/>Snowpark Python worksheet</h2>
<p class="calibre3">Snowflake<a id="_idIndexMarker031" class="calibre6 pcalibre1 pcalibre"/> released <em class="italic">Snowflake Worksheets for Python</em>, a new type of worksheet in Snowsight for developing<a id="_idIndexMarker032" class="calibre6 pcalibre1 pcalibre"/> a Python-based Snowpark environment from within Snowflake. This game-changing feature allows developers to easily leverage the power of Snowpark Python within Snowsight to perform <a id="_idIndexMarker033" class="calibre6 pcalibre1 pcalibre"/>data processing and create data pipelines, <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models, and applications by integrating Snowpark Python directly into the browser without setting up Python environments or installing open source libraries on the client. Instead, developers can easily use pre-existing packages from Anaconda or import their own Python files from stages into the Worksheet. In addition, they can quickly deploy the Python worksheets as a stored procedure.</p>
<h3 class="calibre9">Prerequisites for using Python worksheets</h3>
<p class="calibre3">To enable and use Snowflake Python worksheets, you must first acknowledge the Anaconda<a id="_idIndexMarker034" class="calibre6 pcalibre1 pcalibre"/> terms of service in Snowsight. The following steps are to be performed by the organization’s administrator:</p>
<ol class="calibre13">
<li class="calibre14">First, you must sign into your Snowflake account. In the <strong class="bold">Switch Role</strong> section on the left panel, switch to the <strong class="bold">ORGADMIN</strong> role in the user context:</li>
</ol>
<div><div><img alt="Figure 2.1 – The ORGADMIN role in Snowflake" src="img/B19923_02_1.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.1 – The ORGADMIN role in Snowflake</p>
<ol class="calibre13">
<li value="2" class="calibre14">Then, go to <strong class="bold">Admin</strong> | <strong class="bold">Billing &amp; </strong><strong class="bold">Terms</strong>:</li>
</ol>
<div><div><img alt="Figure 2.2 – Admin | Billing &amp; Terms" src="img/B19923_02_2.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Admin | Billing &amp; Terms</p>
<ol class="calibre13">
<li value="3" class="calibre14">Click <strong class="bold">Enable</strong> next <a id="_idIndexMarker035" class="calibre6 pcalibre1 pcalibre"/>to <strong class="bold">Anaconda </strong><strong class="bold">Python packages</strong>:</li>
</ol>
<div><div><img alt="Figure 2.3 – Enabling Anaconda Python packages" src="img/B19923_02_3.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Enabling Anaconda Python packages</p>
<ol class="calibre13">
<li value="4" class="calibre14">You’ll get a popup, as shown in the following screenshot. Click <strong class="bold">Acknowledge &amp; Continue</strong> to enable the packages:</li>
</ol>
<div><div><img alt="Figure 2.4 – Anaconda’s Terms and Services" src="img/B19923_02_4.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Anaconda’s Terms and Services</p>
<p class="calibre3">You’ll need to enable <a id="_idIndexMarker036" class="calibre6 pcalibre1 pcalibre"/>the Anaconda packages every time you create a new Snowflake environment. Now that we’ve enabled the packages, let’s see how we can work with Python worksheets.</p>
<h3 class="calibre9">Database and schema creation in Snowflake Snowsight</h3>
<p class="calibre3">To create <a id="_idIndexMarker037" class="calibre6 pcalibre1 pcalibre"/>a database called <code>SNOWPARK_DEFINITIVE_GUIDE</code> and a schema called <code>MY_SCHEMA</code> using Snowflake’s Snowsight UI interface, perform the following steps:</p>
<ol class="calibre13">
<li class="calibre14">Go to the Snowsight web interface, log in using your Snowflake credentials, and navigate to the <strong class="bold">Data</strong> | <strong class="bold">Databases</strong> section. This is typically located in the left-hand navigation menu:</li>
</ol>
<div><div><img alt="Figure 2.5 – The Databases section" src="img/B19923_02_5.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.5 – The Databases section</p>
<ol class="calibre13">
<li value="2" class="calibre14">Look for a<a id="_idIndexMarker038" class="calibre6 pcalibre1 pcalibre"/> button or link that says <strong class="bold">+ Database</strong> in the top-right corner and click it. In the dialogue box that appears, enter <strong class="source-inline1">SNOWPARK_DEFINITIVE_GUIDE</strong> as the name for the new database. Optionally, you can specify other settings, such as <strong class="bold">Comment</strong>, if needed, and click <strong class="bold">Create</strong>:</li>
</ol>
<div><div><img alt="Figure 2.6 – The New Database dialogue" src="img/B19923_02_6.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.6 – The New Database dialogue</p>
<ol class="calibre13">
<li value="3" class="calibre14">After creating the database, click on the <strong class="source-inline1">SNOWPARK_DEFINITIVE_GUIDE</strong> database. This will take you to the following page:</li>
</ol>
<div><div><img alt="Figure 2.7 – The SNOWPARK_DEFINTIVE_GUIDE page" src="img/B19923_02_7.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.7 – The SNOWPARK_DEFINTIVE_GUIDE page</p>
<ol class="calibre13">
<li value="4" class="calibre14">On this <a id="_idIndexMarker039" class="calibre6 pcalibre1 pcalibre"/>page, look for a button or link that says <strong class="bold">+ Schema</strong> and click on it. In the dialogue box that appears, enter <strong class="source-inline1">MY_SCHEMA</strong> as the name for the new schema. Optionally, you can specify other settings, such as <strong class="bold">Comment</strong> and <strong class="bold">Managed access</strong>. Click <strong class="bold">Create</strong> to create the schema:</li>
</ol>
<div><div><img alt="Figure 2.8 – The New Schema dialogue" src="img/B19923_02_8.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.8 – The New Schema dialogue</p>
<p class="calibre3">We’ll consistently utilize the same database and schema throughout this book unless explicitly instructed otherwise in specific chapters.</p>
<h3 class="calibre9">Working with Python worksheets in Snowflake</h3>
<p class="calibre3">Python worksheets <a id="_idIndexMarker040" class="calibre6 pcalibre1 pcalibre"/>come with a sample code template that can be used as a starter. The whole experience is embedded in Snowflake based on the Snowsight UI. Perform the following steps to create and work with Python worksheets in Snowflake:</p>
<ol class="calibre13">
<li class="calibre14">Navigate to the <strong class="bold">Worksheets</strong> section from the menu in the Snowsight UI:</li>
</ol>
<div><div><img alt="Figure 2.9 – The Worksheets menu option" src="img/B19923_02_9.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.9 – The Worksheets menu option</p>
<ol class="calibre13">
<li value="2" class="calibre14">On the <strong class="bold">Worksheets</strong> pane, click the <strong class="bold">+</strong> icon on the right and select <strong class="bold">Python Worksheet</strong> to create a Python worksheet:</li>
</ol>
<div><div><img alt="Figure 2.10 – Creating a Python worksheet" src="img/B19923_02_10.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.10 – Creating a Python worksheet</p>
<ol class="calibre13">
<li value="3" class="calibre14">Select a database and schema so that you can work in the worksheet context. The scope of the Python code will operate based on these details:</li>
</ol>
<div><div><img alt="Figure 2.11 – Worksheet context" src="img/B19923_02_11.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.11 – Worksheet context</p>
<ol class="calibre13">
<li value="4" class="calibre14">The<a id="_idIndexMarker041" class="calibre6 pcalibre1 pcalibre"/> worksheet has a handler function that’s invoked when the worksheet is executed. The <strong class="bold">Settings</strong> menu allows you to configure your worksheets. The default handler function is <strong class="source-inline1">main()</strong>, and the return type can be specified as <strong class="source-inline1">Table()</strong>, <strong class="source-inline1">Variant</strong>, or <strong class="source-inline1">String</strong>. The result will be shown in the format you choose:</li>
</ol>
<div><div><img alt="Figure 2.12 – Worksheet settings" src="img/B19923_02_12.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.12 – Worksheet settings</p>
<p class="calibre3">Worksheet preferences such as linting and line wrapping can also be customized here.</p>
<ol class="calibre13">
<li value="5" class="calibre14">The Python code can be executed by clicking on the <strong class="bold">RUN</strong> button at the top right of the worksheet:</li>
</ol>
<div><div><img alt="Figure 2.13 – Executing the worksheet" src="img/B19923_02_13.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.13 – Executing the worksheet</p>
<p class="calibre3">You can<a id="_idIndexMarker042" class="calibre6 pcalibre1 pcalibre"/> start developing and executing the Python code within the worksheet to see the results.</p>
<h3 class="calibre9">Managing Anaconda packages in Python worksheets</h3>
<p class="calibre3">Python worksheets <a id="_idIndexMarker043" class="calibre6 pcalibre1 pcalibre"/>come with an <a id="_idIndexMarker044" class="calibre6 pcalibre1 pcalibre"/>integrated Anaconda environment that supports importing the most common Anaconda libraries without having to worry about managing dependencies. It also supports importing custom packages from the internal stage. In this section, we will look at managing Anaconda packages in Python. To do this, perform the following steps:</p>
<ol class="calibre13">
<li class="calibre14">Navigate to the <strong class="bold">Packages</strong> tab from the menu and select <strong class="bold">Anaconda Packages</strong>. This will show a list of pre-installed packages and their versions:</li>
</ol>
<div><div><img alt="Figure 2.14 – Anaconda Packages" src="img/B19923_02_14.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.14 – Anaconda Packages</p>
<ol class="calibre13">
<li value="2" class="calibre14">You can <a id="_idIndexMarker045" class="calibre6 pcalibre1 pcalibre"/>search for <a id="_idIndexMarker046" class="calibre6 pcalibre1 pcalibre"/>and install the required packages by using the search bar:</li>
</ol>
<div><div><img alt="Figure 2.15 – Searching for packages" src="img/B19923_02_15.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.15 – Searching for packages</p>
<ol class="calibre13">
<li value="3" class="calibre14">You can also modify the version of the packages by selecting the available versions<a id="_idIndexMarker047" class="calibre6 pcalibre1 pcalibre"/> from the <a id="_idIndexMarker048" class="calibre6 pcalibre1 pcalibre"/>respective dropdown:</li>
</ol>
<div><div><img alt="Figure 2.16 – Package versioning" src="img/B19923_02_16.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.16 – Package versioning</p>
<ol class="calibre13">
<li value="4" class="calibre14">You can execute the following query to check the available Python packages in Snowflake using SQL:<pre class="source-code">
SELECT distinct package_name
FROM information_schema.packages
WHERE language = 'python';</pre><p class="calibre3">This query obtains the results from the packages view in the information schema:</p></li> </ol>
<div><div><img alt="Figure 2.17 – Anaconda packages query" src="img/B19923_02_17.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.17 – Anaconda packages query</p>
<ol class="calibre13">
<li value="5" class="calibre14">If you <a id="_idIndexMarker049" class="calibre6 pcalibre1 pcalibre"/>need to check the <a id="_idIndexMarker050" class="calibre6 pcalibre1 pcalibre"/>version information of specific packages, you can filter the query by the package’s name:<pre class="source-code">
SELECT *
FROM information_schema.packages
WHERE (package_name='numpy' AND language = 'python');</pre><p class="calibre3">The output of the preceding code is as follows:</p></li> </ol>
<div><div><img alt="Figure 2.18 – Anaconda Python package query" src="img/B19923_02_18.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.18 – Anaconda Python package query</p>
<p class="calibre3">This query shows the list of Anaconda packages that are available in my account. In the next section, we’ll learn how to manage custom Python packages.</p>
<h3 class="calibre9">Managing custom packages in Python worksheets</h3>
<p class="calibre3">Python worksheets <a id="_idIndexMarker051" class="calibre6 pcalibre1 pcalibre"/>also support the ability to import custom Python packages that can be used in Python code. However, the package must be uploaded into the internal stage, which is the storage part of the Snowflake account, and imported from it. In Snowsight, you can load files into a <a id="_idIndexMarker052" class="calibre6 pcalibre1 pcalibre"/>named internal stage area, allowing you to conveniently view and utilize them within your Python worksheets or load the data into a table using SQL. However, it’s important to note that Snowsight does not support loading files into user or table stages.</p>
<p class="calibre3">To create a named internal stage, ensure that the role you are using has the <strong class="bold">USAGE</strong> privilege on the relevant database and schema and the <strong class="bold">CREATE STAGE</strong> privilege on the schema. Let’s begin:</p>
<ol class="calibre13">
<li class="calibre14">Sign in to Snowsight.</li>
<li class="calibre14">Access the <strong class="bold">Data</strong> section and navigate to <strong class="bold">Databases</strong>.</li>
<li class="calibre14">Choose the desired database and schema where you want to create the stage and load files.</li>
<li class="calibre14">Click <strong class="bold">Create</strong>, select <strong class="bold">Stage</strong>, and click <strong class="bold">Snowflake Managed</strong>:</li>
</ol>
<div><div><img alt="Figure 2.19 – Creating a stage" src="img/B19923_02_19.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.19 – Creating a stage</p>
<ol class="calibre13">
<li value="5" class="calibre14">Provide a name for the stage and opt-in to enable a directory table for the stage, allowing you to visualize the files. Once you’re done, click <strong class="bold">Create</strong>:</li>
</ol>
<div><div><img alt="Figure 2.20 – Creating an internal stage" src="img/B19923_02_20.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.20 – Creating an internal stage</p>
<p class="calibre3">To load <a id="_idIndexMarker053" class="calibre6 pcalibre1 pcalibre"/>files into a Snowflake-managed <a id="_idIndexMarker054" class="calibre6 pcalibre1 pcalibre"/>named internal stage using Snowsight, follow these steps:</p>
<ol class="calibre13">
<li class="calibre14">Access the <strong class="bold">Data</strong> section and select <strong class="bold">Databases</strong>.</li>
<li class="calibre14">Choose the database schema where you created the stage and select the stage itself.</li>
<li class="calibre14">Click <strong class="bold">+ Files</strong> to load the desired files into the stage.</li>
<li class="calibre14">In the <strong class="bold">Upload Your Files</strong> dialogue that appears, select the files you want to upload (multiple files can be selected simultaneously):</li>
</ol>
<div><div><img alt="Figure 2.21 – Uploading a custom package" src="img/B19923_02_21.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.21 – Uploading a custom package</p>
<ol class="calibre13">
<li value="5" class="calibre14">Optionally, specify<a id="_idIndexMarker055" class="calibre6 pcalibre1 pcalibre"/> or <a id="_idIndexMarker056" class="calibre6 pcalibre1 pcalibre"/>create a path within the stage where you want to store the files.</li>
<li class="calibre14">Click <strong class="bold">Upload</strong>.</li>
<li class="calibre14">Once the files have been successfully loaded into the stage, you can perform various actions, depending on your requirements.</li>
</ol>
<p class="calibre3">With that, the package has been uploaded into the stage and is ready to be imported.</p>
<p class="callout-heading">Important note</p>
<p class="callout">The maximum permissible file size is 50 MB. You need a role with the <strong class="bold">USAGE</strong> privilege on the database and schema and the <strong class="bold">WRITE</strong> privilege on the stage.</p>
<p class="calibre3">Once the stage has been created and the package has been uploaded, you can import the module so that you can use it in the program. To import a package, follow these steps:</p>
<ol class="calibre13">
<li class="calibre14">From the <strong class="bold">Packages</strong> menu, select <strong class="bold">Stage Packages</strong>:</li>
</ol>
<div><div><img alt="Figure 2.22 – Stage Packages" src="img/B19923_02_22.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.22 – Stage Packages</p>
<ol class="calibre13">
<li value="2" class="calibre14">Enter<a id="_idIndexMarker057" class="calibre6 pcalibre1 pcalibre"/> the path to the <a id="_idIndexMarker058" class="calibre6 pcalibre1 pcalibre"/>package. You can refer to the stage in the same database and schema using <strong class="source-inline1">@Stage/path/to/package.py</strong>. If the stage is in a different database and schema, you can use <strong class="source-inline1">@Database.Schema.Stage/path/to/package.py</strong>:</li>
</ol>
<div><div><img alt="Figure 2.23 – Importing a stage package" src="img/B19923_02_23.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.23 – Importing a stage package</p>
<ol class="calibre13">
<li value="3" class="calibre14">Click <strong class="bold">Import</strong> to install the package. The module will now be visible under <strong class="bold">Installed Packages</strong>:</li>
</ol>
<div><div><img alt="Figure 2.24 – Installing a stage package" src="img/B19923_02_24.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.24 – Installing a stage package</p>
<ol class="calibre13">
<li value="4" class="calibre14">You can<a id="_idIndexMarker059" class="calibre6 pcalibre1 pcalibre"/> import the package <a id="_idIndexMarker060" class="calibre6 pcalibre1 pcalibre"/>into your code by using <strong class="source-inline1">import &lt;</strong><strong class="source-inline1">package name&gt;</strong>.</li>
</ol>
<p class="calibre3">In the next section, we will cover how to deploy a Python stored procedure using the UI.</p>
<h3 class="calibre9">Deploying a Python stored procedure</h3>
<p class="calibre3">The <a id="_idIndexMarker061" class="calibre6 pcalibre1 pcalibre"/>Python script in our worksheet can be seamlessly deployed as a stored procedure. This can then be used in a<a id="_idIndexMarker062" class="calibre6 pcalibre1 pcalibre"/> regular SQL context or scheduled to execute as a task. To deploy a stored procedure, follow these steps:</p>
<ol class="calibre13">
<li class="calibre14">Click the <strong class="bold">Deploy</strong> button at the top right of the worksheet.</li>
<li class="calibre14">Give the stored procedure a name; an optional comment can also be specified that provides information about it. If the stored procedure already exists, then check the <strong class="bold">Replace if exists</strong> box to replace the existing stored procedure:</li>
</ol>
<div><div><img alt="Figure 2.25 – Deploying a stored procedure" src="img/B19923_02_25.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.25 – Deploying a stored procedure</p>
<ol class="calibre13">
<li value="3" class="calibre14">Click <strong class="bold">Deploy</strong> to deploy the stored procedure. It will be deployed under the database <a id="_idIndexMarker063" class="calibre6 pcalibre1 pcalibre"/>and <a id="_idIndexMarker064" class="calibre6 pcalibre1 pcalibre"/>schema you provided in the worksheet context.</li>
<li class="calibre14">The stored procedure can now be executed in your worksheets using the following code:<pre class="source-code">
CALL DATABASE.SCHEMA.MY_STORED_PROCEDURE();</pre></li> </ol>
<p class="calibre3">In the next section, we will cover the various features of Python worksheets.</p>
<h3 class="calibre9">Features of Python worksheets</h3>
<p class="calibre3">Python worksheets <a id="_idIndexMarker065" class="calibre6 pcalibre1 pcalibre"/>consist of some features that are developer-friendly and support productivity. Here are some of the distinct characteristics of Python worksheets:</p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold">Interactive Python environment</strong>: Worksheets <a id="_idIndexMarker066" class="calibre6 pcalibre1 pcalibre"/>support the Python language and support Snowpark <strong class="bold">user-defined functions</strong> (<strong class="bold">UDFs</strong>) and stored procedures. Features <a id="_idIndexMarker067" class="calibre6 pcalibre1 pcalibre"/>such as syntax highlighting, type-sensitive autocomplete for keywords, and handy diagnostics such as undeclared variables or invalid method usage help increase developers’ productivity:</li>
</ul>
<div><div><img alt="Figure 2.26 – Interactive Python environment" src="img/B19923_02_26.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.26 – Interactive Python environment</p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold">Python libraries support</strong>: Python<a id="_idIndexMarker068" class="calibre6 pcalibre1 pcalibre"/> worksheets come with an integrated Anaconda<a id="_idIndexMarker069" class="calibre6 pcalibre1 pcalibre"/> environment that supports importing the most common Anaconda libraries without the need to worry about managing dependencies. It also supports importing custom packages from the internal stage.</li>
<li class="calibre14"><strong class="bold">Snowpark debugging</strong>: Python <a id="_idIndexMarker070" class="calibre6 pcalibre1 pcalibre"/>worksheets can display the results from a DataFrame inside Snowsight using the <strong class="source-inline1">show()</strong> or <strong class="source-inline1">print()</strong> function. The preview of the DataFrame can also be returned to display the output in tabular format, which is very useful when it comes to debugging Python programs:</li>
</ul>
<div><div><img alt="Figure 2.27 –  Snowpark debugging" src="img/B19923_02_27.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.27 –  Snowpark debugging</p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold">Collaboration</strong>: Snowpark <a id="_idIndexMarker071" class="calibre6 pcalibre1 pcalibre"/>Python worksheets can be shared with developers. This makes collaboration much easier as multiple developers can access and work on the worksheet at the same time:</li>
</ul>
<div><div><img alt="Figure 2.28 – Worksheet collaboration" src="img/B19923_02_28.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.28 – Worksheet collaboration</p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold">Charts and data exploration</strong>: Python<a id="_idIndexMarker072" class="calibre6 pcalibre1 pcalibre"/> worksheets provide a convenient way to visualize <a id="_idIndexMarker073" class="calibre6 pcalibre1 pcalibre"/>data and DataFrames as charts, which helps with data exploration and provides an easy way to analyze the data quickly:</li>
</ul>
<div><div><img alt="Figure 2.29 – Charts and data exploration" src="img/B19923_02_29.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.29 – Charts and data exploration</p>
<p class="calibre3">In the next section, we will cover the limitations of Python worksheets.</p>
<h3 class="calibre9">Limitations of Python worksheets</h3>
<p class="calibre3">Python worksheets are relatively new in Snowsight and have some limitations in terms of functionality. Let’s <a id="_idIndexMarker074" class="calibre6 pcalibre1 pcalibre"/>take a closer look:</p>
<ul class="calibre15">
<li class="calibre14">Logging levels lower than <strong class="bold">WARN</strong> do not appear in the <strong class="bold">Results</strong> area by default.</li>
<li class="calibre14">Python worksheets do not support breakpoints or selective execution of the portions of code. Instead, the entire code is run in the worksheet.</li>
<li class="calibre14">The Python worksheet cannot display images or other artifacts of the Python code it generates. It can only show results that are returned through the DataFrame.</li>
<li class="calibre14">It only supports Python 3.8 and libraries that use Python 3.8.</li>
</ul>
<p class="calibre3">Snowflake constantly updates features for Python worksheets, improving them for developers. As a result, Python worksheets make it easier and faster for developers to code in Python from within the Snowflake environment.</p>
<h2 id="_idParaDest-34" class="calibre7"><a id="_idTextAnchor034" class="calibre6 pcalibre1 pcalibre"/>Snowpark development in a local environment</h2>
<p class="calibre3">Snowpark can <a id="_idIndexMarker075" class="calibre6 pcalibre1 pcalibre"/>also be developed conveniently from a local environment with your favorite IDE. The key benefit of Snowflake’s partnership with Anaconda is Anaconda’s Snowflake Snowpark for Python channel, which contains the necessary Python packages to run Snowpark. To use this channel, Anaconda or Miniconda should be installed and set up on the machine. In this section, we will walk you through how to set up Snowpark in your local development environment using Anaconda.</p>
<p class="callout-heading">Important note</p>
<p class="callout">This book will utilize Anaconda for Snowpark development as it is the recommended approach and utilizes the benefits of Anaconda’s package manager to easily set up and manage the Snowpark development environment.</p>
<p class="calibre3">The Snowpark API requires Python 3.8 to be installed. Snowflake recommends that you use Anaconda for easy package management. You can check out the Python version you have by running the following command<a id="_idIndexMarker076" class="calibre6 pcalibre1 pcalibre"/> in the <strong class="bold">command-line </strong><strong class="bold">interface</strong> (<strong class="bold">CLI</strong>):</p>
<pre class="console">
python –-version</pre> <p class="calibre3">Your output should look similar to the following:</p>
<div><div><img alt="Figure 2.30 – Python CLI version" src="img/B19923_02_30.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.30 – Python CLI version</p>
<p class="calibre3">You can also <a id="_idIndexMarker077" class="calibre6 pcalibre1 pcalibre"/>check the Python version from within the Python code by running the following command:</p>
<pre class="source-code">
from platform import python_version
print(python_version())</pre> <p class="calibre3">This will print an output similar to the following:</p>
<div><div><img alt="Figure 2.31 – Python version" src="img/B19923_02_31.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.31 – Python version</p>
<p class="calibre3">Once Python has been installed, the virtual environment needs to be created. So, let’s create one.</p>
<h3 class="calibre9">Creating a virtual environment</h3>
<p class="calibre3">It’s<a id="_idIndexMarker078" class="calibre6 pcalibre1 pcalibre"/> recommended that you create a Python virtual environment to ensure a seamless developer experience when working with Snowpark; it isolates the Snowpark API and allows you to manage all dependencies that are required for development. To create a virtual environment using Anaconda, run the following command:</p>
<pre class="console">
conda create --name def_gui_3.8_env --override-channels --channel https://repo.anaconda.com/pkgs/snowflake python=3.8</pre> <p class="calibre3">This command<a id="_idIndexMarker079" class="calibre6 pcalibre1 pcalibre"/> creates a new Python virtual environment named <code>def_gui_3.8_env</code> with Python 3.8 and installs the necessary packages, such as <code>numpy</code> and <code>pandas</code>, from Anaconda’s Snowflake channel.</p>
<h3 class="calibre9">Installing the Snowpark Python package</h3>
<p class="calibre3">Before installing the package, let's activate our Python virtual environment using the following command:</p>
<pre class="console">
conda activate def_gui_3.8_env</pre> <p class="calibre3">The Snowpark package can be installed from Anaconda’s Snowflake channel using the following command:</p>
<pre class="console">
conda install --channel https://repo.anaconda.com/pkgs/snowflake Snowflake-snowpark-python</pre> <p class="calibre3">Next, let’s install some additional packages.</p>
<h3 class="calibre9">Installing additional Python packages</h3>
<p class="calibre3">To install <a id="_idIndexMarker080" class="calibre6 pcalibre1 pcalibre"/>additional packages that are necessary for development, such as <code>pandas</code> and <code>numpy</code>, you can use the same Anaconda Snowflake channel:</p>
<pre class="console">
conda install --channel https://repo.anaconda.com/pkgs/snowflake numpy pandas</pre> <p class="calibre3">The virtual environment is now ready for development and is connected to your favorite IDE, Jupyter Notebook, or VS Code development. Similarly, we can install an IPython notebook.</p>
<h3 class="calibre9">Configuring Jupyter Notebook</h3>
<p class="calibre3">Jupyter Notebook<a id="_idIndexMarker081" class="calibre6 pcalibre1 pcalibre"/> is one<a id="_idIndexMarker082" class="calibre6 pcalibre1 pcalibre"/> of the most popular IDEs for developers. In this section, we will cover how to configure the Jupyter IDE for Snowpark since the examples in this book use Jupyter. Jupyter Notebook needs to be installed in your local environment. The Jupyter environment<a id="_idIndexMarker083" class="calibre6 pcalibre1 pcalibre"/> comes installed alongside Anaconda. So, let’s open Jupyter Notebook:</p>
<ol class="calibre13">
<li class="calibre14">The <strong class="source-inline1">def_gui_3.8_env</strong> virtual environment must be activated for development if it is not activated in the previous section. To activate the virtual environment, run the following command:<pre class="source-code">
<strong class="bold1">conda activate def_gui_3.8_env</strong></pre></li> <li class="calibre14">Launch Jupyter Notebook by running the following command:<pre class="source-code">
<strong class="bold1">jupyter notebook</strong></pre></li> <li class="calibre14">On the Jupyter Notebook web page, click the <strong class="bold">New</strong> button in the top-right corner. From the drop-down menu, select <strong class="bold">Python 3</strong> under the <strong class="bold">Notebooks</strong> section. This will open a new notebook with an empty cell that’s ready for code execution.</li>
</ol>
<p class="callout-heading">Important note</p>
<p class="callout">This book will utilize Jupyter Notebook for all the examples.</p>
<h3 class="calibre9">Importing Snowpark modules</h3>
<p class="calibre3">The <a id="_idIndexMarker084" class="calibre6 pcalibre1 pcalibre"/>Python classes for the Snowpark API are part of the <code>snowflake.snowpark</code> module. You can import particular classes from the module by specifying their names. For example, to import the <code>average</code> function, you can use the following code:</p>
<pre class="source-code">
from snowflake.snowpark.functions import avg</pre> <p class="calibre3">Now that the development environment has been set up, let’s learn how to operate with Snowpark.</p>
<h1 id="_idParaDest-35" class="calibre5"><a id="_idTextAnchor035" class="calibre6 pcalibre1 pcalibre"/>Operating with Snowpark</h1>
<p class="calibre3">Snowpark<a id="_idIndexMarker085" class="calibre6 pcalibre1 pcalibre"/> for Python consists of client APIs, UDFs, and stored procedures that execute directly on the Python engine. The following screenshot shows the various Snowpark objects that you can choose from:</p>
<div><div><img alt="Figure 2.32 – Snowpark Python objects" src="img/B19923_02_32.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.32 – Snowpark Python objects</p>
<p class="calibre3">Snowpark uses DataFrame objects to query and process data. The guiding principle in operating with Snowpark is to keep the data in Snowflake and process it right within Snowflake using the various Snowflake objects. The following figure shows Snowpark’s architecture:</p>
<div><div><img alt="Figure 2.33 – Snowpark Python architecture" src="img/B19923_02_33.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.33 – Snowpark Python architecture</p>
<p class="calibre3">In the next section, we will cover the Python Engine.</p>
<h2 id="_idParaDest-36" class="calibre7"><a id="_idTextAnchor036" class="calibre6 pcalibre1 pcalibre"/>The Python Engine</h2>
<p class="calibre3">The Python Engine<a id="_idIndexMarker086" class="calibre6 pcalibre1 pcalibre"/> is an Anaconda-powered secure<a id="_idIndexMarker087" class="calibre6 pcalibre1 pcalibre"/> sandboxed Python environment that’s executed on top of Snowflake’s virtual warehouse and hosted on Snowflake’s compute infrastructure. This lets you process data using Python without the need to extract the data outside the environment. The Python Engine consists of the UDF engine and the stored procedure engine. The UDF engine is a restricted engine that cannot read or write data outside of Snowflake, whereas the stored procedure engine is more permissive and consists of a session object for interacting with the Snowflake database.</p>
<h2 id="_idParaDest-37" class="calibre7"><a id="_idTextAnchor037" class="calibre6 pcalibre1 pcalibre"/>Client APIs</h2>
<p class="calibre3">The client API is <a id="_idIndexMarker088" class="calibre6 pcalibre1 pcalibre"/>the <code>snowflake-snowpark-python</code> library <a id="_idIndexMarker089" class="calibre6 pcalibre1 pcalibre"/>and it can be installed in any Python environment. It provides a session and the DataFrame APIs as methods to support queries being pushed down in Snowflake’s Python Engine. The client APIs consist of notable objects, including sessions, DataFrames, and <a id="_idIndexMarker090" class="calibre6 pcalibre1 pcalibre"/>more. Let’s look at these in detail.</p>
<h3 class="calibre9">Working with sessions</h3>
<p class="calibre3">The Snowpark session<a id="_idIndexMarker091" class="calibre6 pcalibre1 pcalibre"/> is part of the Snowpark API and connects to Snowflake to interact with it and perform operations using Snowpark objects. The <code>Session</code> function in Snowpark API is responsible for operating with the session. To import the <code>Session</code> function for Snowpark, run the following command:</p>
<pre class="source-code">
from snowflake.snowpark import Session</pre> <p class="calibre3">The Snowpark session consists of a Python dictionary containing the parameter values that are required to establish a connection to Snowflake:</p>
<pre class="source-code">
connection_parameters = {
    "account": "&lt;your snowflake account identifier&gt;",
    "user": "&lt;your snowflake username&gt;",
    "password": "&lt;your snowflake password&gt;",
    "role": "&lt;your snowflake role&gt;", # optional
    "warehouse": "&lt;your snowflake warehouse&gt;",  # optional
    "database": "&lt;your snowflake database&gt;",  # optional
    "schema": "&lt;your snowflake schema&gt;" # optional
}</pre> <p class="calibre3">The connection consists of the following mandatory parameters:</p>
<ul class="calibre15">
<li class="calibre14"><strong class="source-inline1">Account</strong>: The Snowflake account identifier is <strong class="source-inline1">&lt;orgname&gt;-&lt;account_name&gt;</strong>. There is no need to specify the <strong class="source-inline1">snowflakecomputing.com</strong> suffix.</li>
<li class="calibre14"><strong class="source-inline1">User</strong>: The username of the Snowflake user.</li>
<li class="calibre14"><strong class="source-inline1">Password</strong>: The password to authenticate to Snowflake.</li>
</ul>
<p class="calibre3">The following are the optional parameters that can be passed to the connection:</p>
<ul class="calibre15">
<li class="calibre14"><strong class="source-inline1">role</strong>: The role to be used in the Snowpark session. If left blank, the user’s default role will be used.</li>
<li class="calibre14"><strong class="source-inline1">warehouse</strong>: The warehouse that’s used to execute the process. If left blank, the user’s default warehouse will be used.</li>
<li class="calibre14"><strong class="source-inline1">database</strong>: The database for the context of execution. If left blank, the user’s default database will be used.</li>
<li class="calibre14"><strong class="source-inline1">schema</strong>: The schema for the context of execution. If left blank, the user’s default schema will be used.</li>
</ul>
<p class="calibre3">You can<a id="_idIndexMarker092" class="calibre6 pcalibre1 pcalibre"/> create the <code>session</code> object by passing this dictionary to the session builder, which is then used to establish the session:</p>
<pre class="source-code">
session = Session.builder.configs(connection_parameters).create()</pre> <p class="calibre3">Once the session has been created, the <code>session</code> object acts as a handle to interact with Snowflake through various methods to perform operations such as reading and manipulating data. Some of the session methods that can be used are shown here:</p>
<pre class="source-code">
print("Session Current Account:", session.get_current_account())</pre> <p class="calibre3">Finally, you can close the session by using the <code>close()</code> method. This terminates the session and the ongoing queries associated with it:</p>
<pre class="source-code">
session.close()</pre> <p class="calibre3">Snowflake recommends that you close the session after the process has been completed. The <code>Session</code> method can be used to establish a session and interact with the <code>Session</code> objects.</p>
<h4 class="calibre16">Advanced authentication</h4>
<p class="calibre3">Snowpark sessions <a id="_idIndexMarker093" class="calibre6 pcalibre1 pcalibre"/>also support advanced authentication, such as key pair authentication, if it’s been configured for the user connecting to Snowflake. The private key must be serialized and then passed to the connection object of the session builder. We will be using the hazmat crypto package to serialize the private key. This private key is provided <a id="_idIndexMarker094" class="calibre6 pcalibre1 pcalibre"/>as a variable:</p>
<pre class="source-code">
from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives import serialization
private_key_plain_text = '''-----BEGIN PRIVATE KEY-----
&lt; your private key &gt;
-----END PRIVATE KEY-----'''
private_key_passphrase = '&lt;your private key passphase&gt;'</pre> <p class="calibre3">We encode the private key using the passphrase and then serialize it to assign it to a variable that’s passed into the Snowflake connection object:</p>
<pre class="source-code">
private_key_encoded = private_key_plain_text.encode()
private_key_passphrase_encoded = private_key_passphrase.encode()
private_key_loaded = serialization.load_pem_private_key(
    private_key_encoded,
    password = private_key_passphrase_encoded,
    backend = default_backend()
)
private_key_serialized = private_key_loaded.private_bytes(
    encoding = serialization.Encoding.DER,
    format = serialization.PrivateFormat.PKCS8,
    encryption_algorithm = serialization.NoEncryption()
)</pre> <p class="calibre3">The connection parameters will have the private key passed in serialized form, and the session can be<a id="_idIndexMarker095" class="calibre6 pcalibre1 pcalibre"/> established using the session builder:</p>
<pre class="source-code">
connection_parameters = {
    "account": "&lt;your snowflake account identifier&gt;",
    "user": "&lt; your snowflake username&gt;",
    "private_key": private_key_serialized,
    "warehouse": "&lt;your snowflake warehouse&gt;",
    "database": "&lt;your snowflake database&gt;",
    "schema": "&lt;your snowflake schema&gt;"
}
session = Session.builder.configs(connection_parameters).create()</pre> <p class="calibre3">Next, let’s discuss Snowpark DataFrames.</p>
<h3 class="calibre9">Snowpark DataFrames</h3>
<p class="calibre3">Snowpark <a id="_idIndexMarker096" class="calibre6 pcalibre1 pcalibre"/>for Python consists of a client API that provides a DataFrame-based approach that queries and processes data with a DataFrame object. The following diagram explains the code blocks that are utilized:</p>
<p class="calibre3"> </p>
<div><div><img alt="Figure 2.34 – Snowpark DataFrame API" src="img/B19923_02_34.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.34 – Snowpark DataFrame API</p>
<p class="calibre3">The<a id="_idIndexMarker097" class="calibre6 pcalibre1 pcalibre"/> Snowpark DataFrame API yields more efficiency with less effort required by the developer as it has a concise syntax that is easy to understand and debug. The following figure compares a DataFrame and a SQL query:</p>
<div><div><img alt="Figure 2.35 – Snowpark DataFrame versus a query" src="img/B19923_02_35.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.35 – Snowpark DataFrame versus a query</p>
<p class="calibre3">In addition, Snowpark DataFrames can directly read data from tables, views, and <code>SELECT</code> statements that support pushdown so that they can be executed in Snowflake. The Snowpark <a id="_idIndexMarker098" class="calibre6 pcalibre1 pcalibre"/>client APIs also support converting pandas DataFrames into Snowflake DataFrames so that data can be written back into Snowflake. Finally, Snowpark DataFrames support lazy evaluation as data computation is performed once an action is invoked.</p>
<p class="callout-heading">Note</p>
<p class="callout">Lazy evaluation in Snowpark means that data processing operations are not executed immediately when they are defined. Instead, Snowpark builds a sequence of transformations without executing them until you explicitly request the result. This approach optimizes performance and resource usage, allowing you to construct complex data workflows efficiently and interactively. Lazy evaluation is a key feature for handling large datasets and optimizing data processing tasks in Snowpark.</p>
<h4 class="calibre16">Working with DataFrames</h4>
<p class="calibre3">DataFrames are<a id="_idIndexMarker099" class="calibre6 pcalibre1 pcalibre"/> the dataset objects in Snowpark in which data is queried and processed. They represent relational datasets that provide lazy evaluation. The DataFrame executes SQL in a push-down manner and can perform operations such as creating objects and reading, writing, and working with data from the Python code. Various methods in the <code>Session</code> object are used to work with DataFrames.</p>
<p class="calibre3">Let’s create an employee data table called <code>SAMPLE_EMPLOYEE_DATA</code>:</p>
<pre class="source-code">
session.sql('CREATE OR REPLACE TABLE SAMPLE_EMPLOYEE_DATA (id INT,name VARCHAR, age INT, email VARCHAR, city VARCHAR,country VARCHAR)').collect()</pre> <p class="calibre3">The preceding code will create a table with the required fields for employee data. Let’s insert some data into the table for operational purposes:</p>
<pre class="source-code">
session.sql("""
    INSERT INTO SAMPLE_EMPLOYEE_DATA VALUES
    (1,'John Doe',25,'johndoe@example.com','New York','USA'),
    (2,'Jane Smith',30,'janesmith@example.com','Los Angeles','USA'),
    (3,'Michael Johnson',35,'michaeljohnson@example.com','London', 
       'UK'),
    (4,'Sarah Williams',28,'sarahwilliams@example.com','Leeds', 
       'UK'),
    (5,'David Brown',32,'davidbrown@example.com','Tokyo','Japan'),
    (6,'Emily Davis',29,'emilydavis@example.com','Sydney',
       'Australia'),
    (7,'James Miller',27,'jamesmiller@example.com','Dallas','USA'),
    (8,'Emma Wilson',33,'emmawilson@example.com','Berlin','Germany'),
    (9,'Alexander Taylor',31,'alexandertaylor@example.com',
       'Rome','Italy'),
    (10,'Olivia Anderson',26,'oliviaanderson@example.com',
        'Melbourne','Australia')
""").collect()</pre> <p class="calibre3">The preceding <a id="_idIndexMarker100" class="calibre6 pcalibre1 pcalibre"/>code will populate the table with the data that we can query. To query the data, we can directly pass the SQL statement:</p>
<pre class="source-code">
session.sql("SELECT count(*) FROM SAMPLE_EMPLOYEE_DATA").collect()</pre> <p class="calibre3">The preceding code will return the results after it’s been executed in Snowflake. We can also store these results in a DataFrame so that we can operate on it in Python:</p>
<pre class="source-code">
from snowflake.snowpark.functions import col
df_subset_row = session.table(
    "SAMPLE_EMPLOYEE_DATA").filter(col("id") == 1)</pre> <p class="calibre3">The <a id="_idIndexMarker101" class="calibre6 pcalibre1 pcalibre"/>following code will save the results in a <code>df_subset_row</code> DataFrame that can be displayed using <code>show()</code>:</p>
<pre class="source-code">
df_subset_row.show()</pre> <p class="calibre3">Here’s the output:</p>
<div><div><img alt="Figure 2.36 – DataFrame data" src="img/B19923_02_36.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.36 – DataFrame data</p>
<p class="calibre3">In the next section, we will look at Snowpark UDFs and stored procedures.</p>
<p class="callout-heading">A note on code snippets</p>
<p class="callout">The examples presented in the following section have been simplified intentionally. Our main objective is to grasp and distinguish the concepts, rather than delving into complex scenarios. However, we’ll delve into more sophisticated examples in the upcoming chapters.</p>
<h2 id="_idParaDest-38" class="calibre7"><a id="_idTextAnchor038" class="calibre6 pcalibre1 pcalibre"/>UDFs</h2>
<p class="calibre3">Snowpark for<a id="_idIndexMarker102" class="calibre6 pcalibre1 pcalibre"/> Python supports UDFs that allow developers<a id="_idIndexMarker103" class="calibre6 pcalibre1 pcalibre"/> to write reusable custom lambdas and functions to process the data through DataFrames. Like built-in functions, UDFs can be called from SQL, which enhances SQL with functionality that it doesn’t have or doesn’t do well. UDFs also provide a way to encapsulate functionality so that you can call it repeatedly from multiple places in your code. For example, you can write a UDF that returns a single value called a <em class="italic">scalar</em> function, also known as a UDF, or a group of values called a <em class="italic">tabular</em> function, also known as a <strong class="bold">user data table function</strong> (<strong class="bold">UDTF</strong>). These UDFs can be developed from within Python worksheets or by using Snowpark from your local development environment.</p>
<h3 class="calibre9">Scalar UDFs</h3>
<p class="calibre3">Scalar UDFs <a id="_idIndexMarker104" class="calibre6 pcalibre1 pcalibre"/>are invoked once per row and return one output row<a id="_idIndexMarker105" class="calibre6 pcalibre1 pcalibre"/> for each input row. These UDFs are called just like a standard SQL function, with columns or expressions as arguments. It produces a row consisting of a single column/value as output. The data gets processed in parallel across each node within a multi-node virtual warehouse.</p>
<h4 class="calibre16">Working with UDFs</h4>
<p class="calibre3">Once a<a id="_idIndexMarker106" class="calibre6 pcalibre1 pcalibre"/> Snowpark session has been created, the UDF can be turned into a standard function that can be registered in Snowflake:</p>
<pre class="source-code">
def &lt;main Python function name&gt;(&lt;arguments&gt;):
    return &lt;function output&gt;
from snowflake.snowpark.types \
    import &lt;specific Snowpark DataType object&gt;
snowpark_session.add_packages(
    'List of native packages in Anaconda Channel')
snowpark_session.add_import('Path to Local File')
snowpark_session.udf.register(
    func = &lt;Main Function Name&gt;
  , return_type = &lt;Return Type of Snowpark DataType object &gt;
  , input_types = &lt;[Input Types of Snowflake DataType object]&gt;
  , is_permanent = True
  , name = '&lt;UDF name&gt;'
  , replace = True
  , stage_location = '@&lt;UDF stage name&gt;'
)</pre> <p class="calibre3">The<a id="_idIndexMarker107" class="calibre6 pcalibre1 pcalibre"/> preceding template delineates the steps in creating a UDF in Snowflake using Python. It involves defining the primary Python function that will be used by the UDF and registering it in Snowflake. Next, the function and UDF names are specified, along with the Snowflake stage, where the UDF files will be uploaded. Finally, the required Snowpark <code>DataType</code> object for the UDF’s return value is imported, and its specific object is determined. This is not the only template we can follow – we can also leverage decorators to perform this. But for beginners, this can be very helpful to templatize and organize UDFS, UDTFs, and stored procedures.</p>
<p class="calibre3">Additionally, any necessary Snowpark <code>DataType</code> objects for the UDF’s input arguments are imported and determined. The template also allows you to include extra packages and imports in the UDF. We can also specify whether the UDF should be temporary and whether an existing UDF with the same name should be overwritten:</p>
<pre class="source-code">
def last_name_finder(input_name:str):
    last_name = input_name.split()[1]
    return last_name
from snowflake.snowpark.types \
    import StringType,IntegerType,ArrayType
test = session.udf.register(
    func = last_name_finder
  , return_type = StringType()
  , input_types = [StringType()]
  , is_permanent = True
  , name = 'LAST_NAME_FINDER'
  , replace = True
  , stage_location = '@MY_STAGE'
)</pre> <p class="calibre3">This simple<a id="_idIndexMarker108" class="calibre6 pcalibre1 pcalibre"/> function gets the input name and splits it so that it returns the last name. The UDF is registered to the internal <code>My_Stage</code> stage and is deployed into Snowflake. The UDF can be invoked directly in SQL as a function:</p>
<pre class="source-code">
session.sql('''SELECT
    NAME,
    LAST_NAME_FINDER(NAME) AS LAST_NAME
    FROM SAMPLE_EMPLOYEE_DATA
''').show()</pre> <p class="calibre3">The output is as follows:</p>
<div><div><img alt="Figure 2.37 – UDF Snowpark execution" src="img/B19923_02_37.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.37 – UDF Snowpark execution</p>
<p class="calibre3">In this example, we invoked the <code>LAST_NAME_FINDER</code> function with the <code>Name</code> column, which<a id="_idIndexMarker109" class="calibre6 pcalibre1 pcalibre"/> returned the last name by splitting it. The function can also be called within the <code>DataFrame</code> function, as follows:</p>
<pre class="source-code">
from snowflake.snowpark.functions import col, call_udf
df = session.table("SAMPLE_EMPLOYEE_DATA")
df.with_column(
    "last_name",call_udf("LAST_NAME_FINDER", col("name"))).show()</pre> <p class="calibre3">The preceding code generates the following output:</p>
<div><div><img alt="Figure 2.38 – UDF DataFrame execution" src="img/B19923_02_38.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.38 – UDF DataFrame execution</p>
<p class="calibre3">Next, let’s look into UDTFs.</p>
<h3 class="calibre9">UDTF</h3>
<p class="calibre3">Tabular UDFs, also<a id="_idIndexMarker110" class="calibre6 pcalibre1 pcalibre"/> known as UDTFs, require stateful operations to<a id="_idIndexMarker111" class="calibre6 pcalibre1 pcalibre"/> be performed on data batches and are invoked once per row, just like scalar UDFs, but they can return multiple rows as output for each input row. The UDTF handler method consists of an additional optional parameter that helps initialize the handler once for each partition and finalize processing for each section. A UDTF is a type of UDF that executes similarly to a UDF but with tabular output. Therefore, they can be developed in Python worksheets and Snowpark development environments.</p>
<h4 class="calibre16">Working with UDTFs</h4>
<p class="calibre3">Creating a UDTF in <a id="_idIndexMarker112" class="calibre6 pcalibre1 pcalibre"/>Snowpark is similar to creating a UDF in that after a Snowpark session is created, the UDTF can be made directly in a standard command that can be registered in Snowflake.</p>
<p class="calibre3">The following Snowpark UDTF template provides a basic outline for creating a UDTF in Snowpark using Python. The following code shows the key elements in this template:</p>
<pre class="source-code">
# Define Python class locally
'''
Define main Python class which is
leveraged to process partitions.
Executes in the following order:
- __init__ | Executes once per partition
- process | Executes once per input row within the partition
- end_partition | Executes once per partition
'''
class &lt;name of main Python class&gt; :
    '''
    Optional __init__ method to
    execute logic for a partition
    before breaking out into rows
    '''
    def __init__(self) :</pre> <p class="calibre3">This template <a id="_idIndexMarker113" class="calibre6 pcalibre1 pcalibre"/>creates a UDTF in Snowflake. First, a main Python handler class is defined for the UDTF, which can utilize other functions from the script or be imported from external sources. It is important to note that only one main Python handler class can be assigned to the UDTF.</p>
<p class="calibre3">In this template, you are expected to replace <code>&lt;name of main Python class&gt;</code> with a meaningful name for your UDTF class. This is the main class where you will define the logic for processing data within your UDTF.</p>
<p class="calibre3">The <code>__init__</code> method is marked as optional, meaning you may or may not include it in your UDTF implementation. If you choose to include it, this method will execute once per partition before breaking out into individual rows.</p>
<p class="calibre3">You can use <a id="_idIndexMarker114" class="calibre6 pcalibre1 pcalibre"/>the <code>__init__</code> method to perform any partition-level setup or initialization specific to your UDTF. For example, you might use it to initialize variables and open connections or set up data structures that will be used throughout the UDTF’s execution:</p>
<pre class="source-code">
'''
Method to process each input row
within a partition, returning a
tabular value as tuples.
'''
def process(self, &lt;arguments&gt;) :
    '''
    Enter Python code here that
    executes for each input row.
    This likely ends with a set of yield
    clauses that output tuples,
    for example:
    '''
    yield (&lt;field_1_value_1&gt;, &lt;field_2_value_1&gt;, ...)
    yield (&lt;field_1_value_2&gt;, &lt;field_2_value_2&gt;, ...)
    '''
    Alternatively, this may end with
    a single return clause containing
    an iterable of tuples, for example:
    '''
    return [
        (&lt;field_1_value_1&gt;, &lt;field_2_value_1&gt;, ...)
      , (&lt;field_1_value_2&gt;, &lt;field_2_value_2&gt;, ...)
    ]</pre> <p class="calibre3">This method is <a id="_idIndexMarker115" class="calibre6 pcalibre1 pcalibre"/>responsible for processing each input row within a partition and generating tabular data as tuples. Inside the <code>process</code> method, you can write custom Python code that executes for every input row in the partition. The key part of this method is the usage of <code>yield</code> statements or a <code>return</code> statement to produce tuples as output.</p>
<p class="calibre3">In terms of <code>yield</code> statements, you can output one or more tuples for each input row, allowing for flexibility in generating tabular results. Alternatively, you can use a <code>return</code> statement with a list of tuples to achieve the same result. In essence, the <code>process</code> method serves as the core logic for your UDTF, where you manipulate and transform data from each input row into tabular format, making it suitable for further processing or analysis:</p>
<pre class="source-code">
'''
Optional end_partition method to
execute logic for a partition
after processing all input rows
'''
def end_partition(self) :
    # Python code at the partition level
    '''
    This ends with a set of yield
    clauses that output tuples,
    for example:
    '''
    yield (&lt;field_1_value_1&gt;, &lt;field_2_value_1&gt;, ...)
    yield (&lt;field_1_value_2&gt;, &lt;field_2_value_2&gt;, ...)
    '''
    Alternatively, this ends with
    a single return clause containing
    an iterable of tuples, for example:
    '''
    return [
        (&lt;field_1_value_1&gt;, &lt;field_2_value_1&gt;, ...)
      , (&lt;field_1_value_2&gt;, &lt;field_2_value_2&gt;, ...)
    ]</pre> <p class="calibre3">This method <a id="_idIndexMarker116" class="calibre6 pcalibre1 pcalibre"/>is used to execute logic that’s specific to a partition after processing all the input rows in that partition. Inside the <code>end_partition</code> method, you can write custom Python code that performs calculations or generates results based on the data that’s processed within that partition. This method can also be used to yield or return tabular data as tuples, similar to the <code>process</code> method, but this data typically represents aggregated or summarized information for the entire partition.</p>
<p class="calibre3">You have the option to use <code>yield</code> statements to output one or more tuples, or you can use a <code>return</code> statement with a list of tuples to provide the partition-level result. This allows you to perform partition-specific operations and return the results in a structured format.</p>
<p class="calibre3">The <code>end_partition</code> method in a Snowpark UDTF template is used for executing partition-level logic and returning tabular data or results specific to that partition after processing all input rows within it. It’s especially useful for tasks such as aggregations or calculations, which require data from the entire partition.</p>
<p class="calibre3">The following code template provides details on how to register a UDTF in Snowflake and the corresponding options to define the UDTF:</p>
<pre class="source-code">
from snowflake.snowpark.types import StructType, StructField
from snowflake.snowpark.types \
    import &lt;specific Snowpark DataType objects&gt;
snowpark_session.add_packages(
    '&lt;list of required packages natively available in Snowflake( 
        i.e. included in Anaconda Snowpark channel)&gt;')
snowpark_session.add_import('&lt;path\\to\\local\\directory\\or\\file&gt;')
snowpark_session.udtf.register(
    handler = &lt;name of main Python class&gt;
  , output_schema = StructType(
        &lt;list of StructField objects with specific field \
        name and Snowpark DataType objects&gt;)
  , input_types = &lt;list of input DataType() \
        objects for input parameters&gt;
  , is_permanent = True
  , name = '&lt;UDTF name&gt;'
  , replace = True
  , stage_location = '@&lt;UDTF stage name&gt;'
)</pre> <p class="calibre3">The <code>udtf()</code> method <a id="_idIndexMarker117" class="calibre6 pcalibre1 pcalibre"/>of the Snowflake Snowpark <code>Session</code> object is used to create the UDTF in Snowflake. The process involves several steps: determining the Python class that the UDTF will use, specifying the name of the UDTF within Snowflake (it can be a fully qualified name or created in the same namespace as the Snowpark <code>Session</code> object), and providing the name of the Snowflake stage where the UDTF files will be uploaded.</p>
<p class="calibre3">Specific Snowpark <code>DataType</code> objects are imported to define the structure of the UDTF. This includes importing objects for defining tabular structures, such as table schemas (using <code>StructType</code>) and fields within a table (using <code>StructField</code>). Furthermore, a specific Snowpark <code>DataType</code> object is imported for the values that are passed into and returned by the UDTF. The output schema of the UDTF is defined using the imported Snowpark <code>DataType</code> objects, embedding them into <code>StructField </code>and <code>StructType</code> objects. Additionally, a list of specific Snowpark <code>DataType</code> objects is defined for the<a id="_idIndexMarker118" class="calibre6 pcalibre1 pcalibre"/> input arguments of the UDTF. It is crucial to include all these <code>DataType</code> objects in the import and ensure they match the expected arguments that are passed to the <code>process</code> method within the <code>handler</code> class.</p>
<p class="calibre3">The template allows a temporary UDTF to be created that only exists within the specific Snowflake Snowpark <code>Session</code> object. Additionally, an option exists to overwrite an existing UDTF with the same name; an error will be returned if it’s set to <code>False</code> and a UDTF already exists. Lastly, the template briefly mentions adding additional packages and imports to the UDTF, which is optional and can be done using the provided rows.</p>
<p class="calibre3">The following example illustrates how to use a Snowpark UDTF to calculate the averages of numeric data within Snowflake tables. This showcases the practical application of UDTFs in Snowpark for custom data processing tasks:</p>
<pre class="source-code">
class CalculateAverage:
    def __init__(self) :
        self._values = []
    def process(self, input_measure: int) :
        self._values.append(input_measure)
    def end_partition(self) :
        values_list = self._values
        average = sum(values_list) / len(values_list)
        yield(average ,)</pre> <p class="calibre3">The <code>CalculateAverage</code> Snowpark UDTF is designed to compute the average of a numeric column within a Snowflake table. It does this by accumulating the input values for each partition of the data and then calculating the average when the partition ends.</p>
<p class="calibre3">The <code>process</code> method collects input values one by one and stores them in a list. When the partition ends (in the <code>end_partition</code> method), it calculates the average by summing up all the collected values and dividing by the count of values. Finally, it yields the calculated <a id="_idIndexMarker119" class="calibre6 pcalibre1 pcalibre"/>average as the UDTF’s output. This UDTF simplifies the process of computing averages in Snowflake SQL queries, especially when dealing with large datasets:</p>
<pre class="source-code">
from snowflake.snowpark.types import StructType, StructField
from snowflake.snowpark.types \
    import FloatType,IntegerType,StringType
output_schema = StructType([
    StructField("Avg_Age", FloatType())
])
session.udtf.register(
    handler = CalculateAverage
  , output_schema = output_schema
  , input_types = [IntegerType()]
  , is_permanent = True
  , name = 'AVERAGE_AGE'
  , replace = True
  , stage_location = '@MY_STAGE'
)</pre> <p class="calibre3">In this example, we’re creating a UDTF function called <code>Average_Age</code> that calculates the average age by getting the age as input. The function is uploaded into <code>MY_STAGE</code> and registered in Snowflake.</p>
<p class="calibre3">The function can<a id="_idIndexMarker120" class="calibre6 pcalibre1 pcalibre"/> be executed to get the average age per country from the sample employee data:</p>
<pre class="source-code">
session.sql('''
    SELECT
        COUNTRY, Avg_Age
    FROM
        SAMPLE_EMPLOYEE_DATA,
        table(AVERAGE_AGE(AGE) over (partition by COUNTRY))
''').show()</pre> <p class="calibre3">This will display the following output:</p>
<div><div><img alt="Figure 2.39 – UDTF Snowpark execution" src="img/B19923_02_39.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.39 – UDTF Snowpark execution</p>
<p class="calibre3">The output shows the execution output of the UDTF. In the next section, we will cover vectorized UDFs.</p>
<h3 class="calibre9">Vectorized UDFs</h3>
<p class="calibre3">Vectorized UDFs <a id="_idIndexMarker121" class="calibre6 pcalibre1 pcalibre"/>operate similarly to scalar UDFs in<a id="_idIndexMarker122" class="calibre6 pcalibre1 pcalibre"/> that they let you define Python functions that receive batches of input rows as pandas DataFrames and return collections of results as pandas arrays or series.</p>
<p class="calibre3">Vectorized UDFs<a id="_idIndexMarker123" class="calibre6 pcalibre1 pcalibre"/> parallelize the operation on batches of data and provide significant performance advantages on sets of rows compared to serial row processing. In addition, they reduce the complexity of using libraries that operate on pandas DataFrames and arrays.</p>
<h4 class="calibre16">Working with vectorized UDFs</h4>
<p class="calibre3">The<a id="_idIndexMarker124" class="calibre6 pcalibre1 pcalibre"/> same example we looked at previously can be executed in the Snowpark environment after establishing the session by passing a vectorized DataFrame as the input to the standard UDF:</p>
<pre class="source-code">
import pandas as pd
from snowflake.snowpark.functions import pandas_udf
from snowflake.snowpark.types \
    import IntegerType, PandasSeriesType,StringType
@pandas_udf(
    name='column_adder'
  , stage_location = '@MY_STAGE'
  , input_types=[PandasSeriesType(StringType()), \
        PandasSeriesType(StringType())]
  , return_type=PandasSeriesType(StringType())
  , is_permanent=True
  , replace=True)
def column_adder(
    column1: pd.Series, column2: pd.Series) -&gt; pd.Series:
    return column1 + "," + column2
df = session.table("SAMPLE_EMPLOYEE_DATA")
df.withColumn('City_Country', column_adder(col('CITY'), \
    col('COUNTRY'))).show()</pre> <p class="calibre3">The<a id="_idIndexMarker125" class="calibre6 pcalibre1 pcalibre"/> example UDF returns the city and country in the <code>CITY_COUNTRY</code> column for each row in the <code>Sample </code><code>Employee</code> table:</p>
<div><div><img alt="Figure 2.40 – Vectorized UDF in Snowpark" src="img/B19923_02_40.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.40 – Vectorized UDF in Snowpark</p>
<p class="calibre3">The output shows the execution output of the vectorized UDF. In the next section, we will cover stored procedures.</p>
<h3 class="calibre9">Stored procedures</h3>
<p class="calibre3">A Python <a id="_idIndexMarker126" class="calibre6 pcalibre1 pcalibre"/>stored procedure is a series of code statements you can parameterize and execute on demand. They run in a less restricted environment than UDFs and support interacting with Snowflake objects, as well as performing DDL and DML operations on tables.</p>
<p class="calibre3">Stored procedures<a id="_idIndexMarker127" class="calibre6 pcalibre1 pcalibre"/> in Snowpark are utilized for executing tasks and streams within Snowflake’s data processing framework. These stored procedures encapsulate specific logic or functionality, allowing users to perform various operations on data seamlessly. Tasks, which are often associated with batch processing, involve executing predefined actions or workflows on datasets at scheduled intervals. Stored procedures enable users to automate these tasks, ensuring consistent and efficient data processing.</p>
<p class="calibre3">On the other hand, streams are continuous data pipelines that capture changes in real-time from a data source. Stored procedures play a vital role in managing and processing streams by defining how incoming data should be processed and integrated into the target destination. With Snowpark, users can create stored procedures to handle these streaming data <a id="_idIndexMarker128" class="calibre6 pcalibre1 pcalibre"/>scenarios, including data transformation, filtering, and loading data into Snowflake tables.</p>
<h4 class="calibre16">Working with stored procedures</h4>
<p class="calibre3">A stored procedure<a id="_idIndexMarker129" class="calibre6 pcalibre1 pcalibre"/> can be created in Snowpark with the following template:</p>
<pre class="source-code">
# Define Python function locally
def &lt;Python Function Name&gt;(
    snowpark_session: snowflake.snowpark.Session, &lt;arguments&gt;):
    return &lt;Output&gt;
# Imports Required For Stored Procedure
from snowflake.snowpark.types \
    import &lt;specific Snowpark DataType object&gt;
# Optional: Import additional packages or files
snowpark_session.add_packages(
    'List of native packages in Anaconda Channel')
snowpark_session.add_import('Path to Local File')
# Upload Stored Procedure to Snowflake
snowpark_session.sproc.register(
    func = &lt;Function name to register&gt;
  , return_type = &lt;Return Type of Snowpark DataType object&gt;
  , input_types = &lt;[Input Types of Snowflake DataType object]&gt;
  , is_permanent = True
  , name = '&lt;Stored Procedure name&gt;'
  , replace = True
  , stage_location = '@&lt;Stored Procedure stage name&gt;'
    &lt;optional: , execute_as = 'CALLER'&gt;
)</pre> <p class="calibre3">Here, we <a id="_idIndexMarker130" class="calibre6 pcalibre1 pcalibre"/>define the main Python function that will be used in the stored procedure and an additional argument called <code>snowpark_session</code>, which allows us to interact with Snowflake objects. Next, we use the <code>sproc.register()</code> method to create the stored procedure, specifying the Python function, stored procedure’s name, and Snowflake stage for file uploads. Finally, we import specific Snowpark <code>DataType</code> objects for the stored procedure’s return value and input arguments.</p>
<p class="calibre3">The <code>snowflake_session</code> argument is implicitly understood and not included in the input arguments. Optional rows allow for additional packages and imports. Here, we can determine whether the stored procedure will be temporary. We can also decide whether to overwrite an existing one with the same name and specify whether it will execute as the caller or the owner:</p>
<pre class="source-code">
def subset_table(snowpark_session:Session):
    df = snowpark_session.table(
        'SAMPLE_EMPLOYEE_DATA').select("NAME","AGE")
    return df.collect()
from snowflake.snowpark.types import StringType
session.add_packages('snowflake-snowpark-python')
session.sproc.register(
    func = subset_table
  , return_type = StringType()
  , input_types = []
  , is_permanent = True
  , name = 'SPROC_SUBSET_TABLE'
  , replace = True
  , stage_location = '@MY_STAGE'
)</pre> <p class="calibre3">The<a id="_idIndexMarker131" class="calibre6 pcalibre1 pcalibre"/> stored procedure returns the column name and the age from the <code>Employee Data</code> table. It’s registered as <code>SPROC_SUBSET_TABLE</code> and uploaded through <code>My_Stage</code>:</p>
<pre class="source-code">
session.sql(''' CALL SPROC_SUBSET_TABLE()''').show()</pre> <p class="calibre3">Here’s the output:</p>
<div><div><img alt="Figure 2.41 – Stored procedure execution" src="img/B19923_02_41.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.41 – Stored procedure execution</p>
<p class="calibre3">The stored procedure can be executed by running the <code>CALL</code> command.</p>
<h3 class="calibre9">The difference between UDFs and stored procedures</h3>
<p class="calibre3">UDFs and<a id="_idIndexMarker132" class="calibre6 pcalibre1 pcalibre"/> stored procedures have significant differences<a id="_idIndexMarker133" class="calibre6 pcalibre1 pcalibre"/> in terms of functionality and usage. The following figure shows the basic differences between UDFs and stored procedures and what they’re used for:</p>
<div><div><img alt="Figure 2.42 – UDFs versus stored procedures" src="img/B19923_02_42.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.42 – UDFs versus stored procedures</p>
<p class="calibre3">The <a id="_idIndexMarker134" class="calibre6 pcalibre1 pcalibre"/>following table shows the differences and <a id="_idIndexMarker135" class="calibre6 pcalibre1 pcalibre"/>similarities between UDFs and stored procedures based on their properties:</p>
<table class="no-table-style" id="table001-1">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<tbody class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2"/>
<td class="no-table-style2">
<p class="calibre3"><strong class="bold">UDF</strong><strong class="bold" lang="en-US" xml:lang="en-US">s</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><strong class="bold">Stored Procedure</strong><strong class="bold" lang="en-US" xml:lang="en-US">s</strong></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3">Purpose</p>
</td>
<td class="no-table-style2">
<p class="calibre3">Perform calculations and return the results. UDFs require a value to be returned.</p>
</td>
<td class="no-table-style2">
<p class="calibre3">Perform complex operations by executing SQL statements. They do not require an explicit value to be returned.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3">Usage</p>
</td>
<td class="no-table-style2">
<p class="calibre3">UDFs can be used when logic needs to be called as part of SQL statements that return a value.</p>
</td>
<td class="no-table-style2">
<p class="calibre3">When database operations or administrative tasks need to be performed.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3">Output</p>
</td>
<td class="no-table-style2">
<p class="calibre3">UDFs always need to return a result.</p>
</td>
<td class="no-table-style2">
<p class="calibre3">Stored procedures don’t need to return a result.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3">Context</p>
</td>
<td class="no-table-style2">
<p class="calibre3">UDF return values are directly accessible in the SQL context.</p>
</td>
<td class="no-table-style2">
<p class="calibre3">Stored procedure return values are not accessible in the SQL context.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3">Execution</p>
</td>
<td class="no-table-style2">
<p class="calibre3">UDFs can be called in the context of another SQL statement. In addition, multiple UDFs can be invoked in a single SQL statement.</p>
</td>
<td class="no-table-style2">
<p class="calibre3">Stored procedures are called independently. Therefore, only a single stored procedure is invoked in a SQL statement.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3">Security</p>
</td>
<td class="no-table-style2">
<p class="calibre3">UDFs cannot access the database or perform operations directly on it.</p>
</td>
<td class="no-table-style2">
<p class="calibre3">Stored procedures can access the database and perform data operations on it.</p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 2.1 – Comparison of UDFs and stored procedures</p>
<p class="calibre3">Both stored<a id="_idIndexMarker136" class="calibre6 pcalibre1 pcalibre"/> procedures and UDFs can be used together <a id="_idIndexMarker137" class="calibre6 pcalibre1 pcalibre"/>to expand the capabilities of Python execution in Snowflake.</p>
<h1 id="_idParaDest-39" class="calibre5"><a id="_idTextAnchor039" class="calibre6 pcalibre1 pcalibre"/>Establishing a project structure for Snowpark</h1>
<p class="calibre3">To assist<a id="_idIndexMarker138" class="calibre6 pcalibre1 pcalibre"/> with the development of Snowpark in Python and to make it easy to create a Snowpark project, Snowflake has released Snowpark<a id="_idIndexMarker139" class="calibre6 pcalibre1 pcalibre"/> project templates for Python. These contain everything you’ll need for developing, testing, and deploying with Snowpark – they provide all the boilerplate required to develop UDFs and stored procedures, along with unit tests and even GitHub Actions workflow files for CI/CD.</p>
<p class="calibre3">The project template has been released as open source on GitHub, making it easy for developers to clone and use the project. To clone the project, follow these steps:</p>
<ol class="calibre13">
<li class="calibre14">Download the files or clone the repository from <a href="https://github.com/Snowflake-Labs/snowpark-python-template" class="calibre6 pcalibre1 pcalibre">https://github.com/Snowflake-Labs/snowpark-python-template</a>. A new GitHub repository can be created from the template by using the GitHub CLI, like so:<pre class="source-code">
<strong class="bold1">gh repo create &lt;new-repo-name&gt; --template="Snowflake-Labs/snowpark-python-template"</strong></pre><p class="calibre3">The new repository’s name needs to be specified. The repository will be similar to the Snowpark project template.</p></li> <li class="calibre14">Set up the<a id="_idIndexMarker140" class="calibre6 pcalibre1 pcalibre"/> following environment variables so that you can configure the necessary Snowflake details:<pre class="source-code">
<strong class="bold1">SNOWSQL_ACCOUNT=&lt;replace with your account identifier&gt;</strong>
<strong class="bold1">SNOWSQL_USER=&lt;replace with your username&gt;</strong>
<strong class="bold1">SNOWSQL_PWD=&lt;replace with your password&gt;</strong>
<strong class="bold1">SNOWSQL_DATABASE=&lt;replace with your database&gt;</strong>
<strong class="bold1">SNOWSQL_SCHEMA=&lt;replace with your schema&gt;</strong>
<strong class="bold1">SNOWSQL_WAREHOUSE=&lt;replace with your warehouse&gt;</strong></pre><p class="calibre3">These <a id="_idIndexMarker141" class="calibre6 pcalibre1 pcalibre"/>environment variables are required to connect to the Snowflake environment and for Snowpark to establish a session.</p></li> <li class="calibre14">Create an Anaconda virtual environment and install the dependencies from the <strong class="source-inline1">environment.yml</strong> file:<pre class="source-code">
<strong class="bold1">conda env create -f environment.yml</strong>
<code>snowpark</code> that can be used for development.</p></li> <li class="calibre14">You can test the connection and check if the environment has been set up by executing the <strong class="source-inline1">app.py</strong> stored procedure. Navigate to the project folder and run the following command:<pre class="source-code">
<strong class="bold1">python src/procs/app.py</strong></pre><p class="calibre3">This produces an output called <strong class="bold">Hello World</strong> that establishes a connection to Snowflake.</p></li> </ol>
<p class="calibre3">The Snowpark project supports functions and stored procedures. The project structure consists of the <code>procs</code> directory for stored procedures, the <code>udf</code> directory for UDFs, and the <code>util</code> directory <a id="_idIndexMarker142" class="calibre6 pcalibre1 pcalibre"/>for the utilities methods and classes that are shared between UDFs and stored procedures.</p>
<p class="calibre3">The <code>test</code> folder <a id="_idIndexMarker143" class="calibre6 pcalibre1 pcalibre"/>consists of the test cases that can be tested via <code>pytest</code>. There’s also a GitHub workflow that can be deployed via GitHub Actions. We will cover this in detail in the following chapters.</p>
<h1 id="_idParaDest-40" class="calibre5"><a id="_idTextAnchor040" class="calibre6 pcalibre1 pcalibre"/>Summary</h1>
<p class="calibre3">Snowpark is very versatile and supports complex development patterns. In this chapter, we learned how to configure the Snowpark development environment and different Snowpark objects, such as sessions, UDFs, and stored procedures, and how to use them. We also learned how to set up a Snowpark development project locally and inside a Python worksheet before looking at some sample code that we could use to start developing.</p>
<p class="calibre3">In the next chapter, we will cover how to perform data processing with Snowpark, as well as how to ingest, prepare, and analyze data.</p>
</div>
</body></html>