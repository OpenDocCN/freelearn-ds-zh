<html><head></head><body>
        

                            
                    <h1 class="header-title">Hadoop Design Consideration</h1>
                
            
            
                
<p>Big data does not necessarily mean huge data. If a dataset is small, it's very easy to analyze it. We can load it on to an Excel spreadsheet and do the required calculations. But, as the volume of data gets bigger, we have to find other alternatives to process it. We may have to load it to an RDMBS table and run a SQL query to find the trend and patterns on the given structure. Further, if the dataset format changes to something like email, then loading to RDBMS becomes a huge challenge. To add more complexity to it, if the data speed changes to something like real time, it becomes almost impossible to analyze the given dataset with traditional RDBMS-based tools. In the modern world, the term <em>big data</em> can be expressed using the five most famous <em>V</em>s. Following is the explanation of each <em>V</em> in a nutshell.</p>
<div><img class="alignnone size-full wp-image-72 image-border" src="img/21dfce7d-6161-4729-94cb-0e9c871736e3.png" style="width:35.92em;height:20.58em;"/></div>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Data structure principles</li>
<li>Installing Hadoop cluster</li>
<li>Exploring Hadoop architecture</li>
<li>Introducing YARN</li>
<li>Hadoop cluster composition</li>
<li>Hadoop file formats</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding data structure principles</h1>
                
            
            
                
<p class="mce-root">Let's go through some important data architecture principles:</p>
<ul>
<li><strong>Data is an asset to an enterprise</strong>: Data has a measurable value. It provides some real value to the enterprise. In modern times, data is treated like real gold.</li>
<li><strong>Data is shared enterprise-wide</strong>: Data is captured only once and then used and analyzed many times. Multiple users access the same data for different uses cases and requirements.</li>
<li><strong>Data governance</strong>: Data is governed to ensure data quality.</li>
<li><strong>Data management</strong>: Data needs to be managed to attain enterprise objectives.</li>
<li><strong>Data access</strong>: All users should have access to data.</li>
<li><strong>Data security</strong>: Data should be properly secured and protected.</li>
<li><strong>Data definition</strong>: Each attribute of the data needs to be consistently defined enterprise-wide.</li>
</ul>
<p class="mce-root">Now that we know the basics of big data and its principles, let's get into some real action.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Installing Hadoop cluster</h1>
                
            
            
                
<p class="mce-root">The following steps need to be performed in order to install Hadoop cluster. As the time of writing this book, Hadoop Version 2.7.3 is a stable release. We will install it.</p>
<ol>
<li>Check the Java version using the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>Java -version</strong><br/><strong>Java(TM) SE Runtime Environment (build 1.8.0_144-b01)</strong><br/><strong>Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)
You need to have Java 1.6 onwards  </strong></pre>
<ol start="2">
<li>Create a Hadoop user account on all the servers, including all NameNodes and DataNodes with the help of the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>useradd hadoop
passwd hadoop1</strong> </pre>
<p style="padding-left: 60px" class="mce-root">Assume that we have four servers and we have to create a Hadoop cluster using all four servers. The IPs of these four servers are as follows: <kbd>192.168.11.1</kbd>, <kbd>192.168.11.2</kbd>, <kbd>192.168.11.3</kbd>, and <kbd>192.168.11.4</kbd>. Out of these four servers, we will first use a server as a master server (NameNode) and all remaining servers will be used as slaves (DataNodes).</p>
<ol start="3">
<li class="mce-root">On both servers, NameNode and DataNodes, change the <kbd>/etc/hosts</kbd> file using the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>vi /etc/hosts--  </strong> </pre>
<ol start="4">
<li class="mce-root">Then add the following to all files on all servers:</li>
</ol>
<pre style="padding-left: 60px">NameNode 192.168.11.1
DataNode1 192.168.11.2
DataNode2 192.168.11.3
DataNode3 192.168.11.4 </pre>
<ol start="5">
<li class="mce-root">Now, set up SSH on NamesNodes and DataNodes:</li>
</ol>
<pre style="padding-left: 60px"><strong>su - hadoop
ssh-keygen -t rsa
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@namenode
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@datanode1
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@datanode2
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@datanode3
chmod 0600 ~/.ssh/authorized_keys
exit</strong></pre>
<ol start="6">
<li class="mce-root">Download and install Hadoop on NameNode and all DataNodes:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>mkdir /opt/hadoop<br/>cd /opt/hadoop<br/>wget http://www-eu.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz<br/>tar </strong><strong>-xvf hadoop-2.7.3.tar.gz <br/>mv Hadoop-2.7.3 hadoop<br/>chown -R hadoop /opt/hadoop<br/>cd /opt/hadoop/Hadoop</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Configuring Hadoop on NameNode</h1>
                
            
            
                
<p class="mce-root">Log in to NameNode:</p>
<pre><strong>cd /opt/Hadoop/conf
    
vi core-site.xml </strong> </pre>
<p>Find and change the following properties with these values:</p>
<table>
<tbody>
<tr>
<td>
<div><strong>Filename</strong></div>
</td>
<td>
<div><strong>Property name</strong></div>
</td>
<td>
<div><strong>Property value</strong></div>
</td>
</tr>
<tr>
<td><kbd>core-site.xml</kbd></td>
<td> <kbd>fs.default.name</kbd></td>
<td> <kbd>hdfs://namenode:9000/</kbd></td>
</tr>
<tr>
<td/>
<td><kbd>dfs.permissions</kbd></td>
<td> <kbd>False</kbd></td>
</tr>
<tr>
<td><kbd>hdfs-site.xml</kbd></td>
<td><kbd>dfs.data.dir</kbd></td>
<td><kbd>/opt/hadoop/hadoop/dfs/namenode/data</kbd></td>
</tr>
<tr>
<td/>
<td><kbd>dfs.name.dir</kbd></td>
<td><kbd>/opt/hadoop/hadoop/dfs/namenode</kbd></td>
</tr>
<tr>
<td/>
<td><kbd>dfs.replication</kbd></td>
<td><kbd>1</kbd></td>
</tr>
<tr>
<td><kbd>mapred-site.xml</kbd></td>
<td><kbd>mapred.job.tracker</kbd></td>
<td><kbd>namenode:9001 </kbd></td>
</tr>
</tbody>
</table>
<pre>    vi masters
    namenode
    
    vi slaves
    datanode1
    datanode2
    datanode3
  </pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Format NameNode</h1>
                
            
            
                
<p class="mce-root">The following code is used to format the NameNode:</p>
<pre><strong>    cd /opt/Hadoop/Hadoop/bin
    
    hadoop -namenode  -format
 </strong> </pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Start all services</h1>
                
            
            
                
<p class="mce-root">We start all the services with the following line of code:</p>
<pre>    ./start-all.sh
  </pre>
<p>For details about how to set up a Hadoop single-node and multi-node cluster, please use the following link: <a href="https://hadoop.apache.org/docs/r2.7.0/hadoop-project-dist/hadoop-common/ClusterSetup.html">https://hadoop.apache.org/docs/r2.7.0/hadoop-project-dist/hadoop-common/ClusterSetup.html</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exploring HDFS architecture</h1>
                
            
            
                
<p class="mce-root">The HDFS architecture is based on master and slave patterns. NameNode is a master node and all DataNodes are SlaveNodes. Following are some important points to be noted about these two nodes.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Defining NameNode</h1>
                
            
            
                
<p class="mce-root">The NameNode is a master node of all DataNodes in the Hadoop cluster. It stores only the metadata of files and directories stored in the form of a tree. The important point is NameNode never stores any other data other than metadata. NameNode keeps track of all data written to DataNodes in the form of blocks. The default block size is 256 MB (which is configurable). Without the NameNode, the data on the DataNodes filesystem cannot be read. The metadata is stored locally on the NameNode using two files—filesystem namespace image file, FSImage, and edit logs. FSImage is the snapshot of the filesystem from the start of the NameNode edit logs—all the changes of the filesystem since the NameNode started, when the NameNode starts, it reads FSImage file and edits log files. All the transactions (edits) are merged into the FSImage file. The FSImage file is written to disk and a new, empty edits log file is created to log all the edits. Since NameNode is not restarted very often, the edits log file becomes very large and unmanageable. When NameNode is restarted, it takes a very long time to restart it as all the edits need to be applied to the FSImage file. In the event of NameNode crashing, all the metadata in the edits log file will not be written to the FSImage file and will be lost.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Secondary NameNode</h1>
                
            
            
                
<p class="mce-root">The name secondary NameNode is confusing. It does not act as a NameNode. Its main function is to get the filesystem changes from the NameNode and merge it to NameNode FSImage at regular intervals. Writing edits log file changes to FSImage are called <strong>commits</strong>. Regular commits help to reduce the NameNode start time. The secondary NameNode is also known as the commit node.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">NameNode safe mode</h1>
                
            
            
                
<p class="mce-root">It is a read-only mode for the HDFS cluster. Clients are not allowed any modifications to the filesystem or blocks. During startup, NameNode automatically starts in safe mode, applies edits to FSImage, disables safe mode automatically, and restarts in normal mode.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">DataNode</h1>
                
            
            
                
<p class="mce-root">DataNodes are the workhorses of the Hadoop cluster. Their main function is to store and retrieve data in the form of blocks. They always communicate their status to the NameNode in the form of heartbeats. That's how NameNode keeps track of any DataNodes, whether they are alive or dead. DataNodes keep three copies of the blocks known and the replication factor. DataNodes communicate with other DataNodes to copy data blocks to maintain data replication.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Data replication</h1>
                
            
            
                
<p class="mce-root">HDFS architecture supports placing very large files across the machines in a cluster. Each file is stored as a series of blocks. In order to ensure fault tolerance, each block is replicated three times to three different machines. It is known as a replication factor, which can be changed at the cluster level or at the individual file level. It is a NameNode that makes all the decisions related to block replication. NameNode gets heartbeat and block reports from each DataNode. Heartbeat makes sure that the DataNode is alive. A block report contains a list of all blocks on a DataNode.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Rack awareness</h1>
                
            
            
                
<p class="mce-root">HDFS block placement will use rack awareness for fault tolerance by placing one block replica on a different rack, as shown in the following diagram:</p>
<div><img src="img/aba679b2-4019-46ed-aea6-8d2e45428bae.png"/></div>
<p>Let's understand the figure in detail:</p>
<ul>
<li>The first replica is placed on the same rack as the initiating request DataNode, for example, Rack 1 and DataNode 1</li>
<li>The second replica is placed on any DataNode of another rack, for example, Rack 2, DataNode 2</li>
<li>The third replica is placed on any DataNode of the same rack, for example, Rack 2, DataNode 3</li>
</ul>
<p class="mce-root">A custom rack topology script, which contains an algorithm to select appropriate DataNodes, can be developed using a Unix shell, Java, or Python. It can be activated on the cluster by changing the <kbd>topology.script.file.name</kbd> parameter in <kbd>Core-site.xml</kbd> file.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">HDFS WebUI</h1>
                
            
            
                
<p class="mce-root">The following table shows the services in the HDFS WebUI:</p>
<table>
<tbody>
<tr>
<td><strong>Service</strong></td>
<td><strong>Protocol</strong></td>
<td><strong>Port</strong></td>
<td><strong>URL</strong></td>
</tr>
<tr>
<td>NameNode WebUI</td>
<td>HTTP</td>
<td><kbd>50070</kbd></td>
<td><kbd>http://namenode:50070/</kbd></td>
</tr>
<tr>
<td>DataNode WebUI</td>
<td>HTTP</td>
<td><kbd>50075</kbd></td>
<td><kbd>http://datanode:50075/</kbd></td>
</tr>
<tr>
<td>Secondary NameNode</td>
<td>HTTP</td>
<td><kbd>50090</kbd></td>
<td><kbd>http://Snamenode:50090/</kbd></td>
</tr>
</tbody>
</table>


            

            
        
    

        

                            
                    <h1 class="header-title">Introducing YARN</h1>
                
            
            
                
<p class="mce-root">The <strong>Yet Another Resource Negotiator</strong> (<strong>YARN</strong>) separates the resource management, scheduling, and processing components. It helps to achieve 100% resource utilization of the cluster resources. YARN manages the CPU and memory of the cluster based on the Hadoop scheduler policy. YARN supports any type of application and is not restricted to just MapReduce. It supports applications written in any type of language, provided binaries can be installed on the Hadoop cluster.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">YARN architecture</h1>
                
            
            
                
<p class="mce-root">Let's understand the YARN architecture in detail in the following sections.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Resource manager</h1>
                
            
            
                
<p class="mce-root">The resource manager is responsible for tracking the resources in a cluster and scheduling applications. The resource manager has two main components: the scheduler and the applications manager.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Node manager</h1>
                
            
            
                
<p class="mce-root">The node manager is responsible for launching and managing containers on a node. Containers execute tasks as specified by the application master. It acts as a slave for the resource manager. Each node manager tracks the available data processing resources on its SlaveNode and sends regular reports to the resource manager. The processing resources in a Hadoop cluster are consumed in byte-size pieces called <strong>containers</strong>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Configuration of YARN</h1>
                
            
            
                
<p class="mce-root">You can perform the following steps for the configuration of YARN:</p>
<ol>
<li>Start Hadoop NameNode, secondary NameNode, and DataNode</li>
<li>Alter <kbd>yarn-env.sh</kbd>.</li>
</ol>
<p>Find corresponding XML files based on your Hadoop installation.</p>
<ol start="3">
<li class="mce-root">Add the following under the definition of <kbd>YARN_CONF_DIR</kbd>:</li>
</ol>
<pre style="padding-left: 60px">export HADOOP_CONF_DIR="${HADOOP_CONF_DIR:-$YARN_HOME/etc/hadoop}"
export HADOOP_COMMON_HOME="${HADOOP_COMMON_HOME:-$YARN_HOME}"
export HADOOP_HDFS_HOME="${HADOOP_HDFS_HOME:-$YARN_HOME}"  </pre>
<ol start="4">
<li>Alter <kbd>yarn-site.xml</kbd>:</li>
</ol>
<pre style="padding-left: 60px">&lt;?xml version="1.0"?&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
    &lt;value&gt;mapreduce.shuffle&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt; </pre>
<ol start="5">
<li>Alter <kbd>mapred-site.xml</kbd>:</li>
</ol>
<pre style="padding-left: 60px">&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet href="configuration.xsl"?&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;mapreduce.framework.name &lt;/name&gt;
    &lt;value&gt;yarn&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;  </pre>
<ol start="6">
<li>Start the YARN services:</li>
</ol>
<pre style="padding-left: 60px"><strong>yarn resourcemanager
yarn nodemanager  </strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Configuring HDFS high availability</h1>
                
            
            
                
<p class="mce-root">Let's take a look at the changes brought about in Hadoop over time.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">During Hadoop 1.x</h1>
                
            
            
                
<p class="mce-root">Hadoop 1.x started with the architecture of a single NameNode. All DataNodes used to send their block reports to that single NameNode. There was a secondary NameNode in the architecture, but its sole responsibility was to merge all edits to FSImage. With this architecture, the NameNode became the <strong>single point of failure</strong> (<strong>SPOF</strong>). Since it has all the metadata of all the DataNodes of the Hadoop cluster, in the event of NameNode crash, the Hadoop cluster becomes unavailable till the next restart of NameNode repair. If the NameNode cannot be recovered, then all the data in all the DataNodes would be completely lost. In the event of shutting down NameNode for planned maintenance, the HDFS becomes unavailable for normal use. Hence, it was necessary to protect the existing NameNode by taking frequent backups of the NameNode filesystem to minimize data loss.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">During Hadoop 2.x and onwards</h1>
                
            
            
                
<p class="mce-root">In order to overcome HDFS <strong>high availability</strong> (<strong>HA</strong>) problems and make NameNode a SPOF, the architecture has changed. The new architecture provides a running of two redundant NameNodes in the same cluster in an active/passive configuration with a hot standby. This allows a fast failover to a new NameNode in the event of a machine crashing, or a graceful administrator-initiated failover for the purpose of planned maintenance. The following two architectural options are provided for HDFS HA:</p>
<ul>
<li>Using shared storage</li>
<li>Using quorum journal manager</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">HDFS HA cluster using NFS</h1>
                
            
            
                
<p class="mce-root">The following diagram depicts the HDFS HA cluster using NFS for shared storage required by the NameNodes architecture:</p>
<div><img src="img/6b9bba22-b44f-4c0c-91b9-00fc64bd9794.png" style="width:43.58em;height:32.83em;"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Important architecture points</h1>
                
            
            
                
<p class="mce-root">Following are some important points to remember about the HDFS HA using shared storage architecture:</p>
<ul>
<li>In the cluster, there are two separate machines: active state NameNode and standby state NameNode.</li>
<li>At any given point in time, one-and-only, one of the NameNodes is in the active state, and the other is in the standby state.</li>
<li>The active NameNode manages the requests from all client DataNodes in the cluster, while the standby remains a slave.</li>
<li>All the DataNodes are configured in such a way that they send their block reports and heartbeats to both the active and standby NameNodes.</li>
<li>The standby NameNode keeps its state synchronized with the active NameNode.</li>
<li>Active and standby nodes both have access to a filesystem on a shared storage device (for example, an NFS mount from a NAS)</li>
<li>When a client makes any filesystem change, the active NameNode makes the corresponding change (edits) to the edit log file residing on the network shared directory.</li>
<li>The standby NameNode makes all the corresponding changes to its own namespace. That way, it remains in sync with the active NameNode.</li>
<li>In the event of the active NameNode being unavailable, the standby NameNode makes sure that it absorbs all the changes (edits) from the shared network directory and promotes itself to an active NameNode.</li>
<li>The Hadoop administrator should apply the fencing method to the shared storage to avoid a scenario that makes both the NameNodes active at a given time. In the event of failover, the fencing method cuts the access to the previous active NameNode to make any changes to the shared storage to ensure smooth failover to standby NameNode. After that, the standby NameNode becomes the active NameNode.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Configuration of HA NameNodes with shared storage</h1>
                
            
            
                
<p class="mce-root">Add the following properties to the <kbd>hdfs-site.xml</kbd>:</p>
<table>
<tbody>
<tr>
<td>
<div><strong>Property</strong></div>
</td>
<td>
<div><strong>Value</strong></div>
</td>
</tr>
<tr>
<td><kbd>dfs.nameservices</kbd></td>
<td><kbd>cluster_name</kbd></td>
</tr>
<tr>
<td><kbd>dfs.ha.namenodes.cluster_name</kbd></td>
<td><kbd>NN1</kbd>, <kbd>NN2</kbd></td>
</tr>
<tr>
<td><kbd>dfs.namenode.rpc-address.cluster_name.NN1</kbd></td>
<td><kbd>machine1:8020</kbd></td>
</tr>
<tr>
<td><kbd>dfs.namenode.rpc-address.cluster_name.NN2</kbd></td>
<td><kbd>machine2:8020</kbd></td>
</tr>
<tr>
<td><kbd>dfs.namenode.http-address.cluster_name.NN1</kbd></td>
<td><kbd>machine1:50070</kbd></td>
</tr>
<tr>
<td><kbd>dfs.namenode.http-address.cluster_name.NN2</kbd></td>
<td><kbd>machine2:50070</kbd></td>
</tr>
<tr>
<td><kbd>dfs.namenode.shared.edits.dir</kbd></td>
<td><kbd>file:///mnt/filer1/dfs/ha-name-dir-shared</kbd></td>
</tr>
<tr>
<td><kbd>dfs.client.failover.proxy.provider.cluster_name</kbd></td>
<td><kbd>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</kbd></td>
</tr>
<tr>
<td><kbd>dfs.ha.fencing.methods</kbd></td>
<td><kbd>sshfence</kbd></td>
</tr>
<tr>
<td><kbd>dfs.ha.fencing.ssh.private-key-files</kbd></td>
<td><kbd>/home/myuser/.ssh/id_rsa</kbd></td>
</tr>
<tr>
<td><kbd>dfs.ha.fencing.methods</kbd></td>
<td><kbd>sshfence([[username][:port]])</kbd></td>
</tr>
<tr>
<td><kbd>dfs.ha.fencing.ssh.connect-timeout</kbd></td>
<td><kbd>30000</kbd></td>
</tr>
</tbody>
</table>
<p>Add the following properties to <kbd>core-site.xml</kbd>:</p>
<table>
<tbody>
<tr>
<td><strong>Property</strong></td>
<td><strong>Value</strong></td>
</tr>
<tr>
<td><kbd>fs.defaultFS</kbd></td>
<td><kbd>hdfs://cluster_name</kbd></td>
</tr>
</tbody>
</table>


            

            
        
    

        

                            
                    <h1 class="header-title">HDFS HA cluster using the quorum journal manager</h1>
                
            
            
                
<p class="mce-root">The following diagram depicts the <strong>quorum journal manager</strong> (<strong>QJM</strong>) architecture to share edit logs between the active and standby NameNodes:</p>
<div><img src="img/d19fd8c6-61cf-47dc-b86b-a68d32a19151.png" style="width:45.42em;height:33.83em;"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Important architecture points</h1>
                
            
            
                
<p class="mce-root">Following are some important points to remember about the HDFS HA using the QJM architecture:</p>
<ul>
<li>In the cluster, there are two separate machines—the active state NameNode and standby state NameNode.</li>
<li>At any point in time, exactly one of the NameNodes is in an active state, and the other is in a standby state.</li>
<li>The active NameNode manages the requests from all client DataNodes in the cluster, while the standby remains a slave.</li>
<li>All the DataNodes are configured in such a way that they send their block reports and heartbeats to both active and standby NameNodes.</li>
<li>Both NameNodes, active and standby, remain synchronized with each other by communicating with a group of separate daemons called <strong>JournalNodes</strong> (<strong>JNs</strong>).</li>
<li>When a client makes any filesystem change, the active NameNode durably logs a record of the modification to the majority of these JNs.</li>
<li>The standby node immediately applies those changes to its own namespace by communicating with JNs.</li>
<li>In the event of the active NameNode being unavailable, the standby NameNode makes sure that it absorbs all the changes (edits) from JNs and promotes itself as an active NameNode.</li>
<li>To avoid a scenario that makes both the NameNodes active at a given time, the JNs will only ever allow a single NameNode to be a writer at a time. This allows the new active NameNode to safely proceed with failover.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Configuration of HA NameNodes with QJM</h1>
                
            
            
                
<p class="mce-root">Add the following properties to <kbd>hdfs-site.xml</kbd>:</p>
<table>
<tbody>
<tr>
<td><strong>Property</strong></td>
<td><strong>Value</strong></td>
</tr>
<tr>
<td><kbd>dfs.nameservices</kbd></td>
<td><kbd>cluster_name</kbd></td>
</tr>
<tr>
<td><kbd>dfs.ha.namenodes.cluster_name</kbd></td>
<td><kbd>NN1</kbd>, <kbd>NN2</kbd></td>
</tr>
<tr>
<td><kbd>dfs.namenode.rpc-address.cluster_name.NN1</kbd></td>
<td><kbd>machine1:8020</kbd></td>
</tr>
<tr>
<td><kbd>dfs.namenode.rpc-address.cluster_name.NN2</kbd></td>
<td><kbd>machine2:8020</kbd></td>
</tr>
<tr>
<td><kbd>dfs.namenode.http-address.cluster_name.NN1</kbd></td>
<td><kbd>machine1:50070</kbd></td>
</tr>
<tr>
<td><kbd>dfs.namenode.http-address.cluster_name.NN2</kbd></td>
<td><kbd>machine2:50070</kbd></td>
</tr>
<tr>
<td><kbd>dfs.namenode.shared.edits.dir</kbd></td>
<td><kbd>qjournal://node1:8485;node2:8485;node3:8485/cluster_name</kbd></td>
</tr>
<tr>
<td><kbd>dfs.client.failover.proxy.provider.cluster_name</kbd></td>
<td><kbd>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</kbd></td>
</tr>
<tr>
<td><kbd>dfs.ha.fencing.methods</kbd></td>
<td><kbd>sshfence</kbd></td>
</tr>
<tr>
<td><kbd>dfs.ha.fencing.ssh.private-key-files</kbd></td>
<td><kbd>/home/myuser/.ssh/id_rsa</kbd></td>
</tr>
<tr>
<td><kbd>dfs.ha.fencing.methods</kbd></td>
<td><kbd>sshfence([[username][:port]])</kbd></td>
</tr>
<tr>
<td>
<p class="mce-root"><kbd>dfs.ha.fencing.ssh.connect-timeout</kbd></p>
</td>
<td><kbd>30000</kbd></td>
</tr>
</tbody>
</table>
<p> </p>
<p>Add the following properties to <kbd>core-site.xml</kbd>:</p>
<table>
<tbody>
<tr>
<td><strong>Property</strong></td>
<td><strong>Value</strong></td>
</tr>
<tr>
<td><kbd>fs.defaultFS</kbd></td>
<td><kbd>hdfs://cluster_name</kbd></td>
</tr>
<tr>
<td><kbd>dfs.journalnode.edits.dir</kbd></td>
<td><kbd>/path/to/journal/node/local/datat</kbd></td>
</tr>
</tbody>
</table>


            

            
        
    

        

                            
                    <h1 class="header-title">Automatic failover</h1>
                
            
            
                
<p class="mce-root">It's very important to know that the above two architectures support only manual failover. In order to do automatic failover, we have to introduce two more components a ZooKeeper quorum, and the <strong>ZKFailoverController</strong> (<strong>ZKFC</strong>) process, and more configuration changes.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Important architecture points</h1>
                
            
            
                
<ul>
<li>Each NameNode, active and standby, runs the ZKFC process.</li>
<li>The state of the NameNode is monitored and managed by the ZKFC.</li>
<li>The ZKFC pings its local NameNode periodically to make sure that that the NameNode is alive. If it doesn't get the ping back, it will mark that NameNode unhealthy.</li>
<li>The healthy NameNode holds a special lock. If the NameNode becomes unhealthy, that lock will be automatically deleted.</li>
<li>If the local NameNode is healthy, and the ZKFC sees the lock is not currently held by any other NameNode, it will try to acquire the lock. If it is successful in acquiring the lock, then it has won the election. It is now the responsibility of this NameNode to run a failover to make its local NameNode active.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Configuring automatic failover</h1>
                
            
            
                
<p class="mce-root">Add the following properties to <kbd>hdfs-site.xml</kbd> to configure automatic failover:</p>
<table>
<tbody>
<tr>
<td><strong>Property</strong></td>
<td><strong>Value</strong></td>
</tr>
<tr>
<td><kbd>dfs.ha.automatic-failover.enabled</kbd></td>
<td><kbd>true</kbd></td>
</tr>
<tr>
<td><kbd>ha.zookeeper.quorum</kbd></td>
<td><kbd>zk1:2181</kbd>, <kbd>zk2:2181</kbd>, <kbd>zk3:2181</kbd></td>
</tr>
</tbody>
</table>


            

            
        
    

        

                            
                    <h1 class="header-title">Hadoop cluster composition</h1>
                
            
            
                
<p class="mce-root">As we know, a Hadoop cluster consists of master and slave servers: MasterNodes—to manage the infrastructure, and SlaveNodes—distributed data store and data processing. EdgeNodes are not a part of the Hadoop cluster. This machine is used to interact with the Hadoop cluster. Users are not given any permission to directly log in to any of the MasterNodes and DataNodes, but they can log in to the EdgeNode to run any jobs on the Hadoop cluster. No application data is stored on the EdgeNode. The data is always stored on the DataNodes on the Hadoop cluster. There can be more than one EdgeNode, depending on the number of users running jobs on the Hadoop cluster. If enough hardware is available, it's always better to host each master and DataNode on a separate machine. But, in a typical Hadoop cluster, there are three MasterNodes.</p>
<p>Please note that it is assumed that we are using HBase as a NoSQL datastore in our cluster.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Typical Hadoop cluster</h1>
                
            
            
                
<p>The Hadoop cluster composition will look like the following:</p>
<div><img src="img/d657d3de-3c51-464f-a724-3d74f6ff3430.png"/></div>
<p class="mce-root">The following are some hardware specifications to be taken into account:</p>
<ul>
<li>NameNode and standby NameNodes.</li>
<li>The memory requirement depends on the number of files and block replicas to be created. Typically, at least 64 GB - 96 GB memory is recommended for NameNodes.</li>
<li>NameNodes need reliable storage to host FSImage and edit logs. It is recommended that these MasterNodes should have at least 4 TB - 6 TB SAS storage. It is a good idea to have RAID 5 - 6 storage for NameNodes. If the cluster is a HA cluster, then plan your Hadoop cluster in such a way that JNs should be configured on the master node.</li>
</ul>
<p class="mce-root">As far as processors are concerned, it is recommended to have at least 2 quad core CPUs running at 2 GHz, to handle messaging traffic for the MasterNodes.</p>
<ul>
<li>DataNodes/SlaveNodes should have at least 64 GB RAM per node. It is recommended that, typically, 2 GB - 3 GB memory is required for each Hadoop daemon, such as DataNode, node manager ZooKeeper, and so on; 5 GB for OS and other services; and 5 GB - 8 GB for each MapReduce task.</li>
<li>DataNodes may have commodity storage with at least 8 TB - 10 TB disk storage with 7,200 RPM SATA drives. Hard disk configuration should be in <strong>Just a Bunch Of Disks</strong> (<strong>JBOD</strong>).</li>
<li>It is recommended to have at least 8 processors—2.5 GHz cores and 24 cores CPUs for all DataNodes.</li>
<li>It is recommended to have 1 GbE to 10 GbE network connectivity within each RACK. For all slaves, 1 GB network bandwidth, and for MasterNodes, 10 GB bandwidth is recommended.</li>
<li>If you plan to expand your Hadoop cluster in future, you can also add additional machines.</li>
</ul>
<p>Please read the following articles from Hortonworks and Cloudera for additional reference:</p>
<ul>
<li><a href="http://docs.hortonworks.com/HDPDocuments/HDP1/HDP-1.3.3/bk_cluster-planning-guide/content/conclusion.html">http://docs.hortonworks.com/HDPDocuments/HDP1/HDP-1.3.3/bk_cluster-planning-guide/content/conclusion.html</a></li>
<li><a href="http://blog.cloudera.com/blog/2013/08/how-to-select-the-right-hardware-for-your-new-hadoop-cluster/">http://blog.cloudera.com/blog/2013/08/how-to-select-the-right-hardware-for-your-new-hadoop-cluster/</a></li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Best practices Hadoop deployment</h1>
                
            
            
                
<p class="mce-root">Following are some best practices to be followed for Hadoop deployment:</p>
<ul>
<li><strong>Start small</strong>: Like other software projects, an implementation Hadoop also involves risks and cost. It's always better to set up a small Hadoop cluster of four nodes. This small cluster can be set up as <strong>proof of concept</strong> (<strong>POC</strong>). Before using any Hadoop component, it can be added to the existing Hadoop POC cluster as <strong>proof of technology</strong> (<strong>POT</strong>). It allows the infrastructure and development team to understand big data project requirements. After successful completion of POC and POT, additional nodes can be added to the existing cluster.</li>
<li><strong>Hadoop cluster monitoring</strong>: Proper monitoring of the NameNode and all DataNodes is required to understand the health of the cluster. It helps to take corrective actions in the event of node problems. If a service goes down, timely action can help avoid big problems in the future. Setting up Gangalia and Nagios are popular choices to configure alerts and monitoring. In the case of the Hortonworks cluster, Ambari monitoring, and the Cloudera cluster, Cloudera (CDH) manager monitoring can be an easy setup.</li>
<li><strong>Automated deployment</strong>: Use of tools like Puppet or Chef is essential for Hadoop deployment. It becomes super easy and productive to deploy the Hadoop cluster with automated tools instead of manual deployment. Give importance to data analysis and data processing using available tools/components. Give preference to using Hive or Pig scripts for problem solving rather than writing heavy, custom MapReduce code. The goal should be to develop less and analyze more.</li>
<li><strong>Implementation of HA</strong>: While deciding about HA infrastructure and architecture, careful consideration should be given to any increase in demand and data growth. In the event of any failure or crash, the system should be able to recover itself or failover to another data center/site.</li>
<li><strong>Security</strong>: Data needs to be protected by creating users and groups, and mapping users to the groups. Setting appropriate permissions and enforcing strong passwords should lock each user group down.</li>
<li><strong>Data protection</strong>: The identification of sensitive data is critical before moving it to the Hadoop cluster. It's very important to understand privacy policies and government regulations for the better identification and mitigation of compliance exposure risks.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Hadoop file formats</h1>
                
            
            
                
<p class="mce-root">In Hadoop, there are many file formats available. A user can select any format based on the use case. Each format has special features in terms of storage and performance. Let's discuss each file format in detail.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Text/CSV file</h1>
                
            
            
                
<p class="mce-root">Text and CSV files are very common in Hadoop data processing algorithms. Each line in the file is treated as a new record. Typically, each line ends with the <em>n</em> character. These files do not support column headers. Hence, while processing, an extra line of the code is always required to remove column headings. CSV files are typically compressed using GZIP codec because they do not support block level compression; it adds to more processing costs. Needless to mention they do not support schema evolution.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">JSON</h1>
                
            
            
                
<p class="mce-root">The JSON format is becoming very popular in all modern programming languages. These files are collection name/value pairs. The JSON format is typically used in data exchange applications and it is treated as an object, record, struct, or an array. These files are text files and support schema evolutions. It's very easy to add or delete attributes from a JSON file. Like text/CSV files, JSON files do not support block-level compression.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Sequence file</h1>
                
            
            
                
<p class="mce-root">A sequence file is a flat file consisting of binary key/value pairs. They are extensively used in MapReduce (<a href="https://wiki.apache.org/hadoop/MapReduce" target="_blank">https://wiki.apache.org/hadoop/MapReduce</a>) as input/output formats. They are mostly used for intermediate data storage within a sequence of MapReduce jobs. Sequence files work well as containers for small files. If there are too many small files in HDFS, they can be packed in a sequence file to make file processing efficient. There are three formats of sequence files: uncompressed, record compressed, and block compressed key/value records. Sequence files support block-level compression but do not support schema evolution.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Avro</h1>
                
            
            
                
<p class="mce-root">Avro is a widely used file type within the Hadoop community. It is popular because it helps schema evolution. It contains serialized data with a binary format. An Avro file is splittable and supports block compression. It contains data and metadata. It uses a separate JSON file to define the schema format. When Avro data is stored in a file, its schema is stored with it so that files may be processed later by any program. If the program reading the data expects a different schema, this can be easily resolved, since both schemas are present.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Parquet</h1>
                
            
            
                
<p class="mce-root">Parquet stores nested data structures in a flat columnar format. Parquet is more efficient in terms of storage and performance than any row-level file formats. Parquet stores binary data in a column-oriented way. In the Parquet format, new columns are added at the end of the structure. Cloudera mainly supports this format for Impala implementation but is aggressively becoming popular recently. This format is good for SQL queries, which read particular columns from a wide table having many columns because only selective columns are read to reduce I/O cost.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">ORC</h1>
                
            
            
                
<p class="mce-root">ORC files are optimized record columnar file format and are the extended version of RC files. These are great for compression and are best suited for Hive SQL performance when Hive is reading, writing, and processing data to reduce access time and the storage space. These files do not support true schema evolution. They are mainly supported by Hortonworks and are not suitable for Impala SQL processing.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Which file format is better?</h1>
                
            
            
                
<p class="mce-root">The answer is: it depends on your use cases. Generally, the criteria for selecting a file format is based on query-read and query-write performance. Also, it depends on which Hadoop distribution you are using. The ORC file format is the best for Hive and Tez using the Hortonworks distribution and a parquet file is recommended for Cloudera Impala implementations. For a use case involving schema evolution, Avro files are best suited. If you want to import data from RDBMS using Sqoop, text/CSV file format is the better choice. For storing map intermediate output, a sequence file is the ultimate choice.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p class="mce-root">In this chapter, the main objective was to learn about various Hadoop design alternatives. We've learned a lot when it comes to the Hadoop cluster and its best practices for deployment in a typical production environment. We started with a basic understanding about Hadoop and we proceeded to Hadoop configuration, installation, and HDFS architecture. We also learned about various techniques for achieving HDFS high availability. We also looked into YARN architecture. Finally, we looked at various file formats and how to choose one based on your use case.</p>
<p class="mce-root">In the next chapter, we will see how to ingest data into a newly created Hadoop cluster.</p>
<p> </p>


            

            
        
    </body></html>