<html><head></head><body>
		<div><h1 id="_idParaDest-61"><em class="italic"><a id="_idTextAnchor063"/>Chapter 5</em>: Cleaning, Transforming, and Enriching Data</h1>
			<p>In the previous two chapters, you learned how to build data pipelines that could read and write from files and databases. In many instances, these skills alone will enable you to build production data pipelines. For example, you will read files from a data lake and insert them into a database. You now have the skills to accomplish this. Sometimes, however, you will need to do something with the data after extraction but prior to loading. What you will need to do is clean the data. Cleaning is a vague term. More specifically, you will need to check the validity of the data and answer questions such as the following: Is it complete? Are the values within the proper ranges? Are the columns the proper type? Are all the columns useful?</p>
			<p>In this chapter, you will learn the basic skills needed to perform exploratory data analysis. Once you have an understanding of the data, you will use that knowledge to fix common data problems that you have discovered – such as dropping columns and replacing nulls. You will learn many useful methods available in the <code>pandas</code> library for Python. These skills will allow you to quickly perform exploratory data analysis and allow you to clean the data, all within Python. These skills will become the tools for the transform stage of the ETL data engineering process.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Performing exploratory data analysis in Python</li>
				<li>Handling common data issues using pandas</li>
				<li>Cleaning data using Airflow</li>
			</ul>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor064"/>Performing exploratory data analysis in Python</h1>
			<p>Before <a id="_idIndexMarker305"/>you can clean your data, you need to<a id="_idIndexMarker306"/> know what your data looks like. As a data engineer, you are not the domain expert and are not the end user of the data, but you should know what the data will be used for and what valid data would look like. For example, you do not need to be a demographer to know that an <code>age</code> field should <a id="_idIndexMarker307"/>not be negative, and the<a id="_idIndexMarker308"/> frequency of values over 100 should be low.</p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor065"/>Downloading the data</h2>
			<p>In this chapter, you will use<a id="_idIndexMarker309"/> real e-scooter data from the City of Albuquerque. The data contains trips taken using e-scooters from May to July 22, 2019. You will need to download the e-scooter data from <a href="https://github.com/PaulCrickard/escooter/blob/master/scooter.csv">https://github.com/PaulCrickard/escooter/blob/master/scooter.csv</a>. The repository also contains the original Excel file as well as some other summary files provided by the City of Albuquerque.</p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor066"/>Basic data exploration</h2>
			<p>Before you can clean your<a id="_idIndexMarker310"/> data, you have to know what your data looks like. The process of understanding your data is called <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>). You will look <a id="_idIndexMarker311"/>at the shape of your data, the number of rows and columns, as well as the data types in the columns, and the ranges of values. You can perform a much more in-depth analysis, such as the distribution of the data, or the skewness, but for this section, you will learn how to quickly understand your data so that in the next section, you can clean it. </p>
			<p>In the two previous chapters, you learned how to import files and databases into pandas DataFrames. That knowledge will be expanded in this section, as DataFrames will be the tool used for the EDA.</p>
			<p>To begin with, you will need to import <code>pandas</code> and read the <code>.csv</code> file:</p>
			<pre>import pandas as pd
df=pd.read_csv('scooter.csv')</pre>
			<p>With the data in a DataFrame, you can now explore it, and then analyze it.</p>
			<h3>Exploring the data</h3>
			<p>Now you can start to<a id="_idIndexMarker312"/> look at the data. The first thing you will probably want to do is print it out. But before you get to that, take a look at the columns and the data types using <code>columns</code> and <code>dtypes</code>:</p>
			<pre>df.columns
<strong class="bold">Index(['month', 'trip_id', 'region_id', 'vehicle_id', </strong>
<strong class="bold">       'started_at', 'ended_at', 'DURATION', </strong>
<strong class="bold">       'start_location_name', 'end_location_name', </strong>
<strong class="bold">       'user_id', 'trip_ledger_id'],</strong>
<strong class="bold">      dtype='object')</strong>
df.dtypes
<strong class="bold">month                  object</strong>
<strong class="bold">trip_id                 int64</strong>
<strong class="bold">region_id               int64</strong>
<strong class="bold">vehicle_id              int64</strong>
<strong class="bold">started_at             object</strong>
<strong class="bold">ended_at               object</strong>
<strong class="bold">DURATION               object</strong>
<strong class="bold">start_location_name    object</strong>
<strong class="bold">end_location_name      object</strong>
<strong class="bold">user_id                 int64</strong>
<strong class="bold">trip_ledger_id          int64</strong></pre>
			<p>You will see that you have eleven columns, five of which are integers (all of the columns with ID in their name) and the rest are objects. Objects are what a DataFrame uses as <code>dtype</code> when there are mixed types. Also, <code>DURATION</code> should stand out because it is the only column name in all capitals. In the next section, you will fix common errors, such as the column cases are not uniform (all lowercase or uppercase) and make the <code>dtypes</code> object proper types, such as <code>strings</code> for text data and <code>datetimes</code> for dates and times.</p>
			<p>Now you know what you have for columns and types, let's look at the data. You can print out the first five records using <code>head()</code>:</p>
			<pre>df.head()
<strong class="bold">  month  trip_id  ...   user_id  trip_ledger_id</strong>
<strong class="bold">0   May  1613335  ...   8417864         1488546</strong>
<strong class="bold">1   May  1613639  ...   8417864         1488838</strong>
<strong class="bold">2   May  1613708  ...   8417864         1488851</strong>
<strong class="bold">3   May  1613867  ...   8417864         1489064</strong>
<strong class="bold">4   May  1636714  ...  35436274         1511212</strong>
<strong class="bold">[5 rows x 11 columns]</strong></pre>
			<p>The opposite of <code>head()</code> is <code>tail()</code>. Both of these methods default to showing 5 rows. However, you can pass an <a id="_idIndexMarker313"/>integer as a parameter that specifies how many rows to show. For example, you could pass <code>head(10)</code> to see the first 10 rows.</p>
			<p>Notice in both the <code>head()</code> and <code>tail()</code> output that the third column is <code>...</code>, and then there are two more columns after this. The display is cropping out the columns in the middle. If you were to print the entire DataFrame, the same thing would happen with the rows as well. To display all the columns, you can change the number of columns to show using the <code>set_options</code> method:</p>
			<pre>pd.set_option('display.max_columns', 500)</pre>
			<p>Now, when you use <code>head()</code>, you will see all the column. However, depending on the width of your display, the output may be wrapped to fit.</p>
			<p>The <code>head</code> and <code>tail</code> methods display all the columns, but if you are only interested in a single column, you can specify it like you would in a Python dictionary. The following code prints the <code>DURATION</code> column:</p>
			<pre>df['DURATION']
<strong class="bold">0        0:07:03</strong>
<strong class="bold">1        0:04:57</strong>
<strong class="bold">2        0:01:14</strong>
<strong class="bold">3        0:06:58</strong>
<strong class="bold">4        0:03:06</strong>
<strong class="bold">          ...   </strong>
<strong class="bold">34221    0:14:00</strong>
<strong class="bold">34222    0:08:00</strong>
<strong class="bold">34223    1:53:00</strong>
<strong class="bold">34224    0:12:00</strong>
<strong class="bold">34225    1:51:00</strong></pre>
			<p>Again, notice that the <a id="_idIndexMarker314"/>output is cropped with <code>...</code>, but this time for the rows. The result is the combination of <code>head()</code> and <code>tail()</code>. You could change this using the <code>display_max_rows</code> option as you did earlier with columns, but for this exploration, it is unnecessary. </p>
			<p>Just like you can display a single column, you can display a list of columns using double <code>[]</code>, as shown in the following code block:</p>
			<pre>df[['trip_id','DURATION','start_location_name']]     
<strong class="bold">       trip_id DURATION     start_location_name</strong>
<strong class="bold">0      1613335  0:07:03       1901 Roma Ave NE, Albuquerque, NM </strong>
<strong class="bold">                              87106, USA</strong>
<strong class="bold">1      1613639  0:04:57  1 Domenici Center en Domenici Center, </strong>
<strong class="bold">                           Albuquer...</strong>
<strong class="bold">2      1613708  0:01:14  1 Domenici Center en Domenici Center, </strong>
<strong class="bold">                           Albuquer...</strong>
<strong class="bold">3      1613867  0:06:58  Rotunda at Science &amp; Technology Park, </strong>
<strong class="bold">                         801 Univ...</strong>
<strong class="bold">4      1636714  0:03:06          401 2nd St NW, Albuquerque, NM</strong>
<strong class="bold">                                 87102, USA</strong>
<strong class="bold">...        ...      ...                                     ...</strong>
<strong class="bold">34221  2482235  0:14:00     Central @ Broadway, Albuquerque, NM </strong>
<strong class="bold">                            87102, USA</strong>
<strong class="bold">34222  2482254  0:08:00     224 Central Ave SW, Albuquerque, NM </strong>
<strong class="bold">                            87102, USA</strong>
<strong class="bold">34223  2482257  1:53:00     105 Stanford Dr SE, Albuquerque, NM </strong>
<strong class="bold">                            87106, USA</strong>
<strong class="bold">34224  2482275  0:12:00   100 Broadway Blvd SE, Albuquerque, NM </strong>
<strong class="bold">                          87102, USA</strong>
<strong class="bold">34225  2482335  1:51:00     105 Stanford Dr SE, Albuquerque, NM </strong>
<strong class="bold">                            87106, USA</strong></pre>
			<p>You can also pull a sample<a id="_idIndexMarker315"/> from your data using <code>sample()</code>. The sample methods allow you to specify how many rows you would like to pull. The results are shown in the following code block:</p>
			<pre>df.sample(5)
<strong class="bold">     month  trip_id  ...   user_id  trip_ledger_id</strong>
<strong class="bold">4974   June  1753394  ...  35569540         1624088</strong>
<strong class="bold">18390  June  1992655  ...  42142022         1857395</strong>
<strong class="bold">3132    May  1717574  ...  37145791         1589327</strong>
<strong class="bold">1144    May  1680066  ...  36146147         1553169</strong>
<strong class="bold">21761  June  2066449  ...  42297442         1929987</strong></pre>
			<p>Notice that the index of the rows is not incremental, but rather it jumps around. It should, as it is a sample.</p>
			<p>You can also slice the data. Slicing takes the format of <code>[start:end]</code>, where a blank is the first or last row depending on which position is blank. To slice the first 10 rows, you can use the following notation: </p>
			<pre>df[:10]  
<strong class="bold">     month  trip_id  ...   user_id  trip_ledger_id</strong>
<strong class="bold">0      May  1613335  ...   8417864         1488546</strong>
<strong class="bold">1      May  1613639  ...   8417864         1488838</strong>
<strong class="bold">2      May  1613708  ...   8417864         1488851</strong>
<strong class="bold">3      May  1613867  ...   8417864         1489064</strong>
<strong class="bold">4      May  1636714  ...  35436274         1511212</strong>
<strong class="bold">5      May  1636780  ...  34352757         1511371</strong>
<strong class="bold">6      May  1636856  ...  35466666         1511483</strong>
<strong class="bold">7      May  1636912  ...  34352757         1511390</strong>
<strong class="bold">8      May  1637035  ...  35466666         1511516</strong>
<strong class="bold">9      May  1637036  ...  34352757         1511666</strong></pre>
			<p>Likewise, to grab the <a id="_idIndexMarker316"/>rows from 10 to the end (34,225), you can use the following notation:</p>
			<pre>df[10:] </pre>
			<p>You can also slice the frame starting on the third row and ending before nine, as shown in the following code block:</p>
			<pre>df[3:9]
<strong class="bold">  month  trip_id  ...   user_id  trip_ledger_id</strong>
<strong class="bold">3   May  1613867  ...   8417864         1489064</strong>
<strong class="bold">4   May  1636714  ...  35436274         1511212</strong>
<strong class="bold">5   May  1636780  ...  34352757         1511371</strong>
<strong class="bold">6   May  1636856  ...  35466666         1511483</strong>
<strong class="bold">7   May  1636912  ...  34352757         1511390</strong>
<strong class="bold">8   May  1637035  ...  35466666         1511516</strong></pre>
			<p>Sometimes, you know the exact row you want, and instead of slicing it, you can select it using <code>loc()</code>. The <code>loc</code> method takes the index name, which, in this example, is an integer. The following code and output show a single row selected with <code>loc()</code>:</p>
			<pre>df.loc[34221]
<strong class="bold">month                                                      July</strong>
<strong class="bold">trip_id                                                 2482235</strong>
<strong class="bold">region_id                                                   202</strong>
<strong class="bold">vehicle_id                                              2893981</strong>
<strong class="bold">started_at                                      7/21/2019 23:51</strong>
<strong class="bold">ended_at                                         7/22/2019 0:05</strong>
<strong class="bold">DURATION                                                0:14:00</strong>
<strong class="bold">start_location_name  Central @ Broadway, Albuquerque, NM 87102,</strong>
<strong class="bold">                                                            USA</strong>
<strong class="bold">end_location_name    1418 4th St NW, Albuquerque, NM 87102, USA</strong>
<strong class="bold">user_id                                                42559731</strong>
<strong class="bold">trip_ledger_id                                          2340035</strong></pre>
			<p>Using <code>at()</code>, with the <a id="_idIndexMarker317"/>position, as you did in the slicing examples, and a column name, you can select a single value. For example, this can be done to know the duration of the trip in the second row:</p>
			<pre>df.at[2,'DURATION']
<strong class="bold">'0:01:14'</strong></pre>
			<p>Slicing and using <code>loc()</code> and <code>at()</code> pull data based on position, but you can also use DataFrames to select rows<a id="_idIndexMarker318"/> based on some condition. Using the <code>where</code> method, you can pass a condition, as shown in the following code block:</p>
			<pre>user=df.where(df['user_id']==8417864)
user
<strong class="bold">      month    trip_id  ...    user_id  trip_ledger_id</strong>
<strong class="bold">0       May  1613335.0  ...  8417864.0       1488546.0</strong>
<strong class="bold">1       May  1613639.0  ...  8417864.0       1488838.0</strong>
<strong class="bold">2       May  1613708.0  ...  8417864.0       1488851.0</strong>
<strong class="bold">3       May  1613867.0  ...  8417864.0       1489064.0</strong>
<strong class="bold">4       NaN        NaN  ...        NaN             NaN</strong>
<strong class="bold">...     ...        ...  ...        ...             ...</strong>
<strong class="bold">34221   NaN        NaN  ...        NaN             NaN</strong>
<strong class="bold">34222   NaN        NaN  ...        NaN             NaN</strong>
<strong class="bold">34223   NaN        NaN  ...        NaN             NaN</strong>
<strong class="bold">34224   NaN        NaN  ...        NaN             NaN</strong>
<strong class="bold">34225   NaN        NaN  ...        NaN             NaN</strong></pre>
			<p>The preceding code and results show the results of <code>where</code> with the condition of the user ID being equal to <code>8417864</code>. The results replace values that do not meet the criteria as <code>NaN</code>. This will be covered in the next section.</p>
			<p>You can get the same results similar to the preceding example with the exception of using a different notation, and this method will not include the <code>NaN</code> rows. You can pass the condition into the DataFrame as you did with column names. The following example shows you how:</p>
			<pre>df[(df['user_id']==8417864)]</pre>
			<p>The results of the preceding code is the same as the <code>where()</code> example, but without the <code>NaN</code> rows, so the DataFrame will only have four rows.</p>
			<p>Using both notations, you can combine conditional statements. By using the same user ID condition, you can add <a id="_idIndexMarker319"/>a trip ID condition. The following example shows you how:</p>
			<pre>one=df['user_id']==8417864
two=df['trip_ledger_id']==1488838
df.where(one &amp; two)
<strong class="bold">      month    trip_id  ...    user_id  trip_ledger_id</strong>
<strong class="bold">0       NaN        NaN  ...        NaN             NaN</strong>
<strong class="bold">1       May  1613639.0  ...  8417864.0       1488838.0</strong>
<strong class="bold">2       NaN        NaN  ...        NaN             NaN</strong>
<strong class="bold">3       NaN        NaN  ...        NaN             NaN</strong>
<strong class="bold">4       NaN        NaN  ...        NaN             NaN</strong></pre>
			<p>Using the second notation, the output is as follows:</p>
			<pre>df[(one)&amp;(two)]
<strong class="bold">  month  trip_id  ...  user_id  trip_ledger_id</strong>
<strong class="bold">1   May  1613639  ...  8417864         1488838</strong></pre>
			<p>In the preceding examples, the conditions were assigned to a variable and combined in both the <code>where</code> and secondary notation, generating the expected results.</p>
			<h3>Analyzing the data</h3>
			<p>Now that you have <a id="_idIndexMarker320"/>seen the data, you can start to analyze it. By using the <code>describe</code> method, you can see a series of statistics pertaining to your data. In statistics, there is a set of statistics referred to as the five-number summary, and <code>describe()</code> is a variant of that:</p>
			<pre>df.describe()
<strong class="bold">           trip_id  region_id    vehicle_id       user_id  trip_ledger_id</strong>
<strong class="bold">count  3.422600e+04    34226.0  3.422600e+04  3.422600e+04    3.422600e+04</strong>
<strong class="bold">mean   2.004438e+06      202.0  5.589507e+06  3.875420e+07    1.869549e+06</strong>
<strong class="bold">std    2.300476e+05        0.0  2.627164e+06  4.275441e+06    2.252639e+05</strong>
<strong class="bold">min    1.613335e+06      202.0  1.034847e+06  1.080200e+04    1.488546e+06</strong>
<strong class="bold">25%    1.813521e+06      202.0  3.260435e+06  3.665710e+07    1.683023e+06</strong>
<strong class="bold">50%    1.962520e+06      202.0  5.617097e+06  3.880750e+07    1.827796e+06</strong>
<strong class="bold">75%    2.182324e+06      202.0  8.012871e+06  4.222774e+07    2.042524e+06</strong>
<strong class="bold">max    2.482335e+06      202.0  9.984848e+06  4.258732e+07    2.342161e+06</strong></pre>
			<p>The <code>describe</code> method <a id="_idIndexMarker321"/>is not very useful unless you have numeric data. If you were looking at ages for example, it would quickly show you the distribution of ages, and you would be able to quickly see errors such as negative ages or too many ages over 100. </p>
			<p>Using <code>describe()</code> on a single column is sometimes more helpful. Let's try looking at the <code>start_location_name</code> column. The code and results are shown in the following code block:</p>
			<pre>df['start_location_name'].describe()
<strong class="bold">count                                               34220</strong>
<strong class="bold">unique                                               2972</strong>
<strong class="bold">top       1898 Mountain Rd NW, Albuquerque, NM 87104, USA</strong>
<strong class="bold">freq                                                 1210</strong></pre>
			<p>The data is not numeric, so we get a different set of statistics, but these provide some insight. Of the <code>34220</code> starting locations, there are actually <code>2972</code> unique locations. The top location (<code>1898 Mountain Rd NW</code>) accounts for <code>1210</code> trip starting locations. Later, you will geocode this data — add coordinates to the address — and knowing the unique values means you only have to geocode those 2,972 and not the full 34,220.</p>
			<p>Another method that allows you to see details about your data is <code>value_counts</code>. The <code>value_counts</code> method will give you the value and count for all unique values. We need to call it to a single column, which is done in the following snippet:</p>
			<pre>df['DURATION'].value_counts()
<strong class="bold">0:04:00    825</strong>
<strong class="bold">0:03:00    807</strong>
<strong class="bold">0:05:00    728</strong>
<strong class="bold">0:06:00    649</strong>
<strong class="bold">0:07:00    627</strong></pre>
			<p>From this method, you <a id="_idIndexMarker322"/>can see that <code>0:04:00</code> is at the top with a frequency of <code>825</code> — which you could have found out with <code>describe()</code> — but you can also see the frequency of all the other values. To see the frequency as a percentage, you can pass the normalize parameter (which is <code>False</code> by default):</p>
			<pre>df['DURATION'].value_counts(normalize=True)
<strong class="bold">0:04:00    0.025847</strong>
<strong class="bold">0:03:00    0.025284</strong>
<strong class="bold">0:05:00    0.022808</strong>
<strong class="bold">0:06:00    0.020333</strong>
<strong class="bold">0:07:00    0.019644</strong></pre>
			<p>You will notice that no single value makes up a significant percentage of the duration. </p>
			<p>You can also pass the <code>dropna</code> parameter. By default, <code>value_counts()</code> sets it to <code>True</code> and you will not see them. Setting it to <code>False</code>, you can see that <code>end_location_name</code> is missing <code>2070</code> entries:</p>
			<pre>df['end_location_name'].value_counts(dropna=False)
<strong class="bold">NaN                                                2070</strong>
<strong class="bold">1898 Mountain Rd NW, Albuquerque, NM 87104, USA     802</strong>
<strong class="bold">Central @ Tingley, Albuquerque, NM 87104, USA       622</strong>
<strong class="bold">330 Tijeras Ave NW, Albuquerque, NM 87102, USA      529</strong>
<strong class="bold">2550 Central Ave NE, Albuquerque, NM 87106, USA     478</strong>
<strong class="bold">                                                   ... </strong>
<strong class="bold">507 Bridge Blvd SW, Albuquerque, NM 87102, USA        1</strong>
<strong class="bold">820 2nd St NW, Albuquerque, NM 87102, USA             1</strong>
<strong class="bold">909 Alcalde Pl SW, Albuquerque, NM 87104, USA         1</strong>
<strong class="bold">817 Slate Ave NW, Albuquerque, NM 87102, USA          1</strong></pre>
			<p>The best way to find out<a id="_idIndexMarker323"/> how many missing values you have in your columns is to use the <code>isnull()</code> method. The following code combines <code>isnull()</code> with <code>sum()</code> to get the counts:</p>
			<pre>df.isnull().sum()
<strong class="bold">month                     0</strong>
<strong class="bold">trip_id                   0</strong>
<strong class="bold">region_id                 0</strong>
<strong class="bold">vehicle_id                0</strong>
<strong class="bold">started_at                0</strong>
<strong class="bold">ended_at                  0</strong>
<strong class="bold">DURATION               2308</strong>
<strong class="bold">start_location_name       6</strong>
<strong class="bold">end_location_name      2070</strong>
<strong class="bold">user_id                   0</strong>
<strong class="bold">trip_ledger_id            0</strong></pre>
			<p>Another parameter of <code>value_counts()</code> is <code>bins</code>. The scooter dataset does not have a good column for this, but <a id="_idIndexMarker324"/>using a numeric column, you would get results like the following:</p>
			<pre>df['trip_id'].value_counts(bins=10)
<strong class="bold">(1787135.0, 1874035.0]      5561</strong>
<strong class="bold">(1700235.0, 1787135.0]      4900</strong>
<strong class="bold">(1874035.0, 1960935.0]      4316</strong>
<strong class="bold">(1960935.0, 2047835.0]      3922</strong>
<strong class="bold">(2047835.0, 2134735.0]      3296</strong>
<strong class="bold">(2221635.0, 2308535.0]      2876</strong>
<strong class="bold">(2308535.0, 2395435.0]      2515</strong>
<strong class="bold">(2134735.0, 2221635.0]      2490</strong>
<strong class="bold">(2395435.0, 2482335.0]      2228</strong>
<strong class="bold">(1612465.999, 1700235.0]    2122</strong></pre>
			<p>These results are fairly meaningless, but if it is used on a column such as <code>age</code>, it would come in handy as you could create age groups quickly and get an idea of the distribution. </p>
			<p>Now that you have explored and analyzed the data, you should have an understanding of what the data is and what the issues are — for example, nulls, improper <code>dtypes</code>, combined, and fields. With that knowledge, you can start to clean the data. The next section will walk you through how to fix common data problems.</p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor067"/>Handling common data issues using pandas </h1>
			<p>Your data may feel<a id="_idIndexMarker325"/> special, it is unique, you have created the world's<a id="_idIndexMarker326"/> best systems for collecting it, and you have done everything you can to ensure it is clean and accurate. Congratulations! But your data will almost certainly have some problems, and these problems are not special, or unique, and are probably a result of your systems or data entry. The e-scooter dataset is collected using GPS with little to no human input, yet there are end locations that are missing. How is it possible that a scooter was rented, ridden, and stopped, yet the data doesn't know where it stopped? Seems strange, yet here we are. In this section, you will learn how to deal with common data problems using the e-scooter dataset.</p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor068"/>Drop rows and columns</h2>
			<p>Before you modify any fields in your data, you should first decide whether you are going to use all the fields. Looking at the e-scooter data, there is a field named <code>region_id</code>. This field is a code used by the vendor to label Albuquerque. Since we are only using the Albuquerque data, we don't need this field as it adds nothing to the data.</p>
			<p>You can drop columns<a id="_idIndexMarker327"/> using the <code>drop</code> method. The method will allow you to specify whether to drop a row or a column. Rows are the default, so we will specify <code>columns</code>, as shown in the following code block:</p>
			<pre>df.drop(columns=['region_id'], inplace=True)</pre>
			<p>Specifying the columns to drop, you also need to add <code>inplace</code> to make it modify the original DataFrame. </p>
			<p>To drop a row, you<a id="_idIndexMarker328"/> only need to specify <code>index</code> instead of <code>columns</code>. To drop the row with the index of <code>34225</code>, you need to use the following code:</p>
			<pre>df.drop(index=[34225],inplace=True)</pre>
			<p>The preceding code works when you want to drop an entire column or row, but what if you wanted to drop them based on conditions?</p>
			<p>The first condition you may want to consider is where there are nulls. If you are missing data, the column and row may not be useful, or may distort the data. To handle this, you can use <code>dropna()</code>.</p>
			<p>By using <code>dropna()</code>, you can pass <code>axis</code>, <code>how</code>, <code>thresh</code>, <code>subset</code>, and <code>inplace</code> as parameters:</p>
			<ul>
				<li><code>axis</code> specifies<a id="_idIndexMarker329"/> rows or columns with indexes or columns (0 or 1). It defaults to rows.</li>
				<li><code>how</code> specifies <a id="_idIndexMarker330"/>whether to drop rows or columns if all the values are null or if any value is null (all or any). It defaults to <code>any</code>.</li>
				<li><code>thresh</code> allows more control than allowing you to specify an integer value of how many nulls must be present.</li>
				<li><code>subset</code> allows you to specify a list of rows or columns to search.</li>
				<li><code>inplace</code> allows you to modify the existing DataFrame. It defaults to <code>False</code>.</li>
			</ul>
			<p>Looking at the e-scooter data, there are six rows with no start location name:</p>
			<pre>df['start_location_name'][(df['start_location_name'].isnull())]
<strong class="bold">26042    NaN</strong>
<strong class="bold">26044    NaN</strong>
<strong class="bold">26046    NaN</strong>
<strong class="bold">26048    NaN</strong>
<strong class="bold">26051    NaN</strong>
<strong class="bold">26053    NaN</strong></pre>
			<p>To drop these rows, you can use <code>dropna</code> on <code>axis=0</code> with <code>how=any</code>, which are the defaults. This will, however, delete rows where other nulls exist, such as <code>end_location_name</code>. So, you will need to specify the column name as a subset, as shown in the following code block:</p>
			<pre>df.dropna(subset=['start_location_name'],inplace=True)</pre>
			<p>Then, when you select nulls in the <code>start_location_name</code> field as in the preceding code block, you will get an empty series:</p>
			<pre>df['start_location_name'][(df['start_location_name'].isnull())]
<strong class="bold">Series([], Name: start_location_name, dtype: object)</strong></pre>
			<p>Dropping an entire column based on missing values may only make sense if a certain percentage of rows are null. For example, if more than 25% of the rows are null, you may want to drop it. You could specify this in the threshold by using something like the following code for the <code>thresh</code> parameter:</p>
			<pre>thresh=int(len(df)*.25)</pre>
			<p>Before showing <a id="_idIndexMarker331"/>more advanced filters for dropping rows, you may not want to drop<a id="_idIndexMarker332"/> nulls. You may want to fill them with a value. You can use <code>fillna()</code> to fill either null columns or rows:</p>
			<pre>df.fillna(value='00:00:00',axis='columns')
<strong class="bold">9201     00:00:00</strong>
<strong class="bold">9207     00:00:00</strong>
<strong class="bold">9213     00:00:00</strong>
<strong class="bold">9219     00:00:00</strong>
<strong class="bold">9225     00:00:00</strong></pre>
			<p>What if you want to use <code>fillna()</code> but use different values depending on the column? You would not want to have to specify a column every time and run <code>fillna()</code> multiple times. You can specify an object to map to the DataFrame and pass it as the <code>value</code> parameter. </p>
			<p>In the following code, we will copy the rows where both the start and end location are null. Then, we will create a <code>value</code> object that assigns a street name to the <code>start_location_name</code> field and a different street address to the <code>end_location_name</code> field. Using <code>fillna()</code>, we pass the value to the <code>value</code> parameter, and then print those two columns in the DataFrame by showing the change:</p>
			<pre>startstop=df[(df['start_location_name'].isnull())&amp;(df['end_location_name'].isnull())]
value={'start_location_name':'Start St.','end_location_name':'Stop St.'}
startstop.fillna(value=value)
startstop[['start_location_name','end_location_name']]
<strong class="bold">      start_location_name end_location_name</strong>
<strong class="bold">26042           Start St.          Stop St.</strong>
<strong class="bold">26044           Start St.          Stop St.</strong>
<strong class="bold">26046           Start St.          Stop St.</strong>
<strong class="bold">26048           Start St.          Stop St.</strong>
<strong class="bold">26051           Start St.          Stop St.</strong>
<strong class="bold">26053           Start St.          Stop St.</strong></pre>
			<p>You can drop <a id="_idIndexMarker333"/>rows based on more advanced filters; for example, what<a id="_idIndexMarker334"/> if you want to drop all the rows where the month was May? You could iterate through the DataFrame and check the month, and then drop it if it is May. Or, a much better way would be to filter out the rows, and then pass the index to the <code>drop</code> method. You can filter the DataFrame and pass it to a new one, as shown in the following code block:</p>
			<pre>may=df[(df['month']=='May')]
may
<strong class="bold">     month  trip_id  ...   user_id trip_ledger_id</strong>
<strong class="bold">0      May  1613335  ...   8417864        1488546</strong>
<strong class="bold">1      May  1613639  ...   8417864        1488838</strong>
<strong class="bold">2      May  1613708  ...   8417864        1488851</strong>
<strong class="bold">3      May  1613867  ...   8417864        1489064</strong>
<strong class="bold">4      May  1636714  ...  35436274        1511212</strong>
<strong class="bold">...    ...      ...  ...       ...            ...</strong>
<strong class="bold">4220   May  1737356  ...  35714580        1608429</strong>
<strong class="bold">4221   May  1737376  ...  37503537        1608261</strong>
<strong class="bold">4222   May  1737386  ...  37485128        1608314</strong>
<strong class="bold">4223   May  1737391  ...  37504521        1608337</strong>
<strong class="bold">4224   May  1737395  ...  37497528        1608342</strong></pre>
			<p>Then you<a id="_idIndexMarker335"/> can use <code>drop()</code> on the original DataFrame and pass the index<a id="_idIndexMarker336"/> for the rows in the <code>may</code> DataFrame, as shown:</p>
			<pre>df.drop(index=may.index,inplace=True)</pre>
			<p>Now, if you look at the months in the original DataFrame, you will see that May is missing:</p>
			<pre>df['month'].value_counts()
<strong class="bold">June    20259</strong>
<strong class="bold">July     9742</strong></pre>
			<p>Now that you have removed the rows and columns that you either do not need, or that were unusable on account of missing data, it is time to format them.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor069"/>Creating and modifying columns</h2>
			<p>The first thing that stood out in the <a id="_idIndexMarker337"/>preceding section was that there was a single column, duration, that<a id="_idIndexMarker338"/> was all in capital letters. Capitalization is a common problem. You will often find columns with all capitals, or with title case — where the first letter of every word is capitalized — and if a coder wrote it, you may find camel case — where the first letter is lowercase and the first letter of the next word is capital with no spaces, as in <strong class="bold">camelCase</strong>. The <a id="_idIndexMarker339"/>following code will make all the columns lowercase:</p>
			<pre>df.columns=[x.lower() for x in df.columns] print(df.columns)
<strong class="bold">Index(['month', 'trip_id', 'region_id', 'vehicle_id', 'started_at', 'ended_at','duration', 'start_location_name', 'end_location_name', 'user_id', 'trip_ledger_id'], dtype='object')</strong></pre>
			<p>The preceding code is a condensed version of a <code>for</code> loop. What happens in the loop comes before the <code>for</code> loop. The preceding code says that for every item in <code>df.columns</code>, make it lowercase, and assign it back to <code>df.columns</code>. You can also use <code>capitalize()</code>, which is titlecase, or <code>upper()</code> as shown:</p>
			<pre>df.columns=[x.upper() for x in df.columns] print(df.columns)
<strong class="bold">Index(['MONTH', 'TRIP_ID', 'REGION_ID', 'VEHICLE_ID', 'STARTED_AT', 'ENDED_AT', 'DURATION', 'START_LOCATION_NAME', 'END_LOCATION_NAME', 'USER_ID', 'TRIP_LEDGER_ID'], dtype='object')</strong></pre>
			<p>You could also make the <code>DURATION</code> field lowercase using the <code>rename</code> method, as shown:</p>
			<pre>df.rename(columns={'DURATION':'duration'},inplace=True)</pre>
			<p>You will <a id="_idIndexMarker340"/>notice an <code>inplace</code> parameter set to <code>True</code>. When you used <a id="_idIndexMarker341"/>psycopg2 to modify databases, you need to use <code>conn.commit()</code> to make it permanent, and you need to do the same with DataFrames. When you modify a DataFrame, the result is returned. You can store that new DataFrame (result) in a variable, and the original DataFrame is left unchanged. If you want to modify the original DataFrame and not assign it to another variable, you must use the <code>inplace</code> parameter. </p>
			<p>The <code>rename</code> method works for fixing the case of column names but is not the best choice. It is better used for actually changing multiple column names. You can pass an object with multiple column name remapping. For example, you can remove the underscore in <code>region_id</code> using <code>rename</code>. In the following code snippet, we change the <code>DURATION</code> column to lowercase and remove the underscore in <code>region_id</code>:</p>
			<pre>df.rename(columns={'DURATION':'duration','region_id':'region'},inplace=True)</pre>
			<p>It is good to know different ways to accomplish the same task, and you can decide which makes the most sense for your use case. Now that you have applied changes to the column names, you can apply these functions to the values in the columns as well. Instead of using <code>df.columns</code>, you will specify which column to modify, and then whether to make it <code>upper()</code>, <code>lower()</code>, or <code>capitalize()</code>. In the following code snippet, we have made the <code>month</code> column all capitals:</p>
			<pre>df['month']=df['month'].str.upper()
df['month'].head()
<strong class="bold">0    MAY</strong>
<strong class="bold">1    MAY</strong>
<strong class="bold">2    MAY</strong>
<strong class="bold">3    MAY</strong>
<strong class="bold">4    MAY</strong></pre>
			<p>It may not matter what the capitalization is on your column names or the values. However, it is best to be consistent. In the case of the scooter data, having one column name in all capitals, while the rest were all lower, would become confusing. Imagine a data scientist <a id="_idIndexMarker342"/>querying data from multiple databases or your data warehouse and<a id="_idIndexMarker343"/> having to remember that all their queries needed to account for the <code>duration</code> field being in all caps, and when they forgot, their code failed.</p>
			<p>You can add data to the DataFrame by creating columns using the <code>df['new column name']=value</code> format.</p>
			<p>The preceding format would create a new column and assign the value to every row. You could iterate through a DataFrame and add a value based on a condition, for example:</p>
			<pre>for i,r in df.head().iterrows():
    if r['trip_id']==1613335:
        df.at[i,'new_column']='Yes'
    else:
        df.at[i,'new_column']='No'
df[['trip_id','new_column']].head()
<strong class="bold">   trip_id new_column</strong>
<strong class="bold">0  1613335        Yes</strong>
<strong class="bold">1  1613639         No</strong>
<strong class="bold">2  1613708         No</strong>
<strong class="bold">3  1613867         No</strong>
<strong class="bold">4  1636714         No</strong></pre>
			<p>Iterating through DataFrames works but can be very slow. To accomplish the same thing as the preceding example, but more efficiently, you can use <code>loc()</code> and pass the condition, the column name, and then the value. The following example shows the code and the results:</p>
			<pre>df.loc[df['trip_id']==1613335,'new_column']='1613335'
df[['trip_id','new_column']].head()
<strong class="bold">   trip_id new_column</strong>
<strong class="bold">0  1613335    1613335</strong>
<strong class="bold">1  1613639         No</strong>
<strong class="bold">2  1613708         No</strong>
<strong class="bold">3  1613867         No</strong>
<strong class="bold">4  1636714         No</strong></pre>
			<p>Another way to create<a id="_idIndexMarker344"/> columns is by splitting the data and then inserting it into the<a id="_idIndexMarker345"/> DataFrame. You can use <code>str.split()</code> on a series to split text on any separator, or a <code>(expand=True)</code>. If you do not set <code>expand</code> to <code>True</code>, you will get a list in the column, which is the default. Furthermore, if you do not specify a separator, whitespace will be used. The defaults are shown:</p>
			<pre>d['started_ad=df[['trip_id','started_at']].head()
d['started_at'].str.split()
d
<strong class="bold">   trip_id          started_at</strong>
<strong class="bold">0  1613335  [5/21/2019, 18:33]</strong>
<strong class="bold">1  1613639  [5/21/2019, 19:07]</strong>
<strong class="bold">2  1613708  [5/21/2019, 19:13]</strong>
<strong class="bold">3  1613867  [5/21/2019, 19:29]</strong>
<strong class="bold">4  1636714  [5/24/2019, 13:38]</strong></pre>
			<p>You can expand the data and pass it to a new variable. Then you can assign the columns to a column in the original DataFrame. For example, if you wanted to create a <code>date</code> and a <code>time</code> column, you<a id="_idIndexMarker346"/> could do the <a id="_idIndexMarker347"/>following:</p>
			<pre>new=d['started_at'].str.split(expand=True)
new
<strong class="bold">           0      1</strong>
<strong class="bold">0  5/21/2019  18:33</strong>
<strong class="bold">1  5/21/2019  19:07</strong>
<strong class="bold">2  5/21/2019  19:13</strong>
<strong class="bold">3  5/21/2019  19:29</strong>
<strong class="bold">4  5/24/2019  13:38</strong>
d['date']=new[0]
d['time']=new[1]
d
<strong class="bold">   trip_id       started_at       date   time</strong>
<strong class="bold">0  1613335  5/21/2019 18:33  5/21/2019  18:33</strong>
<strong class="bold">1  1613639  5/21/2019 19:07  5/21/2019  19:07</strong>
<strong class="bold">2  1613708  5/21/2019 19:13  5/21/2019  19:13</strong>
<strong class="bold">3  1613867  5/21/2019 19:29  5/21/2019  19:29</strong>
<strong class="bold">4  1636714  5/24/2019 13:38  5/24/2019  13:38</strong></pre>
			<p>If you recall from the <em class="italic">Exploring the data</em> section, the data had several <code>dtypes</code> objects. The <code>started_at</code> column is an object and, looking at it, it should be clear that it is a <code>datetime</code> object. If you try to filter on the <code>started_at</code> field using a date, it will return all rows, as shown:</p>
			<pre>when = '2019-05-23'
x=df[(df['started_at']&gt;when)]
len(x)
<strong class="bold">34226</strong></pre>
			<p>The length of the entire <a id="_idIndexMarker348"/>DataFrame is <code>34226</code>, so the filter returned all the rows. That<a id="_idIndexMarker349"/> is not what we wanted. Using <code>to_datetime()</code>, you can specify the column and the format. You can assign the result to the same column or specify a new one. In the following example, the <code>started_at</code> column is replaced with the new <code>datetime</code> data type:</p>
			<pre>d['started_at']=pd.to_datetime(df['started_at'],format='%m/%d/%Y %H:%M')
d.dtypes
<strong class="bold">trip_id                int64</strong>
<strong class="bold">started_at    datetime64[ns]</strong></pre>
			<p>Now, the <code>started_at</code> column is a <code>datetime</code> data type and not an object. You can now run queries using dates, as we attempted earlier on the <code>full</code> DataFrame and failed:</p>
			<pre>when = '2019-05-23'
d[(d['started_at']&gt;when)]
<strong class="bold">   trip_id          started_at</strong>
<strong class="bold">4  1636714 2019-05-24 13:38:00</strong></pre>
			<p>The rest of the rows were all on 2019-05-21, so we got the results we expected.</p>
			<p>Now that you can add and remove rows and columns, replace nulls, and create columns, in the next section, you will learn how to enrich your data with external sources.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor070"/>Enriching data </h2>
			<p>The e-scooter<a id="_idIndexMarker350"/> data is geographic data — it contains locations — but it lacks coordinates. If you want to map, or perform spatial queries on this data, you will need coordinates. You can get coordinates by geocoding the location. As luck would have it, the City of Albuquerque has a public geocoder that we can use.</p>
			<p>For this example, we will take a subset of the data. We will use the top five most frequent starting locations. We will then put them in a DataFrame using the following code:</p>
			<pre>new=pd.DataFrame(df['start_location_name'].value_counts().head())
new.reset_index(inplace=True)
new.columns=['address','count']
new
<strong class="bold">                            address  			count</strong>
<strong class="bold">0  1898 Mountain Rd NW, Albuquerque, NM 87104, USA   1210</strong>
<strong class="bold">1    Central @ Tingley, Albuquerque, NM 87104, USA    920</strong>
<strong class="bold">2  2550 Central Ave NE, Albuquerque, NM 87106, USA    848</strong>
<strong class="bold">3  2901 Central Ave NE, Albuquerque, NM 87106, USA    734</strong>
<strong class="bold">4   330 Tijeras Ave NW, Albuquerque, NM 87102, USA    671</strong></pre>
			<p>The <code>address</code> field has more information than we need to geocode. We only need the street address. You will also notice that the second record is an intersection – <code>Central @ Tingley</code>. The geocoder will want the word <em class="italic">and</em> between the streets. Let's clean the data and put it in its own column:</p>
			<pre>n=new['address'].str.split(pat=',',n=1,expand=True)
replaced=n[0].str.replace("@","and")
new['street']=n[0]
new['street']=replaced
new
<strong class="bold">                                           address  count               street</strong>
<strong class="bold">0  1898 Mountain Rd NW, Albuquerque, NM 87104, USA   1210  1898 Mountain Rd NW</strong>
<strong class="bold">1    Central @ Tingley, Albuquerque, NM 87104, USA    920  Central and Tingley</strong>
<strong class="bold">2  2550 Central Ave NE, Albuquerque, NM 87106, USA    848  2550 Central Ave NE</strong>
<strong class="bold">3  2901 Central Ave NE, Albuquerque, NM 87106, USA    734  2901 Central Ave NE</strong>
<strong class="bold">4   330 Tijeras Ave NW, Albuquerque, NM 87102, USA    671   330 Tijeras Ave NW</strong></pre>
			<p>Now you can iterate through the DataFrame and geocode the street field. For this section, you will use another CSV and join it to the DataFrame. </p>
			<p>You can enrich <a id="_idIndexMarker351"/>data by combining it with other data sources. Just like you can join data from two tables in a database, you can do the same with a pandas DataFrame. You can download the <code>geocodedstreet.csv</code> file from the book's GitHub repository. Load the data using <code>pd.read_csv()</code> and you will have a DataFrame with a <code>street</code> column, as well as a column for the <code>x</code> and <code>y</code> coordinates. The result is shown as follows:</p>
			<pre>geo=pd.read_csv('geocodedstreet.csv')
geo
<strong class="bold">                street           x          y</strong>
<strong class="bold">0  1898 Mountain Rd NW -106.667146  35.098104</strong>
<strong class="bold">1  Central and Tingley -106.679271  35.091205</strong>
<strong class="bold">2  2550 Central Ave NE -106.617420  35.080646</strong>
<strong class="bold">3  2901 Central Ave NE -106.612180  35.081120</strong>
<strong class="bold">4   330 Tijeras Ave NW -106.390355  35.078958</strong>
<strong class="bold">5       nothing street -106.000000  35.000000</strong></pre>
			<p>To enrich the original DataFrame with this new data, you can either join or merge the DataFrames. Using a join, you can start with a DataFrame and then add the other as a parameter. You can pass how to join using <code>left</code>, <code>right</code>, or <code>inner</code>, just like you would in SQL. You can add a <code>left</code> and <code>right</code> suffix so the columns that overlap have a way to determine where<a id="_idIndexMarker352"/> they came from. We have joined the two DataFrames in the following example:</p>
			<pre>joined=new.join(other=geo,how='left',lsuffix='_new',rsuffix='_geo')
joined[['street_new','street_geo','x','y']]
<strong class="bold">            street_new           street_geo           x          y</strong>
<strong class="bold">0  1898 Mountain Rd NW  1898 Mountain Rd NW -106.667146  35.098104</strong>
<strong class="bold">1  Central and Tingley  Central and Tingley -106.679271  35.091205</strong>
<strong class="bold">2  2550 Central Ave NE  2550 Central Ave NE -106.617420  35.080646</strong>
<strong class="bold">3  2901 Central Ave NE  2901 Central Ave NE -106.612180  35.081120</strong>
<strong class="bold">4   330 Tijeras Ave NW   330 Tijeras Ave NW -106.390355  35.078958</strong></pre>
			<p>The <code>street</code> column is duplicated and has a <code>left</code> and <code>right</code> suffix. This works but is unnecessary, and we would end up dropping one column and renaming the remaining column, which is just extra work. </p>
			<p>You can use merge to join the DataFrames on a column and not have the duplicates. Merge allows you to pass the DataFrames to merge as well as the field to join on, as shown:</p>
			<pre>merged=pd.merge(new,geo,on='street')
merged.columns
<strong class="bold">Index(['address', 'count', 'street', 'x', 'y'], dtype='object')</strong></pre>
			<p>Notice how the new fields <code>x</code> and <code>y</code> came over in to the new DataFrame, but there is only a single <code>street</code> column. This is much cleaner. In either case, <code>joined</code> or <code>merged</code>, you can only use the index if you have it set on both DataFrames.</p>
			<p>Now that you <a id="_idIndexMarker353"/>know how to clean, transform, and enrich data, it is time to put these skills together and build a data pipeline using this newfound knowledge. The next two sections will show you how to use Airflow and NiFi to build a data pipeline.</p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor071"/>Cleaning data using Airflow</h1>
			<p>Now<a id="_idIndexMarker354"/> that you can<a id="_idIndexMarker355"/> clean your data in Python, you can create functions to perform different tasks. By combining the functions, you can create a data pipeline in Airflow. The following example will clean data, and then filter it and write it out to disk.</p>
			<p>Starting with the same Airflow code you have used in the previous examples, set up the imports and the default arguments, as shown:</p>
			<pre>import datetime as dt
from datetime import timedelta
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
import pandas as pd
default_args = {
    'owner': 'paulcrickard',
    'start_date': dt.datetime(2020, 4, 13),
    'retries': 1,
    'retry_delay': dt.timedelta(minutes=5),
}</pre>
			<p>Now you <a id="_idIndexMarker356"/>can write the functions that will perform the cleaning tasks. First, you <a id="_idIndexMarker357"/>need to read the file, then you can drop the region ID, convert the columns to lowercase, and change the <code>started_at</code> field to a <code>datetime</code> data type. Lastly, write the changes to a file. The following is the code:</p>
			<pre>def cleanScooter():
    df=pd.read_csv('scooter.csv')
    df.drop(columns=['region_id'], inplace=True)
    df.columns=[x.lower() for x in df.columns]
    df['started_at']=pd.to_datetime(df['started_at'],
                                   format='%m/%d/%Y %H:%M')
    df.to_csv('cleanscooter.csv')</pre>
			<p>Next, the pipeline will read in the cleaned data and filter based on a start and end date. The code is as follows:</p>
			<pre>def filterData():
    df=pd.read_csv('cleanscooter.csv')
    fromd = '2019-05-23'
    tod='2019-06-03'
    tofrom = df[(df['started_at']&gt;fromd)&amp;
                (df['started_at']&lt;tod)]
    tofrom.to_csv('may23-june3.csv')	</pre>
			<p>These two functions should look familiar as the code is line for line the same as in the preceding examples, just regrouped. Next, you need to define the operators and tasks. You will use <code>PythonOperator</code> and point it to your functions. Create the DAG and the tasks as shown:</p>
			<pre>with DAG('CleanData',
         default_args=default_args,
         schedule_interval=timedelta(minutes=5),      
         # '0 * * * *',
         ) as dag:
    cleanData = PythonOperator(task_id='clean',
                                 python_callable=cleanScooter)
    
    selectData = PythonOperator(task_id='filter',
                                 python_callable=filterData)</pre>
			<p>In this <a id="_idIndexMarker358"/>example, we will add in another task using <code>BashOperator</code> again. If you <a id="_idIndexMarker359"/>recall, you used it in <a href="B15739_03_ePub_AM.xhtml#_idTextAnchor039"><em class="italic">Chapter 3</em></a><em class="italic">, Reading and Writing Files</em>, just to print a message to the terminal. This time, you will use it to move the file from the <code>selectData</code> task and copy it to the desktop. The code is as follows:</p>
			<pre>copyFile = BashOperator(task_id='copy',
                                 bash_command='cp /home/paulcrickard/may23-june3.csv /home/paulcrickard/Desktop')</pre>
			<p>The preceding command just uses the Linux copy command to make a copy of the file. When working with files, you need to be careful that your tasks can access them. If multiple processes attempt to touch the same file or a user tries to access it, you could break your pipeline. Lastly, specify the order of the tasks — create the direction of the DAG as shown:</p>
			<pre>cleanData &gt;&gt; selectData &gt;&gt; copyFile</pre>
			<p>Now you have a completed DAG. Copy this file to your <code>$AIRFLOW_HOME/dags</code> folder. Then, start Airflow with the following command:</p>
			<pre>airflow webserver
airflow scheduler</pre>
			<p>Now you can browse to <code>http://localhost:8080/admin</code> to view the GUI. Select your new DAG and click the <strong class="bold">Tree View</strong> tab. You will see your DAG and you can turn it on and run it. In the following screenshot, you will see the DAG and the runs of each task:</p>
			<div><div><img src="img/Figure_5.1_B15739.jpg" alt="Figure 5.1 – Running the DAG&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – Running the DAG</p>
			<p>You will see<a id="_idIndexMarker360"/> that the DAG has two failed runs. This was a result of the file not being <a id="_idIndexMarker361"/>present when a task ran. I had used <code>move</code> instead of <code>copy</code> in <code>BashOperator</code>, hence the warning about being careful when handling files in Airflow.</p>
			<p>Congratulations! You have successfully completed this chapter.</p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor072"/>Summary</h1>
			<p>In this chapter, you learned how to perform basic EDA with an eye toward finding errors or problems within your data. You then learned how to clean your data and fix common data issues. With this set of skills, you built a data pipeline in Apache Airflow.</p>
			<p>In the next chapter, you will walk through a project, building a 311 data pipeline and dashboard in Kibana. This project will utilize all of the skills you have acquired up to this point and will introduce a number of new skills – such as building dashboards and making API calls.</p>
		</div>
	</body></html>