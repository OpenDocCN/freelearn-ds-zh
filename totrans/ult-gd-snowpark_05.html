<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-42"><a id="_idTextAnchor042" class="calibre6 pcalibre1 pcalibre"/>3</h1>
<h1 id="_idParaDest-43" class="calibre5"><a id="_idTextAnchor043" class="calibre6 pcalibre1 pcalibre"/>Simplifying Data Processing Using Snowpark</h1>
<p class="calibre3">In the previous chapter, we learned how to set up a development environment for Snowpark, as well as various Snowpark components, such as DataFrames, UDFs, and stored procedures. We also covered how to operate those objects and run them in Snowflake. In this chapter, we will cover data processing with Snowpark and learn how to load, prepare, analyze, and transform data using Snowpark.</p>
<p class="calibre3">In this chapter, we’re going to cover the following main topics:</p>
<ul class="calibre15">
<li class="calibre14">Data ingestion</li>
<li class="calibre14">Data exploration and transformation</li>
<li class="calibre14">Data grouping and analysis</li>
</ul>
<h1 id="_idParaDest-44" class="calibre5"><a id="_idTextAnchor044" class="calibre6 pcalibre1 pcalibre"/>Technical requirements</h1>
<p class="calibre3">For this chapter, you require an active Snowflake account and Python installed with Anaconda configured locally. You can refer to the following documentation for installation instructions:</p>
<ul class="calibre15">
<li class="calibre14">You can sign up for a Snowflake Trial account at <a href="https://signup.snowflake.com/" class="calibre6 pcalibre1 pcalibre">https://signup.snowflake.com/</a></li>
<li class="calibre14">To configure Anaconda, follow the guide at  https://conda.io/projects/conda/en/latest/user-guide/getting-started.html</li>
<li class="calibre14">To install and set up Python for VS Code, follow the guide at <a href="https://code.visualstudio.com/docs/python/python-tutorial" class="calibre6 pcalibre1 pcalibre">https://code.visualstudio.com/docs/python/python-tutorial</a></li>
<li class="calibre14">To learn how to operate Jupyter Notebook in VS Code, go to <a href="https://code.visualstudio.com/docs/datascience/jupyter-notebooks" class="calibre6 pcalibre1 pcalibre">https://code.visualstudio.com/docs/datascience/jupyter-notebooks</a></li>
</ul>
<p class="calibre3">The supporting materials for this chapter are available in this book’s GitHub repository at <a href="https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark" class="calibre6 pcalibre1 pcalibre">https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark</a>.</p>
<h1 id="_idParaDest-45" class="calibre5"><a id="_idTextAnchor045" class="calibre6 pcalibre1 pcalibre"/>Data ingestion</h1>
<p class="calibre3">The first part <a id="_idIndexMarker144" class="calibre6 pcalibre1 pcalibre"/>of the data engineering process is data ingestion – it is crucial to get all the different data into a usable format in Snowflake for analytics. In the previous chapter, we learned how Snowpark can access data through a DataFrame. This DataFrame can access data from Snowflake tables, views, and objects, such as streams, if we run a query against it. Snowpark supports structured data in various formats, such as Excel and CSV, as well as semi-structured data, such as JSON, XML, Parquet, Avro, and ORC; specialized formats, such as HL7 and DICOM, and unstructured data, such as images and media, can be ingested and handled in Snowpark. Snowpark enables secure and programmatic access to files in Snowflake stages.</p>
<p class="calibre3">The flexibility of Snowpark Python allows you to adapt to changing data requirements effortlessly. Suppose you start with a CSV file as your data source; you can switch to a JSON or packet format at a later stage. With Snowpark, you don’t need to rewrite your entire code base. Instead, you can make minor adjustments or configuration changes to accommodate the new structure while keeping the core logic intact. This flexibility saves you valuable time and effort, enabling you to switch between different data formats as your needs evolve quickly.</p>
<p class="calibre3">By leveraging Snowpark’s capabilities, you can focus more on analyzing and utilizing data rather than worrying about the intricacies of data format handling. This streamlined approach empowers you to experiment with different data sources, adapt to evolving data requirements, and efficiently load data into Snowflake tables, all with minimal code changes and maximum flexibility.</p>
<p class="calibre3">So, let’s delve into the power of Snowpark Python and its ability to effortlessly handle different data formats, allowing you to work with diverse sources without cumbersome code modifications. You will experience the freedom to explore, analyze, and extract insights from your data while enjoying a seamless and flexible integration process.</p>
<p class="calibre3">The data ingestion scripts are provided in this book’s GitHub repository: <a href="https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark" class="calibre6 pcalibre1 pcalibre">https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark</a>. These scripts will simplify the process of uploading any new dataset that will be used for analysis, ensuring a smooth and efficient workflow. Following a similar approach to what was outlined in the preceding chapters, you can effortlessly upload new datasets and explore Snowflake’s data <a id="_idIndexMarker145" class="calibre6 pcalibre1 pcalibre"/>engineering and machine learning functionalities. The provided data ingestion scripts will act as your guide, making the process seamless and hassle-free.</p>
<h2 id="_idParaDest-46" class="calibre7"><a id="_idTextAnchor046" class="calibre6 pcalibre1 pcalibre"/>Important note on datasets</h2>
<p class="calibre3">The dataset we’ll be using<a id="_idIndexMarker146" class="calibre6 pcalibre1 pcalibre"/> in this chapter provides unique insights into customer behavior, campaign responses, and complaints, enabling data-driven decision-making and customer satisfaction improvement. The original dataset is from the Kaggle platform (<a href="https://www.kaggle.com/datasets/rodsaldanha/arketing-campaign" class="calibre6 pcalibre1 pcalibre">https://www.kaggle.com/datasets/rodsaldanha/arketing-campaign</a>). However, the datasets that will be discussed in this section are not directly accessible via a Kaggle link. Instead, we started with a base dataset and generated new data formats to illustrate loading various dataset formats using Snowpark. These datasets can be found in this book’s GitHub repository under the <code>datasets</code> folder.</p>
<p class="calibre3">The datasets include purchase history in CSV format, campaign information in JSON format, and complaint information in Parquet format. These datasets provide valuable information about customer behavior, campaign responses, and complaints:</p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold">Purchase history (CSV)</strong>: This <a id="_idIndexMarker147" class="calibre6 pcalibre1 pcalibre"/>file contains customer information, such as ID, education, marital status, and purchase metrics. The dataset offers insights into customer buying habits and can be further analyzed for data-driven decisions.</li>
<li class="calibre14"><strong class="bold">Campaign information (JSON)</strong>: The JSON dataset includes data on campaign acceptance<a id="_idIndexMarker148" class="calibre6 pcalibre1 pcalibre"/> and customer responses. Analyzing this dataset will help you refine marketing strategies and understand campaign effectiveness.</li>
<li class="calibre14"><strong class="bold">Complaint information (Parquet)</strong>: This file contains details about customer complaints, including<a id="_idIndexMarker149" class="calibre6 pcalibre1 pcalibre"/> contact and revenue metrics. This dataset aids in tracking and addressing customer complaints for improved satisfaction.</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">Moving forward, we will be utilizing our local development environment to execute all Snowpark code, rather than relying on Snowflake worksheets. This approach offers greater flexibility and control over the development and testing of Snowpark scripts. When worksheets are used for specific tasks, we will explicitly call out their usage for clarity and context.</p>
<h2 id="_idParaDest-47" class="calibre7"><a id="_idTextAnchor047" class="calibre6 pcalibre1 pcalibre"/>Ingesting a CSV file into Snowflake</h2>
<p class="calibre3">Snowflake <a id="_idIndexMarker150" class="calibre6 pcalibre1 pcalibre"/>supports ingesting data easily using CSV files. We will load the purchase history data into the <code>PURCHASE_HISTORY</code> table as a CSV file. We’ll upload <code>purchase_history.csv</code> into an internal stage by using a Snowpark session, as shown here:</p>
<pre class="source-code">
session.file.put('./datasets/purchase_history.csv', 'MY_STAGE')</pre> <p class="calibre3">With that, the file has been uploaded to the internal stage. We will reference this directly in Snowpark. The data schema for the marketing table can also be directly defined as a Snowpark type. The following code provides the necessary columns and data types to create the table in Snowflake:</p>
<pre class="source-code">
import snowflake.snowpark.types as T
purchase_history_schema = T.StructType([
    T.StructField("ID", T.IntegerType()),
    T.StructField("Year_Birth", T.IntegerType()),
    T.StructField("Education", T.StringType()),
    T.StructField("Marital_Status", T.StringType()),
    T.StructField("Income", T.IntegerType()),
    T.StructField("Kidhome", T.IntegerType()),
    T.StructField("Teenhome", T.IntegerType()),
    T.StructField("Dt_Customer", T.DateType()),
    T.StructField("Recency", T.IntegerType()),
    T.StructField("MntWines", T.IntegerType()),
    T.StructField("MntFruits", T.IntegerType()),
    T.StructField("MntMeatProducts", T.IntegerType()),
    T.StructField("MntFishProducts", T.IntegerType()),
    T.StructField("MntSweetProducts", T.IntegerType()),
    T.StructField("MntGoldProds", T.IntegerType()),
    T.StructField("NumDealsPurchases", T.IntegerType()),
    T.StructField("NumWebPurchases", T.IntegerType()),
    T.StructField("NumCatalogPurchases", T.IntegerType()),
    T.StructField("NumStorePurchases", T.IntegerType()),
    T.StructField("NumWebVisitsMonth", T.IntegerType())
])</pre> <p class="calibre3">In this code<a id="_idIndexMarker151" class="calibre6 pcalibre1 pcalibre"/> snippet, we take our first step toward understanding the structure of our data by defining a schema for our purchase history dataset. Using the Snowflake Snowpark library, we establish the fields and corresponding data types, setting the foundation for our data analysis journey. This code serves as a starting point, guiding us in defining and working with structured data. This is not the only way we can load the dataset using Snowpark. We will continue to explore different methodologies to load other tabular datasets as we progress.</p>
<p class="calibre3">This code imports the necessary types from the Snowflake Snowpark library. It creates a variable called <code>purchase_history_schema</code> and assigns it a <code>StructType</code> object, representing a structured schema for the dataset. The <code>StructType</code> object contains multiple <code>StructField</code> objects, each representing a field in the dataset. Each <code>StructField</code> object specifies the name of the area and its corresponding data type using the types provided by Snowflake Snowpark. The following code reads the file:</p>
<pre class="source-code">
purchase_history = session.read\
        .option("FIELD_DELIMITER", ',')\
        .option("SKIP_HEADER", 1)\
        .option("ON_ERROR", "CONTINUE")\
        .schema(purchase_history_schema).csv(
            "@MY_Stage/purchase_history.csv.gz")\
        .copy_into_table("PURCHASE_HISTORY")</pre> <p class="calibre3">The CSV file <a id="_idIndexMarker152" class="calibre6 pcalibre1 pcalibre"/>is read with file format options such as <code>FIELD_DELIMITER</code>, <code>SKIP_HEADER</code>, and others, all of which are specified alongside the schema defined in the preceding definition. The <code>PURCHASE_HISTORY</code> table was created with the data from the CSV file, which is now ready for processing:</p>
<pre class="source-code">
session.table("PURCHASE_HISTORY").show()</pre> <p class="calibre3">The preceding code shows the output of the <code>PURCHASE_HISTORY</code> table:</p>
<div><div><img alt="Figure 3.1 – The PURCHASE_HISTORY table" src="img/B19923_03_1.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.1 – The PURCHASE_HISTORY table</p>
<p class="calibre3">The CSV is easy to load as it uses the file format options available in Snowflake. Now, let’s see how we can load JSON files into Snowflake.</p>
<h2 id="_idParaDest-48" class="calibre7"><a id="_idTextAnchor048" class="calibre6 pcalibre1 pcalibre"/>Ingesting JSON into Snowflake</h2>
<p class="calibre3">Snowflake<a id="_idIndexMarker153" class="calibre6 pcalibre1 pcalibre"/> allows JSON structures to be ingested and processed via the <code>Variant</code> data type. We can ingest JSON similar to how we would ingest a CSV file – by uploading it into the internal stage. The <code>campaign_info.json</code> file contains data about marketing campaigns. We can load this into the <code>CAMPAIGN_INFO</code> table by using the following code:</p>
<pre class="source-code">
session.file.put('./datasets/campaign_info.json', 'MY_STAGE')</pre> <p class="calibre3">With that, the file has been uploaded to the internal stage; we will reference it in Snowpark. Snowpark can access the file to load it into a table:</p>
<pre class="source-code">
df_from_json = session.read.json("@My_Stage/campaign_info.json.gz")</pre> <p class="calibre3">The <a id="_idIndexMarker154" class="calibre6 pcalibre1 pcalibre"/>contents of the JSON file are read into the DataFrame as JSON objects. This DataFrame can be written into a table as a variant:</p>
<pre class="source-code">
df_from_json.write.save_as_table("CAMPAIGN_INFO_TEMP", 
    mode = "overwrite")</pre> <p class="calibre3">The <code>CAMPAIGN_INFO_TEMP</code> table contains the JSON data. We can query the table to view the data:</p>
<pre class="source-code">
df_from_json.show()</pre> <p class="calibre3">The preceding command displays the JSON data from the DataFrame:</p>
<div><div><img alt="Figure 3.2 – The Campaign Info table" src="img/B19923_03_2.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.2 – The Campaign Info table</p>
<p class="calibre3">The following code snippet utilizes the Snowpark library in Snowflake to manipulate a DataFrame:</p>
<pre class="source-code">
from snowflake.snowpark.functions import col
df_flatten = df_from_json.select(col("$1")["ID"].as_("ID"),\
    col("$1")["AcceptedCmp1"].as_("AcceptedCmp1"),\
    col("$1")["AcceptedCmp2"].as_("AcceptedCmp2"),\
    col("$1")["AcceptedCmp3"].as_("AcceptedCmp3"),\
    col("$1")["AcceptedCmp4"].as_("AcceptedCmp4"),\
    col("$1")["AcceptedCmp5"].as_("AcceptedCmp5"),\
    col("$1")["Response"].as_("Response"))
df_flatten.write.save_as_table("CAMPAIGN_INFO")</pre> <p class="calibre3">The <a id="_idIndexMarker155" class="calibre6 pcalibre1 pcalibre"/>preceding code selects specific columns from an existing DataFrame and renames them using the <code>col</code> function. The transformed DataFrame is then saved as a new table in Snowflake. The code performs data <strong class="bold">extraction, transformation, and loading </strong>(<strong class="bold">ETL</strong>) operations by selecting and renaming columns within the DataFrame and saving the result as a new table in Snowflake.</p>
<p class="calibre3">The <code>CAMPAIGN_INFO</code> table now contains the flattened data, with the data in separate columns so that it’s easier to process. Let’s have a look at the data:</p>
<pre class="source-code">
session.table("CAMPAIGN_INFO").show()</pre> <p class="calibre3">The preceding code shows the output of the <code>CAMPAIGN_INFO</code> table:</p>
<div><div><img alt="Figure 3.3 – The CAMPAIGN_INFO table" src="img/B19923_03_3.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.3 – The CAMPAIGN_INFO table</p>
<p class="calibre3">Loading and processing JSON <a id="_idIndexMarker156" class="calibre6 pcalibre1 pcalibre"/>files in Snowpark becomes easier when using the <code>Variant</code> column. Next, we will cover how to load a Parquet file into Snowflake using Snowpark.</p>
<h2 id="_idParaDest-49" class="calibre7"><a id="_idTextAnchor049" class="calibre6 pcalibre1 pcalibre"/>Ingesting Parquet files into Snowflake</h2>
<p class="calibre3">Parquet is a <a id="_idIndexMarker157" class="calibre6 pcalibre1 pcalibre"/>popular open source format for storing data licensed under Apache. The column-oriented format is lighter to store and faster to process. Parquet also supports complex data types since the data and the column information are stored in Parquet format. The <code>COMPLAINT_INFO</code> table consists of customer complaint information. Let’s load this into Snowflake:</p>
<pre class="source-code">
session.file.put('./datasets/complain_info.parquet', 'MY_STAGE')</pre> <p class="calibre3">The file will be uploaded into the internal stage. Snowpark can access it to process and load it into a table:</p>
<pre class="source-code">
df_raw = session.read.parquet("@My_Stage/complain_info.parquet")
df_raw.copy_into_table("COMPLAINT_INFO")</pre> <p class="calibre3">The Parquet file is read into the DataFrame and then copied into the <code>COMPLAINT_INFO</code> table. Since the Parquet file already contains the table metadata information, it defines the table structure. We can query the table to view the data:</p>
<pre class="source-code">
session.table("COMPLAINT_INFO").show()</pre> <p class="calibre3">This will output the following <code>COMPLAINT_INFO</code> table:</p>
<div><div><img alt="Figure 3.4 – The COMPLAINT_INFO table" src="img/B19923_03_4.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.4 – The COMPLAINT_INFO table</p>
<p class="calibre3">Parquet is <a id="_idIndexMarker158" class="calibre6 pcalibre1 pcalibre"/>one of the preferred formats for Snowflake since it’s the format that’s used by Apache Iceberg. Parquet stands out in data engineering and data science for its columnar storage, which optimizes compression and query performance. Its support for schema evolution and partitioning ensures flexibility and efficiency in handling evolving data structures. With broad compatibility across various data processing frameworks, Parquet enables seamless integration into existing workflows, making it a cornerstone format in modern data pipelines. In the next section, we will cover how easy it is to load unstructured data, such as an image, into Snowflake.</p>
<p class="callout-heading">Important note</p>
<p class="callout">We’ve chosen to maintain separate stages for handling images and text, although it’s not mandatory to do so. The <strong class="source-inline1">MY_TEXT</strong> and <strong class="source-inline1">MY_IMAGES</strong> stages can be prepared using the same methods we outlined earlier.</p>
<h2 id="_idParaDest-50" class="calibre7"><a id="_idTextAnchor050" class="calibre6 pcalibre1 pcalibre"/>Ingesting images into Snowpark</h2>
<p class="calibre3">Snowflake<a id="_idIndexMarker159" class="calibre6 pcalibre1 pcalibre"/> supports versatile data, such as images, that can be uploaded into a stage and executed directly in Snowpark without the need to manage dependencies as well.</p>
<p class="calibre3">Platforms such as Amazon S3, Google Cloud Storage, and Azure Blob Storage are commonly preferred for managing and storing image data due to their scalability and reliability. However, it’s worth noting that Snowpark also offers flexibility in loading image files, making it a versatile option for handling image data in data engineering and data science workflows. We will be loading a bunch of sample images that can be used for processing:</p>
<pre class="source-code">
session.file.put("./datasets/sample_images/*.png", "@My_Images")</pre> <p class="calibre3">The preceding code loads the images from the local folder to the internal stage. The path can support wildcard entries to upload all the images in a particular folder. The folder in the stage can be queried to get the list of images that were uploaded:</p>
<pre class="source-code">
Session.sql("LS @My_Images").show()</pre> <p class="calibre3">The preceding code shows a list of all the images that are present in the stage:</p>
<div><div><img alt="Figure 3.5 – List of images" src="img/B19923_03_5.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.5 – List of images</p>
<p class="calibre3">Once the image has been uploaded, it can be directly accessed via Snowpark. Snowpark supports the <code>get_stream</code> function to stream the file’s contents as bytes from the stage. We can use a library such as Pillow to read the file from the bytes stream:</p>
<pre class="source-code">
import PIL.Image
bytes_object = session.file.get_stream(
    "@My_Images/101.png.gz", decompress=True)
image = PIL.Image.open(bytes_object)
image.resize((150,150))</pre> <p class="calibre3">This will <a id="_idIndexMarker160" class="calibre6 pcalibre1 pcalibre"/>output the following image:</p>
<div><div><img alt="Figure 3.6 – Rendering images" src="img/B19923_03_6.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.6 – Rendering images</p>
<p class="calibre3">The image is displayed directly in the notebook. Snowpark’s native support for images supports capabilities for use cases such as image classification, image processing, and image recognition. Snowpark also supports rendering images dynamically. We will cover this in the next section.</p>
<h3 class="calibre9">Reading files dynamically with Snowpark</h3>
<p class="calibre3">Snowpark <a id="_idIndexMarker161" class="calibre6 pcalibre1 pcalibre"/>contains the <code>files</code> module and the <code>SnowflakeFile</code> class, both of which provide access to files dynamically and stream them for processing. These dynamic files are also helpful for reading multiple files as we can iterate over them. <code>open()</code> extends the <code>IOBase</code> file objects and provides the functionality to open a file. The <code>SnowflakeFile</code> object also supports other <code>IOBase</code> methods for processing the file. The following code shows an example of reading multiple files using a relative path from the internal stage:</p>
<pre class="source-code">
import snowflake.snowpark as snowpark
from snowflake.snowpark.functions import udf
from snowflake.snowpark.files import SnowflakeFile
from snowflake.snowpark.types import StringType, IntegerType
@udf(
    name="get_bytes_length",
    replace=True,
    input_types=[StringType()],
    return_type=IntegerType(),
    packages=['snowflake-snowpark-python']
)
def get_file_length(file_path):
    with SnowflakeFile.open(file_path) as f:
        s = f.read()
        return len(s)</pre> <p class="calibre3">The <a id="_idIndexMarker162" class="calibre6 pcalibre1 pcalibre"/>preceding code iterates over the <code>@MY_TEXTS</code> stage location and calculates the length of each file using the <code>SnowflakeFile</code> method. The path is passed as the input to the UDF. We can execute the function to get the output:</p>
<pre class="source-code">
session.sql("SELECT RELATIVE_PATH, \
    get_bytes_length(build_scoped_file_url( \
        @MY_TEXTS,RELATIVE_PATH)) \
             as SIZE from DIRECTORY(@MY_TEXTS);").collect()</pre> <p class="calibre3">The preceding code produces the following result:</p>
<div><div><img alt="Figure 3.7 – Dynamic files within Snowpark" src="img/B19923_03_7.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.7 – Dynamic files within Snowpark</p>
<p class="calibre3">The files in the stage are displayed as output. In this section, we covered ingesting different types <a id="_idIndexMarker163" class="calibre6 pcalibre1 pcalibre"/>of files into Snowflake using Snowpark. In the next section, we will learn how to perform data preparation and transformations using Snowpark.</p>
<h1 id="_idParaDest-51" class="calibre5"><a id="_idTextAnchor051" class="calibre6 pcalibre1 pcalibre"/>Data exploration and transformation</h1>
<p class="calibre3">Once the data has been loaded, the next step is to prepare the data so that it can be transformed. In this section, we will cover how to perform data exploration so that we understand how the modify the data as necessary.</p>
<h2 id="_idParaDest-52" class="calibre7"><a id="_idTextAnchor052" class="calibre6 pcalibre1 pcalibre"/>Data exploration</h2>
<p class="calibre3"><strong class="bold">Data exploration</strong> is a<a id="_idIndexMarker164" class="calibre6 pcalibre1 pcalibre"/> critical step in data analysis as it sets the stage for successful insights and informed decision-making. By delving into the data, analysts can deeply understand its characteristics, uncover underlying patterns, and identify potential issues or outliers. Exploring the data provides valuable insights into its structure, distribution, and relationships, enabling analysts to choose the appropriate data transformation techniques.</p>
<p class="calibre3">Understanding the data’s characteristics and patterns helps analysts determine the appropriate transformations and manipulations needed to clean, reshape, or derive new variables from the data. Additionally, data exploration aids in identifying subsets of data that are relevant to the analysis, facilitating the filtering and sub-setting operations required for specific analytical objectives.</p>
<p class="calibre3">Before embarking on data transformation, we must understand the data we have in place. By comprehensively understanding the data, we can effectively identify its structure, quality, and patterns. This understanding is a solid foundation for informed decision-making during the data transformation process, enabling us to extract meaningful insights and derive maximum value from the data. Take a look at the following code:</p>
<pre class="source-code">
purchase_history = session.table("PURCHASE_HISTORY")
campaign_info = session.table("CAMPAIGN_INFO")
complain_info = session.table("COMPLAINT_INFO")</pre> <p class="calibre3">Here, we loaded the necessary tables into a session. These tables are now available in the Snowpark session for further data preparation. We will start by preparing the <code>PURCHASE_HISTORY</code> table:</p>
<pre class="source-code">
purchase_history.show(n=5)</pre> <p class="calibre3">The <code>show()</code> method <a id="_idIndexMarker165" class="calibre6 pcalibre1 pcalibre"/>returns the data from the DataFrame. The preceding code produces the top 5 rows from the <code>PURCHASE_HISTORY</code> table:</p>
<div><div><img alt="Figure 3.8 – PURCHASE_HISTORY – top 5 rows" src="img/B19923_03_8.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.8 – PURCHASE_HISTORY – top 5 rows</p>
<p class="calibre3">We can use the <code>collect()</code> method to display the data in the notebook:</p>
<pre class="source-code">
purchase_history.collect()</pre> <p class="calibre3">The records from the <code>PURCHASE_HISTORY</code> table are shown in the JSON array:</p>
<div><div><img alt="Figure 3.9 – PURCHASE_HISTORY – full table" src="img/B19923_03_9.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.9 – PURCHASE_HISTORY – full table</p>
<p class="callout-heading">The difference between collect() and show()</p>
<p class="callout">In Snowpark Python, there are two essential functions: <strong class="source-inline1">collect()</strong> and <strong class="source-inline1">show()</strong>. These functions serve different purposes in processing and displaying data. The <strong class="source-inline1">collect()</strong> function in Snowpark Python is used to gather or retrieve data from a specified source, such as a table, file, or API. It allows you to perform queries, apply filters, and extract the desired information from the data source. The collected data is stored in a variable or structure, such as a DataFrame, for further analysis or manipulation.</p>
<p class="callout">On the other hand, the <strong class="source-inline1">show()</strong> function in Snowpark Python is primarily used to display the contents of a DataFrame or any other data structure in a tabular format. It provides a convenient way to visualize and inspect the data at different stages of the data processing pipeline. The <strong class="source-inline1">show()</strong> function presents the data in a human-readable manner, showing the rows and columns in a structured table-like format. It can be helpful for debugging, understanding the data’s structure, or performing exploratory data analysis.</p>
<p class="callout">In short, the <strong class="source-inline1">collect()</strong> function focuses on gathering and retrieving data from a source, while the <strong class="source-inline1">show()</strong> function displays the data in a readable format. Both functions play essential roles in Snowpark Python when it comes to working with data, but they serve distinct purposes in the data processing workflow.</p>
<p class="calibre3">Next, we will <a id="_idIndexMarker166" class="calibre6 pcalibre1 pcalibre"/>use the <code>count()</code> method to get the total count of the rows in the table:</p>
<pre class="source-code">
purchase_history.count()</pre> <p class="calibre3">From the resulting output, we can see that the <code>PURCHASE_HISTORY</code> table contains around 2,000 rows of data.</p>
<p class="calibre3">We can now check the columns of the table to understand more about this data:</p>
<pre class="source-code">
purchase_history.columns</pre> <p class="calibre3">This returns the column information, which helps us understand the data better. The column information contains the data related to customer purchase history:</p>
<div><div><img alt="Figure 3.10 – PURCHASE_HISTORY columns" src="img/B19923_03_10.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.10 – PURCHASE_HISTORY columns</p>
<p class="calibre3">We can now filter the data to slice and dice it. We can use the following code to filter specific rows or a single row:</p>
<pre class="source-code">
from snowflake.snowpark.functions import col
purchase_history.filter(col("id") == 1).show()</pre> <p class="calibre3">This returns<a id="_idIndexMarker167" class="calibre6 pcalibre1 pcalibre"/> the column. where <code>id</code> is set to <code>1</code>. We can pass multiple values in the column filter to perform additional row-level operations:</p>
<div><div><img alt="Figure 3.11 – PURCHASE_HISTORY ID filter" src="img/B19923_03_11.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.11 – PURCHASE_HISTORY ID filter</p>
<p class="calibre3">If we need to add multiple filter values, we can use the <code>&amp;</code> operation to pass multiple column filter values to the method:</p>
<pre class="source-code">
purchase_history.filter((col("MARITAL_STATUS") == "Married") &amp; 
                        (col("KIDHOME") == 1)).show()</pre> <p class="calibre3">The preceding code provides data about those with <code>MARITAL_STATUS</code> set to <code>Married</code> and who have kids at home (<code>KIDHOME</code>):</p>
<div><div><img alt="Figure 3.12 – PURCHASE_HISTORY filters" src="img/B19923_03_12.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.12 – PURCHASE_HISTORY filters</p>
<p class="calibre3">This helps us understand the purchase history pattern of married customers with kids. We can also filter it to the year of birth by passing the year of birth range between 1964 and 1980:</p>
<pre class="source-code">
purchase_history.filter((col("YEAR_BIRTH") &gt;= 1964) &amp; 
                        (col("YEAR_BIRTH") &lt;= 1980)).show()</pre> <p class="calibre3">This displays the purchase data for customers born between 1964 and 1980:</p>
<div><div><img alt="Figure 3.13 – PURCHASE_HISTORY filters" src="img/B19923_03_13.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.13 – PURCHASE_HISTORY filters</p>
<p class="calibre3">This data helps <a id="_idIndexMarker168" class="calibre6 pcalibre1 pcalibre"/>us understand their purchases. We can also use the <code>select()</code> method to select only the columns that are required for analysis:</p>
<pre class="source-code">
purchase_history.select(col("ID"), col("YEAR_BIRTH"), 
                        col("EDUCATION")).show()</pre> <p class="calibre3">The preceding returns only the customer’s ID, year, and education status:</p>
<div><div><img alt="Figure 3.14 – PURCHASE_HISTORY columns" src="img/B19923_03_14.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.14 – PURCHASE_HISTORY columns</p>
<p class="calibre3">In the upcoming chapters, we will delve deeper into data exploration, uncovering more techniques to gain insights from our data.</p>
<p class="calibre3">Building upon these basic exploration steps, we will dive into the realm of data transformation operations. By combining our understanding of the data and the power of transformation techniques, we will unlock the full potential of our data and extract valuable insights for <a id="_idIndexMarker169" class="calibre6 pcalibre1 pcalibre"/>informed decision-making.</p>
<p class="calibre3">In the next section, we will discuss how to perform data transformation using this data.</p>
<h2 id="_idParaDest-53" class="calibre7"><a id="_idTextAnchor053" class="calibre6 pcalibre1 pcalibre"/>Data transformations</h2>
<p class="calibre3"><strong class="bold">Data transformation</strong> is a<a id="_idIndexMarker170" class="calibre6 pcalibre1 pcalibre"/> fundamental process that involves modifying and reshaping data to make it more suitable for analysis or other downstream tasks, such as the machine learning model building process. It entails applying a series of operations to the data, such as cleaning, filtering, aggregating, and reformatting, to ensure its quality, consistency, and usability. Data transformation allows us to convert raw data into a structured and organized format that can be easily interpreted and analyzed.</p>
<p class="calibre3">The data requires minimal transformation, and we will cover it extensively in the coming chapters. Our goal for this section is to combine data from different sources, creating a unified table for further processing that we will use in the next chapter. We will leverage Snowpark’s robust join and union capabilities to accomplish this. By utilizing joins, we can merge data based on standard columns or conditions. Unions, on the other hand, allow us to append data from multiple sources vertically. These techniques will enable us to integrate and consolidate our data efficiently, setting the stage for comprehensive analysis and insights. Let’s explore how Snowpark’s join and union capabilities can help us achieve this data combination:</p>
<pre class="source-code">
purchase_campaign = purchase_history.join(
    campaign_info,
    purchase_history.ID == campaign_info.ID ,
    lsuffix="_left", rsuffix="_right"
)</pre> <p class="calibre3">Here, we are joining the purchase history to campaign information to establish the relationship between purchases and campaigns. The standard ID column is used to select the join and defaults to an inner join:</p>
<pre class="source-code">
purchase_campaign = purchase_campaign.drop("ID_RIGHT")</pre> <p class="calibre3">We are dropping the extra ID column from the joined result. The DataFrame now contains just a single ID column:</p>
<pre class="source-code">
purchase_campaign.show()</pre> <p class="calibre3">This displays the <a id="_idIndexMarker171" class="calibre6 pcalibre1 pcalibre"/>data of the purchase campaign combined with the purchase history and the campaign information:</p>
<div><div><img alt="Figure 3.15 – Purchase campaign data" src="img/B19923_03_15.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.15 – Purchase campaign data</p>
<p class="calibre3">Let’s combine this with the complaint information to get the complete data:</p>
<pre class="source-code">
final_combined = purchase_campaign.join(
    complain_info,
    purchase_campaign["ID_LEFT"] == complain_info.ID
)
final_combined = final_combined.drop("ID_LEFT")</pre> <p class="calibre3">Here, we are combining the result of the purchase campaign along with the complaint information by using the standard ID column. The resultant DataFrame contains the complete data required for data analysis. We are dropping the extra ID column from the joined result. The DataFrame now has just a single ID column:</p>
<pre class="source-code">
final_combined.show()</pre> <p class="calibre3">This displays the final data combined from all three tables. We can now write this data into the table for further analysis:</p>
<pre class="source-code">
final_combined.write.save_as_table("MARKETING_DATA")</pre> <p class="calibre3">Here, the data is written into the <code>MARKETING_DATA</code> table, at which point it will be available inside Snowflake. We need to append this data with the additional marketing data that must<a id="_idIndexMarker172" class="calibre6 pcalibre1 pcalibre"/> be loaded into this table.</p>
<p class="callout-heading">The difference between joins and unions</p>
<p class="callout">Joins combine data from two or more tables based on a shared column or condition. In Snowflake Snowpark, you can perform different types of joins, such as inner join, left join, right join, and full outer join. Joins allow you to merge data horizontally by aligning rows based on matching values in the specified columns. This enables you to combine related data from multiple tables, resulting in a combined dataset that includes information from all the joined tables.</p>
<p class="callout">On the other hand, unions are used to append data from multiple tables vertically, or result sets into a single dataset. Unlike joins, unions do not require any specific conditions or matching columns. Instead, they stack rows on top of each other, concatenating the data vertically. This is useful when you have similar datasets with the same structure and want to consolidate them into a single dataset. Unions can be performed in Snowflake Snowpark to create a new dataset that contains all the rows from the input tables or result sets.</p>
<p class="callout">In summary, joins in Snowflake Snowpark are used to combine data horizontally by matching columns, while unions are used to stack data vertically without any specific conditions. Joins merge related data from multiple tables, while unions append similar datasets into a single dataset.</p>
<h3 class="calibre9">Appending data</h3>
<p class="calibre3">The <a id="_idIndexMarker173" class="calibre6 pcalibre1 pcalibre"/>Snowflake Snowpark <code>UNION</code> function is vital in combining and integrating new data into a Snowflake database. The importance of the <code>UNION</code> function lies in its ability to append rows from different data sources vertically, or result sets into a single consolidated dataset. When new data is added to the database, it is often necessary to merge or combine it with existing data for comprehensive analysis. The <code>UNION</code> function allows us to seamlessly integrate the newly added data with the existing dataset, creating a unified view encompassing all relevant information.</p>
<p class="calibre3">This capability of the <code>UNION</code> function is precious in scenarios where data is received or updated periodically. For example, suppose we receive daily sales data or log files. In that case, the <code>UNION</code> function enables us to effortlessly append the new records to the existing dataset, ensuring that our analysis reflects the most up-to-date information. Additionally, it<a id="_idIndexMarker174" class="calibre6 pcalibre1 pcalibre"/> ensures data consistency and allows for seamless continuity in data analysis, enabling us to derive accurate insights and make informed decisions based on the complete and unified dataset.</p>
<p class="calibre3">The additional marketing data is available in the <code>MARKETING_ADDITIONAL</code> table. Let’s see how we can leverage Snowpark’s <code>UNION</code> function to include this additional data for processing:</p>
<pre class="source-code">
marketing_additional = session.table("MARKETING_ADDITIONAL")
marketing_additional.show()</pre> <p class="calibre3">The preceding code displays the data from the <code>MARKETING_ADDITIONAL</code> table:</p>
<div><div><img alt="Figure 3.16 – The MARKETING_ADDITIONAL table" src="img/B19923_03_16.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.16 – The MARKETING_ADDITIONAL table</p>
<p class="calibre3">With that, the table has been loaded into the DataFrame. Let’s look at the number of rows in our original and appended tables:</p>
<pre class="source-code">
print("No of rows in MARKETING_ADDITIONAL table: \
    ",marketing_additional.count())
print("No of rows in PURCHASE_HISTORY table: \
    ",final_combined.count())</pre> <p class="calibre3">This code displays the total number of rows in the <code>MARKETING_ADDITIONAL</code> and <code>PURCHASE_HISTORY</code> tables:</p>
<div><div><img alt="Figure 3.17 – Data row count" src="img/B19923_03_17.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.17 – Data row count</p>
<p class="calibre3">The <code>MARKETING_ ADDITIONAL</code> table contains 240 rows of new data that must be appended with the <code>PURCHASE_HISTORY</code> table, which contains 2,000 rows of data. Since the column names <a id="_idIndexMarker175" class="calibre6 pcalibre1 pcalibre"/>are identical, the data can be appended by using <code>union_by_name</code>:</p>
<pre class="source-code">
final_appended = final_combined.union_by_name(marketing_additional)</pre> <p class="calibre3">Now, the DataFrame contains the appended data. Let’s look at the number of rows in this DataFrame:</p>
<pre class="source-code">
print("No of rows in UPDATED table: ",final_appended.count())
final_appended.show()</pre> <p class="calibre3">The preceding code shows the final data that’s in the DataFrame:</p>
<div><div><img alt="Figure 3.18 – The MARKETING_FINAL table" src="img/B19923_03_18.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.18 – The MARKETING_FINAL table</p>
<p class="calibre3">The total count of the rows is 2,240. With that, the new data has been appended. Now, we will write this data into the <code>MARKETING_FINAL</code> table in Snowflake:</p>
<pre class="source-code">
final_appended.write.save_as_table("MARKETING_FINAL")</pre> <p class="calibre3">The <code>MARKETING_DATA</code> table is now available in Snowflake and can be consumed.</p>
<p class="callout-heading">The difference between union and union_by_name</p>
<p class="callout">Two methods are available for combining data: <strong class="source-inline1">union_by_name</strong> and <strong class="source-inline1">union</strong>. Both methods allow multiple datasets to be merged, but they differ in their approach and functionality.</p>
<p class="callout">The <strong class="source-inline1">union_by_name</strong> method in Snowpark Python is specifically designed to combine datasets by matching and merging columns based on their names. This method ensures that the columns with the same name from different datasets are merged, creating a unified dataset. It is beneficial when you have datasets with similar column structures and want to consolidate them while preserving the column names.</p>
<p class="callout">On the other hand, the <strong class="source-inline1">union</strong> method in Snowpark Python combines datasets by simply appending them vertically, regardless of column names or structures. This method concatenates the rows from one dataset with the rows from another, resulting in a single dataset with all the rows from both sources. The <strong class="source-inline1">union</strong> method is suitable for stacking datasets vertically without considering column names or matching structures. However, note that in certain cases, the column type matters, such as when casting a string column to a numeric value.</p>
<h1 id="_idParaDest-54" class="calibre5"><a id="_idTextAnchor054" class="calibre6 pcalibre1 pcalibre"/>Data grouping and analysis</h1>
<p class="calibre3">Now that the data is ready and has been transformed, the next step is to see how we can group data to understand important patterns and analyze it. In this section, we will aggregate this data and analyze it.</p>
<h2 id="_idParaDest-55" class="calibre7"><a id="_idTextAnchor055" class="calibre6 pcalibre1 pcalibre"/>Data grouping</h2>
<p class="calibre3">In data analysis, understanding<a id="_idIndexMarker176" class="calibre6 pcalibre1 pcalibre"/> patterns within datasets is crucial for gaining insights and making informed decisions. One powerful tool that aids in this process is the <code>group_by</code> function in Snowpark Python. This function allows us to group data based on specific criteria, enabling us to dissect and analyze the dataset in a structured manner.</p>
<p class="calibre3">By utilizing the <code>group_by</code> function, we can uncover valuable insights into how data is distributed and correlated across different categories or attributes. For example, we can group sales data by product category to analyze sales trends, or group customer data by demographics to understand buying behavior.</p>
<p class="calibre3">Furthermore, the <code>group_by</code> function can be combined with other data manipulation and visualization techniques to gain deeper insights. For instance, we can create visualizations such as bar charts or heatmaps to visually represent the aggregated data, making it easier to spot patterns and trends.</p>
<p class="calibre3">To facilitate grouping and conducting deeper analysis, we’ll utilize the <code>MARKETING_FINAL</code> table we established earlier:</p>
<pre class="source-code">
marketing_final = session.table("MARKETING_FINAL")</pre> <p class="calibre3">Here, we <a id="_idIndexMarker177" class="calibre6 pcalibre1 pcalibre"/>are loading the data from the <code>MARKETING_FINAL</code> table into the DataFrame. We will use this DataFrame to perform aggregations:</p>
<pre class="source-code">
marketing_final.group_by("EDUCATION").mean("INCOME").show()</pre> <p class="calibre3">This returns the average income by <code>EDUCATION</code>. People with PhDs have the highest average income, and people with primary education have the lowest average income:</p>
<div><div><img alt="Figure 3.19 – Average income by education" src="img/B19923_03_19.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.19 – Average income by education</p>
<p class="calibre3">Now, we can create an alias for the column:</p>
<pre class="source-code">
marketing_final.group_by("EDUCATION").agg(avg("INCOME").alias( \
    "Avg_Income")).show()</pre> <p class="calibre3">The average income is displayed as an alias – <code>AVG_INCOME</code>:</p>
<div><div><img alt="Figure 3.20 – The AVG_INCOME alias" src="img/B19923_03_20.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.20 – The AVG_INCOME alias</p>
<p class="calibre3">We can also achieve similar results by using the <code>function()</code> method to pass the respective operation from Snowpark functions:</p>
<pre class="source-code">
marketing_final.group_by("MARITAL_STATUS").function("sum")( \
    "Z_REVENUE").show()</pre> <p class="calibre3">This prints the following output:</p>
<div><div><img alt="Figure 3.21 – Sum of revenue by marital status" src="img/B19923_03_21.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.21 – Sum of revenue by marital status</p>
<p class="calibre3">Here, we can <a id="_idIndexMarker178" class="calibre6 pcalibre1 pcalibre"/>see that married customers generate the highest revenue. We can also use <code>agg()</code> to perform this particular aggregation. Let’s calculate the maximum income by marital status:</p>
<pre class="source-code">
marketing_final.group_by("MARITAL_STATUS").agg(max("INCOME")).show()</pre> <p class="calibre3">This generates the following output:</p>
<div><div><img alt="Figure 3.22 – Income by marital status" src="img/B19923_03_22.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.22 – Income by marital status</p>
<p class="calibre3">Here, we can see that customers who are together and married as a family have the maximum income to spend, and hence they generate the maximum revenue. Next, we will find the <a id="_idIndexMarker179" class="calibre6 pcalibre1 pcalibre"/>count of different types of graduates and their maximum income:</p>
<pre class="source-code">
marketing_final.group_by("EDUCATION").agg((col("*"), "count"), 
    max("INCOME")).show()</pre> <p class="calibre3">The preceding code produces the following output:</p>
<div><div><img alt="Figure 3.23 – Count of category" src="img/B19923_03_23.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.23 – Count of category</p>
<p class="calibre3">Here, we can see that <code>PhD</code> has a maximum income of <code>162397</code>, and that people with <code>Basic</code> income have the lowest maximum income – that is, <code>34445</code>.</p>
<p class="calibre3">We can also perform complex multi-level aggregations in Snowpark. Let’s find out how people with different educations and marital statuses spend:</p>
<pre class="source-code">
marketing_final.group_by(["EDUCATION","MARITAL_STATUS"]).agg(
    avg("INCOME").alias("Avg_Income"),
    sum("NUMSTOREPURCHASES").alias("Sum_Purchase")
).show()</pre> <p class="calibre3">Here’s the output:</p>
<div><div><img alt="Figure 3.24 – Multi-level aggregation" src="img/B19923_03_24.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.24 – Multi-level aggregation</p>
<p class="calibre3">Let’s determine<a id="_idIndexMarker180" class="calibre6 pcalibre1 pcalibre"/> the relationship between <code>EDUCATION</code>, <code>MARITAL_STATUS</code>, and <code>SUM_PURCHASE</code>. People who are graduates and married spend the most compared to single people. We can also sort the results by using the <code>sort()</code> function:</p>
<pre class="source-code">
aggregate_result = marketing_final.group_by(["EDUCATION","MARITAL_STATUS"]).agg(
    avg("INCOME").alias("Avg_Income"),
    sum("NUMSTOREPURCHASES").alias("Sum_Purchase")
)
aggregate_result.sort(
    col("EDUCATION").asc(), col("Sum_Purchase").asc()
).show()</pre> <p class="calibre3">Here’s the output:</p>
<div><div><img alt="Figure 3.25 – Sorted result" src="img/B19923_03_25.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.25 – Sorted result</p>
<p class="calibre3">Here, we are sorting the results in ascending order by purchase amount after the aggregation is <a id="_idIndexMarker181" class="calibre6 pcalibre1 pcalibre"/>completed. The following section will cover some standard data analysis that can be performed on this data.</p>
<h2 id="_idParaDest-56" class="calibre7"><a id="_idTextAnchor056" class="calibre6 pcalibre1 pcalibre"/>Data analysis</h2>
<p class="calibre3">In the previous <a id="_idIndexMarker182" class="calibre6 pcalibre1 pcalibre"/>sections, we delved into data exploration, transformation, and aggregation, where we learned about various techniques we can use to find out what our data is all about and how we can combine different datasets. Armed with a solid foundation of general dataset exploration, we are ready to dive deeper into data analysis using Snowpark Python.</p>
<p class="calibre3">This section focuses on leveraging the power of statistical functions, sampling techniques, pivoting operations, and converting data into a pandas DataFrame for advanced analysis. We will explore applying statistical functions to extract meaningful information from our data. Then, we will learn about different sampling techniques to work efficiently with large datasets. Additionally, we will discover how to reshape our data using pivoting operations to facilitate in-depth analysis.</p>
<p class="calibre3">Moreover, we will explore the seamless integration of Snowpark Python with pandas, a widely used data manipulation library. We will understand how to convert our Snowpark data into a pandas DataFrame, enabling us to leverage pandas’ extensive analytical and visualization capabilities.</p>
<p class="calibre3">The following section provides a glimpse into the capabilities of Snowpark Python for data analysis; we will delve deeper into each topic in the subsequent chapter. Here, we aim to provide a foundational understanding of the key concepts and techniques of analyzing data using Snowpark Python. In the next chapter, we will explore these topics in greater detail, unraveling the full potential of Snowpark Python for data analysis.</p>
<h3 class="calibre9">Describing the data</h3>
<p class="calibre3">The first step in our<a id="_idIndexMarker183" class="calibre6 pcalibre1 pcalibre"/> analysis is understanding how our data is distributed. The <code>describe()</code> function in pandas is a valuable tool that helps us gain insights into the statistical properties of our numerical data. When we apply <code>describe()</code> to a DataFrame, it computes various descriptive statistics, including the count, mean, standard deviation, minimum, quartiles, and maximum values for each numerical column.</p>
<p class="calibre3">This summary<a id="_idIndexMarker184" class="calibre6 pcalibre1 pcalibre"/> comprehensively overviews our data’s distribution and central tendencies. By examining these statistics, we can quickly identify key characteristics, such as the range of values, the spread of the data, and any potential outliers. This initial exploration sets the stage for more advanced analysis techniques and allows us to make informed decisions based on a solid understanding of our dataset’s distribution:</p>
<pre class="source-code">
marketing_final.describe().show()</pre> <p class="calibre3">The preceding code shows the data from the <code>MARKETING_FINAL</code> table:</p>
<div><div><img alt="Figure 3.26 – MARKETING_FINAL DataFrame" src="img/B19923_03_26.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.26 – MARKETING_FINAL DataFrame</p>
<p class="calibre3">The result shows the different columns and the data in the <code>MARKETING_FINAL</code> table.</p>
<h3 class="calibre9">Finding distinct data</h3>
<p class="calibre3">In Snowpark DataFrames, the <code>distinct()</code> function is crucial in identifying unique values within a <a id="_idIndexMarker185" class="calibre6 pcalibre1 pcalibre"/>column or set of columns. When applied to a Snowpark DataFrame, <code>distinct()</code> eliminates duplicate records, resulting in a new DataFrame that contains only distinct values. This function is particularly useful for dealing with large datasets or extracting unique records for analysis or data processing:</p>
<pre class="source-code">
marketing_final.distinct().count()</pre> <p class="calibre3">The preceding code shows the total count of the <code>MARKETING_FINAL</code> table:</p>
<div><div><img alt="Figure 3.27 – MARKETING_FINAL count" src="img/B19923_03_27.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.27 – MARKETING_FINAL count</p>
<p class="calibre3">In our case, the entire dataset is returned since we do not have any duplicate rows. <code>distinct()</code> preserves the original rows of the DataFrame and only filters out repeated values within the specified columns.</p>
<h3 class="calibre9">Dropping duplicates</h3>
<p class="calibre3"><code>drop_duplicates()</code> removes<a id="_idIndexMarker186" class="calibre6 pcalibre1 pcalibre"/> duplicate rows from a Snowpark DataFrame. It analyzes the entire row and compares it with other rows in the DataFrame. If a row is found to be an exact duplicate of another row, <code>drop_duplicates()</code> will remove it, keeping only the first occurrence. By default, this function considers all columns in the DataFrame for duplicate detection:</p>
<pre class="source-code">
marketing_final.select(["Education","Marital_Status"]).drop_duplicates().show()</pre> <p class="calibre3">This will display the following output:</p>
<div><div><img alt="Figure 3.28 – Marketing duplicates removed" src="img/B19923_03_28.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.28 – Marketing duplicates removed</p>
<p class="calibre3">Note that you can specify specific columns using the <code>subset</code> parameter to check for duplicates based on those columns alone. <code>drop_duplicates()</code> modifies the original DataFrame by removing duplicate rows.</p>
<h3 class="calibre9">Crosstab analysis</h3>
<p class="calibre3">Once we have identified <a id="_idIndexMarker187" class="calibre6 pcalibre1 pcalibre"/>the unique combinations of the <code>EDUCATION</code> and <code>MARITAL_STATUS</code> columns in our dataset, we might still be curious about how frequently each combination occurs. We can utilize the <code>crosstab</code> function to determine the occurrence of these unique combinations. By applying the <code>crosstab</code> function to our dataset, we can generate a cross-tabulation or contingency table that displays the frequency distribution of the unique combinations of <code>EDUCATION</code> and <code>MARITAL_STATUS</code>:</p>
<pre class="source-code">
marketing_final.stat.crosstab(col1="Education",col2="Marital_Status").show()</pre> <p class="calibre3">The preceding code shows the crosstab data in the DataFrame:</p>
<div><div><img alt="Figure 3.29 – Crosstab data" src="img/B19923_03_29.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.29 – Crosstab data</p>
<p class="calibre3">This table provides a comprehensive overview of how often each unique combination occurs in the dataset, allowing us to gain valuable insights into the relationships between these variables. The <code>crosstab</code> function aids us in understanding the distribution and occurrence patterns of the unique combinations, further enhancing our data analysis capabilities.</p>
<h4 class="calibre16">Pivot analysis</h4>
<p class="calibre3">Upon using <a id="_idIndexMarker188" class="calibre6 pcalibre1 pcalibre"/>the <code>crosstab</code> function to examine the unique combinations of the <code>EDUCATION</code> and <code>MARITAL_STATUS</code> columns in our dataset, we might encounter certain combinations with zero occurrences. We can construct a pivot table to gain a more comprehensive understanding of the data and further investigate the relationships between these variables.</p>
<p class="calibre3">Constructing a pivot table allows us to summarize and analyze the data more dynamically and flexibly. Unlike the <code>crosstab</code> function, which only provides the frequency distribution of unique combinations, a pivot table allows us to explore additional aggregate functions, such as sum, average, or maximum values. This enables us to delve deeper into the dataset and obtain meaningful insights:</p>
<pre class="source-code">
market_subset = marketing_final.select(
    "EDUCATION","MARITAL_STATUS","INCOME"
)
market_pivot = market_subset.pivot(
    "EDUCATION",
    ["Graduation","PhD","Master","Basic","2n Cycle"]
).sum("INCOME")
market_pivot.show()</pre> <p class="calibre3">The <a id="_idIndexMarker189" class="calibre6 pcalibre1 pcalibre"/>preceding code shows the data in the DataFrame:</p>
<div><div><img alt="Figure 3.30 – Pivot table" src="img/B19923_03_30.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.30 – Pivot table</p>
<p class="calibre3">By constructing a pivot table for the <code>EDUCATION</code> and <code>MARITAL_STATUS</code> columns, we can uncover the occurrence counts and various statistical measures or calculations associated with each combination. This expanded analysis provides a more comprehensive view of the data and allows for a more nuanced and detailed exploration.</p>
<p class="callout-heading">Note</p>
<p class="callout">When the <strong class="source-inline1">crosstab</strong> function displays zero occurrences for certain combinations of variables, it is essential to note that those combinations will be represented as <strong class="source-inline1">NULL</strong> values instead of zeros when constructing a pivot table.</p>
<p class="callout">Unlike <strong class="source-inline1">crosstab</strong>, which explicitly highlights zero counts for combinations absent in the dataset, a pivot table considers all possible combinations of the variables. Consequently, if a variety does not exist in the dataset, the corresponding cell in the pivot table will be represented as a <strong class="source-inline1">NULL</strong> value rather than a zero.</p>
<p class="callout">The presence of <strong class="source-inline1">NULL</strong> values in the pivot table highlights the absence of data for those particular combinations. Interpreting and handling these <strong class="source-inline1">NULL</strong> values appropriately during subsequent data analysis processes, such as data cleaning, imputation, or further statistical calculations, is essential.</p>
<h4 class="calibre16">Dropping missing values</h4>
<p class="calibre3">The <code>dropna()</code> function <a id="_idIndexMarker190" class="calibre6 pcalibre1 pcalibre"/>in pandas is a powerful tool for handling missing values in a DataFrame. In this case, we will be utilizing the <code>dropna()</code> functionality of Snowpark, which allows us to remove rows or columns that contain missing or <code>NULL</code> values, helping to ensure the integrity and accuracy of our data. The <code>dropna()</code> function offers several parameters that provide flexibility in controlling the operation’s behavior:</p>
<pre class="source-code">
market_pivot.dropna(how="all").show()</pre> <p class="calibre3">The preceding code shows the data with the applied filter from the DataFrame:</p>
<div><div><img alt="Figure 3.31 – Pivot table – dropna()" src="img/B19923_03_31.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.31 – Pivot table – dropna()</p>
<p class="calibre3">The <code>how</code> parameter determines the criteria that are used to drop rows or columns. It accepts the input as <code>any</code> and <code>all</code>: <code>any</code> drops the row or column if it contains any missing value, and <code>all</code> drops the row or column only if all its values are missing.</p>
<p class="calibre3">The <code>thresh</code> parameter specifies the minimum number of non-null values required to keep a row or column. The row or column is dropped if the <em class="italic">non-null values exceed</em> the threshold:</p>
<pre class="source-code">
market_pivot.dropna(thresh=5).show()</pre> <p class="calibre3">The <a id="_idIndexMarker191" class="calibre6 pcalibre1 pcalibre"/>preceding code shows the data with the applied filter from the DataFrame:</p>
<div><div><img alt="Figure 3.32 – Pivot threshold" src="img/B19923_03_32.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.32 – Pivot threshold</p>
<p class="calibre3">The <code>subset</code> parameter allows us to specify a subset of columns or rows for missing value removal. It accepts a list of column or row labels. By default, <code>dropna()</code> checks all columns or rows for missing values. However, with a subset, we can focus on specific columns or rows for the operation:</p>
<pre class="source-code">
market_pivot.dropna(subset="'Graduation'").show()</pre> <p class="calibre3">The preceding code drops any rows from the <code>market_pivot</code> DataFrame where the <code>Graduation</code> column has missing values and then displays the resulting DataFrame:</p>
<div><div><img alt="Figure 3.33 – Pivot subset" src="img/B19923_03_33.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.33 – Pivot subset</p>
<p class="calibre3">This shows the data with the applied filter from the DataFrame.</p>
<p class="callout-heading">Note</p>
<p class="callout">When working with pivot tables, it is crucial to handle <strong class="source-inline1">NULL</strong> values appropriately because they can impact the accuracy and reliability of subsequent analyses. This allows us to ensure that we have complete data for further analysis and calculations.</p>
<p class="callout">Having <strong class="source-inline1">NULL</strong> values in the pivot result can lead to incorrect interpretations or calculations since <strong class="source-inline1">NULL</strong> values can propagate through the analysis and affect subsequent aggregations, statistics, or visualizations. By replacing <strong class="source-inline1">NULL</strong> values with a specific value, such as 0, we can provide a meaningful representation of the data in the pivot table, allowing us to perform reliable analysis and make informed decisions based on complete information.</p>
<h3 class="calibre9">Filling missing values</h3>
<p class="calibre3">The <code>fillna()</code> function <a id="_idIndexMarker192" class="calibre6 pcalibre1 pcalibre"/>allows us to replace null values with specific values or apply various techniques for imputation. It also allows us to fill in the missing values in a DataFrame, ensuring that we maintain the integrity of the data structure. We can specify the values for filling nulls, such as a constant value, or values derived from statistical calculations such as mean, median, or mode. The <code>fillna()</code> function is useful when we’re treating null values while considering the data’s nature and the desired analysis:</p>
<pre class="source-code">
market_pivot.fillna(0).show()</pre> <p class="calibre3">The preceding code fills any null values in the <code>market_pivot</code> DataFrame with a value of <code>0</code> and then displays the resulting DataFrame:</p>
<div><div><img alt="Figure 3.34 – Missing values" src="img/B19923_03_34.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.34 – Missing values</p>
<p class="calibre3">This is a handy function that fills in missing values that need to be used for calculations.</p>
<h3 class="calibre9">Variable interaction</h3>
<p class="calibre3">The <code>corr()</code> function<a id="_idIndexMarker193" class="calibre6 pcalibre1 pcalibre"/> calculates the correlation coefficient, which measures the strength and direction of the linear relationship between two variables. It returns a value between -1 and 1, where -1 represents a perfect negative correlation, 1 illustrates a perfect positive correlation, and 0 indicates no linear correlation:</p>
<pre class="source-code">
marketing_final.stat.corr("INCOME", "NUMSTOREPURCHASES")</pre> <p class="calibre3">By executing this code, we obtain the correlation coefficient between the <code>INCOME</code> and <code>NUMSTOREPURCHASES</code> columns, providing insights into the potential relationship between income levels and the number of store purchases in the dataset:</p>
<div><div><img alt="Figure 3.35 – Correlation value" src="img/B19923_03_35.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.35 – Correlation value</p>
<p class="calibre3">The <code>cov()</code> function, on the other hand, calculates the covariance, which measures the degree of association between two variables without normalizing for scale:</p>
<pre class="source-code">
marketing_final.stat.cov("INCOME", "NUMSTOREPURCHASES")</pre> <p class="calibre3">Here’s the output:</p>
<div><div><img alt="Figure 3.36 – Covariance value" src="img/B19923_03_36.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.36 – Covariance value</p>
<p class="calibre3">The covariance between the <code>INCOME</code> and <code>NUMSTOREPURCHASES</code> columns helps us understand how changes in income levels correspond to changes in the number of store purchases in the dataset.</p>
<p class="callout-heading">Note</p>
<p class="callout">While both <strong class="source-inline1">corr()</strong> and <strong class="source-inline1">cov()</strong> help analyze relationships between variables, it is essential to note that in Snowpark Python, these functions only support the analysis of two variables at a time. This limitation means we can only calculate the correlation or covariance between two columns in a DataFrame, and not simultaneously across multiple variables. Additional techniques or functions may be required to overcome this limitation and perform correlation or covariance analysis for various variables.</p>
<h3 class="calibre9">Operating with pandas DataFrame</h3>
<p class="calibre3">Converting a <a id="_idIndexMarker194" class="calibre6 pcalibre1 pcalibre"/>Snowpark DataFrame into a pandas DataFrame is a valuable step that opens up a wide range of analysis capabilities. Snowpark <a id="_idIndexMarker195" class="calibre6 pcalibre1 pcalibre"/>provides seamless integration with pandas, allowing us to leverage pandas’ extensive data manipulation, analysis, and visualization functionalities. By converting a Snowpark DataFrame into a pandas DataFrame, we gain access to a vast ecosystem of tools and libraries that are designed explicitly for data analysis.</p>
<p class="calibre3">This transition enables us to leverage pandas’ rich functions and methods, such as statistical calculations, advanced filtering, grouping operations, and time series analysis. pandas also provide many visualization options, such as generating insightful plots, charts, and graphs that are more accessible, to visualize the data. With pandas, we can create meaningful visual representations of our data, facilitating the exploration of patterns, trends, and relationships. Additionally, working with pandas allows us to utilize its extensive community support and resources. The pandas library has a vast user community, making finding documentation, tutorials, and helpful discussions on specific data analysis tasks more accessible.</p>
<h3 class="calibre9">Limitations of pandas DataFrames</h3>
<p class="calibre3">Converting a <a id="_idIndexMarker196" class="calibre6 pcalibre1 pcalibre"/>Snowpark DataFrame into a pandas DataFrame can have its limitations, mainly when dealing with large datasets. The primary constraint is memory consumption as converting the entire dataset simultaneously may exceed available memory resources. This can hinder the analysis process and potentially lead to system crashes or performance issues.</p>
<p class="calibre3">However, these limitations can be mitigated by breaking the DataFrame into batches and sampling the data. We’ll discuss this shortly.</p>
<h3 class="calibre9">Data analysis using pandas</h3>
<p class="calibre3">Converting a<a id="_idIndexMarker197" class="calibre6 pcalibre1 pcalibre"/> Snowpark DataFrame into a pandas DataFrame <a id="_idIndexMarker198" class="calibre6 pcalibre1 pcalibre"/>empowers us to seamlessly transition from Snowpark’s powerful data processing capabilities to pandas’ feature-rich environment. This interoperability expands our analytical possibilities and enables us to perform advanced analysis and gain deeper insights from our data:</p>
<pre class="source-code">
pandas_df = marketing_final.to_pandas()
pandas_df.head()</pre> <p class="calibre3">The preceding code converts the <code>marketing_final</code> Snowpark DataFrame into a pandas <a id="_idIndexMarker199" class="calibre6 pcalibre1 pcalibre"/>DataFrame, allowing us to work with the data using pandas’ extensive data analysis and manipulation functionalities. It will print out the following output:</p>
<div><div><img alt="Figure 3.37 – The resulting pandas DataFrame" src="img/B19923_03_37.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.37 – The resulting pandas DataFrame</p>
<p class="calibre3">This shows <a id="_idIndexMarker200" class="calibre6 pcalibre1 pcalibre"/>the data that has been converted into the pandas DataFrame.</p>
<h4 class="calibre16">Correlation in pandas</h4>
<p class="calibre3">In pandas, calculating<a id="_idIndexMarker201" class="calibre6 pcalibre1 pcalibre"/> correlations among multiple columns is straightforward: it involves selecting the desired columns and applying the <code>corr()</code> function. It generates a correlation matrix, allowing us to examine the relationships between each pair of columns simultaneously:</p>
<pre class="source-code">
pandas_df[["INCOME","KIDHOME","RECENCY"]].corr()</pre> <p class="calibre3">The preceding code calculates the correlation matrix among the <code>INCOME</code>, <code>KIDHOME</code>, and <code>RECENCY</code> columns in the <code>pandas_df</code> pandas DataFrame. It computes the pairwise correlation coefficients between these columns, providing insights into their relationships. The output is as follows:</p>
<div><div><img alt="Figure 3.38 – Pandas correlation" src="img/B19923_03_38.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.38 – Pandas correlation</p>
<p class="calibre3">Next, we’ll look at frequency distribution.</p>
<h4 class="calibre16">Frequency distribution</h4>
<p class="calibre3">Calculating the <a id="_idIndexMarker202" class="calibre6 pcalibre1 pcalibre"/>frequency of values in a single column is simpler in pandas than in Snowpark Python. We can quickly obtain the frequency distribution in pandas by using the <code>value_counts()</code> function on a specific column. It returns a Series with unique values as indices and their corresponding counts as values. This concise method allows us to quickly understand the distribution and prevalence of each unique value in the column. On the other hand, in Snowpark Python, obtaining the frequency of values in a single column requires more steps and additional coding. We typically need to group the DataFrame by the desired column and then perform aggregation operations to count the occurrences of each unique value. Although this can be achieved in Snowpark Python, it involves more complex syntax and multiple transformations, making the process more cumbersome compared to pandas:</p>
<pre class="source-code">
frequency = pandas_df.EDUCATION.value_counts()
frequency</pre> <p class="calibre3"><code>frequency = pandas_df.EDUCATION.value_counts()</code> calculates the frequency distribution of unique values in the <code>EDUCATION</code> column of the <code>pandas_df</code> pandas DataFrame and assigns the result to the <code>frequency</code> variable. The output is as follows:</p>
<div><div><img alt="Figure 3.39 – Pandas data frequency" src="img/B19923_03_39.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.39 – Pandas data frequency</p>
<p class="calibre3">This shows the data frequency values in the pandas DataFrame.</p>
<h4 class="calibre16">Visualization in pandas</h4>
<p class="calibre3">Creating visualizations<a id="_idIndexMarker203" class="calibre6 pcalibre1 pcalibre"/> is made easy with pandas due to its seamless integration with popular visualization libraries such as Matplotlib and Seaborn. pandas provides a simple and intuitive interface to generate various visualizations, including line plots, bar charts, histograms, scatter plots, and more.</p>
<p class="calibre3">By <a id="_idIndexMarker204" class="calibre6 pcalibre1 pcalibre"/>leveraging pandas’ built-in plotting functions, we can effortlessly transform our data into insightful visual representations, enabling us to explore patterns, trends, and relationships within our dataset. With just a few lines of code, pandas <em class="italic">empowers</em> us to produce visually appealing and informative plots, facilitating the communication and interpretation of our data:</p>
<pre class="source-code">
frequency.plot(kind="barh",figsize=(8,3))</pre> <p class="calibre3">The preceding code creates a horizontal bar plot from the frequency distribution data stored in the <code>frequency</code> variable, where each unique value is represented by a bar with a length proportional to its count, and the plot has a customized size of 8 inches in width and 3 inches in height:</p>
<div><div><img alt="Figure 3.40 – Frequency plot" src="img/B19923_03_40.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.40 – Frequency plot</p>
<p class="calibre3">Similarly, we can generate a Hexbin plot by changing <code>kind</code> to <code>hexbin</code>:</p>
<pre class="source-code">
pandas_df.plot(
    kind="hexbin",
    x="INCOME",y="MNTGOLDPRODS",
    xlim=[0,100000],ylim=[0,100],
    figsize=(8,3)
)</pre> <p class="calibre3">The preceding code creates a Hexbin plot that visualizes the relationship between the <code>INCOME</code> and <code>MNTGOLDPRODS</code> columns in the <code>pandas_df</code> pandas DataFrame:</p>
<div><div><img alt="Figure 3.41 – Hexbin plot" src="img/B19923_03_41.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.41 – Hexbin plot</p>
<p class="calibre3">Here, the <em class="italic">X</em>-axis <a id="_idIndexMarker205" class="calibre6 pcalibre1 pcalibre"/>represents income values and the <em class="italic">Y</em>-axis represents the number of gold products. The plot is limited to X-axis limits of 0 to 100,000 and Y-axis limits of 0 to 100, with a customized size of 8 inches in width and 3 inches in height.</p>
<h4 class="calibre16">Breaking a DataFrame into batches</h4>
<p class="calibre3">The <code>to_pandas_batches()</code> function converts a Snowpark DataFrame into multiple smaller pandas <a id="_idIndexMarker206" class="calibre6 pcalibre1 pcalibre"/>DataFrames to be processed in batches. This approach reduces memory usage by converting the data into manageable portions, enabling efficient analysis of large datasets:</p>
<pre class="source-code">
for batch in marketing_final.to_pandas_batches(): print(batch.shape)</pre> <p class="calibre3">Here’s the output:</p>
<div><div><img alt="Figure 3.42 – DataFrame batches" src="img/B19923_03_42.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.42 – DataFrame batches</p>
<p class="calibre3">The preceding code demonstrates how to analyze a large dataset in batches using the <code>to_pandas_batches()</code> function in Snowpark Python. By iterating over the <code>to_pandas_batches()</code> function, the code processes the dataset in manageable batches rather than loading the entire dataset into memory at once. In each iteration, a batch of the dataset is converted into a pandas DataFrame and stored in the <code>batch</code> variable. The <code>print(batch.shape)</code> statement provides the shape of each batch, indicating<a id="_idIndexMarker207" class="calibre6 pcalibre1 pcalibre"/> the number of rows and columns in that specific batch.</p>
<p class="calibre3">Analyzing the dataset in batches allows for more efficient memory utilization, enabling us to process large datasets that might otherwise exceed available memory resources. This approach facilitates the analysis of large datasets by breaking them into smaller, more manageable portions, allowing for faster computations and reducing the risk of memory-related issues.</p>
<h4 class="calibre16">Sampling a DataFrame</h4>
<p class="calibre3">The <code>sample()</code> function <a id="_idIndexMarker208" class="calibre6 pcalibre1 pcalibre"/>in Snowpark Python allows us to retrieve a random subset of data from the Snowpark DataFrame. By specifying the desired fraction or number of rows, we can efficiently extract a representative sample for analysis. This technique reduces the memory footprint required for conversion and subsequent analysis while providing meaningful insights:</p>
<pre class="source-code">
sample_df = marketing_final.sample(frac=0.50)
sample_df.count()</pre> <p class="calibre3">Here’s the output:</p>
<div><div><img alt="Figure 3.43 – Sampling data" src="img/B19923_03_43.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.43 – Sampling data</p>
<p class="calibre3">The preceding code selects a random sample of 50% of the rows from the <code>marketing_final</code> DataFrame and assigns it to the <code>sample_df</code> DataFrame. The final count step produces slightly different output each time you run the code segment  as it involves sampling the original table. The subsequent <code>sample_df.count()</code> function calculates the count of non-null values in each column of the <code>sample_df</code> DataFrame.</p>
<p class="calibre3">By utilizing the methods we covered here in Snowpark Python, we can overcome the limitations of converting large Snowpark DataFrames into pandas DataFrames, allowing for practical analysis while efficiently managing memory resources. These functions provide flexibility and control, enabling us to work with sizable datasets in a manageable and optimized <a id="_idIndexMarker209" class="calibre6 pcalibre1 pcalibre"/>manner.</p>
<h1 id="_idParaDest-57" class="calibre5"><a id="_idTextAnchor057" class="calibre6 pcalibre1 pcalibre"/>Summary</h1>
<p class="calibre3">Snowpark provides different data processing capabilities and supports various techniques. It provides us with an easy and versatile way to ingest different structured and unstructured file formats, and Snowpark’s DataFrames support various data transformation and analysis operations. We covered various Snowpark session variables and different data operations that can be performed using Snowpark.</p>
<p class="calibre3">In the next chapter, we will cover how to build data engineering pipelines with Snowpark.</p>
</div>
</body></html>