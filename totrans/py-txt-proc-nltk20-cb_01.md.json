["```py\n>>> para = \"Hello World. It's good to see you. Thanks for buying this book.\"\n```", "```py\n>>> from nltk.tokenize import sent_tokenize\n>>> sent_tokenize(para)\n['Hello World.', \"It's good to see you.\", 'Thanks for buying this book.']\n```", "```py\n>>> import nltk.data\n>>> tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n>>> tokenizer.tokenize(para)\n['Hello World.', \"It's good to see you.\", 'Thanks for buying this book.']\n```", "```py\n>>> spanish_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n>>> spanish_tokenizer.tokenize('Hola amigo. Estoy bien.')\n```", "```py\n>>> from nltk.tokenize import word_tokenize\n>>> word_tokenize('Hello World.')\n['Hello', 'World', '.']\n```", "```py\n>>> from nltk.tokenize import TreebankWordTokenizer\n>>> tokenizer = TreebankWordTokenizer()\n>>> tokenizer.tokenize('Hello World.')\n['Hello', 'World', '.']\n```", "```py\n>>> word_tokenize(\"can't\")\n['ca', \"n't\"]\n```", "```py\n>>> from nltk.tokenize import PunktWordTokenizer\n>>> tokenizer = PunktWordTokenizer()\n>>> tokenizer.tokenize(\"Can't is a contraction.\")\n['Can', \"'t\", 'is', 'a', 'contraction.']\n```", "```py\n>>> from nltk.tokenize import WordPunctTokenizer\n>>> tokenizer = WordPunctTokenizer()\n>>> tokenizer.tokenize(\"Can't is a contraction.\")\n['Can', \"'\", 't', 'is', 'a', 'contraction', '.']\n```", "```py\n>>> from nltk.tokenize import RegexpTokenizer\n>>> tokenizer = RegexpTokenizer(\"[\\w']+\")\n>>> tokenizer.tokenize(\"Can't is a contraction.\")\n[\"Can't\", 'is', 'a', 'contraction']\n```", "```py\n>>> from nltk.tokenize import regexp_tokenize\n>>> regexp_tokenize(\"Can't is a contraction.\", \"[\\w']+\")\n[\"Can't\", 'is', 'a', 'contraction']\n```", "```py\n>>> tokenizer = RegexpTokenizer('\\s+', gaps=True)\n>>> tokenizer.tokenize(\"Can't is a contraction.\")\n [\"Can't\", 'is', 'a', 'contraction.']\n```", "```py\n>>> from nltk.corpus import stopwords\n>>> english_stops = set(stopwords.words('english'))\n>>> words = [\"Can't\", 'is', 'a', 'contraction']\n>>> [word for word in words if word not in english_stops]\n[\"Can't\", 'contraction']\n```", "```py\n>>> stopwords.fileids()\n['danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'portuguese', 'russian', 'spanish', 'swedish', 'turkish']\n```", "```py\n>>> from nltk.corpus import wordnet\n>>> syn = wordnet.synsets('cookbook')[0]\n>>> syn.name\n'cookbook.n.01'\n>>> syn.definition\n'a book of recipes and cooking directions'\n```", "```py\n>>> wordnet.synset('cookbook.n.01')\nSynset('cookbook.n.01')\n```", "```py\n>>> wordnet.synsets('cooking')[0].examples\n['cooking can be a great art', 'people are needed who have experience in cookery', 'he left the preparation of meals to his wife']\n```", "```py\n>>> syn.hypernyms()\n[Synset('reference_book.n.01')]\n>>> syn.hypernyms()[0].hyponyms()\n[Synset('encyclopedia.n.01'), Synset('directory.n.01'), Synset('source_book.n.01'), Synset('handbook.n.01'), Synset('instruction_book.n.01'), Synset('cookbook.n.01'), Synset('annual.n.02'), Synset('atlas.n.02'), Synset('wordbook.n.01')]\n>>> syn.root_hypernyms()\n[Synset('entity.n.01')]\n```", "```py\n>>> syn.hypernym_paths()\n[[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('artifact.n.01'), Synset('creation.n.02'), Synset('product.n.02'), Synset('work.n.02'), Synset('publication.n.01'), Synset('book.n.01'), Synset('reference_book.n.01'), Synset('cookbook.n.01')]]\n```", "```py\n>>> syn.pos\n'n'\n```", "```py\n>>> len(wordnet.synsets('great'))\n7\n>>> len(wordnet.synsets('great', pos='n'))\n1\n>>> len(wordnet.synsets('great', pos='a'))\n6\n```", "```py\n>>> from nltk.corpus import wordnet\n>>> syn = wordnet.synsets('cookbook')[0]\n>>> lemmas = syn.lemmas\n>>> len(lemmas)\n2\n>>> lemmas[0].name\n'cookbook'\n>>> lemmas[1].name\n'cookery_book'\n>>> lemmas[0].synset == lemmas[1].synset\nTrue\n```", "```py\n>>> [lemma.name for lemma in syn.lemmas]\n['cookbook', 'cookery_book']\n```", "```py\n>>> synonyms = []\n>>> for syn in wordnet.synsets('book'):\n...     for lemma in syn.lemmas:\n...         synonyms.append(lemma.name)\n>>> len(synonyms)\n38\n```", "```py\n>>> len(set(synonyms))\n25\n```", "```py\n>>> gn2 = wordnet.synset('good.n.02')\n>>> gn2.definition\n'moral excellence or admirableness'\n>>> evil = gn2.lemmas[0].antonyms()[0]\n>>> evil.name\n'evil'\n>>> evil.synset.definition\n'the quality of being morally wrong in principle or practice'\n>>> ga1 = wordnet.synset('good.a.01')\n>>> ga1.definition\n'having desirable or positive qualities especially those suitable for a thing specified'\n>>> bad = ga1.lemmas[0].antonyms()[0]\n>>> bad.name\n'bad'\n>>> bad.synset.definition\n'having undesirable or negative qualities'\n```", "```py\n>>> from nltk.corpus import wordnet\n>>> cb = wordnet.synset('cookbook.n.01')\n>>> ib = wordnet.synset('instruction_book.n.01')\n>>> cb.wup_similarity(ib)\n0.91666666666666663\n```", "```py\n>>> ref = cb.hypernyms()[0]\n>>> cb.shortest_path_distance(ref)\n1\n>>> ib.shortest_path_distance(ref)\n1\n>>> cb.shortest_path_distance(ib)\n2\n```", "```py\n>>> dog = wordnet.synsets('dog')[0]\n>>> dog.wup_similarity(cb)\n0.38095238095238093\n```", "```py\n>>> dog.common_hypernyms(cb)\n[Synset('object.n.01'), Synset('whole.n.02'), Synset('physical_entity.n.01'), Synset('entity.n.01')]\n```", "```py\n>>> cook = wordnet.synset('cook.v.01')\n>>> bake = wordnet.synset('bake.v.02')\n>>> cook.wup_similarity(bake)\n0.75\n```", "```py\n>>> cb.path_similarity(ib)\n0.33333333333333331\n>>> cb.path_similarity(dog)\n0.071428571428571425\n>>> cb.lch_similarity(ib)\n2.5389738710582761\n>>> cb.lch_similarity(dog)\n0.99852883011112725\n```", "```py\n>>> from nltk.corpus import webtext\n>>> from nltk.collocations import BigramCollocationFinder\n>>> from nltk.metrics import BigramAssocMeasures\n>>> words = [w.lower() for w in webtext.words('grail.txt')]\n>>> bcf = BigramCollocationFinder.from_words(words)\n>>> bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)\n[(\"'\", 's'), ('arthur', ':'), ('#', '1'), (\"'\", 't')]\n```", "```py\n>>> from nltk.corpus import stopwords\n>>> stopset = set(stopwords.words('english'))\n>>> filter_stops = lambda w: len(w) < 3 or w in stopset\n>>> bcf.apply_word_filter(filter_stops)\n>>> bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)\n[('black', 'knight'), ('clop', 'clop'), ('head', 'knight'), ('mumble', 'mumble')]\n```", "```py\n>>> from nltk.collocations import TrigramCollocationFinder\n>>> from nltk.metrics import TrigramAssocMeasures\n>>> words = [w.lower() for w in webtext.words('singles.txt')]\n>>> tcf = TrigramCollocationFinder.from_words(words)\n>>> tcf.apply_word_filter(filter_stops)\n>>> tcf.apply_freq_filter(3)\n>>> tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 4)\n[('long', 'term', 'relationship')]\n```"]