<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-US">
<head>
<meta charset="utf-8"/>
<meta name="generator" content="packt"/>
<title>13 Using Machine Learning without Premium or Embedded Capacity</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet2.css"/>

</head>
<body>

<section id="using-machine-learning-without-premium-or-embedded-capacity" class="level1 pkt" data-number="14">
<h1 data-number="14">13 Using Machine Learning without Premium or Embedded Capacity</h1>
<p>Thanks to the computing power now available via powerful laptops or through the cloud, you can enrich your analysis with insights from machine learning models easily and instantly. <strong>Power BI</strong> provides integrated tools (closely related to data flows) that allow you to use machine learning models developed by data scientists on Azure Machine Learning, models trained and deployed through <strong>Azure AutoML</strong>, or services exposed by cognitive services directly through a convenient graphical interface. The only drawback is that these tools (known as <strong>Advanced AI</strong>) are only enabled if you use an <strong>Embedded</strong> capacity, Premium capacity, or <strong>Premium Per User</strong> (<strong>PPU</strong>) license. Does this mean that a user using Power BI Desktop or simply the Power BI service with a Pro license cannot benefit from machine learning? Absolutely not, and we'll show you how to do it thanks to <strong>Python</strong> and <strong>R</strong>.</p>
<p>In this chapter, you will cover the following topics:</p>
<ul>
<li>Interacting with ML in Power BI with data flows</li>
<li>Using AutoML solutions</li>
<li>Embedding training code in Power Query</li>
<li>Using trained models in Power Query</li>
<li>Using trained models in Script Visuals</li>
<li>Calling web services in Power Query</li>
</ul>
<section id="technical-requirements-12" class="level2" data-number="14.1">
<h2 data-number="14.1">Technical requirements</h2>
<p>This chapter requires you to have a working internet connection and <strong>Power BI Desktop</strong> already installed on your machine. You must have properly configured the R and Python engines and IDEs as outlined in <em>Chapter 2</em>, <em>Configuring R with Power BI</em>, and <em>Chapter 3</em>, <em>Configuring Python with Power BI</em>.</p>
</section>
<section id="interacting-with-ml-in-power-bi-with-data-flows" class="level2" data-number="14.2">
<h2 data-number="14.2">Interacting with ML in Power BI with data flows</h2>
<p>You can access <strong>Advanced AI features</strong> directly through Power BI Desktop or you can access <strong>Advanced AI features for dataflow</strong> through data flows, which are easy-to-use tools for transforming big data into insights to be shown in dashboards. But, as you can imagine, both modes require the aforementioned licenses in the introduction.</p>
<p>These features are accessible from Power BI Desktop, in the <strong>Power Query Home</strong> ribbon:</p>
<figure>
<img src="../media/file334.png" alt="Figure 13.1 – AI insights in Power BI Desktop" /><figcaption aria-hidden="true">Figure 13.1 – AI insights in Power BI Desktop</figcaption>
</figure>
<p>The first two options (<strong>Text Analytics</strong> and <strong>Vision</strong>) you can see in <em>Figure 13.1</em> use <strong>Azure Cognitive Services</strong> behind the scenes, specifically Text Analytics services and Computer Vision services. Basically, thanks to these features in Power BI, you can now use <em>four functions</em> to enrich your data through the power of machine learning.</p>
<figure>
<img src="../media/file335.png" alt="Figure 13.2 – Cognitive Services functions in Power BI" /><figcaption aria-hidden="true">Figure 13.2 – Cognitive Services functions in Power BI</figcaption>
</figure>
<p>These are as follows:</p>
<ul>
<li><strong>TagImages</strong>. Analyzes images to generate tags based on what they contain</li>
<li><strong>ExtractKeyPhrases</strong>. Evaluates unstructured text, and for each text column, returns a list of key phrases</li>
<li><strong>DetectLanguage</strong>. Evaluates text input, and for each column, returns the language name and ISO identifier</li>
<li><strong>ScoreSentiment</strong>. Evaluates text input and returns a sentiment score for each document, ranging from 0 (negative) to 1 (positive)</li>
</ul>
<p>The other option of AI Insights is to be able to use models hosted in <strong>Azure Machine Learning</strong> as scoring functions in Power Query.</p>
<figure>
<img src="../media/file336.png" alt="Figure 13.3 – Azure Machine Learning functions in Power BI" /><figcaption aria-hidden="true">Figure 13.3 – Azure Machine Learning functions in Power BI</figcaption>
</figure>
<p>To top it off, the Advanced AI features also include the ability to create machine learning models on the fly via a GUI thanks to <strong>AutoML for dataflows</strong>.</p>
<p>AutoML solutions are very convenient, especially for the analyst who doesn’t have much experience with machine learning. You will see this in detail in the next section. Now you just need to know that in Power BI, you can generate three types of models: <strong>classifications</strong> (binary or multi-label), <strong>regressions</strong>, and <strong>time series forecasting</strong> (will be available soon).</p>
<figure>
<img src="../media/file337.png" alt="Figure 13.4 – AutoML for data flows in Power BI" /><figcaption aria-hidden="true">Figure 13.4 – AutoML for data flows in Power BI</figcaption>
</figure>
<p>Behind the scenes, there is the Azure AutoML service that allows you to do model training, but by leveraging data flows, you don't need to instantiate a machine learning workspace to run AutoML experiments.</p>
<p>A user who only has a Power BI Pro license cannot access these fantastic features directly from the Power BI GUI. However, thanks to the introduction of Python and R in Power BI, it is possible to use machine learning algorithms or external services that facilitate their implementation with just a few lines of code.</p>
<p>Is it really possible that just a few lines of code are enough to train a machine learning model? Where is the trick!? Let's explain the mystery.</p>
</section>
<section id="using-automl-solutions" class="level2" data-number="14.3">
<h2 data-number="14.3">Using AutoML solutions</h2>
<p>Writing code from scratch to do machine learning requires specific knowledge that a generic analyst using Power BI often doesn’t know. Therefore, we recommend the use of <strong>Automated Machine Learning</strong> (<strong>AutoML</strong>) processes from here on out for analysts who do not have a data science background. Does this mean that anyone can create an accurate machine learning model without knowing the theory behind this science simply by using AutoML algorithms? Absolutely not! The following applies:</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>An AutoML tool relieves the analyst of all those repetitive tasks typical of a machine learning process (hyperparameter tuning, model selection, and so on). Often, those that require specific theoretical knowledge on the part of the analyst (for example, missing value imputation, dataset balancing strategies, feature selection, and feature engineering) are left out of the automated steps. Therefore, not applying the appropriate transformations that only an expert knows to the dataset before starting an AutoML process leads to the generation of a baseline model that might be sufficiently accurate, but could not ensure product performance.</p>
</blockquote>
<p>You might think that AutoML tools are hated by data scientists. This is also a myth. Many of them use it as a quick and dirty prototyping tool and as an executor of repetitive steps while they focus on more critical tasks.</p>
<p>In this chapter, we will be satisfied with obtaining discrete performance models (sometimes very good if we are lucky enough to have a properly transformed training dataset), and so the outputs provided by AutoML solutions are more than fine.</p>
<p>Moreover, we will exclusively use AutoML solutions in Python as it is the most widely used language in most third-party machine learning platforms. The R language is a little less widely used than Python, but that doesn't mean the results you get in R are any less valid. On the contrary, as you may have noticed in previous chapters, some specialized packages of statistical functions that allow high flexibility of data manipulation exist only for R and not for Python.</p>
<p>Simply put, working with machine learning in Python to date allows models to be easily shared between popular platforms. Therefore, we suggest it for Power BI analysts, who perhaps prefer to delegate model creation to more specialized platforms and then import them in Power BI.</p>
<p>Let's see what AutoML tools we'll be using in the code for this chapter.</p>
<section id="pycaret" class="level3" data-number="14.3.1">
<h3 data-number="14.3.1">PyCaret</h3>
<p><strong>PyCaret</strong> (<a href="https://pycaret.org/">https://pycaret.org/</a>) is an open source, low-code machine learning library in Python that automates the cycle of machine learning experiments, democratizing access to these advanced techniques to business analysts and domain experts, and also helping data scientists to become more efficient and productive.</p>
<p>The types of problems that PyCaret can solve are as follows:</p>
<ul>
<li><strong>Classification</strong> (predicting a categorical target variable)</li>
<li><strong>Regression</strong> (predicting a numeric target variable)</li>
<li><strong>Clustering</strong> (grouping of observations into specific sets, each with its own properties)</li>
<li><strong>Anomaly Detection</strong> (the process of finding outliers in a dataset, and which are far fewer in number than with usual observations)</li>
<li><strong>Natural Language Processing</strong> (text transformations in useful features for classifications and regressions)</li>
<li><strong>Association Rules</strong> (a rule-based technique that finds important relationships between features according to probability theory)</li>
</ul>
<p>For more experienced users, PyCaret also provides convenient functions for model ensembling and model explanation.</p>
</section>
<section id="azure-automl" class="level3" data-number="14.3.2">
<h3 data-number="14.3.2">Azure AutoML</h3>
<p>Azure AutoML is a cloud-based service that can be used to automate the construction of machine learning pipelines for classification, regression, and forecasting tasks. Such pipelines involve a pre-processing phase of the dataset to better fit the training algorithms used in the next phase. After tuning and training multiple models, Azure AutoML selects the most accurate model among them, while also considering two other models resulting from the ensembling of the previously trained models.</p>
<p>The available tasks are as follows:</p>
<ul>
<li>Classification</li>
<li>Regression</li>
<li>Time series forecasting</li>
</ul>
<p>For more detailed coverage of this platform, please take a look at the references.</p>
</section>
<section id="remixautoml-for-r" class="level3" data-number="14.3.3">
<h3 data-number="14.3.3">RemixAutoML for R</h3>
<p>For the sake of completeness, we also suggest one of AutoML's solutions for R, which is <strong>RemixAutoML</strong> (<a href="https://github.com/AdrianAntico/RemixAutoML">https://github.com/AdrianAntico/RemixAutoML</a>). It is a set of functions that facilitate the use of many AutoML packages available for R (CatBoost, LightGBM, XGBoost, and H2O). In addition to giving the inexperienced analyst the ability to create machine learning models with a few lines of code thanks to AutoML, this library also contains very advanced features (for example, features for feature engineering and time series forecasting), often used by more experienced analysts.</p>
<p>Let's now look at the various ways to use machine learning models in Power BI.</p>
</section>
</section>
<section id="embedding-training-code-in-power-query" class="level2" data-number="14.4">
<h2 data-number="14.4">Embedding training code in Power Query</h2>
<p>One of the easiest solutions to train a machine learning model is to write the code needed to do so directly in Power Query, right after importing a dataset on which you will build the model.</p>
<p>Training a model on a fairly large dataset typically takes quite a bit of time to complete. As you embed the code in Power Query, it will run every time the data is refreshed, and this may result in a non-negligible delay in getting the data online. Hence, the following applies:</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>This solution is recommended when you are certain that the time required to complete the model training is acceptable.</p>
</blockquote>
<p>Let's now look at an example of how to write some training code using PyCaret.</p>
<section id="training-and-using-ml-models-with-pycaret" class="level3" data-number="14.4.1">
<h3 data-number="14.4.1">Training and using ML models with PyCaret</h3>
<p>Let's take the Titanic disaster dataset to train a machine learning model. Specifically, we want to create a model that predicts whether a passenger survives (the <code>Survived</code> column) based on their attributes described by the other features in the dataset. Evidently, this is a <em>binary classification</em> (does it survive? yes or no) that we can easily implement with PyCaret.</p>
<p>As PyCaret is constantly evolving and so are all the other dependent libraries, you need to also install the <strong>Visual C++ Build tools</strong> to build the necessary wheels and avoid errors such as <em>Failed building wheel for &lt;package&gt;</em>. Here are all the steps needed to install PyCaret correctly for Windows:</p>
<ol>
<li>Download the installer from <a href="https://visualstudio.microsoft.com/visual-cpp-build-tools/">https://visualstudio.microsoft.com/visual-cpp-build-tools/</a> and run it.</li>
<li>In the next window, check just the <strong>Desktop development with C++</strong> option.</li>
<li>You will be prompted to restart the machine. Do so.</li>
<li><p>Once your laptop has restarted, run your Anaconda Prompt and enter the following command to create the new <code>pycaret_env</code> environment:</p>
<pre><code>conda create --name pycaret_env python=3.7</code></pre></li>
<li><p>Enter the following command to switch to the new environment:</p>
<pre><code>conda activate pycaret_env</code></pre></li>
<li><p>Enter the following command to install the full version of PyCaret:</p>
<pre><code>pip install pycaret</code></pre></li>
</ol>
<p>Having done that, you can move on to see the training code of the model. The only little hiccup is the handling of missing values in the dataset (you already had a chance to analyze them in <em>Chapter 12</em>, <em>Adding Statistics Insights, Outliers, and Missing Values</em>). Unfortunately, PyCaret currently only supports handling missing values using the simplest methods, namely, imputation using the mean or median for numeric values, and imputation using the mode or a fixed string for categorical values. Since we want to show you how to impute missing values using the <em>K-Nearest Neighbors (KNN)</em> algorithm as anticipated in <em>Chapter 12</em>, <em>Adding Statistics Insights, Outliers, and Missing Values</em>, you will write a few more lines of code than usual.</p>
<p>The code used to impute the missing values via the KNN algorithm will be used in the first transformation step in Power BI, after importing the data from the Titanic dataset. You can find the code in the <code>01-impute-dataset-with-knn.py</code> file in the <code>Chapter13/Python</code> folder. It will take care first to operate a simple feature selection, eliminating those fields that could cause noise during the training of the model. After that, since the above imputation algorithm exposed by scikit-learn via the <strong>KNNImputer</strong> module does not handle categorical variables in the dataset, the code also takes care of doing the encoding of the categorical variables using the <strong>ordinal encoding</strong> technique (using a mapping of categories to integers) thanks to the <strong>OrdinalEncoder</strong> module of scikit-learn. At this point, the code imputes the missing values using the default distance measure, that is, a Euclidean distance measure that will not include NaN values when calculating the distance between members of the training dataset.</p>
<p>Once the imputed dataset is available, you can train the model with which you will then score a test dataset. You can find the code in the <code>02-train-model-with-pycaret.py</code> file in the <code>Chapter13/Python</code> folder. For convenience, you will use 95% of the imputed dataset to train the model, while the remaining 5% will be used as a test dataset. All this will go in a transformation step following the previous one used for the imputation of missing values in Power BI.</p>
<p>You'll split the dataset into training and test sets thanks to scikit-learn's <code>train_test_split()</code> method. After that, the model training is done very simply by calling PyCaret's <code>setup()</code> and <code>compare_models()</code> functions. In the <code>setup()</code> function, you will define which dataframe to use for training, the target variable (<code>Survived</code>), and which are the categorical and which are the ordinal variables. Moreover, it is necessary to use silent mode, otherwise user intervention would be required to validate the inferred types of the other variables. The <code>compare_models()</code> function trains and evaluates the performance of all models provided by PyCaret for classification using cross-validation. In addition to returning the best-performing model, this function also returns the performance values of each model returned by cross-validation.</p>
<figure>
<img src="../media/file338.png" alt="Figure 13.5 – Performance of all models" /><figcaption aria-hidden="true">Figure 13.5 – Performance of all models</figcaption>
</figure>
<p><em>Figure 13.5</em> shows several typical classification metrics for each model. One of the most widely used is the <strong>Area Under the ROC Curve</strong> (<strong>AUC</strong> or <strong>AUC-ROC</strong>) when the dataset is balanced (in other words, when there is a slight disproportion between the number of observations associated with one class of the target variable versus the other class). The following remark applies:</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>The <code>compare_models()</code> function doesn’t work in Power BI unless you disable the parallelism, passing <code>n_jobs=1</code> into the <code>setup()</code> function. If you don’t assign <code>1</code> to <code>n_job</code>, by default, PyCaret assigns the value -1 (maximum parallelism) and behind the scenes, the best model is computed correctly using multiple threads, but Power BI can't trace it back to the main process, so it gets stuck.</p>
</blockquote>
<p>With an AUC of about 0.85 (it can vary as the process is stochastic), the <strong>Random Forest Classifier</strong> appears to be the best model obtained by training 95% of the imputed dataset. Then you will use the newly trained model (<code>best_model</code>) to obtain predictions of the remaining 5% of the dataset via PyCaret's <code>predict_model()</code> function. You will get a result similar to this one:</p>
<figure>
<img src="../media/file339.png" alt="Figure 13.6 – Predictions of the test dataset" /><figcaption aria-hidden="true">Figure 13.6 – Predictions of the test dataset</figcaption>
</figure>
<p>As you can see, the result generated by scoring the dataset consists of two new columns for classification: the <code>Score</code> column represents an estimate of a measure, such as the probability with which the predicted classes are those reported in the <code>Label</code> column. If you are interested in having a true probability estimate, you have to <strong>calibrate</strong> the model (have a look at the references for more details). The trained model will also be saved as a PKL file for future reuse.</p>
<p>Let's look at how to implement what's explained up here in Power BI.</p>
</section>
<section id="using-pycaret-in-power-bi" class="level3" data-number="14.4.2">
<h3 data-number="14.4.2">Using PyCaret in Power BI</h3>
<p>First, make sure that Power BI Desktop references the new <code>pycaret_env</code> Python environment in <strong>Options</strong>. Then, follow these steps:</p>
<ol>
<li>Click on <strong>Get Data</strong>, search for <code>web</code>, select <strong>Web</strong>, and then click on <strong>Connect</strong>.</li>
<li>Enter the <code>http://bit.ly/titanic-dataset-csv</code> URL into the URL textbox and click <strong>OK</strong>.</li>
<li>You’ll see a preview of the data. Then, click <strong>Transform Data</strong>.</li>
<li>Click <strong>Transform</strong> on the ribbon and then <strong>Run Python script</strong>.</li>
<li>Enter the script you can find in the <code>01-impute-dataset-with-knn.py</code> file in the <code>Chapter13\Python</code> folder.</li>
<li>We are only interested in the data in the <code>df_imputed</code> dataframe. So, click on its <strong>Table</strong> value.</li>
<li>You'll see a preview of the dataset with all the missing values imputed.</li>
<li>Click <strong>Transform</strong> on the ribbon and then <strong>Run Python script</strong>.</li>
<li>Enter the script you can find in the <code>02-train-model-with-pycaret.py</code> file in the <code>Chapter13\Python</code> folder.</li>
<li>We are only interested in the data in the <code>predictions</code> dataframe. So, click on its <strong>Table</strong> value.</li>
<li>You'll see a preview of the dataset with the predictions generated by the model and the input dataset.</li>
<li>Click <strong>Home</strong> on the ribbon and then click <strong>Close &amp; Apply</strong>.</li>
</ol>
<p>Amazing! You have just trained a machine learning model and then scored a test dataset using a few lines of Python code thanks to PyCaret!</p>
<p>Let's now see how to proceed when the model is trained outside of Power BI.</p>
</section>
</section>
<section id="using-trained-models-in-power-query" class="level2" data-number="14.5">
<h2 data-number="14.5">Using trained models in Power Query</h2>
<p>As you already saw in <em>Chapter 4</em>, <em>Importing Unhandled Data Objects</em>, you used to share objects that were the result of complex, time-consuming processing (thus also a machine learning model) in a serialized format specific to the language you were using. At that point, it was very simple to deserialize the file and get the model ready to be used in Power Query to predict the target variable of new observations. However, it is important to know the dependencies needed by the scoring function (which gets the new observations as input and returns the predictions), since they are closely related to how the training of the model took place. For this reason, we recommend the following:</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>When you need to use a serialized machine learning model provided by a third party, make sure that whoever developed it also provides you with a working scoring function in order to avoid unnecessary headaches when predicting target values for unknown observations.</p>
</blockquote>
<p>If you think about it, the ability to serialize and deserialize a machine learning model could somehow solve the delay problem raised in the case of training the model directly in Power Query in the previous section. Suppose you run the embedded training code for the first time. Immediately afterward, you serialize the model and save it to disk. On the next refresh, instead of running the training code again, you can check whether the serialized model file exists in the expected path. If yes, you load the file, deserialize it, and use that model for the next steps; otherwise, you run the training code again.</p>
<p>Evidently, the aforementioned process involves the intervention of an expert who decides to eliminate the serialized file when the model does not perform very well because perhaps the business data has, in the meantime, changed substantially such that the previous model is not accurate any longer, like it was after the training undertaken with the past data (a process known as <strong>model drift</strong>; take a look at the references for more details).</p>
<p>We will not go into the implementation details of this solution, but we wanted to provide just a tip for a possible solution to the problem raised in the previous section.</p>
<p>Let’s now implement the scoring of a dataset of unseen observations in Power BI using an already trained PyCaret model.</p>
<section id="scoring-observations-in-power-query-using-a-trained-pycaret-model" class="level3" data-number="14.5.1">
<h3 data-number="14.5.1">Scoring observations in Power Query using a trained PyCaret model</h3>
<p>If you remember correctly, in the previous section, you saved the model trained in Power BI to a PKL file on disk. You also exported the test dataset calculated in the same code to CSV. In this session, you will directly use the serialized model, loading it with the <code>load_model()</code> function, and the test CSV dataset to be scored in Power BI. Since the model was trained using PyCaret, the scoring function to use is simply given by the <code>predict_model()</code> function. Keep in mind that the scoring function may be more complex when not using a framework such as PyCaret that simplifies things.</p>
<p>These are the steps to follow in Power BI:</p>
<ol>
<li>Click on <strong>Get Data</strong>, select <strong>Text/CSV</strong>, and then click on <strong>Connect</strong>:</li>
<li>Select the <code>titanic-test.csv</code> file in the <code>Chapter13</code> folder and click <strong>Open</strong>.</li>
<li>You’ll see a preview of the test data. Then, click <strong>Transform Data</strong>.</li>
<li>Click <strong>Transform</strong> on the ribbon and then <strong>Run Python script</strong>.</li>
<li>Enter the script you can find in the <code>03-score-dataset-using-pycaret-model.py</code> file in the <code>Chapter13\Python</code> folder.</li>
<li>We are only interested in the <code>predictions</code> dataframe. So, click on its <strong>Table</strong> value.</li>
<li>You'll see a preview of the test dataset with two additional columns – <code>Label</code> and <code>Score</code>.</li>
<li>Click <strong>Home</strong> on the ribbon and then click <strong>Close &amp; Apply</strong>.</li>
</ol>
<p>As you can see, this is the most immediate and common way to use a custom machine learning model for scoring in Power BI. In fact, we recommend the following:</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>It is convenient to do the training and, in general, to manage a machine learning model in platforms external to Power BI in order to decouple any development/tuning interventions of the model from the rest of the report.</p>
</blockquote>
<p>Let's now see instead how to use serialized machine learning models directly in <strong>Script Visuals</strong>.</p>
</section>
</section>
<section id="using-trained-models-in-script-visuals" class="level2" data-number="14.6">
<h2 data-number="14.6">Using trained models in Script Visuals</h2>
<p>As you learned in <em>Chapter 4</em>, <em>Importing Unhandled Data Objects</em>, thanks to object serialization and its string representation, you can import any object into a Python or R visual in the form of a dataframe of strings. Once said dataframe is available in the script visual, you can revert it to the original object via inverse deserialization transformations. Since you can do what we described with any object, evidently you can also do it for machine learning models already trained outside of Power BI.</p>
<p>When the appropriately deserialized model is available in the <em>Script Visual</em> session, new observations can be predicted immediately using the scoring function described in the previous section.</p>
<p>The first thing you might ask yourself is what's the point of being able to score a dataset inside a script visual when the data must always be available first in the Power BI data model in order to use it in the visual. In fact, if the data of the observations to use as input to the model is already found in the data model of Power BI, it could be better to apply batch scoring directly in Power Query and so use the predictions as a new column of the dataset. All of this is absolutely true. However, there are some cases in which it is convenient to use a script visual:</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>It is convenient to use a machine learning model in a script visual when you need to realize some simulation reports that allow you to explore the outputs of the model and vary the variables in play dynamically without having to refresh the entire report.</p>
</blockquote>
<p>In this case, we suggest using <strong>What-If parameters</strong> (<a href="https://bit.ly/power-bi-what-if">https://bit.ly/power-bi-what-if</a>) in Power BI for numeric features, which are dynamic and give the user a very usable report. For categorical variables, you can enter their content manually in Power BI using the <strong>Enter Data</strong> feature, which creates a disconnected table. What-If parameters create disconnected tables by default in the data model.</p>
<p>To properly understand this paragraph, make sure you understand the content of <em>Chapter 4</em>, <em>Importing Unhandled Data Objects</em>. Suppose you have to provide observations to a machine learning model that expects two variables as input – a numeric one and a categorical one. When passing the information to the script visual’s dataframe, in addition to the fields of the serialized model’s dataframe (<code>model_id</code>, <code>chunk_id</code>, and <code>model_str</code>) coming from Power Query, you will also have to assign the associated values to both the parameter slicers related to the two input variables. Since only one value is selected at a time for each parameter when slicing, the set of all the parameters form a tuple, which in our case is (<code>numeric_value</code>, <code>category_id</code>). This tuple will be replicated as many times as the rows of the string chunk dataframe (consisting of the columns <code>model_id</code>, <code>chunk_id</code>, and <code>model_str</code>), and concatenated to it in order to provide the final dataframe that will be available in the variable named <code>dataset</code> in the Script Visual session.</p>
<figure>
<img src="../media/file340.png" alt="Figure 13.7 – Deserializing the PKL file content into a Python visual" /><figcaption aria-hidden="true">Figure 13.7 – Deserializing the PKL file content into a Python visual</figcaption>
</figure>
<p>Once you have the dataset dataframe available in the script visual, you can apply the deserialization transformations just to the columns (<code>model_id</code>, <code>chunk_id</code>, and <code>model_str</code>), thereby obtaining the machine learning model ready to be used. Selecting instead just the columns (<code>number</code>, <code>category</code>) and applying the distinct function to all the rows of the resulting dataframe, you obtain back the tuple of parameters to provide by way of input to the deserialized model. You can therefore calculate the predicted value from the model, providing to it the tuple of parameters as input. You can then use the prediction in the graph to be shown in the script visual.</p>
<p>Let’s see in practice how to dynamically predict values from a machine learning model in a Python Script Visual.</p>
<section id="scoring-observations-in-a-script-visual-using-a-trained-pycaret-model" class="level3" data-number="14.6.1">
<h3 data-number="14.6.1">Scoring observations in a script visual using a trained PyCaret model</h3>
<p>The first thing you will do is to serialize properly the machine learning models (in our case, only one) contained in a dictionary in Power Query. In this way, a dataframe containing the representation in strings of every serialized model of the aforementioned dictionary is obtained. So, it is possible to select the model of interest through a slicer in the report and to therefore use the respective portion of a dataframe in a Python script visual, inside which it will be possible to deserialize the content of the dataframe and to thereby obtain the model to use for scoring.</p>
<p>So, let's proceed to develop our report in Power BI. Make sure that Power BI correctly references the <code>pycaret_env</code> environment in <strong>Options</strong>. Here are the steps to follow:</p>
<ol>
<li>Click on <strong>Get data</strong> and then <strong>More…</strong>. Start typing <code>script</code> into the search textbox and double-click on <strong>Python script</strong>. The Python Script editor will pop up.</li>
<li>Copy the content of the <code>04-serialize-ml-models-in-power-query.py</code> file into the <code>Chapter13\Python</code> folder. Then, paste it into the Python Script editor, changing the absolute path to the PKL file accordingly, and then click <strong>OK</strong>.</li>
<li><p>The navigator window will open, giving you the option to select which dataframe to import. Select both the <code>model_ids_df</code> dataframe (containing the model IDs) and the <code>models_df</code> one (containing the string representation of serialized models) and then click <strong>Load</strong>. Behind the scenes, a 1:* relationship is automatically created between the model IDs and the serialized model dataframe via the <code>model_id</code> fields.</p>
<figure>
<img src="../media/file341.png" alt="Figure 13.8 – Relationship automatically created between model tables" /><figcaption aria-hidden="true">Figure 13.8 – Relationship automatically created between model tables</figcaption>
</figure>
<p>This relationship allows you to filter the set of rows in the <strong>models_df</strong> table to be used in the Python visual, corresponding to the ID of the model you select via the slicer you’ll create in the next step.</p></li>
<li><p>Click on the slicer visual icon.</p>
<figure>
<img src="../media/file342.png" alt="Figure 13.9 – Selecting the slicer visual" /><figcaption aria-hidden="true">Figure 13.9 – Selecting the slicer visual</figcaption>
</figure>
<p>Then, click on the <strong>model_id</strong> measure of the <strong>model_ids_df</strong> table.</p>
<figure>
<img src="../media/file343.png" alt="Figure 13.10 – Click on the model_id measure to show it in the slicer" /><figcaption aria-hidden="true">Figure 13.10 – Click on the model_id measure to show it in the slicer</figcaption>
</figure></li>
<li><p>Click on the downward-pointing arrow at the top right of the slicer to select the <strong>Dropdown</strong> slicer type.</p>
<figure>
<img src="../media/file344.png" alt="Figure 13.11 – Selecting the Dropdown slicer type" /><figcaption aria-hidden="true">Figure 13.11 – Selecting the Dropdown slicer type</figcaption>
</figure></li>
<li><p>Resize the bottom edge of the slicer, click on its format options, switch on the <strong>Single select</strong> one, switch off <strong>Slicer header</strong>, and then add the title <code>Model IDs</code>.</p>
<figure>
<img src="../media/file345.png" alt="Figure 13.12 – Setting the slicer options" /><figcaption aria-hidden="true">Figure 13.12 – Setting the slicer options</figcaption>
</figure>
<p>Then, move it to the top center of the report.</p></li>
<li>You will now add a set of What-If parameters with their slicers associated with each variable to be passed as input to the model. Click on the <strong>Modeling</strong> tab on the ribbon and then on <strong>New parameter</strong>.</li>
<li><p>On the next dialog, enter <code>Pclass param</code> in the <strong>Name</strong> field, leave the data type as <code>Whole number</code>, enter <code>1</code> in the <strong>Minimum</strong> field, 3 in the <strong>Maximum</strong> field, leave <strong>Increment</strong> as 1, and enter 2 in the <strong>Default</strong> field.</p>
<figure>
<img src="../media/file346.png" alt="Figure 13.13 – Adding the What-if parameter for Pclass" /><figcaption aria-hidden="true">Figure 13.13 – Adding the What-if parameter for Pclass</figcaption>
</figure>
<p>Keep <strong>Add slicer to this page</strong> selected and then click <strong>OK</strong>.</p></li>
<li>Resize the bottom edge of the Pclass slicer. Then, click on its format options, switch off <strong>Slicer header</strong>, switch on <strong>Title</strong>, and enter <code>Passenger class</code> as the text. Then move it to the top left of your report.</li>
<li><p>Be sure to rename the <strong>Pclass</strong> value of <strong>Pclass param</strong> to the same name as the variable that represents it in the model, namely, <code>Pclass</code>.</p>
<figure>
<img src="../media/file347.png" alt="Figure 13.14 – Renaming the Pclass parameter value" /><figcaption aria-hidden="true">Figure 13.14 – Renaming the Pclass parameter value</figcaption>
</figure></li>
<li>As the variable <strong>Sex</strong> is categorical (F or M), you’ll create a disconnected table for it manually. So, click on the <strong>Home</strong> tab in the ribbon and click on <strong>Enter data</strong>.</li>
<li><p>Create the first column, <strong>Sex</strong>, of the table and add the values 0 and 1 to it. Then, create the new column, <strong>SexLabel</strong>, and enter <code>Female</code>, where <strong>Sex</strong> is <code>0</code>, and <code>Male</code>, where <strong>Sex</strong> is <code>1</code>.</p>
<figure>
<img src="../media/file348.png" alt="Figure 13.15 – Enter data manually for Sex" /><figcaption aria-hidden="true">Figure 13.15 – Enter data manually for Sex</figcaption>
</figure>
<p>Enter <code>Sex</code> as the table name and then click <strong>Load</strong>.</p></li>
<li><p>Let’s add a slicer for the <code>Sex</code> variable. Click on an empty spot in the report canvas first. Then, click the slicer visual to add a slicer to the report. Then, click on the <strong>SexLabel</strong> field and then on the <strong>Sex</strong> field (the order is important). Then, click on the downward-pointing arrow at the top right of the slicer to select the <strong>Dropdown</strong> slicer type. Also, click on its format options, switch on the <strong>Single select</strong> option in the <strong>Selection control</strong> group, switch off the <strong>Slicer header</strong> option, switch on <strong>Title</strong>, and then enter <code>Sex</code> as the text. Resize its bottom edge and move it under the <strong>Passenger class</strong> slicer.</p>
<figure>
<img src="../media/file349.png" alt="Figure 13.16 – A new dropdown slicer for Sex" /><figcaption aria-hidden="true">Figure 13.16 – A new dropdown slicer for Sex</figcaption>
</figure></li>
<li>Let’s create a new <code>What-if</code> parameter for the <code>Age</code> variable. Click on the <strong>Modeling</strong> tab on the ribbon and then on <strong>New parameter</strong>. On the next dialog, enter <code>Age param</code> in the <strong>Name</strong> field, leave the data type as <code>Whole number</code>, enter <code>1</code> in the <strong>Minimum</strong> field, <code>80</code> in the <strong>Maximum</strong> field, leave <strong>Increment</strong> as <code>1</code>, and enter <code>30</code> in the <strong>Default</strong> field. Keep <strong>Add slicer to this page</strong> selected and then click <strong>OK</strong>.</li>
<li>Resize the bottom edge of the Age slicer. Then, click on its format options, switch off <strong>Slicer header</strong>, switch on <strong>Title</strong>, and enter <code>Age</code> as the text. Then, move it to the top left of your report under the Sex slicer.</li>
<li><p>Be sure to rename the <strong>Age</strong> value of <strong>Age param</strong> to the same name as the variable that represents it in the model, namely, <code>Age</code>.</p>
<figure>
<img src="../media/file350.png" alt="Figure 13.17 – Renaming the Age parameter value" /><figcaption aria-hidden="true">Figure 13.17 – Renaming the Age parameter value</figcaption>
</figure></li>
<li>Let’s create a new What-if parameter for the <code>SibSp</code> variable. Click on the <strong>Modeling</strong> tab on the ribbon and then on <strong>New parameter</strong>. On the next dialog, enter <code>SibSp param</code> in the <strong>Name</strong> field, leave the data type as <code>Whole number</code>, enter <code>0</code> in the <strong>Minimum</strong> field, <code>8</code> in the <strong>Maximum</strong> field, leave <strong>Increment</strong> as <code>1</code>, and enter <code>0</code> in the <strong>Default</strong> field. Keep <strong>Add slicer to this page</strong> selected and then click <strong>OK</strong>.</li>
<li>Resize the bottom edge of the SibSp slicer. Then, click on its format options, switch off <strong>Slicer header</strong>, switch on <strong>Title</strong>, and enter <code>Siblings/spouse aboard</code> as the text. Then, move it to the top left of your report under the Age slicer.</li>
<li><p>Be sure to rename the <strong>SibSp</strong> value of <strong>SibSp param</strong> to the same name as the variable that represents it in the model, namely, <code>SibSp</code>.</p>
<figure>
<img src="../media/file351.png" alt="Figure 13.18 – Renaming the SibSp parameter value" /><figcaption aria-hidden="true">Figure 13.18 – Renaming the SibSp parameter value</figcaption>
</figure></li>
<li>Let’s create a new What-if parameter for the <code>Parch</code> variable. Click on the <strong>Modeling</strong> tab on the ribbon and then on <strong>New parameter</strong>. On the next dialog, enter <code>Parch param</code> in the <strong>Name</strong> field, and leave Whole number as the data type, enter <code>0</code> in the <strong>Minimum</strong> field, <code>6</code> in the <strong>Maximum</strong> field, leave <strong>Increment</strong> as <code>1</code>, and enter <code>0</code> in the <strong>Default</strong> field. Keep <strong>Add slicer to this page</strong> selected and then click <strong>OK</strong>.</li>
<li>Resize the bottom edge of the Parch slicer. Then, click on its format options, switch off <strong>Slicer header</strong>, switch on <strong>Title</strong>, and enter <code>Parents/children aboard</code> as the text. Then, move it to the top left of your report under the SibSp slicer.</li>
<li><p>Be sure to rename the <strong>Parch</strong> value of <strong>Parch param</strong> to the same name as the variable that represents it in the model, namely Parch:</p>
<figure>
<img src="../media/file352.png" alt="Figure 13.19 – Renaming the Parch parameter value" /><figcaption aria-hidden="true">Figure 13.19 – Renaming the Parch parameter value</figcaption>
</figure></li>
<li>Let’s create a new What-if parameter for the <code>Fare</code> variable. Click on the <strong>Modeling</strong> tab on the ribbon and then on <strong>New parameter</strong>. On the next dialog, enter <code>Fare param</code> in the <strong>Name</strong> field, select <code>Decimal number</code> as the data type, enter <code>0</code> in the <strong>Minimum</strong> field, <code>515</code> in the <strong>Maximum</strong>, field, enter <code>1</code> in the <strong>Increment</strong> field, and then enter <code>250</code> in the <strong>Default</strong> field. Keep <strong>Add slicer to this page</strong> selected and then click <strong>OK</strong>.</li>
<li>Resize the bottom edge of the Fare slicer. Then, click on its format options, switch off <strong>Slicer header</strong>, switch on <strong>Title</strong>, and enter <code>Fare</code> as the text. Then, move it to the top left of your report under the Parch slicer.</li>
<li><p>Be sure to rename the <strong>Fare</strong> value of <strong>Fare param</strong> to the same name as the variable that represents it in the model, namely, <code>Fare</code>.</p>
<figure>
<img src="../media/file353.png" alt="Figure 13.20 – Renaming the Fare parameter value" /><figcaption aria-hidden="true">Figure 13.20 – Renaming the Fare parameter value</figcaption>
</figure></li>
<li>Let’s add a slicer for the <code>Embarked</code> variable. As the <code>Embarked</code> variable is categorical (<code>0</code>, <code>1</code>, or <code>2</code>), you’ll create a disconnected table for it manually. So, click on the <strong>Home</strong> tab in the ribbon and then click on <strong>Enter Data</strong>.</li>
<li><p>Create the first column, <strong>Embarked</strong> (this name must correspond to that of the model’s variable), of the table and then add the values 0, 1, and 2 to it. Then, create a new column, <strong>EmbarkedLabel</strong>, and enter <code>Cherbourg</code>, <code>Queenstown</code>, and <code>Southampton</code>, corresponding to <code>0</code>, <code>1</code>, and <code>2</code>, respectively.</p>
<figure>
<img src="../media/file354.png" alt="Figure 13.21 – Entering data manually for Embarked" /><figcaption aria-hidden="true">Figure 13.21 – Entering data manually for Embarked</figcaption>
</figure>
<p>Enter <code>PortEmbarkation</code> as the table name and then click <strong>Load</strong>.</p></li>
<li>Let’s now add a slicer for the <code>Embarked</code> variable. Click on an empty spot in the report canvas first. Then, click on the slicer visual in order to add a slicer to the report. Click first on the <strong>EmbarkedLabel</strong> field and then on the <strong>Embarked</strong> one (the order is important). Then, click on the downward-pointing arrow at the top right of the slicer to select the <strong>Dropdown</strong> slicer type. Also, click on its format options. Switch on <strong>Single select</strong> in the <strong>Selection control</strong> group, switch off <strong>Slicer header</strong>, switch on <strong>Title</strong>, and enter <code>Port of embarkation</code> as the text. Resize its bottom edge and move it under the Passenger class slicer.</li>
<li>Now, click on an empty spot in the report canvas, then on <strong>Python Visual</strong> in the <strong>Visualizations</strong> field, and enable it when you’re prompted to do so. After that, move it to the center of your report.</li>
<li>Keeping it selected, click on all the three fields of the <code>models_df</code> table (<strong>chunk_id</strong>, <strong>model_id</strong>, and <strong>model_str</strong>).</li>
<li><p>Still keeping the Python visual selected, also click on all measures (the ones with a calculator icon) of all the parameters entered, and on the identifying fields of the categorical variables (the <strong>Embarked</strong> and <strong>Sex</strong> fields). Remember that the names of the measures must necessarily correspond to the names of the variables provided by the model for the report to work. You may need to enable the Python visual again after selecting the measures. You can do this by simply clicking on the yellow button labeled <strong>Select to enable</strong> in the Python visual. At the end of the selection, you should see all the names of the measures plus those of the fields of the <code>models_df</code> table inside the Python visual.</p>
<figure>
<img src="../media/file355.png" alt="Figure 13.22 – Selected measure names visible in the Python visual" /><figcaption aria-hidden="true">Figure 13.22 – Selected measure names visible in the Python visual</figcaption>
</figure></li>
<li>Now, click on the Python visual’s <strong>Format</strong> tab, expand the <strong>Title</strong> area, edit the text with the <strong>Prediction string</strong>, and increase the font size to 28 point.</li>
<li>Copy the code of the <code>05-deserialize-ml-models-in-python-visual.py</code> file into the <code>Chapter13\Python</code> folder and paste it into the Python visual script editor. Then, click on the <strong>Run script</strong> arrow icon in the top-right corner of the Python script editor. You’ll get a prediction (label and score) of whether a person described by the parameters you selected will survive.</li>
</ol>
<figure>
<img src="../media/file356.png" alt="Figure 13.23 – Complete prediction simulation report for the Titanic model" /><figcaption aria-hidden="true">Figure 13.23 – Complete prediction simulation report for the Titanic model</figcaption>
</figure>
<p>Keep in mind that the same report can be made using models trained in R with the same methodology followed here. In fact, for completeness, we added to the repository the <code>Chapter13\R</code> folder containing the scripts corresponding to those used in this section, which is useful for obtaining the same results you got here. In these scripts, we trained the model using a predefined algorithm (<strong>Random Forest</strong>) and used the recently introduced <strong>Tidymodels</strong> framework, which makes use of the Tidyverse principles. For further details, refer to the references.</p>
<p>Wow! You've managed to create a dynamic predictive report in Power BI, something few developers can do!</p>
<p>Let's now see how to invoke the AI and machine learning services exposed by Microsoft in Power BI even if you don't have a Premium capability, an Embedded capability, or a PPU license.</p>
</section>
</section>
<section id="calling-web-services-in-power-query" class="level2" data-number="14.7">
<h2 data-number="14.7">Calling web services in Power Query</h2>
<p>Another way to interact with machine learning models within Power Query is to invoke web services. As you may already know, a machine learning model can be used to carry out the scoring of many observations in batch mode using a trained model (process described previously). Another option for being able to interact with a machine learning model is to deploy it to a web service so that it can be invoked via REST APIs. You've already learned how to work with external APIs in <em>Chapter 9</em>, <em>Calling External APIs to Enrich Your Data</em>. The following applies to external APIs:</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>Remember that you can't consume external services via REST API calls from a Python or R visual because internet access is blocked for security reasons. Therefore, you can only consume these services in Power Query.</p>
</blockquote>
<p>As an example, in this section, you'll see how to invoke predictions from a released endpoint via <strong>Azure Machine Learning</strong> and how to use the services exposed by the <strong>Azure Text Analytics</strong> of cognitive services. You could use some M code in Power Query to access these services, although it's not exactly straightforward. Fortunately, SDKs are available, which make it much easier to access the exposed services. These SDKs are developed for Python, so our examples will be exclusively in Python.</p>
<p>Let's first look at how to interact with a model trained using Azure AutoML.</p>
<section id="using-azure-automl-models-in-power-query" class="level3" data-number="14.7.1">
<h3 data-number="14.7.1">Using Azure AutoML models in Power Query</h3>
<p>In this section, you'll first see how to train a machine learning model using the Azure AutoML GUI. After that, you will use the model released on an Azure container instance as a web service in Power BI.</p>
<section id="training-a-model-using-the-azure-automl-ui" class="level4" data-number="14.7.1.1">
<h4 data-number="14.7.1.1">Training a model using the Azure AutoML UI</h4>
<p>In order to use Azure AutoML, you must first have access to an Azure subscription (remember you can create a free account as shown at this link: <a href="https://bit.ly/azure-free-account">https://bit.ly/azure-free-account</a>). After that, you need to create an <strong>Azure Machine Learning Workspace</strong> to train models via the different technologies that Azure provides. You can do this by simply following the steps in the paragraph at this link: <a href="https://bit.ly/create-azureml-workspace">https://bit.ly/create-azureml-workspace</a>. As soon as the workspace has been allocated, you can log in to <strong>Azure Machine Learning Studio</strong>, an environment in which all the machine learning assets you'll be working with are best organized. Perform the following steps to log in to Azure ML Studio and to start an AutoML experiment:</p>
<ol>
<li>Go to <a href="https://ml.azure.com/">https://ml.azure.com/</a>.</li>
<li><p>You will be prompted to select an Azure subscription of yours and an Azure ML workspace to work on. Click on <strong>Get started</strong>. You will see something like this:</p>
<figure>
<img src="../media/file357.png" alt="Figure 13.24 – Azure ML Studio portal" /><figcaption aria-hidden="true">Figure 13.24 – Azure ML Studio portal</figcaption>
</figure></li>
<li><p>First, you need to import the dataset with which to train the model. You will use the same dataset obtained from the missing value imputation done in the previous sections. Click on <strong>Datasets</strong> in the menu on the left and then on <strong>Create dataset</strong>.</p>
<figure>
<img src="../media/file358.png" alt="Figure 13.25 – Creating a new dataset in Azure ML" /><figcaption aria-hidden="true">Figure 13.25 – Creating a new dataset in Azure ML</figcaption>
</figure></li>
<li><p>You’ll be prompted for the dataset name and type. Enter <code>titanic-imputed</code> as the name and leave <strong>Tabular</strong> as the type.</p>
<figure>
<img src="../media/file359.png" alt="Figure 13.26 – Selecting your dataset name and type" /><figcaption aria-hidden="true">Figure 13.26 – Selecting your dataset name and type</figcaption>
</figure>
<p>Then, click <strong>Next</strong>.</p></li>
<li>You have to upload the CSV file containing the Titanic disaster imputed data. So, click on <strong>Upload</strong>, then on <strong>Upload files</strong>, and finally select the <code>titanic-imputed.csv</code> file in the <code>Chapter13</code> folder via the <strong>Open file</strong> dialog. The file will be uploaded to the default Azure Blob storage (<code>workspaceblobstore</code>) created behind the scenes when instantiating a new Azure ML workspace. Click <strong>Next</strong>.</li>
<li>On the next page, you’ll get a preview of the dataset you’re importing. The engine automatically selects the best import options for you. But if there is something you’d like to change, you can do it on this page. In this case, everything is already OK, so click <strong>Next</strong>.</li>
<li>On the following page, you can change the imputed schema of the data you’re reading. In this case, leave the inferred type for each field, as the exported CSV file has numeric values with integer and decimal numbers. Then, click <strong>Next</strong>.</li>
<li>A recap page will be shown. So, just click <strong>Create</strong> and your dataset will be added to Azure ML.</li>
<li><p>Now you need to create a compute cluster to use for model training. Click on the <strong>Compute</strong> tab on the left menu, then click on <strong>Compute clusters</strong>, and finally click on <strong>New</strong>.</p>
<figure>
<img src="../media/file360.png" alt="Figure 13.27 – Creating a new compute cluster" /><figcaption aria-hidden="true">Figure 13.27 – Creating a new compute cluster</figcaption>
</figure></li>
<li>Then you can select your preferred location for the cluster and the virtual machine type and size to use for each cluster node. You can leave the default selection and click <strong>Next</strong>.</li>
<li>Choose a name for your cluster (in our case, <code>cluster</code>), the minimum number of nodes (keep it at 0 to make it turn off automatically when not used), and the maximum number of nodes (set it to 2). Then, click on <strong>Create</strong> to allocate your compute cluster.</li>
<li><p>Now, click on the <strong>Automated ML</strong> tab on the left menu and then on <strong>New Automated ML run</strong>.</p>
<figure>
<img src="../media/file361.png" alt="Figure 13.28 – Creating a new AutoML experiment" /><figcaption aria-hidden="true">Figure 13.28 – Creating a new AutoML experiment</figcaption>
</figure></li>
<li>On the next page, select the <strong>titanic-imputed</strong> dataset and click <strong>Next</strong>.</li>
<li><p>Now you can configure the run by entering the name of the new experiment (a virtual folder) that will contain all the AutoML runs (we used <code>titanic</code> for the name), the machine learning target column (<code>Survived</code>, the one to predict), and the compute cluster to use to execute the AutoML runs (the <code>cluster</code> one created previously).</p>
<figure>
<img src="../media/file362.png" alt="Figure 13.29 – Configuring your AutoML run" /><figcaption aria-hidden="true">Figure 13.29 – Configuring your AutoML run</figcaption>
</figure></li>
<li><p>You can then declare the machine learning experiment type you would like to run. In our case, it is a classification.</p>
<figure>
<img src="../media/file363.png" alt="Figure 13.30 – Setting up the AutoML task type" /><figcaption aria-hidden="true">Figure 13.30 – Setting up the AutoML task type</figcaption>
</figure></li>
<li>By clicking on <strong>View additional configuration settings</strong>, you can choose the primary metric to use in your experiment. Select the <strong>AUC weighted</strong> one and then click <strong>Save</strong>.</li>
<li>By clicking on <strong>View featurization settings</strong>, you can enable the auto-featurization option that AutoML provides. By default, it’s switched on. You can also choose the feature type for each column and the missing values impute strategy for each of them (the strategies are the naïve ones). Keep everything on <strong>Auto</strong> and then click <strong>Save</strong>.</li>
<li><p>You can now click <strong>Finish</strong> to start your AutoML experiment. You’ll be redirected to the <strong>Run</strong> page, and after a while, you‘ll see your experiment running.</p>
<figure>
<img src="../media/file364.png" alt="Figure 13.31 – Your AutoML experiment running" /><figcaption aria-hidden="true">Figure 13.31 – Your AutoML experiment running</figcaption>
</figure></li>
<li><p>After about 30 minutes, the experiment should end. Click on the <strong>Models</strong> tab on the <strong>AutoML Run</strong> page and you will see the training pipelines according to the best-performing ones.</p>
<figure>
<img src="../media/file365.png" alt="Figure 13.32 – Best performing pipelines found by AutoML" /><figcaption aria-hidden="true">Figure 13.32 – Best performing pipelines found by AutoML</figcaption>
</figure></li>
<li><p>For the best-performing model (<strong>VotingEnsemble</strong>), the <em>Explainability Dashboard</em> is also automatically generated, which you can access by clicking on <strong>View explanation</strong>. For further details on this, check out the references. Now, click on the <strong>VotingEnsemble</strong> link to go to the specific run that trained the model using that pipeline. Then, click on the <strong>Deploy</strong> button.</p>
<figure>
<img src="../media/file366.png" alt="Figure 13.33 – Deploying the best model to a web service" /><figcaption aria-hidden="true">Figure 13.33 – Deploying the best model to a web service</figcaption>
</figure>
<p>A new form will appear on the right asking for information about the model to deploy on a web service. Just give the model endpoint a name (<code>titanic-model</code>), select <strong>Azure Container Instance</strong> as the compute type, as this will not be a production environment, and activate the <strong>Enable authentication</strong> feature. In the case of a production environment, <em>Azure Kubernetes Services</em> (<em>AKS</em>) is the best choice. Then, click on <strong>Deploy</strong> and wait for the model to be deployed. When the <strong>Deploy status</strong> field changes to <strong>Succeeded</strong> in the <strong>Model</strong> summary, click on the <strong>titanic-model</strong> endpoint link.</p></li>
<li>The endpoint <strong>Details</strong> page contains all the information about the service. After at least 10 minutes of deployment, it must be in a healthy deployment state in order to be used. You can click on the <strong>Test</strong> tab to test your endpoint by providing it with test input data. The tab we are most interested in is <strong>Consume</strong>, in which the coordinates (REST endpoint URL and authentication key) are indicated to invoke the REST API from an external system. Also, you can directly copy the code snippet that allows you to consume the service in Python in the <strong>Consumption option</strong> section. We will use a variation of this code to score test observations in Power Query.</li>
</ol>
<p>At this point, the model is ready on a web service to be consumed via REST APIs. Let’s now use it in Power Query.</p>
</section>
<section id="consuming-an-azure-ml-deployed-model-in-power-bi" class="level4" data-number="14.7.1.2">
<h4 data-number="14.7.1.2">Consuming an Azure ML deployed model in Power BI</h4>
<p>Using a variation of the Python code proposed by the <strong>Endpoint Consume</strong> tab on Azure ML Studio, we created a function that accepts as parameters the endpoint URL, the API key, and a dataframe containing the observations to be scored. In the output, we get a dataframe containing just the <code>predicted_label</code> column with the scoring of each observation.</p>
<p>Here are the steps to get the predictions of a test dataset from a model trained via Azure AutoML and deployed as a web service on an Azure container instance:</p>
<ol>
<li>Click on <strong>Get Data</strong>, select <strong>Text/CSV</strong>, and then click on <strong>Connect</strong>:</li>
<li>Select the <code>titanic-test.csv</code> file in the <code>Chapter13</code> folder and then click <strong>Open</strong>.</li>
<li>You’ll see a preview of the test data. Click <strong>Transform Data</strong>.</li>
<li>Click <strong>Transform</strong> on the ribbon and then <strong>Run Python script</strong>.</li>
<li>Enter the script you can find in the <code>06-use-azure-ml-web-service-in-power-bi.py</code> file in the <code>Chapter13\Python</code> folder. Remember to edit the endpoint URL and key accordingly.</li>
<li>We are only interested in the <code>scored_df</code> dataframe. So, click on its <strong>Table</strong> value.</li>
<li>You'll see a preview of the test dataset with an additional column – <code>predicted_label</code>.</li>
<li>Click <strong>Home</strong> on the ribbon and then click <strong>Close &amp; Apply</strong>.</li>
</ol>
<p>Amazing! You were able to consume a model trained on Azure Machine Learning and deployed it to an Azure container instance without having either a PPU license or a Premium capacity.</p>
</section>
</section>
<section id="using-cognitive-services-in-power-query" class="level3" data-number="14.7.2">
<h3 data-number="14.7.2">Using cognitive services in Power Query</h3>
<p>The Azure cognitive services <strong>Text Analytics API</strong> is a service that provides Natural Language Processing (NLP) functions for text mining and analysis. Features made available include sentiment analysis, opinion mining, key phrase extraction, language detection, and named entity recognition.</p>
<p>First, you need to deploy the text analytics resource via the Azure portal.</p>
<section id="configuring-text-analytics" class="level4" data-number="14.7.2.1">
<h4 data-number="14.7.2.1">Configuring text analytics</h4>
<p>You must have an Azure subscription to use these services. Then you need to create a text analytics resource by following these steps:</p>
<ol>
<li>Go to the Azure portal (<a href="https://portal.azure.com/">https://portal.azure.com/</a>) and click on the <strong>Create a resource</strong> plus icon.</li>
<li>Start entering the text string in the search textbox and the <strong>Text Analytics</strong> option will appear. Click on it. Then, click on the <strong>Create</strong> button on the <strong>Text Analytics</strong> page.</li>
<li>Forget about selecting the <strong>Custom question answering</strong> option, and click on <strong>Continue to create your resource</strong> instead.</li>
<li>On the <strong>Create Text Analytics</strong> page, select the region you prefer and give a name to the service (in our case, <code>textanalytics555</code>; you can use a unique name of your choosing). Assign your resource to a new resource group with the name <code>text-analytics</code>. Then, select the <strong>Free F0</strong> pricing tier, check the <strong>Responsible AI Notice</strong> option, and click on <strong>Review + create</strong>. Then, click <strong>Create</strong> on the next page.</li>
<li>Once the deployment of the resource is complete, click on <strong>Go to resource</strong> and click on the <strong>API Key</strong> link on the next page. Then, take note of the details of <strong>KEY 1</strong> (you can click on the <strong>Copy to clipboard</strong> icon on its right) and the endpoint URL. You’ll use this information in your Python code.</li>
</ol>
<p>Your resource is now ready to be used via the dedicated Python SDK.</p>
</section>
<section id="configuring-your-python-environment-and-windows" class="level4" data-number="14.7.2.2">
<h4 data-number="14.7.2.2">Configuring your Python environment and Windows</h4>
<p>In order to consume text analytics, you must first install the <strong>Microsoft Azure Text Analytics Client Library for Python</strong> by following these steps:</p>
<ol>
<li>Open your Anaconda Prompt.</li>
<li>Switch to your PyCaret environment by entering this command: <code>conda activate pycaret_env</code>.</li>
<li>Install the client library by entering this command: <code>pip install azure-ai-textanalytics==5.1.0</code>.</li>
</ol>
<p>After that, to avoid the <em>ssl module in Python is not available</em> error in Windows 10, you need to add the <code>pycaret_env\Library\bin</code> path to the Windows environment <code>PATH</code> variable. These are the steps to do it:</p>
<ol>
<li>Click on the Windows <strong>Start</strong> icon in the bottom-left corner of your screen and start digitizing the x <code>variable</code>.string This will search for all Windows options that have the string variable in their name. Then, click on <strong>Edit environment variables for your account</strong>.</li>
<li>In the <strong>Environment Variables</strong> windows, double-click on the <strong>Path</strong> variable under <strong>User variables for &lt;your-user&gt;</strong> (if you installed Miniconda for all users, you need to change the <strong>Path</strong> system variable). In the <strong>Edit environment variable</strong> dialog that will appear, click on the <strong>New</strong> button and add the path <code>C:\&lt;your-path&gt;\miniconda3\envs\pycaret_env\Library\bin</code>. Then, click <strong>OK</strong> on all the windows.</li>
<li>You need to restart your system to make the change effective.</li>
</ol>
<p>You are now ready to be able to consume the service from Power Query.</p>
</section>
<section id="consuming-the-text-analytics-api-in-power-bi" class="level4" data-number="14.7.2.3">
<h4 data-number="14.7.2.3">Consuming the Text Analytics API in Power BI</h4>
<p>In this section, we will show you how to do sentiment analysis thanks to text analytics on the fictional company <em>Fabrikam Fiber</em>. It provides cable television and related services in the United States, allowing users to enter comments on their website. Your goal is to define for each comment the degree of positivity, neutrality, and negativity.</p>
<p>Basically, once a client has been authenticated with a URL and key, you can easily carry out a sentiment analysis thanks to the <code>analyze_sentiment()</code> method without knowing any NLP basis. Keep in mind that the free tier of text analytics is limited to processing only 10 documents (in our case, comments) at a time. For this reason, the code we built consists of grouping the comments in groups of 10 and invoking the API for each group.</p>
<p>Let’s see how to do that:</p>
<ol>
<li>Click on <strong>Get Data</strong>, select <strong>Text/CSV</strong>, and then click on <strong>Connect</strong>:</li>
<li>Select the <code>FabrikamComments.csv</code> file in the <code>Chapter13</code> folder and click <strong>Open</strong>.</li>
<li>You’ll see a preview of the Fabrikam dataset. Then, click <strong>Transform Data</strong>.</li>
<li>Click <strong>Transform</strong> on the ribbon, followed by <strong>Run Python script</strong>.</li>
<li>Enter the script you can find in the <code>07-use-text-analytics-in-power-bi.py</code> file in the <code>Chapter13\Python</code> folder. Remember to appropriately replace the service URL and its key that you previously copied into the Azure portal.</li>
<li>We are only interested in the <code>sentiment_enriched_df</code> dataframe. So, click on its <strong>Table</strong> value.</li>
<li><p>You'll see a preview of the Fabrikam dataset enriched with the following additional columns: <code>comment_sentiment</code>, <code>overall_positive_score</code>, <code>overall_neutral_score</code>, and <code>overall_negative_score</code>:</p>
<figure>
<img src="../media/file367.png" alt="Figure 13.34 – Additional sentiment analysis columns" /><figcaption aria-hidden="true">Figure 13.34 – Additional sentiment analysis columns</figcaption>
</figure></li>
<li>Click <strong>Home</strong> on the ribbon and then click <strong>Close &amp; Apply</strong>.</li>
</ol>
<p>That’s amazing! Thanks to the Python library <code>azure.ai.textanalytics</code>, you were able, in a few lines of code, to perform sentiment analysis in a very simple way. With the same ease, you can also use in Power BI the other services that cognitive services provide thanks to other Python SDKs.</p>
</section>
</section>
</section>
<section id="summary-12" class="level2" data-number="14.8">
<h2 data-number="14.8">Summary</h2>
<p>In this chapter, you learned how Power BI interacts with Microsoft AI services by default through data flow features. You also learned that by using AutoML platforms, you can get around the licensing problem (PPU license or Premium capacity) that Power BI needs to interface with Microsoft AI services. You used both an on-premises AutoML solution (PyCaret) and Azure AutoML on the cloud to solve a binary classification problem. You also used cognitive services' text analytics to do some sentiment analysis directly using a Python SDK.</p>
<p>You've learned that enrichment via AI mostly happens in Power Query (which allows access to the internet), although you've seen a case where it may be convenient to use a machine learning model directly within a Python visual.</p>
<p>In the next chapter, you will see how to implement data exploration of your dataset in Power BI.</p>
</section>
<section id="references-9" class="level2" data-number="14.9">
<h2 data-number="14.9">References</h2>
<p>For additional reading, check out the following books and articles:</p>
<ol>
<li><em>AI with data flows</em> (<a href="https://docs.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-machine-learning-integration">https://docs.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-machine-learning-integration</a>)</li>
<li><em>A Review of Azure Automated Machine Learning (AutoML)</em> (<a href="https://medium.com/microsoftazure/a-review-of-azure-automated-machine-learning-automl-5d2f98512406">https://medium.com/microsoftazure/a-review-of-azure-automated-machine-learning-automl-5d2f98512406</a>)</li>
<li><em>Automated Machine Learning with Microsoft Azure, by Dennis Michael Sawyers, Packt Publishing</em> (<a href="https://www.amazon.com/Automated-Machine-Learning-Microsoft-Azure/dp/1800565313/">https://www.amazon.com/Automated-Machine-Learning-Microsoft-Azure/dp/1800565313/</a>)</li>
<li><em>A Gentle Introduction to Concept Drift in Machine Learning</em> (<a href="https://machinelearningmastery.com/gentle-introduction-concept-drift-machine-learning/)">https://machinelearningmastery.com/gentle-introduction-concept-drift-machine-learning/)</a></li>
<li><em>Machine Learning Basics with the K-Nearest Neighbors Algorithm</em> (<a href="https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761">https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761</a>)</li>
<li><em>Python’s «predict_proba» Doesn’t Actually Predict Probabilities (and How to Fix It)</em> (<a href="https://towardsdatascience.com/pythons-predict-proba-doesn-t-actually-predict-probabilities-and-how-to-fix-it-f582c21d63fc">https://towardsdatascience.com/pythons-predict-proba-doesn-t-actually-predict-probabilities-and-how-to-fix-it-f582c21d63fc</a>)</li>
<li><em>Use the Interpretability Package to Explain ML Models and Predictions in Python</em> (<a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability-aml">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability-aml</a>)</li>
<li><em>Get Started with Tidymodels</em> (<a href="https://www.tidymodels.org/start/">https://www.tidymodels.org/start/ https://www.tidymodels.org/start/</a>)</li>
</ol>
</section>
</section>
</body>
</html>
