<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-US">
<head>
<meta charset="utf-8"/>
<meta name="generator" content="packt"/>
<title>8 Loading Large Datasets beyond the Available RAM in Power BI</title>



</head>
<body>


<h1 data-number="9">8 Loading Large Datasets beyond the Available RAM in Power BI</h1>
<p>In the previous chapter, you learned how to read from and write to a CSV file, both with Python and in R. When it comes to reading a file, whether you use Power BI's standard data import feature or the techniques shown in the previous chapter, the main limitation on the file size is due to the amount of RAM available on the machine where Power BI Desktop is installed.</p>
<p>In a data enrichment phase, it may be necessary to extract information needed for ongoing analysis from very large files (terabytes in size). In these cases, it is almost always necessary to implement big data solutions to be able to handle such masses of data. Very often, however, it is necessary to import files that are slightly larger than the available RAM, in order to extract aggregate information and then persist it in a small table for reuse during processing. In such cases, it's not necessary to bother with demanding big data platforms, but you can take advantage of the flexibility provided by specific packages that implement distributed computing systems in both Python and R, without having to resort to Apache Spark-based backends.</p>
<p>In this chapter, you will learn the following topics:</p>
<ul>
<li>A typical analytic scenario using large datasets</li>
<li>Importing large datasets with Python</li>
<li>Importing large datasets with R</li>
</ul>

<h2 data-number="9.1">Technical requirements</h2>
<p>This chapter requires you to have a working internet connection and <strong>Power BI Desktop</strong> already installed on your machine. You must have properly configured the R and Python engines and IDEs as outlined in <em>Chapter 2</em>, <em>Configuring R with Power BI</em>, and <em>Chapter 3</em>, <em>Configuring Python with Power BI</em>.</p>


<h2 data-number="9.2">A typical analytic scenario using large datasets</h2>
<p>One of the most frequent activities of a data scientist is to analyze a dataset of information relevant to a business scenario. The objective of the analysis is to be able to identify associations and relationships between variables, which help in some way to discover new measurable aspects of the business (insights) and can then be used to make it grow better. It may be the case that the available data may not be sufficient to determine strong associations between variables, because any additional variables may not be considered. In this case, attempting to obtain new data that is not generated by your business but enriches the context of your dataset (a <strong>data augmentation</strong> process) can improve the strength of the statistical associations between your variables. Being able to link, for example, weather forecast data to a dataset that reports the measurements of the water level of a dam certainly introduces significant variables to better interpret the phenomenon.</p>
<p>It is in this context that you often find yourself having to extract information from CSV files downloaded from external data sources. For example, imagine that you have been assigned the task of analyzing what factors influenced the profitability of a chain of shoe stores located in major airports in the United States from 1987 to 2012. The first thing that comes to your mind is that maybe flight delays are somehow related to people staying at the airport. If you have to spend time at the airport, you definitely have more time to visit the various stores there and therefore the chance of you making a purchase increases. So, how do you find data on the average airline delay at each US airport for each day of the year? Fortunately, the <em>Research and Innovative Technology Administration (RITA)</em>, <em>Bureau of Transportation Statistics</em>, provides aggregated statistics (<a href="http://bit.ly/airline-stats">http://bit.ly/airline-stats</a>) and raw data containing flight arrival and departure details for all commercial flights within the US (<a href="http://bit.ly/airline-stats-data">http://bit.ly/airline-stats-data</a>). A set of CSV files containing monthly airline data from 1987 to 2012 is already collected and zipped by Microsoft, and you can download it directly from this link: <a href="http://bit.ly/AirOnTime87to12">http://bit.ly/AirOnTime87to12</a>. If you would like more information about the fields in the files, please see the <code>AirOnTime87to12.dataset.description.txt</code> file.</p>
<p>The compressed file in question is about 4 GB large and, once unzipped, contains many CSV files with detailed data of flights made in the US across months ranging from 1987 to 2012, with a total size of 30 GB! Your goal is to calculate the average daily flight delay for each origin airport and to save the resulting dataset in a CSV file. How do you import all that data in Power BI!? Let's see how to do this in Python.</p>


<h2 data-number="9.3">Import large datasets with Python</h2>
<p>In <em>Chapter 3</em>, <em>Configuring Python with Power BI</em>, we suggested that you install some of the most commonly used data management packages in your environment, including NumPy, pandas, and scikit-learn. The biggest limitation of these packages is that <em>they cannot handle datasets larger than the RAM of the machine in which they are used</em>, thus they are not able to scale to more than one machine. To comply with this limitation, distributed systems based on <strong>Spark</strong>, which has become a dominant tool in the big data analysis landscape, are often used. However, the move to these systems forces developers to have to rethink already written code using an API called <strong>PySpark</strong>, born to use Spark objects with Python. This process is generally seen as causing delays in project delivery and causing frustration for developers, who master the libraries available for standard Python with much more confidence.</p>
<p>In response to the preceding issues, the community developed a new library for parallel computing in Python called <strong>Dask</strong> (<a href="https://dask.org/">https://dask.org/</a>). This library provides transparent ways for the developer to scale pandas, scikit-learn, and NumPy workflows more natively, with minimal rewriting. Dask APIs are pretty much a copy of most of the APIs of those modules, making the developer's job easier.</p>
<p>One of the advantages of Dask is that <em>you don't need to set up a cluster of machines to be able to manipulate 100+ GB datasets</em>. You just need one of today's laptops with a multi-core CPU and 32 GB of RAM to handle them with ease. Therefore, thanks to Dask, you can perform analysis of moderately large datasets on your own laptop, without incurring the overhead typical of clusters, such as the use of Docker images on various nodes and complex debugging.</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>Evidently, even the Spark team realized the <em>inconvenient</em> points born from the use of PySpark by developers accustomed to developing with pandas as a data wrangling module. For this reason, they have introduced <strong>Koalas</strong> (<a href="https://koalas.readthedocs.io">https://koalas.readthedocs.io</a>), which provides a pandas API on Apache Spark.</p>
</blockquote>
<p>The fact remains that Dask has many advantages over Spark in using a distributed system on your laptop only. For example, Spark is based on a <strong>Java Virtual Machine</strong> (<strong>JVM</strong>) infrastructure, and therefore requires Java and other components to be installed, while Dask is written in pure Python. In addition, the use of Dask enables a faster transition from your laptop to a cluster on the cloud, which can be easily allocated, for example, thanks to Azure. This is made possible thanks to the <strong>Dask Cloud Provider</strong> package (<a href="https://cloudprovider.dask.org/">https://cloudprovider.dask.org/</a>), which provides classes to create and manage temporary Dask clusters on various cloud platforms. Take a look at the references if you need to create a Dask cluster on Azure via Azure Spot Virtual Machines, or by leveraging Azure Machine Learning compute clusters (using, for example, NVIDIA RAPIDS for GPU-accelerated data science).</p>
<p>Coming back to the topic at hand, let's then see how to install Dask on your laptop.</p>

<h3 data-number="9.3.1">Installing Dask on your laptop</h3>
<p>You will install Dask on the <code>pbi_powerquery_env</code> environment, where the pandas and NumPy libraries are already installed. This time, it is not enough to simply run the <code>pip install dask</code> command, because this way youâ€™ll install only core parts of Dask. The correct way for Dask users is to install all components. In order to display the graph of the execution plan of a Dask operation, a <strong>Graphviz</strong> module must also be installed. To do all this, proceed as follows:</p>
<ol>
<li>Open your Anaconda prompt.</li>
<li><p>Switch to your <code>pbi_powerquery_env</code> environment, entering this command:</p>
<pre><code>conda activate pbi_powerquery_env</code></pre></li>
<li><p>Enter the following command to install all components of Dask:</p>
<pre><code>pip install &quot;dask[complete]&quot;</code></pre></li>
<li><p>Enter the following command to install all components of Graphviz:</p>
<pre><code>pip install graphviz</code></pre></li>
</ol>
<p>You also need to install Graphviz executables in Windows:</p>
<ol>
<li>Go to <a href="http://www.graphviz.org/download/">http://www.graphviz.org/download/</a>, and then download and install the stable Windows install package.</li>
<li>During the installation, choose to add Graphviz to the system path for the current user.</li>
</ol>
<p>Letâ€™s explore at this point the structures made available by Dask that allow you to extend common interfaces, such as those of NumPy, pandas, and Python iterators, to handle objects larger than the available memory.</p>


<h3 data-number="9.3.2">Creating a Dask DataFrame</h3>
<p>A <strong>Dask DataFrame</strong> is part of the Dask <em>Big Data</em> collections that allow pandas, NumPy, and Python iterators to scale easily. In addition to Dask DataFrames, which are the counterpart of pandas DataFrames, <strong>Dask Array</strong> (which mimics NumPy), <strong>Dask Bag</strong> (which mimics iterators), and <strong>Dask Delayed</strong> (which mimics loops) are also part of the collections. However, we will focus on Dask DataFrames, which will allow us to achieve the analysis goal set at the beginning of the chapter.</p>
<p>A Dask DataFrame is nothing more than a set of pandas DataFrames, which can reside on disk on a single machine or on multiple nodes in a cluster, allowing you to manage datasets larger than the RAM on your laptop. We assume that you have already unzipped the CSV files containing data on US flights from 1987 to 2012, as mentioned at the beginning of this chapter, in the <code>D:\&lt;your-path&gt;\AirOnTimeCSV</code> folder.</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>If you don't have a laptop with enough hardware resources (at least 16 GB of RAM), you should import a subset of CSV files first (such as 40â€“50 files) to test the scripts without having to wait excessively long execution times or incurring memory errors.</p>
</blockquote>
<p>Then, you can create your Dask DataFrame very easily in the following way:</p>
<pre><code>import os
import dask.dataframe as dd
main_path = os.path.join(&#39;D:\\&#39;, &#39;your-path&#39;, &#39;AirOnTimeCSV&#39;)
ddf = dd.read_csv(
Â Â Â Â os.path.join(main_path, &#39;airOT*.csv&#39;),Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  
Â Â Â Â encoding=&#39;latin-1&#39;,
Â Â Â Â usecols =[&#39;YEAR&#39;, &#39;MONTH&#39;, &#39;DAY_OF_MONTH&#39;, &#39;ORIGIN&#39;, &#39;DEP_DELAY&#39;]
)</code></pre>
<p>Note that the wildcard <code>*</code> character allows the capture of all CSV files contained in the folder that are in the form <code>airOTyyyymm.csv</code>, where <code>yyyy</code> indicates the year and <code>mm</code> the month number of the flight departure date. Moreover, the encoding of CSV files is declared as <code>latin-1</code>.</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>Nowhere is it indicated that the downloaded CSV files had such encoding. By simply trying to import them without declaring it (and therefore assuming <code>utf-8</code> by default), loading returns the following strange error: <code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 4: invalid continuation byte</code>. Searching the web, it is easy to find that this kind of error is encoding-related and that <code>latin-1</code> is the correct one.</p>
</blockquote>
<p>Also, it is a good idea to <em>specify only the columns of interest</em> via the <code>usecols</code> parameter in order to limit the columns to be read. This practice also guarantees to read only those columns that you are sure are not completely empty, thus avoiding reading errors due to the different inferred data type compared to the real one.</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>It may occur that some columns have a number of null values at the beginning and therefore Dask cannot impute the correct data type, as it uses a sample to do that. In this case, you should specifically declare the data type of those columns by using the <code>dtype</code> parameter.</p>
</blockquote>
<p>Now that the Dask DataFrame has been created, let's see how to use it to extract the information we need.</p>


<h3 data-number="9.3.3">Extracting information from a Dask DataFrame</h3>
<p>If you have run the code to read all CSV files, you will have noticed that the operation took very little time. Come to think of it, it's really too little time to read 30 GB of data. Could it be that the reading was not successful? The secret of most parallel computing frameworks is tied to this very feature: the read operation has not been physically performed but has been added to a possible queue of operations to be performed when you explicitly request to use the data. This concept is known as <strong>lazy evaluation</strong> or <strong>delayed computation</strong>.</p>
<p>Then, your Dask DataFrame can be used in subsequent operations as if it already contains the data. In our case, as the average airline delay at each US airport for each day of the year is needed, consider using the following code:</p>
<pre><code>mean_dep_delay_ddf = ddf.groupby([&#39;YEAR&#39;, &#39;MONTH&#39;, &#39;DAY_OF_MONTH&#39;, &#39;ORIGIN&#39;])[[&#39;DEP_DELAY&#39;]].mean().reset_index()</code></pre>
<p>If you are a little bit familiar with pandas DataFrame transformations, you will notice the use of the same methods for the Dask DataFrame. As with pandas, you have to use <em>double square brackets</em> to output a DataFrame; otherwise, you would get a series with a single pair of brackets (take a look here: <a href="http://bit.ly/pandas-subset-df">http://bit.ly/pandas-subset-df</a>). Moreover, in order to use the indexes created by the <code>groupby()</code> method as columns in a DataFrame, you need to reset them in the <code>reset_index()</code> one (for more details, go here: <a href="http://bit.ly/pandas-groupby">http://bit.ly/pandas-groupby</a>).</p>
<p>Executing this piece of code also takes very little time. As you can imagine, the averaging operation has been queued after the data reading operation in the transformation queue, which in this case is assigned to the <code>mean_dep_delay_ddf</code> DataFrame. If you want to get a better idea of what the execution plan of the transformations queued so far is, you can create a graph to represent it. For simplicity, we will implement the graph using only one CSV file as input. These are the necessary steps:</p>
<ol>
<li>Create a folder named <code>AirOnTimeCSVplot</code>.</li>
<li>Copy only the first CSV file you unzipped earlier into the previous folder.</li>
<li><p>Open a new Python script and run the following code on Visual Studio Code:</p>
<pre><code>import os
import dask.dataframe as dd
main_path = r&#39;C:\&lt;your-path&gt;\AirOnTimeCSVplot&#39;
ddf_1_month = dd.read_csv(
Â Â Â Â os.path.join(main_path, &#39;airOT*.csv&#39;),Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  
Â Â Â Â encoding=&#39;latin-1&#39;,
Â Â Â Â usecols =[&#39;YEAR&#39;, &#39;MONTH&#39;, &#39;DAY_OF_MONTH&#39;, &#39;ORIGIN&#39;, &#39;DEP_DELAY&#39;]
)
mean_dep_delay_1_month_ddf = ddf_1_month.groupby([&#39;YEAR&#39;, &#39;MONTH&#39;, &#39;DAY_OF_MONTH&#39;, &#39;ORIGIN&#39;])[[&#39;DEP_DELAY&#39;]].mean().reset_index()
mean_dep_delay_1_month_ddf.visualize(filename=&#39;mean_dep_delay_1_month_dask.pdf&#39;)</code></pre></li>
</ol>
<p>The <code>visualize()</code> method allows you to visualize the graph of the tasks estimated by the engine to realize the queued transformations, even before their execution. Specifically, the code will generate a PDF file in the same folder where the script you ran is located.</p>
<p>Starting at the bottom of <em>Figure 8.1</em>, which represents the contents of the newly generated PDF file, you can see that the single source CSV file is read by the <code>read-csv</code> function from two chunks split by the engine. The <code>dataframe-groupby-count-chunk</code> and <code>dataframe-groupby-sum-chunk</code> functions are applied to each chunk, since for each tuple defined by the keys of the grouping operation (<code>YEAR</code>, <code>MONTH</code>, <code>DAY_OF_MONTH</code>, and <code>ORIGIN</code>), we need to know the sum of the delay (<code>DEP_DELAY</code>) and the count of the occurrences to compute the average. After that, the results of the two <code>dataframe-groupby-sum-chunk</code> operations on the two chunks are aggregated by the <code>dataframe-groupby-sum-agg</code> function. Similarly, the <code>dataframe-groupby-count-agg</code> function aggregates the outputs of the two <code>dataframe-groupby-count-chunk</code> operations. Once the two DataFrames of sums and counts have been determined, the ratio between the two (that is, the mean) is calculated for each grouping using the <code>truediv</code> function. Finally, the <code>reset_index</code> function provides the desired DataFrame, the result of the distributed averaging operation.</p>
<p>If you think about it, the famous problem-solving strategy called <strong>Divide and Conquer</strong> (also known as <strong>Divide et Impera</strong>) has been adopted. It consists of dividing the original problem into smaller and generally simpler subproblems, each solved recursively. The solutions of the subproblems are then properly combined to obtain the solution of the original problem. If you've had any experience with the Hadoop world, the <strong>MapReduce</strong> paradigm follows the same philosophy, which was maintained and optimized later by Spark.</p>
<figure>
<img src="img/file216.png" alt="Figure 8.1 â€“ Computation underlying task graph" /><figcaption aria-hidden="true">Figure 8.1 â€“ Computation underlying task graph</figcaption>
</figure>
<p>That said, let's get back to the initial script. We had defined the <code>mean_dep_delay_ddf</code> Dask DataFrame. All the transformations needed to get the desired result have been queued. How do you tell Dask to actually proceed with all the computations? You have to explicitly ask for the result via the <code>compute()</code> method.</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>Pay attention to the fact that <code>compute()</code> returns <em>in-memory results</em>. It converts Dask DataFrames to pandas DataFrames, Dask arrays to NumPy arrays, and Dask bags to lists. It is a method that should only be invoked when you are certain that the result will fit comfortably in your machine's RAM. Otherwise, you can write your data on disk using specific methods, such as <code>to_textfiles()</code> or <code>to_parquet()</code>.</p>
</blockquote>
<p>If you instantiated a cluster, you could have decided to persist the calculated data not on disk but in memory, using the <code>persist()</code> method. The result would still have been a distributed Dask object but one that references pre-computed results distributed over the cluster's memory.</p>
<p>To add some magic to your script, you can use the <code>ProgressBar()</code> object, which allows you to monitor the progress of your computations. Unfortunately, it happens that even if the bar reaches 100%, it still takes some time for the workers to finish processing:</p>
<figure>
<img src="img/file217.png" alt="Figure 8.2 â€“ The progress bar on Visual Studio Code" /><figcaption aria-hidden="true">Figure 8.2 â€“ The progress bar on Visual Studio Code</figcaption>
</figure>
<p>So, don't give up! Before running the following line of code, keep in mind that processing takes approximately 10 minutes on a machine with 32 GB of RAM and a processor with 6 cores. The script is the following:</p>
<pre><code>with ProgressBar():
Â Â Â Â mean_dep_delay_df = mean_dep_delay_ddf.compute()</code></pre>
<p>Your laptop will commit all available logical processors to parallelize computations, which you will see in the <strong>Logical Processors</strong> view in <strong>Task Manager</strong>:</p>
<figure>
<img src="img/file218.png" alt="Figure 8.3 â€“ Parallel computations shown in Task Manager" /><figcaption aria-hidden="true">Figure 8.3 â€“ Parallel computations shown in Task Manager</figcaption>
</figure>
<p>When processing is complete, your pandas DataFrame will be available in memory, and you can view some rows of it as follows:</p>
<pre><code>mean_dep_delay_df.head(10)</code></pre>
<p>The output will be the following:</p>
<figure>
<img src="img/file219.png" alt="Figure 8.4 â€“ First 10 rows of the output pandas DataFrame" /><figcaption aria-hidden="true">Figure 8.4 â€“ First 10 rows of the output pandas DataFrame</figcaption>
</figure>
<p>You can find the complete script to create a Dask DataFrame and extract information from it in the <code>01-load-large-dataset-in-python.py</code> file in the <code>Chapter08\Python</code> folder.</p>
<p>Finally, you were able to get the dataset of a few thousand rows on average flight delays for each airport of origin and for each day of the year by processing a CSV set as large as 30 GB!</p>
<p>If you want to write the contents of a Dask DataFrame to a CSV file, without going through the generation of a pandas DataFrame, you can invoke directly the <code>to_csv()</code> method, passing the path of the file to generate as a parameter, as the following example shows:</p>
<pre><code>ddf.to_csv(r&#39;D\&lt;your-path&gt;\mean_dep_delay_df.csv&#39;)</code></pre>
<p>As you can well imagine, calling the <code>to_csv()</code> method triggers the actual execution of all queued transformations, just as it did with the <code>compute()</code> method, since youâ€™re forcing Dask to read the DataFrame content in order to write it to disk. For this reason, if you need to generate the CSV file and also create the pandas DataFrame to be used later in your code, you should not call first <code>to_csv()</code> from the Dask DataFrame and then <code>compute()</code> to get the pandas DataFrame, because you would force the actual execution of the transformations pipeline twice. In this case, it is convenient to first generate the pandas DataFrame with <code>compute()</code> and then generate a CSV or an Excel file from it, as you learned in <em>Chapter 7</em>, <em>Logging Data from Power BI to External Sources</em>.</p>
<p>Let's now try to apply what you have learned so far in Power BI.</p>


<h3 data-number="9.3.4">Importing a large dataset in Power BI with Python</h3>
<p>You learned that Power BI can import data using a Python script directly and that the data must be organized in a pandas DataFrame in order to be used in the data model. Therefore, what you are going to do is develop a script that uses the objects illustrated in the previous section in order to instantiate a pandas DataFrame containing the data of flight delay averages in the US. From this DataFrame, you will then generate a CSV file. Here are the steps required to implement it in Power BI:</p>
<ol>
<li>Open Power BI Desktop and make sure it is referencing your Python <code>pbi_powerquery_env</code> environment.</li>
<li>Click on <strong>Get data</strong>, start entering <code>Python</code>, and then double-click on <strong>Python script</strong>.</li>
<li>Copy the content of the <code>02-load-large-dataset-in-power-bi.py</code> file into the <code>Chapter08/Python</code> folder and paste the content into the Python script editor. Remember to edit the path to the CSV files (<code>main_path</code>) and destination path of the CSV file to be generated, and then click <strong>OK</strong>.</li>
<li><p>After about 11 minutes (using a 6-core laptop with 32 GB of RAM), you will see the <strong>Navigator</strong> window showing the aggregated data in the <code>mean_dep_delay_df</code> DataFrame:</p>
<figure>
<img src="img/file220.png" alt="Figure 8.5 â€“ The mean_dep_delay_df DataFrame loaded in Power BI" /><figcaption aria-hidden="true">Figure 8.5 â€“ The mean_dep_delay_df DataFrame loaded in Power BI</figcaption>
</figure>
<p>Select the DataFrame and click on <strong>Load</strong>. After that, a <strong>Load</strong> popup will appear, and it will remain active for about five more minutes:</p>
<figure>
<img src="img/file221.png" alt="Figure 8.6 â€“ Final phase of loading data into Power BI" /><figcaption aria-hidden="true">Figure 8.6 â€“ Final phase of loading data into Power BI</figcaption>
</figure>
<p>When it disappears, your data is loaded into the data model.Moreover, the script also generated the CSV file containing your aggregated data. Youâ€™ll find the <code>mean_dep_delay_df.csv</code> file, sized about 60 MB, in your folder:</p></li>
</ol>
<figure>
<img src="img/file222.png" alt="Figure 8.7 â€“ Content of the CSV file created from the Python script in Power BI" /><figcaption aria-hidden="true">Figure 8.7 â€“ Content of the CSV file created from the Python script in Power BI</figcaption>
</figure>
<p>Impressive! You just imported 30 GB of data to your Power BI Desktop, with just a few lines in Python. Congratulations!</p>
<p>Did you know you can also do this in R? Let's see how.</p>



<h2 data-number="9.4">Importing large datasets with R</h2>
<p>The same scalability limitations illustrated for Python packages used to manipulate data also exist for R packages in the <strong>Tidyverse</strong> ecosystem. Even in R, it is not possible to use a dataset larger than the available RAM on the machine. The first solution that is adopted in these cases is also to switch to Spark-based distributed systems, which provide the <strong>SparkR</strong> language. It provides a distributed implementation of the DataFrame you are used to in R, supporting filtering, aggregation, and selection operations as you do with the <code>dplyr</code> package. For those of us who are fans of the Tidyverse world, RStudio actively develops the <code>sparklyr</code> package, which allows you to use all the functionality of <code>dplyr</code>, even for distributed DataFrames. However, adopting Spark-based systems to process CSVs that together take up little more than the RAM you have available on your machine may be overkill because of the overhead introduced by all the Java infrastructure needed to run them.</p>
<p>It is in these cases that adopting the <code>disk.frame</code> package (<a href="https://github.com/xiaodaigh/disk.frame">https://github.com/xiaodaigh/disk.frame</a>) is the winning approach. What <code>disk.frame</code> allows you to do is to divide a dataset larger than RAM into chunks and store each chunk in a separate file within a folder. <strong>Chunk files</strong> are serializations of DataFrames smaller than the initial one in the compressed <code>fst</code> format, introduced by the <code>fst</code> package (<a href="https://github.com/fstpackage/fst">https://github.com/fstpackage/fst</a>). It is specifically designed to unleash the <em>speed and parallelism of solid-state disks</em> that are easily found in modern computers. DataFrames stored in the <code>fst</code> format have full random access, both column and row, and are therefore able to benefit from parallel computations.</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>If you want to benefit from the parallelism introduced by the <code>disk.frame</code> package, you must necessarily persist the source CSV files on an SSD disk. If you don't have it, and you use an HDD disk, your processing will take too long, and it won't be worthwhile.</p>
</blockquote>
<p>You can define the degree of parallelism with which <code>disk.frame</code> will work by indicating the number of parallel workers you want to use. For each worker, it will create an <strong>R session</strong> using the parallelism mechanism provided by the <code>future</code> package.</p>
<p>Let's see how to use <code>disk.frame</code> in detail.</p>

<h3 data-number="9.4.1">Installing disk.frame on your laptop</h3>
<p>First, you need to install the <code>disk.frame</code> package in your R engine:</p>
<ol>
<li>Open RStudio and make sure it is referencing your latest CRAN (Comprehensive R Archive Network) R engine (in our case, version 4.0.2).</li>
<li>Click on the console prompt and enter this command: <code>install.packages("disk.frame")</code>. Make sure you install the latest version of the package (in our case, version 0.5.0).</li>
</ol>
<p>At this point, you are ready to test it on RStudio.</p>


<h3 data-number="9.4.2">Creating a disk.frame instance</h3>
<p>In order to create a <code>disk.frame</code> instance (a disk-based DataFrame), you must first define the number of parallel workers you want to initialize. Obviously, the higher the number of workers, the faster the <code>disk.frame</code> instance will be created. But you cannot initialize any large number of workers, because the number of parallel threads you can use depends on the processor of your machine. Also, you can define a maximum limit to the amount of data you can exchange from one worker to another. For this reason, at the beginning of the script that uses <code>disk.frame</code>, you will find the following:</p>
<pre><code>library(dplyr)
library(disk.frame)
n_cores &lt;- future::availableCores() - 1
setup_disk.frame(workers = n_cores)
options(future.globals.maxSize = Inf)</code></pre>
<p>The <code>availableCores()</code> function calculates the number of logical processors available on your machine. Usually, itâ€™s best practice to leave one out for computationally demanding tasks, so that the machine doesn't become unresponsive. The <code>setup_disk.frame()</code> function creates the cluster of workers needed for the computations. Furthermore, there is no limit to the amount of data that can be exchanged between workers (<code>Inf</code> stands for <strong>infinite</strong>).</p>
<p>We assume that you have already unzipped the CSV files containing data on US flights from 1987 to 2012, as mentioned at the beginning of this chapter, in the <code>D:\&lt;your-path&gt;\AirOnTimeCSV</code> folder.</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>If you don't have a laptop with enough hardware resources, you should import a subset of CSV files first (such as 40â€“50 files) to test the scripts without having to wait excessively long execution times.</p>
</blockquote>
<p>You have to define a list of paths for each CSV file using the <code>list.files()</code> function, and then you can proceed with the creation of <code>disk.frame</code> using the <code>csv_to_disk.frame()</code> function:</p>
<pre><code>main_path &lt;- &#39;D:/&lt;your-path&gt;/AirOnTime&#39;
air_files &lt;- list.files( paste0(main_path, &#39;/AirOnTimeCSV&#39;), full.names=TRUE )
start_time &lt;- Sys.time()
dkf &lt;- csv_to_disk.frame(
Â Â Â Â infile = air_files,
Â Â Â Â outdir = paste0(main_path, &#39;/AirOnTime.df&#39;),
Â Â Â Â select = c(&#39;YEAR&#39;, &#39;MONTH&#39;, &#39;DAY_OF_MONTH&#39;, &#39;ORIGIN&#39;, &#39;DEP_DELAY&#39;)
)
end_time &lt;- Sys.time()
(create_dkf_exec_time &lt;- end_time - start_time)</code></pre>
<p>The creation of a <code>disk.frame</code> instance consists of two steps:</p>
<ol>
<li>The process creates as many chunks as there are CSV files for each allocated worker in temporary folders.</li>
<li>Chunks are aggregated and persisted in the output folder (defined by the <code>outdir</code> parameter) associated with <code>disk.frame</code>.</li>
</ol>
<p>In short, <code>disk.frame</code> is a folder, preferably with a <code>.df</code> extension, that contains the aggregated chunks (<code>.fst</code> files) generated from the source files. As you've already seen with Dask, it is a good idea to <em>specify only the columns of interest</em> via the <code>select</code> parameter in order to limit the columns to be read. This practice also guarantees to read only those columns you are sure are not completely empty, thus avoiding reading errors due to the different inferred data type compared to the real one.</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>It may occur that some columns have a number of null values at the beginning and therefore <code>disk.frame</code> cannot impute the correct data type, as it uses a sample to do that. In this case, you should specifically declare the data type of those columns using the <code>colClasses</code> parameter.</p>
</blockquote>
<p>Note that once a value is assigned to the <code>create_dkf_exec_time</code> variable, the round brackets print it on the screen, all in one line. The creation time is about 1 minute and 17 seconds on a machine with 6 cores and 32 GB, using 11 workers.</p>
<p>Once the creation of <code>disk.frame</code> is complete, the multiple R sessions that make up the cluster could remain active. In order to force their disconnection, it is useful to invoke the following command:</p>
<pre><code>future:::ClusterRegistry(&quot;stop&quot;)</code></pre>
<p>In this way, the machine's resources are completely released.</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>A correctly created <code>disk.frame</code> can always be referenced later using the <code>disk.frame()</code> function by passing it to the path of the <code>.df</code> folder, without having to create it again.</p>
</blockquote>
<p>You can find the complete script to create <code>disk.frame</code> in the <code>01-create-diskframe-in-r.R</code> file into the <code>Chapter08\R</code> folder.</p>
<p>Now let's see how to get information from the newly created <code>disk.frame</code>.</p>


<h3 data-number="9.4.3">Extracting information from disk.frame</h3>
<p>As we are interested in getting the average airline delay at each US airport for each day of the year, a grouping operation is required on the entire <code>disk.frame</code>. In order to optimize the execution time, in this case it is also necessary to instantiate a cluster of workers that will read in parallel the information from the newly created <code>disk.frame</code>. You can transform the <code>disk.frame</code> instance using the syntax you know from the <code>dplyr</code> package. Moreover, in order to optimize the machine resources, you will force the engine to read only the columns strictly necessary to the requested computation using the <code>srckeep()</code> function from the <code>disk.frame</code> package. In our case, it would be unnecessary to select the columns, since when creating the <code>disk.frame</code> instance we kept only those strictly necessary, but the following recommendation applies.</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>Itâ€™s good practice to always use the <code>srckeep()</code> function in aggregate data extraction scripts, because if the <code>disk.frame</code> instance had all the initial columns, the operation would cause a machine crash.</p>
</blockquote>
<p>The following code should be used to extract aggregated data:</p>
<pre><code>library(dplyr)
library(disk.frame)
n_cores &lt;- future::availableCores() - 1
setup_disk.frame(workers = n_cores)
options(future.globals.maxSize = Inf)
main_path &lt;- &#39; D:/&lt;your-path&gt;/AirOnTime&#39;
dkf &lt;- disk.frame( paste0(main_path, &#39;/AirOnTime.df&#39;) )
start_time &lt;- Sys.time()
mean_dep_delay_df &lt;- dkf %&gt;%
    srckeep(c(&quot;YEAR&quot;, &quot;MONTH&quot;, &quot;DAY_OF_MONTH&quot;, &quot;ORIGIN&quot;, &quot;DEP_DELAY&quot;)) %&gt;%
    group_by(YEAR, MONTH, DAY_OF_MONTH, ORIGIN) %&gt;%
    summarise(avg_delay = mean(DEP_DELAY, na.rm = TRUE)) %&gt;%
    collect()
end_time &lt;- Sys.time()
(aggregate_exec_time &lt;- end_time - start_time)
future:::ClusterRegistry(&quot;stop&quot;)</code></pre>
<p>Again, as already seen for Dask in Python, there are functions that collect all the queued transformations and trigger the execution of calculations. In the case of <code>disk.frame</code>, the function is <code>collect()</code>. The duration of this operation is about 20 minutes with a 32 GB machine with 6 cores, using 11 workers.</p>
<p>The end result is a tibble containing the desired air delay averages:</p>
<figure>
<img src="img/file223.png" alt="Figure 8.8 â€“ First rows of the tibble containing the delay averages" /><figcaption aria-hidden="true">Figure 8.8 â€“ First rows of the tibble containing the delay averages</figcaption>
</figure>
<p>Also in this case, you were able to get the dataset of a few thousand rows on average flight delays by processing a CSV set as large as 30 GB!</p>
<p>You can find the complete script to extract aggregated data from <code>disk.frame</code> in the <code>02-extract-info-from-diskframe-in-r.R</code> file in the <code>Chapter08\R</code> folder.</p>
<p>The following common-sense observation applies here as well.</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>If the results of previous time-consuming processing are to be reused often, it is best to <em>persist them on disk in a reusable format</em>. In this way, it will be avoidedyou can avoid to executinge again all the onerous processing of the queued transformations.</p>
</blockquote>
<p>A <code>disk.frame</code> instance doesnâ€™t have direct methods, like a Dask DataFrame, that allow its contents to be written to disk downstream of all queued transformations. Since the result is a tibble, which for all intents and purposes is a DataFrame, you can write it to disk following the directions you learned in <em>Chapter 7</em>, <em>Logging Data from Power BI to External Sources</em>.</p>
<p>So, let's see how you can apply what you've learned so far in Power BI.</p>


<h3 data-number="9.4.4">Importing a large dataset in Power BI with R</h3>
<p>The optimal solution to load a dataset larger than the available RAM with R via Power BI would be to be able to create the <code>disk.frame</code> instance directly via an R script in Power BI itself.</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>Unfortunately, the operation of creating a <code>disk.frame</code> instance from various CSV files <em>triggers an error</em> in Power BI during the second phase of the process (the aggregation of various chunks into the <code>disk.frame</code> folder).</p>
</blockquote>
<p>Therefore, in order to extract information from a <code>disk.frame</code> instance in Power BI, you must first create it in RStudio (or any other IDE or any automated script). Once the <code>disk.frame</code> instance has been created, it is then possible to use it in Power BI as follows:</p>
<ol>
<li>Make sure you have correctly created your <code>disk.frame</code> instance using RStudio by running the code contained in the <code>01-create-diskframe-in-r.R</code> file in the <code>Chapter08/R</code> folder.</li>
<li>Open Power BI Desktop and make sure it is referencing your latest CRAN R engine.</li>
<li>Click on <strong>Get data</strong>, start entering <code>script</code>, and then double-click on <strong>R script</strong>.</li>
<li>Copy the content of the <code>03-extract-info-from-diskframe-in-power-bi.R</code> file into the <code>Chapter08\R</code> folder and paste it into the R script editor. Remember to edit the destination path of the CSV file to be generated, and then click <strong>OK</strong>.</li>
<li><p>After about 30 minutes (using a 6-core laptop with 32 GB of RAM), you will see the <strong>Navigator</strong> window showing the aggregated data in the <code>mean_dep_delay_df</code> DataFrame:</p>
<figure>
<img src="img/file224.png" alt="Figure 8.9 â€“ The mean_dep_delay_df DataFrame loaded in Power BI" /><figcaption aria-hidden="true">Figure 8.9 â€“ The mean_dep_delay_df DataFrame loaded in Power BI</figcaption>
</figure>
<p>Select the DataFrame and click on <strong>Load</strong>. After that, a <strong>Load</strong> popup will appear, and it will remain active for about 10 more minutes:</p>
<figure>
<img src="img/file225.png" alt="Figure 8.10 â€“ Final phase of loading data into Power BI" /><figcaption aria-hidden="true">Figure 8.10 â€“ Final phase of loading data into Power BI</figcaption>
</figure>
<p>When it disappears, your data is loaded into the data model.Moreover, the script also generated the CSV file containing your aggregated data. Youâ€™ll find the <code>mean_dep_delay_df.csv</code> file, sized about 60 MB, in your folder:</p></li>
</ol>
<figure>
<img src="img/file226.png" alt="Figure 8.11 â€“ Content of the CSV file created from the R script in Power BI" /><figcaption aria-hidden="true">Figure 8.11 â€“ Content of the CSV file created from the R script in Power BI</figcaption>
</figure>
<p>Awesome! Would you ever have thought of importing 30 GB of data into your Power BI Desktop to extract information from it in R? Well, you just did it with just a few lines of code.</p>



<h2 data-number="9.5">Summary</h2>
<p>In this chapter, you learned how to import datasets larger than the RAM available to your laptop into Python and R. You've also applied this knowledge to import a set of CSV files that, in total, take up more than the available RAM on your machine into Power BI Desktop.</p>
<p>In the next chapter, you will learn how to enrich your data with external information extracted from web services made available to users.</p>


<h2 data-number="9.6">References</h2>
<p>For additional reading, check out the following books and articles:</p>
<ol>
<li><em>Create Dask clusters on Azure using Azure (Spot) VMs</em> (<a href="https://cloudprovider.dask.org/en/latest/azure.html">https://cloudprovider.dask.org/en/latest/azure.html</a>)</li>
<li><em>Accelerated Machine Learning at Scale with NVIDIA RAPIDS on Microsoft Azure ML with Dask</em> (<a href="https://github.com/drabastomek/GTC/tree/master/SJ_2020/workshop">https://github.com/drabastomek/GTC/tree/master/SJ_2020/workshop</a>)</li>
</ol>


</body>
</html>
