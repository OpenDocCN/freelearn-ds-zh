<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;Spark SQL"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Spark SQL</h1></div></div></div><p>Most businesses deal with quite a lot of structured data all the time. Even if there are too many ways to deal with unstructured data, many application use cases still have to have structured data. What is the major difference between processing structured data and unstructured data? If the data source is structured, and if the data processing engine knows the data structure a priori, the data processing engine can do lots of optimizations while processing the data, or even beforehand. This is very crucial when the data processing volume is huge and the turn around time is very critical.</p><p>Proliferation of enterprise data mandated the need to empower the end users to query and process the data in simple and easy to use application user interfaces. The RDBMS vendors united and the <span class="strong"><strong>structured query language</strong></span> (<span class="strong"><strong>SQL</strong></span>) came about as a solution for this. Over the last couple of decades, everyone who deals with data became familiar with SQL if not power users.</p><p>The large scale Internet applications in the social networking and microblogging spaces, to name a few, produced data beyond the consumption of many traditional data processing tools. When dealing with such a sea of data, picking and choosing the right piece of data from it became even more important. Spark was a highly prevalent data processing platform and its RDD-based programming model reduced the data processing effort as compared to the Hadoop MapReduce data processing framework. But, the initial versions of Spark's RDD-based programming model remained elusive on making end users, such as data scientists, data analysts, and business analysts from using Spark. The main reason why they could not make use of RDD based Spark programming model is because it requires some amount of functional programming. The solution to this problem is Spark SQL. Spark SQL is a library built on top of Spark. It exposes SQL interface and DataFrame API. DataFrame API supports programming languages Scala, Java, Python, and R.</p><p>If the structure of the data is known in advance, if the data fits into the model of rows and columns, it doesn't matter from where the data is coming and Spark SQL can use all of it together and process it as if all the data is coming from a single source. Moreover, the querying dialect is the ubiquitous SQL.</p><p>We will cover the following topics in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Structure of data</li><li class="listitem" style="list-style-type: disc">Spark SQL</li><li class="listitem" style="list-style-type: disc">Aggregations</li><li class="listitem" style="list-style-type: disc">Multi-datasource joins</li><li class="listitem" style="list-style-type: disc">Dataset</li><li class="listitem" style="list-style-type: disc">Data catalog</li></ul></div><div class="section" title="Understanding the structure of data"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec22"/>Understanding the structure of data</h1></div></div></div><p>The structure of the data that is being discussed here needs some more elucidation. What do we mean by the structure of the data? The data stored in RDBMS has a way of storing the data in rows/columns or records/fields. Every field has a data type and every record is a collection of fields of the same or different data types. In the early days of RDBMS, the data types of the fields were scalar and in the recent versions, it expanded to include collection data types or composite data types as well. So, whether the record contains scalar data types or composite data types, the important point to note here is that there is a structure to the underlying data. Many of the data processing paradigms have adopted the concept of mirroring the underlying data structure persisted in the RDBMS or other stores in memory to make the data processing easy.</p><p>In other words, if the data in an RDBMS table is being processed by a data processing application, if the same table-like data structure is available in memory to the programs, for the end users and programmers it is easy to model the applications and query the data. For example, suppose there is a set of comma-separated data items with a fixed number of values in each row having a specific data type for the values coming in the specific position in all the rows. This is a structured data file. It is a data table and is very similar to an RDBMS table.</p><p>In programming languages such as R, there is a data frame abstraction used to store data tables in memory. The Python data analysis library, named Pandas, also has a similar data frame concept. Once that data structure is available in memory, the programs can extract the data and slice and dice it as per the need. The same data table concept is extended to Spark, known as DataFrame, built on top of RDD, and there is a very comprehensive API known as DataFrame API in Spark SQL to process the data in the DataFrame. A SQL-like query language is also developed on top of the DataFrame abstraction, catering to the needs of the end users to query and process the underlying structured data. In summary, DataFrame is a distributed data table organized in rows and columns and having names for each column.</p><p>The Spark SQL library built on top of Spark is developed based on the research paper titled <span class="emphasis"><em>"Spark SQL: Relational Data Processing in Spark"</em></span>. It talks about four goals for Spark SQL and they are reproduced verbatim as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Support relational processing both within Spark programs (on native RDDs) and on external data sources using a programmer-friendly API</li><li class="listitem" style="list-style-type: disc">Provide high performance using established DBMS techniques</li><li class="listitem" style="list-style-type: disc">Easily support new data sources, including semi-structured data and external databases amenable to query federation</li><li class="listitem" style="list-style-type: disc">Enable extension with advanced analytics algorithms such as graph processing and machine learning</li></ul></div><p>DataFrame holds structured data, and it is distributed. It allows selection, filtering, and aggregation of data. Sounding very similar to RDD? The key difference between RDD and DataFrame is that DataFrame stores much more information about the structure of the data, such as the data types and names of the columns, than RDD. This allows the DataFrame to optimize the processing much more effectively than Spark transformations and Spark actions doing processing on RDD. The other most important aspect to mention here is that all the supported programming languages of Spark can be used to develop applications using the DataFrame API of Spark SQL. For all practical purposes, Spark SQL is a distributed SQL engine.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip23"/>Tip</h3><p>Those who have worked earlier to Spark 1.3 must be familiar with SchemaRDD, and the concept of DataFrame is exactly built on top of SchemaRDD with API-level compatibility.</p></div></div></div></div>
<div class="section" title="Why Spark SQL?"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec23"/>Why Spark SQL?</h1></div></div></div><p>There is no doubt that SQL is the lingua franca for doing data analysis and Spark SQL is the answer from the Spark family of toolsets to do data analysis. So, what does it provide? It provides the ability to run SQL on top of Spark. Whether the data is coming from CSV, Avro, Parquet, Hive, NoSQL data stores such as Cassandra, or even RDBMS, Spark SQL can be used to analyze data and mix in with Spark programs. Many of the data sources mentioned here are supported intrinsically by Spark SQL and many others are supported by external packages. The most important aspect to highlight here is the ability of Spark SQL to deal with data from a very wide variety of data sources. Once it is available as a DataFrame in Spark, Spark SQL can process data in a completely distributed way, combining the DataFrames coming from various data sources to process and query as if the entire dataset were coming from a single source.</p><p>In the previous chapter, the RDD was discussed in detail and introduced as the Spark programming model. Is the DataFrames API and the usage of SQL dialects in Spark SQL replacing RDD-based programming model? Definitely not! The RDD-based programming model is the generic and the basic data processing model in Spark. RDD-based programming requires the use of real programming techniques. The Spark transformations and Spark actions use a lot of functional programming constructs. Even though the amount of code that is required to be written in the RDD-based programming model is less compared to Hadoop MapReduce or any other paradigm, there is still a need to write some amount of functional code. This is a barrier for many data scientists, data analysts, and business analysts, who may perform major exploratory kinds of data analysis or do some prototyping with the data. Spark SQL completely removes those constraints. Simple and easy-to-use <span class="strong"><strong>domain specific language</strong></span> (<span class="strong"><strong>DSL</strong></span>) based methods to read and write data from data sources, SQL-like language to select, filter, and aggregate, and the capability to read data from a wide variety of data sources, make it easy for anybody who knows the data structure to use it.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note24"/>Note</h3><p>What is the best use case to use RDD and which is the best use case to use Spark SQL? The answer is very simple. If the data is structured, if it can be arranged in tables, and if each column can be given a name, then use Spark SQL. This doesn't mean that the RDD and DataFrame are two disparate entities. They interoperate very well. Conversions from RDD to DataFrame and vice versa are very much possible. Many of the Spark transformations and Spark actions that are typically applied on RDDs can also be applied on DataFrames.</p></div></div><p>Typically, during the application design phase, business analysts generally do lots of analysis with the application data using SQL, and that is fed to the application requirements and testing artifacts. While designing big data applications, the same thing is needed, and in such situations, apart from business analysts, data scientists will also be there in the team. In a Hadoop-based ecosystem, Hive is used extensively for data analysis with big data. Now Spark SQL brings that capability to any platform with support for a whole lot of data sources. If there is a standalone Spark installation on commodity hardware, lots of these kinds of activities can be done to analyze the data. A basic Spark installation deployed in standalone mode on commodity hardware is enough to play around with a whole lot of data.</p><p>The SQL-on-Hadoop strategy has introduced many applications, such as Hive and Impala to name a few, providing a SQL-like interface to the underlying big data stored in the <span class="strong"><strong>Hadoop Distributed File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>). Where does Spark SQL fit in that space? Before jumping into that, it is a good idea to touch upon Hive and Impala. Hive is a MapReduce-based data warehousing technology and, because of the use of MapReduce to process queries, Hive queries require lots of I/O operations before completing a query. Impala came up with a brilliant solution by doing the in-memory processing while making use of the Hive meta store that describes the data. Spark SQL uses SQLContext to do all the operations with data. But it can also use HiveContext, which is much more feature rich and advanced than SQLContext. HiveContext can do all that SQLContext can do and, on top of that, it can read from Hive meta store and tables, and can access Hive user-defined functions as well. The only requirement to use HiveContext is obviously that there should be an already existing Hive setup readily available. In this way, Spark SQL can easily co-exist with Hive.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note25"/>Note</h3><p>From Spark 2.0 onwards, SparkSession is the new starting point for Spark SQL-based applications, which are a combination of SQLContext and HiveContext while supporting backward compatibility with SQLContext and HiveContext.</p></div></div><p>Spark SQL can process the data from Hive tables faster than Hive using its Hive Query Language. Another very interesting feature of Spark SQL is that it can read data from different versions of Hive, which is a great feature enabling data source consolidation for data processing.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note26"/>Note</h3><p>The library that exposes Spark SQL and DataFrame API provides interfaces that can be accessed through JDBC/ODBC. This opens up a whole new world of data analysis. For example, a <span class="strong"><strong>business intelligence</strong></span> (<span class="strong"><strong>BI</strong></span>) tool that connects to data sources using JDBC/ODBC can use a whole lot of data sources supported by Spark SQL. Moreover, the BI tools can push down the processor intensive join aggregation operations to a huge cluster of worker nodes in the Spark infrastructure.</p></div></div></div>
<div class="section" title="Anatomy of Spark SQL"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec24"/>Anatomy of Spark SQL</h1></div></div></div><p>Interaction with Spark SQL library is done mainly through two methods. One is through SQL-like queries and the other is through DataFrame API. Before getting into the details of how DataFrame-based programs work, it is a good idea to see how the RDD-based programs work.</p><p>The Spark transformations and Spark actions are converted into Java functions and they act on top of RDDs, which are nothing but Java objects acting upon data. Since RDD is a pure Java object, there is no way, at compile time or at run time, to know about what data is going to process. There is no metadata available to the execution engine beforehand to optimize the Spark transformations or Spark actions. There are no multiple execution paths or query plans available in advance to process that data and so, evaluation of the efficacy of various paths of execution is not available.</p><p>Here, there is no optimized query plan executed because there is no schema associated, with data. In the case of DataFrame, the structure is well-known in advance. Because of this the queries can be optimized and data cache can be built beforehand.</p><p>The following <span class="emphasis"><em>Figure 1</em></span> gives an idea about the same:</p><p>
</p><div class="mediaobject"><img alt="Anatomy of Spark SQL" src="graphics/image_03_002.jpg"/><div class="caption"><p>Figure 1</p></div></div><p>
</p><p>The SQL-like queries and DataFrame API calls made against DataFrame are converted to language-neutral expressions. The language-neutral expression corresponding to a SQL query or DataFrame API is called an unresolved logical plan.</p><p>The unresolved logical plan is converted to a logical plan by doing validations on column names from the metadata of the DataFrame. The logical plan is further optimized by applying standard rules such as simplification of expressions, evaluations of expressions, and other optimization rules, to form an optimized logical plan. The optimized logical plan is converted to multiple physical plans. The physical plans are created by using Spark-specific operators in the logical plan. The best physical plan is chosen and the resultant queries are pushed down to RDDs to act on the data. Because the SQL queries and DataFrame API calls are converted to language-neutral query expressions, the performance of these queries is consistent across all the supported languages. That is the same reason why the DataFrame API is supported by all the Spark supported languages such as Scala, Java, Python, and R. In the future, there is a good chance that many more languages are going to be supporting DataFrame API and Spark SQL because of this reason.</p><p>The query planning and optimizations of Spark SQL are worth mentioning here too. Any query operation done on a DataFrame through SQL queries or through DataFrame API is highly optimized before the corresponding operations are physically applied on the underlying base RDD. There are many processes in between before the real action happening on the RDD.</p><p>
<span class="emphasis"><em>Figure 2</em></span> gives some idea about the whole query optimization process:</p><p>
</p><div class="mediaobject"><img alt="Anatomy of Spark SQL" src="graphics/image_03_004.jpg"/><div class="caption"><p>Figure 2</p></div></div><p>
</p><p>Two types of queries can be called against a DataFrame. They are SQL queries or DataFrame API calls. They go through a proper analysis to come up with a logical query plan of execution. Then, optimizations are applied on the logical query plans to arrive at an optimized logical query plan. From the final optimized logical query plan, one or more physical query plans are made. For each of the physical query plans, cost models are worked out, and based on the optimal cost, an appropriate physical query plan is selected, and highly optimized code is generated and run against the RDDs. This is the reason behind the consistent performance of queries of any type on DataFrame. This is the same reason why the DataFrame API calls from all these different languages, Scala, Java, Python, and R, give consistent performance.</p><p>Let's revisit the bigger picture once again, as given in <span class="emphasis"><em>Figure 3</em></span>, to set the context and see what is being discussed here before getting into and taking up the use cases:</p><p>
</p><div class="mediaobject"><img alt="Anatomy of Spark SQL" src="graphics/image_03_006.jpg"/><div class="caption"><p>Figure 3</p></div></div><p>
</p><p>The use cases that are going to be discussed here will demonstrate the ability to mix SQL queries with Spark programs. Multiple data sources will be chosen, data will be read from those sources using DataFrame, and uniform data access will be demonstrated. The programming languages used to demonstrate are still Scala and Python. The usage of R to manipulate DataFrames is on the agenda of the book and a whole chapter is dedicated to the same.</p></div>
<div class="section" title="DataFrame programming"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec25"/>DataFrame programming</h1></div></div></div><p>The use cases selected for elucidating the Spark SQL way of programming with DataFrame are given as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The transaction records come as comma-separated values.</li><li class="listitem" style="list-style-type: disc">Filter out only the good transaction records from the list. The account number should start with <code class="literal">SB</code> and the transaction amount should be greater than zero.</li><li class="listitem" style="list-style-type: disc">Find all the high-value transaction records with a transaction amount greater than 1000.</li><li class="listitem" style="list-style-type: disc">Find all the transaction records where the account number is bad.</li><li class="listitem" style="list-style-type: disc">Find all the transaction records where the transaction amount is less than or equal to zero.</li><li class="listitem" style="list-style-type: disc">Find a combined list of all the bad transaction records.</li><li class="listitem" style="list-style-type: disc">Find the total of all the transaction amounts.</li><li class="listitem" style="list-style-type: disc">Find the maximum of all the transaction amounts.</li><li class="listitem" style="list-style-type: disc">Find the minimum of all the transaction amounts.</li><li class="listitem" style="list-style-type: disc">Find all the good account numbers.</li></ul></div><p>This is exactly the same set of use cases that were used in the previous chapter as well, but here the programming model is totally different. Using this set of use cases, two types of programming models are demonstrated here. One is using the SQL queries and the other is using DataFrame APIs.</p><div class="section" title="Programming with SQL"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec17"/>Programming with SQL</h2></div></div></div><p>At the Scala REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; // Define the case classes for using in conjunction with DataFrames &#13;
scala&gt; case class Trans(accNo: String, tranAmount: Double) &#13;
defined class Trans &#13;
scala&gt; // Functions to convert the sequence of strings to objects defined by the case classes &#13;
scala&gt; def toTrans =  (trans: Seq[String]) =&gt; Trans(trans(0), trans(1).trim.toDouble) &#13;
toTrans: Seq[String] =&gt; Trans &#13;
scala&gt; // Creation of the list from where the RDD is going to be created &#13;
scala&gt; val acTransList = Array("SB10001,1000", "SB10002,1200", "SB10003,8000", "SB10004,400", "SB10005,300", "SB10006,10000", "SB10007,500", "SB10008,56", "SB10009,30","SB10010,7000", "CR10001,7000", "SB10002,-10") &#13;
acTransList: Array[String] = Array(SB10001,1000, SB10002,1200, SB10003,8000, SB10004,400, SB10005,300, SB10006,10000, SB10007,500, SB10008,56, SB10009,30, SB10010,7000, CR10001,7000, SB10002,-10) &#13;
scala&gt; // Create the RDD &#13;
scala&gt; val acTransRDD = sc.parallelize(acTransList).map(_.split(",")).map(toTrans(_)) &#13;
acTransRDD: org.apache.spark.rdd.RDD[Trans] = MapPartitionsRDD[2] at map at &lt;console&gt;:30 &#13;
scala&gt; // Convert RDD to DataFrame &#13;
scala&gt; val acTransDF = spark.createDataFrame(acTransRDD) &#13;
acTransDF: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] &#13;
scala&gt; // Register temporary view in the DataFrame for using it in SQL &#13;
scala&gt; acTransDF.createOrReplaceTempView("trans") &#13;
scala&gt; // Print the structure of the DataFrame &#13;
scala&gt; acTransDF.printSchema &#13;
root &#13;
 |-- accNo: string (nullable = true) &#13;
 |-- tranAmount: double (nullable = false) &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; acTransDF.show &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10001|    1000.0| &#13;
|SB10002|    1200.0| &#13;
|SB10003|    8000.0| &#13;
|SB10004|     400.0| &#13;
|SB10005|     300.0| &#13;
|SB10006|   10000.0| &#13;
|SB10007|     500.0| &#13;
|SB10008|      56.0| &#13;
|SB10009|      30.0| &#13;
|SB10010|    7000.0| &#13;
|CR10001|    7000.0| &#13;
|SB10002|     -10.0| &#13;
+-------+----------+ &#13;
scala&gt; // Use SQL to create another DataFrame containing the good transaction records &#13;
scala&gt; val goodTransRecords = spark.sql("SELECT accNo, tranAmount FROM trans WHERE accNo like 'SB%' AND tranAmount &gt; 0") &#13;
goodTransRecords: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] &#13;
scala&gt; // Register temporary view in the DataFrame for using it in SQL &#13;
scala&gt; goodTransRecords.createOrReplaceTempView("goodtrans") &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; goodTransRecords.show &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10001|    1000.0| &#13;
|SB10002|    1200.0| &#13;
|SB10003|    8000.0| &#13;
|SB10004|     400.0| &#13;
|SB10005|     300.0| &#13;
|SB10006|   10000.0| &#13;
|SB10007|     500.0| &#13;
|SB10008|      56.0| &#13;
|SB10009|      30.0| &#13;
|SB10010|    7000.0| &#13;
+-------+----------+ &#13;
scala&gt; // Use SQL to create another DataFrame containing the high value transaction records &#13;
scala&gt; val highValueTransRecords = spark.sql("SELECT accNo, tranAmount FROM goodtrans WHERE tranAmount &gt; 1000") &#13;
highValueTransRecords: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; highValueTransRecords.show &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10002|    1200.0| &#13;
|SB10003|    8000.0| &#13;
|SB10006|   10000.0| &#13;
|SB10010|    7000.0| &#13;
+-------+----------+ &#13;
scala&gt; // Use SQL to create another DataFrame containing the bad account records &#13;
scala&gt; val badAccountRecords = spark.sql("SELECT accNo, tranAmount FROM trans WHERE accNo NOT like 'SB%'") &#13;
badAccountRecords: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; badAccountRecords.show &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|CR10001|    7000.0| &#13;
+-------+----------+ &#13;
scala&gt; // Use SQL to create another DataFrame containing the bad amount records &#13;
scala&gt; val badAmountRecords = spark.sql("SELECT accNo, tranAmount FROM trans WHERE tranAmount &lt; 0") &#13;
badAmountRecords: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; badAmountRecords.show &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10002|     -10.0| &#13;
+-------+----------+ &#13;
scala&gt; // Do the union of two DataFrames and create another DataFrame &#13;
scala&gt; val badTransRecords = badAccountRecords.union(badAmountRecords) &#13;
badTransRecords: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [accNo: string, tranAmount: double] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; badTransRecords.show &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|CR10001|    7000.0| &#13;
|SB10002|     -10.0| &#13;
+-------+----------+ &#13;
scala&gt; // Calculate the sum &#13;
scala&gt; val sumAmount = spark.sql("SELECT sum(tranAmount) as sum FROM goodtrans") &#13;
sumAmount: org.apache.spark.sql.DataFrame = [sum: double] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; sumAmount.show &#13;
+-------+ &#13;
|    sum| &#13;
+-------+ &#13;
|28486.0| &#13;
+-------+ &#13;
scala&gt; // Calculate the maximum &#13;
scala&gt; val maxAmount = spark.sql("SELECT max(tranAmount) as max FROM goodtrans") &#13;
maxAmount: org.apache.spark.sql.DataFrame = [max: double] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; maxAmount.show &#13;
+-------+ &#13;
|    max| &#13;
+-------+ &#13;
|10000.0| &#13;
+-------+ &#13;
scala&gt; // Calculate the minimum &#13;
scala&gt; val minAmount = spark.sql("SELECT min(tranAmount) as min FROM goodtrans") &#13;
minAmount: org.apache.spark.sql.DataFrame = [min: double] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; minAmount.show &#13;
+----+ &#13;
| min| &#13;
+----+ &#13;
|30.0| &#13;
+----+ &#13;
scala&gt; // Use SQL to create another DataFrame containing the good account numbers &#13;
scala&gt; val goodAccNos = spark.sql("SELECT DISTINCT accNo FROM trans WHERE accNo like 'SB%' ORDER BY accNo") &#13;
goodAccNos: org.apache.spark.sql.DataFrame = [accNo: string] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; goodAccNos.show &#13;
+-------+ &#13;
|  accNo| &#13;
+-------+ &#13;
|SB10001| &#13;
|SB10002| &#13;
|SB10003| &#13;
|SB10004| &#13;
|SB10005| &#13;
|SB10006| &#13;
|SB10007| &#13;
|SB10008| &#13;
|SB10009| &#13;
|SB10010| &#13;
+-------+ &#13;
scala&gt; // Calculate the aggregates using mixing of DataFrame and RDD like operations &#13;
scala&gt; val sumAmountByMixing = goodTransRecords.map(trans =&gt; trans.getAs[Double]("tranAmount")).reduce(_ + _) &#13;
sumAmountByMixing: Double = 28486.0 &#13;
scala&gt; val maxAmountByMixing = goodTransRecords.map(trans =&gt; trans.getAs[Double]("tranAmount")).reduce((a, b) =&gt; if (a &gt; b) a else b) &#13;
maxAmountByMixing: Double = 10000.0 &#13;
scala&gt; val minAmountByMixing = goodTransRecords.map(trans =&gt; trans.getAs[Double]("tranAmount")).reduce((a, b) =&gt; if (a &lt; b) a else b) &#13;
minAmountByMixing: Double = 30.0 &#13;
</strong></span>
</pre><p>The retail banking transaction records come with account number, transaction amount and are processed using SparkSQL to get the desired results of the use cases. Here is the summary of what the preceding script did:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A Scala case class is defined to describe the structure of the transaction record to be fed into the DataFrame.</li><li class="listitem" style="list-style-type: disc">An array is defined with the necessary transaction records.</li><li class="listitem" style="list-style-type: disc">An RDD is made from the array, split the comma-separated values, mapped it to create objects using the Scala case class that was defined as the first step in the scripts, and the RDD is converted to a DataFrame. This is one use case of interoperability between RDD and DataFrame.</li><li class="listitem" style="list-style-type: disc">A table is registered with the DataFrame with a name. This registered name of the table can be used in SQL statements.</li><li class="listitem" style="list-style-type: disc">Then, all the other activities are just issuing SQL statements using the <code class="literal">spark.sql</code> method. Here the object spark is of type the SparkSession.</li><li class="listitem" style="list-style-type: disc">The result of all these SQL statements is stored as DataFrames and, just like the RDD's <code class="literal">collect</code> action, DataFrame's show method is used to extract the values to the Spark driver program.</li><li class="listitem" style="list-style-type: disc">The aggregate value calculations are done in two different ways. One is in the SQL statement way, which is the easiest way. The other is using the regular RDD-style Spark transformations and Spark actions. This is to show that even DataFrame can be operated like an RDD, and Spark transformations and Spark actions can be applied on top of DataFrame.</li><li class="listitem" style="list-style-type: disc">At times, it is easy to do some data manipulation activities through the functional style operations using functions. So, there is a flexibility here to mix SQL, RDD, and DataFrame to have a very convenient programming model to process data.</li><li class="listitem" style="list-style-type: disc">The DataFrame contents are displayed in table format using the <code class="literal">show</code> method of the DataFrame.</li><li class="listitem" style="list-style-type: disc">A detailed view of the structure of the DataFrame is displayed using the <code class="literal">printSchema</code> method. This is akin to the <code class="literal">describe</code> command of the database tables.</li></ul></div><p>At the Python REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from pyspark.sql import Row &#13;
&gt;&gt;&gt; # Creation of the list from where the RDD is going to be created &#13;
&gt;&gt;&gt; acTransList = ["SB10001,1000", "SB10002,1200", "SB10003,8000", "SB10004,400", "SB10005,300", "SB10006,10000", "SB10007,500", "SB10008,56", "SB10009,30","SB10010,7000", "CR10001,7000", "SB10002,-10"] &#13;
&gt;&gt;&gt; # Create the DataFrame &#13;
&gt;&gt;&gt; acTransDF = sc.parallelize(acTransList).map(lambda trans: trans.split(",")).map(lambda p: Row(accNo=p[0], tranAmount=float(p[1]))).toDF() &#13;
&gt;&gt;&gt; # Register temporary view in the DataFrame for using it in SQL &#13;
&gt;&gt;&gt; acTransDF.createOrReplaceTempView("trans") &#13;
&gt;&gt;&gt; # Print the structure of the DataFrame &#13;
&gt;&gt;&gt; acTransDF.printSchema() &#13;
root &#13;
 |-- accNo: string (nullable = true) &#13;
 |-- tranAmount: double (nullable = true) &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; acTransDF.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10001|    1000.0| &#13;
|SB10002|    1200.0| &#13;
|SB10003|    8000.0| &#13;
|SB10004|     400.0| &#13;
|SB10005|     300.0| &#13;
|SB10006|   10000.0| &#13;
|SB10007|     500.0| &#13;
|SB10008|      56.0| &#13;
|SB10009|      30.0| &#13;
|SB10010|    7000.0| &#13;
|CR10001|    7000.0| &#13;
|SB10002|     -10.0| &#13;
+-------+----------+ &#13;
&gt;&gt;&gt; # Use SQL to create another DataFrame containing the good transaction records &#13;
&gt;&gt;&gt; goodTransRecords = spark.sql("SELECT accNo, tranAmount FROM trans WHERE accNo like 'SB%' AND tranAmount &gt; 0") &#13;
&gt;&gt;&gt; # Register temporary table in the DataFrame for using it in SQL &#13;
&gt;&gt;&gt; goodTransRecords.createOrReplaceTempView("goodtrans") &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; goodTransRecords.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10001|    1000.0| &#13;
|SB10002|    1200.0| &#13;
|SB10003|    8000.0| &#13;
|SB10004|     400.0| &#13;
|SB10005|     300.0| &#13;
|SB10006|   10000.0| &#13;
|SB10007|     500.0| &#13;
|SB10008|      56.0| &#13;
|SB10009|      30.0| &#13;
|SB10010|    7000.0| &#13;
+-------+----------+ &#13;
&gt;&gt;&gt; # Use SQL to create another DataFrame containing the high value transaction records &#13;
&gt;&gt;&gt; highValueTransRecords = spark.sql("SELECT accNo, tranAmount FROM goodtrans WHERE tranAmount &gt; 1000") &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; highValueTransRecords.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10002|    1200.0| &#13;
|SB10003|    8000.0| &#13;
|SB10006|   10000.0| &#13;
|SB10010|    7000.0| &#13;
+-------+----------+ &#13;
&gt;&gt;&gt; # Use SQL to create another DataFrame containing the bad account records &#13;
&gt;&gt;&gt; badAccountRecords = spark.sql("SELECT accNo, tranAmount FROM trans WHERE accNo NOT like 'SB%'") &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; badAccountRecords.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|CR10001|    7000.0| &#13;
+-------+----------+ &#13;
&gt;&gt;&gt; # Use SQL to create another DataFrame containing the bad amount records &#13;
&gt;&gt;&gt; badAmountRecords = spark.sql("SELECT accNo, tranAmount FROM trans WHERE tranAmount &lt; 0") &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; badAmountRecords.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10002|     -10.0| &#13;
+-------+----------+ &#13;
&gt;&gt;&gt; # Do the union of two DataFrames and create another DataFrame &#13;
&gt;&gt;&gt; badTransRecords = badAccountRecords.union(badAmountRecords) &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; badTransRecords.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|CR10001|    7000.0| &#13;
|SB10002|     -10.0| &#13;
+-------+----------+ &#13;
&gt;&gt;&gt; # Calculate the sum &#13;
&gt;&gt;&gt; sumAmount = spark.sql("SELECT sum(tranAmount)as sum FROM goodtrans") &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; sumAmount.show() &#13;
+-------+ &#13;
|    sum| &#13;
+-------+ &#13;
|28486.0| &#13;
+-------+ &#13;
&gt;&gt;&gt; # Calculate the maximum &#13;
&gt;&gt;&gt; maxAmount = spark.sql("SELECT max(tranAmount) as max FROM goodtrans") &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; maxAmount.show() &#13;
+-------+ &#13;
|    max| &#13;
+-------+ &#13;
|10000.0| &#13;
+-------+ &#13;
&gt;&gt;&gt; # Calculate the minimum &#13;
&gt;&gt;&gt; minAmount = spark.sql("SELECT min(tranAmount)as min FROM goodtrans") &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; minAmount.show() &#13;
+----+ &#13;
| min| &#13;
+----+ &#13;
|30.0| &#13;
+----+ &#13;
&gt;&gt;&gt; # Use SQL to create another DataFrame containing the good account numbers &#13;
&gt;&gt;&gt; goodAccNos = spark.sql("SELECT DISTINCT accNo FROM trans WHERE accNo like 'SB%' ORDER BY accNo") &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; goodAccNos.show() &#13;
+-------+ &#13;
|  accNo| &#13;
+-------+ &#13;
|SB10001| &#13;
|SB10002| &#13;
|SB10003| &#13;
|SB10004| &#13;
|SB10005| &#13;
|SB10006| &#13;
|SB10007| &#13;
|SB10008| &#13;
|SB10009| &#13;
|SB10010| &#13;
+-------+ &#13;
&gt;&gt;&gt; # Calculate the sum using mixing of DataFrame and RDD like operations &#13;
&gt;&gt;&gt; sumAmountByMixing = goodTransRecords.rdd.map(lambda trans: trans.tranAmount).reduce(lambda a,b : a+b) &#13;
&gt;&gt;&gt; sumAmountByMixing &#13;
28486.0 &#13;
&gt;&gt;&gt; # Calculate the maximum using mixing of DataFrame and RDD like operations &#13;
&gt;&gt;&gt; maxAmountByMixing = goodTransRecords.rdd.map(lambda trans: trans.tranAmount).reduce(lambda a,b : a if a &gt; b else b) &#13;
&gt;&gt;&gt; maxAmountByMixing &#13;
10000.0 &#13;
&gt;&gt;&gt; # Calculate the minimum using mixing of DataFrame and RDD like operations &#13;
&gt;&gt;&gt; minAmountByMixing = goodTransRecords.rdd.map(lambda trans: trans.tranAmount).reduce(lambda a,b : a if a &lt; b else b) &#13;
&gt;&gt;&gt; minAmountByMixing &#13;
30.0 &#13;
</strong></span>
</pre><p>In the preceding Python code snippet, except for a few language-specific constructs such as importing libraries and the definition of lambda functions, the style of programming is almost the same, most of the time, as the Scala code. This is the advantage of Spark's uniform programming model. As discussed earlier, when business analysts or data analysts provide the SQL for data access, it is very easy to integrate that along with the data processing code in Spark. This uniform programming style of coding is very useful for organizations to use the language of their choice for developing data processing applications in Spark.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip27"/>Tip</h3><p>On DataFrames, if applicable Spark transformations are applied, then a Dataset is returned instead of a DataFrame. The concept of Dataset is introduced toward the end of this chapter. There is a very strong relationship between DataFrame and Dataset, and that is explained in the section covering Datasets. While developing applications, caution must be used in this kind of situation. For example, in the preceding code snippets, if the following transformation is tried in Scala REPL, it will return a dataset: <code class="literal">val amount = goodTransRecords.map(trans =&gt; trans.getAs[Double]("tranAmount"))amount: org.apache.spark.sql.Dataset[Double] = [value: double]</code>
</p></div></div></div><div class="section" title="Programming with DataFrame API"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec18"/>Programming with DataFrame API</h2></div></div></div><p>In this section, the code snippets will be run in the appropriate language REPLs as a continuation of the previous section so that the setup of the data and other initializations are not repeated. Like the preceding code snippets, initially, some DataFrame-specific basic commands are given. These are used regularly to see the contents and for doing some sanity tests on the DataFrame and its contents. These are commands that are typically used in the exploratory stage of the data analysis, quite often to get more insight into the structure and contents of the underlying data.</p><p>At the Scala REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; acTransDF.show &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10001|    1000.0| &#13;
|SB10002|    1200.0| &#13;
|SB10003|    8000.0| &#13;
|SB10004|     400.0| &#13;
|SB10005|     300.0| &#13;
|SB10006|   10000.0| &#13;
|SB10007|     500.0| &#13;
|SB10008|      56.0| &#13;
|SB10009|      30.0| &#13;
|SB10010|    7000.0| &#13;
|CR10001|    7000.0| &#13;
|SB10002|     -10.0| &#13;
+-------+----------+ &#13;
scala&gt; // Create the DataFrame using API for the good transaction records &#13;
scala&gt; val goodTransRecords = acTransDF.filter("accNo like 'SB%'").filter("tranAmount &gt; 0") &#13;
goodTransRecords: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [accNo: string, tranAmount: double] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; goodTransRecords.show &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10001|    1000.0| &#13;
|SB10002|    1200.0| &#13;
|SB10003|    8000.0| &#13;
|SB10004|     400.0| &#13;
|SB10005|     300.0| &#13;
|SB10006|   10000.0| &#13;
|SB10007|     500.0| &#13;
|SB10008|      56.0| &#13;
|SB10009|      30.0| &#13;
|SB10010|    7000.0| &#13;
+-------+----------+ &#13;
scala&gt; // Create the DataFrame using API for the high value transaction records &#13;
scala&gt; val highValueTransRecords = goodTransRecords.filter("tranAmount &gt; 1000") &#13;
highValueTransRecords: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [accNo: string, tranAmount: double] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; highValueTransRecords.show &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10002|    1200.0| &#13;
|SB10003|    8000.0| &#13;
|SB10006|   10000.0| &#13;
|SB10010|    7000.0| &#13;
+-------+----------+ &#13;
scala&gt; // Create the DataFrame using API for the bad account records &#13;
scala&gt; val badAccountRecords = acTransDF.filter("accNo NOT like 'SB%'") &#13;
badAccountRecords: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [accNo: string, tranAmount: double] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; badAccountRecords.show &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|CR10001|    7000.0| &#13;
+-------+----------+ &#13;
scala&gt; // Create the DataFrame using API for the bad amount records &#13;
scala&gt; val badAmountRecords = acTransDF.filter("tranAmount &lt; 0") &#13;
badAmountRecords: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [accNo: string, tranAmount: double] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; badAmountRecords.show &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10002|     -10.0| &#13;
+-------+----------+ &#13;
scala&gt; // Do the union of two DataFrames &#13;
scala&gt; val badTransRecords = badAccountRecords.union(badAmountRecords) &#13;
badTransRecords: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [accNo: string, tranAmount: double] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; badTransRecords.show &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|CR10001|    7000.0| &#13;
|SB10002|     -10.0| &#13;
+-------+----------+ &#13;
scala&gt; // Calculate the aggregates in one shot &#13;
scala&gt; val aggregates = goodTransRecords.agg(sum("tranAmount"), max("tranAmount"), min("tranAmount")) &#13;
aggregates: org.apache.spark.sql.DataFrame = [sum(tranAmount): double, max(tranAmount): double ... 1 more field] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; aggregates.show &#13;
+---------------+---------------+---------------+ &#13;
|sum(tranAmount)|max(tranAmount)|min(tranAmount)| &#13;
+---------------+---------------+---------------+ &#13;
|        28486.0|        10000.0|           30.0| &#13;
+---------------+---------------+---------------+ &#13;
scala&gt; // Use DataFrame using API for creating the good account numbers &#13;
scala&gt; val goodAccNos = acTransDF.filter("accNo like 'SB%'").select("accNo").distinct().orderBy("accNo") &#13;
goodAccNos: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [accNo: string] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; goodAccNos.show &#13;
+-------+ &#13;
|  accNo| &#13;
+-------+ &#13;
|SB10001| &#13;
|SB10002| &#13;
|SB10003| &#13;
|SB10004| &#13;
|SB10005| &#13;
|SB10006| &#13;
|SB10007| &#13;
|SB10008| &#13;
|SB10009| &#13;
|SB10010| &#13;
+-------+ &#13;
scala&gt; // Persist the data of the DataFrame into a Parquet file &#13;
scala&gt; acTransDF.write.parquet("scala.trans.parquet") &#13;
scala&gt; // Read the data into a DataFrame from the Parquet file &#13;
scala&gt; val acTransDFfromParquet = spark.read.parquet("scala.trans.parquet") &#13;
acTransDFfromParquet: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; acTransDFfromParquet.show &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10002|    1200.0| &#13;
|SB10003|    8000.0| &#13;
|SB10005|     300.0| &#13;
|SB10006|   10000.0| &#13;
|SB10008|      56.0| &#13;
|SB10009|      30.0| &#13;
|CR10001|    7000.0| &#13;
|SB10002|     -10.0| &#13;
|SB10001|    1000.0| &#13;
|SB10004|     400.0| &#13;
|SB10007|     500.0| &#13;
|SB10010|    7000.0| &#13;
+-------+----------+</strong></span>
</pre><p>Here is the summary of what the preceding script did from a DataFrame API perspective:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The DataFrame containing the superset of data used in the preceding section is used here.</li><li class="listitem" style="list-style-type: disc">Filtering of the records is demonstrated next. Here, the most important aspect to notice is that the filter predicate is to be given exactly like the predicates in the SQL statements. Filters can be chained.</li><li class="listitem" style="list-style-type: disc">The aggregation methods are calculated in one go as three columns in the resultant DataFrame.</li><li class="listitem" style="list-style-type: disc">The final statements in this set are doing the selection, filtering, choosing distinct records, and ordering in one single chained statement.</li><li class="listitem" style="list-style-type: disc">Finally, the transaction records are persisted in Parquet format, read from the Parquet store and create a DataFrame. More details on the persistence formats is coming in the following section.</li><li class="listitem" style="list-style-type: disc">In this code snippet, the Parquet format data is stored in the current directory from where the corresponding REPL is invoked. When it is run as a Spark program, the directory again will be the current directory from where the Spark submit is invoked.</li></ul></div><p>At the Python REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; acTransDF.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10001|    1000.0| &#13;
|SB10002|    1200.0| &#13;
|SB10003|    8000.0| &#13;
|SB10004|     400.0| &#13;
|SB10005|     300.0| &#13;
|SB10006|   10000.0| &#13;
|SB10007|     500.0| &#13;
|SB10008|      56.0| &#13;
|SB10009|      30.0| &#13;
|SB10010|    7000.0| &#13;
|CR10001|    7000.0| &#13;
|SB10002|     -10.0| &#13;
+-------+----------+ &#13;
&gt;&gt;&gt; # Print the structure of the DataFrame &#13;
&gt;&gt;&gt; acTransDF.printSchema() &#13;
root &#13;
 |-- accNo: string (nullable = true) &#13;
 |-- tranAmount: double (nullable = true) &#13;
&gt;&gt;&gt; # Create the DataFrame using API for the good transaction records &#13;
&gt;&gt;&gt; goodTransRecords = acTransDF.filter("accNo like 'SB%'").filter("tranAmount &gt; 0") &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; goodTransRecords.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10001|    1000.0| &#13;
|SB10002|    1200.0| &#13;
|SB10003|    8000.0| &#13;
|SB10004|     400.0| &#13;
|SB10005|     300.0| &#13;
|SB10006|   10000.0| &#13;
|SB10007|     500.0| &#13;
|SB10008|      56.0| &#13;
|SB10009|      30.0| &#13;
|SB10010|    7000.0| &#13;
+-------+----------+ &#13;
&gt;&gt;&gt; # Create the DataFrame using API for the high value transaction records &#13;
&gt;&gt;&gt; highValueTransRecords = goodTransRecords.filter("tranAmount &gt; 1000") &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; highValueTransRecords.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10002|    1200.0| &#13;
|SB10003|    8000.0| &#13;
|SB10006|   10000.0| &#13;
|SB10010|    7000.0| &#13;
+-------+----------+ &#13;
&gt;&gt;&gt; # Create the DataFrame using API for the bad account records &#13;
&gt;&gt;&gt; badAccountRecords = acTransDF.filter("accNo NOT like 'SB%'") &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; badAccountRecords.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|CR10001|    7000.0| &#13;
+-------+----------+ &#13;
&gt;&gt;&gt; # Create the DataFrame using API for the bad amount records &#13;
&gt;&gt;&gt; badAmountRecords = acTransDF.filter("tranAmount &lt; 0") &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; badAmountRecords.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10002|     -10.0| &#13;
+-------+----------+ &#13;
&gt;&gt;&gt; # Do the union of two DataFrames and create another DataFrame &#13;
&gt;&gt;&gt; badTransRecords = badAccountRecords.union(badAmountRecords) &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; badTransRecords.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|CR10001|    7000.0| &#13;
|SB10002|     -10.0| &#13;
+-------+----------+ &#13;
&gt;&gt;&gt; # Calculate the sum &#13;
&gt;&gt;&gt; sumAmount = goodTransRecords.agg({"tranAmount": "sum"}) &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; sumAmount.show() &#13;
+---------------+ &#13;
|sum(tranAmount)| &#13;
+---------------+ &#13;
|        28486.0| &#13;
+---------------+ &#13;
&gt;&gt;&gt; # Calculate the maximum &#13;
&gt;&gt;&gt; maxAmount = goodTransRecords.agg({"tranAmount": "max"}) &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; maxAmount.show() &#13;
+---------------+ &#13;
|max(tranAmount)| &#13;
+---------------+ &#13;
|        10000.0| &#13;
+---------------+ &#13;
&gt;&gt;&gt; # Calculate the minimum &#13;
&gt;&gt;&gt; minAmount = goodTransRecords.agg({"tranAmount": "min"}) &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; minAmount.show() &#13;
+---------------+ &#13;
|min(tranAmount)| &#13;
+---------------+ &#13;
|           30.0| &#13;
+---------------+ &#13;
&gt;&gt;&gt; # Create the DataFrame using API for the good account numbers &#13;
&gt;&gt;&gt; goodAccNos = acTransDF.filter("accNo like 'SB%'").select("accNo").distinct().orderBy("accNo") &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; goodAccNos.show() &#13;
+-------+ &#13;
|  accNo| &#13;
+-------+ &#13;
|SB10001| &#13;
|SB10002| &#13;
|SB10003| &#13;
|SB10004| &#13;
|SB10005| &#13;
|SB10006| &#13;
|SB10007| &#13;
|SB10008| &#13;
|SB10009| &#13;
|SB10010| &#13;
+-------+ &#13;
&gt;&gt;&gt; # Persist the data of the DataFrame into a Parquet file &#13;
&gt;&gt;&gt; acTransDF.write.parquet("python.trans.parquet") &#13;
&gt;&gt;&gt; # Read the data into a DataFrame from the Parquet file &#13;
&gt;&gt;&gt; acTransDFfromParquet = spark.read.parquet("python.trans.parquet") &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; acTransDFfromParquet.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10002|    1200.0| &#13;
|SB10003|    8000.0| &#13;
|SB10005|     300.0| &#13;
|SB10006|   10000.0| &#13;
|SB10008|      56.0| &#13;
|SB10009|      30.0| &#13;
|CR10001|    7000.0| &#13;
|SB10002|     -10.0| &#13;
|SB10001|    1000.0| &#13;
|SB10004|     400.0| &#13;
|SB10007|     500.0| &#13;
|SB10010|    7000.0| &#13;
+-------+----------+ &#13;
</strong></span>
</pre><p>In the preceding Python code snippet, except for a very few variations in the aggregation calculations, the programming constructs are almost similar to its Scala counterpart.</p><p>The last few statements of the preceding Scala and Python sections are about the persisting of the DataFrame contents into the media. The writing and reading operations are very much required in any kind of data processing operations, but most of the tools don't have a uniform way of writing and reading. Spark SQL is different. The DataFrame API comes with a rich set of persistence mechanisms. It is very easy to write contents of a DataFrame into many supported persistence stores. All these writing and reading operations have very simple DSL style interfaces. Here are some of the built-in formats in which DataFrames can be written to and read from.</p><p>Apart from these, there are so many other external data sources supported through third-party packages:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">JSON</li><li class="listitem" style="list-style-type: disc">Parquet</li><li class="listitem" style="list-style-type: disc">Hive</li><li class="listitem" style="list-style-type: disc">MySQL</li><li class="listitem" style="list-style-type: disc">PostgreSQL</li><li class="listitem" style="list-style-type: disc">HDFS</li><li class="listitem" style="list-style-type: disc">Plain Text</li><li class="listitem" style="list-style-type: disc">Amazon S3</li><li class="listitem" style="list-style-type: disc">ORC</li><li class="listitem" style="list-style-type: disc">JDBC</li></ul></div><p>The write and read of DataFrame into and from Parquet has been demonstrated in the preceding code snippets. All the preceding inherently supported data stores have very simple DSL style syntax for persistence and reading back, which makes the programming style uniform once again. The DataFrame API reference is a great source to know about the details of dealing with each of these data stores. </p><p>The sample code in this chapter persists data in Parquet and JSON formats. The data store location names chosen are <code class="literal">python.trans.parquet</code>, <code class="literal">scala.trans.parquet</code>, and so on. This is just to give an indication of which programming language is used and which is the format of the data. This is not a proper convention but a convenience. When one run of the program is completed, these directories would have been created. Next time the same program is run, it will try to create the same and will result in an error. The workaround is to remove the directories manually, before the subsequent runs, and proceed. Proper error handling mechanisms and other nuances of fine programming are going to dilute the focus and hence are deliberately left out of this book.</p></div></div>
<div class="section" title="Understanding Aggregations in Spark SQL"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec26"/>Understanding Aggregations in Spark SQL</h1></div></div></div><p>In SQL, aggregation of data is very flexible. The same thing is true in Spark SQL too. Instead of running SQL statements on a single data source located in a single machine, here Spark SQL can do the same on distributed data sources. In the previous chapter, a MapReduce use case was discussed to do data aggregation and the same is being used here to demonstrate the aggregation capabilities of Spark SQL. In this section also, the use cases are approached in the SQL query way as well as in the DataFrame API way. </p><p>The use cases selected for elucidating the MapReduce kind of data processing here are given as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The retail banking transaction records come with account number and transaction amount in comma-separated strings</li><li class="listitem" style="list-style-type: disc">Find an account level summary of all the transactions to get the account balance</li></ul></div><p>At the Scala REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; // Define the case classes for using in conjunction with DataFrames &#13;
scala&gt; case class Trans(accNo: String, tranAmount: Double) &#13;
defined class Trans &#13;
scala&gt; // Functions to convert the sequence of strings to objects defined by the case classes &#13;
scala&gt; def toTrans =  (trans: Seq[String]) =&gt; Trans(trans(0), trans(1).trim.toDouble) &#13;
toTrans: Seq[String] =&gt; Trans &#13;
scala&gt; // Creation of the list from where the RDD is going to be created &#13;
scala&gt; val acTransList = Array("SB10001,1000", "SB10002,1200","SB10001,8000", "SB10002,400", "SB10003,300", "SB10001,10000","SB10004,500","SB10005,56", "SB10003,30","SB10002,7000","SB10001,-100", "SB10002,-10") &#13;
acTransList: Array[String] = Array(SB10001,1000, SB10002,1200, SB10001,8000, SB10002,400, SB10003,300, SB10001,10000, SB10004,500, SB10005,56, SB10003,30, SB10002,7000, SB10001,-100, SB10002,-10) &#13;
scala&gt; // Create the DataFrame &#13;
scala&gt; val acTransDF = sc.parallelize(acTransList).map(_.split(",")).map(toTrans(_)).toDF() &#13;
acTransDF: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; acTransDF.show &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10001|    1000.0| &#13;
|SB10002|    1200.0| &#13;
|SB10001|    8000.0| &#13;
|SB10002|     400.0| &#13;
|SB10003|     300.0| &#13;
|SB10001|   10000.0| &#13;
|SB10004|     500.0| &#13;
|SB10005|      56.0| &#13;
|SB10003|      30.0| &#13;
|SB10002|    7000.0| &#13;
|SB10001|    -100.0| &#13;
|SB10002|     -10.0| &#13;
+-------+----------+ &#13;
scala&gt; // Register temporary view in the DataFrame for using it in SQL &#13;
scala&gt; acTransDF.createOrReplaceTempView("trans") &#13;
scala&gt; // Use SQL to create another DataFrame containing the account summary records &#13;
scala&gt; val acSummary = spark.sql("SELECT accNo, sum(tranAmount) as TransTotal FROM trans GROUP BY accNo") &#13;
acSummary: org.apache.spark.sql.DataFrame = [accNo: string, TransTotal: double] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; acSummary.show &#13;
+-------+----------+ &#13;
|  accNo|TransTotal| &#13;
+-------+----------+ &#13;
|SB10005|      56.0| &#13;
|SB10004|     500.0| &#13;
|SB10003|     330.0| &#13;
|SB10002|    8590.0| &#13;
|SB10001|   18900.0| &#13;
+-------+----------+ &#13;
scala&gt; // Create the DataFrame using API for the account summary records &#13;
scala&gt; val acSummaryViaDFAPI = acTransDF.groupBy("accNo").agg(sum("tranAmount") as "TransTotal") &#13;
acSummaryViaDFAPI: org.apache.spark.sql.DataFrame = [accNo: string, TransTotal: double] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; acSummaryViaDFAPI.show &#13;
+-------+----------+ &#13;
|  accNo|TransTotal| &#13;
+-------+----------+ &#13;
|SB10005|      56.0| &#13;
|SB10004|     500.0| &#13;
|SB10003|     330.0| &#13;
|SB10002|    8590.0| &#13;
|SB10001|   18900.0| &#13;
+-------+----------+</strong></span>
</pre><p>In this code snippet, everything is very similar to the preceding section's code. The only difference is that, here, aggregations are used in the SQL queries as well as in the DataFrame API. </p><p>At the Python REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from pyspark.sql import Row &#13;
&gt;&gt;&gt; # Creation of the list from where the RDD is going to be created &#13;
&gt;&gt;&gt; acTransList = ["SB10001,1000", "SB10002,1200", "SB10001,8000","SB10002,400", "SB10003,300", "SB10001,10000","SB10004,500","SB10005,56","SB10003,30","SB10002,7000", "SB10001,-100","SB10002,-10"] &#13;
&gt;&gt;&gt; # Create the DataFrame &#13;
&gt;&gt;&gt; acTransDF = sc.parallelize(acTransList).map(lambda trans: trans.split(",")).map(lambda p: Row(accNo=p[0], tranAmount=float(p[1]))).toDF() &#13;
&gt;&gt;&gt; # Register temporary view in the DataFrame for using it in SQL &#13;
&gt;&gt;&gt; acTransDF.createOrReplaceTempView("trans") &#13;
&gt;&gt;&gt; # Use SQL to create another DataFrame containing the account summary records &#13;
&gt;&gt;&gt; acSummary = spark.sql("SELECT accNo, sum(tranAmount) as transTotal FROM trans GROUP BY accNo") &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; acSummary.show()     &#13;
+-------+----------+ &#13;
|  accNo|transTotal| &#13;
+-------+----------+ &#13;
|SB10005|      56.0| &#13;
|SB10004|     500.0| &#13;
|SB10003|     330.0| &#13;
|SB10002|    8590.0| &#13;
|SB10001|   18900.0| &#13;
+-------+----------+ &#13;
&gt;&gt;&gt; # Create the DataFrame using API for the account summary records &#13;
&gt;&gt;&gt; acSummaryViaDFAPI = acTransDF.groupBy("accNo").agg({"tranAmount": "sum"}).selectExpr("accNo", "`sum(tranAmount)` as transTotal") &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; acSummaryViaDFAPI.show() &#13;
+-------+----------+ &#13;
|  accNo|transTotal| &#13;
+-------+----------+ &#13;
|SB10005|      56.0| &#13;
|SB10004|     500.0| &#13;
|SB10003|     330.0| &#13;
|SB10002|    8590.0| &#13;
|SB10001|   18900.0| &#13;
+-------+----------+</strong></span>
</pre><p>In the DataFrame API for Python, there are some minor syntax differences as compared to its Scala counterpart.</p></div>
<div class="section" title="Understanding multi-datasource joining with SparkSQL"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec27"/>Understanding multi-datasource joining with SparkSQL</h1></div></div></div><p>In the previous chapter, the joining of multiple RDDs based on the key has been discussed. In this section, the same use case is implemented using Spark SQL. The use cases selected for elucidating the joining of multiple datasets using a key are given here.</p><p>The first dataset contains a retail banking master records summary with account number, first name, and last name. The second dataset contains the retail banking account balance with account number and balance amount. The key on both of the datasets is account number. Join the two datasets and create one dataset containing account number, first name, last name, and balance amount. From this report, pick up the top three accounts in terms of the balance amount.</p><p>In this section, the concept of joining data from multiple data sources is also demonstrated. First the DataFrames are created from two arrays. They are persisted in Parquet and JSON formats. Then they are read from the disk to form the DataFrames, and they are joined together.</p><p>At the Scala REPL prompt, try the following statements: </p><pre class="programlisting">
<span class="strong"><strong>scala&gt; // Define the case classes for using in conjunction with DataFrames &#13;
scala&gt; case class AcMaster(accNo: String, firstName: String, lastName: String) &#13;
defined class AcMaster &#13;
scala&gt; case class AcBal(accNo: String, balanceAmount: Double) &#13;
defined class AcBal &#13;
scala&gt; // Functions to convert the sequence of strings to objects defined by the case classes &#13;
scala&gt; def toAcMaster =  (master: Seq[String]) =&gt; AcMaster(master(0), master(1), master(2)) &#13;
toAcMaster: Seq[String] =&gt; AcMaster &#13;
scala&gt; def toAcBal =  (bal: Seq[String]) =&gt; AcBal(bal(0), bal(1).trim.toDouble) &#13;
toAcBal: Seq[String] =&gt; AcBal &#13;
scala&gt; // Creation of the list from where the RDD is going to be created &#13;
scala&gt; val acMasterList = Array("SB10001,Roger,Federer","SB10002,Pete,Sampras", "SB10003,Rafael,Nadal","SB10004,Boris,Becker", "SB10005,Ivan,Lendl") &#13;
acMasterList: Array[String] = Array(SB10001,Roger,Federer, SB10002,Pete,Sampras, SB10003,Rafael,Nadal, SB10004,Boris,Becker, SB10005,Ivan,Lendl) &#13;
scala&gt; // Creation of the list from where the RDD is going to be created &#13;
scala&gt; val acBalList = Array("SB10001,50000", "SB10002,12000","SB10003,3000", "SB10004,8500", "SB10005,5000") &#13;
acBalList: Array[String] = Array(SB10001,50000, SB10002,12000, SB10003,3000, SB10004,8500, SB10005,5000) &#13;
scala&gt; // Create the DataFrame &#13;
scala&gt; val acMasterDF = sc.parallelize(acMasterList).map(_.split(",")).map(toAcMaster(_)).toDF() &#13;
acMasterDF: org.apache.spark.sql.DataFrame = [accNo: string, firstName: string ... 1 more field] &#13;
scala&gt; // Create the DataFrame &#13;
scala&gt; val acBalDF = sc.parallelize(acBalList).map(_.split(",")).map(toAcBal(_)).toDF() &#13;
acBalDF: org.apache.spark.sql.DataFrame = [accNo: string, balanceAmount: double] &#13;
scala&gt; // Persist the data of the DataFrame into a Parquet file &#13;
scala&gt; acMasterDF.write.parquet("scala.master.parquet") &#13;
scala&gt; // Persist the data of the DataFrame into a JSON file &#13;
scala&gt; acBalDF.write.json("scalaMaster.json") &#13;
scala&gt; // Read the data into a DataFrame from the Parquet file &#13;
scala&gt; val acMasterDFFromFile = spark.read.parquet("scala.master.parquet") &#13;
acMasterDFFromFile: org.apache.spark.sql.DataFrame = [accNo: string, firstName: string ... 1 more field] &#13;
scala&gt; // Register temporary view in the DataFrame for using it in SQL &#13;
scala&gt; acMasterDFFromFile.createOrReplaceTempView("master") &#13;
scala&gt; // Read the data into a DataFrame from the JSON file &#13;
scala&gt; val acBalDFFromFile = spark.read.json("scalaMaster.json") &#13;
acBalDFFromFile: org.apache.spark.sql.DataFrame = [accNo: string, balanceAmount: double] &#13;
scala&gt; // Register temporary view in the DataFrame for using it in SQL &#13;
scala&gt; acBalDFFromFile.createOrReplaceTempView("balance") &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; acMasterDFFromFile.show &#13;
+-------+---------+--------+ &#13;
|  accNo|firstName|lastName| &#13;
+-------+---------+--------+ &#13;
|SB10001|    Roger| Federer| &#13;
|SB10002|     Pete| Sampras| &#13;
|SB10003|   Rafael|   Nadal| &#13;
|SB10004|    Boris|  Becker| &#13;
|SB10005|     Ivan|   Lendl| &#13;
+-------+---------+--------+ &#13;
scala&gt; acBalDFFromFile.show &#13;
+-------+-------------+ &#13;
|  accNo|balanceAmount| &#13;
+-------+-------------+ &#13;
|SB10001|      50000.0| &#13;
|SB10002|      12000.0| &#13;
|SB10003|       3000.0| &#13;
|SB10004|       8500.0| &#13;
|SB10005|       5000.0| &#13;
+-------+-------------+ &#13;
scala&gt; // Use SQL to create another DataFrame containing the account detail records &#13;
scala&gt; val acDetail = spark.sql("SELECT master.accNo, firstName, lastName, balanceAmount FROM master, balance WHERE master.accNo = balance.accNo ORDER BY balanceAmount DESC") &#13;
acDetail: org.apache.spark.sql.DataFrame = [accNo: string, firstName: string ... 2 more fields] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; acDetail.show &#13;
+-------+---------+--------+-------------+ &#13;
|  accNo|firstName|lastName|balanceAmount| &#13;
+-------+---------+--------+-------------+ &#13;
|SB10001|    Roger| Federer|      50000.0| &#13;
|SB10002|     Pete| Sampras|      12000.0| &#13;
|SB10004|    Boris|  Becker|       8500.0| &#13;
|SB10005|     Ivan|   Lendl|       5000.0| &#13;
|SB10003|   Rafael|   Nadal|       3000.0| &#13;
+-------+---------+--------+-------------+</strong></span>
</pre><p>Continuing from the same Scala REPL session, the following lines of code get the same result through the DataFrame API:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; // Create the DataFrame using API for the account detail records &#13;
scala&gt; val acDetailFromAPI = acMasterDFFromFile.join(acBalDFFromFile, acMasterDFFromFile("accNo") === acBalDFFromFile("accNo"), "inner").sort($"balanceAmount".desc).select(acMasterDFFromFile("accNo"), acMasterDFFromFile("firstName"), acMasterDFFromFile("lastName"), acBalDFFromFile("balanceAmount")) &#13;
acDetailFromAPI: org.apache.spark.sql.DataFrame = [accNo: string, firstName: string ... 2 more fields] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; acDetailFromAPI.show &#13;
+-------+---------+--------+-------------+ &#13;
|  accNo|firstName|lastName|balanceAmount| &#13;
+-------+---------+--------+-------------+ &#13;
|SB10001|    Roger| Federer|      50000.0| &#13;
|SB10002|     Pete| Sampras|      12000.0| &#13;
|SB10004|    Boris|  Becker|       8500.0| &#13;
|SB10005|     Ivan|   Lendl|       5000.0| &#13;
|SB10003|   Rafael|   Nadal|       3000.0| &#13;
+-------+---------+--------+-------------+ &#13;
scala&gt; // Use SQL to create another DataFrame containing the top 3 account detail records &#13;
scala&gt; val acDetailTop3 = spark.sql("SELECT master.accNo, firstName, lastName, balanceAmount FROM master, balance WHERE master.accNo = balance.accNo ORDER BY balanceAmount DESC").limit(3) &#13;
acDetailTop3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [accNo: string, firstName: string ... 2 more fields] &#13;
scala&gt; // Show the first few records of the DataFrame &#13;
scala&gt; acDetailTop3.show &#13;
+-------+---------+--------+-------------+ &#13;
|  accNo|firstName|lastName|balanceAmount| &#13;
+-------+---------+--------+-------------+ &#13;
|SB10001|    Roger| Federer|      50000.0| &#13;
|SB10002|     Pete| Sampras|      12000.0| &#13;
|SB10004|    Boris|  Becker|       8500.0| &#13;
+-------+---------+--------+-------------+</strong></span>
</pre><p>The join type selected in the preceding section of the code is inner join. Instead of that, any other type of join can be used, either through the SQL query way or through the DataFrame API way. In this particular use case, it can be seen that the DataFrame API is becoming a bit clunky, while the SQL query looks very straightforward. The point here is that depending on the situation, in the application code, the SQL query way and the DataFrame API way can be mixed to produce the desired result. The DataFrame <code class="literal">acDetailTop3</code> given in the following scripts is an example of that.</p><p>At the Python REPL prompt, try the following statements: </p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from pyspark.sql import Row &#13;
&gt;&gt;&gt; # Creation of the list from where the RDD is going to be created &#13;
&gt;&gt;&gt; AcMaster = Row('accNo', 'firstName', 'lastName') &#13;
&gt;&gt;&gt; AcBal = Row('accNo', 'balanceAmount') &#13;
&gt;&gt;&gt; acMasterList = ["SB10001,Roger,Federer","SB10002,Pete,Sampras", "SB10003,Rafael,Nadal","SB10004,Boris,Becker", "SB10005,Ivan,Lendl"] &#13;
&gt;&gt;&gt; acBalList = ["SB10001,50000", "SB10002,12000","SB10003,3000", "SB10004,8500", "SB10005,5000"] &#13;
&gt;&gt;&gt; # Create the DataFrame &#13;
&gt;&gt;&gt; acMasterDF = sc.parallelize(acMasterList).map(lambda trans: trans.split(",")).map(lambda r: AcMaster(*r)).toDF() &#13;
&gt;&gt;&gt; acBalDF = sc.parallelize(acBalList).map(lambda trans: trans.split(",")).map(lambda r: AcBal(r[0], float(r[1]))).toDF() &#13;
&gt;&gt;&gt; # Persist the data of the DataFrame into a Parquet file &#13;
&gt;&gt;&gt; acMasterDF.write.parquet("python.master.parquet") &#13;
&gt;&gt;&gt; # Persist the data of the DataFrame into a JSON file &#13;
&gt;&gt;&gt; acBalDF.write.json("pythonMaster.json") &#13;
&gt;&gt;&gt; # Read the data into a DataFrame from the Parquet file &#13;
&gt;&gt;&gt; acMasterDFFromFile = spark.read.parquet("python.master.parquet") &#13;
&gt;&gt;&gt; # Register temporary table in the DataFrame for using it in SQL &#13;
&gt;&gt;&gt; acMasterDFFromFile.createOrReplaceTempView("master") &#13;
&gt;&gt;&gt; # Register temporary table in the DataFrame for using it in SQL &#13;
&gt;&gt;&gt; acBalDFFromFile = spark.read.json("pythonMaster.json") &#13;
&gt;&gt;&gt; # Register temporary table in the DataFrame for using it in SQL &#13;
&gt;&gt;&gt; acBalDFFromFile.createOrReplaceTempView("balance") &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; acMasterDFFromFile.show() &#13;
+-------+---------+--------+ &#13;
|  accNo|firstName|lastName| &#13;
+-------+---------+--------+ &#13;
|SB10001|    Roger| Federer| &#13;
|SB10002|     Pete| Sampras| &#13;
|SB10003|   Rafael|   Nadal| &#13;
|SB10004|    Boris|  Becker| &#13;
|SB10005|     Ivan|   Lendl| &#13;
+-------+---------+--------+ &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; acBalDFFromFile.show() &#13;
+-------+-------------+ &#13;
|  accNo|balanceAmount| &#13;
+-------+-------------+ &#13;
|SB10001|      50000.0| &#13;
|SB10002|      12000.0| &#13;
|SB10003|       3000.0| &#13;
|SB10004|       8500.0| &#13;
|SB10005|       5000.0| &#13;
+-------+-------------+ &#13;
&gt;&gt;&gt; # Use SQL to create another DataFrame containing the account detail records &#13;
&gt;&gt;&gt; acDetail = spark.sql("SELECT master.accNo, firstName, lastName, balanceAmount FROM master, balance WHERE master.accNo = balance.accNo ORDER BY balanceAmount DESC") &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; acDetail.show() &#13;
+-------+---------+--------+-------------+ &#13;
|  accNo|firstName|lastName|balanceAmount| &#13;
+-------+---------+--------+-------------+ &#13;
|SB10001|    Roger| Federer|      50000.0| &#13;
|SB10002|     Pete| Sampras|      12000.0| &#13;
|SB10004|    Boris|  Becker|       8500.0| &#13;
|SB10005|     Ivan|   Lendl|       5000.0| &#13;
|SB10003|   Rafael|   Nadal|       3000.0| &#13;
+-------+---------+--------+-------------+ &#13;
&gt;&gt;&gt; # Create the DataFrame using API for the account detail records &#13;
&gt;&gt;&gt; acDetailFromAPI = acMasterDFFromFile.join(acBalDFFromFile, acMasterDFFromFile.accNo == acBalDFFromFile.accNo).sort(acBalDFFromFile.balanceAmount, ascending=False).select(acMasterDFFromFile.accNo, acMasterDFFromFile.firstName, acMasterDFFromFile.lastName, acBalDFFromFile.balanceAmount) &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; acDetailFromAPI.show() &#13;
+-------+---------+--------+-------------+ &#13;
|  accNo|firstName|lastName|balanceAmount| &#13;
+-------+---------+--------+-------------+ &#13;
|SB10001|    Roger| Federer|      50000.0| &#13;
|SB10002|     Pete| Sampras|      12000.0| &#13;
|SB10004|    Boris|  Becker|       8500.0| &#13;
|SB10005|     Ivan|   Lendl|       5000.0| &#13;
|SB10003|   Rafael|   Nadal|       3000.0| &#13;
+-------+---------+--------+-------------+ &#13;
&gt;&gt;&gt; # Use SQL to create another DataFrame containing the top 3 account detail records &#13;
&gt;&gt;&gt; acDetailTop3 = spark.sql("SELECT master.accNo, firstName, lastName, balanceAmount FROM master, balance WHERE master.accNo = balance.accNo ORDER BY balanceAmount DESC").limit(3) &#13;
&gt;&gt;&gt; # Show the first few records of the DataFrame &#13;
&gt;&gt;&gt; acDetailTop3.show() &#13;
+-------+---------+--------+-------------+ &#13;
|  accNo|firstName|lastName|balanceAmount| &#13;
+-------+---------+--------+-------------+ &#13;
|SB10001|    Roger| Federer|      50000.0| &#13;
|SB10002|     Pete| Sampras|      12000.0| &#13;
|SB10004|    Boris|  Becker|       8500.0| &#13;
+-------+---------+--------+-------------+ &#13;
</strong></span>
</pre><p>In the preceding sections, application of the RDD operations on DataFrame has been demonstrated. This shows the capability of Spark SQL to interoperate with the RDDs and vice versa. In the same way, SQL queries and the DataFrame API can be mixed in to have flexibility to use the easiest method of computation when solving real-world use cases in the applications.</p></div>
<div class="section" title="Introducing datasets"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec28"/>Introducing datasets</h1></div></div></div><p>The Spark programming paradigm has many abstractions to choose from when it comes to developing data processing applications. The fundamentals of Spark programming start with RDDs that can easily deal with unstructured, semi-structured, and structured data. The Spark SQL library offers highly optimized performance when processing structured data. This makes the basic RDDs look inferior in terms of performance. To fill this gap, from Spark 1.6 onwards, a new abstraction, named Dataset, was introduced that complements the RDD-based Spark programming model. It works pretty much the same way as RDD when it comes to Spark transformations and Spark actions, and at the same time, it is highly optimized like the Spark SQL. Dataset API provides strong compile-time type safety when it comes to writing programs and, because of that, the Dataset API is available only in Scala and Java.</p><p>The transaction banking use case discussed in the chapter covering the Spark programming model is taken up again here to elucidate the dataset-based programming model, because this programming model has a very close resemblance to RDD-based programming. The use case mainly deals with a set of banking transaction records and various processing done on those records to extract various information from it. The use case descriptions are not repeated here and it is not difficult to understand by looking at the comments and the code.</p><p>The following code snippet demonstrates the methods used to create Dataset, along with its usage, conversion of RDD to DataFrame, and conversion of DataFrame to dataset. The RDD to DataFrame conversion has already been discussed, but is captured here again to to keep the concepts in context. This is mainly to prove that various programming models in Spark and the data abstractions are highly interoperable.</p><p>At the Scala REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; // Define the case classes for using in conjunction with DataFrames and Dataset &#13;
scala&gt; case class Trans(accNo: String, tranAmount: Double)  &#13;
defined class Trans &#13;
scala&gt; // Creation of the list from where the Dataset is going to be created using a case class. &#13;
scala&gt; val acTransList = Seq(Trans("SB10001", 1000), Trans("SB10002",1200), Trans("SB10003", 8000), Trans("SB10004",400), Trans("SB10005",300), Trans("SB10006",10000), Trans("SB10007",500), Trans("SB10008",56), Trans("SB10009",30),Trans("SB10010",7000), Trans("CR10001",7000), Trans("SB10002",-10)) &#13;
acTransList: Seq[Trans] = List(Trans(SB10001,1000.0), Trans(SB10002,1200.0), Trans(SB10003,8000.0), Trans(SB10004,400.0), Trans(SB10005,300.0), Trans(SB10006,10000.0), Trans(SB10007,500.0), Trans(SB10008,56.0), Trans(SB10009,30.0), Trans(SB10010,7000.0), Trans(CR10001,7000.0), Trans(SB10002,-10.0)) &#13;
scala&gt; // Create the Dataset &#13;
scala&gt; val acTransDS = acTransList.toDS() &#13;
acTransDS: org.apache.spark.sql.Dataset[Trans] = [accNo: string, tranAmount: double] &#13;
scala&gt; acTransDS.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10001|    1000.0| &#13;
|SB10002|    1200.0| &#13;
|SB10003|    8000.0| &#13;
|SB10004|     400.0| &#13;
|SB10005|     300.0| &#13;
|SB10006|   10000.0| &#13;
|SB10007|     500.0| &#13;
|SB10008|      56.0| &#13;
|SB10009|      30.0| &#13;
|SB10010|    7000.0| &#13;
|CR10001|    7000.0| &#13;
|SB10002|     -10.0| &#13;
+-------+----------+ &#13;
scala&gt; // Apply filter and create another Dataset of good transaction records &#13;
scala&gt; val goodTransRecords = acTransDS.filter(_.tranAmount &gt; 0).filter(_.accNo.startsWith("SB")) &#13;
goodTransRecords: org.apache.spark.sql.Dataset[Trans] = [accNo: string, tranAmount: double] &#13;
scala&gt; goodTransRecords.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10001|    1000.0| &#13;
|SB10002|    1200.0| &#13;
|SB10003|    8000.0| &#13;
|SB10004|     400.0| &#13;
|SB10005|     300.0| &#13;
|SB10006|   10000.0| &#13;
|SB10007|     500.0| &#13;
|SB10008|      56.0| &#13;
|SB10009|      30.0| &#13;
|SB10010|    7000.0| &#13;
+-------+----------+ &#13;
scala&gt; // Apply filter and create another Dataset of high value transaction records &#13;
scala&gt; val highValueTransRecords = goodTransRecords.filter(_.tranAmount &gt; 1000) &#13;
highValueTransRecords: org.apache.spark.sql.Dataset[Trans] = [accNo: string, tranAmount: double] &#13;
scala&gt; highValueTransRecords.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10002|    1200.0| &#13;
|SB10003|    8000.0| &#13;
|SB10006|   10000.0| &#13;
|SB10010|    7000.0| &#13;
+-------+----------+ &#13;
scala&gt; // The function that identifies the bad amounts &#13;
scala&gt; val badAmountLambda = (trans: Trans) =&gt; trans.tranAmount &lt;= 0 &#13;
badAmountLambda: Trans =&gt; Boolean = &lt;function1&gt; &#13;
scala&gt; // The function that identifies bad accounts &#13;
scala&gt; val badAcNoLambda = (trans: Trans) =&gt; trans.accNo.startsWith("SB") == false &#13;
badAcNoLambda: Trans =&gt; Boolean = &lt;function1&gt; &#13;
scala&gt; // Apply filter and create another Dataset of bad amount records &#13;
scala&gt; val badAmountRecords = acTransDS.filter(badAmountLambda) &#13;
badAmountRecords: org.apache.spark.sql.Dataset[Trans] = [accNo: string, tranAmount: double] &#13;
scala&gt; badAmountRecords.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10002|     -10.0| &#13;
+-------+----------+ &#13;
scala&gt; // Apply filter and create another Dataset of bad account records &#13;
scala&gt; val badAccountRecords = acTransDS.filter(badAcNoLambda) &#13;
badAccountRecords: org.apache.spark.sql.Dataset[Trans] = [accNo: string, tranAmount: double] &#13;
scala&gt; badAccountRecords.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|CR10001|    7000.0| &#13;
+-------+----------+ &#13;
scala&gt; // Do the union of two Dataset and create another Dataset &#13;
scala&gt; val badTransRecords  = badAmountRecords.union(badAccountRecords) &#13;
badTransRecords: org.apache.spark.sql.Dataset[Trans] = [accNo: string, tranAmount: double] &#13;
scala&gt; badTransRecords.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10002|     -10.0| &#13;
|CR10001|    7000.0| &#13;
+-------+----------+ &#13;
scala&gt; // Calculate the sum &#13;
scala&gt; val sumAmount = goodTransRecords.map(trans =&gt; trans.tranAmount).reduce(_ + _) &#13;
sumAmount: Double = 28486.0 &#13;
scala&gt; // Calculate the maximum &#13;
scala&gt; val maxAmount = goodTransRecords.map(trans =&gt; trans.tranAmount).reduce((a, b) =&gt; if (a &gt; b) a else b) &#13;
maxAmount: Double = 10000.0 &#13;
scala&gt; // Calculate the minimum &#13;
scala&gt; val minAmount = goodTransRecords.map(trans =&gt; trans.tranAmount).reduce((a, b) =&gt; if (a &lt; b) a else b) &#13;
minAmount: Double = 30.0 &#13;
scala&gt; // Convert the Dataset to DataFrame &#13;
scala&gt; val acTransDF = acTransDS.toDF() &#13;
acTransDF: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] &#13;
scala&gt; acTransDF.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10001|    1000.0| &#13;
|SB10002|    1200.0| &#13;
|SB10003|    8000.0| &#13;
|SB10004|     400.0| &#13;
|SB10005|     300.0| &#13;
|SB10006|   10000.0| &#13;
|SB10007|     500.0| &#13;
|SB10008|      56.0| &#13;
|SB10009|      30.0| &#13;
|SB10010|    7000.0| &#13;
|CR10001|    7000.0| &#13;
|SB10002|     -10.0| &#13;
+-------+----------+ &#13;
scala&gt; // Use Spark SQL to find out invalid transaction records &#13;
scala&gt; acTransDF.createOrReplaceTempView("trans") &#13;
scala&gt; val invalidTransactions = spark.sql("SELECT accNo, tranAmount FROM trans WHERE (accNo NOT LIKE 'SB%') OR tranAmount &lt;= 0") &#13;
invalidTransactions: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] &#13;
scala&gt; invalidTransactions.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|CR10001|    7000.0| &#13;
|SB10002|     -10.0| &#13;
+-------+----------+ &#13;
scala&gt; // Interoperability of RDD, DataFrame and Dataset &#13;
scala&gt; // Create RDD &#13;
scala&gt; val acTransRDD = sc.parallelize(acTransList) &#13;
acTransRDD: org.apache.spark.rdd.RDD[Trans] = ParallelCollectionRDD[206] at parallelize at &lt;console&gt;:28 &#13;
scala&gt; // Convert RDD to DataFrame &#13;
scala&gt; val acTransRDDtoDF = acTransRDD.toDF() &#13;
acTransRDDtoDF: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] &#13;
scala&gt; // Convert the DataFrame to Dataset with the type checking &#13;
scala&gt; val acTransDFtoDS = acTransRDDtoDF.as[Trans] &#13;
acTransDFtoDS: org.apache.spark.sql.Dataset[Trans] = [accNo: string, tranAmount: double] &#13;
scala&gt; acTransDFtoDS.show() &#13;
+-------+----------+ &#13;
|  accNo|tranAmount| &#13;
+-------+----------+ &#13;
|SB10001|    1000.0| &#13;
|SB10002|    1200.0| &#13;
|SB10003|    8000.0| &#13;
|SB10004|     400.0| &#13;
|SB10005|     300.0| &#13;
|SB10006|   10000.0| &#13;
|SB10007|     500.0| &#13;
|SB10008|      56.0| &#13;
|SB10009|      30.0| &#13;
|SB10010|    7000.0| &#13;
|CR10001|    7000.0| &#13;
|SB10002|     -10.0| &#13;
+-------+----------+</strong></span>
</pre><p>It is very clear that the dataset-based programming has good applicability in many of the data processing use cases; at the same time, it has got high interoperability with other data processing abstractions within Spark itself.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip28"/>Tip</h3><p>In the preceding code snippet, the DataFrame was converted to Dataset with a type specification <code class="literal">acTransRDDToDF.as[Trans]</code>. This type of conversion is really required when reading data from external data sources such as JSON, Avro, or Parquet files. That is when strong type checking is needed. Typically, structured data is read into DataFrame, and that can be converted to DataSet with strong type safety check like this in one shot: <code class="literal">spark.read.json("/transaction.json").as[Trans]</code>
</p></div></div><p>If the Scala code snippets throughout this chapter are examined, when some of the methods are called on a DataFrame, instead of returning a DataFrame object, an object of type <code class="literal">org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]</code> is returned. This is the important relationship between DataFrame and dataset. In other words, DataFrame is a dataset of type <code class="literal">org.apache.spark.sql.Row</code>. If required, this object of type <code class="literal">org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]</code> can be explicitly converted to DataFrame using the <code class="literal">toDF()</code> method.</p><p>Too many choices confuse everybody. Here in the Spark programming model, the same problem is seen. But it is not as confusing as in many other programming paradigms. Whenever there is a need to process any kind of data with very high flexibility in terms of the data processing requirements and having the lowest API level control such as library development, the RDD-based programming model is ideal. Whenever there is a need to process structured data with flexibility for accessing and processing data with optimized performance across all the supported programming languages, the DataFrame-based Spark SQL programming model is ideal.</p><p>Whenever there is a need to process unstructured data with optimized performance requirements as well as compile-time type safety but not very complex Spark transformations and Spark actions usage requirements, the dataset-based programming model is ideal. At a data processing application development level, if the programming language of choice permits, it is better to use dataset and DataFrame to have better performance.</p></div>
<div class="section" title="Understanding Data Catalogs"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec29"/>Understanding Data Catalogs</h1></div></div></div><p>The previous sections of this chapter covered programming models with DataFrames and datasets. Both of these programming models can deal with structured data. The structured data comes with metadata or the data describing the structure of the data. Spark SQL provides a minimalist API known as Catalog API for data processing applications to query and use the metadata in the applications. The Catalog API exposes a catalog abstraction with many databases in it. For the regular SparkSession, it will have only one database, namely default. But if Spark is used with Hive, then the entire Hive meta store will be available through the Catalog API. The following code snippets demonstrate the usage of Catalog API in Scala and Python.</p><p>Continuing from the same Scala REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; // Get the catalog object from the SparkSession object
scala&gt; val catalog = spark.catalog
catalog: org.apache.spark.sql.catalog.Catalog = org.apache.spark.sql.internal.CatalogImpl@14b8a751
scala&gt; // Get the list of databases
scala&gt; val dbList = catalog.listDatabases()
dbList: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Database] = [name: string, description: string ... 1 more field]
scala&gt; // Display the details of the databases
scala&gt; dbList.select("name", "description", "locationUri").show()<span class="strong"><strong>+-------+----------------+--------------------+</strong></span>
<span class="strong"><strong>| name| description| locationUri|</strong></span>
<span class="strong"><strong>+-------+----------------+--------------------+</strong></span>
<span class="strong"><strong>|default|default database|file:/Users/RajT/...|</strong></span>
<span class="strong"><strong>+-------+----------------+--------------------+</strong></span>
scala&gt; // Display the details of the tables in the database
scala&gt; val tableList = catalog.listTables()
tableList: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Table] = [name: string, database: string ... 3 more fields]
scala&gt; tableList.show()<span class="strong"><strong>+-----+--------+-----------+---------+-----------+</strong></span>
<span class="strong"><strong> | name|database|description|tableType|isTemporary|</strong></span>
<span class="strong"><strong>+-----+--------+-----------+---------+-----------+</strong></span>
<span class="strong"><strong>|trans| null| null|TEMPORARY| true|</strong></span>
<span class="strong"><strong>+-----+--------+-----------+---------+-----------+</strong></span>
scala&gt; // The above list contains the temporary view that was created in the Dataset use case discussed in the previous section
// The views created in the applications can be removed from the database using the Catalog APIscala&gt; catalog.dropTempView("trans")
// List the available tables after dropping the temporary viewscala&gt; val latestTableList = catalog.listTables()
latestTableList: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Table] = [name: string, database: string ... 3 more fields]
scala&gt; latestTableList.show()<span class="strong"><strong>+----+--------+-----------+---------+-----------+</strong></span>
<span class="strong"><strong>|name|database|description|tableType|isTemporary|</strong></span>
<span class="strong"><strong>+----+--------+-----------+---------+-----------+</strong></span>
<span class="strong"><strong>+----+--------+-----------+---------+-----------+</strong></span>
</strong></span>
</pre><p>Similarly, the Catalog API can be used from Python code as well. Since the dataset example was not applicable in Python, the table list will be empty. At the Python REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; #Get the catalog object from the SparkSession object
&gt;&gt;&gt; catalog = spark.catalog
&gt;&gt;&gt; #Get the list of databases and their details.
&gt;&gt;&gt; catalog.listDatabases()   [Database(name='default', description='default database', locationUri='file:/Users/RajT/source-code/spark-source/spark-2.0/spark-warehouse')]&#13;
// Display the details of the tables in the database
&gt;&gt;&gt; catalog.listTables()
&gt;&gt;&gt; []</strong></span>
</pre><p>The Catalog API is very handy when writing data processing applications with the ability to process data dynamically, based on the contents in the meta store, especially when using it in conjunction with Hive.</p></div>
<div class="section" title="References"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec30"/>References</h1></div></div></div><p>For more information you can refer to:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://amplab.cs.berkeley.edu/wp-content/uploads/2015/03/SparkSQLSigmod2015.pdf">https://amplab.cs.berkeley.edu/wp-content/uploads/2015/03/SparkSQLSigmod2015.pdf</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://pandas.pydata.org/">http://pandas.pydata.org/</a></li></ul></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec31"/>Summary</h1></div></div></div><p>Spark SQL is a very highly useful library on top of the Spark core infrastructure. This library makes the Spark programming more inclusive to a wider group of programmers who are well versed with the imperative style of programming but not as competent in functional programming. Apart from this, Spark SQL is the best library to process structured data in the Spark family of data processing libraries. Spark SQL-based data processing application programs can be written with SQL-like queries or DSL style imperative programs of DataFrame API. This chapter has also demonstrated various strategies of mixing RDD and DataFrames, mixing SQL-like queries and DataFrame API. This gives amazing flexibility for the application developers to write data processing programs in the way they are most comfortable with, or that is more appropriate to the use cases, and at the same time, without compromising performance.</p><p>The Dataset API is as the next generation of programming model based on dataset in Spark, providing optimized performance and compile-time type safety.</p><p>The Catalog API comes as a very handy tool to process data dynamically, based on the contents of the meta store.</p><p>R is the language of data scientists. Till the support of R as a programming language in Spark SQL was available, major distributed data processing was not easy for them. Now, using R as a language of choice, they can seamlessly write distributed data processing applications as if they are using an R data frame from their individual machines. The next chapter is going to discuss the use of R to do data processing in Spark SQL.</p></div></body></html>