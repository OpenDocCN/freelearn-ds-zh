<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch01"/>Chapter 1. Tokenizing Text and WordNet Basics</h1></div></div></div><p>In this chapter, we will cover:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Tokenizing text into sentences</li><li class="listitem" style="list-style-type: disc">Tokenizing sentences into words</li><li class="listitem" style="list-style-type: disc">Tokenizing sentences using regular expressions</li><li class="listitem" style="list-style-type: disc">Filtering stopwords in a tokenized sentence</li><li class="listitem" style="list-style-type: disc">Looking up synsets for a word in WordNet</li><li class="listitem" style="list-style-type: disc">Looking up lemmas and synonyms in WordNet</li><li class="listitem" style="list-style-type: disc">Calculating WordNet synset similarity</li><li class="listitem" style="list-style-type: disc">Discovering word collocations</li></ul></div><div><div><div><div><h1 class="title"><a id="ch01lvl1sec07"/>Introduction</h1></div></div></div><a id="id0" class="indexterm"/><p>
<strong>NLTK</strong> is the <strong>Natural Language Toolkit</strong>, a comprehensive Python library for natural language processing and text analytics. Originally designed for teaching, it has been adopted in the industry for research and development due to its usefulness and breadth of coverage.</p><p>This chapter will cover the basics of tokenizing text and using WordNet. <strong>Tokenization</strong>
<a id="id1" class="indexterm"/> is a method of breaking up a piece of text into many pieces, and is an essential first step for recipes in later chapters.</p><a id="id2" class="indexterm"/><p>
<strong>WordNet</strong> is a dictionary designed for programmatic access by natural language processing systems. NLTK includes a WordNet corpus reader, which we will use to access and explore WordNet. We'll be using WordNet again in later chapters, so it's important to familiarize yourself with the basics first.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec08"/>Tokenizing text into sentences</h1></div></div></div><a id="id3" class="indexterm"/><a id="id4" class="indexterm"/><p>Tokenization is the process of splitting a string into a list of pieces, or <em>tokens</em>. We'll start by splitting a paragraph into a list of sentences.</p><div><div><div><div><h2 class="title"><a id="ch01lvl2sec04"/>Getting ready</h2></div></div></div><a id="id5" class="indexterm"/><p>Installation instructions for NLTK are available at <a class="ulink" href="http://www.nltk.org/download">http://www.nltk.org/download</a> and the latest version as of this writing is 2.0b9. NLTK requires Python 2.4 or higher, but is <strong>not compatible with Python 3.0</strong>. The <strong>recommended Python version is 2.6</strong>.</p><a id="id6" class="indexterm"/><a id="id7" class="indexterm"/><p>Once you've installed NLTK, you'll also need to install the data by following the instructions at <a class="ulink" href="http://www.nltk.org/data">http://www.nltk.org/data</a>. We recommend installing everything, as we'll be using a number of corpora and pickled objects. The data is installed in a data directory, which on Mac and Linux/Unix is usually <code class="literal">/usr/share/nltk_data</code>, or on Windows is <code class="literal">C:\nltk_data</code>. Make sure that <code class="literal">tokenizers/punkt.zip</code> is in the data directory and has been unpacked so that there's a file at <code class="literal">tokenizers/punkt/english.pickle</code>.</p><p>Finally, to run the code examples, you'll need to start a Python console. Instructions on how to do so are available at <a class="ulink" href="http://www.nltk.org/getting-started">http://www.nltk.org/getting-started</a>. For Mac with Linux/Unix users, you can open a terminal and type <strong>python</strong>.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec05"/>How to do it...</h2></div></div></div><p>Once NLTK is installed and you have a Python console running, we can start by creating a paragraph of text:</p><div><pre class="programlisting">&gt;&gt;&gt; para = "Hello World. It's good to see you. Thanks for buying this book."</pre></div><p>Now we want to split <code class="literal">para</code> into sentences. First we need to import the sentence tokenization function, and then we can call it with the paragraph as an argument.</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.tokenize import sent_tokenize
&gt;&gt;&gt; sent_tokenize(para)
['Hello World.', "It's good to see you.", 'Thanks for buying this book.']</pre></div><p>So now we have a list of sentences that we can use for further processing.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec06"/>How it works...</h2></div></div></div><a id="id8" class="indexterm"/><p>
<code class="literal">sent_tokenize</code> uses an instance of <a id="id9" class="indexterm"/>
<code class="literal">PunktSentenceTokenizer</code> from the <a id="id10" class="indexterm"/>
<code class="literal">nltk.tokenize.punkt</code> module. This instance has already been trained on and works well for many European languages. So it knows what punctuation and characters mark the end of a sentence and the beginning of a new sentence.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec07"/>There's more...</h2></div></div></div><p>The instance used in <code class="literal">sent_tokenize()</code> is actually loaded on demand from a pickle file. So if you're going to be tokenizing a lot of sentences, it's more efficient to load the <code class="literal">PunktSentenceTokenizer</code> once, and call its <code class="literal">tokenize()</code> method instead.</p><div><pre class="programlisting">&gt;&gt;&gt; import nltk.data
&gt;&gt;&gt; tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
&gt;&gt;&gt; tokenizer.tokenize(para)
['Hello World.', "It's good to see you.", 'Thanks for buying this book.']</pre></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec01"/>Other languages</h3></div></div></div><p>If you want to tokenize sentences in languages other than English, you can load one of the other pickle files in <code class="literal">tokenizers/punkt</code> and use it just like the English sentence tokenizer. Here's an example for Spanish:</p><div><pre class="programlisting">&gt;&gt;&gt; spanish_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')
&gt;&gt;&gt; spanish_tokenizer.tokenize('Hola amigo. Estoy bien.')</pre></div></div></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec08"/>See also</h2></div></div></div><p>In the next recipe, we'll learn how to split sentences into individual words. After that, we'll cover how to use regular expressions for tokenizing text.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec09"/>Tokenizing sentences into words</h1></div></div></div><a id="id11" class="indexterm"/><p>In this recipe, we'll split a sentence into individual words. The simple task of creating a list of words from a string is an essential part of all text processing.</p><div><div><div><div><h2 class="title"><a id="ch01lvl2sec09"/>How to do it...</h2></div></div></div><a id="id12" class="indexterm"/><a id="id13" class="indexterm"/><p>Basic word tokenization is very simple: use the <code class="literal">word_tokenize()</code> function:</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.tokenize import word_tokenize
&gt;&gt;&gt; word_tokenize('Hello World.')
['Hello', 'World', '.']</pre></div></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec10"/>How it works...</h2></div></div></div><a id="id14" class="indexterm"/><p>
<code class="literal">word_tokenize()</code> is a wrapper function that calls <code class="literal">tokenize()</code> on an instance of the <code class="literal">TreebankWordTokenizer</code>
<a id="id15" class="indexterm"/>. It's equivalent to the following:</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.tokenize import TreebankWordTokenizer
&gt;&gt;&gt; tokenizer = TreebankWordTokenizer()
&gt;&gt;&gt; tokenizer.tokenize('Hello World.')
['Hello', 'World', '.']</pre></div><p>It works by separating words using spaces and punctuation. And as you can see, it does not discard the punctuation, allowing you to decide what to do with it.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec11"/>There's more...</h2></div></div></div><p>Ignoring the obviously named <a id="id16" class="indexterm"/>
<code class="literal">WhitespaceTokenizer</code> and <code class="literal">SpaceTokenizer</code>
<a id="id17" class="indexterm"/>, there are two other word tokenizers worth looking at: <code class="literal">PunktWordTokenizer</code> and <code class="literal">WordPunctTokenizer</code>. These differ from the <code class="literal">TreebankWordTokenizer</code> by how they handle punctuation and contractions, but they all inherit from <code class="literal">TokenizerI</code>. The inheritance tree looks like this:</p><div><img src="img/3609OS_01_01.jpg" alt="There's more..."/></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec02"/>Contractions</h3></div></div></div><a id="id18" class="indexterm"/><p>
<code class="literal">TreebankWordTokenizer</code> uses conventions found in the Penn Treebank corpus, which we'll be using for training in <a class="link" href="ch04.html" title="Chapter 4. Part-of-Speech Tagging">Chapter 4</a>, <em>Part-of-Speech Tagging</em> and <a class="link" href="ch05.html" title="Chapter 5. Extracting Chunks">Chapter 5</a>, <em>Extracting Chunks</em>. One of these conventions is to separate contractions. For example:</p><div><pre class="programlisting">&gt;&gt;&gt; word_tokenize("can't")
['ca', "n't"]</pre></div><p>If you find this convention unacceptable, then read on for alternatives, and see the next recipe for tokenizing with regular expressions.</p></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec03"/>PunktWordTokenizer</h3></div></div></div><a id="id19" class="indexterm"/><p>An alternative word tokenizer is the <code class="literal">PunktWordTokenizer</code>. It splits on punctuation, but keeps it with the word instead of creating separate tokens.</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.tokenize import PunktWordTokenizer
&gt;&gt;&gt; tokenizer = PunktWordTokenizer()
&gt;&gt;&gt; tokenizer.tokenize("Can't is a contraction.")
['Can', "'t", 'is', 'a', 'contraction.']</pre></div></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec04"/>WordPunctTokenizer</h3></div></div></div><a id="id20" class="indexterm"/><p>Another alternative word tokenizer is <code class="literal">WordPunctTokenizer</code>. It splits all punctuations into separate tokens.</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.tokenize import WordPunctTokenizer
&gt;&gt;&gt; tokenizer = WordPunctTokenizer()
&gt;&gt;&gt; tokenizer.tokenize("Can't is a contraction.")
['Can', "'", 't', 'is', 'a', 'contraction', '.']</pre></div></div></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec12"/>See also</h2></div></div></div><p>For more control over word tokenization, you'll want to read the next recipe to learn how to use regular expressions and the <code class="literal">RegexpTokenizer</code> for tokenization.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec10"/>Tokenizing sentences using regular expressions</h1></div></div></div><a id="id21" class="indexterm"/><a id="id22" class="indexterm"/><p>Regular expression can be used if you want complete control over how to tokenize text. As regular expressions can get complicated very quickly, we only recommend using them if the word tokenizers covered in the previous recipe are unacceptable.</p><div><div><div><div><h2 class="title"><a id="ch01lvl2sec13"/>Getting ready</h2></div></div></div><p>First you need to decide how you want to tokenize a piece of text, as this will determine how you construct your regular expression. The choices are:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Match on the tokens</li><li class="listitem" style="list-style-type: disc">Match on the separators, or gaps</li></ul></div><p>We'll start with an example of the first, matching alphanumeric tokens plus single quotes so that we don't split up contractions.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec14"/>How to do it...</h2></div></div></div><a id="id23" class="indexterm"/><a id="id24" class="indexterm"/><p>We'll create an instance of the <code class="literal">RegexpTokenizer</code>, giving it a regular expression string to use for matching tokens.</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.tokenize import RegexpTokenizer
&gt;&gt;&gt; tokenizer = RegexpTokenizer("[\w']+")
&gt;&gt;&gt; tokenizer.tokenize("Can't is a contraction.")
["Can't", 'is', 'a', 'contraction']</pre></div><p>There's also a simple helper function you can use in case you don't want to instantiate the class.</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.tokenize import regexp_tokenize
&gt;&gt;&gt; regexp_tokenize("Can't is a contraction.", "[\w']+")
["Can't", 'is', 'a', 'contraction']</pre></div><p>Now we finally have something that can treat contractions as whole words, instead of splitting them into tokens.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec15"/>How it works...</h2></div></div></div><a id="id25" class="indexterm"/><p>The <code class="literal">RegexpTokenizer</code> works by compiling your pattern, then calling <a id="id26" class="indexterm"/>
<code class="literal">re.findall()</code> on your text. You could do all this yourself using the <code class="literal">re</code> module, but the <code class="literal">RegexpTokenizer</code> implements the <code class="literal">TokenizerI</code> interface, just like all the word tokenizers from the previous recipe. This means it can be used by other parts of the NLTK package, such as corpus readers, which we'll cover in detail in <a class="link" href="ch03.html" title="Chapter 3. Creating Custom Corpora">Chapter 3</a>, <em>Creating Custom Corpora</em>. Many corpus readers need a way to tokenize the text they're reading, and can take optional keyword arguments specifying an instance of a <code class="literal">TokenizerI</code> subclass. This way, you have the ability to provide your own tokenizer instance if the default tokenizer is unsuitable.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec16"/>There's more...</h2></div></div></div><p>
<code class="literal">RegexpTokenizer</code> can also work by matching the gaps, instead of the tokens. Instead of using <code class="literal">re.findall()</code>, the <code class="literal">RegexpTokenizer</code> will use <code class="literal">re.split()</code>. This is how the <code class="literal">BlanklineTokenizer</code> in <code class="literal">nltk.tokenize</code> is implemented.</p><div><div><div><div><h3 class="title"><a id="ch01lvl3sec05"/>Simple whitespace tokenizer</h3></div></div></div><a id="id27" class="indexterm"/><p>Here's a simple example of using the <code class="literal">RegexpTokenizer</code> to tokenize on whitespace:</p><div><pre class="programlisting">&gt;&gt;&gt; tokenizer = RegexpTokenizer('\s+', gaps=True)
&gt;&gt;&gt; tokenizer.tokenize("Can't is a contraction.")
<a id="id28" class="indexterm"/> ["Can't", 'is', 'a', 'contraction.']</pre></div><p>Notice that punctuation still remains in the tokens.</p></div></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec17"/>See also</h2></div></div></div><p>For simpler word tokenization, see the previous recipe.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec11"/>Filtering stopwords in a tokenized sentence</h1></div></div></div><a id="id29" class="indexterm"/><a id="id30" class="indexterm"/><a id="id31" class="indexterm"/><p>
<strong>Stopwords</strong> are common words that generally do not contribute to the meaning of a sentence, at least for the purposes of information retrieval and natural language processing. Most search engines will filter stopwords out of search queries and documents in order to save space in their index.</p><div><div><div><div><h2 class="title"><a id="ch01lvl2sec18"/>Getting ready</h2></div></div></div><p>NLTK comes with a stopwords corpus that contains word lists for many languages. Be sure to unzip the datafile so NLTK can find these word lists in <code class="literal">nltk_data/corpora/stopwords/</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec19"/>How to do it...</h2></div></div></div><p>We're going to create a set of all English stopwords, then use it to filter stopwords from a sentence.</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus import stopwords
&gt;&gt;&gt; english_stops = set(stopwords.words('english'))
&gt;&gt;&gt; words = ["Can't", 'is', 'a', 'contraction']
&gt;&gt;&gt; [word for word in words if word not in english_stops]
["Can't", 'contraction']</pre></div></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec20"/>How it works...</h2></div></div></div><p>The stopwords corpus is an instance of <code class="literal">nltk.corpus.reader.WordListCorpusReader</code>. As such, it has a <code class="literal">words()</code> method that can take a single argument for the file ID, which in this case is <code class="literal">'english'</code>, referring to a file containing a list of English stopwords. You could also call <code class="literal">stopwords.words()</code> with no argument to get a list of all stopwords in every language available.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec21"/>There's more...</h2></div></div></div><p>You can see the list of all English stopwords using <code class="literal">stopwords.words('english')</code> or by examining the word list file at <code class="literal">nltk_data/corpora/stopwords/english</code>. There are also stopword lists for many other languages. You can see the complete list of languages using the <code class="literal">fileids()</code> method:</p><div><pre class="programlisting">&gt;&gt;&gt; stopwords.fileids()
['danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'portuguese', 'russian', 'spanish', 'swedish', 'turkish']</pre></div><p>Any of these <code class="literal">fileids</code> can be used as an argument to the <code class="literal">words()</code> method to get a list of stopwords for that language.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec22"/>See also</h2></div></div></div><p>If you'd like to create your own stopwords corpus, see the <em>Creating a word list corpus</em> recipe in <a class="link" href="ch03.html" title="Chapter 3. Creating Custom Corpora">Chapter 3</a>, <em>Creating Custom Corpora</em>, to learn how to use the <code class="literal">WordListCorpusReader</code>. We'll also be using stopwords in the <em>Discovering word collocations</em> recipe, later in this chapter.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec12"/>Looking up synsets for a word in WordNet</h1></div></div></div><a id="id32" class="indexterm"/><a id="id33" class="indexterm"/><p>WordNet is a lexical database for the English language. In other words, it's a dictionary designed specifically for natural language processing.</p><a id="id34" class="indexterm"/><p>NLTK comes with a simple interface for looking up words in WordNet. What you get is a list of <strong>synset</strong> instances, which are groupings of synonymous words that express the same concept. Many words have only one synset, but some have several. We'll now explore a single synset, and in the next recipe, we'll look at several in more detail.</p><div><div><div><div><h2 class="title"><a id="ch01lvl2sec23"/>Getting ready</h2></div></div></div><p>Be sure you've unzipped the <code class="literal">wordnet</code> corpus in <code class="literal">nltk_data/corpora/wordnet</code>. This will allow the <code class="literal">WordNetCorpusReader</code> to access it.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec24"/>How to do it...</h2></div></div></div><p>Now we're going to lookup the <code class="literal">synset</code> for <code class="literal">cookbook</code>, and explore some of the properties and methods of a synset.</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus import wordnet
&gt;&gt;&gt; syn = wordnet.synsets('cookbook')[0]
&gt;&gt;&gt; syn.name
'cookbook.n.01'
&gt;&gt;&gt; syn.definition
'a book of recipes and cooking directions'</pre></div></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec25"/>How it works...</h2></div></div></div><a id="id35" class="indexterm"/><p>You can look up any word in WordNet using <code class="literal">wordnet.synsets(word)</code> to get a list of synsets. The list may be empty if the word is not found. The list may also have quite a few elements, as some words can have many possible meanings and therefore many synsets.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec26"/>There's more...</h2></div></div></div><p>Each synset in the list has a number of attributes you can use to learn more about it. The <code class="literal">name</code> attribute will give you a unique name for the synset, which you can use to get the synset directly.</p><div><pre class="programlisting">&gt;&gt;&gt; wordnet.synset('cookbook.n.01')
Synset('cookbook.n.01')</pre></div><p>The <code class="literal">definition</code> attribute should be self-explanatory. Some synsets also have an <code class="literal">examples</code> attribute, which contains a list of phrases that use the word in context.</p><div><pre class="programlisting">&gt;&gt;&gt; wordnet.synsets('cooking')[0].examples
['cooking can be a great art', 'people are needed who have experience in cookery', 'he left the preparation of meals to his wife']</pre></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec06"/>Hypernyms</h3></div></div></div><a id="id36" class="indexterm"/><p>Synsets are organized in a kind of inheritance tree. More abstract terms are known as <a id="id37" class="indexterm"/>
<strong>hypernyms</strong> and more specific terms are <strong>hyponyms</strong>
<a id="id38" class="indexterm"/>. This tree can be traced all the way up to a root hypernym.</p><p>Hypernyms provide a way to categorize and group words based on their similarity to each other. The synset similarity recipe details the functions used to calculate similarity based on the distance between two words in the hypernym tree.</p><div><pre class="programlisting">&gt;&gt;&gt; syn.hypernyms()
[Synset('reference_book.n.01')]
&gt;&gt;&gt; syn.hypernyms()[0].hyponyms()
[Synset('encyclopedia.n.01'), Synset('directory.n.01'), Synset('source_book.n.01'), Synset('handbook.n.01'), Synset('instruction_book.n.01'), Synset('cookbook.n.01'), Synset('annual.n.02'), Synset('atlas.n.02'), Synset('wordbook.n.01')]
&gt;&gt;&gt; syn.root_hypernyms()
[Synset('entity.n.01')]</pre></div><p>As you can see, <code class="literal">reference book</code> is a <em>hypernym</em> of <code class="literal">cookbook</code>, but <code class="literal">cookbook</code> is only one of many <em>hyponyms</em> of <code class="literal">reference book</code>. All these types of books have the same root hypernym, <code class="literal">entity</code>, one of the most abstract terms in the English language. You can trace the entire path from <code class="literal">entity</code> down to <code class="literal">cookbook</code> using the <code class="literal">hypernym_paths()</code> method.</p><div><pre class="programlisting">&gt;&gt;&gt; syn.hypernym_paths()
[[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('artifact.n.01'), Synset('creation.n.02'), Synset('product.n.02'), Synset('work.n.02'), Synset('publication.n.01'), Synset('book.n.01'), Synset('reference_book.n.01'), Synset('cookbook.n.01')]]</pre></div><p>This method returns a list of lists, where each list starts at the root hypernym and ends with the original <code class="literal">Synset</code>. Most of the time you'll only get one nested list of synsets.</p></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec07"/>Part-of-speech (POS)</h3></div></div></div><p>You can also look up a simplified part-of-speech tag.</p><div><pre class="programlisting">&gt;&gt;&gt; syn.pos
'n'</pre></div><a id="id39" class="indexterm"/><a id="id40" class="indexterm"/><p>There are four common POS found in WordNet.</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Part-of-speech</p>
</th><th style="text-align: left" valign="bottom">
<p>Tag</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Noun</p>
</td><td style="text-align: left" valign="top">
<p>n</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Adjective</p>
</td><td style="text-align: left" valign="top">
<p>a</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Adverb</p>
</td><td style="text-align: left" valign="top">
<p>r</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Verb</p>
</td><td style="text-align: left" valign="top">
<p>v</p>
</td></tr></tbody></table></div><a id="id41" class="indexterm"/><p>These POS tags can be used for looking up specific <code class="literal">synsets</code> for a word. For example, the word <code class="literal">great</code> can be used as a noun or an adjective. In WordNet, <code class="literal">great</code> has one noun synset and six adjective synsets.</p><div><pre class="programlisting">&gt;&gt;&gt; len(wordnet.synsets('great'))
7
&gt;&gt;&gt; len(wordnet.synsets('great', pos='n'))
1
&gt;&gt;&gt; len(wordnet.synsets('great', pos='a'))
6</pre></div><p>These POS tags will be referenced more in the <em>Using WordNet for Tagging</em> recipe of <a class="link" href="ch04.html" title="Chapter 4. Part-of-Speech Tagging">Chapter 4</a>, <em>Part-of-Speech Tagging</em>.</p></div></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec27"/>See also</h2></div></div></div><p>In the next two recipes, we'll explore lemmas and how to calculate synset similarity. In <a class="link" href="ch02.html" title="Chapter 2. Replacing and Correcting Words">Chapter 2</a>, <em>Replacing and Correcting Words</em>, we'll use WordNet for lemmatization, synonym replacement, and then explore the use of antonyms.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec13"/>Looking up lemmas and synonyms in WordNet</h1></div></div></div><a id="id42" class="indexterm"/><a id="id43" class="indexterm"/><a id="id44" class="indexterm"/><a id="id45" class="indexterm"/><p>Building on the previous recipe, we can also look up lemmas in WordNet to find <strong>synonyms</strong> of a word. <a id="id46" class="indexterm"/>A <strong>lemma</strong> (in linguistics) is the canonical form, or morphological form, of a word.</p><div><div><div><div><h2 class="title"><a id="ch01lvl2sec28"/>How to do it...</h2></div></div></div><p>In the following block of code, we'll find that there are two lemmas for the <code class="literal">cookbook synset</code> by using the <code class="literal">lemmas</code> attribute:</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus import wordnet
&gt;&gt;&gt; syn = wordnet.synsets('cookbook')[0]
&gt;&gt;&gt; lemmas = syn.lemmas
&gt;&gt;&gt; len(lemmas)
2
&gt;&gt;&gt; lemmas[0].name
'cookbook'
&gt;&gt;&gt; lemmas[1].name
'cookery_book'
&gt;&gt;&gt; lemmas[0].synset == lemmas[1].synset
True</pre></div></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec29"/>How it works...</h2></div></div></div><p>As you can see, <code class="literal">cookery_book</code> and <code class="literal">cookbook</code> are two distinct <code class="literal">lemmas</code> in the same <code class="literal">synset</code>. In fact, a lemma can only belong to a single synset. In this way, a synset represents a group of lemmas that all have the same meaning, while a lemma represents a distinct word form.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec30"/>There's more...</h2></div></div></div><p>Since lemmas in a synset all have the same meaning, they can be treated as synonyms. So if you wanted to get all synonyms for a <code class="literal">synset</code>, you could do:</p><div><pre class="programlisting">&gt;&gt;&gt; [lemma.name for lemma in syn.lemmas]
['cookbook', 'cookery_book']</pre></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec08"/>All possible synonyms</h3></div></div></div><a id="id47" class="indexterm"/><a id="id48" class="indexterm"/><a id="id49" class="indexterm"/><a id="id50" class="indexterm"/><p>As mentioned before, many words have multiple <code class="literal">synsets</code> because the word can have different meanings depending on the context. But let's say you didn't care about the context, and wanted to get all possible synonyms for a word.</p><div><pre class="programlisting">&gt;&gt;&gt; synonyms = []
&gt;&gt;&gt; for syn in wordnet.synsets('book'):
...     for lemma in syn.lemmas:
...         synonyms.append(lemma.name)
&gt;&gt;&gt; len(synonyms)
38</pre></div><p>As you can see, there appears to be 38 possible synonyms for the word <code class="literal">book</code>. But in fact, some are verb forms, and many are just different usages of <code class="literal">book</code>. Instead, if we take the set of synonyms, there are fewer unique words.</p><div><pre class="programlisting">&gt;&gt;&gt; len(set(synonyms))
25</pre></div></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec09"/>Antonyms</h3></div></div></div><a id="id51" class="indexterm"/><p>Some lemmas also have <strong>antonyms</strong>. The word <code class="literal">good</code>, for example, has 27 <code class="literal">synsets</code>, five of which have <code class="literal">lemmas</code> with antonyms.</p><div><pre class="programlisting">&gt;&gt;&gt; gn2 = wordnet.synset('good.n.02')
&gt;&gt;&gt; gn2.definition
'moral excellence or admirableness'
&gt;&gt;&gt; evil = gn2.lemmas[0].antonyms()[0]
&gt;&gt;&gt; evil.name
'evil'
&gt;&gt;&gt; evil.synset.definition
'the quality of being morally wrong in principle or practice'
&gt;&gt;&gt; ga1 = wordnet.synset('good.a.01')
&gt;&gt;&gt; ga1.definition
'having desirable or positive qualities especially those suitable for a thing specified'
&gt;&gt;&gt; bad = ga1.lemmas[0].antonyms()[0]
&gt;&gt;&gt; bad.name
'bad'
&gt;&gt;&gt; bad.synset.definition
'having undesirable or negative qualities'</pre></div><a id="id52" class="indexterm"/><p>The <code class="literal">antonyms()</code> method returns a list of <code class="literal">lemmas</code>. In the first case here, we see that the second <code class="literal">synset</code> for <code class="literal">good</code> as a noun is defined as <code class="literal">moral excellence</code>, and its first antonym is <code class="literal">evil</code>, defined as <code class="literal">morally wrong</code>. In the second case, when <code class="literal">good</code> is used as an adjective to describe positive qualities, the first antonym is <code class="literal">bad</code>, which describes negative qualities.</p></div></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec31"/>See also</h2></div></div></div><p>In the next recipe, we'll learn how to calculate <code class="literal">synset</code> similarity. Then in <a class="link" href="ch02.html" title="Chapter 2. Replacing and Correcting Words">Chapter 2</a>, <em>Replacing and Correcting Words</em>, we'll revisit lemmas for lemmatization, synonym replacement, and antonym replacement.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec14"/>Calculating WordNet synset similarity</h1></div></div></div><a id="id53" class="indexterm"/><p>Synsets are organized in a <em>hypernym</em> tree. This tree can be used for reasoning about the similarity between the synsets it contains. Two synsets are more similar, the closer they are in the tree.</p><div><div><div><div><h2 class="title"><a id="ch01lvl2sec32"/>How to do it...</h2></div></div></div><p>If you were to look at all the hyponyms of <code class="literal">reference book</code> (which is the hypernym of <code class="literal">cookbook</code>) you'd see that one of them is <code class="literal">instruction_book</code>. These seem intuitively very similar to <code class="literal">cookbook</code>, so let's see what WordNet similarity has to say about it.</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus import wordnet
&gt;&gt;&gt; cb = wordnet.synset('cookbook.n.01')
&gt;&gt;&gt; ib = wordnet.synset('instruction_book.n.01')
&gt;&gt;&gt; cb.wup_similarity(ib)
0.91666666666666663</pre></div><p>So they are over 91% similar!</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec33"/>How it works...</h2></div></div></div><p>
<code class="literal">wup_similarity</code>
<a id="id54" class="indexterm"/> is short for <em>Wu-Palmer Similarity</em>, which is a scoring method based on how similar the word senses are and where the synsets occur relative to each other in the hypernym tree. One of the core metrics used to calculate similarity is the shortest path distance between the two synsets and their common hypernym.</p><div><pre class="programlisting">&gt;&gt;&gt; ref = cb.hypernyms()[0]
&gt;&gt;&gt; cb.shortest_path_distance(ref)
1
&gt;&gt;&gt; ib.shortest_path_distance(ref)
1
&gt;&gt;&gt; cb.shortest_path_distance(ib)
2</pre></div><a id="id55" class="indexterm"/><p>So <code class="literal">cookbook</code> and <code class="literal">instruction book</code> must be very similar, because they are only one step away from the same hypernym, <code class="literal">reference book</code>, and therefore only two steps away from each other.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec34"/>There's more...</h2></div></div></div><p>Let's look at two dissimilar words to see what kind of score we get. We'll compare <code class="literal">dog</code> with <code class="literal">cookbook</code>, two seemingly very different words.</p><div><pre class="programlisting">&gt;&gt;&gt; dog = wordnet.synsets('dog')[0]
&gt;&gt;&gt; dog.wup_similarity(cb)
0.38095238095238093</pre></div><p>Wow, <code class="literal">dog</code> and <code class="literal">cookbook</code> are apparently 38% similar! This is because they share common hypernyms farther up the tree.</p><div><pre class="programlisting">&gt;&gt;&gt; dog.common_hypernyms(cb)
[Synset('object.n.01'), Synset('whole.n.02'), Synset('physical_entity.n.01'), Synset('entity.n.01')]</pre></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec10"/>Comparing verbs</h3></div></div></div><p>The previous comparisons were all between nouns, but the same can be done for verbs as well.</p><div><pre class="programlisting">&gt;&gt;&gt; cook = wordnet.synset('cook.v.01')
&gt;&gt;&gt; bake = wordnet.synset('bake.v.02')
&gt;&gt;&gt; cook.wup_similarity(bake)
0.75</pre></div><a id="id56" class="indexterm"/><p>The previous synsets were obviously handpicked for demonstration, and the reason is that the hypernym tree for verbs has a lot more breadth and a lot less depth. While most nouns can be traced up to <code class="literal">object</code>, thereby providing a basis for similarity, many verbs do not share common hypernyms, making WordNet unable to calculate similarity. For example, if you were to use the <code class="literal">synset</code> for <code class="literal">bake.v.01</code> here, instead of <code class="literal">bake.v.02</code>, the return value would be <code class="literal">None</code>. This is because the root hypernyms of the two synsets are different, with no overlapping paths. For this reason, you also cannot calculate similarity between words with different parts of speech.</p></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec11"/>Path and LCH similarity</h3></div></div></div><p>Two other similarity comparisons are the<a id="id57" class="indexterm"/> path similarity and <a id="id58" class="indexterm"/>
<strong>Leacock Chodorow (LCH)</strong> similarity.</p><div><pre class="programlisting">&gt;&gt;&gt; cb.path_similarity(ib)
0.33333333333333331
&gt;&gt;&gt; cb.path_similarity(dog)
0.071428571428571425
&gt;&gt;&gt; cb.lch_similarity(ib)
2.5389738710582761
&gt;&gt;&gt; cb.lch_similarity(dog)
0.99852883011112725</pre></div><p>As you can see, the number ranges are very different for these scoring methods, which is why we prefer the<a id="id59" class="indexterm"/> <code class="literal">wup_similarity()</code> method.</p></div></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec35"/>See also</h2></div></div></div><p>The recipe on <em>Looking up synsets for a word in WordNet,</em> discussed earlier in this chapter, has more details about hypernyms and the hypernym tree.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec15"/>Discovering word collocations</h1></div></div></div><a id="id60" class="indexterm"/><p>
<strong>Collocations</strong> are two or more words that tend to appear frequently together, such as "United States". Of course, there are many other words that can come after "United", for example "United Kingdom", "United Airlines", and so on. As with many aspects of natural language processing, context is very important, and for collocations, context is everything!</p><p>In the case of collocations, the context will be a document in the form of a list of words. Discovering collocations in this list of words means that we'll find common phrases that occur frequently throughout the text. For fun, we'll start with the script for <em>Monty Python and the Holy Grail</em>.</p><div><div><div><div><h2 class="title"><a id="ch01lvl2sec36"/>Getting ready</h2></div></div></div><p>The script for <em>Monty Python and the Holy Grail</em> is found in the <code class="literal">webtext</code> corpus, so be sure that it's unzipped in <code class="literal">nltk_data/corpora/webtext/</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec37"/>How to do it...</h2></div></div></div><p>We're going to create a list of all lowercased words in the text, and then produce a <a id="id61" class="indexterm"/>
<code class="literal">BigramCollocationFinder</code>, which we can use to find <strong>bigrams</strong>, which are pairs of words. These bigrams are found using association measurement functions found in the <code class="literal">nltk.metrics</code> package.</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus import webtext
&gt;&gt;&gt; from nltk.collocations import BigramCollocationFinder
&gt;&gt;&gt; from nltk.metrics import BigramAssocMeasures
&gt;&gt;&gt; words = [w.lower() for w in webtext.words('grail.txt')]
&gt;&gt;&gt; bcf = BigramCollocationFinder.from_words(words)
&gt;&gt;&gt; bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)
[("'", 's'), ('arthur', ':'), ('#', '1'), ("'", 't')]</pre></div><p>Well that's not very useful! Let's refine it a bit by adding a word filter to remove punctuation and stopwords.</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus import stopwords
&gt;&gt;&gt; stopset = set(stopwords.words('english'))
&gt;&gt;&gt; filter_stops = lambda w: len(w) &lt; 3 or w in stopset
&gt;&gt;&gt; bcf.apply_word_filter(filter_stops)
&gt;&gt;&gt; bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)
[('black', 'knight'), ('clop', 'clop'), ('head', 'knight'), ('mumble', 'mumble')]</pre></div><p>Much better—we can clearly see four of the most common bigrams in <em>Monty Python and the Holy Grail</em>. If you'd like to see more than four, simply increase the number to whatever you want, and the collocation finder will do its best.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec38"/>How it works...</h2></div></div></div><a id="id62" class="indexterm"/><p>The <code class="literal">BigramCollocationFinder</code> constructs two frequency distributions: one for each word, and another for bigrams. A <a id="id63" class="indexterm"/>
<strong>frequency distribution</strong>, or <code class="literal">FreqDist</code> in NLTK, is basically an enhanced dictionary where the keys are what's being counted, and the values are the counts. Any filtering functions that are applied, reduce the size of these two <code class="literal">FreqDists</code> by eliminating any words that don't pass the filter. By using a filtering function to eliminate all words that are one or two characters, and all English stopwords, we can get a much cleaner result. After filtering, the collocation finder is ready to accept a generic scoring function for finding collocations. Additional scoring functions are covered in the <em>Scoring functions</em> section further in this chapter.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec39"/>There's more...</h2></div></div></div><p>In addition to <code class="literal">BigramCollocationFinder</code>, there's also <code class="literal">TrigramCollocationFinder</code><a id="id64" class="indexterm"/>, for finding triples instead of pairs. This time, we'll look for <strong>trigrams</strong> in Australian singles ads.</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.collocations import TrigramCollocationFinder
&gt;&gt;&gt; from nltk.metrics import TrigramAssocMeasures
&gt;&gt;&gt; words = [w.lower() for w in webtext.words('singles.txt')]
&gt;&gt;&gt; tcf = TrigramCollocationFinder.from_words(words)
&gt;&gt;&gt; tcf.apply_word_filter(filter_stops)
&gt;&gt;&gt; tcf.apply_freq_filter(3)
&gt;&gt;&gt; tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 4)
[('long', 'term', 'relationship')]</pre></div><p>Now, we don't know whether people are looking for a long-term relationship or not, but clearly it's an important topic. In addition to the stopword filter, we also applied a frequency filter which removed any trigrams that occurred less than three times. This is why only one result was returned when we asked for four—because there was only one result that occurred more than twice.</p><div><div><div><div><h3 class="title"><a id="ch01lvl3sec12"/>Scoring functions</h3></div></div></div><a id="id65" class="indexterm"/><p>There are many more scoring functions available besides <code class="literal">likelihood_ratio()</code>. But other than <code class="literal">raw_freq()</code>, you may need a bit of a statistics background to understand how they work. Consult the NLTK API documentation for <code class="literal">NgramAssocMeasures</code> in the <a id="id66" class="indexterm"/>
<code class="literal">nltk.metrics</code> package, to see all the possible scoring functions.</p></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec13"/>Scoring ngrams</h3></div></div></div><a id="id67" class="indexterm"/><p>In addition to the <code class="literal">nbest()</code> method, there are two other ways to get <strong>ngrams</strong> (a generic term for describing <em>bigrams</em> and <em>trigrams</em>) from a collocation finder.</p><div><ol class="orderedlist arabic"><li class="listitem"><code class="literal">above_score(score_fn, min_score)</code> can be used to get all ngrams with scores that are at least <code class="literal">min_score</code>. The <code class="literal">min_score</code> that you choose will depend heavily on the <code class="literal">score_fn</code> you use.</li><li class="listitem"><code class="literal">score_ngrams(score_fn)</code> will return a list with tuple pairs of <code class="literal">(ngram, score)</code>. This can be used to inform your choice for <code class="literal">min_score</code> in the previous step.</li></ol></div></div></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec40"/>See also</h2></div></div></div><p>The <code class="literal">nltk.metrics</code> module will be used again in <a class="link" href="ch07.html" title="Chapter 7. Text Classification">Chapter 7</a>, <em>Text Classification</em>.</p></div></div></body></html>