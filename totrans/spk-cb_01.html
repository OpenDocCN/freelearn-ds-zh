<html><head></head><body><div class="chapter" title="Chapter&#xA0;1.&#xA0;Getting Started with Apache Spark"><div class="titlepage"><div><div><h1 class="title"><a id="ch01"/>Chapter 1. Getting Started with Apache Spark</h1></div></div></div><p>In this chapter, we will set up Spark and configure it. This chapter is divided into the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Installing Spark from binaries</li><li class="listitem" style="list-style-type: disc">Building the Spark source code with Maven</li><li class="listitem" style="list-style-type: disc">Launching Spark on Amazon EC2</li><li class="listitem" style="list-style-type: disc">Deploying Spark on a cluster in standalone mode</li><li class="listitem" style="list-style-type: disc">Deploying Spark on a cluster with Mesos</li><li class="listitem" style="list-style-type: disc">Deploying Spark on a cluster with YARN</li><li class="listitem" style="list-style-type: disc">Using Tachyon as an off-heap storage layer</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec09"/>Introduction</h1></div></div></div><p>Apache Spark<a id="id0" class="indexterm"/> is a general-purpose cluster computing system to process big data workloads. What sets Spark apart from its predecessors, such as MapReduce, is its speed, ease-of-use, and sophisticated analytics.</p><p>Apache Spark was originally developed at AMPLab, UC Berkeley, in 2009. It was made open source in 2010 under the BSD license and switched to the Apache 2.0 license in 2013. Toward the later part of 2013, the creators of Spark founded Databricks to focus on Spark's development and future releases.</p><p>Talking about speed, Spark can achieve sub-second latency on big data workloads. To achieve such low latency, Spark makes use of the memory for storage. In MapReduce, memory is primarily used for actual computation. Spark uses memory both to compute and store objects.</p><p>Spark also provides a unified runtime connecting to various big data storage sources, such as HDFS, Cassandra, HBase, and S3. It also provides a rich set of higher-level libraries for different <a id="id1" class="indexterm"/>big data compute tasks, such as machine learning, SQL processing, graph processing, and real-time streaming. These libraries make development faster and can be combined in an arbitrary fashion.</p><p>Though Spark is written in Scala, and this book only focuses on recipes in Scala, Spark also supports Java and Python.</p><p>Spark is an open source community project, and everyone uses the pure open source Apache distributions for deployments, unlike Hadoop, which has multiple distributions available with vendor enhancements.</p><p>The following figure<a id="id2" class="indexterm"/> shows the Spark ecosystem:</p><div class="mediaobject"><img src="graphics/3056_01_01.jpg" alt="Introduction"/></div><p>The Spark runtime runs on top of a variety of cluster managers, including YARN (Hadoop's compute framework), Mesos, and Spark's own cluster manager called <span class="strong"><strong>standalone mode</strong></span>. Tachyon<a id="id3" class="indexterm"/> is a memory-centric distributed file system that<a id="id4" class="indexterm"/> enables reliable file sharing at memory speed across cluster frameworks. In short, it is an off-heap storage layer in memory, which helps share data across jobs and users. Mesos<a id="id5" class="indexterm"/> is a cluster manager, which is evolving into a data center operating system. YARN<a id="id6" class="indexterm"/> is Hadoop's compute framework that has a robust resource management feature that Spark can seamlessly use.</p></div></div>
<div class="section" title="Installing Spark from binaries"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec10"/>Installing Spark from binaries</h1></div></div></div><p>Spark can be either built from the source code or precompiled binaries can be downloaded from <a class="ulink" href="http://spark.apache.org">http://spark.apache.org</a>. For a standard use case, binaries are good enough, and <a id="id7" class="indexterm"/>this recipe will focus on installing Spark using binaries.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec12"/>Getting ready</h2></div></div></div><p>All the<a id="id8" class="indexterm"/> recipes in this book are developed using Ubuntu Linux<a id="id9" class="indexterm"/> but should work fine on any POSIX environment. Spark expects Java to be installed and the <code class="literal">JAVA_HOME</code> environment variable to be set.</p><p>In Linux/Unix systems, there are certain standards for the location of files and directories, which we are going to follow in this book. The following is a quick cheat sheet:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Directory</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><code class="literal">/bin</code></p>
</td><td style="text-align: left" valign="top">
<p>Essential command binaries</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">/etc</code></p>
</td><td style="text-align: left" valign="top">
<p>Host-specific system configuration</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">/opt</code></p>
</td><td style="text-align: left" valign="top">
<p>Add-on application software packages</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">/var</code></p>
</td><td style="text-align: left" valign="top">
<p>Variable data</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">/tmp</code></p>
</td><td style="text-align: left" valign="top">
<p>Temporary files</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">/home</code></p>
</td><td style="text-align: left" valign="top">
<p>User home directories</p>
</td></tr></tbody></table></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec13"/>How to do it...</h2></div></div></div><p>At the time of writing this, Spark's current version is 1.4. Please check the latest version from Spark's download<a id="id10" class="indexterm"/> page at <a class="ulink" href="http://spark.apache.org/downloads.html">http://spark.apache.org/downloads.html</a>. Binaries are developed with a most recent and stable version of Hadoop. To use a specific version of Hadoop, the recommended approach is to build from sources, which will be covered in the next recipe.</p><p>The following are the installation steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Open the terminal and download binaries using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget http://d3kbcqa49mib13.cloudfront.net/spark-1.4.0-bin-hadoop2.4.tgz</strong></span>
</pre></div></li><li class="listitem">Unpack binaries:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ tar -zxf spark-1.4.0-bin-hadoop2.4.tgz</strong></span>
</pre></div></li><li class="listitem">Rename the folder containing binaries by stripping the version information:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo mv spark-1.4.0-bin-hadoop2.4 spark</strong></span>
</pre></div></li><li class="listitem">Move<a id="id11" class="indexterm"/> the configuration folder to the <code class="literal">/etc</code> folder<a id="id12" class="indexterm"/> so that it can be made a symbolic link later:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo mv spark/conf/* /etc/spark</strong></span>
</pre></div></li><li class="listitem">Create your company-specific installation directory under <code class="literal">/opt</code>. As the recipes in this book are tested on <code class="literal">infoobjects</code> sandbox, we are going to use <code class="literal">infoobjects</code> as directory name. Create the <code class="literal">/opt/infoobjects</code> directory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo mkdir -p /opt/infoobjects</strong></span>
</pre></div></li><li class="listitem">Move the <code class="literal">spark</code> directory to <code class="literal">/opt/infoobjects</code> as it's an add-on software package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo mv spark /opt/infoobjects/</strong></span>
</pre></div></li><li class="listitem">Change the ownership of the <code class="literal">spark</code> home directory to <code class="literal">root</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo chown -R root:root /opt/infoobjects/spark</strong></span>
</pre></div></li><li class="listitem">Change permissions of the <code class="literal">spark</code> home directory, <code class="literal">0755 = user:read-write-execute group:read-execute world:read-execute</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo chmod -R 755 /opt/infoobjects/spark</strong></span>
</pre></div></li><li class="listitem">Move to the <code class="literal">spark</code> home directory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd /opt/infoobjects/spark</strong></span>
</pre></div></li><li class="listitem">Create the symbolic link:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo ln -s /etc/spark conf</strong></span>
</pre></div></li><li class="listitem">Append to <code class="literal">PATH</code> in <code class="literal">.bashrc</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export PATH=$PATH:/opt/infoobjects/spark/bin" &gt;&gt; /home/hduser/.bashrc</strong></span>
</pre></div></li><li class="listitem">Open a new terminal.</li><li class="listitem">Create the <code class="literal">log</code> directory in <code class="literal">/var</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo mkdir -p /var/log/spark</strong></span>
</pre></div></li><li class="listitem">Make <code class="literal">hduser</code> the owner of the Spark <code class="literal">log</code> directory.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo chown -R hduser:hduser /var/log/spark</strong></span>
</pre></div></li><li class="listitem">Create the Spark <code class="literal">tmp</code> directory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir /tmp/spark</strong></span>
</pre></div></li><li class="listitem">Configure <a id="id13" class="indexterm"/>Spark with the help of the following<a id="id14" class="indexterm"/> command lines:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd /etc/spark</strong></span>
<span class="strong"><strong>$ echo "export HADOOP_CONF_DIR=/opt/infoobjects/hadoop/etc/hadoop" &gt;&gt; spark-env.sh</strong></span>
<span class="strong"><strong>$ echo "export YARN_CONF_DIR=/opt/infoobjects/hadoop/etc/Hadoop" &gt;&gt; spark-env.sh</strong></span>
<span class="strong"><strong>$ echo "export SPARK_LOG_DIR=/var/log/spark" &gt;&gt; spark-env.sh</strong></span>
<span class="strong"><strong>$ echo "export SPARK_WORKER_DIR=/tmp/spark" &gt;&gt; spark-env.sh</strong></span>
</pre></div></li></ol></div></div></div>
<div class="section" title="Building the Spark source code with Maven"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec11"/>Building the Spark source code with Maven</h1></div></div></div><p>Installing <a id="id15" class="indexterm"/>Spark using binaries works fine in most <a id="id16" class="indexterm"/>cases. For advanced cases, such as the following (but not limited to), compiling from the source code is a better option:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Compiling for a specific Hadoop version</li><li class="listitem" style="list-style-type: disc">Adding the Hive integration</li><li class="listitem" style="list-style-type: disc">Adding the YARN integration</li></ul></div><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec14"/>Getting ready</h2></div></div></div><p>The following are the prerequisites for this recipe to work:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Java 1.6 or a later version</li><li class="listitem" style="list-style-type: disc">Maven 3.x</li></ul></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec15"/>How to do it...</h2></div></div></div><p>The following are the steps to build the Spark source code with Maven:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Increase <code class="literal">MaxPermSize</code> for heap:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export _JAVA_OPTIONS=\"-XX:MaxPermSize=1G\""  &gt;&gt; /home/hduser/.bashrc</strong></span>
</pre></div></li><li class="listitem">Open a new terminal window and download the Spark source code from GitHub:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget https://github.com/apache/spark/archive/branch-1.4.zip</strong></span>
</pre></div></li><li class="listitem">Unpack the archive:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ gunzip branch-1.4.zip</strong></span>
</pre></div></li><li class="listitem">Move to the <code class="literal">spark</code> directory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd spark</strong></span>
</pre></div></li><li class="listitem">Compile<a id="id17" class="indexterm"/> the sources with these<a id="id18" class="indexterm"/> flags: Yarn enabled, Hadoop version 2.4, Hive enabled, and skipping tests for faster compilation:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -Phive -DskipTests clean package</strong></span>
</pre></div></li><li class="listitem">Move the <code class="literal">conf</code> folder to the <code class="literal">etc</code> folder so that it can be made a symbolic link:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo mv spark/conf /etc/</strong></span>
</pre></div></li><li class="listitem">Move the <code class="literal">spark</code> directory to <code class="literal">/opt</code> as it's an add-on software package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo mv spark /opt/infoobjects/spark</strong></span>
</pre></div></li><li class="listitem">Change the ownership of the <code class="literal">spark</code> home directory to <code class="literal">root</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo chown -R root:root /opt/infoobjects/spark</strong></span>
</pre></div></li><li class="listitem">Change the permissions of the <code class="literal">spark</code> home directory <code class="literal">0755 = user:rwx group:r-x world:r-x</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo chmod -R 755 /opt/infoobjects/spark</strong></span>
</pre></div></li><li class="listitem">Move to the <code class="literal">spark</code> home directory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd /opt/infoobjects/spark</strong></span>
</pre></div></li><li class="listitem">Create a symbolic link:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo ln -s /etc/spark conf</strong></span>
</pre></div></li><li class="listitem">Put the Spark executable in the path by editing <code class="literal">.bashrc</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export PATH=$PATH:/opt/infoobjects/spark/bin" &gt;&gt; /home/hduser/.bashrc</strong></span>
</pre></div></li><li class="listitem">Create the <code class="literal">log</code> directory in <code class="literal">/var</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo mkdir -p /var/log/spark</strong></span>
</pre></div></li><li class="listitem">Make <code class="literal">hduser</code> the owner of the Spark <code class="literal">log</code> directory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo chown -R hduser:hduser /var/log/spark</strong></span>
</pre></div></li><li class="listitem">Create the Spark <code class="literal">tmp</code> directory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir /tmp/spark</strong></span>
</pre></div></li><li class="listitem">Configure Spark with the help of the following command lines:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd /etc/spark</strong></span>
<span class="strong"><strong>$ echo "export HADOOP_CONF_DIR=/opt/infoobjects/hadoop/etc/hadoop" &gt;&gt; spark-env.sh</strong></span>
<span class="strong"><strong>$ echo "export YARN_CONF_DIR=/opt/infoobjects/hadoop/etc/Hadoop" &gt;&gt; spark-env.sh</strong></span>
<span class="strong"><strong>$ echo "export SPARK_LOG_DIR=/var/log/spark" &gt;&gt; spark-env.sh</strong></span>
<span class="strong"><strong>$ echo "export SPARK_WORKER_DIR=/tmp/spark" &gt;&gt; spark-env.sh</strong></span>
</pre></div></li></ol></div></div></div>
<div class="section" title="Launching Spark on Amazon EC2"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec12"/>Launching Spark on Amazon EC2</h1></div></div></div><p><span class="strong"><strong>Amazon Elastic Compute Cloud</strong></span> (<span class="strong"><strong>Amazon EC2</strong></span>) is a web service that provides resizable <a id="id19" class="indexterm"/>compute instances in the cloud. Amazon EC2 provides the following features:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">On-demand <a id="id20" class="indexterm"/>delivery of IT resources via the Internet</li><li class="listitem" style="list-style-type: disc">The <a id="id21" class="indexterm"/>provision of as many instances as you like</li><li class="listitem" style="list-style-type: disc">Payment<a id="id22" class="indexterm"/> for the hours you use instances like your utility bill</li><li class="listitem" style="list-style-type: disc">No setup cost, no installation, and no overhead at all</li><li class="listitem" style="list-style-type: disc">When you no longer need instances, you either shut down or terminate and walk away</li><li class="listitem" style="list-style-type: disc">The availability of these instances on all familiar operating systems</li></ul></div><p>EC2 provides different types of instances to meet all compute needs, such as general-purpose instances, micro instances, memory-optimized instances, storage-optimized instances, and others. They have a free tier of micro-instances to try.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec16"/>Getting ready</h2></div></div></div><p>The <code class="literal">spark-ec2</code> script<a id="id23" class="indexterm"/> comes bundled with Spark and makes it easy to launch, manage, and shut down clusters on Amazon EC2.</p><p>Before you start, you <a id="id24" class="indexterm"/>need to do the following things:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Log in to the Amazon AWS account (<a class="ulink" href="http://aws.amazon.com">http://aws.amazon.com</a>).</li><li class="listitem">Click on <span class="strong"><strong>Security Credentials</strong></span> under your account name in the top-right corner.</li><li class="listitem">Click on <span class="strong"><strong>Access Keys</strong></span> and <span class="strong"><strong>Create New Access Key</strong></span>:<div class="mediaobject"><img src="graphics/3056_01_02.jpg" alt="Getting ready"/></div></li><li class="listitem">Note down the access key ID and secret access key.</li><li class="listitem">Now go to <span class="strong"><strong>Services</strong></span> | <span class="strong"><strong>EC2</strong></span>.</li><li class="listitem">Click on <span class="strong"><strong>Key Pairs</strong></span> in left-hand menu under NETWORK &amp; SECURITY.</li><li class="listitem">Click on <span class="strong"><strong>Create Key Pair</strong></span> and enter <code class="literal">kp-spark</code> as key-pair name:<div class="mediaobject"><img src="graphics/3056_01_15.jpg" alt="Getting ready"/></div></li><li class="listitem">Download the <a id="id25" class="indexterm"/>private key file and copy it in the <code class="literal">/home/hduser/keypairs folder</code>.</li><li class="listitem">Set <a id="id26" class="indexterm"/>permissions on key file to <code class="literal">600</code>.</li><li class="listitem">Set environment variables to reflect access key ID and secret access key (please replace sample values with your own values):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export AWS_ACCESS_KEY_ID=\"AKIAOD7M2LOWATFXFKQ\"" &gt;&gt; /home/hduser/.bashrc</strong></span>
<span class="strong"><strong>$ echo "export AWS_SECRET_ACCESS_KEY=\"+Xr4UroVYJxiLiY8DLT4DLT4D4sxc3ijZGMx1D3pfZ2q\"" &gt;&gt; /home/hduser/.bashrc</strong></span>
<span class="strong"><strong>$ echo "export PATH=$PATH:/opt/infoobjects/spark/ec2" &gt;&gt; /home/hduser/.bashrc</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec17"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Spark comes bundled with scripts to launch the Spark cluster on Amazon EC2. Let's launch the cluster using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd /home/hduser</strong></span>
<span class="strong"><strong>$ spark-ec2 -k &lt;key-pair&gt; -i &lt;key-file&gt; -s &lt;num-slaves&gt; launch &lt;cluster-name&gt;</strong></span>
</pre></div></li><li class="listitem">Launch the cluster with the example value:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-ec2 -k kp-spark -i /home/hduser/keypairs/kp-spark.pem --hadoop-major-version 2  -s 3 launch spark-cluster</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note003"/>Note</h3><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">&lt;key-pair&gt;</code>: This is the name of EC2 key-pair created in AWS</li><li class="listitem" style="list-style-type: disc"><code class="literal">&lt;key-file&gt;</code>: This is the private key file you downloaded</li><li class="listitem" style="list-style-type: disc"><code class="literal">&lt;num-slaves&gt;</code>: This is the number of slave nodes to launch</li><li class="listitem" style="list-style-type: disc"><code class="literal">&lt;cluster-name&gt;</code>: This is the name of the cluster</li></ul></div></div></div></li><li class="listitem">Sometimes, the default availability zones are not available; in that case, retry sending the request by specifying the specific availability zone you are requesting:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-ec2 -k kp-spark -i /home/hduser/keypairs/kp-spark.pem -z us-east-1b --hadoop-major-version 2  -s 3 launch spark-cluster</strong></span>
</pre></div></li><li class="listitem">If your <a id="id27" class="indexterm"/>application needs to retain data after<a id="id28" class="indexterm"/> the instance shuts down, attach EBS volume to it (for example, a 10 GB space):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-ec2 -k kp-spark -i /home/hduser/keypairs/kp-spark.pem --hadoop-major-version 2 -ebs-vol-size 10 -s 3 launch spark-cluster</strong></span>
</pre></div></li><li class="listitem">If you use Amazon spot instances, here's the way to do it:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-ec2 -k kp-spark -i /home/hduser/keypairs/kp-spark.pem -spot-price=0.15 --hadoop-major-version 2  -s 3 launch spark-cluster</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note03"/>Note</h3><p>Spot instances allow you to name your own price for Amazon EC2 computing capacity. You simply bid on spare Amazon EC2 instances and run them whenever your bid exceeds the current spot price, which varies in real-time based on supply and demand (source: <a class="ulink" href="http://amazon.com">amazon.com</a>).</p></div></div></li><li class="listitem">After everything is launched, check the status of the cluster by going to the web UI URL that will be printed at the end.<div class="mediaobject"><img src="graphics/3056_01_03.jpg" alt="How to do it..."/></div></li><li class="listitem">Check the status of the cluster:<div class="mediaobject"><img src="graphics/3056_01_04.jpg" alt="How to do it..."/></div></li><li class="listitem">Now, to <a id="id29" class="indexterm"/>access the Spark cluster on EC2, let's <a id="id30" class="indexterm"/>connect to the master node <a id="id31" class="indexterm"/>using <span class="strong"><strong>secure shell protocol</strong></span> (<span class="strong"><strong>SSH</strong></span>):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-ec2 -k kp-spark -i /home/hduser/kp/kp-spark.pem  login spark-cluster</strong></span>
</pre></div><p>You should get something like the following:</p><div class="mediaobject"><img src="graphics/3056_01_05.jpg" alt="How to do it..."/></div></li><li class="listitem">Check directories in the master node and see what they do:<div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Directory</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><code class="literal">ephemeral-hdfs</code></p>
</td><td style="text-align: left" valign="top">
<p>This<a id="id32" class="indexterm"/> is the Hadoop instance for which data is ephemeral and gets deleted when you stop or restart the machine.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">persistent-hdfs</code></p>
</td><td style="text-align: left" valign="top">
<p>Each <a id="id33" class="indexterm"/>node has a very small amount of persistent storage (approximately 3 GB). If you use this instance, data will be retained in that space.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">hadoop-native</code></p>
</td><td style="text-align: left" valign="top">
<p>These<a id="id34" class="indexterm"/> are native libraries to support Hadoop, such as snappy compression libraries.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">Scala</code></p>
</td><td style="text-align: left" valign="top">
<p>This<a id="id35" class="indexterm"/> is Scala installation.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">shark</code></p>
</td><td style="text-align: left" valign="top">
<p>This<a id="id36" class="indexterm"/> is Shark installation (Shark is no longer supported and is replaced by Spark SQL).</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">spark</code></p>
</td><td style="text-align: left" valign="top">
<p>This<a id="id37" class="indexterm"/> is Spark installation</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">spark-ec2</code></p>
</td><td style="text-align: left" valign="top">
<p>These <a id="id38" class="indexterm"/>are files to support this cluster deployment.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">tachyon</code></p>
</td><td style="text-align: left" valign="top">
<p>This<a id="id39" class="indexterm"/> is Tachyon installation</p>
</td></tr></tbody></table></div></li><li class="listitem">Check <a id="id40" class="indexterm"/>the HDFS version in an ephemeral<a id="id41" class="indexterm"/> instance:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ephemeral-hdfs/bin/hadoop version</strong></span>
<span class="strong"><strong>Hadoop 2.0.0-chd4.2.0</strong></span>
</pre></div></li><li class="listitem">Check the HDFS version in persistent instance with the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ persistent-hdfs/bin/hadoop version</strong></span>
<span class="strong"><strong>Hadoop 2.0.0-chd4.2.0</strong></span>
</pre></div></li><li class="listitem">Change the configuration level in logs:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd spark/conf</strong></span>
</pre></div></li><li class="listitem">The default log level information is too verbose, so let's change it to Error:<div class="mediaobject"><img src="graphics/3056_01_06.jpg" alt="How to do it..."/></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create <a id="id42" class="indexterm"/>the <code class="literal">log4.properties</code> file by renaming the template:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mv log4j.properties.template log4j.properties</strong></span>
</pre></div></li><li class="listitem">Open <code class="literal">log4j.properties</code> in vi or your favorite editor:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ vi log4j.properties</strong></span>
</pre></div></li><li class="listitem">Change<a id="id43" class="indexterm"/> second line from <code class="literal">| log4j.rootCategory=INFO, console</code> to <code class="literal">| log4j.rootCategory=ERROR, console</code>.</li></ol></div></li><li class="listitem">Copy the configuration to all slave nodes after the change:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-ec2/copydir spark/conf</strong></span>
</pre></div><p>You should get something like this:</p><div class="mediaobject"><img src="graphics/3056_01_07.jpg" alt="How to do it..."/></div></li><li class="listitem">Destroy the Spark cluster:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-ec2 destroy spark-cluster</strong></span>
</pre></div></li></ol></div><div class="section" title="See also"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec18"/>See also</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://aws.amazon.com/ec2">http://aws.amazon.com/ec2</a></li></ul></div></div></div></div>
<div class="section" title="Deploying on a cluster in standalone mode"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec13"/>Deploying on a cluster in standalone mode</h1></div></div></div><p>Compute resources in a distributed environment need to be managed so that resource utilization is<a id="id44" class="indexterm"/> efficient and every job gets a fair <a id="id45" class="indexterm"/>chance to run. Spark comes along with its own cluster manager conveniently called <span class="strong"><strong>standalone mode</strong></span>. Spark also supports working with YARN and Mesos cluster managers.</p><p>The cluster manager that should be chosen is mostly driven by both legacy concerns and whether other frameworks, such as MapReduce, are sharing the same compute resource pool. If your cluster has legacy MapReduce jobs running, and all of them cannot be converted to Spark jobs, it is a good idea to use YARN as the cluster manager. Mesos is emerging as a data center operating system to conveniently manage jobs across frameworks, and is very compatible with Spark.</p><p>If the Spark framework is the only framework in your cluster, then standalone mode is good enough. As Spark evolves as technology, you will see more and more use cases of Spark being used as the standalone framework serving all big data compute needs. For example, some jobs may be using Apache Mahout at present because MLlib does not have a specific machine-learning library, which the job needs. As soon as MLlib gets this library, this particular job can be moved to Spark.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec19"/>Getting ready</h2></div></div></div><p>Let's consider a cluster of six nodes as an example setup: one master and five slaves (replace them with actual node names in your cluster):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Master</strong></span>
<span class="strong"><strong>m1.zettabytes.com</strong></span>
<span class="strong"><strong>Slaves</strong></span>
<span class="strong"><strong>s1.zettabytes.com</strong></span>
<span class="strong"><strong>s2.zettabytes.com</strong></span>
<span class="strong"><strong>s3.zettabytes.com</strong></span>
<span class="strong"><strong>s4.zettabytes.com</strong></span>
<span class="strong"><strong>s5.zettabytes.com</strong></span>
</pre></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec20"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Since Spark's standalone mode is the default, all you need to do is to have Spark binaries installed on both master and slave machines. Put <code class="literal">/opt/infoobjects/spark/sbin</code> in path on every node:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export PATH=$PATH:/opt/infoobjects/spark/sbin" &gt;&gt; /home/hduser/.bashrc</strong></span>
</pre></div></li><li class="listitem">Start the<a id="id46" class="indexterm"/> standalone master <a id="id47" class="indexterm"/>server (SSH to master first):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hduser@m1.zettabytes.com~] start-master.sh</strong></span>
</pre></div><p>Master, by default, starts on port 7077, which slaves use to connect to it. It also has a web UI at port 8088.</p></li><li class="listitem">Please SSH to master node and start slaves:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hduser@s1.zettabytes.com~] spark-class org.apache.spark.deploy.worker.Worker spark://m1.zettabytes.com:7077</strong></span>
</pre></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Argument (for fine-grained configuration, the following parameters work with both master and slaves)</p>
</th><th style="text-align: left" valign="bottom">
<p>Meaning</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><code class="literal">-i &lt;ipaddress&gt;,-ip &lt;ipaddress&gt;</code></p>
</td><td style="text-align: left" valign="top">
<p>IP address/DNS service listens on</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">-p &lt;port&gt;, --port &lt;port&gt;</code></p>
</td><td style="text-align: left" valign="top">
<p>Port service listens on</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">--webui-port &lt;port&gt;</code></p>
</td><td style="text-align: left" valign="top">
<p>Port for web UI (by default, 8080 for master and 8081 for worker)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">-c &lt;cores&gt;,--cores &lt;cores&gt;</code></p>
</td><td style="text-align: left" valign="top">
<p>Total CPU cores Spark applications that can be used on a machine (worker only)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">-m &lt;memory&gt;,--memory &lt;memory&gt;</code></p>
</td><td style="text-align: left" valign="top">
<p>Total RAM Spark applications that can be used on a machine (worker only)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">-d &lt;dir&gt;,--work-dir &lt;dir&gt;</code></p>
</td><td style="text-align: left" valign="top">
<p>The directory to use for scratch space and job output logs</p>
</td></tr></tbody></table></div></li><li class="listitem">Rather than manually starting master and slave daemons on each node, it can also be accomplished using cluster launch scripts.</li><li class="listitem">First, create the <code class="literal">conf/slaves</code> file on a master node and add one line per slave hostname (using an example of five slaves nodes, replace with the DNS of slave nodes in your cluster):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hduser@m1.zettabytes.com~] echo "s1.zettabytes.com" &gt;&gt; conf/slaves</strong></span>
<span class="strong"><strong>hduser@m1.zettabytes.com~] echo "s2.zettabytes.com" &gt;&gt; conf/slaves</strong></span>
<span class="strong"><strong>hduser@m1.zettabytes.com~] echo "s3.zettabytes.com" &gt;&gt; conf/slaves</strong></span>
<span class="strong"><strong>hduser@m1.zettabytes.com~] echo "s4.zettabytes.com" &gt;&gt; conf/slaves</strong></span>
<span class="strong"><strong>hduser@m1.zettabytes.com~] echo "s5.zettabytes.com" &gt;&gt; conf/slaves</strong></span>
</pre></div><p>Once<a id="id48" class="indexterm"/> the slave machine is set up, you can<a id="id49" class="indexterm"/> call the following scripts to start/stop cluster:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Script name</p>
</th><th style="text-align: left" valign="bottom">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><code class="literal">start-master.sh</code></p>
</td><td style="text-align: left" valign="top">
<p>Starts <a id="id50" class="indexterm"/>a master instance on the host machine</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">start-slaves.sh</code></p>
</td><td style="text-align: left" valign="top">
<p>Starts<a id="id51" class="indexterm"/> a slave instance on each node in the slaves file</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">start-all.sh</code></p>
</td><td style="text-align: left" valign="top">
<p>Starts<a id="id52" class="indexterm"/> both master and slaves</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">stop-master.sh</code></p>
</td><td style="text-align: left" valign="top">
<p>Stops <a id="id53" class="indexterm"/>the master instance on the host machine</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">stop-slaves.sh</code></p>
</td><td style="text-align: left" valign="top">
<p>Stops<a id="id54" class="indexterm"/> the slave instance on all nodes in the slaves file</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">stop-all.sh</code></p>
</td><td style="text-align: left" valign="top">
<p>Stops <a id="id55" class="indexterm"/>both master and slaves</p>
</td></tr></tbody></table></div></li><li class="listitem">Connect an application to the cluster through the Scala code:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val sparkContext = new SparkContext(new SparkConf().setMaster("spark://m1.zettabytes.com:7077")</strong></span>
</pre></div></li><li class="listitem">Connect to the cluster through Spark shell:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --master spark://master:7077</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec21"/>How it works...</h2></div></div></div><p>In standalone mode, Spark follows the master slave architecture, very much like Hadoop, MapReduce, and YARN. The <a id="id56" class="indexterm"/>compute master daemon is called <span class="strong"><strong>Spark master</strong></span> and runs on one master node. Spark master can be made highly available using ZooKeeper. You can also add more standby masters on the fly, if needed.</p><p>The compute slave daemon is <a id="id57" class="indexterm"/>called <span class="strong"><strong>worker</strong></span> and is on each slave node. The worker daemon does the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Reports the availability of compute resources on a slave node, such as the number of cores, memory, and others, to Spark master</li><li class="listitem" style="list-style-type: disc">Spawns the executor when asked to do so by Spark master</li><li class="listitem" style="list-style-type: disc">Restarts the executor if it dies</li></ul></div><p>There is,<a id="id58" class="indexterm"/> at most, one executor<a id="id59" class="indexterm"/> per application per slave machine.</p><p>Both Spark master and worker are very lightweight. Typically, memory allocation between 500 MB to 1 GB is sufficient. This value can be set in <code class="literal">conf/spark-env.sh</code> by setting the <code class="literal">SPARK_DAEMON_MEMORY</code> parameter. For example, the following configuration will set the memory to 1 gigabits for both master and worker daemon. Make sure you have <code class="literal">sudo</code> as the super user before running it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export SPARK_DAEMON_MEMORY=1g" &gt;&gt; /opt/infoobjects/spark/conf/spark-env.sh</strong></span>
</pre></div><p>By default, each slave node has one worker instance running on it. Sometimes, you may have a few machines that are more powerful than others. In that case, you can spawn more than one worker on that machine by the following configuration (only on those machines):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export SPARK_WORKER_INSTANCES=2" &gt;&gt; /opt/infoobjects/spark/conf/spark-env.sh</strong></span>
</pre></div><p>Spark worker, by default, uses all cores on the slave machine for its executors. If you would like to limit the number of cores the worker can use, you can set it to that number (for example, 12) by the following configuration:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export SPARK_WORKER_CORES=12" &gt;&gt; /opt/infoobjects/spark/conf/spark-env.sh</strong></span>
</pre></div><p>Spark worker, by default, uses all the available RAM (1 GB for executors). Note that you cannot allocate how much memory each specific executor will use (you can control this from the driver configuration). To assign another value for the total memory (for example, 24 GB) to be used by all executors combined, execute the following setting:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export SPARK_WORKER_MEMORY=24g" &gt;&gt; /opt/infoobjects/spark/conf/spark-env.sh</strong></span>
</pre></div><p>There are some settings you can do at the driver level:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">To specify the maximum number of CPU cores to be used by a given application across the cluster, you can set the <code class="literal">spark.cores.max</code> configuration in Spark submit or Spark shell as follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-submit --conf spark.cores.max=12</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc">To specify the amount of memory each executor should be allocated (the minimum recommendation is 8 GB), you can set the <code class="literal">spark.executor.memory</code> configuration in Spark submit or Spark shell as follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-submit --conf spark.executor.memory=8g</strong></span>
</pre></div></li></ul></div><p>The <a id="id60" class="indexterm"/>following diagram depicts the <a id="id61" class="indexterm"/>high-level architecture of a Spark cluster:</p><div class="mediaobject"><img src="graphics/3056_01_08.jpg" alt="How it works..."/></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec22"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html">http://spark.apache.org/docs/latest/spark-standalone.html</a> to find more <a id="id62" class="indexterm"/>configuration options</li></ul></div></div></div>
<div class="section" title="Deploying on a cluster with Mesos"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec14"/>Deploying on a cluster with Mesos</h1></div></div></div><p>Mesos is <a id="id63" class="indexterm"/>slowly emerging as a data center <a id="id64" class="indexterm"/>operating system to manage all compute resources <a id="id65" class="indexterm"/>across a data center. Mesos runs on any computer running the Linux operating system. Mesos is built using the same principles as Linux kernel. Let's see how we can install Mesos.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec23"/>How to do it...</h2></div></div></div><p>Mesosphere provides a binary distribution of Mesos. The most recent package for the Mesos distribution can be installed from the Mesosphere repositories by performing the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Execute Mesos on Ubuntu OS with the trusty version:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv E56151BF DISTRO=$(lsb_release -is | tr '[:upper:]' '[:lower:]') CODENAME=$(lsb_release -cs)</strong></span>
<span class="strong"><strong>$ sudo vi /etc/apt/sources.list.d/mesosphere.list</strong></span>

<span class="strong"><strong>deb http://repos.mesosphere.io/Ubuntu trusty main</strong></span>
</pre></div></li><li class="listitem">Update the repositories:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo apt-get -y update</strong></span>
</pre></div></li><li class="listitem">Install Mesos:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo apt-get -y install mesos</strong></span>
</pre></div></li><li class="listitem">To connect Spark to Mesos to integrate Spark with Mesos, make Spark binaries available to Mesos and configure the Spark driver to connect to Mesos.</li><li class="listitem">Use Spark binaries from the first recipe and upload to HDFS:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ </strong></span>
<span class="strong"><strong>hdfs dfs</strong></span>
<span class="strong"><strong> -put spark-1.4.0-bin-hadoop2.4.tgz spark-1.4.0-bin-hadoop2.4.tgz</strong></span>
</pre></div></li><li class="listitem">The master URL for single master Mesos is <code class="literal">mesos://host:5050</code>, and for the ZooKeeper managed Mesos cluster, it is <code class="literal">mesos://zk://host:2181</code>.</li><li class="listitem">Set the following variables in <code class="literal">spark-env.sh</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo vi spark-env.sh</strong></span>
<span class="strong"><strong>export MESOS_NATIVE_LIBRARY=/usr/local/lib/libmesos.so</strong></span>
<span class="strong"><strong>export SPARK_EXECUTOR_URI= hdfs://localhost:9000/user/hduser/spark-1.4.0-bin-hadoop2.4.tgz</strong></span>
</pre></div></li><li class="listitem">Run from the Scala program:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val conf = new SparkConf().setMaster("mesos://host:5050")</strong></span>
<span class="strong"><strong>val sparkContext = new SparkContext(conf)</strong></span>
</pre></div></li><li class="listitem">Run from <a id="id66" class="indexterm"/>the Spark shell:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --master mesos://host:5050</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note04"/>Note</h3><p>Mesos<a id="id67" class="indexterm"/> has two run modes:</p><p><span class="strong"><strong>Fine-grained</strong></span>: In<a id="id68" class="indexterm"/> fine-grained (default) mode, every Spark task runs as a separate Mesos task</p><p><span class="strong"><strong>Coarse-grained</strong></span>: This <a id="id69" class="indexterm"/>mode will launch only one long-running Spark task on each Mesos machine</p></div></div></li><li class="listitem">To run in the coarse-grained mode, set the <code class="literal">spark.mesos.coarse</code> property:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>conf.set("spark.mesos.coarse","true")</strong></span>
</pre></div></li></ol></div></div></div>
<div class="section" title="Deploying on a cluster with YARN"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec15"/>Deploying on a cluster with YARN</h1></div></div></div><p><span class="strong"><strong>Yet another resource negotiator</strong></span> (<span class="strong"><strong>YARN</strong></span>) is <a id="id70" class="indexterm"/>Hadoop's compute framework that runs on top of HDFS, which is Hadoop's storage layer.</p><p>YARN<a id="id71" class="indexterm"/> follows the master slave architecture. The <a id="id72" class="indexterm"/>master daemon is called <code class="literal">ResourceManager</code> and the slave daemon is called <code class="literal">NodeManager</code>. Besides this application, life cycle management is done by <code class="literal">ApplicationMaster</code>, which can be spawned on any slave node and is alive for the lifetime of an application.</p><p>When Spark is run on YARN, <code class="literal">ResourceManager</code> performs the role of Spark master and <code class="literal">NodeManagers</code> work as executor nodes.</p><p>While running Spark with YARN, each Spark executor is run as YARN container.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec24"/>Getting ready</h2></div></div></div><p>Running Spark on YARN requires a binary distribution of Spark that has YARN support. In both Spark installation recipes, we have taken care of it.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec25"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">To run Spark on YARN, the first step is to set the configuration:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>HADOOP_CONF_DIR: to write to HDFS</strong></span>
<span class="strong"><strong>YARN_CONF_DIR: to connect to YARN ResourceManager</strong></span>
<span class="strong"><strong>$ cd /opt/infoobjects/spark/conf (or /etc/spark)</strong></span>
<span class="strong"><strong>$ sudo vi spark-env.sh</strong></span>
<span class="strong"><strong>export HADOOP_CONF_DIR=/opt/infoobjects/hadoop/etc/Hadoop</strong></span>
<span class="strong"><strong>export YARN_CONF_DIR=/opt/infoobjects/hadoop/etc/hadoop</strong></span>
</pre></div><p>You can see this in the following screenshot:</p><div class="mediaobject"><img src="graphics/3056_01_09.jpg" alt="How to do it..."/></div></li><li class="listitem">The <a id="id73" class="indexterm"/>following command launches <a id="id74" class="indexterm"/>YARN Spark in the <code class="literal">yarn-client</code> mode:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-submit --class path.to.your.Class --master yarn-client [options] &lt;app jar&gt; [app options]</strong></span>
</pre></div><p>Here's an example:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-submit --class com.infoobjects.TwitterFireHose --master yarn-client --num-executors 3 --driver-memory 4g --executor-memory 2g --executor-cores 1 target/sparkio.jar 10</strong></span>
</pre></div></li><li class="listitem">The following command launches Spark shell in the <code class="literal">yarn-client</code> mode:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --master yarn-client</strong></span>
</pre></div></li><li class="listitem">The command to launch in the <code class="literal">yarn-cluster</code> mode is as follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-submit --class path.to.your.Class --master yarn-cluster [options] &lt;app jar&gt; [app options]</strong></span>
</pre></div><p>Here's an example:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-submit --class com.infoobjects.TwitterFireHose --master yarn-cluster --num-executors 3 --driver-memory 4g --executor-memory 2g --executor-cores 1 targe</strong></span>
<span class="strong"><strong>t/sparkio.jar 10</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec26"/>How it works…</h2></div></div></div><p>Spark <a id="id75" class="indexterm"/>applications on YARN run in two modes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">yarn-client</code>: Spark Driver runs in the client process outside of YARN cluster, and<a id="id76" class="indexterm"/> <code class="literal">ApplicationMaster</code> is only used to negotiate resources from ResourceManager</li><li class="listitem" style="list-style-type: disc"><code class="literal">yarn-cluster</code>: Spark<a id="id77" class="indexterm"/> Driver runs in <code class="literal">ApplicationMaster</code> spawned by <code class="literal">NodeManager</code> on a slave node</li></ul></div><p>The <code class="literal">yarn-cluster</code> mode is <a id="id78" class="indexterm"/>recommended for production deployments, while the y<code class="literal">arn-client</code> mode is good for development and debugging when you would like to see immediate output. There is no need to specify Spark master in either mode as it's picked from the Hadoop configuration, and the master parameter is either <code class="literal">yarn-client</code> or <code class="literal">yarn-cluster</code>.</p><p>The following figure shows how Spark is run with YARN in the client mode:</p><div class="mediaobject"><img src="graphics/3056_01_10.jpg" alt="How it works…"/></div><p>The<a id="id79" class="indexterm"/> following figure shows how Spark is run <a id="id80" class="indexterm"/>with YARN in the cluster mode:</p><div class="mediaobject"><img src="graphics/3056_01_11.jpg" alt="How it works…"/></div><p>In the<a id="id81" class="indexterm"/> YARN mode, the following configuration <a id="id82" class="indexterm"/>parameters can be set:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">--num-executors</code>: Configure<a id="id83" class="indexterm"/> how many executors will be allocated</li><li class="listitem" style="list-style-type: disc"><code class="literal">--executor-memory</code>: RAM per executor</li><li class="listitem" style="list-style-type: disc"><code class="literal">--executor-cores</code>: CPU cores per executor</li></ul></div></div></div>
<div class="section" title="Using Tachyon as an off-heap storage layer"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec16"/>Using Tachyon as an off-heap storage layer</h1></div></div></div><p>Spark RDDs<a id="id84" class="indexterm"/> are a great way to store datasets in memory while ending up with multiple copies of the same data in different applications. Tachyon solves some of the <a id="id85" class="indexterm"/>challenges with Spark RDD management. A few of them <a id="id86" class="indexterm"/>are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">RDD only exists for the duration of the Spark application</li><li class="listitem" style="list-style-type: disc">The same process performs the compute and RDD in-memory storage; so, if a process crashes, in-memory storage also goes away</li><li class="listitem" style="list-style-type: disc">Different jobs cannot share an RDD even if they are for the same underlying data, for <a id="id87" class="indexterm"/>example, an HDFS block that leads to:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Slow writes to disk</li><li class="listitem" style="list-style-type: disc">Duplication of data in memory, higher memory footprint</li></ul></div></li><li class="listitem" style="list-style-type: disc">If the<a id="id88" class="indexterm"/> output of one application needs to be shared with the other application, it's slow due to the replication in the disk</li></ul></div><p>Tachyon provides an off-heap memory layer to solve these problems. This layer, being off-heap, is immune to process crashes and is also not subject to garbage collection. This also lets RDDs be shared across applications and outlive a specific job or session; in essence, one single copy of data resides in memory, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/3056_01_12.jpg" alt="Using Tachyon as an off-heap storage layer"/></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec27"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's download and compile Tachyon (Tachyon, by default, comes configured for Hadoop 1.0.4, so it needs to be compiled from sources for the right Hadoop version). Replace the version with the current version. The current version at the time of writing this book is 0.6.4:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget https://github.com/amplab/tachyon/archive/v&lt;version&gt;.zip</strong></span>
</pre></div></li><li class="listitem">Unarchive the source code:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ unzip  v-&lt;version&gt;.zip</strong></span>
</pre></div></li><li class="listitem">Remove the version from the <code class="literal">tachyon</code> source folder name for convenience:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mv tachyon-&lt;version&gt; tachyon</strong></span>
</pre></div></li><li class="listitem">Change <a id="id89" class="indexterm"/>the directory to the <code class="literal">tachyon</code> folder:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd tachyon</strong></span>
<span class="strong"><strong>$ mvn -Dhadoop.version=2.4.0 clean package -DskipTests=true</strong></span>
<span class="strong"><strong>$ cd conf</strong></span>
<span class="strong"><strong>$ sudo mkdir -p /var/tachyon/journal</strong></span>
<span class="strong"><strong>$ sudo chown -R hduser:hduser /var/tachyon/journal</strong></span>
<span class="strong"><strong>$ sudo mkdir -p /var/tachyon/ramdisk</strong></span>
<span class="strong"><strong>$ sudo chown -R hduser:hduser /var/tachyon/ramdisk</strong></span>

<span class="strong"><strong>$ mv tachyon-env.sh.template tachyon-env.sh</strong></span>
<span class="strong"><strong>$ vi tachyon-env.sh</strong></span>
</pre></div></li><li class="listitem">Comment the following line:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export TACHYON_UNDERFS_ADDRESS=$TACHYON_HOME/underfs</strong></span>
</pre></div></li><li class="listitem">Uncomment the following line:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export TACHYON_UNDERFS_ADDRESS=hdfs://localhost:9000</strong></span>
</pre></div></li><li class="listitem">Change the following properties:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>-Dtachyon.master.journal.folder=/var/tachyon/journal/</strong></span>

<span class="strong"><strong>export TACHYON_RAM_FOLDER=/var/tachyon/ramdisk</strong></span>

<span class="strong"><strong>$ sudo mkdir -p /var/log/tachyon</strong></span>
<span class="strong"><strong>$ sudo chown -R hduser:hduser /var/log/tachyon</strong></span>
<span class="strong"><strong>$ vi log4j.properties</strong></span>
</pre></div></li><li class="listitem">Replace <code class="literal">${tachyon.home}</code> with <code class="literal">/var/log/tachyon</code>.</li><li class="listitem">Create a new <code class="literal">core-site.xml</code> file in the <code class="literal">conf</code> directory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo vi core-site.xml</strong></span>
<span class="strong"><strong>&lt;configuration&gt;</strong></span>
<span class="strong"><strong>&lt;property&gt;</strong></span>
<span class="strong"><strong>    &lt;name&gt;fs.tachyon.impl&lt;/name&gt;</strong></span>
<span class="strong"><strong>    &lt;value&gt;tachyon.hadoop.TFS&lt;/value&gt;</strong></span>
<span class="strong"><strong>  &lt;/property&gt;</strong></span>
<span class="strong"><strong>&lt;/configuration&gt;</strong></span>
<span class="strong"><strong>$ cd ~</strong></span>
<span class="strong"><strong>$ sudo mv tachyon /opt/infoobjects/</strong></span>
<span class="strong"><strong>$ sudo chown -R root:root /opt/infoobjects/tachyon</strong></span>
<span class="strong"><strong>$ sudo chmod -R 755 /opt/infoobjects/tachyon</strong></span>
</pre></div></li><li class="listitem">Add <code class="literal">&lt;tachyon home&gt;/bin</code> to the path:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export PATH=$PATH:/opt/infoobjects/tachyon/bin" &gt;&gt; /home/hduser/.bashrc</strong></span>
</pre></div></li><li class="listitem">Restart <a id="id90" class="indexterm"/>the shell and format Tachyon:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ tachyon format</strong></span>
<span class="strong"><strong>$ tachyon-start.sh local //you need to enter root password as RamFS needs to be formatted</strong></span>
</pre></div><p>Tachyon's web interface is <code class="literal">http://hostname:19999</code>:</p><div class="mediaobject"><img src="graphics/3056_01_13.jpg" alt="How to do it..."/></div></li><li class="listitem">Run the sample program to see whether Tachyon is running fine:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ tachyon runTest Basic CACHE_THROUGH</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/3056_01_14.jpg" alt="How to do it..."/></div></li><li class="listitem">You can <a id="id91" class="indexterm"/>stop Tachyon any time by running the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ tachyon-stop.sh</strong></span>
</pre></div></li><li class="listitem">Run Spark on Tachyon:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
<span class="strong"><strong>scala&gt; val words = sc.textFile("tachyon://localhost:19998/words")</strong></span>
<span class="strong"><strong>scala&gt; words.count</strong></span>
<span class="strong"><strong>scala&gt; words.saveAsTextFile("tachyon://localhost:19998/w2")</strong></span>
<span class="strong"><strong>scala&gt; val person = sc.textFile("hdfs://localhost:9000/user/hduser/person")</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.api.java._</strong></span>
<span class="strong"><strong>scala&gt; person.persist(StorageLevels.OFF_HEAP)</strong></span>
</pre></div></li></ol></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec28"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://www.cs.berkeley.edu/~haoyuan/papers/2013_ladis_tachyon.pdf">http://www.cs.berkeley.edu/~haoyuan/papers/2013_ladis_tachyon.pdf</a> to learn<a id="id92" class="indexterm"/> about the origins of Tachyon</li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://www.tachyonnexus.com">http://www.tachyonnexus.com</a></li></ul></div></div></div></body></html>