- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Putting Everything Together with Airﬂow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have covered the different aspects and steps of data ingestion. We
    have seen how to configure and ingest structured and unstructured data, what analytical
    data is, and how to improve logs for more insightful monitoring and error handling.
    Now is the time to group all this information to create something similar to a
    real-world project.
  prefs: []
  type: TYPE_NORMAL
- en: From now on, in the following chapters, we will use Apache Airflow, an open
    source platform that allows us to create, schedule, and monitor workflows. Let’s
    start our journey by configuring and understanding the fundamental concepts of
    Apache Airflow and how powerful this tool is.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Airflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating DAGs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating custom operators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conﬁguring sensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating connectors in Airﬂow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating parallel ingest tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deﬁning ingest-dependent DAGs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have learned about the most important components
    of Airﬂow and how to conﬁgure them, including how to solve related issues in this
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find the code for this chapter in the GitHub repository here: [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook)'
  prefs: []
  type: TYPE_NORMAL
- en: Installing Airflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter requires that Airflow is installed on your local machine. You can
    install it directly on your **Operating System** (**OS**) or using a Docker image.
    For more information regarding this, refer to the *Configuring Docker for Airflow*
    recipe in [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022).
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Airflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Apache Airflow** has many capabilities and a quick setup, which helps us
    start designing our workflows as code. Some additional configurations might be
    required as we progress with the workflows and into data processing. Gladly, Airflow
    has a dedicated file for inserting other arrangements without changing anything
    within its core.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn more about the `airflow.conf` file, how to use
    it, and other valuable configurations required to execute the other recipes in
    this chapter. We will also cover where to find this file and how other environment
    variables work with this tool. Understanding these concepts in practice helps
    us to identify potential improvements or solve problems.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before moving on to the code, ensure your Airflow runs correctly. You can do
    that by checking the Airflow UI at this link: `http://localhost:8080`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using a Docker container (as I am) to host your Airflow application,
    you can check its status on the terminal with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Airflow Docker containers running](img/Figure_9.01_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Airflow Docker containers running
  prefs: []
  type: TYPE_NORMAL
- en: 'Or, you can check the container status on Docker Desktop, as in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Docker Desktop view of Airflow containers running](img/Figure_9.02_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Docker Desktop view of Airflow containers running
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to perform this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by installing the MongoDB additional provider for Airflow. If you
    are using the `docker-compose.yaml` file, open it and add `apache-airflow-providers-mongo`
    inside `_PIP_ADDITIONAL_REQUIREMENTS`. Your code will look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.3 – The docker-compose.yaml file in the environment variables section](img/Figure_9.03_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – The docker-compose.yaml file in the environment variables section
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are hosting Airflow directly on your machine, you can do the same installation
    using **PyPi**: [https://pypi.org/project/apache-airflow-providers-mongo/](https://pypi.org/project/apache-airflow-providers-mongo/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will create a folder called `files_to_test`, and inside it, create
    two more folders: `output_files` and `sensors_files`. You don’t need to worry
    about its usage yet since it will be used later in this chapter. Your Airflow
    folder structure should look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Airflow local directory folder structure](img/Figure_9.04_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Airflow local directory folder structure
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s mount the volumes of our Docker image. You can skip this part if
    you are not using Docker to host Airflow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In your `docker-compose.yaml` file, under the `volume` parameter, add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Your final `volumes` section will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – docker-compose.yaml volumes](img/Figure_9.05_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – docker-compose.yaml volumes
  prefs: []
  type: TYPE_NORMAL
- en: Stop and restart your container so these changes can be propagated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will fix a bug in the `docker-compose.yaml` file. This official
    fix for this bug is within the Airflow official documentation and therefore wasn’t
    included in the Docker image. You can see the complete issue and the solution
    here: [https://github.com/apache/airflow/discussions/24809](https://github.com/apache/airflow/discussions/24809).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To fix the bug, go to the `airflow-init` section of the `docker-compose` file
    and insert `_PIP_ADDITIONAL_REQUIREMENTS: ''''` inside the `environment` parameter.
    Your code will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – The docker-compose.yaml environment variables section with PIP_ADDITIONAL_REQUIREMENTS](img/Figure_9.06_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – The docker-compose.yaml environment variables section with PIP_ADDITIONAL_REQUIREMENTS
  prefs: []
  type: TYPE_NORMAL
- en: 'This action will fix the following issue registered on GitHub: [https://github.com/apache/airflow/pull/23517](https://github.com/apache/airflow/pull/23517).'
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The configuration presented here is simple. However, it guarantees the application
    will keep working through the chapter recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with the package we installed in *step 1*. Like other frameworks
    or platforms, Airflow has its *batteries* included, which means it already comes
    with various packages. But, as its popularity started to increase, it started
    to require other types of connections or operators, which the open source community
    took care of.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find a list of released packages that can be installed on Airflow here:
    [https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.xhtml](https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Before jumping into other code explanations, let’s understand the `volume` section
    inside the `docker-compose.yaml` file. This configuration allows Airflow to see
    which folders reflect the same respective ones inside the Docker container without
    the necessity to upload code using a Docker command every time. In other words,
    we can synchronously add our **Directed Acyclic Graph** (**DAG**) files and new
    operators and see some logs, among other things, and this will be reflected inside
    the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we declared the Docker mount volume configurations for two parts: the
    new folder we created (`files_to_test`) and the `airflow.cfg` file. The first
    one will allow Airflow to replicate the `files_to_test` local folder inside the
    container, so we can use it to use files in a more simplified way. Otherwise,
    if we try to use it without the mounting volume, the following error will appear
    when trying to retrieve any file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Error in Airflow when the folder is not referred to in the container
    volume](img/Figure_9.07_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Error in Airflow when the folder is not referred to in the container
    volume
  prefs: []
  type: TYPE_NORMAL
- en: Although we will not use the `airflow.cfg` file, for now, it is a good practice
    to know how to access this file and what it is used for. This file contains the
    Airflow configurations and can be edited to include more. Usually, sensitive data
    is stored inside it to prevent other people from having improper access since,
    by default, the content of the `airflow.cfg` file cannot be accessed in the UI.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Be very cautious when changing or handling the `airflow.cfg` file. This file
    contains all the required configurations and other relevant settings to make Airflow
    work. We will explore more about this in [*Chapter 10*](B19453_10.xhtml#_idTextAnchor364)*.*
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more information about the Docker image, see the documentation page here:
    [https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Creating DAGs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The core concept of Airflow is based on DAGs, which collect, group, and organize
    tasks to be executed in a specific order. A DAG is also responsible for managing
    the dependencies between its tasks. Simply put, it is not concerned about what
    a task is doing but just *how* to execute it. Typically, a DAG starts at a scheduled
    time, but we can also define dependencies between other DAGs so that they will
    start based on their execution statuses.
  prefs: []
  type: TYPE_NORMAL
- en: We will create our first DAG in this recipe and set it to run based on a specific
    schedule. With this first step, we enter into practically designing our first
    workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Please refer to the *Getting ready* section in the *Configuring Airflow* recipe
    for this recipe since we will handle it with the same technology.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, let’s create a directory called `ids_ingest` inside our `dags` folder.
    Inside the `ids_ingest` folder, we will create two files: `__init__.py` and `ids_ingest_dag.py`.
    The final structure will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Airflow’s local directory structure with the ids_ingest DAG](img/Figure_9.08_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – Airflow’s local directory structure with the ids_ingest DAG
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will write a DAG that retrieves the IDs of the `github_events.json`
    file. Open `ids_ingest_dag.py`, and let’s add the content to write our first DAG:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing the libraries we will use in this script. I like to
    separate the imports from the Airflow library and Python’s library as a good practice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we will define `default_args` for our DAG, as you can see here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will create a Python function that receives the JSON file and returns
    the IDs inside it. Since it is a small function, we can create it inside the DAG’s
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will instantiate our DAG object, and inside it, we will define two
    operators: a `BashOperator` instance to show a console message and `PythonOperator`
    to execute the function we just created, as you can see here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Make sure you save the file before jumping to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, head over to the Airflow UI. Although plenty of DAG examples are provided
    by the Airflow team, you should look for a DAG called `simple_ids_ingest`. You
    will notice the DAG is not enabled. Click on the toggle button to enable it, and
    you should have something like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.9 – The Airflow DAG enabled on the UI](img/Figure_9.09_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – The Airflow DAG enabled on the UI
  prefs: []
  type: TYPE_NORMAL
- en: 'As soon as you enable it, the DAG will start running. Click on the DAG name
    to be redirected to the DAG’s page, as you can see in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.10 – DAG Grid page view](img/Figure_9.10_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – DAG Grid page view
  prefs: []
  type: TYPE_NORMAL
- en: 'If everything is well configured, your page should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.11 – DAG running successfully in Graph page view](img/Figure_9.11_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 – DAG running successfully in Graph page view
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, click on the `get_id_from_json` task. A small window will show up as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.12 – Task options](img/Figure_9.12_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 – Task options
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, click the **Log** button. You will be redirected to a new page with the
    logs for this task, as seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.13 – Task logs in the Airflow UI](img/Figure_9.13_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 – Task logs in the Airflow UI
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the preceding screenshot, our task successfully finished and
    returned the IDs as we expected. You can see the results in the `INFO` log under
    the `AIRFLOW_CTX_DAG_RUN` message.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We created our first DAG with a few lines to retrieve and show a list of IDs
    from a JSON file. Now, let’s understand how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, we created our files under the `dags` directory. It happens
    because, by default, Airflow will understand everything inside of it as a DAG
    file. The folder we created inside of it was just for organization purposes, and
    Airflow will ignore it. Along with the `ids_ingest_dag.py` file, we also made
    an `__init__.py` file. This file internally tells Airflow to look inside this
    folder. As a result, you will see the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.14 – Airflow local directory structure with the ids_ingest DAG](img/Figure_9.14_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 – Airflow local directory structure with the ids_ingest DAG
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: As you might be wondering, it is possible to change this configuration, but
    I don’t recommend this at all since other internal packages might depend on it.
    Do it only in the case of extreme necessity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s take a look at our instantiated DAG:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As you can observe, creating a DAG is simple, and its parameters are spontaneous.
    `dag_id` is crucial and must be unique; otherwise, it can create confusion and
    merge with other DAGs. The `default_args` we declared in *step 2* will guide the
    DAG, telling when it needs to be executed, its user owner, the number of retries
    in case of a failure, and other valuable parameters. After the `as dag` declaration,
    we inserted the bash and Python operators, and they must be indented to be understood
    as the DAG’s tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, to set our workflow, we declared the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As we might guess, it sets the order of which task should be executed first.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We saw how easy it is to create a task to execute a Python function and a bash
    command. By default, Airflow comes with some handy operators to be used daily
    within a data ingestion pipeline. For more information, you can refer to the official
    documentation page here: [https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Tasks, operators, XCom, and others
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Airflow DAGs are a powerful way to group and execute operations. Besides the
    task and operators we saw here, DAGs support other types of workloads and communication
    across other tasks or DAGs. Unfortunately, since that is not the main subject
    of this book, we will not cover those concepts in detail, but I highly recommend
    reading the official documentation here: [https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/index.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/index.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Error handling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you encounter any errors while building this DAG, you can use the instructions
    from *step 7* and *step 8* to debug it. You can see a preview here of how the
    tasks look when an error occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.15 – DAG Graph page view showing a running error](img/Figure_9.15_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.15 – DAG Graph page view showing a running error
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can find the code for the Airflow example DAGs on their official GitHub
    page here: [https://github.com/apache/airflow/tree/main/airflow/example_dags](https://github.com/apache/airflow/tree/main/airflow/example_dags).'
  prefs: []
  type: TYPE_NORMAL
- en: Creating custom operators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As seen in the previous recipe, *Creating DAGs*, it is nearly impossible to
    create a DAG without instantiating a task or, in other words, defining an operator.
    Operators are responsible for holding the logic required to process data in the
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: We also know that Airflow already has predefined operators, allowing dozens
    of ways to ingest and process data. Now, it is time to put into practice how to
    create custom operators. Custom operators allow us to apply specific logic to
    a related project or data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: You will learn how to create a simple customized operator in this recipe. Although
    it is very basic, you will be able to apply the foundations of this technique
    to different scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will create a custom operator to connect to and retrieve
    data from the HolidayAPI, the same as we saw previously, in [*Chapter 2*](B19453_02.xhtml#_idTextAnchor064).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Please, refer to the *Getting ready* section in the *Configuring Airflow* recipe
    for this recipe since we will handle it with the same technology.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to add an environment variable to store our API secret. To do
    so, select the **Variable** item under the **Admin** menu in the Airflow UI, and
    you will be redirected to the desired page. Now, click the **+** button to add
    a new variable, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.16 – The Variable page in the Airflow UI](img/Figure_9.16_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.16 – The Variable page in the Airflow UI
  prefs: []
  type: TYPE_NORMAL
- en: 'On the `SECRET_HOLIDAY_API` under the **Key** field and your API secret under
    the **Value** field. Use the same values you used to execute the *Retrieving data
    using API authentication* recipe in [*Chapter 2*](B19453_02.xhtml#_idTextAnchor064).
    Save it and you will be redirected to the **Variables** page, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.17 – The Airflow UI with a new variable to store the HolidayAPI
    secret](img/Figure_9.17_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.17 – The Airflow UI with a new variable to store the HolidayAPI secret
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are ready to create our custom operator.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code we will use to create the custom operator is the same one we saw in
    [*Chapter 2*](B19453_02.xhtml#_idTextAnchor064), in the *Retrieving data using
    API authentication* recipe, with some alterations to fit Airflow’s requirements.
    Here are the steps for it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by creating the structure inside the `plugins` folder. Since we
    want to make a custom operator, we need to create a folder called `operators`,
    where we will put a Python file called `holiday_api_plugin.py`. Your file tree
    should look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.18 – Airflow’s local directory structure for the plugins folder](img/Figure_9.18_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.18 – Airflow’s local directory structure for the plugins folder
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create some code inside `holiday_api_plugin.py`, starting with the
    library imports and declaring a global variable for where our file output needs
    to be placed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we need to create a Python class, declare its constructors, and finally
    insert the exact code from [*Chapter 2*](B19453_02.xhtml#_idTextAnchor064) inside
    a function called `execute`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Save the file and our operator is ready. Now, we need to create the DAG to execute
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the same logic as in the *Creating DAGs* recipe, we will create a file
    called `holiday_ingest_dag.py`. Your new DAG directory tree should look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.19 – Airflow’s directory structure for the holiday_ingest DAG folder](img/Figure_9.19_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.19 – Airflow’s directory structure for the holiday_ingest DAG folder
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s insert our DAG code inside the `holiday_ingest_dag.py` file and
    save it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the full code, refer to the GitHub repository here: [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_custom_operators](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_custom_operators).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, go to the Airflow UI, look for the `holiday_ingest` DAG, and enable it.
    It will look like the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.20 – The holiday_ingest DAG enabled in the Airflow UI](img/Figure_9.20_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.20 – The holiday_ingest DAG enabled in the Airflow UI
  prefs: []
  type: TYPE_NORMAL
- en: Your job will start to run immediately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s find the task logs by following the same steps from the *Creating
    DAGs* recipe, but now clicking on the `holiday_api_ingestion` task. Your log page
    should look like the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.21 – holiday_api_ingestion task logs](img/Figure_9.21_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.21 – holiday_api_ingestion task logs
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s see whether the output file was created successfully. Go to
    the `files_to_test` folder, click on the `output_files` folder, and if everything
    was successfully configured, a file called `holiday_brazil.json` will be inside
    it. See the following figure for reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.22 – holiday_brazil.json inside the output_files screenshot](img/Figure_9.22_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.22 – holiday_brazil.json inside the output_files screenshot
  prefs: []
  type: TYPE_NORMAL
- en: 'The beginning of the output file should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.23 – The first lines of holiday_brazil.json](img/Figure_9.23_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.23 – The first lines of holiday_brazil.json
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you can see, a custom Airflow operator is an isolated class with a unique
    purpose. Usually, custom operators are created with the intention to also be used
    by other teams or DAGs, which avoids code redundancy or duplication. Now, let’s
    understand how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'We started the recipe by creating the file to host the new operator inside
    the `plugin` folder. We do this because, internally, Airflow understands that
    everything inside of it is custom code. Since we wanted to only create an operator,
    we put it inside a folder with the same name. However, it is also possible to
    create another resource called **Hooks**. You can learn more about creating hooks
    in Airflow here: [https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, heading to the operator code, we declare our code to ingest the HolidayAPI
    inside a class, as you can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We did this to extend Airflow’s `BaseOperator` so that we could customize it
    and insert new constructors. `filename`, `secret_key`, `country`, and `year` are
    the parameters we need to execute the API ingest.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we declared the `execute` function to ingest data from the API. The context
    is an Airflow parameter that allows the function to read configuration values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, our final step was to create a DAG to execute the operator we made. The
    code is like the previous DAG we created in the *Creating DAGs* recipe, only with
    a few new items. The first item was the new `import` instances, as you can see
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The first `import` statement allows us to use the value of `SECRET_HOLIDAY_API`
    we inserted using the UI, and the second imports our custom operator. Observe
    that we only used the `operators.holiday_api_plugin` path. Due to Airflow’s internal
    configuration, it understands that the code inside an `operators` folder (inside
    the `plugins` folder) is an operator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can instantiate the custom operator like any other built-in operator
    in Airflow by passing the required parameters, as you can see in the code here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If an entire project has the same form of authentication for retrieving data
    from a specific API or database, creating custom operators or hooks is a valuable
    way to avoid code duplication.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, before jumping excitedly into creating your plugin, remember that
    Airflow’s community already provides many operators. For example, if you use AWS
    in your daily work, you don’t need to worry about creating a new operator to connect
    with AWS Glue since that already has been done and approved by the Apache community.
    See the documentation here: [https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/glue.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/glue.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the complete list of AWS operators here: [https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/index.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/index.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more custom operator examples, see *Virajdatt Kohir’s* blog here: [https://kvirajdatt.medium.com/airflow-writing-custom-operators-and-publishing-them-as-a-package-part-2-3f4603899ec2](https://kvirajdatt.medium.com/airflow-writing-custom-operators-and-publishing-them-as-a-package-part-2-3f4603899ec2).'
  prefs: []
  type: TYPE_NORMAL
- en: Conﬁguring sensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Under the operator’s umbrella, we have sensors. Sensors are designed to wait
    to execute a task until something happens. For example, a sensor triggers a pipeline
    (or task) when a file lands in an `HDFS` folder, as shown here: [https://airflow.apache.org/docs/apache-airflow-providers-apache-hdfs/stable/_api/airflow/providers/apache/hdfs/sensors/hdfs/index.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-apache-hdfs/stable/_api/airflow/providers/apache/hdfs/sensors/hdfs/index.xhtml).
    As you might be wondering, there are also sensors for specific schedules or time
    deltas.'
  prefs: []
  type: TYPE_NORMAL
- en: Sensors are a fundamental part of creating an automated and event-driven pipeline.
    In this recipe, we will configure a `weekday` sensor, which executes our data
    pipeline on a specific day of the week.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the *Getting ready* section in the *Configuring Airflow* recipe for
    this recipe since we will handle it with the same technology.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides that, let’s put a JSON file to the following path inside the Airflow
    folder: `files_to_test/sensors_files/.`'
  prefs: []
  type: TYPE_NORMAL
- en: In my case, I will use the `github_events.json` file, but you can use any of
    your preferences.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to perform this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start our DAG script by importing the required libraries, defining `default_args`,
    and instantiating our DAG, as you can see here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s define our first task using `DayOfWeekSensor`. See the code here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: I suggest setting the day of the week as a parameter while doing this exercise
    to ensure no confusion. For example, if you want it to be executed on a Monday,
    set `week_day` to `WeekDay.MONDAY`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we will define another task using `BashOperator`. This task will execute
    the command to move a JSON file from `files_to_test/sensors_files/` to `files_to_test/output_files/`.
    Your code should look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we will define the execution workflow of our DAG, as you can see here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `.set_downstream()` function will work similarly to the double arrows (`>>`)
    we already used to define the workflow. You can read more about this here: [https://airflow.apache.org/docs/apache-airflow/1.10.3/concepts.xhtml?highlight=trigger#bitshift-composition](https://airflow.apache.org/docs/apache-airflow/1.10.3/concepts.xhtml?highlight=trigger#bitshift-composition).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As seen in the previous two recipes of this chapter, now we will enable our
    `sensors_move_file` DAG, which will start immediately. If you set the weekday
    as the same day on which you are executing this exercise, your DAG **Graph** view
    will look like this, indicating success:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.24 – sensors_move_file tasks showing a success status](img/Figure_9.24_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.24 – sensors_move_file tasks showing a success status
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see whether our file was moved to the directories. As described
    in the *Getting ready* section, I put a JSON file called `github_events.json`
    inside the `sensor_files` folder. Now, it will be inside `output_files`, as you
    can see here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.25 – github_events.json inside the output_files folder](img/Figure_9.25_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.25 – github_events.json inside the output_files folder
  prefs: []
  type: TYPE_NORMAL
- en: This indicates our sensor executed as expected!
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sensors are valuable operators that execute an action based on a state. They
    can be triggered when a file lands in a directory, during the day, when an external
    task finishes, and so on. Here, we approached an example using a day of the week
    commonly used in data teams to change files from an ingested folder to a cold
    storage folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sensors count with an internal method called `poke`, which will check a resource’s
    status until the criteria are met. If you look at the `move_file_on_saturday`
    log, you will see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the following code, we did not define a `reschedule` parameter,
    so the job will stop until we manually trigger it again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Other parameters we defined were `timeout`, which indicates the time in seconds
    before it fails or stops retrying, and `soft_fail`, which marks the task as `SKIPPED`
    in the case of failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see other allowed parameters here: [https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/base/index.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/base/index.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: And, of course, like the rest of the operators, we can create our custom sensor
    by extending the `BaseSensorOperator` class from Airflow. The main challenge here
    is that to be considered a sensor, it needs to overwrite the `poke` parameter
    without creating a recursing or non-ending function.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can see a list of the default Airflow sensors on the official documentation
    page here: [https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/index.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/index.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Creating connectors in Airﬂow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having DAGs and operators without connecting to any external source is useless.
    Of course, there are many ways to ingest files, even from other DAGs or task results.
    Still, data ingestion usually involves using external sources such as APIs or
    databases as the first step of a data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: To make this happen, in this recipe, we will understand how to create a connector
    in Airflow to connect to a sample database.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the *Getting ready* section of the *Configuring Airflow* recipe for
    this recipe since we will handle it with the same technology.
  prefs: []
  type: TYPE_NORMAL
- en: This exercise will also require the MongoDB local database to be up and running.
    Ensure you have configured it as seen in [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022)
    and have at least one database and collection. You can use the instructions from
    the *Connecting to a NoSQL database (MongoDB)* recipe in [*Chapter 5*](B19453_05.xhtml#_idTextAnchor161).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to perform this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by opening the Airflow UI. On the top menu, select the **Admin**
    button and then **Connections**, and you will be redirected to the **Connections**
    page. Since we haven’t configured anything yet, this page will be empty, as you
    can see in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.26 – The Connections page in the Airflow UI](img/Figure_9.26_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.26 – The Connections page in the Airflow UI
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, click the **+** button to be redirected to the **Add Connection** page.
    Under the **Connection Type** field, search for and select **MongoDB**. Insert
    your connection values under the respective fields, as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.27 – Creating a new connector in the Airflow UI](img/Figure_9.27_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.27 – Creating a new connector in the Airflow UI
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **Save** button, and you should have something similar to this
    on the **Connection** page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.28 – The MongoDB connector created in the Airflow UI](img/Figure_9.28_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.28 – The MongoDB connector created in the Airflow UI
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create our new DAG using the same folder and file tree structure that
    we saw in the *Creating DAGs* recipe. I will call the `mongodb_check_conn_dag.py`
    DAG file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inside the DAG file, let’s start by importing and declaring the required libraries
    and variables, as you can see here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will create a function to connect with MongoDB locally and print `collection`
    `reviews` from the `db_airbnb` database, as you can see here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, let’s proceed with the DAG object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s use `PythonOperator` to call our `get_mongo_collection` function
    defined in *step 5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Don’t forget to put the name of your task in the indentation of the DAG, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Heading to the Airflow UI, let’s enable the DAG and wait for it to be executed.
    After finishing successfully, your `mongodb_task` log should look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.29 – mongodb_task logs](img/Figure_9.29_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.29 – mongodb_task logs
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we connected and retrieved the `Collection` object from MongoDB.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating a connection in Airflow is straightforward, as demonstrated here using
    the UI. It is also possible to create connections programmatically using the `Connection`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: After we set our MongoDB connection parameters, we needed to create a form to
    access it, and we did so using a hook. A **hook** is a high-level interface that
    allows connections to external sources without the need to be preoccupied with
    low-level code or special libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that we configured an external package in the *Configuring Airflow*
    recipe? It was a provider to allow an easy connection with MongoDB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside the `get_mongo_collection` function, we instantiated `MongoHook` and
    passed the same connection ID name set in the `Connection` page, as you can see
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'With that instance, we can call the methods of the `MongoHook` class and even
    pass other parameters to configure the connection. See the documentation for this
    class here: [https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/hooks/mongo/index.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/hooks/mongo/index.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can also use the `airflow.cfg` file to set the connection strings or any
    other environment variable. It is a good practice to store sensitive information
    here, such as credentials, since they will not be shown in the UI. It is also
    possible to encrypt these values with additional configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information, see the documentation here: [https://airflow.apache.org/docs/apache-airflow/stable/howto/set-config.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/howto/set-config.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can learn about the MongoDB provider on the Airflow official documentation
    page here: [https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/index.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/index.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you are interested in reading more about connections, see this link: https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.xhtml#custom-connection-types'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating parallel ingest tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with data, we hardly ever just perform one ingestion at a time,
    and a real-world project involves many ingestions happening simultaneously, often
    in parallel. We know scheduling two or more DAGs to run alongside each other is
    possible, but what about tasks inside one DAG?
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will illustrate how to create parallel task execution in Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Please refer to the *Getting ready* section of the *Configuring Airflow* recipe
    for this recipe since we will handle it with the same technology.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid redundancy in this exercise, we won’t explicitly include the imports
    and main DAG configuration. Instead, the focus is on organizing the operator’s
    workflow. You can use the same logic to create your DAG structure as in the *Creating*
    *DAGs* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the complete Python file used here, go to the GitHub page here: [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_parallel_ingest_tasks](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_parallel_ingest_tasks).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this exercise, my DAG will be called `parallel_tasks_dag`. Now, let’s try
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by creating five `BashOperator` instances, as you can see here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The idea is for three of them to be executed in parallel so they will be inside
    square brackets. The first and last tasks will have the same workflow declared
    as we saw in the *Creating DAGs* recipe, using the `>>` character. The final flow
    structure will look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, enable your DAG, and let’s see what it looks like on the DAG **Graph**
    page. It would be best if you had something like the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.30 – parallel_tasks_dag tasks in Airflow](img/Figure_9.30_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.30 – parallel_tasks_dag tasks in Airflow
  prefs: []
  type: TYPE_NORMAL
- en: As you can observe, the tasks inside the square brackets are displayed in parallel
    and will start after `t_0` finishes its work.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although creating parallel tasks inside a DAG is simple, this type of workflow
    arrangement is advantageous when working with data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider an example of data ingestion: we need to guarantee we have ingested
    all the desired endpoints before moving out to the next pipeline phase. See the
    following figure as a reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.31 – Example of parallel execution](img/Figure_9.31_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.31 – Example of parallel execution
  prefs: []
  type: TYPE_NORMAL
- en: The parallel execution will only move to the final task when all the parallel
    ones finish successfully. With this, we guarantee the data pipeline will not ingest
    only a small portion of the data but all the required data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Back to our exercise, we can simulate this behavior, creating an `t_2` by removing
    one of the simple quotation marks. In the following figure, you can see what the
    DAG graph will look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.32 – Airflow’s parallel_tasks_dag with an error t_2 task](img/Figure_9.32_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.32 – Airflow’s parallel_tasks_dag with an error t_2 task
  prefs: []
  type: TYPE_NORMAL
- en: The `t_final` task will retry executing until we fix `t_2` or the number of
    retries reaches its limit.
  prefs: []
  type: TYPE_NORMAL
- en: However, avoiding many parallel tasks is a good practice, mainly if you have
    limited infrastructure resources to handle them. There are many ways to create
    dependency on external tasks or DAGs, and we can use them to make more efficient
    pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Along with the concept of task parallelism, we have `BranchOperator`. `BranchOperator`
    executes one or more tasks simultaneously based on a criteria match. Let’s illustrate
    this with the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.33 – Branching task diagram example](img/Figure_9.33_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.33 – Branching task diagram example
  prefs: []
  type: TYPE_NORMAL
- en: Based on the day-of-the-week criteria, the `day_of_the_week_branch` task will
    trigger a specific task assigned for that day.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to know more about it, *Analytics Vidhya* has a good blog post
    about it, which you can read here: [https://www.analyticsvidhya.com/blog/2023/01/data-engineering-101-branchpythonoperator-in-apache-airflow/](https://www.analyticsvidhya.com/blog/2023/01/data-engineering-101-branchpythonoperator-in-apache-airflow/).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'BetterDataScience has a good blog post about parallel tasks in Airflow. You
    can find it here: [https://betterdatascience.com/apache-airflow-run-tasks-in-parallel/](https://betterdatascience.com/apache-airflow-run-tasks-in-parallel/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can read more about Airflow task parallelism here: [https://hevodata.com/learn/airflow-parallelism/](https://hevodata.com/learn/airflow-parallelism/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deﬁning ingest-dependent DAGs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the data world, considerable discussion exists about how to organize Airflow
    DAGs. The approach I generally use is to create a DAG for a specific pipeline
    based on the business logic or final destination. Nevertheless, sometimes, to
    proceed with a task inside a DAG, we depend on another DAG to finish the process
    and get the output.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will create two DAGs, where the first depends on the result
    of the second to be successful. Otherwise, it will not be completed. To assist
    us, we will use the `ExternalTaskSensor` operator.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Please refer to the *Getting ready* section of the *Configuring Airflow* recipe
    for this recipe since we will handle it with the same technology.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe depends on the `holiday_ingest` DAG, created in the *Creating custom
    operators* recipe, so ensure you have that.
  prefs: []
  type: TYPE_NORMAL
- en: We will not explicitly cite the imports and main DAG configuration to prevent
    redundancy and repetition in this exercise. The aim here is how to organize the
    operator’s workflow. You can use the same logic to create your DAG structure as
    in the *Creating* *DAGs* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the complete Python file used here, go to the GitHub page here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/de%EF%AC%81ning_dependent_ingests_DAGs](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/de%EF%AC%81ning_dependent_ingests_DAGs)'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this exercise, let’s create a DAG triggered when `holiday_ingest` finishes
    successfully, and returns all the holiday dates in the console output. My DAG
    will be called `external_sensor_dag`, but feel free to provide any other ID name.
    Just ensure it is unique and therefore will not impair other DAGs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Along with the default imports we have, let’s add the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will insert a Python function to return the holiday dates in the `holiday_brazil.json`
    file, which is the output of the `holiday_ingest` DAG:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we will make the two operators of the DAG and define the workflow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save it and enable this DAG in the Airflow UI. Once enabled, you will notice
    the `wait_holiday_api_ingest` task will be in the `RUNNING` state and will not
    proceed to the other task, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.34 – The wait_holiday_api_ingest task in the running state](img/Figure_9.34_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.34 – The wait_holiday_api_ingest task in the running state
  prefs: []
  type: TYPE_NORMAL
- en: 'You will also notice the log for this task looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Now, we will enable and run `holiday_ingest` (if it is not enabled yet).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, go back to `external_sensor_dag`, and you will see it finished successfully,
    as shown in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.35 – external_sensor_dag showing success](img/Figure_9.35_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.35 – external_sensor_dag showing success
  prefs: []
  type: TYPE_NORMAL
- en: 'If we examine the logs of `date_tasks`, you will see the following output on
    the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the complete log for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.36 – date_tasks logs in the Airflow UI](img/Figure_9.36_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.36 – date_tasks logs in the Airflow UI
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s understand how it works in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start by taking a look at our `wait_holiday_api_ingest` task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '`ExternalTaskSensor` is a sensor type that will only execute if another task
    outside its DAG finishes with a specific status defined on the `allowed_states`
    parameter. The default value for this parameter is `SUCCESS`.'
  prefs: []
  type: TYPE_NORMAL
- en: The sensor will search for a specific DAG and task in Airflow using the `external_dag_id`
    and `external_task_id` parameters, which we have defined as `holiday_ingest` and
    `holiday_api_ingestion`, respectively. Finally, `execution_delta` will determine
    the time interval at which to poke the external DAG again.
  prefs: []
  type: TYPE_NORMAL
- en: Once it finishes, the DAG will remain in the `SUCCESS` state unless we define
    a different behavior in the default arguments. If we clear its status, it will
    return to the `RUNNING` mode until the sensor criteria are met again.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We know Airflow is a powerful tool, but it is not immune from occasional failures.
    Internally, Airflow has its routes to reach internal and external DAGs, which
    can occasionally fail. For example, one of these errors might be a DAG not being
    found, which can happen due to various reasons such as misconfiguration or connectivity
    issues. You can see a screenshot of one of these errors here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.37 – Occasional 403 error log in an Airflow task](img/Figure_9.37_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.37 – Occasional 403 error log in an Airflow task
  prefs: []
  type: TYPE_NORMAL
- en: Looking closely, we can observe that for several seconds, one of the Airflow
    workers lost permission to access or retrieve information from another worker.
    If this happens to you, disable your DAG and enable it again.
  prefs: []
  type: TYPE_NORMAL
- en: XCom
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we used an output file to perform an action, but we can also
    use the output of a task without requiring it to write a file somewhere. Instead,
    we can use the **XCom** (short for **cross-communications**) mechanism to help
    us with it.
  prefs: []
  type: TYPE_NORMAL
- en: To use XCom across tasks, we can simply use *push* and *pull* the values using
    the `xcom_push` and `xcom_pull` methods inside the required tasks. Behind the
    scenes, Airflow stores those values temporarily in one of its databases, making
    it easier to access them again.
  prefs: []
  type: TYPE_NORMAL
- en: To check your stored XComs in the Airflow UI, click on the **Admin** button
    and select **XCom**.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In production environments, XComs might have a purge routine. Check with the
    Airflows administrators if you need to keep a value for longer.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read more about XComs on the official documentation page here: [https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can learn more about this operator on the Airflow official documentation
    page: [https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/external_task_sensor.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/external_task_sensor.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://airflow.apache.org/docs/apache-airflow/2.2.4/best-practices.xhtml](https://airflow.apache.org/docs/apache-airflow/2.2.4/best-practices.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.qubole.com/tech-blog/apache-airflow-tutorial-dags-tasks-operators-sensors-hooks-xcom](https://www.qubole.com/tech-blog/apache-airflow-tutorial-dags-tasks-operators-sensors-hooks-xcom)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://python.plainenglish.io/apache-airflow-how-to-correctly-setup-custom-plugins-2f80fe5e3dbe](https://python.plainenglish.io/apache-airflow-how-to-correctly-setup-custom-plugins-2f80fe5e3dbe)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://copyprogramming.com/howto/airflow-how-to-mount-airflow-cfg-in-docker-container](https://copyprogramming.com/howto/airflow-how-to-mount-airflow-cfg-in-docker-container)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
