- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Putting Everything Together with Airﬂow
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Airflow 整合一切
- en: So far, we have covered the different aspects and steps of data ingestion. We
    have seen how to configure and ingest structured and unstructured data, what analytical
    data is, and how to improve logs for more insightful monitoring and error handling.
    Now is the time to group all this information to create something similar to a
    real-world project.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了数据采集的不同方面和步骤。我们看到了如何配置和采集结构化和非结构化数据，什么是分析数据，以及如何改进日志以进行更有洞察力的监控和错误处理。现在是时候将这些信息组合起来，创建一个类似于真实世界项目的结构。
- en: From now on, in the following chapters, we will use Apache Airflow, an open
    source platform that allows us to create, schedule, and monitor workflows. Let’s
    start our journey by configuring and understanding the fundamental concepts of
    Apache Airflow and how powerful this tool is.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，在接下来的章节中，我们将使用 Apache Airflow，这是一个开源平台，允许我们创建、调度和监控工作流程。让我们通过配置和理解 Apache
    Airflow 的基本概念以及这个工具的强大功能开始我们的旅程。
- en: 'In this chapter, you will learn about the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解以下主题：
- en: Configuring Airflow
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 Airflow
- en: Creating DAGs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建 DAG
- en: Creating custom operators
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建自定义操作符
- en: Conﬁguring sensors
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置传感器
- en: Creating connectors in Airﬂow
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Airflow 中创建连接器
- en: Creating parallel ingest tasks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建并行数据采集任务
- en: Deﬁning ingest-dependent DAGs
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义依赖数据采集的 DAG
- en: By the end of this chapter, you will have learned about the most important components
    of Airﬂow and how to conﬁgure them, including how to solve related issues in this
    process.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将了解 Airflow 的最重要的组件以及如何配置它们，包括在此过程中解决相关问题的方法。
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can find the code for this chapter in the GitHub repository here: [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此 GitHub 仓库中找到本章的代码：[https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook)
- en: Installing Airflow
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 Airflow
- en: This chapter requires that Airflow is installed on your local machine. You can
    install it directly on your **Operating System** (**OS**) or using a Docker image.
    For more information regarding this, refer to the *Configuring Docker for Airflow*
    recipe in [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求您在本地机器上安装 Airflow。您可以直接在您的**操作系统**（**OS**）上安装它，或者使用 Docker 镜像。有关更多信息，请参阅[*第
    1 章*](B19453_01.xhtml#_idTextAnchor022)中的*配置 Docker 用于 Airflow*配方。
- en: Configuring Airflow
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置 Airflow
- en: '**Apache Airflow** has many capabilities and a quick setup, which helps us
    start designing our workflows as code. Some additional configurations might be
    required as we progress with the workflows and into data processing. Gladly, Airflow
    has a dedicated file for inserting other arrangements without changing anything
    within its core.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apache Airflow** 具有许多功能和快速设置，这有助于我们以代码的形式开始设计我们的工作流程。随着我们工作流程和数据处理的进展，可能需要一些额外的配置。幸运的是，Airflow
    有一个专门的文件，用于插入其他安排，而无需更改其核心中的任何内容。'
- en: In this recipe, we will learn more about the `airflow.conf` file, how to use
    it, and other valuable configurations required to execute the other recipes in
    this chapter. We will also cover where to find this file and how other environment
    variables work with this tool. Understanding these concepts in practice helps
    us to identify potential improvements or solve problems.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将了解更多关于 `airflow.conf` 文件的信息，如何使用它，以及执行本章中其他配方所需的其他有价值配置。我们还将介绍在哪里可以找到此文件，以及其他环境变量如何与此工具一起工作。在实践中理解这些概念有助于我们识别潜在的改进或解决问题。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Before moving on to the code, ensure your Airflow runs correctly. You can do
    that by checking the Airflow UI at this link: `http://localhost:8080`.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续代码之前，请确保您的 Airflow 运行正确。您可以通过检查此链接的 Airflow UI 来做到这一点：`http://localhost:8080`。
- en: 'If you are using a Docker container (as I am) to host your Airflow application,
    you can check its status on the terminal with the following command:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您像我一样使用 Docker 容器来托管您的 Airflow 应用程序，您可以使用以下命令在终端检查其状态：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This is the output:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '![Figure 9.1 – Airflow Docker containers running](img/Figure_9.01_B19453.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – 运行的 Airflow Docker 容器](img/Figure_9.01_B19453.jpg)'
- en: Figure 9.1 – Airflow Docker containers running
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 运行的 Airflow Docker 容器
- en: 'Or, you can check the container status on Docker Desktop, as in the following
    screenshot:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以在 Docker Desktop 上检查容器状态，如下面的截图所示：
- en: '![Figure 9.2 – Docker Desktop view of Airflow containers running](img/Figure_9.02_B19453.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2 – Docker Desktop中运行Airflow容器的视图](img/Figure_9.02_B19453.jpg)'
- en: Figure 9.2 – Docker Desktop view of Airflow containers running
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 – Docker Desktop中运行Airflow容器的视图
- en: How to do it…
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Here are the steps to perform this recipe:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此食谱的步骤如下：
- en: 'Let’s start by installing the MongoDB additional provider for Airflow. If you
    are using the `docker-compose.yaml` file, open it and add `apache-airflow-providers-mongo`
    inside `_PIP_ADDITIONAL_REQUIREMENTS`. Your code will look like this:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先安装Airflow的MongoDB附加提供程序。如果你使用的是`docker-compose.yaml`文件，打开它，并在`_PIP_ADDITIONAL_REQUIREMENTS`内添加`apache-airflow-providers-mongo`。你的代码将看起来像这样：
- en: '![Figure 9.3 – The docker-compose.yaml file in the environment variables section](img/Figure_9.03_B19453.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3 – 环境变量部分中的docker-compose.yaml文件](img/Figure_9.03_B19453.jpg)'
- en: Figure 9.3 – The docker-compose.yaml file in the environment variables section
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – 环境变量部分中的docker-compose.yaml文件
- en: 'If you are hosting Airflow directly on your machine, you can do the same installation
    using **PyPi**: [https://pypi.org/project/apache-airflow-providers-mongo/](https://pypi.org/project/apache-airflow-providers-mongo/).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你直接在你的机器上托管Airflow，你可以使用**PyPi**进行相同的安装：[https://pypi.org/project/apache-airflow-providers-mongo/](https://pypi.org/project/apache-airflow-providers-mongo/)。
- en: 'Next, we will create a folder called `files_to_test`, and inside it, create
    two more folders: `output_files` and `sensors_files`. You don’t need to worry
    about its usage yet since it will be used later in this chapter. Your Airflow
    folder structure should look like this:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个名为`files_to_test`的文件夹，并在其中创建两个额外的文件夹：`output_files`和`sensors_files`。你目前不需要担心它的用途，因为它将在本章的后面使用。你的Airflow文件夹结构应该看起来像这样：
- en: '![Figure 9.4 – Airflow local directory folder structure](img/Figure_9.04_B19453.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 – Airflow本地目录文件夹结构](img/Figure_9.04_B19453.jpg)'
- en: Figure 9.4 – Airflow local directory folder structure
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – Airflow本地目录文件夹结构
- en: Now, let’s mount the volumes of our Docker image. You can skip this part if
    you are not using Docker to host Airflow.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们挂载Docker镜像的卷。如果你不是使用Docker托管Airflow，可以跳过这部分。
- en: 'In your `docker-compose.yaml` file, under the `volume` parameter, add the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的`docker-compose.yaml`文件中，在`volume`参数下，添加以下内容：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Your final `volumes` section will look like this:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你的最终`volumes`部分将看起来像这样：
- en: '![Figure 9.5 – docker-compose.yaml volumes](img/Figure_9.05_B19453.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5 – docker-compose.yaml的volumes](img/Figure_9.05_B19453.jpg)'
- en: Figure 9.5 – docker-compose.yaml volumes
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – docker-compose.yaml的volumes部分
- en: Stop and restart your container so these changes can be propagated.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 停止并重新启动你的容器，以便这些更改可以传播。
- en: 'Finally, we will fix a bug in the `docker-compose.yaml` file. This official
    fix for this bug is within the Airflow official documentation and therefore wasn’t
    included in the Docker image. You can see the complete issue and the solution
    here: [https://github.com/apache/airflow/discussions/24809](https://github.com/apache/airflow/discussions/24809).'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将修复`docker-compose.yaml`文件中的错误。这个错误的官方修复在Airflow官方文档中，因此没有包含在Docker镜像中。你可以在这里看到完整的问题和解决方案：[https://github.com/apache/airflow/discussions/24809](https://github.com/apache/airflow/discussions/24809)。
- en: 'To fix the bug, go to the `airflow-init` section of the `docker-compose` file
    and insert `_PIP_ADDITIONAL_REQUIREMENTS: ''''` inside the `environment` parameter.
    Your code will look like this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '要修复错误，请转到`docker-compose`文件的`airflow-init`部分，并在`environment`参数内插入`_PIP_ADDITIONAL_REQUIREMENTS:
    ''''`。你的代码将看起来像这样：'
- en: '![Figure 9.6 – The docker-compose.yaml environment variables section with PIP_ADDITIONAL_REQUIREMENTS](img/Figure_9.06_B19453.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图9.6 – docker-compose.yaml中带有PIP_ADDITIONAL_REQUIREMENTS的环境变量部分](img/Figure_9.06_B19453.jpg)'
- en: Figure 9.6 – The docker-compose.yaml environment variables section with PIP_ADDITIONAL_REQUIREMENTS
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – docker-compose.yaml中带有PIP_ADDITIONAL_REQUIREMENTS的环境变量部分
- en: 'This action will fix the following issue registered on GitHub: [https://github.com/apache/airflow/pull/23517](https://github.com/apache/airflow/pull/23517).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此操作将修复GitHub上注册的以下问题：[https://github.com/apache/airflow/pull/23517](https://github.com/apache/airflow/pull/23517)。
- en: How it works…
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The configuration presented here is simple. However, it guarantees the application
    will keep working through the chapter recipes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示的配置很简单。然而，它保证了应用程序将保持通过本章食谱的正常运行。
- en: Let’s start with the package we installed in *step 1*. Like other frameworks
    or platforms, Airflow has its *batteries* included, which means it already comes
    with various packages. But, as its popularity started to increase, it started
    to require other types of connections or operators, which the open source community
    took care of.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 *步骤 1* 中安装的软件包开始。与其他框架或平台一样，Airflow 也包含其 *电池*，这意味着它已经包含了各种软件包。但随着其知名度的提高，它开始需要其他类型的连接或操作符，这是开源社区负责的。
- en: 'You can find a list of released packages that can be installed on Airflow here:
    [https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.xhtml](https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.xhtml).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此处找到可以安装在 Airflow 上的已发布软件包列表：[https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.xhtml](https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.xhtml)。
- en: Before jumping into other code explanations, let’s understand the `volume` section
    inside the `docker-compose.yaml` file. This configuration allows Airflow to see
    which folders reflect the same respective ones inside the Docker container without
    the necessity to upload code using a Docker command every time. In other words,
    we can synchronously add our **Directed Acyclic Graph** (**DAG**) files and new
    operators and see some logs, among other things, and this will be reflected inside
    the container.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在跳转到其他代码解释之前，让我们先了解 `docker-compose.yaml` 文件中的 `volume` 部分。这个配置允许 Airflow 看到哪些文件夹反映了
    Docker 容器内部相应的文件夹，而无需每次都使用 Docker 命令上传代码。换句话说，我们可以同步添加我们的 **有向无环图** (**DAG**)
    文件和新的操作符，并查看一些日志等，这些都会在容器内部反映出来。
- en: 'Next, we declared the Docker mount volume configurations for two parts: the
    new folder we created (`files_to_test`) and the `airflow.cfg` file. The first
    one will allow Airflow to replicate the `files_to_test` local folder inside the
    container, so we can use it to use files in a more simplified way. Otherwise,
    if we try to use it without the mounting volume, the following error will appear
    when trying to retrieve any file:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们声明了 Docker 挂载卷配置的两个部分：我们创建的新文件夹（`files_to_test`）和 `airflow.cfg` 文件。第一个将允许
    Airflow 在容器内部复制 `files_to_test` 本地文件夹，这样我们就可以更简单地使用文件。否则，如果我们尝试在不挂载卷的情况下使用它，当尝试检索任何文件时，将会出现以下错误：
- en: '![Figure 9.7 – Error in Airflow when the folder is not referred to in the container
    volume](img/Figure_9.07_B19453.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.7 – 当文件夹未在容器卷中引用时 Airflow 中的错误](img/Figure_9.07_B19453.jpg)'
- en: Figure 9.7 – Error in Airflow when the folder is not referred to in the container
    volume
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – 当文件夹未在容器卷中引用时 Airflow 中的错误
- en: Although we will not use the `airflow.cfg` file, for now, it is a good practice
    to know how to access this file and what it is used for. This file contains the
    Airflow configurations and can be edited to include more. Usually, sensitive data
    is stored inside it to prevent other people from having improper access since,
    by default, the content of the `airflow.cfg` file cannot be accessed in the UI.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们现在不会使用 `airflow.cfg` 文件，但了解如何访问此文件以及它的用途是一个好的实践。此文件包含 Airflow 的配置，并且可以编辑以包含更多内容。通常，敏感数据存储在其中，以防止其他人不当访问，因为默认情况下，`airflow.cfg`
    文件的内容在 UI 中无法访问。
- en: Note
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Be very cautious when changing or handling the `airflow.cfg` file. This file
    contains all the required configurations and other relevant settings to make Airflow
    work. We will explore more about this in [*Chapter 10*](B19453_10.xhtml#_idTextAnchor364)*.*
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在更改或处理 `airflow.cfg` 文件时要非常小心。此文件包含所有必需的配置以及其他相关设置，以使 Airflow 运作。我们将在 [*第 10
    章*](B19453_10.xhtml#_idTextAnchor364)* 中进一步探讨这个问题。
- en: See also
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'For more information about the Docker image, see the documentation page here:
    [https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.xhtml).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Docker 镜像的更多信息，请参阅以下文档页面：[https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.xhtml)。
- en: Creating DAGs
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 DAG
- en: The core concept of Airflow is based on DAGs, which collect, group, and organize
    tasks to be executed in a specific order. A DAG is also responsible for managing
    the dependencies between its tasks. Simply put, it is not concerned about what
    a task is doing but just *how* to execute it. Typically, a DAG starts at a scheduled
    time, but we can also define dependencies between other DAGs so that they will
    start based on their execution statuses.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 的核心概念基于 DAG，它收集、分组和组织以特定顺序执行的任务。DAG 还负责管理其任务之间的依赖关系。简单来说，它不关心任务在做什么，而只是关心*如何*执行它。通常，DAG
    从预定时间开始，但我们也可以定义其他 DAG 之间的依赖关系，以便它们根据其执行状态启动。
- en: We will create our first DAG in this recipe and set it to run based on a specific
    schedule. With this first step, we enter into practically designing our first
    workflow.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本食谱中创建我们的第一个 DAG，并设置它根据特定的计划运行。通过这一步，我们实际上进入了设计第一个工作流程。
- en: Getting ready
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: Please refer to the *Getting ready* section in the *Configuring Airflow* recipe
    for this recipe since we will handle it with the same technology.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考“准备就绪”部分中的“配置 Airflow”食谱，因为我们将使用相同的技术来处理它。
- en: 'Also, let’s create a directory called `ids_ingest` inside our `dags` folder.
    Inside the `ids_ingest` folder, we will create two files: `__init__.py` and `ids_ingest_dag.py`.
    The final structure will look as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，让我们在 `dags` 文件夹内创建一个名为 `ids_ingest` 的目录。在 `ids_ingest` 文件夹内，我们将创建两个文件：`__init__.py`
    和 `ids_ingest_dag.py`。最终的目录结构将如下所示：
- en: '![Figure 9.8 – Airflow’s local directory structure with the ids_ingest DAG](img/Figure_9.08_B19453.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.8 – 包含 ids_ingest DAG 的 Airflow 本地目录结构](img/Figure_9.08_B19453.jpg)'
- en: Figure 9.8 – Airflow’s local directory structure with the ids_ingest DAG
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 – 包含 ids_ingest DAG 的 Airflow 本地目录结构
- en: How to do it…
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'In this exercise, we will write a DAG that retrieves the IDs of the `github_events.json`
    file. Open `ids_ingest_dag.py`, and let’s add the content to write our first DAG:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将编写一个 DAG，用于检索 `github_events.json` 文件的 IDs。打开 `ids_ingest_dag.py`，让我们添加内容来编写我们的第一个
    DAG：
- en: 'Let’s start by importing the libraries we will use in this script. I like to
    separate the imports from the Airflow library and Python’s library as a good practice:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先导入在这个脚本中将使用的库。我喜欢将 Airflow 库和 Python 库的导入分开，作为一种良好的实践：
- en: '[PRE2]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we will define `default_args` for our DAG, as you can see here:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将为我们的 DAG 定义 `default_args`，正如您在这里可以看到的：
- en: '[PRE3]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we will create a Python function that receives the JSON file and returns
    the IDs inside it. Since it is a small function, we can create it inside the DAG’s
    file:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个 Python 函数，该函数接收 JSON 文件并返回其中的 IDs。由于这是一个小型函数，我们可以在 DAG 的文件中创建它：
- en: '[PRE4]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we will instantiate our DAG object, and inside it, we will define two
    operators: a `BashOperator` instance to show a console message and `PythonOperator`
    to execute the function we just created, as you can see here:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实例化我们的 DAG 对象，并在其中定义两个操作符：一个 `BashOperator` 实例用于显示控制台消息，以及 `PythonOperator`
    来执行我们刚刚创建的函数，如下所示：
- en: '[PRE5]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Make sure you save the file before jumping to the next step.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在跳到下一步之前保存文件。
- en: 'Now, head over to the Airflow UI. Although plenty of DAG examples are provided
    by the Airflow team, you should look for a DAG called `simple_ids_ingest`. You
    will notice the DAG is not enabled. Click on the toggle button to enable it, and
    you should have something like the following:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，前往 Airflow UI。尽管 Airflow 团队提供了大量的 DAG 示例，但您应该寻找一个名为 `simple_ids_ingest` 的
    DAG。您会注意到 DAG 未启用。点击切换按钮以启用它，您应该看到如下内容：
- en: '![Figure 9.9 – The Airflow DAG enabled on the UI](img/Figure_9.09_B19453.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.9 – 在 UI 上启用的 Airflow DAG](img/Figure_9.09_B19453.jpg)'
- en: Figure 9.9 – The Airflow DAG enabled on the UI
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 – 在 UI 上启用的 Airflow DAG
- en: 'As soon as you enable it, the DAG will start running. Click on the DAG name
    to be redirected to the DAG’s page, as you can see in the following screenshot:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦启用，DAG 将开始运行。点击 DAG 名称，将重定向到 DAG 的页面，如下面的截图所示：
- en: '![Figure 9.10 – DAG Grid page view](img/Figure_9.10_B19453.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.10 – DAG 网格页面视图](img/Figure_9.10_B19453.jpg)'
- en: Figure 9.10 – DAG Grid page view
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10 – DAG 网格页面视图
- en: 'If everything is well configured, your page should look like this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切配置正确，您的页面应该看起来像这样：
- en: '![Figure 9.11 – DAG running successfully in Graph page view](img/Figure_9.11_B19453.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.11 – 图形页面视图中的 DAG 成功运行](img/Figure_9.11_B19453.jpg)'
- en: Figure 9.11 – DAG running successfully in Graph page view
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11 – 图形页面视图中的 DAG 成功运行
- en: 'Then, click on the `get_id_from_json` task. A small window will show up as
    follows:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，点击 `get_id_from_json` 任务。会出现一个小窗口，如下所示：
- en: '![Figure 9.12 – Task options](img/Figure_9.12_B19453.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图9.12 – 任务选项](img/Figure_9.12_B19453.jpg)'
- en: Figure 9.12 – Task options
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12 – 任务选项
- en: 'Then, click the **Log** button. You will be redirected to a new page with the
    logs for this task, as seen here:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，点击**日志**按钮。您将被重定向到一个新页面，其中包含此任务的日志，如下所示：
- en: '![Figure 9.13 – Task logs in the Airflow UI](img/Figure_9.13_B19453.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图9.13 – Airflow UI中的任务日志](img/Figure_9.13_B19453.jpg)'
- en: Figure 9.13 – Task logs in the Airflow UI
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13 – Airflow UI中的任务日志
- en: As we can see in the preceding screenshot, our task successfully finished and
    returned the IDs as we expected. You can see the results in the `INFO` log under
    the `AIRFLOW_CTX_DAG_RUN` message.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个屏幕截图所示，我们的任务成功完成并返回了我们预期的ID。您可以在`AIRFLOW_CTX_DAG_RUN`消息下的`INFO`日志中看到结果。
- en: How it works…
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We created our first DAG with a few lines to retrieve and show a list of IDs
    from a JSON file. Now, let’s understand how it works.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用几行代码创建了我们的第一个DAG，用于检索并显示从JSON文件中获取的ID列表。现在，让我们了解它是如何工作的。
- en: 'To start with, we created our files under the `dags` directory. It happens
    because, by default, Airflow will understand everything inside of it as a DAG
    file. The folder we created inside of it was just for organization purposes, and
    Airflow will ignore it. Along with the `ids_ingest_dag.py` file, we also made
    an `__init__.py` file. This file internally tells Airflow to look inside this
    folder. As a result, you will see the following structure:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在`dags`目录下创建了我们的文件。这是因为，默认情况下，Airflow会将它里面的所有内容都理解为一个DAG文件。我们在其中创建的文件夹只是为了组织目的，Airflow会忽略它。除了`ids_ingest_dag.py`文件外，我们还创建了一个`__init__.py`文件。此文件内部告诉Airflow查看这个文件夹。因此，您将看到以下结构：
- en: '![Figure 9.14 – Airflow local directory structure with the ids_ingest DAG](img/Figure_9.14_B19453.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图9.14 – 带有ids_ingest DAG的Airflow本地目录结构](img/Figure_9.14_B19453.jpg)'
- en: Figure 9.14 – Airflow local directory structure with the ids_ingest DAG
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14 – 带有ids_ingest DAG的Airflow本地目录结构
- en: Note
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: As you might be wondering, it is possible to change this configuration, but
    I don’t recommend this at all since other internal packages might depend on it.
    Do it only in the case of extreme necessity.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能想知道的，可以更改此配置，但我根本不推荐这样做，因为其他内部包可能依赖于它。只有在极端必要的情况下才这样做。
- en: 'Now, let’s take a look at our instantiated DAG:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的实例化DAG：
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can observe, creating a DAG is simple, and its parameters are spontaneous.
    `dag_id` is crucial and must be unique; otherwise, it can create confusion and
    merge with other DAGs. The `default_args` we declared in *step 2* will guide the
    DAG, telling when it needs to be executed, its user owner, the number of retries
    in case of a failure, and other valuable parameters. After the `as dag` declaration,
    we inserted the bash and Python operators, and they must be indented to be understood
    as the DAG’s tasks.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所观察到的，创建一个DAG很简单，其参数是自发的。`dag_id`至关重要，必须是唯一的；否则，它可能会造成混淆并与其他DAG合并。我们在*步骤2*中声明的`default_args`将指导DAG，告诉它何时需要执行，它的用户所有者，在失败情况下的重试次数以及其他有价值的参数。在`as
    dag`声明之后，我们插入了bash和Python运算符，并且它们必须缩进来被理解为DAG的任务。
- en: 'Finally, to set our workflow, we declared the following line:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了设置我们的工作流程，我们声明了以下行：
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As we might guess, it sets the order of which task should be executed first.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们可能猜测的那样，它设置了任务执行的顺序。
- en: There's more…
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: 'We saw how easy it is to create a task to execute a Python function and a bash
    command. By default, Airflow comes with some handy operators to be used daily
    within a data ingestion pipeline. For more information, you can refer to the official
    documentation page here: [https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.xhtml).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了创建一个任务来执行Python函数和bash命令是多么容易。默认情况下，Airflow附带了一些方便的运算符，可以在数据摄取管道中每天使用。更多信息，您可以参考官方文档页面：[https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.xhtml)。
- en: Tasks, operators, XCom, and others
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 任务、运算符、XCom和其他
- en: 'Airflow DAGs are a powerful way to group and execute operations. Besides the
    task and operators we saw here, DAGs support other types of workloads and communication
    across other tasks or DAGs. Unfortunately, since that is not the main subject
    of this book, we will not cover those concepts in detail, but I highly recommend
    reading the official documentation here: [https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/index.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/index.xhtml).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow DAG 是一种强大的方式来分组和执行操作。除了我们在这里看到的任务和操作符之外，DAG 还支持其他类型的工作负载和跨其他任务或 DAG
    的通信。不幸的是，由于这并不是本书的主要内容，我们不会详细涵盖这些概念，但我强烈建议您阅读官方文档：[https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/index.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/index.xhtml)。
- en: Error handling
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 错误处理
- en: 'If you encounter any errors while building this DAG, you can use the instructions
    from *step 7* and *step 8* to debug it. You can see a preview here of how the
    tasks look when an error occurs:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在构建此 DAG 时遇到任何错误，您可以使用 *步骤 7* 和 *步骤 8* 中的说明来调试它。您可以看到错误发生时任务的外观预览：
- en: '![Figure 9.15 – DAG Graph page view showing a running error](img/Figure_9.15_B19453.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.15 – DAG 图页面视图显示运行错误](img/Figure_9.15_B19453.jpg)'
- en: Figure 9.15 – DAG Graph page view showing a running error
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15 – DAG 图页面视图显示运行错误
- en: See also
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'You can find the code for the Airflow example DAGs on their official GitHub
    page here: [https://github.com/apache/airflow/tree/main/airflow/example_dags](https://github.com/apache/airflow/tree/main/airflow/example_dags).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在他们的官方 GitHub 页面找到 Airflow 示例 DAG 的代码：[https://github.com/apache/airflow/tree/main/airflow/example_dags](https://github.com/apache/airflow/tree/main/airflow/example_dags)。
- en: Creating custom operators
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建自定义操作符
- en: As seen in the previous recipe, *Creating DAGs*, it is nearly impossible to
    create a DAG without instantiating a task or, in other words, defining an operator.
    Operators are responsible for holding the logic required to process data in the
    pipeline.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个菜谱 *创建 DAG* 所见，创建一个没有实例化任务或换句话说，定义操作符的 DAG 几乎是不可能的。操作符负责在管道中处理数据所需的逻辑。
- en: We also know that Airflow already has predefined operators, allowing dozens
    of ways to ingest and process data. Now, it is time to put into practice how to
    create custom operators. Custom operators allow us to apply specific logic to
    a related project or data pipeline.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还知道 Airflow 已经有预定义的操作符，允许以数十种方式摄取和处理数据。现在，是时候将创建自定义操作符的实践付诸实施了。自定义操作符允许我们将特定的逻辑应用于相关项目或数据管道。
- en: You will learn how to create a simple customized operator in this recipe. Although
    it is very basic, you will be able to apply the foundations of this technique
    to different scenarios.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在本菜谱中，您将学习如何创建一个简单的自定义操作符。虽然它非常基础，但您将能够将此技术的基石应用于不同的场景。
- en: In this recipe, we will create a custom operator to connect to and retrieve
    data from the HolidayAPI, the same as we saw previously, in [*Chapter 2*](B19453_02.xhtml#_idTextAnchor064).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本菜谱中，我们将创建一个自定义操作符来连接并从 HolidayAPI 检索数据，就像我们在 [*第 2 章*](B19453_02.xhtml#_idTextAnchor064)
    中看到的那样。
- en: Getting ready
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Please, refer to the *Getting ready* section in the *Configuring Airflow* recipe
    for this recipe since we will handle it with the same technology.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅 *准备工作* 部分，该部分在 *配置 Airflow* 菜单中，因为我们将使用相同的技术来处理它。
- en: 'We also need to add an environment variable to store our API secret. To do
    so, select the **Variable** item under the **Admin** menu in the Airflow UI, and
    you will be redirected to the desired page. Now, click the **+** button to add
    a new variable, as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要添加一个环境变量来存储我们的 API 密钥。为此，在 Airflow UI 的 **Admin** 菜单下选择 **Variable** 项，然后您将被重定向到所需页面。现在，点击
    **+** 按钮添加一个新变量，如下所示：
- en: '![Figure 9.16 – The Variable page in the Airflow UI](img/Figure_9.16_B19453.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.16 – Airflow UI 中的变量页面](img/Figure_9.16_B19453.jpg)'
- en: Figure 9.16 – The Variable page in the Airflow UI
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16 – Airflow UI 中的变量页面
- en: 'On the `SECRET_HOLIDAY_API` under the **Key** field and your API secret under
    the **Value** field. Use the same values you used to execute the *Retrieving data
    using API authentication* recipe in [*Chapter 2*](B19453_02.xhtml#_idTextAnchor064).
    Save it and you will be redirected to the **Variables** page, as shown in the
    following screenshot:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **Key** 字段下的 `SECRET_HOLIDAY_API` 和 **Value** 字段下的您的 API 密钥。使用您在 [*第 2 章*](B19453_02.xhtml#_idTextAnchor064)
    中执行 *使用 API 认证检索数据* 菜单时使用的相同值。保存它，然后您将被重定向到 **Variables** 页面，如下面的截图所示：
- en: '![Figure 9.17 – The Airflow UI with a new variable to store the HolidayAPI
    secret](img/Figure_9.17_B19453.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.17 – 带有存储 HolidayAPI 密钥的新变量的 Airflow UI](img/Figure_9.17_B19453.jpg)'
- en: Figure 9.17 – The Airflow UI with a new variable to store the HolidayAPI secret
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.17 – 带有存储 HolidayAPI 密钥的新变量的 Airflow UI
- en: Now, we are ready to create our custom operator.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已准备好创建我们的自定义操作符。
- en: How to do it…
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'The code we will use to create the custom operator is the same one we saw in
    [*Chapter 2*](B19453_02.xhtml#_idTextAnchor064), in the *Retrieving data using
    API authentication* recipe, with some alterations to fit Airflow’s requirements.
    Here are the steps for it:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于创建自定义操作符的代码与我们在 [*第 2 章*](B19453_02.xhtml#_idTextAnchor064) 的 *使用 API 认证检索数据*
    菜单中看到的代码相同，但进行了一些修改以适应 Airflow 的要求。以下是它的步骤：
- en: 'Let’s start by creating the structure inside the `plugins` folder. Since we
    want to make a custom operator, we need to create a folder called `operators`,
    where we will put a Python file called `holiday_api_plugin.py`. Your file tree
    should look like this:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从在 `plugins` 文件夹内创建结构开始。由于我们想要创建一个自定义操作符，我们需要创建一个名为 `operators` 的文件夹，我们将在这里放置一个名为
    `holiday_api_plugin.py` 的 Python 文件。您的文件树应该看起来像这样：
- en: '![Figure 9.18 – Airflow’s local directory structure for the plugins folder](img/Figure_9.18_B19453.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.18 – Airflow 插件文件夹的本地目录结构](img/Figure_9.18_B19453.jpg)'
- en: Figure 9.18 – Airflow’s local directory structure for the plugins folder
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.18 – Airflow 插件文件夹的本地目录结构
- en: 'We will create some code inside `holiday_api_plugin.py`, starting with the
    library imports and declaring a global variable for where our file output needs
    to be placed:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在 `holiday_api_plugin.py` 中创建一些代码，从库导入和声明一个全局变量开始，该变量用于指定我们的文件输出需要放置的位置：
- en: '[PRE8]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we need to create a Python class, declare its constructors, and finally
    insert the exact code from [*Chapter 2*](B19453_02.xhtml#_idTextAnchor064) inside
    a function called `execute`:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们需要创建一个 Python 类，声明其构造函数，并最终将 [*第 2 章*](B19453_02.xhtml#_idTextAnchor064)
    中的确切代码插入到名为 `execute` 的函数中：
- en: '[PRE9]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Save the file and our operator is ready. Now, we need to create the DAG to execute
    it.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 保存文件，我们的操作符就准备好了。现在，我们需要创建 DAG 来执行它。
- en: 'Using the same logic as in the *Creating DAGs* recipe, we will create a file
    called `holiday_ingest_dag.py`. Your new DAG directory tree should look like this:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用与 *创建 DAG* 菜单相同的逻辑，我们将创建一个名为 `holiday_ingest_dag.py` 的文件。您的新 DAG 目录树应该看起来像这样：
- en: '![Figure 9.19 – Airflow’s directory structure for the holiday_ingest DAG folder](img/Figure_9.19_B19453.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.19 – holiday_ingest DAG 文件夹的 Airflow 目录结构](img/Figure_9.19_B19453.jpg)'
- en: Figure 9.19 – Airflow’s directory structure for the holiday_ingest DAG folder
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.19 – holiday_ingest DAG 文件夹的 Airflow 目录结构
- en: 'Now, let’s insert our DAG code inside the `holiday_ingest_dag.py` file and
    save it:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将我们的 DAG 代码插入到 `holiday_ingest_dag.py` 文件中并保存：
- en: '[PRE10]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'For the full code, refer to the GitHub repository here: [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_custom_operators](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_custom_operators).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 对于完整代码，请参阅以下 GitHub 仓库：[https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_custom_operators](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_custom_operators)。
- en: 'Then, go to the Airflow UI, look for the `holiday_ingest` DAG, and enable it.
    It will look like the following figure:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，转到 Airflow UI，查找 `holiday_ingest` DAG，并启用它。它将看起来像以下图所示：
- en: '![Figure 9.20 – The holiday_ingest DAG enabled in the Airflow UI](img/Figure_9.20_B19453.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.20 – 在 Airflow UI 中启用的 holiday_ingest DAG](img/Figure_9.20_B19453.jpg)'
- en: Figure 9.20 – The holiday_ingest DAG enabled in the Airflow UI
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.20 – 在 Airflow UI 中启用的 holiday_ingest DAG
- en: Your job will start to run immediately.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 您的工作将立即开始。
- en: 'Now, let’s find the task logs by following the same steps from the *Creating
    DAGs* recipe, but now clicking on the `holiday_api_ingestion` task. Your log page
    should look like the following figure:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们按照 *创建 DAG* 菜单中的相同步骤查找任务日志，但现在点击 `holiday_api_ingestion` 任务。您的日志页面应该看起来像以下图所示：
- en: '![Figure 9.21 – holiday_api_ingestion task logs](img/Figure_9.21_B19453.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.21 – holiday_api_ingestion 任务日志](img/Figure_9.21_B19453.jpg)'
- en: Figure 9.21 – holiday_api_ingestion task logs
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.21 – holiday_api_ingestion 任务日志
- en: 'Finally, let’s see whether the output file was created successfully. Go to
    the `files_to_test` folder, click on the `output_files` folder, and if everything
    was successfully configured, a file called `holiday_brazil.json` will be inside
    it. See the following figure for reference:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们看看输出文件是否成功创建。前往 `files_to_test` 文件夹，点击 `output_files` 文件夹，如果一切配置成功，里面将包含一个名为
    `holiday_brazil.json` 的文件。参见以下图示以供参考：
- en: '![Figure 9.22 – holiday_brazil.json inside the output_files screenshot](img/Figure_9.22_B19453.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.22 – output_files 截图中的 holiday_brazil.json](img/Figure_9.22_B19453.jpg)'
- en: Figure 9.22 – holiday_brazil.json inside the output_files screenshot
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.22 – output_files 截图中的 holiday_brazil.json
- en: 'The beginning of the output file should look like this:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 输出文件的开始部分应如下所示：
- en: '![Figure 9.23 – The first lines of holiday_brazil.json](img/Figure_9.23_B19453.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.23 – holiday_brazil.json 的第一行](img/Figure_9.23_B19453.jpg)'
- en: Figure 9.23 – The first lines of holiday_brazil.json
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.23 – holiday_brazil.json 的第一行
- en: How it works…
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: As you can see, a custom Airflow operator is an isolated class with a unique
    purpose. Usually, custom operators are created with the intention to also be used
    by other teams or DAGs, which avoids code redundancy or duplication. Now, let’s
    understand how it works.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，自定义 Airflow 操作符是一个具有独特目的的独立类。通常，自定义操作符是为了被其他团队或 DAG 使用而创建的，这避免了代码冗余或重复。现在，让我们了解它是如何工作的。
- en: 'We started the recipe by creating the file to host the new operator inside
    the `plugin` folder. We do this because, internally, Airflow understands that
    everything inside of it is custom code. Since we wanted to only create an operator,
    we put it inside a folder with the same name. However, it is also possible to
    create another resource called **Hooks**. You can learn more about creating hooks
    in Airflow here: [https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.xhtml).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在 `plugin` 文件夹内创建用于托管新操作符的文件来开始配方。我们这样做是因为，从内部来看，Airflow 理解它内部的一切都是自定义代码。由于我们只想创建一个操作符，所以我们将其放在一个具有相同名称的文件夹中。然而，也可以创建另一个名为
    **Hooks** 的资源。您可以在以下位置了解更多关于在 Airflow 中创建钩子的信息：[https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.xhtml)。
- en: 'Now, heading to the operator code, we declare our code to ingest the HolidayAPI
    inside a class, as you can see here:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们转向操作符代码，我们在这里声明我们的代码，在类中摄入 HolidayAPI，正如您在这里可以看到的：
- en: '[PRE11]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We did this to extend Airflow’s `BaseOperator` so that we could customize it
    and insert new constructors. `filename`, `secret_key`, `country`, and `year` are
    the parameters we need to execute the API ingest.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这样做是为了扩展 Airflow 的 `BaseOperator`，以便我们可以自定义它并插入新的构造函数。`filename`、`secret_key`、`country`
    和 `year` 是执行 API 摄入所需的参数。
- en: 'Then, we declared the `execute` function to ingest data from the API. The context
    is an Airflow parameter that allows the function to read configuration values:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们声明了 `execute` 函数来从 API 摄入数据。上下文是一个 Airflow 参数，允许函数读取配置值：
- en: '[PRE12]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, our final step was to create a DAG to execute the operator we made. The
    code is like the previous DAG we created in the *Creating DAGs* recipe, only with
    a few new items. The first item was the new `import` instances, as you can see
    here:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们的最后一步是创建一个 DAG 来执行我们制作的操作符。代码类似于我们在“创建 DAGs”配方中创建的之前的 DAG，只是增加了一些新项目。第一个项目是新的
    `import` 实例，正如您在这里可以看到的：
- en: '[PRE13]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The first `import` statement allows us to use the value of `SECRET_HOLIDAY_API`
    we inserted using the UI, and the second imports our custom operator. Observe
    that we only used the `operators.holiday_api_plugin` path. Due to Airflow’s internal
    configuration, it understands that the code inside an `operators` folder (inside
    the `plugins` folder) is an operator.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个 `import` 语句允许我们使用通过 UI 插入的 `SECRET_HOLIDAY_API` 的值，第二个导入了我们的自定义操作符。注意，我们只使用了
    `operators.holiday_api_plugin` 路径。由于 Airflow 的内部配置，它理解 `operators` 文件夹（位于 `plugins`
    文件夹内）中的代码是一个操作符。
- en: 'Now we can instantiate the custom operator like any other built-in operator
    in Airflow by passing the required parameters, as you can see in the code here:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过传递所需的参数，像在 Airflow 中使用任何内置操作符一样实例化自定义操作符，正如代码所示：
- en: '[PRE14]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: There's more…
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: If an entire project has the same form of authentication for retrieving data
    from a specific API or database, creating custom operators or hooks is a valuable
    way to avoid code duplication.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果整个项目在从特定 API 或数据库检索数据时具有相同的身份验证形式，创建自定义操作符或钩子是避免代码重复的有价值方式。
- en: 'However, before jumping excitedly into creating your plugin, remember that
    Airflow’s community already provides many operators. For example, if you use AWS
    in your daily work, you don’t need to worry about creating a new operator to connect
    with AWS Glue since that already has been done and approved by the Apache community.
    See the documentation here: [https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/glue.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/glue.xhtml).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在兴奋地开始创建你的插件之前，请记住，Airflow社区已经提供了许多操作符。例如，如果你在日常工作中使用AWS，你不需要担心创建一个新的操作符来连接AWS
    Glue，因为那已经由Apache社区完成并批准。请参阅以下文档：[https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/glue.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/glue.xhtml)。
- en: 'You can see the complete list of AWS operators here: [https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/index.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/index.xhtml).'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里看到AWS操作符的完整列表：[https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/index.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/index.xhtml)。
- en: See also
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'For more custom operator examples, see *Virajdatt Kohir’s* blog here: [https://kvirajdatt.medium.com/airflow-writing-custom-operators-and-publishing-them-as-a-package-part-2-3f4603899ec2](https://kvirajdatt.medium.com/airflow-writing-custom-operators-and-publishing-them-as-a-package-part-2-3f4603899ec2).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更多自定义操作符示例，请参阅以下Virajdatt Kohir的博客：[https://kvirajdatt.medium.com/airflow-writing-custom-operators-and-publishing-them-as-a-package-part-2-3f4603899ec2](https://kvirajdatt.medium.com/airflow-writing-custom-operators-and-publishing-them-as-a-package-part-2-3f4603899ec2)。
- en: Conﬁguring sensors
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置传感器
- en: 'Under the operator’s umbrella, we have sensors. Sensors are designed to wait
    to execute a task until something happens. For example, a sensor triggers a pipeline
    (or task) when a file lands in an `HDFS` folder, as shown here: [https://airflow.apache.org/docs/apache-airflow-providers-apache-hdfs/stable/_api/airflow/providers/apache/hdfs/sensors/hdfs/index.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-apache-hdfs/stable/_api/airflow/providers/apache/hdfs/sensors/hdfs/index.xhtml).
    As you might be wondering, there are also sensors for specific schedules or time
    deltas.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在操作符的伞下，我们有传感器。传感器被设计成等待直到某个事件发生才执行任务。例如，当文件落在`HDFS`文件夹中时，传感器会触发管道（或任务），如下所示：[https://airflow.apache.org/docs/apache-airflow-providers-apache-hdfs/stable/_api/airflow/providers/apache/hdfs/sensors/hdfs/index.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-apache-hdfs/stable/_api/airflow/providers/apache/hdfs/sensors/hdfs/index.xhtml)。你可能想知道，也有针对特定时间表或时间差的传感器。
- en: Sensors are a fundamental part of creating an automated and event-driven pipeline.
    In this recipe, we will configure a `weekday` sensor, which executes our data
    pipeline on a specific day of the week.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器是创建自动化和事件驱动管道的基本部分。在这个菜谱中，我们将配置一个`weekday`传感器，它会在一周中的特定一天执行我们的数据管道。
- en: Getting ready
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the *Getting ready* section in the *Configuring Airflow* recipe for
    this recipe since we will handle it with the same technology.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用相同的技术来处理它，因此请参考“准备工作”部分中的“配置Airflow”菜谱。
- en: 'Besides that, let’s put a JSON file to the following path inside the Airflow
    folder: `files_to_test/sensors_files/.`'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，让我们将一个JSON文件放入Airflow文件夹内的以下路径：`files_to_test/sensors_files/`。
- en: In my case, I will use the `github_events.json` file, but you can use any of
    your preferences.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，我将使用`github_events.json`文件，但你也可以使用你喜欢的任何文件。
- en: How to do it…
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Here are the steps to perform this recipe:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是执行此菜谱的步骤：
- en: 'Let’s start our DAG script by importing the required libraries, defining `default_args`,
    and instantiating our DAG, as you can see here:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从导入所需的库、定义`default_args`和实例化我们的DAG开始编写我们的DAG脚本，如下所示：
- en: '[PRE15]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, let’s define our first task using `DayOfWeekSensor`. See the code here:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用`DayOfWeekSensor`定义我们的第一个任务。代码如下：
- en: '[PRE16]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: I suggest setting the day of the week as a parameter while doing this exercise
    to ensure no confusion. For example, if you want it to be executed on a Monday,
    set `week_day` to `WeekDay.MONDAY`, and so on.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议在执行此练习时设置一周中的某一天作为参数，以确保没有混淆。例如，如果你想它在星期一执行，将`week_day`设置为`WeekDay.MONDAY`，依此类推。
- en: 'Then, we will define another task using `BashOperator`. This task will execute
    the command to move a JSON file from `files_to_test/sensors_files/` to `files_to_test/output_files/`.
    Your code should look like this:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将使用`BashOperator`定义另一个任务。这个任务将执行将JSON文件从`files_to_test/sensors_files/`移动到`files_to_test/output_files/`的命令。你的代码应该看起来像这样：
- en: '[PRE17]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, we will define the execution workflow of our DAG, as you can see here:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将定义我们的DAG的执行工作流程，正如你所看到的：
- en: '[PRE18]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `.set_downstream()` function will work similarly to the double arrows (`>>`)
    we already used to define the workflow. You can read more about this here: [https://airflow.apache.org/docs/apache-airflow/1.10.3/concepts.xhtml?highlight=trigger#bitshift-composition](https://airflow.apache.org/docs/apache-airflow/1.10.3/concepts.xhtml?highlight=trigger#bitshift-composition).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`.set_downstream()` 函数的工作方式将与我们已经用来定义工作流的两个箭头（`>>`）类似。你可以在这里了解更多信息：[https://airflow.apache.org/docs/apache-airflow/1.10.3/concepts.xhtml?highlight=trigger#bitshift-composition](https://airflow.apache.org/docs/apache-airflow/1.10.3/concepts.xhtml?highlight=trigger#bitshift-composition).'
- en: 'As seen in the previous two recipes of this chapter, now we will enable our
    `sensors_move_file` DAG, which will start immediately. If you set the weekday
    as the same day on which you are executing this exercise, your DAG **Graph** view
    will look like this, indicating success:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如同本章前两个食谱所示，现在我们将启用我们的 `sensors_move_file` DAG，它将立即启动。如果你将工作日设置为执行此练习的同一日，你的
    DAG **图形** 视图将看起来像这样，表示成功：
- en: '![Figure 9.24 – sensors_move_file tasks showing a success status](img/Figure_9.24_B19453.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图9.24 – sensors_move_file任务显示成功状态](img/Figure_9.24_B19453.jpg)'
- en: Figure 9.24 – sensors_move_file tasks showing a success status
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.24 – sensors_move_file任务显示成功状态
- en: 'Now, let’s see whether our file was moved to the directories. As described
    in the *Getting ready* section, I put a JSON file called `github_events.json`
    inside the `sensor_files` folder. Now, it will be inside `output_files`, as you
    can see here:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的文件是否已移动到目录中。正如 *准备就绪* 部分所述，我在 `sensor_files` 文件夹中放置了一个名为 `github_events.json`
    的JSON文件。现在，它将位于 `output_files` 中，正如你所看到的：
- en: '![Figure 9.25 – github_events.json inside the output_files folder](img/Figure_9.25_B19453.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图9.25 – output_files文件夹内的github_events.json](img/Figure_9.25_B19453.jpg)'
- en: Figure 9.25 – github_events.json inside the output_files folder
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.25 – output_files文件夹内的github_events.json
- en: This indicates our sensor executed as expected!
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明我们的传感器按预期执行了！
- en: How it works…
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: Sensors are valuable operators that execute an action based on a state. They
    can be triggered when a file lands in a directory, during the day, when an external
    task finishes, and so on. Here, we approached an example using a day of the week
    commonly used in data teams to change files from an ingested folder to a cold
    storage folder.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器是有价值的操作符，根据状态执行动作。它们可以在文件进入目录、白天、外部任务完成等情况下触发。在这里，我们使用数据团队常用的一天作为示例，将文件从摄取文件夹移动到冷存储文件夹。
- en: 'Sensors count with an internal method called `poke`, which will check a resource’s
    status until the criteria are met. If you look at the `move_file_on_saturday`
    log, you will see something like this:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器依赖于一个名为 `poke` 的内部方法，它将检查资源的状态，直到满足条件。如果你查看 `move_file_on_saturday` 日志，你会看到类似以下内容：
- en: '[PRE19]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Looking at the following code, we did not define a `reschedule` parameter,
    so the job will stop until we manually trigger it again:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 观察以下代码，我们没有定义 `reschedule` 参数，因此作业将停止，直到我们手动再次触发它：
- en: '[PRE20]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Other parameters we defined were `timeout`, which indicates the time in seconds
    before it fails or stops retrying, and `soft_fail`, which marks the task as `SKIPPED`
    in the case of failure.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义的其他参数包括 `timeout`，它表示在失败或停止重试之前的时间（以秒为单位），以及 `soft_fail`，它将任务标记为 `SKIPPED`
    以表示失败。
- en: 'You can see other allowed parameters here: [https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/base/index.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/base/index.xhtml).'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里看到其他允许的参数：[https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/base/index.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/base/index.xhtml).
- en: And, of course, like the rest of the operators, we can create our custom sensor
    by extending the `BaseSensorOperator` class from Airflow. The main challenge here
    is that to be considered a sensor, it needs to overwrite the `poke` parameter
    without creating a recursing or non-ending function.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，就像其他操作符一样，我们可以通过扩展Airflow中的 `BaseSensorOperator` 类来创建我们的自定义传感器。这里的挑战主要是，为了被视为传感器，它需要覆盖
    `poke` 参数，而不创建递归或无限循环的功能。
- en: See also
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'You can see a list of the default Airflow sensors on the official documentation
    page here: [https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/index.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/index.xhtml).'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在官方文档页面这里看到默认的 Airflow 传感器的列表：[https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/index.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/index.xhtml)。
- en: Creating connectors in Airﬂow
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Airflow 中创建连接器
- en: Having DAGs and operators without connecting to any external source is useless.
    Of course, there are many ways to ingest files, even from other DAGs or task results.
    Still, data ingestion usually involves using external sources such as APIs or
    databases as the first step of a data pipeline.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 没有连接到任何外部源就拥有 DAGs 和操作符是没有用的。当然，有很多方法可以摄取文件，甚至来自其他 DAGs 或任务结果。然而，数据摄取通常涉及使用外部源，如
    API 或数据库作为数据管道的第一步。
- en: To make this happen, in this recipe, we will understand how to create a connector
    in Airflow to connect to a sample database.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，在这个菜谱中，我们将了解如何在 Airflow 中创建连接器以连接到示例数据库。
- en: Getting ready
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the *Getting ready* section of the *Configuring Airflow* recipe for
    this recipe since we will handle it with the same technology.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用相同的技术来处理它，因此请参考 *Configuring Airflow* 菜谱中的 *Getting ready* 部分。
- en: This exercise will also require the MongoDB local database to be up and running.
    Ensure you have configured it as seen in [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022)
    and have at least one database and collection. You can use the instructions from
    the *Connecting to a NoSQL database (MongoDB)* recipe in [*Chapter 5*](B19453_05.xhtml#_idTextAnchor161).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习还需要 MongoDB 本地数据库处于运行状态。请确保您已按照 [*第 1 章*](B19453_01.xhtml#_idTextAnchor022)
    中的说明进行配置，并且至少有一个数据库和集合。您可以使用 [*第 5 章*](B19453_05.xhtml#_idTextAnchor161) 中 *连接到
    NoSQL 数据库 (MongoDB)* 菜单中的说明。
- en: How to do it…
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Here are the steps to perform this recipe:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此菜谱的步骤如下：
- en: 'Let’s start by opening the Airflow UI. On the top menu, select the **Admin**
    button and then **Connections**, and you will be redirected to the **Connections**
    page. Since we haven’t configured anything yet, this page will be empty, as you
    can see in the following screenshot:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从打开 Airflow UI 开始。在顶部菜单中，选择 **Admin** 按钮，然后选择 **Connections**，您将被重定向到 **Connections**
    页面。由于我们还没有进行任何配置，这个页面将是空的，正如您在下面的屏幕截图中所看到的：
- en: '![Figure 9.26 – The Connections page in the Airflow UI](img/Figure_9.26_B19453.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.26 – Airflow UI 中的连接页面](img/Figure_9.26_B19453.jpg)'
- en: Figure 9.26 – The Connections page in the Airflow UI
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.26 – Airflow UI 中的连接页面
- en: 'Then, click the **+** button to be redirected to the **Add Connection** page.
    Under the **Connection Type** field, search for and select **MongoDB**. Insert
    your connection values under the respective fields, as shown in the following
    screenshot:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，点击 **+** 按钮将被重定向到 **Add Connection** 页面。在 **Connection Type** 字段下，搜索并选择 **MongoDB**。在相应的字段下插入您的连接值，如以下屏幕截图所示：
- en: '![Figure 9.27 – Creating a new connector in the Airflow UI](img/Figure_9.27_B19453.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.27 – 在 Airflow UI 中创建新的连接器](img/Figure_9.27_B19453.jpg)'
- en: Figure 9.27 – Creating a new connector in the Airflow UI
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.27 – 在 Airflow UI 中创建新的连接器
- en: 'Click on the **Save** button, and you should have something similar to this
    on the **Connection** page:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 点击 **Save** 按钮，您应该在 **Connection** 页面上看到类似的内容：
- en: '![Figure 9.28 – The MongoDB connector created in the Airflow UI](img/Figure_9.28_B19453.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.28 – 在 Airflow UI 中创建的 MongoDB 连接器](img/Figure_9.28_B19453.jpg)'
- en: Figure 9.28 – The MongoDB connector created in the Airflow UI
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.28 – 在 Airflow UI 中创建的 MongoDB 连接器
- en: Let’s create our new DAG using the same folder and file tree structure that
    we saw in the *Creating DAGs* recipe. I will call the `mongodb_check_conn_dag.py`
    DAG file.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用与我们在 *创建 DAGs* 菜谱中看到的相同的文件夹和文件树结构创建我们的新 DAG。我将称这个 `mongodb_check_conn_dag.py`
    DAG 文件。
- en: 'Inside the DAG file, let’s start by importing and declaring the required libraries
    and variables, as you can see here:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 DAG 文件内部，让我们首先导入和声明所需的库和变量，正如您在这里所看到的：
- en: '[PRE21]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we will create a function to connect with MongoDB locally and print `collection`
    `reviews` from the `db_airbnb` database, as you can see here:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个函数来连接本地 MongoDB 并从 `db_airbnb` 数据库中打印 `collection` `reviews`，正如您在这里所看到的：
- en: '[PRE22]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, let’s proceed with the DAG object:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，让我们继续处理 DAG 对象：
- en: '[PRE23]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, let’s use `PythonOperator` to call our `get_mongo_collection` function
    defined in *step 5*:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们使用 `PythonOperator` 来调用我们在 *步骤 5* 中定义的 `get_mongo_collection` 函数：
- en: '[PRE24]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Don’t forget to put the name of your task in the indentation of the DAG, as
    follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记将您的任务名称放在 DAG 的缩进中，如下所示：
- en: '[PRE25]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Heading to the Airflow UI, let’s enable the DAG and wait for it to be executed.
    After finishing successfully, your `mongodb_task` log should look like this:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到 Airflow UI，让我们启用 DAG 并等待其执行。成功完成后，您的 `mongodb_task` 日志应如下所示：
- en: '![Figure 9.29 – mongodb_task logs](img/Figure_9.29_B19453.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.29 – mongodb_task 日志](img/Figure_9.29_B19453.jpg)'
- en: Figure 9.29 – mongodb_task logs
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.29 – mongodb_task 日志
- en: As you can see, we connected and retrieved the `Collection` object from MongoDB.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们已经连接并从 MongoDB 中检索了 `Collection` 对象。
- en: How it works…
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Creating a connection in Airflow is straightforward, as demonstrated here using
    the UI. It is also possible to create connections programmatically using the `Connection`
    class.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Airflow 中创建连接非常简单，就像这里使用 UI 展示的那样。也可以使用 `Connection` 类编程创建连接。
- en: After we set our MongoDB connection parameters, we needed to create a form to
    access it, and we did so using a hook. A **hook** is a high-level interface that
    allows connections to external sources without the need to be preoccupied with
    low-level code or special libraries.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们设置了 MongoDB 连接参数后，我们需要创建一个表单来访问它，我们使用钩子做到了这一点。**钩子**是一个高级接口，允许连接到外部源，而无需担心低级代码或特殊库。
- en: 'Remember that we configured an external package in the *Configuring Airflow*
    recipe? It was a provider to allow an easy connection with MongoDB:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们在 *配置 Airflow* 的配方中配置了一个外部包？它是一个提供程序，允许轻松连接到 MongoDB：
- en: '[PRE26]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Inside the `get_mongo_collection` function, we instantiated `MongoHook` and
    passed the same connection ID name set in the `Connection` page, as you can see
    here:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `get_mongo_collection` 函数中，我们实例化了 `MongoHook` 并传递了在 `Connection` 页面上设置的相同连接
    ID 名称，正如您在这里所看到的：
- en: '[PRE27]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'With that instance, we can call the methods of the `MongoHook` class and even
    pass other parameters to configure the connection. See the documentation for this
    class here: [https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/hooks/mongo/index.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/hooks/mongo/index.xhtml).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 使用该实例，我们可以调用 `MongoHook` 类的方法，甚至可以传递其他参数来配置连接。有关此类文档，请参阅此处：[https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/hooks/mongo/index.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/hooks/mongo/index.xhtml)。
- en: There's more…
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: You can also use the `airflow.cfg` file to set the connection strings or any
    other environment variable. It is a good practice to store sensitive information
    here, such as credentials, since they will not be shown in the UI. It is also
    possible to encrypt these values with additional configuration.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用 `airflow.cfg` 文件来设置连接字符串或任何其他环境变量。将敏感信息（如凭证）存储在此处是一个好习惯，因为它们不会在 UI 中显示。还可能通过额外的配置对这些值进行加密。
- en: 'For more information, see the documentation here: [https://airflow.apache.org/docs/apache-airflow/stable/howto/set-config.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/howto/set-config.xhtml).'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参阅此处文档：[https://airflow.apache.org/docs/apache-airflow/stable/howto/set-config.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/howto/set-config.xhtml)。
- en: See also
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'You can learn about the MongoDB provider on the Airflow official documentation
    page here: [https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/index.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/index.xhtml)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在 Airflow 官方文档页面了解有关 MongoDB 提供程序的更多信息：[https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/index.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-mongo/stable/_api/airflow/providers/mongo/index.xhtml)
- en: 'If you are interested in reading more about connections, see this link: https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.xhtml#custom-connection-types'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于连接的信息，请参阅此链接：https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.xhtml#custom-connection-types
- en: Creating parallel ingest tasks
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建并行摄取任务
- en: When working with data, we hardly ever just perform one ingestion at a time,
    and a real-world project involves many ingestions happening simultaneously, often
    in parallel. We know scheduling two or more DAGs to run alongside each other is
    possible, but what about tasks inside one DAG?
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理数据时，我们很少一次只执行一个摄取操作，现实世界的项目通常涉及许多同时发生的摄取操作，通常是并行的。我们知道可以安排两个或更多 DAG 同时运行，但一个
    DAG 内部的任务呢？
- en: This recipe will illustrate how to create parallel task execution in Airflow.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱将说明如何在 Airflow 中创建并行任务执行。
- en: Getting ready
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Please refer to the *Getting ready* section of the *Configuring Airflow* recipe
    for this recipe since we will handle it with the same technology.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考 *准备工作* 部分，该部分在 *配置 Airflow* 食谱中，因为我们将使用相同的技术来处理它。
- en: To avoid redundancy in this exercise, we won’t explicitly include the imports
    and main DAG configuration. Instead, the focus is on organizing the operator’s
    workflow. You can use the same logic to create your DAG structure as in the *Creating*
    *DAGs* recipe.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在这个练习中产生冗余，我们不会明确包含导入和主 DAG 配置。相反，重点是组织操作符的工作流程。你可以使用与 *创建* *DAG* 食谱中相同的逻辑来创建你的
    DAG 结构。
- en: 'For the complete Python file used here, go to the GitHub page here: [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_parallel_ingest_tasks](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_parallel_ingest_tasks).'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这里使用的完整 Python 文件，请访问 GitHub 页面：[https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_parallel_ingest_tasks](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/creating_parallel_ingest_tasks)。
- en: How to do it…
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'For this exercise, my DAG will be called `parallel_tasks_dag`. Now, let’s try
    it:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，我的 DAG 将被称为 `parallel_tasks_dag`。现在，让我们试试看：
- en: 'Let’s start by creating five `BashOperator` instances, as you can see here:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从创建五个 `BashOperator` 实例开始，如下所示：
- en: '[PRE28]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The idea is for three of them to be executed in parallel so they will be inside
    square brackets. The first and last tasks will have the same workflow declared
    as we saw in the *Creating DAGs* recipe, using the `>>` character. The final flow
    structure will look like this:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 策略是让其中三个并行执行，因此它们将位于方括号内。第一个和最后一个任务将具有与我们在 *创建 DAG* 章节中看到的相同的流程声明，使用 `>>` 字符。最终的流程结构将如下所示：
- en: '[PRE29]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, enable your DAG, and let’s see what it looks like on the DAG **Graph**
    page. It would be best if you had something like the following figure:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，启用你的 DAG，让我们看看它在 DAG **图形** 页面上看起来像什么。它应该类似于以下图：
- en: '![Figure 9.30 – parallel_tasks_dag tasks in Airflow](img/Figure_9.30_B19453.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.30 – Airflow 中的 parallel_tasks_dag 任务](img/Figure_9.30_B19453.jpg)'
- en: Figure 9.30 – parallel_tasks_dag tasks in Airflow
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.30 – Airflow 中的 parallel_tasks_dag 任务
- en: As you can observe, the tasks inside the square brackets are displayed in parallel
    and will start after `t_0` finishes its work.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，方括号内的任务以并行方式显示，将在 `t_0` 完成其工作后开始。
- en: How it works…
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Although creating parallel tasks inside a DAG is simple, this type of workflow
    arrangement is advantageous when working with data.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在 DAG 内创建并行任务很简单，但这种工作流程安排在处理数据时具有优势。
- en: 'Consider an example of data ingestion: we need to guarantee we have ingested
    all the desired endpoints before moving out to the next pipeline phase. See the
    following figure as a reference:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个数据摄取的例子：我们需要保证在移动到下一个管道阶段之前，我们已经摄取了所有期望的端点。以下图作为参考：
- en: '![Figure 9.31 – Example of parallel execution](img/Figure_9.31_B19453.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.31 – 并行执行的示例](img/Figure_9.31_B19453.jpg)'
- en: Figure 9.31 – Example of parallel execution
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.31 – 并行执行的示例
- en: The parallel execution will only move to the final task when all the parallel
    ones finish successfully. With this, we guarantee the data pipeline will not ingest
    only a small portion of the data but all the required data.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 并行执行只有在所有并行任务成功完成后才会移动到最后一个任务。通过这种方式，我们保证数据管道不会只摄取一小部分数据，而是所有所需的数据。
- en: 'Back to our exercise, we can simulate this behavior, creating an `t_2` by removing
    one of the simple quotation marks. In the following figure, you can see what the
    DAG graph will look like:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的练习，我们可以通过删除一个简单引号来模拟这种行为，创建一个 `t_2`。在下面的图中，你可以看到 DAG 图将如何看起来：
- en: '![Figure 9.32 – Airflow’s parallel_tasks_dag with an error t_2 task](img/Figure_9.32_B19453.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.32 – Airflow 的 parallel_tasks_dag 中有错误 t_2 任务](img/Figure_9.32_B19453.jpg)'
- en: Figure 9.32 – Airflow’s parallel_tasks_dag with an error t_2 task
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.32 – Airflow 的 parallel_tasks_dag 中有错误 t_2 任务
- en: The `t_final` task will retry executing until we fix `t_2` or the number of
    retries reaches its limit.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`t_final` 任务将重试执行，直到我们修复 `t_2` 或重试次数达到其限制。'
- en: However, avoiding many parallel tasks is a good practice, mainly if you have
    limited infrastructure resources to handle them. There are many ways to create
    dependency on external tasks or DAGs, and we can use them to make more efficient
    pipelines.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，避免执行许多并行任务是一种良好的实践，尤其是在你只有有限的资源来处理这些任务时。有许多方法可以创建对外部任务或DAG的依赖，我们可以利用它们来创建更高效的管道。
- en: There's more…
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容…
- en: 'Along with the concept of task parallelism, we have `BranchOperator`. `BranchOperator`
    executes one or more tasks simultaneously based on a criteria match. Let’s illustrate
    this with the following figure:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 除了任务并行性的概念，我们还有`BranchOperator`。`BranchOperator`根据匹配的准则同时执行一个或多个任务。让我们用以下图例来说明这一点：
- en: '![Figure 9.33 – Branching task diagram example](img/Figure_9.33_B19453.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![图9.33 – 分支任务图示例](img/Figure_9.33_B19453.jpg)'
- en: Figure 9.33 – Branching task diagram example
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.33 – 分支任务图示例
- en: Based on the day-of-the-week criteria, the `day_of_the_week_branch` task will
    trigger a specific task assigned for that day.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 根据一周中的日期标准，`day_of_the_week_branch`任务将触发特定于该天的任务。
- en: 'If you want to know more about it, *Analytics Vidhya* has a good blog post
    about it, which you can read here: [https://www.analyticsvidhya.com/blog/2023/01/data-engineering-101-branchpythonoperator-in-apache-airflow/](https://www.analyticsvidhya.com/blog/2023/01/data-engineering-101-branchpythonoperator-in-apache-airflow/).'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多，*Analytics Vidhya*有一个关于它的很好的博客文章，你可以在这里阅读：[https://www.analyticsvidhya.com/blog/2023/01/data-engineering-101-branchpythonoperator-in-apache-airflow/](https://www.analyticsvidhya.com/blog/2023/01/data-engineering-101-branchpythonoperator-in-apache-airflow/)。
- en: See also
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: 'BetterDataScience has a good blog post about parallel tasks in Airflow. You
    can find it here: [https://betterdatascience.com/apache-airflow-run-tasks-in-parallel/](https://betterdatascience.com/apache-airflow-run-tasks-in-parallel/).'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BetterDataScience有一个关于Airflow中并行任务的很好的博客文章。你可以在这里找到它：[https://betterdatascience.com/apache-airflow-run-tasks-in-parallel/](https://betterdatascience.com/apache-airflow-run-tasks-in-parallel/)。
- en: 'You can read more about Airflow task parallelism here: [https://hevodata.com/learn/airflow-parallelism/](https://hevodata.com/learn/airflow-parallelism/).'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在这里了解更多关于Airflow任务并行性的信息：[https://hevodata.com/learn/airflow-parallelism/](https://hevodata.com/learn/airflow-parallelism/)。
- en: Deﬁning ingest-dependent DAGs
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义依赖导入的DAGs
- en: In the data world, considerable discussion exists about how to organize Airflow
    DAGs. The approach I generally use is to create a DAG for a specific pipeline
    based on the business logic or final destination. Nevertheless, sometimes, to
    proceed with a task inside a DAG, we depend on another DAG to finish the process
    and get the output.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据世界中，关于如何组织Airflow DAGs的讨论相当多。我通常使用的方法是创建一个基于业务逻辑或最终目的地的特定管道的DAG。然而，有时，为了在DAG内部执行任务，我们依赖于另一个DAG来完成流程并获取输出。
- en: In this recipe, we will create two DAGs, where the first depends on the result
    of the second to be successful. Otherwise, it will not be completed. To assist
    us, we will use the `ExternalTaskSensor` operator.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将创建两个DAG，其中第一个依赖于第二个的成功结果。否则，它将不会完成。为了帮助我们，我们将使用`ExternalTaskSensor`操作员。
- en: Getting ready
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备
- en: Please refer to the *Getting ready* section of the *Configuring Airflow* recipe
    for this recipe since we will handle it with the same technology.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考*配置Airflow*配方中的*准备*部分，因为我们将使用相同的技术来处理它。
- en: This recipe depends on the `holiday_ingest` DAG, created in the *Creating custom
    operators* recipe, so ensure you have that.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方依赖于在*创建自定义操作员*配方中创建的`holiday_ingest` DAG，所以请确保你有它。
- en: We will not explicitly cite the imports and main DAG configuration to prevent
    redundancy and repetition in this exercise. The aim here is how to organize the
    operator’s workflow. You can use the same logic to create your DAG structure as
    in the *Creating* *DAGs* recipe.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将不会明确引用导入和主DAG配置，以避免冗余和重复。这里的目的是如何组织操作员的作业流程。你可以使用与*创建* *DAGs*配方中相同的逻辑来创建你的DAG结构。
- en: 'For the complete Python file used here, go to the GitHub page here:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这里使用的完整Python文件，请访问以下GitHub页面：
- en: '[https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/de%EF%AC%81ning_dependent_ingests_DAGs](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/de%EF%AC%81ning_dependent_ingests_DAGs)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/de%EF%AC%81ning_dependent_ingests_DAGs](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_9/de%EF%AC%81ning_dependent_ingests_DAGs)'
- en: How to do it…
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'For this exercise, let’s create a DAG triggered when `holiday_ingest` finishes
    successfully, and returns all the holiday dates in the console output. My DAG
    will be called `external_sensor_dag`, but feel free to provide any other ID name.
    Just ensure it is unique and therefore will not impair other DAGs:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，让我们创建一个在 `holiday_ingest` 成功完成后触发的 DAG，并在控制台输出中返回所有假日日期。我的 DAG 将被称为 `external_sensor_dag`，但请随意提供任何其他
    ID 名称。只需确保它是唯一的，因此不会影响其他 DAG：
- en: 'Along with the default imports we have, let’s add the following:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了我们已有的默认导入外，我们还可以添加以下内容：
- en: '[PRE30]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, we will insert a Python function to return the holiday dates in the `holiday_brazil.json`
    file, which is the output of the `holiday_ingest` DAG:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将在 `holiday_brazil.json` 文件中插入一个 Python 函数来返回假日日期，这是 `holiday_ingest` DAG
    的输出：
- en: '[PRE31]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, we will make the two operators of the DAG and define the workflow:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将定义 DAG 的两个操作符和流程：
- en: '[PRE32]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Save it and enable this DAG in the Airflow UI. Once enabled, you will notice
    the `wait_holiday_api_ingest` task will be in the `RUNNING` state and will not
    proceed to the other task, as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 保存它并在 Airflow UI 中启用此 DAG。一旦启用，你会注意到 `wait_holiday_api_ingest` 任务将处于 `RUNNING`
    状态，并且不会继续到其他任务，如下所示：
- en: '![Figure 9.34 – The wait_holiday_api_ingest task in the running state](img/Figure_9.34_B19453.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.34 – 运行状态下的 wait_holiday_api_ingest 任务](img/Figure_9.34_B19453.jpg)'
- en: Figure 9.34 – The wait_holiday_api_ingest task in the running state
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.34 – 运行状态下的 wait_holiday_api_ingest 任务
- en: 'You will also notice the log for this task looks like the following:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 你还会注意到这个任务的日志看起来像以下这样：
- en: '[PRE33]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Now, we will enable and run `holiday_ingest` (if it is not enabled yet).
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将启用并运行 `holiday_ingest`（如果尚未启用）。
- en: 'Then, go back to `external_sensor_dag`, and you will see it finished successfully,
    as shown in the following figure:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，回到 `external_sensor_dag`，你会看到它已成功完成，如下所示：
- en: '![Figure 9.35 – external_sensor_dag showing success](img/Figure_9.35_B19453.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.35 – 显示成功的 external_sensor_dag](img/Figure_9.35_B19453.jpg)'
- en: Figure 9.35 – external_sensor_dag showing success
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.35 – 显示成功的 external_sensor_dag
- en: 'If we examine the logs of `date_tasks`, you will see the following output on
    the console:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查 `date_tasks` 的日志，你将在控制台看到以下输出：
- en: '[PRE34]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Here is the complete log for reference:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是完整的日志供参考：
- en: '![Figure 9.36 – date_tasks logs in the Airflow UI](img/Figure_9.36_B19453.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.36 – Airflow UI 中的 date_tasks 日志](img/Figure_9.36_B19453.jpg)'
- en: Figure 9.36 – date_tasks logs in the Airflow UI
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.36 – Airflow UI 中的 date_tasks 日志
- en: Now, let’s understand how it works in the next section.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在下一节中了解它是如何工作的。
- en: How it works…
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: 'Let’s start by taking a look at our `wait_holiday_api_ingest` task:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看我们的 `wait_holiday_api_ingest` 任务：
- en: '[PRE35]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`ExternalTaskSensor` is a sensor type that will only execute if another task
    outside its DAG finishes with a specific status defined on the `allowed_states`
    parameter. The default value for this parameter is `SUCCESS`.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExternalTaskSensor` 是一种传感器类型，只有当其 DAG 外部的另一个任务以在 `allowed_states` 参数上定义的特定状态完成时才会执行。此参数的默认值为
    `SUCCESS`。'
- en: The sensor will search for a specific DAG and task in Airflow using the `external_dag_id`
    and `external_task_id` parameters, which we have defined as `holiday_ingest` and
    `holiday_api_ingestion`, respectively. Finally, `execution_delta` will determine
    the time interval at which to poke the external DAG again.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器将使用 `external_dag_id` 和 `external_task_id` 参数在 Airflow 中搜索特定的 DAG 和任务，我们将它们分别定义为
    `holiday_ingest` 和 `holiday_api_ingestion`。最后，`execution_delta` 将确定再次戳击外部 DAG
    的时间间隔。
- en: Once it finishes, the DAG will remain in the `SUCCESS` state unless we define
    a different behavior in the default arguments. If we clear its status, it will
    return to the `RUNNING` mode until the sensor criteria are met again.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，除非我们在默认参数中定义不同的行为，否则 DAG 将保持 `SUCCESS` 状态。如果我们清除其状态，它将返回到 `RUNNING` 模式，直到传感器条件再次满足。
- en: There's more…
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'We know Airflow is a powerful tool, but it is not immune from occasional failures.
    Internally, Airflow has its routes to reach internal and external DAGs, which
    can occasionally fail. For example, one of these errors might be a DAG not being
    found, which can happen due to various reasons such as misconfiguration or connectivity
    issues. You can see a screenshot of one of these errors here:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 Airflow 是一个强大的工具，但它并非不会偶尔出现故障。内部，Airflow 有其路线来访问内部和外部 DAG，这些路线偶尔会失败。例如，这些错误之一可能是一个
    DAG 未找到，这可能是由于各种原因，如配置错误或连接问题。你可以在这里看到这些错误之一的截图：
- en: '![Figure 9.37 – Occasional 403 error log in an Airflow task](img/Figure_9.37_B19453.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.37 – Airflow 任务中的偶尔 403 错误日志](img/Figure_9.37_B19453.jpg)'
- en: Figure 9.37 – Occasional 403 error log in an Airflow task
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.37 – Airflow任务中的偶尔403错误日志
- en: Looking closely, we can observe that for several seconds, one of the Airflow
    workers lost permission to access or retrieve information from another worker.
    If this happens to you, disable your DAG and enable it again.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察，我们可以发现，在几秒钟内，Airflow的一个工作节点失去了访问或从另一个工作节点检索信息的权限。如果这种情况发生在您身上，请禁用您的DAG，然后再次启用它。
- en: XCom
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XCom
- en: In this exercise, we used an output file to perform an action, but we can also
    use the output of a task without requiring it to write a file somewhere. Instead,
    we can use the **XCom** (short for **cross-communications**) mechanism to help
    us with it.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们使用输出文件来执行操作，但也可以使用任务输出而不需要它写入文件。相反，我们可以使用**XCom**（即**跨通信**）机制来帮助我们。
- en: To use XCom across tasks, we can simply use *push* and *pull* the values using
    the `xcom_push` and `xcom_pull` methods inside the required tasks. Behind the
    scenes, Airflow stores those values temporarily in one of its databases, making
    it easier to access them again.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 要在任务间使用XCom，我们可以简单地使用`xcom_push`和`xcom_pull`方法在所需任务中*推送*和*拉取*值。在幕后，Airflow将这些值临时存储在其数据库之一中，使其更容易再次访问。
- en: To check your stored XComs in the Airflow UI, click on the **Admin** button
    and select **XCom**.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Airflow UI中检查您的存储XComs，请点击**管理员**按钮并选择**XCom**。
- en: Note
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In production environments, XComs might have a purge routine. Check with the
    Airflows administrators if you need to keep a value for longer.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，XComs可能有一个清除程序。如果您需要保留值更长时间，请咨询Airflow管理员。
- en: 'You can read more about XComs on the official documentation page here: [https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.xhtml).'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在官方文档页面了解更多关于XComs的信息：[https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.xhtml)。
- en: See also
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'You can learn more about this operator on the Airflow official documentation
    page: [https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/external_task_sensor.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/external_task_sensor.xhtml).'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Airflow官方文档页面了解更多关于此操作符的信息：[https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/external_task_sensor.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/external_task_sensor.xhtml)。
- en: Further reading
  id: totrans-356
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.xhtml)'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.xhtml)'
- en: '[https://airflow.apache.org/docs/apache-airflow/2.2.4/best-practices.xhtml](https://airflow.apache.org/docs/apache-airflow/2.2.4/best-practices.xhtml)'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://airflow.apache.org/docs/apache-airflow/2.2.4/best-practices.xhtml](https://airflow.apache.org/docs/apache-airflow/2.2.4/best-practices.xhtml)'
- en: '[https://www.qubole.com/tech-blog/apache-airflow-tutorial-dags-tasks-operators-sensors-hooks-xcom](https://www.qubole.com/tech-blog/apache-airflow-tutorial-dags-tasks-operators-sensors-hooks-xcom)'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.qubole.com/tech-blog/apache-airflow-tutorial-dags-tasks-operators-sensors-hooks-xcom](https://www.qubole.com/tech-blog/apache-airflow-tutorial-dags-tasks-operators-sensors-hooks-xcom)'
- en: '[https://python.plainenglish.io/apache-airflow-how-to-correctly-setup-custom-plugins-2f80fe5e3dbe](https://python.plainenglish.io/apache-airflow-how-to-correctly-setup-custom-plugins-2f80fe5e3dbe)'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://python.plainenglish.io/apache-airflow-how-to-correctly-setup-custom-plugins-2f80fe5e3dbe](https://python.plainenglish.io/apache-airflow-how-to-correctly-setup-custom-plugins-2f80fe5e3dbe)'
- en: '[https://copyprogramming.com/howto/airflow-how-to-mount-airflow-cfg-in-docker-container](https://copyprogramming.com/howto/airflow-how-to-mount-airflow-cfg-in-docker-container)'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://copyprogramming.com/howto/airflow-how-to-mount-airflow-cfg-in-docker-container](https://copyprogramming.com/howto/airflow-how-to-mount-airflow-cfg-in-docker-container)'
