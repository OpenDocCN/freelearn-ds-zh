<html><head></head><body><div><div><h1 class="header-title">Debugging and Profiling Your CUDA Code</h1>
                
            
            
                
<p class="mce-root">In this chapter, we will finally learn how to debug and profile our GPU code using several different methods and tools. While we can easily debug pure Python code using IDEs such as Spyder and PyCharm, we can't use these tools to debug the actual GPU code, remembering that the GPU code itself is written in CUDA-C with PyCUDA providing an interface. The first and easiest method for debugging a CUDA kernel is the usage of <kbd>printf</kbd> statements, which we can actually call directly in the middle of a CUDA kernel to print to the standard output. We will see how to use <kbd>printf</kbd> in the context of CUDA and how to apply it effectively for debugging. </p>
<p class="mce-root">Next, we will fill in some of the gaps in our CUDA-C programming so that we can directly write CUDA programs within the NVIDIA Nsight IDE, which will allow us to make test cases in CUDA-C for some of the code we have been writing. We will take a look at how to compile CUDA-C programs, both from the command line with <kbd>nvcc</kbd> and also with the Nsight IDE. We will then see how to debug within Nsight and use Nsight to understand the CUDA lockstep property. Finally, we will have an overview of the NVIDIA command line and Visual Profilers for profiling our code.</p>
<p>The learning outcomes for this chapter include the following:</p>
<ul>
<li>Using <kbd>printf</kbd> effectively as a debugging tool for CUDA kernels</li>
<li>Writing complete CUDA-C programs outside of Python, especially for creating test cases for debugging</li>
<li>Compiling CUDA-C programs on the command line with the <kbd>nvcc</kbd> compiler</li>
<li>Developing and debugging CUDA programs with the NVIDIA Nsight IDE</li>
<li>Understanding the CUDA warp lockstep property and why we should avoid branch divergence within a single CUDA warp</li>
<li>Learn to effectively use the NVIDIA command line and Visual Profilers for GPU code</li>
</ul>
<p class="mce-root"/>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>A Linux or Windows 10 PC with a modern NVIDIA GPU (2016—onward) is required for this chapter, with all necessary GPU drivers and the CUDA Toolkit (9.0–onward) installed. A suitable Python 2.7 installation (such as Anaconda Python 2.7) with the PyCUDA module is also required.</p>
<p>This chapter's code is also available on GitHub at <a href="https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA">https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA</a>.<a href="https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA"/></p>
<p>For more information about the prerequisites, check the <em>Preface</em> of this book, and for the software and hardware requirements, check the README in <a href="https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA">https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA</a>.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Using printf from within CUDA kernels</h1>
                
            
            
                
<p>It may come as a surprise, but we can actually print text to the standard output from directly within a CUDA kernel; not only that, each individual thread can print its own output. This will come in particularly handy when we are debugging our kernels, as we may need to monitor the values of particular variables or computations at particular points in our code and it will also free us from the shackles of using a debugger to go through step by step. Printing output from a CUDA kernel is done with none other than the most fundamental function in all of C/C++ programming, the function that most people will learn when they write their first <kbd>Hello world</kbd> program in C: <kbd>printf</kbd>. Of course, <kbd>printf</kbd> is the standard function that prints a string to the standard output, and is really the equivalent in the C programming language of Python's <kbd>print</kbd> function. </p>
<p>Let's now briefly review how to use <kbd>printf</kbd> before we see how to use it in CUDA. The first thing to remember is that <kbd>printf</kbd> always takes a string as its first parameter; so printing "Hello world!" in C is done with <kbd>printf("Hello world!\n");</kbd>. (Of course, <kbd>\n</kbd> indicates "new line" or "return", which moves the output in the Terminal to the next line.) <kbd>printf</kbd> can also take a variable number of parameters in the case that we want to print any constants or variables from directly within C: if we want to print the <kbd>123</kbd> integers to the output, we do this with <kbd>printf("%d", 123);</kbd> (where <kbd>%d</kbd> indicates that an integer follows the string.)</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Similarly, we use <kbd>%f</kbd>, <kbd>%e</kbd>, or <kbd>%g</kbd> to print floating-point values (where <kbd>%f</kbd> is the decimal notation, <kbd>%e</kbd> is the scientific notation, and <kbd>%g</kbd> is the shortest representation whether decimal or scientific). We can even print several values in a row, remembering to place these specifiers in the correct order: <kbd>printf("%d is a prime number, %f is close to pi, and %d is even.\n", 17, 3.14, 4);</kbd> will print "17 is a prime number, 3.14 is close to pi, and 4 is even." on the Terminal.</p>
<p>Now, nearly halfway through this book, we will finally embark on creating our first parallel <kbd>Hello world</kbd> program in CUDA! We start by importing the appropriate modules into Python and then write our kernel. We will start out by printing the thread and grid identification of each individual thread (we will only launch this in one-dimensional blocks and grids, so we only need the <kbd>x</kbd> values):</p>
<pre>ker = SourceModule('''<br/>__global__ void hello_world_ker()<br/>{<br/>    printf("Hello world from thread %d, in block %d!\\n", threadIdx.x, blockIdx.x);</pre>
<p>Let's stop for a second and note that we wrote <kbd>\\n</kbd> rather than <kbd>\n</kbd>. This is due to the fact that the triple quote in Python itself will interpret <kbd>\n</kbd> as a "new line", so we have to indicate that we mean this literally by using a double backslash so as to pass the <kbd>\n</kbd> directly into the CUDA compiler.</p>
<p>We will now print some information about the block and grid dimensions, but we want to ensure that it is printed after every thread has already finished its initial <kbd>printf</kbd> command. We can do this by putting in <kbd>__syncthreads();</kbd> to ensure each individual thread will be synchronized after the first <kbd>printf</kbd> function is executed.</p>
<p>Now, we only want to print the block and grid dimensions to the terminal only once; if we just place <kbd>printf</kbd> statements here, every single thread will print out the same information. We can do this by having only one specified thread print to the output; let's go with the 0th thread of the 0th block, which is the only thread that is guaranteed to exist no matter the block and grid dimensionality we choose. We can do this with a C <kbd>if</kbd> statement:</p>
<pre> if(threadIdx.x == 0 &amp;&amp; blockIdx.x == 0)<br/> {</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We will now print the dimensionality of our block and grid and close up the <kbd>if</kbd> statement, and that will be the end of our CUDA kernel:</p>
<pre> printf("-------------------------------------\\n");<br/> printf("This kernel was launched over a grid consisting of %d blocks,\\n", gridDim.x);<br/> printf("where each block has %d threads.\\n", blockDim.x);<br/> }<br/>}<br/>''')</pre>
<p>We will now extract the kernel and then launch it over a grid consisting of two blocks, where each block has five threads:</p>
<pre>hello_ker = ker.get_function("hello_world_ker")<br/>hello_ker( block=(5,1,1), grid=(2,1,1) )</pre>
<p>Let's run this right now (this program is also available in <kbd>hello-world_gpu.py</kbd> under <kbd>6</kbd> in the repository):</p>
<div><img src="img/2c945416-8c8a-4eae-8d57-ef1479dd2798.png" style="" width="962" height="499"/></div>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Using printf for debugging</h1>
                
            
            
                
<p>Let's go over an example to see how we can approach debugging a CUDA kernel with <kbd>printf</kbd> with an example before we move on. There is no exact science to this method, but it is a skill that can be learned through experience. We will start with a CUDA kernel that is for matrix-matrix multiplication, but that has several bugs in it. (The reader is encouraged to go through the code as we go along, which is available as the <kbd>broken_matrix_ker.py</kbd> file in the <kbd>6</kbd> directories within the repository.)</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's briefly review matrix-matrix multiplication before we continue. Suppose we have two matrices <img class="fm-editor-equation" src="img/0e03608a-7c0c-4629-b61b-ec90b15d0cf7.png" style="width:3.50em;height:1.00em;" width="560" height="160"/>, <em>A</em> and <em>B</em>, and we multiply these together to get another matrix, <em>C</em>, of the same size as follows: <img class="fm-editor-equation" src="img/60101910-7794-4484-b69f-7ddaad3994db.png" style="width:3.92em;height:1.00em;" width="670" height="170"/>. We do this by iterating over all tuples <img class="fm-editor-equation" src="img/f550491f-6b50-49dd-be38-817b363525b9.png" style="width:9.92em;height:1.33em;" width="1630" height="220"/> and setting the value of <img class="fm-editor-equation" src="img/54c310e8-02c5-4bbc-b645-9f33465d4e77.png" style="width:2.92em;height:1.33em;" width="470" height="220"/> to the dot product of the <em>i</em><sup>th</sup> row of <em>A</em> and the <em>j</em><sup>th</sup> column of <em>B</em>: <img class="fm-editor-equation" src="img/615e6730-f5b3-48d2-9ac2-fb9a7dc535de.png" style="width:11.25em;height:1.42em;" width="1750" height="220"/>.</p>
<p>In other words, we set each <em>i, j</em> element in the output matrix <em>C</em> as follows:   <img class="fm-editor-equation" src="img/8870cb22-35f8-4612-aa78-b1cc92fd38f9.png" style="width:21.58em;height:3.17em;" width="3960" height="580"/></p>
<p>Suppose we already wrote a kernel that is to perform matrix-matrix multiplication, which takes in two arrays representing the input matrices, an additional pre allocated float array that the output will be written to, and an integer that indicates the height and width of each matrix (we will assume that all matrices are the same size and square-shaped). These matrices are all to be represented as one-dimensional <kbd>float *</kbd> arrays in a row-wise one-dimensional layout. Furthermore, this will be implemented so that each CUDA thread will handle a single row/column tuple in the output matrix.</p>
<p>We make a small test case and check it against the output of the matrix multiplication in CUDA, and it fails as an assertion check on two 4 x 4 matrices, as follows:</p>
<pre>test_a = np.float32( [xrange(1,5)] * 4 )<br/>test_b = np.float32([xrange(14,10, -1)]*4 )<br/>output_mat = np.matmul(test_a, test_b)<br/><br/>test_a_gpu = gpuarray.to_gpu(test_a)<br/>test_b_gpu = gpuarray.to_gpu(test_b)<br/>output_mat_gpu = gpuarray.empty_like(test_a_gpu)<br/><br/>matrix_ker(test_a_gpu, test_b_gpu, output_mat_gpu, np.int32(4), block=(2,2,1), grid=(2,2,1))<br/><br/>assert( np.allclose(output_mat_gpu.get(), output_mat) )</pre>
<p>We will run this program right now, and unsurprisingly get the following output:</p>
<div><img src="img/7fb29cbf-af3a-4fd1-a75a-cd05d3f56a44.png" style="" width="949" height="227"/></div>
<p>Let's now look at the CUDA C code, which consists of a kernel and a device function:</p>
<pre>ker = SourceModule('''<br/>// row-column dot-product for matrix multiplication<br/>__device__ float rowcol_dot(float *matrix_a, float *matrix_b, int row, int col, int N)<br/>{<br/> float val = 0;<br/><br/> for (int k=0; k &lt; N; k++)<br/> {<br/>     val += matrix_a[ row + k*N ] * matrix_b[ col*N + k];<br/> }<br/> return(val);<br/>}<br/><br/>// matrix multiplication kernel that is parallelized over row/column tuples.<br/><br/>__global__ void matrix_mult_ker(float * matrix_a, float * matrix_b, float * output_matrix, int N)<br/>{<br/> int row = blockIdx.x + threadIdx.x;<br/> int col = blockIdx.y + threadIdx.y;<br/> <br/> output_matrix[col + row*N] = rowcol_dot(matrix_a, matrix_b, col, row, N);<br/>}<br/>''')</pre>
<p>Our goal is to place <kbd>printf</kbd> invocations intelligently throughout our CUDA code so that we can monitor a number of appropriate values and variables in the kernel and device function; we should also be sure to print out the thread and block numbers alongside these values at every <kbd>printf</kbd> invocation.</p>
<p>Let's start at the entry point of our kernel. We see two variables, <kbd>row</kbd> and <kbd>col</kbd>, so we should check these right away. Let's put the following line right after we set them (since this is parallelized over two dimensions, we should print the <em>x</em> and <em>y</em> values of <kbd>threadIdx</kbd> and <kbd>blockIdx</kbd>):</p>
<pre>printf("threadIdx.x,y: %d,%d blockIdx.x,y: %d,%d -- row is %d, col is %d.\\n", threadIdx.x, threadIdx.y, blockIdx.x, blockIdx.y, row, col);</pre>
<p>Running the code again, we get this output:</p>
<div><img src="img/579c6bb8-3bee-4788-b606-aff8d5d15812.png" style="" width="949" height="801"/></div>
<p>There are two things that are immediately salient: that there are repeated values for row and column tuples (every individual tuple should be represented only once), and that the row and column values never exceed two, when they both should reach three (since this unit test is using 4 x 4 matrices). This should indicate to us that we are calculating the row and column values wrongly; indeed, we are forgetting to multiply the <kbd>blockIdx</kbd> values by the <kbd>blockDim</kbd> values to find the objective row/column values. We fix this as follows:</p>
<pre>int row = blockIdx.x*blockDim.x + threadIdx.x;<br/>int col = blockIdx.y*blockDim.y + threadIdx.y;</pre>
<p>If we run the program again, though, we still get an assertion error. Let's keep our original <kbd>printf</kbd> invocation in place, so we can monitor the values as we continue. We see that there is an invocation to a device function in the kernel, <kbd>rowcol_dot</kbd>, so we decide to look into there. Let's first ensure that the variables are being passed into the device function correctly by putting this <kbd>printf</kbd> invocation at the beginning:</p>
<pre>printf("threadIdx.x,y: %d,%d blockIdx.x,y: %d,%d -- row is %d, col is %d, N is %d.\\n", threadIdx.x, threadIdx.y, blockIdx.x, blockIdx.y, row, col, N);</pre>
<p>When we run our program, even more lines will come out, however, we will see one that says—<kbd>threadIdx.x,y: 0,0 blockIdx.x,y: 1,0 -- row is 2, col is 0.</kbd> and yet another that says—<kbd>threadIdx.x,y: 0,0 blockIdx.x,y: 1,0 -- row is 0, col is 2, N is 4</kbd>. By the <kbd>threadIdx</kbd> and <kbd>blockIdx</kbd> values, we see that this is the same thread in the same block, but with the <kbd>row</kbd> and <kbd>col</kbd> values reversed. Indeed, when we look at the invocation of the <kbd>rowcol_dot</kbd> device function, we see that <kbd>row</kbd> and <kbd>col</kbd> are indeed reversed from that in the declaration of the device function. We fix this, but when we run the program again, we get yet another assertion error.</p>
<p>Let's place another <kbd>printf</kbd> invocation in the device function, within the <kbd>for</kbd> loop; this, of course, is the <em>dot product</em> that is to perform a dot product between rows of matrix <kbd>A</kbd> with columns of matrix <kbd>B</kbd>. We will check the values of the matrices we are multiplying, as well as <kbd>k</kbd>; we will also only look at the values of the very first thread, or else we will get an incoherent mess of an output:</p>
<pre>if(threadIdx.x == 0 &amp;&amp; threadIdx.y == 0 &amp;&amp; blockIdx.x == 0 &amp;&amp; blockIdx.y == 0)<br/>            printf("Dot-product loop: k value is %d, matrix_a value is %f, matrix_b is %f.\\n", k, matrix_a[ row + k*N ], matrix_b[ col*N + k]);<br/> </pre>
<p>Let's look at the values of the <kbd>A</kbd> and <kbd>B</kbd> matrices that are set up for our unit tests before we continue:</p>
<div><img src="img/b2580d2a-f34e-4d83-abbd-43ebc163aba7.png" style="" width="369" height="481"/></div>
<p>We see that both matrices vary when we switch between columns but are constant when we change between rows. Therefore, by the nature of matrix multiplication, the values of matrix <kbd>A</kbd> should vary across <kbd>k</kbd> in our <kbd>for</kbd> loop, while the values of <kbd>B</kbd> should remain constant. Let's run the program again and check the pertinent output:</p>
<div><img src="img/47f4367a-3282-4718-9ec9-e818e0a16ffb.png" width="1312" height="150"/></div>
<p>So, it appears that we are not accessing the elements of the matrices in a correct way; remembering that these matrices are stored in a row-wise format, we modify the indices so that their values are accessed in the proper manner:</p>
<pre>val += matrix_a[ row*N + k ] * matrix_b[ col + k*N];</pre>
<p>Running the program again will yield no assertion errors. Congratulations, you just debugged a CUDA kernel using the only <kbd>printf</kbd>!</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Filling in the gaps with CUDA-C</h1>
                
            
            
                
<p>We will now go through the very basics of how to write a full-on CUDA-C program. We'll start small and just translate the <em>fixed</em> version of the little matrix multiplication test program we just debugged in the last section to a pure CUDA-C program, which we will then compile from the command line with NVIDIA's <kbd>nvcc</kbd> compiler into a native Windows or Linux executable file (we will see how to use the Nsight IDE in the next section, so we will just be doing this with only a text editor and the command line for now). Again, the reader is encouraged to look at the code we are translating from Python as we go along, which is available as the <kbd>matrix_ker.py</kbd> file in the repository.</p>
<p>Now, let's open our favorite text editor and create a new file entitled <kbd>matrix_ker.cu</kbd>. The extension will indicate that this is a CUDA-C program, which can be compiled with the <kbd>nvcc</kbd> compiler.</p>
<p>CUDA-C program and library source code filenames always use the <kbd>.cu</kbd> file extension. </p>
<p>Let's start at the beginning—as Python uses the <kbd>import</kbd> keyword at the beginning of a program for libraries, we recall the C language uses <kbd>#include</kbd>. We will need to include a few import libraries before we continue.</p>
<p class="mce-root"/>
<p>Let's start with these:</p>
<pre>#include &lt;cuda_runtime.h&gt;<br/>#include &lt;stdio.h&gt;<br/>#include &lt;stdlib.h&gt;</pre>
<p class="mce-root">Let's briefly think about what we need these for: <kbd>cuda_runtime.h</kbd> is the header file that has the declarations of all of the particular CUDA datatypes, functions, and structures that we will need for our program. We will need to include this for any pure CUDA-C program that we write. <kbd>stdio.h</kbd>, of course, gives us all of the standard I/O functions for the host such as <kbd>printf</kbd>, and we need <kbd>stdlib.h</kbd> for using the <kbd>malloc</kbd> and <kbd>free</kbd> dynamic memory allocation functions on the host.</p>
<p>Remember to always put <kbd>#include &lt;cuda_runtime.h&gt;</kbd> at the beginning of every pure CUDA-C program!</p>
<p>Now, before we continue, we remember that we will ultimately have to check the output of our kernel with a correct known output, as we did with NumPy's <kbd>allclose</kbd> function. Unfortunately, we don't have a standard or easy-to-use numerical math library in C as Python has with NumPy. More often than not, it's just easier to write your own equivalent function if it's something simple, as in this case. This means that we will now explicitly have to make our own equivalent to NumPy's <kbd>allclose</kbd>. We will do so as such: we will use the <kbd>#define</kbd> macro in C to set up a value called <kbd>_EPSILON</kbd>, which will act as a constant to indicate the minimum value between the output and expected output to be considered the same, and we will also set up a macro called <kbd>_ABS</kbd>, which will tell us the absolute difference between two numbers. We do so as follows:</p>
<pre>#define _EPSILON 0.001<br/>#define _ABS(x) ( x &gt; 0.0f ? x : -x )</pre>
<p>We can now create our own version of <kbd>allclose</kbd>. This will take in two float pointers and an integer value, <kbd>len</kbd>. We loop through both arrays and check them: if any points differ by more than <kbd>_EPSILON</kbd>, we return -1, otherwise we return 0 to indicate that the two arrays do indeed match.</p>
<p>We note one thing: since we are using CUDA-C, we precede the definition of the function with <kbd>__host__</kbd>, to indicate that this function is intended to be run on the CPU rather than on the GPU:</p>
<pre>__host__ int allclose(float *A, float *B, int len)<br/>{<br/><br/>  int returnval = 0;<br/>  <br/>  for (int i = 0; i &lt; len; i++)<br/>  {<br/>    if ( _ABS(A[i] - B[i]) &gt; _EPSILON )<br/>    {<br/>      returnval = -1;<br/>      break;<br/>    }<br/>  }<br/>  <br/>  return(returnval);<br/>}</pre>
<p class="mce-root">We now can cut and paste the device and kernel functions exactly as they appear in our Python version here:</p>
<pre><br/>__device__ float rowcol_dot(float *matrix_a, float *matrix_b, int row, int col, int N)<br/>{<br/>  float val = 0;<br/>  <br/>  for (int k=0; k &lt; N; k++)<br/>  {<br/>        val += matrix_a[ row*N + k ] * matrix_b[ col + k*N];<br/>  }<br/>  <br/>  return(val);<br/>}<br/><br/>__global__ void matrix_mult_ker(float * matrix_a, float * matrix_b, float * output_matrix, int N)<br/>{<br/><br/>    int row = blockIdx.x*blockDim.x + threadIdx.x;<br/>    int col = blockIdx.y*blockDim.y + threadIdx.y;<br/><br/>  output_matrix[col + row*N] = rowcol_dot(matrix_a, matrix_b, row, col, N);<br/>}</pre>
<p class="mce-root">Again, in contrast with <kbd>__host__</kbd>, notice that the CUDA device function is preceded by <kbd>__device__</kbd>, while the CUDA kernel is preceded by <kbd>__global__</kbd>.  </p>
<p>Now, as in any C program, we will need to write the <kbd>main</kbd> function, which will run on the host, where we will set up our test case and from which we explicitly launch our CUDA kernel onto GPU. Again, in contrast to vanilla C, we will have explicitly to specify that this is also to be run on the CPU with <kbd>__host__</kbd>:</p>
<pre>__host__ int main()<br/>{</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The first thing we will have to do is select and initialize our GPU. We do so with <kbd>cudaSetDevice</kbd> as follows:</p>
<pre>cudaSetDevice(0);</pre>
<div><kbd>cudaSetDevice(0)</kbd> will select the default GPU. If you have multiple GPUs installed in your system, you can select and use them instead with <kbd>cudaSetDevice(1)</kbd>, <kbd>cudaSetDevice(2)</kbd>, and so on.</div>
<p>We will now set up <kbd>N</kbd> as in Python to indicate the height/width of our matrix. Since our test case will consist only of 4 x 4 matrices, we set it to <kbd>4</kbd>. Since we will be working with dynamically allocated arrays and pointers, we will also have to set up a value that will indicate the number of bytes our test matrices will require. The matrices will consist of <em>N</em> x <em>N</em> floats, and we can determine the number of bytes required by a float with the <kbd>sizeof</kbd> keyword in C:</p>
<pre>int N = 4;<br/>int num_bytes = sizeof(float)*N*N;</pre>
<p>We now set up our test matrices as such; these will correspond exactly to the <kbd>test_a</kbd> and <kbd>test_b</kbd> matrices that we saw in our Python test program (notice how we use the <kbd>h_</kbd> prefix to indicate that these arrays are stored on the host, rather than on the device):</p>
<pre><br/> float h_A[] = { 1.0, 2.0, 3.0, 4.0, \<br/>                 1.0, 2.0, 3.0, 4.0, \<br/>                 1.0, 2.0, 3.0, 4.0, \<br/>                 1.0, 2.0, 3.0, 4.0 };<br/> <br/> float h_B[] = { 14.0, 13.0, 12.0, 11.0, \<br/>                 14.0, 13.0, 12.0, 11.0, \<br/>                 14.0, 13.0, 12.0, 11.0, \<br/>                 14.0, 13.0, 12.0, 11.0 };</pre>
<p>We now set up another array, which will indicate the expected output of the matrix multiplication of the prior test matrices. We will have to calculate this explicitly and put these values into our C code. Ultimately, we will compare this to the GPU output at the end of the program, but let's just set it up and get it out of the way:</p>
<pre>float h_AxB[] = { 140.0, 130.0, 120.0, 110.0, \<br/>                 140.0, 130.0, 120.0, 110.0, \<br/>                 140.0, 130.0, 120.0, 110.0, \<br/>                 140.0, 130.0, 120.0, 110.0 };</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We now declare some pointers for arrays that will live on the GPU, and for that we will copy the values of <kbd>h_A</kbd> and <kbd>h_B</kbd> and pointer to the GPU's output. Notice how we just use standard float pointers for this. Also, notice the prefix <kbd>d_</kbd>— this is another standard CUDA-C convention that indicates that these will exist on the device:</p>
<pre>float * d_A;<br/>float * d_B;<br/>float * d_output;</pre>
<p>Now, we will allocate some memory on the device for <kbd>d_A</kbd> and <kbd>d_B</kbd> with <kbd>cudaMalloc</kbd>, which is almost the same as <kbd>malloc</kbd> in C; this is what PyCUDA <kbd>gpuarray</kbd> functions such as <kbd>empty</kbd> or <kbd>to_gpu</kbd> have been calling us invisibly to allocate memory arrays on the GPU throughout this book:</p>
<pre>cudaMalloc((float **) &amp;d_A, num_bytes);<br/>cudaMalloc((float **) &amp;d_B, num_bytes);</pre>
<p>Let's think a bit about how this works: in C functions, we can get the address of a variable by preceding it with an ampersand (<kbd>&amp;</kbd>); if you have an integer, <kbd>x</kbd>, we can get its address with <kbd>&amp;x</kbd>. <kbd>&amp;x</kbd> will be a pointer to an integer, so its type will be <kbd>int *</kbd>. We can use this to set values of parameters into a C function, rather than use only pure return values.</p>
<p>Since <kbd>cudaMalloc</kbd> sets the pointer through a parameter rather than with the return value (in contrast to the regular <kbd>malloc</kbd>), we have to use the ampersand operator, which will be a pointer to a pointer, as it is a pointer to a float pointer as here (<kbd>float **</kbd>). We have to typecast this value explicitly with the parenthesis since <kbd>cudaMalloc</kbd> can allocate arrays of any type. Finally, in the second parameter, we have to indicate how many bytes to allocate on the GPU; we already set up <kbd>num_bytes</kbd> previously to be the number of bytes we will need to hold a 4 x 4 matrix consisting of floats, so we plug this in and continue.</p>
<p>We can now copy the values from <kbd>h_A</kbd> and <kbd>h_B</kbd> to <kbd>d_A</kbd> and <kbd>d_B</kbd> respectively with two invocations of the function <kbd>cudaMemcpy</kbd>, as follows:</p>
<pre>cudaMemcpy(d_A, h_A, num_bytes, cudaMemcpyHostToDevice);<br/>cudaMemcpy(d_B, h_B, num_bytes, cudaMemcpyHostToDevice);</pre>
<div><kbd>cudaMemcpy</kbd> always takes a destination pointer as the first argument, a source pointer as the second, the number of bytes to copy as the third argument, and a final parameter. The last parameter will indicate if we are copying from the host to the GPU with <kbd>cudaMemcpyHostToDevice</kbd> , from the GPU to the host with <kbd>cudaMemcpyDeviceToHost</kbd>, or between two arrays on the GPU with <kbd>cudaMemcpyDeviceToDevice</kbd>.  </div>
<p class="mce-root"/>
<p>We will now allocate an array to hold the output of our matrix multiplication on the GPU with another invocation of <kbd>cudaMalloc</kbd>:</p>
<pre>cudaMalloc((float **) &amp;d_output, num_bytes);</pre>
<p>Finally, we will have to have some memory set up on the host that will store the output of the GPU when we want to check the output of our kernel. Let's set up a regular C float pointer and allocate memory with <kbd>malloc</kbd> as we would normally:</p>
<pre>float * h_output;<br/>h_output = (float *) malloc(num_bytes);</pre>
<p>Now, we are almost ready to launch our kernel. CUDA uses a data structure called <kbd>dim3</kbd> to indicate block and grid sizes for kernel launches; we will set these up as such, since we want a grid with a dimension of 2 x 2 and blocks that are also of a dimension of 2 x 2:</p>
<pre>dim3 block(2,2,1);<br/>dim3 grid(2,2,1);</pre>
<p>We are now ready to launch our kernel; we use the triple-triangle brackets to indicate to the CUDA-C compiler the block and grid sizes that the kernel should be launched over:</p>
<pre>matrix_mult_ker &lt;&lt;&lt; grid, block &gt;&gt;&gt; (d_A, d_B, d_output, N);</pre>
<p>Now, of course, before we can copy the output of the kernel back to the host, we have to ensure that the kernel has finished executing. We do this by calling <kbd>cudaDeviceSynchronize</kbd>, which will block the host from issuing any more commands to the GPU until the kernel has finished execution:</p>
<pre>cudaDeviceSynchronize();</pre>
<p>We now can copy the output of our kernel to the array we've allocated on the host:</p>
<pre>cudaMemcpy(h_output, d_output, num_bytes, cudaMemcpyDeviceToHost);</pre>
<p>Again, we synchronize:</p>
<pre>cudaDeviceSynchronize();</pre>
<p>Before we check the output, we realize that we no longer need any of the arrays we allocated on the GPU. We free this memory by calling <kbd>cudaFree</kbd> on each array:</p>
<pre>cudaFree(d_A);<br/>cudaFree(d_B);<br/>cudaFree(d_output);</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We're done with the GPU, so we call <kbd>cudaDeviceReset</kbd>:</p>
<pre>cudaDeviceReset();</pre>
<p>Now, we finally check the output we copied onto the host with the <kbd>allclose</kbd> function we wrote at the beginning of this chapter. If the actual output doesn't match the expected output, we print an error and return <kbd>-1</kbd>, otherwise, we print that it does match and we return 0. We then put a closing bracket on our program's <kbd>main</kbd> function:</p>
<pre>if (allclose(h_AxB, h_output, N*N) &lt; 0)<br/> {<br/>     printf("Error! Output of kernel does not match expected output.\n");<br/>     free(h_output);<br/>     return(-1);<br/> }<br/> else<br/> {<br/>     printf("Success! Output of kernel matches expected output.\n");<br/>     free(h_output);<br/>     return(0);<br/> }<br/>}</pre>
<p>Notice that we make one final invocation to the standard C free function since we have allocated memory to <kbd>h_output </kbd>, in both cases. </p>
<p>We now save our file, and compile it into a Windows or Linux executable file from the command line with <kbd>nvcc matrix_ker.cu -o matrix_ker</kbd>. This should output a binary executable file, <kbd>matrix_ker.exe</kbd> (in Windows) or <kbd>matrix_ker</kbd> (in Linux). Let's try compiling and running it right now:</p>
<div><img src="img/ada8e5c8-9443-40ce-81d9-03100f570855.png" style="" width="998" height="220"/></div>
<p>Congratulations, you've just created your first pure CUDA-C program! (This example is available as <kbd>matrix_ker.cu</kbd> in the repository, under <kbd>7</kbd>.)</p>
<p class="mce-root"/>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Using the Nsight IDE for CUDA-C development and debugging</h1>
                
            
            
                
<p>Let's now learn how to use the Nsight IDE for developing CUDA-C programs. We will see how to import the program we just wrote, and compile and debug it from within Nsight. Note that there are differences between the Windows and Linux versions of Nsight, since it is effectively a plugin of the Visual Studio IDE under Windows and in the Eclipse IDE under Linux. We will cover both in the following two subsections; feel free to skip whatever operating system does not apply to you here.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Using Nsight with Visual Studio in Windows</h1>
                
            
            
                
<p>Open up Visual Studio, and click on File, then choose New | Project.... A window will pop up where you set the type of project: choose the NVIDIA drop-down item, and then choose CUDA 9.2:</p>
<div><img src="img/e5b3c450-388f-44ac-9d77-8f7cf574bf69.png" width="1966" height="1101"/></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Give the project some appropriate name and then click OK. A project should appear in the solution explorer window with a simple premade CUDA test program, consisting of one source file, <kbd>kernel.cu</kbd>, which consists of a simple parallel add kernel with test code. If you want to see whether this compiles and runs, click the green right-pointing arrow at the top marked Local Windows Debugger. A Terminal should pop up with some text output from the kernel and then close immediately.</p>
<p>If you have problems with a Windows Terminal-based application closing after you run it from Visual Studio, try adding <kbd>getchar();</kbd> to the end of the main function, which will keep the Terminal open until you press a key. (Alternatively, you can also use a debugger breakpoint at the end of the program.)</p>
<div><p>Now, let's add the CUDA-C program we just wrote. In the Solution Explorer window, right-click  <kbd>kernel.cu</kbd>, and click Remove on <kbd>kernel.cu</kbd>. Now, right-click on the project name, and choose Add, and then choose Existing item. We will now be able to select an existing file, so find where the path is to <kbd>matrix_ker.cu</kbd> and add it to the project. Click on the green arrow marked Local Windows Debugger at the top of the IDE and the program should compile and run, again in a Windows Terminal. So, that's it—we can set up and compile a complete CUDA program in Visual Studio now, just from those few steps. </p>
<p>Let's now see how to debug our CUDA kernel. Let's start by adding one breakpoint to our code at the entry point of the kernel <kbd>matrix_mult_ker</kbd>, where we set the value of <kbd>row</kbd> and <kbd>col</kbd>. We can add this breakpoint by clicking on the gray column left of the line numbers on the window; a red dot should appear there for every breakpoint we add. (You can ignore any red squiggly lines that the Visual Studio editor may place under your code; this is due to the fact that CUDA is not a <em>native</em> language to Visual Studio):</p>
<div><img src="img/b4408001-ab3d-42fb-9741-1e1a3c896491.png" width="1950" height="597"/></div>
</div>
<p>We can now start debugging. From the top menu, choose the Nsight drop-down menu and choose Start CUDA Debugging. There may be two options here, Start CUDA Debugging (Next-Gen) and Start CUDA Debugging (Legacy). It doesn't matter which one, but you may have issues with Next-Gen depending on your GPU; in that case, choose Legacy.</p>
<p>Your program should start up, and the debugger should halt at the breakpoint in our kernel that we just set. Let's press <em>F10</em> to step over the line, and now see if the <kbd>row</kbd> variable gets set correctly. Let's look at the Locals window in the Variable Explorer:</p>
<div><img src="img/4bbfaa46-46cb-44cc-ac0b-055dc9ce89ca.png" style="" width="1129" height="804"/></div>
<p>We can see that we are currently in the very first thread in the very first block in the grid by checking the values of <kbd>threadIdx</kbd> and <kbd>blockIdx</kbd>; <kbd>row</kbd> is set to <kbd>0</kbd>, which does indeed correspond to the correct value. Now, let's check the value of row for some different thread. To do this, we have to switch the <strong>thread focus</strong> in the IDE; we do this by clicking the Nsight drop-down menu above, then choosing Windows|CUDA Debug Focus.... A new menu should appear allowing you to choose a new thread and block. Change thread from 0, 0, 0 to 1, 0, 0 in the menu, and click OK:</p>
<div><img src="img/c5f2b196-8ce2-45b6-807a-11f8d86fe86b.png" style="" width="785" height="576"/></div>
<p>When you check the variables again, you should see the correct value is set for <kbd>row</kbd> for this thread:</p>
<div><img src="img/f2f27821-5df5-4ea7-a847-385967ba54a0.png" style="" width="1132" height="806"/></div>
<p>In a nutshell, that is how you debug with Nsight in Visual Studio. We now have the basics of how to debug a CUDA program from Nsight/Visual Studio in Windows, and we can use all of the regular conventions as we would for debugging a regular Windows program as with any other IDE (setting breakpoints, starting the debugger, continue/resume, step over, step in, and step out). Namely, the main difference is you have to know how to switch between CUDA threads and blocks to check variables, otherwise, it's pretty much the same.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Using Nsight with Eclipse in Linux</h1>
                
            
            
                
<p>We will now see how to use Nsight in Linux. You can open Nsight from either your desktop by selecting it or you can run it from a command line with the <kbd>nsight</kbd> command. The Nsight IDE will open. From the top of the IDE, click on File, then choose New... from the drop-down menu, and from there choose New CUDA C/C++ Project. A new window will appear, and from here choose CUDA Runtime Project. Give the project some appropriate name, and then click Next. You'll be prompted to give further settings options, but the defaults will work fine for our purposes for now. (Be sure to note where the source folder and project paths will be located in the third and fourth screens here.) You'll get to a final screen, where you can press Finish to create the project:</p>
<div><img src="img/e2f72961-a2fe-4c06-a4c2-2ab6653c140b.png" style="" width="1595" height="1634"/></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Finally, you'll end up at a project view with your new project and some placeholder code open; as of CUDA 9.2, this will consist of a reciprocal kernel example.</p>
<p>We can now import our code. Either you can just use the editor in Nsight to delete all of the code in the default source file and cut and paste it in, or you can manually delete the file from the project's source directory, manually copy the <kbd>matrix_ker.cu</kbd> file into the source directory, and then choose to refresh the source directory view in Nsight by selecting it and then pressing <em>F5</em>. You can now build the project with <em>Ctrl</em> + <em>B</em>, and run it with <em>F11</em>. The output of our program should appear within the IDE itself within the Console subwindow, as follows:</p>
<div><img src="img/1e287f83-ed1a-416e-aca7-511b375c1568.png" width="1950" height="324"/></div>
<p>We can now set a breakpoint within our CUDA code; let's set it at the entry point of our kernel where the row value is set. We set the cursor onto that row in the Eclipse editor, and then press <em>Ctrl</em> + <em>Shift</em> + <em>B</em> to set it. </p>
<p>We can now begin debugging by pressing <em>F11</em> (or clicking the bug icon). The program should be paused at the very beginning of the <kbd>main</kbd> function, so press <em>F8</em> to <em>resume</em> to the first breakpoint. You should see the first line in our CUDA kernel highlighted with an arrow pointing to it in the IDE; let's step over the current line by pressing <em>F6</em>, which will ensure that the row has been set.</p>
<p>Now, we can easily switch between different threads and blocks in our CUDA grid to check the current values that they hold as follows: from the top of the IDE, click on the Window drop-down menu, then click Show view, and then choose CUDA. A window with the currently running kernel should open, and from here you can see a list of all of the blocks that this kernel is running over.</p>
<p>Click on the first one and from here you will be able to see all of the individual threads that are running within the block:</p>
<div><img src="img/429490aa-3a80-48af-8833-9cc6a4988a7f.png" style="" width="1617" height="888"/></div>
<p>Now, we can look at the variable corresponding to the very first thread in the very first block by clicking on the Variables tab—here, row should be 0, as expected:</p>
<div><img src="img/2d42ba20-c3bb-4f76-acc9-054fa1cc3960.png" style="" width="1453" height="627"/></div>
<p>Now, we can check the values for a different thread by again going to the CUDA tab, choosing the appropriate thread, and switching back. Let's stay in the same block, but choose thread (1, 0, 0) this time, and check the value of row again:</p>
<div><img src="img/ca99eb7a-04e5-40a0-982e-8a605b3cc697.png" style="" width="1459" height="568"/></div>
<p>We see that the value of row is now 1, as we expect. </p>
<p>We now have the basics of how to debug a CUDA program from Nsight/Eclipse in Linux, and we can use all of the regular conventions as you would for debugging a regular Linux program as with any other IDE (setting breakpoints, starting the debugger, continue/resume, step over, step in, and step out). Namely, the main difference here is we have to know how to switch between CUDA threads and blocks to check variables, otherwise, it's pretty much the same.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Using Nsight to understand the warp lockstep property in CUDA</h1>
                
            
            
                
<p>We will now use Nsight to step through some code to help us better understand some of the CUDA GPU architecture, and how <strong>branching</strong> within a kernel is handled. This will give us some insight about how to write more efficient CUDA kernels. By branching, we mean how the GPU handles control flow statements such as <kbd>if</kbd>, <kbd>else</kbd>, or <kbd>switch</kbd> within a CUDA kernel. In particular, we are interested in how <strong>branch divergence</strong> is handled within a kernel, which is what happens when one thread in a kernel satisfies the conditions to be an <kbd>if</kbd> statement, while another doesn't and is an <kbd>else</kbd> statement: they are divergent because they are executing different pieces of code.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's write a small CUDA-C program as an experiment: we will start with a small kernel that prints one output if its <kbd>threadIdx.x</kbd> value is even and another if it is odd. We then write a <kbd>main</kbd> function that will launch this kernel over one single block consisting of 32 different threads:</p>
<pre>#include &lt;cuda_runtime.h&gt;<br/>#include &lt;stdio.h&gt;<br/><br/>__global__ void divergence_test_ker()<br/>{<br/>    if( threadIdx.x % 2 == 0)<br/>        printf("threadIdx.x %d : This is an even thread.\n", threadIdx.x);<br/>    else<br/>        printf("threadIdx.x %d : This is an odd thread.\n", threadIdx.x);<br/>}<br/><br/>__host__ int main()<br/>{<br/>    cudaSetDevice(0);<br/>    divergence_test_ker&lt;&lt;&lt;1, 32&gt;&gt;&gt;();<br/>    cudaDeviceSynchronize();<br/>    cudaDeviceReset();<br/>}</pre>
<p>(This code is also available as <kbd>divergence_test.cu</kbd> in the repository.)</p>
<p>If we compile and run this from the command line, we might naively expect there to be an interleaved sequence of strings from even and odd threads; or maybe they will be randomly interleaved—since all of the threads run concurrently and branch about the same time, this would make sense.</p>
<p>Instead, every single time we run this, we always get this output:</p>
<div><img src="img/38c855ca-4bd8-4ab6-9e6b-371a9c9c0bb1.png" style="" width="828" height="1230"/></div>
<p>All of the strings corresponding to even threads are printed first, while all of the odd strings are printed second. Perhaps the Nsight debugger can shed some light on this; let's import this little program into an Nsight project as we did in the last section, putting a breakpoint at the first <kbd>if</kbd> statement in our kernel. We will then do a <em>step over</em>, so that the debugger stops where the first <kbd>printf</kbd> statement is. Since the default thread in Nsight is (0,0,0), this should have satisfied the first <kbd>if</kbd> statement so it will be stuck there until the debugger continues.</p>
<p>Let's switch over to an odd thread, say (1,0,0), and see where it is in our program now:</p>
<div><img src="img/58816167-07da-44ae-954a-4b6c3e911bbd.png" width="1950" height="402"/></div>
<p>Very strange! Thread (1,0,0) is also at the same place in execution as thread (0,0,0). Indeed, if we check every single other odd thread here, it will be stuck in the same place—at a <kbd>printf</kbd> statement that all of the odd threads should have skipped right past.</p>
<p>What gives? This is known as the <strong>warp lockstep property</strong>. A <strong>warp</strong> in the CUDA architecture is a unit of 32 "lanes" within which our GPU executes kernels and grids over, where each lane will execute a single thread. A major limitation of warps is that all threads executing on a single warp must step through the same exact code in <strong>lockstep</strong>; this means that not every thread does indeed run the same code, but just ignores steps that are not applicable to it. (This is called lockstep because it's like a group of soldiers marching <em>lockstep</em> in unison—whether they want to march, or not!)</p>
<p>The lockstep property implies that if one single thread running on a warp diverges from all 31 other threads in a single <kbd>if</kbd> statement, all 31 other threads have their execution delayed until this single anomalous thread finishes and returns from its solitary <kbd>if</kbd> divergence. This is a property that you should always keep in mind when writing kernels, and why branch divergence should be minimized as much as possible as a general rule in CUDA programming.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Using the NVIDIA nvprof profiler and Visual Profiler</h1>
                
            
            
                
<p>We will end with a brief overview of the command-line Nvidia <kbd>nvprof</kbd> profiler. In contrast to the Nsight IDE, we can freely use any Python code that we have written—we won't be compelled here to write full-on, pure CUDA-C test function code.</p>
<p>We can do a basic profiling of a binary executable program with the <kbd>nvprof program</kbd> command; we can likewise profile a Python script by using the <kbd>python</kbd> command as the first argument, and the script as the second as follows: <kbd>nvprof python program.py</kbd>. Let's profile the simple matrix-multiplication CUDA-C executable program that we wrote earlier, with <kbd>nvprof matrix_ker</kbd>:</p>
<div><img src="img/41fb00d0-582b-4b22-81ea-b9ae1988ad84.png" width="1930" height="613"/></div>
<p>We see that this is very similar to the output of the Python cProfiler module that we first used to analyze a Mandelbrot algorithm way back in <a href="f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml">Chapter 1</a>, <em>Why GPU Programming?</em>—only now, this exclusively tells us only about all of the CUDA operations that were executed. So, we can use this when we specifically want to optimize on the GPU, rather than concern ourselves with any of the Python or other commands that executed on the host. (We can further analyze each individual CUDA kernel operation with block and grid size launch parameters if we add the command-line option, <kbd>--print-gpu-trace</kbd>.)</p>
<p>Let's look at one more trick to help us <em>visualize</em> the execution time of all of the operations of a program; we will use <kbd>nvprof</kbd> to dump a file that can then be read by the NVIDIA Visual Profiler, which will show this to us graphically. Let's do this using an example from the last chapter, <kbd>multi-kernel_streams.py</kbd> (this is available in the repository under <kbd>5</kbd>). Let's recall that this was one of our introductory examples to the idea of CUDA streams, which allow us to execute and organize multiple GPU operations concurrently. Let's dump the output to a file with the <kbd>.nvvp</kbd> file suffix with the <kbd>-o</kbd> command-line option as follows: <kbd>nvprof -o m.nvvp python multi-kernel_streams.py</kbd>. We can now load this file into the NVIDIA Visual Profiler with the <kbd>nvvp m.nvvp</kbd> command.</p>
<p>We should see a timeline across all CUDA streams as such (remembering that the name of the kernel used in this program is called <kbd>mult_ker</kbd>):</p>
<div><img src="img/37df8c10-9b93-4d86-bb74-ff00920e7470.png" width="1950" height="1026"/></div>
<p>Not only can we see all kernel launches, but also memory allocations, memory copies, and other operations. This can be useful for getting an intuitive and visual understanding of how your program is using your GPU over time.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>We started out in this chapter by seeing how <kbd>printf</kbd> can be used within a CUDA kernel to output data from individual threads; we saw in particular how useful this can be for debugging code. We then covered some of the gaps in our knowledge in CUDA-C, so that we can write full test programs that we can compile into proper executable binary files: there is a lot of overhead here that was hidden from us before that we have to be meticulous about. Next, we saw how to create and compile a project in the Nsight IDE and how to use it for debugging. We saw how to stop at any breakpoint we set in a CUDA kernel and switch between individual threads to see the different local variables. We also used the Nsight debugger to learn about the warp lockstep property and why it is important to avoid branch divergence in CUDA kernels. Finally, we had a very brief overview of the NVIDIA command-line <kbd>nvprof</kbd> profiler and Visual Profiler for analyzing our GPU code.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Questions</h1>
                
            
            
                
<ol>
<li>In the first CUDA-C program that we wrote, we didn't use a <kbd>cudaDeviceSynchronize</kbd> command after the calls we made to allocate memory arrays on the GPU with <kbd>cudaMalloc</kbd>. Why was this not necessary? (Hint: Review the last chapter.)</li>
<li>Suppose we have a single kernel that is launched over a grid consisting of two blocks, where each block has 32 threads. Suppose all of the threads in the first block execute an <kbd>if</kbd> statement, while all of the threads in the second block execute the corresponding <kbd>else</kbd> statement. Will all of the threads in the second block have to "lockstep" through the commands in the <kbd>if</kbd> statement as the threads in the first block are actually executing them?</li>
<li>What if we executed a similar piece of code, only over a grid consisting of one single block executed over 64 threads, where the first 32 threads execute an <kbd>if</kbd> and the second 32 execute an <kbd>else</kbd> statement?</li>
<li>What can the <kbd>nvprof</kbd> profiler measure for us that Python's cProfiler cannot?</li>
<li>Name some contexts where we might prefer to use <kbd>printf</kbd> to debug a CUDA kernel and other contexts where it might be easier to use Nsight to debug a CUDA kernel.</li>
<li>What is the purpose of the <kbd>cudaSetDevice</kbd> command in CUDA-C?</li>
<li>Why do we have to use <kbd>cudaDeviceSynchronize</kbd> after every kernel launch or memory copy in CUDA-C?</li>
</ol>


            

            
        
    </div></div></body></html>