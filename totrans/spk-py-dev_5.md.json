["```py\n#\n# Socket Server\n#\nan@an-VB:~$ nc -lk 9999\nhello world\nhow are you\nhello  world\ncool it works\n```", "```py\n\"\"\"\n Counts words in UTF8 encoded, '\\n' delimited text received from the network every second.\n Usage: network_wordcount.py <hostname> <port>\n   <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.\n To run this on your local machine, you need to first run a Netcat server\n    `$ nc -lk 9999`\n and then run the example\n    `$ bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999`\n\"\"\"\nfrom __future__ import print_function\n\nimport sys\n\nfrom pyspark import SparkContext\nfrom pyspark.streaming import StreamingContext\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 3:\n        print(\"Usage: network_wordcount.py <hostname> <port>\", file=sys.stderr)\n        exit(-1)\n    sc = SparkContext(appName=\"PythonStreamingNetworkWordCount\")\n    ssc = StreamingContext(sc, 1)\n\n    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))\n    counts = lines.flatMap(lambda line: line.split(\" \"))\\\n                  .map(lambda word: (word, 1))\\\n                  .reduceByKey(lambda a, b: a+b)\n    counts.pprint()\n\n    ssc.start()\n    ssc.awaitTermination()\n```", "```py\n    ssc = StreamingContext(sc, 1)\n\n    ```", "```py\n    stream = ssc.socketTextStream(\"127.0.0.1\", 9999)\n\n    ```", "```py\n    stream.map(x: lambda (x,1))\n    .reduce(a+b)\n    .print()\n    ```", "```py\n    ssc.start()\n\n    ```", "```py\n    ssc.awaitTermination()\n\n    ```", "```py\n    ssc.stop()\n\n    ```", "```py\n#\n# Socket Client\n# an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6$ ./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999\n```", "```py\nan@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6$ ./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999\n-------------------------------------------\nTime: 2015-10-18 20:06:06\n-------------------------------------------\n(u'world', 1)\n(u'hello', 1)\n\n-------------------------------------------\nTime: 2015-10-18 20:06:07\n-------------------------------------------\n. . .\n-------------------------------------------\nTime: 2015-10-18 20:06:17\n-------------------------------------------\n(u'you', 1)\n(u'how', 1)\n(u'are', 1)\n\n-------------------------------------------\nTime: 2015-10-18 20:06:18\n-------------------------------------------\n\n. . .\n\n-------------------------------------------\nTime: 2015-10-18 20:06:26\n-------------------------------------------\n(u'', 1)\n(u'world', 1)\n(u'hello', 1)\n\n-------------------------------------------\nTime: 2015-10-18 20:06:27\n-------------------------------------------\n. . .\n-------------------------------------------\nTime: 2015-10-18 20:06:37\n-------------------------------------------\n(u'works', 1)\n(u'it', 1)\n(u'cool', 1)\n\n-------------------------------------------\nTime: 2015-10-18 20:06:38\n-------------------------------------------\n\n```", "```py\n\"\"\"\nTwitter Streaming API Spark Streaming into an RDD-Queue to process tweets live\n\n Create a queue of RDDs that will be mapped/reduced one at a time in\n 1 second intervals.\n\n To run this example use\n    '$ bin/spark-submit examples/AN_Spark/AN_Spark_Code/s07_twitterstreaming.py'\n\n\"\"\"\n#\nimport time\nfrom pyspark import SparkContext\nfrom pyspark.streaming import StreamingContext\nimport twitter\nimport dateutil.parser\nimport json\n\n# Connecting Streaming Twitter with Streaming Spark via Queue\nclass Tweet(dict):\n    def __init__(self, tweet_in):\n        super(Tweet, self).__init__(self)\n        if tweet_in and 'delete' not in tweet_in:\n            self['timestamp'] = dateutil.parser.parse(tweet_in[u'created_at']\n                                ).replace(tzinfo=None).isoformat()\n            self['text'] = tweet_in['text'].encode('utf-8')\n            #self['text'] = tweet_in['text']\n            self['hashtags'] = [x['text'].encode('utf-8') for x in tweet_in['entities']['hashtags']]\n            #self['hashtags'] = [x['text'] for x in tweet_in['entities']['hashtags']]\n            self['geo'] = tweet_in['geo']['coordinates'] if tweet_in['geo'] else None\n            self['id'] = tweet_in['id']\n            self['screen_name'] = tweet_in['user']['screen_name'].encode('utf-8')\n            #self['screen_name'] = tweet_in['user']['screen_name']\n            self['user_id'] = tweet_in['user']['id']\n\ndef connect_twitter():\n    twitter_stream = twitter.TwitterStream(auth=twitter.OAuth(\n        token = \"get_your_own_credentials\",\n        token_secret = \"get_your_own_credentials\",\n        consumer_key = \"get_your_own_credentials\",\n        consumer_secret = \"get_your_own_credentials\"))\n    return twitter_stream\n\ndef get_next_tweet(twitter_stream):\n    stream = twitter_stream.statuses.sample(block=True)\n    tweet_in = None\n    while not tweet_in or 'delete' in tweet_in:\n        tweet_in = stream.next()\n        tweet_parsed = Tweet(tweet_in)\n    return json.dumps(tweet_parsed)\n\ndef process_rdd_queue(twitter_stream):\n    # Create the queue through which RDDs can be pushed to\n    # a QueueInputDStream\n    rddQueue = []\n    for i in range(3):\n        rddQueue += [ssc.sparkContext.parallelize([get_next_tweet(twitter_stream)], 5)]\n\n    lines = ssc.queueStream(rddQueue)\n    lines.pprint()\n\nif __name__ == \"__main__\":\n    sc = SparkContext(appName=\"PythonStreamingQueueStream\")\n    ssc = StreamingContext(sc, 1)\n\n    # Instantiate the twitter_stream\n    twitter_stream = connect_twitter()\n    # Get RDD queue of the streams json or parsed\n    process_rdd_queue(twitter_stream)\n\n    ssc.start()\n    time.sleep(2)\n    ssc.stop(stopSparkContext=True, stopGraceFully=True)\n```", "```py\nan@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6$ bin/spark-submit examples/AN_Spark/AN_Spark_Code/s07_twitterstreaming.py\n-------------------------------------------\nTime: 2015-11-03 21:53:14\n-------------------------------------------\n{\"user_id\": 3242732207, \"screen_name\": \"cypuqygoducu\", \"timestamp\": \"2015-11-03T20:53:04\", \"hashtags\": [], \"text\": \"RT @VIralBuzzNewss: Our Distinctive Edition Holiday break Challenge Is In this article! Hooray!... -  https://t.co/9d8wumrd5v https://t.co/\\u2026\", \"geo\": null, \"id\": 661647303678259200}\n\n-------------------------------------------\nTime: 2015-11-03 21:53:15\n-------------------------------------------\n{\"user_id\": 352673159, \"screen_name\": \"melly_boo_orig\", \"timestamp\": \"2015-11-03T20:53:05\", \"hashtags\": [\"eminem\"], \"text\": \"#eminem https://t.co/GlEjPJnwxy\", \"geo\": null, \"id\": 661647307847409668}\n\n-------------------------------------------\nTime: 2015-11-03 21:53:16\n-------------------------------------------\n{\"user_id\": 500620889, \"screen_name\": \"NBAtheist\", \"timestamp\": \"2015-11-03T20:53:06\", \"hashtags\": [\"tehInterwebbies\", \"Nutters\"], \"text\": \"See? That didn't take long or any actual effort. This is #tehInterwebbies ... #Nutters Abound! https://t.co/QS8gLStYFO\", \"geo\": null, \"id\": 661647312062709761}\n\n```", "```py\n    > tar -xzf kafka_2.10-0.8.2.0.tgz\n    > cd kafka_2.10-0.8.2.0\n\n    ```", "```py\n    > bin/zookeeper-server-start.sh config/zookeeper.properties\n    an@an-VB:~/kafka/kafka_2.10-0.8.2.0$ bin/zookeeper-server-start.sh config/zookeeper.properties\n\n    [2015-10-31 22:49:14,808] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n    [2015-10-31 22:49:14,816] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)...\n\n    ```", "```py\n    > bin/kafka-server-start.sh config/server.properties\n\n    an@an-VB:~/kafka/kafka_2.10-0.8.2.0$ bin/kafka-server-start.sh config/server.properties\n    [2015-10-31 22:52:04,643] INFO Verifying properties (kafka.utils.VerifiableProperties)\n    [2015-10-31 22:52:04,714] INFO Property broker.id is overridden to 0 (kafka.utils.VerifiableProperties)\n    [2015-10-31 22:52:04,715] INFO Property log.cleaner.enable is overridden to false (kafka.utils.VerifiableProperties)\n    [2015-10-31 22:52:04,715] INFO Property log.dirs is overridden to /tmp/kafka-logs (kafka.utils.VerifiableProperties) [2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)\n\n    ```", "```py\n    > bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test\n\n    ```", "```py\n    > bin/kafka-topics.sh --list --zookeeper localhost:2181\n    Test\n    an@an-VB:~/kafka/kafka_2.10-0.8.2.0$ bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test\n    Created topic \"test\".\n    an@an-VB:~/kafka/kafka_2.10-0.8.2.0$ bin/kafka-topics.sh --list --zookeeper localhost:2181\n    test\n\n    ```", "```py\n    an@an-VB:~/kafka/kafka_2.10-0.8.2.0$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test\n    [2015-10-31 22:54:43,698] WARN Property topic is not valid (kafka.utils.VerifiableProperties)\n    This is a message\n    This is another message\n\n    ```", "```py\n    an@an-VB:~$ cd kafka/\n    an@an-VB:~/kafka$ cd kafka_2.10-0.8.2.0/\n    an@an-VB:~/kafka/kafka_2.10-0.8.2.0$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning\n    This is a message\n    This is another message\n\n    ```", "```py\n    ./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka_2.10:1.5.0 \\ examples/src/main/python/streaming/kafka_wordcount.py \\\n\n    localhost:2181 test\n\n    ```", "```py\n    an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6$ ./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka_2.10:1.5.0 examples/src/main/python/streaming/kafka_wordcount.py \n    localhost:2181 test\n\n    -------------------------------------------\n    Time: 2015-10-31 23:46:33\n    -------------------------------------------\n    (u'', 1)\n    (u'from', 2)\n    (u'Hello', 2)\n    (u'Kafka', 2)\n\n    -------------------------------------------\n    Time: 2015-10-31 23:46:34\n    -------------------------------------------\n\n    -------------------------------------------\n    Time: 2015-10-31 23:46:35\n    -------------------------------------------\n\n    ```", "```py\n    > pip install kafka-python\n    an@an-VB:~$ pip install kafka-python\n    Collecting kafka-python\n     Downloading kafka-python-0.9.4.tar.gz (63kB)\n    ...\n    Successfully installed kafka-python-0.9.4\n\n    ```", "```py\n#\n# kafka producer\n#\n#\nimport time\nfrom kafka.common import LeaderNotAvailableError\nfrom kafka.client import KafkaClient\nfrom kafka.producer import SimpleProducer\nfrom datetime import datetime\n\ndef print_response(response=None):\n    if response:\n        print('Error: {0}'.format(response[0].error))\n        print('Offset: {0}'.format(response[0].offset))\n\ndef main():\n    kafka = KafkaClient(\"localhost:9092\")\n    producer = SimpleProducer(kafka)\n    try:\n        time.sleep(5)\n        topic = 'test'\n        for i in range(5):\n            time.sleep(1)\n            msg = 'This is a message sent from the kafka producer: ' \\\n                  + str(datetime.now().time()) + ' -- '\\\n                  + str(datetime.now().strftime(\"%A, %d %B %Y %I:%M%p\"))\n            print_response(producer.send_messages(topic, msg))\n    except LeaderNotAvailableError:\n        # https://github.com/mumrah/kafka-python/issues/249\n        time.sleep(1)\n        print_response(producer.send_messages(topic, msg))\n\n    kafka.close()\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\nan@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/AN_Spark_Code$ python s08_kafka_producer_01.py\nError: 0\nOffset: 13\nError: 0\nOffset: 14\nError: 0\nOffset: 15\nError: 0\nOffset: 16\nError: 0\nOffset: 17\nan@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/AN_Spark_Code$\n\n```", "```py\n# kafka consumer\n# consumes messages from \"test\" topic and writes them to console.\n#\nfrom kafka.client import KafkaClient\nfrom kafka.consumer import SimpleConsumer\n\ndef main():\n  kafka = KafkaClient(\"localhost:9092\")\n  print(\"Consumer established connection to kafka\")\n  consumer = SimpleConsumer(kafka, \"my-group\", \"test\")\n  for message in consumer:\n    # This will wait and print messages as they become available\n    print(message)\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\nan@an-VB:~$ cd ~/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/AN_Spark_Code/\nan@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/AN_Spark_Code$ python s08_kafka_consumer_01.py\nConsumer established connection to kafka\nOffsetAndMessage(offset=13, message=Message(magic=0, attributes=0, key=None, value='This is a message sent from the kafka producer: 11:50:17.867309Sunday, 01 November 2015 11:50AM'))\n...\nOffsetAndMessage(offset=17, message=Message(magic=0, attributes=0, key=None, value='This is a message sent from the kafka producer: 11:50:22.051423Sunday, 01 November 2015 11:50AM'))\n\n```", "```py\n#\n# Kafka Spark Streaming Consumer    \n#\nfrom __future__ import print_function\n\nimport sys\n\nfrom pyspark import SparkContext\nfrom pyspark.streaming import StreamingContext\nfrom pyspark.streaming.kafka import KafkaUtils\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 3:\n        print(\"Usage: kafka_spark_consumer_01.py <zk> <topic>\", file=sys.stderr)\n        exit(-1)\n\n    sc = SparkContext(appName=\"PythonStreamingKafkaWordCount\")\n    ssc = StreamingContext(sc, 1)\n\n    zkQuorum, topic = sys.argv[1:]\n    kvs = KafkaUtils.createStream(ssc, zkQuorum, \"spark-streaming-consumer\", {topic: 1})\n    lines = kvs.map(lambda x: x[1])\n    counts = lines.flatMap(lambda line: line.split(\" \")) \\\n        .map(lambda word: (word, 1)) \\\n        .reduceByKey(lambda a, b: a+b)\n    counts.pprint()\n\n    ssc.start()\n    ssc.awaitTermination()\n```", "```py\n./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka_2.10:1.5.0 examples/AN_Spark/AN_Spark_Code/s08_kafka_spark_consumer_01.py localhost:2181 test\n```", "```py\nan@an-VB:~$ cd spark/spark-1.5.0-bin-hadoop2.6/\nan@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6$ ./bin/spark-submit \\\n>     --packages org.apache.spark:spark-streaming-kafka_2.10:1.5.0 \\\n>     examples/AN_Spark/AN_Spark_Code/s08_kafka_spark_consumer_01.py localhost:2181 test\n...\n:: retrieving :: org.apache.spark#spark-submit-parent\n  confs: [default]\n  0 artifacts copied, 10 already retrieved (0kB/18ms)\n-------------------------------------------\nTime: 2015-11-01 12:13:16\n-------------------------------------------\n\n-------------------------------------------\nTime: 2015-11-01 12:13:17\n-------------------------------------------\n\n-------------------------------------------\nTime: 2015-11-01 12:13:18\n-------------------------------------------\n\n-------------------------------------------\nTime: 2015-11-01 12:13:19\n-------------------------------------------\n(u'a', 5)\n(u'the', 5)\n(u'11:50AM', 5)\n(u'from', 5)\n(u'This', 5)\n(u'11:50:21.044374Sunday,', 1)\n(u'message', 5)\n(u'11:50:20.036422Sunday,', 1)\n(u'11:50:22.051423Sunday,', 1)\n(u'11:50:17.867309Sunday,', 1)\n...\n\n-------------------------------------------\nTime: 2015-11-01 12:13:20\n-------------------------------------------\n\n-------------------------------------------\nTime: 2015-11-01 12:13:21\n-------------------------------------------\n```"]