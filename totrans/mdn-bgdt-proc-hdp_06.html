<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Designing Real-Time Streaming Data Pipelines</h1>
                </header>
            
            <article>
                
<p><span>The first three chapters of this book all dealt with batch data. Having learned about the installation of Hadoop, data ingestion tools and techniques, and data stores, let's turn to data streaming. Not only will we look at how we can handle real-time data streams, but also how to design pipelines around them.</span></p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Real-time streaming concepts</li>
<li>Real-time streaming components</li>
<li>Apache Flink versus Spark</li>
<li>Apache Spark versus Storm</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Real-time streaming concepts</h1>
                </header>
            
            <article>
                
<p>Let's understand a few key concepts relating to real-time streaming applications in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data stream</h1>
                </header>
            
            <article>
                
<p>The data stream is a continuous flow of data from one end to another end, from sender to receiver, from producer to consumer. The speed and volume of the data may vary; it may be 1 GB of data per second or it may be 1 KB of data per second or per minute.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Batch processing versus real-time data processing</h1>
                </header>
            
            <article>
                
<p>In batch processing, data is collected in batches and each batch is sent for processing. The batch interval can be anything from one day to one minute. In today's data analytics and business intelligence world, data will not be processed in a batch for more than one day. Otherwise, business teams will not have any insight about what's happening to the business in a day-to-day basis. For example, the enterprise data warehousing team may collect all the orders made during the last 24 hours and send all these collected orders to the analytics engine for reporting.</p>
<p>The batch can be of one minute too. In the Spark framework (we will learn Spark in <a href="39cf9925-e4cc-473a-9032-a21b81e7b400.xhtml" target="_blank">Chapter 7</a>, <em>Large-Scale Data Processing Frameworks</em>), data is processed in micro batches.</p>
<p>In real-time processing, data (event) is transferred (streamed) from the producer (sender) to the consumer (receiver) as soon as an event is produced at the source end. For example, on an e-commerce website, orders gets processed immediately in an analytics engine as soon as the customer places the same order on that website. The advantage is that the business team of that company gets full insights about its business in real time (within a few milliseconds or sub milliseconds). It will help them adjust their promotions to increase their revenue, all in real-time.</p>
<p>The following image explains stream-processing architecture:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/01854cc6-c9ec-45f8-b35d-c5d9528176c0.png" style="width:41.92em;height:14.83em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Complex event processing </h1>
                </header>
            
            <article>
                
<p><strong>Complex event processing</strong> (<strong>CEP</strong>) is event processing that combines data from multiple sources to discover complex relationships or patterns. The goal of CEP is to identify meaningful events (such as opportunities or threats) and respond to them as quickly as possible. Fundamentally, CEP is about applying business rules to streaming event data. For example, CEP is used in use cases, such as stock trading, fraud detection, medical claim processing, and so on.</p>
<p>The following image explains stream-processing architecture:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c7c76b39-7e99-4115-9d6c-8b65f0dbec50.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Continuous availability</h1>
                </header>
            
            <article>
                
<p>Any real-time application is expected to be available all the time with no stoppage whatsoever. The event collection, processing, and storage components should be configured with the underlined assumptions of high availability. Any failure to any components will cause major disruptions to the running of the business. For example, in a credit card fraud detection application, all the fraudulent transactions need to be declined. If the application stops midway and is unable to decline fraudulent transactions, then it will result in heavy losses.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Low latency</h1>
                </header>
            
            <article>
                
<p>In any real-time application, the event should flow from source to target in a few milliseconds. The source collects the event, and a processing framework moves the event to its target data store where it can be analyzed further to find trends and patterns. All these should happen in real time, otherwise it may impact business decisions. For example, in a credit card fraud detection application, it is expected that all incoming transactions should be analyzed to find possible fraudulent transactions, if any. If the stream processing takes more than the desired period of time, it may be possible that these transactions may pass through the system, causing heavy losses to the business.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scalable processing frameworks</h1>
                </header>
            
            <article>
                
<p>Hardware failure may cause disruption to the stream processing application. To avoid this common scenario, we always need a processing framework that offers built-in APIs to support continuous computation, fault tolerant event state management, checkpoint features in the event of failures, in-flight aggregations, windowing, and so on. Fortunately, all the recent Apache projects such as Storm, Spark, Flink, and Kafka do support all and more of these features out of the box. The developer can use these APIs using Java, Python, and Scala.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Horizontal scalability</h1>
                </header>
            
            <article>
                
<p>The stream-processing platform should support horizontal scalability. That means adding more physical servers to the cluster in the event of a higher incoming data load to maintain throughput SLA. This way, the performance of processing can be increased by adding more nodes rather than adding more CPUs and memory to the existing servers; this is called <strong>vertical scalability</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Storage</h1>
                </header>
            
            <article>
                
<p>The preferable format of a stream is key-value pair. This format is very well represented by the JSON and Avro formats. The preferred storage to persist key-value type data is NoSQL data stores such as HBase and Cassandra. There are in total 100 NoSQL open source databases in the market these days. It's very challenging to choose the right database, one which supports storage to real-time events, because all these databases offer some unique features for data persistence. A few examples are schema agnostic, highly distributable, commodity hardware support, data replication, and so on.</p>
<p>The following image explains all stream processing components:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ddc50632-2e71-4f82-805a-f60ab3170c1e.png"/></div>
<p>In this chapter we will talk about message queue and stream processing frameworks in detail. In the next chapter, we will focus on data indexing techniques.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Real-time streaming components</h1>
                </header>
            
            <article>
                
<p>In the following sections we will walk through some important real-time streaming components.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Message queue</h1>
                </header>
            
            <article>
                
<p>The message queue lets you publish and subscribe to a stream of events/records. There are various alternatives we can use as a message queue in our real-time stream architecture. For example, there is RabbitMQ, ActiveMQ, and Kafka. Out of these, Kafka has gained tremendous popularity due to its various unique features. Hence, we will discuss the architecture of Kafka in detail. A discussion of RabbitMQ and ActiveMQ is beyond the scope of this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">So what is Kafka?</h1>
                </header>
            
            <article>
                
<p>Kafka is a fast, scalable, durable, and fault-tolerant publish-subscribe messaging system. Apache Kafka is an open-source stream-processing project. It provides a unified, high-throughput, and is a low-latency platform for handling real-time data streams. It provides a distributed storage layer, which supports massively scalable pub/sub message queues. Kafka Connect supports data import and export by connecting to external systems. Kafka Streams provides Java APIs for stream processing. Kafka works in combination with Apache Spark, Apache Cassandra, Apache HBase, Apache Spark, and more for real-time stream processing.</p>
<p>Apache Kafka was originally developed by LinkedIn, and was subsequently open sourced in early 2011. In November 2014, several engineers who worked on Kafka at LinkedIn created a new company named Confluent with a focus on Kafka. Please use this URL <a href="https://www.confluent.io/">https://www.confluent.io/</a> to learn more about the Confluent platform.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka features</h1>
                </header>
            
            <article>
                
<p>The following are features of Kafka:</p>
<ul>
<li class="mce-root"><strong>Kafka is scalable</strong>: Kafka Cluster consists of more than one physical server, which helps to distribute the data load. It is easily scalable in the event that additional throughputs are required as additional servers can be added to maintain the SLA.</li>
<li class="mce-root"><strong>Kafka is durable</strong>: During stream processing, Kafka persists messages on the persistent storage. This storage can be server local disks or Hadoop Cluster. In the event of message processing failure, the message can be accessed from the disk and replayed to process the message again. By default, the message is stored for seven days; this can be configured further.</li>
<li class="mce-root"><strong>Kafka is reliable</strong>: Kafka provides message reliability with the help of a feature called <strong>data replication</strong>. Each message is replicated at least three times (this is configurable) so that in the event of data loss, the copy of the message can be used for processing.</li>
<li class="mce-root"><strong>Kafka supports high performance throughput</strong>: Due to its unique architecture, partitioning, message storage, and horizontal scalability, Kafka helps to process terabytes of data per second.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka architecture</h1>
                </header>
            
            <article>
                
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">The following image shows Kafka's architecture:</p>
<div class="CDPAlignCenter CDPAlign"><strong><img src="assets/50297cdb-a34b-439f-be63-572e999c5c87.png" style="width:26.42em;height:22.25em;"/></strong></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka architecture components</h1>
                </header>
            
            <article>
                
<p>Let's take a look at each component in detail:</p>
<ul>
<li><strong>Producers</strong>: Producers publish messages to a specific Kafka topic. Producers may attach a key to each message record. By default, producers publish messages to topic partitions in round robin fashion. Sometimes, producers can be configured to write messages to a particular topic partition based on the hash value of message key.</li>
<li><strong>Topic</strong>: All messages are stored in a topic. A topic is a category or feed name to which records are published. A topic can be compared to a table in a relational database. Multiple consumers can be subscribed to a single topic to consume message records.</li>
<li><strong>Partition</strong>: A topic is divided into multiple partitions. Kafka offers topic parallelism by dividing topic into partitions and by placing each partition on a separate broker (server) of a Kafka Cluster. Each partition has a separate partition log on a disk where messages are stored. Each partition contains an ordered, immutable sequence of messages. Each message is assigned unique sequence numbers called <strong>offset</strong>. Consumers can read messages from any point from a partition—from the beginning or from any offset.</li>
<li><strong>Consumers</strong>: A consumer subscribes to a topic and consumes the messages. In order to increase the scalability, consumers of the same application can be grouped into a consumer group where each consumer can read messages from a unique partition.</li>
<li><strong>Brokers</strong>: Kafka is divided into multiple servers called <strong>brokers</strong>. All the brokers combined are called <strong>Kafka Cluster</strong>. Kafka brokers handle message writes from producers and message reads from consumers. Kafka Brokers stores all the messages coming from producers. The default period is of seven days. This period (retention period) can be configured based on the requirements. The retention period directly impacts the local storage of Kafka brokers. It takes more storage if a higher retention period is configured. After the retention period is over, the message is automatically discarded.</li>
<li><strong>Kafka Connect</strong>: According to Kafka documentation, Kafka Connect allows the building and running of reusable producers or consumers that connect Kafka topics to existing applications or data systems. For example, a connector to a relational database might capture every change to a table.</li>
<li><strong>Kafka Streams</strong>: Stream APIs allow an application to act as a stream processor, consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka Connect deep dive</h1>
                </header>
            
            <article>
                
<p>Kafka Connect is a part of the Confluent platform. It is integrated with Kafka. Using Kafka Connect, it's very easy to build data pipelines from multiple sources to multiple targets. <strong>Source Connectors</strong> import data from another system (for example, from a relational database into Kafka) and <strong>Sink Connectors </strong>export data (for <span>example,</span> the contents of a Kafka topic to an HDFS file).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka Connect architecture</h1>
                </header>
            
            <article>
                
<p>The following image shows Kafka Connect's architecture:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cbd6b15a-6f36-4e57-b6fd-0e01ae874bb4.png" style="width:45.00em;height:29.17em;"/></div>
<p>The data flow can be explained as follows:</p>
<ul>
<li>Various sources are connected to <strong>Kafka Connect Cluster</strong>. <strong>Kafka Connect Cluster</strong> pulls data from the sources.</li>
<li><strong>Kafka Connect Cluster</strong> consists of a set of worker processes that are containers that execute connectors, and tasks automatically coordinate with each other to distribute work and provide scalability and fault tolerance.</li>
<li><strong>Kafka Connect Cluster</strong> pushes data to <strong>Kafka Cluster</strong>.</li>
<li><strong>Kafka Cluster</strong> persists the data on to the broker local disk or on Hadoop.</li>
<li>Streams applications such as Storm, Spark Streaming, and Flink pull the data from <strong>Kafka Cluster</strong> for stream transformation, aggregation, join, and so on. These applications can send back the data to Kafka or persist it to external data stores such as HBase, Cassandra, MongoDB, HDFS, and so on.</li>
<li><strong>Kafka Connect Cluster</strong> pulls data from <strong>Kafka Cluster</strong> and pushes it Sinks.</li>
<li>Users can extend the existing Kafka connectors or develop brand new connectors.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka Connect workers standalone versus distributed mode</h1>
                </header>
            
            <article>
                
<p>Users can run Kafka Connect in two ways: standalone mode or distributed mode.</p>
<p>In standalone mode, a single process runs all the connectors. It is not fault tolerant. Since it uses only a single process, it is not scalable. Generally, it is useful for users for development and testing purposes.</p>
<p>In distributed mode, multiple workers run Kafka Connect. In this mode, Kafka Connect is scalable and fault tolerant, so it is used in production deployment.</p>
<p>Let's learn more about Kafka and Kafka Connect (standalone mode). In this example, we will do the following:</p>
<ol>
<li>Install Kafka</li>
<li>Create a topic</li>
<li>Generate a few messages to verify the producer and consumer</li>
<li>Kafka Connect-File-source and file-sink</li>
<li>Kafka Connect-JDBC -Source</li>
</ol>
<p>The following image shows a use case using <strong>Kafka Connect</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a8818e04-6229-45ec-9063-ca5a4c86a8a9.png"/></div>
<p>Let's see how Kafka and Kafka Connect works by running a few examples. For more details, use the following link for Kafka Confluent's documentation: <a href="https://docs.confluent.io/current/">https://docs.confluent.io/current/</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Install Kafka</h1>
                </header>
            
            <article>
                
<p>Let's perform the following steps to install Kafka:</p>
<ol>
<li>Download Confluent from <a href="https://www.confluent.io/download/">https://www.confluent.io/download/</a></li>
<li>Click on <span class="packt_screen">C</span><span class="packt_screen">onfluent Open Source</span></li>
<li>Download the <span>file </span><span><kbd>confluent-oss-4.0.0-2.11.tar.gz</kbd> from</span> <kbd>tar.gz</kbd> and perform the following:</li>
</ol>
<pre style="padding-left: 60px">tar xvf confluent-oss-4.0.0-2.11.tar.gz<br/>cd /opt/confluent-4.0.0/etc/kafka<br/>vi server.properties</pre>
<ol start="4">
<li>Uncomment <kbd>listeners=PLAINTEXT://:9092</kbd></li>
<li>Start Confluent:</li>
</ol>
<pre style="padding-left: 60px">$ ./bin/confluent start schema-registry</pre>
<ol start="6">
<li>Start <kbd>zookeeper</kbd>:</li>
</ol>
<pre style="padding-left: 60px">zookeeper is [UP]</pre>
<ol start="7">
<li>Start <kbd>kafka</kbd>:</li>
</ol>
<pre style="padding-left: 60px">kafka is [UP]</pre>
<ol start="8">
<li>Start <kbd>schema-registry</kbd>:</li>
</ol>
<pre style="padding-left: 60px">schema-registry is [UP]<br/>A4774045:confluent-4.0.0 m046277$</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Create topics</h1>
                </header>
            
            <article>
                
<p>Perform the following steps to create topics:</p>
<ol>
<li>List the existing topics</li>
<li>Open another terminal and enter the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>/opt/confluent-4.0.0</strong><br/><strong>bin/kafka-topics --list --zookeeper localhost:2181</strong><br/><strong>_schemas</strong></pre>
<ol start="3">
<li>Create a topic:</li>
</ol>
<pre style="padding-left: 60px">bin/kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic my-first-topic<br/><br/>Created topic "my-first-topic"</pre>
<ol start="4">
<li>Double check the newly created topic:</li>
</ol>
<pre style="padding-left: 60px">bin/kafka-topics --list --zookeeper localhost:2181<br/>_schemas<br/>my-first-topic</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generate messages to verify the producer and consumer</h1>
                </header>
            
            <article>
                
<p>Perform the following steps to generate messages to verify the producer and consumer:</p>
<ol>
<li>Send messages to Kafka <kbd>my-first-topic</kbd>:</li>
</ol>
<pre style="padding-left: 60px">bin/kafka-console-producer --broker-list localhost:9092 --topic my-first-topic<br/>test1<br/>test2<br/>test3</pre>
<ol start="2">
<li>Start consumer to consume messages</li>
<li>Open another terminal and enter the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ bin/kafka-console-consumer --bootstrap-server localhost:9092 --topic my-first-topic --from-beginning</strong><br/><strong>test3</strong><br/><strong>test2</strong><br/><strong>test1</strong></pre>
<ol start="4">
<li>Go to the producer terminal and enter another message:</li>
</ol>
<pre style="padding-left: 60px">test4</pre>
<ol start="5">
<li>Verify the consumer terminal to check whether you can see the message <kbd>test4</kbd></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka Connect using file Source and Sink</h1>
                </header>
            
            <article>
                
<p>Let's take a look at how to create topics using file Source and Sink, with the help of the following:</p>
<pre>cd /opt/confluent-4.0.0/etc/kafka<br/>vi connect-file-test-source.properties<br/>name=local-file-source<br/>connector.class=FileStreamSource<br/>tasks.max=1<br/>file=/opt/kafka_2.10-0.10.2.1/source-file.txt<br/>topic=my-first-topic<br/>vi connect-file-test-sink.properties<br/>name=local-file-sink<br/>connector.class=FileStreamSink<br/>tasks.max=1<br/>file=/opt/kafka_2.10-0.10.2.1/target-file.txt<br/>topics=my-first-topic</pre>
<p>Perform the following steps:</p>
<ol>
<li>Start the Source Connector and Sink Connector:</li>
</ol>
<pre style="padding-left: 60px">cd /opt/confluent-4.0.0<br/>$ ./bin/connect-standalone config/connect-standalone.properties config/connect-file-test-source.properties config/connect-file-test-sink.properties<br/><br/>echo 'test-kafka-connect-1' &gt;&gt; source-file.txt<br/>echo 'test-kafka-connect-2' &gt;&gt; source-file.txt<br/>echo 'test-kafka-connect-3' &gt;&gt; source-file.txt<br/>echo 'test-kafka-connect-4' &gt;&gt; source-file.txt</pre>
<ol start="2">
<li>Double check whether the Kafka topic has received the messages:</li>
</ol>
<pre style="padding-left: 60px">$ ./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic my-first-topic<br/><br/>test3<br/>test1<br/>test4<br/><br/>{"schema":{"type":"string","optional":false},"payload":"test-kafka-connect-1"}<br/>{"schema":{"type":"string","optional":false},"payload":"test-kafka-connect-2"}<br/>{"schema":{"type":"string","optional":false},"payload":"test-kafka-connect-3"}<br/>{"schema":{"type":"string","optional":false},"payload":"test-kafka-connect-4"}<br/><br/>test2</pre>
<ol start="3">
<li>Verify <kbd>target-file.txt</kbd>:</li>
</ol>
<pre style="padding-left: 60px">$ cat target-file.txt<br/><br/>{"schema":{"type":"string","optional":false},"payload":"test-kafka-connect-1"}<br/>{"schema":{"type":"string","optional":false},"payload":"test-kafka-connect-2"}<br/>{"schema":{"type":"string","optional":false},"payload":"test-kafka-connect-3"}<br/>{"schema":{"type":"string","optional":false},"payload":"test-kafka-connect-4"}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka Connect using JDBC and file Sink Connectors</h1>
                </header>
            
            <article>
                
<p>The following image shows how we can push all records from the database table to a text file:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6d5fb5d5-d98a-4ec5-aef5-f3207b15beab.png"/></div>
<p>Let's implement the preceding example using Kafka Connect:</p>
<ol>
<li>Install SQLite:</li>
</ol>
<pre style="padding-left: 60px">$ sqlite3 firstdb.db<br/><br/>SQLite version 3.16.0 2016-11-04 19:09:39<br/>Enter ".help" for usage hints.<br/><br/>sqlite&gt;<br/>sqlite&gt; CREATE TABLE customer(cust_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, cust_name VARCHAR(255));<br/>sqlite&gt; INSERT INTO customer(cust_id,cust_name) VALUES(1,'Jon');<br/>sqlite&gt; INSERT INTO customer(cust_id,cust_name) VALUES(2,'Harry');<br/>sqlite&gt; INSERT INTO customer(cust_id,cust_name) VALUES(3,'James');<br/>sqlite&gt; select * from customer;<br/><br/>1|Jon<br/>2|Harry<br/>3|James</pre>
<ol start="2">
<li>Configure the JDBC Source Connector:</li>
</ol>
<pre style="padding-left: 60px">cd /opt/confluent-4.0.0<br/>vi ./etc/kafka-connect-jdbc/source-quickstart-sqlite.properties<br/>name=test-sqlite-jdbc-autoincrement<br/>connector.class=io.confluent.connect.jdbc.JdbcSourceConnector<br/>tasks.max=1<br/>connection.url=jdbc:sqlite:firstdb.db<br/>mode=incrementing<br/>incrementing.column.name=cust_id<br/>topic.prefix=test-sqlite-jdbc-</pre>
<ol start="3">
<li>Configure the file Sink Connector:</li>
</ol>
<pre style="padding-left: 60px">cd /opt/confluent-4.0.0<br/>vi etc/kafka/connect-file-sink.properties<br/>name=local-file-sink<br/>connector.class=FileStreamSink<br/>tasks.max=1<br/>file=/opt/confluent-4.0.0/test.sink.txt<br/>topics=test-sqlite-jdbc-customer</pre>
<ol start="4">
<li>Start Kafka Connect (<kbd>.jdbs</kbd> source and file Sink):</li>
</ol>
<pre>./bin/connect-standalone ./etc/schema-registry/connect-avro-standalone.properties ./etc/kafka-connect-jdbc/source-quickstart-sqlite.properties ./etc/kafka/connect-file-sink.properties</pre>
<ol start="5">
<li>Verify the consumer:</li>
</ol>
<pre style="padding-left: 60px">$ ./bin/kafka-avro-console-consumer --new-consumer --bootstrap-server localhost:9092 --topic test-sqlite-jdbc-customer --from-beginning</pre>
<p style="padding-left: 60px">The <kbd>--new-consumer</kbd> option is deprecated and will be removed in a future major release. The new consumer is used by default if the <kbd>--bootstrap-server</kbd> option is provided:</p>
<pre style="padding-left: 60px">{"cust_id":1,"cust_name":{"string":"Jon"}}<br/>{"cust_id":2,"cust_name":{"string":"Harry"}}<br/>{"cust_id":3,"cust_name":{"string":"James"}}</pre>
<ol start="6">
<li>Verify the target file:</li>
</ol>
<pre style="padding-left: 60px">tail -f /opt/confluent-4.0.0/test.sink.txt<br/><br/>Struct{cust_id=1,cust_name=Jon}<br/>Struct{cust_id=2,cust_name=Harry}<br/>Struct{cust_id=3,cust_name=James}</pre>
<ol start="7">
<li>Insert a few more records in the customer table:</li>
</ol>
<pre style="padding-left: 60px">sqlite&gt; INSERT INTO customer(cust_id,cust_name) VALUES(4,'Susan');<br/>sqlite&gt; INSERT INTO customer(cust_id,cust_name) VALUES(5,'Lisa');</pre>
<ol start="8">
<li>Verify the target file:</li>
</ol>
<pre style="padding-left: 60px">tail -f /opt/confluent-4.0.0/test.sink.txt</pre>
<p>You will see all customer records (<kbd>cust_id</kbd>) in the target file. Using the preceding example, you can customize and experiment with any other Sink.</p>
<p>The following table presents the available Kafka connectors on the Confluent platform (developed and fully supported by Confluent):</p>
<table class="table">
<tbody>
<tr>
<td>
<p><strong>Connector Name</strong></p>
</td>
<td>
<p><strong>Source/Sink</strong></p>
</td>
</tr>
<tr>
<td>
<p>JDBC</p>
</td>
<td>
<p>Source and Sink</p>
</td>
</tr>
<tr>
<td>
<p>HDFS</p>
</td>
<td>Sink</td>
</tr>
<tr>
<td>
<p>Elasticsearch</p>
</td>
<td>Sink</td>
</tr>
<tr>
<td>
<p>Amazon S3</p>
</td>
<td>Sink</td>
</tr>
</tbody>
</table>
<p> </p>
<p>For more information on other certified Connectors by Confluent, please use this URL: <a href="https://www.confluent.io/product/connectors/">https://www.confluent.io/product/connectors/</a>.<a href="https://www.confluent.io/product/connectors/"/></p>
<p>You must have observed that Kafka Connect is a configuration-based stream-processing framework. It means we have to configure only the Source and Sink Connector files. We don't need to write any code using low-level languages like Java or Scala. But, now, let's turn to one more popular real-time stream processing framework called <strong>Apache Storm</strong>. Let's understand some cool features of Apache Storm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache Storm</h1>
                </header>
            
            <article>
                
<p>Apache Storm is a free and open source distributed real-time stream processing framework. At the time of writing this book, the stable release version of Apache Storm is 1.0.5. The Storm framework is predominantly written in the Clojure programming language. Originally, it was created and developed by Nathan Marz and the team at Backtype. The project was later acquired by Twitter.</p>
<p>During one of his talks on the Storm framework, Nathan Marz talked about stream processing applications using any framework, such as Storm. These applications involved queues and worker threads. Some of the data source threads write messages to queues and other threads pick up these messages and write to target data stores. The main drawback here is that source threads and targets threads do not match the data load of each other and this results in data pileup. It also results in data loss and additional thread maintenance.</p>
<p>To avoid the preceding challenges, Nathan Marz came up with a great architecture that abstracts source threads and worker threads into Spouts and Bolts. These Spouts and Bolts are submitted to the Topology framework, which takes care of entire stream processing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Features of Apache Storm</h1>
                </header>
            
            <article>
                
<p>Apache Storm is distributed. In case of an increase in a stream's workload, multiple nodes can be added to the Storm Cluster to add more workers and more processing power to process.</p>
<p>It is a truly real-time stream processing system and supports <strong>low-latency</strong>. The event can be reached from source to target in in milliseconds, seconds, or minutes depending on the use cases.</p>
<p>Storm framework supports <strong>multiple programming languages</strong>, but Java is the top preference. Storm is <strong>fault-tolerant</strong>. It continues to operate even though the failure of any node in the cluster. Storm is <strong>reliable</strong>. It supports at least once or exactly-once processing.</p>
<p>There is <strong>no complexity</strong> to using Storm framework. For more detail information, refer to the Storm documentation: <a href="http://storm.apache.org/releases/1.0.4/index.html">http://storm.apache.org/releases/1.0.4/index.html</a>.<a href="http://storm.apache.org/releases/1.0.4/index.html"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Storm topology</h1>
                </header>
            
            <article>
                
<p>The following image shows a typical <strong>Storm Topology</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f1bc575d-ac6b-465d-aee2-2d3573f4f495.png" style="width:42.25em;height:22.58em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Storm topology components</h1>
                </header>
            
            <article>
                
<p>The following sections explain all the components of a Storm topology:</p>
<ul>
<li><strong>Topology</strong>:<strong> </strong>A topology is a <strong>DAG</strong> (<strong>directed acyclic graph</strong>) of spouts and bolts that are connected with stream groupings. A topology runs continuously untill it is killed.</li>
<li><strong>Stream</strong>:<strong> </strong>A stream is an unbounded sequence of tuples. A tuple can be of any data type. It supports all the Java data types.</li>
<li><strong>Stream groupings</strong>:<strong> </strong>Stream grouping decides which bolt receives a tuple from a spout. Basically, these are the strategies about how the stream will flow among different bolts. The following are the built-in stream groupings in Storm.</li>
<li><strong>Shuffle grouping</strong>:<strong> </strong>It is a default grouping strategy. Tuples are randomly distributed and each bolt gets an equal number of streams to process.</li>
<li><strong>Field grouping</strong>:<strong> </strong>In this strategy, the same value of a stream field will be sent to one bolt. For example, if all the tuples are grouped by <kbd>customer_id</kbd>, then all the tuples of the same <kbd>customer_id</kbd> will be sent one bolt task and all the tuples of another <kbd>customer_id</kbd> will be sent to another bolt task.</li>
<li><strong>All grouping</strong>:<strong> </strong>In all grouping, each tuple is sent to each bolt task. It can be used when two different functions have to be performed on the same set of data. In that case, the stream can be replicated and each function can be calculated on each copy of the data.</li>
<li><strong>Direct grouping</strong>: This is a special kind of grouping. Here, the developer can define the grouping logic within the component where tuple is emitted itself. The producer of the tuple decides which task of the consumer will receive this tuple.</li>
<li><strong>Custom grouping</strong>:<strong> </strong>The developer may decide to implement his/her own grouping strategy by implementing the <kbd>CustomGrouping</kbd> method.</li>
<li><strong>Spout</strong>:<strong> </strong>A spout connects to the data source and ingests streams into a Storm topology.</li>
<li><strong>Bolt</strong>:<strong> </strong>A spout emits a tuple to a bolt. A bolt is responsible for event transformation, joining events to other events, filtering, aggregation, and windowing. It emits the tuple to another bolt or persists it to a target. All processing in topologies is done in bolts. Bolts can do anything from filtering to functions, aggregations, joins, talking to databases, and more.</li>
<li><strong>Storm Cluster</strong>: The<strong> </strong>following image shows all the components of a <strong>Storm Cluster</strong>:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img src="assets/668cfc89-82eb-40c3-89cd-a8ec15a32143.png" style="width:34.25em;height:19.17em;"/></div>
<ul>
<li><strong>Storm Cluster nodes</strong>:<strong> </strong>The three main nodes of a Storm Cluster are Nimbus, Supervisor, and Zookeeper. The following section explains all the components in detail.</li>
<li><strong>Nimbus node</strong>:<strong> </strong>In Storm, this is the master node of a Storm Cluster. It distributes code and launches the worker tasks across the cluster. Basically, it assigns tasks to each node in a cluster. It also monitors the status of each job submitted. In the case of any job failure, Nimbus reallocates the job to a different supervisor within a cluster. In the case of Nimbus being unavailable, the workers will still continue to function. However, without Nimbus, workers won't be reassigned to other machines when necessary. In the case of an unavailable node, the tasks assigned to that node will time-out and Nimbus will reassign those tasks to other machines. In the case of both Nimbus and Supervisor being unavailable, they need to be restarted like nothing happened and no worker processes will be affected.</li>
<li><strong>Supervisor node</strong>:<strong> </strong>In Storm, this is a slave node. It communicates with Nimbus through ZooKeeper. It starts and stops the worker processes within a supervisor itself. For example, if Supervisor finds that a particular worker process has died, then it immediately restarts that worker process. If Supervisor fails to restart the worker after trying few times, then it communicates this to Nimbus and Nimbus will restart that worker on a different Supervisor node.</li>
<li><strong>Zookeeper node</strong>:<strong> </strong>It acts as a coordinator between masters (Nimbus) and slaves (supervisors) within a Storm Cluster. In a production environment, it is typical to set up a Zookeeper cluster that has three instances (nodes) of Zookeeper.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Storm on a single node cluster</h1>
                </header>
            
            <article>
                
<p>The following are the steps to install Storm Cluster on a single machine:</p>
<ol>
<li>Install <kbd>jdk</kbd>.<strong> </strong>Make sure you have installed 1.8:</li>
</ol>
<pre style="padding-left: 60px">$ java -version</pre>
<p style="padding-left: 60px">You should see the following output:</p>
<pre style="padding-left: 60px">openjdk version "1.8.0_141"<br/>OpenJDK Runtime Environment (build 1.8.0_141-b16)<br/>OpenJDK 64-Bit Server VM (build 25.141-b16, mixed mod</pre>
<ol start="2">
<li>Create a folder to download the <kbd>.tar</kbd> file of Storm:</li>
</ol>
<pre style="padding-left: 60px">$ mkdir /opt/storm<br/>$ cd storm</pre>
<ol start="3">
<li>Create a folder to persist Zookeeper and Storm data:</li>
</ol>
<pre style="padding-left: 60px">$ mkdir /usr/local/zookeeper/data<br/>$ mkdir /usr/local/storm/data</pre>
<ol start="4">
<li>Download Zookeeper and Storm:</li>
</ol>
<pre style="padding-left: 60px">$ wget <a href="http://apache.osuosl.org/zookeeper/stable/zookeeper-3.4.10.tar.gz">http://apache.osuosl.org/zookeeper/stable/zookeeper-3.4.10.tar.gz</a><br/>$ gunzip zookeeper-3.4.10.tar.gz<br/>$ tar -xvf zookeeper-3.4.10.tar<br/>$ wget <a href="http://mirrors.ibiblio.org/apache/storm/apache-storm-1.0.5/apache-storm-1.0.5.tar.gz">http://mirrors.ibiblio.org/apache/storm/apache-storm-1.0.5/apache-storm-1.0.5.tar.gz</a><br/>$ gunzip apache-storm-1.0.5.tar.gz<br/>$ tar -xvf apache-storm-1.0.5.tar</pre>
<ol start="5">
<li>Configure Zookeeper and set the following to Zookeeper (<kbd>zoo.cfg</kbd>):</li>
</ol>
<pre style="padding-left: 60px">$ cd zookeeper-3.4.10<br/>$ vi con/zoo.cfg<br/>tickTime = 2000<br/>dataDir = /usr/local/zookeeper/data<br/>clientPort = 2181</pre>
<ol start="6">
<li>Configure Storm as follows:</li>
</ol>
<pre style="padding-left: 60px">$ cd /opt/ apache-storm-1.0.5<br/>$ vi conf/storm.yaml</pre>
<ol start="7">
<li>Add the following:</li>
</ol>
<pre style="padding-left: 60px">storm.zookeeper.servers:<br/> - "127.0.0.1"<br/> nimbus.host: "127.0.0.1"<br/> storm.local.dir: "/usr/local/storm/data"<br/> supervisor.slots.ports:<br/> - 6700<br/> - 6701<br/> - 6702<br/> - 6703</pre>
<p style="padding-left: 60px">(for additional workers, add more ports, such as 6704 and so on)</p>
<ol start="8">
<li>Start Zookeeper:</li>
</ol>
<pre style="padding-left: 60px">$ cd /opt/zookeeper-3.4.10<br/>$ bin/zkServer.sh start &amp;amp;amp;</pre>
<ol start="9">
<li>Start Nimbus:</li>
</ol>
<pre style="padding-left: 60px">$ cd /opt/ apache-storm-1.0.5<br/>$ bin/storm nimbus &amp;amp;amp;</pre>
<ol start="10">
<li>Start Supervisor:</li>
</ol>
<pre style="padding-left: 60px">$ bin/storm supervisor &amp;amp;amp;</pre>
<ol start="11">
<li>Verify installation in the Storm UI:</li>
</ol>
<pre style="padding-left: 60px">http://127.0.0.1:8080</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing a real-time streaming pipeline with Storm</h1>
                </header>
            
            <article>
                
<p>In this section, we will create the following three pipelines:</p>
<ul>
<li>Streaming pipeline with Kafka - Storm - MySQL</li>
<li>Streaming pipeline with Kafka - Storm - HDFS - Hive</li>
</ul>
<p>In this section, we will see how data streams flow from Kafka to Storm to MySQL table.</p>
<p>The whole pipeline will work as follows:</p>
<ol>
<li>We will ingest customer records (<kbd>customer_firstname</kbd> and <kbd>customer_lastname</kbd>) in Kafka using the Kafka console-producer API.</li>
<li>After that, Storm will pull the messages from Kafka.</li>
<li>A connection to MySQL will be established.</li>
<li>Storm will use MySQL-Bolt to ingest records into MySQL table. MySQL will automatically generate <kbd>customer_id</kbd>.</li>
<li>The MySQL table data (<kbd>customer_id</kbd>, <kbd>customer_firstname</kbd>, and <kbd>customer_lastname</kbd>) will be accessed using SQL.</li>
</ol>
<p>We will develop the following Java classes:</p>
<ul>
<li><kbd>MysqlConnection.java</kbd>: This class will establish a connection with the local MySQL database.</li>
<li><kbd>MysqlPrepare.java</kbd>: This class will prepare the SQL statements to be inserted into the database.</li>
<li><kbd>MysqlBolt</kbd>: This class is a storm bolt framework to emit the tuple from Kafka to MySQL.</li>
<li><kbd>MySQLKafkaTopology</kbd>: This is a Storm Topology Framework that builds a workflow to bind spouts (Kafka) to Bolts (MySQL). Here, we are using a Local Storm Cluster.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Streaming a pipeline from Kafka to Storm to MySQL</h1>
                </header>
            
            <article>
                
<p><span>The following image shows the components of the pipeline. In this pipeline, we will learn how the messages will flow from Kafka to Storm to MySQL in real-time:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/24ae013a-817c-478e-a2c5-04a08eb38f29.png" style="width:47.58em;height:18.00em;"/></div>
<p>The following is the complete Java code for <kbd>MysqlConnection.java</kbd>:</p>
<pre><strong>package</strong> com.StormMysql;<br/><strong>import</strong> java.sql.Connection;<br/><strong>import</strong> java.sql.DriverManager;<br/><strong>public class</strong> MysqlConnection {<br/><strong>private</strong> String <strong>server_name</strong>;<br/> <strong>private</strong> String <strong>database_name</strong>;<br/> <strong>private</strong> String <strong>user_name</strong>;<br/> <strong>private</strong> String <strong>password</strong>;<br/> <strong>private</strong> Connection <strong>connection</strong>;<br/><br/><strong>public</strong> MysqlConnection(String server_name, String database_name, String user_name, String password)<br/> {<br/> <strong>this</strong>.<strong>server_name</strong>=server_name;<br/> <strong>this</strong>.<strong>database_name</strong>=database_name;<br/> <strong>this</strong>.<strong>user_name</strong>=user_name;<br/> <strong>this</strong>.<strong>password</strong>=password;<br/> }<br/><br/><strong>public</strong> Connection getConnection()<br/> {<br/> <strong>return</strong> <strong>connection</strong>;<br/> }<br/><br/><strong>public boolean</strong> open()<br/> {<br/> <strong>boolean</strong> successful=<strong>true</strong>;<br/> <strong>try</strong>{<br/> Class.<em>forName</em>(<strong>"com.mysql.jdbc.Driver"</strong>);<br/> <strong>connection</strong> = DriverManager.<em>getConnection</em>(<strong>"jdbc:mysql://"</strong>+<strong>server_name</strong>+<strong>"/"</strong>+<strong>database_name</strong>+<strong>"?"</strong>+<strong>"user="</strong>+<strong>user_name</strong>+<strong>"&amp;amp;amp;password="</strong>+<strong>password</strong>);<br/> }<strong>catch</strong>(Exception ex)<br/> {<br/> successful=<strong>false</strong>;<br/> ex.printStackTrace();<br/> }<br/> <strong>return</strong> successful;<br/> }<br/><br/><strong>public boolean</strong> close()<br/> {<br/> <strong>if</strong>(<strong>connection</strong>==<strong>null</strong>)<br/> {<br/> <strong>return false</strong>;<br/> }<br/><br/><strong>boolean</strong> successful=<strong>true</strong>;<br/> <strong>try</strong>{<br/> <strong>connection</strong>.close();<br/> }<strong>catch</strong>(Exception ex)<br/> {<br/> successful=<strong>false</strong>;<br/> ex.printStackTrace();<br/> }<br/><br/><strong>return</strong> successful;<br/> }<br/> }</pre>
<p>The following is the complete code for <kbd>MySqlPrepare.java</kbd>:</p>
<pre><strong>package</strong> com.StormMysql;<br/><strong>import</strong> org.apache.storm.tuple.Tuple;<br/><strong>import</strong> java.sql.PreparedStatement;<br/><strong>public class</strong> MySqlPrepare {<br/> <strong>private</strong> MysqlConnection <strong>conn</strong>;<br/><br/><strong>public</strong> MySqlPrepare(String server_name, String database_name, String user_name, String password)<br/> {<br/> <strong>conn</strong> = <strong>new</strong> MysqlConnection(server_name, database_name, user_name, password);<br/> <strong>conn</strong>.open();<br/> }<br/><br/><strong>public void</strong> persist(Tuple tuple)<br/> {<br/> PreparedStatement statement=<strong>null</strong>;<br/> <strong>try</strong>{<br/> statement = <strong>conn</strong>.getConnection().prepareStatement(<strong>"insert into customer (cust_id,cust_firstname, cust_lastname) values (default, ?,?)"</strong>);<br/> statement.setString(1, tuple.getString(0));<br/><br/>statement.executeUpdate();<br/> }<strong>catch</strong>(Exception ex)<br/> {<br/> ex.printStackTrace();<br/> }<strong>finally<br/></strong>{<br/> <strong>if</strong>(statement != <strong>null</strong>)<br/> {<br/> <strong>try</strong>{<br/> statement.close();<br/> }<strong>catch</strong>(Exception ex)<br/> {<br/> ex.printStackTrace();<br/> }<br/> }<br/> }<br/> }<br/><br/><strong>public void</strong> close()<br/> {<br/> <strong>conn</strong>.close();<br/> }<br/> }</pre>
<p>The following is the complete code for <kbd>MySqlBolt.java</kbd>:</p>
<pre><strong>package</strong> com.StormMysql;<br/><br/><strong>import</strong> java.util.Map;<br/><br/><strong>import</strong> org.apache.storm.topology.BasicOutputCollector;<br/> <strong>import</strong> org.apache.storm.topology.OutputFieldsDeclarer;<br/> <strong>import</strong> org.apache.storm.topology.base.BaseBasicBolt;<br/> <strong>import</strong> org.apache.storm.tuple.Fields;<br/> <strong>import</strong> org.apache.storm.tuple.Tuple;<br/> <strong>import</strong> org.apache.storm.tuple.Values;<br/> <strong>import</strong> org.apache.storm.task.TopologyContext;<br/><strong><br/>import</strong><span> java.util.Map;<br/><br/></span><strong>public class</strong> MySqlBolt <strong>extends</strong> BaseBasicBolt {<br/><br/><strong>private static final long</strong> <strong><em>serialVersionUID</em></strong> = 1L;<br/> <strong>private</strong> MySqlPrepare <strong>mySqlPrepare</strong>;<br/><br/>@Override<br/> <strong>public void</strong> prepare(Map stormConf, TopologyContext context)<br/> {<br/> <strong>mySqlPrepare</strong>=<strong>new</strong> MySqlPrepare(<strong>"localhost"</strong>, <strong>"sales"</strong>,<strong>"root"</strong>,<strong>""</strong>);<br/> }<br/><br/><strong>public void</strong> execute(Tuple input, BasicOutputCollector collector) {<br/> <em>//</em> <strong><em>TODO Auto-generated method stub<br/></em></strong><strong>mySqlPrepare</strong>.persist(input);<br/> <em>//System.out.println(input);<br/></em>}<br/>@Override<br/> <strong>public void</strong> cleanup() {<br/> <strong>mySqlPrepare</strong>.close();<br/> }<br/>}</pre>
<p>The following is the complete code for <kbd>KafkaMySQLTopology.java</kbd>:</p>
<pre><strong>package</strong> com.StormMysql;</pre>
<pre><strong>import</strong> org.apache.storm.Config;<br/> <strong>import</strong> org.apache.storm.spout.SchemeAsMultiScheme;<br/> <strong>import</strong> org.apache.storm.topology.TopologyBuilder;<br/> <strong>import</strong> org.apache.storm.kafka.*;<br/> <strong>import</strong> org.apache.storm.LocalCluster;<br/> <strong>import</strong> org.apache.storm.generated.AlreadyAliveException;<br/> <strong>import</strong> org.apache.storm.generated.InvalidTopologyException;</pre>
<pre><strong>public class</strong> KafkaMySQLTopology<br/> {<br/> <strong>public static void</strong> main( String[] args ) <strong>throws</strong> AlreadyAliveException, InvalidTopologyException<br/> {<br/> ZkHosts zkHosts=<strong>new</strong> ZkHosts(<strong>"localhost:2181"</strong>);</pre>
<pre>String topic=<strong>"mysql-topic"</strong>;<br/> String consumer_group_id=<strong>"id7"</strong>;</pre>
<pre>SpoutConfig kafkaConfig=<strong>new</strong> SpoutConfig(zkHosts, topic, <strong>""</strong>, consumer_group_id);</pre>
<pre>kafkaConfig.<strong>scheme</strong>=<strong>new</strong> SchemeAsMultiScheme(<strong>new</strong> StringScheme());</pre>
<pre>KafkaSpout kafkaSpout=<strong>new</strong> KafkaSpout(kafkaConfig);</pre>
<pre>TopologyBuilder builder=<strong>new</strong> TopologyBuilder();<br/> builder.setSpout(<strong>"KafkaSpout"</strong>, kafkaSpout);<br/> builder.setBolt(<strong>"MySqlBolt"</strong>, <strong>new</strong> MySqlBolt()).globalGrouping(<strong>"KafkaSpout"</strong>);</pre>
<pre>LocalCluster cluster=<strong>new</strong> LocalCluster();</pre>
<pre>Config config=<strong>new</strong> Config();</pre>
<pre>cluster.submitTopology(<strong>"KafkaMySQLTopology"</strong>, config, builder.createTopology());</pre>
<pre><strong>try</strong>{<br/> Thread.<em>sleep</em>(10000);<br/> }<strong>catch</strong>(InterruptedException ex)<br/> {<br/> ex.printStackTrace();<br/> }</pre>
<pre>// cluster.killTopology("KafkaMySQLTopology");<br/> // cluster.shutdown();<br/>}<br/> }</pre>
<p>Use the <kbd>pom.xml</kbd> file to build your project in IDE.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Streaming a pipeline with Kafka to Storm to HDFS</h1>
                </header>
            
            <article>
                
<p>In this section, we will see how the data streams will flow from Kafka to Storm to HDFS and access them with a Hive external table.</p>
<p>The following image shows the components of the pipeline. In this pipeline, we will learn how the messages will flow from Kafka to Storm to HDFS in real-time:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f7174858-5105-44ae-8eee-e52c6e8480af.png"/></div>
<p>The whole pipeline will work as follows:</p>
<ol>
<li>We will ingest customer records (<kbd>customer_id</kbd>, <kbd>customer_firstname</kbd>, and <kbd>customer_lastname</kbd>) in Kafka using the Kafka console-producer API</li>
<li>After that, Storm will pull the messages from Kafka</li>
<li>A Connection to HDFS will be established</li>
<li>Storm will use HDFS-Bolt to ingest records into HDFS</li>
<li>Hive external table will be created to store (<kbd>customer_id</kbd>, <kbd>customer_firstname</kbd>, and <kbd>customer_lastname</kbd>)</li>
<li>The Hive table data (<kbd>customer_id</kbd>, <kbd>customer_firstname</kbd>, and <kbd>customer_lastname</kbd>) will be accessed using SQL</li>
</ol>
<p>We will develop the following Java classes:</p>
<p><kbd>KafkaTopology.java</kbd>: This is a Storm Topology framework that builds a workflow to bind spouts (Kafka) to Bolts (HDFS). Here we are using a Local Storm cluster.</p>
<p>In the previous example pipeline, multiple separate classes for data streams parsing and transformations can be developed to handle Kafka producers and consumers.</p>
<p>The following is the complete Java code for <kbd>KafkaToplogy.java</kbd>:</p>
<pre><strong>package</strong> com.stormhdfs;</pre>
<pre><strong>import</strong> org.apache.storm.Config;<br/> <strong>import</strong> org.apache.storm.LocalCluster;<br/> <strong>import</strong> org.apache.storm.generated.AlreadyAliveException;<br/> <strong>import</strong> org.apache.storm.generated.InvalidTopologyException;<br/> <strong>import</strong> org.apache.storm.hdfs.bolt.HdfsBolt;<br/> <strong>import</strong> org.apache.storm.hdfs.bolt.format.DefaultFileNameFormat;<br/> <strong>import</strong> org.apache.storm.hdfs.bolt.format.DelimitedRecordFormat;<br/> <strong>import</strong> org.apache.storm.hdfs.bolt.format.RecordFormat;<br/> <strong>import</strong> org.apache.storm.hdfs.bolt.rotation.FileRotationPolicy;<br/> <strong>import</strong> org.apache.storm.hdfs.bolt.rotation.FileSizeRotationPolicy;<br/> <strong>import</strong> org.apache.storm.hdfs.bolt.sync.CountSyncPolicy;<br/> <strong>import</strong> org.apache.storm.hdfs.bolt.sync.SyncPolicy;<br/> <strong>import</strong> org.apache.storm.kafka.KafkaSpout;<br/> <strong>import</strong> org.apache.storm.kafka.SpoutConfig;<br/> <strong>import</strong> org.apache.storm.kafka.StringScheme;<br/> <strong>import</strong> org.apache.storm.kafka.ZkHosts;<br/> <strong>import</strong> org.apache.storm.spout.SchemeAsMultiScheme;<br/> <strong>import</strong> org.apache.storm.topology.TopologyBuilder;</pre>
<pre><strong>public class</strong> KafkaTopology {<br/> <strong>public static void</strong> main(String[] args) <strong>throws<br/></strong>AlreadyAliveException, InvalidTopologyException {</pre>
<pre>// zookeeper hosts for the Kafka cluster<em><br/></em>ZkHosts zkHosts = <strong>new</strong> ZkHosts(<strong>"localhost:2181"</strong>);</pre>
<pre>// Create the KafkaSpout configuartion<br/> // Second argument is the topic name<br/> // Third argument is the zookeeper root for Kafka<br/> // Fourth argument is consumer group id<br/>SpoutConfig kafkaConfig = <strong>new</strong> SpoutConfig(zkHosts,<br/> <strong>"data-pipleline-topic"</strong>, <strong>""</strong>, <strong>"id7"</strong>);</pre>
<pre>// Specify that the kafka messages are String<br/>kafkaConfig.<strong>scheme</strong> = <strong>new</strong> SchemeAsMultiScheme(<strong>new</strong> StringScheme());</pre>
<pre>// We want to consume all the first messages in the topic everytime<br/> // we run the topology to help in debugging. In production, this<br/> // property should be false<br/>kafkaConfig.<strong>startOffsetTime</strong> = kafka.api.OffsetRequest.<em>EarliestTime</em>();</pre>
<pre>RecordFormat format = <strong>new</strong> DelimitedRecordFormat().withFieldDelimiter(<strong>"|"</strong>);<br/> SyncPolicy syncPolicy = <strong>new</strong> CountSyncPolicy(1000);</pre>
<pre>FileRotationPolicy rotationPolicy = <strong>new</strong> FileSizeRotationPolicy(1.0f,FileSizeRotationPolicy.Units.<strong><em>MB</em></strong>);</pre>
<pre>DefaultFileNameFormat fileNameFormat = <strong>new</strong> DefaultFileNameFormat();<br/><br/>fileNameFormat.withPath(<strong>"/user/storm-data"</strong>);<br/><br/>fileNameFormat.withPrefix(<strong>"records-"</strong>);<br/><br/>fileNameFormat.withExtension(<strong>".txt"</strong>);<br/><br/>HdfsBolt bolt =<br/> <strong>new</strong> HdfsBolt().withFsUrl(<strong>"hdfs://127.0.0.1:8020"</strong>)<br/> .withFileNameFormat(fileNameFormat)<br/> .withRecordFormat(format)<br/> .withRotationPolicy(rotationPolicy)<br/> .withSyncPolicy(syncPolicy);<br/><br/>// Now we create the topology<br/>TopologyBuilder builder = <strong>new</strong> TopologyBuilder();<br/><br/>// set the kafka spout class<br/>builder.setSpout(<strong>"KafkaSpout"</strong>, <strong>new</strong> KafkaSpout(kafkaConfig), 1);<br/><br/>// configure the bolts<br/> // builder.setBolt("SentenceBolt", new SentenceBolt(), 1).globalGrouping("KafkaSpout");<br/> // builder.setBolt("PrinterBolt", new PrinterBolt(), 1).globalGrouping("SentenceBolt");<br/>builder.setBolt(<strong>"HDFS-Bolt"</strong>, bolt ).globalGrouping(<strong>"KafkaSpout"</strong>);<br/><br/>// create an instance of LocalCluster class for executing topology in local mode.<br/>LocalCluster cluster = <strong>new</strong> LocalCluster();<br/> Config conf = <strong>new</strong> Config();<br/><br/>// Submit topology for execution<br/>cluster.submitTopology(<strong>"KafkaTopology"</strong>, conf, builder.createTopology());<br/><br/><strong>try</strong> {<br/> // Wait for some time before exiting<br/>System.<strong>out</strong>.println(<strong>"Waiting to consume from kafka"</strong>);<br/> Thread.sleep(10000);<br/> } <strong>catch</strong> (Exception exception) {<br/> System.<strong>out</strong>.println(<strong>"Thread interrupted exception : "</strong> + exception);<br/> }<br/><br/>// kill the KafkaTopology<br/> //cluster.killTopology("KafkaTopology");<br/><br/>// shut down the storm test cluster<br/> // cluster.shutdown();<br/>}<br/> }</pre>
<p>The Hive table for the same is as follows:</p>
<pre>CREATE EXTERNAL TABLE IF NOT EXISTS customer (<br/>customer_id INT,<br/>customer_firstname String,<br/>customer_lastname String))<br/>COMMENT 'customer table'<br/>ROW FORMAT DELIMITED<br/>FIELDS TERMINATED BY '|'<br/>STORED AS TEXTFILE<br/>location '/user/storm-data';<br/>$ hive &gt; select * from customer;</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Other popular real-time data streaming frameworks</h1>
                </header>
            
            <article>
                
<p>Apart from Apache Storm, there are quite a few other open source real-time data streaming frameworks. In this section, I will discuss in brief only open source non-commercial frameworks. But, at the end of this section, I will provide a few URLs for a few commercial vendor products that offer some very interesting features.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka Streams API</h1>
                </header>
            
            <article>
                
<p>Kafka Streams is a library for building streaming applications. Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka Clusters. The Kafka Streams API transforms and enriches the data.</p>
<p>The following are the important features of the Kafka Streams API:</p>
<ul>
<li>It is part of the open source Apache Kafka project.</li>
<li>It supports per record streams processing with a very low latency (milliseconds). There is no micro- batching concept in the Kafka Streams API. Every record that comes into the stream is processed on its own.</li>
<li>It supports stateless processing (filtering and mapping), stateful processing (joins and aggregations), and windowing operations (for example, counting the last minute, last 5 minutes, last 30 minutes, or last day's worth of data, and so on).</li>
<li>To run the Kafka Streams API, there is no need to build a separate cluster that has multiple machines. Developers can use the Kafka Streams API in their Java applications or microservices to process real-time data.</li>
<li>The Kafka Streams API is highly scalable and fault-tolerant.</li>
<li>The Kafka Streams API is completely deployment agnostic. It can be deployed on a bare metal machine, VMs, Kubernetes containers, and on Cloud. There are no restrictions at all. Stream APIs are never deployed on Kafka Brokers. It is a separate application just like any other Java application, which is deployed outside of Kafka brokers.</li>
<li>It uses the Kafka security model.</li>
<li>It supports exactly-once semantics since version 0.11.0.</li>
</ul>
<p>Let's review the earlier image again to find out the exact place of the Kafka Streams API in the overall Kafka architecture.</p>
<p>Here are a few useful URLs to understand Kafka Streams in detail:</p>
<ul>
<li><a href="https://kafka.apache.org/documentation/">https://kafka.apache.org/documentation/</a></li>
<li><a href="https://www.confluent.io/blog/">https://www.confluent.io/blog/</a></li>
<li><a href="https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/">https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/</a></li>
<li><a href="https://docs.confluent.io/current/streams/index.html">https://docs.confluent.io/current/streams/index.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Spark Streaming</h1>
                </header>
            
            <article>
                
<p>Please note that we will discuss Spark in <a href="39cf9925-e4cc-473a-9032-a21b81e7b400.xhtml" target="_blank">Chapter 7</a>, <em>Large-Scale Data Processing Frameworks,</em> which is fully dedicated to Spark. However, in this section, I will discuss some important features of Spark Streaming. For better understanding, readers are advised to study <a href="39cf9925-e4cc-473a-9032-a21b81e7b400.xhtml" target="_blank">Chapter 7</a><span>, </span><em>Large-Scale Data Processing Frameworks</em> first and come back to read this section further to understand more about Spark Streaming.</p>
<p>It is a general practice to use Hadoop MapReduce for batch processing and Apache Storm for real-time stream processing.</p>
<p>The use of these two different programming models causes an increase in code size, number of bugs to fix, and development effort; it also introduces a learning curve and causes other issues. Spark Streaming helps fix these issues and provides a scalable, efficient, resilient, and integrated (with batch processing) system.</p>
<p>The strength of Spark Streaming lies in its ability to combine with batch processing. It's possible to create a RDD using normal Spark programming and join it with a Spark stream. Moreover, the code base is similar and allows easy migration if required—and there is zero to no learning curve from Spark.</p>
<p>Spark Streaming is an extension of the core Spark API. It extends Spark for doing real-time stream processing. Spark Streaming has the following features:</p>
<ul>
<li>It's scalable—it scales on hundreds of nodes</li>
<li>It provides high-throughput and achieves second level latency</li>
<li>It's fault-tolerant and it efficiently receives from the failures</li>
<li>It integrates with batch and interactive data processing</li>
</ul>
<p>Spark Streaming processes data streams application as a series of very small, deterministic batch jobs.</p>
<p>Spark Streaming provides an API in Scala, Java, and Python. Spark Streaming divides live stream of data into multiple batches based on time. The time can range from one second to a few minutes/hours. In general, batches are divided into a few seconds. Spark treats each batch as a RDD and process each based on RDD operations (map, filter, join flatmap, distinct, reduceByKey, and so on). Lastly, the processed results of RDDs are returned in batches.</p>
<p>The following image depicts the Spark Streaming data flow:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5a221209-8e20-480b-8121-1d6c49e99f61.png"/></div>
<p>Here are few useful URLs for understanding Spark Streaming in detail:</p>
<ul>
<li><a href="https://databricks.com/blog">https://databricks.com/blog</a></li>
<li><a href="https://databricks.com/blog/category/engineering/streaming">https://databricks.com/blog/category/engineering/streaming</a></li>
<li><a href="https://spark.apache.org/streaming/">https://spark.apache.org/streaming/</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache Flink</h1>
                </header>
            
            <article>
                
<p class="mce-root">Apache Flink's documentation describes Flink in the following way: Flink is an open-source framework for distributed stream processing.</p>
<p>Flink provides accurate results and supports out-of-order or late-arriving datasets. It is stateful and fault-tolerant and can seamlessly recover from failures while maintaining an exactly-once application state. It performs at a large scale, running on thousands of nodes with very good throughput and latency characteristics.</p>
<p>The following are the features of Apache Flink:</p>
<ul>
<li>Flink guarantees exactly-once semantics for stateful computations</li>
<li>Flink supports stream processing and windowing with event time semantics</li>
<li>Flink supports flexible windowing based on time, count, or sessions, in addition to data-driven windows</li>
<li>Flink is capable of high throughput and low latency</li>
<li>Flink's savepoints provide a state versioning mechanism, making it possible to update applications or reprocess historic data with no lost state and minimal downtime</li>
<li>Flink is designed to run on large-scale clusters with many thousands of nodes, and, in addition to a standalone cluster mode, Flink provides support for YARN and Mesos</li>
</ul>
<p>Flink's core is a distributed streaming dataflow engine. It supports processing one stream at a time rather than processing an entire batch of streams at a time.</p>
<p>Flink supports the following libraries:</p>
<ul>
<li>CEP</li>
<li>Machine learning</li>
<li>Graph processing</li>
<li>Apache Storm compatibility</li>
</ul>
<p>Flink supports the following APIs:</p>
<ul>
<li><strong>DataStream API</strong>: This API helps all the streams, transformations, that is, filtering, aggregations, counting, and windowing</li>
<li><strong>DataSet API</strong>: This API helps all the batch data transformations, that is, join, group, map, and filter</li>
<li><strong>Table API</strong>: Supports SQL over relational data streams</li>
<li><strong>Streaming SQL</strong>: Supports SQL over batch and streaming tables</li>
</ul>
<p>The following image describes the Flink programming model:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d7ae2f42-7921-40b3-a961-a6a3e13bd11c.png" style="width:33.67em;height:19.92em;"/></div>
<p>The following image describes the Flink architecture:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4f2dfbe0-b9ed-4693-8908-d73d07e4c0f9.png"/></div>
<p>The following are the components of the Flink programming model:</p>
<ul>
<li><strong>Source</strong>: A data source where data is collected and sent to the Flink engine</li>
<li><strong>Transformation</strong>: In this component the whole transformation takes place</li>
<li><strong>Sink</strong>: A target where processed streams are sent</li>
</ul>
<p>Here are a few useful URLs to understand Spark Streaming in detail:</p>
<ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/">https://ci.apache.org/projects/flink/flink-docs-release-1.4/</a></li>
<li><a href="https://www.youtube.com/watch?v=ACS6OM1-xgE&amp;feature=youtu.be">https://www.youtube.com/watch?v=ACS6OM1-xgE&amp;amp;amp;feature=youtu.be</a></li>
</ul>
<p>In the following sections, we will take a look at a comparison of various stream frameworks. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache Flink versus Spark</h1>
                </header>
            
            <article>
                
<p>The main focus of Spark Streaming is stream-batching operation, called <strong>micro-batching</strong>. This programming model suits many use cases, but not all use cases require real-time stream processing with sub-second latency. For example, a use case such as credit card fraud prevention requires millisecond latency. Hence, the micro-batching programming model is not suited there. (But, the latest version of Spark, 2.4, supports millisecond data latency).</p>
<p>Apache Flink supports millisecond latency and is suited for use cases such as fraud detection and like.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache Spark versus Storm</h1>
                </header>
            
            <article>
                
<p>Spark uses micro-batches to process events while Storm processes events one by one. It means that Spark has a latency of seconds while Storm provides a millisecond of latency. Spark Streaming provides a high-level abstraction called a <strong>Discretized Stream</strong> or <strong>DStream</strong>, which represents a continuous sequence of RDDs. (But, the latest version of Spark, 2.4 supports millisecond data latency.) The latest Spark version supports DataFrames.</p>
<p>Almost the same code (API) can be used for Spark Streaming and Spark batch jobs. That helps to reuse most of the code base for both programming models. Also, Spark supports Machine learning and the Graph API. So, again, the same codebase can be used for those use cases as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we started with a detailed understanding of real-time stream processing concepts, including data stream, batch vs. real-time processing, CEP, low latency, continuous availability, horizontal scalability, storage, and so on. Later, we learned about Apache Kafka, which is a very important component of modern real-time stream data pipelines. The main features of Kafka are scalability, durability, reliability, and high throughput.</p>
<p>We also learned about Kafka Connect; its architecture, data flow, sources, and connectors. We studied case studies to design a data pipeline with Kafka Connect using file source, file Sink, JDBC source, and file Sink Connectors.</p>
<p>In the later sections, we learned about various open source real-time stream-processing frameworks, such as the Apache Storm framework. We have seen a few practical examples, as well. Apache Storm is distributed and supports low-latency and multiple programming languages. Storm is fault-tolerant and reliable. It supports at least once or exactly-once processing.</p>
<p>Spark Streaming helps to fix these issues and provides a scalable, efficient, resilient, and integrated (with batch processing) system. The strength of Spark Streaming lies in its ability to combine with batch processing. Spark Streaming is scalable, and provides high-throughput. It supports micro-batching for second level latency, is fault-tolerant, and integrates with batch and interactive data processing.</p>
<p>Apache Flink guarantees exactly-once semantics, supports event time semantics, high throughput, and low latency. It is designed to run on large-scale clusters.</p>


            </article>

            
        </section>
    </body></html>