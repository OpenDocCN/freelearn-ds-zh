- en: Chapter 6.  Spark Stream Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章  Spark流处理
- en: Data processing use cases can be mainly divided into two types. The first type
    is the use cases where the data is static and processing is done in its entirety
    as one unit of work, or by dividing it into smaller batches. While doing the data
    processing, the underlying data set does not change nor do new data sets get added
    to the processing units. This is batch processing.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理用例可以主要分为两种类型。第一种类型是数据静态，处理作为一个工作单元整体完成，或者将其分成更小的批次。在数据处理过程中，基础数据集不会改变，也不会有新的数据集添加到处理单元中。这是批处理。
- en: The second type is the use cases where the data is getting generated like a
    stream, and the processing is done as and when the data is generated. This is
    stream processing. In the previous chapters of this book, all the data processing
    use cases were pertaining to the former type. This chapter is going to focus on
    the latter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种类型是数据像流一样生成，并且数据处理是在数据生成时进行的。这是流处理。在这本书的前几章中，所有的数据处理用例都属于前一种类型。本章将重点关注后一种。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Data stream processing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据流处理
- en: Micro batch data processing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微批数据处理
- en: A log event processor
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志事件处理器
- en: Windowed data processing and other options
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 窗口数据处理和其他选项
- en: Kafka stream processing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka流处理
- en: Streaming jobs with Spark
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark的流式作业
- en: Data stream processing
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据流处理
- en: Data sources generate data like a stream, and many real-world use cases require
    them to be processed in real time. The meaning of *real time* can change from
    use case to use case. The main parameter that defines what is meant by real time
    for a given use case is how soon the ingested data or the frequent interval in
    which all the data ingested since the last interval needs to be processed. For
    example, when a major sports event is happening, the application that consumes
    the score events and sends them to the subscribed users should be processing the
    data as fast as it can. The faster can be sent, the better it is.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据源生成数据就像流一样，许多现实世界的用例需要它们实时处理。*实时*的含义可能因用例而异。定义特定用例中*实时*含义的主要参数是，摄入的数据或自上次间隔以来所有摄入数据的频繁间隔需要多快被处理。例如，当重大体育赛事正在进行时，消耗比分事件并将它们发送给订阅用户的程序应该尽可能快地处理数据。发送得越快，越好。
- en: But what is the definition of *fast* here? Is it fine to process the score data
    within, say, an hour of the score event happening? Probably not. Is it fine to
    process the data within a minute of the score event happening? It is definitely
    better than processing after an hour. Is it fine to process the data within a
    second of the score event happening? Probably yes, and much better than the earlier
    data processing time intervals.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但这里的*快*是什么意思？在比分事件发生后，比如说一个小时之内处理比分数据可以吗？可能不行。在比分事件发生后一分钟内处理数据可以吗？这肯定比一个小时后处理要好。在比分事件发生后一秒内处理数据可以吗？可能可以，并且比之前的数据处理时间间隔要好得多。
- en: In any data stream processing use cases, this time interval is very important.
    The data processing framework should have the capability to process the data stream
    in an appropriate time interval of choice to deliver good business value.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何数据流处理用例中，这个时间间隔非常重要。数据处理框架应该具备在选择的适当时间间隔内处理数据流的能力，以提供良好的商业价值。
- en: When processing stream data in regular intervals of choice, the data is collected
    from the beginning of the time interval to the end of the time interval, grouped
    in a micro batch, and data processing is done on that batch of data. Over an extended
    period of time, the data processing application would have processed many such
    micro batches of data. In this type of processing, the data processing application
    will have visibility of only the specific micro batch that is getting processed
    at a given point in time. In other words, the application will not have any visibility
    or access to the already processed micro batches of data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当在选择的常规时间间隔内处理流数据时，数据是从时间间隔的开始收集到结束，分组在一个微批中，然后对这个数据批次进行处理。在较长一段时间内，数据处理应用程序会处理许多这样的微批数据。在这种处理类型中，数据处理应用程序只能看到在特定时间点正在处理的具体微批数据。换句话说，应用程序将没有任何可见性或访问权来查看已经处理过的微批数据。
- en: Now, there is another dimension to this type of processing. Suppose a given
    use case mandates the need to process the data every minute, but at the same time,
    while processing the data of a given micro batch, there is a need to peek into
    the data that was already processed in the last 15 minutes. A fraud detection
    module of a retail banking transaction processing application is a good example
    of this particular business requirement. There is no doubt that the retail banking
    transactions are to be processed within milliseconds of their occurrence. When
    processing an ATM cash withdrawal transaction, it is a good idea to see whether
    somebody is trying to continuously withdraw cash and, if found, send the proper
    alert. For this, when processing a given cash withdrawal transaction, the application
    checks whether there are any other cash withdrawals from the same ATM using the
    same card that was used in the last 15 minutes. The business rule is to send an
    alert when such transactions are more than two in the last 15 minutes. In this
    use case, the fraud detection application should have visibility of all the transactions
    that happened in a window of 15 minutes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这种处理类型又增加了一个维度。假设一个特定的用例要求每分钟处理数据，但同时，在处理给定微批量的数据时，还需要查看在最后 15 分钟内已经处理过的数据。零售银行交易处理应用程序的欺诈检测模块是满足这种特定业务需求的良好例子。毫无疑问，零售银行交易应在发生后的毫秒内进行处理。在处理
    ATM 现金取款交易时，查看是否有人试图连续取款是一个好主意，如果发现这种情况，应发送适当的警报。为此，在处理特定的现金取款交易时，应用程序会检查在最后 15
    分钟内是否还有使用相同卡片从同一 ATM 取款的现金取款。业务规则是在最后 15 分钟内有超过两次此类交易时发送警报。在这种情况下，欺诈检测应用程序应该能够看到在
    15 分钟窗口内发生的所有交易。
- en: A good stream data processing framework should have the ability to processing
    the data in any given interval of time, as well as the ability to peek into the
    data ingested within a sliding window of time. The Spark Streaming library that
    is working on top of Spark is one of the best data stream processing frameworks
    that has both of these capabilities.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的流数据处理框架应该能够处理任何给定时间间隔内的数据，以及能够查看在滑动时间窗口内摄取的数据。在 Spark 上工作的 Spark Streaming
    库是具有这两种能力的最佳数据流处理框架之一。
- en: Look again at the bigger picture of the Spark library stack as given in *Figure
    1* to set the context and see what is being discussed here before getting into
    and taking up the use cases.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 再次查看 *图1* 中给出的 Spark 库堆栈的更大图景，以设置上下文并了解在这里讨论的内容，然后再进入并处理用例。
- en: '![Data stream processing](img/image_06_001.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![数据流处理](img/image_06_001.jpg)'
- en: Figure 1
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1
- en: Micro batch data processing
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微批数据处理
- en: Every Spark Streaming data processing application will be running continuously
    till it is terminated. This application will be constantly *listening* to the
    data source to receive the incoming stream of data. The Spark Streaming data processing
    application would have a configured batch interval. At the end of every batch
    interval, it will produce a data abstraction named **Discretized Stream** (**DStream**)
    which works very similar to Spark's RDD. Just like RDD, a DStream supports an
    equivalents method for the commonly used Spark transformations and Spark actions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Spark Streaming数据处理应用程序将持续运行，直到被终止。此应用程序将不断 *监听* 数据源以接收传入的数据流。Spark Streaming
    数据处理应用程序将有一个配置的批处理间隔。在每一个批处理间隔结束时，它将产生一个名为 **离散流**（**DStream**）的数据抽象，其工作方式与 Spark
    的 RDD 非常相似。就像 RDD 一样，DStream 支持用于常用 Spark 转换和 Spark 操作的等效方法。
- en: Tip
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Just like RDD, a DStream is also immutable and distributed.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 RDD 一样，DStream 也是不可变和分布式的。
- en: '*Figure 2* shows how DStreams are being produced in a Spark Streaming data
    processing application.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2* 展示了在 Spark Streaming 数据处理应用程序中 DStream 的生成方式。'
- en: '![Micro batch data processing](img/image_06_004.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![微批数据处理](img/image_06_004.jpg)'
- en: Figure 2
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图2
- en: '*Figure 2* depicts the most important elements of a Spark Streaming application.
    For the configured batch interval, the application produces one DStream. Each
    DStream is a collection of RDDs consisting of the data collected within that batch
    interval. The number of RDDs within a DStream for a given batch interval will
    vary.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2* 展示了 Spark Streaming 应用程序最重要的元素。对于配置的批处理间隔，应用程序会产生一个 DStream。每个 DStream
    是由在该批处理间隔内收集的数据组成的 RDD 集合。对于给定的批处理间隔，DStream 中 RDD 的数量可能会有所不同。'
- en: Tip
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Since Spark Streaming applications are continuously running applications collecting
    data, in this chapter, rather than running the code in REPL, the complete application
    is discussed, including the instructions to compile, package and run.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Spark Streaming应用程序是持续运行并收集数据的应用程序，在本章中，我们讨论了完整的应用程序，包括编译、打包和运行的指令，而不是在REPL中运行代码。
- en: The Spark programming model was discussed in [Chapter 2](ch02.html "Chapter 2. Spark
    Programming Model"), *Spark Programming Model*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Spark编程模型在[第2章](ch02.html "第2章。Spark编程模型")中进行了讨论，*Spark编程模型*。
- en: Programming with DStreams
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用DStreams进行编程
- en: Programming with DStreams in a Spark Streaming data processing application also
    follows a very similar model, as DStreams consist of one or more RDDs. When methods
    such as Spark transformations or Spark actions are invoked on a DStream, the equivalent
    operation is applied to all the RDDs that constitute the DStream.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark Streaming数据流处理应用程序中使用DStreams进行编程也遵循一个非常类似的模型，因为DStreams由一个或多个RDD组成。当在DStream上调用Spark转换或Spark操作时，等效操作会被应用到构成DStream的所有RDD上。
- en: Note
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: An important point to note here is that not all the Spark transformations and
    Spark actions that work on RDD are unsupported on DStreams. The other notable
    change is the differences in capability across programming languages.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的一个重要点是，并非所有在RDD上工作的Spark转换和Spark操作都不支持在DStreams上使用。另一个值得注意的变化是不同编程语言之间能力的差异。
- en: The Scala and Java APIs for Spark Streaming are ahead of the Python API in terms
    of the number of features supported for Spark Streaming data processing application
    development.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming的Scala和Java API在支持Spark Streaming数据流处理应用程序开发的功能数量上优于Python API。
- en: '*Figure 3* depicts how methods applied on a DStream are applied on the underlying
    RDDs. The Spark Streaming programming guide is to be consulted before using any
    of the methods on DStreams. The Spark Streaming programming guide is marked with
    special callouts containing the text *Python API* wherever the Python API deviates
    from its Scala or Java counterparts.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3*描述了应用于DStream的方法是如何应用于底层的RDDs的。在使用DStream上的任何方法之前，应查阅Spark Streaming编程指南。当Python
    API与其Scala或Java对应版本不同时，Spark Streaming编程指南会带有包含文本*Python API*的特殊提示。'
- en: 'Assume that, for a given batch interval in a Spark Streaming data processing
    application, a DStream is generated consisting of multiple RDDs. When a filter
    method is applied on that DStream, here is how it gets translated into the underlying
    RDDs. *Figure 3* shows a filter transformation applied on a DStream with two RDDs,
    resulting in another DStream containing only one RDD because of the filter condition:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在一个Spark Streaming数据流处理应用程序中，给定一个批次间隔，会生成一个包含多个RDD的DStream。当对这个DStream应用过滤方法时，它会被转换成底层的RDDs。*图3*展示了在一个包含两个RDD的DStream上应用过滤转换，由于过滤条件，结果生成另一个只包含一个RDD的DStream：
- en: '![Programming with DStreams](img/image_06_003.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![使用DStreams进行编程](img/image_06_003.jpg)'
- en: Figure 3
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图3
- en: A log event processor
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志事件处理器
- en: These days, it is very common to have a central repository of application log
    events in many enterprises. Also, the log events are streamed live to data processing
    applications in order to monitor the performance of the running applications on
    a real-time basis so that timely remediation measures can be taken. Such a use
    case is discussed here to demonstrate the real-time processing of log events using
    a Spark Streaming data processing application. In this use case, the live application
    log events are written to a TCP socket. The Spark Streaming data processing application
    constantly listens to a given port on a given host to collect the stream of log
    events.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些天，在许多企业中，拥有一个中央应用程序日志事件存储库是非常常见的。此外，日志事件会实时流式传输到数据处理应用程序，以便实时监控运行应用程序的性能，以便及时采取补救措施。这里讨论了这样一个用例，以展示使用Spark
    Streaming数据流处理应用程序实时处理日志事件。在这个用例中，实时应用程序日志事件被写入TCP套接字。Spark Streaming数据流处理应用程序持续监听给定主机上的指定端口，以收集日志事件流。
- en: Getting ready with the Netcat server
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备Netcat服务器
- en: 'The Netcat utility that comes with most UNIX installations is used here as
    the data server. To make sure that Netcat is installed in the system, type the
    manual command as given in the following scripts, and, after coming out of it,
    run it and make sure that there is no error message. Once the server is up and
    running, whatever is typed in the standard input of the Netcat server console
    is considered as the application logs events for simplicity and demonstration
    purposes. The following commands run from a terminal prompt will start the Netcat
    data server on localhost port `9999`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数UNIX安装中附带使用的Netcat实用程序在此用作数据服务器。为了确保Netcat已安装在系统中，请输入以下脚本中给出的手册命令，并在退出后运行它，确保没有错误信息。一旦服务器启动并运行，标准输入的Netcat服务器控制台中输入的内容将被视为应用程序日志事件，以简化演示。以下从终端提示符运行的命令将在本地主机端口`9999`上启动Netcat数据服务器：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once the preceding steps are completed, the Netcat server is ready and the Spark
    Streaming data processing application will process all the lines that are typed
    in the previous console window. Leave this console window alone; all the following
    shell commands will be run in a different terminal window.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 完成前面的步骤后，Netcat服务器就绪，Spark Streaming数据处理应用程序将处理之前控制台窗口中输入的所有行。不要操作这个控制台窗口；所有后续的shell命令将在不同的终端窗口中运行。
- en: Since there is a lack of parity of Spark Streaming features between different
    programming languages, the Scala code is used to explain all the Spark Streaming
    concepts and use cases. After that, the Python code is given, and, if there is
    a lack of support for any of the features being discussed in Python, that is also
    captured.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于不同编程语言之间Spark Streaming功能的对等性不足，因此使用Scala代码来解释所有Spark Streaming概念和用例。之后，将给出Python代码，如果Python中不支持正在讨论的任何功能，也会捕获。
- en: The Scala and Python code are organized in the way demonstrated in *Figure 4*.
    For the compilation, packaging and running of the code, bash scripts are used
    so that it is easy for the readers to run them to produce consistent results.
    Each of these script file contents are discussed here.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Scala和Python代码的组织方式如*图4*所示。为了编译、打包和运行代码，使用了bash脚本，以便读者可以轻松运行它们以产生一致的结果。这里讨论了每个脚本文件的内容。
- en: Organizing files
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文件组织
- en: 'In the following folder tree, the `project`and `target`folders are created
    at runtime. The source code that comes with this book can be copied directly to
    a convenient folder in the system:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下文件夹结构中，`project`和`target`文件夹是在运行时创建的。本书附带源代码可以直接复制到系统中的方便文件夹中：
- en: '![Organizing files](img/image_06_007.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![文件组织](img/image_06_007.jpg)'
- en: Figure 4
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图4
- en: 'For compiling and packaging, the **Scala build tool** (**sbt**) is used. In
    order to make sure that sbt is working properly, run the following commands from
    the `Scala` folder of the tree in *Figure 4* in the terminal window. This is to
    make sure that sbt is working fine and the code is compiling:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了编译和打包，使用了**Scala构建工具**（**sbt**）。为了确保sbt正常工作，请在终端窗口中从*图4*中的树形结构的`Scala`文件夹运行以下命令。这是为了确保sbt运行良好且代码正在编译：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The following table captures the representative sample list of files and the
    purpose of each of them in the context of the Spark Streaming data processing
    application that is being discussed here.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格捕捉了正在讨论的Spark Streaming数据处理应用程序中代表性文件样本列表以及每个文件的目的。
- en: '| **File Name** | **Purpose** |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| **文件名** | **用途** |'
- en: '| `README.txt` | Instructions to run the application. One for the Scala application
    and the other one for the Python application. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| `README.txt` | 运行应用程序的说明。一个用于Scala应用程序，另一个用于Python应用程序。|'
- en: '| `submitPy.sh` | Bash script to submit the Python job to the Spark cluster.
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| `submitPy.sh` | 用于将Python作业提交到Spark集群的Bash脚本。|'
- en: '| `compile.sh` | Bash script to compile the Scala code. |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| `compile.sh` | 用于编译Scala代码的Bash脚本。|'
- en: '| `submit.sh` | Bash script to submit the Scala job to the Spark cluster. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| `submit.sh` | 用于将Scala作业提交到Spark集群的Bash脚本。|'
- en: '| `config.sbt` | The sbt configuration file. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `config.sbt` | sbt配置文件。|'
- en: '| `*.scala` | Spark Streaming data processing application code in Scala. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `*.scala` | 使用Scala编写的Spark Streaming数据处理应用程序代码。|'
- en: '| `*.py` | Spark Streaming data processing application code in Python. |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `*.py` | 使用Python编写的Spark Streaming数据处理应用程序代码。|'
- en: '| `*.jar` | The Spark Streaming and Kafka integration JAR file that needs to
    be downloaded and placed under the `lib` folder for the proper functioning of
    the applications. This is being used in `submit.sh` as well as in `submitPy.sh`
    for submitting the job to the cluster. |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `*.jar` | 需要下载并放置在`lib`文件夹下以使应用程序正常运行的Spark Streaming和Kafka集成JAR文件。这个文件在`submit.sh`以及`submitPy.sh`中也被使用，用于将作业提交到集群。|'
- en: Submitting the jobs to the Spark cluster
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将作业提交到Spark集群
- en: 'To properly run the application, some of the configurations depend on the system
    in which it is being run. They are to be edited in the `submit.sh` file and the
    `submitPy.sh` file. Wherever such edits are required, comments are given with
    the `[FILLUP]` tag. The most important of these are the setting of the Spark installation
    directory and the Spark master configuration, which can differ from system to
    system. The source of the preceding script `submit.sh` file is given as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确运行应用程序，一些配置取决于运行它的系统。它们需要在`submit.sh`文件和`submitPy.sh`文件中进行编辑。无论何时需要此类编辑，都会使用`[FILLUP]`标签给出注释。其中最重要的是Spark安装目录和Spark主配置的设置，这些可能因系统而异。前面`submit.sh`文件的源代码如下：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The source of the preceding script file `submitPy.sh` is given as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 前面脚本文件`submitPy.sh`的源代码如下：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Monitoring running applications
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控运行中的应用程序
- en: As described in [Chapter 2](ch02.html "Chapter 2. Spark Programming Model"),
    *Spark Programming Model*, Spark installation comes with a powerful Spark web
    UI for monitoring the Spark applications that are running.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第2章](ch02.html "第2章。Spark编程模型")中所述，*Spark编程模型*，Spark安装附带了一个强大的Spark Web UI，用于监控正在运行的Spark应用程序。
- en: There are additional visualizations available specifically for the Spark Streaming
    jobs that are running.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于正在运行的Spark Streaming作业，还有额外的可视化可用。
- en: 'The following scripts start the Spark master and workers, and enable monitoring.
    The assumption here is that the reader has made all the configuration changes
    suggested in [Chapter 2](ch02.html "Chapter 2. Spark Programming Model"), *Spark
    Programming Model* to enable Spark application monitoring. If that is not done,
    the applications can still be run. The only change to be made is to put the cases
    in the `submit.sh` file and the `submitPy.sh` file to make sure that instead of
    the Spark master URL, something like `local[4]` is used. Run the following commands
    on the terminal window:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本启动Spark主节点和工作者节点，并启用监控。这里的假设是读者已经按照[第2章](ch02.html "第2章。Spark编程模型")中建议的配置更改进行了所有配置，以启用Spark应用程序监控。如果没有这样做，应用程序仍然可以运行。唯一需要更改的是，在`submit.sh`文件和`submitPy.sh`文件中将情况放入，以确保使用`local[4]`之类的而不是Spark主节点URL。在终端窗口中运行以下命令：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Make sure that the Spark web UI is up and running by visiting `http://localhost:8080/`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 确保Spark Web UI正在运行，可以通过访问`http://localhost:8080/`来检查。
- en: Implementing the application in Scala
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Scala实现应用程序
- en: 'The following code snippet is the Scala code for the log event processing application:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段是日志事件处理应用程序的Scala代码：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In the previous code snippet, there are two Scala objects. One is for setting
    the proper logging levels, to make sure that unwanted messages are not displayed
    on the console. The `StreamingApps` Scala object holds the logic of the stream
    processing. The following list captures the essence of the functionality:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，有两个Scala对象。一个是用于设置适当的日志级别，以确保不显示不想要的消息。`StreamingApps` Scala对象包含流处理的逻辑。以下列表捕捉了功能的核心：
- en: A Spark configuration is created with the application name.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用应用程序名称创建一个Spark配置。
- en: A Spark `StreamingContext` object is created, which is the heart of the stream
    processing. The second parameter of the `StreamingContext` constructor is the
    batch interval, which is 10 seconds. The line containing `ssc.socketTextStream`
    creates DStreams at every batch interval, which is 10 seconds here, containing
    the lines typed in the Netcat console.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建了一个Spark `StreamingContext`对象，这是流处理的核心。`StreamingContext`构造函数的第二个参数是批处理间隔，这里是10秒。包含`ssc.socketTextStream`的行在每一个批处理间隔（这里为10秒）创建DStream，包含在Netcat控制台中输入的行。
- en: A filter transformation is applied next on the DStream, to have only the lines
    containing the word `ERROR`. The filter transformation creates new DStreams containing
    only the filtered lines.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在DStream上应用了一个过滤器转换，以只包含包含单词`ERROR`的行。过滤器转换创建新的DStream，其中只包含过滤后的行。
- en: The next line prints the DStream contents to the console. In other words, for
    every batch interval, if there are lines containing the word `ERROR`, that get
    displayed in the console.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一行将 DStream 内容打印到控制台。换句话说，对于每个批次间隔，如果有包含单词 `ERROR` 的行，这些行将在控制台中显示。
- en: At the end of this data processing logic, the given `StreamingContext` is started
    and will run until it is terminated.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据处理逻辑的末尾，启动了给定的 `StreamingContext` 并将一直运行，直到被终止。
- en: In the previous code snippet, there is no loop construct telling the application
    to repeat till the running application is terminated. This is achieved by the
    Spark Streaming library itself. From the beginning till the termination of the
    data processing application, all the statements are run once. All the operations
    on the DStreams are repeated (internally) for every batch. If the output of the
    previous application is closely examined, the output from the println() statements
    are seen only once in the console, even though these statements are between the
    initialization and termination of the `StreamingContext`. That is because the
    *magic loop* is repeating only for the statements containing original and derived
    DStreams.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码片段中，没有循环结构告诉应用程序重复执行直到运行中的应用程序终止。这是由 Spark Streaming 库本身实现的。从开始到数据处理应用程序的终止，所有语句都只运行一次。DStreams
    上的所有操作（内部）都会为每个批次重复执行。如果仔细检查上一个应用程序的输出，println() 语句的输出在控制台中只出现一次，尽管这些语句位于 `StreamingContext`
    的初始化和终止之间。这是因为 *魔法循环* 只会重复包含原始和派生 DStreams 的语句。
- en: Because of the special nature of the looping implemented in Spark Streaming
    applications, it is futile to give print statements and log statements within
    the streaming logic in the application code, like the one that is given in the
    code snippet. If that is a must, then these logging statements are to be instrumented
    within the functions that are passed to DStreams for the transformations and actions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Spark Streaming 应用程序中实现的循环的特殊性质，在应用程序代码中的流逻辑内给出打印语句和日志语句是徒劳的，就像代码片段中给出的那样。如果这是必须的，那么这些日志语句必须在传递给
    DStreams 的转换和操作的功能中进行配置。
- en: Tip
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: If persistence is required for the processed data, there are many output operations
    available for DStreams, just as there are for RDDs.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要处理数据的持久性，DStreams 提供了许多输出操作，就像 RDDs 一样。
- en: Compiling and running the application
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编译和运行应用程序
- en: The following commands are run on the terminal window to compile and run the
    application. Instead of using `./compile.sh`, a simple sbt compile command can
    also be used.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令在终端窗口中运行以编译和运行应用程序。除了使用 `./compile.sh` 之外，还可以使用简单的 sbt compile 命令。
- en: Note
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that, as discussed previously, the Netcat server must be running before
    these commands are executed.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，正如之前讨论的，在执行这些命令之前，Netcat 服务器必须正在运行。
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If no error messages are shown, and the results are showing in line with the
    previous output, the Spark Streaming data processing application has started properly.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有显示错误消息，并且结果显示与之前的输出一致，则 Spark Streaming 数据处理应用程序已正确启动。
- en: Handling the output
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理输出
- en: Note that the output of the print statements comes before the DStream output
    print. So far, nothing has been typed in the Netcat console and, therefore, there
    is nothing to process.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，打印语句的输出在 DStream 输出打印之前。到目前为止，Netcat 控制台中还没有输入任何内容，因此没有内容可以处理。
- en: 'Now go to the Netcat console that was started earlier and enter the following
    lines of log event messages by giving a gap of few seconds to make sure that the
    output goes to more than one batch, where the batch size is 10 seconds:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在转到之前启动的 Netcat 控制台，并输入以下行日志事件消息，在每行之间留出几秒钟的间隔以确保输出超过一个批次，其中批次大小为 10 秒：
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Once the log event messages are entered into the Netcat console window, the
    following results will start showing up in the Spark Streaming data processing
    application, filtering only the log event messages containing the keyword ERROR.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦将日志事件消息输入到 Netcat 控制台窗口中，以下结果将开始在 Spark Streaming 数据处理应用程序中显示，仅过滤包含关键字 ERROR
    的日志事件消息。
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The Spark web UI (`http://localhost:8080/`) was already enabled, and Figures
    5 and 6 show the Spark applications and statistics.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 网页 UI (`http://localhost:8080/`) 已经启用，图 5 和 6 显示了 Spark 应用程序和统计信息。
- en: From the main page (after visiting the URL `http://localhost:8080/`), click
    the running Spark Streaming data processing application's name link to bring up
    the regular monitoring page. From that page, click the **Streaming** tab, to reveal
    the page containing the streaming statistics.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从主页（访问URL `http://localhost:8080/`）开始，点击运行中的Spark Streaming数据处理应用程序的名称链接，以打开常规监控页面。从该页面，点击**Streaming**标签，以显示包含流统计信息的页面。
- en: 'The link and tab to be clicked are circled in red:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 需要点击的链接和标签页用红色圆圈标注：
- en: '![Handling the output](img/image_06_008.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![处理输出](img/image_06_008.jpg)'
- en: Figure 5
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图5
- en: 'From the page shown in *Figure 5*, click on the circled application link; it
    will take you to the relevant page. From that page, once the **Streaming** tab
    is clicked, the page containing the streaming statistics will show up as captured
    in *Figure 6*:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图5*所示的页面，点击圆圈中的应用程序链接；它将带您到相关页面。从该页面，一旦点击**Streaming**标签，包含流统计信息的页面将显示，如*图6*所示：
- en: '![Handling the output](img/image_06_009.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![处理输出](img/image_06_009.jpg)'
- en: Figure 6
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图6
- en: There are a whole lot of application statistics available from these Spark web
    UI pages, and exploring them extensively is a good idea to gain a deeper understanding
    of the behavior of the Spark Streaming data processing applications submitted.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些Spark web UI页面中可以获取大量应用程序统计信息，广泛探索它们是一个好主意，以更深入地了解提交的Spark Streaming数据处理应用程序的行为。
- en: Tip
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Care must be taken while enabling the monitoring of streaming applications as
    it should not affect the performance of the application itself.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在启用流应用程序监控时必须小心，因为它不应影响应用程序本身的性能。
- en: Implementing the application in Python
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Python中实现应用程序
- en: 'The same use case is implemented in Python, and the following code snippet
    saved in `StreamingApps.py`is used to do this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的用例在Python中实现，并在`StreamingApps.py`中保存以下代码片段来完成此操作：
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The following commands are run on the terminal window to run the Python Spark
    Streaming data processing application from the directory where the code is downloaded.
    Before running the application, in the same way that the modifications are made
    to the script that is used to run the Scala application, the `submitPy.sh` file
    also has to be changed to point to the right Spark installation directory and
    configure the Spark master. If monitoring is enabled, and if the submission is
    pointing to the right Spark master, the same Spark web UI will capture the statistics
    of the Python Spark Streaming data processing applications as well.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令在终端窗口中运行以从代码下载的目录中运行Python Spark Streaming数据处理应用程序。在运行应用程序之前，与用于运行Scala应用程序的脚本所做的修改相同，`submitPy.sh`文件也必须更改以指向正确的Spark安装目录并配置Spark
    master。如果启用了监控，并且提交指向正确的Spark master，则相同的Spark web UI将捕获Python Spark Streaming数据处理应用程序的统计信息。
- en: 'The following commands are run on the terminal window to run the Python application:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令在终端窗口中运行以运行Python应用程序：
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once the same log event messages used in the Scala implementation are entered
    into the Netcat console window, the following results will start showing up in
    the streaming application, filtering only the log event messages containing the
    keyword `ERROR`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦将用于Scala实现的相同日志事件消息输入到Netcat控制台窗口中，以下结果将开始在流应用程序中显示，仅过滤包含关键字`ERROR`的日志事件消息：
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If you look at the outputs from both the Scala and Python programs, you can
    clearly see whether there are any log event messages containing the word `ERROR`
    in a given batch interval. Once the data is processed, the application discards
    the processed data without retaining them for any future use.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看Scala和Python程序输出的结果，您可以清楚地看到在给定的批次间隔中是否有包含单词`ERROR`的任何日志事件消息。一旦数据被处理，应用程序会丢弃处理过的数据，而不会保留它们供将来使用。
- en: In other words, the application never retains or remembers any of the log event
    messages from the previous batch intervals. If there is a need to capture the
    number of error messages, say in the last 5 minutes or so, then the previous approach
    will not work. We will discuss that in the next section.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，应用程序永远不会保留或记住之前批次间隔中的任何日志事件消息。如果需要捕获错误消息的数量，例如在最后5分钟或更长时间内，则之前的方法将不起作用。我们将在下一节中讨论这个问题。
- en: Windowed data processing
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窗口数据处理
- en: 'In the Spark Streaming data processing application discussed in the previous
    section, assume that there is a need to count the number of log event messages
    containing the keyword ERROR in the previous three batches. In other words, there
    should be the ability to count the number of such event messages across a window
    of three batches. At any given point in time, the window should be sliding along
    with time as and when a new batch of data is available. Three important terms
    have been discussed here, and *Figure 7* explains them. They are:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节讨论的Spark Streaming数据处理应用程序中，假设需要计数前三个批次中包含关键字ERROR的日志事件消息的数量。换句话说，应该有在三个批次窗口中计数此类事件消息的能力。在任何给定的时间点，窗口应随着新数据批次的出现而滑动。这里讨论了三个重要术语，*图7*解释了它们。它们是：
- en: 'Batch interval: The time interval at which a DStream is produced'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量间隔：生成DStream的时间间隔
- en: 'Window length: The duration of the number of batch intervals where there is
    a need to peek into all the DStreams produced in those batch intervals'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 窗口长度：需要查看那些批量间隔中产生的所有DStreams的批次数量。
- en: Sliding interval:  The interval at which the window operation, such as counting
    the event messages, is performed
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滑动间隔：执行窗口操作（如计数事件消息）的间隔
- en: '![Windowed data processing](img/image_06_011.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![窗口化数据处理](img/image_06_011.jpg)'
- en: Figure 7
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图7
- en: In *Figure 7*, at a given point in time, the DStreams used for the operation
    to be performed are enclosed in a rectangle.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7*中，在特定的时间点，用于执行操作的DStreams被包含在一个矩形内。
- en: In every batch interval, a new DStream is generated. Here, the window length
    is three and the operation to be performed in a window is counting the number
    of event messages in that window. The sliding interval is kept the same as the
    batch interval so that the counting operation is done as and when a new DStream
    is generated, so that the count is correct all the time.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个批量间隔中，都会生成一个新的DStream。在这里，窗口长度为三，窗口中要执行的操作是计数该窗口中的事件消息数量。滑动间隔保持与批量间隔相同，以便在生成新的DStream时执行计数操作，确保计数始终正确。
- en: At time **t2**, the counting operation is done on the DStreams generated at
    times **t0**, **t1**, and **t2**. At time **t3**, the counting operation is done
    again since the sliding window is kept the same as the batch interval, and this
    time counting the events is done on the DStreams generated at time **t1**, **t2**,
    and **t3**. At time **t4**, the counting operation is done again, counting the
    events done on the DStreams generated at time **t2**, **t3**, and **t4**. The
    operations continue in that fashion till the application is terminated.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间**t2**，对在时间**t0**、**t1**和**t2**生成的DStreams执行计数操作。在时间**t3**，由于滑动窗口保持与批量间隔相同，因此再次执行计数操作，这次是在时间**t1**、**t2**和**t3**生成的DStreams上计数事件。在时间**t4**，再次执行计数操作，这次是在时间**t2**、**t3**和**t4**生成的DStreams上计数事件。操作以这种方式继续，直到应用程序终止。
- en: Counting the number of log event messages processed in Scala
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Scala中计数处理日志事件消息的数量
- en: 'In the preceding section, the processing of the log event messages is discussed.
    In the same application code after the printing of the log event messages containing
    the word `ERROR`, include the following lines of code in the Scala application:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，讨论了日志事件消息的处理。在相同的Scala应用程序代码中，在打印包含单词`ERROR`的日志事件消息之后，包括以下代码行：
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The first parameter is the window length and the second one is the sliding
    window interval. This single magic line will print a count of log event messages
    processed once the following lines are typed in the Netcat console:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是窗口长度，第二个参数是滑动窗口间隔。在Netcat控制台中输入以下行后，这一行魔法代码将打印出处理过的日志事件消息的计数：
- en: '[PRE13]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The same Spark Streaming data processing application in Scala, with the additional
    lines of code, produces the following output:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 添加了额外代码的相同Scala Spark Streaming数据处理应用程序产生了以下输出：
- en: '[PRE14]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: If the output is studied properly, it can be noticed that, in the first batch
    interval, one log event message is processed. Obviously the count displayed is
    `1` for that batch interval. In the next batch interval, one more log event message
    is processed. The count displayed for that batch interval is `2`. In the next
    batch interval, no log event message is processed. But the count for that window
    is still `2`. For one more window, the count is displayed as `2`. Then it reduces
    to `1`, and then 0.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仔细研究输出，可以注意到，在第一个批次间隔中，处理了一个日志事件消息。显然，显示的计数为`1`。在下一个批次间隔中，处理了一个额外的日志事件消息。该批次间隔显示的计数为`2`。在下一个批次间隔中，没有处理日志事件消息。但是该窗口的计数仍然是`2`。对于另一个窗口，计数显示为`2`。然后减少到`1`，然后是`0`。
- en: 'The most important point to be noted here is that, in the application codes
    for both Scala and Python, immediately after StreamingContext creation, the following
    line of code needs to be inserted to specify the checkpoint directory:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的最重要的一点是，在Scala和Python的应用代码中，在创建StreamingContext之后，需要立即插入以下代码行以指定检查点目录：
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Counting the number of log event messages processed in Python
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Python中计算处理过的日志事件消息的数量
- en: 'In the Python application code, after the printing of the log event messages
    containing the word ERROR, include the following lines of code in the Scala application:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python应用程序代码中，在打印包含单词ERROR的日志事件消息之后，在Scala应用程序中包含以下代码行：
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The first parameter is the window length and the second one is the sliding
    window interval. This single magic line will print a count of log event messages
    processed once the following lines are typed in the Netcat console:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是窗口长度，第二个参数是滑动窗口间隔。在Netcat控制台中输入以下行后，这一行神奇的代码将打印处理过的日志事件消息的数量：
- en: '[PRE17]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The same Spark Streaming data processing application in Python, with the additional
    lines of code, produces the following output:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，相同的Spark Streaming数据处理应用程序，通过添加额外的代码行，产生以下输出：
- en: '[PRE18]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The output pattern of the Python application is also very similar to the Scala
    application.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Python应用程序的输出模式也与Scala应用程序非常相似。
- en: More processing options
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多处理选项
- en: Apart from the count operation in a window, there are more operations that can
    be done on DStreams in conjunction with windowing. The following table captures
    the important transformations. All these transformations are acting on the selected
    window and return a DStream.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 除了窗口中的计数操作外，还可以在DStreams上执行更多与窗口结合的操作。以下表格总结了重要的转换。所有这些转换都在选定的窗口上操作，并返回一个DStream。
- en: '| **Transformation** | **Description** |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| **转换** | **描述** |'
- en: '| `window(windowLength, slideInterval)` | Returns DStreams computed in the
    window |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| `window(windowLength, slideInterval)` | 返回窗口中的DStreams计算结果 |'
- en: '| `countByWindow(windowLength, slideInterval)` | Returns the count of elements
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| `countByWindow(windowLength, slideInterval)` | 返回元素的数量 |'
- en: '| `reduceByWindow(func, windowLength, slideInterval)` | Returns one element
    by applying the aggregation function |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByWindow(func, windowLength, slideInterval)` | 通过应用聚合函数返回一个元素 |'
- en: '| `reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])` | Returns
    one key/value pair per key after applying the aggregation function over  multiple
    values per key |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])` | 在每个键上应用聚合函数后，返回每个键的一个键/值对
    |'
- en: '| `countByValueAndWindow(windowLength, slideInterval, [numTasks])` | Returns
    one key/count pair per key after applying the count of multiple values per key
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| `countByValueAndWindow(windowLength, slideInterval, [numTasks])` | 在每个键上应用每个键的多个值的计数后，返回每个键的一个键/计数对
    |'
- en: One of the most important steps of stream processing is the persisting of the
    stream data into secondary storage. Since the velocity of the data in Spark Streaming
    data processing applications is going to be very high, any kind of persistence
    mechanism that introduces additional latency into the whole process is not an
    advisable solution.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理最重要的步骤之一是将流数据持久化到二级存储。由于Spark Streaming数据处理应用程序中的数据速度将非常快，任何引入额外延迟的持久化机制都不是一个可取的解决方案。
- en: In batch processing scenarios, it is fine to write to the HDFS and other file
    system based storage. But when it comes to the storage of stream output, depending
    on the use case, an ideal stream data storage mechanism should be chosen.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在批处理场景中，写入HDFS和其他基于文件系统的存储是可行的。但是，当涉及到流输出存储时，根据用例，应选择理想的流数据存储机制。
- en: NoSQL data stores such as Cassandra support fast writes of temporal data. It
    is also ideal to read the data that is stored for any further analysis purposes.
    Spark Streaming library supports many output methods on DStreams. They include
    options to save the stream data as text file, object file, Hadoop files, and so
    on. As well as this, there are many third-party drivers available to save the
    data into various data stores.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如 Cassandra 这样的 NoSQL 数据存储支持快速写入时间序列数据。它也适合读取存储的数据以进行进一步分析。Spark Streaming 库支持
    DStreams 的许多输出方法。它们包括将流数据保存为文本文件、对象文件、Hadoop 文件等选项。此外，还有许多第三方驱动程序可用于将数据保存到各种数据存储中。
- en: Kafka stream processing
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka 流处理
- en: The log event processor example covered in this chapter was listening to a TCP
    socket for the stream of messages to be processed by the Spark Streaming data
    processing application. But in real-world use cases, this is not going to be the
    case.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中涵盖的日志事件处理器示例正在监听 Spark Streaming 数据处理应用要处理的流消息的 TCP 套接字。但在现实世界的用例中，情况并非如此。
- en: Message queueing systems with publish-subscribe capability are generally used
    for processing messages. The traditional message queueing systems failed to perform
    because of the huge volume of messages to be processed per second for the needs
    of large-scale data processing applications.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 具有发布-订阅能力的消息队列系统通常用于处理消息。传统的消息队列系统由于需要处理每秒大量消息而无法胜任，这对于大规模数据处理应用的需求来说。
- en: 'Kafka is a publish-subscribe messaging system used by many IoT applications
    to process a huge number of messages. The following capabilities of Kafka made
    it one of the most widely used messaging systems:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 是许多物联网应用使用的发布-订阅消息系统，用于处理大量消息。以下 Kafka 的功能使其成为最广泛使用的消息系统之一：
- en: 'Extremely fast: Kafka can process huge amounts of data by handling reading
    and writing in short intervals of time from many application clients'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极其快速：Kafka 通过处理来自许多应用客户端的短时间间隔内的读写操作，可以处理大量数据
- en: 'Highly scalable: Kafka is designed to scale up and scale out to form a cluster
    using commodity hardware'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度可扩展：Kafka 设计用于向上和向外扩展，使用通用硬件形成一个集群
- en: 'Persists a huge number of messages: Messages reaching Kafka topics are persisted
    into the secondary storage, while at the same time it is handling huge number
    of messages flowing through'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持存大量消息：达到 Kafka 主题的消息被持久化到二级存储中，同时它还在处理通过的大量消息
- en: Note
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: A detailed treatment of Kafka is outside the scope of this book. It is assumed
    that the reader is familiar with and has working knowledge of Kafka. From a Spark
    Streaming data processing application perspective, it doesn't really make a difference
    whether it is a TCP socket or Kafka that is being used as a message source. But
    working on a teaser use case with Kafka as the message producer will give a good
    appreciation of the toolsets enterprises are using heavily. *Learning Apache Kafka*
    *- Second Edition* by *Nishant Garg* ([https://www.packtpub.com/big-data-and-business-intelligence/learning-apache-kafka-second-edition](https://www.packtpub.com/big-data-and-business-intelligence/learning-apache-kafka-second-edition))
    is a good reference book to learn more about Kafka.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 的详细讨论超出了本书的范围。假设读者熟悉 Kafka 并具有实际操作知识。从 Spark Streaming 数据处理应用的角度来看，使用
    TCP 套接字或 Kafka 作为消息源实际上并没有太大区别。但是，使用 Kafka 作为消息生产者的示例用例将有助于更好地理解企业大量使用的工具集。"学习
    Apache Kafka" *第二版* 由 *Nishant Garg* 编著 ([https://www.packtpub.com/big-data-and-business-intelligence/learning-apache-kafka-second-edition](https://www.packtpub.com/big-data-and-business-intelligence/learning-apache-kafka-second-edition))
    是一本很好的参考书，可以了解更多关于 Kafka 的信息。
- en: 'The following are some of the important elements of Kafka, and are terms to
    be understood before proceeding further:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些 Kafka 的重要元素，是在进一步了解之前需要理解的概念：
- en: 'Producer: The real source of the messages, such as weather sensors or mobile
    phone network'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产者：消息的真实来源，例如气象传感器或移动电话网络
- en: 'Broker: The Kafka cluster, which receives and persists the messages published
    to its topics by various producers'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理：接收并持久化各种生产者发布到其主题的消息的 Kafka 集群
- en: 'Consumer: The data processing applications subscribed to the Kafka topics that
    consume the messages published to the topics'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消费者：订阅 Kafka 主题并消费发布到主题的消息的数据处理应用
- en: The same log event processing application use case discussed in the preceding
    section is used again here to elucidate the usage of Kafka with Spark Streaming.
    Instead of collecting the log event messages from the TCP socket, here the Spark
    Streaming data processing application will act as a consumer of a Kafka topic
    and the messages published to the topic will be consumed.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中讨论的相同日志事件处理用例在此处再次使用，以阐明 Kafka 与 Spark Streaming 的使用方法。与从 TCP 套接字收集日志事件消息不同，这里
    Spark Streaming 数据处理应用将充当 Kafka 主题的消费者，并将发布到主题的消息进行消费。
- en: 'The Spark Streaming data processing application uses the version 0.8.2.2 of
    Kafka as the message broker, and the assumption is that the reader has already
    installed Kafka, at least in a standalone mode. The following activities are to
    be performed to make sure that Kafka is ready to process the messages produced
    by the producers and that the Spark Streaming data processing application can
    consume those messages:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 数据处理应用使用 Kafka 的 0.8.2.2 版本作为消息代理，假设读者已经安装了 Kafka，至少是以独立模式安装。以下活动是为了确保
    Kafka 准备好处理生产者产生的消息，并且 Spark Streaming 数据处理应用可以消费这些消息：
- en: Start the Zookeeper that comes with Kafka installation.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 Kafka 安装包中包含的 Zookeeper。
- en: Start the Kafka server.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 Kafka 服务器。
- en: Create a topic for the producers to send the messages to.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为生产者创建一个发送消息的主题。
- en: Pick up one Kafka producer and start publishing log event messages to the newly
    created topic.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个 Kafka 生产者并开始向新创建的主题发布日志事件消息。
- en: Use the Spark Streaming data processing application to process the log events
    published to the newly created topic.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Spark Streaming 数据处理应用处理发布到新创建主题的日志事件。
- en: Starting Zookeeper and Kafka
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动 Zookeeper 和 Kafka
- en: 'The following scripts are run from separate terminal windows in order to start
    Zookeeper and the Kafka broker, and to create the required Kafka topics:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本是在单独的终端窗口中运行的，以启动 Zookeeper 和 Kafka 代理，并创建所需的 Kafka 主题：
- en: '[PRE19]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Tip
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Make sure that the environment variable `$KAFKA_HOME` is pointing to the directory
    where Kafka is installed. Also, it is very important to start Zookeeper, Kafka
    server, Kafka producer, and Spark Streaming log event data processing application
    in separate terminal windows.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 确保环境变量 `$KAFKA_HOME` 指向 Kafka 安装的目录。此外，在单独的终端窗口中启动 Zookeeper、Kafka 服务器、Kafka
    生产者和 Spark Streaming 日志事件数据处理应用非常重要。
- en: The Kafka message producer can be any application capable of publishing messages
    to the Kafka topics. Here, the `kafka-console-producer` that comes with Kafka
    is used as the producer of choice. Once the producer starts running, whatever
    is typed into its console window will be treated as a message that is published
    to the chosen Kafka topic. The Kafka topic is given as a command line argument
    when starting the `kafka-console-producer`.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 消息生产者可以是任何能够向 Kafka 主题发布消息的应用程序。在此，选择 Kafka 中的 `kafka-console-producer`
    作为首选的生产者。一旦生产者开始运行，在其控制台窗口中输入的内容将被视为发布到所选 Kafka 主题的消息。Kafka 主题在启动 `kafka-console-producer`
    时作为命令行参数给出。
- en: 'The submission of the Spark Streaming data processing application that consumes
    log event messages produced by the Kafka producer is slightly different from the
    application covered in the preceding section. Here, many Kafka jar files are required
    for the data processing. Since they are not part of the Spark infrastructure,
    they have to be submitted to the Spark cluster. The following jar files are required
    for the successful running of this application:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 消费由 Kafka 生产者产生的日志事件消息的 Spark Streaming 数据处理应用的提交方式与上一节中介绍的应用略有不同。在此，需要许多 Kafka
    jar 文件进行数据处理。由于它们不是 Spark 基础设施的一部分，因此必须将它们提交到 Spark 集群。以下 jar 文件对于此应用的正常运行是必需的：
- en: '`$KAFKA_HOME/libs/kafka-clients-0.8.2.2.jar`'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$KAFKA_HOME/libs/kafka-clients-0.8.2.2.jar`'
- en: '`$KAFKA_HOME/libs/kafka_2.11-0.8.2.2.jar`'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$KAFKA_HOME/libs/kafka_2.11-0.8.2.2.jar`'
- en: '`$KAFKA_HOME/libs/metrics-core-2.2.0.jar`'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$KAFKA_HOME/libs/metrics-core-2.2.0.jar`'
- en: '`$KAFKA_HOME/libs/zkclient-0.3.jar`'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`$KAFKA_HOME/libs/zkclient-0.3.jar`'
- en: '`Code/Scala/lib/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar`'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Code/Scala/lib/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar`'
- en: '`Code/Python/lib/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar`'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Code/Python/lib/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar`'
- en: In the preceding list of jar files, the maven repository co-ordinate for `spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar`
    is `"org.apache.spark" %% "spark-streaming-kafka-0-8" % "2.0.0-preview"`. This
    particular jar file has to be downloaded and placed in the lib folder of the directory
    structure given in Figure 4\. It is being used in the `submit.sh` and the `submitPy.sh`
    scripts, which  submit the application to the Spark cluster. The download URL
    for this jar file is given in the reference section of this chapter.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的 jar 文件列表中，`spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar` 的 Maven 仓库坐标为
    `"org.apache.spark" %% "spark-streaming-kafka-0-8" % "2.0.0-preview"`。这个特定的 jar
    文件必须下载并放置在图 4 所示的目录结构中的 lib 文件夹中。它被用于 `submit.sh` 和 `submitPy.sh` 脚本中，这些脚本将应用程序提交到
    Spark 集群。该 jar 文件的下载 URL 在本章的参考部分给出。
- en: In the `submit.sh` and `submitPy.sh` files, the last few lines contain a conditional
    statement looking for the second parameter value of 1 to identify this application
    and ship the required jar files to the Spark cluster.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `submit.sh` 和 `submitPy.sh` 文件中，最后几行包含一个条件语句，寻找第二个参数值为 1 以识别此应用程序并将所需的 jar
    文件发送到 Spark 集群。
- en: Tip
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Instead of shipping these individual jar files separately to the Spark cluster
    when the job is submitted, an assembly jar can be used by creating it using sbt.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在提交作业时，不需要分别将这些单独的 jar 文件发送到 Spark 集群，可以使用 sbt 创建的 assembly jar。
- en: Implementing the application in Scala
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Scala 中实现应用程序
- en: 'The following code snippet is the Scala code for the log event processing application
    that processes the messages produced by the Kafka producer. The use case of this
    application is the same as the one discussed in the preceding section concerning
    windowing operations:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段是处理 Kafka 生产者产生的消息的日志事件处理应用程序的 Scala 代码。该应用程序的使用案例与前述部分关于窗口操作讨论的使用案例相同：
- en: '[PRE20]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Compared to the Scala code in the preceding section, the major difference is
    in the way the stream is created.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 与前述部分的 Scala 代码相比，主要区别在于创建流的方式。
- en: Implementing the application in Python
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Python 中实现应用程序
- en: 'The following code snippet is the Python code for the log event processing
    application that processes the message produced by the Kafka producer. The use
    case of this application is also the same as the one discussed in the preceding
    section concerning windowing operations:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段是处理 Kafka 生产者产生的消息的日志事件处理应用程序的 Python 代码。该应用程序的使用案例也与前述部分关于窗口操作讨论的使用案例相同：
- en: '[PRE21]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following commands are run on the terminal window to run the Scala application:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端窗口中运行以下命令以运行 Scala 应用程序：
- en: '[PRE22]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following commands are run on the terminal window to run the Python application:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端窗口中运行以下命令以运行 Python 应用程序：
- en: '[PRE23]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'When both of the preceding programs are running, whatever log event messages
    are typed into the console window of the Kafka console producer, and invoked using
    the following command and inputs, will be processed by the application. The outputs
    of this program will be very similar to the ones that are given in the preceding
    section:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当先前的两个程序都在运行时，无论在 Kafka 控制台生产者的控制台窗口中键入什么日志事件消息，并使用以下命令和输入调用，都将由应用程序处理。该程序的输出将与前述部分给出的输出非常相似：
- en: '[PRE24]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Spark provides two approaches to process Kafka streams. The first one is the
    receiver-based approach that was discussed previously and the second one is the
    direct approach.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了两种处理 Kafka 流的方法。第一种是之前讨论过的基于接收器的方案，第二种是直接方法。
- en: This direct approach to processing Kafka messages is a simplified method in
    which Spark Streaming is using all the possible capabilities of Kafka just like
    any of the Kafka topic consumers, and polls for the messages in the specific topic,
    and the partition by the offset number of the messages. Depending on the batch
    interval of the Spark Streaming data processing application, it picks up a certain
    number of offsets from the Kafka cluster, and this range of offsets is processed
    as a batch. This is highly efficient and ideal for processing messages with a
    requirement to have exactly-once processing. This method also reduces the Spark
    Streaming library's need to do additional work to implement the exactly-once semantics
    of the message processing and delegates that responsibility to Kafka. The programming
    constructs of this approach are slightly different in the APIs used for the data
    processing. Consult the appropriate reference material for the details.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这种直接处理Kafka消息的方法是一种简化方法，其中Spark Streaming像任何Kafka主题消费者一样，使用Kafka的所有可能功能，并针对特定主题轮询消息，以及通过消息的偏移量来分区。根据Spark
    Streaming数据处理应用程序的批处理间隔，它从Kafka集群中选取一定数量的偏移量，并将这个偏移量范围作为一批处理。这种方法非常高效，非常适合需要精确一次处理的消息。此方法还减少了Spark
    Streaming库执行消息处理精确一次语义所需进行额外工作的需求，并将该责任委托给Kafka。此方法的编程结构在用于数据处理的应用程序接口中略有不同。有关详细信息，请参阅适当的参考材料。
- en: The preceding sections introduced the concept of a Spark Streaming library and
    discussed some of the real-world use cases. There is a big difference between
    Spark data processing applications developed to process static batch data and
    those developed to process dynamic stream data in a deployment perspective. The
    availability of data processing applications to process a stream of data must
    be constant. In other words, such applications should not have components that
    are single points of failure. The following section is going to discuss this topic.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的章节介绍了Spark Streaming库的概念，并讨论了一些实际应用案例。从部署的角度来看，用于处理静态批量数据的Spark数据处理应用程序与用于处理动态流数据的Spark数据处理应用程序之间存在很大差异。数据处理应用程序处理数据流的能力必须是持续的。换句话说，此类应用程序不应具有单点故障的组件。下一节将讨论这个话题。
- en: Spark Streaming jobs in production
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生产中的Spark Streaming作业
- en: When a Spark Streaming application is processing the incoming data, it is very
    important to have uninterrupted data processing capability so that all the data
    that is getting ingested is processed. In business-critical streaming applications,
    most of the time missing even one piece of data can have a huge business impact.
    To deal with such situations, it is important to avoid single points of failure
    in the application infrastructure.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 当Spark Streaming应用程序正在处理传入的数据时，拥有不间断的数据处理能力非常重要，以确保所有被摄取的数据都得到处理。在业务关键型流式应用程序中，大多数情况下，丢失哪怕一条数据都可能对业务产生巨大影响。为了处理这种情况，避免应用程序基础设施中的单点故障非常重要。
- en: From a Spark Streaming application perspective, it is good to understand how
    the underlying components in the ecosystem are laid out so that the appropriate
    measures can be taken to avoid single points of failure.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 从Spark Streaming应用程序的角度来看，了解生态系统中底层组件的布局是很有好处的，这样就可以采取适当的措施来避免单点故障。
- en: 'A Spark Streaming application deployed in a cluster such as Hadoop YARN, Mesos
    or Spark Standalone mode has two main components very similar to any other type
    of Spark application:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 部署在Hadoop YARN、Mesos或Spark Standalone模式等集群中的Spark Streaming应用程序有两个主要组件，与任何其他类型的Spark应用程序非常相似：
- en: '**Spark driver**: This contains the application code written by the user'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark驱动程序**：这包含用户编写的应用程序代码'
- en: '**Executors**: The executors that execute the jobs submitted by the Spark driver'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行器**：执行Spark驱动程序提交的作业的执行器'
- en: But the executors have an additional component called a receiver that receives
    the data getting ingested as a stream and saves it as blocks of data in memory.
    When one receiver is receiving the data and forming the data blocks, they are
    replicated to another executor for fault-tolerance. In other words, in-memory
    replication of the data blocks is done onto a different executor. At the end of
    every batch interval, these data blocks are combined to form a DStream and sent
    out for further processing downstream.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，执行器有一个额外的组件，称为接收器，它接收作为流输入的数据，并将其保存为内存中的数据块。当一个接收器正在接收数据并形成数据块时，它们会被复制到另一个执行器以实现容错。换句话说，数据块的内存复制是在不同的执行器上完成的。在每个批处理间隔结束时，这些数据块会被组合成一个DStream，并输出以进行进一步的处理。
- en: '*Figure 8* depicts the components working together in a Spark Streaming application
    infrastructure deployed in a cluster:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8*展示了在集群中部署的Spark Streaming应用程序基础设施中协同工作的组件：'
- en: '![Spark Streaming jobs in production](img/image_06_013.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![Spark Streaming生产中的作业](img/image_06_013.jpg)'
- en: Figure 8
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图8
- en: In *Figure 8*, there are two executors. The receiver component is deliberately
    not displayed in the second executor to show that it is not using the receiver
    and instead just collects the replicated data blocks from the other executor.
    But when needed, such as on the failure of the first executor, the receiver in
    the second executor can start functioning.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图8*中，有两个执行器。接收组件在第二个执行器中故意没有显示，以表明它没有使用接收器，而是仅仅从另一个执行器收集复制的数据块。但是，当需要时，例如在第一个执行器失败的情况下，第二个执行器中的接收器可以开始工作。
- en: Implementing fault-tolerance in Spark Streaming data processing applications
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Spark Streaming数据处理应用程序中实现容错性
- en: Spark Streaming data processing application infrastructure has many moving parts.
    Failures can happen to any one of them, resulting in the interruption of the data
    processing. Typically failures can happen to the Spark driver or the executors.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming数据处理应用程序的基础设施有许多动态部分。任何一部分都可能发生故障，从而导致数据处理的中断。通常，故障可能发生在Spark驱动程序或执行器上。
- en: Note
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: This section is not intended to give detailed treatment to running the Spark
    Streaming applications in production with fault-tolerance. The intention is to
    make the reader appreciate the precautions to be taken when deploying Spark Streaming
    data processing applications in production.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目的不是详细说明在生产环境中运行具有容错能力的Spark Streaming应用程序，而是让读者了解在生产环境中部署Spark Streaming数据处理应用程序时应采取的预防措施。
- en: When an executor fails, since the replication of data is happening on a regular
    basis, the task of receiving the data stream will be taken over by the executor
    on which the data was getting replicated. There is a situation in which when an
    executor fails, all the data that is unprocessed will be lost. To circumvent this
    problem, there is a way to persist the data blocks into HDFS or Amazon S3 in the
    form of write-ahead logs.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个执行器失败时，由于数据复制是定期发生的，接收数据流的任务将由数据正在复制的执行器接管。有一种情况是，当一个执行器失败时，所有未处理的数据都将丢失。为了避免这个问题，有一种方法可以将数据块以预写日志的形式持久化到HDFS或Amazon
    S3。
- en: Tip
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: There is no need to have both the in-memory replication of the data blocks and
    write-ahead logs together in one infrastructure. Keep only one of them, depending
    on the need.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个基础设施中不需要同时拥有数据块的内存复制和预写日志。根据需要，只保留其中之一。
- en: When the Spark driver fails, the driven program is stopped, all the executors
    lose connection, and they stop functioning. This is the most dangerous situation.
    To deal with this situation, some configuration and code changes are necessary.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 当Spark驱动程序失败时，被驱动的程序会停止，所有执行器都会失去连接，并停止工作。这是最危险的情况。为了处理这种情况，需要进行一些配置和代码更改。
- en: 'The Spark driver has to be configured to have an automatic driver restart,
    which is supported by the cluster managers. This includes a change in the Spark
    job submission method to have the cluster mode in whichever may be the cluster
    manager. When a restart of the driver happens, to start from the place when it
    crashed, a checkpointing mechanism has to be implemented in the driver program.
    This has already been done in the code samples that are used. The following lines
    of code do that job:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Spark驱动程序必须配置为具有自动驱动程序重启功能，这由集群管理器支持。这包括更改Spark作业提交方法，以便在任何集群管理器中都具有集群模式。当驱动程序重启时，为了从崩溃的地方重新开始，驱动程序程序中必须实现一个检查点机制。这已经在使用的代码示例中完成。以下代码行执行这项任务：
- en: '[PRE25]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Tip
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: In a sample application, it is fine to use a local system directory as the checkpoint
    directory. But in a production environment, it is better to keep this checkpoint
    directory as an HDFS location in the case of Hadoop or an S3 location in the case
    of an Amazon cloud.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个示例应用中，使用本地系统目录作为检查点目录是可以的。但在生产环境中，如果使用Hadoop，最好将此检查点目录保持在HDFS位置；如果使用亚马逊云，则保持在S3位置。
- en: From an application coding perspective, the way the `StreamingContext`is created
    is slightly different. Instead of creating a new `StreamingContext`every time,
    the factory method `getOrCreate`of the `StreamingContext`is to be used with a
    function, as shown in the following code segment. If that is done, when the driver
    is restarted, the factory method will check the checkpoint directory to see whether
    an earlier `StreamingContext`was in use, and, if found in the checkpoint data,
    it is created. Otherwise, a new `StreamingContext`is created.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 从应用程序编码的角度来看，创建`StreamingContext`的方式略有不同。不是每次都创建一个新的`StreamingContext`，而是应该使用一个函数与`StreamingContext`的工厂方法`getOrCreate`一起使用，如以下代码段所示。如果这样做，当驱动程序重启时，工厂方法将检查检查点目录以查看是否正在使用早期的`StreamingContext`，如果找到检查点数据，则创建它。否则，将创建一个新的`StreamingContext`。
- en: 'The following code snippet gives the definition of a function that can be used
    with the `getOrCreate`factory method of the `StreamingContext`. As mentioned earlier,
    a detailed treatment of these aspects is beyond the scope of this book:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段给出了一个函数的定义，该函数可以与`StreamingContext`的`getOrCreate`工厂方法一起使用。如前所述，这些方面的详细处理超出了本书的范围：
- en: '[PRE26]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: At a data source level, it is a good idea to build parallelism for faster data
    processing and, depending on the source of data, this can be accomplished in different
    ways. Kafka inherently supports partition at the topic level, and that kind of
    scaling out mechanism supports a good amount of parallelism. As a consumer of
    Kafka topics, the Spark Streaming data processing application can have multiple
    receivers by creating multiple streams, and the data generated by those streams
    can be combined by the union operation on the Kafka streams.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据源级别，为了加快数据处理速度，构建并行性是一个好主意，并且根据数据源的不同，可以通过不同的方式实现。Kafka在主题级别内建支持分区，这种扩展机制支持大量的并行性。作为Kafka主题的消费者，Spark
    Streaming数据处理应用程序可以通过创建多个流来拥有多个接收器，并且这些流生成的数据可以通过在Kafka流上执行联合操作来合并。
- en: The production deployment of Spark Streaming data processing applications is
    to be done purely based on the type of application that is being used. Some of
    the guidelines given previously are just introductory and conceptual in nature.
    There is no silver bullet approach to solving production deployment problems,
    and they have to evolve along with the application development.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming数据处理应用程序的生产部署应完全基于所使用的应用程序类型。之前给出的某些指南只是介绍性和概念性的。没有一劳永逸的解决生产部署问题的方法，它们必须随着应用程序开发而发展。
- en: Structured streaming
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化流
- en: In the data streaming use cases that have been covered so far, there are many
    developer tasks in terms of building of the structure data and implementing fault
    tolerance for the application. The data that has been dealt with so far in data
    streaming applications is unstructured data. Just like the batch data processing
    use cases, even in streaming use cases, if there is the capability to process
    structured data, that is a great advantage, and lots of pre-processing can be
    avoided. Data stream processing applications are continuously running applications
    and they are bound to develop failures or interruptions. In such situations, it
    is imperative to build fault tolerance in the data streaming applications.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在迄今为止涵盖的数据流用例中，有许多关于构建结构化数据和实现应用程序容错性的开发任务。迄今为止在数据流应用程序中处理的数据是无结构化数据。就像批处理数据处理的用例一样，即使在流用例中，如果能够处理结构化数据，那将是一个巨大的优势，可以避免大量的预处理。数据流处理应用程序是持续运行的应用程序，它们注定会发展出故障或中断。在这种情况下，在数据流应用程序中构建容错性是至关重要的。
- en: In any data streaming application, the data is getting ingested continuously,
    and if there is a need to interrogate the data received at any given point in
    time, the application developers have to persist the data processed into a data
    store that supports querying. In Spark 2.0, the structured streaming concept is
    built around these aspects, and the whole idea behind building this brand new
    feature from the ground up is to relieve application developers of these pain
    areas. There is a feature with the reference number SPARK-8360 being built at
    the time of writing this chapter, and its progress can be monitored by visiting
    the corresponding page.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何数据流应用程序中，数据都在持续摄入，如果需要在任何给定时间点查询接收到的数据，应用开发者必须将处理过的数据持久化到支持查询的数据存储中。在 Spark
    2.0 中，结构化流的概念围绕这些方面构建，而构建这个全新功能的整个理念是从根本上减轻应用开发者的这些痛点。在撰写本章时，正在构建一个具有参考编号 SPARK-8360
    的功能，其进度可以通过访问相应的页面进行监控。
- en: The structured streaming concept can be explained using a real-world use case,
    such as the banking transaction use case we looked at before. Assume that the
    comma-separated transaction records containing the account number and transaction
    amount are coming in a stream. In the structured stream processing method, all
    these data items get ingested into an unbounded table or DataFrame that supports
    querying using Spark SQL. In other words, since the data is accumulated in a DataFrame,
    whatever data processing is possible using a DataFrame will be possible with the
    stream data as well. This reduces the burden on application developers and they
    can focus on the business logic of the application rather than the infrastructure
    related-aspects.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流的概念可以通过一个现实世界的用例来解释，例如我们之前看过的银行交易用例。假设包含账户号码和交易金额的逗号分隔的交易记录正在以流的形式传入。在结构化流处理方法中，所有这些数据项都会被摄入到一个支持使用
    Spark SQL 查询的无界表或 DataFrame 中。换句话说，由于数据累积在 DataFrame 中，使用 DataFrame 可以进行的数据处理也可以在流数据上进行。这减轻了应用开发者的负担，他们可以专注于应用程序的业务逻辑，而不是与基础设施相关的问题。
- en: References
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'For more information, visit the following:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请访问以下链接：
- en: '[https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Spark Streaming 编程指南](https://spark.apache.org/docs/latest/streaming-programming-guide.html)'
- en: '[http://kafka.apache.org/](http://kafka.apache.org/)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Apache Kafka](http://kafka.apache.org/)'
- en: '[http://spark.apache.org/docs/latest/streaming-kafka-integration.html](http://spark.apache.org/docs/latest/streaming-kafka-integration.html)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Spark Streaming 与 Kafka 集成](http://spark.apache.org/docs/latest/streaming-kafka-integration.html)'
- en: '[https://www.packtpub.com/big-data-and-business-intelligence/learning-apache-kafka-second-edition](https://www.packtpub.com/big-data-and-business-intelligence/learning-apache-kafka-second-edition)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习 Apache Kafka 第二版](https://www.packtpub.com/big-data-and-business-intelligence/learning-apache-kafka-second-edition)'
- en: '[http://search.maven.org/remotecontent?filepath=org/apache/spark/spark-streaming-kafka-0-8_2.11/2.0.0-preview/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar](http://search.maven.org/remotecontent?filepath=org/apache/spark/spark-streaming-kafka-0-8_2.11/2.0.0-preview/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://search.maven.org/remotecontent?filepath=org/apache/spark/spark-streaming-kafka-0-8_2.11/2.0.0-preview/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar](http://search.maven.org/remotecontent?filepath=org/apache/spark/spark-streaming-kafka-0-8_2.11/2.0.0-preview/spark-streaming-kafka-0-8_2.11-2.0.0-preview.jar)'
- en: '[https://issues.apache.org/jira/browse/SPARK-836](https://issues.apache.org/jira/browse/SPARK-836)'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://issues.apache.org/jira/browse/SPARK-836](https://issues.apache.org/jira/browse/SPARK-836)'
- en: Summary
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Spark provides a very powerful library on top of the Spark core to process
    the stream of data getting ingested at a high velocity. This chapter introduced
    the basics of the Spark Streaming library, and a simple log event message processing
    system has been developed with two types of data source: one uses a TCP data server
    and the other uses Kafka. At the end of the chapter, a brief look at the production
    deployment of Spark Streaming data processing applications is provided and the
    possible ways of implementing fault-tolerance in Spark Streaming data processing
    applications as discussed.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在Spark核心之上提供了一个非常强大的库来处理以高速摄入的数据流。本章介绍了Spark Streaming库的基本知识，并开发了一个简单的日志事件消息处理系统，该系统使用了两种类型的数据源：一种使用TCP数据服务器，另一种使用Kafka。本章末尾简要介绍了Spark
    Streaming数据处理应用程序的生产部署，并讨论了在Spark Streaming数据处理应用程序中实现容错性的可能方法。
- en: Spark 2.0 brings the capability to process and query structured data in streaming
    applications, and the concept has been introduced, which relieves application
    developers from pre-processing the unstructured data, building fault-tolerance
    and querying the data that is being ingested on a near-real-time basis.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0引入了在流式应用程序中处理和查询结构化数据的能力，并引入了这一概念，从而减轻了应用开发者对非结构化数据进行预处理、构建容错性和查询近实时摄入数据的负担。
- en: 'Applied mathematicians and statisticians have come up with ways and means to
    answer questions related to a new piece of data based on the *learning* that has
    already been done on an existing bank of data. Typically these questions include,
    but are not limited to: does this piece of data fit a given model, can this piece
    of data be classified in a certain way, and does this piece of data belong to
    any group or cluster?'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 应用数学家和统计学家已经找到了基于对现有数据集已完成的*学习*来回答与新数据相关问题的方法和手段。通常这些问题包括但不限于：这块数据是否符合给定的模型，这块数据能否以某种方式分类，以及这块数据是否属于任何组或聚类？
- en: There are lots of algorithms available to *train* a data model and ask questions
    to this *model* about the new piece of data. This rapidly evolving branch of data
    science has huge applicability in data processing, and is popularly known as machine
    learning. The next chapter is going to discuss the machine learning library of
    Spark.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 可用于*训练*数据模型并向此*模型*询问关于新数据的各种算法很多。这一快速发展的数据科学分支在数据处理中具有巨大的应用性，通常被称为机器学习。下一章将讨论Spark的机器学习库。
