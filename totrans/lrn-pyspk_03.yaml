- en: Chapter 3. DataFrames
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 3 章 数据帧
- en: A DataFrame is an immutable distributed collection of data that is organized
    into named columns analogous to a table in a relational database. Introduced as
    an experimental feature within Apache Spark 1.0 as `SchemaRDD`, they were renamed
    to `DataFrames` as part of the Apache Spark 1.3 release. For readers who are familiar
    with Python Pandas `DataFrame` or R `DataFrame`, a Spark DataFrame is a similar
    concept in that it allows users to easily work with structured data (for example,
    data tables); there are some differences as well so please temper your expectations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 是一个不可变的分布式数据集合，它组织成命名的列，类似于关系数据库中的表。作为 Apache Spark 1.0 中的实验性功能 `SchemaRDD`
    的一部分引入，它们在 Apache Spark 1.3 发布中更名为 `DataFrames`。对于熟悉 Python Pandas `DataFrame`
    或 R `DataFrame` 的读者，Spark DataFrame 是一个类似的概念，它允许用户轻松地处理结构化数据（例如，数据表）；也有一些差异，所以请调整您的期望。
- en: By imposing a structure onto a distributed collection of data, this allows Spark
    users to query structured data in Spark SQL or using expression methods (instead
    of lambdas). In this chapter, we will include code samples using both methods.
    By structuring your data, this allows the Apache Spark engine – specifically,
    the Catalyst Optimizer – to significantly improve the performance of Spark queries.
    In earlier APIs of Spark (that is, RDDs), executing queries in Python could be
    significantly slower due to communication overhead between the Java JVM and Py4J.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对分布式数据集合施加结构，这允许 Spark 用户在 Spark SQL 或使用表达式方法（而不是 lambda 函数）中查询结构化数据。在本章中，我们将包括使用这两种方法的代码示例。通过结构化您的数据，这允许
    Apache Spark 引擎——特别是 Catalyst 优化器——显著提高 Spark 查询的性能。在 Spark 早期 API（即 RDDs）中，由于
    Java JVM 和 Py4J 之间的通信开销，执行 Python 中的查询可能会显著变慢。
- en: Note
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you are familiar with working with DataFrames in previous versions of Spark
    (that is Spark 1.x), you will notice that in Spark 2.0 we are using SparkSession
    instead of `SQLContext`. The various Spark contexts: `HiveContext`, `SQLContext`,
    `StreamingContext`, and `SparkContext` have merged together in SparkSession. This
    way you will be working with this session only as an entry point for reading data,
    working with metadata, configuration, and cluster resource management.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉在 Spark 早期版本（即 Spark 1.x）中与 DataFrame 一起工作，您会注意到在 Spark 2.0 中，我们使用 SparkSession
    而不是 `SQLContext`。各种 Spark 上下文：`HiveContext`、`SQLContext`、`StreamingContext` 和
    `SparkContext` 已合并到 SparkSession 中。这样，您将只作为读取数据、处理元数据、配置和集群资源管理的入口点与这个会话一起工作。
- en: For more information, please refer to *How to use SparkSession in Apache Spark
    2.0*([http://bit.ly/2br0Fr1](http://bit.ly/2br0Fr1)).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参阅 *如何在 Apache Spark 2.0 中使用 SparkSession*([http://bit.ly/2br0Fr1](http://bit.ly/2br0Fr1))。
- en: 'In this chapter, you will learn about the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解以下内容：
- en: Python to RDD communications
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 到 RDD 通信
- en: A quick refresh of Spark's Catalyst Optimizer
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速回顾 Spark 的 Catalyst 优化器
- en: Speeding up PySpark with DataFrames
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DataFrames 加速 PySpark
- en: Creating DataFrames
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建 DataFrame
- en: Simple DataFrame queries
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单的 DataFrame 查询
- en: Interoperating with RDDs
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 RDD 交互
- en: Querying with the DataFrame API
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DataFrame API 进行查询
- en: Querying with Spark SQL
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Spark SQL 进行查询
- en: Using DataFrames for an on-time flight performance
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DataFrame 进行准点航班性能分析
- en: Python to RDD communications
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 到 RDD 通信
- en: Whenever a PySpark program is executed using RDDs, there is a potentially large
    overhead to execute the job. As noted in the following diagram, in the PySpark
    driver, the `Spark Context` uses `Py4j` to launch a JVM using the `JavaSparkContext`.
    Any RDD transformations are initially mapped to `PythonRDD` objects in Java.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 每当使用 RDDs 执行 PySpark 程序时，执行作业可能会有很大的开销。如以下图所示，在 PySpark 驱动程序中，`Spark Context`
    使用 `Py4j` 通过 `JavaSparkContext` 启动 JVM。任何 RDD 转换最初都映射到 Java 中的 `PythonRDD` 对象。
- en: 'Once these tasks are pushed out to the Spark Worker(s), `PythonRDD` objects
    launch Python `subprocesses` using pipes to send *both code and data* to be processed
    within Python:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这些任务被推送到 Spark Worker(s)，`PythonRDD` 对象将使用管道启动 Python `subprocesses`，以发送 *代码和数据*
    到 Python 中进行处理：
- en: '![Python to RDD communications](img/B05793_03_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![Python 到 RDD 通信](img/B05793_03_01.jpg)'
- en: While this approach allows PySpark to distribute the processing of the data
    to multiple Python subprocesses on multiple workers, as you can see, there is
    a lot of context switching and communications overhead between Python and the
    JVM.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种方法允许 PySpark 将数据处理分布到多个工作者的多个 Python 子进程中，但如您所见，Python 和 JVM 之间存在大量的上下文切换和通信开销。
- en: Note
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'An excellent resource on PySpark performance is Holden Karau''s *Improving
    PySpark Performance: Spark performance beyond the JVM*: [http://bit.ly/2bx89bn](http://bit.ly/2bx89bn).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 关于PySpark性能的优秀资源是Holden Karau的*改进PySpark性能：Spark性能超越JVM*：[http://bit.ly/2bx89bn](http://bit.ly/2bx89bn)。
- en: Catalyst Optimizer refresh
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Catalyst优化器刷新
- en: 'As noted in [Chapter 1](ch01.html "Chapter 1. Understanding Spark"), *Understanding
    Spark*, one of the primary reasons the Spark SQL engine is so fast is because
    of the **Catalyst Optimizer**. For readers with a database background, this diagram
    looks similar to the logical/physical planner and cost model/cost-based optimization
    of a **relational database management system** (**RDBMS**):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](ch01.html "第1章. 理解Spark")中所述，*理解Spark*，Spark SQL引擎之所以如此快速，其中一个主要原因是**Catalyst优化器**。对于有数据库背景的读者来说，这个图看起来与关系数据库管理系统（**RDBMS**）的逻辑/物理规划器和基于成本的优化模型/基于成本的优化类似：
- en: '![Catalyst Optimizer refresh](img/B05793_03_02.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![Catalyst优化器刷新](img/B05793_03_02.jpg)'
- en: The significance of this is that, as opposed to immediately processing the query,
    the Spark engine's Catalyst Optimizer compiles and optimizes a logical plan and
    has a cost optimizer that determines the most efficient physical plan generated.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这的重要性在于，与立即处理查询相反，Spark引擎的Catalyst优化器编译并优化一个逻辑计划，并有一个成本优化器来确定生成的最有效物理计划。
- en: Note
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: As noted in earlier chapters, while the Spark SQL Engine has both rules-based
    and cost-based optimizations that include (but are not limited to) predicate push
    down and column pruning. Targeted for the Apache Spark 2.2 release, the jira item
    *[SPARK-16026] Cost-based Optimizer Framework* at [https://issues.apache.org/jira/browse/SPARK-16026](https://issues.apache.org/jira/browse/SPARK-16026)
    is an umbrella ticket to implement a cost-based optimizer framework beyond broadcast
    join selection. For more information, please refer to the *Design Specification
    of Spark Cost-Based Optimization* at [http://bit.ly/2li1t4T](http://bit.ly/2li1t4T).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，虽然Spark SQL引擎既有基于规则的优化也有基于成本的优化，包括（但不限于）谓词下推和列剪枝。针对Apache Spark 2.2版本，jira项目*[SPARK-16026]基于成本的优化器框架*在[https://issues.apache.org/jira/browse/SPARK-16026](https://issues.apache.org/jira/browse/SPARK-16026)是一个涵盖广播连接选择之外基于成本的优化器框架的通用票据。更多信息，请参阅[http://bit.ly/2li1t4T](http://bit.ly/2li1t4T)上的*Spark基于成本优化设计规范*。
- en: As part of **Project Tungsten**, there are further improvements to performance
    by generating byte code (code generation or `codegen`) instead of interpreting
    each row of data. Find more details on Tungsten in the *Project Tungsten* section
    in [Chapter 1](ch01.html "Chapter 1. Understanding Spark"), *Understanding Spark*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作为**Project Tungsten**的一部分，通过生成字节码（代码生成或`codegen`）而不是解释每一行数据来进一步提高性能。更多关于Tungsten的详细信息，请参阅[第1章](ch01.html
    "第1章. 理解Spark")中*理解Spark*章节的*Project Tungsten*部分。
- en: 'As previously noted, the optimizer is based on functional programming constructs
    and was designed with two purposes in mind: to ease the adding of new optimization
    techniques and features to Spark SQL, and to allow external developers to extend
    the optimizer (for example, adding data-source-specific rules, support for new
    data types, and so on).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，优化器基于函数式编程结构，并设计有两个目的：简化向Spark SQL添加新的优化技术和功能，并允许外部开发者扩展优化器（例如，添加数据源特定的规则、支持新的数据类型等）。
- en: Note
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information, please refer to Michael Armbrust''s excellent presentation,
    *Structuring Spark: SQL DataFrames, Datasets, and Streaming* at [http://bit.ly/2cJ508x](http://bit.ly/2cJ508x).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参阅Michael Armbrust的优秀演示文稿，*结构化Spark：SQL DataFrames、Datasets和Streaming*：[http://bit.ly/2cJ508x](http://bit.ly/2cJ508x)。
- en: For further understanding of the *Catalyst Optimizer*, please refer to *Deep
    Dive into Spark SQL's Catalyst Optimizer* at [http://bit.ly/2bDVB1T](http://bit.ly/2bDVB1T).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要进一步了解*Catalyst优化器*，请参阅[http://bit.ly/2bDVB1T](http://bit.ly/2bDVB1T)上的*深入Spark
    SQL的Catalyst优化器*。
- en: 'Also, for more information on *Project Tungsten*, please refer to *Project
    Tungsten: Bringing Apache Spark Closer to Bare Metal* at [http://bit.ly/2bQIlKY](http://bit.ly/2bQIlKY),
    and *Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop*
    at [http://bit.ly/2bDWtnc](http://bit.ly/2bDWtnc).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，有关*Project Tungsten*的更多信息，请参阅[http://bit.ly/2bQIlKY](http://bit.ly/2bQIlKY)上的*Project
    Tungsten：将Apache Spark带到裸金属更近一步*，以及[http://bit.ly/2bDWtnc](http://bit.ly/2bDWtnc)上的*Apache
    Spark作为编译器：在笔记本电脑上每秒处理十亿行数据*。
- en: Speeding up PySpark with DataFrames
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DataFrames加速PySpark
- en: 'The significance of DataFrames and the *Catalyst Optimizer* (and *Project Tungsten*)
    is the increase in performance of PySpark queries when compared to non-optimized
    RDD queries. As shown in the following figure, prior to the introduction of DataFrames,
    Python query speeds were often twice as slow as the same Scala queries using RDD.
    Typically, this slowdown in query performance was due to the communications overhead
    between Python and the JVM:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame和*Catalyst Optimizer*（以及*Project Tungsten*）的重要性在于与未优化的RDD查询相比，PySpark查询性能的提升。如图所示，在引入DataFrame之前，Python查询速度通常比使用RDD的相同Scala查询慢两倍。通常，这种查询性能的下降是由于Python和JVM之间的通信开销：
- en: '![Speeding up PySpark with DataFrames](img/B05793_03_03.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![使用DataFrame加速PySpark](img/B05793_03_03.jpg)'
- en: 'Source: *Introducing DataFrames in Apache-spark for Large Scale Data Science*
    at [http://bit.ly/2blDBI1](http://bit.ly/2blDBI1)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：*在Apache-spark中介绍DataFrame用于大规模数据科学*，请参阅[http://bit.ly/2blDBI1](http://bit.ly/2blDBI1)
- en: With DataFrames, not only was there a significant improvement in Python performance,
    there is now performance parity between Python, Scala, SQL, and R.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DataFrame，不仅Python性能有了显著提升，现在Python、Scala、SQL和R之间的性能也实现了对等。
- en: Tip
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: It is important to note that while, with DataFrames, PySpark is often significantly
    faster, there are some exceptions. The most prominent one is the use of Python
    UDFs, which results in round-trip communication between Python and the JVM. Note,
    this would be the worst-case scenario which would be similar if the compute was
    done on RDDs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，虽然DataFrame使得PySpark通常运行得更快，但也有一些例外。最突出的是Python UDF的使用，这会导致Python和JVM之间的往返通信。注意，这将是最坏的情况，如果计算是在RDD上完成的，情况将相似。
- en: Python can take advantage of the performance optimizations in Spark even while
    the codebase for the Catalyst Optimizer is written in Scala. Basically, it is
    a Python wrapper of approximately 2,000 lines of code that allows PySpark DataFrame
    queries to be significantly faster.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 即使Catalyst Optimizer的代码库是用Scala编写的，Python也可以利用Spark的性能优化。基本上，它是一个大约2,000行代码的Python包装器，允许PySpark
    DataFrame查询显著加快。
- en: 'Altogether, Python DataFrames (as well as SQL, Scala DataFrames, and R DataFrames)
    are all able to make use of the Catalyst Optimizer (as per the following updated
    diagram):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，Python DataFrame（以及SQL、Scala DataFrame和R DataFrame）都能够利用Catalyst Optimizer（如下面的更新图所示）：
- en: '![Speeding up PySpark with DataFrames](img/B05793_03_04.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![使用DataFrame加速PySpark](img/B05793_03_04.jpg)'
- en: Note
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information, please refer to the blog post *Introducing DataFrames
    in Apache Spark for Large Scale Data Science* at [http://bit.ly/2blDBI1](http://bit.ly/2blDBI1),
    as well as Reynold Xin''s Spark Summit 2015 presentation, *From DataFrames to
    Tungsten: A Peek into Spark''s Future* at [http://bit.ly/2bQN92T](http://bit.ly/2bQN92T).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参阅博客文章*在Apache Spark中介绍DataFrame用于大规模数据科学*，请参阅[http://bit.ly/2blDBI1](http://bit.ly/2blDBI1)，以及Reynold
    Xin在Spark Summit 2015上的演讲，*从DataFrame到Tungsten：一瞥Spark的未来*，请参阅[http://bit.ly/2bQN92T](http://bit.ly/2bQN92T)。
- en: Creating DataFrames
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建DataFrame
- en: Typically, you will create DataFrames by importing data using SparkSession (or
    calling `spark` in the PySpark shell).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你将通过使用SparkSession（或在PySpark shell中调用`spark`）导入数据来创建DataFrame。
- en: Tip
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: In Spark 1.x versions, you typically had to use `sqlContext`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 1.x版本中，你通常必须使用`sqlContext`。
- en: In future chapters, we will discuss how to import data into your local file
    system, **Hadoop Distributed File System** (**HDFS**), or other cloud storage
    systems (for example, S3 or WASB). For this chapter, we will focus on generating
    your own DataFrame data directly within Spark or utilizing the data sources already
    available within Databricks Community Edition.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的章节中，我们将讨论如何将数据导入你的本地文件系统、**Hadoop分布式文件系统**（**HDFS**）或其他云存储系统（例如，S3或WASB）。对于本章，我们将专注于在Spark中直接生成自己的DataFrame数据或利用Databricks社区版中已有的数据源。
- en: Note
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For instructions on how to sign up for the Community Edition of Databricks,
    see the bonus chapter, *Free Spark Cloud Offering*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何注册Databricks社区版的说明，请参阅附录章节，*免费Spark云服务提供*。
- en: First, instead of accessing the file system, we will create a DataFrame by generating
    the data. In this case, we'll first create the `stringJSONRDD` RDD and then convert
    it into a DataFrame. This code snippet creates an RDD comprised of swimmers (their
    ID, name, age, and eye color) in JSON format.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们不会访问文件系统，而是通过生成数据来创建一个DataFrame。在这种情况下，我们首先创建`stringJSONRDD` RDD，然后将其转换为DataFrame。此代码片段创建了一个包含游泳者（他们的ID、姓名、年龄和眼睛颜色）的JSON格式的RDD。
- en: Generating our own JSON data
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成我们自己的 JSON 数据
- en: 'Below, we will generate initially generate the `stringJSONRDD` RDD:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，我们将首先生成 `stringJSONRDD` RDD：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that we have created the RDD, we will convert this into a DataFrame by using
    the SparkSession `read.json` method (that is, `spark.read.json(...)`). We will
    also create a temporary table by using the `.createOrReplaceTempView` method.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了 RDD，我们将使用 SparkSession 的 `read.json` 方法（即 `spark.read.json(...)`）将其转换为
    DataFrame。我们还将使用 `.createOrReplaceTempView` 方法创建一个临时表。
- en: Note
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In Spark 1.x, this method was`.registerTempTable`, which is being deprecated
    as part of Spark 2.x.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 1.x 中，此方法为 `.registerTempTable`，它作为 Spark 2.x 的一部分已被弃用。
- en: Creating a DataFrame
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 DataFrame
- en: 'Here is the code to create a DataFrame:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是创建 DataFrame 的代码：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Creating a temporary table
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建临时表
- en: 'Here is the code for creating a temporary table:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是创建临时表的代码：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As noted in the previous chapters, many RDD operations are transformations,
    which are not executed until an action operation is executed. For example, in
    the preceding code snippet, the `sc.parallelize` is a transformation that is executed
    when converting from an RDD to a DataFrame by using `spark.read.json`. Notice
    that, in the screenshot of this code snippet notebook (near the bottom left),
    the Spark job is not executed until the second cell containing the `spark.read.json`
    operation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，许多 RDD 操作是转换，它们只有在执行动作操作时才会执行。例如，在前面的代码片段中，`sc.parallelize` 是一个转换，它在将
    RDD 转换为 DataFrame 时执行，即使用 `spark.read.json`。注意，在这个代码片段笔记本的截图（靠近左下角）中，Spark 作业直到包含
    `spark.read.json` 操作的第二个单元格才会执行。
- en: Tip
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: These are screenshots from Databricks Community Edition, but all the code samples
    and Spark UI screenshots can be executed/viewed in any flavor of Apache Spark
    2.x.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些截图来自 Databricks Community Edition，但所有代码示例和 Spark UI 截图都可以在任何 Apache Spark
    2.x 版本中执行/查看。
- en: To further emphasize the point, in the right pane of the following figure, we
    present the DAG graph of execution.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步强调这一点，在以下图例的右侧面板中，我们展示了执行 DAG 图。
- en: Note
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: A great resource to better understand the Spark UI DAG visualization is the
    blog post *Understanding Your Apache Spark Application Through Visualization*
    at [http://bit.ly/2cSemkv](http://bit.ly/2cSemkv).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 更好地理解 Spark UI DAG 可视化的一个绝佳资源是博客文章《通过可视化理解您的 Apache Spark 应用程序》([http://bit.ly/2cSemkv](http://bit.ly/2cSemkv))。
- en: 'In the following screenshot, you can see the Spark job'' s`parallelize` operation
    is from the first cell generating the RDD `stringJSONRDD`, while the `map` and
    `mapPartitions` operations are the operations required to create the DataFrame:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下截图，你可以看到 Spark 作业的 `parallelize` 操作来自于生成 RDD `stringJSONRDD` 的第一个单元格，而 `map`
    和 `mapPartitions` 操作是创建 DataFrame 所需的操作：
- en: '![Creating a temporary table](img/B05793_03_05.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![创建临时表](img/B05793_03_05.jpg)'
- en: Spark UI of the DAG visualization of the spark.read.json(stringJSONRDD) job.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Spark UI 中 spark.read.json(stringJSONRDD) 作业的 DAG 可视化。
- en: 'In the following screenshot, you can see the *stages* for the `parallelize`
    operation are from the first cell generating the RDD `stringJSONRDD`, while the
    `map` and `mapPartitions` operations are the operations required to create the
    DataFrame:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下截图，你可以看到 `parallelize` 操作的 *阶段* 来自于生成 RDD `stringJSONRDD` 的第一个单元格，而 `map`
    和 `mapPartitions` 操作是创建 DataFrame 所需的操作：
- en: '![Creating a temporary table](img/B05793_03_06.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![创建临时表](img/B05793_03_06.jpg)'
- en: Spark UI of the DAG visualization of the stages within the spark.read.json(stringJSONRDD)
    job.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Spark UI 中 spark.read.json(stringJSONRDD) 作业的 DAG 可视化阶段。
- en: It is important to note that `parallelize`, `map`, and `mapPartitions` are all
    RDD *transformations*. Wrapped within the DataFrame operation, `spark.read.json`
    (in this case), are not only the RDD transformations, but also the *action* which
    converts the RDD into a DataFrame. This is an important call out, because even
    though you are executing DataFrame *operations*, to debug your operations you
    will need to remember that you will be making sense of *RDD operations* within
    the Spark UI.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，`parallelize`、`map` 和 `mapPartitions` 都是 RDD 转换。包裹在 DataFrame 操作（在这种情况下为
    `spark.read.json`）中的不仅仅是 RDD 转换，还包括将 RDD 转换为 DataFrame 的 *动作*。这是一个重要的说明，因为尽管你正在执行
    DataFrame *操作*，但在调试操作时，你需要记住你将需要在 Spark UI 中理解 *RDD 操作*。
- en: Note that creating the temporary table is a DataFrame transformation and not
    executed until a DataFrame action is executed (for example, in the SQL query to
    be executed in the following section).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，创建临时表是 DataFrame 转换，并且不会在执行 DataFrame 动作之前执行（例如，在下一节中要执行的 SQL 查询中）。
- en: Note
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: DataFrame transformations and actions are similar to RDD transformations and
    actions in that there is a set of operations that are lazy (transformations).
    But, in comparison to RDDs, DataFrames operations are not as lazy, primarily due
    to the Catalyst Optimizer. For more information, please refer to Holden Karau
    and Rachel Warren's book *High Performance Spark*, [http://highperformancespark.com/](http://highperformancespark.com/).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 转换和操作与 RDD 转换和操作类似，因为存在一组惰性操作（转换）。但是，与 RDD 相比，DataFrame 操作的惰性程度较低，这主要是由于
    Catalyst 优化器。有关更多信息，请参阅 Holden Karau 和 Rachel Warren 的书籍 *High Performance Spark*，[http://highperformancespark.com/](http://highperformancespark.com/)。
- en: Simple DataFrame queries
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单的 DataFrame 查询
- en: Now that you have created the `swimmersJSON` DataFrame, we will be able to run
    the DataFrame API, as well as SQL queries against it. Let's start with a simple
    query showing all the rows within the DataFrame.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经创建了 `swimmersJSON` DataFrame，我们将能够运行 DataFrame API，以及针对它的 SQL 查询。让我们从一个简单的查询开始，显示
    DataFrame 中的所有行。
- en: DataFrame API query
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrame API 查询
- en: 'To do this using the DataFrame API, you can use the `show(<n>)` method, which
    prints the first `n` rows to the console:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 DataFrame API 执行此操作，您可以使用 `show(<n>)` 方法，该方法将前 `n` 行打印到控制台：
- en: Tip
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Running the`.show()` method will default to present the first 10 rows.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 `.show()` 方法将默认显示前 10 行。
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This gives the following output:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![DataFrame API query](img/B05793_03_07.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![DataFrame API 查询](img/B05793_03_07.jpg)'
- en: SQL query
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SQL 查询
- en: 'If you prefer writing SQL statements, you can write the following query:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您更喜欢编写 SQL 语句，可以编写以下查询：
- en: '[PRE4]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will give the following output:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![SQL query](img/B05793_03_08.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![SQL 查询](img/B05793_03_08.jpg)'
- en: 'We are using the `.collect()` method, which returns all the records as a list
    of **Row** objects. Note that you can use either the `collect()` or `show()` method
    for both DataFrames and SQL queries. Just make sure that if you use `.collect()`,
    this is for a small DataFrame, since it will return all of the rows in the DataFrame
    and move them back from the executors to the driver. You can instead use `take(<n>)`
    or `show(<n>)`, which allow you to limit the number of rows returned by specifying
    `<n>`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用 `.collect()` 方法，它返回所有记录作为 **Row** 对象的列表。请注意，您可以使用 `collect()` 或 `show()`
    方法对 DataFrame 和 SQL 查询进行操作。只需确保如果您使用 `.collect()`，这仅适用于小型 DataFrame，因为它将返回 DataFrame
    中的所有行并将它们从执行器移动到驱动器。您还可以使用 `take(<n>)` 或 `show(<n>)`，这允许您通过指定 `<n>` 来限制返回的行数：
- en: Tip
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Note that, if you are using Databricks, you can use the `%sql` command and run
    your SQL statement directly within a notebook cell, as noted.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果您使用 Databricks，可以使用 `%sql` 命令并在笔记本单元中直接运行 SQL 语句，如上所述。
- en: '![SQL query](img/B05793_03_09.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![SQL 查询](img/B05793_03_09.jpg)'
- en: Interoperating with RDDs
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 RDD 互操作
- en: 'There are two different methods for converting existing RDDs to DataFrames
    (or Datasets[T]): inferring the schema using reflection, or programmatically specifying
    the schema. The former allows you to write more concise code (when your Spark
    application already knows the schema), while the latter allows you to construct
    DataFrames when the columns and their data types are only revealed at run time.
    Note, **reflection** is in reference to *schema reflection* as opposed to Python
    `reflection`.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 将现有的 RDD 转换为 DataFrame（或 Datasets[T]）有两种不同的方法：使用反射推断模式，或程序化指定模式。前者允许您编写更简洁的代码（当您的
    Spark 应用程序已经知道模式时），而后者允许您在运行时仅当列及其数据类型被揭示时构建 DataFrame。请注意，**反射**是指 *模式反射*，而不是
    Python `反射`。
- en: Inferring the schema using reflection
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用反射推断模式
- en: In the process of building the DataFrame and running the queries, we skipped
    over the fact that the schema for this DataFrame was automatically defined. Initially,
    row objects are constructed by passing a list of key/value pairs as `**kwargs`
    to the row class. Then, Spark SQL converts this RDD of row objects into a DataFrame,
    where the keys are the columns and the data types are inferred by sampling the
    data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建 DataFrame 和运行查询的过程中，我们跳过了这样一个事实，即此 DataFrame 的模式是自动定义的。最初，通过将键/值对列表作为 `**kwargs`
    传递给行类来构建行对象。然后，Spark SQL 将此行对象 RDD 转换为 DataFrame，其中键是列，数据类型通过采样数据推断。
- en: Tip
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The `**kwargs` construct allows you to pass a variable number of parameters
    to a method at runtime.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`**kwargs` 构造函数允许你在运行时传递一个可变数量的参数给一个方法。'
- en: 'Going back to the code, after initially creating the `swimmersJSON` DataFrame,
    without specifying the schema, you will notice the schema definition by using
    the `printSchema()` method:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到代码，在最初创建 `swimmersJSON` DataFrame，没有指定模式的情况下，你会注意到通过使用 `printSchema()` 方法来定义模式：
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This gives the following output:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![Inferring the schema using reflection](img/B05793_03_10.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![使用反射推断模式](img/B05793_03_10.jpg)'
- en: But what if we want to specify the schema because, in this example, we know
    that the `id` is actually a `long` instead of a `string`?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们想指定模式，因为在这个例子中我们知道 `id` 实际上是一个 `long` 而不是一个 `string` 呢？
- en: Programmatically specifying the schema
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编程指定模式
- en: 'In this case, let''s programmatically specify the schema by bringing in Spark
    SQL data types (`pyspark.sql.types`) and generate some `.csv` data for this example:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，让我们通过引入 Spark SQL 数据类型（`pyspark.sql.types`）来编程指定模式，并生成一些 `.csv` 数据：
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'First, we will encode the schema as a string, per the `[schema]` variable below.
    Then we will define the schema using `StructType` and `StructField`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将按照下面的 `[schema]` 变量将模式编码为字符串。然后我们将使用 `StructType` 和 `StructField` 定义模式：
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note, the `StructField` class is broken down in terms of:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`StructField` 类可以从以下几个方面进行分解：
- en: '`name`: The name of this field'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`: 此字段的名称'
- en: '`dataType`: The data type of this field'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataType`: 该字段的类型'
- en: '`nullable`: Indicates whether values of this field can be null'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nullable`: 指示此字段的值是否可以为 null'
- en: 'Finally, we will apply the schema (`schema`) we created to the `stringCSVRDD`
    RDD (that is, the generated`.csv` data) and create a temporary view so we can
    query it using SQL:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将我们将创建的 `schema`（模式）应用到 `stringCSVRDD` RDD（即生成的 `.csv` 数据）上，并创建一个临时视图，这样我们就可以使用
    SQL 来查询它：
- en: '[PRE8]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With this example, we have finer-grain control over the schema and can specify
    that `id` is a `long` (as opposed to a string in the previous section):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个例子，我们对模式有了更细粒度的控制，可以指定 `id` 是一个 `long`（与前文中的字符串相反）：
- en: '[PRE9]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This gives the following output:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![Programmatically specifying the schema](img/B05793_03_11.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![编程指定模式](img/B05793_03_11.jpg)'
- en: Tip
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: In many cases, the schema can be inferred (as per the previous section) and
    you do not need to specify the schema, as in this preceding example.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，模式可以被推断（如前文所述）并且你不需要指定模式，就像前一个例子中那样。
- en: Querying with the DataFrame API
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DataFrame API 查询
- en: As noted in the previous section, you can start off by using `collect()`, `show()`,
    or `take()` to view the data within your DataFrame (with the last two including
    the option to limit the number of returned rows).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如前文所述，你可以从使用 `collect()`、`show()` 或 `take()` 开始来查看 DataFrame 中的数据（后两个选项包括限制返回行数的选项）。
- en: Number of rows
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 行数
- en: 'To get the number of rows within your DataFrame, you can use the `count()`
    method:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取 DataFrame 中的行数，你可以使用 `count()` 方法：
- en: '[PRE10]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This gives the following output:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Running filter statements
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行过滤语句
- en: 'To run a filter statement, you can use the `filter` clause; in the following
    code snippet, we are using the `select` clause to specify the columns to be returned
    as well:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行一个过滤语句，你可以使用 `filter` 子句；在下面的代码片段中，我们使用 `select` 子句来指定要返回的列：
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output of this query is to choose only the `id` and `age` columns, where
    `age` = `22`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询的输出是选择 `id` 和 `age` 列，其中 `age` = `22`：
- en: '![Running filter statements](img/B05793_03_12.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![运行过滤语句](img/B05793_03_12.jpg)'
- en: 'If we only want to get back the name of the swimmers who have an eye color
    that begins with the letter `b`, we can use a SQL-like syntax, `like`, as shown
    in the following code:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只想获取那些眼睛颜色以字母 `b` 开头的游泳者的名字，我们可以使用类似 SQL 的语法，即 `like`，如下面的代码所示：
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Running filter statements](img/B05793_03_13.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![运行过滤语句](img/B05793_03_13.jpg)'
- en: Querying with SQL
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SQL 查询
- en: Let's run the same queries, except this time, we will do so using SQL queries
    against the same DataFrame. Recall that this DataFrame is accessible because we
    executed the `.createOrReplaceTempView` method for `swimmers`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行相同的查询，但这次我们将使用针对同一 DataFrame 的 SQL 查询。回想一下，这个 DataFrame 是可访问的，因为我们为 `swimmers`
    执行了 `.createOrReplaceTempView` 方法。
- en: Number of rows
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 行数
- en: 'The following is the code snippet to get the number of rows within your DataFrame
    using SQL:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段用于使用 SQL 获取 DataFrame 中的行数：
- en: '[PRE14]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Number of rows](img/B05793_03_14.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![行数](img/B05793_03_14.jpg)'
- en: Running filter statements using the where Clauses
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用where子句运行过滤语句
- en: 'To run a filter statement using SQL, you can use the `where` clause, as noted
    in the following code snippet:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用SQL运行过滤语句，您可以使用`where`子句，如下面的代码片段所示：
- en: '[PRE15]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output of this query is to choose only the `id` and `age` columns where
    `age` = `22`:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 此查询的输出是仅选择`age`等于`22`的`id`和`age`列：
- en: '![Running filter statements using the where Clauses](img/B05793_03_15.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![使用where子句运行过滤语句](img/B05793_03_15.jpg)'
- en: 'As with the DataFrame API querying, if we want to get back the name of the
    swimmers who have an eye color that begins with the letter `b` only, we can use
    the `like` syntax as well:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 与DataFrame API查询类似，如果我们只想获取具有以字母`b`开眼的游泳者的名字，我们也可以使用`like`语法：
- en: '[PRE16]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Running filter statements using the where Clauses](img/B05793_03_16.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![使用where子句运行过滤语句](img/B05793_03_16.jpg)'
- en: Tip
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: For more information, please refer to the *Spark SQL, DataFrames, and Datasets
    Guide* at [http://bit.ly/2cd1wyx](http://bit.ly/2cd1wyx).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参阅[Spark SQL、DataFrames和Datasets指南](http://bit.ly/2cd1wyx)。
- en: Note
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'An important note when working with Spark SQL and DataFrames is that while
    it is easy to work with CSV, JSON, and a variety of data formats, the most common
    storage format for Spark SQL analytics queries is the *Parquet* file format. It
    is a columnar format that is supported by many other data processing systems and
    Spark SQL supports both reading and writing Parquet files that automatically preserves
    the schema of the original data. For more information, please refer to the latest
    *Spark SQL Programming Guide > Parquet Files* at: [http://spark.apache.org/docs/latest/sql-programming-guide.html#parquet-files](http://spark.apache.org/docs/latest/sql-programming-guide.html#parquet-files).
    Also, there are many performance optimizations that pertain to Parquet, including
    (but not limited to) *Automatic Partition Discovery and Schema Migration for Parquet*
    at [https://databricks.com/blog/2015/03/24/spark-sql-graduates-from-alpha-in-spark-1-3.html](https://databricks.com/blog/2015/03/24/spark-sql-graduates-from-alpha-in-spark-1-3.html)
    and *How Apache Spark performs a fast count using the parquet metadata* at [https://github.com/dennyglee/databricks/blob/master/misc/parquet-count-metadata-explanation.md](https://github.com/dennyglee/databricks/blob/master/misc/parquet-count-metadata-explanation.md).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Spark SQL和DataFrames时，一个重要的注意事项是，虽然处理CSV、JSON和多种数据格式都很方便，但Spark SQL分析查询最常用的存储格式是*Parquet*文件格式。它是一种列式格式，被许多其他数据处理系统支持，并且Spark
    SQL支持读取和写入Parquet文件，自动保留原始数据的模式。更多信息，请参阅最新的*Spark SQL编程指南 > Parquet文件*，链接为：[http://spark.apache.org/docs/latest/sql-programming-guide.html#parquet-files](http://spark.apache.org/docs/latest/sql-programming-guide.html#parquet-files)。此外，还有许多与Parquet相关的性能优化，包括但不限于[Parquet的自动分区发现和模式迁移](https://databricks.com/blog/2015/03/24/spark-sql-graduates-from-alpha-in-spark-1-3.html)和[Apache
    Spark如何使用Parquet元数据快速计数](https://github.com/dennyglee/databricks/blob/master/misc/parquet-count-metadata-explanation.md)。
- en: DataFrame scenario – on-time flight performance
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrame场景 – 准时飞行性能
- en: 'To showcase the types of queries you can do with DataFrames, let''s look at
    the use case of on-time flight performance. We will analyze the *Airline On-Time
    Performance and Causes of Flight Delays: On-Time Data* ([http://bit.ly/2ccJPPM](http://bit.ly/2ccJPPM)),
    and join this with the airports dataset, obtained from the *Open Flights Airport,
    airline, and route data* ([http://bit.ly/2ccK5hw](http://bit.ly/2ccK5hw)), to
    better understand the variables associated with flight delays.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示您可以使用DataFrames执行的查询类型，让我们看看准时飞行性能的使用案例。我们将分析*航空公司准时性能和航班延误原因：准时数据*([http://bit.ly/2ccJPPM](http://bit.ly/2ccJPPM))，并将其与从*Open
    Flights机场、航空公司和航线数据*([http://bit.ly/2ccK5hw](http://bit.ly/2ccK5hw))获得的机场数据集合并，以更好地理解与航班延误相关的变量。
- en: Tip
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: For this section, we will be using Databricks Community Edition (a free offering
    of the Databricks product), which you can get at [https://databricks.com/try-databricks](https://databricks.com/try-databricks).
    We will be using visualizations and pre-loaded datasets within Databricks to make
    it easier for you to focus on writing the code and analyzing the results.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 Databricks Community Edition（Databricks 产品的免费提供），您可以在[https://databricks.com/try-databricks](https://databricks.com/try-databricks)获取。我们将使用
    Databricks 内部的可视化和预加载数据集，以便您更容易专注于编写代码和分析结果。
- en: If you would prefer to run this on your own environment, you can find the datasets
    available in our GitHub repository for this book at [https://github.com/drabastomek/learningPySpark](https://github.com/drabastomek/learningPySpark).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望在自己的环境中运行此操作，您可以在本书的 GitHub 仓库中找到可用的数据集，网址为 [https://github.com/drabastomek/learningPySpark](https://github.com/drabastomek/learningPySpark)。
- en: Preparing the source datasets
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备源数据集
- en: 'We will first process the source airports and flight performance datasets by
    specifying their file path location and importing them using SparkSession:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先通过指定文件路径位置并使用 SparkSession 导入来处理源机场和飞行性能数据集：
- en: '[PRE17]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that we're importing the data using the CSV reader (`com.databricks.spark.csv`),
    which works for any specified delimiter (note that the airports data is tab-delimited,
    while the flight performance data is comma-delimited). Finally, we cache the flight
    dataset so subsequent queries will be faster.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用 CSV 读取器（`com.databricks.spark.csv`）导入数据，它适用于任何指定的分隔符（注意，机场数据是制表符分隔的，而飞行性能数据是逗号分隔的）。最后，我们缓存飞行数据集，以便后续查询更快。
- en: Joining flight performance and airports
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接飞行性能和机场
- en: 'One of the more common tasks with DataFrames/SQL is to join two different datasets;
    it is often one of the more demanding operations (from a performance perspective).
    With DataFrames, a lot of the performance optimizations for these joins are included
    by default:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DataFrame/SQL 的更常见任务之一是将两个不同的数据集连接起来；这通常是一项性能要求较高的操作。使用 DataFrame，这些连接的性能优化默认情况下已经包括在内：
- en: '[PRE18]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In our scenario, we are querying the total delays by city and origin code for
    the state of Washington. This will require joining the flight performance data
    with the airports data by **International Air Transport Association** (**IATA**)
    code. The output of the query is as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的场景中，我们正在查询华盛顿州的按城市和起始代码的总延误。这需要通过**国际航空运输协会**（**IATA**）代码将飞行性能数据与机场数据连接起来。查询的输出如下：
- en: '![Joining flight performance and airports](img/B05793_03_17.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![连接飞行性能和机场](img/B05793_03_17.jpg)'
- en: 'Using notebooks (such as Databricks, iPython, Jupyter, and Apache Zeppelin),
    you can more easily execute and visualize your queries. In the following examples,
    we will be using the Databricks notebook. Within our Python notebook, we can use
    the `%sql` function to execute SQL statements within that notebook cell:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用笔记本（如 Databricks、iPython、Jupyter 和 Apache Zeppelin），您可以更轻松地执行和可视化您的查询。在以下示例中，我们将使用
    Databricks 笔记本。在我们的 Python 笔记本中，我们可以使用 `%sql` 函数在该笔记本单元格中执行 SQL 语句：
- en: '[PRE19]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This is the same as the previous query, but due to formatting, easier to read.
    In our Databricks notebook example, we can quickly visualize this data into a
    bar chart:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这与之前的查询相同，但由于格式化，更容易阅读。在我们的 Databricks 笔记本示例中，我们可以快速将此数据可视化成条形图：
- en: '![Joining flight performance and airports](img/B05793_03_18.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![连接飞行性能和机场](img/B05793_03_18.jpg)'
- en: Visualizing our flight-performance data
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化我们的飞行性能数据
- en: 'Let''s continue visualizing our data, but broken down by all states in the
    continental US:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续可视化我们的数据，但按美国大陆的所有州进行细分：
- en: '[PRE20]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output bar chart is as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的条形图如下：
- en: '![Visualizing our flight-performance data](img/B05793_03_19.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![可视化我们的飞行性能数据](img/B05793_03_19.jpg)'
- en: 'But, it would be cooler to view this data as a map; click on the bar chart
    icon at the bottom-left of the chart, and you can choose from many different native
    navigations, including a map:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，将此数据作为地图查看会更酷；点击图表左下角的条形图图标，您可以选择许多不同的原生导航，包括地图：
- en: '![Visualizing our flight-performance data](img/B05793_03_20.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![可视化我们的飞行性能数据](img/B05793_03_20.jpg)'
- en: One of the key benefits of DataFrames is that the information is structured
    similar to a table. Therefore, whether you are using notebooks or your favorite
    BI tool, you will be able to quickly visualize your data.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 的一个关键好处是信息结构类似于表格。因此，无论您是使用笔记本还是您喜欢的 BI 工具，您都将能够快速可视化您的数据。
- en: Tip
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: You can find the full list of `pyspark.sql.DataFrame` methods at [http://bit.ly/2bkUGnT](http://bit.ly/2bkUGnT).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [http://bit.ly/2bkUGnT](http://bit.ly/2bkUGnT) 找到 `pyspark.sql.DataFrame`
    方法的完整列表。
- en: You can find the full list of `pyspark.sql.functions` at [http://bit.ly/2bTAzLT](http://bit.ly/2bTAzLT).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [http://bit.ly/2bTAzLT](http://bit.ly/2bTAzLT) 找到 `pyspark.sql.functions`
    的完整列表。
- en: Spark Dataset API
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Dataset API
- en: 'After this discussion about Spark DataFrames, let''s have a quick recap of
    the Spark Dataset API. Introduced in Apache Spark 1.6, the goal of Spark Datasets
    was to provide an API that allows users to easily express transformations on domain
    objects, while also providing the performance and benefits of the robust Spark
    SQL execution engine. As part of the Spark 2.0 release (and as noted in the diagram
    below), the DataFrame APIs is merged into the Dataset API thus unifying data processing
    capabilities across all libraries. Because of this unification, developers now
    have fewer concepts to learn or remember, and work with a single high-level and
    *type-safe* API – called Dataset:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在关于 Spark DataFrames 的讨论之后，让我们快速回顾一下 Spark Dataset API。Apache Spark 1.6 中引入的
    Spark Datasets 的目标是提供一个 API，使用户能够轻松地表达对域对象的转换，同时提供强大 Spark SQL 执行引擎的性能和优势。作为 Spark
    2.0 版本发布的一部分（如以下图所示），DataFrame API 被合并到 Dataset API 中，从而统一了所有库的数据处理能力。由于这种统一，开发者现在需要学习或记住的概念更少，并且可以使用一个单一的高级和
    *类型安全* API —— 被称为 Dataset：
- en: '![Spark Dataset API](img/B05793_03_21.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![Spark Dataset API](img/B05793_03_21.jpg)'
- en: 'Conceptually, the Spark DataFrame is an *alias* for a collection of generic
    objects Dataset[Row], where a Row is a generic *untyped* JVM object. Dataset,
    by contrast, is a collection of *strongly-typed* JVM objects, dictated by a case
    class you define, in Scala or Java. This last point is particularly important
    as this means that the Dataset API is *not supported* by PySpark due to the lack
    of benefit from the type enhancements. Note, for the parts of the Dataset API
    that are not available in PySpark, they can be accessed by converting to an RDD
    or by using UDFs. For more information, please refer to the jira [SPARK-13233]:
    Python Dataset at [http://bit.ly/2dbfoFT](http://bit.ly/2dbfoFT).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，Spark DataFrame 是一个 Dataset[Row] 集合的 *别名*，其中 Row 是一个通用的 *未类型化* JVM 对象。相比之下，Dataset
    是一个由您在 Scala 或 Java 中定义的 case 类决定的 *强类型化* JVM 对象集合。这一点尤其重要，因为这意味着 Dataset API
    由于缺乏类型增强的好处，*不支持* PySpark。注意，对于 Dataset API 中 PySpark 中不可用的部分，可以通过转换为 RDD 或使用
    UDFs 来访问。有关更多信息，请参阅 jira [SPARK-13233]：Python Dataset 在 [http://bit.ly/2dbfoFT](http://bit.ly/2dbfoFT)。
- en: Summary
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: With Spark DataFrames, Python developers can make use of a simpler abstraction
    layer that is also potentially significantly faster. One of the main reasons Python
    is initially slower within Spark is due to the communication layer between Python
    sub-processes and the JVM. For Python DataFrame users, we have a Python wrapper
    around Scala DataFrames that avoids the Python sub-process/JVM communication overhead.
    Spark DataFrames has many performance enhancements through the Catalyst Optimizer
    and Project Tungsten which we have reviewed in this chapter. In this chapter,
    we also reviewed how to work with Spark DataFrames and worked on an on-time flight
    performance scenario using DataFrames.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Spark DataFrames，Python 开发者可以利用一个更简单的抽象层，这个层也可能会显著更快。Python 在 Spark 中最初较慢的一个主要原因是
    Python 子进程和 JVM 之间的通信层。对于 Python DataFrame 用户，我们提供了一个围绕 Scala DataFrames 的 Python
    包装器，它避免了 Python 子进程/JVM 通信开销。Spark DataFrames 通过 Catalyst 优化器和 Project Tungsten
    实现了许多性能提升，这些我们在本章中已进行了回顾。在本章中，我们还回顾了如何使用 Spark DataFrames，并使用 DataFrames 实现了一个准时航班性能场景。
- en: In this chapter, we created and worked with DataFrames by generating the data
    or making use of existing datasets.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过生成数据或利用现有数据集创建了 DataFrame 并与之工作。
- en: In the next chapter, we will discuss how to transform and understand your own
    data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论如何转换和理解您自己的数据。
