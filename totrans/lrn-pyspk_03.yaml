- en: Chapter 3. DataFrames
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A DataFrame is an immutable distributed collection of data that is organized
    into named columns analogous to a table in a relational database. Introduced as
    an experimental feature within Apache Spark 1.0 as `SchemaRDD`, they were renamed
    to `DataFrames` as part of the Apache Spark 1.3 release. For readers who are familiar
    with Python Pandas `DataFrame` or R `DataFrame`, a Spark DataFrame is a similar
    concept in that it allows users to easily work with structured data (for example,
    data tables); there are some differences as well so please temper your expectations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: By imposing a structure onto a distributed collection of data, this allows Spark
    users to query structured data in Spark SQL or using expression methods (instead
    of lambdas). In this chapter, we will include code samples using both methods.
    By structuring your data, this allows the Apache Spark engine – specifically,
    the Catalyst Optimizer – to significantly improve the performance of Spark queries.
    In earlier APIs of Spark (that is, RDDs), executing queries in Python could be
    significantly slower due to communication overhead between the Java JVM and Py4J.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you are familiar with working with DataFrames in previous versions of Spark
    (that is Spark 1.x), you will notice that in Spark 2.0 we are using SparkSession
    instead of `SQLContext`. The various Spark contexts: `HiveContext`, `SQLContext`,
    `StreamingContext`, and `SparkContext` have merged together in SparkSession. This
    way you will be working with this session only as an entry point for reading data,
    working with metadata, configuration, and cluster resource management.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: For more information, please refer to *How to use SparkSession in Apache Spark
    2.0*([http://bit.ly/2br0Fr1](http://bit.ly/2br0Fr1)).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Python to RDD communications
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A quick refresh of Spark's Catalyst Optimizer
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speeding up PySpark with DataFrames
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating DataFrames
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple DataFrame queries
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interoperating with RDDs
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying with the DataFrame API
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying with Spark SQL
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using DataFrames for an on-time flight performance
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python to RDD communications
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whenever a PySpark program is executed using RDDs, there is a potentially large
    overhead to execute the job. As noted in the following diagram, in the PySpark
    driver, the `Spark Context` uses `Py4j` to launch a JVM using the `JavaSparkContext`.
    Any RDD transformations are initially mapped to `PythonRDD` objects in Java.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'Once these tasks are pushed out to the Spark Worker(s), `PythonRDD` objects
    launch Python `subprocesses` using pipes to send *both code and data* to be processed
    within Python:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![Python to RDD communications](img/B05793_03_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: While this approach allows PySpark to distribute the processing of the data
    to multiple Python subprocesses on multiple workers, as you can see, there is
    a lot of context switching and communications overhead between Python and the
    JVM.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An excellent resource on PySpark performance is Holden Karau''s *Improving
    PySpark Performance: Spark performance beyond the JVM*: [http://bit.ly/2bx89bn](http://bit.ly/2bx89bn).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Catalyst Optimizer refresh
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As noted in [Chapter 1](ch01.html "Chapter 1. Understanding Spark"), *Understanding
    Spark*, one of the primary reasons the Spark SQL engine is so fast is because
    of the **Catalyst Optimizer**. For readers with a database background, this diagram
    looks similar to the logical/physical planner and cost model/cost-based optimization
    of a **relational database management system** (**RDBMS**):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![Catalyst Optimizer refresh](img/B05793_03_02.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: The significance of this is that, as opposed to immediately processing the query,
    the Spark engine's Catalyst Optimizer compiles and optimizes a logical plan and
    has a cost optimizer that determines the most efficient physical plan generated.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As noted in earlier chapters, while the Spark SQL Engine has both rules-based
    and cost-based optimizations that include (but are not limited to) predicate push
    down and column pruning. Targeted for the Apache Spark 2.2 release, the jira item
    *[SPARK-16026] Cost-based Optimizer Framework* at [https://issues.apache.org/jira/browse/SPARK-16026](https://issues.apache.org/jira/browse/SPARK-16026)
    is an umbrella ticket to implement a cost-based optimizer framework beyond broadcast
    join selection. For more information, please refer to the *Design Specification
    of Spark Cost-Based Optimization* at [http://bit.ly/2li1t4T](http://bit.ly/2li1t4T).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: As part of **Project Tungsten**, there are further improvements to performance
    by generating byte code (code generation or `codegen`) instead of interpreting
    each row of data. Find more details on Tungsten in the *Project Tungsten* section
    in [Chapter 1](ch01.html "Chapter 1. Understanding Spark"), *Understanding Spark*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'As previously noted, the optimizer is based on functional programming constructs
    and was designed with two purposes in mind: to ease the adding of new optimization
    techniques and features to Spark SQL, and to allow external developers to extend
    the optimizer (for example, adding data-source-specific rules, support for new
    data types, and so on).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For more information, please refer to Michael Armbrust''s excellent presentation,
    *Structuring Spark: SQL DataFrames, Datasets, and Streaming* at [http://bit.ly/2cJ508x](http://bit.ly/2cJ508x).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: For further understanding of the *Catalyst Optimizer*, please refer to *Deep
    Dive into Spark SQL's Catalyst Optimizer* at [http://bit.ly/2bDVB1T](http://bit.ly/2bDVB1T).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, for more information on *Project Tungsten*, please refer to *Project
    Tungsten: Bringing Apache Spark Closer to Bare Metal* at [http://bit.ly/2bQIlKY](http://bit.ly/2bQIlKY),
    and *Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop*
    at [http://bit.ly/2bDWtnc](http://bit.ly/2bDWtnc).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Speeding up PySpark with DataFrames
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The significance of DataFrames and the *Catalyst Optimizer* (and *Project Tungsten*)
    is the increase in performance of PySpark queries when compared to non-optimized
    RDD queries. As shown in the following figure, prior to the introduction of DataFrames,
    Python query speeds were often twice as slow as the same Scala queries using RDD.
    Typically, this slowdown in query performance was due to the communications overhead
    between Python and the JVM:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![Speeding up PySpark with DataFrames](img/B05793_03_03.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: 'Source: *Introducing DataFrames in Apache-spark for Large Scale Data Science*
    at [http://bit.ly/2blDBI1](http://bit.ly/2blDBI1)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: With DataFrames, not only was there a significant improvement in Python performance,
    there is now performance parity between Python, Scala, SQL, and R.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is important to note that while, with DataFrames, PySpark is often significantly
    faster, there are some exceptions. The most prominent one is the use of Python
    UDFs, which results in round-trip communication between Python and the JVM. Note,
    this would be the worst-case scenario which would be similar if the compute was
    done on RDDs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Python can take advantage of the performance optimizations in Spark even while
    the codebase for the Catalyst Optimizer is written in Scala. Basically, it is
    a Python wrapper of approximately 2,000 lines of code that allows PySpark DataFrame
    queries to be significantly faster.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Altogether, Python DataFrames (as well as SQL, Scala DataFrames, and R DataFrames)
    are all able to make use of the Catalyst Optimizer (as per the following updated
    diagram):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![Speeding up PySpark with DataFrames](img/B05793_03_04.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: Note
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For more information, please refer to the blog post *Introducing DataFrames
    in Apache Spark for Large Scale Data Science* at [http://bit.ly/2blDBI1](http://bit.ly/2blDBI1),
    as well as Reynold Xin''s Spark Summit 2015 presentation, *From DataFrames to
    Tungsten: A Peek into Spark''s Future* at [http://bit.ly/2bQN92T](http://bit.ly/2bQN92T).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Creating DataFrames
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Typically, you will create DataFrames by importing data using SparkSession (or
    calling `spark` in the PySpark shell).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Spark 1.x versions, you typically had to use `sqlContext`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: In future chapters, we will discuss how to import data into your local file
    system, **Hadoop Distributed File System** (**HDFS**), or other cloud storage
    systems (for example, S3 or WASB). For this chapter, we will focus on generating
    your own DataFrame data directly within Spark or utilizing the data sources already
    available within Databricks Community Edition.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For instructions on how to sign up for the Community Edition of Databricks,
    see the bonus chapter, *Free Spark Cloud Offering*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: First, instead of accessing the file system, we will create a DataFrame by generating
    the data. In this case, we'll first create the `stringJSONRDD` RDD and then convert
    it into a DataFrame. This code snippet creates an RDD comprised of swimmers (their
    ID, name, age, and eye color) in JSON format.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Generating our own JSON data
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成我们自己的 JSON 数据
- en: 'Below, we will generate initially generate the `stringJSONRDD` RDD:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，我们将首先生成 `stringJSONRDD` RDD：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that we have created the RDD, we will convert this into a DataFrame by using
    the SparkSession `read.json` method (that is, `spark.read.json(...)`). We will
    also create a temporary table by using the `.createOrReplaceTempView` method.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了 RDD，我们将使用 SparkSession 的 `read.json` 方法（即 `spark.read.json(...)`）将其转换为
    DataFrame。我们还将使用 `.createOrReplaceTempView` 方法创建一个临时表。
- en: Note
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In Spark 1.x, this method was`.registerTempTable`, which is being deprecated
    as part of Spark 2.x.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 1.x 中，此方法为 `.registerTempTable`，它作为 Spark 2.x 的一部分已被弃用。
- en: Creating a DataFrame
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 DataFrame
- en: 'Here is the code to create a DataFrame:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是创建 DataFrame 的代码：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Creating a temporary table
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建临时表
- en: 'Here is the code for creating a temporary table:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是创建临时表的代码：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As noted in the previous chapters, many RDD operations are transformations,
    which are not executed until an action operation is executed. For example, in
    the preceding code snippet, the `sc.parallelize` is a transformation that is executed
    when converting from an RDD to a DataFrame by using `spark.read.json`. Notice
    that, in the screenshot of this code snippet notebook (near the bottom left),
    the Spark job is not executed until the second cell containing the `spark.read.json`
    operation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，许多 RDD 操作是转换，它们只有在执行动作操作时才会执行。例如，在前面的代码片段中，`sc.parallelize` 是一个转换，它在将
    RDD 转换为 DataFrame 时执行，即使用 `spark.read.json`。注意，在这个代码片段笔记本的截图（靠近左下角）中，Spark 作业直到包含
    `spark.read.json` 操作的第二个单元格才会执行。
- en: Tip
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: These are screenshots from Databricks Community Edition, but all the code samples
    and Spark UI screenshots can be executed/viewed in any flavor of Apache Spark
    2.x.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些截图来自 Databricks Community Edition，但所有代码示例和 Spark UI 截图都可以在任何 Apache Spark
    2.x 版本中执行/查看。
- en: To further emphasize the point, in the right pane of the following figure, we
    present the DAG graph of execution.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步强调这一点，在以下图例的右侧面板中，我们展示了执行 DAG 图。
- en: Note
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: A great resource to better understand the Spark UI DAG visualization is the
    blog post *Understanding Your Apache Spark Application Through Visualization*
    at [http://bit.ly/2cSemkv](http://bit.ly/2cSemkv).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 更好地理解 Spark UI DAG 可视化的一个绝佳资源是博客文章《通过可视化理解您的 Apache Spark 应用程序》([http://bit.ly/2cSemkv](http://bit.ly/2cSemkv))。
- en: 'In the following screenshot, you can see the Spark job'' s`parallelize` operation
    is from the first cell generating the RDD `stringJSONRDD`, while the `map` and
    `mapPartitions` operations are the operations required to create the DataFrame:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下截图，你可以看到 Spark 作业的 `parallelize` 操作来自于生成 RDD `stringJSONRDD` 的第一个单元格，而 `map`
    和 `mapPartitions` 操作是创建 DataFrame 所需的操作：
- en: '![Creating a temporary table](img/B05793_03_05.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![创建临时表](img/B05793_03_05.jpg)'
- en: Spark UI of the DAG visualization of the spark.read.json(stringJSONRDD) job.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Spark UI 中 spark.read.json(stringJSONRDD) 作业的 DAG 可视化。
- en: 'In the following screenshot, you can see the *stages* for the `parallelize`
    operation are from the first cell generating the RDD `stringJSONRDD`, while the
    `map` and `mapPartitions` operations are the operations required to create the
    DataFrame:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下截图，你可以看到 `parallelize` 操作的 *阶段* 来自于生成 RDD `stringJSONRDD` 的第一个单元格，而 `map`
    和 `mapPartitions` 操作是创建 DataFrame 所需的操作：
- en: '![Creating a temporary table](img/B05793_03_06.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![创建临时表](img/B05793_03_06.jpg)'
- en: Spark UI of the DAG visualization of the stages within the spark.read.json(stringJSONRDD)
    job.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Spark UI 中 spark.read.json(stringJSONRDD) 作业的 DAG 可视化阶段。
- en: It is important to note that `parallelize`, `map`, and `mapPartitions` are all
    RDD *transformations*. Wrapped within the DataFrame operation, `spark.read.json`
    (in this case), are not only the RDD transformations, but also the *action* which
    converts the RDD into a DataFrame. This is an important call out, because even
    though you are executing DataFrame *operations*, to debug your operations you
    will need to remember that you will be making sense of *RDD operations* within
    the Spark UI.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，`parallelize`、`map` 和 `mapPartitions` 都是 RDD 转换。包裹在 DataFrame 操作（在这种情况下为
    `spark.read.json`）中的不仅仅是 RDD 转换，还包括将 RDD 转换为 DataFrame 的 *动作*。这是一个重要的说明，因为尽管你正在执行
    DataFrame *操作*，但在调试操作时，你需要记住你将需要在 Spark UI 中理解 *RDD 操作*。
- en: Note that creating the temporary table is a DataFrame transformation and not
    executed until a DataFrame action is executed (for example, in the SQL query to
    be executed in the following section).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DataFrame transformations and actions are similar to RDD transformations and
    actions in that there is a set of operations that are lazy (transformations).
    But, in comparison to RDDs, DataFrames operations are not as lazy, primarily due
    to the Catalyst Optimizer. For more information, please refer to Holden Karau
    and Rachel Warren's book *High Performance Spark*, [http://highperformancespark.com/](http://highperformancespark.com/).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Simple DataFrame queries
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have created the `swimmersJSON` DataFrame, we will be able to run
    the DataFrame API, as well as SQL queries against it. Let's start with a simple
    query showing all the rows within the DataFrame.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame API query
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To do this using the DataFrame API, you can use the `show(<n>)` method, which
    prints the first `n` rows to the console:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Running the`.show()` method will default to present the first 10 rows.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This gives the following output:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![DataFrame API query](img/B05793_03_07.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: SQL query
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you prefer writing SQL statements, you can write the following query:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will give the following output:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![SQL query](img/B05793_03_08.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: 'We are using the `.collect()` method, which returns all the records as a list
    of **Row** objects. Note that you can use either the `collect()` or `show()` method
    for both DataFrames and SQL queries. Just make sure that if you use `.collect()`,
    this is for a small DataFrame, since it will return all of the rows in the DataFrame
    and move them back from the executors to the driver. You can instead use `take(<n>)`
    or `show(<n>)`, which allow you to limit the number of rows returned by specifying
    `<n>`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that, if you are using Databricks, you can use the `%sql` command and run
    your SQL statement directly within a notebook cell, as noted.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![SQL query](img/B05793_03_09.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: Interoperating with RDDs
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two different methods for converting existing RDDs to DataFrames
    (or Datasets[T]): inferring the schema using reflection, or programmatically specifying
    the schema. The former allows you to write more concise code (when your Spark
    application already knows the schema), while the latter allows you to construct
    DataFrames when the columns and their data types are only revealed at run time.
    Note, **reflection** is in reference to *schema reflection* as opposed to Python
    `reflection`.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Inferring the schema using reflection
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the process of building the DataFrame and running the queries, we skipped
    over the fact that the schema for this DataFrame was automatically defined. Initially,
    row objects are constructed by passing a list of key/value pairs as `**kwargs`
    to the row class. Then, Spark SQL converts this RDD of row objects into a DataFrame,
    where the keys are the columns and the data types are inferred by sampling the
    data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `**kwargs` construct allows you to pass a variable number of parameters
    to a method at runtime.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to the code, after initially creating the `swimmersJSON` DataFrame,
    without specifying the schema, you will notice the schema definition by using
    the `printSchema()` method:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This gives the following output:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![Inferring the schema using reflection](img/B05793_03_10.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: But what if we want to specify the schema because, in this example, we know
    that the `id` is actually a `long` instead of a `string`?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Programmatically specifying the schema
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this case, let''s programmatically specify the schema by bringing in Spark
    SQL data types (`pyspark.sql.types`) and generate some `.csv` data for this example:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'First, we will encode the schema as a string, per the `[schema]` variable below.
    Then we will define the schema using `StructType` and `StructField`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note, the `StructField` class is broken down in terms of:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '`name`: The name of this field'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataType`: The data type of this field'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nullable`: Indicates whether values of this field can be null'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we will apply the schema (`schema`) we created to the `stringCSVRDD`
    RDD (that is, the generated`.csv` data) and create a temporary view so we can
    query it using SQL:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With this example, we have finer-grain control over the schema and can specify
    that `id` is a `long` (as opposed to a string in the previous section):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This gives the following output:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '![Programmatically specifying the schema](img/B05793_03_11.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
- en: Tip
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In many cases, the schema can be inferred (as per the previous section) and
    you do not need to specify the schema, as in this preceding example.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Querying with the DataFrame API
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As noted in the previous section, you can start off by using `collect()`, `show()`,
    or `take()` to view the data within your DataFrame (with the last two including
    the option to limit the number of returned rows).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Number of rows
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get the number of rows within your DataFrame, you can use the `count()`
    method:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This gives the following output:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Running filter statements
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run a filter statement, you can use the `filter` clause; in the following
    code snippet, we are using the `select` clause to specify the columns to be returned
    as well:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output of this query is to choose only the `id` and `age` columns, where
    `age` = `22`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![Running filter statements](img/B05793_03_12.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: 'If we only want to get back the name of the swimmers who have an eye color
    that begins with the letter `b`, we can use a SQL-like syntax, `like`, as shown
    in the following code:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '![Running filter statements](img/B05793_03_13.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
- en: Querying with SQL
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's run the same queries, except this time, we will do so using SQL queries
    against the same DataFrame. Recall that this DataFrame is accessible because we
    executed the `.createOrReplaceTempView` method for `swimmers`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Number of rows
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following is the code snippet to get the number of rows within your DataFrame
    using SQL:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![Number of rows](img/B05793_03_14.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: Running filter statements using the where Clauses
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run a filter statement using SQL, you can use the `where` clause, as noted
    in the following code snippet:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output of this query is to choose only the `id` and `age` columns where
    `age` = `22`:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![Running filter statements using the where Clauses](img/B05793_03_15.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: 'As with the DataFrame API querying, if we want to get back the name of the
    swimmers who have an eye color that begins with the letter `b` only, we can use
    the `like` syntax as well:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![Running filter statements using the where Clauses](img/B05793_03_16.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: Tip
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information, please refer to the *Spark SQL, DataFrames, and Datasets
    Guide* at [http://bit.ly/2cd1wyx](http://bit.ly/2cd1wyx).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An important note when working with Spark SQL and DataFrames is that while
    it is easy to work with CSV, JSON, and a variety of data formats, the most common
    storage format for Spark SQL analytics queries is the *Parquet* file format. It
    is a columnar format that is supported by many other data processing systems and
    Spark SQL supports both reading and writing Parquet files that automatically preserves
    the schema of the original data. For more information, please refer to the latest
    *Spark SQL Programming Guide > Parquet Files* at: [http://spark.apache.org/docs/latest/sql-programming-guide.html#parquet-files](http://spark.apache.org/docs/latest/sql-programming-guide.html#parquet-files).
    Also, there are many performance optimizations that pertain to Parquet, including
    (but not limited to) *Automatic Partition Discovery and Schema Migration for Parquet*
    at [https://databricks.com/blog/2015/03/24/spark-sql-graduates-from-alpha-in-spark-1-3.html](https://databricks.com/blog/2015/03/24/spark-sql-graduates-from-alpha-in-spark-1-3.html)
    and *How Apache Spark performs a fast count using the parquet metadata* at [https://github.com/dennyglee/databricks/blob/master/misc/parquet-count-metadata-explanation.md](https://github.com/dennyglee/databricks/blob/master/misc/parquet-count-metadata-explanation.md).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame scenario – on-time flight performance
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To showcase the types of queries you can do with DataFrames, let''s look at
    the use case of on-time flight performance. We will analyze the *Airline On-Time
    Performance and Causes of Flight Delays: On-Time Data* ([http://bit.ly/2ccJPPM](http://bit.ly/2ccJPPM)),
    and join this with the airports dataset, obtained from the *Open Flights Airport,
    airline, and route data* ([http://bit.ly/2ccK5hw](http://bit.ly/2ccK5hw)), to
    better understand the variables associated with flight delays.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For this section, we will be using Databricks Community Edition (a free offering
    of the Databricks product), which you can get at [https://databricks.com/try-databricks](https://databricks.com/try-databricks).
    We will be using visualizations and pre-loaded datasets within Databricks to make
    it easier for you to focus on writing the code and analyzing the results.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: If you would prefer to run this on your own environment, you can find the datasets
    available in our GitHub repository for this book at [https://github.com/drabastomek/learningPySpark](https://github.com/drabastomek/learningPySpark).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the source datasets
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will first process the source airports and flight performance datasets by
    specifying their file path location and importing them using SparkSession:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that we're importing the data using the CSV reader (`com.databricks.spark.csv`),
    which works for any specified delimiter (note that the airports data is tab-delimited,
    while the flight performance data is comma-delimited). Finally, we cache the flight
    dataset so subsequent queries will be faster.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Joining flight performance and airports
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the more common tasks with DataFrames/SQL is to join two different datasets;
    it is often one of the more demanding operations (from a performance perspective).
    With DataFrames, a lot of the performance optimizations for these joins are included
    by default:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In our scenario, we are querying the total delays by city and origin code for
    the state of Washington. This will require joining the flight performance data
    with the airports data by **International Air Transport Association** (**IATA**)
    code. The output of the query is as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![Joining flight performance and airports](img/B05793_03_17.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: 'Using notebooks (such as Databricks, iPython, Jupyter, and Apache Zeppelin),
    you can more easily execute and visualize your queries. In the following examples,
    we will be using the Databricks notebook. Within our Python notebook, we can use
    the `%sql` function to execute SQL statements within that notebook cell:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This is the same as the previous query, but due to formatting, easier to read.
    In our Databricks notebook example, we can quickly visualize this data into a
    bar chart:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![Joining flight performance and airports](img/B05793_03_18.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: Visualizing our flight-performance data
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s continue visualizing our data, but broken down by all states in the
    continental US:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output bar chart is as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing our flight-performance data](img/B05793_03_19.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: 'But, it would be cooler to view this data as a map; click on the bar chart
    icon at the bottom-left of the chart, and you can choose from many different native
    navigations, including a map:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing our flight-performance data](img/B05793_03_20.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: One of the key benefits of DataFrames is that the information is structured
    similar to a table. Therefore, whether you are using notebooks or your favorite
    BI tool, you will be able to quickly visualize your data.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can find the full list of `pyspark.sql.DataFrame` methods at [http://bit.ly/2bkUGnT](http://bit.ly/2bkUGnT).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [http://bit.ly/2bkUGnT](http://bit.ly/2bkUGnT) 找到 `pyspark.sql.DataFrame`
    方法的完整列表。
- en: You can find the full list of `pyspark.sql.functions` at [http://bit.ly/2bTAzLT](http://bit.ly/2bTAzLT).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [http://bit.ly/2bTAzLT](http://bit.ly/2bTAzLT) 找到 `pyspark.sql.functions`
    的完整列表。
- en: Spark Dataset API
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Dataset API
- en: 'After this discussion about Spark DataFrames, let''s have a quick recap of
    the Spark Dataset API. Introduced in Apache Spark 1.6, the goal of Spark Datasets
    was to provide an API that allows users to easily express transformations on domain
    objects, while also providing the performance and benefits of the robust Spark
    SQL execution engine. As part of the Spark 2.0 release (and as noted in the diagram
    below), the DataFrame APIs is merged into the Dataset API thus unifying data processing
    capabilities across all libraries. Because of this unification, developers now
    have fewer concepts to learn or remember, and work with a single high-level and
    *type-safe* API – called Dataset:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在关于 Spark DataFrames 的讨论之后，让我们快速回顾一下 Spark Dataset API。Apache Spark 1.6 中引入的
    Spark Datasets 的目标是提供一个 API，使用户能够轻松地表达对域对象的转换，同时提供强大 Spark SQL 执行引擎的性能和优势。作为 Spark
    2.0 版本发布的一部分（如以下图所示），DataFrame API 被合并到 Dataset API 中，从而统一了所有库的数据处理能力。由于这种统一，开发者现在需要学习或记住的概念更少，并且可以使用一个单一的高级和
    *类型安全* API —— 被称为 Dataset：
- en: '![Spark Dataset API](img/B05793_03_21.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![Spark Dataset API](img/B05793_03_21.jpg)'
- en: 'Conceptually, the Spark DataFrame is an *alias* for a collection of generic
    objects Dataset[Row], where a Row is a generic *untyped* JVM object. Dataset,
    by contrast, is a collection of *strongly-typed* JVM objects, dictated by a case
    class you define, in Scala or Java. This last point is particularly important
    as this means that the Dataset API is *not supported* by PySpark due to the lack
    of benefit from the type enhancements. Note, for the parts of the Dataset API
    that are not available in PySpark, they can be accessed by converting to an RDD
    or by using UDFs. For more information, please refer to the jira [SPARK-13233]:
    Python Dataset at [http://bit.ly/2dbfoFT](http://bit.ly/2dbfoFT).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，Spark DataFrame 是一个 Dataset[Row] 集合的 *别名*，其中 Row 是一个通用的 *未类型化* JVM 对象。相比之下，Dataset
    是一个由您在 Scala 或 Java 中定义的 case 类决定的 *强类型化* JVM 对象集合。这一点尤其重要，因为这意味着 Dataset API
    由于缺乏类型增强的好处，*不支持* PySpark。注意，对于 Dataset API 中 PySpark 中不可用的部分，可以通过转换为 RDD 或使用
    UDFs 来访问。有关更多信息，请参阅 jira [SPARK-13233]：Python Dataset 在 [http://bit.ly/2dbfoFT](http://bit.ly/2dbfoFT)。
- en: Summary
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: With Spark DataFrames, Python developers can make use of a simpler abstraction
    layer that is also potentially significantly faster. One of the main reasons Python
    is initially slower within Spark is due to the communication layer between Python
    sub-processes and the JVM. For Python DataFrame users, we have a Python wrapper
    around Scala DataFrames that avoids the Python sub-process/JVM communication overhead.
    Spark DataFrames has many performance enhancements through the Catalyst Optimizer
    and Project Tungsten which we have reviewed in this chapter. In this chapter,
    we also reviewed how to work with Spark DataFrames and worked on an on-time flight
    performance scenario using DataFrames.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Spark DataFrames，Python 开发者可以利用一个更简单的抽象层，这个层也可能会显著更快。Python 在 Spark 中最初较慢的一个主要原因是
    Python 子进程和 JVM 之间的通信层。对于 Python DataFrame 用户，我们提供了一个围绕 Scala DataFrames 的 Python
    包装器，它避免了 Python 子进程/JVM 通信开销。Spark DataFrames 通过 Catalyst 优化器和 Project Tungsten
    实现了许多性能提升，这些我们在本章中已进行了回顾。在本章中，我们还回顾了如何使用 Spark DataFrames，并使用 DataFrames 实现了一个准时航班性能场景。
- en: In this chapter, we created and worked with DataFrames by generating the data
    or making use of existing datasets.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过生成数据或利用现有数据集创建了 DataFrame 并与之工作。
- en: In the next chapter, we will discuss how to transform and understand your own
    data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论如何转换和理解您自己的数据。
