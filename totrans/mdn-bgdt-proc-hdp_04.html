<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Data Movement Techniques</h1>
                </header>
            
            <article>
                
<p>In the last chapter, we learned about how to create and configure a Hadoop cluster, HDFS architecture, various file formats, and the best practices for a Hadoop cluster. We also learned about Hadoop high availability techniques.</p>
<p>Since we now know how to create and configure a Hadoop cluster, in this chapter, we will learn about various techniques of data ingestion into a Hadoop cluster. We know about the advantages of Hadoop, but now, we need data in our Hadoop cluster to utilize its real power.</p>
<p>Data ingestion is considered the very first step in the Hadoop data life cycle. Data can be ingested into Hadoop as either a batch or a <span>(real-time) </span>stream of records. Hadoop is a complete ecosystem, and MapReduce is a batch ecosystem of Hadoop.</p>
<p>The following diagram shows various data ingestion tools:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/ec9306a3-4d18-460e-91bb-1376bdbd2ac4.png" style="width:27.83em;height:22.33em;"/></div>
<p>We will learn about each tool in detail in the next few sections.</p>
<p>In this chapter, we will cover the following methods of transferring data to and from our Hadoop cluster:</p>
<ul>
<li><span>Apache Sqoop</span></li>
<li>Apache Flume</li>
<li>Apache NiFi</li>
<li>Apache Kafka Connect</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Batch processing versus real-time processing</h1>
                </header>
            
            <article>
                
<p>Before we dive deep into different data ingestion techniques, let's discuss the difference between batch and real-time (stream) processing. The following explains the difference between these two ecosystems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Batch processing</h1>
                </header>
            
            <article>
                
<p>The following points describe the batch processing system:</p>
<ul>
<li>Very efficient in processing a high volume of data.</li>
<li>All data processing steps (that is, data collection, data ingestion, data processing, and results presentation) <span>are done as one single batch job.</span></li>
<li>Throughput carries more importance than latency. Latency is always more than a single minute.</li>
<li>Throughput directly depends on the size of the data and available computational system resources.</li>
<li>Available tools include Apache Sqoop, MapReduce jobs, Spark jobs, Hadoop DistCp utility, and so on.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Real-time processing</h1>
                </header>
            
            <article>
                
<p>The following points describe how real-time processing is different from batch processing:</p>
<ul>
<li>Latency is extremely important, for example, less than one second</li>
<li>Computation is relatively simple</li>
<li>Data is processed as an independent unit</li>
<li>Available tools include Apache Storm, Spark Streaming, Apache Fink, Apache Kafka, and so on</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache Sqoop</h1>
                </header>
            
            <article>
                
<p>Apache Sqoop is a tool designed for efficiently transferring bulk data between a Hadoop cluster and structured data stores, such as relational databases. In a typical use case, such as a data lake, there is always a need to import data from RDBMS-based data warehouse stores into the Hadoop cluster. After data import and data aggregation, the data needs to be exported back to RDBMS. Sqoop allows easy import and export of data from structured data stores like RDBMS, enterprise data warehouses, and NoSQL systems. With the help of Sqoop, data can be provisioned from external systems into a Hadoop cluster and populate tables in Hive and HBase. Sqoop uses a connector-based architecture, which supports plugins that provide connectivity to external systems. Internally, Sqoop uses MapReduce algorithms to import and export data. By default, all Sqoop jobs run four map jobs. We will see Sqoop import and export functions in detail in the next few sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sqoop Import</h1>
                </header>
            
            <article>
                
<p>The following diagram shows the <strong>Sqoop Import</strong> function to import data from an RDBMS table into a Hadoop cluster:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/c39b5ad8-a2ea-4dec-b372-d3eeb008a52d.png" style="width:42.58em;height:22.58em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Import into HDFS</h1>
                </header>
            
            <article>
                
<p>The following is a sample command to import data into HDFS:</p>
<pre><strong>$sqoop import -connect jdbc:mysql://localhost/dbname -table &lt;table_name&gt;   --username &lt;username&gt; --password &gt;password&gt; -m 4</strong></pre>
<p>The import is done in two steps, which are as follows.</p>
<ol>
<li>Sqoop scans the database and collects the table metadata to be imported</li>
<li>Sqoop submits a map-only job and transfers the actual data using necessary metadata</li>
</ol>
<p>The imported data is saved in HDFS folders. The user can specify alternative folders. The imported data is saved in a directory on HDFS, based on the table being imported. As is the case with most aspects of a Sqoop operation, the user can specify any alternative directory where the files should be populated. You can easily override the format in which data is copied over by explicitly specifying the field separator and record terminator characters. The user can use different formats, like Avro, ORC, Parquet, sequence files, text files, and so on, to store files onto HDFS, for example, importing a MySQL table to HDFS. The following is an example to import a MySQL table to HDFS:</p>
<pre><strong>$ mysql&gt;  create database  sales;</strong><br/><strong>$ mysql&gt;  use sales;</strong><br/><strong>$  mysql&gt;   create table customer </strong><br/><strong>    (cust_num int not null,cust_fname  varchar(30),cust_lname varchar     (30),cust_address  varchar (30),cust_city varchar (20),cust_state      varchar (3), cust_zip  varchar (6),primary key (cust_num));</strong><br/><strong>$ ctrl-C   -- to exit from MySQL</strong></pre>
<p>On the Command Prompt, run the following <kbd>sqoop</kbd> command to import the MySQL sales database table, <kbd>customer</kbd>:</p>
<pre><strong>$  sqoop import --connect jdbc:mysql://127.0.0.1:3306/sales --username root --password hadoop --table customer  --fields-terminated-by ","  --driver com.mysql.jdbc.Driver --target-dir /user/data/customer</strong></pre>
<p>Verify the <kbd>customer</kbd> folder on HDFS as follows:</p>
<pre><strong>$ hadoop fs -ls /user/data/customerFound 5 items-rw-r--r--   1 root hdfs          0 2017-04-28 23:35 /user/data/customer/_SUCCESS-rw-r--r--   1 root hdfs        154 2017-04-28 23:35 /user/data/customer/part-m-00000-rw-r--r--   1 root hdfs         95 2017-04-28 23:35 /user/data/customer/part-m-00001-rw-r--r--   1 root hdfs         96 2017-04-28 23:35 /user/data/customer/part-m-00002-rw-r--r--   1 root hdfs        161 2017-04-28 23:35 /user/data/customer/part-m-00003</strong></pre>
<p>Let's create an external Hive table to verify the records, as shown in the following snippet:</p>
<pre><strong>$ hive$hive &gt; CREATE EXTERNAL TABLE customer_H <br/>(cust_num int,cust_fname  string,cust_lname  string,cust_address string,  cust_city  string,cust_state  string,cust_zip   string) <br/>ROW FORMAT DELIMITED FIELDS TERMINATED BY ','LINES TERMINATED BY 'n'LOCATION '/user/data/customer';<br/>$hive&gt; select * from customer_H;<br/></strong></pre>
<table>
<tbody>
<tr>
<td>
<p><strong>Custnum</strong></p>
</td>
<td>
<p><strong>Cust Fname</strong></p>
</td>
<td>
<p><strong>Cust Lname</strong></p>
</td>
<td>
<p><strong>Cust address</strong></p>
</td>
<td>
<p><strong>City</strong></p>
</td>
<td>
<p><strong>State</strong></p>
</td>
<td>
<p><strong>Zip</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>James</p>
</td>
<td>
<p>Butt</p>
</td>
<td>
<p>6649 N Blue Gum St</p>
</td>
<td>
<p>New Orleans</p>
</td>
<td>
<p>LA</p>
</td>
<td>
<p>70116</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>Art</p>
</td>
<td>
<p>Venere 8</p>
</td>
<td>
<p>W Cerritos Ave #54</p>
</td>
<td>
<p>Bridgeport</p>
</td>
<td>
<p>NJ</p>
</td>
<td>
<p>8014</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>Lenna</p>
</td>
<td>
<p>Paprocki</p>
</td>
<td>
<p>639 Main St</p>
</td>
<td>
<p>Anchorage</p>
</td>
<td>
<p>AK</p>
</td>
<td>
<p>99501</p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p>Donette</p>
</td>
<td>
<p>Foller</p>
</td>
<td>
<p>34 Center St</p>
</td>
<td>
<p>Hamilton</p>
</td>
<td>
<p>OH</p>
</td>
<td>
<p>45011</p>
</td>
</tr>
<tr>
<td>
<p>5</p>
</td>
<td>
<p>Simona</p>
</td>
<td>
<p>Morasca</p>
</td>
<td>
<p>3 Mcauley Dr</p>
</td>
<td>
<p>Ashland</p>
</td>
<td>
<p>OH</p>
</td>
<td>
<p>44805</p>
</td>
</tr>
<tr>
<td>
<p>6</p>
</td>
<td>
<p>Mitsue</p>
</td>
<td>
<p>Tollner</p>
</td>
<td>
<p>7 Eads St</p>
</td>
<td>
<p>Chicago</p>
</td>
<td>
<p>IL</p>
</td>
<td>
<p>60632</p>
</td>
</tr>
<tr>
<td>
<p>7</p>
</td>
<td>
<p>Leota</p>
</td>
<td>
<p>Dilliard</p>
</td>
<td>
<p>7 W Jackson Blvd</p>
</td>
<td>
<p>San Jose</p>
</td>
<td>
<p>CA</p>
</td>
<td>
<p>95111</p>
</td>
</tr>
<tr>
<td>
<p>8</p>
</td>
<td>
<p>Sage</p>
</td>
<td>
<p>Wieser</p>
</td>
<td>
<p>5 Boston Ave #88</p>
</td>
<td>
<p>Sioux Falls</p>
</td>
<td>
<p>SD</p>
</td>
<td>
<p>57105</p>
</td>
</tr>
<tr>
<td>
<p>9</p>
</td>
<td>
<p>Kris</p>
</td>
<td>
<p>Marrier</p>
</td>
<td>
<p>228 Runamuck Pl #2808</p>
</td>
<td>
<p>Baltimore</p>
</td>
<td>
<p>MD</p>
</td>
<td>
<p>21224</p>
</td>
</tr>
<tr>
<td>
<p>10</p>
</td>
<td>
<p>Minna</p>
</td>
<td>
<p>Amigon</p>
</td>
<td>
<p>2371 Jerrold Ave</p>
</td>
<td>
<p>Kulpsville</p>
</td>
<td>
<p>PA</p>
</td>
<td>
<p>19443</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The following is an example of importing a MySQL table to Hive:</p>
<pre><strong>$ sqoop import --connect jdbc:mysql://127.0.0.1:3306/sales --username root --password hadoop --table customer  --driver com.mysql.jdbc.Driver --m 1 --hive-import  --hive-table customor_H</strong></pre>
<p>Verify the table Hive:</p>
<pre><strong>$hive$use default;<br/>$ show tables;</strong></pre>
<p>You will see that the <kbd>customer_H</kbd> table is created under a default database. If you want to create the <kbd>customer_H</kbd> table under a different database, for example, a sales database, you have to create the sales database in advance. Also, you have to change the <kbd>-hive-table</kbd> parameter to the <kbd>--hive-table</kbd> sales <kbd>cutomer_H</kbd> incremental load (insert only). It's a typical data load requirement of loading only the incremental changes happening in the source table. Let's assume that a new customer, <kbd>11</kbd>, is inserted into the source <kbd>customer</kbd> MySQL table:</p>
<pre><strong>insert into customer values (11,'Abel','Maclead','25 E 75th St #69','Los Angeles','CA','90034');</strong></pre>
<p>To accommodate only the new record (that is, customer 11), we have to add a few additional parameters to our original <kbd>sqoop</kbd> command. The new <kbd>sqoop</kbd> command is as follows:</p>
<pre><strong>sqoop import --connect jdbc:mysql://127.0.0.1:3306/sales --username root --password hadoop --table customer  --driver com.mysql.jdbc.Driver --incremental append --check-column cust_num 
      --last-value 10 
    --m 1 --split-by cust_state --target-dir /user/data/customer</strong></pre>
<p>After running this command, Sqoop will pick up only the new row (that is, <kbd>cust_num</kbd>, which is <kbd>11</kbd>):</p>
<pre><strong>$hive&gt; select * from  customer_H;</strong></pre>
<table>
<tbody>
<tr>
<td><strong>Custnum</strong></td>
<td><strong>Cust Fname</strong></td>
<td><strong>Cust Lname</strong></td>
<td><strong>Cust address</strong></td>
<td><strong>City</strong></td>
<td><strong>State</strong></td>
<td><strong>Zip</strong></td>
</tr>
<tr>
<td>1</td>
<td>James</td>
<td>Butt</td>
<td>6649 N Blue Gum St</td>
<td>New Orleans</td>
<td>LA</td>
<td>70116</td>
</tr>
<tr>
<td>2</td>
<td>Art</td>
<td>Venere 8</td>
<td>W Cerritos Ave #54</td>
<td>Bridgeport</td>
<td>NJ</td>
<td>8014</td>
</tr>
<tr>
<td>3</td>
<td>Lenna</td>
<td>Paprocki</td>
<td>639 Main St</td>
<td>Anchorage</td>
<td>AK</td>
<td>99501</td>
</tr>
<tr>
<td>4</td>
<td>Donette</td>
<td>Foller</td>
<td>34 Center St</td>
<td>Hamilton</td>
<td>OH</td>
<td>45011</td>
</tr>
<tr>
<td>5</td>
<td>Simona</td>
<td>Morasca</td>
<td>3 Mcauley Dr</td>
<td>Ashland</td>
<td>OH</td>
<td>44805</td>
</tr>
<tr>
<td>6</td>
<td>Mitsue</td>
<td>Tollner</td>
<td>7 Eads St</td>
<td>Chicago</td>
<td>IL</td>
<td>60632</td>
</tr>
<tr>
<td>7</td>
<td>Leota</td>
<td>Dilliard</td>
<td>7 W Jackson Blvd</td>
<td>San Jose</td>
<td>CA</td>
<td>95111</td>
</tr>
<tr>
<td>8</td>
<td>Sage</td>
<td>Wieser</td>
<td>5 Boston Ave #88</td>
<td>Sioux Falls</td>
<td>SD</td>
<td>57105</td>
</tr>
<tr>
<td>9</td>
<td>Kris</td>
<td>Marrier</td>
<td>228 Runamuck Pl #2808</td>
<td>Baltimore</td>
<td>MD</td>
<td>21224</td>
</tr>
<tr>
<td>10</td>
<td>Minna</td>
<td>Amigon</td>
<td>2371 Jerrold Ave</td>
<td>Kulpsville</td>
<td>PA</td>
<td>19443</td>
</tr>
<tr>
<td>11</td>
<td>Abel</td>
<td>Maclead</td>
<td>25 E 75th St #69</td>
<td>Los Angeles</td>
<td>CA</td>
<td>90034</td>
</tr>
</tbody>
</table>
<p> </p>
<p>For incremental load we cannot update the data directly using Sqoop import.</p>
<p>Please follow the steps in the given link for row-level updates: <a href="http://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/">http://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/</a>.<a href="http://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/"><br/></a>Now, let's look at an example of importing a subset of a MySQL table into Hive. The following command shows how to import only a subset of a <kbd>customer</kbd> table in MySQL into Hive. For example, we have import-only customer data of <kbd>State = "OH"</kbd>:</p>
<pre><strong>$ sqoop import --connect jdbc:mysql://127.0.0.1:3306/sales --username root --password hadoop --table sales.customer  --driver com.mysql.jdbc.Driver --m 1 --where "city = 'OH' --hive-import  --hive-table customer_H_1$ hive&gt; select * from customer_H_1;</strong></pre>
<table>
<tbody>
<tr>
<td><strong>Custnum</strong></td>
<td><strong>Cust Fname</strong></td>
<td><strong>Cust Lname</strong></td>
<td><strong>Cust address</strong></td>
<td><strong>City</strong></td>
<td><strong>State</strong></td>
<td><strong>Zip</strong></td>
</tr>
<tr>
<td>4</td>
<td>Donette</td>
<td>Foller</td>
<td>34 Center St</td>
<td>Hamilton</td>
<td>OH</td>
<td>45011</td>
</tr>
<tr>
<td>5</td>
<td>Simona</td>
<td>Morasca</td>
<td>3 Mcauley Dr</td>
<td>Ashland</td>
<td>OH</td>
<td>44805</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Import a MySQL table into an HBase table</h1>
                </header>
            
            <article>
                
<p>The following is a sample command to import data into an HBase table:</p>
<pre><strong>$sqoop import -connect jdbc:mysql://localhost/dbname -table &lt;table_name&gt;  --username &lt;username&gt; --password &gt;password&gt;  --hive-import -m 4                                    --hbase-create-table --hbase-table &lt;table_name&gt;--column-family &lt;col family name&gt;</strong></pre>
<p>Sqoop imports data into the HBase table column family. The data is converted and inserted as a UTF-8 bytes format.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sqoop export</h1>
                </header>
            
            <article>
                
<p>The following diagram shows the <strong>Sqoop Export</strong> function to export data from a Hadoop cluster:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/9d11503e-3fb0-466b-9bca-7370e6571495.png" style="width:42.17em;height:22.42em;"/></div>
<p>Data processed in a data lake-like use case may be needed for additional business functions. Sqoop can be used to export that data back to RDBMS from HDFS or from a Hive table. In the case of exporting data back to an RDBMS table, the target table must exist in a MySQL database. The rows in HDFS files or records from a Hive table are given as input to the <kbd>sqoop</kbd> command and are called rows in a target table. Those records are read and parsed into a set of records and delimited with a user-specified delimiter.<br/>
<br/>
The following are the commands to export data from HDFS to a MySQL table. Let's create a table in MySQL to store data exported from HDFS:</p>
<pre><strong>$ mysql&gt;  use sales;$  mysql&gt;   create table customer_export (      cust_num int not null,      cust_fname  varchar(30),      cust_lname varchar (30),      cust_address  varchar (30),      cust_city varchar (20),      cust_state  varchar (3),      cust_zip  varchar (6),      primary key (cust_num));</strong><br/><br/><strong>$  sqoop export --connect jdbc:mysql://127.0.0.1:3306/sales --driver com.mysql.jdbc.Driver --username root --password hadoop --table customer_exported  --export-dir /user/data/customer</strong></pre>
<p>The <kbd>--table</kbd> parameter specifies the table which will be populated. Sqoop splits the data and uses individual map tasks to push the splits into the database. Each map task does the actual data transfer. The <kbd>--export-dir &lt;directory h&gt;</kbd> is the directory from which data will be exported:</p>
<pre><strong>$ mysql&gt;  use sales;$ mysql&gt;  select * from customer_exported;</strong></pre>
<table>
<tbody>
<tr>
<td><strong>Custnum</strong></td>
<td><strong>Cust Fname</strong></td>
<td><strong>Cust Lname</strong></td>
<td><strong>Cust Address</strong></td>
<td><strong>City</strong></td>
<td><strong>State</strong></td>
<td><strong>Zip</strong></td>
</tr>
<tr>
<td>1</td>
<td>James</td>
<td>Butt</td>
<td>6649 N Blue Gum St</td>
<td>New Orleans</td>
<td>LA</td>
<td>70116</td>
</tr>
<tr>
<td>2</td>
<td>Art</td>
<td>Venere 8</td>
<td>W Cerritos Ave #54</td>
<td>Bridgeport</td>
<td>NJ</td>
<td>8014</td>
</tr>
<tr>
<td>3</td>
<td>Lenna</td>
<td>Paprocki</td>
<td>639 Main St</td>
<td>Anchorage</td>
<td>AK</td>
<td>99501</td>
</tr>
<tr>
<td>4</td>
<td>Donette</td>
<td>Foller</td>
<td>34 Center St</td>
<td>Hamilton</td>
<td>OH</td>
<td>45011</td>
</tr>
<tr>
<td>5</td>
<td>Simona</td>
<td>Morasca</td>
<td>3 Mcauley Dr</td>
<td>Ashland</td>
<td>OH</td>
<td>44805</td>
</tr>
<tr>
<td>6</td>
<td>Mitsue</td>
<td>Tollner</td>
<td>7 Eads St</td>
<td>Chicago</td>
<td>IL</td>
<td>60632</td>
</tr>
<tr>
<td>7</td>
<td>Leota</td>
<td>Dilliard</td>
<td>7 W Jackson Blvd</td>
<td>San Jose</td>
<td>CA</td>
<td>95111</td>
</tr>
<tr>
<td>8</td>
<td>Sage</td>
<td>Wieser</td>
<td>5 Boston Ave #88</td>
<td>Sioux Falls</td>
<td>SD</td>
<td>57105</td>
</tr>
<tr>
<td>9</td>
<td>Kris</td>
<td>Marrier</td>
<td>228 Runamuck Pl #2808</td>
<td>Baltimore</td>
<td>MD</td>
<td>21224</td>
</tr>
<tr>
<td>10</td>
<td>Minna</td>
<td>Amigon</td>
<td>2371 Jerrold Ave</td>
<td>Kulpsville</td>
<td>PA</td>
<td>19443</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Flume</h1>
                </header>
            
            <article>
                
<p>Flume is a reliable, available and distributed service to efficiently collect, aggregate, and transport large amounts of log data. It has a flexible and simple architecture that is based on streaming data flows. The current version of Apache Flume is 1.7.0, which was released in October 2016.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache Flume architecture</h1>
                </header>
            
            <article>
                
<p>The following diagram depicts the architecture of Apache Flume:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/a2b108b9-89fc-46e0-9651-61c7ae3566b5.png"/></div>
<p>Let's take a closer look at the components of the Apache Flume architecture:</p>
<ul>
<li><strong>Event</strong>: An event is a byte payload with optional string headers. It represents the unit of data that Flume can carry from its source to destination.</li>
<li><strong>Flow</strong>: The transport of events from source to destination is considered a data flow, or just flow.</li>
<li><strong>Agent</strong>: It is an independent process that hosts the components of Flume, such as sources, channels, and sinks. It thus has the ability to receive, store, and forward events to its next-hop destination.</li>
<li><strong>Source</strong>: The source is an interface implementation. It has the ability to consume events that are delivered to it with the help of a specific mechanism.</li>
<li><strong>Channel</strong>: It is a store where events are delivered to the channel through sources that operate within the agent. An event placed in a channel remains there until a sink takes it out for further transport. Channels play an important role in ensuring this.</li>
<li><strong>Sink</strong>: It is an interface implementation, just like the source. It can remove events from a channel and transport them to the next agent in the flow, or to its final destination.</li>
<li><strong>Interceptors</strong>: They help to change an event in transit. An event can be removed or modified on the basis of the chosen criteria. Interceptors are the classes, which implement the <kbd>org.apache.flume.interceptor.Interceptor</kbd> interface.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data flow using Flume</h1>
                </header>
            
            <article>
                
<p>The entire Flume agent runs in a JVM process, which includes all the components (source, channel, and sink). The Flume source receives events from the external sources, like a web server, external files, and so on. The source pushes events to the channel, which stores it until picked up by the sink. The channel stores the payload (message stream) in either the local filesystem or in a memory, depending on the type of the source. For example, if the source is a file, the payload is stored locally. The sink picks up the payload from the channel and pushes it to external data stores. The source and sink within the agent run asynchronously. Sometimes, it may be possible for the sink to push the payload to yet another Flume agent. We will talk about that scenario in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Flume complex data flow architecture</h1>
                </header>
            
            <article>
                
<p>In the following architecture, there are three sources (servers). In order to pull data from the log files stored on these servers, we have to install Flume software on each of these servers. After installation, the filenames need to be added to the <kbd>flume.conf</kbd> file. Flume collects all the data from files and pushes it to the corresponding sink through channels. There are multiple sinks in the above architecture; Hive HDFS, and another sink, which is connected to another installation of the Flume agent installed on another server. It pushes data from sink to source and writes data to the Cassendra data store.</p>
<p>Please note that this is not a good architecture, but I have mentioned it to explain how a Flume sink and Flume sources can be connected.</p>
<p>The following diagram shows complex data flow involving multiple agents:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/9888efdd-2fa8-48a2-be83-b794f8f5fd5d.png" style="width:40.33em;height:17.75em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Flume setup</h1>
                </header>
            
            <article>
                
<p>Flume agent configuration is stored in a local text file. Please refer to the sample Flume agent configuration file in the code repository of this book. Flume 1.7.0 supports various sources and sinks. Widely used Flume sources (a summary) are as follows:</p>
<table>
<tbody>
<tr>
<td><strong>Source</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td>Avro source</td>
<td>Listens on Avro port and receives events from external Avro client streams</td>
</tr>
<tr>
<td>Exec source</td>
<td>Runs a given Unix command and expects that process to continuously produce data on standard out</td>
</tr>
<tr>
<td>Spooling directory source</td>
<td>Ingests data from files on disk</td>
</tr>
<tr>
<td>Taildir source</td>
<td>Tails files in near real-time after new lines are detected in the files</td>
</tr>
<tr>
<td>Kafka source</td>
<td>Reads messages from Kafka topics</td>
</tr>
<tr>
<td>Syslog source</td>
<td>Reads syslog data (supports syslog-TCP and syslog-UDP)</td>
</tr>
<tr>
<td>HTTP source</td>
<td>Accepts Flume events by HTTP <kbd>POST</kbd> and <kbd>GET</kbd></td>
</tr>
</tbody>
</table>
<p> </p>
<p>The widely used Flume sinks can be summarized as follows:</p>
<table>
<tbody>
<tr>
<td><strong>Sink</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td>Avro sink</td>
<td>Events are turned into Avro events and sent to the configured hostname/port pair</td>
</tr>
<tr>
<td>HDFS sink</td>
<td>Writes events into the HDFS</td>
</tr>
<tr>
<td>Hive sink</td>
<td>Writes text or JSON data into a Hive table</td>
</tr>
<tr>
<td>HBase sink</td>
<td>Writes data to HBase</td>
</tr>
<tr>
<td>Morphline Solr sink</td>
<td>Loads it in near real-time into Apache Solr servers</td>
</tr>
<tr>
<td>Elasticsearch sink</td>
<td>Writes data to an Elasticsearch cluster</td>
</tr>
<tr>
<td>Kafka sink</td>
<td>Writes data to a Kafka topic</td>
</tr>
</tbody>
</table>
<p>The widely used Flume channels (a summary) are as follows:</p>
<table>
<tbody>
<tr>
<td><strong>Channel</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td>JDBC channel</td>
<td>Events are stored in storage supported by database</td>
</tr>
<tr>
<td>Kafka channel</td>
<td>Events are stored in a Kafka cluster</td>
</tr>
<tr>
<td>File channel</td>
<td>Events are stored in files</td>
</tr>
<tr>
<td>Spillable memory channel</td>
<td>Events are stored in memory; if memory gets full, then stored on disk</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The widely used Flume interceptors can be summarized as follows:</p>
<table>
<tbody>
<tr>
<td><strong>Interceptor</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td>Timestamp interceptor</td>
<td>Adds the processing time of an event into event headers</td>
</tr>
<tr>
<td>Host interceptor</td>
<td>Adds hostname of agent</td>
</tr>
<tr>
<td>Search and replace interceptor</td>
<td>Supports Java regular expressions</td>
</tr>
<tr>
<td>Regex filtering interceptor</td>
<td>Filters the events against RegEx</td>
</tr>
<tr>
<td>Regex extractor interceptor</td>
<td>Extracts and appends the match RegEx groups as headers on the event</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Log aggregation use case</h1>
                </header>
            
            <article>
                
<p>In day-to-day business scenarios, we always find the need to get log files and make sense out of them. For example, we always find the need to get logs from different applications and servers and merge them together to find trends and patterns. Let me extend this example further. Let's assume that we have five web servers deployed on five different servers. We want to get all five web server logs and merge/aggregate them together to analyze them further by storing one copy on HDFS and another copy to be shipped on to a Kafka topic for real-time analytics. The question is how we design Flume-based log aggregation architecture. The following is the Flume architecture for our web server log aggregation scenario:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/afaa72af-94ab-43dd-9f56-0f4c1e6083fc.png" style="width:41.67em;height:33.58em;"/></div>
<p>Let us walk through the architecture in detail: There are a total of five web servers. Each web server generates a log file and stores it locally. The Flume agent is installed on each web server. The Flume agent is nothing but a (JVM) process that hosts the components through which events flow from an external source to the next destination (hop). Each Flume agent accesses log files based on local configuration of <kbd>flume.conf</kbd>. Each Flume agent reads the log files and pushes data to the Flume collector. Each line of the log file is treated as one message (a payload). The Flume collector gets messages from all web servers, fitters and aggregates all messages, and pushes these messages to the data store. The following is the sample <kbd>flume.conf</kbd> of the Flume agent and the collectors agent <kbd>flume.conf</kbd>:</p>
<pre>## Sample Flume Agent Configuration  
## This conf file should deploy on each webserver 
##   
  
a1.sources = apache 
a1.sources.apache.type = exec 
a1.sources.apache.command = gtail -F /var/log/httpd/access_log 
a1.sources.apache.batchSize = 1 
a1.sources.apache.channels = memoryChannel 
 
a1.channels = memoryChannel 
a1.channels.memoryChannel.type = memory 
a1.channels.memoryChannel.capacity = 100 
 
## Collector Details 
 
a1.sinks = AvroSink 
a1.sinks.AvroSink.type = avro 
a1.sinks.AvroSink.channel = memoryChannel 
a1.sinks.AvroSink.hostname = 10.0.0.10 
a1.sinks.AvroSink.port = 6565 </pre>
<p>The collector <kbd>flume.conf</kbd> file is as follows:</p>
<pre> 
## Collector get data from all agents 
 
collector.sources = AvroIn 
collector.sources.AvroIn.type = avro 
collector.sources.AvroIn.bind = 0.0.0.0 
collector.sources.AvroIn.port = 4545 
collector.sources.AvroIn.channels = mc1 mc2 
 
 
collector.channels = mc1 mc2 
collector.channels.mc1.type = memory 
collector.channels.mc1.capacity = 100 
 
collector.channels.mc2.type = memory 
collector.channels.mc2.capacity = 100 
 
 
## Write copy to Local Filesystem (Debugging) 
# http://flume.apache.org/FlumeUserGuide.html#file-roll-sink 
collector.sinks.LocalOut.type = file_roll 
collector.sinks.LocalOut.sink.directory = /var/log/flume 
collector.sinks.LocalOut.sink.rollInterval = 0 
collector.sinks.LocalOut.channel = mc1 
 
## Write to HDFS 
collector.sinks.HadoopOut.type = hdfs 
collector.sinks.HadoopOut.channel = mc2 
collector.sinks.HadoopOut.hdfs.path = /flume/events/%{log_type}/%{host}/%y-%m-%d 
collector.sinks.HadoopOut.hdfs.fileType = DataStream 
collector.sinks.HadoopOut.hdfs.writeFormat = Text 
collector.sinks.HadoopOut.hdfs.rollSize = 0 
collector.sinks.HadoopOut.hdfs.rollCount = 10000 
collector.sinks.HadoopOut.hdfs.rollInterval = 600 </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache NiFi</h1>
                </header>
            
            <article>
                
<p>What is Apache NiFi? In any organization, we know that there is a variety of systems. Some systems generate the data and other systems consume that data. Apache NiFi is built to automate that data flow from one system to another. Apache NiFi is a data flow management system that comes with a web UI that helps to build data flows in real time. It supports flow-based programming. The graph programming includes a series of nodes and edges through which data moves. In NiFi, these nodes are translated into processors, and the edges into connectors. The data is stored in a packet of information called a <strong>FlowFile</strong>. This FlowFile includes content, attributes, and edges. As a user, you connect processors together using connectors to define how the data should be handled.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Main concepts of Apache NiFi</h1>
                </header>
            
            <article>
                
<p>The following table describes the main components of Apache NiFi:</p>
<table>
<tbody>
<tr>
<td><strong>Component name</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td>FlowFile</td>
<td>Data packet running through the system</td>
</tr>
<tr>
<td>FlowFile processor</td>
<td>Performs the actual work of data routing, transformation, and data movement</td>
</tr>
<tr>
<td>Connetion</td>
<td>Actual data linkage between processors</td>
</tr>
<tr>
<td>Flow controller</td>
<td>Facilitates the exchange of FlowFiles between the processors</td>
</tr>
<tr>
<td>Process group</td>
<td>Specific group of data inputs and data output processors</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache NiFi architecture</h1>
                </header>
            
            <article>
                
<p>The following diagram shows the components of the Apache NiFi architecture (source: <a href="https://nifi.apache.org/docs.html" target="_blank">https://nifi.apache.org/docs.html</a>):</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/33e12830-db99-4454-8e0b-4d842a72c0c0.png" style="width:31.58em;height:18.92em;"/></div>
<p>The components are as follows:</p>
<ul>
<li><strong>Web server</strong>: This hosts NiFi's HTTP-based UI</li>
<li><strong>File controller</strong>: This provides threads and manages the schedule for the extensions to run on</li>
<li><strong>Extensions</strong>: The extensions operate and execute within the JVM</li>
<li><strong>FileFlow repository</strong>: This keeps track of the state of what it knows about a given FlowFile that is presently active in the flow</li>
<li><strong>Content repository</strong>: This is where the actual content bytes of a given FlowFile live</li>
<li><strong>Provenance repository</strong>: This is where all provenance event data is stored</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Key features</h1>
                </header>
            
            <article>
                
<p>The following are the key features of Apache NiFi:</p>
<ul>
<li><strong>Guaranteed delivery</strong>: In the event of increased volume of data, power failures, and network and system failures in NiFi, it becomes necessary to have a robust guaranteed delivery of the data. NiFi ensures, within the dataflow system itself, the transactional communication between NiFi and the data where it is coming to the points to which it is delivered to.</li>
<li><strong>Data buffering with back pressure and pressure release</strong>: In any dataflow, it may be possible that there are some issues with the systems involved; some might be down or some might be slow. In that case, data buffering becomes very essential to coping with the data coming into or going out of the dataflow.</li>
</ul>
<p style="padding-left: 60px">NiFi supports the buffering of all queues with back pressure when it reaches specific limits and age of the data. NiFi does it with a maximum possible throughput rate, while maintaining a good response time.</p>
<ul>
<li><strong>Prioritized queuing</strong>: In general, the data queues maintain natural order or insertion order. But, many times, when the rate of data insertion is faster than the bandwidth, you have to prioritize your data retrieval from the queue. The default is the oldest data first. But NiFi supports prioritization of queues to pull data out based on size, time, and so on that is, largest first or newest first.</li>
<li><strong>Flow-specific quality of service (QoS)</strong>: There are some situations where we have to process the data in a specific time period, for example, within a second and so on, otherwise the data loses its value. The fine-grained flow of specific configuration of these concerns is enabled by Apache NiFi.</li>
<li><strong>Data provenance</strong>: NiFi automatically records, indexes, and makes available provenance data as objects flow through the system—even across fan-in, fan-out, transformations, and more. This information becomes extremely critical in supporting compliance, troubleshooting, optimization, and other scenarios.</li>
<li><strong>Visual command and control</strong>: Apache NiFi allows users to have interactive management of dataflow. It provides immediate feedback to each and every change to the dataflow. Hence, users understand and immediately correct any problems, mistakes, or issues in their dataflows. Based on analytical results of the dataflows, users can make changes to their dataflow, prioritize of queues, add more data flows, and so on.</li>
<li><strong>Flow templates</strong>: Data flows can be developed, designed, and shared. Templates allow subject matter experts to build and publish their flow designs and for others to benefit and collaborate on them.</li>
<li><strong>Extension</strong>: NiFi allows us to extend its key components.</li>
<li><strong>Points of extension:</strong> Processors, controller services, reporting tasks, prioritizers, and customer UIs.</li>
<li><strong>Multi-role security</strong>: Multi-grained, multi-role security can be applied to each component, which allows the admin user to have a fine-grained level of access control.</li>
<li><strong>Clustering</strong>: NiFi is designed to scale-out through the use of clustering many nodes together. That way, it can handle more data by adding more nodes to the cluster.</li>
</ul>
<p>For getting started with Apache NiFi, please use this link: <a href="https://nifi.apache.org/docs/nifi-docs/html/getting-started.html" target="_blank">https://nifi.apache.org/docs/nifi-docs/html/getting-started.html</a>.<a href="https://nifi.apache.org/docs/nifi-docs/html/getting-started.html" target="_blank"/></p>
<p>Let's imagine a scenario. I have a running log file. It is updated on the fly. I want to capture and monitor each line in that file, based on its contents. I want to send it to my Kafka brokers. I also want to deliver all my error records to HDFS for archival and further analysis. Different line types will be sent to different Kafka brokers. For example, error, info, and success types will be sent to three different Kafka topics, namely error, info, and success. I have developed the following NiFi workflow for that. The following table gives the detailed explanation of each processor:</p>
<table>
<tbody>
<tr>
<td><strong>Processor</strong></td>
<td><strong>Purpose</strong></td>
<td><strong>Property</strong></td>
<td><strong>Value</strong></td>
</tr>
<tr>
<td>TailFile</td>
<td>To tail log files</td>
<td>File to tail</td>
<td><kbd>/var/log/apache.log</kbd></td>
</tr>
<tr>
<td>SplitText</td>
<td>To split the log entries to the lines</td>
<td>Line split count</td>
<td><kbd>1</kbd></td>
</tr>
<tr>
<td><span>RouteOnContent</span></td>
<td><span>To make a routing decision</span></td>
<td/>
<td/>
</tr>
<tr>
<td><span>PutHDFS</span></td>
<td><span>To send errors to HDFS</span></td>
<td/>
<td><span>HDFS details</span></td>
</tr>
<tr>
<td><span>PublishKafka</span></td>
<td><span>To deliver data to Kafka topic</span></td>
<td><span>Broker and topic name</span></td>
<td><span>Hostname: port, topic pane</span></td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Real-time log capture dataflow</h1>
                </header>
            
            <article>
                
<p>The following example workflow shows how log file data can be pushed to HDFS and then to moved to Kafka brokers:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img style="font-size: 1em;width:50.00em;height:24.17em;" src="assets/ff6f0904-e113-449d-ae0c-df29aebdb730.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka Connect</h1>
                </header>
            
            <article>
                
<p>Kafka Connect is a part of Apache Kafka. It is a framework to ingest data from one to another system using connectors. There are two types of connectors: source connectors and sink connectors. The sink connectors import data from source systems and write to Kafka topics. The sink connectors read data from the Kafka topic and export it to target systems. Kafka Connect provides various source and sink connectors out of the box.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka Connect – a brief history</h1>
                </header>
            
            <article>
                
<p>Kafka Connect was mainly introduced in November 2015 in Kafka 0.9.x. In addition to the various features of Kafka 0.9.x, Connect APIs was a brand new feature. Then, in May 2016, the new version Kafka 0.10.0 was released. In that version, Kafka Streams API was a new and exciting feature. But, in March 2017, it was Kafka Version 0.10.2 where Kafka Connect got its real momentum. As a part of Kafka 0.10.2, improved simplified Connect APIs and single message transform APIs were released.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why Kafka Connect?</h1>
                </header>
            
            <article>
                
<p>Kafka Connect helps to simplify getting data in and out of Kafka. It provides a lot of connectors to do that <span>out of the box</span>. In my opinion, that's the best incentive to a developer like me, because I do not have to develop a separate code to develop my own connector to import and export data; I can always reuse the out-of-the-box connector for that. Also, if I want, I can always develop my own unique connector using Kafka Connect APIs. Also, all the connectors are configuration-based. The common sources and targets are databases, search engines, NoSQL data stores, and applications like SAP, GoldenGate, Salesforce, HDFS, Elasticsearch, and so on. For a detailed listing of all available sources and connectors, please refer to <a href="https://www.confluent.io/product/connectors/">https://www.confluent.io/product/connectors/</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka Connect features</h1>
                </header>
            
            <article>
                
<p>The following are some features of Kafka connect:</p>
<ul>
<li><strong>Scalable</strong>: This is a framework for scalable and reliable streaming data between Apache Kafka and other systems</li>
<li><strong>Simple</strong>: This makes it simple to define connectors that move large collections of data into and out of Kafka</li>
<li><strong>Offset management</strong>: The framework does most of the hard work of properly recording the offsets of the connectors</li>
<li><strong>Easy operation</strong>: This has a service that has a RESTful API for managing and deploying connectors</li>
<li><strong>Distributed</strong>: The framework can be clustered and will automatically distribute the connectors across the cluster, ensuring that the connector is always running</li>
<li><strong>Out-of-the-box connectors</strong>: For a detailed listing of all available sources and connectors, please refer to <a href="https://www.confluent.io/product/connectors/">https://www.confluent.io/product/connectors/</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka Connect architecture</h1>
                </header>
            
            <article>
                
<p>The following diagram represents the Kafka Connect architecture:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/2a2af5eb-0cbb-4e6c-b3b8-1511e6d0c823.png" style="width:51.67em;height:30.17em;"/></div>
<p>The Kafka cluster is made of Kafka brokers: three brokers, as shown in the diagram. Sources can be of any type, for example, databases, NoSQL, Twitter, and so on. In between the source and Kafka cluster, there is a Kafka Connect cluster, which is made up of workers. The working of Kafka Connect comprises the following steps:</p>
<ol>
<li>Workers, based on configuration, pull data from sources</li>
<li>After getting data, the connector pushes data to the Kafka cluster</li>
<li>If data needs to be transformed, filtered, joined, or aggregated using stream applications such as Spark, Storm, and so on, stream APIs will change data in and out of Kafka</li>
<li>Based on the configuration, the connector will pull data out of Kafka and write it to the sink</li>
</ol>
<p>Some Kafka Connect concepts are as follows:</p>
<ul>
<li>Source connectors get data from common data sources.</li>
<li>Sink connectors publish data to common data sources.</li>
<li>Kafka Connect makes it easy to quickly get data reliably into Kafka.</li>
<li>It is a part of the ETL pipeline.</li>
<li>Scaling from a small pipeline to a company-wide pipeline is very easy.</li>
<li>The code is reusable.</li>
<li>The Kafka Connect cluster has multiple loaded connectors. Each connector is a reusable piece of code, <kbd>(Java JARs)</kbd>. There are a lot of open source connectors available, which can be leveraged.</li>
<li>Each connector task is a combination of connector class and configuration. A task is linked to a connector configuration. A job creation may create multiple tasks. So, if you have one connector and one configuration, then two or more tasks can be created.</li>
<li>Kafka Connect workers and servers execute tasks. A worker is a single java process. A worker can be standalone or distributed.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka Connect workers modes</h1>
                </header>
            
            <article>
                
<p>There are two modes of Kafka Connect workers:</p>
<ul>
<li>Standalone mode</li>
<li>Distributed mode</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Standalone mode</h1>
                </header>
            
            <article>
                
<p>Standalone mode is a single process (worker) that runs all the connectors and tasks. The configuration is bundled in a single process. It is not fault tolerant or scalable, and it is very difficult to monitor. Since it is easy to set up, it is mainly used during development and testing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Distributed mode</h1>
                </header>
            
            <article>
                
<p>With distributed mode, multiple workers (processes) run your connectors and tasks. The configuration is submitted using the REST API. It is scalable and fault tolerant. It automatically rebalances all the tasks on the cluster if any worker dies. Since it is scalable and fault tolerant, it is mainly used in a production environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kafka Connect cluster distributed architecture</h1>
                </header>
            
            <article>
                
<p>The following is the representation of the Kafka Connect cluster distributed architecture details:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9a72723b-b90b-4c5d-a5ad-525ebb59d0f4.png"/></div>
<p>In the preceding diagram, we can see the following details:</p>
<ul>
<li>We have source <strong>Connector 1</strong>, with three tasks: <strong>Task 1</strong>, <strong>Task 2</strong>, and <strong>Task 3</strong>. These three tasks are spread out among four workers: <strong>Worker 1</strong>, <strong>Worker 3</strong>, and <strong>Worker 4</strong>.</li>
<li>We also have source <strong>Connector 2</strong>, with two tasks: <strong>Task 1</strong> and <strong>Task 2</strong>. These two tasks are spread out between two workers: <strong>Worker 2</strong> and <strong>Worker 3</strong>.</li>
<li>We also have sink <strong>Connector 3</strong> with four tasks: <strong>Task 1</strong>, <strong>Task 2</strong>, <strong>Task 3</strong>, and <strong>Task 4</strong>. These four tasks are spread out among four workers: <strong>Worker 1</strong>, <strong>Worker 2</strong>, <strong>Worker 3</strong>, and <strong>Worker 4</strong>.</li>
<li>Now, something happens and <strong>Worker 4</strong> dies and we lose that worker completely.</li>
<li>As a part of fault tolerance, the rebalance activity kicks off. <strong>Connector 1</strong> and <strong>Task 3</strong> move from <strong>Worker 4</strong> to <strong>Worker 2</strong>. Similarly, <strong>Connector 3</strong> and <strong>Task 4</strong> move from <strong>Connector 4</strong> to <strong>Connector 1</strong>.</li>
</ul>
<p>The following diagram represents the Kafka Connect cluster after rebalance:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e65c676a-a02c-4805-a38a-95770edbdcf8.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example 1</h1>
                </header>
            
            <article>
                
<p>Streamed data from source file <kbd>Demo-Source.txt</kbd> is moved to destination file <kbd>Demo-Sink.txt</kbd> in standalone mode, as shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4682bda0-9403-4eb5-a809-14abaf451b54.png"/></div>
<p>In order to stream data from source file <kbd>Demo-Source.txt</kbd> to destination file <kbd>Demo-Sink.txt</kbd> in standalone mode, we need to perform the following steps:</p>
<ol>
<li>Start Kafka:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ /bin/kafka-server-start.sh config/server.properties</strong> </pre>
<ol start="2">
<li>Create topic:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ .bin/kafka-topics --create --topic demo-1-standalone --partitions 3 --replication-factor 1 --zookeeper 127.0.0.1:2181</strong> </pre>
<ol start="3">
<li>Configure the <kbd>source-file-stream-standalone.properties</kbd> file:</li>
</ol>
<pre style="padding-left: 60px">name=source-file-stream-standalone 
connector.class=org.apache.kafka.connect.file.FileStreamSourceConnector 
tasks.max=1 
file=demo-source.txt 
topic=file-source-topic </pre>
<ol start="4">
<li>Configure <kbd>file-stream-standalone.properties</kbd> file:</li>
</ol>
<pre style="padding-left: 60px">name=sinkfile-stream-standalone 
connector.class=org.apache.kafka.file.FileStreamSourceConnector 
tasks.max=1 
file=demo-sink.txt 
topics=file-source-topic </pre>
<ol start="5">
<li><span>Configure </span><kbd>file-worker.properties</kbd> file:</li>
</ol>
<pre style="padding-left: 60px">bootstrap.servers=127.0.0.1:9092 
key.converter=org.apache.kafka.connect.json.JsonConverter 
key.converter.schemas.enable=false 
value.converter=org.apache.kafka.connect.json.JsonConverter 
value.converter.schemas.enable=false 
# we always leave the internal key to JsonConverter 
internal.key.converter=org.apache.kafka.connect.json.JsonConverter 
internal.key.converter.schemas.enable=false 
internal.value.converter=org.apache.kafka.connect.json.JsonConverter 
internal.value.converter.schemas.enable=false 
rest.port=8086 
rest.host.name=127.0.0.1 
# this config is only for standalone workers 
offset.storage.file.filename=standalone.offsets 
offset.flush.interval.ms=10000 </pre>
<ol start="6">
<li>Start Kafka Connect. Open another terminal and run the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ .bin/connect-standalone config/file-worker.properties config/source-file-stream-standalone.properties config/ sink-file-stream-standalone.properties</strong> </pre>
<ol start="7">
<li>Add data to the <kbd>demo-source.txt</kbd> file. Open another terminal and run the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ touch demo-source.txt 
 
$ echo "Test Line 1 " &gt;&gt;  demo-source.txt 
 
$ echo "Test Line 2 " &gt;&gt;  demo-source.txt 
 
$ echo "Test Line 2 " &gt;&gt;  demo-source.txt</strong> </pre>
<ol start="8">
<li>Read the <kbd>demo-sink.txt</kbd> file:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cat demo-sink.file</strong> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example 2</h1>
                </header>
            
            <article>
                
<p>Streamed data from source file <kbd>Demo-Source.txt</kbd> is moved to destination file <kbd>Demo-Sink.txt</kbd> in distributed mode. If you want to run the previous example using distributed mode, you have to add the following parameter to <kbd>source-file-stream</kbd> and <kbd>sink-file-stream</kbd> in <em>steps 3</em> and <em>4</em>:</p>
<pre>key.converter=org.apache.kafka.connect.json.JsonConverter 
key.converter.schemas.enable=true 
value.converter=org.apache.kafka.connect.json.JsonConverter 
value.converter.schemas.enable=true </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have learned all the popular data ingestion tools used in production environments. Sqoop is mainly used to import and export data in and out of RDBMS data stores. Apache Flume is used in real-time systems to import data, mainly from files sources. It supports a wide variety of sources and sinks. Apache NiFi is a fairly new tool and getting very popular these days. It also supports GUI-based ETL development. Hortonworks has started supporting this tool since their HDP 2.4 release. Apache Kafka Connect is another popular tool in the market. It is also a part of the Confluent Data Platform. Kafka Connect can ingest entire databases or collect metrics from all your application servers into Kafka topics, making the data available for stream processing with low latency.</p>
<p>Since we so far know how to build Hadoop clusters and how to ingest data in them, we will learn data modeling techniques in the next chapter.</p>


            </article>

            
        </section>
    </body></html>