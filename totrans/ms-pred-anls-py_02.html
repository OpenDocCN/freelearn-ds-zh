<html><head></head><body>
  <div><div><div><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Exploratory Data Analysis and Visualization in Python</h1></div></div></div><p>Analytic pipelines are not built from raw data in a single step. Rather, development is an iterative process that involves understanding the data in greater detail and systematically refining both model and inputs to solve a problem. A key part of this cycle is interactive data analysis and visualization, which can provide initial ideas for features in our predictive modeling or clues as to why an application is not behaving as expected.</p><p>Spreadsheet programs are one kind of interactive tool for this sort of exploration: they allow the user to import tabular information, pivot and summarize data, and generate charts. However, what if the data in question is too large for such a spreadsheet application? What if the data is not tabular, or is not displayed effectively as a line or bar chart? In the former case, we could simply obtain a more powerful computer, but the latter is more problematic. Simply put, many traditional data visualization tools are not well suited to complex data types such as text or images. Additionally, spreadsheet programs often assume data is in a finalized form, whereas in practice we will often need to clean up the raw data before analysis. We might also want to calculate more complex statistics than simple averages or sums. Finally, using the same programming tools to clean up and visualize our data as well as generate the model itself and test its performance allows a more streamlined development process.</p><p>In this chapter we introduce interactive Python (IPython) notebook applications (Pérez, Fernando, and Brian E. Granger. <em>IPython: a system for interactive scientific computing</em>. <em>Computing in Science &amp; Engineering</em> 9.3 (2007): 21-29). The notebooks form a data preparation, exploration, and modeling environment that runs inside a web browser. The commands typed in the input cells of an IPython notebook are translated and executed as they are received: this kind of interactive programming is helpful for data exploration, where we may refine our efforts and successively develop more detailed analyses. Recording our work in these Notebooks will help to both backtrack during debugging and serve as a record of insights that can be easily shared with colleagues.</p><p>In this chapter we will discuss the following topics:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Reading raw data into an IPython notebook, cleaning it, and manipulating it using the Pandas library.</li><li class="listitem" style="list-style-type: disc">Using IPython to process numerical, categorical, geospatial, or time-series data, and perform basic statistical analyses.</li><li class="listitem" style="list-style-type: disc">Basic exploratory analyses: summary statistics (mean, variance, median), distributions (histogram and kernel density), and auto-correlation (time-series).</li><li class="listitem" style="list-style-type: disc">An introduction to distributed data processing with Spark RDDs and DataFrames.</li></ul></div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec12"/>Exploring categorical and numerical data in IPython</h1></div></div></div><p>We will start <a id="id54" class="indexterm"/>our explorations in IPython by loading a text file into a DataFrame, calculating some summary statistics, and visualizing distributions. For this exercise we'll use a set of movie ratings and metadata from the Internet Movie Database (<a class="ulink" href="http://www.imdb.com/">http://www.imdb.com/</a>) to investigate what factors might correlate with high <a id="id55" class="indexterm"/>ratings for films on this website. Such information might be helpful, for example, in developing a recommendation system based on this kind of user feedback.</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec21"/>Installing IPython notebook</h2></div></div></div><p>To follow along <a id="id56" class="indexterm"/>with the examples, you should have a Windows, Linux, or Mac OSX operating system installed on your computer and access to the Internet. There are a number of options available to install IPython: since each of these resources includes installation guides, we provide a summary of the available sources and direct the reader to the relevant documentation for more in-depth instructions.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">For most users, a pre-bundled Python environment such as Anaconda (Continuum Analytics) or Canopy (Enthought) provides an out-of-the-box distribution with IPython and all the libraries we will use in these exercises: these products are self-contained, and thus you should not  have to worry about conflicting versions or dependency management.</li><li class="listitem" style="list-style-type: disc">For more ambitious users, you can install a python distribution of your choice, followed by individual installation of the required libraries using package managers such as <code class="literal">pip</code> or <code class="literal">easy_install</code>.</li></ul></div></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec22"/>The notebook interface</h2></div></div></div><p>Let's get <a id="id57" class="indexterm"/>started with the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Once you've installed IPython, open the command prompt (terminal) on your computer and type:<div><pre class="programlisting">
<strong>jupyter notebook</strong>
</pre></div><p>Note that depending upon where you installed the program, the <code class="literal">jupyter</code> command may require the binary file that launches <code class="literal">jupyter</code> to be on your <a id="id58" class="indexterm"/>system path. You should see a series of commands like the following in your terminal:</p><div><img src="img/B04881_chapter02_01.jpg" alt="The notebook interface"/></div><p>This starts the <strong>kernel</strong>, the python interpreter that computes the result of commands entered into the notebook. If you want to stop the notebook, type <em>Ctrl</em> + <em>C</em>, and enter <strong>yes</strong>, and the kernel will shut down.</p></li><li class="listitem">When the kernel starts, your default web browser should also open, giving you a homepage that looks like this:<div><img src="img/B04881_chapter02_02.jpg" alt="The notebook interface"/></div></li><li class="listitem">The <strong>Files</strong> tab (see above) will show you all of the files in the directory where you started the IPython process. Clicking <strong>Running</strong> will give you a list of all running <a id="id59" class="indexterm"/>notebooks – there are none when you start:<div><img src="img/B04881_chapter02_30.jpg" alt="The notebook interface"/></div></li><li class="listitem">Finally, the Clusters panel gives a list of external clusters, should we decide to parallelize our calculations by submitting commands to be processed on more than one machine. We won't worry about this for now, but it will come in useful later when we begin to train predictive models, a task that may often be accelerated by distributing the work among many computers or processors.</li><li class="listitem">Returning to the <strong>Files</strong> tab, you will notice two options in the top right-hand corner. One is to <strong>Upload</strong> a file: while we are running IPython locally, it could just as easily be running on a remote server, with the analyst accessing the notebook through a browser. In this case, to interact with files stored on our own machine, we can use this button to open a prompt and selected the desired files to upload to the server, where we could then analyze them in the notebook. The <strong>New</strong> tab lets you create a new folder, text file, a Python terminal running in the browser, or a notebook.<p>For now, let's open the sample notebook for this chapter by double clicking on <strong>B04881_chapter02_code01.ipynb</strong>. This opens the notebook:</p><div><img src="img/B04881_chapter02_03.jpg" alt="The notebook interface"/></div>
The notebook consists of a series of cells, areas of text where we can type python code, execute it, and see the results of the commands. The python code in each cell can be executed by clicking the <img src="img/B04881_chapter02_55.jpg" alt="The notebook interface"/> button on the toolbar, and a new cell can be inserted below the current one by clicking <img src="img/B04881_chapter02_56.jpg" alt="The notebook interface"/>.
</li><li class="listitem">
While the import statements in the first cell probably look familiar from your experience of using python in the command line or in a script, the <code class="literal">%matplotlib</code> inline command is not actually python: it is a markup instruction to the notebook that <code class="literal">matplotlib</code> images are to be displayed inline the browser. We enter this command at the beginning of the notebook so that all of our <a id="id60" class="indexterm"/>later plots use this setting. To run the import statements, click the <img src="img/B04881_chapter02_57.jpg" alt="The notebook interface"/> button or press <em>Ctrl</em> + <em>Enter</em>. The <code class="literal">ln[1]</code> on the cell may briefly change to <code class="literal">[*]</code> as the command executes. There will be no output in this case, as all we did was import library dependencies. Now that our environment is ready, we can start examining some data.
</li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec23"/>Loading and inspecting data</h2></div></div></div><p>To start, we will <a id="id61" class="indexterm"/>import the data in <code class="literal">movies.csv</code> into a DataFrame <a id="id62" class="indexterm"/>object using the Pandas library (McKinney, Wes. <em>Python for data analysis: Data wrangling with Pandas</em>, NumPy, and IPython. O'Reilly Media, Inc., 2012). This DataFrame resembles traditional spreadsheet software and allows powerful extensions such as custom transformations and aggregations. These may be combined with numerical methods, such as those available in NumPy, for more advanced statistical analysis of the data. Let us continue our analysis:</p><div><ol class="orderedlist arabic"><li class="listitem">
If this were a new notebook, to add new cells we would go to the toolbar, click <strong>Insert</strong> and <strong>Insert Cell Below</strong>, or use the <img src="img/B04881_chapter02_57.jpg" alt="Loading and inspecting data"/>. button. However, in this example all the cells are already generated, therefore we run the following command in the second cell:
<div><pre class="programlisting">
<strong>&gt;&gt;&gt; imdb_ratings = pd.read_csv('movies.csv')</strong>
</pre></div>We've now created a DataFrame object using the Pandas library, <code class="literal">imdb_ratings</code>, and can begin analyzing the data.</li><li class="listitem">Let's start by peeking at the beginning and end of the data using <code class="literal">head()</code> and <code class="literal">tail()</code>. Notice that by default this command returns the first five rows of data, but we can supply an integer argument to the command to specify the number of lines to return. Also, by default the first line of the file is assumed to contain the column names, which in this case is correct. Typing:<div><pre class="programlisting">
<strong>&gt;&gt;&gt; imdb_ratings.head()</strong>
</pre></div><p>Gives the following output:</p><div><img src="img/B04881_chapter02_04.jpg" alt="Loading and inspecting data"/></div><p>We can <a id="id63" class="indexterm"/>similarly look at the last 15 <a id="id64" class="indexterm"/>lines of the data by typing:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; imdb_ratings.tail(15)</strong>
</pre></div><div><img src="img/B04881_chapter02_05.jpg" alt="Loading and inspecting data"/></div></li><li class="listitem">Looking at individual rows gives us a sense of what kind of data the file contains: we <a id="id65" class="indexterm"/>can also look at summaries for <a id="id66" class="indexterm"/>all rows in each column using the command <code class="literal">describe()</code>, which returns the number of records, mean value, and other aggregate statistics. Try typing:<div><pre class="programlisting">
<strong>&gt;&gt;&gt; imdb_ratings.describe()</strong>
</pre></div><p>This gives the following output:</p><div><img src="img/B04881_chapter02_06.jpg" alt="Loading and inspecting data"/></div></li><li class="listitem">Column names and their datatypes can be accessed using the properties <code class="literal">columns</code> and <code class="literal">dtypes</code>. Typing:<div><pre class="programlisting">
<strong>&gt;&gt;&gt; imdb_ratings.columns</strong>
</pre></div><p>Gives us the names of the columns:</p><div><img src="img/B04881_chapter02_58.jpg" alt="Loading and inspecting data"/></div><p>If we issue the command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; imdb_ratings.dtypes</strong>
</pre></div></li><li class="listitem">As we can see, the datatypes of the columns have been automatically inferred when we first loaded the file:<div><img src="img/B04881_chapter02_07.jpg" alt="Loading and inspecting data"/></div></li><li class="listitem">If we want <a id="id67" class="indexterm"/>to access the data in individual <a id="id68" class="indexterm"/>columns, we can do so using either <code class="literal">{DataFrame_name}.{column_name}</code> or <code class="literal">{DataFrame_name}['column_name']</code> (similar to a python dictionary). For example, typing:<div><pre class="programlisting">
<strong>&gt;&gt;&gt; imdb_ratings.year.head()</strong>
</pre></div><p>or</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; imdb_ratings['year'].head()</strong>
</pre></div><p>Gives the following output:</p><div><img src="img/B04881_chapter02_08.jpg" alt="Loading and inspecting data"/></div></li></ol></div><p>Without much work, we can already use these simple commands to  ask a number of diagnostic questions about the data. Do the summary statistics we generated using  <code class="literal">describe()</code>  make sense (for example, the max rating should be 10, while the minimum is 1)? Is the data correctly parsed into the columns we expect?</p><p>Looking back at the <a id="id69" class="indexterm"/>first five rows of data we visualized using the <a id="id70" class="indexterm"/>
<code class="literal">head()</code> command, this initial inspection also reveals some formatting issues we might want to consider. In the <strong>budget</strong> column, several entries have the value <code class="literal">NaN</code>, representing missing values. If we were going to try to predict movie ratings based on features including <strong>budget</strong>, we might need to come up with a rule to fill in these missing values, or encode them in a way that is correctly represented to the algorithm.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec24"/>Basic manipulations – grouping, filtering, mapping, and pivoting</h2></div></div></div><p>Now that we have <a id="id71" class="indexterm"/>looked at the basic features of the Pandas DataFrame, let us start applying some transformations and calculations to this data beyond the simple statistics we obtained through <code class="literal">describe()</code>. For example, if we wanted to calculate how many films belong to each release year, we can use following command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; imdb_ratings.value_counts()</strong>
</pre></div><p>Which gives the output:</p><div><img src="img/B04881_chapter02_09.jpg" alt="Basic manipulations – grouping, filtering, mapping, and pivoting"/></div><p>Notice that the result is by default sorted by the count of records in each year (with the most films in this <a id="id72" class="indexterm"/>dataset released in 2002). What if we wanted to sort by the release year? The <code class="literal">sort_index()</code> command orders the result by its index (the year to which the count belongs). The index is similar to the axis of a plot, with values representing the point at each axis tick. Using the command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; imdb_ratings.year.value_counts().sort_index(ascending=False)</strong>
</pre></div><p>Gives the following output:</p><div><img src="img/B04881_chapter02_31.jpg" alt="Basic manipulations – grouping, filtering, mapping, and pivoting"/></div><p>We can also <a id="id73" class="indexterm"/>use the DataFrame to begin asking analytical questions about the data, logically slicing and sub-selecting as we might in a database query. For example, let us select the subset of films released after 1999 with an R rating using the following command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; imdb_ratings[(imdb_ratings.year &gt; 1999) &amp; (imdb_ratings.mpaa == 'R')].head()</strong>
</pre></div><p>This gives the following output:</p><div><img src="img/B04881_chapter02_11.jpg" alt="Basic manipulations – grouping, filtering, mapping, and pivoting"/></div><p>Similarly, we can group the data by any column(s) and calculate aggregated statistics using the <code class="literal">groupby</code> <a id="id74" class="indexterm"/>command and pass an array of calculations to perform as an argument to <code class="literal">aggregate</code>. Let us use the mean and standard deviation functions from NumPy to find the average and variation in ratings for films released in a given year:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; imdb_ratings.groupby('year').rating.aggregate([np.mean,np.std])</strong>
</pre></div><p>This gives:</p><div><img src="img/B04881_chapter02_32.jpg" alt="Basic manipulations – grouping, filtering, mapping, and pivoting"/></div><p>However, sometimes the questions we want to ask require us to reshape or transform the raw data we are given. This will happen frequently in later chapters, when we develop features for predictive models. Pandas provide many tools for performing this kind of transformation. For <a id="id75" class="indexterm"/>example, while it would also be interesting to aggregate the data based on genre, we notice that in this dataset each genre is represented as a single column, with 1 or 0 indicating whether a film belongs to a given genre. It would be more useful for us to have a single column indicating which genre the film belongs to for use in aggregation operations. We can make such a column using the command <code class="literal">idxmax()</code> with the argument 1 to represent the maximum argument across columns (0 would represent the max index along rows), which returns the column with the greatest value out of those selected. Typing:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt;imdb_ratings['genre']=imdb_ratings[['Action','Animation','Comedy','Drama','Documentary','Romance']].idxmax(1)</strong>
</pre></div><p>Gives the following result when we examine this new genre column using:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; imdb_ratings['genre'].head()</strong>
</pre></div><div><img src="img/B04881_chapter02_33.jpg" alt="Basic manipulations – grouping, filtering, mapping, and pivoting"/></div><p>We may also perhaps like to plot the data with colors representing a particular genre. To generate a color code for each genre, we can use a custom mapping function with the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; genres_map = {"Action": 'red', "Animation": 'blue', "Comedy": 'yellow', "Drama": 'green', "Documentary": 'orange', "Romance": 'purple'}</strong>
<strong>&gt;&gt;&gt; imdb_ratings['genre_color'] = imdb_ratings['genre'].apply(lambda x: genres_map[x])</strong>
</pre></div><p>We can verify the output by typing:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; imdb_ratings['genre_color'].head()</strong>
</pre></div><p>Which gives:</p><div><img src="img/B04881_chapter02_34.jpg" alt="Basic manipulations – grouping, filtering, mapping, and pivoting"/></div><p>We can also transpose the table and perform statistical calculations using the <code class="literal">pivot_table</code> <a id="id76" class="indexterm"/>command, which can perform aggregate calculations on groupings of rows and columns as in a spreadsheet. For example, to calculate the average rating per genre per year we can use the following command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt;pd.pivot_table(imdb_ratings,values='rating',index='year',columns=['genre'],aggfunc=np.mean)</strong>
</pre></div><p>Which gives the output:</p><div><img src="img/B04881_chapter02_35.jpg" alt="Basic manipulations – grouping, filtering, mapping, and pivoting"/></div><p>Now that we have performed some exploratory calculations, let us look at some visualizations of this information.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec25"/>Charting with Matplotlib</h2></div></div></div><p>One of the practical <a id="id77" class="indexterm"/>features of IPython notebooks <a id="id78" class="indexterm"/>is the ability to plot data inline with our analyses. For example, if we wanted to visualize the distribution of film lengths we could use the command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; imdb_ratings.length.plot()</strong>
</pre></div><div><img src="img/B04881_chapter02_16.jpg" alt="Charting with Matplotlib"/></div><p>However, this is not really a very attractive image. To make a more aesthetically pleasing plot, we can change the <a id="id79" class="indexterm"/>default style using the <code class="literal">style.use()</code> command. Let us change the style to <code class="literal">ggplot</code>, which is used in the <code class="literal">ggplot</code> graphical library (Wickham, Hadley. <em>ggplot: An Implementation of the Grammar of Graphics</em>. R package version 0.4. 0 (2006)). Typing the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; matplotlib.style.use('ggplot')</strong>
<strong>&gt;&gt;&gt; imdb_ratings.length.plot()</strong>
</pre></div><p>Gives a much more attractive graphic:</p><div><img src="img/B04881_chapter02_36.jpg" alt="Charting with Matplotlib"/></div><p>As you can see preceding, the default plot is a line chart. The line chart plots each datapoint (movie runtime) as a line, ordered from left to right by their row number in the DataFrame. To make a density plot of films by their genre, we can plot using the <code class="literal">groupby</code> command <a id="id80" class="indexterm"/>with the argument <code class="literal">type=kde</code>. <strong>KDE</strong> is an abbreviation for <strong>Kernel Density Estimate</strong> (Rosenblatt, Murray. Remarks on <a id="id81" class="indexterm"/>some nonparametric estimates of a density function. <em>The Annals of Mathematical Statistics 27.3 (1956): 832-837</em>; Parzen, Emanuel. On estimation of a probability density function and mode. The annals of mathematical statistics 33.3 (1962): 1065-1076), meaning that for each point (film runtime) we estimate the density (proportion of the population with that runtime) with the equation:</p><div><img src="img/B04881_chapter02_60.jpg" alt="Charting with Matplotlib"/></div><p>Where <code class="literal">f(x)</code> is an estimate of the probability density, n is the number of records in our dataset, <code class="literal">h</code> is a bandwidth parameter, and <code class="literal">K</code> is a kernel function. As an example, if <code class="literal">K</code> were the Gaussian kernel given by:</p><div><img src="img/B04881_chapter02_59.jpg" alt="Charting with Matplotlib"/></div><p>where σ is the standard deviation and μ is the mean of the normal distribution, then the KDE represents the average density of all other datapoints in a normally distributed 'window' around a given point x. The width of this window is given by <em>h</em>. Thus, the KDE allows us to plot a smoothed representation of a histogram by plotting not the absolute count at a given <a id="id82" class="indexterm"/>point, but a continuous probability estimate at the point. To this KDE plot, let us also add annotations for the axes, and limit <a id="id83" class="indexterm"/>the maximum runtime to 2 hrs using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; plot1 = imdb_ratings.groupby('genre').length.plot(kind='kde',xlim=(0,120),legend='genre')</strong>
<strong>&gt;&gt;&gt;plot1[0].set_xlabel('Number of Minutes')</strong>
<strong>&gt;&gt;&gt;plot1[0].set_title('Distribution of Films by Runtime Minutes')</strong>
</pre></div><p>Which gives the following plot:</p><div><img src="img/B04881_chapter02_39.jpg" alt="Charting with Matplotlib"/></div><p>We see, unsurprisingly, that many animated films are short, while others categories average around 90 minutes in length. We can also plot similar density curves to examine the distribution of <a id="id84" class="indexterm"/>ratings between genres using the <a id="id85" class="indexterm"/>following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; plot2 = imdb_ratings.groupby('genre').rating.plot(kind='kde',xlim=(0,10),legend='genre')</strong>
<strong>&gt;&gt;&gt; plot2[0].set_xlabel('Ratings')</strong>
<strong>&gt;&gt;&gt; plot2[0].set_title('Distribution of Ratings')</strong>
</pre></div><p>Which gives the following plot:</p><div><img src="img/B04881_chapter02_40.jpg" alt="Charting with Matplotlib"/></div><p>Interestingly, documentaries have on average the highest rating, while action films have the lowest. We could also visualize this same information using a boxplot using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; pd.pivot_table(imdb_ratings,values='rating',index='title',columns=['genre']).\</strong>
<strong>plot(kind='box',by='genre').\</strong>
<strong>set_ylabel('Rating')</strong>
</pre></div><p>This gives the boxplot as follows:</p><div><img src="img/B04881_chapter02_41.jpg" alt="Charting with Matplotlib"/></div><p>We can also use <a id="id86" class="indexterm"/>the notebook to start to make this sort <a id="id87" class="indexterm"/>of plotting automated for a dataset. For example, we often would like to look at the marginal plot of each variable (its single-dimensional distribution) compared to all others in order to find correlations between columns in our dataset. We can do this using the built-in <code class="literal">scatter_matrix</code> function:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from pandas.tools.plotting import scatter_matrix</strong>
<strong>&gt;&gt;&gt; scatter_matrix(imdb_ratings[['year','length','budget','rating','votes']], alpha=0.2, figsize=(6, 6), diagonal='kde')</strong>
</pre></div><p>This will allow us to plot the pairwise distribution of all the variables we have selected, giving us an overview of potential correlations between them:</p><div><img src="img/B04881_chapter02_42.jpg" alt="Charting with Matplotlib"/></div><p>This single plot actually gives a lot of information. For example, it shows that in general higher budget films have higher ratings, and films made in the 1920s have higher average rating than those <a id="id88" class="indexterm"/>before. Using this sort of scatter matrix, we <a id="id89" class="indexterm"/>can look for correlations that might guide the development of a predictive model, such as a predictor of ratings given other movie features. All we need to do is give this function a subset of columns in the DataFrame to plot (since we want to exclude non-numerical data which cannot be visualized in this way), and we can replicate this analysis for any new dataset.</p><p>What if we want to visualize these distributions in more detail? As an example, lets break the correlation between length and rating by genre using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15,15))</strong>
<strong>&gt;&gt;&gt; row = 0</strong>
<strong>&gt;&gt;&gt; col = 0</strong>
<strong>&gt;&gt;&gt; for index, genre in imdb_ratings.groupby('genre'):</strong>
<strong>…    if row &gt; 2:</strong>
<strong>…        row = 0</strong>
<strong>…       col += 1</strong>
<strong>…    genre.groupby('genre').\</strong>
<strong>....plot(ax=axes[row,col],kind='scatter',x='length',y='rating',s=np.sqrt(genre['votes']),c=genre['genre_color'],xlim=(0,120),ylim=(0,10),alpha=0.5,label=index)</strong>
<strong>…    row += 1</strong>
</pre></div><p>In this command, we <a id="id90" class="indexterm"/>create a 3x2 grid to hold plots for our six genres. We then iterate over the data groups by genre, and if we have reached the third <a id="id91" class="indexterm"/>row we reset and move to the second column. We then plot the data, using the <code class="literal">genre_color</code> column we generated previously, along with the index (the genre group) to label the plot. We scale the size of each point (representing an individual film) by the number of votes it received. The resulting scatterplots show the relationship between length and genre, with the size of the point giving sense of how much confidence we should place in the value of the point.</p><div><img src="img/B04881_chapter02_43.jpg" alt="Charting with Matplotlib"/></div><p>Now that we have <a id="id92" class="indexterm"/>looked at some basic analysis using <a id="id93" class="indexterm"/>categorical data and numerical data, let's continue with a special case of numerical data – time series.</p></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec13"/>Time series analysis</h1></div></div></div><p>While the <code class="literal">imdb</code> <a id="id94" class="indexterm"/>data contained movie release years, fundamentally the objects of interest were the individual films and the ratings, not a linked series of events over time that might be correlated with one another. This latter type of data – a time series – raises a different set of questions. Are datapoints correlated with one another? If so, over what timeframe are they correlated? How noisy is the signal? Pandas DataFrames have many built-in tools for time series analysis, which we will examine in the next section.</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec26"/>Cleaning and converting</h2></div></div></div><p>In our previous <a id="id95" class="indexterm"/>example, we were able to use the data more or less in the form in which it was supplied. However, there is not always a guarantee that this will be the case. In our second example, we'll look at a time series of oil prices in the US by year over the last century (Makridakis, Spyros, Steven C. Wheelwright, and Rob J. Hyndman. <em>Forecasting methods and applications</em>, John Wiley &amp; Sons. Inc, New York(1998). We'll start again by loading this data into the notebook, and inspecting it visually using <code class="literal">tail()</code> by typing:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; oil_prices = pd.read_csv('oil.csv')</strong>
<strong>&gt;&gt;&gt; oil_prices.tail()</strong>
</pre></div><p>Which gives the output:</p><div><img src="img/B04881_chapter02_10.jpg" alt="Cleaning and converting"/></div><p>The last row is unexpected, since it does not look like a year at all. In fact, it is a footer comment in the spreadsheet. As it is not actually part of the data, we will need to remove it from the dataset, which we can do with the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; oil_prices = oil_prices[~np.isnan(oil_prices[oil_prices.columns[1]])] </strong>
</pre></div><p>This will remove from the dataset and rows in which the second column is NaN (not a correctly formatted number). We can verify that we have cleaned up the dataset by using the tail command again</p><p>The second aspect <a id="id96" class="indexterm"/>of this data that we would like to clean up is the format. If we look at the format of the columns using:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; oil_prices.dtypes</strong>
</pre></div><p>we see that the year is not by default interpreted as a Python date type:</p><div><img src="img/B04881_chapter_02_25.jpg" alt="Cleaning and converting"/></div><p>We would like the <strong>Year</strong> column to be a Python date. type Pandas provides the built-in capability to perform this conversion using the <code class="literal">convert_object()</code> command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; oil_prices = oil_prices.convert_objects(convert_dates='coerce')</strong>
</pre></div><p>At the same time, we can rename the column with prices something a little less verbose using the <code class="literal">rename</code> command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; oil_prices.rename(columns = {oil_prices.columns[1]: 'Oil_Price_1997_Dollars'},inplace=True)</strong>
</pre></div><p>We can then verify that the output from using the head() command shows these changes:</p><div><img src="img/B04881_chapter02_26.jpg" alt="Cleaning and converting"/></div><p>We now have the <a id="id97" class="indexterm"/>data in a format in which we can start running some diagnostics on this time series.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec27"/>Time series diagnostics</h2></div></div></div><p>We can plot this <a id="id98" class="indexterm"/>data using the <code class="literal">matplotlib</code> commands covered in the previous section using the following:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; oil_prices.plot(x='Year',y='Oil_Price_1997_Dollars')</strong>
</pre></div><p>This produces the time series plot as follows:</p><p> </p><div><img src="img/B04881_chapter02_44.jpg" alt="Time series diagnostics"/></div><p>There are a number of natural questions we might ask of this data. Are the fluctuations in oil prices per year completely random, or do year-by-year measurements correlate with one another? There seem to be some cycles in the data, but it is difficult to quantify the degree of this correlation. A visual tool we can use to help diagnose this feature is a <code class="literal">lag_plot, which is available in Pandas using the following commands</code>:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from pandas.tools.plotting import lag_plot</strong>
<strong>&gt;&gt;&gt; lag_plot(oil_prices.Oil_Price_1997_Dollars)</strong>
</pre></div><div><img src="img/B04881_chapter02_45.jpg" alt="Time series diagnostics"/></div><p>A lag plot simply plots a yearly oil price (x-axis) versus the oil price in the year immediately following it (y-axis). If there is no correlation, we would expect a circular cloud. The linear pattern here shows that there is some structure in the data, which fits with the fact that <a id="id99" class="indexterm"/>year-by-year prices go up or down. How strong is this correlation compared to expectation? We can use an autocorrelation plot to answer this question, using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from pandas.tools.plotting import autocorrelation_plot</strong>
<strong>&gt;&gt;&gt; autocorrelation_plot(oil_prices['Oil_Price_1997_Dollars'])</strong>
</pre></div><p>Which gives the following autocorrelation plot:</p><div><img src="img/B04881_chapter02_46.jpg" alt="Time series diagnostics"/></div><p>In this plot, the <a id="id100" class="indexterm"/>correlation between points at different lags (difference in years) is plotted along with a 95% confidence interval (solid) and 99% confidence interval (dashed) line for the expected range of correlation on random data. Based on this visualization, there appears to be exceptional correlation for lags of &lt;10 years, which fits with the approximate duration of the peak price periods in the first plot of this data above.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec28"/>Joining signals and correlation</h2></div></div></div><p>Lastly, let us look <a id="id101" class="indexterm"/>at an example of comparing the oil price time series to another dataset, the number of car crash fatalities in the US for the given years (<em>List of Motor Vehicle Deaths in U.S. by Year</em>. Wikipedia. Wikimedia Foundation. Web. 02 May 2016. <a class="ulink" href="https://en.wikipedia.org/wiki/List_of_motor_vehicle_deaths_in_U.S._by_year">https://en.wikipedia.org/wiki/List_of_motor_vehicle_deaths_in_U.S._by_year</a>).</p><p>We might hypothesize, for instance, that as the price of oil increases, on average consumers will drive less, leading to future car crashes. Again, we will need to convert the dataset time to date format, after first converting it from a number to a string, using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; car_crashes=pd.read_csv("car_crashes.csv")</strong>
<strong>&gt;&gt;&gt; car_crashes.Year=car_crashes.Year.astype(str)</strong>
<strong>&gt;&gt;&gt; car_crashes=car_crashes.convert_objects(convert_dates='coerce') </strong>
</pre></div><p>Checking the first <a id="id102" class="indexterm"/>few lines with the <code class="literal">head()</code> command confirms that we have successfully formatted the data:</p><div><img src="img/B04881_chapter02_47.jpg" alt="Joining signals and correlation"/></div><p>We can join this data to the oil prices statistics and compare the two trends over time. Notice that we need to rescale the crash data by dividing by 1000 so that it can be easily viewed on the same axis in the following command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; car_crashes['Car_Crash_Fatalities_US']=car_crashes['Car_Crash_Fatalities_US']/1000</strong>
</pre></div><p>We then use <code class="literal">merge()</code> to join the data, specifying the column to use to match rows in each dataset through the <code class="literal">on</code> variable, and plot the result using:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; oil_prices_car_crashes = pd.merge(oil_prices,car_crashes,on='Year')</strong>
<strong>&gt;&gt;&gt; oil_prices_car_crashes.plot(x='Year')</strong>
</pre></div><p>The resulting plot is shown below:</p><div><img src="img/B04881_chapter02_48.jpg" alt="Joining signals and correlation"/></div><p>How correlated <a id="id103" class="indexterm"/>are these two signals? We can again use an <code class="literal">auto_correlation</code> plot to explore this question:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; autocorrelation_plot(oil_prices_car_crashes[['Car_Crash_Fatalities_US','Oil_Price_1997_Dollars']])</strong>
</pre></div><p>Which gives:</p><div><img src="img/B04881_chapter02_49.jpg" alt="Joining signals and correlation"/></div><p>So it appears that <a id="id104" class="indexterm"/>the correlation is outside the expected fluctuation at 20 years or less, a longer range of correlation than appears in the oil prices alone.</p><div><div><h3 class="title"><a id="tip05"/>Tip</h3><p><strong>Working with large datasets</strong></p><p>The examples we give in this section are of modest size. In real-world applications, we may deal with datasets that will not fit on our computer, or require analyses that are so computationally intensive that they must be split across multiple machines to run in a reasonable timeframe. For these use cases, it may not be possible to use IPython Notebook in the form we have illustrated using Pandas DataFrames. A number of alternative applications are available for processing data at this <a id="id105" class="indexterm"/>scale, including PySpark, (<a class="ulink" href="http://spark.apache.org/docs/latest/api/python/">http://spark.apache.org/docs/latest/api/python/</a>), H20 (<a class="ulink" href="http://www.h2o.ai/">http://www.h2o.ai/</a>), <a id="id106" class="indexterm"/>and <a id="id107" class="indexterm"/>XGBoost (<a class="ulink" href="https://github.com/dmlc/xgboost">https://github.com/dmlc/xgboost</a>). We can also use many of these tools through a notebook, and thus achieve interactive manipulation and modeling for extremely large data volumes.</p></div></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec14"/>Working with geospatial data</h1></div></div></div><p>For our last case <a id="id108" class="indexterm"/>study, let us explore the analysis of geospatial data using an extension to the Pandas library, GeoPandas. You will need to have GeoPandas installed in your IPython environment to follow this example. If it is not already installed, you can add it using <code class="literal">easy_install</code> or pip.</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec29"/>Loading geospatial data</h2></div></div></div><p>In addition to our other <a id="id109" class="indexterm"/>dependencies, we will import the <code class="literal">GeoPandas</code> library using the command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; import GeoPandas as geo.</strong>
</pre></div><p>We load dataset for this example, the coordinates of countries in Africa ("Africa." Maplibrary.org. Web. 02 May 2016. <a class="ulink" href="http://www.mapmakerdata.co.uk.s3-website-eu-west-1.amazonaws.com/library/stacks/Africa/">http://www.mapmakerdata.co.uk.s3-website-eu-west-1.amazonaws.com/library/stacks/Africa/</a>) which are contained in a shape (<code class="literal">.shp</code>) file as before into a <strong>GeoDataFrame</strong>, an extension of the Pandas DataFrame, using:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; africa_map = geo.GeoDataFrame.from_file('Africa_SHP/Africa.shp')</strong>
</pre></div><p>Examining the first few lines using <code class="literal">head()</code>:</p><div><img src="img/B04881_chapter02_50.jpg" alt="Loading geospatial data"/></div><p>We can see that the data consists of identifier columns, along with a geometry object representing the shape of the country. The <code class="literal">GeoDataFrame</code> also has a <code class="literal">plot()</code> function, to which we can pass a <code class="literal">column</code> argument that gives the field to use for generating the color of each polygon using:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; africa_map.plot(column='CODE')</strong>
</pre></div><p>Which gives the following visualization:</p><div><img src="img/B04881_chapter02_51.jpg" alt="Loading geospatial data"/></div><p>However, right now this color code is based on the country name, so does not offer much insight about the map. Instead, let us try to color each country based on its population using information about the population density of each country (<em>Population by Country – Thematic Map – World</em>. <em>Population by Country – Thematic Map-World</em>. Web. 02 May 2016, <a class="ulink" href="http://www.indexmundi.com/map/?v=21">http://www.indexmundi.com/map/?v=21</a>). First we read in the population using:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; africa_populations = pd.read_csv('Africa_populations.tsv',sep='\t')</strong>
</pre></div><p>Note that here we have <a id="id110" class="indexterm"/>applied the <code class="literal">sep='\t'</code> argument to <code class="literal">read_csv()</code>, as the columns in this file are not comma separated like the other examples thus far. Now we can join this data to the geographical coordinates using merge:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; africa_map = pd.merge(africa_map,africa_populations,left_on='COUNTRY',right_on='Country_Name')</strong>
</pre></div><p>Unlike the example with oil prices and crash fatalities above, here the columns we wish to use to join the data has a different name in each dataset, so we must use the <code class="literal">left_on</code> and <code class="literal">right_on</code> arguments to specify the desired column in each table. We can then plot the map with colors derived from the population data using:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; africa_map.plot(column='Population',colormap='hot')</strong>
</pre></div><p>Which gives the new map as follows:</p><div><img src="img/B04881_chapter02_52.jpg" alt="Loading geospatial data"/></div><p>Now we can clearly <a id="id111" class="indexterm"/>see the most populous countries (Ethiopia, Democratic Republic of Congo, and Egypt) highlighted in white.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec30"/>Working in the cloud</h2></div></div></div><p>In the previous <a id="id112" class="indexterm"/>examples, we have assumed you are running the IPython notebook locally on your computer through your web browser. As mentioned, it is also possible for the application to run on an external server, with the user uploading files through the interface to interact with remotely. One convenient form <a id="id113" class="indexterm"/>of such external services are cloud platforms such as <strong>Amazon Web Services</strong> (<strong>AWS</strong>), Google Compute Cloud, and Microsoft Azure. Besides offering a hosting platform to run applications like the notebook, these services also offer storage for data sets much larger than what we would be able to store in our personal computers. By running our notebook in the cloud, we can more easily interact with these distributed storage systems using a shared infrastructure for data access and manipulation that also enforces desirable security and data governance. Lastly, cheap computing resources available via these cloud services may also allow us to scale the sorts of computation we describe in later chapters, adding extra servers to handle commands entered in the notebook on the backend.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec15"/>Introduction to PySpark</h1></div></div></div><p>So far we've mainly <a id="id114" class="indexterm"/>focused on datasets that can fit on a single machine. For larger datasets, we may need to access them through distributed file systems such as Amazon S3 or HDFS. For this purpose, we can utilize the open-source distributed computing framework PySpark (<a class="ulink" href="http://spark.apache.org/docs/latest/api/python/">http://spark.apache.org/docs/latest/api/python/</a>). PySpark is a distributed computing framework that <a id="id115" class="indexterm"/>uses the abstraction of <strong>Resilient Distributed Datasets</strong> (<strong>RDDs</strong>) for parallel collections of objects, which allows us to programmatically <a id="id116" class="indexterm"/>access a dataset as if it fits on a single machine. In later chapters we will demonstrate how to build predictive models in PySpark, but for this introduction we focus on data manipulation functions in PySpark.</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec31"/>Creating the SparkContext</h2></div></div></div><p>The first step in any <a id="id117" class="indexterm"/>spark application is the generation of the <a id="id118" class="indexterm"/>SparkContext. The SparkContext contains any job-specific configurations (such as memory settings or the number of worker tasks), and allows us to connect to a Spark cluster by specifying the master. We start the SparkContext with the following command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; sc = SparkContext('local','job_.{0}'.format(uuid.uuid4()))</strong>
</pre></div><p>The first argument gives the URL for our Spark master, the machine which coordinates execution of Spark jobs and distributes tasks to the worker machines in a cluster. All Spark jobs consist of two kinds of task: the <a id="id119" class="indexterm"/>
<strong>Driver</strong> (which issues commands and collects information <a id="id120" class="indexterm"/>about the progress of the job), and <strong>Executors</strong> (which execute operations on the RDD). These could be created on the same machine (as is the case in our example), or on different machines, allowing a dataset that will not fit in memory on a single machine to be analyzed using parallel computation across several computers. In this case we will run locally, so give the argument for the master as <code class="literal">localhost</code>, but otherwise this could be the URL of a remote machine in our cluster. The second argument is just the name we give to our application, which we specify with a uniquely generated id using the <code class="literal">uuid</code> library. If this command is successful, you should see in your terminal where you are running the notebook a stack trace such as the following:</p><div><img src="img/B04881_chapter02_53.jpg" alt="Creating the SparkContext"/></div><p>We can open the <a id="id121" class="indexterm"/>SparkUI using the address <code class="literal">http://localhost:4040</code>, which looks like this:</p><div><img src="img/B04881_chapter02_54.jpg" alt="Creating the SparkContext"/></div><p>You can see our job name in the top-right hand corner, and we can use this page to track the progress of our jobs once we begin running them. The SparkContext is now ready to receive commands, and we can see the progress of any operations we execute in our notebook in the <code class="literal">ui</code>. If you want to stop the SparkContext, we can simply use the following command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; sc.stop()</strong>
</pre></div><p>Note that if we are running locally we can only start one SparkContext on <code class="literal">localhost</code> at a time, so if we want to make changes to the context we will need to stop and restart it.  Once we have created the base SparkContext, we can instantiate other contexts objects that contain parameters and functionality for particular kinds of datasets. For this example, we will use a SqlContext, which allows us to operate on DataFrames and use SQL logic to <a id="id122" class="indexterm"/>query a dataset. We generate the SqlContext using the SparkContext as an argument:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; sqlContext = SQLContext(sc)</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec32"/>Creating an RDD</h2></div></div></div><p>To generate our first <a id="id123" class="indexterm"/>RDD, let us load the movies dataset again, and turn it <a id="id124" class="indexterm"/>into a list of tuples using all columns but the index and the row number:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; data = pd.read_csv("movies.csv")</strong>
<strong>&gt;&gt;&gt; rdd_data = sc.parallelize([ list(r)[2:-1] for r in data.itertuples()])</strong>
</pre></div><p>The <code class="literal">itertuples()</code> command returns each row of a pandas DataFrame as a tuple, which we then slice by turning it into a list and taking the indices <code class="literal">2</code> and greater (representing all columns but the index of the row, which is automatically inserted by Pandas, and the row number, which was one of the original columns in the file ). To convert this local collection, we call <code class="literal">sc.parallelize</code>, which converts a collection into an RDD. We can examine how many partitions exist in this distributed collection using the function <code class="literal">getNumPartitions()</code>:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; rdd_data.getNumPartitions()</strong>
</pre></div><p>Since we just created this dataset locally, it only has one partition. We can change the number of partitions in an RDD, which can change the load of work done on each subset of data, using the <code class="literal">repartition()</code> (to increase the number of partitions) and <code class="literal">coalesce()</code> (to decrease) functions. You can verify that the following commands change the number of partitions in our example:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; rdd_data.repartition(10).getNumPartitions() </strong>
<strong>&gt;&gt;&gt; rdd_data.coalesce(2).getNumPartitions()</strong>
</pre></div><p>If we want to examine a small sample of data from the RDD we can use the <code class="literal">take()</code> function. The following command will return five rows:</p><div><pre class="programlisting">
<strong>rdd_data.take(5)</strong>
</pre></div><p>You may notice that there is no activity on the Spark UI until you enter commands that require a result to be printed to the notebook, such as <code class="literal">getNumPartitions()</code> or <code class="literal">take()</code>. This is because Spark follows a model of lazy execution, only returning results when they are required for a downstream operation and otherwise waiting for such an operation. Besides those mentioned, other operations that will force execution are writes to disk and <code class="literal">collect()</code> (described below).</p><p>In order to load our data using the PySpark DataFrames API (similar to Pandas DataFrames) instead of an RDD (which does not have many of the utility functions for DataFrame manipulation we illustrated above), we will need a file in <strong>JavaScript Object Notation</strong> (<strong>JSON</strong>) format. We can generate this file using the following command, which maps the elements of each row into a dictionary and casts it to JSON:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; rdd_data.map( lambda x: json.JSONEncoder().encode({ str(k):str(v) for (k,v) in zip(data.columns[2:-1],x)})).\</strong>
<strong>&gt;&gt;&gt; saveAsTextFile('movies.json')</strong>
</pre></div><p>If you examine the output directory, you will notice that we have actually saved a directory with the <a id="id125" class="indexterm"/>name <code class="literal">movies.json</code> containing individual files (as many as there are partitions in our RDD). This is the same way in which data is stored in the <a id="id126" class="indexterm"/>
<strong>Hadoop distributed file system</strong> (<strong>HDFS</strong>) in directories.</p><p>Note that we have just scratched the surface of everything we can do with an RDD. We can perform other actions such as filtering, grouping RDDs by a key, projecting subsets of each row, ordering data within groups, joining to other RDDs, and many other operations. The full <a id="id127" class="indexterm"/>range of available transformations and <a id="id128" class="indexterm"/>operations is documented at <a class="ulink" href="http://spark.apache.org/docs/latest/api/python/pyspark.html">http://spark.apache.org/docs/latest/api/python/pyspark.html</a>.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec33"/>Creating a Spark DataFrame</h2></div></div></div><p>Now that we have <a id="id129" class="indexterm"/>our file in the JSON format, we can <a id="id130" class="indexterm"/>load it as a Spark DataFrame using:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; df = sqlContext.read.json("movies.json")</strong>
</pre></div><p>If we intend to perform many operations on this data, we can cache it (persist it in temporary storage), allowing us to operate on the data Spark's own internal storage format, which is optimized for repeated access. We cache the dataset using the following command.</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; df.cache()</strong>
</pre></div><p><code class="literal">SqlContext</code> also allows us to declare a table alias for the dataset:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; df.registerTempTable('movies')</strong>
</pre></div><p>We can then query this data as if it were a table in a relational database system:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; sqlContext.sql(' select * from movies limit 5 ').show()</strong>
</pre></div><p>Like the Pandas DataFrames, we can aggregate them by particular columns:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; df.groupby('year').count().collect()</strong>
</pre></div><p>We can also access individual columns using similar syntax to Pandas:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; df.year</strong>
</pre></div><p>If we want to bring all data to a single machine rather than operating on dataset partitions which may be spread across several computers, we can call the <code class="literal">collect()</code> command. Use this command with caution: for large datasets it will cause all of the partitions of the data to be combined and sent to the Drive, which could potentially overload the memory of the Driver. The <code class="literal">collect()</code> command will return an array of row objects, for which we can use <code class="literal">get()</code> to access individual elements (columns):</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; df.collect()[0].get(0)</strong>
</pre></div><p>Not all operations we are interested in performing on our data may be available in the DataFrame API, so if necessary we can convert the DataFrame into an RDD of rows using the following command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; rdd_data = df.rdd</strong>
</pre></div><p>We can even convert a PySpark DataFrame into Pandas DataFrame using:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; df.toPandas()</strong>
</pre></div><p>In later chapters, we <a id="id131" class="indexterm"/>will cover setting up applications and <a id="id132" class="indexterm"/>building models in Spark, but you should now be able to perform many of the same basic data manipulations you used in Pandas.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec16"/>Summary</h1></div></div></div><p>We have now examined many of the tasks needed to start building analytical applications. Using the IPython notebook, we have covered how to load data in a file into a DataFrame in Pandas, rename columns in the dataset, filter unwanted rows, convert column data types, and create new columns. In addition, we have joined data from different sources and performed some basic statistical analyses using aggregations and pivots. We have visualized the data using histograms, scatter plots, and density plots as well as autocorrelation and log plots for time series. We also visualized geospatial data, using coordinate files to overlay data on maps. In addition, we processed the movies dataset using PySpark, creating both an RDD and a PySpark DataFrame, and performed some basic operations on these datatypes.</p><p>We will build on these tools in future sections, manipulating the raw input to develop features for building predictive analytics pipelines. We will later utilize similar tools to visualize and understand the features and performance of the predictive models we develop, as well as reporting the insights that they may deliver.</p></div></div>
</body></html>