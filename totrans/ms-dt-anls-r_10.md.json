["```py\n> d <- dist(mtcars)\n> h <- hclust(d)\n> h\n\nCall:\nhclust(d = d)\n\nCluster method   : complete \nDistance         : euclidean \nNumber of objects: 32\n\n```", "```py\n> plot(h)\n\n```", "```py\n> plot(h)\n> rect.hclust(h, k=3, border = \"red\")\n\n```", "```py\n> (cn <- cutree(h, k = 3))\n Mazda RX4       Mazda RX4 Wag          Datsun 710 \n 1                   1                   1 \n Hornet 4 Drive   Hornet Sportabout             Valiant \n 2                   3                   2 \n Duster 360           Merc 240D            Merc 230 \n 3                   1                   1 \n Merc 280           Merc 280C          Merc 450SE \n 1                   1                   2 \n Merc 450SL         Merc 450SLC  Cadillac Fleetwood \n 2                   2                   3 \nLincoln Continental   Chrysler Imperial            Fiat 128 \n 3                   3                   1 \n Honda Civic      Toyota Corolla       Toyota Corona \n 1                   1                   1 \n Dodge Challenger         AMC Javelin          Camaro Z28 \n 2                   2                   3 \n Pontiac Firebird           Fiat X1-9       Porsche 914-2 \n 3                   1                   1 \n Lotus Europa      Ford Pantera L        Ferrari Dino \n 1                   3                   1 \n Maserati Bora          Volvo 142E \n 3                   1\n\n```", "```py\n> table(cn)\n 1  2  3 \n16  7  9\n\n```", "```py\n> round(aggregate(mtcars, FUN = mean, by = list(cn)), 1)\n Group.1  mpg cyl  disp    hp drat  wt qsec  vs  am gear carb\n1       1 24.5 4.6 122.3  96.9  4.0 2.5 18.5 0.8 0.7  4.1  2.4\n2       2 17.0 7.4 276.1 150.7  3.0 3.6 18.1 0.3 0.0  3.0  2.1\n3       3 14.6 8.0 388.2 232.1  3.3 4.2 16.4 0.0 0.2  3.4  4.0\n\n```", "```py\n> round(aggregate(mtcars, FUN = sd, by = list(cn)), 1)\n Group.1 mpg cyl disp   hp drat  wt qsec  vs  am gear carb\n1       1 5.0   1 34.6 31.0  0.3 0.6  1.8 0.4 0.5  0.5  1.5\n2       2 2.2   1 30.2 32.5  0.2 0.3  1.2 0.5 0.0  0.0  0.9\n3       3 3.1   0 58.1 49.4  0.4 0.9  1.3 0.0 0.4  0.9  1.7\n\n```", "```py\n> round(sapply(mtcars, sd), 1)\n mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb \n 6.0   1.8 123.9  68.6   0.5   1.0   1.8   0.5   0.5   0.7   1.6\n\n```", "```py\n> round(apply(\n+   aggregate(mtcars, FUN = mean, by = list(cn)),\n+   2, sd), 1)\nGroup.1     mpg     cyl    disp      hp    drat      wt    qsec \n 1.0     5.1     1.8   133.5    68.1     0.5     0.8     1.1 \n vs      am    gear    carb \n 0.4     0.4     0.6     1.0\n\n```", "```py\n> library(NbClust)\n> NbClust(mtcars, method = 'complete', index = 'dindex')\n\n```", "```py\n> NbClust(mtcars, method = 'complete', index = 'hartigan')$Best.nc\nAll 32 observations were used. \n\nNumber_clusters     Value_Index \n 3.0000         34.1696 \n> NbClust(mtcars, method = 'complete', index = 'kl')$Best.nc\nAll 32 observations were used. \n\nNumber_clusters     Value_Index \n 3.0000          6.8235\n\n```", "```py\n> NbClust(iris[, -5], method = 'complete', index = 'all')$Best.nc[1,]\nAll 150 observations were used. \n\n******************************************************************* \n* Among all indices: \n* 2 proposed 2 as the best number of clusters \n* 13 proposed 3 as the best number of clusters \n* 5 proposed 4 as the best number of clusters \n* 1 proposed 6 as the best number of clusters \n* 2 proposed 15 as the best number of clusters \n\n ***** Conclusion ***** \n\n* According to the majority rule, the best number of clusters is  3 \n\n ******************************************************************* \n KL         CH   Hartigan        CCC      Scott    Marriot \n 4          4          3          3          3          3 \n TrCovW     TraceW   Friedman      Rubin     Cindex         DB \n 3          3          4          6          3          3 \nSilhouette       Duda   PseudoT2      Beale  Ratkowsky       Ball \n 2          4          4          3          3          3 \nPtBiserial       Frey    McClain       Dunn     Hubert    SDindex \n 3          1          2         15          0          3 \n Dindex       SDbw \n 0         15\n\n```", "```py\n> (k <- kmeans(mtcars, 3))\nK-means clustering with 3 clusters of sizes 16, 7, 9\n\nCluster means:\n mpg      cyl     disp       hp     drat       wt     qsec\n1 24.50000 4.625000 122.2937  96.8750 4.002500 2.518000 18.54312\n2 17.01429 7.428571 276.0571 150.7143 2.994286 3.601429 18.11857\n3 14.64444 8.000000 388.2222 232.1111 3.343333 4.161556 16.40444\n vs        am     gear     carb\n1 0.7500000 0.6875000 4.125000 2.437500\n2 0.2857143 0.0000000 3.000000 2.142857\n3 0.0000000 0.2222222 3.444444 4.000000\n\nClustering vector:\n Mazda RX4       Mazda RX4 Wag          Datsun 710 \n 1                   1                   1 \n Hornet 4 Drive   Hornet Sportabout             Valiant \n 2                   3                   2 \n Duster 360           Merc 240D            Merc 230 \n 3                   1                   1 \n Merc 280           Merc 280C          Merc 450SE \n 1                   1                   2 \n Merc 450SL         Merc 450SLC  Cadillac Fleetwood \n 2                   2                   3 \nLincoln Continental   Chrysler Imperial            Fiat 128 \n 3                   3                   1 \n Honda Civic      Toyota Corolla       Toyota Corona \n 1                   1                   1 \n Dodge Challenger         AMC Javelin          Camaro Z28 \n 2                   2                   3 \n Pontiac Firebird           Fiat X1-9       Porsche 914-2 \n 3                   1                   1 \n Lotus Europa      Ford Pantera L        Ferrari Dino \n 1                   3                   1 \n Maserati Bora          Volvo 142E \n 3                   1 \n\nWithin cluster sum of squares by cluster:\n[1] 32838.00 11846.09 46659.32\n (between_SS / total_SS =  85.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\" \n[5] \"tot.withinss\" \"betweenss\"    \"size\"         \"iter\" \n[9] \"ifault\" \n\n```", "```py\n> all(cn == k$cluster)\n[1] TRUE\n\n```", "```py\n> library(cluster) \n> clusplot(mtcars, k$cluster, color = TRUE, shade = TRUE, labels = 2)\n\n```", "```py\n> factors <- c('cyl', 'vs', 'am', 'carb', 'gear')\n> mtcars[, factors] <- lapply(mtcars[, factors], factor)\n\n```", "```py\n> library(poLCA)\n> p <- poLCA(cbind(cyl, vs, am, carb, gear) ~ 1,\n+   data = mtcars, graphs = TRUE, nclass = 3)\n\n```", "```py\n> p$probs\nConditional item response (column) probabilities,\n by outcome variable, for each class (row) \n\n$cyl\n 4      6 8\nclass 1:  0.3333 0.6667 0\nclass 2:  0.6667 0.3333 0\nclass 3:  0.0000 0.0000 1\n\n$vs\n 0      1\nclass 1:  0.0000 1.0000\nclass 2:  0.2667 0.7333\nclass 3:  1.0000 0.0000\n\n$am\n 0      1\nclass 1:  1.0000 0.0000\nclass 2:  0.2667 0.7333\nclass 3:  0.8571 0.1429\n\n$carb\n 1      2      3      4      6      8\nclass 1:  1.0000 0.0000 0.0000 0.0000 0.0000 0.0000\nclass 2:  0.2667 0.4000 0.0000 0.2667 0.0667 0.0000\nclass 3:  0.0000 0.2857 0.2143 0.4286 0.0000 0.0714\n$gear\n 3   4      5\nclass 1:  1.0000 0.0 0.0000\nclass 2:  0.0000 0.8 0.2000\nclass 3:  0.8571 0.0 0.1429\n\n```", "```py\n> p$P\n[1] 0.09375 0.46875 0.43750\n\n```", "```py\n> rm(mtcars)\n> mtcars$gear <- factor(mtcars$gear)\n\n```", "```py\n> library(MASS)\n> d <- lda(gear ~ ., data = mtcars, CV =TRUE)\n\n```", "```py\n> (tab <- table(mtcars$gear, d$class)) \n 3  4  5\n 3 14  1  0\n 4  2 10  0\n 5  1  1  3\n\n```", "```py\n> tab / rowSums(tab)\n 3          4          5\n 3 0.93333333 0.06666667 0.00000000\n 4 0.16666667 0.83333333 0.00000000\n 5 0.20000000 0.20000000 0.60000000\n\n```", "```py\n> sum(diag(tab)) / sum(tab)\n[1] 0.84375\n\n```", "```py\n> round(d$posterior, 4)\n 3      4      5\nMazda RX4           0.0000 0.8220 0.1780\nMazda RX4 Wag       0.0000 0.9905 0.0095\nDatsun 710          0.0018 0.6960 0.3022\nHornet 4 Drive      0.9999 0.0001 0.0000\nHornet Sportabout   1.0000 0.0000 0.0000\nValiant             0.9999 0.0001 0.0000\nDuster 360          0.9993 0.0000 0.0007\nMerc 240D           0.6954 0.2990 0.0056\nMerc 230            1.0000 0.0000 0.0000\nMerc 280            0.0000 1.0000 0.0000\nMerc 280C           0.0000 1.0000 0.0000\nMerc 450SE          1.0000 0.0000 0.0000\nMerc 450SL          1.0000 0.0000 0.0000\nMerc 450SLC         1.0000 0.0000 0.0000\nCadillac Fleetwood  1.0000 0.0000 0.0000\nLincoln Continental 1.0000 0.0000 0.0000\nChrysler Imperial   1.0000 0.0000 0.0000\nFiat 128            0.0000 0.9993 0.0007\nHonda Civic         0.0000 1.0000 0.0000\nToyota Corolla      0.0000 0.9995 0.0005\nToyota Corona       0.0112 0.8302 0.1586\nDodge Challenger    1.0000 0.0000 0.0000\nAMC Javelin         1.0000 0.0000 0.0000\nCamaro Z28          0.9955 0.0000 0.0044\nPontiac Firebird    1.0000 0.0000 0.0000\nFiat X1-9           0.0000 0.9991 0.0009\nPorsche 914-2       0.0000 1.0000 0.0000\nLotus Europa        0.0000 0.0234 0.9766\nFord Pantera L      0.9965 0.0035 0.0000\nFerrari Dino        0.0000 0.0670 0.9330\nMaserati Bora       0.0000 0.0000 1.0000\nVolvo 142E          0.0000 0.9898 0.0102\n\n```", "```py\n> d <- lda(gear ~ ., data = mtcars)\n> plot(d)\n\n```", "```py\n> plot(d, dimen = 1, type = \"both\" )\n\n```", "```py\n> lr <- glm(am ~ hp + wt, data = mtcars, family = binomial)\n> summary(lr)\n\nCall:\nglm(formula = am ~ hp + wt, family = binomial, data = mtcars)\n\nDeviance Residuals: \n Min       1Q   Median       3Q      Max \n-2.2537  -0.1568  -0.0168   0.1543   1.3449 \n\nCoefficients:\n Estimate Std. Error z value Pr(>|z|) \n(Intercept) 18.86630    7.44356   2.535  0.01126 * \nhp           0.03626    0.01773   2.044  0.04091 * \nwt          -8.08348    3.06868  -2.634  0.00843 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 10.059  on 29  degrees of freedom\nAIC: 16.059\n\nNumber of Fisher Scoring iterations: 8\n\n```", "```py\n> table(mtcars$am, round(predict(lr, type = 'response')))\n 0  1\n 0 18  1\n 1  1 12\n\n```", "```py\n> library(nnet) \n> (mlr <- multinom(factor(gear) ~ ., data = mtcars)) \n# weights:  36 (22 variable)\ninitial  value 35.155593 \niter  10 value 5.461542\niter  20 value 0.035178\niter  30 value 0.000631\nfinal  value 0.000000 \nconverged\nCall:\nmultinom(formula = factor(gear) ~ ., data = mtcars)\n\nCoefficients:\n (Intercept)       mpg       cyl      disp         hp     drat\n4  -12.282953 -1.332149 -10.29517 0.2115914 -1.7284924 15.30648\n5    7.344934  4.934189 -38.21153 0.3972777 -0.3730133 45.33284\n wt        qsec        vs       am     carb\n4 21.670472   0.1851711  26.46396 67.39928 45.79318\n5 -4.126207 -11.3692290 -38.43033 32.15899 44.28841\n\nResidual Deviance: 4.300374e-08 \nAIC: 44\n\n```", "```py\n> table(mtcars$gear, predict(mlr))\n 3  4  5\n 3 15  0  0\n 4  0 12  0\n 5  0  0  5\n\n```", "```py\n> rm(mtcars)\n\n```", "```py\n> set.seed(42)\n> n     <- nrow(mtcars)\n> train <- mtcars[sample(n, n/2), ]\n\n```", "```py\n> library(dplyr)\n> train <- sample_n(mtcars, n / 2)\n\n```", "```py\n> test <- mtcars[setdiff(row.names(mtcars), row.names(train)), ]\n\n```", "```py\n> library(class)\n> (cm <- knn(\n+     train = subset(train, select = -gear),\n+     test  = subset(test, select = -gear),\n+     cl    = train$gear,\n+     k     = 5))\n[1] 4 4 4 4 3 4 4 3 3 3 3 3 4 4 4 3\nLevels: 3 4 5\n\n```", "```py\n> cor(test$gear, as.numeric(as.character(cm)))\n[1] 0.5459487\n\n```", "```py\n> table(test$gear, as.numeric(as.character(cm)))\n 3 4\n 3 6 1\n 4 0 6\n 5 1 2\n\n```", "```py\n> table(train$gear)\n3 4 5 \n8 6 2\n\n```", "```py\n> library(rpart)\n> ct <- rpart(factor(gear) ~ ., data = train, minsplit = 3)\n> summary(ct)\nCall:\nrpart(formula = factor(gear) ~ ., data = train, minsplit = 3)\n n= 16 \n\n CP nsplit rel error xerror      xstd\n1 0.75      0      1.00  1.000 0.2500000\n2 0.25      1      0.25  0.250 0.1653595\n3 0.01      2      0.00  0.125 0.1210307\n\nVariable importance\ndrat qsec  cyl disp   hp  mpg   am carb \n 18   16   12   12   12   12    9    9 \n\nNode number 1: 16 observations,    complexity param=0.75\n predicted class=3  expected loss=0.5  P(node) =1\n class counts:     8     6     2\n probabilities: 0.500 0.375 0.125 \n left son=2 (10 obs) right son=3 (6 obs)\n Primary splits:\n drat < 3.825 to the left,  improve=6.300000, (0 missing)\n disp < 212.8 to the right, improve=4.500000, (0 missing)\n am   < 0.5   to the left,  improve=3.633333, (0 missing)\n hp   < 149   to the right, improve=3.500000, (0 missing)\n qsec < 18.25 to the left,  improve=3.500000, (0 missing)\n Surrogate splits:\n mpg  < 22.15 to the left,  agree=0.875, adj=0.667, (0 split)\n cyl  < 5     to the right, agree=0.875, adj=0.667, (0 split)\n disp < 142.9 to the right, agree=0.875, adj=0.667, (0 split)\n hp   < 96    to the right, agree=0.875, adj=0.667, (0 split)\n qsec < 18.25 to the left,  agree=0.875, adj=0.667, (0 split)\n\nNode number 2: 10 observations,    complexity param=0.25\n predicted class=3  expected loss=0.2  P(node) =0.625\n class counts:     8     0     2\n probabilities: 0.800 0.000 0.200 \n left son=4 (8 obs) right son=5 (2 obs)\n Primary splits:\n am   < 0.5   to the left,  improve=3.200000, (0 missing)\n carb < 5     to the left,  improve=3.200000, (0 missing)\n qsec < 16.26 to the right, improve=1.866667, (0 missing)\n hp   < 290   to the left,  improve=1.422222, (0 missing)\n disp < 325.5 to the right, improve=1.200000, (0 missing)\n Surrogate splits:\n carb < 5     to the left,  agree=1.0, adj=1.0, (0 split)\n qsec < 16.26 to the right, agree=0.9, adj=0.5, (0 split)\n\nNode number 3: 6 observations\n predicted class=4  expected loss=0  P(node) =0.375\n class counts:     0     6     0\n probabilities: 0.000 1.000 0.000 \n\nNode number 4: 8 observations\n predicted class=3  expected loss=0  P(node) =0.5\n class counts:     8     0     0\n probabilities: 1.000 0.000 0.000 \n\nNode number 5: 2 observations\n predicted class=5  expected loss=0  P(node) =0.125\n class counts:     0     0     2\n probabilities: 0.000 0.000 1.000 \n\n```", "```py\n> plot(ct); text(ct)\n\n```", "```py\n> table(test$gear, predict(ct, newdata = test, type = 'class'))\n 3 4 5\n 3 7 0 0\n 4 1 5 0\n 5 0 2 1\n\n```", "```py\n> library(party)\n> ct <- ctree(factor(gear) ~ drat, data = train,\n+   controls = ctree_control(minsplit = 3)) \n> plot(ct, main = \"Conditional Inference Tree\")\n\n```", "```py\n> table(test$gear, predict(ct, newdata = test, type = 'node'))\n 2 3\n 3 7 0\n 4 1 5\n 5 0 3\n\n```", "```py\n> library(randomForest)\n> (rf <- randomForest(factor(gear) ~ ., data = train, ntree = 250))\nCall:\n randomForest(formula = factor(gear) ~ ., data = train, ntree = 250) \n Type of random forest: classification\n Number of trees: 250\nNo. of variables tried at each split: 3\n\n OOB estimate of  error rate: 25%\nConfusion matrix:\n 3 4 5 class.error\n3 7 1 0   0.1250000\n4 1 5 0   0.1666667\n5 2 0 0   1.0000000\n\n```", "```py\n> table(test$gear, predict(rf, test)) \n 3 4 5\n 3 7 0 0\n 4 1 5 0\n 5 1 2 0\n\n```", "```py\n> plot(rf)\n> legend('topright',\n+   legend = colnames(rf$err.rate),\n+   col    = 1:4,\n+   fill   = 1:4,\n+   bty    = 'n')\n\n```", "```py\n> library(caret)\n\n```", "```py\n> library(C50)\n> C50 <- train(factor(gear) ~ ., data = train, method = 'C5.0')\n> summary(C50)\n\nC5.0 [Release 2.07 GPL Edition]    Fri Mar 20 23:22:10 2015\n-------------------------------\n\nClass specified by attribute `outcome'\n\nRead 16 cases (11 attributes) from undefined.data\n\n-----  Trial 0:  -----\n\nRules:\n\nRule 0/1: (8, lift 1.8)\n drat <= 3.73\n am <= 0\n ->  class 3  [0.900]\n\nRule 0/2: (6, lift 2.3)\n drat > 3.73\n ->  class 4  [0.875]\n\nRule 0/3: (2, lift 6.0)\n drat <= 3.73\n am > 0\n ->  class 5  [0.750]\n\nDefault class: 3\n\n*** boosting reduced to 1 trial since last classifier is very accurate\n\n*** boosting abandoned (too few classifiers)\n\nEvaluation on training data (16 cases):\n\n Rules \n ----------------\n No      Errors\n\n 3    0( 0.0%)   <<\n\n (a)   (b)   (c)    <-classified as\n ----  ----  ----\n 8                (a): class 3\n 6          (b): class 4\n 2    (c): class 5\n\n Attribute usage:\n\n 100.00%  drat\n 62.50%  am\n\n```", "```py\n> table(test$gear, predict(C50, test))\n 3 4 5\n 3 7 0 0\n 4 1 5 0\n 5 0 3 0\n\n```"]