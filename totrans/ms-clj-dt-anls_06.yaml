- en: Chapter 6. Sentiment Analysis – Categorizing Hotel Reviews
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: People talk about a lot of things online. There are forums and communities for
    almost everything under the sun, and some of them may be about your product or
    service. People may complain, or they may praise, and you would want to know which
    of the two they're doing.
  prefs: []
  type: TYPE_NORMAL
- en: This is where sentiment analysis helps. It can automatically track whether the
    reviews and discussions are positive or negative overall, and it can pull out
    items from either category to make them easier to respond to or draw attention
    to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Over the course of this chapter, we''ll cover a lot of ground. Some of it will
    be a little hazy, but in general, here''s what we''ll cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring and preparing the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining the error rates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we go any further, let's learn what sentiment analysis is.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sentiment analysis is a form of text categorization that works on opinions
    instead of topics. Often, texts are categorized according to the subject they
    discuss. For example, sentiment analysis attempts to categorize texts according
    to the opinions or emotions of the writers, whether the text is about cars or
    pets. Often, these are cast in binary terms: good or bad, like or dislike, positive
    or negative, and so on. Does this person love Toyotas or hate them? Are Pugs the
    best or German Shepherds? Would they go back to this restaurant? Questions like
    these have proven to be an important area of research, simply because so many
    companies want to know what people say about their goods and services online.
    This provides a way for companies'' marketing departments to monitor people''s
    opinions about their products or services as they talk on Twitter and other online
    public forums. They can reach out to unhappy customers to provide better, more
    proactive customer service or reach out to satisfied ones to strengthen their
    relationships and opinions.'
  prefs: []
  type: TYPE_NORMAL
- en: As you can imagine, categorizing based on opinion than on topics is much more
    difficult. Even basic words tend to take on multiple meanings that are very dependent
    on their contexts.
  prefs: []
  type: TYPE_NORMAL
- en: For example, take the word *good*. In a review, I can say that something is
    *good*. I can also say that it's not good, no good, or so far from good that It
    can almost see it on a clear day. On the other hand, I can say that something's
    *bad*. Or can I say that it's *not bad*. Or, if I'm stuck in the '80s, I can say
    that "I love it, it's so bad."
  prefs: []
  type: TYPE_NORMAL
- en: This is a very important and interesting problem, so people have been working
    on it for a number of years. An early paper on this topic came in 2002, *Thumbs
    up? Sentiment classification using machine learning techniques*, published by
    *Bo Pang*, *Lillian Lee*, and *Shivakumar Vaithyanathan*. In this paper, they
    compared movie reviews using naive Bayes' maximum entropy and support vector machines
    to categorize movie reviews into positive and negative. They also compared a variety
    of feature types such as unigrams, bigrams, and other combinations. In general,
    they found that support vector machines with single tokens performed best, although
    the difference wasn't usually huge.
  prefs: []
  type: TYPE_NORMAL
- en: Together and separately, *Bo Pang*, *Lillian Lee*, and many others have extended
    sentiment analysis in interesting ways. They've attempted to go beyond simple
    binary classifications toward predicting finer-grained sentiments. For example,
    they've worked on systems to predict from a document the number of stars the author
    of the review would give the reviewed service or object on a four-star or five-star
    rating system.
  prefs: []
  type: TYPE_NORMAL
- en: Part of what makes this interesting is that the baseline is how well the system
    explicitly agrees with the judgment of the human raters. However, in research,
    human raters only agree about 79 percent of the time, so a system that agrees
    with human raters 60 or 70 percent of the time is doing pretty well.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hotel review data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, we'll look at the **OpinRank Review** dataset ([http://archive.ics.uci.edu/ml/datasets/OpinRank+Review+Dataset](http://archive.ics.uci.edu/ml/datasets/OpinRank+Review+Dataset)).
    This is a dataset that contains almost 260,000 reviews for hotels ([http://tripadvisor.com/](http://tripadvisor.com/))
    from around the world on **TripAdvisor** as well as more than 42,000 car reviews
    ([http://edmunds.com/](http://edmunds.com/)) from 2007, 2008, and 2009 on **Edmunds**.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we look at some of these reviews, we can see just how difficult categorizing
    the reviews as positive or negative is, even for humans.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, some words are used in ways that aren''t associated with their
    straightforward meaning. For example, look at the use of the term *greatest* in
    the following quote from a review for a Beijing hotel:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Not the greatest area but no problems, even at 3:00 AM."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Also, many reviews recount both good and bad aspects of the hotel that they''re
    discussing, even if the final review decidedly comes down one way or the other.
    This review of a London hotel starts off listing the positives, but then it pivots:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"… These are the only real positives. Everything else was either average or
    below average...."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Another reason why reviews are difficult to classify is that many reviews just
    don''t wholeheartedly endorse whatever it is they''re reviewing. Instead, the
    review will be tepid, or the reviewers qualify their conclusions as they did in
    this review for a Las Vegas hotel:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"It''s faded, but it''s fine. If you''re on a budget and want to stay on the
    Strip, this is the place. But for a really great inexpensive experience, try the
    Main Street Station downtown."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All of these factors contribute toward making this task more difficult than
    standard document classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this experiment, I've randomly selected 500 hotel reviews and classified
    them manually. A better option might be to use Amazon's Mechanical Turk ([https://www.mturk.com/mturk/](https://www.mturk.com/mturk/))
    to get more reviews classified than any one person might be able to do easily.
    Really, a few hundred is about the minimum that we'd like to use as both the training
    and test sets need to come from this. I made sure that the sample contained an
    equal number of positive and negative reviews. (You can find the sample in the
    `data` directory of the code download.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The data files are **tab-separated values** (**TSV**). After being manually
    classified, each line had four fields: the classification as a `+` or `-` sign,
    the date of the review, the title of the review, and the review itself. Some of
    the reviews are quite long.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After tagging the files, we''ll take those files and create feature vectors
    from the vocabulary of the title and create a review for each one. For this chapter,
    we''ll see what works best: unigrams (single tokens), bigrams, trigrams, or part-of-speech
    annotated unigrams. These features comprise several common ways to extract features
    from the text:'
  prefs: []
  type: TYPE_NORMAL
- en: Unigrams are single tokens, for example, features from the preceding sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bigrams are two tokens next to each other, for example, *features comprise*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trigrams are three tokens next to each other, for example, *features comprise
    several*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part-of-speech annotated unigrams would look something like `features_N`, which
    just means that the unigram features is a noun.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll also use these features to train a variety of classifiers on the reviews.
    Just like *Bo Pang* and *Lillian Lee* did, we'll try experiments with naive Bayes
    maximum entropy classifiers. To compare how well each of these does, we'll use
    cross validation to train and test our classifier multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we get started on the code for this chapter, note that the Leiningen
    2 `project.clj` file looks like the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: First, let's create some functions to handle tokenization. Under the cover's,
    we'll use methods from the **OpenNLP** library ([http://opennlp.apache.org/](http://opennlp.apache.org/))
    to process the next methods from the **Weka machine learning** library ([http://www.cs.waikato.ac.nz/ml/weka/](http://www.cs.waikato.ac.nz/ml/weka/))
    to perform the sentiment analysis. However, we'll wrap these to provide a more
    natural, Clojure-like interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start in the `src/sentiment/tokens.clj` file, which will begin in the
    following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Our tokenizer will use `SimpleTokenizer` from the OpenNLP library and normalize
    all characters to lowercase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'I''ve aliased the `sentiment.tokens` namespace to `t` in the REPL. This function
    is used to break an input string into a sequence of token substrings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Next, we'll take the token streams and create feature vectors from them.
  prefs: []
  type: TYPE_NORMAL
- en: Creating feature vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A feature vector is a vector that summarizes an observation or document. Each
    vector contains the values associated with each variable or feature. The values
    may be boolean, indicating the presence or absence with 0 or 1, they may be raw
    counts, or they may be proportions scaled by the size of the overall document.
    As much of machine learning is based on linear algebra, vectors and matrices are
    very convenient data structures.
  prefs: []
  type: TYPE_NORMAL
- en: In order to maintain consistent indexes for each feature, we have to maintain
    a mapping from feature to indexes. Whenever we encounter a new feature, we need
    to assign it to a new index.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the following table traces the steps to create a feature vector
    based on token frequencies from the phrase *the cat in the hat*.
  prefs: []
  type: TYPE_NORMAL
- en: '| Step | Feature | Index | Feature Vector |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | the | 0 | [1] |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | cat | 1 | [1, 1] |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | in | 2 | [1, 1, 1] |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | the | 0 | [2, 1, 1] |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | hat | 3 | [2, 1, 1, 1] |'
  prefs: []
  type: TYPE_TB
- en: So, the final feature vector for *the cat in the hat* would be `[2, 1, 1, 1]`.
    In this case, we're counting the features. In other applications, we might use
    a bag-of-words approach that only tests the presence of the features. In that
    case, the feature vector would be `[1, 1, 1, 1]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll include the code to do this in the `sentiment.tokens` namespace. First,
    we''ll create a function that increments the value of a feature in the feature
    vector. It looks up the index of the feature in the vector from the feature index
    (`f-index`). If the feature hasn''t been seen yet, this function also allocates
    an index for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use this function to convert a feature sequence into a feature vector.
    This function initially creates a vector of zeroes for the feature sequence, and
    then it reduces over the features, updating the feature index and vector as necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, for this task, we have several functions that we''ll look at together.
    The first function, `accum-features`, builds the index and the list of feature
    vectors. Each time it''s called, it takes the sequence of features passed to it
    and creates a feature vector. It appends this to the collection of feature vectors
    also passed into it. The next function, `pad-to`, makes sure that the feature
    vector has the same number of elements as the feature index. This makes it slightly
    easier to work with the feature vectors later on. The final function takes a list
    of feature vectors and returns the feature index and vectors for this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use these functions to build up a matrix of feature vectors from a set
    of input sentences. Let''s see how this works in the first few sentences of an
    *Emily Dickinson* poem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Notice that after tokenizing each document, we created a set of the tokens.
    This changes the system here to use a bag-of-words approach. We're only looking
    at the presence or absence of a feature, not its frequency. This does put the
    tokens out of order, `nobody` was evidently the first token indexed, but this
    doesn't matter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, by inverting the feature index, we can look up the words in a document
    from the features that it contains. This allows us to recreate a frequency map
    for each document as well as to recreate the tokens in each document. In this
    case, we''ll look up the words from the first feature vector, `I''m nobody`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This block of code gets the indexes for each position in the feature vector,
    removes the features that didn't occur, and then looks up the index in the inverted
    feature index. This provides us with the sequence of features that occurred in
    that document. Notice that they're out of order. This is to be expected because
    neither the input sequence of features (in this case a set) nor the feature vector
    itself preserves the order of the features.
  prefs: []
  type: TYPE_NORMAL
- en: Creating feature vector functions and POS tagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll also include some functions to turn a list of tokens into a list of features.
    By wrapping these into functions, we make it easier to compose pipelines of processing
    functions and experiment with different feature sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest and probably the most common type of feature is the unigram or
    a single token. As the `tokenize` function already outputs single functions, the
    `unigram` function is very simple to implement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Another way to construct features is to use a number of consecutive tokens.
    In the abstract, these are called n-grams. Bigrams (two tokens) and trigrams (three
    tokens) are common instances of this type of function. We''ll define all of these
    as functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a number of different features we could create and experiment with,
    but we won''t show them all here. However, before we move on, here''s one more
    common type of feature: the token tagged with its **part of speech** (**POS**).
    POS is the category for words, which determines their range of uses in sentences.
    You probably remember these from elementary school. Nouns are people, places,
    and things. Verbs are actions.'
  prefs: []
  type: TYPE_NORMAL
- en: To get this information, we'll use OpenNLP's trained POS tagger. This takes
    a word and associates it with a part of speech. In order to use this, we need
    to download the training model file. You can find it at [http://opennlp.sourceforge.net/models-1.5/](http://opennlp.sourceforge.net/models-1.5/).
    Download **en POS tagger** (English) with a description of **Maxent model with
    tag dictionary**. The file itself is named `en-pos-maxent.bin`, and I put it into
    the `data` directory of my project.
  prefs: []
  type: TYPE_NORMAL
- en: This tagger uses the POS tags defined by the Penn Treebank ([http://www.cis.upenn.edu/~treebank/](http://www.cis.upenn.edu/~treebank/)).
    It uses a trained, probabilistic tagger to associate tags with each token from
    a sentence. For example, it might associate the token things with the `NNS` tag,
    which is the abbreviation for plural nouns. We'll create the string for this feature
    by putting these two together so that this feature would look like `things_NNS`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the data file, we need to load it into a POS model. We''ll write
    a function to do this and return the tagger object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the tagger is pretty easy. We just call its tag method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have these functions ready, let's take a short sentence and generate
    the features for it. For this set of examples, we'll use the clauses, `Time flies
    like an arrow; fruit flies like a banana`. To begin with, we'll define the input
    data and load the POS tagger.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the last output, the words are associated with part-of-speech tags. This
    output uses the tags from the Penn Treebank ([http://www.cis.upenn.edu/~treebank/](http://www.cis.upenn.edu/~treebank/)).
    You can look at it for more information, but briefly, here are the tags used in
    the preceding code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NN` means noun;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VBZ` means the present tense verb, third person, singular;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IN` means and, the preposition or subordinating conjunction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DT` means the determiner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So we can see that the POS-tagged features provide the most data on the single
    tokens; however, the n-grams (bigrams and trigrams) provide more information about
    the context around each word. Later on, we'll see which one gets better results.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the preprocessing out of way, let's turn our attention to the
    documents and how we want to structure the rest of the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validating the results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As I've already mentioned, the dataset for this chapter is a manually coded
    group of 500 hotel reviews taken from the OpinRank dataset. For this experiment,
    we'll break these into 10 chunks of 50 reviews each.
  prefs: []
  type: TYPE_NORMAL
- en: These chunks will allow us to use **K-fold cross validation** to test how our
    system is doing. Cross validation is a way of checking your algorithm and procedures
    by splitting your data up into equally sized chunks. You then train your data
    on all of the chunks but one; that is the training set. You calculate the error
    after running the trained system on the validation set. Then, you use the next
    chunk as a validation set and start over again. Finally, we can average the error
    for all of the trials.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the validation procedure uses four folds, A, B, C, and D. For the
    first run, A, B, and C would be the training set, and D would be the test set.
    Next, A, B, and D would be the training set, and C would be the test set. This
    would continue until every fold is used as the test set once.
  prefs: []
  type: TYPE_NORMAL
- en: This may seem like a lot of work, but it helps us makes sure that we didn't
    just get lucky with our choice of training or validation data. It provides a much
    more robust way of estimating the error rates and accuracy of our classifier.
  prefs: []
  type: TYPE_NORMAL
- en: The main trick in implementing cross validation is that Clojure's native partitioning
    functions (`partition` and `partition-all`) don't handle extra items exactly the
    way we'd like. The `partition` function just throws the extras away, and `partition-all`
    sticks all of the extras to the end in a smaller group. What we'd like is to include
    the extras in the previous chunks. Each chunk should have one extra until all
    of the remainders are exhausted. To handle this, we'll define a function named
    `partition-spread`. It will partition the first part of the collection into larger
    chunks and the second part into smaller chunks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, we''ll need to know the size of the input collection. To do
    this, we must hold the entire collection in the memory at once, so this algorithm
    isn''t good for very large sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now see how these partitioning functions differ:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We can also see that the semantics of the first parameter have changed. Instead
    of indicating the size of the partitions, it specifies the number of partitions.
    Now the partitions are all of a roughly equal size.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll create a couple of functions that pull out each chunk to use as
    the validation set and concatenates all the other chunks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, by partitioning into chunks with one element each, we can clearly see
    just how the K-fold partitioning works. Each time, a new chunk is selected as
    the validation set (the first item), and the rest of the chunks are concatenated
    into the training set (the second item):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can define a function that controls the K-fold validation process. It
    takes the training and error steps as function parameters, and it just handles
    partitioning the data into groups, calling the training and error functions, and
    combining their output into one result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Now we need to decide what constitutes an error and how we'll compute it.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating error rates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To calculate the error rates on classification algorithms, we''ll keep count
    of several things. We''ll track how many positives are correctly and incorrectly
    identified as well as how many negatives are correctly and incorrectly identified.
    These values are usually called true positives, false positives, true negatives,
    and false negatives. The relationship of these values to the expected values and
    the classifier''s outputs and to each other can be seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Calculating error rates](img/4139OS_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From these numbers, we'll first calculate the precision of the algorithm. This
    is the ratio of true positives to the number of all identified positives (both
    true and false positives). This tells us how many of the items that it identified
    as positives actually are positives.
  prefs: []
  type: TYPE_NORMAL
- en: We'll then calculate the recall. This is the ratio of true positives to all
    actual positives (true positives and false negatives). This gives us an idea of
    how many positives it's missing.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate this, we''ll use a standard `reduce` loop. First, we''ll write
    the accumulator function for it. This will take a mapping of the counts that we
    need to tally and a pair of ratings, the expected and the actual. Depending on
    what they are and whether they match, we''ll increment one of the counts as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the counts for a test set, we''ll need to summarize these counts
    into the figure for precision and recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'With these two defined, the function to actually calculate the error is standard
    Clojure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can do something similar to determine the mean error of a collection of
    precision/recall mappings. We could simply figure the value for each key separately,
    but rather than walking over the collection multiple times, we will do something
    more complicated and walk over it once while calculating the sums for each key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: These functions will give us a good grasp of the performance of our classifiers
    and how well they do at identifying the sentiments expressed in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Weka machine learning library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're going to test a couple of machine learning algorithms that are commonly
    used for sentiment analysis. Some of them are implemented in the OpenNLP library.
    However, they do not have anything for others algorithms. So instead, we'll use
    the Weka machine learning library ([http://www.cs.waikato.ac.nz/ml/weka/](http://www.cs.waikato.ac.nz/ml/weka/)).
    This doesn't have the classes to tokenize or segment the data that an application
    in a natural language processing requires, but it does have a more complete palette
    of machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: All of the classes in the Weka library also have a standard, consistent interface.
    These classes are really designed to be used from the command line, so each takes
    its options as an array of strings with a command-line-like syntax. For example,
    the array for a naive Bayesian classifier may have a flag to indicate that it
    should use the kernel density estimator rather than the normal distribution. This
    would be indicated by the `-K` flag being included in the option array. Other
    options may include a parameter that would follow the option in the array. For
    example, the logistic regression classifier can take a parameter to indicate the
    maximum number of iterations it should run. This would include the items `-M`
    and `1000` (say) in the options array.
  prefs: []
  type: TYPE_NORMAL
- en: The Clojure interface functions for these classes are very regular. In fact,
    they're almost boilerplate. Unfortunately, they're also a little redundant. Option
    names are repeated in the functions' parameter list, the default values for those
    parameters, and where the parameters are fed into the options array. It would
    be better to have one place for a specification of each option, its name, its
    flag, its semantics, and its default value.
  prefs: []
  type: TYPE_NORMAL
- en: This is a perfect application of Clojure's macro system. The data to create
    the functions can be transformed into the function definition, which is then compiled
    into the interface function.
  prefs: []
  type: TYPE_NORMAL
- en: The final product of this is the `defanalysis` macro, which takes the name of
    the function, the class, the method it's based on, and the options it accepts.
    We'll see several uses of it later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, at almost 40 lines, this system is a little long and disruptive
    to include here, however interesting it may be. You can find this in the `src/sentiment/weka.clj`
    file in the code download, and I have discussed it in a bit more length in *Clojure
    Data Analysis Cookbook*, *Packt Publishing*.
  prefs: []
  type: TYPE_NORMAL
- en: We do still need to convert the `HotelReview` records that we loaded earlier
    into a `WekaInstances` collection. We'll need to do this several times as we train
    and test the classifiers, and this will provide us with a somewhat shorter example
    of interacting with Weka.
  prefs: []
  type: TYPE_NORMAL
- en: To store a data matrix, Weka uses an `Instances` object. This implements a number
    of standard Java collection interfaces, and it holds objects that implement the
    `Instance` interface, such as `DenseInstance` or `SparseInstance`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instances also keep track of which fields each item has in its collection of
    `Attribute` objects. To create these, we''ll populate `ArrayList` with all of
    the features that we accumulated in the feature index. We''ll also create a feature
    for the ratings and add it to `ArrayList`. We''ll return both the full collection
    of the attributes and the single attribute for the review''s rating:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: (At this point, we're hardcoding the markers for the sentiments as a plus sign
    and a negative sign. However, these could easily be made into parameters for a
    more flexible system.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Each hotel review itself can be converted separately. As most documents will
    only have a fraction of the full number of features, we''ll use `SparseInstance`.
    Sparse vectors are more memory efficient if most of the values in the instance
    are zero. If the feature is nonzero in the feature vector, we''ll set it in `Instance`.
    Finally, we''ll also set the rating attribute as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'With these, we can populate `Instances` with the data from the `HotelReview`
    records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Now we can define some functions to sit between the cross-validation functions
    we defined earlier and the Weka interface functions.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting Weka and cross-validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first of these functions will classify an instance and determine which
    rating symbol it is classified by (`+` or `-`), given the distribution of probabilities
    for each category. This function is used to run the classifier on all data in
    an `Instances` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The next function defines the cross-validation procedure for a group of `HotelReview`
    records. This function actually takes a training function and returns a function
    that takes the feature index and collection of `HotelReview` records and actually
    performs the cross validation. This will allow us to create some wrapper functions
    for each type of classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: When executed, this function will return a list of ten of whatever the `do-test`
    function returns. In this case, that means a list of ten precision and recall
    mappings. We can average the output of this to get a summary of each classifier's
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can start actually defining and testing classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding maximum entropy classifiers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Maximum entropy (maxent) classifiers are, in a sense, very conservative classifiers.
    They assume nothing about hidden variables and base their classifications strictly
    upon the evidence they've been trained on. They are consistent with the facts
    that they've seen, but all other distributions are assumed to be completely uniform
    otherwise. What does this mean?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that we have a set of reviews and positive or negative ratings,
    and we wish to be able to predict the value of ratings when the ratings are unavailable,
    given the tokens or other features in the reviews. The probability that a rating
    is positive would be p(+). Initially, before we see any actual evidence, we may
    intuit that this probability would be uniform across all possible features. So,
    for a set of five features, before training, we might expect the probability function
    to return these values:'
  prefs: []
  type: TYPE_NORMAL
- en: '| p(+) | ½ |'
  prefs: []
  type: TYPE_TB
- en: '| p(-) | ½ |'
  prefs: []
  type: TYPE_TB
- en: This is perfectly uniform but not very useful. We have to make observations
    from the data in order to train the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: The process of training involves observing the features in each document and
    its rating and determining the probability of any given feature that is found
    in a document with a given rating. We'll denote this as p(x, y) or the probability
    as feature x and rating y.
  prefs: []
  type: TYPE_NORMAL
- en: These features impose constraints on our model. As we gather more and more constraints,
    figuring a consistent and uniform distribution for the non-constrained probabilities
    in the model becomes increasingly difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, this is the maxent algorithm's job. It takes into account all of
    the constraints imposed by the probabilities found in the training data, but it
    maintains a uniform distribution on everything that's unconstrained. This provides
    a more consistent, stronger algorithm overall, and it still performs very well,
    usually. Also, cross validation can help us evaluate its performance.
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit is that maxent doesn't make any assumptions about the relationships
    between different features. In a bit, we'll look at a naive Bayesian classifier,
    and it does make an assumption about the relationships between the features, an
    often unrealistic assumption. Because maxent does not make that assumption, it
    can better match the data involved.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this chapter, we''ll use the maxent classifier found in the Weka class,
    `weka.classifiers.functions.Logistic` (maxent is equivalent to the logistic regression,
    which attempts to classify data based on a binary categorical label, which is
    based on one or more features). We''ll use the `defanalysis` macro to define a
    utility function that cross validates a logistic regression classifier as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Now let's define something similar for a naive Bayesian classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding naive Bayesian classifiers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common, generally well-performing classifier is the naive Bayesian classifier.
    It's naive because it makes an assumption about that data and the features; it
    assumes that the features are independent of each other. That is, the probability
    of, say, *good* occurring in a document is not influenced at all by the probability
    of any other token or feature, such as, say, *not*. Unfortunately, language doesn't
    work this way, and there are dependencies all through the features of any linguistic
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, even when the data and features are not completely independent,
    this classifier often still performs quite well in practice. For example, in *An
    analysis of data characteristics that affect naive Bayes performance* by *Irina
    Rish*, *Joseph Hellerstein*, and *Jayram Thathachar*, it was found that Bayesian
    classifiers perform best with features that are completely independent or functionally
    dependent.
  prefs: []
  type: TYPE_NORMAL
- en: 'This classifier works by knowing several probabilities and then using Bayes''
    theorem to turn them around to predict the classification of the document. The
    following are the probabilities that it needs to know:'
  prefs: []
  type: TYPE_NORMAL
- en: It needs to know the probability for each feature in the training set. We'll
    call this **p(F)**. Say the word *good* occurs in 40 percent of the documents.
    This is the evidence of the classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It needs to know the probability that a document will be part of a classification.
    We'll call this **p(C)**. Say that the rate of positive ratings in the corpus
    of reviews is 80 percent. This is the prior distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now it needs to know the probability that the good feature is in the document
    if the document is rated positively. This is **p(F|C)**. For this hypothetical
    example, say that *good* appears in 40 percent of the positive reviews. This is
    the likelihood.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayes theorem allows us to turn this around and compute the probability that
    a document is positively rated, if it contains the feature *good*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding naive Bayesian classifiers](img/4139OS_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For this example, this turns out to be `(0.8)(0.4) / 0.4`, or 0.8 (80 percent).
    So, if the document contains the feature *good*, it is very likely to be positively
    rated.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, things begin to get more and more interesting as we start to track
    more and more features. If the document contains both *not* and *good*, for instance,
    the probability that the review is positive may change drastically.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Weka implementation of a naive Bayesian classifier is found in `weka.classifiers.bayes.NaiveBayes`,
    and we''ll wrap it in a manner that is similar to the one we used for the maxent
    classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have both the classifiers in place, let's look again at the features
    we'll use and how we'll compare everything.
  prefs: []
  type: TYPE_NORMAL
- en: Running the experiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Remember, earlier we defined functions to break a sequence of tokens into features
    of various sorts: unigrams, bigrams, trigrams, and POS-tagged unigrams. We can
    take these and automatically test both the classifiers against all of these types
    of features. Let''s see how.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll define some top-level variables that associate label keywords
    with the functions that we want to test at that point in the process (that is,
    classifiers or feature-generators):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We can now iterate over both of these hash maps and cross-validate these classifiers
    on these features. We'll average the error information (the precision and recall)
    for all of them and return the averages. Once we've executed that, we can spend
    some time looking at the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the inner-most loop of this process, we''ll take a collection of features
    and a classifier and cross validate them. This is pretty straightforward; it simply
    constructs an identifying key out of the keywords for the feature generator and
    the classifier, runs the cross validation, and averages the output error information
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, given a set of features, we''ll call `do-class` on each classifier one
    loop up. Constructing the loop this way by generating the features and then looping
    on the classifiers keeps us from needing to regenerate the same set of features
    multiple times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The controlling function for this process simply calls `do-features` on each
    set of feature-generating functions and stores all the outputs into a hash map:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This takes a while to execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Now we can start looking at the data in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Examining the results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s examine the precision of the classifiers. Remember that the precision
    is how well the classifiers do at only returning positive reviews. This indicates
    the percentage of reviews that each classifier has identified as being positive
    is actually positive in the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Examining the results](img/4139OS_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We need to remember a couple of things while looking at this graph. First, sentiment
    analysis is difficult, compared to other categorization tasks. Most importantly,
    human raters only agree about 80 percent of the time. So, the bar seen in the
    preceding figure that almost reaches 65 percent is actually decent, if not great.
    Still, we can see that the naive Bayesian classifier generally outperforms the
    maxent one for this dataset, especially when using unigram features. It performed
    less well for the bigram and trigram features, and slightly lesser for the POS-tagged
    unigrams.
  prefs: []
  type: TYPE_NORMAL
- en: We didn't try tagging the bigram and trigrams with POS information, but that
    might have been an interesting experiment. Based on what we can see here, these
    feature generators would not get better results than what we've already tested,
    but it would be good to know that more definitively.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s interesting to see that maxent performed best with trigrams. Generally,
    compared to unigrams, trigrams pack more information into each feature, as they
    encode some implicit syntactical information into each feature. However, each
    feature also occurs fewer times, which makes performing some statistical processes
    on it more difficult. Remember that recall is the percentage of positives in the
    test set that were correctly identified by each classifier. Now let''s look at
    the recall of these classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Examining the results](img/4139OS_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: First, while the naive Bayesian classifier still outperforms the maxent classifier,
    this time the bigram and trigram get much better results than the unigram or POS-tagged
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the recall numbers on these two tests are better than any of the values
    for the precision. The best part is that the naive Bayes bigram test had a recall
    of just over 90 percent.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, just looking at the results, there appeared to be an inverse relationship
    between the precision and the recall, as there typically is. Tests with high precision
    tended to have lower recall numbers and vice versa. This makes intuitive sense.
    A classifier can get a high recall number by marking more reviews as positive,
    but that negatively impacts its precision. Or, a classifier can have better precision
    by being more selective in what it marks as positive but also noting that will
    drag down its recall.
  prefs: []
  type: TYPE_NORMAL
- en: Combining the error rates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can combine these two into a single metric using the harmonic mean of the
    precision and recall, also known as the F-measure. We''ll compute this with the
    following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us a way to combine the precision and recall in a rational, meaningful
    manner. Let''s see what values it gives for the F-measure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Combining the error rates](img/4139OS_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, as we've already noticed, the naive Bayesian classifier performed better
    than the maxent classifier in general, and on balance, the bigram features worked
    best for this classifier.
  prefs: []
  type: TYPE_NORMAL
- en: While this gives us a good starting point, we'll also want to consider why we're
    looking for this information, how we'll use it, and what penalties are involved.
    If it's vitally important that we get all the positive reviews, then we will definitely
    want to use the naive Bayesian classifier with the bigram features. However, if
    the cost of missing some isn't so high but the cost of having to sort through
    too many false results is high, then we'll probably want to use unigram features,
    which would minimize the number of false results we have to manually sort through
    later.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What could we do to improve these results?
  prefs: []
  type: TYPE_NORMAL
- en: First, we should improve the test and training sets. It would be good to have
    multiple raters, say, have each review independently reviewed three times and
    use the rating that was chosen two or three times.
  prefs: []
  type: TYPE_NORMAL
- en: Most importantly, we'd like to have a larger and better test set and training
    set. For this type of problem, having 500 observations is really on the low end
    of what you can do anything useful with, and you can expect the results to improve
    with more observations. However, I do need to stress on the fact that more training
    data doesn't necessarily imply better results. It could help, but there are no
    guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: We could also look at improving the features. We could select them more carefully,
    because having too many useless or unneeded features can make the classifier perform
    poorly. We could also select different features such as dates or information about
    the informants; if we had any data on them, it might be useful.
  prefs: []
  type: TYPE_NORMAL
- en: There has also been more recent work in moving beyond polarity classification,
    such as looking at emotional classification. Another way of being more fine grained
    than binary categorization is to classify the documents on a scale. For instance,
    instead of positive or negative, these classifiers could try to predict how the
    user would rate the product on a five-star scale, such as what has become popular
    on **Amazon** and many websites that include user ratings and reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have identified the positive or negative reviews, we can apply other
    analyses separately to those reviews, whether its topic modeling, named entity
    recognition, or something else.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the end, sentiment analysis is a simple tool to analyze documents according
    to two complex, possibly ill-defined categories. Although language is used in
    complex ways, modern sentiment analysis techniques can do almost as well as humans,
    which, admittedly, isn't particularly efficient.
  prefs: []
  type: TYPE_NORMAL
- en: What's most powerful about these techniques is that they can provide answers
    to questions that cannot be answered in other ways. As such, they're an important
    part of the data analyst's toolbox.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll look at null hypothesis testing, which is a standard
    and foundational technique of traditional statistics. This informs how we approach
    many experiments and how we frame the questions that we're asking. By following
    these guides, we can make sure that our results are more valid and generalizable.
  prefs: []
  type: TYPE_NORMAL
