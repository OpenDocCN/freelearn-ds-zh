["```py\nBangalore,Onion,60\nBangalore,Chilli,10\nBangalore,Pizza,120\nBangalore,Burger,80\nNewDelhi,Onion,80\nNewDelhi,Chilli,30\nNewDelhi,Pizza,150\nNewDelhi,Burger,180\nKolkata,Onion,90\nKolkata,Chilli,20\nKolkata,Pizza,120\nKolkata,Burger,160\n```", "```py\n#!/usr/bin/env perl -wl\n\nuse strict;\nuse warnings;\n\nwhile(<STDIN>) {\n    chomp;\n    my ($city, $product, $cost) = split(',');\n    print \"$city $cost\";\n}\n```", "```py\n#!/usr/bin/perl\n\nuse strict;\nuse warnings;\n\nmy %reduce;\n\nwhile(<STDIN>) {\n    chomp;\n    my ($city, $cost) = split(/\\s+/);\n    $reduce{$city} = 0 if not defined $reduce{$city};\n    $reduce{$city} += $cost;\n}\n\nprint \"-\" x 24;\nprintf(\"%-10s : %s\\n\", \"City\", \"Total Cost\");\nprint \"-\" x 24;\n\nforeach my $city (sort keys %reduce) {\n    printf(\"%-10s : %d\\n\", $city, $reduce{$city});\n}\n```", "```py\n[user@node-1 ~]$ cat input.txt | perl map.pl | perl reduce.pl \n------------------------\nCity : Total Cost\n------------------------\nBangalore : 270\nKolkata : 390\nNewDelhi : 440\n```", "```py\nBangalore,Onion,60\nNewDelhi,Onion,80\nBangalore,Pizza,120\nBangalore,Burger,80\nKolkata,Onion,90\nKolkata,Pizza,120\nKolkata,Chilli,20\nNewDelhi,Chilli,30\nNewDelhi,Burger,180\nKolkata,Burger,160\nNewDelhi,Pizza,150\nBangalore,Chilli,10\n```", "```py\n[user@node-1 ~]$ cat input-shuffled.txt | perl map.pl | perl reduce.pl \n------------------------\nCity : Total Cost\n------------------------\nBangalore : 270\nKolkata : 390\nNewDelhi : 440\n```", "```py\n[user@node-3 ~]$ cat ./input.txt\n Bangalore,Onion,60\n NewDelhi,Onion,80\n Bangalore,Pizza,120\n Bangalore,Burger,80\n Kolkata,Onion,90\n Kolkata,Pizza,120\n Kolkata,Chilli,20\n NewDelhi,Chilli,30\n NewDelhi,Burger,180\n Kolkata,Burger,160\n NewDelhi,Pizza,150\n Bangalore,Chilli,10\n```", "```py\n[user@master ~]$ scp *.pl node-1:~\n[user@master ~]$ scp *.pl node-2:~\n[user@master ~]$ scp *.pl node-3:~\n```", "```py\n[user@node-3 ~]$ hadoop fs -put ./input.txt /tmp/\n```", "```py\n[user@node-3 ~]$ hadoop jar \\\n    /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n    -input hdfs:///tmp/input.txt \\\n    -output hdfs:///tmp/output-7 \\\n    -mapper $(pwd)/map.pl \\\n    -reducer $(pwd)/reduce.pl\n```", "```py\n[user@node-3 ~]$ hadoop fs -cat /tmp/output-7/part*\n NewDelhi, 440\n Kolkata, 390\n Bangalore, 270\n[user@node-3 ~]$\n```", "```py\n[user@node-3 ~]$ cat TotalPrice.java \nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class TotalPrice {\n  public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable>{\n    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n      StringTokenizer itr = new StringTokenizer(value.toString(), \",\");\n      Text city = new Text(itr.nextToken());\n      itr.nextToken();\n      IntWritable price = new IntWritable(Integer.parseInt(itr.nextToken()));\n      context.write(city, price);\n    }\n  }\n\n  public static class IntSumReducer extends Reducer<Text,IntWritable,Text,IntWritable> {\n  private IntWritable result = new IntWritable();\n\n    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n      int sum = 0;\n      for (IntWritable val : values) {\n        sum += val.get();\n      }\n      result.set(sum);\n      context.write(key, result);\n    }\n  }\n\n  public static void main(String[] args) throws Exception {\n    Configuration conf = new Configuration();\n    Job job = Job.getInstance(conf, \"TotalPriceCalculator\");\n    job.setJarByClass(TotalPrice.class);\n    job.setMapperClass(TokenizerMapper.class);\n    job.setCombinerClass(IntSumReducer.class);\n    job.setReducerClass(IntSumReducer.class);\n    job.setOutputKeyClass(Text.class);\n    job.setOutputValueClass(IntWritable.class);\n    FileInputFormat.addInputPath(job, new Path(args[0]));\n    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\n  }\n}\n```", "```py\n [user@node-3 ~]$ javac -cp `hadoop classpath` TotalPrice.java \n [user@node-3 ~]$ jar cf tp.jar TotalPrice*.class\n```", "```py\n [user@node-3 ~]$ hadoop jar tp.jar TotalPrice /tmp/input.txt /tmp/output-12\n```", "```py\n[user@node-3 ~]$ hadoop fs -cat /tmp/output-12/part*\nBangalore       270\nKolkata 390\nNewDelhi        440\n```", "```py\nssh user@node-3\n[user@node-3 ~]$ sudo su - hive\n```", "```py\n[hive@node-3 ~]$ mysql -usuperset -A -psuperset -h master employees -e \"select * from vw_employee_salaries\" > vw_employee_salaries.tsv\n[hive@node-3 ~]$ wc -l vw_employee_salaries.tsv \n2844048 vw_employee_salaries.tsv\n[hive@node-3 ~]$ \n```", "```py\n[hive@node-3 ~]$ hadoop fs -put ./vw_employee_salaries.tsv /user/hive/employees.csv\n```", "```py\n[hive@node-3 ~]$ cd /usr/hdp/current/spark2-client/\n[hive@node-3 spark2-client]$ ./bin/pyspark \nPython 2.7.5 (default, Aug  4 2017, 00:39:18) \n[GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.2.0.2.6.4.0-91\n      /_/\n\nUsing Python version 2.7.5 (default, Aug  4 2017 00:39:18)\nSparkSession available as 'spark'.\n>>> \n```", "```py\n>>> ds = spark.read.text(\"employees.csv\")\n>>> ds.count()\n2844048                                                                         \n>>> \n```", "```py\n>>> ds.first()\nRow(value=u'emp_no\\tbirth_date\\tfirst_name\\tlast_name\\tgender\\thire_date\\tsalary\\tfrom_date\\tto_date')\n>>> ds.head(5)\n[Row(value=u'emp_no\\tbirth_date\\tfirst_name\\tlast_name\\tgender\\thire_date\\tsalary\\tfrom_date\\tto_date'), Row(value=u'10001\\t1953-09-02\\tGeorgi\\tFacello\\tM\\t1986-06-26\\t60117\\t1986-06-26\\t1987-06-26'), Row(value=u'10001\\t1953-09-02\\tGeorgi\\tFacello\\tM\\t1986-06-26\\t62102\\t1987-06-26\\t1988-06-25'), Row(value=u'10001\\t1953-09-02\\tGeorgi\\tFacello\\tM\\t1986-06-26\\t66074\\t1988-06-25\\t1989-06-25'), Row(value=u'10001\\t1953-09-02\\tGeorgi\\tFacello\\tM\\t1986-06-26\\t66596\\t1989-06-25\\t1990-06-25')]\n>>> ds.printSchema()\nroot\n |-- value: string (nullable = true)\n\n>>> \n```", "```py\n>>> ds.filter(ds.value.contains(\"Georgi\")).count()\n2323                                                                            \n>>> \n```", "```py\n>>> ds = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(\"employees.csv\")\n>>> ds.count()\n2844047   \n>>> ds.show(5)\n+------+----------+----------+---------+------+----------+------+----------+----------+\n|emp_no|birth_date|first_name|last_name|gender| hire_date|salary| from_date|   to_date|\n+------+----------+----------+---------+------+----------+------+----------+----------+\n| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 60117|1986-06-26|1987-06-26|\n| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 62102|1987-06-26|1988-06-25|\n| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66074|1988-06-25|1989-06-25|\n| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66596|1989-06-25|1990-06-25|\n| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66961|1990-06-25|1991-06-25|\n+------+----------+----------+---------+------+----------+------+----------+----------+\nonly showing top 5 rows\n\n>>> \n>>> ds.printSchema()\nroot\n |-- emp_no: string (nullable = true)\n |-- birth_date: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- hire_date: string (nullable = true)\n |-- salary: string (nullable = true)\n |-- from_date: string (nullable = true)\n |-- to_date: string (nullable = true)\n\n>>> \n```", "```py\n>>> ds = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").option(\"inferSchema\", \"true\").load(\"employees.csv\")\n18/03/25 19:21:15 WARN FileStreamSink: Error while looking for metadata directory.\n18/03/25 19:21:15 WARN FileStreamSink: Error while looking for metadata directory.\n>>> ds.count()                                                                  \n2844047                                                                         \n>>> ds.show(2)\n+------+-------------------+----------+---------+------+-------------------+------+-------------------+-------------------+\n|emp_no|         birth_date|first_name|last_name|gender|          hire_date|salary|          from_date|            to_date|\n+------+-------------------+----------+---------+------+-------------------+------+-------------------+-------------------+\n| 10001|1953-09-02 00:00:00|    Georgi|  Facello|     M|1986-06-26 00:00:00| 60117|1986-06-26 00:00:00|1987-06-26 00:00:00|\n| 10001|1953-09-02 00:00:00|    Georgi|  Facello|     M|1986-06-26 00:00:00| 62102|1987-06-26 00:00:00|1988-06-25 00:00:00|\n+------+-------------------+----------+---------+------+-------------------+------+-------------------+-------------------+\nonly showing top 2 rows\n\n>>> ds.printSchema()\nroot\n |-- emp_no: integer (nullable = true)\n |-- birth_date: timestamp (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- hire_date: timestamp (nullable = true)\n |-- salary: integer (nullable = true)\n |-- from_date: timestamp (nullable = true)\n |-- to_date: timestamp (nullable = true)\n\n>>> \n```", "```py\n>>> ds.filter(ds.gender == \"M\").count()\n1706321 \n```", "```py\n>>> ds.filter(ds.gender == \"M\").filter(ds.salary > 100000).count()\n57317   \n```", "```py\n[hive@node-3 ~]$ cat MyFirstApp.py \nfrom pyspark.sql import SparkSession\n\n# Path to the file in HDFS\ncsvFile = \"employees.csv\"\n\n# Create a session for this application\nspark = SparkSession.builder.appName(\"MyFirstApp\").getOrCreate()\n\n# Read the CSV File\ncsvTable = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(csvFile)\n\n# Print the total number of records in this file\nprint \"Total records in the input : {}\".format(csvTable.count())\n\n# Stop the application\nspark.stop()\n[hive@node-3 ~]$ \n```", "```py\n[hive@node-3 ~]$ /usr/hdp/current/spark2-client/bin/spark-submit ./MyFirstApp.py 2>&1 | grep -v -e INFO -e WARN\nTotal records in the input : 2844047\n```", "```py\n[hive@node-3 ~]$ cat StreamingDedup.py \nfrom pyspark import SparkContext\nfrom pyspark.streaming import StreamingContext\n\ncontext = SparkContext(appName=\"StreamingDedup\")\nstream = StreamingContext(context, 5)\n\nrecords = stream.socketTextStream(\"localhost\", 5000)\nrecords\n    .map(lambda record: (record, 1))\n    .reduceByKey(lambda x,y: x + y)\n    .pprint()\n\nssc.start()\nssc.awaitTermination()\n```", "```py\nfor i in $(seq 1 10)\ndo\n  for j in $(seq 1 5)\n  do\n   sleep 1\n   tail -n+$(($i * 3)) /usr/share/dict/words | head -3\n  done\ndone | nc -l 5000\n\n```", "```py\n[hive@node-3 ~]$ /usr/hdp/current/spark2-client/bin/spark-submit ./StreamingDedup.py 2>&1 | grep -v -e INFO -e WARN\n```", "```py\n-------------------------------------------\nTime: 2018-03-26 04:33:45\n-------------------------------------------\n(u'16-point', 5)\n(u'18-point', 5)\n(u'1st', 5)\n\n-------------------------------------------\nTime: 2018-03-26 04:33:50\n-------------------------------------------\n(u'2', 5)\n(u'20-point', 5)\n(u'2,4,5-t', 5)\n```", "```py\n[hive@node-3 ~]$ cat SQLApp.py \nfrom pyspark.sql import SparkSession\n\n# Path to the file in HDFS\ncsvFile = \"employees.csv\"\n\n# Create a session for this application\nspark = SparkSession.builder.appName(\"SQLApp\").getOrCreate()\n\n# Read the CSV File\ncsvTable = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(csvFile)\ncsvTable.show(3)\n\n# Create a temporary view\ncsvView = csvTable.createOrReplaceTempView(\"employees\")\n\n# Find the total salary of employees and print the highest salary makers\nhighPay = spark.sql(\"SELECT first_name, last_name, emp_no, SUM(salary) AS total FROM employees GROUP BY emp_no, first_name, last_name ORDER BY SUM(salary)\")\n\n# Generate list of records\nresults = highPay.rdd.map(lambda rec: \"Total: {}, Emp No: {}, Full Name: {} {}\".format(rec.total, rec.emp_no, rec.first_name, rec.last_name)).collect()\n\n# Show the top 5 of them\nfor r in results[:5]:\n    print(r)\n\n# Stop the application\nspark.stop()\n[hive@node-3 ~]$ \n```", "```py\n[hive@node-3 ~]$ /usr/hdp/current/spark2-client/bin/spark-submit ./SQLApp.py 2>&1 | grep -v -e INFO -e WARN\n[rdd_10_0]\n+------+----------+----------+---------+------+----------+------+----------+----------+\n|emp_no|birth_date|first_name|last_name|gender| hire_date|salary| from_date|   to_date|\n+------+----------+----------+---------+------+----------+------+----------+----------+\n| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 60117|1986-06-26|1987-06-26|\n| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 62102|1987-06-26|1988-06-25|\n| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66074|1988-06-25|1989-06-25|\n+------+----------+----------+---------+------+----------+------+----------+----------+\nonly showing top 3 rows\n\nTotal: 40000.0, Emp No: 15084, Full Name: Aloke Birke\nTotal: 40000.0, Emp No: 24529, Full Name: Mario Antonakopoulos\nTotal: 40000.0, Emp No: 30311, Full Name: Tomofumi Coombs\nTotal: 40000.0, Emp No: 55527, Full Name: Kellyn Ouhyoung\nTotal: 40000.0, Emp No: 284677, Full Name: Richara Eastman\n```"]