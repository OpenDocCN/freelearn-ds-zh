["```py\nscala> import org.apache.spark.ml.regression.LinearRegression\n      import org.apache.spark.ml.regression.LinearRegression\n\n\tscala> import org.apache.spark.ml.param.ParamMap\n\n      import org.apache.spark.ml.param.ParamMap\n\n\tscala> import org.apache.spark.ml.linalg.{Vector, Vectors}\n\n      import org.apache.spark.ml.linalg.{Vector, Vectors}\n\n\tscala> import org.apache.spark.sql.Row\n\n      import org.apache.spark.sql.Row\n\n\tscala> // TODO - Change this directory to the right location where the data\n    is stored\n\tscala> val dataDir = \"/Users/RajT/Downloads/wine-quality/\"\n\n      dataDir: String = /Users/RajT/Downloads/wine-quality/\n\n\tscala> // Define the case class that holds the wine data\n\tscala> case class Wine(FixedAcidity: Double, VolatileAcidity: Double, CitricAcid: Double, ResidualSugar: Double, Chlorides: Double, FreeSulfurDioxide: Double, TotalSulfurDioxide: Double, Density: Double, PH: Double, Sulphates: Double, Alcohol: Double, Quality: Double)\n\n      defined class Wine\n\n\tscala> // Create the the RDD by reading the wine data from the disk \n\tscala> //TODO - The wine data has to be downloaded to the appropriate working directory in the system where this is being run and the following line of code should use that path\n\tscala> val wineDataRDD = sc.textFile(dataDir + \"winequality-red.csv\").map(_.split(\";\")).map(w => Wine(w(0).toDouble, w(1).toDouble, w(2).toDouble, w(3).toDouble, w(4).toDouble, w(5).toDouble, w(6).toDouble, w(7).toDouble, w(8).toDouble, w(9).toDouble, w(10).toDouble, w(11).toDouble))\n\n      wineDataRDD: org.apache.spark.rdd.RDD[Wine] = MapPartitionsRDD[3] at map at <console>:32\n\n\tscala> // Create the data frame containing the training data having two columns. 1) The actual output or label of the data 2) The vector containing the features\n\tscala> //Vector is a data type with 0 based indices and double-typed values. In that there are two types namely dense and sparse.\n\tscala> //A dense vector is backed by a double array representing its entry values \n\tscala> //A sparse vector is backed by two parallel arrays: indices and values\n\tscala> val trainingDF = wineDataRDD.map(w => (w.Quality, Vectors.dense(w.FixedAcidity, w.VolatileAcidity, w.CitricAcid, w.ResidualSugar, w.Chlorides, w.FreeSulfurDioxide, w.TotalSulfurDioxide, w.Density, w.PH, w.Sulphates, w.Alcohol))).toDF(\"label\", \"features\")\n\n      trainingDF: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n    scala> trainingDF.show()\n\n      +-----+--------------------+\n\n      |label|            features|\n\n      +-----+--------------------+\n\n      |  5.0|[7.4,0.7,0.0,1.9,...|\n\n      |  5.0|[7.8,0.88,0.0,2.6...|\n\n      |  5.0|[7.8,0.76,0.04,2....|\n\n      |  6.0|[11.2,0.28,0.56,1...|\n\n      |  5.0|[7.4,0.7,0.0,1.9,...|\n\n      |  5.0|[7.4,0.66,0.0,1.8...|\n\n      |  5.0|[7.9,0.6,0.06,1.6...|\n\n      |  7.0|[7.3,0.65,0.0,1.2...|\n\n      |  7.0|[7.8,0.58,0.02,2....|\n\n      |  5.0|[7.5,0.5,0.36,6.1...|\n\n      |  5.0|[6.7,0.58,0.08,1....|\n\n      |  5.0|[7.5,0.5,0.36,6.1...|\n\n      |  5.0|[5.6,0.615,0.0,1....|\n\n      |  5.0|[7.8,0.61,0.29,1....|\n\n      |  5.0|[8.9,0.62,0.18,3....|\n\n      |  5.0|[8.9,0.62,0.19,3....|\n\n      |  7.0|[8.5,0.28,0.56,1....|\n\n      |  5.0|[8.1,0.56,0.28,1....|\n\n      |  4.0|[7.4,0.59,0.08,4....|\n\n      |  6.0|[7.9,0.32,0.51,1....|\n\n      +-----+--------------------+\n\n      only showing top 20 rows\n    scala> // Create the object of the algorithm which is the Linear Regression\n\tscala> val lr = new LinearRegression()\n      lr: org.apache.spark.ml.regression.LinearRegression = linReg_f810f0c1617b\n    scala> // Linear regression parameter to make lr.fit() use at most 10 iterations\n\tscala> lr.setMaxIter(10)\n      res1: lr.type = linReg_f810f0c1617b\n    scala> // Create a trained model by fitting the parameters using the training data\n\tscala> val model = lr.fit(trainingDF)\n      model: org.apache.spark.ml.regression.LinearRegressionModel = linReg_f810f0c1617b\n    scala> // Once the model is prepared, to test the model, prepare the test data containing the labels and feature vectors\n\tscala> val testDF = spark.createDataFrame(Seq((5.0, Vectors.dense(7.4, 0.7, 0.0, 1.9, 0.076, 25.0, 67.0, 0.9968, 3.2, 0.68,9.8)),(5.0, Vectors.dense(7.8, 0.88, 0.0, 2.6, 0.098, 11.0, 34.0, 0.9978, 3.51, 0.56, 9.4)),(7.0, Vectors.dense(7.3, 0.65, 0.0, 1.2, 0.065, 15.0, 18.0, 0.9968, 3.36, 0.57, 9.5)))).toDF(\"label\", \"features\")\n      testDF: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n    scala> testDF.show()\n      +-----+--------------------+\n\n      |label|            features|\n\n      +-----+--------------------+\n\n      |  5.0|[7.4,0.7,0.0,1.9,...|\n\n      |  5.0|[7.8,0.88,0.0,2.6...|\n\n      |  7.0|[7.3,0.65,0.0,1.2...|\n\n      +-----+--------------------+\n    scala> testDF.createOrReplaceTempView(\"test\")scala> // Do the transformation of the test data using the model and predict the output values or lables. This is to compare the predicted value and the actual label value\n\tscala> val tested = model.transform(testDF).select(\"features\", \"label\", \"prediction\")\n      tested: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\n    scala> tested.show()\n      +--------------------+-----+-----------------+\n\n      |            features|label|       prediction|\n\n      +--------------------+-----+-----------------+\n\n      |[7.4,0.7,0.0,1.9,...|  5.0|5.352730835898477|\n\n      |[7.8,0.88,0.0,2.6...|  5.0|4.817999362011964|\n\n      |[7.3,0.65,0.0,1.2...|  7.0|5.280106355653388|\n\n      +--------------------+-----+-----------------+\n    scala> // Prepare a dataset without the output/lables to predict the output using the trained model\n\tscala> val predictDF = spark.sql(\"SELECT features FROM test\")predictDF: org.apache.spark.sql.DataFrame = [features: vector]\n\tscala> predictDF.show()\n      +--------------------+\n\n      |            features|\n\n      +--------------------+\n\n      |[7.4,0.7,0.0,1.9,...|\n\n      |[7.8,0.88,0.0,2.6...|\n\n      |[7.3,0.65,0.0,1.2...|\n\n      +--------------------+\n    scala> // Do the transformation with the predict dataset and display the predictions\n\tscala> val predicted = model.transform(predictDF).select(\"features\", \"prediction\")\n      predicted: org.apache.spark.sql.DataFrame = [features: vector, prediction: double]\n    scala> predicted.show()\n      +--------------------+-----------------+\n\n      |            features|       prediction|\n\n      +--------------------+-----------------+\n\n      |[7.4,0.7,0.0,1.9,...|5.352730835898477|\n\n      |[7.8,0.88,0.0,2.6...|4.817999362011964|\n\n      |[7.3,0.65,0.0,1.2...|5.280106355653388|\n\n      +--------------------+-----------------+\n    scala> //IMPORTANT - To continue with the model persistence coming in the next section, keep this session on.\n\n```", "```py\n >>> from pyspark.ml.linalg import Vectors\n\t>>> from pyspark.ml.regression import LinearRegression\n\t>>> from pyspark.ml.param import Param, Params\n\t>>> from pyspark.sql import Row\n\t>>> # TODO - Change this directory to the right location where the data is stored\n\t>>> dataDir = \"/Users/RajT/Downloads/wine-quality/\"\n\t>>> # Create the the RDD by reading the wine data from the disk \n\t>>> lines = sc.textFile(dataDir + \"winequality-red.csv\")\n\t>>> splitLines = lines.map(lambda l: l.split(\";\"))\n\t>>> # Vector is a data type with 0 based indices and double-typed values. In that there are two types namely dense and sparse.\n\t>>> # A dense vector is backed by a double array representing its entry values\n\t>>> # A sparse vector is backed by two parallel arrays: indices and values\n\t>>> wineDataRDD = splitLines.map(lambda p: (float(p[11]), Vectors.dense([float(p[0]), float(p[1]), float(p[2]), float(p[3]), float(p[4]), float(p[5]), float(p[6]), float(p[7]), float(p[8]), float(p[9]), float(p[10])])))\n\t>>> # Create the data frame containing the training data having two columns. 1) The actula output or label of the data 2) The vector containing the features\n\t>>> trainingDF = spark.createDataFrame(wineDataRDD, ['label', 'features'])\n\t>>> trainingDF.show()\n\n      +-----+--------------------+\n\n      |label|            features|\n\n      +-----+--------------------+\n\n      |  5.0|[7.4,0.7,0.0,1.9,...|\n\n      |  5.0|[7.8,0.88,0.0,2.6...|\n\n      |  5.0|[7.8,0.76,0.04,2....|\n\n      |  6.0|[11.2,0.28,0.56,1...|\n\n      |  5.0|[7.4,0.7,0.0,1.9,...|\n\n      |  5.0|[7.4,0.66,0.0,1.8...|\n\n      |  5.0|[7.9,0.6,0.06,1.6...|\n\n      |  7.0|[7.3,0.65,0.0,1.2...|\n\n      |  7.0|[7.8,0.58,0.02,2....|\n\n      |  5.0|[7.5,0.5,0.36,6.1...|\n\n      |  5.0|[6.7,0.58,0.08,1....|\n\n      |  5.0|[7.5,0.5,0.36,6.1...|\n\n      |  5.0|[5.6,0.615,0.0,1....|\n\n      |  5.0|[7.8,0.61,0.29,1....|\n\n      |  5.0|[8.9,0.62,0.18,3....|\n\n      |  5.0|[8.9,0.62,0.19,3....|\n\n      |  7.0|[8.5,0.28,0.56,1....|\n\n      |  5.0|[8.1,0.56,0.28,1....|\n\n      |  4.0|[7.4,0.59,0.08,4....|\n\n      |  6.0|[7.9,0.32,0.51,1....|\n\n      +-----+--------------------+\n\n      only showing top 20 rows\n\n\t>>> # Create the object of the algorithm which is the Linear Regression with the parameters\n\t>>> # Linear regression parameter to make lr.fit() use at most 10 iterations\n\t>>> lr = LinearRegression(maxIter=10)\n\t>>> # Create a trained model by fitting the parameters using the training data\n\t>>> model = lr.fit(trainingDF)\n\t>>> # Once the model is prepared, to test the model, prepare the test data containing the labels and feature vectors \n\t>>> testDF = spark.createDataFrame([(5.0, Vectors.dense([7.4, 0.7, 0.0, 1.9, 0.076, 25.0, 67.0, 0.9968, 3.2, 0.68,9.8])),(5.0,Vectors.dense([7.8, 0.88, 0.0, 2.6, 0.098, 11.0, 34.0, 0.9978, 3.51, 0.56, 9.4])),(7.0, Vectors.dense([7.3, 0.65, 0.0, 1.2, 0.065, 15.0, 18.0, 0.9968, 3.36, 0.57, 9.5]))], [\"label\", \"features\"])\n\t>>> testDF.createOrReplaceTempView(\"test\")\n\t>>> testDF.show()\n\n      +-----+--------------------+\n\n      |label|            features|\n\n      +-----+--------------------+\n\n      |  5.0|[7.4,0.7,0.0,1.9,...|\n\n      |  5.0|[7.8,0.88,0.0,2.6...|\n\n      |  7.0|[7.3,0.65,0.0,1.2...|\n\n      +-----+--------------------+\n    >>> # Do the transformation of the test data using the model and predict the output values or lables. This is to compare the predicted value and the actual label value\n\t>>> testTransform = model.transform(testDF)\n\t>>> tested = testTransform.select(\"features\", \"label\", \"prediction\")\n\t>>> tested.show()\n\n      +--------------------+-----+-----------------+\n\n      |            features|label|       prediction|\n\n      +--------------------+-----+-----------------+\n\n      |[7.4,0.7,0.0,1.9,...|  5.0|5.352730835898477|\n\n      |[7.8,0.88,0.0,2.6...|  5.0|4.817999362011964|\n\n      |[7.3,0.65,0.0,1.2...|  7.0|5.280106355653388|\n\n      +--------------------+-----+-----------------+\n\n\t>>> # Prepare a dataset without the output/lables to predict the output using the trained model\n\t>>> predictDF = spark.sql(\"SELECT features FROM test\")\n\t>>> predictDF.show()\n\n      +--------------------+\n\n      |            features|\n\n      +--------------------+\n\n      |[7.4,0.7,0.0,1.9,...|\n\n      |[7.8,0.88,0.0,2.6...|\n\n      |[7.3,0.65,0.0,1.2...|\n\n      +--------------------+\n\n\t>>> # Do the transformation with the predict dataset and display the predictions\n\t>>> predictTransform = model.transform(predictDF)\n\t>>> predicted = predictTransform.select(\"features\", \"prediction\")\n\t>>> predicted.show()\n\n      +--------------------+-----------------+\n\n      |            features|       prediction|\n\n      +--------------------+-----------------+\n\n      |[7.4,0.7,0.0,1.9,...|5.352730835898477|\n\n      |[7.8,0.88,0.0,2.6...|4.817999362011964|\n\n      |[7.3,0.65,0.0,1.2...|5.280106355653388|\n\n      +--------------------+-----------------+\n\n\t>>> #IMPORTANT - To continue with the model persistence coming in the next section, keep this session on.\n\n```", "```py\n scala> // Assuming that the model definition line \"val model = \n    lr.fit(trainingDF)\" is still in context\n\tscala> import org.apache.spark.ml.regression.LinearRegressionModel\n\n      import org.apache.spark.ml.regression.LinearRegressionModel\n\n\tscala> model.save(\"wineLRModelPath\")\n\tscala> val newModel = LinearRegressionModel.load(\"wineLRModelPath\")\n\n      newModel: org.apache.spark.ml.regression.LinearRegressionModel = \n      linReg_6a880215ab96 \n\n```", "```py\n >>> from pyspark.ml.regression import LinearRegressionModel\n\t>>> newModel = LinearRegressionModel.load(\"wineLRModelPath\")\n\t>>> newPredictTransform = newModel.transform(predictDF) \n\t>>> newPredicted = newPredictTransform.select(\"features\", \"prediction\")\n\t>>> newPredicted.show()\n\n      +--------------------+-----------------+\n\n      |            features|       prediction|\n\n      +--------------------+-----------------+\n\n      |[7.4,0.7,0.0,1.9,...|5.352730835898477|\n\n      |[7.8,0.88,0.0,2.6...|4.817999362011964|\n\n      |[7.3,0.65,0.0,1.2...|5.280106355653388|\n\n      +--------------------+-----------------+ \n\n```", "```py\n\t scala> import org.apache.spark.ml.classification.LogisticRegression\n\n      import org.apache.spark.ml.classification.LogisticRegression\n\n\tscala> import org.apache.spark.ml.param.ParamMap\n\n      import org.apache.spark.ml.param.ParamMap\n\n\tscala> import org.apache.spark.ml.linalg.{Vector, Vectors}\n\n      import org.apache.spark.ml.linalg.{Vector, Vectors}\n    scala> import org.apache.spark.sql.Row\n\n      import org.apache.spark.sql.Row\n\n\tscala> // TODO - Change this directory to the right location where the data is stored\n\tscala> val dataDir = \"/Users/RajT/Downloads/wine-quality/\"\n\n      dataDir: String = /Users/RajT/Downloads/wine-quality/\n\n\tscala> // Define the case class that holds the wine data\n\tscala> case class Wine(FixedAcidity: Double, VolatileAcidity: Double, CitricAcid: Double, ResidualSugar: Double, Chlorides: Double, FreeSulfurDioxide: Double, TotalSulfurDioxide: Double, Density: Double, PH: Double, Sulphates: Double, Alcohol: Double, Quality: Double)\n\n      defined class Wine\n\n\tscala> // Create the the RDD by reading the wine data from the disk \n\tscala> val wineDataRDD = sc.textFile(dataDir + \"winequality-white.csv\").map(_.split(\";\")).map(w => Wine(w(0).toDouble, w(1).toDouble, w(2).toDouble, w(3).toDouble, w(4).toDouble, w(5).toDouble, w(6).toDouble, w(7).toDouble, w(8).toDouble, w(9).toDouble, w(10).toDouble, w(11).toDouble))\n\n      wineDataRDD: org.apache.spark.rdd.RDD[Wine] = MapPartitionsRDD[35] at map at <console>:36\n\n\tscala> // Create the data frame containing the training data having two columns. 1) The actula output or label of the data 2) The vector containing the features\n\tscala> val trainingDF = wineDataRDD.map(w => (if(w.Quality < 7) 0D else 1D, Vectors.dense(w.FixedAcidity, w.VolatileAcidity, w.CitricAcid, w.ResidualSugar, w.Chlorides, w.FreeSulfurDioxide, w.TotalSulfurDioxide, w.Density, w.PH, w.Sulphates, w.Alcohol))).toDF(\"label\", \"features\")\n\n      trainingDF: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n\n\tscala> trainingDF.show()\n\n      +-----+--------------------+\n\n      |label|            features|\n\n      +-----+--------------------+\n\n      |  0.0|[7.0,0.27,0.36,20...|\n\n      |  0.0|[6.3,0.3,0.34,1.6...|\n\n      |  0.0|[8.1,0.28,0.4,6.9...|\n\n      |  0.0|[7.2,0.23,0.32,8....|\n\n      |  0.0|[7.2,0.23,0.32,8....|\n\n      |  0.0|[8.1,0.28,0.4,6.9...|\n\n      |  0.0|[6.2,0.32,0.16,7....|\n\n      |  0.0|[7.0,0.27,0.36,20...|\n\n      |  0.0|[6.3,0.3,0.34,1.6...|\n\n      |  0.0|[8.1,0.22,0.43,1....|\n\n      |  0.0|[8.1,0.27,0.41,1....|\n\n      |  0.0|[8.6,0.23,0.4,4.2...|\n\n      |  0.0|[7.9,0.18,0.37,1....|\n\n      |  1.0|[6.6,0.16,0.4,1.5...|\n\n      |  0.0|[8.3,0.42,0.62,19...|\n\n      |  1.0|[6.6,0.17,0.38,1....|\n\n      |  0.0|[6.3,0.48,0.04,1....|\n\n      |  1.0|[6.2,0.66,0.48,1....|\n\n      |  0.0|[7.4,0.34,0.42,1....|\n\n      |  0.0|[6.5,0.31,0.14,7....|\n\n      +-----+--------------------+\n\n      only showing top 20 rows\n\n\tscala> // Create the object of the algorithm which is the Logistic Regression\n\tscala> val lr = new LogisticRegression()\n\n      lr: org.apache.spark.ml.classification.LogisticRegression = logreg_a7e219daf3e1\n\n\tscala> // LogisticRegression parameter to make lr.fit() use at most 10 iterations and the regularization parameter.\n\tscala> // When a higher degree polynomial used by the algorithm to fit a set of points in a linear regression model, to prevent overfitting, regularization is used and this parameter is just for that\n\tscala> lr.setMaxIter(10).setRegParam(0.01)\n\n      res8: lr.type = logreg_a7e219daf3e1\n\n\tscala> // Create a trained model by fitting the parameters using the training data\n\tscala> val model = lr.fit(trainingDF)\n\n      model: org.apache.spark.ml.classification.LogisticRegressionModel = logreg_a7e219daf3e1\n\n\tscala> // Once the model is prepared, to test the model, prepare the test data containing the labels and feature vectors\n\tscala> val testDF = spark.createDataFrame(Seq((1.0, Vectors.dense(6.1,0.32,0.24,1.5,0.036,43,140,0.9894,3.36,0.64,10.7)),(0.0, Vectors.dense(5.2,0.44,0.04,1.4,0.036,38,124,0.9898,3.29,0.42,12.4)),(0.0, Vectors.dense(7.2,0.32,0.47,5.1,0.044,19,65,0.9951,3.38,0.36,9)),(0.0,Vectors.dense(6.4,0.595,0.14,5.2,0.058,15,97,0.991,3.03,0.41,12.6)))).toDF(\"label\", \"features\")\n\n      testDF: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n\n\tscala> testDF.show()\n\n      +-----+--------------------+\n\n      |label|            features|\n\n      +-----+--------------------+\n\n      |  1.0|[6.1,0.32,0.24,1....|\n\n      |  0.0|[5.2,0.44,0.04,1....|\n\n      |  0.0|[7.2,0.32,0.47,5....|\n\n      |  0.0|[6.4,0.595,0.14,5...|\n\n      +-----+--------------------+\n    scala> testDF.createOrReplaceTempView(\"test\")\n\tscala> // Do the transformation of the test data using the model and predict the output values or labels. This is to compare the predicted value and the actual label value\n\tscala> val tested = model.transform(testDF).select(\"features\", \"label\", \"prediction\")\n      tested: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\n\n\tscala> tested.show()\n      +--------------------+-----+----------+\n\n      |            features|label|prediction|\n\n      +--------------------+-----+----------+\n\n      |[6.1,0.32,0.24,1....|  1.0|       0.0|\n\n      |[5.2,0.44,0.04,1....|  0.0|       0.0|\n\n      |[7.2,0.32,0.47,5....|  0.0|       0.0|\n\n      |[6.4,0.595,0.14,5...|  0.0|       0.0|\n\n      +--------------------+-----+----------+\n\n\tscala> // Prepare a dataset without the output/lables to predict the output using the trained model\n\tscala> val predictDF = spark.sql(\"SELECT features FROM test\")\n\n      predictDF: org.apache.spark.sql.DataFrame = [features: vector]\n\n\tscala> predictDF.show()\n\n      +--------------------+\n\n      |            features|\n\n      +--------------------+\n\n      |[6.1,0.32,0.24,1....|\n\n      |[5.2,0.44,0.04,1....|\n\n      |[7.2,0.32,0.47,5....|\n\n      |[6.4,0.595,0.14,5...|\n\n      +--------------------+\n\n\tscala> // Do the transformation with the predict dataset and display the predictions\n\tscala> val predicted = model.transform(predictDF).select(\"features\", \"prediction\")\n\n      predicted: org.apache.spark.sql.DataFrame = [features: vector, prediction: double]\n\n\tscala> predicted.show()\n\n      +--------------------+----------+\n\n      |            features|prediction|\n\n      +--------------------+----------+\n\n      |[6.1,0.32,0.24,1....|       0.0|\n\n      |[5.2,0.44,0.04,1....|       0.0|\n\n      |[7.2,0.32,0.47,5....|       0.0|\n\n      |[6.4,0.595,0.14,5...|       0.0|\n\n      +--------------------+----------+ \n\n```", "```py\n >>> from pyspark.ml.linalg import Vectors\n\t  >>> from pyspark.ml.classification import LogisticRegression\n\t  >>> from pyspark.ml.param import Param, Params\n\t  >>> from pyspark.sql import Row\n\t  >>> # TODO - Change this directory to the right location where the data is stored\n\t  >>> dataDir = \"/Users/RajT/Downloads/wine-quality/\"\n\t  >>> # Create the the RDD by reading the wine data from the disk \n\t  >>> lines = sc.textFile(dataDir + \"winequality-white.csv\")\n\t  >>> splitLines = lines.map(lambda l: l.split(\";\"))\n\t  >>> wineDataRDD = splitLines.map(lambda p: (float(0) if (float(p[11]) < 7) else float(1), Vectors.dense([float(p[0]), float(p[1]), float(p[2]), float(p[3]), float(p[4]), float(p[5]), float(p[6]), float(p[7]), float(p[8]), float(p[9]), float(p[10])])))\n\t  >>> # Create the data frame containing the training data having two columns. 1) The actula output or label of the data 2) The vector containing the features\n\t  >>> trainingDF = spark.createDataFrame(wineDataRDD, ['label', 'features'])\n\t  >>> trainingDF.show()\n\t  +-----+--------------------+\n\t  |label|            features|\n\n      +-----+--------------------+\n\n      |  0.0|[7.0,0.27,0.36,20...|\n\n      |  0.0|[6.3,0.3,0.34,1.6...|\n\n      |  0.0|[8.1,0.28,0.4,6.9...|\n\n      |  0.0|[7.2,0.23,0.32,8....|\n\n      |  0.0|[7.2,0.23,0.32,8....|\n\n      |  0.0|[8.1,0.28,0.4,6.9...|\n\n      |  0.0|[6.2,0.32,0.16,7....|\n\n      |  0.0|[7.0,0.27,0.36,20...|\n\n      |  0.0|[6.3,0.3,0.34,1.6...|\n\n      |  0.0|[8.1,0.22,0.43,1....|\n\n      |  0.0|[8.1,0.27,0.41,1....|\n\n      |  0.0|[8.6,0.23,0.4,4.2...|\n\n      |  0.0|[7.9,0.18,0.37,1....|\n\n      |  1.0|[6.6,0.16,0.4,1.5...|\n\n      |  0.0|[8.3,0.42,0.62,19...|\n\n      |  1.0|[6.6,0.17,0.38,1....|\n\n      |  0.0|[6.3,0.48,0.04,1....|\n\n      |  1.0|[6.2,0.66,0.48,1....|\n\n      |  0.0|[7.4,0.34,0.42,1....|\n\n      |  0.0|[6.5,0.31,0.14,7....|\n\n      +-----+--------------------+\n\n      only showing top 20 rows\n\n\t>>> # Create the object of the algorithm which is the Logistic Regression with the parameters\n\t>>> # LogisticRegression parameter to make lr.fit() use at most 10 iterations and the regularization parameter.\n\t>>> # When a higher degree polynomial used by the algorithm to fit a set of points in a linear regression model, to prevent overfitting, regularization is used and this parameter is just for that\n\t>>> lr = LogisticRegression(maxIter=10, regParam=0.01)\n\t>>> # Create a trained model by fitting the parameters using the training data>>> model = lr.fit(trainingDF)\n\t>>> # Once the model is prepared, to test the model, prepare the test data containing the labels and feature vectors\n\t>>> testDF = spark.createDataFrame([(1.0, Vectors.dense([6.1,0.32,0.24,1.5,0.036,43,140,0.9894,3.36,0.64,10.7])),(0.0, Vectors.dense([5.2,0.44,0.04,1.4,0.036,38,124,0.9898,3.29,0.42,12.4])),(0.0, Vectors.dense([7.2,0.32,0.47,5.1,0.044,19,65,0.9951,3.38,0.36,9])),(0.0, Vectors.dense([6.4,0.595,0.14,5.2,0.058,15,97,0.991,3.03,0.41,12.6]))], [\"label\", \"features\"])\n\t>>> testDF.createOrReplaceTempView(\"test\")\n\t>>> testDF.show()\n\n      +-----+--------------------+\n\n      |label|            features|\n\n      +-----+--------------------+\n\n      |  1.0|[6.1,0.32,0.24,1....|\n\n      |  0.0|[5.2,0.44,0.04,1....|\n\n      |  0.0|[7.2,0.32,0.47,5....|\n\n      |  0.0|[6.4,0.595,0.14,5...|\n\n      +-----+--------------------+\n\n\t>>> # Do the transformation of the test data using the model and predict the output values or lables. This is to compare the predicted value and the actual label value\n\t>>> testTransform = model.transform(testDF)\n\t>>> tested = testTransform.select(\"features\", \"label\", \"prediction\")\n\t>>> tested.show()\n\n      +--------------------+-----+----------+\n\n      |            features|label|prediction|\n\n      +--------------------+-----+----------+\n\n      |[6.1,0.32,0.24,1....|  1.0|       0.0|\n\n      |[5.2,0.44,0.04,1....|  0.0|       0.0|\n\n      |[7.2,0.32,0.47,5....|  0.0|       0.0|\n\n      |[6.4,0.595,0.14,5...|  0.0|       0.0|\n\n      +--------------------+-----+----------+\n\n\t>>> # Prepare a dataset without the output/lables to predict the output using the trained model\n\t>>> predictDF = spark.sql(\"SELECT features FROM test\")\n\t>>> predictDF.show()\n\n      +--------------------+\n\n      |            features|\n\n      +--------------------+\n\n      |[6.1,0.32,0.24,1....|\n\n      |[5.2,0.44,0.04,1....|\n\n      |[7.2,0.32,0.47,5....|\n\n      |[6.4,0.595,0.14,5...|\n\n      +--------------------+\n\n\t>>> # Do the transformation with the predict dataset and display the predictions\n\t>>> predictTransform = model.transform(predictDF)\n\t>>> predicted = testTransform.select(\"features\", \"prediction\")\n\t>>> predicted.show()\n      +--------------------+----------+\n\n      |            features|prediction|\n\n      +--------------------+----------+\n\n      |[6.1,0.32,0.24,1....|       0.0|\n\n      |[5.2,0.44,0.04,1....|       0.0|\n\n      |[7.2,0.32,0.47,5....|       0.0|\n\n      |[6.4,0.595,0.14,5...|       0.0|\n\n      +--------------------+----------+\n\n```", "```py\n scala> import org.apache.spark.ml.classification.LogisticRegression\n\n      import org.apache.spark.ml.classification.LogisticRegression\n\n\tscala> import org.apache.spark.ml.param.ParamMap\n\n      import org.apache.spark.ml.param.ParamMap\n\n\tscala> import org.apache.spark.ml.linalg.{Vector, Vectors}\n\n      import org.apache.spark.ml.linalg.{Vector, Vectors}\n\n\tscala> import org.apache.spark.sql.Row\n\n      import org.apache.spark.sql.Row\n\n\tscala> import org.apache.spark.ml.Pipeline\n\n      import org.apache.spark.ml.Pipeline\n\n\tscala> import org.apache.spark.ml.feature.{HashingTF, Tokenizer, RegexTokenizer, Word2Vec, StopWordsRemover}\n\n      import org.apache.spark.ml.feature.{HashingTF, Tokenizer, RegexTokenizer, Word2Vec, StopWordsRemover}\n\n\tscala> // Prepare training documents from a list of messages from emails used to filter them as spam or not spam\n\tscala> // If the original message is a spam then the label is 1 and if the message is genuine then the label is 0\n\tscala> val training = spark.createDataFrame(Seq((\"you@example.com\", \"hope you are well\", 0.0),(\"raj@example.com\", \"nice to hear from you\", 0.0),(\"thomas@example.com\", \"happy holidays\", 0.0),(\"mark@example.com\", \"see you tomorrow\", 0.0),(\"xyz@example.com\", \"save money\", 1.0),(\"top10@example.com\", \"low interest rate\", 1.0),(\"marketing@example.com\", \"cheap loan\", 1.0))).toDF(\"email\", \"message\", \"label\")\n\n      training: org.apache.spark.sql.DataFrame = [email: string, message: string ... 1 more field]\n\n\tscala> training.show()\n\n      +--------------------+--------------------+-----+\n\n      |               email|             message|label|\n\n      +--------------------+--------------------+-----+\n\n      |     you@example.com|   hope you are well|  0.0|\n\n      |     raj@example.com|nice to hear from...|  0.0|\n\n      |  thomas@example.com|      happy holidays|  0.0|\n\n      |    mark@example.com|    see you tomorrow|  0.0|\n\n      |     xyz@example.com|          save money|  1.0|\n\n      |   top10@example.com|   low interest rate|  1.0|\n\n      |marketing@example...|          cheap loan|  1.0|\n\n      +--------------------+--------------------+-----+\n\n\tscala>  // Configure an Spark machine learning pipeline, consisting of three stages: tokenizer, hashingTF, and lr.\n\tscala> val tokenizer = new Tokenizer().setInputCol(\"message\").setOutputCol(\"words\")\n\n      tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_166809bf629c\n\n\tscala> val hashingTF = new HashingTF().setNumFeatures(1000).setInputCol(\"words\").setOutputCol(\"features\")\n\n      hashingTF: org.apache.spark.ml.feature.HashingTF = hashingTF_e43616e13d19\n\n\tscala> // LogisticRegression parameter to make lr.fit() use at most 10 iterations and the regularization parameter.\n\tscala> // When a higher degree polynomial used by the algorithm to fit a set of points in a linear regression model, to prevent overfitting, regularization is used and this parameter is just for that\n\tscala> val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.01)\n\n      lr: org.apache.spark.ml.classification.LogisticRegression = logreg_ef3042fc75a3\n\n\tscala> val pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, lr))\n\n      pipeline: org.apache.spark.ml.Pipeline = pipeline_658b5edef0f2\n\n\tscala> // Fit the pipeline to train the model to study the messages\n\tscala> val model = pipeline.fit(training)\n\n      model: org.apache.spark.ml.PipelineModel = pipeline_658b5edef0f2\n\n\tscala> // Prepare messages for prediction, which are not categorized and leaving upto the algorithm to predict\n\tscala> val test = spark.createDataFrame(Seq((\"you@example.com\", \"how are you\"),(\"jain@example.com\", \"hope doing well\"),(\"caren@example.com\", \"want some money\"),(\"zhou@example.com\", \"secure loan\"),(\"ted@example.com\",\"need loan\"))).toDF(\"email\", \"message\")\n\n      test: org.apache.spark.sql.DataFrame = [email: string, message: string]\n\n\tscala> test.show()\n\n      +-----------------+---------------+\n\n      |            email|        message|\n\n      +-----------------+---------------+\n\n      |  you@example.com|    how are you|\n\n      | jain@example.com|hope doing well|\n\n      |caren@example.com|want some money|\n\n      | zhou@example.com|    secure loan|\n\n      |  ted@example.com|      need loan|\n\n      +-----------------+---------------+\n\n\tscala> // Make predictions on the new messages\n\tscala> val prediction = model.transform(test).select(\"email\", \"message\", \"prediction\")\n\n      prediction: org.apache.spark.sql.DataFrame = [email: string, message: string ... 1 more field]\n\n\tscala> prediction.show()\n\n      +-----------------+---------------+----------+\n\n      |            email|        message|prediction|\n\n      +-----------------+---------------+----------+\n\n      |  you@example.com|    how are you|       0.0|\n\n      | jain@example.com|hope doing well|       0.0|\n\n      |caren@example.com|want some money|       1.0|\n\n      | zhou@example.com|    secure loan|       1.0|\n\n      |  ted@example.com|      need loan|       1.0|\n\n      +-----------------+---------------+----------+ \n\n```", "```py\n scala> val wordsDF = tokenizer.transform(training)\n\n      wordsDF: org.apache.spark.sql.DataFrame = [email: string, message: string ... 2 more fields]\n\n\tscala> wordsDF.createOrReplaceTempView(\"word\")\n\tscala> val selectedFieldstDF = spark.sql(\"SELECT message, words FROM word\")\n\n      selectedFieldstDF: org.apache.spark.sql.DataFrame = [message: string, words: array<string>]\n\n\tscala> selectedFieldstDF.show()\n\n      +--------------------+--------------------+\n\n      |             message|               words|\n\n      +--------------------+--------------------+\n\n      |   hope you are well|[hope, you, are, ...|\n\n      |nice to hear from...|[nice, to, hear, ...|\n\n      |      happy holidays|   [happy, holidays]|\n\n      |    see you tomorrow|[see, you, tomorrow]|\n\n      |          save money|       [save, money]|\n\n      |   low interest rate|[low, interest, r...|\n\n      |          cheap loan|       [cheap, loan]|\n\n      +--------------------+--------------------+\n    scala> val featurizedDF = hashingTF.transform(wordsDF)\n\n      featurizedDF: org.apache.spark.sql.DataFrame = [email: string, message: string ... 3 more fields]\n\n\tscala> featurizedDF.createOrReplaceTempView(\"featurized\")\n\tscala> val selectedFeaturizedFieldstDF = spark.sql(\"SELECT words, features FROM featurized\")\n\n      selectedFeaturizedFieldstDF: org.apache.spark.sql.DataFrame = [words: array<string>, features: vector]\n\n\tscala> selectedFeaturizedFieldstDF.show()\n\n      +--------------------+--------------------+\n\n      |               words|            features|\n\n      +--------------------+--------------------+\n\n      |[hope, you, are, ...|(1000,[0,138,157,...|\n\n      |[nice, to, hear, ...|(1000,[370,388,42...|\n\n      |   [happy, holidays]|(1000,[141,457],[...|\n\n      |[see, you, tomorrow]|(1000,[25,425,515...|\n\n      |       [save, money]|(1000,[242,520],[...|\n\n      |[low, interest, r...|(1000,[70,253,618...|\n\n      |       [cheap, loan]|(1000,[410,666],[...| \n\t +--------------------+--------------------+ \n\n```", "```py\n\t  >>> from pyspark.ml import Pipeline\n\t  >>> from pyspark.ml.classification import LogisticRegression\n\t  >>> from pyspark.ml.feature import HashingTF, Tokenizer\n\t  >>> from pyspark.sql import Row\n\t  >>> # Prepare training documents from a list of messages from emails used to filter them as spam or not spam\n\t  >>> # If the original message is a spam then the label is 1 and if the message is genuine then the label is 0\n\t  >>> LabeledDocument = Row(\"email\", \"message\", \"label\")\n\t  >>> training = spark.createDataFrame([(\"you@example.com\", \"hope you are well\", 0.0),(\"raj@example.com\", \"nice to hear from you\", 0.0),(\"thomas@example.com\", \"happy holidays\", 0.0),(\"mark@example.com\", \"see you tomorrow\", 0.0),(\"xyz@example.com\", \"save money\", 1.0),(\"top10@example.com\", \"low interest rate\", 1.0),(\"marketing@example.com\", \"cheap loan\", 1.0)], [\"email\", \"message\", \"label\"])\n\t  >>> training.show()\n\n      +--------------------+--------------------+-----+\n\n      |               email|             message|label|\n\n      +--------------------+--------------------+-----+\n\n      |     you@example.com|   hope you are well|  0.0|\n\n      |     raj@example.com|nice to hear from...|  0.0|\n\n      |  thomas@example.com|      happy holidays|  0.0|\n\n      |    mark@example.com|    see you tomorrow|  0.0|\n\n      |     xyz@example.com|          save money|  1.0|\n\n      |   top10@example.com|   low interest rate|  1.0|\n\n      |marketing@example...|          cheap loan|  1.0|\n\n      +--------------------+--------------------+-----+\n\n\t>>> # Configure an Spark machin learning pipeline, consisting of three stages: tokenizer, hashingTF, and lr.\n\t>>> tokenizer = Tokenizer(inputCol=\"message\", outputCol=\"words\")\n\t>>> hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\")\n\t>>> # LogisticRegression parameter to make lr.fit() use at most 10 iterations and the regularization parameter.\n\t>>> # When a higher degree polynomial used by the algorithm to fit a set of points in a linear regression model, to prevent overfitting, regularization is used and this parameter is just for that\n\t>>> lr = LogisticRegression(maxIter=10, regParam=0.01)\n\t>>> pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n\t>>> # Fit the pipeline to train the model to study the messages\n\t>>> model = pipeline.fit(training)\n\t>>> # Prepare messages for prediction, which are not categorized and leaving upto the algorithm to predict\n\t>>> test = spark.createDataFrame([(\"you@example.com\", \"how are you\"),(\"jain@example.com\", \"hope doing well\"),(\"caren@example.com\", \"want some money\"),(\"zhou@example.com\", \"secure loan\"),(\"ted@example.com\",\"need loan\")], [\"email\", \"message\"])\n\t>>> test.show()\n\n      +-----------------+---------------+\n\n      |            email|        message|\n\n      +-----------------+---------------+\n\n      |  you@example.com|    how are you|\n\n      | jain@example.com|hope doing well|\n\n      |caren@example.com|want some money|\n\n      | zhou@example.com|    secure loan|\n\n      |  ted@example.com|      need loan|\n\n      +-----------------+---------------+\n\n\t>>> # Make predictions on the new messages\n\t>>> prediction = model.transform(test).select(\"email\", \"message\", \"prediction\")\n\t>>> prediction.show()\n\n      +-----------------+---------------+----------+\n\n      |            email|        message|prediction|\n\n      +-----------------+---------------+----------+\n\n      |  you@example.com|    how are you|       0.0|\n\n      | jain@example.com|hope doing well|       0.0|\n\n      |caren@example.com|want some money|       1.0|\n\n      | zhou@example.com|    secure loan|       1.0|\n\n      |  ted@example.com|      need loan|       1.0|    \n\n      +-----------------+---------------+----------+ \n\n```", "```py\n\t  >>> wordsDF = tokenizer.transform(training)\n\t  >>> wordsDF.createOrReplaceTempView(\"word\")\n\t  >>> selectedFieldstDF = spark.sql(\"SELECT message, words FROM word\")\n\t  >>> selectedFieldstDF.show()\n\n      +--------------------+--------------------+\n\n      |             message|               words|\n\n      +--------------------+--------------------+\n\n      |   hope you are well|[hope, you, are, ...|\n\n      |nice to hear from...|[nice, to, hear, ...|\n\n      |      happy holidays|   [happy, holidays]|\n\n      |    see you tomorrow|[see, you, tomorrow]|\n\n      |          save money|       [save, money]|\n\n      |   low interest rate|[low, interest, r...|\n\n      |          cheap loan|       [cheap, loan]|\n\n      +--------------------+--------------------+\n\n\t>>> featurizedDF = hashingTF.transform(wordsDF)\n\t>>> featurizedDF.createOrReplaceTempView(\"featurized\")\n\t>>> selectedFeaturizedFieldstDF = spark.sql(\"SELECT words, features FROM featurized\")\n\t>>> selectedFeaturizedFieldstDF.show()\n\n      +--------------------+--------------------+\n\n      |               words|            features|\n\n      +--------------------+--------------------+\n\n      |[hope, you, are, ...|(262144,[128160,1...|\n\n      |[nice, to, hear, ...|(262144,[22346,10...|\n\n      |   [happy, holidays]|(262144,[86293,23...|\n\n      |[see, you, tomorrow]|(262144,[29129,21...|\n\n      |       [save, money]|(262144,[199496,2...|\n\n      |[low, interest, r...|(262144,[68685,13...|\n\n      |       [cheap, loan]|(262144,[12946,16...|\n\n      +--------------------+--------------------+\n\n```", "```py\n\n\t  scala> import org.apache.spark.ml.feature.{HashingTF, Tokenizer, RegexTokenizer, Word2Vec, StopWordsRemover}\n\n      import org.apache.spark.ml.feature.{HashingTF, Tokenizer, RegexTokenizer, Word2Vec, StopWordsRemover}\n\n\tscala> // TODO - Change this directory to the right location where the data is stored\n\tscala> val dataDir = \"/Users/RajT/Downloads/20_newsgroups/*\"\n\n      dataDir: String = /Users/RajT/Downloads/20_newsgroups/*\n\n\tscala> //Read the entire text into a DataFrame\n\tscala> // Only the following directories under the data directory has benn considered for running this program talk.politics.guns, talk.politics.mideast, talk.politics.misc, talk.religion.misc. All other directories have been removed before running this program. There is no harm in retaining all the data. The only difference will be in the output.\n\tscala>  val textDF = sc.wholeTextFiles(dataDir).map{case(file, text) => text}.map(Tuple1.apply).toDF(\"sentence\")\n\n      textDF: org.apache.spark.sql.DataFrame = [sentence: string]\n\n\tscala>  // Tokenize the sentences to words\n\tscala>  val regexTokenizer = new RegexTokenizer().setInputCol(\"sentence\").setOutputCol(\"words\").setPattern(\"\\\\w+\").setGaps(false)\n\n      regexTokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_ba7ce8ec2333\n\n\tscala> val tokenizedDF = regexTokenizer.transform(textDF)\n\n      tokenizedDF: org.apache.spark.sql.DataFrame = [sentence: string, words: array<string>]\n\n\tscala>  // Remove the stop words such as a, an the, I etc which doesn't have any specific relevance to the synonyms\n\tscala> val remover = new StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\")\n\n      remover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_775db995b8e8\n\n\tscala> //Remove the stop words from the text\n\tscala> val filteredDF = remover.transform(tokenizedDF)\n\n      filteredDF: org.apache.spark.sql.DataFrame = [sentence: string, words: array<string> ... 1 more field]\n\n\tscala> //Prepare the Estimator\n\tscala> //It sets the vector size, and the method setMinCount sets the minimum number of times a token must appear to be included in the word2vec model's vocabulary.\n\tscala> val word2Vec = new Word2Vec().setInputCol(\"filtered\").setOutputCol(\"result\").setVectorSize(3).setMinCount(0)\n\n      word2Vec: org.apache.spark.ml.feature.Word2Vec = w2v_bb03091c4439\n\n\tscala> //Train the model\n\tscala> val model = word2Vec.fit(filteredDF)\n\n      model: org.apache.spark.ml.feature.Word2VecModel = w2v_bb03091c4439   \n\n\tscala> //Find 10 synonyms of a given word\n\tscala> val synonyms1 = model.findSynonyms(\"gun\", 10)\n\n      synonyms1: org.apache.spark.sql.DataFrame = [word: string, similarity: double]\n\n\tscala> synonyms1.show()\n\n      +---------+------------------+\n\n      |     word|        similarity|\n\n      +---------+------------------+\n\n      |      twa|0.9999976163843671|\n\n      |cigarette|0.9999943935045497|\n\n      |    sorts|0.9999885527530025|\n\n      |       jj|0.9999827967650881|\n\n      |presently|0.9999792188771406|\n\n      |    laden|0.9999775888361028|\n\n      |   notion|0.9999775296680583|\n\n      | settlers|0.9999746245431419|\n\n      |motivated|0.9999694932468436|\n\n      |qualified|0.9999678135106314|\n\n      +---------+------------------+\n\n\tscala> //Find 10 synonyms of a different word\n\tscala> val synonyms2 = model.findSynonyms(\"crime\", 10)\n\n      synonyms2: org.apache.spark.sql.DataFrame = [word: string, similarity: double]\n\n\tscala> synonyms2.show()\n\n      +-----------+------------------+\n\n      |       word|        similarity|\n\n      +-----------+------------------+\n\n      | abominable|0.9999997331058447|\n\n      |authorities|0.9999946968941679|\n\n      |cooperation|0.9999892536435327|\n\n      |  mortazavi| 0.999986396931714|\n\n      |herzegovina|0.9999861828226779|\n\n      |  important|0.9999853354260315|\n\n      |      1950s|0.9999832312575262|\n\n      |    analogy|0.9999828272311249|\n\n      |       bits|0.9999820987679822|\n\n      |technically|0.9999808208936487|\n\n      +-----------+------------------+\n\n```", "```py\n >>> from pyspark.ml.feature import Word2Vec\n\t  >>> from pyspark.ml.feature import RegexTokenizer\n\t  >>> from pyspark.sql import Row\n\t  >>> # TODO - Change this directory to the right location where the data is stored\n\t  >>> dataDir = \"/Users/RajT/Downloads/20_newsgroups/*\"\n\t  >>> # Read the entire text into a DataFrame. Only the following directories under the data directory has benn considered for running this program talk.politics.guns, talk.politics.mideast, talk.politics.misc, talk.religion.misc. All other directories have been removed before running this program. There is no harm in retaining all the data. The only difference will be in the output.\n\t  >>> textRDD = sc.wholeTextFiles(dataDir).map(lambda recs: Row(sentence=recs[1]))\n\t  >>> textDF = spark.createDataFrame(textRDD)\n\t  >>> # Tokenize the sentences to words\n\t  >>> regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", gaps=False, pattern=\"\\\\w+\")\n\t  >>> tokenizedDF = regexTokenizer.transform(textDF)\n\t  >>> # Prepare the Estimator\n\t  >>> # It sets the vector size, and the parameter minCount sets the minimum number of times a token must appear to be included in the word2vec model's vocabulary.\n\t  >>> word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"words\", outputCol=\"result\")\n\t  >>> # Train the model\n\t  >>> model = word2Vec.fit(tokenizedDF)\n\t  >>> # Find 10 synonyms of a given word\n\t  >>> synonyms1 = model.findSynonyms(\"gun\", 10)\n\t  >>> synonyms1.show()\n\n      +---------+------------------+\n\n      |     word|        similarity|\n\n      +---------+------------------+\n\n      | strapped|0.9999918504219028|\n\n      |    bingo|0.9999909957939888|\n\n      |collected|0.9999907658056393|\n\n      |  kingdom|0.9999896797527402|\n\n      | presumed|0.9999806586578037|\n\n      | patients|0.9999778970248504|\n\n      |    azats|0.9999718388241235|\n\n      |  opening| 0.999969723774294|\n\n      |  holdout|0.9999685636131942|\n\n      | contrast|0.9999677676714386|\n\n      +---------+------------------+\n\n\t>>> # Find 10 synonyms of a different word\n\t>>> synonyms2 = model.findSynonyms(\"crime\", 10)\n\t>>> synonyms2.show()\n\n      +-----------+------------------+\n\n      |       word|        similarity|\n\n      +-----------+------------------+\n\n      |   peaceful|0.9999983523475047|\n\n      |  democracy|0.9999964568156694|\n\n      |      areas| 0.999994036518118|\n\n      |  miniscule|0.9999920828755365|\n\n      |       lame|0.9999877327660102|\n\n      |    strikes|0.9999877253180771|\n\n      |terminology|0.9999839393584438|\n\n      |      wrath|0.9999829348358952|\n\n      |    divided| 0.999982619125983|\n\n      |    hillary|0.9999795817857984|\n\n      +-----------+------------------+ \n\n```"]