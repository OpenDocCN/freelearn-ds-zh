- en: Chapter 1. Spark Fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is one of the most important assets of any organization. The scale at which
    data is being collected and used in organizations is growing beyond imagination.
    The speed at which data is being ingested, the variety of the data types in use,
    and the amount of data that is being processed and stored are breaking all-time
    records every moment. It is very common these days, even in small-scale organizations,
    that data is growing from gigabytes to terabytes to petabytes. For the same reason,
    the processing needs are also growing that ask for capability to process data
    at rest as well as data on the move.
  prefs: []
  type: TYPE_NORMAL
- en: Take any organization; its success depends on the decisions made by its leaders
    and for making sound decisions, you need the backing of good data and the information
    generated by processing the data. This poses a big challenge on how to process
    the data in a timely and cost-effective manner so that right decisions can be
    made. Data processing techniques have evolved since the early days of computers.
    Countless data processing products and frameworks came into the market and disappeared
    over these years. Most of these data processing products and frameworks were not
    general purpose in nature. Most of the organizations relied on their own bespoke
    applications for their data processing needs, in a silo way, or in conjunction
    with specific products.
  prefs: []
  type: TYPE_NORMAL
- en: Large-scale Internet applications, popularly known as **Internet of Things**
    (**IoT**) applications, heralded the common need to have open frameworks to process
    huge amounts of data ingested at great speed dealing with various types of data.
    Large-scale web sites, media streaming applications, and the huge batch processing
    needs of organizations made the need even more relevant. The open source community
    is also growing considerably along with the growth of the Internet, delivering
    production quality software supported by reputed software companies. A huge number
    of companies started using open source software and started deploying them in
    their production environments.
  prefs: []
  type: TYPE_NORMAL
- en: In a technological perspective, the data processing needs were facing huge challenges.
    The amount of data started overflowing from single machines to clusters of huge
    numbers of machines. The processing power of the single CPU plateaued and modern
    computers started combining them together to get more processing power, known
    as multi-core computers. The applications were not designed and developed to make
    use of all the processors in a multi-core computer and wasted lots of the processing
    power available in a typical modern computer.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Throughout this book, the terms *node*, *host*, and *machine* refer to a computer
    that is running in a standalone mode or in a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In this context, what are the qualities an ideal data processing framework should
    possess?
  prefs: []
  type: TYPE_NORMAL
- en: It should be capable of processing the blocks of data distributed across a cluster
    of computers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be able to process the data in a parallel fashion so that a huge data
    processing job can be divided into multiple tasks processed in parallel so that
    the processing time can be reduced considerably
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be capable of using the processing power of all the cores or processors
    in a computer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be capable of using all the available computers in a cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be capable of running on commodity hardware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are two open source data processing frameworks that are worth mentioning
    that satisfy all these requirements. The first is being Apache Hadoop and the
    second one is Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark 2.0 installation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of Apache Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Hadoop is an open source software framework designed from ground-up
    to do distributed data storage on a cluster of computers and to do distributed
    data processing of the data that is spread across the cluster of computers. This
    framework comes with a distributed filesystem for the data storage, namely, **Hadoop
    Distributed File System** (**HDFS**), and a data processing framework, namely,
    MapReduce. The creation of HDFS is inspired from the Google research paper, *The
    Google File System* and MapReduce is based on the Google research paper, *MapReduce:
    Simplified Data Processing on Large Clusters*.'
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop was adopted by organizations in a really big way by implementing huge
    Hadoop clusters for data processing. It saw tremendous growth from Hadoop MapReduce
    version 1 (MRv1) to Hadoop MapReduce version 2 (MRv2). From a pure data processing
    perspective, MRv1 consisted of HDFS and MapReduce as the core components. Many
    applications, generally called SQL-on-Hadoop applications, such as Hive and Pig,
    were stacked on top of the MapReduce framework. It is very common to see that
    even though these types of applications are separate Apache projects, as a suite,
    many such projects provide great value.
  prefs: []
  type: TYPE_NORMAL
- en: The **Yet Another Resource Negotiator** (**YARN**) project  came to the fore
    with computing frameworks other than MapReduce type to run on the Hadoop ecosystem.
    With the introduction of YARN sitting on top of HDFS, and below MapReduce in a
    component architecture layering perspective, the users could write their own applications
    that can run on YARN and HDFS to make use of the distributed data storage and
    data processing capabilities of the Hadoop ecosystem. In other words, the newly
    overhauled MapReduce version 2 (MRv2) became one of the application frameworks
    sitting on top of HDFS and YARN.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 1* gives a brief idea about these components and how they are stacked
    together:'
  prefs: []
  type: TYPE_NORMAL
- en: '![An overview of Apache Hadoop](img/image_01_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce is a generic data processing model. The data processing goes through
    two steps, namely, *map* step and *reduce* step. In the first step, the input
    data is divided into a number of smaller parts so that each one of them can be
    processed independently. Once the *map* step is completed, its output is consolidated
    and the final result is generated in the *reduce* step. In a typical word count
    example, the creation of key-value pairs with each word as the key and the value
    1 is the *map* step. The sorting of these pairs on the key, summing the values
    of the pairs with the same key falls into an intermediate *combine* step. Producing
    the pairs containing unique words and their occurrence count is the *reduce* step.
  prefs: []
  type: TYPE_NORMAL
- en: 'From an application programming perspective, the basic ingredients for an over-simplified
    MapReduce application are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Input location
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output location
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Map function implemented for the data processing need from the appropriate interfaces
    and classes from the `MapReduce` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce function implemented for the data processing need from the appropriate
    interfaces and classes from the `MapReduce` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The MapReduce job is submitted for running in Hadoop and once the job is completed,
    the output can be taken from the output location specified.
  prefs: []
  type: TYPE_NORMAL
- en: This two-step process of dividing a `MapReduce` data processing job to *map*
    and *reduce* tasks was highly effective and turned out to be a perfect fit for
    many batch data processing use cases. There is a lot of Input/Output (I/O) operations
    with the disk happening under the hood during the whole process. Even in the intermediate
    steps of the MapReduce job, if the internal data structures are filled with data
    or when the tasks are completed beyond a certain percentage, writing to the disk
    happens. Because of this, the subsequent steps in the MapReduce jobs have to read
    from the disk.
  prefs: []
  type: TYPE_NORMAL
- en: Then the other biggest challenge comes when there are multiple MapReduce jobs
    to be completed in a chained fashion. In other words, if a big data processing
    work is to be accomplished by two MapReduce jobs in such a way that the output
    of the first MapReduce job is the input of the second MapReduce job. In this situation,
    whatever may be the size of the output of the first MapReduce job, it has to be
    written to the disk before the second MapReduce could use it as its input. So
    in this simple case, there is a definite and *unnecessary* write operation.
  prefs: []
  type: TYPE_NORMAL
- en: In many of the batch data processing use cases, these I/O operations are not
    a big issue. If the results are highly reliable, for many batch data processing
    use cases, latency is tolerated. But the biggest challenge comes when doing real-time
    data processing. The huge amount of I/O operations involved in MapReduce jobs
    makes it unsuitable for real-time data processing with the lowest possible latency.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark is a **Java Virtual Machine** (**JVM**) based distributed data processing
    engine that scales, and it is fast compared to many other data processing frameworks.
    Spark was originated at the *University of California Berkeley* and later became
    one of the top projects in Apache. The research paper, *Mesos: A Platform for
    Fine-Grained Resource Sharing in the Data Center*, talks about the philosophy
    behind the design of Spark. The research paper states:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"To test the hypothesis that simple specialized frameworks provide value,
    we identified one class of jobs that were found to perform poorly on Hadoop by
    machine learning researchers at our lab: iterative jobs, where a dataset is reused
    across a number of iterations. We built a specialized framework called Spark optimized
    for these workloads."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The biggest claim from Spark regarding speed is that it is able to *"Run programs
    up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk"*. Spark
    could make this claim because it does the processing in the main memory of the
    worker nodes and prevents the *unnecessary* I/O operations with the disks. The
    other advantage Spark offers is the ability to chain the tasks even at an application
    programming level without writing onto the disks at all or minimizing the number
    of writes to the disks.
  prefs: []
  type: TYPE_NORMAL
- en: 'How did Spark become so efficient in data processing as compared to MapReduce?
    It comes with a very advanced **Directed Acyclic Graph** (**DAG**) data processing
    engine. What it means is that for every Spark job, a DAG of tasks is created to
    be executed by the engine. The DAG in mathematical parlance consists of a set
    of vertices and directed edges connecting them. The tasks are executed as per
    the DAG layout. In the MapReduce case, the DAG consists of only two vertices,
    with one vertex for the *map* task and the other one for the *reduce* task. The
    edge is directed from the *map* vertex to the *reduce* vertex. The in-memory data
    processing combined with its DAG-based data processing engine makes Spark very
    efficient. In Spark''s case, the DAG of tasks can be as complicated as it can.
    Thankfully, Spark comes with utilities that can give excellent visualization of
    the DAG of any Spark job that is running. In a word count example, Spark''s Scala
    code will look something like the following code snippet . The details of this
    programming aspects will be covered in the coming chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The web application that comes with Spark is capable of monitoring the workers
    and applications. The DAG of the preceding Spark job generated on the fly will
    look like *Figure 2*, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Apache Spark](img/image_01_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2
  prefs: []
  type: TYPE_NORMAL
- en: The Spark programming paradigm is very powerful and exposes a uniform programming
    model supporting the application development in multiple programming languages.
    Spark supports programming in Scala, Java, Python, and R even though there is
    no functional parity across all the programming languages supported. Apart from
    writing Spark applications in these programming languages, Spark has an interactive
    shell with **Read, Evaluate, Print, and Loop** (**REPL**) capabilities for the
    programming languages Scala, Python, and R. At this moment, there is no REPL support
    for Java in Spark. The Spark REPL is a very versatile tool that can be used to
    try and test Spark application code in an interactive fashion. The Spark REPL
    enables easy prototyping, debugging, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the core data processing engine, Spark comes with a powerful
    stack of domain specific libraries that use the core Spark libraries and provide
    various functionalities useful for various big data processing needs. The following
    table lists the supported libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Library** | **Use** | **Supported Languages** |'
  prefs: []
  type: TYPE_TB
- en: '| Spark SQL | Enables the use of SQL statements or DataFrame API inside Spark
    applications | Scala, Java, Python, and R |'
  prefs: []
  type: TYPE_TB
- en: '| Spark Streaming | Enables processing of live data streams | Scala, Java,
    and Python |'
  prefs: []
  type: TYPE_TB
- en: '| Spark MLlib | Enables development of machine learning applications | Scala,
    Java, Python, and R |'
  prefs: []
  type: TYPE_TB
- en: '| Spark GraphX | Enables graph processing and supports a growing library of
    graph algorithms | Scala |'
  prefs: []
  type: TYPE_TB
- en: Spark can be deployed on a variety of platforms. Spark runs on the **operating
    systems** (**OS**) Windows and UNIX (such as Linux and Mac OS). Spark can be deployed
    in a standalone mode on a single node having a supported OS. Spark can also be
    deployed in cluster node on Hadoop YARN as well as Apache Mesos. Spark can be
    deployed in the Amazon EC2 cloud as well. Spark can access data from a wide variety
    of data stores, and some of the most popular ones include HDFS, Apache Cassandra,
    Hbase, Hive, and so on. Apart from the previously listed data stores, if there
    is a driver or connector program available, Spark can access data from pretty
    much any data source.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All the examples used in this book are developed, tested, and run on a Mac OS
    X Version 10.9.5 computer. The same instructions are applicable for all the other
    platforms except Windows. In Windows, corresponding to all the UNIX commands,
    there is a file with a `.cmd` extension and it has to be used. For example, for
    `spark-shell` in UNIX, there is a `spark-shell.cmd` in Windows. The program behavior
    and results should be the same across all the supported OS.
  prefs: []
  type: TYPE_NORMAL
- en: 'In any distributed application, it is common to have a driver program that
    controls the execution and there will be one or more worker nodes. The driver
    program allocates the tasks to the appropriate workers. This is the same even
    if Spark is running in standalone mode. In the case of a Spark application, its
    **SparkContext** object is the driver program and it communicates with the appropriate
    cluster manager to run the tasks. The Spark master, which is part of the Spark
    core library, the Mesos master, and the Hadoop YARN Resource Manager, are some
    of the cluster managers that Spark supports. In the case of a Hadoop YARN deployment
    of Spark, the Spark driver program runs inside the Hadoop YARN application master
    process or the Spark driver program runs as a client to the Hadoop YARN. *Figure
    3* describes the standalone deployment of Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Apache Spark](img/image_01_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Mesos deployment mode of Spark, the cluster manager will be the **Mesos
    Master**. *Figure 4* describes the Mesos deployment of Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Apache Spark](img/image_01_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Hadoop YARN deployment mode of Spark, the cluster manager will be the
    Hadoop Resource Manager and its address will be picked up from the Hadoop configuration.
    In other words, when submitting the Spark jobs, there is no need to give an explicit
    master URL and it will pick up the details of the cluster manager from the Hadoop
    configuration. *Figure 5* describes the Hadoop YARN deployment of Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Apache Spark](img/image_01_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5
  prefs: []
  type: TYPE_NORMAL
- en: Spark runs in the cloud too. In the case of the deployment of Spark on Amazon
    EC2, apart from accessing the data from the regular supported data sources, Spark
    can also access data from Amazon S3, which is the online data storage service
    from Amazon.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Spark on your machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark supports application development in Scala, Java, Python, and R. In this
    book, Scala, Python, and R, are used. Here is the reason behind the choice of
    the languages for the examples in this book. The Spark interactive shell, or REPL,
    allows the user to execute programs on the fly just like entering OS commands
    on a terminal prompt and it is available only for the languages Scala, Python
    and R. REPL is the best way to try out Spark code before putting them together
    in a file and running them as applications. REPL helps even the experienced programmer
    to try and test the code and thus facilitates fast prototyping. So, especially
    for beginners, using REPL is the best way to get started with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: As a pre-requisite to Spark installation and to do Spark programming in Python
    and R, both Python and R are to be installed prior to the installation of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Python installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Visit [https://www.python.org](https://www.python.org/) for downloading and
    installing Python for your computer. Once the installation is complete, make sure
    that the required binaries are in the OS search path and the Python interactive
    shell is coming up properly. The shell should display some content similar to
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For charting and plotting, the `matplotlib` library is being used.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Python version 3.5.0 is used as a version of choice for Python. Even though
    Spark supports programming in Python version 2.7, as a forward looking practice,
    the latest and most stable version of Python available is used. Moreover, most
    of the important libraries are getting ported to Python version 3.x as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visit [http://matplotlib.org](http://matplotlib.org/) for downloading and installing
    the library. To make sure that the library is installed properly and that charts
    and plots are getting displayed properly, visit the [http://matplotlib.org/examples/index.html](http://matplotlib.org/examples/index.html)
    page to pick up some example code and see that your computer has all the required
    resources and components for charting and plotting. While trying to run some of
    these charting and plotting samples, in the context of the import of the libraries
    in Python code, there is a possibility that it may complain about the missing
    locale. In that case, set the following environment variables in the appropriate
    user profile to get rid of the error messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: R installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Visit [https://www.r-project.org](https://www.r-project.org/) for downloading
    and installing R for your computer. Once the installation is complete, make sure
    that the required binaries are in the OS search path and the R interactive shell
    is coming up properly. The shell should display some content similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: R version 3.2.2 is the choice for R.
  prefs: []
  type: TYPE_NORMAL
- en: Spark installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark installation can be done in many different ways. The most important pre-requisite
    for Spark installation is that the Java 1.8 JDK is installed in the system and
    the `JAVA_HOME` environment variable is set to point to the Java 1.8 JDK installation
    directory. Visit [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)
    for understanding, choosing, and downloading the right type of installation for
    your computer. Spark version 2.0.0 is the version of choice for following the
    examples given in this book. Anyone who is interested in building and using Spark
    from the source code should visit: [http://spark.apache.org/docs/latest/building-spark.html](http://spark.apache.org/docs/latest/building-spark.html)
    for the instructions. By default, when you build Spark from the source code, it
    will not build the R libraries for Spark. For that, the SparkR libraries have
    to be built and the appropriate profile has to be included while building Spark
    from source code. The following command shows how to include the profile required
    to build the SparkR libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the Spark installation is complete, define the following environment variables
    in the appropriate user profile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If there are multiple versions of Python executables in the system, then it
    is better to explicitly specify the Python executable to be used by Spark in the
    following environment variable setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `$SPARK_HOME/bin/pyspark` script, there is a block of code that determines
    the Python executable to be used by Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: So, it is always better to explicitly set the Python executable for Spark, even
    if there is only one version of Python available in the system. This is a safeguard
    to prevent unexpected behavior when an additional version of Python is installed
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once all the preceding steps are completed successfully, make sure that all
    the Spark shells for the languages Scala, Python, and R are working properly.
    Run the following commands on the OS terminal prompt and make sure that there
    are no errors and that content similar to the following is getting displayed.
    The following set of commands is used to bring up the Scala REPL of Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding display, verify that the JDK version, Scala version, and Spark
    version are correct as per the settings in the computer in which Spark is installed.
    The most important point to verify is that no error messages are displayed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following set of commands is used to bring up the Python REPL of Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding display, verify that the Python version, and Spark version
    are correct as per the settings in the computer in which Spark is installed. The
    most important point to verify is that no error messages are displayed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following set of commands are used to bring up the R REPL of Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding display, verify that the R version and Spark version are correct
    as per the settings in the computer in which Spark is installed. The most important
    point to verify is that no error messages are displayed.
  prefs: []
  type: TYPE_NORMAL
- en: 'If all the REPL for Scala, Python, and R are working fine, it is almost certain
    that the Spark installation is good. As a final test, run some of the example
    programs that came with Spark and make sure that they are giving proper results
    close to the results shown below the commands and not throwing any error messages
    in the console. When these example programs are run, apart from the output shown
    below the commands, there will be lot of other messages displayed in the console.
    They are omitted to focus on the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Development tool installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most of the code that is going to be discussed in this book can be tried and
    tested in the appropriate REPL. But the proper Spark application development is
    not possible without some basic build tools. As a bare minimum requirement, for
    developing and building Spark applications in Scala, the **Scala build tool**
    (**sbt**) is a must. Visit [http://www.scala-sbt.org](http://www.scala-sbt.org/)
    for downloading and installing sbt.
  prefs: []
  type: TYPE_NORMAL
- en: Maven is the preferred build tool for building Java applications. This book
    is not talking about Spark application development in Java, but it is good to
    have Maven also installed in the system. Maven will come in handy if Spark is
    to be built from source. Visit [https://maven.apache.org](https://maven.apache.org/)
    for downloading and installing Maven.
  prefs: []
  type: TYPE_NORMAL
- en: There are many **Integrated Development Environments** (**IDEs**) available
    for Scala as well as Java. It is a personal choice, and the developer can choose
    the tool of his/her choice for the language in which he/she is developing Spark
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Optional software installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark REPL for Scala is a good start to get into the prototyping and testing
    of some small snippets of code. But when there is a need to develop, build, and
    package Spark applications in Scala, it is good to have sbt-based Scala projects
    and develop them using a supported IDE, including but not limited to Eclipse or
    IntelliJ IDEA. Visit the appropriate website for downloading and installing the
    preferred IDE for Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Notebook style application development tools are very common these days among
    data analysts and researchers. This is akin to a lab notebook. In a typical lab
    notebook, there will be instructions, detailed descriptions, and steps to follow
    to conduct an experiment. Then the experiments are conducted. Once the experiments
    are completed, there will be results captured in the notebook. If all these constructs
    are combined together and fit into the context of a software program and modeled
    in a lab notebook format, there will be documentation, code, input, and the output
    generated by running the code. This will give a very good effect, especially if
    the programs generate a lot of charts and plots.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For those who are not familiar with notebook style application development
    IDEs, there is a very nice article entitled *Interactive Notebooks: Sharing the
    Code* that can be read from [http://www.nature.com/news/interactive-notebooks-sharing-the-code-1.16261](http://www.nature.com/news/interactive-notebooks-sharing-the-code-1.16261).
    As an optional software development IDE for Python, the IPython notebook is described
    in the following section. After the installation, get yourself familiar with the
    tool before getting into serious development with it.'
  prefs: []
  type: TYPE_NORMAL
- en: IPython
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the case of Spark application development in Python, IPython provides an
    excellent notebook-style development tool, which is a Python language kernel for
    Jupyter. Spark can be integrated with IPython, so that when the Spark REPL for
    Python is invoked, it will start the IPython notebook. Then, create a notebook
    and start writing code in the notebook just like the way commands are given in
    the Spark REPL for Python. Visit [http://ipython.org](http://ipython.org/) to
    download and install the IPython notebook. Once the installation is complete,
    invoke the IPython notebook interface and make sure that some example Python code
    is running fine. Invoke commands from the directory from where the notebooks are
    stored or where the notebooks are to be stored. Here, the IPython notebook is
    started from a temporary directory. When the following commands are invoked, it
    will open up the web interface and from there create a new notebook by clicking
    the New drop-down box and picking up the appropriate Python version.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows how to combine a markdown style documentation,
    a Python program, and the generated output together in an IPython notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![IPython](img/image_01_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6* shows how the IPython notebook can be used to write simple Python
    programs. The IPython notebook can be configured as a shell of choice for Spark,
    and when the Spark REPL for Python is invoked, it will start up the IPython notebook
    and Spark application development can be done using IPython notebook. To achieve
    that, define the following environment variables in the appropriate user profile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, instead of invoking the IPython notebook from the command prompt, invoke
    the Spark REPL for Python. Just like what has been done before, create a new IPython
    notebook and start writing Spark code in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![IPython](img/image_01_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the standard Spark REPL for any language, it is possible to refer the files
    located in the local filesystem with their relative path. When the IPython notebook
    is being used, local files are to be referred with their full path.
  prefs: []
  type: TYPE_NORMAL
- en: RStudio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Among the R user community, the preferred IDE for R is the RStudio. RStudio
    can be used to develop Spark applications in R as well. Visit [https://www.rstudio.com](https://www.rstudio.com/)
    to download and install RStudio. Once the installation is complete, before running
    any Spark R code, it is mandatory to include the `SparkR` libraries and set some
    variables to make sure that the Spark R programs are running smoothly from RStudio.
    The following code snippet does that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding R code, change the `SPARK_HOME_DIR` variable definition to
    point to the directory where Spark is installed. *Figure 8* shows a sample run
    of the Spark R code from RStudio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![RStudio](img/image_01_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8
  prefs: []
  type: TYPE_NORMAL
- en: Once all the required software is installed, configured, and working as per
    the details given previously, the stage is set for Spark application development
    in Scala, Python, and R.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Jupyter notebook supports multiple languages through the custom kernel implementation
    strategy for various languages. There is a native R kernel, namely IRkernel, for
    Jupyter which can be installed as an R package.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Zeppelin
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apache Zeppelin is another promising project that is getting incubated right
    now. It is a web-based notebook similar to Jupyter but supporting multiple languages,
    shells, and technologies through its interpreter strategy enabling Spark application
    development inherently. Right now it is in its infant stage, but it has a lot
    of potential to become one of the best notebook-based application development
    platforms. Zeppelin has very powerful built-in charting and plotting capabilities
    using the data generated by the programs written in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Zeppelin is built with high extensibility having the ability to plug in many
    types of interpreters using its Interpreter Framework. End users, just like any
    other notebook-based system, enter various commands in the notebook interface.
    These commands are to be processed by some interpreter to generate the output.
    Unlike many other notebook-style systems, Zeppelin supports a good number of interpreters
    or backends out of the box such as Spark, Spark SQL, Shell, Markdown, and many
    more. In terms of the frontend, again it is a pluggable architecture, namely,
    the **Helium Framework**. The data generated by the backend is displayed by the
    frontend components such as Angular JS. There are various options to display the
    data in tabular format, raw format as generated by the interpreters, charts, and
    plots. Because of the architectural separation of concerns such as the backend,
    the frontend, and the ability to plug in various components, it is a great way
    to choose heterogeneous components for the right job. At the same time, it integrates
    very well to provide a harmonious end-user-friendly data processing ecosystem.
    Even though there is pluggable architecture capability for various components
    in Zeppelin, the visualizations are limited. In other words, there are only a
    few charting and plotting options available out of the box in Zeppelin. Once the
    notebooks are working fine and producing the expected results, typically, the
    notebooks are shared with other people and for that, the notebooks are to be persisted.
    Zeppelin is different again here and it has a highly versatile notebook storage
    system. The notebooks can be persisted to the filesystem, Amazon S3, or Git, and
    other storage targets can be added if required.
  prefs: []
  type: TYPE_NORMAL
- en: '**Platform as a Service** (**PaaS**) has been evolving over the last couple
    of years since the massive innovations happening around Cloud as an application
    development and deployment platform. For software developers, there are many PaaS
    platforms available delivered through Cloud, which obviates the need for them
    to have their own application development stack. Databricks has introduced a Cloud-based
    big data platform in which users can have access to a notebook-based Spark application
    development interface in conjunction with micro-cluster infrastructure to which
    the Spark applications can be submitted. There is a community edition as well,
    catering to the needs of a wider development community. The biggest advantage
    of this PaaS platform is that it is a browser-based interface and users can run
    their code against multiple versions of Spark and on different types of clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information please refer to the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf](http://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf](http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.cs.berkeley.edu/~alig/papers/mesos.pdf](https://www.cs.berkeley.edu/~alig/papers/mesos.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/](http://spark.apache.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://jupyter.org/](https://jupyter.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/IRkernel/IRkernel](https://github.com/IRkernel/IRkernel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://zeppelin.incubator.apache.org/](https://zeppelin.incubator.apache.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://community.cloud.databricks.com/](https://community.cloud.databricks.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark is a very powerful data processing platform supporting a uniform programming
    model. It supports application development in Scala, Java, Python, and R, providing
    a stack of highly interoperable libraries used for various types of data processing
    needs, and a plethora of third-party libraries that make use of the Spark ecosystem
    covering various other data processing use cases. This chapter gave a brief introduction
    to Spark and setting up the development environment for the Spark application
    development that is going to be covered in forthcoming chapters of the book.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is going to discuss the Spark programming model, the basic
    abstractions and terminologies, Spark transformations, and Spark actions, in conjunction
    with real-world use cases.
  prefs: []
  type: TYPE_NORMAL
