- en: Chapter 1. Spark Fundamentals
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章：Spark基础
- en: Data is one of the most important assets of any organization. The scale at which
    data is being collected and used in organizations is growing beyond imagination.
    The speed at which data is being ingested, the variety of the data types in use,
    and the amount of data that is being processed and stored are breaking all-time
    records every moment. It is very common these days, even in small-scale organizations,
    that data is growing from gigabytes to terabytes to petabytes. For the same reason,
    the processing needs are also growing that ask for capability to process data
    at rest as well as data on the move.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是任何组织最重要的资产之一。在组织中收集和使用数据的规模正在超越想象。数据被摄入的速度、正在使用的不同数据类型的多样性，以及正在处理和存储的数据量都在每一刻打破历史记录。如今，即使在小型组织中，数据量也从千兆字节增长到太字节再到拍字节，这种情况非常普遍。同样，处理需求也在增长，需要具备处理静态数据和移动数据的能力。
- en: Take any organization; its success depends on the decisions made by its leaders
    and for making sound decisions, you need the backing of good data and the information
    generated by processing the data. This poses a big challenge on how to process
    the data in a timely and cost-effective manner so that right decisions can be
    made. Data processing techniques have evolved since the early days of computers.
    Countless data processing products and frameworks came into the market and disappeared
    over these years. Most of these data processing products and frameworks were not
    general purpose in nature. Most of the organizations relied on their own bespoke
    applications for their data processing needs, in a silo way, or in conjunction
    with specific products.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 任何组织的成功都取决于其领导者的决策，而为了做出明智的决策，你需要有良好的数据和数据处理所产生信息的支持。这给如何在及时且成本效益的方式下处理数据带来了巨大挑战，以便能够做出正确的决策。自从计算机的早期阶段以来，数据处理技术已经发展。无数的数据处理产品和框架进入市场，并在这些年中消失。大多数这些数据处理产品和框架在本质上不是通用的。大多数组织依赖他们自己的定制应用程序来满足他们的数据处理需求，以隔离的方式，或者与特定产品结合使用。
- en: Large-scale Internet applications, popularly known as **Internet of Things**
    (**IoT**) applications, heralded the common need to have open frameworks to process
    huge amounts of data ingested at great speed dealing with various types of data.
    Large-scale web sites, media streaming applications, and the huge batch processing
    needs of organizations made the need even more relevant. The open source community
    is also growing considerably along with the growth of the Internet, delivering
    production quality software supported by reputed software companies. A huge number
    of companies started using open source software and started deploying them in
    their production environments.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模互联网应用，通常称为**物联网**（**IoT**）应用，预示了处理大量以极快速度摄入的各种类型数据的开放框架的普遍需求。大型网站、媒体流应用以及组织的大量批量处理需求使这一需求更加相关。随着互联网的增长，开源社区也在显著增长，提供由知名软件公司支持的量产级软件。大量公司开始使用开源软件，并将它们部署到他们的生产环境中。
- en: In a technological perspective, the data processing needs were facing huge challenges.
    The amount of data started overflowing from single machines to clusters of huge
    numbers of machines. The processing power of the single CPU plateaued and modern
    computers started combining them together to get more processing power, known
    as multi-core computers. The applications were not designed and developed to make
    use of all the processors in a multi-core computer and wasted lots of the processing
    power available in a typical modern computer.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度来看，数据处理需求面临着巨大的挑战。数据量开始从单个机器溢出到由大量机器组成的集群。单个CPU的处理能力达到了顶峰，现代计算机开始将它们组合在一起以获得更多的处理能力，这被称为多核计算机。应用程序没有设计和开发来利用多核计算机中的所有处理器，浪费了典型现代计算机中可用的大量处理能力。
- en: Note
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Throughout this book, the terms *node*, *host*, and *machine* refer to a computer
    that is running in a standalone mode or in a cluster.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的整个过程中，术语*节点*、*主机*和*机器*指的是以独立模式或集群模式运行的计算机。
- en: In this context, what are the qualities an ideal data processing framework should
    possess?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种背景下，一个理想的数据处理框架应该具备哪些品质？
- en: It should be capable of processing the blocks of data distributed across a cluster
    of computers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该能够处理分布在计算机集群中的数据块
- en: It should be able to process the data in a parallel fashion so that a huge data
    processing job can be divided into multiple tasks processed in parallel so that
    the processing time can be reduced considerably
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该能够以并行方式处理数据，以便将巨大的数据处理作业分成多个并行处理的任务，从而大大减少处理时间
- en: It should be capable of using the processing power of all the cores or processors
    in a computer
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该能够使用计算机中所有核心或处理器的处理能力
- en: It should be capable of using all the available computers in a cluster
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该能够使用集群中所有可用的计算机
- en: It should be capable of running on commodity hardware
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该能够在通用硬件上运行
- en: There are two open source data processing frameworks that are worth mentioning
    that satisfy all these requirements. The first is being Apache Hadoop and the
    second one is Apache Spark.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个开源数据处理框架值得提及，它们满足所有这些要求。第一个是Apache Hadoop，第二个是Apache Spark。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中介绍以下主题：
- en: Apache Hadoop
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Hadoop
- en: Apache Spark
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark
- en: Spark 2.0 installation
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 2.0安装
- en: An overview of Apache Hadoop
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Hadoop概述
- en: 'Apache Hadoop is an open source software framework designed from ground-up
    to do distributed data storage on a cluster of computers and to do distributed
    data processing of the data that is spread across the cluster of computers. This
    framework comes with a distributed filesystem for the data storage, namely, **Hadoop
    Distributed File System** (**HDFS**), and a data processing framework, namely,
    MapReduce. The creation of HDFS is inspired from the Google research paper, *The
    Google File System* and MapReduce is based on the Google research paper, *MapReduce:
    Simplified Data Processing on Large Clusters*.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 'Apache Hadoop是一个从头开始设计的开源软件框架，旨在在计算机集群上执行分布式数据存储，并对分布在计算机集群中的数据进行分布式数据处理。该框架包含一个用于数据存储的分布式文件系统，即**Hadoop分布式文件系统**（**HDFS**），以及一个数据处理框架，即MapReduce。HDFS的创建灵感来源于谷歌的研究论文《The
    Google File System》，而MapReduce基于谷歌的研究论文《MapReduce: Simplified Data Processing
    on Large Clusters》。'
- en: Hadoop was adopted by organizations in a really big way by implementing huge
    Hadoop clusters for data processing. It saw tremendous growth from Hadoop MapReduce
    version 1 (MRv1) to Hadoop MapReduce version 2 (MRv2). From a pure data processing
    perspective, MRv1 consisted of HDFS and MapReduce as the core components. Many
    applications, generally called SQL-on-Hadoop applications, such as Hive and Pig,
    were stacked on top of the MapReduce framework. It is very common to see that
    even though these types of applications are separate Apache projects, as a suite,
    many such projects provide great value.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实施大型Hadoop集群进行数据处理，Hadoop被组织大规模采用。从Hadoop MapReduce版本1（MRv1）到Hadoop MapReduce版本2（MRv2），它经历了巨大的增长。从纯粹的数据处理角度来看，MRv1由HDFS和MapReduce作为核心组件组成。许多应用程序，通常称为Hadoop上的SQL应用程序，如Hive和Pig，堆叠在MapReduce框架之上。非常常见的是，尽管这些类型的应用程序是独立的Apache项目，但作为一个套件，许多这样的项目提供了巨大的价值。
- en: The **Yet Another Resource Negotiator** (**YARN**) project  came to the fore
    with computing frameworks other than MapReduce type to run on the Hadoop ecosystem.
    With the introduction of YARN sitting on top of HDFS, and below MapReduce in a
    component architecture layering perspective, the users could write their own applications
    that can run on YARN and HDFS to make use of the distributed data storage and
    data processing capabilities of the Hadoop ecosystem. In other words, the newly
    overhauled MapReduce version 2 (MRv2) became one of the application frameworks
    sitting on top of HDFS and YARN.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**另一个资源协调器**（**YARN**）项目随着除MapReduce类型之外的计算框架在Hadoop生态系统中的应用而变得突出。在YARN引入到HDFS之上，并在组件架构分层视角中位于MapReduce之下，用户可以编写自己的应用程序，这些应用程序可以在YARN和HDFS上运行，以利用Hadoop生态系统的分布式数据存储和数据处理能力。换句话说，全新改版的MapReduce版本2（MRv2）成为了一个位于HDFS和YARN之上的应用框架之一。'
- en: '*Figure 1* gives a brief idea about these components and how they are stacked
    together:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1* 简要介绍了这些组件以及它们是如何堆叠在一起的：'
- en: '![An overview of Apache Hadoop](img/image_01_002.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Hadoop概述](img/image_01_002.jpg)'
- en: Figure 1
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1
- en: MapReduce is a generic data processing model. The data processing goes through
    two steps, namely, *map* step and *reduce* step. In the first step, the input
    data is divided into a number of smaller parts so that each one of them can be
    processed independently. Once the *map* step is completed, its output is consolidated
    and the final result is generated in the *reduce* step. In a typical word count
    example, the creation of key-value pairs with each word as the key and the value
    1 is the *map* step. The sorting of these pairs on the key, summing the values
    of the pairs with the same key falls into an intermediate *combine* step. Producing
    the pairs containing unique words and their occurrence count is the *reduce* step.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce是一个通用的数据处理模型。数据处理分为两个步骤，即*map*步骤和*reduce*步骤。在第一步中，输入数据被分成多个较小的部分，以便每个部分都可以独立处理。一旦*map*步骤完成，其输出将被整合，最终结果在*reduce*步骤中生成。在一个典型的词频统计示例中，将每个单词作为键，值为1的键值对创建是*map*步骤。根据键对这些对进行排序，对具有相同键的值的求和属于一个中间的*combine*步骤。生成包含唯一单词及其出现次数的对是*reduce*步骤。
- en: 'From an application programming perspective, the basic ingredients for an over-simplified
    MapReduce application are as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从应用编程的角度来看，一个过度简化的MapReduce应用程序的基本要素如下：
- en: Input location
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入位置
- en: Output location
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出位置
- en: Map function implemented for the data processing need from the appropriate interfaces
    and classes from the `MapReduce` library
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现数据处理所需的Map函数需要从`MapReduce`库的适当接口和类中
- en: Reduce function implemented for the data processing need from the appropriate
    interfaces and classes from the `MapReduce` library
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现数据处理所需的Reduce函数需要从`MapReduce`库的适当接口和类中
- en: The MapReduce job is submitted for running in Hadoop and once the job is completed,
    the output can be taken from the output location specified.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 将MapReduce作业提交到Hadoop中运行，一旦作业完成，可以从指定的输出位置获取输出。
- en: This two-step process of dividing a `MapReduce` data processing job to *map*
    and *reduce* tasks was highly effective and turned out to be a perfect fit for
    many batch data processing use cases. There is a lot of Input/Output (I/O) operations
    with the disk happening under the hood during the whole process. Even in the intermediate
    steps of the MapReduce job, if the internal data structures are filled with data
    or when the tasks are completed beyond a certain percentage, writing to the disk
    happens. Because of this, the subsequent steps in the MapReduce jobs have to read
    from the disk.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 将`MapReduce`数据处理作业分为*map*和*reduce*任务的这个两步过程非常有效，并且最终证明非常适合许多批处理数据处理用例。在整个过程中，在幕后发生了大量的输入/输出（I/O）操作。即使在MapReduce作业的中间步骤中，如果内部数据结构填充了数据或当任务完成超过一定百分比时，也会发生写入操作。正因为如此，MapReduce作业的后续步骤必须从磁盘读取。
- en: Then the other biggest challenge comes when there are multiple MapReduce jobs
    to be completed in a chained fashion. In other words, if a big data processing
    work is to be accomplished by two MapReduce jobs in such a way that the output
    of the first MapReduce job is the input of the second MapReduce job. In this situation,
    whatever may be the size of the output of the first MapReduce job, it has to be
    written to the disk before the second MapReduce could use it as its input. So
    in this simple case, there is a definite and *unnecessary* write operation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后另一个最大的挑战出现在有多个MapReduce作业需要按顺序完成时。换句话说，如果一个大型的数据处理工作需要通过两个MapReduce作业来完成，使得第一个MapReduce作业的输出是第二个MapReduce作业的输入。在这种情况下，无论第一个MapReduce作业的输出大小如何，都必须将其写入磁盘，第二个MapReduce才能将其用作其输入。因此，在这种情况下，存在一个明确且*不必要的*写入操作。
- en: In many of the batch data processing use cases, these I/O operations are not
    a big issue. If the results are highly reliable, for many batch data processing
    use cases, latency is tolerated. But the biggest challenge comes when doing real-time
    data processing. The huge amount of I/O operations involved in MapReduce jobs
    makes it unsuitable for real-time data processing with the lowest possible latency.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多批处理数据处理的用例中，这些I/O操作并不是一个大问题。如果结果高度可靠，对于许多批处理数据处理的用例，延迟是可以容忍的。但最大的挑战出现在进行实时数据处理时。MapReduce作业中涉及的巨大I/O操作使得它不适合具有最低延迟的实时数据处理。
- en: Understanding Apache Spark
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Apache Spark
- en: 'Spark is a **Java Virtual Machine** (**JVM**) based distributed data processing
    engine that scales, and it is fast compared to many other data processing frameworks.
    Spark was originated at the *University of California Berkeley* and later became
    one of the top projects in Apache. The research paper, *Mesos: A Platform for
    Fine-Grained Resource Sharing in the Data Center*, talks about the philosophy
    behind the design of Spark. The research paper states:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是一个基于**Java虚拟机**（**JVM**）的分布式数据处理引擎，它可以扩展，并且与其他许多数据处理框架相比，速度很快。Spark起源于**加州大学伯克利分校**，后来成为Apache项目中的顶级项目之一。研究论文《Mesos：数据中心细粒度资源共享平台》讨论了Spark设计背后的哲学。该研究论文指出：
- en: '*"To test the hypothesis that simple specialized frameworks provide value,
    we identified one class of jobs that were found to perform poorly on Hadoop by
    machine learning researchers at our lab: iterative jobs, where a dataset is reused
    across a number of iterations. We built a specialized framework called Spark optimized
    for these workloads."*'
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*"为了测试简单专用框架提供价值的假设，我们确定了一类在Hadoop上被机器学习研究人员发现表现不佳的工作：迭代型工作，其中数据集在多次迭代中被重复使用。我们构建了一个针对这些工作负载优化的专用框架，称为Spark。"*'
- en: The biggest claim from Spark regarding speed is that it is able to *"Run programs
    up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk"*. Spark
    could make this claim because it does the processing in the main memory of the
    worker nodes and prevents the *unnecessary* I/O operations with the disks. The
    other advantage Spark offers is the ability to chain the tasks even at an application
    programming level without writing onto the disks at all or minimizing the number
    of writes to the disks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Spark关于速度的最大声明是它能够*"在内存中运行程序比Hadoop MapReduce快100倍，或在磁盘上快10倍"*. Spark能够提出这个声明，因为它在工作节点的内存中进行处理，并防止与磁盘进行*不必要的*I/O操作。Spark提供的另一个优势是能够在应用程序编程级别链式连接任务，而无需写入磁盘或最小化写入磁盘的次数。
- en: 'How did Spark become so efficient in data processing as compared to MapReduce?
    It comes with a very advanced **Directed Acyclic Graph** (**DAG**) data processing
    engine. What it means is that for every Spark job, a DAG of tasks is created to
    be executed by the engine. The DAG in mathematical parlance consists of a set
    of vertices and directed edges connecting them. The tasks are executed as per
    the DAG layout. In the MapReduce case, the DAG consists of only two vertices,
    with one vertex for the *map* task and the other one for the *reduce* task. The
    edge is directed from the *map* vertex to the *reduce* vertex. The in-memory data
    processing combined with its DAG-based data processing engine makes Spark very
    efficient. In Spark''s case, the DAG of tasks can be as complicated as it can.
    Thankfully, Spark comes with utilities that can give excellent visualization of
    the DAG of any Spark job that is running. In a word count example, Spark''s Scala
    code will look something like the following code snippet . The details of this
    programming aspects will be covered in the coming chapters:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Spark与MapReduce相比，在数据处理方面是如何变得如此高效的？它附带一个非常先进的**有向无环图**（**DAG**）数据处理引擎。这意味着对于每个Spark作业，都会创建一个DAG任务由引擎执行。在数学术语中，DAG由一组顶点和连接它们的定向边组成。任务将按照DAG布局执行。在MapReduce的情况下，DAG只包含两个顶点，一个顶点用于*map*任务，另一个顶点用于*reduce*任务。边从*map*顶点指向*reduce*顶点。内存中的数据处理加上其基于DAG的数据处理引擎使得Spark非常高效。在Spark的情况下，任务的DAG可以非常复杂。幸运的是，Spark附带了一些工具，可以提供任何正在运行的Spark作业的DAG的优秀可视化。在一个词频计数示例中，Spark的Scala代码将类似于以下代码片段。这个编程方面的细节将在接下来的章节中介绍：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The web application that comes with Spark is capable of monitoring the workers
    and applications. The DAG of the preceding Spark job generated on the fly will
    look like *Figure 2*, as shown here:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Spark附带的一个Web应用程序能够监控工作节点和应用程序。前面Spark作业动态生成的DAG（有向无环图）将看起来像*图2*，如下所示：
- en: '![Understanding Apache Spark](img/image_01_003.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![理解Apache Spark](img/image_01_003.jpg)'
- en: Figure 2
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图2
- en: The Spark programming paradigm is very powerful and exposes a uniform programming
    model supporting the application development in multiple programming languages.
    Spark supports programming in Scala, Java, Python, and R even though there is
    no functional parity across all the programming languages supported. Apart from
    writing Spark applications in these programming languages, Spark has an interactive
    shell with **Read, Evaluate, Print, and Loop** (**REPL**) capabilities for the
    programming languages Scala, Python, and R. At this moment, there is no REPL support
    for Java in Spark. The Spark REPL is a very versatile tool that can be used to
    try and test Spark application code in an interactive fashion. The Spark REPL
    enables easy prototyping, debugging, and much more.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 编程范式非常强大，并公开了一个统一的编程模型，支持在多种编程语言中的应用程序开发。尽管不是所有支持的编程语言都具有功能等价性，Spark 支持使用
    Scala、Java、Python 和 R 进行编程。除了用这些编程语言编写 Spark 应用程序之外，Spark 还有一个具有 **读取、评估、打印和循环**（**REPL**）功能的交互式
    shell，适用于 Scala、Python 和 R 编程语言。目前，Spark 中没有对 Java 的 REPL 支持。Spark REPL 是一个非常通用的工具，可以用来以交互式方式尝试和测试
    Spark 应用程序代码。Spark REPL 使原型设计、调试以及更多操作变得简单。
- en: 'In addition to the core data processing engine, Spark comes with a powerful
    stack of domain specific libraries that use the core Spark libraries and provide
    various functionalities useful for various big data processing needs. The following
    table lists the supported libraries:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 除了核心数据处理引擎之外，Spark 还附带了一组强大的特定领域库，这些库使用核心 Spark 库并提供各种功能，这些功能对各种大数据处理需求非常有用。以下表格列出了支持的库：
- en: '| **Library** | **Use** | **Supported Languages** |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **库** | **用途** | **支持的语言** |'
- en: '| Spark SQL | Enables the use of SQL statements or DataFrame API inside Spark
    applications | Scala, Java, Python, and R |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Spark SQL | 允许在 Spark 应用程序中使用 SQL 语句或 DataFrame API | Scala, Java, Python
    和 R |'
- en: '| Spark Streaming | Enables processing of live data streams | Scala, Java,
    and Python |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Spark Streaming | 允许处理实时数据流 | Scala, Java 和 Python |'
- en: '| Spark MLlib | Enables development of machine learning applications | Scala,
    Java, Python, and R |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Spark MLlib | 允许开发机器学习应用程序 | Scala, Java, Python 和 R |'
- en: '| Spark GraphX | Enables graph processing and supports a growing library of
    graph algorithms | Scala |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Spark GraphX | 允许图处理并支持不断增长的图算法库 | Scala |'
- en: Spark can be deployed on a variety of platforms. Spark runs on the **operating
    systems** (**OS**) Windows and UNIX (such as Linux and Mac OS). Spark can be deployed
    in a standalone mode on a single node having a supported OS. Spark can also be
    deployed in cluster node on Hadoop YARN as well as Apache Mesos. Spark can be
    deployed in the Amazon EC2 cloud as well. Spark can access data from a wide variety
    of data stores, and some of the most popular ones include HDFS, Apache Cassandra,
    Hbase, Hive, and so on. Apart from the previously listed data stores, if there
    is a driver or connector program available, Spark can access data from pretty
    much any data source.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 可以部署在各种平台上。Spark 在操作系统 **操作系统**（**OS**） Windows 和 UNIX（例如 Linux 和 Mac
    OS）上运行。Spark 可以在具有支持操作系统的单个节点上以独立模式部署。Spark 还可以在 Hadoop YARN 以及 Apache Mesos 的集群节点上部署。Spark
    还可以部署在 Amazon EC2 云上。Spark 可以从各种数据存储中访问数据，其中一些最受欢迎的包括 HDFS、Apache Cassandra、Hbase、Hive
    等。除了之前列出的数据存储之外，如果有一个驱动程序或连接器程序可用，Spark 可以从几乎任何数据源访问数据。
- en: Tip
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: All the examples used in this book are developed, tested, and run on a Mac OS
    X Version 10.9.5 computer. The same instructions are applicable for all the other
    platforms except Windows. In Windows, corresponding to all the UNIX commands,
    there is a file with a `.cmd` extension and it has to be used. For example, for
    `spark-shell` in UNIX, there is a `spark-shell.cmd` in Windows. The program behavior
    and results should be the same across all the supported OS.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中所使用的所有示例都是在 Mac OS X 版本 10.9.5 的计算机上开发、测试和运行的。对于所有其他平台（除了 Windows）都适用相同的说明。在
    Windows 上，对应于所有 UNIX 命令，有一个具有 `.cmd` 扩展名的文件，并且必须使用它。例如，对于 UNIX 中的 `spark-shell`，Windows
    中有一个 `spark-shell.cmd`。程序行为和结果应跨所有支持的操作系统保持一致。
- en: 'In any distributed application, it is common to have a driver program that
    controls the execution and there will be one or more worker nodes. The driver
    program allocates the tasks to the appropriate workers. This is the same even
    if Spark is running in standalone mode. In the case of a Spark application, its
    **SparkContext** object is the driver program and it communicates with the appropriate
    cluster manager to run the tasks. The Spark master, which is part of the Spark
    core library, the Mesos master, and the Hadoop YARN Resource Manager, are some
    of the cluster managers that Spark supports. In the case of a Hadoop YARN deployment
    of Spark, the Spark driver program runs inside the Hadoop YARN application master
    process or the Spark driver program runs as a client to the Hadoop YARN. *Figure
    3* describes the standalone deployment of Spark:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何分布式应用程序中，通常会有一个控制执行的驱动程序程序，并且将有一个或多个工作节点。驱动程序程序将任务分配给适当的工作节点。即使Spark以独立模式运行，也是如此。在Spark应用程序的情况下，其**SparkContext**对象是驱动程序程序，它与适当的集群管理器通信以运行任务。Spark核心库的一部分Spark
    master、Mesos master和Hadoop YARN资源管理器是Spark支持的集群管理器之一。在Hadoop YARN部署Spark的情况下，Spark驱动程序程序在Hadoop
    YARN应用程序主进程中运行，或者Spark驱动程序程序作为Hadoop YARN的客户端运行。*图3*描述了Spark的独立部署：
- en: '![Understanding Apache Spark](img/image_01_006.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![理解Apache Spark](img/image_01_006.jpg)'
- en: Figure 3
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图3
- en: 'In the Mesos deployment mode of Spark, the cluster manager will be the **Mesos
    Master**. *Figure 4* describes the Mesos deployment of Spark:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark的Mesos部署模式下，集群管理器将是**Mesos Master**。*图4*描述了Spark的Mesos部署：
- en: '![Understanding Apache Spark](img/image_01_008.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![理解Apache Spark](img/image_01_008.jpg)'
- en: Figure 4
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图4
- en: 'In the Hadoop YARN deployment mode of Spark, the cluster manager will be the
    Hadoop Resource Manager and its address will be picked up from the Hadoop configuration.
    In other words, when submitting the Spark jobs, there is no need to give an explicit
    master URL and it will pick up the details of the cluster manager from the Hadoop
    configuration. *Figure 5* describes the Hadoop YARN deployment of Spark:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark的Hadoop YARN部署模式下，集群管理器将是Hadoop资源管理器，其地址将从Hadoop配置中获取。换句话说，在提交Spark作业时，不需要提供显式的master
    URL，它将从Hadoop配置中获取集群管理器的详细信息。*图5*描述了Spark的Hadoop YARN部署：
- en: '![Understanding Apache Spark](img/image_01_010.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![理解Apache Spark](img/image_01_010.jpg)'
- en: Figure 5
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图5
- en: Spark runs in the cloud too. In the case of the deployment of Spark on Amazon
    EC2, apart from accessing the data from the regular supported data sources, Spark
    can also access data from Amazon S3, which is the online data storage service
    from Amazon.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Spark也可以在云中运行。在Spark部署到Amazon EC2的情况下，除了从常规支持的数据源访问数据外，Spark还可以访问来自Amazon的在线数据存储服务Amazon
    S3的数据。
- en: Installing Spark on your machines
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在您的机器上安装Spark
- en: Spark supports application development in Scala, Java, Python, and R. In this
    book, Scala, Python, and R, are used. Here is the reason behind the choice of
    the languages for the examples in this book. The Spark interactive shell, or REPL,
    allows the user to execute programs on the fly just like entering OS commands
    on a terminal prompt and it is available only for the languages Scala, Python
    and R. REPL is the best way to try out Spark code before putting them together
    in a file and running them as applications. REPL helps even the experienced programmer
    to try and test the code and thus facilitates fast prototyping. So, especially
    for beginners, using REPL is the best way to get started with Spark.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Spark支持使用Scala、Java、Python和R进行应用程序开发。在这本书中，使用了Scala、Python和R。以下是选择这些语言作为本书示例原因的说明。Spark交互式shell或REPL允许用户即时执行程序，就像在终端提示符下输入操作系统命令一样，并且它仅适用于Scala、Python和R语言。REPL是在将代码组合到文件中并作为应用程序运行之前尝试和测试Spark代码的最佳方式。REPL甚至可以帮助经验丰富的程序员尝试和测试代码，从而促进快速原型设计。因此，特别是对于初学者来说，使用REPL是开始使用Spark的最佳方式。
- en: As a pre-requisite to Spark installation and to do Spark programming in Python
    and R, both Python and R are to be installed prior to the installation of Spark.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装Spark和用Python和R进行Spark编程之前，需要先安装Python和R。
- en: Python installation
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python安装
- en: 'Visit [https://www.python.org](https://www.python.org/) for downloading and
    installing Python for your computer. Once the installation is complete, make sure
    that the required binaries are in the OS search path and the Python interactive
    shell is coming up properly. The shell should display some content similar to
    the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 [https://www.python.org](https://www.python.org/) 下载并安装适用于您的计算机的Python。安装完成后，请确保所需的二进制文件在操作系统搜索路径中，并且Python交互式shell能够正常启动。shell应显示类似以下内容：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For charting and plotting, the `matplotlib` library is being used.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图表和绘图，正在使用`matplotlib`库。
- en: Note
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Python version 3.5.0 is used as a version of choice for Python. Even though
    Spark supports programming in Python version 2.7, as a forward looking practice,
    the latest and most stable version of Python available is used. Moreover, most
    of the important libraries are getting ported to Python version 3.x as well.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python版本3.5.0作为首选的Python版本。尽管Spark支持使用Python 2.7进行编程，但作为前瞻性的实践，使用的是最新且最稳定的Python版本。此外，大多数重要的库也正在移植到Python
    3.x版本。
- en: 'Visit [http://matplotlib.org](http://matplotlib.org/) for downloading and installing
    the library. To make sure that the library is installed properly and that charts
    and plots are getting displayed properly, visit the [http://matplotlib.org/examples/index.html](http://matplotlib.org/examples/index.html)
    page to pick up some example code and see that your computer has all the required
    resources and components for charting and plotting. While trying to run some of
    these charting and plotting samples, in the context of the import of the libraries
    in Python code, there is a possibility that it may complain about the missing
    locale. In that case, set the following environment variables in the appropriate
    user profile to get rid of the error messages:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 [http://matplotlib.org](http://matplotlib.org/) 下载并安装库。为确保库已正确安装并且图表和绘图能够正确显示，请访问
    [http://matplotlib.org/examples/index.html](http://matplotlib.org/examples/index.html)
    页面，获取一些示例代码，并检查您的计算机是否具备图表和绘图所需的全部资源和组件。在尝试运行这些图表和绘图示例时，如果在Python代码中导入库的上下文中出现缺少区域设置的错误，请在该用户的适当配置文件中设置以下环境变量以消除错误信息：
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: R installation
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: R安装
- en: 'Visit [https://www.r-project.org](https://www.r-project.org/) for downloading
    and installing R for your computer. Once the installation is complete, make sure
    that the required binaries are in the OS search path and the R interactive shell
    is coming up properly. The shell should display some content similar to the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 [https://www.r-project.org](https://www.r-project.org/) 下载并安装适用于您的计算机的R。安装完成后，请确保所需的二进制文件在操作系统搜索路径中，并且R交互式shell能够正常启动。shell应显示类似以下内容：
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: R version 3.2.2 is the choice for R.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: R版本3.2.2是选择使用的R版本。
- en: Spark installation
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark安装
- en: 'Spark installation can be done in many different ways. The most important pre-requisite
    for Spark installation is that the Java 1.8 JDK is installed in the system and
    the `JAVA_HOME` environment variable is set to point to the Java 1.8 JDK installation
    directory. Visit [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)
    for understanding, choosing, and downloading the right type of installation for
    your computer. Spark version 2.0.0 is the version of choice for following the
    examples given in this book. Anyone who is interested in building and using Spark
    from the source code should visit: [http://spark.apache.org/docs/latest/building-spark.html](http://spark.apache.org/docs/latest/building-spark.html)
    for the instructions. By default, when you build Spark from the source code, it
    will not build the R libraries for Spark. For that, the SparkR libraries have
    to be built and the appropriate profile has to be included while building Spark
    from source code. The following command shows how to include the profile required
    to build the SparkR libraries:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的安装可以通过多种不同的方式进行。Spark安装最重要的先决条件是系统已安装Java 1.8 JDK，并且`JAVA_HOME`环境变量设置为指向Java
    1.8 JDK的安装目录。访问 [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)
    了解、选择和下载适合您计算机的正确类型的安装。本书中给出的示例选择Spark版本2.0.0。任何对从源代码构建和使用Spark感兴趣的人应访问：[http://spark.apache.org/docs/latest/building-spark.html](http://spark.apache.org/docs/latest/building-spark.html)
    获取说明。默认情况下，从源代码构建Spark时，它不会构建Spark的R库。为此，必须构建SparkR库，并在从源代码构建Spark时包含适当的配置文件。以下命令显示了如何包含构建SparkR库所需的配置文件：
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once the Spark installation is complete, define the following environment variables
    in the appropriate user profile:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Spark安装完成，请在适当的用户配置文件中定义以下环境变量：
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If there are multiple versions of Python executables in the system, then it
    is better to explicitly specify the Python executable to be used by Spark in the
    following environment variable setting:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统中存在多个Python可执行版本，那么在以下环境变量设置中明确指定Spark使用的Python可执行版本会更好：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the `$SPARK_HOME/bin/pyspark` script, there is a block of code that determines
    the Python executable to be used by Spark:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在`$SPARK_HOME/bin/pyspark`脚本中，有一段代码用于确定Spark使用的Python可执行版本：
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: So, it is always better to explicitly set the Python executable for Spark, even
    if there is only one version of Python available in the system. This is a safeguard
    to prevent unexpected behavior when an additional version of Python is installed
    in the future.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，始终明确设置Spark的Python可执行版本会更好，即使系统中只有一种Python版本。这是一种预防措施，以防将来安装了额外的Python版本时出现意外行为。
- en: 'Once all the preceding steps are completed successfully, make sure that all
    the Spark shells for the languages Scala, Python, and R are working properly.
    Run the following commands on the OS terminal prompt and make sure that there
    are no errors and that content similar to the following is getting displayed.
    The following set of commands is used to bring up the Scala REPL of Spark:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成所有前面的步骤，请确保Scala、Python和R的所有Spark外壳都能正常工作。在操作系统终端提示符下运行以下命令，并确保没有错误，并且显示的内容类似于以下内容。以下命令集用于启动Spark的Scala
    REPL：
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the preceding display, verify that the JDK version, Scala version, and Spark
    version are correct as per the settings in the computer in which Spark is installed.
    The most important point to verify is that no error messages are displayed.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的显示中，请核实JDK版本、Scala版本和Spark版本是否与Spark安装的计算机中的设置一致。需要验证的最重要的一点是不要显示任何错误信息。
- en: 'The following set of commands is used to bring up the Python REPL of Spark:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令集用于启动Spark的Python REPL：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the preceding display, verify that the Python version, and Spark version
    are correct as per the settings in the computer in which Spark is installed. The
    most important point to verify is that no error messages are displayed.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的显示中，请核实Python版本和Spark版本是否与Spark安装的计算机中的设置一致。需要验证的最重要的一点是不要显示任何错误信息。
- en: 'The following set of commands are used to bring up the R REPL of Spark:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令集用于启动Spark的R REPL：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the preceding display, verify that the R version and Spark version are correct
    as per the settings in the computer in which Spark is installed. The most important
    point to verify is that no error messages are displayed.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的显示中，请核实R版本和Spark版本是否与Spark安装的计算机中的设置一致。需要验证的最重要的一点是不要显示任何错误信息。
- en: 'If all the REPL for Scala, Python, and R are working fine, it is almost certain
    that the Spark installation is good. As a final test, run some of the example
    programs that came with Spark and make sure that they are giving proper results
    close to the results shown below the commands and not throwing any error messages
    in the console. When these example programs are run, apart from the output shown
    below the commands, there will be lot of other messages displayed in the console.
    They are omitted to focus on the results:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Scala、Python和R的所有REPL都工作正常，那么几乎可以肯定Spark安装是好的。作为最后的测试，运行一些Spark附带的一些示例程序，并确保它们给出的结果接近以下命令显示的结果，并且在控制台没有抛出任何错误信息。当运行这些示例程序时，除了命令下面的输出外，控制台还会显示很多其他信息。为了专注于结果，这些信息被省略了：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Development tool installation
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发工具安装
- en: Most of the code that is going to be discussed in this book can be tried and
    tested in the appropriate REPL. But the proper Spark application development is
    not possible without some basic build tools. As a bare minimum requirement, for
    developing and building Spark applications in Scala, the **Scala build tool**
    (**sbt**) is a must. Visit [http://www.scala-sbt.org](http://www.scala-sbt.org/)
    for downloading and installing sbt.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将要讨论的大多数代码都可以在适当的REPL中进行尝试和测试。但是，没有一些基本的构建工具，就无法进行适当的Spark应用程序开发。作为最低限度的要求，在Scala中开发和构建Spark应用程序时，**Scala构建工具**（**sbt**）是必需的。请访问[http://www.scala-sbt.org](http://www.scala-sbt.org/)下载和安装sbt。
- en: Maven is the preferred build tool for building Java applications. This book
    is not talking about Spark application development in Java, but it is good to
    have Maven also installed in the system. Maven will come in handy if Spark is
    to be built from source. Visit [https://maven.apache.org](https://maven.apache.org/)
    for downloading and installing Maven.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Maven是构建Java应用程序的首选构建工具。本书不讨论Java中的Spark应用程序开发，但最好也在系统中安装Maven。如果要从源代码构建Spark，Maven将非常有用。请访问[https://maven.apache.org](https://maven.apache.org/)下载和安装Maven。
- en: There are many **Integrated Development Environments** (**IDEs**) available
    for Scala as well as Java. It is a personal choice, and the developer can choose
    the tool of his/her choice for the language in which he/she is developing Spark
    applications.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Scala和Java，都有许多可用的**集成开发环境**（**IDEs**）。这是一个个人选择的问题，开发者可以根据他/她正在开发的Spark应用程序的语言选择他/她喜欢的工具。
- en: Optional software installation
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可选软件安装
- en: Spark REPL for Scala is a good start to get into the prototyping and testing
    of some small snippets of code. But when there is a need to develop, build, and
    package Spark applications in Scala, it is good to have sbt-based Scala projects
    and develop them using a supported IDE, including but not limited to Eclipse or
    IntelliJ IDEA. Visit the appropriate website for downloading and installing the
    preferred IDE for Scala.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Scala的Spark REPL是一个很好的起点，用于原型设计和测试一些小的代码片段。但是，当需要用Scala开发、构建和打包Spark应用程序时，拥有基于sbt的Scala项目和使用支持的IDE（包括但不限于Eclipse或IntelliJ
    IDEA）进行开发是很好的。请访问相应的网站下载和安装Scala首选的IDE。
- en: Notebook style application development tools are very common these days among
    data analysts and researchers. This is akin to a lab notebook. In a typical lab
    notebook, there will be instructions, detailed descriptions, and steps to follow
    to conduct an experiment. Then the experiments are conducted. Once the experiments
    are completed, there will be results captured in the notebook. If all these constructs
    are combined together and fit into the context of a software program and modeled
    in a lab notebook format, there will be documentation, code, input, and the output
    generated by running the code. This will give a very good effect, especially if
    the programs generate a lot of charts and plots.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本风格的应用程序开发工具在数据分析师和研究人员中非常普遍。这类似于实验室笔记本。在一个典型的实验室笔记本中，会有实验的说明、详细描述和遵循的步骤。然后进行实验。一旦实验完成，笔记本中就会记录结果。如果将这些结构组合在一起，并适应软件程序的环境，并以实验室笔记本的格式进行建模，那么将会有文档、代码、输入以及运行代码生成的输出。这将产生非常好的效果，尤其是如果程序生成大量的图表和绘图。
- en: Tip
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'For those who are not familiar with notebook style application development
    IDEs, there is a very nice article entitled *Interactive Notebooks: Sharing the
    Code* that can be read from [http://www.nature.com/news/interactive-notebooks-sharing-the-code-1.16261](http://www.nature.com/news/interactive-notebooks-sharing-the-code-1.16261).
    As an optional software development IDE for Python, the IPython notebook is described
    in the following section. After the installation, get yourself familiar with the
    tool before getting into serious development with it.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些不熟悉笔记本风格的应用程序开发IDE的用户来说，有一篇非常棒的名为《交互式笔记本：共享代码》的文章，可以在[http://www.nature.com/news/interactive-notebooks-sharing-the-code-1.16261](http://www.nature.com/news/interactive-notebooks-sharing-the-code-1.16261)上阅读。在以下部分中，我们将描述作为Python可选软件开发IDE的IPython笔记本。安装后，在开始严肃的开发之前，先熟悉一下这个工具。
- en: IPython
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IPython
- en: In the case of Spark application development in Python, IPython provides an
    excellent notebook-style development tool, which is a Python language kernel for
    Jupyter. Spark can be integrated with IPython, so that when the Spark REPL for
    Python is invoked, it will start the IPython notebook. Then, create a notebook
    and start writing code in the notebook just like the way commands are given in
    the Spark REPL for Python. Visit [http://ipython.org](http://ipython.org/) to
    download and install the IPython notebook. Once the installation is complete,
    invoke the IPython notebook interface and make sure that some example Python code
    is running fine. Invoke commands from the directory from where the notebooks are
    stored or where the notebooks are to be stored. Here, the IPython notebook is
    started from a temporary directory. When the following commands are invoked, it
    will open up the web interface and from there create a new notebook by clicking
    the New drop-down box and picking up the appropriate Python version.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 进行 Spark 应用程序开发的情况下，IPython 提供了一个优秀的笔记本式开发工具，这是一个用于 Jupyter 的 Python
    语言内核。Spark 可以与 IPython 集成，因此当调用 Python 的 Spark REPL 时，它将启动 IPython 笔记本。然后，创建一个笔记本并开始像在
    Spark REPL 中给出命令一样在笔记本中编写代码。访问 [http://ipython.org](http://ipython.org/) 下载并安装
    IPython 笔记本。安装完成后，调用 IPython 笔记本界面并确保一些示例 Python 代码运行良好。从存储笔记本的目录或笔记本将要存储的目录调用命令。在这里，IPython
    笔记本是从临时目录启动的。当调用以下命令时，它将打开网络界面，并从那里通过点击下拉菜单中的“新建”并选择适当的 Python 版本来创建一个新的笔记本。
- en: 'The following screenshot shows how to combine a markdown style documentation,
    a Python program, and the generated output together in an IPython notebook:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了如何在 IPython 笔记本中将 Markdown 风格的文档、Python 程序和生成的输出组合在一起：
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![IPython](img/image_01_011.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![IPython](img/image_01_011.jpg)'
- en: Figure 6
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图6
- en: '*Figure 6* shows how the IPython notebook can be used to write simple Python
    programs. The IPython notebook can be configured as a shell of choice for Spark,
    and when the Spark REPL for Python is invoked, it will start up the IPython notebook
    and Spark application development can be done using IPython notebook. To achieve
    that, define the following environment variables in the appropriate user profile:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6* 展示了如何使用 IPython 笔记本来编写简单的 Python 程序。IPython 笔记本可以被配置为 Spark 的首选外壳，当调用
    Python 的 Spark REPL 时，它将启动 IPython 笔记本，并可以使用 IPython 笔记本来进行 Spark 应用程序的开发。为了实现这一点，需要在适当的用户配置文件中定义以下环境变量：'
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, instead of invoking the IPython notebook from the command prompt, invoke
    the Spark REPL for Python. Just like what has been done before, create a new IPython
    notebook and start writing Spark code in Python:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，不再从命令提示符调用 IPython 笔记本，而是调用 Python 的 Spark REPL。就像之前所做的那样，创建一个新的 IPython
    笔记本并开始用 Python 编写 Spark 代码：
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Take a look at the following screenshot:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下截图：
- en: '![IPython](img/image_01_012.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![IPython](img/image_01_012.jpg)'
- en: Figure 7
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图7
- en: Tip
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: In the standard Spark REPL for any language, it is possible to refer the files
    located in the local filesystem with their relative path. When the IPython notebook
    is being used, local files are to be referred with their full path.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何语言的标准 Spark REPL 中，可以使用相对路径引用位于本地文件系统中的文件。当使用 IPython 笔记本时，本地文件需要使用它们的完整路径来引用。
- en: RStudio
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RStudio
- en: 'Among the R user community, the preferred IDE for R is the RStudio. RStudio
    can be used to develop Spark applications in R as well. Visit [https://www.rstudio.com](https://www.rstudio.com/)
    to download and install RStudio. Once the installation is complete, before running
    any Spark R code, it is mandatory to include the `SparkR` libraries and set some
    variables to make sure that the Spark R programs are running smoothly from RStudio.
    The following code snippet does that:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在 R 用户社区中，首选的 R IDE 是 RStudio。RStudio 也可以用来开发 R 语言的 Spark 应用程序。访问 [https://www.rstudio.com](https://www.rstudio.com/)
    下载并安装 RStudio。安装完成后，在运行任何 Spark R 代码之前，必须包含 `SparkR` 库并设置一些变量以确保 Spark R 程序在 RStudio
    中顺利运行。以下代码片段实现了这一点：
- en: '[PRE15]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the preceding R code, change the `SPARK_HOME_DIR` variable definition to
    point to the directory where Spark is installed. *Figure 8* shows a sample run
    of the Spark R code from RStudio:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的 R 代码中，将 `SPARK_HOME_DIR` 变量定义更改为指向 Spark 安装的目录。*图8* 展示了从 RStudio 运行 Spark
    R 代码的示例运行情况：
- en: '![RStudio](img/image_01_013.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![RStudio](img/image_01_013.jpg)'
- en: Figure 8
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图8
- en: Once all the required software is installed, configured, and working as per
    the details given previously, the stage is set for Spark application development
    in Scala, Python, and R.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装、配置了所有必要的软件，并且它们按照之前给出的详细信息正常工作，就可以为使用Scala、Python和R进行Spark应用程序开发做好准备。
- en: Tip
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The Jupyter notebook supports multiple languages through the custom kernel implementation
    strategy for various languages. There is a native R kernel, namely IRkernel, for
    Jupyter which can be installed as an R package.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter笔记本通过为各种语言定制内核实现策略来支持多种语言。对于Jupyter有一个本地的R内核，即IRkernel，它可以作为一个R包安装。
- en: Apache Zeppelin
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Apache Zeppelin
- en: Apache Zeppelin is another promising project that is getting incubated right
    now. It is a web-based notebook similar to Jupyter but supporting multiple languages,
    shells, and technologies through its interpreter strategy enabling Spark application
    development inherently. Right now it is in its infant stage, but it has a lot
    of potential to become one of the best notebook-based application development
    platforms. Zeppelin has very powerful built-in charting and plotting capabilities
    using the data generated by the programs written in the notebook.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Zeppelin是另一个目前正处于孵化阶段的具有潜力的项目。它是一个基于Web的笔记本，类似于Jupyter，但通过其解释器策略支持多种语言、shell和技术，从而实现Spark应用程序的本能开发。目前它还处于起步阶段，但它有很大的潜力成为最好的基于笔记本的应用程序开发平台之一。Zeppelin使用笔记本中编写的程序生成数据，具有非常强大的内置图表和绘图功能。
- en: Zeppelin is built with high extensibility having the ability to plug in many
    types of interpreters using its Interpreter Framework. End users, just like any
    other notebook-based system, enter various commands in the notebook interface.
    These commands are to be processed by some interpreter to generate the output.
    Unlike many other notebook-style systems, Zeppelin supports a good number of interpreters
    or backends out of the box such as Spark, Spark SQL, Shell, Markdown, and many
    more. In terms of the frontend, again it is a pluggable architecture, namely,
    the **Helium Framework**. The data generated by the backend is displayed by the
    frontend components such as Angular JS. There are various options to display the
    data in tabular format, raw format as generated by the interpreters, charts, and
    plots. Because of the architectural separation of concerns such as the backend,
    the frontend, and the ability to plug in various components, it is a great way
    to choose heterogeneous components for the right job. At the same time, it integrates
    very well to provide a harmonious end-user-friendly data processing ecosystem.
    Even though there is pluggable architecture capability for various components
    in Zeppelin, the visualizations are limited. In other words, there are only a
    few charting and plotting options available out of the box in Zeppelin. Once the
    notebooks are working fine and producing the expected results, typically, the
    notebooks are shared with other people and for that, the notebooks are to be persisted.
    Zeppelin is different again here and it has a highly versatile notebook storage
    system. The notebooks can be persisted to the filesystem, Amazon S3, or Git, and
    other storage targets can be added if required.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 气球（Zeppelin）是通过其解释器框架构建的，具有高度的可扩展性，能够插入多种类型的解释器。最终用户，就像任何其他基于笔记本的系统一样，在笔记本界面中输入各种命令。这些命令将由某些解释器处理以生成输出。与许多其他笔记本式系统不同，Zeppelin自带支持大量解释器或后端，例如Spark、Spark
    SQL、Shell、Markdown等。在前端方面，它同样采用可插拔架构，即**氦框架**。后端生成数据由前端组件，如Angular JS，进行展示。有多种选项可以以表格格式、由解释器生成的原始格式、图表和图形等形式展示数据。由于后端、前端和可插拔各种组件等关注点的架构分离，选择适合特定任务的异构组件是一种很好的方法。同时，它很好地集成了各种组件，提供了一个和谐且用户友好的数据处理生态系统。尽管Zeppelin具有各种组件的可插拔架构能力，但其可视化功能有限。换句话说，在Zeppelin中，开箱即用的图表和绘图选项只有少数。一旦笔记本运行良好并产生预期的结果，通常情况下，笔记本会被与他人共享，为此，笔记本需要被持久化。在这方面，Zeppelin再次表现出其高度灵活的笔记本存储系统。笔记本可以被持久化到文件系统、Amazon
    S3或Git，如果需要，还可以添加其他存储目标。
- en: '**Platform as a Service** (**PaaS**) has been evolving over the last couple
    of years since the massive innovations happening around Cloud as an application
    development and deployment platform. For software developers, there are many PaaS
    platforms available delivered through Cloud, which obviates the need for them
    to have their own application development stack. Databricks has introduced a Cloud-based
    big data platform in which users can have access to a notebook-based Spark application
    development interface in conjunction with micro-cluster infrastructure to which
    the Spark applications can be submitted. There is a community edition as well,
    catering to the needs of a wider development community. The biggest advantage
    of this PaaS platform is that it is a browser-based interface and users can run
    their code against multiple versions of Spark and on different types of clusters.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**平台即服务**（**PaaS**）在过去几年中随着云作为应用开发和部署平台的大量创新而不断发展。对于软件开发者来说，通过云提供了许多 PaaS 平台，这消除了他们需要拥有自己的应用开发栈的需求。Databricks
    引入了一个基于云的大数据平台，用户可以通过基于笔记本的 Spark 应用开发界面访问该平台，同时结合微集群基础设施，Spark 应用可以提交到该基础设施。还有一个社区版，满足更广泛的开发社区的需求。这个
    PaaS 平台最大的优势是它是一个基于浏览器的界面，用户可以在多个版本的 Spark 和不同类型的集群上运行他们的代码。'
- en: References
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'For more information please refer to the following links:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多信息，请参阅以下链接：
- en: '[http://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf](http://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf](http://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf)'
- en: '[http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf](http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf](http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)'
- en: '[https://www.cs.berkeley.edu/~alig/papers/mesos.pdf](https://www.cs.berkeley.edu/~alig/papers/mesos.pdf)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.cs.berkeley.edu/~alig/papers/mesos.pdf](https://www.cs.berkeley.edu/~alig/papers/mesos.pdf)'
- en: '[http://spark.apache.org/](http://spark.apache.org/)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/](http://spark.apache.org/)'
- en: '[https://jupyter.org/](https://jupyter.org/)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://jupyter.org/](https://jupyter.org/)'
- en: '[https://github.com/IRkernel/IRkernel](https://github.com/IRkernel/IRkernel)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/IRkernel/IRkernel](https://github.com/IRkernel/IRkernel)'
- en: '[https://zeppelin.incubator.apache.org/](https://zeppelin.incubator.apache.org/)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://zeppelin.incubator.apache.org/](https://zeppelin.incubator.apache.org/)'
- en: '[https://community.cloud.databricks.com/](https://community.cloud.databricks.com/)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://community.cloud.databricks.com/](https://community.cloud.databricks.com/)'
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Spark is a very powerful data processing platform supporting a uniform programming
    model. It supports application development in Scala, Java, Python, and R, providing
    a stack of highly interoperable libraries used for various types of data processing
    needs, and a plethora of third-party libraries that make use of the Spark ecosystem
    covering various other data processing use cases. This chapter gave a brief introduction
    to Spark and setting up the development environment for the Spark application
    development that is going to be covered in forthcoming chapters of the book.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是一个非常强大的数据处理平台，支持统一的编程模型。它支持使用 Scala、Java、Python 和 R 进行应用程序开发，提供一系列高度互操作的库，用于满足各种类型的数据处理需求，以及大量利用
    Spark 生态系统并覆盖各种其他数据处理用例的第三方库。本章简要介绍了 Spark 以及为 Spark 应用开发设置的开发环境，这些内容将在本书后续章节中介绍。
- en: The next chapter is going to discuss the Spark programming model, the basic
    abstractions and terminologies, Spark transformations, and Spark actions, in conjunction
    with real-world use cases.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将讨论 Spark 编程模型、基本抽象和术语、Spark 转换和 Spark 动作，并结合实际应用案例。
