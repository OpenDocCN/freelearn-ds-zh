["```py\npip install pyspark\n```", "```py\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n```", "```py\nimport pandas as pd\nfrom datetime import datetime, date\nfrom pyspark.sql import Row\ndata_df = spark.createDataFrame([\n    Row(col_1=100, col_2=200., col_3='string_test_1', col_4=date(2023, 1, 1), col_5=datetime(2023, 1, 1, 12, 0)),\n    Row(col_1=200, col_2=300., col_3='string_test_2', col_4=date(2023, 2, 1), col_5=datetime(2023, 1, 2, 12, 0)),\n    Row(col_1=400, col_2=500., col_3='string_test_3', col_4=date(2023, 3, 1), col_5=datetime(2023, 1, 3, 12, 0))\n])\n```", "```py\nDataFrame[col_1: bigint, col_2: double, col_3: string, col_4: date, col_5: timestamp]\n```", "```py\nimport pandas as pd\nfrom datetime import datetime, date\nfrom pyspark.sql import Row\ndata_df = spark.createDataFrame([\n    Row(col_1=100, col_2=200., col_3='string_test_1', col_4=date(2023, 1, 1), col_5=datetime(2023, 1, 1, 12, 0)),\n    Row(col_1=200, col_2=300., col_3='string_test_2', col_4=date(2023, 2, 1), col_5=datetime(2023, 1, 2, 12, 0)),\n    Row(col_1=400, col_2=500., col_3='string_test_3', col_4=date(2023, 3, 1), col_5=datetime(2023, 1, 3, 12, 0))\n], schema=' col_1 long, col_2 double, col_3 string, col_4 date, col_5 timestamp')\n```", "```py\ndata_df\nDataFrame[col_1: bigint, col_2: double, col_3: string, col_4: date, col_5: timestamp]\n```", "```py\nfrom datetime import datetime, date\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nrdd = spark.sparkContext.parallelize([\n    (100, 200., 'string_test_1', date(2023, 1, 1), datetime(2023, 1, 1, 12, 0)),\n    (200, 300., 'string_test_2', date(2023, 2, 1), datetime(2023, 1, 2, 12, 0)),\n    (300, 400., 'string_test_3', date(2023, 3, 1), datetime(2023, 1, 3, 12, 0))\n])\ndata_df = spark.createDataFrame(rdd, schema=['col_1', 'col_2', 'col_3', 'col_4', 'col_5'])\n```", "```py\nDataFrame[col_1: bigint, col_2: double, col_3: string, col_4: date, col_5: timestamp]\n```", "```py\nimport pandas as pd\nfrom datetime import datetime, date\nfrom pyspark.sql import Row\nrdd = spark.sparkContext.parallelize([\n    (100, 200., 'string_test_1', date(2023, 1, 1), datetime(2023, 1, 1, 12, 0)),\n    (200, 300., 'string_test_2', date(2023, 2, 1), datetime(2023, 1, 2, 12, 0)),\n    (300, 400., 'string_test_3', date(2023, 3, 1), datetime(2023, 1, 3, 12, 0))\n])\ndata_df = spark.createDataFrame(rdd, schema=['col_1', 'col_2', 'col_3', 'col_4', 'col_5'])\n```", "```py\nDataFrame[col_1: bigint, col_2: double, col_3: string, col_4: date, col_5: timestamp]\n```", "```py\ndata_df.show()\n```", "```py\n+-----+-----+-------------+----------+-------------------+\n|col_1|col_2|   col_3     |  col_4   |       col_5       |\n+-----+-----+-------------+----------+-------------------+\n|  100|200.0|string_test_1|2023-01-01|2023-01-01 12:00:00|\n|  200|300.0|string_test_2|2023-02-01|2023-01-02 12:00:00|\n|  300|400.0|string_test_3|2023-03-01|2023-01-03 12:00:00|\n+-----+-----+-------------+----------+-------------------+\n```", "```py\ndata_df.show(2)\n```", "```py\n+-----+-----+-------------+----------+-------------------+\n|col_1|col_2|    col_3    |   col_4  |        col_5      |\n+------+------+-----------+----------+-------------------+\n|  100|200.0|string_test_1|2023-01-01|2023-01-01 12:00:00|\n|  200|300.0|string_test_2|2023-02-01|2023-01-02 12:00:00|\n------+-----+-------------+----------+-------------------+\nonly showing top 2 rows.\n```", "```py\ndata_df.printSchema()\n```", "```py\nroot\n |-- col_1: long (nullable = true)\n |-- col_2: double (nullable = true)\n |-- col_3: string (nullable = true)\n |-- col_4: date (nullable = true)\n |-- col_5: timestamp (nullable = true)\n```", "```py\ndata_df.show(1, vertical=True)\n```", "```py\n-RECORD 0------------------\n col_1   | 100\n col_2   | 200.0\n col_3   | string_test_1\n col_4   | 2023-01-01\n col_5   | 2023-01-01 12:00:00\nonly showing top 1 row\n```", "```py\ndata_df.columns\n```", "```py\n['col_1', 'col_2', 'col_3', 'col_4', 'col_5']\n```", "```py\nShow the summary of the DataFrame\ndata_df.select('col_1', 'col_2', 'col_3').describe().show()\n```", "```py\n+-------+-------+-------+-------------+\n|summary| col_1 | col_2 |    col_3    |\n+-------+-------+-------+-------------+\n|  count|   3   |   3   |            3|\n|   mean| 200.0 | 300.0 |         null|\n| stddev| 100.0 | 100.0 |         null|\n|    min| 100   | 200.0 |string_test_1|\n|    max| 300   | 400.0 |string_test_3|\n+-------+-------+-------+-------------+\n```", "```py\ndata_df.collect()\n```", "```py\n[Row(col_1=100, col_2=200.0, col_3='string_test_1', col_4=datetime.date(2023, 1, 1), col_5=datetime.datetime(2023, 1, 1, 12, 0)),\n Row(col_1=200, col_2=300.0, col_3='string_test_2', col_4=datetime.date(2023, 2, 1), col_5=datetime.datetime(2023, 1, 2, 12, 0)),\n Row(col_1=300, col_2=400.0, col_3='string_test_3', col_4=datetime.date(2023, 3, 1), col_5=datetime.datetime(2023, 1, 3, 12, 0))]\n```", "```py\ndata_df.take(1)\n```", "```py\n[Row(col_1=100, col_2=200.0, col_3='string_test_1', col_4=datetime.date(2023, 1, 1), col_5=datetime.datetime(2023, 1, 1, 12, 0))]\n```", "```py\ndata_df.tail(1)\n```", "```py\n[Row(col_1=300, col_2=400.0, col_3='string_test_3', col_4=datetime.date(2023, 3, 1), col_5=datetime.datetime(2023, 1, 3, 12, 0))]\n```", "```py\ndata_df.head(1)\n```", "```py\n[Row(col_1=100, col_2=200.0, col_3='string_test_1', col_4=datetime.date(2023, 1, 1), col_5=datetime.datetime(2023, 1, 1, 12, 0))]\n```", "```py\ndata_df.count()\n```", "```py\n3\n```", "```py\ndata_df.toPandas()\n```", "```py\nfrom pyspark.sql import Column\ndata_df.select(data_df.col_3).show()\n```", "```py\n+-------------+\n|    col_3    |\n+-------------+\n|string_test_1|\n|string_test_2|\n|string_test_3|\n+-------------+\nThe important thing to note here is that the resulting DataFrame with one column is a new DataFrame. Recalling what we discussed in *Chapter 3*, RDDs are immutable. The underlying data structure for DataFrames is RDDs, therefore, DataFrames are also immutable. This means every time you make a change to a DataFrame, a new DataFrame would be created out of it. You would either have to assign the resultant DataFrame to a new DataFrame or overwrite the original DataFrame.\n```", "```py\ndata_df.select('col_3').show()\ndata_df.select(data_df['col_3']).show()\n```", "```py\nfrom pyspark.sql import functions as F\ndata_df = data_df.withColumn(\"col_6\", F.lit(\"A\"))\ndata_df.show()\n```", "```py\ndata_df = data_df.drop(\"col_5\")\ndata_df.show()\n```", "```py\n+-----+-----+-------------+----------+------+\n|col_1|col_2|    col_3    |  col_4   | col_6|\n+-----+-----+-------------+----------+------+\n|  100|200.0|string_test_1|2023-01-01|     A|\n|  200|300.0|string_test_2|2023-02-01|     A|\n|  300|400.0|string_test_3|2023-03-01|     A|\n+-----+-----+-------------+----------+------+\n```", "```py\ndata_df = data_df.drop(\"col_4\", \"col_5\")\n```", "```py\ndata_df.withColumn(\"col_2\", F.col(\"col_2\") / 100).show()\n```", "```py\n+-----+-----+-------------+----------+------+\n|col_1|col_2|    col_3    |   col_4  | col_6|\n+-----+-----+-------------+----------+------+\n|  100|200.0|string_test_1|2023-01-01|     A|\n|  200|300.0|string_test_2|2023-02-01|     A|\n|  300|400.0|string_test_3|2023-03-01|     A|\n+-----+-----+-------------+----------+------+\n```", "```py\ndata_df = data_df.withColumnRenamed(\"col_3\", \"string_col\")\ndata_df.show()\n```", "```py\n+-----+-----+-------------+----------+------+\n|col_1|col_2|  string_col |   col_4  | col_6|\n+-----+-----+-------------+----------+------+\n|  100|200.0|string_test_1|2023-01-01|   A  |\n|  200|300.0|string_test_2|2023-02-01|   A  |\n|  300|400.0|string_test_3|2023-03-01|   A  |\n+-----+-----+-------------+----------+------+\n```", "```py\ndata_df.select(\"col_6\").distinct().show()\n```", "```py\n+------+\n|col_6 |\n+------+\n|   A  |\n+------+\n```", "```py\ndata_df.select(F.countDistinct(\"col_6\").alias(\"Total_Unique\")).show()\n```", "```py\n+------+\n|col_6 |\n+------+\n|  1   |\n+------+\n```", "```py\nfrom pyspark.sql.functions import upper\ndata_df.withColumn('upper_string_col', upper(data_df.string_col)).show()\n```", "```py\ndata_df.filter(data_df.col_1 == 100).show()\n```", "```py\n+-----+-----+-------------+----------+------+\n|col_1|col_2| string_col  |   col_4  |col_6 |\n+-----+-----+-------------+----------+------+\n|  100|200.0|string_test_1|2023-01-01|   A  |\n+-----+-----+-------------+----------+------+\n```", "```py\ndata_df.filter((data_df.col_1 == 100)\n                  & (data_df.col_6 == 'A')).show()\n```", "```py\n+-----+------+-------------+----------+------+\n|col_1| col_2|  string_col |   col_4  |col_6 |\n+-----+------+-------------+----------+------+\n|  100| 200.0|string_test_1|2023-01-01|   A  |\n+-----+------+-------------+----------+------+\n```", "```py\ndata_df.filter((data_df.col_1 == 100)\n                  | (data_df.col_2 == 300.00)).show()\n```", "```py\n+-----+------+-------------+----------+------+\n|col_1| col_2|  string_col |   col_4  | col_6|\n+-----+------+-------------+----------+------+\n|  100| 200.0|string_test_1|2023-01-01|   A  |\n|  200| 300.0|string_test_2|2023-02-01|   A  |\n+-----+------+-------------+----------+------+\n```", "```py\nlist = [100, 200]\ndata_df.filter(data_df.col_1.isin(list)).show()\n```", "```py\n+-----+-----+-------------+----------+------+\n|col_1|col_2|  string_col |   col_4  |col_6 |\n+-----+-----+-------------+----------+------+\n|  100|200.0|string_test_1|2023-01-01|   A  |\n|  200|300.0|string_test_2|2023-02-01|   A  |\n+-----+-----+-------------+----------+------+\n```", "```py\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import StringType,BooleanType,DateType,IntegerType\ndata_df_2 = data_df.withColumn(\"col_4\",col(\"col_4\").cast(StringType())) \\\n    .withColumn(\"col_1\",col(\"col_1\").cast(IntegerType()))\ndata_df_2.printSchema()\ndata_df.show()\n```", "```py\nroot\n |-- col_1: integer (nullable = true)\n |-- col_2: double (nullable = true)\n |-- string_col: string (nullable = true)\n |-- col_4: string (nullable = true)\n |-- col_6: string (nullable = false)\n+-----+-----+-------------+----------+-----+\n|col_1|col_2|   string_col|     col_4|col_6|\n+-----+-----+-------------+----------+-----+\n|  100|200.0|string_test_1|2023-01-01|    A|\n|  200|300.0|string_test_2|2023-02-01|    A|\n|  300|400.0|string_test_3|2023-03-01|    A|\n+-----+-----+-------------+----------+-----+\n```", "```py\nroot\n |-- col_1: long (nullable = true)\n |-- col_2: double (nullable = true)\n |-- string_col: string (nullable = true)\n |-- col_4: date (nullable = true)\n |-- col_5: timestamp (nullable = true)\n |-- col_6: string (nullable = false)\n```", "```py\ndata_df_3 = data_df_2.selectExpr(\"cast(col_4 as date) col_4\",\n    \"cast(col_1 as long) col_1\")\ndata_df_3.printSchema()\ndata_df_3.show(truncate=False)\n```", "```py\nroot\n |-- col_4: date (nullable = true)\n |-- col_1: long (nullable = true)\n+----------+-----+\n|  col_4   |col_1|\n+----------+-----+\n|2023-01-01|  100|\n|2023-02-01|  200|\n|2023-03-01|  300|\n+----------+-----+\n```", "```py\ndata_df_3.createOrReplaceTempView(\"CastExample\")\ndata_df_4 = spark.sql(\"SELECT DOUBLE(col_1), DATE(col_4) from CastExample\")\ndata_df_4.printSchema()\ndata_df_4.show(truncate=False)\n```", "```py\nroot\n |-- col_1: double (nullable = true)\n |-- col_4: date (nullable = true)\n+-----+----------+\n|col_1|  col_4   |\n+-----+----------+\n|100.0|2023-01-01|\n|200.0|2023-02-01|\n|300.0|2023-03-01|\n+-----+----------+\n```", "```py\nsalary_data = [(\"John\", \"Field-eng\", 3500),\n    (\"Michael\", \"Field-eng\", 4500),\n    (\"Robert\", None, 4000),\n    (\"Maria\", \"Finance\", 3500),\n    (\"John\", \"Sales\", 3000),\n    (\"Kelly\", \"Finance\", 3500),\n    (\"Kate\", \"Finance\", 3000),\n    (\"Martin\", None, 3500),\n    (\"Kiran\", \"Sales\", 2200),\n    (\"Michael\", \"Field-eng\", 4500)\n  ]\ncolumns= [\"Employee\", \"Department\", \"Salary\"]\nsalary_data = spark.createDataFrame(data = salary_data, schema = columns)\nsalary_data.printSchema()\nsalary_data.show()\n```", "```py\nroot\n |-- Employee: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- Salary: long (nullable = true)\n+--------+----------+------+\n|Employee|Department|Salary|\n+--------+----------+------+\n|    John| Field-eng| 3500 |\n| Michael| Field-eng| 4500 |\n|  Robert|      null| 4000 |\n|   Maria|   Finance| 3500 |\n|    John|     Sales| 3000 |\n|   Kelly|   Finance| 3500 |\n|    Kate|   Finance| 3000 |\n|  Martin|      null| 3500 |\n|   Kiran|     Sales| 2200 |\n| Michael| Field-eng| 4500 |\n+--------+----------+------+\n```", "```py\nsalary_data.dropna().show()\n```", "```py\n+--------+----------+------+\n|Employee|Department|Salary|\n+--------+----------+------+\n| John   | Field-eng| 3500 |\n| Michael| Field-eng| 4500 |\n| Maria  |   Finance| 3500 |\n| John   |     Sales| 3000 |\n| Kelly  |   Finance| 3500 |\n| Kate   |   Finance| 3000 |\n| Kiran  |     Sales| 2200 |\n| Michael| Field-eng| 4500 |\n+--------+----------+------+\n```", "```py\nnew_salary_data = salary_data.dropDuplicates().show()\n```", "```py\n+--------+----------+------+\n|Employee|Department|Salary|\n+--------+----------+------+\n|    John| Field-eng|  3500|\n| Michael| Field-eng|  4500|\n|   Maria|   Finance|  3500|\n|    John|     Sales|  3000|\n|   Kelly|   Finance|  3500|\n|    Kate|   Finance|  3000|\n|   Kiran|     Sales|  2200|\n+--------+----------+------+\n```", "```py\nfrom pyspark.sql.functions import countDistinct, avg\nsalary_data.select(avg('Salary')).show()\n```", "```py\n+-----------+\n|avg(Salary)|\n+-----------+\n|     3520.0|\n+-----------+\n```", "```py\nsalary_data.agg({'Salary':'count'}).show()\n```", "```py\n+-------------+\n|count(Salary)|\n+-------------+\n|           10|\n+-------------+\n```", "```py\nsalary_data.select(countDistinct(\"Salary\").alias(\"Distinct Salary\")).show()\n```", "```py\n+---------------+\n|Distinct Salary|\n+---------------+\n|              5|\n+---------------+\n```", "```py\nsalary_data.agg({'Salary':'max'}).show()\n```", "```py\n+-----------+\n|max(Salary)|\n+-----------+\n|       4500|\n+-----------+\n```", "```py\nsalary_data.agg({'Salary':'sum'}).show()\n```", "```py\n+-----------+\n|sum(Salary)|\n+-----------+\n|      35200|\n+-----------+\n```", "```py\nsalary_data.orderBy(\"Salary\").show()\n```", "```py\n+--------+----------+------+\n|Employee|Department|Salary|\n+--------+----------+------+\n| Kiran  | Sales    | 2200 |\n| Kate   | Finance  | 3000 |\n| John   | Sales    | 3000 |\n| John   | Field-eng| 3500 |\n| Martin | null     | 3500 |\n| Kelly  | Finance  | 3500 |\n| Maria  | Finance  | 3500 |\n| Robert | null     | 4000 |\n| Michael| Field-eng| 4500 |\n| Michael| Field-eng| 4500 |\n+--------+----------+------+\n```", "```py\nsalary_data.orderBy(salary_data[\"Salary\"].desc()).show()\n```", "```py\n+--------+----------+------+\n|Employee|Department|Salary|\n+--------+----------+------+\n| Michael| Field-eng| 4500 |\n| Michael| Field-eng| 4500 |\n| Robert | null     | 4000 |\n| Martin | null     | 3500 |\n| Kelly  | Finance  | 3500 |\n| Maria  | Finance  | 3500 |\n| John   | Field-eng| 3500 |\n| John   | Sales    | 3000 |\n| Kate   | Finance  | 3000 |\n| Kiran  | Sales    | 2200 |\n+--------+----------+------+\n```"]