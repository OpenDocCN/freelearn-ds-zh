<html><head></head><body><div><div><h1 id="_idParaDest-170"><a id="_idTextAnchor173"/>Chapter 10: Putting Everything Together: Designing Your Chatbot with spaCy</h1>
			<p>In this chapter, you will use everything you have learned so far to design a chatbot. You will perform entity extraction, intent recognition, and context handling. You will use different ways of syntactic and semantic parsing, entity extraction, and text classification. </p>
			<p>First, you'll explore the dataset we'll use to collect linguistic information about the utterances within it. Then, you'll perform entity extraction by combining the spaCy <code>Matcher</code> class. After that, you'll perform intent recognition with two different techniques: a pattern-based method and statistical text classification with TensorFlow and Keras. You'll train a character-level LSTM to classify the utterance intents. </p>
			<p>The final section is a section dedicated to sentence- and dialog-level semantics. You'll take a deep dive into semantic subjects such as <strong class="bold">anaphora resolution</strong>, <strong class="bold">grammatical question types</strong>, and <strong class="bold">differentiating subjects from objects</strong>. </p>
			<p>By the end of this chapter, you'll be ready to design a real chatbot <strong class="bold">natural language understanding</strong> (<strong class="bold">NLU</strong>) pipeline. You will bring together what you learned in all previous chapters – linguistically and statistically – by combining several spaCy pipeline components such as <strong class="bold">NER</strong>, a <strong class="bold">dependency parser</strong>, and a <strong class="bold">POS tagger</strong>.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Introduction to conversational AI</li>
				<li>Entity extraction</li>
				<li>Intent recognition</li>
			</ul>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor174"/>Technical requirements </h1>
			<p>In this chapter, we'll be using NumPy, TensorFlow, and scikit-learn along with spaCy. You can install these libraries via <code>pip</code> using the following commands:</p>
			<pre>pip install numpy
pip install tensorflow
pip install scikit-learn</pre>
			<p>You can find the chapter code and data at the book's GitHub repository: <a href="https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter10">https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter10</a>. </p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor175"/>Introduction to conversational AI</h1>
			<p>We welcome you to our last and very exciting chapter, where you'll be designing a chatbot NLU pipeline<a id="_idIndexMarker670"/> with spaCy and TensorFlow. In this chapter, you'll learn the NLU techniques for extracting meaning from multiturn chatbot-user interactions. By learning and applying these techniques, you'll take a step into <strong class="bold">conversational AI development</strong>.  </p>
			<p>Before diving into the technical details, there's one fundamental question: what is a chatbot? Where can we find one? What exactly does conversational AI mean?</p>
			<p><strong class="bold">Conversational artificial intelligence </strong>(<strong class="bold">conversational AI</strong>) is a field of machine learning that aims to create<a id="_idIndexMarker671"/> technology that enables users to have text- or speech-based interactions with machines. Chatbots, virtual assistants, and voice assistants are typical conversational AI products.</p>
			<p>A <strong class="bold">chatbot</strong> is a software application that is designed to make conversations with humans in chat applications. Chatbots are<a id="_idIndexMarker672"/> popular in a wide variety of commercial areas including HR, marketing and sales, banking, and healthcare, as well as in personal, non-commercial areas such as small talk. Many commercial companies, such as Sephora (Sephora owns two chatbots – a virtual make-up artist chatbot on Facebook messenger platform and a customer service chatbot again on Facebook messenger), IKEA (IKEA have a customer service chatbot called Anna), AccuWeather, and many more, own customer service and FAQ chatbots. </p>
			<p>Instant messaging services such as Facebook Messenger and Telegram provide interfaces to developers for connecting their bots. These platforms provide detailed guidelines for developers as well, such as the Facebook Messenger API documentation: (<a href="https://developers.facebook.com/docs/messenger-platform/getting-started/quick-start/">https://developers.facebook.com/docs/messenger-platform/getting-started/quick-start/</a>) or the Telegram bot API <a id="_idIndexMarker673"/>documentation: (<a href="https://core.telegram.org/bots">https://core.telegram.org/bots</a>).</p>
			<p>A <strong class="bold">virtual assistant</strong> is also a software<a id="_idIndexMarker674"/> agent that performs some tasks upon user request or question. A well-known example is <strong class="bold">Amazon Alexa</strong>. Alexa is a voice-based virtual assistant and<a id="_idIndexMarker675"/> can perform many tasks, including playing music, setting alarms, reading audiobooks, playing podcasts, and giving real-time information for weather, traffic, sports, and so on. Alexa Home can control connected smart home devices and perform a variety of tasks, including switching the lights on and off, controlling the garage door, and so on.</p>
			<p>Other well-known<a id="_idIndexMarker676"/> examples are Google Assistant and Siri. Siri is integrated into a number of Apple products, including iPhone, iPad, iPod, and macOS. On iPhone, Siri can make calls, answer calls, and send and receive text messages as well as WhatsApp messages. Google Assistant also can perform a wide variety of tasks, such as providing real-time flight, weather, and traffic information; sending and receiving text messages; setting alarms; providing device battery information; checking your email inbox; integrating with smart home devices; and so on. Google Assistant is available on Google Maps, Google Search, and standalone Android and iOS applications.</p>
			<p>Here is a list of the most popular and well-known virtual assistants to give you some more ideas of what's out there:</p>
			<ul>
				<li>Amazon Alexa</li>
				<li>AllGenie from Alibaba Group</li>
				<li>Bixby from Samsung</li>
				<li>Celia from Huawei</li>
				<li>Duer from Baidu</li>
				<li>Google Assistant</li>
				<li>Microsoft Cortana</li>
				<li>Siri from Apple</li>
				<li>Xiaowei from Tencent</li>
			</ul>
			<p>All of these virtual assistants are<a id="_idIndexMarker677"/> voice-based and are usually invoked with a <strong class="bold">wake word</strong>. A wake word is a special word or phrase that is used to activate a voice assistant. Some examples are <em class="italic">Hey Alexa</em>, <em class="italic">Hey Google</em>, and <em class="italic">Hey Siri</em>, which are the wake words of Amazon Alexa, Google Assistant, and Siri, respectively. If you want to know more about the development details of these products, please refer to the <em class="italic">References</em> section of this chapter.</p>
			<p>Now, we come to the technical details. What are the NLP components of these products? Let's look at these NLP components in detail.</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor176"/>NLP components of conversational AI products</h2>
			<p>A typical voice-based conversational AI <a id="_idIndexMarker678"/>product consists of the following components:</p>
			<ul>
				<li><strong class="bold">Speech-to-text component</strong>: Converts <a id="_idIndexMarker679"/>user speech into text. Input to this component is a WAV/mp3 file and the output is a text file containing the user utterance as a text.</li>
				<li><strong class="bold">Conversational NLU component</strong>: This component performs intent recognition and entity extraction on the user <a id="_idIndexMarker680"/>utterance text. The output is the user intent and a list of entities. Resolving references in the current utterance to the previous utterances is done in this component (please refer to the <em class="italic">Anaphora resolution</em> section).</li>
				<li><strong class="bold">Dialog manager</strong>: Keeps the conversation<a id="_idIndexMarker681"/> memory to make a meaningful and coherent chat. You can think of this component as the dialog memory as this component usually holds a <strong class="bold">dialog state</strong>. The dialog state is the state of the conversation: the entities that have appeared so far, the intents that have appeared so far, and so on. Input to this component is the previous dialog state and the current user parsed with intent and entities. The output of this component is the new dialog state. </li>
				<li><strong class="bold">Answer generator</strong>: Given all the inputs from the previous stages, generates the system's answer to the user<a id="_idIndexMarker682"/> utterance.</li>
				<li><strong class="bold">Text-to-speech</strong>: This component <a id="_idIndexMarker683"/>generates a speech file (WAV or mp3) from the system's answer. </li>
			</ul>
			<p>Each of the components is trained and evaluated separately. For example, the speech-to-text component is trained on an annotated speech corpus (training is done on speech files and the corresponding transcriptions). The NLU component is trained on intent and an entity labeled<a id="_idIndexMarker684"/> corpus (similar to the datasets we used in <em class="italic">Chapters 6, 7, 8,</em> and <em class="italic">9</em>). In this chapter, we'll focus on the NLU component tasks. For text-based products, the first and last components are not necessary and are replaced with email or chat client integration.</p>
			<p>There's another paradigm that is called <strong class="bold">end-to-end spoken language understanding </strong>(<strong class="bold">SLU</strong>). In SLU architectures, the<a id="_idIndexMarker685"/> system is trained end to end, which means that the input to the system is a speech file and the output is the system response. Each approach has pros and cons; you can refer to the <em class="italic">References</em> section for more material.</p>
			<p>As the author of this book, I'm happy to present this chapter to you with my domain experience. I've been working in the conversational AI area for quite some time and tackle challenges of language and speech processing every day for our product. Me and my colleagues are building the world's first driver digital assistant, Chris (<em class="italic">Tips &amp; Tricks: How to talk to Chris – basic voice commands</em>, <a href="https://www.youtube.com/watch?v=Qwnjszu3exY">https://www.youtube.com/watch?v=Qwnjszu3exY</a>). Chris can make calls, answer incoming calls, read and write WhatsApp and text messages, play music, navigate, and make small talk. Here is Chris:</p>
			<div><div><img src="img/Figure_10_1.jpg" alt="Figure 10.1 – In-car voice assistant Chris (this is the product that the author is working on" width="713" height="733"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – In-car voice assistant Chris (this is the product that the author is working on)</p>
			<p>As we see from the <a id="_idIndexMarker686"/>preceding examples, conversational AI has become a hot topic recently. As an NLP professional, it's quite likely that you'll work for a conversational product or work in a related area such as speech recognition, text-to-speech, or question answering. Techniques presented in this chapter such as intent recognition, entity extraction, and anaphora resolution are applicable to a wide set of NLU problems as well. Let's dive into the technical sections. We'll start by exploring the dataset that we'll use throughout this chapter. </p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor177"/>Getting to know the dataset</h2>
			<p>In <em class="italic">Chapters 6</em>, <em class="italic">7</em>, <em class="italic">8</em>, and <em class="italic">9</em>, we worked on <a id="_idIndexMarker687"/>well-known real-world datasets for text classification and entity extraction purposes. In these chapters, we always explored our dataset as the very first task. The main point of data exploration is to understand the nature of the dataset text in order to develop strategies in our algorithms that can tackle this dataset. If we recall from <a href="B16570_06_Final_JM_ePub.xhtml#_idTextAnchor103"><em class="italic">Chapter 6</em></a><em class="italic">, Putting Everything Together: Semantic Parsing with spaCy</em>, the following are the main points we should keep an eye on during our exploration:</p>
			<ul>
				<li>What kind of utterances there are? Are utterances short text or full sentences or long paragraphs or documents? What is the average utterance length?</li>
				<li>What sort of entities does the corpus include? Person names, organization names, geographical locations, street names? Which ones do we want to extract?</li>
				<li>How is punctuation used? Is the text correctly punctuated or is no punctuation used at all? </li>
				<li>How are the grammatical rules followed? Is capitalization correct, and did the users follow the grammatical rules? Are there misspelled words?</li>
			</ul>
			<p>The previous datasets we used consisted of <code>(text, class_label)</code> pairs to be used in text classification tasks or <code>(text, list_of_entities)</code> pairs to be used in entity extraction tasks. In this chapter, we'll tackle a much more complicated task, chatbot design. Hence, the dataset will be more structured and more complicated. </p>
			<p>Chatbot design datasets are usually in JSON format to maintain the dataset structure. Here, structure means the following:</p>
			<ul>
				<li>Keeping the order of user and system utterances</li>
				<li>Marking slots of the user utterances</li>
				<li>Labeling the intent of the user utterances</li>
			</ul>
			<p>Throughout this chapter, we'll use Google Research's <strong class="bold">The Schema-Guided Dialogue</strong> dataset (<strong class="bold">SGD</strong>) (<a href="https://github.com/google-research-datasets/dstc8-schema-guided-dialogue">https://github.com/google-research-datasets/dstc8-schema-guided-dialogue</a>). This dataset consists of annotated user-virtual assistant interactions. The original dataset contains over 20,000 dialog segments in several areas, including restaurant reservations, movie reservations, weather queries, and travel ticket booking. Dialogs include utterances of user and virtual assistant turn by turn. In this chapter, we won't use all of<a id="_idIndexMarker688"/> this massive dataset; instead, we'll use a subset about restaurant reservations. </p>
			<p>Let's get started with downloading the dataset. You can download the dataset from the book's GitHub repository at <a href="https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/data/restaurants.json">https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/data/restaurants.json</a>. Alternatively, you can write the following code:</p>
			<pre><strong class="bold">$ wget https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/data/restaurants.json</strong></pre>
			<p>If you open the file with a text editor and look at the first few lines, you'll see the following:</p>
			<pre>{
  "dialogue_id": "1_00000",
  "turns": [
    {
      "speaker": "USER",
      "utterance": "I am feeling hungry so I would like to find a place to eat.",
      "slots": [],
      "intent": "FindRestaurants"
     },
     {
       "speaker": "SYSTEM",
       "utterance": "Do you have a specific which you want the eating place to be located at?",
        "slots": []
       }</pre>
			<p>First of all, the dataset consists of dialog segments and each dialog segment has a <code>dialogue_id</code> instance. Each dialog segment is an ordered list of turns and each turn belongs to the user or to the system. A <code>turns</code> field is a list of the user/system turns. Each element of the <code>turns</code> list is a turn. One turn consists of a speaker (user or system), the speaker's utterance, a list of slots, and an intent for<a id="_idIndexMarker690"/> the user utterances.</p>
			<p>Here are some example user utterances from the dataset:</p>
			<pre>Hi. I'd like to find a place to eat.
I want some ramen, I'm really craving it. Can you find me an afforadable place in Morgan Hill?
I would like for it to be in San Jose.
Yes, please make a reservation for me.
No, Thanks
Hi i need a help, i am very hungry, I am looking for a restaurant
Yes, on the 7th for four people.
No. Can you change it to 1 pm on the 9th?
Yes. What is the phone number? Can I buy alcohol there? </pre>
			<p>As we see from these example utterances, capital letters and punctuation are used in the user utterances. Users can make typos, such as the word <code>afforadable</code> in the second sentence. There are some grammatical errors as well, such as the wrong usage of a capital letter in the word <code>Thanks</code> of the fifth sentence. Another capitalization mistake occurs in the sixth sentence, where the pronoun <em class="italic">I</em> is written as <code>i</code> twice.</p>
			<p>Also, one utterance can contain multiple sentences. The first utterance starts with a greeting sentence and the last two sentences start with an affirmative or negative answer sentence each. The fourth sentence also starts with a <code>Yes</code>, but not as a standalone sentence; instead it's separated from the second sentence with a comma. </p>
			<p>Intent recognition for multiple sentence utterances is a point we need to pay attention to in general – these types of utterances can contain multiple intents. Also, answer generation for multi-sentence utterances is a bit tricky; sometimes we need to generate only one answer (such as for the second sentence in the preceding code) or sometimes we need to generate an answer per each user sentence (such as for the last sentence in the preceding code).    </p>
			<p>This is a dataset for<a id="_idIndexMarker691"/> restaurant reservations, so naturally it includes some slots in user utterances such as the location, cuisine, time, date, number of people, and so on. Our dataset includes the following slots:</p>
			<pre>city
cuisine
date
phone_number
restaurant_name
street_address
time</pre>
			<p>Here are some example sentences with the preceding slot types and their values:</p>
			<pre>Find me Ethiopian/cuisine cuisine in Berkeley/city.
The phone number is 707-421-0835/phone_number. Your reservation is confirmed.
No, change the time to 7 pm/time and for one person only.
No, change it on next friday/date.</pre>
			<p>Now, we come to the class labels for the intent recognition and the distribution of these class labels. Here's the class labels distribution:</p>
			<pre>552 FindRestaurants
625 ReserveRestaurant
56  NONE</pre>
			<p><code>NONE</code> is a special class label for utterances that indicate the end of a conversation or just saying thank you. This class of utterances is not related to restaurant reservation in general. Utterances that intend to list restaurants and get some information are labeled with the class label <code>FindRestaurants</code>, and utterances that include the intent to make a booking are labeled with <code>ReserveRestaurants</code>. Let's see some example utterances of each class:</p>
			<pre>No, Thanks  NONE
No, thank you very much. NONE
Nothing much. I'm good.  NONE
I am feeling hungry so I would like to find a place to eat. FindRestaurants
Hi i need a help, i am very hungry, I am looking for a restaurant  FindRestaurants
Ok, What is the address? How pricey are they? FindRestaurants
Please can you make the reservation ReserveRestaurant
That's good. Do they serve liquor and what is there number? ReserveRestaurant
Thank you so much for setting that up. ReserveRestaurant</pre>
			<p>We notice that the <a id="_idIndexMarker692"/>follow-up sentences, such as utterances 6, 8, and 9, are marked with the intents <code>FindRestaurants</code> and <code>ReserveRestaurant</code>. These utterances don't contain the intents of finding/reserving directly, but they continue the dialog about finding/reserving a restaurant and still make queries about the restaurant/reservation. Hence, although there are no explicit actions of finding/reserving stated in these utterances, still the intents are to find/reserve a restaurant. </p>
			<p>That's it – we collected enough insights about our dataset using the preliminary work of this section. With these insights, we're ready to build our NLU pipeline. We'll start with extracting the user utterance entities. </p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor178"/>Entity extraction</h1>
			<p>In this section, we'll implement the <a id="_idIndexMarker693"/>first step of our chatbot NLU pipeline and extract entities from the dataset utterances. The following are the entities marked in our dataset:</p>
			<pre>city
date
time
phone_number
cuisine
restaurant_name
street_address</pre>
			<p>To extract the entities, we'll use the spaCy NER model and the spaCy <code>Matcher</code> class. Let's get started by extracting the <code>city</code> entities.</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor179"/>Extracting city entities</h2>
			<p>We'll first extract the <code>city</code> entities. We'll get<a id="_idIndexMarker694"/> started by recalling some information about the spaCy NER model and entity labels from <a href="B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055"><em class="italic">Chapter 3</em></a><em class="italic">, Linguistic Features</em>, and <a href="B16570_06_Final_JM_ePub.xhtml#_idTextAnchor103"><em class="italic">Chapter 6</em></a><em class="italic">, Putting Everything Together: Semantic Parsing with spaCy</em>:  </p>
			<ul>
				<li>First, we recall that the spaCy named entity label for cities and countries is <code>GPE</code>. Let's ask spaCy to explain what <code>GPE</code> label corresponds to once again:<pre>import spacy
nlp = spacy.load("en_core_web_md")
spacy.explain("GPE")
'Countries, cities, states'</pre></li>
				<li>Secondly, we also recall that we can access entities of a <code>Doc</code> object via the <code>ents</code> property. We can find all entities in an utterance that are labeled by the spaCy NER model as follows:<pre>import spacy
nlp = spacy.load("en_core_web_md")
doc = nlp("Can you please confirm that you want to book a table for 2 at 11:30 am at the Bird restaurant in Palo Alto for today")
doc.ents
(2, 11:30 am, Bird, Palo Alto, today)
for ent in doc.ents:
  print(ent.text, ent.label_) 
2 CARDINAL
11:30 am TIME
Bird PRODUCT
Palo Alto GPE
today DATE</pre></li>
			</ul>
			<p>In this code segment, we listed <a id="_idIndexMarker695"/>all named entities of this utterance by calling <code>doc.ents</code>. Then, we examined the entity labels by calling <code>ent.label_</code>. Examining the output, we see that this utterance contains five entities – one cardinal number entity (<code>2</code>), one <code>TIME</code> entity (<code>11:30 am</code>), one <code>PRODUCT</code> entity (<code>Bird</code>, which is not an ideal label for a restaurant), one <code>CITY</code> entity (<code>Palo Alto</code>), and one <code>DATE</code> entity (<code>today</code>). The <code>GPE</code> type entity is what we're looking for; <code>Palo Alto</code> is a city in the US and hence is labeled by the spaCy NER model as <code>GPE</code>.</p>
			<p>The script at <a href="https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/extract_city_ents.py">https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/extract_city_ents.py</a> in the book's GitHub outputs all the utterances that include a city entity together with the city entities. From the output of this script, we can see that the spaCy NER model performs very well on this corpus for <code>GPE</code> entities. We don't need to train the spaCy NER model with our custom data.</p>
			<p>We extracted city entities, and our chatbot knows in which city to look for a restaurant. Now, we'll extract dates and times to allow our chatbot to make a real reservation.</p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor180"/>Extracting date and time entities</h2>
			<p>Extracting <code>DATE</code> and <code>TIME</code> entities is <a id="_idIndexMarker696"/>similar to extracting <code>CITY</code> entities, which we saw in the previous section. We'll again go over the corpus utterances and see how successful the spaCy NER model is at extracting <code>DATE</code> and <code>TIME</code> entities from our corpus.</p>
			<p>Let's see some example utterances from the corpus:</p>
			<pre>import spacy
nlp = spacy.load("en_core_web_md")
sentences = [
   "I will be eating there at 11:30 am so make it for then.",
   "I'll reach there at 1:30 pm.",
   "No, change it on next friday",
   "Sure. Please confirm that the date is now next Friday and for 1 person.",
   "I need to make it on Monday next week at half past 12 in the afternoon.",
   "A quarter past 5 in the evening, please."
]</pre>
			<p>In the following code, we'll extract the entities of these example utterances:</p>
			<pre>for sent in sentences:
   doc = nlp(sent)
   ents = doc.ents
   print([(ent.text, ent.label_) for ent in ents])
[('11:30 am', 'TIME')]
[('1:30 pm', 'TIME')]
[('next friday', 'DATE')]
[('next Friday', 'DATE'), ('1', 'CARDINAL')]
[('Monday next week', 'DATE'), ('half past 12', 'DATE')]
[('A quarter past 5', 'DATE')]
[('the evening', 'TIME'), ('4:45', 'TIME')]</pre>
			<p>Looks good! The output looks quite successful:</p>
			<ul>
				<li>The time entities <code>11:30 am</code> and <code>1:30 pm</code> of the first and second sentences are extracted successfully.</li>
				<li>The <code>DATE</code> entities <code>next friday</code> and <code>next Friday</code> of the third and fourth sentences are<a id="_idIndexMarker697"/> extracted as well. Notice the first entity includes a typo: <code>friday</code> should be written as <em class="italic">Friday</em> – still, the spaCy NER model successfully extracted this entity.  </li>
				<li>The fifth sentence included both a <code>DATE</code> entity and a <code>TIME</code> entity. We can break the <code>DATE</code> entity <code>Monday next week</code> into two parts: <code>Monday</code> – a weekday and <code>next week</code> – a relative date (the exact date depends on the date of the utterance). This entity consists of two noun phrases: <code>Monday</code> (noun) and <code>next week</code> (adjective noun). spaCy can handle such multiword entities. The time entity, <code>half past 12</code>, of this utterance is also a multiword entity. This entity consists of a noun (<code>half</code>), a preposition (<code>past</code>), and a number (<code>12</code>). </li>
				<li>The same goes for the sixth utterance's multiword <code>TIME</code> entity, <code>A quarter past 5</code>. Here is the dependency tree of this entity:</li>
			</ul>
			<div><div><img src="img/Figure_10_2.jpg" alt="Figure 10.2 – Dependency tree of the time entity &quot;A quarter past 5&quot;&#13;&#10;" width="657" height="182"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – Dependency tree of the time entity "A quarter past 5"</p>
			<p>The preceding examples<a id="_idIndexMarker698"/> look quite good indeed, but how about the following utterances:</p>
			<pre>sentences = [
   "Have a great day.",
   "Have a nice day.",
   "Have a good day",
   "Have a wonderful day.",
   "Have a sunny and nice day"
]
for sent in sentences:
   doc = nlp(sent) 
   ents = doc.ents 
   print([(ent.text, ent.label_) for ent in ents])  
[('a great day', 'DATE')]
[('a nice day', 'DATE')]
[]
[]
[]</pre>
			<p>Oops-a-daisy – looks like we <a id="_idIndexMarker699"/>have some <code>day</code>, as date entities incorrectly. What can we do here? </p>
			<p>Fortunately, these <a id="_idIndexMarker700"/>false matches don't form a pattern such as <code>a good day</code> and <code>a wonderful day</code> of the third and fourth sentence are not labeled as entities. Only the word sequences <code>a great day</code> and <code>a nice day</code> are labeled as entities. Then, we can just filter the spaCy NER results with the following two patterns:</p>
			<pre>sentence = 'Have a nice day.'
doc = nlp(sentence)
wrong_matches = ["a great day", "a nice day"]
date_ents = [ent for ent in doc.ents if ent.label_ == "DATE"]
date_ents = list(filter(lambda e: e.text not in wrong_matches, date_ents))
date_ents
[]</pre>
			<p>The preceding code block performs the following steps:</p>
			<ol>
				<li>First, we defined a list of phrases that we don't want to come up as <code>DATE</code> entities.</li>
				<li>We extracted the <code>DATE</code> entities of the Doc object on the third line by iterating over all entities of <code>doc</code> and picking the entities whose labels were <code>DATE</code>.</li>
				<li>In the next line, we filtered the entities that didn't appear in the <code>wrong_matches</code> list. </li>
				<li>We printed the result. As expected, the final result of the <code>date</code> entity is an empty list.</li>
			</ol>
			<p>Great, we have extracted <code>DATE</code> and <code>TIME</code> entities along with <code>CITY</code> entities. For all the three entity types, we used the spaCy NER model directly, because spaCy NER recognizes date, time, and location entities. How about <code>phone_number</code> entities? SpaCy NER doesn't include<a id="_idIndexMarker702"/> such a label at all. So, we'll use some <code>Matcher</code> class tricks to handle this entity type. Let's extract the phone numbers.</p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor181"/>Extracting phone numbers</h2>
			<p>We had some <code>Matcher</code> class practice <a id="_idIndexMarker703"/>on entities that include numbers in <a href="B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069"><em class="italic">Chapter 4</em></a><em class="italic">, Rule-Based Matching</em>. We can also recall from <a href="B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069"><em class="italic">Chapter 4</em></a><em class="italic">, Rule-Based Matching</em>, that matching number type entities can be indeed quite tricky; extracting telephone numbers especially requires attention. Phone numbers can come in different formats, with dashes (212-44-44), area codes ((312) 790 12 31), country and area codes (+49 30 456 222), and the number of digits differing from country to country. As a result, we usually examine the following points:</p>
			<ul>
				<li>How many country formats are the corpus phone number entities written in?</li>
				<li>How are the digit blocks separated – with a dash, or whitespace, or both?</li>
				<li>Is there an area code block in some phone numbers?</li>
				<li>Is there a country code block in some phone numbers?</li>
				<li>Are the country code blocks preceded with a + or 00, or are both formats used?</li>
			</ul>
			<p>Let's examine some of our phone number entities, then:</p>
			<pre>You can call them at 415-775-1800. And they do not serve alcohol.
Their phone number is 408-374-3400 and they don't have live music.
Unfortunately no, they do not have live music, however here is the number: 510-558-8367.</pre>
			<p>All the phone-type entities occur in system utterances. The chatbot fetches phone numbers of restaurants and provides them to the users. The chatbot formed phone number entities by placing a dash between the digit blocks. Also, all the phone numbers are in USA phone number format. Hence the phone number format is uniform and is of the form <code>ddd-ddd-dddd</code>. This is very good for defining a Matcher pattern. We can define only one pattern and it matches all the phone number entities.</p>
			<p>Let's first see how an example phone number tokenizes:</p>
			<pre>doc= nlp("The phone number is 707-766-7600.")
[token for token in doc]
[The, phone, number, is, 707, -, 766, -, 7600, .] </pre>
			<p>Each digit block is tokenized as one token and each dash character is tokenized as one token as well. Hence, in our <a id="_idIndexMarker704"/>Matcher pattern, we'll look for a sequence of five tokens: a three-digit number, a dash, a three-digit number again, a dash again, and finally a four-digit number. Then, our Matcher pattern should look like this:</p>
			<pre>{"SHAPE": "ddd"}, {"TEXT": "-"}, {"SHAPE": "ddd"}, {"TEXT": "-"}, {"SHAPE": "dddd"}</pre>
			<p>If you recall from <a href="B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069"><em class="italic">Chapter 4</em></a><em class="italic">, Rule-Based Matching</em>, the <code>SHAPE</code> attribute refers to the token shape. The token shape represents the shape of the characters: <code>d</code> means a digit, <code>X</code> means a capital character, and <code>x</code> means a lowercase character. Hence <code>{"SHAPE": "ddd"}</code> means a token that consists of three digits. This pattern will match five tokens of the form <code>ddd-ddd-dddd</code>. Let's try our brand-new pattern on a corpus utterance:</p>
			<pre>from spacy.matcher import Matcher
matcher = Matcher(nlp.vocab)
pattern = [{"SHAPE": "ddd"}, {"TEXT": "-"}, {"SHAPE": "ddd"}, {"TEXT": "-"}, {"SHAPE": "dddd"}]
matcher.add("usPhoneNum", [pattern])
doc= nlp("The phone number is 707-766-7600.")
matches = matcher(doc)
for mid, start, end in matches:
    print(doc[start:end])
707-766-7600</pre>
			<p>Voila! Our new pattern matched a phone number type entity as expected! Now, we'll deal with the cuisine type so that our chatbot can make a reservation. Let's see how to extract the cuisine type.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor182"/>Extracting cuisine types</h2>
			<p>Extracting cuisine types is<a id="_idIndexMarker705"/> much easier than extracting a number of people or phone types; indeed, it's similar to extracting city entities. We can use a spaCy NER label directly for cuisine types – <code>NORP</code>. The <code>NORP</code> entity label refers to ethnic or political groups:</p>
			<pre>spacy.explain("NORP")
'Nationalities or religious or political groups'</pre>
			<p>Fortunately, cuisine names in our corpus coincide with nationalities. So, cuisine names are labeled as NORP by spaCy's NER. </p>
			<p>First, let's have a look at some example utterances:</p>
			<pre>Is there a specific cuisine type you enjoy, such as Mexican, Italian or something else?
I usually like eating the American type of food.
Find me Ethiopian cuisine in Berkeley.
I'm looking for a Filipino place to eat.
I would like some Italian food.
Malaysian sounds good right now.</pre>
			<p>Let's extract the entities of these utterances and examine how spaCy's NER labels cuisine types as follows:</p>
			<pre>for sent in sentences:
  doc = nlp(sent
  [(ent.text, ent.label_) for ent in doc.ents] 
[('Mexican', 'NORP'), ('Italian', 'NORP')]
[('American', 'NORP')]
[('Ethiopian', 'NORP'), ('Berkeley', 'GPE')]
[('Filipino', 'NORP')]
[('Italian', 'NORP')]
[('Malaysian', 'NORP')]</pre>
			<p>Now, we are able to extract the city, date and time, number of people, and cuisine entities from user utterances. The result of the named entity extraction module we built here carries all the information the chatbot needs to provide to the reservation system. Here's an example utterance annotated with extracted entities:</p>
			<pre>I'd like to reserve an Italian place for 4 people by tomorrow 19:00 in Berkeley.
{
entities: {
   "cuisine": "Italian",
   "date": "tomorrow",
   "time": "19:00",
   "number_people": 4,
   "city": "Berkeley"
}</pre>
			<p>Here, we completed the <a id="_idIndexMarker706"/>first part of our semantic parsing, extracting entities. A full semantic parse needs an intent too. Now, we'll move on to the next section and do intent recognition with TensorFlow and Keras.  </p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor183"/>Intent recognition</h1>
			<p><strong class="bold">Intent recognition</strong> (also called <strong class="bold">intent classification</strong>) is the task of classifying user utterances with <a id="_idIndexMarker707"/>predefined labels (intents). Intent classification is basically text<a id="_idIndexMarker708"/> classification. Intent classification is a well-known and common NLP task. GitHub and Kaggle host many intent classification datasets (please refer to the <em class="italic">References</em> section for the names of some example datasets).</p>
			<p>In real-world chatbot applications, we first determine the domain our chatbot has to function in, such as finance and banking, healthcare, marketing, and so on. Then we perform the following <a id="_idIndexMarker709"/>loop of actions:</p>
			<ol>
				<li value="1">We determine a set of intents we want to support and prepare a labeled dataset of <code>(utterance, label)</code> pairs. We train our intent classifier on this dataset.</li>
				<li>Next, we deploy our chatbot to the users and gather real user data.</li>
				<li>Then we examine how our chatbot performed on real user data. At this stage, usually, we spot some new intents and some utterances our chatbot failed to recognize. We extend our set of intents with the new intents, add the unrecognized utterances to our training set, and retrain our intent classifier.</li>
				<li>We go to <em class="italic">step 2</em> and perform <em class="italic">steps 2-3</em> until chatbot NLU quality reaches a good level of accuracy (&gt; 0.95)</li>
			</ol>
			<p>Our dataset is a real-world dataset; it contains typos and grammatical mistakes. While designing our intent classifiers – especially while doing pattern-based classification – we need to be robust to such mistakes. </p>
			<p>We'll do the intent recognition in two steps: pattern-based text classification and statistical text classification. We saw how to do statistical text classification with TensorFlow and Keras in <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a><em class="italic">, Text Classification with spaCy</em>. In this section, we'll work with Tensorflow and Keras again. Before that, we'll see how to design a pattern-based text classifier.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor184"/>Pattern-based text classification </h2>
			<p><strong class="bold">Pattern-based classification</strong> means <a id="_idIndexMarker710"/>classifying text by matching <a id="_idIndexMarker711"/>a predefined list of patterns to the text. We compare a precompiled list of patterns against the utterances and check whether there's a match.</p>
			<p>An immediate example<a id="_idIndexMarker712"/> is <strong class="bold">spam classification</strong>. If an email contains one of the patterns, such as <em class="italic">you won a lottery</em> and <em class="italic">I'm a Nigerian prince</em>, then this email should be classified as spam. Pattern-based classifiers are combined with <strong class="bold">statistical classifiers</strong> to boost the<a id="_idIndexMarker713"/> overall system accuracy. </p>
			<p>Contrary to statistical classifiers, pattern-based classifiers are easy to build. We don't need to put any effort into training a TensorFlow model at all. We will compile a list of patterns from our corpus and feed them to Matcher. Then, Matcher can look for pattern matches in utterances.</p>
			<p>To build a pattern-based classifier, we first need to collect some patterns. In this section, we'll classify utterances with the <code>NONE</code> label. Let's see some utterance examples first:</p>
			<pre>No, Thanks
No, thank you very much.
That is all thank you so much.
No, that is all.
Nope, that'll be all. Thanks!
No, that's okay.
No thanks. That's all I needed help with.
No. This should be enough for now.
No, thanks.
No, thanks a lot.
No, thats all thanks.</pre>
			<p>By looking at these utterances, we see that the utterances with the <code>NONE</code> label follow some patterns:</p>
			<ul>
				<li>Most of the utterances start with <code>No,</code> or <code>No.</code>.</li>
				<li>Patterns of saying <em class="italic">thank you</em> are also quite common. The patterns <code>Thanks</code>, <code>thank you</code>, and <code>thanks a lot</code> occur in most of the utterances in the preceding code.</li>
				<li>Some helper<a id="_idIndexMarker714"/> phrases such as <code>that is all</code>, <code>that'll be all</code>, <code>that's OK</code>, and <code>this should be enough</code> are also commonly used.</li>
			</ul>
			<p>Based on this information, we can create three Matcher patterns as follows:</p>
			<pre>[{"LOWER": {"IN": ["no", "nope"]}}, {"TEXT": {"IN": [",", "."]}}]
[{"TEXT": {"REGEX": "[Tt]hanks?"}}, {"LOWER": {"IN": ["you", "a lot"]}, "OP": "*"}]
[{"LOWER": {"IN": ["that", "that's", "thats", "that'll",
"thatll"]}}, {"LOWER": {"IN": ["is", "will"]}, "OP": "*"}, {"LOWER": "all"}]</pre>
			<p>Let's go over the patterns one by one:</p>
			<ul>
				<li>The first pattern matches token sequences <code>no,</code>, <code>no.</code>, <code>nope,</code>, <code>nope.</code>, <code>No,</code>, <code>No.</code>, <code>Nope,</code>, and <code>Nope.</code>. The first item matches two tokens <code>no</code> and <code>nope</code> either in capitals or small letters. The second item matches the punctuation marks <code>,</code> and <code>.</code>. </li>
				<li>The second pattern matches <code>thank</code>, <code>thank you</code>, <code>thanks</code>, and <code>thanks a lot</code>, either in capitals or small letters. The first item matches <code>thank</code> and <code>thanks</code> <code>s?</code>. In regex syntax, the <code>s</code> character is optional. The second item corresponds to the words <code>you</code> and <code>a lot</code>, which can possibly follow <code>thanks?</code>. The second item is optional; hence, the pattern matches <code>thanks</code> and <code>thank</code> as well. We used the operator <code>OP: *</code> to make the second item optional; recall from <a href="B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069"><em class="italic">Chapter 4</em></a><em class="italic">, Rule-Based Matching</em>, that Matcher supports operator syntax with different operators, such as <code>*</code> , <code>+</code>, and <code>?</code>.</li>
				<li>The third pattern matches the token sequences <code>that is all</code>, <code>that's all</code>, <code>thats all</code>, and so on. Notice that the first item includes some misspelled words, such as <code>thats</code> and <code>thatll</code>. We included the misspelled words on purpose, so the matching will be more robust to user typos.</li>
			</ul>
			<p>Different combinations of the preceding three patterns will match the utterances of the <code>NONE</code> class. You can<a id="_idIndexMarker715"/> try the patterns by adding them to a Matcher object and see how they match.</p>
			<p class="callout-heading">Pro tip</p>
			<p class="callout">While designing a rule-based system, always keep in mind that user data is not perfect. User data contains typos, grammatical mistakes, and wrong capitalization. Always keep robustness as a high priority and test your patterns on user data.</p>
			<p>We made a statistical model-free classifier by making use of some common patterns and classified one intent successfully. How about the other two intents – <code>FindRestaurants</code> and <code>ReserveRestaurant</code>? Utterances of these two intents are semantically much more complicated, so we cannot cope with pattern lists. We need statistical models to recognize these two intents. Let's go ahead and train our statistical text classifiers with TensorFlow and Keras.</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor185"/>Classifying text with a character-level LSTM</h2>
			<p>In this section, we'll train a <strong class="bold">character-level LSTM architecture</strong> for<a id="_idIndexMarker716"/> recognizing the intents. We already practiced text classification with TensorFlow and Keras in <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a><em class="italic">, Text Classification with spaCy</em>. Recall from this chapter that LSTMs are sequential models that process one input at one time step. We fed one word at each time step as follows:</p>
			<div><div><img src="img/Figure_10_3.jpg" alt="Figure 10.3 – Feeding one word to an LSTM at each time step&#13;&#10;" width="1174" height="495"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.3 – Feeding one word to an LSTM at each time step</p>
			<p>As we remarked in <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a><em class="italic">, Text Classification with spaCy</em>, LSTMs have an internal state (you can think of it as a memory), so LSTMs can model the sequential dependencies in the input sequence by holding past information in their internal state.</p>
			<p>In this section, we'll train a character-level LSTM. As the name suggests, we'll feed utterances character by character, not word by word. Each utterance will be represented as a sequence of characters. At each time step, we'll feed one character. This is what feeding the utterance from <em class="italic">Figure 10.3</em> looks like:</p>
			<div><div><img src="img/Figure_10_4.jpg" alt="Figure 10.4 – Feeding the first two words of the utterance &quot;I want Italian food&quot;&#13;&#10;" width="1462" height="389"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.4 – Feeding the first two words of the utterance "I want Italian food"</p>
			<p>We notice that the <a id="_idIndexMarker717"/>space character is fed as an input as well, because the space character is also a part of the utterance; for character-level tasks, there is no distinction between digits, spaces, and letters. </p>
			<p>Let's start building the Keras model. We'll skip the data preparation stage here. You can find the complete code in the intent classification notebook <a href="https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/Intent-classifier-char-LSTM.ipynb">https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/Intent-classifier-char-LSTM.ipynb</a>.</p>
			<p>We'll directly start with Keras' Tokenizer to create a vocabulary. Recall from <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a><em class="italic">, Text Classification with spaCy</em>, that we use Tokenizer to do the following:</p>
			<ul>
				<li>Create a vocabulary from the dataset sentences.</li>
				<li>Assign a token ID to each token of the dataset.</li>
				<li>Transform input sentences to token IDs.</li>
			</ul>
			<p>Let's see how to <a id="_idIndexMarker718"/>perform each step:</p>
			<ol>
				<li value="1">In <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a><em class="italic">, Text Classification with spaCy</em>, we tokenized the sentences into words and assigned token IDs to words. This time, we'll break the input sentence into its characters, then assign token IDs to characters. Tokenizer provides a parameter named <code>char_level</code>. Here's the Tokenizer code for character-level tokenization:<pre>from tensorflow.keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer(char_level=True, lower=True)
tokenizer.fit_on_texts(utterances)</pre></li>
				<li>The preceding code segment will create a vocabulary from the input characters. We used the <code>lower=True</code> parameter, so all characters of the input sentence are made lowercase by Tokenizer. After initializing the <code>Tokenizer</code> object on our vocabulary, we can now examine its vocabulary. Here are the first 10 items of the Tokenizer vocabulary:<pre>tokenizer.word_index
{' ': 1, 'e': 2, 'a': 3, 't': 4, 'o': 5, 'n': 6, 'i': 7, 'r': 8, 's': 9, 'h': 10}</pre><p>Just as with the word-level vocabulary, index <code>0</code> is reserved for a special token, which is the padding character. Recall from <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a><em class="italic">, Text Classification with spaCy</em>, that Keras cannot process variable-length sequences; each sentence in the dataset should be of the same length. Hence, we pad all sentences to a maximum length by appending a padding character to the sentence end or sentence start. </p></li>
				<li>Next, we'll convert each dataset sentence into token IDs. This is achieved by calling the <code>texts_to_sequences</code> method of Tokenizer:<pre>utterances = tokenizer.texts_to_sequences(utterances)
utterances[0]
[17, 2, 9, 25, 1, 7, 1, 22, 3, 6, 4, 1, 7, 4, 1, 5, 6, 1, 4, 10, 2, 1, 28, 28, 4, 10]</pre></li>
				<li>Next, we'll pad all the input sentences to a length of <code>150</code>:<pre>MAX_LEN = 150
utterances =\
 pad_sequences(utterances, MAX_LEN, padding="post")</pre><p>We're ready to feed our transformed dataset into our LSTM model. Our model is a simple yet very efficient one: we placed a dense layer on top of a bidirectional LSTM layer. Here's the model architecture:</p><pre>utt_in = Input(shape=(MAX_LEN,))
embedding_layer =  Embedding(input_dim = len(tokenizer.word_index)+1, output_dim = 100, input_length=MAX_LEN)
lstm =\
Bidirectional(LSTM(units=100, return_sequences=False))
utt_embedding = embedding_layer(utt_in)
utt_encoded = lstm(utt_embedding)
output = Dense(1, activation='sigmoid')(utt_encoded)
model = Model(utt_in, output)</pre><p>A bidirectional LSTM layer means two LSTMs stacked on top of each other. The first LSTM goes<a id="_idIndexMarker719"/> through the input sequence from left to right (in a forward direction) and the second LSTM goes through the input sequence right to left (in a backward direction). For each time step, the outputs of the forward LSTM and backward LSTM are concatenated to generate a single output vector. The following figure exhibits our architecture with a<a id="_idIndexMarker720"/> bidirectional LSTM: </p><div><img src="img/Figure_10_5.jpg" alt="Figure 10.5 – Bidirectional LSTM architecture &#13;&#10;" width="874" height="813"/></div><p class="figure-caption">Figure 10.5 – Bidirectional LSTM architecture </p></li>
				<li>Next, we compile our model and train it on our dataset by calling <code>model.fit</code>: <pre>model.compile(loss = 'binary_crossentropy', optimizer = "adam", metrics=["accuracy"])
model.fit(utterances, labels, validation_split=0.1, epochs = 10, batch_size = 64)</pre><p>Here, we compiled our model with the following:</p><p>a) Binary cross-entropy loss, because this is a binary classification task (we have two class labels).</p><p>b) The <code>Adam</code> optimizer, which will help the training procedure to run faster by arranging the size of the training steps. Please refer to the <em class="italic">References</em> section and <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a><em class="italic">, Text Classification with spaCy</em>, for more information about the <code>Adam</code> optimizer.</p><p>c) Accuracy as our success <a id="_idIndexMarker721"/>metric. Accuracy is calculated by comparing how often the predicted label is equal to the actual label. </p></li>
			</ol>
			<p>After fitting our model, our model gives a <code>0.8226</code> accuracy on the validation set, which is quite good. </p>
			<p>Now, only one question remains: why did we prefer to train a character-level model this time? Character-level models definitely have some advantages:</p>
			<ul>
				<li>Character-level models are highly misspelling-tolerant. Consider the misspelled word <em class="italic">charactr</em> – whether or not the <em class="italic">e</em> is missing does not affect the overall sentence semantics that much. For our dataset, we will benefit from this robustness, as we have already seen spelling mistakes by users in our dataset exploration.</li>
				<li>The vocabulary size is smaller than a word-level model. The number of characters in the alphabet (for any given language) is fixed and low (a maximum of 50 characters, including uppercase and lowercase letters, digits, and some punctuation); but the number of words in a language is much greater. As a result, model sizes can differ. The main difference lies at the embedding layer; an embedding table is of size <code>(vocabulary_size, output_dim)</code> (refer to the model code). Given that the output dimensions are the same, 50 rows is really small compared to thousands of rows.</li>
			</ul>
			<p>In this section, we were<a id="_idIndexMarker722"/> able to extract the user intent from the utterances. Intent recognition is the main step in understanding sentence semantics, but is there something more? In the next section, we'll dive into sentence-level and dialog-level semantics. More semantic parsing</p>
			<p>This is a <a id="_idIndexMarker723"/>section solely on chatbot NLU. In this section, we'll explore sentence-level semantic and syntactic information to generate a deeper understanding of the input utterances, as well as providing clues to answer generation. </p>
			<p>In the rest of this section, please think of the answer generation component as a black box. We provide the semantic parse of the sentence and it generates an answer based on this semantic parse. Let's start by dissecting sentence syntax and examining the subjects and objects of the utterances.</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor186"/>Differentiating subjects from objects</h2>
			<p>Recall from <a href="B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055"><em class="italic">Chapter 3</em></a><em class="italic">, Linguistic Features</em>, that a sentence has two important grammatical<a id="_idIndexMarker724"/> components: a <strong class="bold">subject</strong> and an <strong class="bold">object</strong>. The subject is the person or thing that performs the action given by the verb of the sentence:</p>
			<pre><strong class="bold">Mary</strong> picked up her brother.
<strong class="bold">He</strong> was a great performer.
<strong class="bold">It</strong> was rainy on Sunday.
<strong class="bold">Who</strong> is responsible for this mess?
<strong class="bold">The cat</strong> is very cute.
<strong class="bold">Seeing you</strong> makes me happy.</pre>
			<p>A subject can be a noun, a pronoun, or a noun phrase. </p>
			<p>An object is the thing or person on which the subject performs the action given by the verb. An object can be a noun, a pronoun, or a noun phrase too. Here are some examples:</p>
			<pre>Lauren lost <strong class="bold">her book</strong>.
I gave <strong class="bold">her</strong>/direct object <strong class="bold">a book</strong>/indirect object.</pre>
			<p>So far, so good, but how does this information help us in our chatbot NLU? </p>
			<p>Extracting the subject <a id="_idIndexMarker725"/>and the object helps us understand the sentence structure, hence adding one more layer to the semantic parse of the sentence. Sentence subject and object information directly relates to answer generation. Let's see some examples of utterances from our dataset:</p>
			<pre>Where is this restaurant? </pre>
			<p>The following figure shows the dependency parse of this utterance. The subject is the noun phrase <code>this restaurant</code>: </p>
			<div><div><img src="img/Figure_10_6.jpg" alt="Figure 10.6 – Dependency parse of the example utterance&#13;&#10;" width="504" height="172"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.6 – Dependency parse of the example utterance</p>
			<p>How can we generate an answer to this sentence? Obviously, the answer should have <code>this restaurant</code> (or the restaurant it refers to) as the subject. Some answers could be as follows:</p>
			<pre>The restaurant is located at the corner of 5th Avenue and 7th Avenue.
The Bird is located at the corner of 5Th Avenue and 7Th Avenue.</pre>
			<p>What if the user puts <code>this restaurant</code> into the object role? Would the answer change? Let's take some example utterances from the dataset:</p>
			<pre>Do you know where this restaurant is?
Can you tell me where this restaurant is?</pre>
			<p>Obviously, the user is asking<a id="_idIndexMarker726"/> about the address of the restaurant again. The system needs to give the restaurant address information. However, this time, the subject of these sentences is <code>you</code>:</p>
			<div><div><img src="img/Figure_10_7.jpg" alt="Figure 10.7 – Dependency parse of the first sentence&#13;&#10;" width="983" height="328"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.7 – Dependency parse of the first sentence</p>
			<p>This question's subject is <code>you</code>, so the answer can start with an <em class="italic">I</em>. This a question sentence, hence the answer can start with a <em class="italic">yes</em>/<em class="italic">no</em> or the answer can just provide the restaurant's address directly. The following sentences are all possible answers:</p>
			<pre>I can give the address. Here it is: 5th Avenue, no:2
Yes, of course. Here's the address: 5th Avenue, no:2
Here's the address: 5Th Avenue, no:2 </pre>
			<p>The same phrase, <code>the restaurant</code>, being the subject or the object doesn't affect the user's intent, but it affects the sentence structure of the answer. Let's look at the information more <a id="_idIndexMarker727"/>systematically. The semantic parses of the preceding example sentences look as follows:</p>
			<pre>{
utt: "Where is this restaurant?",
intent: "FindRestaurants",
entities: [],
structure: {
    subjects: ["this restaurant"]
  }
}
{
utt: "Do you know where is this restaurant is?",
intent: "FindRestaurants",
entities: [],
structure: {
    subjects: ["you"]
  }
}</pre>
			<p>When we feed these semantic parses to the answer generator module, this module can generate answers by taking the current utterance, the dialog history, the utterance intent, and the utterance's sentence structure (for the time being, only the sentence subject information) into account.</p>
			<p>Here, we extracted the utterance's sentence structure information by looking at the utterance's dependency tree. Can a dependency parse provide us with more information about the utterance? The answer is yes. We'll see how to extract the sentence type in the<a id="_idIndexMarker728"/> next section.</p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor187"/>Parsing the sentence type</h2>
			<p>In this section, we'll extract the sentence type of the user utterances. The grammar has four main<a id="_idIndexMarker729"/> sentence types, classified by their purpose:</p>
			<pre>Declarative: John saw Mary.
Interrogative: Can you go there?
Imperative: Go there immediately.
Exclamation: I'm excited too!</pre>
			<p>Sentence types in chatbot NLU are a bit different; we classify sentences according to the POS tag of the subject and objects as well as the purpose. Here are some sentence types that are used in chatbot NLU:  </p>
			<pre>Question sentence
Imperative sentence
Wish sentence</pre>
			<p>Let's examine each sentence type and its structural properties. We start with question sentences.</p>
			<h3>Question sentences</h3>
			<p>A question<a id="_idIndexMarker730"/> sentence is used when the user wants to ask something. A question sentence can be formed in two ways, either by using an interrogative pronoun or by placing a modal/auxiliary verb at the beginning of the sentence:</p>
			<pre><strong class="bold">How</strong> did you go there?
<strong class="bold">Is</strong> this the book that you recommended?</pre>
			<p>Hence, we also divide the question sentences into<a id="_idIndexMarker731"/> two classes, <strong class="bold">wh-questions</strong> and <strong class="bold">yes/no questions</strong>. As the name suggests, wh-questions start with a <strong class="bold">wh-word </strong>(a wh-word means an interrogative pronoun, such as where, what, who, and how) and yes/no questions are formed by using a modal/auxiliary verb. </p>
			<p>How will this classification help us? Syntactically, yes/no<a id="_idIndexMarker732"/> questions should be answered with a yes or no. Hence, if our chatbot NLU passes a yes/no question to the answer generation module, the answer generator should evaluate this information and generate an answer that starts with a yes/no. Wh-questions aim to get information about the subject or objects, hence the answer generator module should provide information about the sentence subject or objects. Consider the following utterance:</p>
			<pre>Where is this restaurant?</pre>
			<p>This utterance <a id="_idIndexMarker733"/>generates the following dependency parse:</p>
			<div><div><img src="img/Figure_10_8.jpg" alt="Figure 10.8 – Dependency parse of the example wh-question&#13;&#10;" width="507" height="174"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.8 – Dependency parse of the example wh-question</p>
			<p>Here, the subject of the utterance is <code>this restaurant</code>; hence the answer generator should generate an answer by relating <code>Where</code> and <code>this restaurant</code>. How about the following utterance:</p>
			<pre>Which city is this restaurant located?</pre>
			<p>The dependency parse of this utterance looks as follows:</p>
			<div><div><img src="img/Figure_10_9.jpg" alt="Figure 10.9 -- Dependency parse of the example wh-question&#13;&#10;" width="1083" height="250"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.9 -- Dependency parse of the example wh-question</p>
			<p>Here, the sentence <a id="_idIndexMarker734"/>structure is a bit different. <code>Which city</code> is the subject of the sentence and <code>this restaurant</code> is the subject of the clause. Here, the answer generation module should generate an answer by relating <code>which city</code> and <code>this restaurant</code>. </p>
			<p>We will now move on to imperative sentence type. </p>
			<h3>Imperative sentence</h3>
			<p><strong class="bold">Imperative sentences</strong> occur quite <a id="_idIndexMarker735"/>frequently in chatbot user utterances. An imperative sentence is formed by placing the main verb at the beginning of the sentence. Here are some utterance examples from our dataset:</p>
			<pre>Find me Ethiopian cuisine in Berkeley.
Find me a sushi place in Alameda.
Find a place in Vallejo with live music.
Please reserve me at 6:15 in the evening.
Reserve for six in the evening.
Reserve it for half past 1 in the afternoon.</pre>
			<p>As we see, imperative utterances occur quite a lot in user utterances, because they're succinct and to-the-point. We can spot these types of sentences by looking at the POS tags of the words: either the first word is a verb or the sentence starts with <em class="italic">please</em> and the second word is a verb. The following Matcher patterns match imperative utterances: </p>
			<pre>[{"POS": "VERB, "IS_SENT_START": True}]
[{"LOWER": "please", IS_SENT_START: True}, {"POS": "VERB"}]</pre>
			<p>How would the answer generator process these types of sentences? Imperative sentences usually include the syntactic and semantic elements to generate an answer; the main verb provides<a id="_idIndexMarker736"/> the action and is usually followed by a list of objects. Here's an example parse for the utterance <code>Find me Ethiopian cuisine in Berkeley</code>:</p>
			<div><div><img src="img/Figure_10_10.jpg" alt="Figure 10.10 – Dependency parse of the example utterance&#13;&#10;" width="877" height="249"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.10 – Dependency parse of the example utterance</p>
			<p>From the figure, we can see the <a id="_idIndexMarker737"/>syntactic components of this sentence as <code>Find</code> (the action), <code>Ethiopian cuisine</code> (an object), and <code>Berkeley</code> (an object). These components provide a clear template to the answer generator for generating an answer to this utterance: the answer generator should ask the restaurants database for matches of <code>Ethiopian cuisine</code> and <code>Berkeley</code> and list the matching restaurants.</p>
			<p>Now we move on to the next sentence type, wish sentences. Let's see look at sentences in detail.</p>
			<h3>Wish sentences</h3>
			<p><strong class="bold">Wish sentences</strong> are semantically similar to imperative sentences. The difference is syntactic: wish sentences start with phrases such as <em class="italic">I'd like to</em>, <em class="italic">Can I</em>, <em class="italic">Can you</em>, and <em class="italic">May I</em>, pointing to a wish. Here are some examples from our dataset:</p>
			<pre>I'd like to make a reservation.
I would like to find somewhere to eat, preferably Asian food.
I'd love some Izakaya type food.
Can you find me somewhere to eat in Dublin?
Can we make it three people at 5:15 pm?
Can I make a reservation for 6 pm?</pre>
			<p>Extracting the verb and the objects is similar to what we do for imperative sentences, hence the semantic<a id="_idIndexMarker738"/> parse is quite similar.</p>
			<p>After extracting the sentence type, we can include it into our semantic parse result as follows:</p>
			<pre>{
utt: "Where is this restaurant?",
intent: "FindRestaurants",
entities: [],
structure: {
    sentence_type: "wh-question",
    subjects: ["this restaurant"]
  }
}</pre>
			<p>Now, we have a rich semantic and syntactic representation of the input utterance. In the next section, we'll go one step beyond the sentence-level semantics and go through the dialog-level semantics. Let's move on to the next section and see how we tackle dialog-level semantics.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor188"/>Anaphora resolution</h2>
			<p>In this section, we'll explore the linguistic concepts of <strong class="bold">anaphora</strong> and <strong class="bold">cohesion</strong>. In linguistics, cohesion <a id="_idIndexMarker739"/>means the grammatical links that glue a text together semantically. This text can be a single <a id="_idIndexMarker740"/>sentence, a paragraph, or a dialog segment. Consider the following two sentences:</p>
			<pre>I didn't like this dress. Can I see another <strong class="bold">one</strong></pre>
			<p>Here, the word <code>one</code> refers to the dress from the first sentence. A human can resolve this link easily. It's not so straightforward for software programs, though. </p>
			<p>Also, consider the following dialog segment:</p>
			<pre>Where are you going?
To my grandma's.</pre>
			<p>The second sentence is completely understandable, though some parts of the sentence are missing:</p>
			<pre><strong class="bold">I'm going</strong> to my grandma's <strong class="bold">house</strong>. </pre>
			<p>In written and spoken language, we <a id="_idIndexMarker741"/>use such <strong class="bold">shortcuts</strong> every day. However, resolving such shortcuts needs attention while programming, especially in chatbot NLU. Consider these utterances and dialog segments from our dataset:</p>
			<p>Example 1:</p>
			<pre>- Do you want to make<strong class="bold"> a reservation</strong>?
- Yes, I want to make <strong class="bold">one</strong>.</pre>
			<p>Example 2:</p>
			<pre>- I've found <strong class="bold">2 Malaysian restaurants</strong> in Cupertino. Merlion Restaurant &amp; Bar is one.
- What is the other <strong class="bold">one</strong>?</pre>
			<p>Example 3:</p>
			<pre>- There's another restaurant in San Francisco that's called Bourbon Steak Restaurant.
- Yes, I'm interested in that <strong class="bold">one</strong>.</pre>
			<p>Example 4: </p>
			<pre>- Found 3 results, <strong class="bold">Asian pearl Seafood Restaurant</strong> is the best one in Fremont city, hope you like it.
- Yes, I like the <strong class="bold">same</strong>.</pre>
			<p>Example 5: </p>
			<pre>- Do you have <strong class="bold">a specific</strong> which you want the <strong class="bold">eating place</strong> to be located at?
- I would like for <strong class="bold">it</strong> to be in San Jose.</pre>
			<p>Example 6:</p>
			<pre>- Would you like <strong class="bold">a reservation</strong>?
- Yes make <strong class="bold">it</strong> for March 10th.</pre>
			<p>All the highlighted parts of the preceding sentences and dialogs are examples of a linguistic event named <code>one</code>, <code>more</code>, <code>same</code>, <code>it</code>, and so on. Anaphora resolution means to resolve exactly the phrases anaphoric words point to.</p>
			<p>How do we apply this information to our chatbot NLU then?</p>
			<p>First of all, we need to determine whether an utterance involves an anaphora and whether we need an anaphora resolution. Consider the following dialog segment again:</p>
			<pre>Do you want to make a reservation?
Yes, I want to make one.</pre>
			<p>The dependency parse of the second utterance looks like this:</p>
			<div><div><img src="img/Figure_10_11.jpg" alt="Figure 10.11 – Dependency parse of the example utterance&#13;&#10;" width="1059" height="252"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.11 – Dependency parse of the example utterance</p>
			<p>First of all, <code>one</code> appears as the<a id="_idIndexMarker743"/> direct object of the sentence and there are no other direct objects. This means that <code>one</code> should be an anaphora. In order to resolve what <code>one</code> refers to, we'll look back to the first utterance of the dialog. The following dependency parse belongs to the first utterance, <code>Do you want to make a reservation?</code>:</p>
			<div><div><img src="img/Figure_10_12.jpg" alt="Figure 10.12 – Dependency parse of the example utterance&#13;&#10;" width="1085" height="249"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.12 – Dependency parse of the example utterance</p>
			<p>If we look at <em class="italic">Figure 10.11</em>, we see that the sentence has a direct object, <code>a reservation</code>, so <code>one</code> should refer to <code>a reservation</code>. Then, we can arrange the resulting semantic parse as follows:</p>
			<pre>{
utt: "Where is this restaurant?",
intent: "ReserveRestaurant",
entities: [],
structure: {
    sentence_type: "declarative",
    subjects: ["one"]
    anaphoras: {
       "one": "a reservation"
       }
  }
}</pre>
			<p>Replacing <code>one</code> with <code>a reservation</code> makes the sentence intent clearer. In our chatbot NLU, we only have two intents, but what if there are more intents, such as reservation cancellation, refunds, and so on? Then <code>I want to make one</code> can mean making a cancellation or getting a refund as well. </p>
			<p>Therefore, we make<a id="_idIndexMarker744"/> anaphora resolution come before the intent recognition and feed the full sentence, where anaphora words are replaced with the phrases they refer to. This way, the intent classifier is fed with a sentence where the direct object is a noun phrase, not one of the words <code>one</code>, <code>same</code>, <code>it</code>, or <code>more</code>, which do not carry any meaning on their own.</p>
			<p>Now after extracting meaning (by extracting intent) statistically with Keras, in this section you learned ways of processing sentence syntax and semantics with special NLU techniques. You're ready to combine all the techniques you know and design your own chatbot NLU for your future career. This book started with linguistic concepts, continued with statistical applications, and in this chapter, we combined it all. You're ready to keep going. In all the NLU pipelines you'll design, always try to look at the problem from a different view and remember what you learned in this book.</p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor189"/>Summary</h1>
			<p>That's it! You made it to the end of this exhaustive chapter and also to the end of this book! </p>
			<p>In this chapter, we designed an end-to-end chatbot NLU pipeline. As a first task, we explored our dataset. By doing this, we collected linguistic information about the utterances and understood the slot types and their corresponding values. Then, we performed a significant task of chatbot NLU, entity extraction. We extracted several types of entities such as city, date/time, and cuisine with the spaCy NER model as well as Matcher. Then, we performed another traditional chatbot NLU pipeline task – intent recognition. We trained a character-level LSTM model with TensorFlow and Keras. </p>
			<p>In the last section, we dived into sentence-level and dialog-level semantics. We worked on sentence syntax by differentiating subjects from objects, then learned about sentence types and finally learned about the linguistic concept of anaphora resolution. We applied what we learned in the previous chapters, both linguistically and statistically, by combining several spaCy pipeline components such as NER, dependency parsers, and POS taggers.</p>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor190"/>References</h1>
			<p>Here are some references for this chapter:</p>
			<p>On voice assistant products:</p>
			<ul>
				<li>Alexa developer blog: <a href="https://developer.amazon.com/blogs/home/tag/Alexa%20">https://developer.amazon.com/blogs/home/tag/Alexa</a></li>
				<li>Alexa science blog: <a href="https://www.amazon.science/tag/alexa%20">https://www.amazon.science/tag/alexa</a></li>
				<li>Microsoft's publication on chatbots: <a href="https://academic.microsoft.com/search?q=chatbot">https://academic.microsoft.com/search?q=chatbot</a></li>
				<li>Google Assistant: <a href="https://assistant.google.com/%20">https://assistant.google.com/</a></li>
			</ul>
			<p>Keras layers and optimizers:</p>
			<ul>
				<li>Keras layers: <a href="https://keras.io/api/layers/%20">https://keras.io/api/layers/</a></li>
				<li>Keras optimizers: <a href="https://keras.io/api/optimizers/%20">https://keras.io/api/optimizers/</a></li>
				<li>An overview of optimizers: <a href="https://ruder.io/optimizing-gradient-descent/%20">https://ruder.io/optimizing-gradient-descent/</a></li>
				<li>Adam optimizer: <a href="https://arxiv.org/abs/1412.6980%20">https://arxiv.org/abs/1412.6980</a></li>
			</ul>
			<p>Datasets for conversational AI:</p>
			<ul>
				<li>Taskmaster from Google Research: <a href="https://github.com/google-research-datasets/Taskmaster/tree/master/TM-1-2019%20">https://github.com/google-research-datasets/Taskmaster/tree/master/TM-1-2019</a></li>
				<li>Simulated Dialogue dataset from Google Research: <a href="https://github.com/google-research-datasets/simulated-dialogue%20">https://github.com/google-research-datasets/simulated-dialogue</a></li>
				<li>Dialog Challenge dataset from Microsoft: <a href="https://github.com/xiul-msr/e2e_dialog_challenge%20">https://github.com/xiul-msr/e2e_dialog_challenge</a></li>
				<li>Dialog State Tracking Challenge dataset: <a href="https://github.com/matthen/dstc">https://github.com/matthen/dstc</a></li>
			</ul>
		</div>
	</div></body></html>