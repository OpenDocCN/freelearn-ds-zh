<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer157">
			<h1 id="_idParaDest-170"><a id="_idTextAnchor173"/>Chapter 10: Putting Everything Together: Designing Your Chatbot with spaCy</h1>
			<p>In this chapter, you will use everything you have learned so far to design a chatbot. You will perform entity extraction, intent recognition, and context handling. You will use different ways of syntactic and semantic parsing, entity extraction, and text classification. </p>
			<p>First, you'll explore the dataset we'll use to collect linguistic information about the utterances within it. Then, you'll perform entity extraction by combining the spaCy <strong class="bold">named entity recognition</strong> (<strong class="bold">NER</strong>) model and the spaCy <strong class="source-inline">Matcher</strong> class. After that, you'll perform intent recognition with two different techniques: a pattern-based method and statistical text classification with TensorFlow and Keras. You'll train a character-level LSTM to classify the utterance intents. </p>
			<p>The final section is a section dedicated to sentence- and dialog-level semantics. You'll take a deep dive into semantic subjects such as <strong class="bold">anaphora resolution</strong>, <strong class="bold">grammatical question types</strong>, and <strong class="bold">differentiating subjects from objects</strong>. </p>
			<p>By the end of this chapter, you'll be ready to design a real chatbot <strong class="bold">natural language understanding</strong> (<strong class="bold">NLU</strong>) pipeline. You will bring together what you learned in all previous chapters – linguistically and statistically – by combining several spaCy pipeline components such as <strong class="bold">NER</strong>, a <strong class="bold">dependency parser</strong>, and a <strong class="bold">POS tagger</strong>.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Introduction to conversational AI</li>
				<li>Entity extraction</li>
				<li>Intent recognition</li>
			</ul>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor174"/>Technical requirements </h1>
			<p>In this chapter, we'll be using NumPy, TensorFlow, and scikit-learn along with spaCy. You can install these libraries via <strong class="source-inline">pip</strong> using the following commands:</p>
			<p class="source-code">pip install numpy</p>
			<p class="source-code">pip install tensorflow</p>
			<p class="source-code">pip install scikit-learn</p>
			<p>You can find the chapter code and data at the book's GitHub repository: <a href="https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter10">https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter10</a>. </p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor175"/>Introduction to conversational AI</h1>
			<p>We welcome you to our last and very exciting chapter, where you'll be designing a chatbot NLU pipeline<a id="_idIndexMarker670"/> with spaCy and TensorFlow. In this chapter, you'll learn the NLU techniques for extracting meaning from multiturn chatbot-user interactions. By learning and applying these techniques, you'll take a step into <strong class="bold">conversational AI development</strong>.  </p>
			<p>Before diving into the technical details, there's one fundamental question: what is a chatbot? Where can we find one? What exactly does conversational AI mean?</p>
			<p><strong class="bold">Conversational artificial intelligence </strong>(<strong class="bold">conversational AI</strong>) is a field of machine learning that aims to create<a id="_idIndexMarker671"/> technology that enables users to have text- or speech-based interactions with machines. Chatbots, virtual assistants, and voice assistants are typical conversational AI products.</p>
			<p>A <strong class="bold">chatbot</strong> is a software application that is designed to make conversations with humans in chat applications. Chatbots are<a id="_idIndexMarker672"/> popular in a wide variety of commercial areas including HR, marketing and sales, banking, and healthcare, as well as in personal, non-commercial areas such as small talk. Many commercial companies, such as Sephora (Sephora owns two chatbots – a virtual make-up artist chatbot on Facebook messenger platform and a customer service chatbot again on Facebook messenger), IKEA (IKEA have a customer service chatbot called Anna), AccuWeather, and many more, own customer service and FAQ chatbots. </p>
			<p>Instant messaging services such as Facebook Messenger and Telegram provide interfaces to developers for connecting their bots. These platforms provide detailed guidelines for developers as well, such as the Facebook Messenger API documentation: (<a href="https://developers.facebook.com/docs/messenger-platform/getting-started/quick-start/">https://developers.facebook.com/docs/messenger-platform/getting-started/quick-start/</a>) or the Telegram bot API <a id="_idIndexMarker673"/>documentation: (<a href="https://core.telegram.org/bots">https://core.telegram.org/bots</a>).</p>
			<p>A <strong class="bold">virtual assistant</strong> is also a software<a id="_idIndexMarker674"/> agent that performs some tasks upon user request or question. A well-known example is <strong class="bold">Amazon Alexa</strong>. Alexa is a voice-based virtual assistant and<a id="_idIndexMarker675"/> can perform many tasks, including playing music, setting alarms, reading audiobooks, playing podcasts, and giving real-time information for weather, traffic, sports, and so on. Alexa Home can control connected smart home devices and perform a variety of tasks, including switching the lights on and off, controlling the garage door, and so on.</p>
			<p>Other well-known<a id="_idIndexMarker676"/> examples are Google Assistant and Siri. Siri is integrated into a number of Apple products, including iPhone, iPad, iPod, and macOS. On iPhone, Siri can make calls, answer calls, and send and receive text messages as well as WhatsApp messages. Google Assistant also can perform a wide variety of tasks, such as providing real-time flight, weather, and traffic information; sending and receiving text messages; setting alarms; providing device battery information; checking your email inbox; integrating with smart home devices; and so on. Google Assistant is available on Google Maps, Google Search, and standalone Android and iOS applications.</p>
			<p>Here is a list of the most popular and well-known virtual assistants to give you some more ideas of what's out there:</p>
			<ul>
				<li>Amazon Alexa</li>
				<li>AllGenie from Alibaba Group</li>
				<li>Bixby from Samsung</li>
				<li>Celia from Huawei</li>
				<li>Duer from Baidu</li>
				<li>Google Assistant</li>
				<li>Microsoft Cortana</li>
				<li>Siri from Apple</li>
				<li>Xiaowei from Tencent</li>
			</ul>
			<p>All of these virtual assistants are<a id="_idIndexMarker677"/> voice-based and are usually invoked with a <strong class="bold">wake word</strong>. A wake word is a special word or phrase that is used to activate a voice assistant. Some examples are <em class="italic">Hey Alexa</em>, <em class="italic">Hey Google</em>, and <em class="italic">Hey Siri</em>, which are the wake words of Amazon Alexa, Google Assistant, and Siri, respectively. If you want to know more about the development details of these products, please refer to the <em class="italic">References</em> section of this chapter.</p>
			<p>Now, we come to the technical details. What are the NLP components of these products? Let's look at these NLP components in detail.</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor176"/>NLP components of conversational AI products</h2>
			<p>A typical voice-based conversational AI <a id="_idIndexMarker678"/>product consists of the following components:</p>
			<ul>
				<li><strong class="bold">Speech-to-text component</strong>: Converts <a id="_idIndexMarker679"/>user speech into text. Input to this component is a WAV/mp3 file and the output is a text file containing the user utterance as a text.</li>
				<li><strong class="bold">Conversational NLU component</strong>: This component performs intent recognition and entity extraction on the user <a id="_idIndexMarker680"/>utterance text. The output is the user intent and a list of entities. Resolving references in the current utterance to the previous utterances is done in this component (please refer to the <em class="italic">Anaphora resolution</em> section).</li>
				<li><strong class="bold">Dialog manager</strong>: Keeps the conversation<a id="_idIndexMarker681"/> memory to make a meaningful and coherent chat. You can think of this component as the dialog memory as this component usually holds a <strong class="bold">dialog state</strong>. The dialog state is the state of the conversation: the entities that have appeared so far, the intents that have appeared so far, and so on. Input to this component is the previous dialog state and the current user parsed with intent and entities. The output of this component is the new dialog state. </li>
				<li><strong class="bold">Answer generator</strong>: Given all the inputs from the previous stages, generates the system's answer to the user<a id="_idIndexMarker682"/> utterance.</li>
				<li><strong class="bold">Text-to-speech</strong>: This component <a id="_idIndexMarker683"/>generates a speech file (WAV or mp3) from the system's answer. </li>
			</ul>
			<p>Each of the components is trained and evaluated separately. For example, the speech-to-text component is trained on an annotated speech corpus (training is done on speech files and the corresponding transcriptions). The NLU component is trained on intent and an entity labeled<a id="_idIndexMarker684"/> corpus (similar to the datasets we used in <em class="italic">Chapters 6, 7, 8,</em> and <em class="italic">9</em>). In this chapter, we'll focus on the NLU component tasks. For text-based products, the first and last components are not necessary and are replaced with email or chat client integration.</p>
			<p>There's another paradigm that is called <strong class="bold">end-to-end spoken language understanding </strong>(<strong class="bold">SLU</strong>). In SLU architectures, the<a id="_idIndexMarker685"/> system is trained end to end, which means that the input to the system is a speech file and the output is the system response. Each approach has pros and cons; you can refer to the <em class="italic">References</em> section for more material.</p>
			<p>As the author of this book, I'm happy to present this chapter to you with my domain experience. I've been working in the conversational AI area for quite some time and tackle challenges of language and speech processing every day for our product. Me and my colleagues are building the world's first driver digital assistant, Chris (<em class="italic">Tips &amp; Tricks: How to talk to Chris – basic voice commands</em>, <a href="https://www.youtube.com/watch?v=Qwnjszu3exY">https://www.youtube.com/watch?v=Qwnjszu3exY</a>). Chris can make calls, answer incoming calls, read and write WhatsApp and text messages, play music, navigate, and make small talk. Here is Chris:</p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="Images/Figure_10_1.jpg" alt="Figure 10.1 – In-car voice assistant Chris (this is the product that the author is working on" width="713" height="733"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – In-car voice assistant Chris (this is the product that the author is working on)</p>
			<p>As we see from the <a id="_idIndexMarker686"/>preceding examples, conversational AI has become a hot topic recently. As an NLP professional, it's quite likely that you'll work for a conversational product or work in a related area such as speech recognition, text-to-speech, or question answering. Techniques presented in this chapter such as intent recognition, entity extraction, and anaphora resolution are applicable to a wide set of NLU problems as well. Let's dive into the technical sections. We'll start by exploring the dataset that we'll use throughout this chapter. </p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor177"/>Getting to know the dataset</h2>
			<p>In <em class="italic">Chapters 6</em>, <em class="italic">7</em>, <em class="italic">8</em>, and <em class="italic">9</em>, we worked on <a id="_idIndexMarker687"/>well-known real-world datasets for text classification and entity extraction purposes. In these chapters, we always explored our dataset as the very first task. The main point of data exploration is to understand the nature of the dataset text in order to develop strategies in our algorithms that can tackle this dataset. If we recall from <a href="B16570_06_Final_JM_ePub.xhtml#_idTextAnchor103"><em class="italic">Chapter 6</em></a><em class="italic">, Putting Everything Together: Semantic Parsing with spaCy</em>, the following are the main points we should keep an eye on during our exploration:</p>
			<ul>
				<li>What kind of utterances there are? Are utterances short text or full sentences or long paragraphs or documents? What is the average utterance length?</li>
				<li>What sort of entities does the corpus include? Person names, organization names, geographical locations, street names? Which ones do we want to extract?</li>
				<li>How is punctuation used? Is the text correctly punctuated or is no punctuation used at all? </li>
				<li>How are the grammatical rules followed? Is capitalization correct, and did the users follow the grammatical rules? Are there misspelled words?</li>
			</ul>
			<p>The previous datasets we used consisted of <strong class="source-inline">(text, class_label)</strong> pairs to be used in text classification tasks or <strong class="source-inline">(text, list_of_entities)</strong> pairs to be used in entity extraction tasks. In this chapter, we'll tackle a much more complicated task, chatbot design. Hence, the dataset will be more structured and more complicated. </p>
			<p>Chatbot design datasets are usually in JSON format to maintain the dataset structure. Here, structure means the following:</p>
			<ul>
				<li>Keeping the order of user and system utterances</li>
				<li>Marking slots of the user utterances</li>
				<li>Labeling the intent of the user utterances</li>
			</ul>
			<p>Throughout this chapter, we'll use Google Research's <strong class="bold">The Schema-Guided Dialogue</strong> dataset (<strong class="bold">SGD</strong>) (<a href="https://github.com/google-research-datasets/dstc8-schema-guided-dialogue">https://github.com/google-research-datasets/dstc8-schema-guided-dialogue</a>). This dataset consists of annotated user-virtual assistant interactions. The original dataset contains over 20,000 dialog segments in several areas, including restaurant reservations, movie reservations, weather queries, and travel ticket booking. Dialogs include utterances of user and virtual assistant turn by turn. In this chapter, we won't use all of<a id="_idIndexMarker688"/> this massive dataset; instead, we'll use a subset about restaurant reservations. </p>
			<p>Let's get started with downloading the dataset. You can download the dataset from the book's GitHub repository at <a href="https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/data/restaurants.json">https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/data/restaurants.json</a>. Alternatively, you can write the following code:</p>
			<p class="source-code"><strong class="bold">$ wget https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/data/restaurants.json</strong></p>
			<p>If you open the file with a text editor and look at the first few lines, you'll see the following:</p>
			<p class="source-code">{</p>
			<p class="source-code">  "dialogue_id": "1_00000",</p>
			<p class="source-code">  "turns": [</p>
			<p class="source-code">    {</p>
			<p class="source-code">      "speaker": "USER",</p>
			<p class="source-code">      "utterance": "I am feeling hungry so I would like to find a place to eat.",</p>
			<p class="source-code">      "slots": [],</p>
			<p class="source-code">      "intent": "FindRestaurants"</p>
			<p class="source-code">     },</p>
			<p class="source-code">     {</p>
			<p class="source-code">       "speaker": "SYSTEM",</p>
			<p class="source-code">       "utterance": "Do you have a specific which you want the eating place to be located at?",</p>
			<p class="source-code">        "slots": []</p>
			<p class="source-code">       }</p>
			<p>First of all, the dataset consists of dialog segments and each dialog segment has a <strong class="source-inline">dialogue_id</strong> instance. Each dialog segment is an ordered list of turns and each turn belongs to the user or to the system. A <strong class="bold">dialog segment</strong> contains <a id="_idIndexMarker689"/>multiple turns; here, the <strong class="source-inline">turns</strong> field is a list of the user/system turns. Each element of the <strong class="source-inline">turns</strong> list is a turn. One turn consists of a speaker (user or system), the speaker's utterance, a list of slots, and an intent for<a id="_idIndexMarker690"/> the user utterances.</p>
			<p>Here are some example user utterances from the dataset:</p>
			<p class="source-code">Hi. I'd like to find a place to eat.</p>
			<p class="source-code">I want some ramen, I'm really craving it. Can you find me an afforadable place in Morgan Hill?</p>
			<p class="source-code">I would like for it to be in San Jose.</p>
			<p class="source-code">Yes, please make a reservation for me.</p>
			<p class="source-code">No, Thanks</p>
			<p class="source-code">Hi i need a help, i am very hungry, I am looking for a restaurant</p>
			<p class="source-code">Yes, on the 7th for four people.</p>
			<p class="source-code">No. Can you change it to 1 pm on the 9th?</p>
			<p class="source-code">Yes. What is the phone number? Can I buy alcohol there? </p>
			<p>As we see from these example utterances, capital letters and punctuation are used in the user utterances. Users can make typos, such as the word <strong class="source-inline">afforadable</strong> in the second sentence. There are some grammatical errors as well, such as the wrong usage of a capital letter in the word <strong class="source-inline">Thanks</strong> of the fifth sentence. Another capitalization mistake occurs in the sixth sentence, where the pronoun <em class="italic">I</em> is written as <strong class="source-inline">i</strong> twice.</p>
			<p>Also, one utterance can contain multiple sentences. The first utterance starts with a greeting sentence and the last two sentences start with an affirmative or negative answer sentence each. The fourth sentence also starts with a <strong class="source-inline">Yes</strong>, but not as a standalone sentence; instead it's separated from the second sentence with a comma. </p>
			<p>Intent recognition for multiple sentence utterances is a point we need to pay attention to in general – these types of utterances can contain multiple intents. Also, answer generation for multi-sentence utterances is a bit tricky; sometimes we need to generate only one answer (such as for the second sentence in the preceding code) or sometimes we need to generate an answer per each user sentence (such as for the last sentence in the preceding code).    </p>
			<p>This is a dataset for<a id="_idIndexMarker691"/> restaurant reservations, so naturally it includes some slots in user utterances such as the location, cuisine, time, date, number of people, and so on. Our dataset includes the following slots:</p>
			<p class="source-code">city</p>
			<p class="source-code">cuisine</p>
			<p class="source-code">date</p>
			<p class="source-code">phone_number</p>
			<p class="source-code">restaurant_name</p>
			<p class="source-code">street_address</p>
			<p class="source-code">time</p>
			<p>Here are some example sentences with the preceding slot types and their values:</p>
			<p class="source-code">Find me Ethiopian/cuisine cuisine in Berkeley/city.</p>
			<p class="source-code">The phone number is 707-421-0835/phone_number. Your reservation is confirmed.</p>
			<p class="source-code">No, change the time to 7 pm/time and for one person only.</p>
			<p class="source-code">No, change it on next friday/date.</p>
			<p>Now, we come to the class labels for the intent recognition and the distribution of these class labels. Here's the class labels distribution:</p>
			<p class="source-code">552 FindRestaurants</p>
			<p class="source-code">625 ReserveRestaurant</p>
			<p class="source-code">56  NONE</p>
			<p><strong class="source-inline">NONE</strong> is a special class label for utterances that indicate the end of a conversation or just saying thank you. This class of utterances is not related to restaurant reservation in general. Utterances that intend to list restaurants and get some information are labeled with the class label <strong class="source-inline">FindRestaurants</strong>, and utterances that include the intent to make a booking are labeled with <strong class="source-inline">ReserveRestaurants</strong>. Let's see some example utterances of each class:</p>
			<p class="source-code">No, Thanks  NONE</p>
			<p class="source-code">No, thank you very much. NONE</p>
			<p class="source-code">Nothing much. I'm good.  NONE</p>
			<p class="source-code">I am feeling hungry so I would like to find a place to eat. FindRestaurants</p>
			<p class="source-code">Hi i need a help, i am very hungry, I am looking for a restaurant  FindRestaurants</p>
			<p class="source-code">Ok, What is the address? How pricey are they? FindRestaurants</p>
			<p class="source-code">Please can you make the reservation ReserveRestaurant</p>
			<p class="source-code">That's good. Do they serve liquor and what is there number? ReserveRestaurant</p>
			<p class="source-code">Thank you so much for setting that up. ReserveRestaurant</p>
			<p>We notice that the <a id="_idIndexMarker692"/>follow-up sentences, such as utterances 6, 8, and 9, are marked with the intents <strong class="source-inline">FindRestaurants</strong> and <strong class="source-inline">ReserveRestaurant</strong>. These utterances don't contain the intents of finding/reserving directly, but they continue the dialog about finding/reserving a restaurant and still make queries about the restaurant/reservation. Hence, although there are no explicit actions of finding/reserving stated in these utterances, still the intents are to find/reserve a restaurant. </p>
			<p>That's it – we collected enough insights about our dataset using the preliminary work of this section. With these insights, we're ready to build our NLU pipeline. We'll start with extracting the user utterance entities. </p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor178"/>Entity extraction</h1>
			<p>In this section, we'll implement the <a id="_idIndexMarker693"/>first step of our chatbot NLU pipeline and extract entities from the dataset utterances. The following are the entities marked in our dataset:</p>
			<p class="source-code">city</p>
			<p class="source-code">date</p>
			<p class="source-code">time</p>
			<p class="source-code">phone_number</p>
			<p class="source-code">cuisine</p>
			<p class="source-code">restaurant_name</p>
			<p class="source-code">street_address</p>
			<p>To extract the entities, we'll use the spaCy NER model and the spaCy <strong class="source-inline">Matcher</strong> class. Let's get started by extracting the <strong class="source-inline">city</strong> entities.</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor179"/>Extracting city entities</h2>
			<p>We'll first extract the <strong class="source-inline">city</strong> entities. We'll get<a id="_idIndexMarker694"/> started by recalling some information about the spaCy NER model and entity labels from <a href="B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055"><em class="italic">Chapter 3</em></a><em class="italic">, Linguistic Features</em>, and <a href="B16570_06_Final_JM_ePub.xhtml#_idTextAnchor103"><em class="italic">Chapter 6</em></a><em class="italic">, Putting Everything Together: Semantic Parsing with spaCy</em>:  </p>
			<ul>
				<li>First, we recall that the spaCy named entity label for cities and countries is <strong class="source-inline">GPE</strong>. Let's ask spaCy to explain what <strong class="source-inline">GPE</strong> label corresponds to once again:<p class="source-code">import spacy</p><p class="source-code">nlp = spacy.load("en_core_web_md")</p><p class="source-code">spacy.explain("GPE")</p><p class="source-code">'Countries, cities, states'</p></li>
				<li>Secondly, we also recall that we can access entities of a <strong class="source-inline">Doc</strong> object via the <strong class="source-inline">ents</strong> property. We can find all entities in an utterance that are labeled by the spaCy NER model as follows:<p class="source-code">import spacy</p><p class="source-code">nlp = spacy.load("en_core_web_md")</p><p class="source-code">doc = nlp("Can you please confirm that you want to book a table for 2 at 11:30 am at the Bird restaurant in Palo Alto for today")</p><p class="source-code">doc.ents</p><p class="source-code">(2, 11:30 am, Bird, Palo Alto, today)</p><p class="source-code">for ent in doc.ents:</p><p class="source-code">  print(ent.text, ent.label_) </p><p class="source-code">2 CARDINAL</p><p class="source-code">11:30 am TIME</p><p class="source-code">Bird PRODUCT</p><p class="source-code">Palo Alto GPE</p><p class="source-code">today DATE</p></li>
			</ul>
			<p>In this code segment, we listed <a id="_idIndexMarker695"/>all named entities of this utterance by calling <strong class="source-inline">doc.ents</strong>. Then, we examined the entity labels by calling <strong class="source-inline">ent.label_</strong>. Examining the output, we see that this utterance contains five entities – one cardinal number entity (<strong class="source-inline">2</strong>), one <strong class="source-inline">TIME</strong> entity (<strong class="source-inline">11:30 am</strong>), one <strong class="source-inline">PRODUCT</strong> entity (<strong class="source-inline">Bird</strong>, which is not an ideal label for a restaurant), one <strong class="source-inline">CITY</strong> entity (<strong class="source-inline">Palo Alto</strong>), and one <strong class="source-inline">DATE</strong> entity (<strong class="source-inline">today</strong>). The <strong class="source-inline">GPE</strong> type entity is what we're looking for; <strong class="source-inline">Palo Alto</strong> is a city in the US and hence is labeled by the spaCy NER model as <strong class="source-inline">GPE</strong>.</p>
			<p>The script at <a href="https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/extract_city_ents.py">https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/extract_city_ents.py</a> in the book's GitHub outputs all the utterances that include a city entity together with the city entities. From the output of this script, we can see that the spaCy NER model performs very well on this corpus for <strong class="source-inline">GPE</strong> entities. We don't need to train the spaCy NER model with our custom data.</p>
			<p>We extracted city entities, and our chatbot knows in which city to look for a restaurant. Now, we'll extract dates and times to allow our chatbot to make a real reservation.</p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor180"/>Extracting date and time entities</h2>
			<p>Extracting <strong class="source-inline">DATE</strong> and <strong class="source-inline">TIME</strong> entities is <a id="_idIndexMarker696"/>similar to extracting <strong class="source-inline">CITY</strong> entities, which we saw in the previous section. We'll again go over the corpus utterances and see how successful the spaCy NER model is at extracting <strong class="source-inline">DATE</strong> and <strong class="source-inline">TIME</strong> entities from our corpus.</p>
			<p>Let's see some example utterances from the corpus:</p>
			<p class="source-code">import spacy</p>
			<p class="source-code">nlp = spacy.load("en_core_web_md")</p>
			<p class="source-code">sentences = [</p>
			<p class="source-code">   "I will be eating there at 11:30 am so make it for then.",</p>
			<p class="source-code">   "I'll reach there at 1:30 pm.",</p>
			<p class="source-code">   "No, change it on next friday",</p>
			<p class="source-code">   "Sure. Please confirm that the date is now next Friday and for 1 person.",</p>
			<p class="source-code">   "I need to make it on Monday next week at half past 12 in the afternoon.",</p>
			<p class="source-code">   "A quarter past 5 in the evening, please."</p>
			<p class="source-code">]</p>
			<p>In the following code, we'll extract the entities of these example utterances:</p>
			<p class="source-code">for sent in sentences:</p>
			<p class="source-code">   doc = nlp(sent)</p>
			<p class="source-code">   ents = doc.ents</p>
			<p class="source-code">   print([(ent.text, ent.label_) for ent in ents])</p>
			<p class="source-code">[('11:30 am', 'TIME')]</p>
			<p class="source-code">[('1:30 pm', 'TIME')]</p>
			<p class="source-code">[('next friday', 'DATE')]</p>
			<p class="source-code">[('next Friday', 'DATE'), ('1', 'CARDINAL')]</p>
			<p class="source-code">[('Monday next week', 'DATE'), ('half past 12', 'DATE')]</p>
			<p class="source-code">[('A quarter past 5', 'DATE')]</p>
			<p class="source-code">[('the evening', 'TIME'), ('4:45', 'TIME')]</p>
			<p>Looks good! The output looks quite successful:</p>
			<ul>
				<li>The time entities <strong class="source-inline">11:30 am</strong> and <strong class="source-inline">1:30 pm</strong> of the first and second sentences are extracted successfully.</li>
				<li>The <strong class="source-inline">DATE</strong> entities <strong class="source-inline">next friday</strong> and <strong class="source-inline">next Friday</strong> of the third and fourth sentences are<a id="_idIndexMarker697"/> extracted as well. Notice the first entity includes a typo: <strong class="source-inline">friday</strong> should be written as <em class="italic">Friday</em> – still, the spaCy NER model successfully extracted this entity.  </li>
				<li>The fifth sentence included both a <strong class="source-inline">DATE</strong> entity and a <strong class="source-inline">TIME</strong> entity. We can break the <strong class="source-inline">DATE</strong> entity <strong class="source-inline">Monday next week</strong> into two parts: <strong class="source-inline">Monday</strong> – a weekday and <strong class="source-inline">next week</strong> – a relative date (the exact date depends on the date of the utterance). This entity consists of two noun phrases: <strong class="source-inline">Monday</strong> (noun) and <strong class="source-inline">next week</strong> (adjective noun). spaCy can handle such multiword entities. The time entity, <strong class="source-inline">half past 12</strong>, of this utterance is also a multiword entity. This entity consists of a noun (<strong class="source-inline">half</strong>), a preposition (<strong class="source-inline">past</strong>), and a number (<strong class="source-inline">12</strong>). </li>
				<li>The same goes for the sixth utterance's multiword <strong class="source-inline">TIME</strong> entity, <strong class="source-inline">A quarter past 5</strong>. Here is the dependency tree of this entity:</li>
			</ul>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="Images/Figure_10_2.jpg" alt="Figure 10.2 – Dependency tree of the time entity &quot;A quarter past 5&quot;&#13;&#10;" width="657" height="182"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – Dependency tree of the time entity "A quarter past 5"</p>
			<p>The preceding examples<a id="_idIndexMarker698"/> look quite good indeed, but how about the following utterances:</p>
			<p class="source-code">sentences = [</p>
			<p class="source-code">   "Have a great day.",</p>
			<p class="source-code">   "Have a nice day.",</p>
			<p class="source-code">   "Have a good day",</p>
			<p class="source-code">   "Have a wonderful day.",</p>
			<p class="source-code">   "Have a sunny and nice day"</p>
			<p class="source-code">]</p>
			<p class="source-code">for sent in sentences:</p>
			<p class="source-code">   doc = nlp(sent) </p>
			<p class="source-code">   ents = doc.ents </p>
			<p class="source-code">   print([(ent.text, ent.label_) for ent in ents])  </p>
			<p class="source-code">[('a great day', 'DATE')]</p>
			<p class="source-code">[('a nice day', 'DATE')]</p>
			<p class="source-code">[]</p>
			<p class="source-code">[]</p>
			<p class="source-code">[]</p>
			<p>Oops-a-daisy – looks like we <a id="_idIndexMarker699"/>have some <strong class="bold">false positives</strong> here. The spaCy NER model labeled some phrases, including the word <strong class="source-inline">day</strong>, as date entities incorrectly. What can we do here? </p>
			<p>Fortunately, these <a id="_idIndexMarker700"/>false matches don't form a pattern such as <strong class="bold">determiner adjective day</strong>, because<a id="_idIndexMarker701"/> the word sequences <strong class="source-inline">a good day</strong> and <strong class="source-inline">a wonderful day</strong> of the third and fourth sentence are not labeled as entities. Only the word sequences <strong class="source-inline">a great day</strong> and <strong class="source-inline">a nice day</strong> are labeled as entities. Then, we can just filter the spaCy NER results with the following two patterns:</p>
			<p class="source-code">sentence = 'Have a nice day.'</p>
			<p class="source-code">doc = nlp(sentence)</p>
			<p class="source-code">wrong_matches = ["a great day", "a nice day"]</p>
			<p class="source-code">date_ents = [ent for ent in doc.ents if ent.label_ == "DATE"]</p>
			<p class="source-code">date_ents = list(filter(lambda e: e.text not in wrong_matches, date_ents))</p>
			<p class="source-code">date_ents</p>
			<p class="source-code">[]</p>
			<p>The preceding code block performs the following steps:</p>
			<ol>
				<li>First, we defined a list of phrases that we don't want to come up as <strong class="source-inline">DATE</strong> entities.</li>
				<li>We extracted the <strong class="source-inline">DATE</strong> entities of the Doc object on the third line by iterating over all entities of <strong class="source-inline">doc</strong> and picking the entities whose labels were <strong class="source-inline">DATE</strong>.</li>
				<li>In the next line, we filtered the entities that didn't appear in the <strong class="source-inline">wrong_matches</strong> list. </li>
				<li>We printed the result. As expected, the final result of the <strong class="source-inline">date</strong> entity is an empty list.</li>
			</ol>
			<p>Great, we have extracted <strong class="source-inline">DATE</strong> and <strong class="source-inline">TIME</strong> entities along with <strong class="source-inline">CITY</strong> entities. For all the three entity types, we used the spaCy NER model directly, because spaCy NER recognizes date, time, and location entities. How about <strong class="source-inline">phone_number</strong> entities? SpaCy NER doesn't include<a id="_idIndexMarker702"/> such a label at all. So, we'll use some <strong class="source-inline">Matcher</strong> class tricks to handle this entity type. Let's extract the phone numbers.</p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor181"/>Extracting phone numbers</h2>
			<p>We had some <strong class="source-inline">Matcher</strong> class practice <a id="_idIndexMarker703"/>on entities that include numbers in <a href="B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069"><em class="italic">Chapter 4</em></a><em class="italic">, Rule-Based Matching</em>. We can also recall from <a href="B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069"><em class="italic">Chapter 4</em></a><em class="italic">, Rule-Based Matching</em>, that matching number type entities can be indeed quite tricky; extracting telephone numbers especially requires attention. Phone numbers can come in different formats, with dashes (212-44-44), area codes ((312) 790 12 31), country and area codes (+49 30 456 222), and the number of digits differing from country to country. As a result, we usually examine the following points:</p>
			<ul>
				<li>How many country formats are the corpus phone number entities written in?</li>
				<li>How are the digit blocks separated – with a dash, or whitespace, or both?</li>
				<li>Is there an area code block in some phone numbers?</li>
				<li>Is there a country code block in some phone numbers?</li>
				<li>Are the country code blocks preceded with a + or 00, or are both formats used?</li>
			</ul>
			<p>Let's examine some of our phone number entities, then:</p>
			<p class="source-code">You can call them at 415-775-1800. And they do not serve alcohol.</p>
			<p class="source-code">Their phone number is 408-374-3400 and they don't have live music.</p>
			<p class="source-code">Unfortunately no, they do not have live music, however here is the number: 510-558-8367.</p>
			<p>All the phone-type entities occur in system utterances. The chatbot fetches phone numbers of restaurants and provides them to the users. The chatbot formed phone number entities by placing a dash between the digit blocks. Also, all the phone numbers are in USA phone number format. Hence the phone number format is uniform and is of the form <strong class="source-inline">ddd-ddd-dddd</strong>. This is very good for defining a Matcher pattern. We can define only one pattern and it matches all the phone number entities.</p>
			<p>Let's first see how an example phone number tokenizes:</p>
			<p class="source-code">doc= nlp("The phone number is 707-766-7600.")</p>
			<p class="source-code">[token for token in doc]</p>
			<p class="source-code">[The, phone, number, is, 707, -, 766, -, 7600, .] </p>
			<p>Each digit block is tokenized as one token and each dash character is tokenized as one token as well. Hence, in our <a id="_idIndexMarker704"/>Matcher pattern, we'll look for a sequence of five tokens: a three-digit number, a dash, a three-digit number again, a dash again, and finally a four-digit number. Then, our Matcher pattern should look like this:</p>
			<p class="source-code">{"SHAPE": "ddd"}, {"TEXT": "-"}, {"SHAPE": "ddd"}, {"TEXT": "-"}, {"SHAPE": "dddd"}</p>
			<p>If you recall from <a href="B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069"><em class="italic">Chapter 4</em></a><em class="italic">, Rule-Based Matching</em>, the <strong class="source-inline">SHAPE</strong> attribute refers to the token shape. The token shape represents the shape of the characters: <strong class="source-inline">d</strong> means a digit, <strong class="source-inline">X</strong> means a capital character, and <strong class="source-inline">x</strong> means a lowercase character. Hence <strong class="source-inline">{"SHAPE": "ddd"}</strong> means a token that consists of three digits. This pattern will match five tokens of the form <strong class="source-inline">ddd-ddd-dddd</strong>. Let's try our brand-new pattern on a corpus utterance:</p>
			<p class="source-code">from spacy.matcher import Matcher</p>
			<p class="source-code">matcher = Matcher(nlp.vocab)</p>
			<p class="source-code">pattern = [{"SHAPE": "ddd"}, {"TEXT": "-"}, {"SHAPE": "ddd"}, {"TEXT": "-"}, {"SHAPE": "dddd"}]</p>
			<p class="source-code">matcher.add("usPhoneNum", [pattern])</p>
			<p class="source-code">doc= nlp("The phone number is 707-766-7600.")</p>
			<p class="source-code">matches = matcher(doc)</p>
			<p class="source-code">for mid, start, end in matches:</p>
			<p class="source-code">    print(doc[start:end])</p>
			<p class="source-code">707-766-7600</p>
			<p>Voila! Our new pattern matched a phone number type entity as expected! Now, we'll deal with the cuisine type so that our chatbot can make a reservation. Let's see how to extract the cuisine type.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor182"/>Extracting cuisine types</h2>
			<p>Extracting cuisine types is<a id="_idIndexMarker705"/> much easier than extracting a number of people or phone types; indeed, it's similar to extracting city entities. We can use a spaCy NER label directly for cuisine types – <strong class="source-inline">NORP</strong>. The <strong class="source-inline">NORP</strong> entity label refers to ethnic or political groups:</p>
			<p class="source-code">spacy.explain("NORP")</p>
			<p class="source-code">'Nationalities or religious or political groups'</p>
			<p>Fortunately, cuisine names in our corpus coincide with nationalities. So, cuisine names are labeled as NORP by spaCy's NER. </p>
			<p>First, let's have a look at some example utterances:</p>
			<p class="source-code">Is there a specific cuisine type you enjoy, such as Mexican, Italian or something else?</p>
			<p class="source-code">I usually like eating the American type of food.</p>
			<p class="source-code">Find me Ethiopian cuisine in Berkeley.</p>
			<p class="source-code">I'm looking for a Filipino place to eat.</p>
			<p class="source-code">I would like some Italian food.</p>
			<p class="source-code">Malaysian sounds good right now.</p>
			<p>Let's extract the entities of these utterances and examine how spaCy's NER labels cuisine types as follows:</p>
			<p class="source-code">for sent in sentences:</p>
			<p class="source-code">  doc = nlp(sent</p>
			<p class="source-code">  [(ent.text, ent.label_) for ent in doc.ents] </p>
			<p class="source-code">[('Mexican', 'NORP'), ('Italian', 'NORP')]</p>
			<p class="source-code">[('American', 'NORP')]</p>
			<p class="source-code">[('Ethiopian', 'NORP'), ('Berkeley', 'GPE')]</p>
			<p class="source-code">[('Filipino', 'NORP')]</p>
			<p class="source-code">[('Italian', 'NORP')]</p>
			<p class="source-code">[('Malaysian', 'NORP')]</p>
			<p>Now, we are able to extract the city, date and time, number of people, and cuisine entities from user utterances. The result of the named entity extraction module we built here carries all the information the chatbot needs to provide to the reservation system. Here's an example utterance annotated with extracted entities:</p>
			<p class="source-code">I'd like to reserve an Italian place for 4 people by tomorrow 19:00 in Berkeley.</p>
			<p class="source-code">{</p>
			<p class="source-code">entities: {</p>
			<p class="source-code">   "cuisine": "Italian",</p>
			<p class="source-code">   "date": "tomorrow",</p>
			<p class="source-code">   "time": "19:00",</p>
			<p class="source-code">   "number_people": 4,</p>
			<p class="source-code">   "city": "Berkeley"</p>
			<p class="source-code">}</p>
			<p>Here, we completed the <a id="_idIndexMarker706"/>first part of our semantic parsing, extracting entities. A full semantic parse needs an intent too. Now, we'll move on to the next section and do intent recognition with TensorFlow and Keras.  </p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor183"/>Intent recognition</h1>
			<p><strong class="bold">Intent recognition</strong> (also called <strong class="bold">intent classification</strong>) is the task of classifying user utterances with <a id="_idIndexMarker707"/>predefined labels (intents). Intent classification is basically text<a id="_idIndexMarker708"/> classification. Intent classification is a well-known and common NLP task. GitHub and Kaggle host many intent classification datasets (please refer to the <em class="italic">References</em> section for the names of some example datasets).</p>
			<p>In real-world chatbot applications, we first determine the domain our chatbot has to function in, such as finance and banking, healthcare, marketing, and so on. Then we perform the following <a id="_idIndexMarker709"/>loop of actions:</p>
			<ol>
				<li value="1">We determine a set of intents we want to support and prepare a labeled dataset of <strong class="source-inline">(utterance, label)</strong> pairs. We train our intent classifier on this dataset.</li>
				<li>Next, we deploy our chatbot to the users and gather real user data.</li>
				<li>Then we examine how our chatbot performed on real user data. At this stage, usually, we spot some new intents and some utterances our chatbot failed to recognize. We extend our set of intents with the new intents, add the unrecognized utterances to our training set, and retrain our intent classifier.</li>
				<li>We go to <em class="italic">step 2</em> and perform <em class="italic">steps 2-3</em> until chatbot NLU quality reaches a good level of accuracy (&gt; 0.95)</li>
			</ol>
			<p>Our dataset is a real-world dataset; it contains typos and grammatical mistakes. While designing our intent classifiers – especially while doing pattern-based classification – we need to be robust to such mistakes. </p>
			<p>We'll do the intent recognition in two steps: pattern-based text classification and statistical text classification. We saw how to do statistical text classification with TensorFlow and Keras in <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a><em class="italic">, Text Classification with spaCy</em>. In this section, we'll work with Tensorflow and Keras again. Before that, we'll see how to design a pattern-based text classifier.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor184"/>Pattern-based text classification </h2>
			<p><strong class="bold">Pattern-based classification</strong> means <a id="_idIndexMarker710"/>classifying text by matching <a id="_idIndexMarker711"/>a predefined list of patterns to the text. We compare a precompiled list of patterns against the utterances and check whether there's a match.</p>
			<p>An immediate example<a id="_idIndexMarker712"/> is <strong class="bold">spam classification</strong>. If an email contains one of the patterns, such as <em class="italic">you won a lottery</em> and <em class="italic">I'm a Nigerian prince</em>, then this email should be classified as spam. Pattern-based classifiers are combined with <strong class="bold">statistical classifiers</strong> to boost the<a id="_idIndexMarker713"/> overall system accuracy. </p>
			<p>Contrary to statistical classifiers, pattern-based classifiers are easy to build. We don't need to put any effort into training a TensorFlow model at all. We will compile a list of patterns from our corpus and feed them to Matcher. Then, Matcher can look for pattern matches in utterances.</p>
			<p>To build a pattern-based classifier, we first need to collect some patterns. In this section, we'll classify utterances with the <strong class="source-inline">NONE</strong> label. Let's see some utterance examples first:</p>
			<p class="source-code">No, Thanks</p>
			<p class="source-code">No, thank you very much.</p>
			<p class="source-code">That is all thank you so much.</p>
			<p class="source-code">No, that is all.</p>
			<p class="source-code">Nope, that'll be all. Thanks!</p>
			<p class="source-code">No, that's okay.</p>
			<p class="source-code">No thanks. That's all I needed help with.</p>
			<p class="source-code">No. This should be enough for now.</p>
			<p class="source-code">No, thanks.</p>
			<p class="source-code">No, thanks a lot.</p>
			<p class="source-code">No, thats all thanks.</p>
			<p>By looking at these utterances, we see that the utterances with the <strong class="source-inline">NONE</strong> label follow some patterns:</p>
			<ul>
				<li>Most of the utterances start with <strong class="source-inline">No,</strong> or <strong class="source-inline">No.</strong>.</li>
				<li>Patterns of saying <em class="italic">thank you</em> are also quite common. The patterns <strong class="source-inline">Thanks</strong>, <strong class="source-inline">thank you</strong>, and <strong class="source-inline">thanks a lot</strong> occur in most of the utterances in the preceding code.</li>
				<li>Some helper<a id="_idIndexMarker714"/> phrases such as <strong class="source-inline">that is all</strong>, <strong class="source-inline">that'll be all</strong>, <strong class="source-inline">that's OK</strong>, and <strong class="source-inline">this should be enough</strong> are also commonly used.</li>
			</ul>
			<p>Based on this information, we can create three Matcher patterns as follows:</p>
			<p class="source-code">[{"LOWER": {"IN": ["no", "nope"]}}, {"TEXT": {"IN": [",", "."]}}]</p>
			<p class="source-code">[{"TEXT": {"REGEX": "[Tt]hanks?"}}, {"LOWER": {"IN": ["you", "a lot"]}, "OP": "*"}]</p>
			<p class="source-code">[{"LOWER": {"IN": ["that", "that's", "thats", "that'll",</p>
			<p class="source-code">"thatll"]}}, {"LOWER": {"IN": ["is", "will"]}, "OP": "*"}, {"LOWER": "all"}]</p>
			<p>Let's go over the patterns one by one:</p>
			<ul>
				<li>The first pattern matches token sequences <strong class="source-inline">no,</strong>, <strong class="source-inline">no.</strong>, <strong class="source-inline">nope,</strong>, <strong class="source-inline">nope.</strong>, <strong class="source-inline">No,</strong>, <strong class="source-inline">No.</strong>, <strong class="source-inline">Nope,</strong>, and <strong class="source-inline">Nope.</strong>. The first item matches two tokens <strong class="source-inline">no</strong> and <strong class="source-inline">nope</strong> either in capitals or small letters. The second item matches the punctuation marks <strong class="source-inline">,</strong> and <strong class="source-inline">.</strong>. </li>
				<li>The second pattern matches <strong class="source-inline">thank</strong>, <strong class="source-inline">thank you</strong>, <strong class="source-inline">thanks</strong>, and <strong class="source-inline">thanks a lot</strong>, either in capitals or small letters. The first item matches <strong class="source-inline">thank</strong> and <strong class="source-inline">thanks</strong> <strong class="source-inline">s?</strong>. In regex syntax, the <strong class="source-inline">s</strong> character is optional. The second item corresponds to the words <strong class="source-inline">you</strong> and <strong class="source-inline">a lot</strong>, which can possibly follow <strong class="source-inline">thanks?</strong>. The second item is optional; hence, the pattern matches <strong class="source-inline">thanks</strong> and <strong class="source-inline">thank</strong> as well. We used the operator <strong class="source-inline">OP: *</strong> to make the second item optional; recall from <a href="B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069"><em class="italic">Chapter 4</em></a><em class="italic">, Rule-Based Matching</em>, that Matcher supports operator syntax with different operators, such as <strong class="source-inline">*</strong> , <strong class="source-inline">+</strong>, and <strong class="source-inline">?</strong>.</li>
				<li>The third pattern matches the token sequences <strong class="source-inline">that is all</strong>, <strong class="source-inline">that's all</strong>, <strong class="source-inline">thats all</strong>, and so on. Notice that the first item includes some misspelled words, such as <strong class="source-inline">thats</strong> and <strong class="source-inline">thatll</strong>. We included the misspelled words on purpose, so the matching will be more robust to user typos.</li>
			</ul>
			<p>Different combinations of the preceding three patterns will match the utterances of the <strong class="source-inline">NONE</strong> class. You can<a id="_idIndexMarker715"/> try the patterns by adding them to a Matcher object and see how they match.</p>
			<p class="callout-heading">Pro tip</p>
			<p class="callout">While designing a rule-based system, always keep in mind that user data is not perfect. User data contains typos, grammatical mistakes, and wrong capitalization. Always keep robustness as a high priority and test your patterns on user data.</p>
			<p>We made a statistical model-free classifier by making use of some common patterns and classified one intent successfully. How about the other two intents – <strong class="source-inline">FindRestaurants</strong> and <strong class="source-inline">ReserveRestaurant</strong>? Utterances of these two intents are semantically much more complicated, so we cannot cope with pattern lists. We need statistical models to recognize these two intents. Let's go ahead and train our statistical text classifiers with TensorFlow and Keras.</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor185"/>Classifying text with a character-level LSTM</h2>
			<p>In this section, we'll train a <strong class="bold">character-level LSTM architecture</strong> for<a id="_idIndexMarker716"/> recognizing the intents. We already practiced text classification with TensorFlow and Keras in <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a><em class="italic">, Text Classification with spaCy</em>. Recall from this chapter that LSTMs are sequential models that process one input at one time step. We fed one word at each time step as follows:</p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="Images/Figure_10_3.jpg" alt="Figure 10.3 – Feeding one word to an LSTM at each time step&#13;&#10;" width="1174" height="495"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.3 – Feeding one word to an LSTM at each time step</p>
			<p>As we remarked in <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a><em class="italic">, Text Classification with spaCy</em>, LSTMs have an internal state (you can think of it as a memory), so LSTMs can model the sequential dependencies in the input sequence by holding past information in their internal state.</p>
			<p>In this section, we'll train a character-level LSTM. As the name suggests, we'll feed utterances character by character, not word by word. Each utterance will be represented as a sequence of characters. At each time step, we'll feed one character. This is what feeding the utterance from <em class="italic">Figure 10.3</em> looks like:</p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="Images/Figure_10_4.jpg" alt="Figure 10.4 – Feeding the first two words of the utterance &quot;I want Italian food&quot;&#13;&#10;" width="1462" height="389"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.4 – Feeding the first two words of the utterance "I want Italian food"</p>
			<p>We notice that the <a id="_idIndexMarker717"/>space character is fed as an input as well, because the space character is also a part of the utterance; for character-level tasks, there is no distinction between digits, spaces, and letters. </p>
			<p>Let's start building the Keras model. We'll skip the data preparation stage here. You can find the complete code in the intent classification notebook <a href="https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/Intent-classifier-char-LSTM.ipynb">https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter10/Intent-classifier-char-LSTM.ipynb</a>.</p>
			<p>We'll directly start with Keras' Tokenizer to create a vocabulary. Recall from <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a><em class="italic">, Text Classification with spaCy</em>, that we use Tokenizer to do the following:</p>
			<ul>
				<li>Create a vocabulary from the dataset sentences.</li>
				<li>Assign a token ID to each token of the dataset.</li>
				<li>Transform input sentences to token IDs.</li>
			</ul>
			<p>Let's see how to <a id="_idIndexMarker718"/>perform each step:</p>
			<ol>
				<li value="1">In <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a><em class="italic">, Text Classification with spaCy</em>, we tokenized the sentences into words and assigned token IDs to words. This time, we'll break the input sentence into its characters, then assign token IDs to characters. Tokenizer provides a parameter named <strong class="source-inline">char_level</strong>. Here's the Tokenizer code for character-level tokenization:<p class="source-code">from tensorflow.keras.preprocessing.text import Tokenizer</p><p class="source-code">tokenizer = Tokenizer(char_level=True, lower=True)</p><p class="source-code">tokenizer.fit_on_texts(utterances)</p></li>
				<li>The preceding code segment will create a vocabulary from the input characters. We used the <strong class="source-inline">lower=True</strong> parameter, so all characters of the input sentence are made lowercase by Tokenizer. After initializing the <strong class="source-inline">Tokenizer</strong> object on our vocabulary, we can now examine its vocabulary. Here are the first 10 items of the Tokenizer vocabulary:<p class="source-code">tokenizer.word_index</p><p class="source-code">{' ': 1, 'e': 2, 'a': 3, 't': 4, 'o': 5, 'n': 6, 'i': 7, 'r': 8, 's': 9, 'h': 10}</p><p>Just as with the word-level vocabulary, index <strong class="source-inline">0</strong> is reserved for a special token, which is the padding character. Recall from <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a><em class="italic">, Text Classification with spaCy</em>, that Keras cannot process variable-length sequences; each sentence in the dataset should be of the same length. Hence, we pad all sentences to a maximum length by appending a padding character to the sentence end or sentence start. </p></li>
				<li>Next, we'll convert each dataset sentence into token IDs. This is achieved by calling the <strong class="source-inline">texts_to_sequences</strong> method of Tokenizer:<p class="source-code">utterances = tokenizer.texts_to_sequences(utterances)</p><p class="source-code">utterances[0]</p><p class="source-code">[17, 2, 9, 25, 1, 7, 1, 22, 3, 6, 4, 1, 7, 4, 1, 5, 6, 1, 4, 10, 2, 1, 28, 28, 4, 10]</p></li>
				<li>Next, we'll pad all the input sentences to a length of <strong class="source-inline">150</strong>:<p class="source-code">MAX_LEN = 150</p><p class="source-code">utterances =\</p><p class="source-code"> pad_sequences(utterances, MAX_LEN, padding="post")</p><p>We're ready to feed our transformed dataset into our LSTM model. Our model is a simple yet very efficient one: we placed a dense layer on top of a bidirectional LSTM layer. Here's the model architecture:</p><p class="source-code">utt_in = Input(shape=(MAX_LEN,))</p><p class="source-code">embedding_layer =  Embedding(input_dim = len(tokenizer.word_index)+1, output_dim = 100, input_length=MAX_LEN)</p><p class="source-code">lstm =\</p><p class="source-code">Bidirectional(LSTM(units=100, return_sequences=False))</p><p class="source-code">utt_embedding = embedding_layer(utt_in)</p><p class="source-code">utt_encoded = lstm(utt_embedding)</p><p class="source-code">output = Dense(1, activation='sigmoid')(utt_encoded)</p><p class="source-code">model = Model(utt_in, output)</p><p>A bidirectional LSTM layer means two LSTMs stacked on top of each other. The first LSTM goes<a id="_idIndexMarker719"/> through the input sequence from left to right (in a forward direction) and the second LSTM goes through the input sequence right to left (in a backward direction). For each time step, the outputs of the forward LSTM and backward LSTM are concatenated to generate a single output vector. The following figure exhibits our architecture with a<a id="_idIndexMarker720"/> bidirectional LSTM: </p><div id="_idContainer149" class="IMG---Figure"><img src="Images/Figure_10_5.jpg" alt="Figure 10.5 – Bidirectional LSTM architecture &#13;&#10;" width="874" height="813"/></div><p class="figure-caption">Figure 10.5 – Bidirectional LSTM architecture </p></li>
				<li>Next, we compile our model and train it on our dataset by calling <strong class="source-inline">model.fit</strong>: <p class="source-code">model.compile(loss = 'binary_crossentropy', optimizer = "adam", metrics=["accuracy"])</p><p class="source-code">model.fit(utterances, labels, validation_split=0.1, epochs = 10, batch_size = 64)</p><p>Here, we compiled our model with the following:</p><p>a) Binary cross-entropy loss, because this is a binary classification task (we have two class labels).</p><p>b) The <strong class="source-inline">Adam</strong> optimizer, which will help the training procedure to run faster by arranging the size of the training steps. Please refer to the <em class="italic">References</em> section and <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a><em class="italic">, Text Classification with spaCy</em>, for more information about the <strong class="source-inline">Adam</strong> optimizer.</p><p>c) Accuracy as our success <a id="_idIndexMarker721"/>metric. Accuracy is calculated by comparing how often the predicted label is equal to the actual label. </p></li>
			</ol>
			<p>After fitting our model, our model gives a <strong class="source-inline">0.8226</strong> accuracy on the validation set, which is quite good. </p>
			<p>Now, only one question remains: why did we prefer to train a character-level model this time? Character-level models definitely have some advantages:</p>
			<ul>
				<li>Character-level models are highly misspelling-tolerant. Consider the misspelled word <em class="italic">charactr</em> – whether or not the <em class="italic">e</em> is missing does not affect the overall sentence semantics that much. For our dataset, we will benefit from this robustness, as we have already seen spelling mistakes by users in our dataset exploration.</li>
				<li>The vocabulary size is smaller than a word-level model. The number of characters in the alphabet (for any given language) is fixed and low (a maximum of 50 characters, including uppercase and lowercase letters, digits, and some punctuation); but the number of words in a language is much greater. As a result, model sizes can differ. The main difference lies at the embedding layer; an embedding table is of size <strong class="source-inline">(vocabulary_size, output_dim)</strong> (refer to the model code). Given that the output dimensions are the same, 50 rows is really small compared to thousands of rows.</li>
			</ul>
			<p>In this section, we were<a id="_idIndexMarker722"/> able to extract the user intent from the utterances. Intent recognition is the main step in understanding sentence semantics, but is there something more? In the next section, we'll dive into sentence-level and dialog-level semantics. More semantic parsing</p>
			<p>This is a <a id="_idIndexMarker723"/>section solely on chatbot NLU. In this section, we'll explore sentence-level semantic and syntactic information to generate a deeper understanding of the input utterances, as well as providing clues to answer generation. </p>
			<p>In the rest of this section, please think of the answer generation component as a black box. We provide the semantic parse of the sentence and it generates an answer based on this semantic parse. Let's start by dissecting sentence syntax and examining the subjects and objects of the utterances.</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor186"/>Differentiating subjects from objects</h2>
			<p>Recall from <a href="B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055"><em class="italic">Chapter 3</em></a><em class="italic">, Linguistic Features</em>, that a sentence has two important grammatical<a id="_idIndexMarker724"/> components: a <strong class="bold">subject</strong> and an <strong class="bold">object</strong>. The subject is the person or thing that performs the action given by the verb of the sentence:</p>
			<p class="source-code"><strong class="bold">Mary</strong> picked up her brother.</p>
			<p class="source-code"><strong class="bold">He</strong> was a great performer.</p>
			<p class="source-code"><strong class="bold">It</strong> was rainy on Sunday.</p>
			<p class="source-code"><strong class="bold">Who</strong> is responsible for this mess?</p>
			<p class="source-code"><strong class="bold">The cat</strong> is very cute.</p>
			<p class="source-code"><strong class="bold">Seeing you</strong> makes me happy.</p>
			<p>A subject can be a noun, a pronoun, or a noun phrase. </p>
			<p>An object is the thing or person on which the subject performs the action given by the verb. An object can be a noun, a pronoun, or a noun phrase too. Here are some examples:</p>
			<p class="source-code">Lauren lost <strong class="bold">her book</strong>.</p>
			<p class="source-code">I gave <strong class="bold">her</strong>/direct object <strong class="bold">a book</strong>/indirect object.</p>
			<p>So far, so good, but how does this information help us in our chatbot NLU? </p>
			<p>Extracting the subject <a id="_idIndexMarker725"/>and the object helps us understand the sentence structure, hence adding one more layer to the semantic parse of the sentence. Sentence subject and object information directly relates to answer generation. Let's see some examples of utterances from our dataset:</p>
			<p class="source-code">Where is this restaurant? </p>
			<p>The following figure shows the dependency parse of this utterance. The subject is the noun phrase <strong class="source-inline">this restaurant</strong>: </p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="Images/Figure_10_6.jpg" alt="Figure 10.6 – Dependency parse of the example utterance&#13;&#10;" width="504" height="172"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.6 – Dependency parse of the example utterance</p>
			<p>How can we generate an answer to this sentence? Obviously, the answer should have <strong class="source-inline">this restaurant</strong> (or the restaurant it refers to) as the subject. Some answers could be as follows:</p>
			<p class="source-code">The restaurant is located at the corner of 5th Avenue and 7th Avenue.</p>
			<p class="source-code">The Bird is located at the corner of 5Th Avenue and 7Th Avenue.</p>
			<p>What if the user puts <strong class="source-inline">this restaurant</strong> into the object role? Would the answer change? Let's take some example utterances from the dataset:</p>
			<p class="source-code">Do you know where this restaurant is?</p>
			<p class="source-code">Can you tell me where this restaurant is?</p>
			<p>Obviously, the user is asking<a id="_idIndexMarker726"/> about the address of the restaurant again. The system needs to give the restaurant address information. However, this time, the subject of these sentences is <strong class="source-inline">you</strong>:</p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="Images/Figure_10_7.jpg" alt="Figure 10.7 – Dependency parse of the first sentence&#13;&#10;" width="983" height="328"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.7 – Dependency parse of the first sentence</p>
			<p>This question's subject is <strong class="source-inline">you</strong>, so the answer can start with an <em class="italic">I</em>. This a question sentence, hence the answer can start with a <em class="italic">yes</em>/<em class="italic">no</em> or the answer can just provide the restaurant's address directly. The following sentences are all possible answers:</p>
			<p class="source-code">I can give the address. Here it is: 5th Avenue, no:2</p>
			<p class="source-code">Yes, of course. Here's the address: 5th Avenue, no:2</p>
			<p class="source-code">Here's the address: 5Th Avenue, no:2 </p>
			<p>The same phrase, <strong class="source-inline">the restaurant</strong>, being the subject or the object doesn't affect the user's intent, but it affects the sentence structure of the answer. Let's look at the information more <a id="_idIndexMarker727"/>systematically. The semantic parses of the preceding example sentences look as follows:</p>
			<p class="source-code">{</p>
			<p class="source-code">utt: "Where is this restaurant?",</p>
			<p class="source-code">intent: "FindRestaurants",</p>
			<p class="source-code">entities: [],</p>
			<p class="source-code">structure: {</p>
			<p class="source-code">    subjects: ["this restaurant"]</p>
			<p class="source-code">  }</p>
			<p class="source-code">}</p>
			<p class="source-code">{</p>
			<p class="source-code">utt: "Do you know where is this restaurant is?",</p>
			<p class="source-code">intent: "FindRestaurants",</p>
			<p class="source-code">entities: [],</p>
			<p class="source-code">structure: {</p>
			<p class="source-code">    subjects: ["you"]</p>
			<p class="source-code">  }</p>
			<p class="source-code">}</p>
			<p>When we feed these semantic parses to the answer generator module, this module can generate answers by taking the current utterance, the dialog history, the utterance intent, and the utterance's sentence structure (for the time being, only the sentence subject information) into account.</p>
			<p>Here, we extracted the utterance's sentence structure information by looking at the utterance's dependency tree. Can a dependency parse provide us with more information about the utterance? The answer is yes. We'll see how to extract the sentence type in the<a id="_idIndexMarker728"/> next section.</p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor187"/>Parsing the sentence type</h2>
			<p>In this section, we'll extract the sentence type of the user utterances. The grammar has four main<a id="_idIndexMarker729"/> sentence types, classified by their purpose:</p>
			<p class="source-code">Declarative: John saw Mary.</p>
			<p class="source-code">Interrogative: Can you go there?</p>
			<p class="source-code">Imperative: Go there immediately.</p>
			<p class="source-code">Exclamation: I'm excited too!</p>
			<p>Sentence types in chatbot NLU are a bit different; we classify sentences according to the POS tag of the subject and objects as well as the purpose. Here are some sentence types that are used in chatbot NLU:  </p>
			<p class="source-code">Question sentence</p>
			<p class="source-code">Imperative sentence</p>
			<p class="source-code">Wish sentence</p>
			<p>Let's examine each sentence type and its structural properties. We start with question sentences.</p>
			<h3>Question sentences</h3>
			<p>A question<a id="_idIndexMarker730"/> sentence is used when the user wants to ask something. A question sentence can be formed in two ways, either by using an interrogative pronoun or by placing a modal/auxiliary verb at the beginning of the sentence:</p>
			<p class="source-code"><strong class="bold">How</strong> did you go there?</p>
			<p class="source-code"><strong class="bold">Is</strong> this the book that you recommended?</p>
			<p>Hence, we also divide the question sentences into<a id="_idIndexMarker731"/> two classes, <strong class="bold">wh-questions</strong> and <strong class="bold">yes/no questions</strong>. As the name suggests, wh-questions start with a <strong class="bold">wh-word </strong>(a wh-word means an interrogative pronoun, such as where, what, who, and how) and yes/no questions are formed by using a modal/auxiliary verb. </p>
			<p>How will this classification help us? Syntactically, yes/no<a id="_idIndexMarker732"/> questions should be answered with a yes or no. Hence, if our chatbot NLU passes a yes/no question to the answer generation module, the answer generator should evaluate this information and generate an answer that starts with a yes/no. Wh-questions aim to get information about the subject or objects, hence the answer generator module should provide information about the sentence subject or objects. Consider the following utterance:</p>
			<p class="source-code">Where is this restaurant?</p>
			<p>This utterance <a id="_idIndexMarker733"/>generates the following dependency parse:</p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="Images/Figure_10_8.jpg" alt="Figure 10.8 – Dependency parse of the example wh-question&#13;&#10;" width="507" height="174"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.8 – Dependency parse of the example wh-question</p>
			<p>Here, the subject of the utterance is <strong class="source-inline">this restaurant</strong>; hence the answer generator should generate an answer by relating <strong class="source-inline">Where</strong> and <strong class="source-inline">this restaurant</strong>. How about the following utterance:</p>
			<p class="source-code">Which city is this restaurant located?</p>
			<p>The dependency parse of this utterance looks as follows:</p>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="Images/Figure_10_9.jpg" alt="Figure 10.9 -- Dependency parse of the example wh-question&#13;&#10;" width="1083" height="250"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.9 -- Dependency parse of the example wh-question</p>
			<p>Here, the sentence <a id="_idIndexMarker734"/>structure is a bit different. <strong class="source-inline">Which city</strong> is the subject of the sentence and <strong class="source-inline">this restaurant</strong> is the subject of the clause. Here, the answer generation module should generate an answer by relating <strong class="source-inline">which city</strong> and <strong class="source-inline">this restaurant</strong>. </p>
			<p>We will now move on to imperative sentence type. </p>
			<h3>Imperative sentence</h3>
			<p><strong class="bold">Imperative sentences</strong> occur quite <a id="_idIndexMarker735"/>frequently in chatbot user utterances. An imperative sentence is formed by placing the main verb at the beginning of the sentence. Here are some utterance examples from our dataset:</p>
			<p class="source-code">Find me Ethiopian cuisine in Berkeley.</p>
			<p class="source-code">Find me a sushi place in Alameda.</p>
			<p class="source-code">Find a place in Vallejo with live music.</p>
			<p class="source-code">Please reserve me at 6:15 in the evening.</p>
			<p class="source-code">Reserve for six in the evening.</p>
			<p class="source-code">Reserve it for half past 1 in the afternoon.</p>
			<p>As we see, imperative utterances occur quite a lot in user utterances, because they're succinct and to-the-point. We can spot these types of sentences by looking at the POS tags of the words: either the first word is a verb or the sentence starts with <em class="italic">please</em> and the second word is a verb. The following Matcher patterns match imperative utterances: </p>
			<p class="source-code">[{"POS": "VERB, "IS_SENT_START": True}]</p>
			<p class="source-code">[{"LOWER": "please", IS_SENT_START: True}, {"POS": "VERB"}]</p>
			<p>How would the answer generator process these types of sentences? Imperative sentences usually include the syntactic and semantic elements to generate an answer; the main verb provides<a id="_idIndexMarker736"/> the action and is usually followed by a list of objects. Here's an example parse for the utterance <strong class="source-inline">Find me Ethiopian cuisine in Berkeley</strong>:</p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="Images/Figure_10_10.jpg" alt="Figure 10.10 – Dependency parse of the example utterance&#13;&#10;" width="877" height="249"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.10 – Dependency parse of the example utterance</p>
			<p>From the figure, we can see the <a id="_idIndexMarker737"/>syntactic components of this sentence as <strong class="source-inline">Find</strong> (the action), <strong class="source-inline">Ethiopian cuisine</strong> (an object), and <strong class="source-inline">Berkeley</strong> (an object). These components provide a clear template to the answer generator for generating an answer to this utterance: the answer generator should ask the restaurants database for matches of <strong class="source-inline">Ethiopian cuisine</strong> and <strong class="source-inline">Berkeley</strong> and list the matching restaurants.</p>
			<p>Now we move on to the next sentence type, wish sentences. Let's see look at sentences in detail.</p>
			<h3>Wish sentences</h3>
			<p><strong class="bold">Wish sentences</strong> are semantically similar to imperative sentences. The difference is syntactic: wish sentences start with phrases such as <em class="italic">I'd like to</em>, <em class="italic">Can I</em>, <em class="italic">Can you</em>, and <em class="italic">May I</em>, pointing to a wish. Here are some examples from our dataset:</p>
			<p class="source-code">I'd like to make a reservation.</p>
			<p class="source-code">I would like to find somewhere to eat, preferably Asian food.</p>
			<p class="source-code">I'd love some Izakaya type food.</p>
			<p class="source-code">Can you find me somewhere to eat in Dublin?</p>
			<p class="source-code">Can we make it three people at 5:15 pm?</p>
			<p class="source-code">Can I make a reservation for 6 pm?</p>
			<p>Extracting the verb and the objects is similar to what we do for imperative sentences, hence the semantic<a id="_idIndexMarker738"/> parse is quite similar.</p>
			<p>After extracting the sentence type, we can include it into our semantic parse result as follows:</p>
			<p class="source-code">{</p>
			<p class="source-code">utt: "Where is this restaurant?",</p>
			<p class="source-code">intent: "FindRestaurants",</p>
			<p class="source-code">entities: [],</p>
			<p class="source-code">structure: {</p>
			<p class="source-code">    sentence_type: "wh-question",</p>
			<p class="source-code">    subjects: ["this restaurant"]</p>
			<p class="source-code">  }</p>
			<p class="source-code">}</p>
			<p>Now, we have a rich semantic and syntactic representation of the input utterance. In the next section, we'll go one step beyond the sentence-level semantics and go through the dialog-level semantics. Let's move on to the next section and see how we tackle dialog-level semantics.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor188"/>Anaphora resolution</h2>
			<p>In this section, we'll explore the linguistic concepts of <strong class="bold">anaphora</strong> and <strong class="bold">cohesion</strong>. In linguistics, cohesion <a id="_idIndexMarker739"/>means the grammatical links that glue a text together semantically. This text can be a single <a id="_idIndexMarker740"/>sentence, a paragraph, or a dialog segment. Consider the following two sentences:</p>
			<p class="source-code">I didn't like this dress. Can I see another <strong class="bold">one</strong></p>
			<p>Here, the word <strong class="source-inline">one</strong> refers to the dress from the first sentence. A human can resolve this link easily. It's not so straightforward for software programs, though. </p>
			<p>Also, consider the following dialog segment:</p>
			<p class="source-code">Where are you going?</p>
			<p class="source-code">To my grandma's.</p>
			<p>The second sentence is completely understandable, though some parts of the sentence are missing:</p>
			<p class="source-code"><strong class="bold">I'm going</strong> to my grandma's <strong class="bold">house</strong>. </p>
			<p>In written and spoken language, we <a id="_idIndexMarker741"/>use such <strong class="bold">shortcuts</strong> every day. However, resolving such shortcuts needs attention while programming, especially in chatbot NLU. Consider these utterances and dialog segments from our dataset:</p>
			<p>Example 1:</p>
			<p class="source-code">- Do you want to make<strong class="bold"> a reservation</strong>?</p>
			<p class="source-code">- Yes, I want to make <strong class="bold">one</strong>.</p>
			<p>Example 2:</p>
			<p class="source-code">- I've found <strong class="bold">2 Malaysian restaurants</strong> in Cupertino. Merlion Restaurant &amp; Bar is one.</p>
			<p class="source-code">- What is the other <strong class="bold">one</strong>?</p>
			<p>Example 3:</p>
			<p class="source-code">- There's another restaurant in San Francisco that's called Bourbon Steak Restaurant.</p>
			<p class="source-code">- Yes, I'm interested in that <strong class="bold">one</strong>.</p>
			<p>Example 4: </p>
			<p class="source-code">- Found 3 results, <strong class="bold">Asian pearl Seafood Restaurant</strong> is the best one in Fremont city, hope you like it.</p>
			<p class="source-code">- Yes, I like the <strong class="bold">same</strong>.</p>
			<p>Example 5: </p>
			<p class="source-code">- Do you have <strong class="bold">a specific</strong> which you want the <strong class="bold">eating place</strong> to be located at?</p>
			<p class="source-code">- I would like for <strong class="bold">it</strong> to be in San Jose.</p>
			<p>Example 6:</p>
			<p class="source-code">- Would you like <strong class="bold">a reservation</strong>?</p>
			<p class="source-code">- Yes make <strong class="bold">it</strong> for March 10th.</p>
			<p>All the highlighted parts of the preceding sentences and dialogs are examples of a linguistic event named <strong class="bold">anaphora</strong>. Anaphora<a id="_idIndexMarker742"/> means to look backward linguistically. An anaphora consists of two phrases: a phrase that refers to a phrase previously used in the context and the phrase that is referred to. Commonly used anaphoric words are <strong class="source-inline">one</strong>, <strong class="source-inline">more</strong>, <strong class="source-inline">same</strong>, <strong class="source-inline">it</strong>, and so on. Anaphora resolution means to resolve exactly the phrases anaphoric words point to.</p>
			<p>How do we apply this information to our chatbot NLU then?</p>
			<p>First of all, we need to determine whether an utterance involves an anaphora and whether we need an anaphora resolution. Consider the following dialog segment again:</p>
			<p class="source-code">Do you want to make a reservation?</p>
			<p class="source-code">Yes, I want to make one.</p>
			<p>The dependency parse of the second utterance looks like this:</p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="Images/Figure_10_11.jpg" alt="Figure 10.11 – Dependency parse of the example utterance&#13;&#10;" width="1059" height="252"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.11 – Dependency parse of the example utterance</p>
			<p>First of all, <strong class="source-inline">one</strong> appears as the<a id="_idIndexMarker743"/> direct object of the sentence and there are no other direct objects. This means that <strong class="source-inline">one</strong> should be an anaphora. In order to resolve what <strong class="source-inline">one</strong> refers to, we'll look back to the first utterance of the dialog. The following dependency parse belongs to the first utterance, <strong class="source-inline">Do you want to make a reservation?</strong>:</p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="Images/Figure_10_12.jpg" alt="Figure 10.12 – Dependency parse of the example utterance&#13;&#10;" width="1085" height="249"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.12 – Dependency parse of the example utterance</p>
			<p>If we look at <em class="italic">Figure 10.11</em>, we see that the sentence has a direct object, <strong class="source-inline">a reservation</strong>, so <strong class="source-inline">one</strong> should refer to <strong class="source-inline">a reservation</strong>. Then, we can arrange the resulting semantic parse as follows:</p>
			<p class="source-code">{</p>
			<p class="source-code">utt: "Where is this restaurant?",</p>
			<p class="source-code">intent: "ReserveRestaurant",</p>
			<p class="source-code">entities: [],</p>
			<p class="source-code">structure: {</p>
			<p class="source-code">    sentence_type: "declarative",</p>
			<p class="source-code">    subjects: ["one"]</p>
			<p class="source-code">    anaphoras: {</p>
			<p class="source-code">       "one": "a reservation"</p>
			<p class="source-code">       }</p>
			<p class="source-code">  }</p>
			<p class="source-code">}</p>
			<p>Replacing <strong class="source-inline">one</strong> with <strong class="source-inline">a reservation</strong> makes the sentence intent clearer. In our chatbot NLU, we only have two intents, but what if there are more intents, such as reservation cancellation, refunds, and so on? Then <strong class="source-inline">I want to make one</strong> can mean making a cancellation or getting a refund as well. </p>
			<p>Therefore, we make<a id="_idIndexMarker744"/> anaphora resolution come before the intent recognition and feed the full sentence, where anaphora words are replaced with the phrases they refer to. This way, the intent classifier is fed with a sentence where the direct object is a noun phrase, not one of the words <strong class="source-inline">one</strong>, <strong class="source-inline">same</strong>, <strong class="source-inline">it</strong>, or <strong class="source-inline">more</strong>, which do not carry any meaning on their own.</p>
			<p>Now after extracting meaning (by extracting intent) statistically with Keras, in this section you learned ways of processing sentence syntax and semantics with special NLU techniques. You're ready to combine all the techniques you know and design your own chatbot NLU for your future career. This book started with linguistic concepts, continued with statistical applications, and in this chapter, we combined it all. You're ready to keep going. In all the NLU pipelines you'll design, always try to look at the problem from a different view and remember what you learned in this book.</p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor189"/>Summary</h1>
			<p>That's it! You made it to the end of this exhaustive chapter and also to the end of this book! </p>
			<p>In this chapter, we designed an end-to-end chatbot NLU pipeline. As a first task, we explored our dataset. By doing this, we collected linguistic information about the utterances and understood the slot types and their corresponding values. Then, we performed a significant task of chatbot NLU, entity extraction. We extracted several types of entities such as city, date/time, and cuisine with the spaCy NER model as well as Matcher. Then, we performed another traditional chatbot NLU pipeline task – intent recognition. We trained a character-level LSTM model with TensorFlow and Keras. </p>
			<p>In the last section, we dived into sentence-level and dialog-level semantics. We worked on sentence syntax by differentiating subjects from objects, then learned about sentence types and finally learned about the linguistic concept of anaphora resolution. We applied what we learned in the previous chapters, both linguistically and statistically, by combining several spaCy pipeline components such as NER, dependency parsers, and POS taggers.</p>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor190"/>References</h1>
			<p>Here are some references for this chapter:</p>
			<p>On voice assistant products:</p>
			<ul>
				<li>Alexa developer blog: <a href="https://developer.amazon.com/blogs/home/tag/Alexa%20">https://developer.amazon.com/blogs/home/tag/Alexa</a></li>
				<li>Alexa science blog: <a href="https://www.amazon.science/tag/alexa%20">https://www.amazon.science/tag/alexa</a></li>
				<li>Microsoft's publication on chatbots: <a href="https://academic.microsoft.com/search?q=chatbot">https://academic.microsoft.com/search?q=chatbot</a></li>
				<li>Google Assistant: <a href="https://assistant.google.com/%20">https://assistant.google.com/</a></li>
			</ul>
			<p>Keras layers and optimizers:</p>
			<ul>
				<li>Keras layers: <a href="https://keras.io/api/layers/%20">https://keras.io/api/layers/</a></li>
				<li>Keras optimizers: <a href="https://keras.io/api/optimizers/%20">https://keras.io/api/optimizers/</a></li>
				<li>An overview of optimizers: <a href="https://ruder.io/optimizing-gradient-descent/%20">https://ruder.io/optimizing-gradient-descent/</a></li>
				<li>Adam optimizer: <a href="https://arxiv.org/abs/1412.6980%20">https://arxiv.org/abs/1412.6980</a></li>
			</ul>
			<p>Datasets for conversational AI:</p>
			<ul>
				<li>Taskmaster from Google Research: <a href="https://github.com/google-research-datasets/Taskmaster/tree/master/TM-1-2019%20">https://github.com/google-research-datasets/Taskmaster/tree/master/TM-1-2019</a></li>
				<li>Simulated Dialogue dataset from Google Research: <a href="https://github.com/google-research-datasets/simulated-dialogue%20">https://github.com/google-research-datasets/simulated-dialogue</a></li>
				<li>Dialog Challenge dataset from Microsoft: <a href="https://github.com/xiul-msr/e2e_dialog_challenge%20">https://github.com/xiul-msr/e2e_dialog_challenge</a></li>
				<li>Dialog State Tracking Challenge dataset: <a href="https://github.com/matthen/dstc">https://github.com/matthen/dstc</a></li>
			</ul>
		</div>
	</div></body></html>