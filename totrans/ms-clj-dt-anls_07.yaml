- en: Chapter 7. Null Hypothesis Tests – Analyzing Crime Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting started with data analysis can be so easy. We just plug numbers into
    a function or library and retrieve the results. But sometimes, it's easy to forget
    that we have to pay attention to how the data and experiments are constructed
    and how the questions are framed. Much of the reliability of statistics comes
    from following good practices and developed processes for framing and executing
    the tests and experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there's a lot to setting up statistical experiments and following
    best practices in gathering data and applying statistical tests. We won't be able
    to do more than cursorily glance at this topic. Hopefully, either it will serve
    as a reminder of things you already know or it will outline what you need to know
    and point you in the right direction to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: Over the course of this chapter, we'll move back and forth between looking at
    the problem we're tackling and seeing what null hypothesis testing is, how it
    can help us, and how we can apply it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing confirmatory data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding null hypothesis testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding crime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conducting the experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpreting the results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So without any further delay, let's learn about the techniques and the problems
    we'll address with these methods in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing confirmatory data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Oftentimes, data analysis seems like a menu of analyses applied to problems,
    but lacking an overall structure. Of course, this isn't the case, but it seems
    that way to programmers without a strong background in statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Frameworks such as **confirmatory data analysis** and **null hypothesis testing**
    provide the structure that may be missing. Generally, when you begin working with
    data, you start by generating some summary statistics that highlight some of the
    basic characteristics of the data. Afterwards, you probably generate some graphs
    that further elucidate the essential qualities of the data. This all falls into
    the realm of **exploratory data analysis**.
  prefs: []
  type: TYPE_NORMAL
- en: However, as the exploration wraps up, you'll probably start to think of some
    theories about the data that you'd like to test. You'll generate some hypotheses,
    and you'll need to test whether they're true or not. And based on those tests,
    you'll further refine your knowledge of the data, what's in it, and what it means.
  prefs: []
  type: TYPE_NORMAL
- en: This more formal stage of data analysis represents confirmatory data analysis.
    At this stage, you're concerned with using reliable tests that match your data,
    and you're trying to determine how representative your sample is. You are minimizing
    error and trying to get a **pvalue**—the probability that a result so extreme
    could have happened by chance—that means that the results are statistically significant.
  prefs: []
  type: TYPE_NORMAL
- en: But what does all this mean, exactly? How do we go about conceptualizing, planning,
    and executing these tests?
  prefs: []
  type: TYPE_NORMAL
- en: Understanding null hypothesis testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One common way of structuring and processing these tests is to use null hypothesis
    testing. This represents a **frequentist** approach to statistical inference.
    This draws inferences based upon the frequencies or proportions in the data, paying
    attention to confidence intervals and error rates. Another approach is Bayesian
    inference, which focuses on degrees of belief, but we won't go into that in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Frequentist inference has been very successful. Its use is assumed in many fields,
    such as the social sciences and biology. Its techniques are widely implemented
    in many libraries and software packages, and it's relatively easy to start using
    it. It's the approach we'll use in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To use the null hypothesis process, we should understand what we''ll be doing
    at each step of the way. The following is the basic process that we''ll work through
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Formulate an initial hypothesis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: State the null (*H[0]*) and alternative (*H[1]*) hypotheses.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify the statistical assumptions in the sample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine which tests (*T*) are appropriate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the significance level (*a*), such as *p<0.05* or *p<0.01*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the critical region, that is, the region of the distribution in which
    the null hypothesis will be rejected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the test statistic and the probability of the observation under the
    null hypothesis (*p*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Either reject the null hypothesis or fail to reject it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll go into these step-by-step, and we'll walk through this process twice
    to get a good feel for how it works. Most of this is pretty simple, really.
  prefs: []
  type: TYPE_NORMAL
- en: Formulating an initial hypothesis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we can start testing a theory about our data, we need to have something
    to test. This is generally something that might be true or false, and we want
    to determine which of the two it is. Some examples of initial hypotheses might
    be height correlating to diet, speed limit correlating to accident mortality,
    or a Super Bowl win for an old American Football League team (AFC division) correlating
    to a declining stock market (the so-called Super Bowl indicator).
  prefs: []
  type: TYPE_NORMAL
- en: Stating the null and alternative hypotheses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we have to reformulate the initial hypothesis into the statistical phrases
    that we'll use more directly the rest of the time. This is a useful point that
    helps to clarify the rest of the process.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the null hypothesis is the control, or what we're trying to disprove.
    It's the opposite of the alternative hypothesis, which is what we want to prove.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the last example from the previous section, the Super Bowl
    indicator, the re-cast hypotheses might be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Null hypothesis**: Who wins the Super Bowl has no effect upon the stock market.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alternative hypothesis**: When an AFC division team wins the Super Bowl,
    the stock market will decline; when an NFC division team wins the Super Bowl,
    the stock market will be up.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the rest of the process, we will concern ourselves with rejecting the null
    hypothesis. That can only happen when we''ve determined two things: first, that
    the data we have supports the alternative hypothesis, and second, that this is
    very unlikely to be a mistake; that is, the results we see probably are not a
    sample that misrepresents the underlying population.'
  prefs: []
  type: TYPE_NORMAL
- en: This is going to keep coming up, so let's unpack it a little.
  prefs: []
  type: TYPE_NORMAL
- en: 'You''re interested in making an observation about a population—all men; all
    women; all people; all statisticians; or past, present, and future stock market
    trends—but obviously you can''t make an observation for every person or aspect
    in the population. So instead, you select a sample. It should be random. The question
    then becomes: does the sample accurately represent the population? Say you''re
    interested in people''s heights. How close is the sample''s average height to
    the population''s average height?'
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume that what we're interested in falls on a normal distribution, as
    height generally does. What would this look like? For the following chart, I generated
    some random height data. The blue bars (appearing as dark gray in physical books)
    represent the histogram for the population, and the red bars (appearing as light
    gray in physical books) are the histogram for the sample.
  prefs: []
  type: TYPE_NORMAL
- en: '![Stating the null and alternative hypotheses](img/4139OS_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see from the preceding graph that the distributions are similar, but
    certainly not the same. And in fact, the mean for the population is 6.01, while
    the mean for the sample is 5.94\. They're not too far apart in this case, but
    some samples would be much further off.
  prefs: []
  type: TYPE_NORMAL
- en: 'It has been proven theoretically that the difference between the population
    mean and the possible sample means will fall on a normal distribution. The following
    is the plot for the difference in the means from 500 sets of samples drawn from
    the same population:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stating the null and alternative hypotheses](img/4139OS_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This histogram makes it clear that large differences between the population
    mean and the sample mean are unlikely, and the larger the difference, the more
    improbable it is. This is important for several reasons. First, if we know the
    distribution of the differences of means, it allows us to set constraints on results.
    If we are working with sample data, we know that the same values for the population
    will fall within a set bound.
  prefs: []
  type: TYPE_NORMAL
- en: Also, if we know the distribution of differences, then we know if our results
    are significant. This means that we can reject the null hypothesis that the averages
    are the same. Any two sample means should fall within the same boundaries. Large
    differences between any two sets of sample means are similarly improbable.
  prefs: []
  type: TYPE_NORMAL
- en: For example, one sample would be the control data, and one would be the test
    data. If the difference between the two samples is large enough to be improbable,
    then we can infer that the test behavior produced a significant difference (assuming
    the rest of the experiment is well designed and other things aren't complicating
    the experiment). If it's unlikely enough, then we say that it's significant, and
    we reject the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on what we''re testing, we may be interested in results that are
    on the left-hand side of the graph, the right-hand side, or either. That is, the
    test statistic for the alternative hypothesis may be significantly less than,
    significantly greater than, or equal to the null hypothesis. We express this in
    notation using one of the following three forms. (These use the character mu,
    *μ*, using the sample mean as the test statistic.) In each of these notations,
    the first line states the null hypothesis, and the second states the alternative
    hypothesis. For instance, the first pair in the following notation says that the
    null hypothesis is that the test sample''s mean should be greater than or equal
    to the control sample''s mean, and the alternative hypothesis is that the test
    sample''s mean should be less than the control sample''s mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stating the null and alternative hypotheses](img/4139OS_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We've taken our time to understand this more thoroughly because it's fundamental
    to the rest of the process. However, if you don't understand it at this point,
    throughout the rest of the chapter, we'll keep going over this. By the end, you
    should have a good understanding of this graph of sample mean differences and
    what it implies.
  prefs: []
  type: TYPE_NORMAL
- en: Determining appropriate tests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another aspect of your data that you''ll need to pay attention to is the shape
    of the data. This can often be easily visualized using a histogram. For example,
    the following screenshot shows a normal distribution and two distributions that
    are skewed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Determining appropriate tests](img/4139OS_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The red curve is skewed left (appearing as dark gray), and the yellow curve
    is skewed right (appearing as white). The blue curve (appearing as light gray)
    is a normal distribution with no skew.
  prefs: []
  type: TYPE_NORMAL
- en: Many statistical tests are designed for normal data, and they won't give good
    results for skewed data. For example, t-test and regression analysis both give
    good results only for normally distributed data.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the significance level
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we need to select the significance level that we want to achieve for our
    test. This is the level of certainty that we'll need to have before we can reject
    the null hypothesis. More to the point, this is the maximum chance that the results
    could be an outlying sample from the population, which would cause you to incorrectly
    reject the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: Often, the significance level, usually given as the p-value, is given as *p<0.05*
    or *p<0.01*. This means that the results have a less than 5 percent or 1 percent
    chance of being caused by a sample with an outlying mean.
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the graph of sample mean differences given earlier, we can see
    that we're looking at differences of about 2.4 inches to be significant. In other
    words, based on this population, the average difference in height would need to
    be more than 2 inches for it to be considered statistically significant.
  prefs: []
  type: TYPE_NORMAL
- en: Say we wanted to see if men and women were on average, of different heights.
    If the average height difference were only 1 inch, that could likely be the result
    of the samples that we picked. However, if the average height difference were
    2.4 inches or more, that would be unlikely to have come from the sample.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the critical region
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we have determined two important pieces of information: we''ve expressed
    our null and alternative hypotheses, and we''ve decided on a needed level of significance.
    We can use these two to determine the critical region for the test results, that
    is, the region for which we can reject the null hypothesis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that our hypotheses can take one of three forms. The following conditions
    determine where our critical region is:'
  prefs: []
  type: TYPE_NORMAL
- en: For the alternative hypothesis that the two samples' means are not equal, we'll
    perform a two-tailed test.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the alternative hypothesis that the test sample's mean is less than the
    control sample's, we'll perform a one-tailed test with the critical region on
    the left-hand side of the graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And for the alternative hypothesis that the test sample's mean is greater than
    the control sample's, we'll perform a one-tailed test with the critical region
    on the right-hand side of the graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following hypothetical graph highlights the part of the curve in which the
    critical regions occur. The curve represents the distribution of the test statistic
    for the sample, and the shaded parts will be the areas that the critical region(s)
    might come from.
  prefs: []
  type: TYPE_NORMAL
- en: '![Determining the critical region](img/4139OS_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The exact size of the critical regions is determined by the *p* value that we
    decided upon. In all cases, the area of the critical regions is the *p* percentage
    of the entire curve. That is, if we've decided that we're trying for *p<0.05*,
    and the area under the whole curve is 100, the area in the critical region will
    be 5.
  prefs: []
  type: TYPE_NORMAL
- en: If we are performing a two-tailed test, then that area will be divided into
    two, so in the example we just outlined, each side will have an area of 2.5\.
    However, for one-tailed tests, the entire critical region will fall on one side.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the test statistics and its probability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we have to calculate the test statistic. Depending on the nature of the
    data, the sample, and on what you're trying to answer, this could involve comparing
    means, a student's t-test, X² test, or any number of other tests.
  prefs: []
  type: TYPE_NORMAL
- en: These tests will give you a number, but interpreting it directly is often not
    helpful. Instead, you then need to calculate the value of *p* for that test's
    distribution. If you're doing things by hand, this can involve either looking
    up the value in tables or if you're using a software program, this is often done
    for you and returned as a part of the results.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use Incanter in several sections later in this chapter, starting with
    calculating the test statistic and its probability. Its functions generally return
    both the test value and the *p* value.
  prefs: []
  type: TYPE_NORMAL
- en: Deciding whether to reject the null hypothesis or not
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we can find the value of *p* in relation to the critical regions and determine
    whether we can reject the null hypothesis or not.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, say that we've decided that the level of significance that we
    want to achieve is *p<0.05* and the actual value of *p* is *0.001*. This will
    allow us to reject the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: However, if the value of *p* is *0.055*, we would fail to reject the null hypothesis.
    We would have to assume that the alternative hypothesis is incorrect, at least
    until more information is available.
  prefs: []
  type: TYPE_NORMAL
- en: Flipping coins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we've been over the process of null hypothesis testing, let's walk
    through the process one more time with an example. This should be simple and straightforward
    enough that we can focus on the process, and not on the test itself.
  prefs: []
  type: TYPE_NORMAL
- en: For that purpose, we'll test whether a dice is loaded or not. If it is balanced,
    then the expected probability of any given side should be 1/6, or about 16 percent.
    However, if the die is loaded, then the probability for rolling one side should
    be greater than 16 percent, and the probabilities for rolling the other sides
    would be less than 16 percent.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, generally this isn't something that you would worry about. But before
    you agree to play craps with the dice that your friend 3D printed, you may want
    to test them.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this test, I''ve rolled one die 1,000 times. The following is the table
    of how many times each side came up:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Side | Frequency |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 157 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 151 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 175 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 187 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 143 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 187 |'
  prefs: []
  type: TYPE_TB
- en: So we can see that the frequencies are relatively close, within a range of 44,
    but they aren't exactly the same. This is what we'd expect. The question is whether
    they're different enough that we can say with some certainty that the die is loaded.
  prefs: []
  type: TYPE_NORMAL
- en: Formulating an initial hypothesis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So we suspect that our test die is fair, but we don''t know that. We''ll frame
    our hypothesis this way: on any roll, all sides have an equal chance of appearing.'
  prefs: []
  type: TYPE_NORMAL
- en: Stating the null and alternative hypotheses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our initial hypothesis can act as our null hypothesis. And in this case, we
    expect to fail to reject it. Let''s state both hypotheses explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '*H[0]*: All sides have an equal chance of appearing on any roll.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*H[1]*: One side has a greater chance of appearing on any roll.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, we let *H[0]* be such that the two sides are equal because we
    want there to be more latitude in what counts as fair, and we want to enforce
    a high burden of proof before we declare a die loaded.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the statistical assumptions in the sample
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For our sample, we''ll roll the die in question 1,000 times. We''ll assume
    that each roll is identical: that it''s being done with approximately the same
    arm and hand movements, and that the die is landing on a flat surface. We''ll
    also assume that before being thrown, the die is being shaken enough to be appropriately
    random.'
  prefs: []
  type: TYPE_NORMAL
- en: This way, no biases are introduced because of the mechanics of how the die is
    being thrown.
  prefs: []
  type: TYPE_NORMAL
- en: Determining appropriate tests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For this, we'll use a Pearson's Χ² goodness-of-fit test. This is used to test
    whether an observed frequency distribution matches a theoretical distribution.
    It works by calculating a normalized sum of squared deviations. We're trying to
    test whether some observations match an expected distribution, so this test is
    a great fit.
  prefs: []
  type: TYPE_NORMAL
- en: We'll see exactly how to apply this test in a minute.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the significance level
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Proving that a die is loaded does require a higher burden of proof than assuming
    that it's fair, but we don't want the bar to be too high. Because of that, we'll
    use *p<0.05* for this.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the critical region
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The output of the Χ² test fits an Χ² distribution, not a normal distribution,
    so the graph won't look the same. Also, Χ² tests are intrinsically one sided.
    When the number is too far out on the right, then it indicates that the data fits
    the theoretical values poorly. A value to the left on the Χ² distribution just
    indicates that the fit is very good, which isn't really a problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a graph comparing the normal distribution, centered on 50,
    with the X² distribution, with 3 degrees of freedom:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Determining the critical region](img/4139OS_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Either way, the statistics library that we're going to use (Incanter) will take
    care of this for us.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the test statistic and its probability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'So let''s fire up the Leiningen REPL and see what we can do. For this project,
    we''re going to use the following `project.clj` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we''ll load Incanter, then we''ll create a matrix containing our data,
    and finally we''ll run an Χ² test over it with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at this code in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The function `incanter.stats/chisq-test` returns a lot of information, including
    its own input. So, before displaying it at the end, I filtered out most of the
    data and only returned the three keys that we're particularly interested in. The
    following are those keys and the values that they returned.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:X-sq`: This is the Χ² statistic. Higher values of this indicate that the
    data does not fit their expected values.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:df`: This is the degrees of freedom. This represents the number of parameters
    that are free to vary. For nominal data (data without natural ordering), such
    as rolls of dice, this is the number of values that the data can take, minus one.
    In this case, since it''s a six-sided die, the degree of freedom is five.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:p-value`: This is the value of *p* that we''ve been talking about. This is
    the probability that we''d see these results from the Χ² test if the null hypothesis
    were true.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have these numbers, how do we apply them to our hypotheses?
  prefs: []
  type: TYPE_NORMAL
- en: Deciding whether to reject the null hypothesis or not
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this case, since *p>0.05*, we fail to reject the null hypothesis. We can't
    really rule it out, but we don't have enough evidence to support it either. In
    this case, we can assume that the die is fair.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, this example gives you a better understanding of the null hypothesis
    testing process and how it works. With that under our belts, let's turn our attention
    to a bigger, more meaningful problem than the fairness of imaginary dice.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding burglary rates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding crime seems like a universal problem. Earlier, societies grappled
    with the problem of evil in the universe from a theological perspective; today,
    sociologists and criminologists construct theories and study society using a variety
    of tools and techniques. However the problem is cast, the aim is to better understand
    why some people violate social norms in ways that are often violent and harmful
    to those around them and even themselves. By better understanding this problem,
    ultimately we'd like to be able to create social programs and government policies
    that minimize the damage and create a safer and hopefully more just society for
    all involved.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, as data scientists and programmers engaging in data analysis, we're
    inclined to approach this problem as a data problem. That's what we'll do in the
    rest of this chapter. We'll gather some crime and economic data and look for a
    tie between the two. In the course of our analysis, we'll explore the data, tentatively
    suggest a hypothesis, and test it against the data.
  prefs: []
  type: TYPE_NORMAL
- en: We'll look at crime data from the United Nations and see what relationships
    it has with data from the World Bank data site.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to get the data, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to download the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the crime data, we'll go to the website of the United Nations Office on
    Drugs and Crime ([http://www.unodc.org/](http://www.unodc.org/)). It publishes
    crime data for countries around the world over a number of years. Their data page,
    [http://www.unodc.org/unodc/en/data-and-analysis/statistics/data.html](http://www.unodc.org/unodc/en/data-and-analysis/statistics/data.html),
    has links to Excel files for a number of different categories of crime in the
    section of the page labeled **Statistics on crime**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should download each of these and save them to the directory `unodc-data`.
    You can extract the data from these in a minute. First, you can get the data that
    we want to correlate to the crime data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We''ll get this data from the World Bank''s data site ([http://data.worldbank.org/](http://data.worldbank.org/)).
    Navigating the site is a little complicated, and in my experience it changes regularly.
    For the moment, at least, this seems to be the easiest way to get the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visit the **Indicators** page at [http://data.worldbank.org/indicator](http://data.worldbank.org/indicator).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the search box, enter `land area` and select **Land area (sq. km)**, as
    shown in the following screenshot:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Getting the data](img/4139OS_07_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Then hit the **Go** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next page, you'll be given the option to download the dataset in a number
    of formats. Choose **CSV**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the data and unzip it into a directory named `ag.lnd`, based on the
    indicator codes that the World Bank uses. (You can use a different directory name,
    but you'll need to modify the directions that follow.)![Getting the data](img/4139OS_07_08.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We''ll also want some economic data. To get that, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Go back to the **Indicators** page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Search for **GNI per capita** (it's the default selection for the search box).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the filtered results, select **GNI per capita, Atlas method (current US$)**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Go**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the data as CSV again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unzip the data into a directory named `ny.gnp`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At this point, you should have a directory with several subdirectories containing
    data files. The structure should look something like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting the data](img/4139OS_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Some of the data is ready to go, but before we use it, we need to extract the
    data from the Excel files. Let's turn our attention there.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing the Excel files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we can extract the data from the Excel files, we need to find out what
    our input for this will be. If we open up one of the Excel files, in this case
    `CTS_Assault.xls`, we''ll see something similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Parsing the Excel files](img/4139OS_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s list out some of the features of the sheets that we''ll need to take
    into account:'
  prefs: []
  type: TYPE_NORMAL
- en: There are about thirteen rows of headers, most of which are hidden in the preceding
    screenshot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, not shown in the preceding screenshot, but some of the files have more
    than one tab of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are some hidden columns between columns A and D.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The subregion isn't listed on each row, so we'll need some way to carry this
    over.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the years for each crime and country combination are listed on one row.
    We'll probably want to pivot that so that there's a column for the crime, one
    for the country, one for the year, and one for the data value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a lot of missing data. We can filter that out.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To get into the Excel files, we'll use the Apache POI project ([http://poi.apache.org/](http://poi.apache.org/)).
    This library provides access to file formats of Microsoft Office's suites.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use this library to extract the data from the Excel files in several
    stages, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Pull raw data rows out of the Excel files
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Populate a tree of data that groups the data hierarchically by region, subregion,
    and country
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Flatten the hierarchically arranged data back into a sequence of maps containing
    all the data for each row
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wrap all of this in one easy-to-use function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's follow the preceding steps for the rest of this section, and in the end
    we'll add a controller function that pulls it all together.
  prefs: []
  type: TYPE_NORMAL
- en: We'll keep all of this code in a single module. The following namespace declaration
    for this will include all the dependencies that we'll need. For the fully specified
    `project.clj` file that includes all of these, refer to the code download for
    this chapter. I named the project `nullh`, so the file that I'm working with here
    is named `src/nullh/unodc.clj`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now we can start populating this namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Pulling out raw data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the first stage of the process, in which we read the data into a series
    of raw data rows, we'll use a couple of record types, as shown in the following
    code. The first, `sheet-data`, associates the title of the worksheet with the
    data in it. The second, `xl-row`, simply stores the data in each row's cells into
    named fields.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As we interact with the worksheet''s data and API, we''ll use a number of utilities
    that makes access to the worksheet objects more like working with native Clojure
    objects. The following are some of those utilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll spend a lot of time accessing cells'' values. We''ll want to make a
    simpler, more Clojure-like wrapper around the Java library''s API for accessing
    them. How we do this will depend on the cell''s type, and we can use *multimethods*
    to handle dispatching for it, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, with these methods in place, we can easily read the data into a sequence
    of data rows. First, we''ll need to open the workbook file with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can take each sheet and read it into a `sheet-data` record with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The rows themselves will need to go through a number of transformations, all
    without touching the sheet name field. To facilitate this, we''ll define a higher
    order function that maps a function over the rows field, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The first row transformation will involve skipping the header rows for each
    sheet, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can take the sequence of `sheet-data` records and flatten them by adding
    the sheet name onto the row data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We do need to take each row and clean it up by rearranging the field order,
    making sure it has exactly the right number of fields with the help of the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we''ve hardened our data a little, we can take the Clojure vectors
    and populate the `xl-row` records with them as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we have a fairly clean sequence of row data.
  prefs: []
  type: TYPE_NORMAL
- en: Growing a data tree
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unfortunately, we haven't yet dealt with some problems, such as the subregion
    not being populated in every row. Let's take care of that now.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll tackle that problem by changing the sequence of records into a hierarchical
    tree of data. The tree is represented by a number of record types as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: To build the tree, we'll have a number of functions. Each takes a group of data
    that will go into one tree or subtree. It populates that part of the tree and
    returns it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first of these functions is `xl-rows->regions`. It takes a sequence of
    `xl-rows`, groups them by region, and constructs a tree of `region` records for
    it as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The most complicated part of building this tree is dealing with the missing
    subregions. We''ll use three functions to deal with that. The first, `conj-into`,
    conjugates onto a value in a map, or adds a new vector containing the data if
    there''s no data for that key. The second, `fold-sub-region`, folds each row into
    a map based on either the subregion referred to in the row, or the last specified
    subregion. Finally, `xl-rows->sub-regions` takes a sequence of rows from one region,
    divides them into subregions, and creates the `sub-region` records for them, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the subregions identified, we can build a tree for each country.
    For that, we''ll pull the count data and the rate data into their own structures
    and put it all together into a `country` record with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The counts and rates are represented by the same record type, so we''ll use
    a shared function to pull the fields from the row that populate the fields in
    the type as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: These functions all build the hierarchy of data that's stored in the worksheets.
  prefs: []
  type: TYPE_NORMAL
- en: Cutting down the data tree
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We reverse the process to flatten the data again. In the process, this implicitly
    populates the missing subregions into all of the rows. Let's see how this works.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, we take a sequence of regions and convert each one into a sequence
    of `xl-row` records, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Just as before, this work will be delegated to other functions; in this case,
    `sub-regions->xl-rows`, which again delegates to `country->xl-rows`. The second
    function in the following code is a little long (and so I''ve omitted some lines
    from it), but both are conceptually simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we have a sequence of data rows with the missing subregions supplied.
    But we're still not done.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''ll provide several levels of function to make this easier. First, one that
    ties together everything that we''ve seen so far. It takes a filename and returns
    a sequence of `xl-row` records as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it. We have our data read in. It''s been processed a little, but it''s
    still pretty raw. The following is an example row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We still need to clean it up a little and pivot the data to put each data value
    into its own row. Instead of having one row with `:count-2003`, `:count-2004`,
    and so on, we'll have many rows, each with `:count` and `:year`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's turn our attention there next.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we've only lightly cleaned part of our data. We haven't even looked
    at the data that we want to correlate the crime data with. Also, the shape of
    the data is awkward for the analyses that we want to conduct, so we'll need to
    pivot it the way we described earlier. We'll see more about this in a minute.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this stage of processing, we want to put all of the code into a new file.
    We''ll name this file `src/nullh/data.clj`, and the namespace declaration for
    it looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We'll now start working with Incanter datasets. We haven't used Incanter much
    so far in this book, and that's a little unusual, because Incanter is one of the
    go-to libraries for working with numbers and statistics in Clojure. It's powerful
    and flexible, and it makes working with data easy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the data that we read from the Excel files and import it into an
    Incanter dataset. We need to read the data into one long sequence, pull out the
    keys for the data fields, and then create the dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can read the data that we downloaded from the World Bank into another
    dataset. Both data files have roughly the same fields, so we can use the same
    function for both. Unfortunately, we need to load the CSV ourselves, because Incanter''s
    introspection doesn''t quite give us the results that we want. Because of this,
    we''ll also include a few functions for converting the data into doubles as we
    read it in, and we''ll define those columns that the data contains, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We can use the `read-indicator-data` function to load data from the two World
    Bank indicators that we downloaded earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Now we want to put all the data from UNODC together with either of the World
    Bank datasets. As we do that, we'll also pivot the data tables so that instead
    of one column for each year, there's one column containing the year and one containing
    the value for that year. At the same time, we'll remove rows with missing data
    and aggregate the counts for all of the crimes for a country for each year.
  prefs: []
  type: TYPE_NORMAL
- en: Joining the data sources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Bringing the two data sources together is relatively simple and can be done
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Basically, we just let Incanter join the two data structures on the fields by
    matching the World Bank data's `:country-name` field with the UNODC data's `:country`
    field.
  prefs: []
  type: TYPE_NORMAL
- en: Pivoting the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that the data has been joined, we can pivot it. In the end, we want to
    have the following fields on every row:'
  prefs: []
  type: TYPE_NORMAL
- en: '`region`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subregion`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`country`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`country-code`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indicator`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indicator-code`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`crime`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`year`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indicator-value`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, some of these fields are from the UNODC data and some are from
    the World Bank data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll do this translation on a sequence of maps instead of the dataset. We''ll
    get started with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: First, we use `->maps` to convert the dataset to a sequence of maps. Then, pass
    the processing off to `pivot-map`. This function pivots the data for each year.
  prefs: []
  type: TYPE_NORMAL
- en: We pivot the data for each year separately. We do this by repeatedly transforming
    the data map for a row. This is a great example of how Clojure's immutability
    makes things easier. We don't have to worry about copying the map or clobbering
    any data. We can just modify the original data multiple times, saving the result
    of each transformation process as a separate, new data row.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process itself is fairly simple. First, we use the year to create keywords
    for the fields that we are interested in. Next, we select the rows that we want
    to keep from the original data map. Then we rename a few to make them clearer.
    And finally, we add the year to the output map as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: That's it. This should make the data easier to work with. We can do some more
    transformations on the data and clean it up a bit further.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering the missing data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, there are a lot of holes in the data, and we don''t want to have to
    worry about that. So if a row is missing any of the three data fields (`:count`,
    `:rate`,or `:indicator-value`), let''s get rid of it with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We just check whether any of these fields has a `nil` value. If any of them
    do, we remove that row.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s make a wrapper function around this process. That''ll help us stay consistent
    and make the library easier to use. This loads the data from UNODC and one of
    the World Bank datasets. It joins, pivots, and removes the missing rows before
    returning an Incanter dataset, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s use these functions to load up one of the datasets as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: At this point, the data is in decent shape—actually, as good as this data is
    probably going to get (more about that near the end of this chapter). So let's
    see what's in the data and what it has to tell us.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's explore a little and try to get a feel for the data. First, let's try
    to get some summary statistics for the various datasets. Afterward, we'll generate
    some graphs to get a more intuitive sense for what's in the data and how they're
    related.
  prefs: []
  type: TYPE_NORMAL
- en: Generating summary statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Incanter makes generating summary statistics easy. You can pass a dataset to
    the `incanter.stats/summary` function. It returns a sequence of maps. Each map
    represents the summary data for each column in the original dataset. This includes
    whether the data is numeric or not. For nominal data, it returns some sample items
    and their counts. For numeric data, it returns the mean, median, minimum, and
    maximum.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing UNODC crime data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If we load the data and filter it for the crime of "burglary", we can get the
    summary statistics for those fields as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'And if we pick apart the data structures that it outputs, the following are
    the summary statistics for the primary data fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Column | Minimum | Maximum | Mean | Median |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Rate** | 0.1 | 1939.23 | 376.4 | 292.67 |'
  prefs: []
  type: TYPE_TB
- en: '| **Count** | 11 | 443010 | 60380 | 17184 |'
  prefs: []
  type: TYPE_TB
- en: So, from the preceding table, we see that both fields have wide variance and
    are skewed somewhat, based on the differences between the means and the medians.
    These two having similar distributions is to be expected, since the rate is derived
    from the count.
  prefs: []
  type: TYPE_NORMAL
- en: Charts and graphs can also help to understand our data better. Incanter makes
    generating charts quite simple. Let's see how to do that.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll load the data and pivot it, since that will make it easier to
    pull the data out of the graph. For this example, we''ll load the UNODC crime
    data joined to the World Bank land area data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll filter the dataset to contain only the burglary data as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we use the `incanter.charts/histogram` function to create the graph,
    and the `incanter.core/view` function to display it to the screen with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the histogram of the `:count` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Summarizing UNODC crime data](img/4139OS_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From this graph, we can see that this data does not follow a normal distribution.
    How does the other data correspond?
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing World Bank land area and GNI data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can use the same function, `incanter.stats/summary`, to generate the same
    statistics for the land area data that is given in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Column | Minimum | Maximum | Mean | Median |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Land area** | 300 | 16381390 | 822324 | 100250 |'
  prefs: []
  type: TYPE_TB
- en: '| **GNI** | 240 | 88500 | 17170 | 8140 |'
  prefs: []
  type: TYPE_TB
- en: 'The World Bank land area data has a distribution that is similar to the crime
    data. Smaller, less wealthy countries are, of course, more numerous. The distribution
    of the land area values is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Summarizing World Bank land area and GNI data](img/4139OS_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the distribution of the GNI values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Summarizing World Bank land area and GNI data](img/4139OS_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This gives us some feel for the data. All of these follow an exponential distribution,
    as we can see in the next graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Summarizing World Bank land area and GNI data](img/4139OS_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This makes it clear that all graphs with exponential distribution start with
    a steep drop and quickly flatten out into a near-flat line.
  prefs: []
  type: TYPE_NORMAL
- en: Generating more charts and graphs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some more charts can help us begin to understand the relationship between some
    of these variables. We'll write a function to plot any crime against the World
    Bank indicator data joined into the current dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, however, we''ll need a utility function to filter the data rows by the
    crime. This is a data-oriented function, so we''ll store it in `nullh.data`, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The next function, `plot-crime`, pulls out the data points and then passes
    everything to the `incanter.charts/scatter-plot` function to generate the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This makes it easy to get a quick, visual comparison of data about different
    types of crimes and how they relate to the World Bank indicator data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following code shows us how the burglary ("CTS 2012 Burglary")
    relates to the land area data (the `plot-crime` function is in the `nullh.charts`
    namespace, which is aliased as `n-ch`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating more charts and graphs](img/4139OS_07_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This data appears to have some strange artifacts. Look at the line of data points
    where the land area is around 9,000,000, stretching from about 500 burglaries
    per year to almost 1,000 burglaries. What is that about?
  prefs: []
  type: TYPE_NORMAL
- en: Well, when we think about it, the land area of a country rarely changes, but
    if a country has burglary data for several years, we'll have the land area represented
    those many times. We could simplify the data by getting the average of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to do this, we aggregate all of the year data for each country. To
    do that, we''ll use the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code uses the `incanter.core/$rollup` function to get each country's
    average for each data column. It then uses `reduce` and `incanter.core/$join`
    to fold the data back into one dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we graph aggregated data, we get the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating more charts and graphs](img/4139OS_07_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This makes it clearer that there is probably no relationship between these two
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: The following graph compares the burglary data to the GNI per capita. Since
    that indicator doesn't typically vary much over the time span represented in the
    data (China and a few other countries not withstanding), we have again aggregated
    each country's data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating more charts and graphs](img/4139OS_07_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This data appears to possibly have a correlation between these two variables,
    although it may not be very strong. This is something we can test.
  prefs: []
  type: TYPE_NORMAL
- en: Conducting the experiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we're ready to frame and perform the experiment. Let's walk through the
    steps to do that one more time.
  prefs: []
  type: TYPE_NORMAL
- en: Formulating an initial hypothesis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this case, our hypothesis is that *there is a relationship between the per
    capita gross national income and the rate of burglaries*. We could go further
    and make the hypothesis stronger by specifying that higher GNI correlates to a
    higher burglary rate, somewhat counter-intuitively.
  prefs: []
  type: TYPE_NORMAL
- en: Stating the null and alternative hypotheses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given that statement of our working hypothesis, we can now formulate the null
    and alternative hypotheses.
  prefs: []
  type: TYPE_NORMAL
- en: '*H[0]*: There is no relationship between the per capita gross national income
    and a country''s burglary rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*H[1]*: There is a relationship between the per capita gross national income
    and the country''s burglary rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These statements will now guide us through the rest of the process.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the statistical assumptions in the sample
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a number of assumptions in this data that we need to be aware of.
    First, since the crime data comes from multiple sources, there's going to be very
    little consistency in it.
  prefs: []
  type: TYPE_NORMAL
- en: To start with, the very definitions of these crimes may vary widely between
    different countries. Also, data collection procedures and practices will make
    the reliability of those numbers difficult.
  prefs: []
  type: TYPE_NORMAL
- en: The World Bank data is perhaps more consistent—things like land area can be
    measured and validated externally—but GNI can be reliant upon the own country's
    reporting, and that may often be inflated, as countries attempt to make themselves
    look more important and influential.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, there are also a lot of holes in the data. Because we haven't normalized
    the country names, there are no observations for the United States. It's listed
    as "United States" in one dataset and as "United States of America" in the other.
    And while this single instance would be simple to correct, we really have to do
    a more thorough audit of the country names.
  prefs: []
  type: TYPE_NORMAL
- en: So while there's nothing systematic that we need to take into account, there
    are several problems with the data that we need to keep in mind. We'll revisit
    these closer to the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Determining which tests are appropriate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we have to determine which tests to run. Some tests are appropriate to different
    types of data and to different distributions of data. For example, nominal and
    numeric data require very different analyses.
  prefs: []
  type: TYPE_NORMAL
- en: If the relationship were known to be linear, we could use Pearson's correlation
    coefficient. Unfortunately, the relationship in our data appears to be more complicated
    than that.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, our data is continuous numeric data. And we're interested in the
    relationship between two variables, but neither is truly independent, because
    we're not really sure exactly how the *sampling* was done, based on the description
    of the assumptions given earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Because of all these factors, we'll use Spearman's rank correlation.
  prefs: []
  type: TYPE_NORMAL
- en: How did I pick this? It's fairly simple, but just complicated enough that we
    will not go into the details here.
  prefs: []
  type: TYPE_NORMAL
- en: The main point is that which statistical test you use is highly dependent on
    the nature of your data. Much of this knowledge comes from learning and experience,
    but once you've determined your data, a good statistical textbook or any of a
    number of online flowcharts can help you pick the right test.
  prefs: []
  type: TYPE_NORMAL
- en: But what is Spearman's rank correlation? Let's take a minute and find out.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Spearman's rank correlation coefficient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spearman's rank correlation coefficient measures the association between two
    variables. It is particularly useful when only the rank of the data is known,
    but it can also be useful in other situations. For instance, it isn't thrown off
    by outliers, because it only looks at the rank.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula for this statistic is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Spearman''s rank correlation coefficient](img/4139OS_07_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The value of *n* is the size of the sample. The value of *d* is each observation's
    difference in rank for the two variables. For example, in the data we've been
    looking at, Denmark ranks first for burglary (interesting), but third for per
    capita GNI. So Spearman's rank correlation would look at *3 – 1 = 2*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A coefficient of *0* means there is no relationship between the two variables,
    and a coefficient of *-1* or *+1* means that the two variables are perfectly related.
    That is, the data can be perfectly described using a **monotonic** function: a
    function from one variable to the other that preserves the order of the items.
    The function doesn''t have to be linear. In fact, it could easily describe a curve.
    But it does capture the data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The coefficient doesn''t give us statistical significance (the *p* value),
    however. To get that, we just need to know that the Spearman''s rank correlation
    coefficient is distributed approximately normally, when *n ≥ 10*. It has a mean
    of *0* and a standard deviation given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Spearman''s rank correlation coefficient](img/4139OS_07_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: With these formulae, we can compute the *z* score of coefficient for our test.
    The *z* score is the distance of a data point from the mean, measured in standard
    deviations. The *p* value is closely related to the *z* score. So if we know the
    *z* score, we also know the *p* value.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the significance level
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we need to select how high of a bar we need the significance to rise to.
    The target *p* value is known as the *α* value. In general, *α = 0.05* is commonly
    used, although if you want to be extra careful, *α = 0.01* is also normal.
  prefs: []
  type: TYPE_NORMAL
- en: For this test, we'll just use *α = 0.05*.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the critical region
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll accept any kind of relationship for rejecting the null hypothesis, so
    this will be a two-tailed test. That means that the critical region will come
    from both sides of the curve, with their areas being 0.05 divided equally for
    0.025 on each side.
  prefs: []
  type: TYPE_NORMAL
- en: This corresponds to a *z* score of *z < -1.96* or *z > 1.96*.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the test statistic and its probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use Incanter's function, `incanter.stats/spearmans-rho`, to calculate
    the Spearman's coefficient. However, it doesn't only calculate the *z* score.
    We can easily create the following function that wraps all of these calculations.
    We'll put this into `src/nullh/stats.clj`. We'll name the function `spearmans`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can run this on the dataset. Let''s start from the beginning and load
    the datasets from the disk with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The preceding commands allowed us to see the process from front to back, and
    we can take the output and consider how the test went.
  prefs: []
  type: TYPE_NORMAL
- en: Deciding whether to reject the null hypothesis or not
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final *z*-score was 16.03\. Going by the book, a *z*-score this high is
    usually not even included on the charts. This would be a significant result, which
    would allow us to reject the null hypothesis. So, from this we can conclude that
    there is a relationship between the per capita GNI and the burglary rate.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of course, the results don't tell us a whole lot. For one, we have to remember
    that just because there's a relationship, that doesn't imply causality. Moreover,
    because the result is so significant, we should probably be skeptical about the
    results and whether they're caused by some artifact in the data or the procedures.
  prefs: []
  type: TYPE_NORMAL
- en: We've already talked about the problems in the data, and some of them may be
    at fault. Particularly, some of the data is missing because of normalization problems,
    which may change the results. Another possibility is that industrialized nations
    keep better records, so they would appear to have more burglaries.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, in this chapter, we learned how null hypothesis testing can help us structure
    our analyses. Having a well thought out and standard procedure also ensures that
    we are thorough in our analysis. For example, in this chapter, we were forced
    to confront the ugly truths about the data we were working with, and that gave
    us insights into the results that we achieved later.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll actually get a chance to use these techniques again,
    when we look at conducting A/B testing on websites.
  prefs: []
  type: TYPE_NORMAL
