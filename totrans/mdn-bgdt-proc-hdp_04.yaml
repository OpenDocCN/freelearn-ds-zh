- en: Data Movement Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据移动技术
- en: In the last chapter, we learned about how to create and configure a Hadoop cluster,
    HDFS architecture, various file formats, and the best practices for a Hadoop cluster.
    We also learned about Hadoop high availability techniques.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何创建和配置 Hadoop 集群、HDFS 架构、各种文件格式以及 Hadoop 集群的最佳实践。我们还学习了 Hadoop 的高可用性技术。
- en: Since we now know how to create and configure a Hadoop cluster, in this chapter,
    we will learn about various techniques of data ingestion into a Hadoop cluster.
    We know about the advantages of Hadoop, but now, we need data in our Hadoop cluster
    to utilize its real power.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在已经知道如何创建和配置一个 Hadoop 集群，在本章中，我们将学习各种数据导入 Hadoop 集群的技术。我们了解 Hadoop 的优势，但现在，我们需要在我们的
    Hadoop 集群中有数据来利用其真正的力量。
- en: Data ingestion is considered the very first step in the Hadoop data life cycle.
    Data can be ingested into Hadoop as either a batch or a (real-time) stream of
    records. Hadoop is a complete ecosystem, and MapReduce is a batch ecosystem of
    Hadoop.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数据导入被认为是 Hadoop 数据生命周期中的第一步。数据可以以批处理或（实时）记录流的形式导入 Hadoop。Hadoop 是一个完整的生态系统，MapReduce
    是 Hadoop 的批处理生态系统。
- en: 'The following diagram shows various data ingestion tools:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了各种数据导入工具：
- en: '![](img/ec9306a3-4d18-460e-91bb-1376bdbd2ac4.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec9306a3-4d18-460e-91bb-1376bdbd2ac4.png)'
- en: We will learn about each tool in detail in the next few sections.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将详细了解每个工具。
- en: 'In this chapter, we will cover the following methods of transferring data to
    and from our Hadoop cluster:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下将数据传输到和从我们的 Hadoop 集群的方法：
- en: Apache Sqoop
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Sqoop
- en: Apache Flume
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Flume
- en: Apache NiFi
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache NiFi
- en: Apache Kafka Connect
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Kafka Connect
- en: Batch processing versus real-time processing
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批处理与实时处理
- en: Before we dive deep into different data ingestion techniques, let's discuss
    the difference between batch and real-time (stream) processing. The following
    explains the difference between these two ecosystems.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨不同的数据导入技术之前，让我们讨论一下批处理和实时（流）处理之间的区别。以下解释了这两个生态系统的区别。
- en: Batch processing
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批处理
- en: 'The following points describe the batch processing system:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下要点描述了批处理系统：
- en: Very efficient in processing a high volume of data.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理大量数据方面非常高效。
- en: All data processing steps (that is, data collection, data ingestion, data processing,
    and results presentation) are done as one single batch job.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有数据处理步骤（即数据收集、数据导入、数据处理和结果展示）都作为一个单独的批处理作业完成。
- en: Throughput carries more importance than latency. Latency is always more than
    a single minute.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吞吐量比延迟更重要。延迟总是超过一分钟。
- en: Throughput directly depends on the size of the data and available computational
    system resources.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吞吐量直接取决于数据的大小和可用的计算系统资源。
- en: Available tools include Apache Sqoop, MapReduce jobs, Spark jobs, Hadoop DistCp
    utility, and so on.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用的工具包括 Apache Sqoop、MapReduce 作业、Spark 作业、Hadoop DistCp 工具等。
- en: Real-time processing
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时处理
- en: 'The following points describe how real-time processing is different from batch
    processing:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下要点描述了实时处理与批处理的不同之处：
- en: Latency is extremely important, for example, less than one second
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 延迟非常重要，例如，不到一秒
- en: Computation is relatively simple
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算相对简单
- en: Data is processed as an independent unit
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据作为独立的单元进行处理
- en: Available tools include Apache Storm, Spark Streaming, Apache Fink, Apache Kafka,
    and so on
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用的工具包括 Apache Storm、Spark Streaming、Apache Fink、Apache Kafka 等
- en: Apache Sqoop
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Sqoop
- en: Apache Sqoop is a tool designed for efficiently transferring bulk data between
    a Hadoop cluster and structured data stores, such as relational databases. In
    a typical use case, such as a data lake, there is always a need to import data
    from RDBMS-based data warehouse stores into the Hadoop cluster. After data import
    and data aggregation, the data needs to be exported back to RDBMS. Sqoop allows
    easy import and export of data from structured data stores like RDBMS, enterprise
    data warehouses, and NoSQL systems. With the help of Sqoop, data can be provisioned
    from external systems into a Hadoop cluster and populate tables in Hive and HBase.
    Sqoop uses a connector-based architecture, which supports plugins that provide
    connectivity to external systems. Internally, Sqoop uses MapReduce algorithms
    to import and export data. By default, all Sqoop jobs run four map jobs. We will
    see Sqoop import and export functions in detail in the next few sections.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Sqoop 是一个工具，旨在高效地在 Hadoop 集群和结构化数据存储（如关系数据库）之间传输大量数据。在典型的用例中，例如数据湖，总是需要将基于
    RDBMS 的数据仓库存储中的数据导入到 Hadoop 集群中。在数据导入和数据聚合之后，需要将数据导回到 RDBMS。Sqoop 允许轻松地从结构化数据存储（如
    RDBMS、企业数据仓库和 NoSQL 系统）导入和导出数据。借助 Sqoop，可以从外部系统将数据配置到 Hadoop 集群中，并在 Hive 和 HBase
    中填充表。Sqoop 使用基于连接器的架构，支持提供外部系统连接性的插件。内部，Sqoop 使用 MapReduce 算法导入和导出数据。默认情况下，所有
    Sqoop 作业运行四个 map 作业。我们将在接下来的几节中详细看到 Sqoop 的导入和导出功能。
- en: Sqoop Import
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sqoop 导入
- en: 'The following diagram shows the **Sqoop Import** function to import data from
    an RDBMS table into a Hadoop cluster:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了 **Sqoop 导入** 功能，用于将 RDBMS 表中的数据导入到 Hadoop 集群中：
- en: '![](img/c39b5ad8-a2ea-4dec-b372-d3eeb008a52d.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c39b5ad8-a2ea-4dec-b372-d3eeb008a52d.png)'
- en: Import into HDFS
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入到 HDFS
- en: 'The following is a sample command to import data into HDFS:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是将数据导入 HDFS 的示例命令：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The import is done in two steps, which are as follows.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 导入分为两个步骤，如下所述。
- en: Sqoop scans the database and collects the table metadata to be imported
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sqoop 扫描数据库并收集要导入的表元数据
- en: Sqoop submits a map-only job and transfers the actual data using necessary metadata
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sqoop 提交一个仅 map 作业并使用必要的元数据传输实际数据
- en: 'The imported data is saved in HDFS folders. The user can specify alternative
    folders. The imported data is saved in a directory on HDFS, based on the table
    being imported. As is the case with most aspects of a Sqoop operation, the user
    can specify any alternative directory where the files should be populated. You
    can easily override the format in which data is copied over by explicitly specifying
    the field separator and record terminator characters. The user can use different
    formats, like Avro, ORC, Parquet, sequence files, text files, and so on, to store
    files onto HDFS, for example, importing a MySQL table to HDFS. The following is
    an example to import a MySQL table to HDFS:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 导入的数据存储在 HDFS 文件夹中。用户可以指定替代文件夹。导入的数据根据导入的表存储在 HDFS 的一个目录中。与 Sqoop 操作的大部分方面一样，用户可以指定任何替代目录，以便填充文件。您可以通过明确指定字段分隔符和记录终止符字符来轻松覆盖数据复制的格式。用户可以使用不同的格式，如
    Avro、ORC、Parquet、序列文件、文本文件等，将文件存储到 HDFS 上，例如，将 MySQL 表导入到 HDFS。以下是将 MySQL 表导入
    HDFS 的示例：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'On the Command Prompt, run the following `sqoop` command to import the MySQL
    sales database table, `customer`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令提示符下，运行以下 `sqoop` 命令以导入 MySQL 销售数据库表 `customer`：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Verify the `customer` folder on HDFS as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式验证 HDFS 上的 `customer` 文件夹：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s create an external Hive table to verify the records, as shown in the
    following snippet:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个外部 Hive 表来验证记录，如下面的片段所示：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '| **Custnum** | **Cust Fname** | **Cust Lname** | **Cust address** | **City**
    | **State** | **Zip** |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **Custnum** | **Cust Fname** | **Cust Lname** | **Cust address** | **City**
    | **State** | **Zip** |'
- en: '| 1 | James | Butt | 6649 N Blue Gum St | New Orleans | LA | 70116 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 1 | James | Butt | 6649 N Blue Gum St | New Orleans | LA | 70116 |'
- en: '| 2 | Art | Venere 8 | W Cerritos Ave #54 | Bridgeport | NJ | 8014 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 2 | Art | Venere 8 | W Cerritos Ave #54 | Bridgeport | NJ | 8014 |'
- en: '| 3 | Lenna | Paprocki | 639 Main St | Anchorage | AK | 99501 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 3 | Lenna | Paprocki | 639 Main St | Anchorage | AK | 99501 |'
- en: '| 4 | Donette | Foller | 34 Center St | Hamilton | OH | 45011 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 4 | Donette | Foller | 34 Center St | Hamilton | OH | 45011 |'
- en: '| 5 | Simona | Morasca | 3 Mcauley Dr | Ashland | OH | 44805 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 5 | Simona | Morasca | 3 Mcauley Dr | Ashland | OH | 44805 |'
- en: '| 6 | Mitsue | Tollner | 7 Eads St | Chicago | IL | 60632 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 6 | Mitsue | Tollner | 7 Eads St | Chicago | IL | 60632 |'
- en: '| 7 | Leota | Dilliard | 7 W Jackson Blvd | San Jose | CA | 95111 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 7 | Leota | Dilliard | 7 W Jackson Blvd | San Jose | CA | 95111 |'
- en: '| 8 | Sage | Wieser | 5 Boston Ave #88 | Sioux Falls | SD | 57105 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 8 | Sage | Wieser | 5 Boston Ave #88 | Sioux Falls | SD | 57105 |'
- en: '| 9 | Kris | Marrier | 228 Runamuck Pl #2808 | Baltimore | MD | 21224 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 9 | Kris | Marrier | 228 Runamuck Pl #2808 | Baltimore | MD | 21224 |'
- en: '| 10 | Minna | Amigon | 2371 Jerrold Ave | Kulpsville | PA | 19443 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 10 | Minna | Amigon | 2371 Jerrold Ave | Kulpsville | PA | 19443 |'
- en: 'The following is an example of importing a MySQL table to Hive:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是将MySQL表导入Hive的示例：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Verify the table Hive:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 验证Hive表：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You will see that the `customer_H` table is created under a default database.
    If you want to create the `customer_H` table under a different database, for example,
    a sales database, you have to create the sales database in advance. Also, you
    have to change the `-hive-table` parameter to the `--hive-table` sales `cutomer_H` incremental
    load (insert only). It''s a typical data load requirement of loading only the
    incremental changes happening in the source table. Let''s assume that a new customer, `11`,
    is inserted into the source `customer` MySQL table:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到`customer_H`表在默认数据库下创建。如果你想将`customer_H`表创建在不同的数据库下，例如，销售数据库，你必须事先创建销售数据库。此外，你必须将`-hive-table`参数更改为`--hive-table`
    sales `cutomer_H`增量加载（仅插入）。这是一个典型仅加载数据源表中发生增量变化的数据加载需求。让我们假设一个新的客户`11`被插入到源`customer`
    MySQL表中：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To accommodate only the new record (that is, customer 11), we have to add a
    few additional parameters to our original `sqoop` command. The new `sqoop` command
    is as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了仅适应新的记录（即客户11），我们不得不在我们的原始`sqoop`命令中添加一些额外的参数。新的`sqoop`命令如下：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After running this command, Sqoop will pick up only the new row (that is, `cust_num`,
    which is `11`):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此命令后，Sqoop将仅获取新行（即`cust_num`，其值为`11`）：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '| **Custnum** | **Cust Fname** | **Cust Lname** | **Cust address** | **City**
    | **State** | **Zip** |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| **客户编号** | **客户名** | **客户姓** | **客户地址** | **城市** | **州** | **邮编** |'
- en: '| 1 | James | Butt | 6649 N Blue Gum St | New Orleans | LA | 70116 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 1 | James | Butt | 6649 N Blue Gum St | New Orleans | LA | 70116 |'
- en: '| 2 | Art | Venere 8 | W Cerritos Ave #54 | Bridgeport | NJ | 8014 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 2 | Art | Venere 8 | W Cerritos Ave #54 | Bridgeport | NJ | 8014 |'
- en: '| 3 | Lenna | Paprocki | 639 Main St | Anchorage | AK | 99501 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 3 | Lenna | Paprocki | 639 Main St | Anchorage | AK | 99501 |'
- en: '| 4 | Donette | Foller | 34 Center St | Hamilton | OH | 45011 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 4 | Donette | Foller | 34 Center St | Hamilton | OH | 45011 |'
- en: '| 5 | Simona | Morasca | 3 Mcauley Dr | Ashland | OH | 44805 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 5 | Simona | Morasca | 3 Mcauley Dr | Ashland | OH | 44805 |'
- en: '| 6 | Mitsue | Tollner | 7 Eads St | Chicago | IL | 60632 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 6 | Mitsue | Tollner | 7 Eads St | Chicago | IL | 60632 |'
- en: '| 7 | Leota | Dilliard | 7 W Jackson Blvd | San Jose | CA | 95111 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 7 | Leota | Dilliard | 7 W Jackson Blvd | San Jose | CA | 95111 |'
- en: '| 8 | Sage | Wieser | 5 Boston Ave #88 | Sioux Falls | SD | 57105 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 8 | Sage | Wieser | 5 Boston Ave #88 | Sioux Falls | SD | 57105 |'
- en: '| 9 | Kris | Marrier | 228 Runamuck Pl #2808 | Baltimore | MD | 21224 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 9 | Kris | Marrier | 228 Runamuck Pl #2808 | Baltimore | MD | 21224 |'
- en: '| 10 | Minna | Amigon | 2371 Jerrold Ave | Kulpsville | PA | 19443 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 10 | Minna | Amigon | 2371 Jerrold Ave | Kulpsville | PA | 19443 |'
- en: '| 11 | Abel | Maclead | 25 E 75th St #69 | Los Angeles | CA | 90034 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 11 | Abel | Maclead | 25 E 75th St #69 | Los Angeles | CA | 90034 |'
- en: For incremental load we cannot update the data directly using Sqoop import.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于增量加载，我们无法直接使用Sqoop导入来更新数据。
- en: 'Please follow the steps in the given link for row-level updates: [http://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/](http://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/).
    [](http://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/) Now,
    let''s look at an example of importing a subset of a MySQL table into Hive. The
    following command shows how to import only a subset of a `customer` table in MySQL
    into Hive. For example, we have import-only customer data of `State = "OH"`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照给定链接中的步骤进行行级更新：[http://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/](http://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/)
    [](http://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/) 现在，让我们看看一个将MySQL表的子集导入Hive的示例。以下命令显示了如何将MySQL中`customer`表的子集仅导入Hive。例如，我们只导入了`State
    = "OH"`的客户数据：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '| **Custnum** | **Cust Fname** | **Cust Lname** | **Cust address** | **City**
    | **State** | **Zip** |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| **客户编号** | **客户名** | **客户姓** | **客户地址** | **城市** | **州** | **邮编** |'
- en: '| 4 | Donette | Foller | 34 Center St | Hamilton | OH | 45011 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 4 | Donette | Foller | 34 Center St | Hamilton | OH | 45011 |'
- en: '| 5 | Simona | Morasca | 3 Mcauley Dr | Ashland | OH | 44805 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 5 | Simona | Morasca | 3 Mcauley Dr | Ashland | OH | 44805 |'
- en: Import a MySQL table into an HBase table
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将MySQL表导入HBase表
- en: 'The following is a sample command to import data into an HBase table:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个将数据导入HBase表的示例命令：
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Sqoop imports data into the HBase table column family. The data is converted
    and inserted as a UTF-8 bytes format.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Sqoop将数据导入HBase表的列族。数据被转换为UTF-8字节格式。
- en: Sqoop export
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sqoop导出
- en: 'The following diagram shows the **Sqoop Export** function to export data from
    a Hadoop cluster:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了**Sqoop导出**功能，用于从Hadoop集群导出数据：
- en: '![](img/9d11503e-3fb0-466b-9bca-7370e6571495.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d11503e-3fb0-466b-9bca-7370e6571495.png)'
- en: Data processed in a data lake-like use case may be needed for additional business
    functions. Sqoop can be used to export that data back to RDBMS from HDFS or from
    a Hive table. In the case of exporting data back to an RDBMS table, the target
    table must exist in a MySQL database. The rows in HDFS files or records from a
    Hive table are given as input to the `sqoop` command and are called rows in a
    target table. Those records are read and parsed into a set of records and delimited
    with a user-specified delimiter.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在类似数据湖的使用案例中处理的数据可能需要用于额外的业务功能。Sqoop可以用于将那些数据从HDFS或Hive表导出到RDBMS。在将数据导回到RDBMS表的情况下，目标表必须存在于MySQL数据库中。HDFS文件中的行或Hive表中的记录作为`sqoop`命令的输入，并被称为目标表中的行。这些记录被读取并解析成一组记录，并以用户指定的分隔符分隔。
- en: 'The following are the commands to export data from HDFS to a MySQL table. Let''s
    create a table in MySQL to store data exported from HDFS:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从HDFS导出到MySQL表的命令。让我们在MySQL中创建一个表来存储从HDFS导出的数据：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `--table` parameter specifies the table which will be populated. Sqoop
    splits the data and uses individual map tasks to push the splits into the database.
    Each map task does the actual data transfer. The `--export-dir <directory h>`
    is the directory from which data will be exported:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`--table`参数指定了将被填充的表。Sqoop将数据拆分，并使用单独的map任务将拆分推入数据库。每个map任务执行实际的数据传输。`--export-dir
    <directory h>`是数据将从中导出的目录：'
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '| **Custnum** | **Cust Fname** | **Cust Lname** | **Cust Address** | **City**
    | **State** | **Zip** |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| **客户编号** | **客户名** | **客户姓** | **客户地址** | **城市** | **州** | **邮编** |'
- en: '| 1 | James | Butt | 6649 N Blue Gum St | New Orleans | LA | 70116 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 1 | James | Butt | 6649 N Blue Gum St | 新奥尔良 | LA | 70116 |'
- en: '| 2 | Art | Venere 8 | W Cerritos Ave #54 | Bridgeport | NJ | 8014 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 2 | Art | Venere 8 | W Cerritos Ave #54 | 桥港 | NJ | 8014 |'
- en: '| 3 | Lenna | Paprocki | 639 Main St | Anchorage | AK | 99501 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 3 | Lenna | Paprocki | 639 Main St | 安克雷奇 | AK | 99501 |'
- en: '| 4 | Donette | Foller | 34 Center St | Hamilton | OH | 45011 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 4 | Donette | Foller | 34 Center St | Hamilton | OH | 45011 |'
- en: '| 5 | Simona | Morasca | 3 Mcauley Dr | Ashland | OH | 44805 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 5 | Simona | Morasca | 3 Mcauley Dr | Ashland | OH | 44805 |'
- en: '| 6 | Mitsue | Tollner | 7 Eads St | Chicago | IL | 60632 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 6 | Mitsue | Tollner | 7 Eads St | 芝加哥 | IL | 60632 |'
- en: '| 7 | Leota | Dilliard | 7 W Jackson Blvd | San Jose | CA | 95111 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 7 | Leota | Dilliard | 7 W Jackson Blvd | 圣何塞 | CA | 95111 |'
- en: '| 8 | Sage | Wieser | 5 Boston Ave #88 | Sioux Falls | SD | 57105 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 8 | Sage | Wieser | 5 Boston Ave #88 | 硅谷 | SD | 57105 |'
- en: '| 9 | Kris | Marrier | 228 Runamuck Pl #2808 | Baltimore | MD | 21224 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 9 | Kris | Marrier | 228 Runamuck Pl #2808 | Baltimore | MD | 21224 |'
- en: '| 10 | Minna | Amigon | 2371 Jerrold Ave | Kulpsville | PA | 19443 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 10 | Minna | Amigon | 2371 Jerrold Ave | Kulpsville | PA | 19443 |'
- en: Flume
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Flume
- en: Flume is a reliable, available and distributed service to efficiently collect,
    aggregate, and transport large amounts of log data. It has a flexible and simple
    architecture that is based on streaming data flows. The current version of Apache
    Flume is 1.7.0, which was released in October 2016.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Flume是一个可靠、可用且分布式的服务，用于高效地收集、聚合和传输大量日志数据。它具有基于流数据流的灵活和简单架构。Apache Flume的当前版本是1.7.0，于2016年10月发布。
- en: Apache Flume architecture
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Flume架构
- en: 'The following diagram depicts the architecture of Apache Flume:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示描绘了Apache Flume的架构：
- en: '![](img/a2b108b9-89fc-46e0-9651-61c7ae3566b5.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2b108b9-89fc-46e0-9651-61c7ae3566b5.png)'
- en: 'Let''s take a closer look at the components of the Apache Flume architecture:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看Apache Flume架构的组件：
- en: '**Event**: An event is a byte payload with optional string headers. It represents
    the unit of data that Flume can carry from its source to destination.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件**：事件是一个带有可选字符串头部的字节有效负载。它代表Flume可以从其来源传输到目的地的数据单元。'
- en: '**Flow**: The transport of events from source to destination is considered
    a data flow, or just flow.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流**：从来源到目的地的事件传输被视为数据流，或简称流。'
- en: '**Agent**: It is an independent process that hosts the components of Flume,
    such as sources, channels, and sinks. It thus has the ability to receive, store,
    and forward events to its next-hop destination.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理**：它是一个独立进程，承载Flume的组件，如来源、通道和接收器。因此，它具有接收、存储并将事件转发到其下一跳目的地的能力。'
- en: '**Source**: The source is an interface implementation. It has the ability to
    consume events that are delivered to it with the help of a specific mechanism.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来源**：来源是一个接口实现。它具有通过特定机制消费发送给它的事件的 capability。'
- en: '**Channel**: It is a store where events are delivered to the channel through
    sources that operate within the agent. An event placed in a channel remains there
    until a sink takes it out for further transport. Channels play an important role
    in ensuring this.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通道**：它是一个存储库，事件通过在代理内运行的源传递到通道。放置在通道中的事件将保留在那里，直到汇入端将其取出以进行进一步传输。通道在确保这一点方面发挥着重要作用。'
- en: '**Sink**: It is an interface implementation, just like the source. It can remove
    events from a channel and transport them to the next agent in the flow, or to
    its final destination.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**汇入端（Sink）**：它是一个接口实现，就像源一样。它可以从通道中移除事件并将它们传输到流程中的下一个代理或最终目的地。'
- en: '**Interceptors**: They help to change an event in transit. An event can be
    removed or modified on the basis of the chosen criteria. Interceptors are the
    classes, which implement the `org.apache.flume.interceptor.Interceptor` interface.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拦截器（Interceptors）**：它们有助于在传输过程中更改事件。事件可以根据选择的准则被移除或修改。拦截器是实现 `org.apache.flume.interceptor.Interceptor`
    接口的类。'
- en: Data flow using Flume
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Flume 的数据流
- en: The entire Flume agent runs in a JVM process, which includes all the components
    (source, channel, and sink). The Flume source receives events from the external
    sources, like a web server, external files, and so on. The source pushes events
    to the channel, which stores it until picked up by the sink. The channel stores
    the payload (message stream) in either the local filesystem or in a memory, depending
    on the type of the source. For example, if the source is a file, the payload is
    stored locally. The sink picks up the payload from the channel and pushes it to
    external data stores. The source and sink within the agent run asynchronously.
    Sometimes, it may be possible for the sink to push the payload to yet another
    Flume agent. We will talk about that scenario in the next section.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 整个 Flume 代理都在 JVM 进程中运行，包括所有组件（源、通道和汇入端）。Flume 源从外部源接收事件，如 Web 服务器、外部文件等。源将事件推送到通道，通道将其存储，直到汇入端取走。通道根据源的类型将有效载荷（消息流）存储在本地文件系统或内存中。例如，如果源是文件，则有效载荷将本地存储。汇入端从通道中提取有效载荷并将其推送到外部数据存储。代理内的源和汇入端异步运行。有时，汇入端可能可以将有效载荷推送到另一个
    Flume 代理。我们将在下一节中讨论这种情况。
- en: Flume complex data flow architecture
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Flume 复杂数据流架构
- en: In the following architecture, there are three sources (servers). In order to
    pull data from the log files stored on these servers, we have to install Flume
    software on each of these servers. After installation, the filenames need to be
    added to the `flume.conf` file. Flume collects all the data from files and pushes
    it to the corresponding sink through channels. There are multiple sinks in the
    above architecture; Hive HDFS, and another sink, which is connected to another
    installation of the Flume agent installed on another server. It pushes data from
    sink to source and writes data to the Cassendra data store.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下架构中，有三个源（服务器）。为了从这些服务器上存储的日志文件中提取数据，我们必须在每个服务器上安装 Flume 软件。安装后，需要将文件名添加到
    `flume.conf` 文件中。Flume 收集所有文件数据并通过通道将其推送到相应的汇入端。上述架构中有多个汇入端；Hive HDFS，以及连接到另一个服务器上安装的另一个
    Flume 代理的另一个汇入端。它从汇入端将数据推送到源，并将数据写入 Cassandra 数据存储。
- en: Please note that this is not a good architecture, but I have mentioned it to
    explain how a Flume sink and Flume sources can be connected.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这不是一个好的架构，但我提到它是为了解释 Flume 汇入端和 Flume 源如何连接。
- en: 'The following diagram shows complex data flow involving multiple agents:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示展示了涉及多个代理的复杂数据流：
- en: '![](img/9888efdd-2fa8-48a2-be83-b794f8f5fd5d.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9888efdd-2fa8-48a2-be83-b794f8f5fd5d.png)'
- en: Flume setup
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Flume 设置
- en: 'Flume agent configuration is stored in a local text file. Please refer to the
    sample Flume agent configuration file in the code repository of this book. Flume
    1.7.0 supports various sources and sinks. Widely used Flume sources (a summary)
    are as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Flume 代理配置存储在本地文本文件中。请参阅本书代码库中的示例 Flume 代理配置文件。Flume 1.7.0 支持各种源和汇入端。广泛使用的 Flume
    源（摘要）如下：
- en: '| **Source** | **Description** |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| **源** | **描述** |'
- en: '| Avro source | Listens on Avro port and receives events from external Avro
    client streams |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Avro 源 | 监听 Avro 端口并从外部 Avro 客户端流接收事件 |'
- en: '| Exec source | Runs a given Unix command and expects that process to continuously
    produce data on standard out |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 执行源 | 运行给定的 Unix 命令并期望该进程持续在标准输出上产生数据 |'
- en: '| Spooling directory source | Ingests data from files on disk |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 存储目录源 | 从磁盘上的文件中摄取数据 |'
- en: '| Taildir source | Tails files in near real-time after new lines are detected
    in the files |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Taildir源 | 在检测到文件中的新行后，近实时地跟踪文件 |'
- en: '| Kafka source | Reads messages from Kafka topics |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Kafka源 | 从Kafka主题中读取消息 |'
- en: '| Syslog source | Reads syslog data (supports syslog-TCP and syslog-UDP) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Syslog源 | 读取syslog数据（支持syslog-TCP和syslog-UDP） |'
- en: '| HTTP source | Accepts Flume events by HTTP `POST` and `GET` |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| HTTP源 | 通过HTTP `POST` 和 `GET` 接受Flume事件 |'
- en: 'The widely used Flume sinks can be summarized as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛使用的Flume输出端可以总结如下：
- en: '| **Sink** | **Description** |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| **输出端** | **描述** |'
- en: '| Avro sink | Events are turned into Avro events and sent to the configured
    hostname/port pair |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Avro输出端 | 将事件转换为Avro事件并发送到配置的主机名/端口号 |'
- en: '| HDFS sink | Writes events into the HDFS |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| HDFS输出端 | 将事件写入HDFS |'
- en: '| Hive sink | Writes text or JSON data into a Hive table |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Hive输出端 | 将文本或JSON数据写入Hive表 |'
- en: '| HBase sink | Writes data to HBase |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| HBase输出端 | 将数据写入HBase |'
- en: '| Morphline Solr sink | Loads it in near real-time into Apache Solr servers
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Morphline Solr输出端 | 在近实时中将数据加载到Apache Solr服务器 |'
- en: '| Elasticsearch sink | Writes data to an Elasticsearch cluster |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Elasticsearch输出端 | 将数据写入Elasticsearch集群 |'
- en: '| Kafka sink | Writes data to a Kafka topic |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Kafka输出端 | 将数据写入Kafka主题 |'
- en: 'The widely used Flume channels (a summary) are as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛使用的Flume通道（总结）如下：
- en: '| **Channel** | **Description** |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| **通道** | **描述** |'
- en: '| JDBC channel | Events are stored in storage supported by database |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| JDBC通道 | 事件存储在数据库支持的存储中 |'
- en: '| Kafka channel | Events are stored in a Kafka cluster |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Kafka通道 | 事件存储在Kafka集群中 |'
- en: '| File channel | Events are stored in files |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 文件通道 | 事件存储在文件中 |'
- en: '| Spillable memory channel | Events are stored in memory; if memory gets full,
    then stored on disk |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 可溢出内存通道 | 事件存储在内存中；如果内存满了，则存储在磁盘上 |'
- en: 'The widely used Flume interceptors can be summarized as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛使用的Flume拦截器可以总结如下：
- en: '| **Interceptor** | **Description** |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| **拦截器** | **描述** |'
- en: '| Timestamp interceptor | Adds the processing time of an event into event headers
    |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 时间戳拦截器 | 将事件的处理时间添加到事件头中 |'
- en: '| Host interceptor | Adds hostname of agent |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 主机拦截器 | 添加代理的主机名 |'
- en: '| Search and replace interceptor | Supports Java regular expressions |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 搜索和替换拦截器 | 支持Java正则表达式 |'
- en: '| Regex filtering interceptor | Filters the events against RegEx |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 正则表达式过滤拦截器 | 对事件进行正则表达式过滤 |'
- en: '| Regex extractor interceptor | Extracts and appends the match RegEx groups
    as headers on the event |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 正则表达式提取拦截器 | 从事件中提取并附加匹配的正则表达式组作为事件头 |'
- en: Log aggregation use case
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志聚合用例
- en: 'In day-to-day business scenarios, we always find the need to get log files
    and make sense out of them. For example, we always find the need to get logs from
    different applications and servers and merge them together to find trends and
    patterns. Let me extend this example further. Let''s assume that we have five
    web servers deployed on five different servers. We want to get all five web server
    logs and merge/aggregate them together to analyze them further by storing one
    copy on HDFS and another copy to be shipped on to a Kafka topic for real-time
    analytics. The question is how we design Flume-based log aggregation architecture.
    The following is the Flume architecture for our web server log aggregation scenario:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在日常业务场景中，我们总是需要获取日志文件并对其进行分析。例如，我们总是需要从不同的应用程序和服务器中获取日志，并将它们合并在一起以找到趋势和模式。让我进一步扩展这个例子。假设我们有五个部署在五个不同服务器上的Web服务器。我们想要获取所有五个Web服务器的日志并将它们合并/聚合在一起，通过在HDFS上存储一份副本，并将另一份副本发送到Kafka主题进行实时分析。问题是我们是如何设计基于Flume的日志聚合架构的。以下是我们Web服务器日志聚合场景的Flume架构：
- en: '![](img/afaa72af-94ab-43dd-9f56-0f4c1e6083fc.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/afaa72af-94ab-43dd-9f56-0f4c1e6083fc.png)'
- en: 'Let us walk through the architecture in detail: There are a total of five web
    servers. Each web server generates a log file and stores it locally. The Flume
    agent is installed on each web server. The Flume agent is nothing but a (JVM)
    process that hosts the components through which events flow from an external source
    to the next destination (hop). Each Flume agent accesses log files based on local
    configuration of `flume.conf`. Each Flume agent reads the log files and pushes
    data to the Flume collector. Each line of the log file is treated as one message
    (a payload). The Flume collector gets messages from all web servers, fitters and
    aggregates all messages, and pushes these messages to the data store. The following
    is the sample `flume.conf` of the Flume agent and the collectors agent `flume.conf`:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细地了解架构：总共有五个 Web 服务器。每个 Web 服务器生成一个日志文件并将其本地存储。Flume 代理安装在每个 Web 服务器上。Flume
    代理实际上是一个（JVM）进程，它通过宿主事件从外部源流向下一个目的地（跳转）的组件。每个 Flume 代理根据 `flume.conf` 的本地配置访问日志文件。每个
    Flume 代理读取日志文件并将数据推送到 Flume 收集器。日志文件的每一行都被视为一条消息（有效负载）。Flume 收集器从所有 Web 服务器、适配器接收消息，聚合所有消息，并将这些消息推送到数据存储。以下是
    Flume 代理的示例 `flume.conf` 和收集器代理的 `flume.conf`：
- en: '[PRE14]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The collector `flume.conf` file is as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 收集器 `flume.conf` 文件如下：
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Apache NiFi
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache NiFi
- en: What is Apache NiFi? In any organization, we know that there is a variety of
    systems. Some systems generate the data and other systems consume that data. Apache
    NiFi is built to automate that data flow from one system to another. Apache NiFi
    is a data flow management system that comes with a web UI that helps to build
    data flows in real time. It supports flow-based programming. The graph programming
    includes a series of nodes and edges through which data moves. In NiFi, these
    nodes are translated into processors, and the edges into connectors. The data
    is stored in a packet of information called a **FlowFile**. This FlowFile includes
    content, attributes, and edges. As a user, you connect processors together using
    connectors to define how the data should be handled.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是 Apache NiFi？在任何组织中，我们都知道存在各种系统。一些系统生成数据，而其他系统则消费这些数据。Apache NiFi 是为了自动化数据从系统到系统的流动而构建的。Apache
    NiFi 是一个带有 Web UI 的数据流管理系统，它可以帮助实时构建数据流。它支持基于流的编程。图编程包括一系列节点和边，数据通过这些节点和边移动。在
    NiFi 中，这些节点被转换为处理器，而边被转换为连接器。数据存储在一个称为 **FlowFile** 的信息包中。这个 FlowFile 包括内容、属性和边。作为用户，您可以使用连接器将处理器连接起来，以定义数据应该如何处理。
- en: Main concepts of Apache NiFi
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache NiFi 的主要概念
- en: 'The following table describes the main components of Apache NiFi:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 下表描述了 Apache NiFi 的主要组件：
- en: '| **Component name** | **Description** |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| **组件名称** | **描述** |'
- en: '| FlowFile | Data packet running through the system |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| FlowFile | 在系统中运行的数据包 |'
- en: '| FlowFile processor | Performs the actual work of data routing, transformation,
    and data movement |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| FlowFile 处理器 | 执行数据路由、转换和数据移动的实际工作 |'
- en: '| Connetion | Actual data linkage between processors |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 连接 | 处理器之间的实际数据链接 |'
- en: '| Flow controller | Facilitates the exchange of FlowFiles between the processors
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 流控制器 | 促进处理器之间的 FlowFile 交换 |'
- en: '| Process group | Specific group of data inputs and data output processors
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 流程组 | 特定的数据输入和输出处理器组 |'
- en: Apache NiFi architecture
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache NiFi 架构
- en: 'The following diagram shows the components of the Apache NiFi architecture
    (source: [https://nifi.apache.org/docs.html](https://nifi.apache.org/docs.html)):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了 Apache NiFi 架构的组件（来源：[https://nifi.apache.org/docs.html](https://nifi.apache.org/docs.html))：
- en: '![](img/33e12830-db99-4454-8e0b-4d842a72c0c0.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/33e12830-db99-4454-8e0b-4d842a72c0c0.png)'
- en: 'The components are as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 组件如下：
- en: '**Web server**: This hosts NiFi''s HTTP-based UI'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Web 服务器**：这是 NiFi 的基于 HTTP 的 UI 的宿主'
- en: '**File controller**: This provides threads and manages the schedule for the
    extensions to run on'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文件控制器**：这提供线程并管理扩展运行的调度'
- en: '**Extensions**: The extensions operate and execute within the JVM'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展**：扩展在 JVM 中运行和执行'
- en: '**FileFlow repository**: This keeps track of the state of what it knows about
    a given FlowFile that is presently active in the flow'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FileFlow 存储库**：这跟踪它所知道的关于当前在流程中活动的给定 FlowFile 的状态'
- en: '**Content repository**: This is where the actual content bytes of a given FlowFile
    live'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内容存储库**：这是给定 FlowFile 的实际内容字节存储的地方'
- en: '**Provenance repository**: This is where all provenance event data is stored'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来源存储库**：这是所有来源事件数据存储的地方'
- en: Key features
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键特性
- en: 'The following are the key features of Apache NiFi:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Apache NiFi 的以下是一些关键特性：
- en: '**Guaranteed delivery**: In the event of increased volume of data, power failures,
    and network and system failures in NiFi, it becomes necessary to have a robust
    guaranteed delivery of the data. NiFi ensures, within the dataflow system itself,
    the transactional communication between NiFi and the data where it is coming to
    the points to which it is delivered to.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保证交付**：在数据量增加、电源故障、NiFi 中的网络和系统故障的情况下，确保数据的稳健交付变得必要。NiFi 确保在数据流系统内部，NiFi
    与其接收到的数据点之间的交易性通信。'
- en: '**Data buffering with back pressure and pressure release**: In any dataflow,
    it may be possible that there are some issues with the systems involved; some
    might be down or some might be slow. In that case, data buffering becomes very
    essential to coping with the data coming into or going out of the dataflow.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带背压和压力释放的数据缓冲**：在任何数据流中，都可能存在涉及系统的某些问题；一些可能已经关闭或运行缓慢。在这种情况下，数据缓冲变得非常关键，以应对进入或离开数据流的数据。'
- en: NiFi supports the buffering of all queues with back pressure when it reaches
    specific limits and age of the data. NiFi does it with a maximum possible throughput
    rate, while maintaining a good response time.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当 NiFi 达到特定限制和数据年龄时，它支持所有队列的带背压缓冲。NiFi 以最大可能的吞吐量率进行，同时保持良好的响应时间。
- en: '**Prioritized queuing**: In general, the data queues maintain natural order
    or insertion order. But, many times, when the rate of data insertion is faster
    than the bandwidth, you have to prioritize your data retrieval from the queue.
    The default is the oldest data first. But NiFi supports prioritization of queues
    to pull data out based on size, time, and so on that is, largest first or newest
    first.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优先级队列**：通常，数据队列保持自然顺序或插入顺序。但是，很多时候，当数据插入速率快于带宽时，您必须优先从队列中检索数据。默认情况下是先处理最旧的数据。但是，NiFi
    支持基于大小、时间等优先级队列，以拉取数据，即先处理最大的或最新的数据。'
- en: '**Flow-specific quality of service (QoS)**: There are some situations where
    we have to process the data in a specific time period, for example, within a second
    and so on, otherwise the data loses its value. The fine-grained flow of specific
    configuration of these concerns is enabled by Apache NiFi.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特定流的质量服务（QoS）**：有些情况下，我们必须在特定时间段内处理数据，例如，在一秒内等，否则数据会失去其价值。Apache NiFi 通过这些特定配置的细粒度流来启用这些关注点。'
- en: '**Data provenance**: NiFi automatically records, indexes, and makes available
    provenance data as objects flow through the system—even across fan-in, fan-out,
    transformations, and more. This information becomes extremely critical in supporting
    compliance, troubleshooting, optimization, and other scenarios.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据溯源**：NiFi 自动记录、索引并使系统中的对象流通过时的溯源数据可用——甚至包括扇入、扇出、转换等。这些信息在支持合规性、故障排除、优化和其他场景中变得极其关键。'
- en: '**Visual command and control**: Apache NiFi allows users to have interactive
    management of dataflow. It provides immediate feedback to each and every change
    to the dataflow. Hence, users understand and immediately correct any problems,
    mistakes, or issues in their dataflows. Based on analytical results of the dataflows,
    users can make changes to their dataflow, prioritize of queues, add more data
    flows, and so on.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可视化命令和控制**：Apache NiFi 允许用户对数据流进行交互式管理。它对数据流的每次更改都提供即时反馈。因此，用户可以理解和立即纠正他们数据流中的任何问题、错误或问题。基于数据流的分析结果，用户可以对其数据流进行更改，优先处理队列，添加更多数据流等。'
- en: '**Flow templates**: Data flows can be developed, designed, and shared. Templates
    allow subject matter experts to build and publish their flow designs and for others
    to benefit and collaborate on them.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流模板**：可以开发、设计和共享数据流。模板允许主题专家构建和发布他们的流设计，并允许其他人从中受益并协作。'
- en: '**Extension**: NiFi allows us to extend its key components.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展**：NiFi 允许我们扩展其关键组件。'
- en: '**Points of extension:** Processors, controller services, reporting tasks,
    prioritizers, and customer UIs.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展点**：处理器、控制器服务、报告任务、优先级排序器和客户 UI。'
- en: '**Multi-role security**: Multi-grained, multi-role security can be applied
    to each component, which allows the admin user to have a fine-grained level of
    access control.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多角色安全**：多粒度、多角色安全可以应用于每个组件，这允许管理员用户拥有细粒度的访问控制级别。'
- en: '**Clustering**: NiFi is designed to scale-out through the use of clustering
    many nodes together. That way, it can handle more data by adding more nodes to
    the cluster.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：NiFi 通过将多个节点组合在一起进行扩展设计。这样，通过向集群添加更多节点，它可以处理更多数据。'
- en: For getting started with Apache NiFi, please use this link: [https://nifi.apache.org/docs/nifi-docs/html/getting-started.html](https://nifi.apache.org/docs/nifi-docs/html/getting-started.html).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用 Apache NiFi，请使用此链接： [https://nifi.apache.org/docs/nifi-docs/html/getting-started.html](https://nifi.apache.org/docs/nifi-docs/html/getting-started.html)。
- en: 'Let''s imagine a scenario. I have a running log file. It is updated on the
    fly. I want to capture and monitor each line in that file, based on its contents.
    I want to send it to my Kafka brokers. I also want to deliver all my error records
    to HDFS for archival and further analysis. Different line types will be sent to
    different Kafka brokers. For example, error, info, and success types will be sent
    to three different Kafka topics, namely error, info, and success. I have developed
    the following NiFi workflow for that. The following table gives the detailed explanation
    of each processor:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设想一个场景。我有一个正在运行的日志文件。它实时更新。我想根据其内容捕获和监控该文件中的每一行，并将其发送到我的 Kafka 代理。我还想将所有错误记录发送到
    HDFS 以进行归档和进一步分析。不同类型的行将被发送到不同的 Kafka 代理。例如，错误、信息和成功类型的行将被发送到三个不同的 Kafka 主题，即错误、信息和成功。为此，我开发了以下
    NiFi 工作流程。以下表格详细说明了每个处理器的说明：
- en: '| **Processor** | **Purpose** | **Property** | **Value** |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| **处理器** | **目的** | **属性** | **值** |'
- en: '| TailFile | To tail log files | File to tail | `/var/log/apache.log` |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| TailFile | 尾随日志文件 | 要尾随的文件 | `/var/log/apache.log` |'
- en: '| SplitText | To split the log entries to the lines | Line split count | `1`
    |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| SplitText | 将日志条目拆分为行 | 行拆分计数 | `1` |'
- en: '| RouteOnContent | To make a routing decision |  |  |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| RouteOnContent | 进行路由决策 |  |  |'
- en: '| PutHDFS | To send errors to HDFS |  | HDFS details |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| PutHDFS | 将错误发送到 HDFS |  | HDFS 详细信息 |'
- en: '| PublishKafka | To deliver data to Kafka topic | Broker and topic name | Hostname:
    port, topic pane |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| PublishKafka | 向 Kafka 主题发送数据 | 代理和主题名称 | 主机名：端口，主题面板 |'
- en: Real-time log capture dataflow
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时日志捕获数据流
- en: 'The following example workflow shows how log file data can be pushed to HDFS
    and then to moved to Kafka brokers:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例工作流程展示了如何将日志文件数据推送到 HDFS，然后将其移动到 Kafka 代理：
- en: '![](img/ff6f0904-e113-449d-ae0c-df29aebdb730.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff6f0904-e113-449d-ae0c-df29aebdb730.png)'
- en: Kafka Connect
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Connect
- en: 'Kafka Connect is a part of Apache Kafka. It is a framework to ingest data from
    one to another system using connectors. There are two types of connectors: source
    connectors and sink connectors. The sink connectors import data from source systems
    and write to Kafka topics. The sink connectors read data from the Kafka topic
    and export it to target systems. Kafka Connect provides various source and sink
    connectors out of the box.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Connect 是 Apache Kafka 的一部分。它是一个使用连接器从一种系统到另一种系统摄取数据的框架。有两种类型的连接器：源连接器和目标连接器。目标连接器从源系统导入数据并将其写入
    Kafka 主题。目标连接器从 Kafka 主题读取数据并将其导出到目标系统。Kafka Connect 提供了各种内置的源和目标连接器。
- en: Kafka Connect – a brief history
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Connect 的简要历史
- en: Kafka Connect was mainly introduced in November 2015 in Kafka 0.9.x. In addition
    to the various features of Kafka 0.9.x, Connect APIs was a brand new feature.
    Then, in May 2016, the new version Kafka 0.10.0 was released. In that version,
    Kafka Streams API was a new and exciting feature. But, in March 2017, it was Kafka
    Version 0.10.2 where Kafka Connect got its real momentum. As a part of Kafka 0.10.2,
    improved simplified Connect APIs and single message transform APIs were released.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Connect 主要在 2015 年 11 月的 Kafka 0.9.x 版本中引入。除了 Kafka 0.9.x 的各种功能外，Connect
    API 是一个全新的功能。然后，在 2016 年 5 月，发布了新版本 Kafka 0.10.0。在该版本中，Kafka Streams API 是一个新且令人兴奋的功能。但是，在
    2017 年 3 月，Kafka 版本 0.10.2 是 Kafka Connect 获得真正动力的版本。作为 Kafka 0.10.2 的一部分，发布了改进的简化
    Connect API 和单消息转换 API。
- en: Why Kafka Connect?
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择 Kafka Connect？
- en: Kafka Connect helps to simplify getting data in and out of Kafka. It provides
    a lot of connectors to do that out of the box. In my opinion, that's the best
    incentive to a developer like me, because I do not have to develop a separate
    code to develop my own connector to import and export data; I can always reuse
    the out-of-the-box connector for that. Also, if I want, I can always develop my
    own unique connector using Kafka Connect APIs. Also, all the connectors are configuration-based.
    The common sources and targets are databases, search engines, NoSQL data stores,
    and applications like SAP, GoldenGate, Salesforce, HDFS, Elasticsearch, and so
    on. For a detailed listing of all available sources and connectors, please refer
    to [https://www.confluent.io/product/connectors/](https://www.confluent.io/product/connectors/).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Connect 有助于简化数据进出 Kafka 的过程。它提供大量开箱即用的连接器来完成这项工作。在我看来，这是像我这样的开发者最好的激励，因为我无需开发单独的代码来开发自己的连接器以导入和导出数据；我总是可以重用开箱即用的连接器。此外，如果我想，我总是可以使用
    Kafka Connect API 开发自己的独特连接器。此外，所有连接器都是基于配置的。常见的源和目标包括数据库、搜索引擎、NoSQL 数据存储以及像 SAP、GoldenGate、Salesforce、HDFS、Elasticsearch
    等应用程序。有关所有可用源和连接器的详细列表，请参阅 [https://www.confluent.io/product/connectors/](https://www.confluent.io/product/connectors/)。
- en: Kafka Connect features
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Connect 特性
- en: 'The following are some features of Kafka connect:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些 Kafka Connect 的特性：
- en: '**Scalable**: This is a framework for scalable and reliable streaming data
    between Apache Kafka and other systems'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展**：这是一个在 Apache Kafka 和其他系统之间进行可扩展和可靠流数据传输的框架'
- en: '**Simple**: This makes it simple to define connectors that move large collections
    of data into and out of Kafka'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单**：这使得定义将大量数据移动到和从 Kafka 中移动的连接器变得简单'
- en: '**Offset management**: The framework does most of the hard work of properly
    recording the offsets of the connectors'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏移量管理**：框架负责记录连接器的偏移量的大部分繁重工作'
- en: '**Easy operation**: This has a service that has a RESTful API for managing
    and deploying connectors'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易于操作**：这提供了一个具有 RESTful API 的服务，用于管理和部署连接器'
- en: '**Distributed**: The framework can be clustered and will automatically distribute
    the connectors across the cluster, ensuring that the connector is always running'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式**：框架可以集群化，并将连接器自动分布到集群中，确保连接器始终运行'
- en: '**Out-of-the-box connectors**: For a detailed listing of all available sources
    and connectors, please refer to [https://www.confluent.io/product/connectors/](https://www.confluent.io/product/connectors/)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开箱即用的连接器**：有关所有可用源和连接器的详细列表，请参阅 [https://www.confluent.io/product/connectors/](https://www.confluent.io/product/connectors/)'
- en: Kafka Connect architecture
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Connect 架构
- en: 'The following diagram represents the Kafka Connect architecture:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表示 Kafka Connect 架构：
- en: '![](img/2a2af5eb-0cbb-4e6c-b3b8-1511e6d0c823.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2a2af5eb-0cbb-4e6c-b3b8-1511e6d0c823.png)'
- en: 'The Kafka cluster is made of Kafka brokers: three brokers, as shown in the
    diagram. Sources can be of any type, for example, databases, NoSQL, Twitter, and
    so on. In between the source and Kafka cluster, there is a Kafka Connect cluster,
    which is made up of workers. The working of Kafka Connect comprises the following
    steps:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 集群由 Kafka 代理组成：如图所示，有三个代理。源可以是任何类型，例如，数据库、NoSQL、Twitter 等。在源和 Kafka 集群之间，有一个
    Kafka Connect 集群，由工作节点组成。Kafka Connect 的工作包括以下步骤：
- en: Workers, based on configuration, pull data from sources
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工作节点根据配置从源中拉取数据
- en: After getting data, the connector pushes data to the Kafka cluster
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在获取数据后，连接器将数据推送到 Kafka 集群
- en: If data needs to be transformed, filtered, joined, or aggregated using stream
    applications such as Spark, Storm, and so on, stream APIs will change data in
    and out of Kafka
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果需要使用流应用程序（如 Spark、Storm 等）对数据进行转换、过滤、连接或聚合，流 API 将改变 Kafka 中的数据
- en: Based on the configuration, the connector will pull data out of Kafka and write
    it to the sink
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据配置，连接器将从 Kafka 中拉取数据并写入到目标
- en: 'Some Kafka Connect concepts are as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 一些 Kafka Connect 概念如下：
- en: Source connectors get data from common data sources.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源连接器从常见的数据源获取数据。
- en: Sink connectors publish data to common data sources.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标连接器将数据发布到常见的数据源。
- en: Kafka Connect makes it easy to quickly get data reliably into Kafka.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka Connect 使得将数据可靠地快速导入 Kafka 变得容易。
- en: It is a part of the ETL pipeline.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是 ETL 管道的一部分。
- en: Scaling from a small pipeline to a company-wide pipeline is very easy.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从小型管道扩展到公司级管道非常容易。
- en: The code is reusable.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码是可重用的。
- en: The Kafka Connect cluster has multiple loaded connectors. Each connector is
    a reusable piece of code, `(Java JARs)`. There are a lot of open source connectors
    available, which can be leveraged.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka Connect 集群有多个加载的连接器。每个连接器是一段可重用的代码，`(Java JARs)`。有许多开源连接器可用，可以加以利用。
- en: Each connector task is a combination of connector class and configuration. A
    task is linked to a connector configuration. A job creation may create multiple
    tasks. So, if you have one connector and one configuration, then two or more tasks
    can be created.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个连接器任务是由连接器类和配置的组合。任务链接到连接器配置。作业创建可能会创建多个任务。因此，如果您有一个连接器和配置，则可以创建两个或更多任务。
- en: Kafka Connect workers and servers execute tasks. A worker is a single java process.
    A worker can be standalone or distributed.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka Connect 工作者和服务器执行任务。工作者是一个单一的 Java 进程。工作者可以是独立的或分布式的。
- en: Kafka Connect workers modes
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Connect 工作者模式
- en: 'There are two modes of Kafka Connect workers:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Connect 工作者有两种模式：
- en: Standalone mode
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独立模式
- en: Distributed mode
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式模式
- en: Standalone mode
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独立模式
- en: Standalone mode is a single process (worker) that runs all the connectors and
    tasks. The configuration is bundled in a single process. It is not fault tolerant
    or scalable, and it is very difficult to monitor. Since it is easy to set up,
    it is mainly used during development and testing.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 独立模式是一个运行所有连接器和任务的单一进程（工作者）。配置捆绑在一个进程中。它不具有容错性或可扩展性，并且很难监控。由于它易于设置，因此主要用于开发和测试。
- en: Distributed mode
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式模式
- en: With distributed mode, multiple workers (processes) run your connectors and
    tasks. The configuration is submitted using the REST API. It is scalable and fault
    tolerant. It automatically rebalances all the tasks on the cluster if any worker
    dies. Since it is scalable and fault tolerant, it is mainly used in a production
    environment.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式模式下，多个工作者（进程）运行您的连接器和任务。配置通过 REST API 提交。它是可扩展的和容错的。如果任何工作者死亡，它会自动在集群上重新平衡所有任务。由于它可扩展且容错，因此主要用于生产环境。
- en: Kafka Connect cluster distributed architecture
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Connect 集群分布式架构
- en: 'The following is the representation of the Kafka Connect cluster distributed
    architecture details:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对 Kafka Connect 集群分布式架构细节的表示：
- en: '![](img/9a72723b-b90b-4c5d-a5ad-525ebb59d0f4.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a72723b-b90b-4c5d-a5ad-525ebb59d0f4.png)'
- en: 'In the preceding diagram, we can see the following details:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到以下细节：
- en: We have source **Connector 1**, with three tasks: **Task 1**, **Task 2**, and
    **Task 3**. These three tasks are spread out among four workers: **Worker 1**,
    **Worker 3**, and **Worker 4**.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有一个带有三个任务的**连接器 1**：**任务 1**、**任务 2**和**任务 3**。这三个任务分布在四个工作者之间：**工作者 1**、**工作者
    3**和**工作者 4**。
- en: 'We also have source **Connector 2**, with two tasks: **Task 1** and **Task
    2**. These two tasks are spread out between two workers: **Worker 2** and **Worker
    3**.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还有一个带有两个任务的**连接器 2**：**任务 1**和**任务 2**。这两个任务分布在两个工作者之间：**工作者 2**和**工作者 3**。
- en: 'We also have sink **Connector 3** with four tasks: **Task 1**, **Task 2**,
    **Task 3**, and **Task 4**. These four tasks are spread out among four workers:
    **Worker 1**, **Worker 2**, **Worker 3**, and **Worker 4**.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还有一个带有四个任务的**连接器 3**：**任务 1**、**任务 2**、**任务 3**和**任务 4**。这四个任务分布在四个工作者之间：**工作者
    1**、**工作者 2**、**工作者 3**和**工作者 4**。
- en: Now, something happens and **Worker 4** dies and we lose that worker completely.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，发生了一些事情，**工作者 4** 死机了，我们完全失去了这个工作者。
- en: As a part of fault tolerance, the rebalance activity kicks off. **Connector
    1** and **Task 3** move from **Worker 4** to **Worker 2**. Similarly, **Connector
    3** and **Task 4** move from **Connector 4** to **Connector 1**.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为容错的一部分，重新平衡活动启动。**连接器 1**和**任务 3**从**工作者 4**移动到**工作者 2**。同样，**连接器 3**和**任务
    4**从**连接器 4**移动到**连接器 1**。
- en: 'The following diagram represents the Kafka Connect cluster after rebalance:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表示重新平衡后的 Kafka Connect 集群：
- en: '![](img/e65c676a-a02c-4805-a38a-95770edbdcf8.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e65c676a-a02c-4805-a38a-95770edbdcf8.png)'
- en: Example 1
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 1
- en: 'Streamed data from source file `Demo-Source.txt` is moved to destination file
    `Demo-Sink.txt` in standalone mode, as shown in the following diagram:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在独立模式下，从源文件 `Demo-Source.txt` 流式传输的数据被移动到目标文件 `Demo-Sink.txt`，如下所示：
- en: '![](img/4682bda0-9403-4eb5-a809-14abaf451b54.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4682bda0-9403-4eb5-a809-14abaf451b54.png)'
- en: 'In order to stream data from source file `Demo-Source.txt` to destination file
    `Demo-Sink.txt` in standalone mode, we need to perform the following steps:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在独立模式下从源文件 `Demo-Source.txt` 流式传输数据到目标文件 `Demo-Sink.txt`，我们需要执行以下步骤：
- en: 'Start Kafka:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 Kafka：
- en: '[PRE16]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Create topic:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建主题：
- en: '[PRE17]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Configure the `source-file-stream-standalone.properties` file:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置`source-file-stream-standalone.properties`文件：
- en: '[PRE18]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Configure `file-stream-standalone.properties` file:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置`file-stream-standalone.properties`文件：
- en: '[PRE19]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Configure `file-worker.properties` file:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置`file-worker.properties`文件：
- en: '[PRE20]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Start Kafka Connect. Open another terminal and run the following command:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Kafka Connect。打开另一个终端并运行以下命令：
- en: '[PRE21]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Add data to the `demo-source.txt` file. Open another terminal and run the following
    command:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向`demo-source.txt`文件添加数据。打开另一个终端并运行以下命令：
- en: '[PRE22]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Read the `demo-sink.txt` file:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取`demo-sink.txt`文件：
- en: '[PRE23]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Example 2
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 2
- en: 'Streamed data from source file `Demo-Source.txt` is moved to destination file
    `Demo-Sink.txt` in distributed mode. If you want to run the previous example using
    distributed mode, you have to add the following parameter to `source-file-stream`
    and `sink-file-stream` in *steps 3* and *4*:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 从源文件`Demo-Source.txt`流式传输的数据在分布式模式下移动到目标文件`Demo-Sink.txt`。如果您想使用分布式模式运行前面的示例，您必须在步骤3和步骤4中的`source-file-stream`和`sink-file-stream`中添加以下参数：
- en: '[PRE24]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Summary
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we have learned all the popular data ingestion tools used in
    production environments. Sqoop is mainly used to import and export data in and
    out of RDBMS data stores. Apache Flume is used in real-time systems to import
    data, mainly from files sources. It supports a wide variety of sources and sinks.
    Apache NiFi is a fairly new tool and getting very popular these days. It also
    supports GUI-based ETL development. Hortonworks has started supporting this tool
    since their HDP 2.4 release. Apache Kafka Connect is another popular tool in the
    market. It is also a part of the Confluent Data Platform. Kafka Connect can ingest
    entire databases or collect metrics from all your application servers into Kafka
    topics, making the data available for stream processing with low latency.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了在生产环境中使用的所有流行数据摄取工具。Sqoop主要用于在关系型数据库管理系统（RDBMS）数据存储中导入和导出数据。Apache
    Flume用于实时系统以导入数据，主要来自文件源。它支持广泛的源和目标。Apache NiFi是一个相对较新的工具，最近非常受欢迎。它还支持基于GUI的ETL开发。Hortonworks从他们的HDP
    2.4版本开始支持这个工具。Apache Kafka Connect是市场上另一个流行的工具。它也是Confluent数据平台的一部分。Kafka Connect可以摄取整个数据库或从所有应用程序服务器收集指标到Kafka主题中，使数据能够以低延迟进行流处理。
- en: Since we so far know how to build Hadoop clusters and how to ingest data in
    them, we will learn data modeling techniques in the next chapter.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们到目前为止已经知道如何构建Hadoop集群以及如何在其中摄取数据，我们将在下一章学习数据建模技术。
