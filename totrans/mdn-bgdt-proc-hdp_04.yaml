- en: Data Movement Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we learned about how to create and configure a Hadoop cluster,
    HDFS architecture, various file formats, and the best practices for a Hadoop cluster.
    We also learned about Hadoop high availability techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Since we now know how to create and configure a Hadoop cluster, in this chapter,
    we will learn about various techniques of data ingestion into a Hadoop cluster.
    We know about the advantages of Hadoop, but now, we need data in our Hadoop cluster
    to utilize its real power.
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion is considered the very first step in the Hadoop data life cycle.
    Data can be ingested into Hadoop as either a batch or a (real-time) stream of
    records. Hadoop is a complete ecosystem, and MapReduce is a batch ecosystem of
    Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows various data ingestion tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec9306a3-4d18-460e-91bb-1376bdbd2ac4.png)'
  prefs: []
  type: TYPE_IMG
- en: We will learn about each tool in detail in the next few sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following methods of transferring data to
    and from our Hadoop cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Sqoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Flume
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache NiFi
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Kafka Connect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch processing versus real-time processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we dive deep into different data ingestion techniques, let's discuss
    the difference between batch and real-time (stream) processing. The following
    explains the difference between these two ecosystems.
  prefs: []
  type: TYPE_NORMAL
- en: Batch processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following points describe the batch processing system:'
  prefs: []
  type: TYPE_NORMAL
- en: Very efficient in processing a high volume of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All data processing steps (that is, data collection, data ingestion, data processing,
    and results presentation) are done as one single batch job.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughput carries more importance than latency. Latency is always more than
    a single minute.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughput directly depends on the size of the data and available computational
    system resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Available tools include Apache Sqoop, MapReduce jobs, Spark jobs, Hadoop DistCp
    utility, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following points describe how real-time processing is different from batch
    processing:'
  prefs: []
  type: TYPE_NORMAL
- en: Latency is extremely important, for example, less than one second
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computation is relatively simple
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data is processed as an independent unit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Available tools include Apache Storm, Spark Streaming, Apache Fink, Apache Kafka,
    and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Sqoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Sqoop is a tool designed for efficiently transferring bulk data between
    a Hadoop cluster and structured data stores, such as relational databases. In
    a typical use case, such as a data lake, there is always a need to import data
    from RDBMS-based data warehouse stores into the Hadoop cluster. After data import
    and data aggregation, the data needs to be exported back to RDBMS. Sqoop allows
    easy import and export of data from structured data stores like RDBMS, enterprise
    data warehouses, and NoSQL systems. With the help of Sqoop, data can be provisioned
    from external systems into a Hadoop cluster and populate tables in Hive and HBase.
    Sqoop uses a connector-based architecture, which supports plugins that provide
    connectivity to external systems. Internally, Sqoop uses MapReduce algorithms
    to import and export data. By default, all Sqoop jobs run four map jobs. We will
    see Sqoop import and export functions in detail in the next few sections.
  prefs: []
  type: TYPE_NORMAL
- en: Sqoop Import
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following diagram shows the **Sqoop Import** function to import data from
    an RDBMS table into a Hadoop cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c39b5ad8-a2ea-4dec-b372-d3eeb008a52d.png)'
  prefs: []
  type: TYPE_IMG
- en: Import into HDFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a sample command to import data into HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The import is done in two steps, which are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Sqoop scans the database and collects the table metadata to be imported
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sqoop submits a map-only job and transfers the actual data using necessary metadata
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The imported data is saved in HDFS folders. The user can specify alternative
    folders. The imported data is saved in a directory on HDFS, based on the table
    being imported. As is the case with most aspects of a Sqoop operation, the user
    can specify any alternative directory where the files should be populated. You
    can easily override the format in which data is copied over by explicitly specifying
    the field separator and record terminator characters. The user can use different
    formats, like Avro, ORC, Parquet, sequence files, text files, and so on, to store
    files onto HDFS, for example, importing a MySQL table to HDFS. The following is
    an example to import a MySQL table to HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'On the Command Prompt, run the following `sqoop` command to import the MySQL
    sales database table, `customer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify the `customer` folder on HDFS as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create an external Hive table to verify the records, as shown in the
    following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '| **Custnum** | **Cust Fname** | **Cust Lname** | **Cust address** | **City**
    | **State** | **Zip** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | James | Butt | 6649 N Blue Gum St | New Orleans | LA | 70116 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Art | Venere 8 | W Cerritos Ave #54 | Bridgeport | NJ | 8014 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Lenna | Paprocki | 639 Main St | Anchorage | AK | 99501 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Donette | Foller | 34 Center St | Hamilton | OH | 45011 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Simona | Morasca | 3 Mcauley Dr | Ashland | OH | 44805 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Mitsue | Tollner | 7 Eads St | Chicago | IL | 60632 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Leota | Dilliard | 7 W Jackson Blvd | San Jose | CA | 95111 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Sage | Wieser | 5 Boston Ave #88 | Sioux Falls | SD | 57105 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Kris | Marrier | 228 Runamuck Pl #2808 | Baltimore | MD | 21224 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | Minna | Amigon | 2371 Jerrold Ave | Kulpsville | PA | 19443 |'
  prefs: []
  type: TYPE_TB
- en: 'The following is an example of importing a MySQL table to Hive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify the table Hive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see that the `customer_H` table is created under a default database.
    If you want to create the `customer_H` table under a different database, for example,
    a sales database, you have to create the sales database in advance. Also, you
    have to change the `-hive-table` parameter to the `--hive-table` sales `cutomer_H` incremental
    load (insert only). It''s a typical data load requirement of loading only the
    incremental changes happening in the source table. Let''s assume that a new customer, `11`,
    is inserted into the source `customer` MySQL table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To accommodate only the new record (that is, customer 11), we have to add a
    few additional parameters to our original `sqoop` command. The new `sqoop` command
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this command, Sqoop will pick up only the new row (that is, `cust_num`,
    which is `11`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '| **Custnum** | **Cust Fname** | **Cust Lname** | **Cust address** | **City**
    | **State** | **Zip** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | James | Butt | 6649 N Blue Gum St | New Orleans | LA | 70116 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Art | Venere 8 | W Cerritos Ave #54 | Bridgeport | NJ | 8014 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Lenna | Paprocki | 639 Main St | Anchorage | AK | 99501 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Donette | Foller | 34 Center St | Hamilton | OH | 45011 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Simona | Morasca | 3 Mcauley Dr | Ashland | OH | 44805 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Mitsue | Tollner | 7 Eads St | Chicago | IL | 60632 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Leota | Dilliard | 7 W Jackson Blvd | San Jose | CA | 95111 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Sage | Wieser | 5 Boston Ave #88 | Sioux Falls | SD | 57105 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Kris | Marrier | 228 Runamuck Pl #2808 | Baltimore | MD | 21224 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | Minna | Amigon | 2371 Jerrold Ave | Kulpsville | PA | 19443 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | Abel | Maclead | 25 E 75th St #69 | Los Angeles | CA | 90034 |'
  prefs: []
  type: TYPE_TB
- en: For incremental load we cannot update the data directly using Sqoop import.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please follow the steps in the given link for row-level updates: [http://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/](http://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/).
    [](http://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/) Now,
    let''s look at an example of importing a subset of a MySQL table into Hive. The
    following command shows how to import only a subset of a `customer` table in MySQL
    into Hive. For example, we have import-only customer data of `State = "OH"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '| **Custnum** | **Cust Fname** | **Cust Lname** | **Cust address** | **City**
    | **State** | **Zip** |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Donette | Foller | 34 Center St | Hamilton | OH | 45011 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Simona | Morasca | 3 Mcauley Dr | Ashland | OH | 44805 |'
  prefs: []
  type: TYPE_TB
- en: Import a MySQL table into an HBase table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a sample command to import data into an HBase table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Sqoop imports data into the HBase table column family. The data is converted
    and inserted as a UTF-8 bytes format.
  prefs: []
  type: TYPE_NORMAL
- en: Sqoop export
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following diagram shows the **Sqoop Export** function to export data from
    a Hadoop cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d11503e-3fb0-466b-9bca-7370e6571495.png)'
  prefs: []
  type: TYPE_IMG
- en: Data processed in a data lake-like use case may be needed for additional business
    functions. Sqoop can be used to export that data back to RDBMS from HDFS or from
    a Hive table. In the case of exporting data back to an RDBMS table, the target
    table must exist in a MySQL database. The rows in HDFS files or records from a
    Hive table are given as input to the `sqoop` command and are called rows in a
    target table. Those records are read and parsed into a set of records and delimited
    with a user-specified delimiter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the commands to export data from HDFS to a MySQL table. Let''s
    create a table in MySQL to store data exported from HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `--table` parameter specifies the table which will be populated. Sqoop
    splits the data and uses individual map tasks to push the splits into the database.
    Each map task does the actual data transfer. The `--export-dir <directory h>`
    is the directory from which data will be exported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '| **Custnum** | **Cust Fname** | **Cust Lname** | **Cust Address** | **City**
    | **State** | **Zip** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | James | Butt | 6649 N Blue Gum St | New Orleans | LA | 70116 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Art | Venere 8 | W Cerritos Ave #54 | Bridgeport | NJ | 8014 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Lenna | Paprocki | 639 Main St | Anchorage | AK | 99501 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Donette | Foller | 34 Center St | Hamilton | OH | 45011 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Simona | Morasca | 3 Mcauley Dr | Ashland | OH | 44805 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Mitsue | Tollner | 7 Eads St | Chicago | IL | 60632 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Leota | Dilliard | 7 W Jackson Blvd | San Jose | CA | 95111 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Sage | Wieser | 5 Boston Ave #88 | Sioux Falls | SD | 57105 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Kris | Marrier | 228 Runamuck Pl #2808 | Baltimore | MD | 21224 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | Minna | Amigon | 2371 Jerrold Ave | Kulpsville | PA | 19443 |'
  prefs: []
  type: TYPE_TB
- en: Flume
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Flume is a reliable, available and distributed service to efficiently collect,
    aggregate, and transport large amounts of log data. It has a flexible and simple
    architecture that is based on streaming data flows. The current version of Apache
    Flume is 1.7.0, which was released in October 2016.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Flume architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following diagram depicts the architecture of Apache Flume:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2b108b9-89fc-46e0-9651-61c7ae3566b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a closer look at the components of the Apache Flume architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Event**: An event is a byte payload with optional string headers. It represents
    the unit of data that Flume can carry from its source to destination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flow**: The transport of events from source to destination is considered
    a data flow, or just flow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agent**: It is an independent process that hosts the components of Flume,
    such as sources, channels, and sinks. It thus has the ability to receive, store,
    and forward events to its next-hop destination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Source**: The source is an interface implementation. It has the ability to
    consume events that are delivered to it with the help of a specific mechanism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Channel**: It is a store where events are delivered to the channel through
    sources that operate within the agent. An event placed in a channel remains there
    until a sink takes it out for further transport. Channels play an important role
    in ensuring this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sink**: It is an interface implementation, just like the source. It can remove
    events from a channel and transport them to the next agent in the flow, or to
    its final destination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interceptors**: They help to change an event in transit. An event can be
    removed or modified on the basis of the chosen criteria. Interceptors are the
    classes, which implement the `org.apache.flume.interceptor.Interceptor` interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data flow using Flume
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The entire Flume agent runs in a JVM process, which includes all the components
    (source, channel, and sink). The Flume source receives events from the external
    sources, like a web server, external files, and so on. The source pushes events
    to the channel, which stores it until picked up by the sink. The channel stores
    the payload (message stream) in either the local filesystem or in a memory, depending
    on the type of the source. For example, if the source is a file, the payload is
    stored locally. The sink picks up the payload from the channel and pushes it to
    external data stores. The source and sink within the agent run asynchronously.
    Sometimes, it may be possible for the sink to push the payload to yet another
    Flume agent. We will talk about that scenario in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Flume complex data flow architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following architecture, there are three sources (servers). In order to
    pull data from the log files stored on these servers, we have to install Flume
    software on each of these servers. After installation, the filenames need to be
    added to the `flume.conf` file. Flume collects all the data from files and pushes
    it to the corresponding sink through channels. There are multiple sinks in the
    above architecture; Hive HDFS, and another sink, which is connected to another
    installation of the Flume agent installed on another server. It pushes data from
    sink to source and writes data to the Cassendra data store.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that this is not a good architecture, but I have mentioned it to
    explain how a Flume sink and Flume sources can be connected.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows complex data flow involving multiple agents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9888efdd-2fa8-48a2-be83-b794f8f5fd5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Flume setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Flume agent configuration is stored in a local text file. Please refer to the
    sample Flume agent configuration file in the code repository of this book. Flume
    1.7.0 supports various sources and sinks. Widely used Flume sources (a summary)
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Source** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Avro source | Listens on Avro port and receives events from external Avro
    client streams |'
  prefs: []
  type: TYPE_TB
- en: '| Exec source | Runs a given Unix command and expects that process to continuously
    produce data on standard out |'
  prefs: []
  type: TYPE_TB
- en: '| Spooling directory source | Ingests data from files on disk |'
  prefs: []
  type: TYPE_TB
- en: '| Taildir source | Tails files in near real-time after new lines are detected
    in the files |'
  prefs: []
  type: TYPE_TB
- en: '| Kafka source | Reads messages from Kafka topics |'
  prefs: []
  type: TYPE_TB
- en: '| Syslog source | Reads syslog data (supports syslog-TCP and syslog-UDP) |'
  prefs: []
  type: TYPE_TB
- en: '| HTTP source | Accepts Flume events by HTTP `POST` and `GET` |'
  prefs: []
  type: TYPE_TB
- en: 'The widely used Flume sinks can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sink** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Avro sink | Events are turned into Avro events and sent to the configured
    hostname/port pair |'
  prefs: []
  type: TYPE_TB
- en: '| HDFS sink | Writes events into the HDFS |'
  prefs: []
  type: TYPE_TB
- en: '| Hive sink | Writes text or JSON data into a Hive table |'
  prefs: []
  type: TYPE_TB
- en: '| HBase sink | Writes data to HBase |'
  prefs: []
  type: TYPE_TB
- en: '| Morphline Solr sink | Loads it in near real-time into Apache Solr servers
    |'
  prefs: []
  type: TYPE_TB
- en: '| Elasticsearch sink | Writes data to an Elasticsearch cluster |'
  prefs: []
  type: TYPE_TB
- en: '| Kafka sink | Writes data to a Kafka topic |'
  prefs: []
  type: TYPE_TB
- en: 'The widely used Flume channels (a summary) are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Channel** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| JDBC channel | Events are stored in storage supported by database |'
  prefs: []
  type: TYPE_TB
- en: '| Kafka channel | Events are stored in a Kafka cluster |'
  prefs: []
  type: TYPE_TB
- en: '| File channel | Events are stored in files |'
  prefs: []
  type: TYPE_TB
- en: '| Spillable memory channel | Events are stored in memory; if memory gets full,
    then stored on disk |'
  prefs: []
  type: TYPE_TB
- en: 'The widely used Flume interceptors can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Interceptor** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Timestamp interceptor | Adds the processing time of an event into event headers
    |'
  prefs: []
  type: TYPE_TB
- en: '| Host interceptor | Adds hostname of agent |'
  prefs: []
  type: TYPE_TB
- en: '| Search and replace interceptor | Supports Java regular expressions |'
  prefs: []
  type: TYPE_TB
- en: '| Regex filtering interceptor | Filters the events against RegEx |'
  prefs: []
  type: TYPE_TB
- en: '| Regex extractor interceptor | Extracts and appends the match RegEx groups
    as headers on the event |'
  prefs: []
  type: TYPE_TB
- en: Log aggregation use case
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In day-to-day business scenarios, we always find the need to get log files
    and make sense out of them. For example, we always find the need to get logs from
    different applications and servers and merge them together to find trends and
    patterns. Let me extend this example further. Let''s assume that we have five
    web servers deployed on five different servers. We want to get all five web server
    logs and merge/aggregate them together to analyze them further by storing one
    copy on HDFS and another copy to be shipped on to a Kafka topic for real-time
    analytics. The question is how we design Flume-based log aggregation architecture.
    The following is the Flume architecture for our web server log aggregation scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/afaa72af-94ab-43dd-9f56-0f4c1e6083fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let us walk through the architecture in detail: There are a total of five web
    servers. Each web server generates a log file and stores it locally. The Flume
    agent is installed on each web server. The Flume agent is nothing but a (JVM)
    process that hosts the components through which events flow from an external source
    to the next destination (hop). Each Flume agent accesses log files based on local
    configuration of `flume.conf`. Each Flume agent reads the log files and pushes
    data to the Flume collector. Each line of the log file is treated as one message
    (a payload). The Flume collector gets messages from all web servers, fitters and
    aggregates all messages, and pushes these messages to the data store. The following
    is the sample `flume.conf` of the Flume agent and the collectors agent `flume.conf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The collector `flume.conf` file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Apache NiFi
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is Apache NiFi? In any organization, we know that there is a variety of
    systems. Some systems generate the data and other systems consume that data. Apache
    NiFi is built to automate that data flow from one system to another. Apache NiFi
    is a data flow management system that comes with a web UI that helps to build
    data flows in real time. It supports flow-based programming. The graph programming
    includes a series of nodes and edges through which data moves. In NiFi, these
    nodes are translated into processors, and the edges into connectors. The data
    is stored in a packet of information called a **FlowFile**. This FlowFile includes
    content, attributes, and edges. As a user, you connect processors together using
    connectors to define how the data should be handled.
  prefs: []
  type: TYPE_NORMAL
- en: Main concepts of Apache NiFi
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following table describes the main components of Apache NiFi:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Component name** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| FlowFile | Data packet running through the system |'
  prefs: []
  type: TYPE_TB
- en: '| FlowFile processor | Performs the actual work of data routing, transformation,
    and data movement |'
  prefs: []
  type: TYPE_TB
- en: '| Connetion | Actual data linkage between processors |'
  prefs: []
  type: TYPE_TB
- en: '| Flow controller | Facilitates the exchange of FlowFiles between the processors
    |'
  prefs: []
  type: TYPE_TB
- en: '| Process group | Specific group of data inputs and data output processors
    |'
  prefs: []
  type: TYPE_TB
- en: Apache NiFi architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following diagram shows the components of the Apache NiFi architecture
    (source: [https://nifi.apache.org/docs.html](https://nifi.apache.org/docs.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33e12830-db99-4454-8e0b-4d842a72c0c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The components are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Web server**: This hosts NiFi''s HTTP-based UI'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**File controller**: This provides threads and manages the schedule for the
    extensions to run on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extensions**: The extensions operate and execute within the JVM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FileFlow repository**: This keeps track of the state of what it knows about
    a given FlowFile that is presently active in the flow'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content repository**: This is where the actual content bytes of a given FlowFile
    live'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Provenance repository**: This is where all provenance event data is stored'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the key features of Apache NiFi:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Guaranteed delivery**: In the event of increased volume of data, power failures,
    and network and system failures in NiFi, it becomes necessary to have a robust
    guaranteed delivery of the data. NiFi ensures, within the dataflow system itself,
    the transactional communication between NiFi and the data where it is coming to
    the points to which it is delivered to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data buffering with back pressure and pressure release**: In any dataflow,
    it may be possible that there are some issues with the systems involved; some
    might be down or some might be slow. In that case, data buffering becomes very
    essential to coping with the data coming into or going out of the dataflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NiFi supports the buffering of all queues with back pressure when it reaches
    specific limits and age of the data. NiFi does it with a maximum possible throughput
    rate, while maintaining a good response time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prioritized queuing**: In general, the data queues maintain natural order
    or insertion order. But, many times, when the rate of data insertion is faster
    than the bandwidth, you have to prioritize your data retrieval from the queue.
    The default is the oldest data first. But NiFi supports prioritization of queues
    to pull data out based on size, time, and so on that is, largest first or newest
    first.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flow-specific quality of service (QoS)**: There are some situations where
    we have to process the data in a specific time period, for example, within a second
    and so on, otherwise the data loses its value. The fine-grained flow of specific
    configuration of these concerns is enabled by Apache NiFi.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data provenance**: NiFi automatically records, indexes, and makes available
    provenance data as objects flow through the system—even across fan-in, fan-out,
    transformations, and more. This information becomes extremely critical in supporting
    compliance, troubleshooting, optimization, and other scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visual command and control**: Apache NiFi allows users to have interactive
    management of dataflow. It provides immediate feedback to each and every change
    to the dataflow. Hence, users understand and immediately correct any problems,
    mistakes, or issues in their dataflows. Based on analytical results of the dataflows,
    users can make changes to their dataflow, prioritize of queues, add more data
    flows, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flow templates**: Data flows can be developed, designed, and shared. Templates
    allow subject matter experts to build and publish their flow designs and for others
    to benefit and collaborate on them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extension**: NiFi allows us to extend its key components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Points of extension:** Processors, controller services, reporting tasks,
    prioritizers, and customer UIs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-role security**: Multi-grained, multi-role security can be applied
    to each component, which allows the admin user to have a fine-grained level of
    access control.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering**: NiFi is designed to scale-out through the use of clustering
    many nodes together. That way, it can handle more data by adding more nodes to
    the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For getting started with Apache NiFi, please use this link: [https://nifi.apache.org/docs/nifi-docs/html/getting-started.html](https://nifi.apache.org/docs/nifi-docs/html/getting-started.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s imagine a scenario. I have a running log file. It is updated on the
    fly. I want to capture and monitor each line in that file, based on its contents.
    I want to send it to my Kafka brokers. I also want to deliver all my error records
    to HDFS for archival and further analysis. Different line types will be sent to
    different Kafka brokers. For example, error, info, and success types will be sent
    to three different Kafka topics, namely error, info, and success. I have developed
    the following NiFi workflow for that. The following table gives the detailed explanation
    of each processor:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Processor** | **Purpose** | **Property** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| TailFile | To tail log files | File to tail | `/var/log/apache.log` |'
  prefs: []
  type: TYPE_TB
- en: '| SplitText | To split the log entries to the lines | Line split count | `1`
    |'
  prefs: []
  type: TYPE_TB
- en: '| RouteOnContent | To make a routing decision |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| PutHDFS | To send errors to HDFS |  | HDFS details |'
  prefs: []
  type: TYPE_TB
- en: '| PublishKafka | To deliver data to Kafka topic | Broker and topic name | Hostname:
    port, topic pane |'
  prefs: []
  type: TYPE_TB
- en: Real-time log capture dataflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following example workflow shows how log file data can be pushed to HDFS
    and then to moved to Kafka brokers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff6f0904-e113-449d-ae0c-df29aebdb730.png)'
  prefs: []
  type: TYPE_IMG
- en: Kafka Connect
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kafka Connect is a part of Apache Kafka. It is a framework to ingest data from
    one to another system using connectors. There are two types of connectors: source
    connectors and sink connectors. The sink connectors import data from source systems
    and write to Kafka topics. The sink connectors read data from the Kafka topic
    and export it to target systems. Kafka Connect provides various source and sink
    connectors out of the box.'
  prefs: []
  type: TYPE_NORMAL
- en: Kafka Connect – a brief history
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka Connect was mainly introduced in November 2015 in Kafka 0.9.x. In addition
    to the various features of Kafka 0.9.x, Connect APIs was a brand new feature.
    Then, in May 2016, the new version Kafka 0.10.0 was released. In that version,
    Kafka Streams API was a new and exciting feature. But, in March 2017, it was Kafka
    Version 0.10.2 where Kafka Connect got its real momentum. As a part of Kafka 0.10.2,
    improved simplified Connect APIs and single message transform APIs were released.
  prefs: []
  type: TYPE_NORMAL
- en: Why Kafka Connect?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka Connect helps to simplify getting data in and out of Kafka. It provides
    a lot of connectors to do that out of the box. In my opinion, that's the best
    incentive to a developer like me, because I do not have to develop a separate
    code to develop my own connector to import and export data; I can always reuse
    the out-of-the-box connector for that. Also, if I want, I can always develop my
    own unique connector using Kafka Connect APIs. Also, all the connectors are configuration-based.
    The common sources and targets are databases, search engines, NoSQL data stores,
    and applications like SAP, GoldenGate, Salesforce, HDFS, Elasticsearch, and so
    on. For a detailed listing of all available sources and connectors, please refer
    to [https://www.confluent.io/product/connectors/](https://www.confluent.io/product/connectors/).
  prefs: []
  type: TYPE_NORMAL
- en: Kafka Connect features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some features of Kafka connect:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalable**: This is a framework for scalable and reliable streaming data
    between Apache Kafka and other systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simple**: This makes it simple to define connectors that move large collections
    of data into and out of Kafka'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Offset management**: The framework does most of the hard work of properly
    recording the offsets of the connectors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Easy operation**: This has a service that has a RESTful API for managing
    and deploying connectors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed**: The framework can be clustered and will automatically distribute
    the connectors across the cluster, ensuring that the connector is always running'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Out-of-the-box connectors**: For a detailed listing of all available sources
    and connectors, please refer to [https://www.confluent.io/product/connectors/](https://www.confluent.io/product/connectors/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka Connect architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following diagram represents the Kafka Connect architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a2af5eb-0cbb-4e6c-b3b8-1511e6d0c823.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Kafka cluster is made of Kafka brokers: three brokers, as shown in the
    diagram. Sources can be of any type, for example, databases, NoSQL, Twitter, and
    so on. In between the source and Kafka cluster, there is a Kafka Connect cluster,
    which is made up of workers. The working of Kafka Connect comprises the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Workers, based on configuration, pull data from sources
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After getting data, the connector pushes data to the Kafka cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If data needs to be transformed, filtered, joined, or aggregated using stream
    applications such as Spark, Storm, and so on, stream APIs will change data in
    and out of Kafka
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the configuration, the connector will pull data out of Kafka and write
    it to the sink
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some Kafka Connect concepts are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Source connectors get data from common data sources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sink connectors publish data to common data sources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka Connect makes it easy to quickly get data reliably into Kafka.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a part of the ETL pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling from a small pipeline to a company-wide pipeline is very easy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code is reusable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Kafka Connect cluster has multiple loaded connectors. Each connector is
    a reusable piece of code, `(Java JARs)`. There are a lot of open source connectors
    available, which can be leveraged.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each connector task is a combination of connector class and configuration. A
    task is linked to a connector configuration. A job creation may create multiple
    tasks. So, if you have one connector and one configuration, then two or more tasks
    can be created.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka Connect workers and servers execute tasks. A worker is a single java process.
    A worker can be standalone or distributed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka Connect workers modes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two modes of Kafka Connect workers:'
  prefs: []
  type: TYPE_NORMAL
- en: Standalone mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standalone mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Standalone mode is a single process (worker) that runs all the connectors and
    tasks. The configuration is bundled in a single process. It is not fault tolerant
    or scalable, and it is very difficult to monitor. Since it is easy to set up,
    it is mainly used during development and testing.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With distributed mode, multiple workers (processes) run your connectors and
    tasks. The configuration is submitted using the REST API. It is scalable and fault
    tolerant. It automatically rebalances all the tasks on the cluster if any worker
    dies. Since it is scalable and fault tolerant, it is mainly used in a production
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka Connect cluster distributed architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is the representation of the Kafka Connect cluster distributed
    architecture details:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a72723b-b90b-4c5d-a5ad-525ebb59d0f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding diagram, we can see the following details:'
  prefs: []
  type: TYPE_NORMAL
- en: We have source **Connector 1**, with three tasks: **Task 1**, **Task 2**, and
    **Task 3**. These three tasks are spread out among four workers: **Worker 1**,
    **Worker 3**, and **Worker 4**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also have source **Connector 2**, with two tasks: **Task 1** and **Task
    2**. These two tasks are spread out between two workers: **Worker 2** and **Worker
    3**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also have sink **Connector 3** with four tasks: **Task 1**, **Task 2**,
    **Task 3**, and **Task 4**. These four tasks are spread out among four workers:
    **Worker 1**, **Worker 2**, **Worker 3**, and **Worker 4**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, something happens and **Worker 4** dies and we lose that worker completely.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a part of fault tolerance, the rebalance activity kicks off. **Connector
    1** and **Task 3** move from **Worker 4** to **Worker 2**. Similarly, **Connector
    3** and **Task 4** move from **Connector 4** to **Connector 1**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram represents the Kafka Connect cluster after rebalance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e65c676a-a02c-4805-a38a-95770edbdcf8.png)'
  prefs: []
  type: TYPE_IMG
- en: Example 1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Streamed data from source file `Demo-Source.txt` is moved to destination file
    `Demo-Sink.txt` in standalone mode, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4682bda0-9403-4eb5-a809-14abaf451b54.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to stream data from source file `Demo-Source.txt` to destination file
    `Demo-Sink.txt` in standalone mode, we need to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start Kafka:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Create topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the `source-file-stream-standalone.properties` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure `file-stream-standalone.properties` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure `file-worker.properties` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Start Kafka Connect. Open another terminal and run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Add data to the `demo-source.txt` file. Open another terminal and run the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the `demo-sink.txt` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Example 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Streamed data from source file `Demo-Source.txt` is moved to destination file
    `Demo-Sink.txt` in distributed mode. If you want to run the previous example using
    distributed mode, you have to add the following parameter to `source-file-stream`
    and `sink-file-stream` in *steps 3* and *4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned all the popular data ingestion tools used in
    production environments. Sqoop is mainly used to import and export data in and
    out of RDBMS data stores. Apache Flume is used in real-time systems to import
    data, mainly from files sources. It supports a wide variety of sources and sinks.
    Apache NiFi is a fairly new tool and getting very popular these days. It also
    supports GUI-based ETL development. Hortonworks has started supporting this tool
    since their HDP 2.4 release. Apache Kafka Connect is another popular tool in the
    market. It is also a part of the Confluent Data Platform. Kafka Connect can ingest
    entire databases or collect metrics from all your application servers into Kafka
    topics, making the data available for stream processing with low latency.
  prefs: []
  type: TYPE_NORMAL
- en: Since we so far know how to build Hadoop clusters and how to ingest data in
    them, we will learn data modeling techniques in the next chapter.
  prefs: []
  type: TYPE_NORMAL
