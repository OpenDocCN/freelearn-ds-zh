- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mock Test 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Try your hand at these practice questions to test your knowledge of Apache
    Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question 1**:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a task in Spark?
  prefs: []
  type: TYPE_NORMAL
- en: The unit of work performed for each data partition within a task is the slots
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A task is the second-smallest entity that can be executed within Spark
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tasks featuring wide dependencies can be combined into a single task
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A task is the smallest component that can be executed within Spark
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 2**:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the role of an executor in Spark?
  prefs: []
  type: TYPE_NORMAL
- en: The executor’s role is to request the transformation of operations into a directed
    acyclic graph (DAG)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There can only be one executor within a Spark environment
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Executors are tasked with executing the assignments provided to them by the
    driver
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The executor schedules queries for execution
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 3**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following is one of the tasks of Adaptive Query Execution in Spark?
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive Query Execution collects runtime statistics during query execution
    to optimize query plans
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adaptive Query Execution is responsible for distributing tasks to executors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adaptive Query Execution is responsible for wide operations in Spark
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adaptive Query Execution is responsible for fault tolerance in Spark
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 4**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which is the lowest level in Spark’s execution hierarchy?
  prefs: []
  type: TYPE_NORMAL
- en: Task
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Slot
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Job
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stage
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 5**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which one of these operations is an action?
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrame.count()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`DataFrame.filter()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`DataFrame.select()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`DataFrame.groupBy()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 6**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following describes the characteristics of the DataFrame API?
  prefs: []
  type: TYPE_NORMAL
- en: The DataFrame API is based on resilient distributed dataset (RDD) at the backend
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The DataFrame API is available in Scala, but it is not available in Python
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The DataFrame API does not have data manipulation functions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The DataFrame API is used for distributing tasks in executors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 7**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following statements is accurate about executors?
  prefs: []
  type: TYPE_NORMAL
- en: Slots are not a part of an executor
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Executors are able to run tasks in parallel via slots
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Executors are always equal to tasks
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An executor is responsible for distributing tasks for a job
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 8**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following statements is accurate about the Spark driver?
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple drivers in a Spark application
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Slots are a part of a driver
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drivers execute tasks in parallel
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is the responsibility of the Spark driver to transform operations into DAG
    computations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 9**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which one of these operations is a wide transformation?
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrame.show()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`DataFrame.groupBy()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`DataFrame.repartition()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`DataFrame.select()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`DataFrame.filter()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 10**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following statements is correct about lazy evaluation?
  prefs: []
  type: TYPE_NORMAL
- en: Execution is triggered by transformations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execution is triggered by actions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Statements are executed as they appear in the code
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spark distributes tasks to different executors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 11**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following is true about DAGs in Spark?
  prefs: []
  type: TYPE_NORMAL
- en: DAGs are lazily evaluated
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DAGs can be scaled horizontally in Spark
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DAGs are responsible for processing partitions in an optimized and distributed
    fashion
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DAG is comprised of tasks that can run in parallel
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 12**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following statements is true about Spark’s fault tolerance mechanism?
  prefs: []
  type: TYPE_NORMAL
- en: Spark achieves fault tolerance via DAGs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is the responsibility of the executor to enable fault tolerance in Spark
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Because of fault tolerance, Spark can recompute any failed RDD
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spark builds a fault-tolerant layer on top of the legacy RDD data system, which
    by itself is not fault tolerant
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 13**:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the core of Spark’s fault-tolerant mechanism?
  prefs: []
  type: TYPE_NORMAL
- en: RDD is at the core of Spark, which is fault tolerant by design
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data partitions, since data can be recomputed
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DataFrame is at the core of Spark since it is immutable
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Executors ensure that Spark remains fault tolerant
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 14**:'
  prefs: []
  type: TYPE_NORMAL
- en: What is accurate about jobs in Spark?
  prefs: []
  type: TYPE_NORMAL
- en: Different stages in a job may be executed in parallel
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Different stages in a job cannot be executed in parallel
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A task consists of many jobs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A stage consists of many jobs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 15**:'
  prefs: []
  type: TYPE_NORMAL
- en: What is accurate about a shuffle in Spark?
  prefs: []
  type: TYPE_NORMAL
- en: In a shuffle, data is sent to multiple partitions to be processed
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In a shuffle, data is sent to a single partition to be processed
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A shuffle is an action that triggers evaluation in Spark
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In a shuffle, all data remains in memory to be processed
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 16**:'
  prefs: []
  type: TYPE_NORMAL
- en: What is accurate about the cluster manager in Spark?
  prefs: []
  type: TYPE_NORMAL
- en: The cluster manager is responsible for managing resources for Spark
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cluster manager is responsible for working with executors directly
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cluster manager is responsible for creating query plans
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cluster manager is responsible for optimizing DAGs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 17**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block needs to take the sum and average of the `salary`
    column for each department in the `df` DataFrame. Then, it should calculate the
    sum and maximum value for the `bonus` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Choose the answer that correctly fills the blanks in the code block to accomplish
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`groupBy`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`agg`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`avg`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`max`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`filter`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`agg`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`avg`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`max`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`groupBy`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`avg`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`agg`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`max`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`groupBy`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`agg`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`avg`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`avg`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 18**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block contains an error. The code block needs to join the
    `salaryDf` DataFrame with the bigger `employeeDf` DataFrame on the `employeeID`
    column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Identify the error:'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of `join`, the code should use `innerJoin`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`broadcast` is not a `join` type in Spark for joining two DataFrames'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salaryDf` and `employeeDf` should be swapped'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `how` parameter, `crossJoin` should be used instead of `broadcast`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 19**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks shuffles the `df` DataFrame to have 20 partitions
    instead of 5 partitions?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.repartition(5)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.repartition(20)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.coalesce(20)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.coalesce(5)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 20**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following operations will trigger evaluation?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.filter()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.distinct()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.intersect()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.join()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.count()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 21**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns unique values for the `age` and `name`
    columns in the `df` DataFrame in its respective columns where all values are unique
    in these columns?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.select(''age'').join(df.select(''name''),` `col(state)==col(''name''),
    ''inner'').show()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.select(col(''age''),` `col(''name'')).agg({''*'': ''count''}).show()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.select(''age'', ''name'').distinct().show()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.select(''age'').unionAll(df.select(''name'')).distinct().show()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 22**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns the count of the total number of
    rows in the `df` DataFrame?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.count()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.select(col(''state''),` `col(''department'')).agg({''*'': ''count''}).show()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.select(''state'', ''department'').distinct().show()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.select(''state'').union(df.select(''department'')).distinct().show()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 23**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block contains an error. The code block should save the
    `df` DataFrame at the `filePath` path as a new parquet file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Identify the error:'
  prefs: []
  type: TYPE_NORMAL
- en: The code block should have `overwrite` instead of `append` as an option
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code should be `write.parquet` instead of `write.mode`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `df.write` operation cannot be called directly from the DataFrame
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first part of the code should be `df.write.mode(append)`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 24**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks adds a `salary_squared` column to the `df`
    DataFrame that is the square of the `salary` column?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.withColumnRenamed("salary_squared",` `pow(col("salary"), 2))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumn("salary_squared", col("salary"*2))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumn("salary_squared",` `pow(col("salary"), 2))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumn("salary_squared", square(col("salary")))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 25**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks performs a join in which the small `salaryDf`
    DataFrame is sent to all executors so that it can be joined with the `employeeDf`
    DataFrame on the `employeeSalaryID` and `EmployeeID` columns, respectively?
  prefs: []
  type: TYPE_NORMAL
- en: '`employeeDf.join(salaryDf, "employeeDf.employeeID ==` `salaryDf.employeeSalaryID",
    "inner")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`employeeDf.join(salaryDf, "employeeDf.employeeID ==` `salaryDf.employeeSalaryID",
    "broadcast")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`employeeDf.join(broadcast(salaryDf), employeeDf.employeeID ==` `salaryDf.employeeSalaryID)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salaryDf.join(broadcast(employeeDf), employeeDf.employeeID ==` `salaryDf.employeeSalaryID)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 26**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks performs an outer join between the `salarydf`
    DataFrame and the `employeedf` DataFrame, using the `employeeID` and `salaryEmployeeID`
    columns as join keys respectively?
  prefs: []
  type: TYPE_NORMAL
- en: '`Salarydf.join(employeedf, "outer", salarydf.employeedf ==` `employeeID.salaryEmployeeID)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salarydf.join(employeedf, employeeID ==` `salaryEmployeeID)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salarydf.join(employeedf, salarydf.salaryEmployeeID ==` `employeedf.employeeID,
    "outer")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salarydf.join(employeedf, salarydf.employeeID ==` `employeedf.salaryEmployeeID,
    "outer")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 27**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following pieces of code would print the schema of the `df` DataFrame?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.rdd.printSchema`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.rdd.printSchema()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.printSchema`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.printSchema()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 28**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks performs a left join between the `salarydf`
    DataFrame and the `employeedf` DataFrame, using the `employeeID` column?
  prefs: []
  type: TYPE_NORMAL
- en: '`salaryDf.join(employeeDf, salaryDf["employeeID"] ==` `employeeDf["employeeID"],
    "outer")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salaryDf.join(employeeDf, salaryDf["employeeID"] ==` `employeeDf["employeeID"],
    "left")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salaryDf.join(employeeDf, salaryDf["employeeID"] ==` `employeeDf["employeeID"],
    "inner")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salaryDf.join(employeeDf, salaryDf["employeeID"] ==` `employeeDf["employeeID"],
    "right")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 29**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks aggregates the `bonus` column of the `df`
    DataFrame in ascending order with `nulls` being last?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.agg(asc_nulls_last("bonus").alias("bonus_agg"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.agg(asc_nulls_first("bonus").alias("bonus_agg"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.agg(asc_nulls_last("bonus", asc).alias("bonus_agg"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.agg(asc_nulls_first("bonus", asc).alias("bonus_agg"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 30**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The code block should return a DataFrame
    by joining the `employeeDf` and `salaryDf` DataFrames on the `employeeID` and
    `employeeSalaryID` columns, respectively, excluding the `bonus` and `department`
    columns from the `employeeDf` DataFrame and the `salary` column from the `salaryDf`
    DataFrame in the final DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Identify the error:'
  prefs: []
  type: TYPE_NORMAL
- en: '`groupBy` should be replaced with the `innerJoin` operator'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`groupBy` should be replaced with a `join` operator and `delete` should be
    replaced with `drop`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`groupBy` should be replaced with the `crossJoin` operator and `delete` should
    be replaced with `withColumn`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`groupBy` should be replaced with a `join` operator and `delete` should be
    replaced with `withColumnRenamed`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 31**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks reads a `/loc/example.csv` CSV file as a
    `df` DataFrame?
  prefs: []
  type: TYPE_NORMAL
- en: '`df =` `spark.read.csv("/loc/example.csv")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df =` `spark.mode("csv").read("/loc/example.csv")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df =` `spark.read.path("/loc/example.csv")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df =` `spark.read().csv("/loc/example.csv")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 32**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks reads a parquet file at the `my_path` location
    using a schema file named `my_schema`?
  prefs: []
  type: TYPE_NORMAL
- en: '`spark.read.schema(my_schema).format("parquet").load(my_path)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`spark.read.schema("my_schema").format("parquet").load(my_path)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`spark.read.schema(my_schema).parquet(my_path)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`spark.read.parquet(my_path).schema(my_schema)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 33**:'
  prefs: []
  type: TYPE_NORMAL
- en: We want to find the number of records in the resulting DataFrame when we join
    the `employeedf` and `salarydf` DataFrames on the `employeeID` and `employeeSalaryID`
    columns respectively. Which code blocks should be executed to achieve this?
  prefs: []
  type: TYPE_NORMAL
- en: '`.``filter(~isnull(col(department)))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``count()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`employeedf.join(salarydf, col("employeedf.employeeID")==col("salarydf.employeeSalaryID"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`employeedf.join(salarydf, employeedf. employeeID ==salarydf.` `employeeSalaryID,
    how=''inner'')`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``filter(col(department).isnotnull())`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``sum(col(department))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3, 1, 6
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 3, 1, 2
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 4, 2
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 3, 5, 2
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 34**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns a copy of the `df` DataFrame where
    the name of the `state` column has been changed to `stateID`?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.withColumnRenamed("state", "stateID")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumnRenamed("stateID", "state")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumn("state", "stateID")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumn("stateID", "state")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 35**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns a copy of the `df` DataFrame where
    the `salary` column has been converted to `integer`?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.col("salary").cast("integer"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumn("salary", col("salary").castType("integer"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumn("salary", col("salary").convert("integerType()"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumn("salary", col("salary").cast("integer"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 36**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks splits a `df` DataFrame in half with the
    exact same values even when the code is run multiple times?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.randomSplit([0.5,` `0.5], seed=123)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.split([0.5,` `0.5], seed=123)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.split([0.5, 0.5])`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.randomSplit([0.5, 0.5])`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 37**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks sorts the `df` DataFrame by two columns,
    `salary` and `department`, where `salary` is in ascending order and `department`
    is in descending order?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.sort("salary", asc("department"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.sort("salary", desc(department))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.sort(col(salary)).desc(col(department))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.sort("salary", desc("department"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 38**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks calculates the average of the `bonus` column
    from the `salaryDf` DataFrame and adds that in a new column called `average_bonus`?
  prefs: []
  type: TYPE_NORMAL
- en: '`salaryDf.avg("bonus").alias("average_bonus"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salaryDf.agg(avg("bonus").alias("average_bonus"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salaryDf.agg(sum("bonus").alias("average_bonus"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salaryDf.agg(average("bonus").alias("average_bonus"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 39**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks saves the `df` DataFrame in the `/FileStore/file.csv`
    location as a CSV file and throws an error if a file already exists in the location?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.write.mode("error").csv("/FileStore/file.csv")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.write.mode.error.csv("/FileStore/file.csv")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.write.mode("exception").csv("/FileStore/file.csv")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.write.mode("exists").csv("/FileStore/file.csv")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 40**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks reads the `my_csv.csv` CSV file located at
    `/my_path/` into a DataFrame?
  prefs: []
  type: TYPE_NORMAL
- en: '`spark.read().mode("csv").path("/my_path/my_csv.csv")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`spark.read.format("csv").path("/my_path/my_csv.csv")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`spark.read("csv", "/my_path/my_csv.csv")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`spark.read.csv("/my_path/my_csv.csv")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 41**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks displays the top 100 rows of the `df` DataFrame,
    where the `salary` column is present, in descending order?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.sort(asc(value)).show(100)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.sort(col("value")).show(100)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.sort(col("value").desc()).show(100)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.sort(col("value").asc()).print(100)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 42**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks creates a DataFrame that shows the mean of
    the `salary` column of the `salaryDf` DataFrame based on the `department` and
    `state` columns, where `age` is greater than `35` and the returned DataFrame should
    be sorted in ascending order by the `employeeID` column such that there are no
    nulls in that column?
  prefs: []
  type: TYPE_NORMAL
- en: '`salaryDf.filter(col("age") >` `35)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``filter(col("employeeID")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``filter(col("employeeID").isNotNull())`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``groupBy("department")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``groupBy("department", "state")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``agg(avg("salary").alias("mean_salary"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``agg(average("salary").alias("mean_salary"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``orderBy("employeeID")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1, 2, 5, 6, 8
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 1, 3, 5, 6, 8
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 1, 3, 6, 7, 8
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 1, 2, 4, 6, 8
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 43**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The code block should return a new
    DataFrame without the `employee` and `salary` columns and with an additional `fixed_value`
    column, which has a value of `100`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Identify the error:'
  prefs: []
  type: TYPE_NORMAL
- en: '`withcolumnRenamed` should be replaced with `withcolumn` and the `lit()` function
    should be used to fill the `100` value'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`withcolumnRenamed` should be replaced with `withcolumn`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`employee` and `salary` should be swapped in a `drop` function'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `lit()` function call is missing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 44**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns the basic statistics for numeric
    and string columns of the `df` DataFrame?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.describe()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.detail()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.head()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.explain()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 45**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns the top 5 rows of the `df` DataFrame?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.select(5)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.head(5)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.top(5)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.show()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 46**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks creates a new DataFrame with the `department`,
    `age`, and `salary` columns from the `df` DataFrame?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.select("department", "``age", "salary")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.drop("department", "``age", "salary")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.filter("department", "``age", "salary")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.where("department", "``age", "salary")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 47**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks creates a new DataFrame with three columns,
    `department`, `age`, and `max_salary`, which has the maximum salary for each employee
    from each department and each age group from the `df` DataFrame?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Identify the correct answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '`filter`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`agg`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`max`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: groupBy
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: agg
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: max
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: filter
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: agg
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: sum
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: groupBy
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: agg
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: sum
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 48**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The code block should return a new
    DataFrame, filtered by the rows, where the `salary` column is greater than or
    equal to `1000` in the `df` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Identify the error:'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of `filter()`, `where()` should be used
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `F(salary)` operation should be replaced with `F.col("salary")`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of `>=`, the `>` operator should be used
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The argument to the `where` method should be `"salary >` `1000"`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 49**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns a copy of the `df` DataFrame where
    the `department` column has been renamed `business_unit`?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.withColumn(["department", "business_unit"])`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`itemsDf.withColumn("department").alias("business_unit")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`itemsDf.withColumnRenamed("department", "business_unit")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`itemsDf.withColumnRenamed("business_unit", "department")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 50**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns a DataFrame with the total count
    of employees in each department from the `df` DataFrame?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.groupBy("department").agg(count("*").alias("total_employees"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.filter("department").agg(count("*").alias("total_employees"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.groupBy("department").agg(sum("*").alias("total_employees"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.filter("department").agg(sum("*").alias("total_employees"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 51**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns a DataFrame with the `employee` column
    from the `df` DataFrame case to the `string` type?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.withColumn("employee", col("employee").cast_type("string"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumn("employee", col("employee").cast("string"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumn("employee", col("employee").cast_type("stringType()"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumnRenamed("employee", col("employee").cast("string"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 52**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns a DataFrame with a new `fixed_value`
    column, which has `Z` in all rows in the df DataFrame?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.withColumn("fixed_value", F.lit("Z"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumn("fixed_value", F("Z"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumnRenamed("fixed_value", F.lit("Z"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumnRenamed("fixed_value", lit("Z"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 53**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns a new DataFrame with a new `upper_string`
    column, which is the capitalized version of the `employeeName` column in the `df`
    DataFrame?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.withColumnRenamed(''employeeName'', upper(df.upper_string))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumnRenamed(''upper_string'', upper(df.employeeName))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumn(''upper_string'', upper(df.employeeName))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumn(''` `employeeName'', upper(df.upper_string))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 54**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block contains an error. The code block is supposed to capitalize
    the employee names using a udf:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Identify the error:'
  prefs: []
  type: TYPE_NORMAL
- en: The `capitalize_udf` function should be called instead of `capitalize`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `udf` function, `capitalize_udf`, is not capitalizing correctly
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of `StringType()`, `IntegerType()` should be used
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of `df.withColumn("capitalized_name", capitalize("employee"))`, it should
    use `df.withColumn("employee", capitalize("capitalized_name"))`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 55**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The code block is supposed to sort
    the `df` DataFrame by salary in ascending order. Then, it should sort based on
    the `bonus` column, putting `nulls` last.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Identify the error:'
  prefs: []
  type: TYPE_NORMAL
- en: The `salary` column should be sorted in descending order and `desc_nulls_last`
    should be used instead of `asc_nulls_first`. Moreover, it should be wrapped in
    a `col()` operator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `salary` column should be wrapped by the `col()` operator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `bonus` column should be sorted in a descending way, putting nulls last.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `bonus` column should be sorted by `desc_nulls_first()` instead.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 56**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The code block needs to group the
    `df` DataFrame based on the `department` column and calculate the total salary
    and average salary for each department.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Identify the error:'
  prefs: []
  type: TYPE_NORMAL
- en: The `avg` method should also be called through the `agg` function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of `filter`, `groupBy` should be used
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `agg` method syntax is incorrect
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of filtering on `department`, the code should filter on `salary`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 57**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which code block will write the `df` DataFrame as a parquet file on the `filePath`
    path partitioning it on the `department` column?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.write.partitionBy("department").parquet(filePath)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.write.partition("department").parquet(filePath)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.write.parquet("department").partition(filePath)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.write.coalesce("department").parquet(filePath)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 58**:'
  prefs: []
  type: TYPE_NORMAL
- en: The `df` DataFrame contains columns `[employeeID, salary, department]`. Which
    of the following pieces of code would return the `df` DataFrame with only columns
    `[``employeeID, salary]`?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.drop("department")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.select(col(employeeID))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.drop("department", "salary")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.select("employeeID", "department")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 59**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns a new DataFrame with the same columns
    as the `df` DataFrame, except for the `salary` column?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.drop(col("salary"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.delete(salary)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.drop(salary)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.delete("salary")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 60**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The code block should return the
    `df` DataFrame with `employeeID` renamed as `employeeIdColumn`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Identify the error:'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of `withColumnRenamed`, the `withColumn` method should be used
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of `withColumnRenamed`, the `withColumn` method should be used and the
    `"employeeIdColumn"` argument should be swapped with the `"``employeeID"` argument
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `"employeeIdColumn"` and `"employeeID"` arguments should be swapped
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`withColumnRenamed` is not a method for DataFrames'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: E
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
