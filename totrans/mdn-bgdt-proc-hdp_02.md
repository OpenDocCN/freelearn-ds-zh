# Hadoop 生命周期管理

在本章中，我们将了解以下主题：

+   数据清洗

+   数据掩码

+   数据安全

# 数据清洗

如果你有一些处理某种类型数据的经验，你将记得大多数时候数据需要预处理，这样我们才能将其作为更大分析的一部分进一步使用。这个过程被称为**数据清洗**。

让我们看看这个过程中的典型流程是什么样的：

+   数据获取

+   数据结构分析

+   信息提取

+   无用数据移除

+   数据转换

+   数据标准化

让我们尝试详细了解这些内容。

# 数据获取

虽然这不是数据清洗的一部分，但这个阶段涉及从某处获取数据的过程。通常，所有数据都是在中央位置生成和存储的，或者位于某些共享存储上的文件中。

理解这一步有助于我们构建接口或使用现有的库从获取的数据源位置拉取数据。

# 数据结构分析

数据获取后，我们必须理解数据的结构。记住，我们得到的数据可以是以下任何一种形式：

+   文本数据：

    +   结构化数据

    +   非结构化数据

+   二进制数据

这是我们需要某些工具来帮助我们理解数据结构的地方。

一旦我们对所处理的数据有了彻底的了解，下一个任务是理解我们需要从这个结构中提取的各个部分。有时，根据我们处理的数据的复杂性和大小，我们可能需要时间来真正找到并提取我们所需的信息。

一旦我们知道了我们要找什么，并且对数据的结构有了坚实的理解，我们就能更容易地提出简单的算法来从输入数据中提取所需的信息。

# 信息提取

在这个阶段，我们感兴趣的是从输入数据中提取必要的细节。在前一个阶段，我们已经确定了对我们感兴趣的有必要部分。这里我们可以采用以下技术进行信息提取：

+   识别并定位文本所在的位置

    +   分析并找出最佳的信息提取方法：

    +   分词并提取信息

    +   前往偏移量并提取信息

    +   基于正则表达式的信息提取

    +   基于复杂算法的信息提取

根据数据的复杂性，我们可能需要采用上述一种或多种技术来从目标数据中提取信息。

# 无用数据移除

这个阶段可以在信息提取步骤之前或之后发生。这取决于哪个更容易（缩短文本或提取信息）。这是分析师可以做出的设计选择。

在这个阶段，我们正在从信息或输入数据中删除不需要的数据，以便数据进一步提炼，并可以轻松地满足我们的业务需求。

# 数据转换

这也是一个非常重要的阶段，我们强制执行企业定义的标准来定义最终的数据输出。例如，一个组织可以建议所有国家代码应采用 ISO 3166-1 alpha-2 格式。为了遵守这一标准，我们可能需要对包含国家全名的输入数据进行转换。因此，需要进行映射和转换。

可以对输入数据进行许多其他转换，以便以定义良好的形式和符合组织标准的方式，使组织中的任何人都能够消费最终数据。

此步骤还重视拥有企业级标准以提高协作。

# 数据标准化

一旦信息提取完成并且进行了任何必要的清理，我们需要决定如何保存此过程的结果。通常，我们可以使用简单的**CSV**（**逗号分隔值**）格式来保存这些数据。如果我们处理的是复杂输出格式，我们可以选择**XML**（**可扩展标记语言**）或**JSON**（**JavaScript 对象表示法**）格式。

这些格式非常标准化，我们今天拥有的几乎所有技术都能非常容易地理解这些格式。但为了使事情简单起见，最好从 CSV 格式开始。

# 数据掩码

处理客户数据的业务必须确保这些客户的**PII**（**个人可识别信息**）在整个数据管道中不会自由流动。这一标准不仅适用于客户数据，也适用于任何其他被视为机密的数据，如 GDPR、SOX 等标准。为了确保我们保护客户、员工、承包商和供应商的隐私，我们需要采取必要的预防措施，确保当数据通过多个管道时，数据使用者只能看到匿名数据。我们进行的匿名化程度取决于公司遵守的标准以及所在国家的现行标准。

因此，数据掩码可以称为隐藏/转换原始数据部分为其他数据的过程，同时不丢失意义或上下文。

在本节中，我们将了解可用于完成此任务的各种技术：

+   替换：

    +   静态

    +   动态：

        +   加密

        +   哈希

+   隐藏

+   删除

+   截断

+   方差

+   洗牌

# 替换

替换是将数据部分替换为计算数据的过程。它可以数学上定义为：

![图片](img/c470b3b9-8183-41c9-b435-60a02375f6f7.png)

其中 *x* 是源，*y* 是该函数的输出。

为了选择正确的替换机制，我们需要了解这些数据将如何被使用，目标受众以及数据流环境。让我们看看各种可用的替换机制。

# 静态

在此方法中，我们有一个查找表；它包含给定输入集的所有可能的替换。此查找表可以可视化如下：

| **源文本 (y)** | **替换文本 (y)** |
| --- | --- |
| 史蒂夫·乔布斯 | AAPL-1 |
| 猫 | 123456789 |
| 网球 | 板球 |

此表说明了如何构建查找表以替换源文本为不同的文本。当有预定义数量的替换可用时，此方法可扩展性良好。

这种基于查找表替换的另一个例子是我们遵循国家代码的命名标准，例如，ISO-8661：

| **源文本 (x)** | **替换文本 (y)** |
| --- | --- |
| 埃及 | EG |
| 印度 | IN |
| 圣文森特和格林纳丁斯 | VN |
| 英国 | GB |
| 美利坚合众国 | US |

# 动态

当有大量可能性并且我们想使用某些算法更改数据时，这些替换技术是有用的。这些方法可以分为两种类型。

# 加密

这是通过使用某种形式的密钥将给定文本转换为其他形式的过程。这些是数学上定义的函数：

![](img/9891878e-37f3-416b-b943-034b45a84acb.png)

如您所见，这些函数接受输入和密钥，并生成可以使用相同密钥和输出解密的数据：

`VSNN4EtlgZi3/`

如果我们仔细观察，这里起着重要作用的是密钥。在密码学中，根据这个密钥有两种类型的算法可用。这些算法的使用取决于具体情况和密钥传输的挑战。

不深入探讨密码学，让我们尝试理解这些方法：

+   对称密钥加密

+   非对称密钥加密

这两种方法的基本区别在于，在前一种中，我们使用相同的密钥进行加密和解密。但在后一种中，我们使用两个不同的密钥进行加密和解密。

让我们看看几个对称密钥加密的实际例子：

| **算法** | **输入数据** | **输出数据** | **方法** |
| --- | --- | --- | --- |
| ROT13 | `hello` | `uryyb` | 加密 |
|  | `uryyb` | `hello` | 解密 |
| DES | `hello` | `yOYffF4rl8lxCQ4HS2fpMg==` | 加密（密钥是 `hello`） |
|  | `yOYffF4rl8lxCQ4HS2fpMg==` | `hello` | 解密（密钥是 `hello`） |

| RIJNDAEL-256 | `hello` | `v8QbYPszQX/TFeYKbSfPL/` `rNJDywBIQKtxzOzWhBm16/`

![](img/38b93fdb-fc83-4469-9583-b27b217d37b4.png)

`iPqJZpCiXXzDu0sKmKSl6IxbBKhYw==` | 加密（密钥是 `hello`）|

|  | `v8QbYPszQX/TFeYKbSfPL/` `rNJDywBIQKtxzOzWhBm16/`

`VSNN4EtlgZi3/`

`iPqJZpCiXXzDu0sKmKSl6IxbBKhYw==` | `hello` | 加密（密钥是 `hello`）|

如您所见，生成的数据在复杂性和长度上因我们使用的加密算法而异。它还取决于用于加密的秘密密钥。

加密提出了更多计算要求和存储空间的问题。如果我们想将加密作为屏蔽过程中的方法之一，我们需要相应地规划我们的系统。

# 哈希

这也是一种基于密码学的技术，其中原始数据被转换为不可逆的形式。让我们看看这个数学形式：

![](img/70ba8a6e-dc2c-42bd-b83f-28678beefdfb.png)

在这里，与加密的情况不同，我们不能使用输出发现输入是什么。

让我们通过一些例子来更好地理解这一点：

| **输入** | **输出** | **方法** |
| --- | --- | --- |
| `10-point` | `7d862a9dc7b743737e39dd0ea3522e9f` | MD5 |
| `10th` | `8d9407b7f819b7f25b9cfab0fe20d5b3` | MD5 |
| `10-point` | `c10154e1bdb6ea88e5c424ee63185d2c1541efe1bc3d4656a4c3c99122ba9256` | SHA256 |
| `10th` | `5b6e8e1fcd052d6a73f3f0f99ced4bd54b5b22fd4f13892eaa3013ca65f4e2b5` | SHA256 |

我们可以看到，根据我们使用的加密算法，输出大小会有所不同。另一个需要注意的事情是，给定的哈希函数无论输入大小如何，都会产生相同大小的输出。

# 隐藏

在这种方法中，数据被认为甚至对原始所有者来说都过于敏感，以至于不能透露。因此，为了保护数据的机密性，某些文本部分被预定义的字符（例如 X 或任何其他字符）屏蔽，这样只有了解这些片段的人才能提取必要的信息。

**示例**：信用卡信息被认为是高度机密的，绝不应该透露给任何人。如果您在亚马逊等网站上购买过在线商品，您就会看到您的完整信用卡信息不会显示；只显示最后四位数字。由于我是这种信用卡的真正持卡人，我可以轻松地识别它并继续交易。

同样，当需要让数据的一部分被分析师看到时，屏蔽重要的数据部分很重要，这样最终用户就不会看到完整的画面，但仍然可以同时使用这些数据来进行他们正在进行的任何分析。

让我们通过一些例子来更好地理解这一点：

| **数据类型** | **输入** | **输出** | **网络** |
| --- | --- | --- | --- |
| 信用卡 | 4485 **4769 3682** 9843 | 4485 **XXXX XXXX** 9843 | 维萨卡 |
| 信用卡 | 5402 **1324 5087** 3314 | 5402 **XXXX XXXX** 3314 | 万事达卡 |
| 信用卡 | 3772 **951960** 72673 | 3772 **XXXXXX** 72673 | 美国运通卡 |

在前面的例子中，这些数字遵循一个预定义的算法和大小。因此，在固定位置屏蔽数字的简单技术可以更有效。

让我们再举一个隐藏电子邮件地址部分，这些部分在大小和复杂性上都有所不同的例子。在这种情况下，我们必须遵循不同的技术来隐藏字符，以防止泄露完整信息：

| **数据类型** | **输入** | **输出** | **方法** |
| --- | --- | --- | --- |
| 电子邮件 | `hello@world.com` | `h.l.o@w.r.d.com` | 偶数隐藏 |
|  | `simple@book.com` | `.i.p.e@.o.k.c.m` | 奇数隐藏 |
|  | `something@something.com` | `s...th.ng@..me...com` | 复杂隐藏 |

这些技术可以非常简单：

+   **偶数隐藏**：在这个技术中，我们隐藏所有位于偶数位置的字符

+   **奇数隐藏**：我们在输入数据中隐藏每个奇数字符

+   **复杂隐藏**：在这个技术中，我们使用自然语言处理（NLP）来理解我们正在处理的数据，然后尝试应用一个不会泄露太多信息，从而允许任何有智能的人解码的算法

# 删除

如其名所示，当应用于输入数据时，这会导致数据丢失。根据我们处理的数据的重要性，我们需要应用这种技术。这种技术的典型例子是为列中的所有记录设置 `NULL` 值。由于这些空数据无法用来推断任何有意义的信息，这种技术有助于确保机密数据不会被发送到数据处理的其他阶段。

让我们来看几个删除的例子：

| **输入数据** | **输出数据** | **被删除的内容** |
| --- | --- | --- |
| NULL 每月赚取 1000 INR | 拉维每月赚取 NULL | 薪水和姓名 |
| NULL 手机号码是 0123456789 | 拉维的手机号码是 NULL | 手机号码和姓名 |

从例子中，你可能想知道：为什么我们要使这些值无效？当我们对 PII 不感兴趣，而只对数据库/输入中薪资记录或手机号码记录的总结感兴趣时，这种技术很有用。

这个概念也可以扩展到其他用例。

# 截断

另一种删除的变体是截断，其中我们将所有输入数据调整为统一的大小。当我们相当确信在管道的进一步处理中可以接受信息损失时，这很有用。

这也可以是一种智能截断，其中我们了解我们正在处理的数据。让我们看看以下电子邮件地址的例子：

| **输入** | **输出** | **被截断的内容** |
| --- | --- | --- |
| `alice@localhost.com` | `alice` | `@localhost.com` |
| `bob@localhost.com` | `bob` | `@localhost.com` |
| `rob@localhost.com` | `rob` | `@localhost.com` |

从前面的例子中，我们可以看到，所有来自电子邮件的域名部分都被截断，因为它们都属于同一个域名。这种技术节省了存储空间。

# 变化

这种技术适用于本质上是数字的数据类型。它也可以应用于日期/时间值。

这遵循一种统计方法，我们试图通过算法以+/- X 百分比的因子改变输入数据。X 的值纯粹取决于我们进行的分析，并且不应对理解业务数据产生整体影响。

让我们看看几个例子：

| **输入数据** | **输出数据** | **方法** | **说明** |
| --- | --- | --- | --- |
| 100 | 110 | 固定方差 | 增加 10% |
| -100 | 90 | 固定方差 | 减少 10% |
| 1-Jan-2000 | 1-Feb-2000 | 固定方差 | 增加 1 个月 |
| 1-Aug-2000 | 1-Jul-2000 | 固定方差 | 减少一个月 |
| 100 | 101 | 动态方差 | 增加 1%到 5%或减少 1%到 5% |
| 100 | 105 | 动态 | 增加 1%到 5%或减少 1%到 5% |

# 洗牌

这也被认为是实现数据匿名化的一种标准技术。这个过程在拥有具有多个属性（在数据库术语中称为列）的数据记录时更为适用。在这个技术中，记录中的数据会在某一列周围进行洗牌，以确保记录级别的信息发生变化。但从统计学的角度来看，该列中的数据值保持不变。

**示例**：在进行一个组织薪资范围的分析时，我们实际上可以对整个薪资列进行洗牌，其中所有员工的薪资都不会与现实相符。但我们可以使用这些数据来分析薪资范围。

在这个情况下，也可以采用复杂的方法，我们可以根据其他字段（如资历、地理位置等）进行洗牌。这种技术的最终目标是保留数据的含义，同时使发现这些属性原始所有者变得不可能。

让我们用一些示例数据来看看：

![](img/652dfb7b-645d-4f3e-9723-81b5c6a68dc7.png)

有五个带有薪资信息的员工样本记录。上面的表格包含原始薪资详情，下面的表格包含洗牌后的薪资记录。仔细观察数据，你就会明白。记住，在洗牌时，可以应用随机算法来增加发现真相的复杂性。

# 数据安全

在做出非常关键的决定时，数据已经成为企业的重要资产。由于生成和使用这些数据的基础设施复杂性很高，因此对数据的访问模式进行一些控制非常重要。在 Hadoop 生态系统中，我们有 Apache Ranger，这是另一个开源项目，有助于管理大数据的安全性。

# 什么是 Apache Ranger？

Apache Ranger 是一个应用程序，它使数据架构师能够在大数据生态系统中实施安全策略。这个项目的目标是提供一个统一的方式，让所有 Hadoop 应用程序都能遵守定义的安全指南。

这里是 Apache Ranger 的一些特性：

+   集中管理

+   细粒度授权

+   标准化授权

+   多种授权方法

+   集中审计

# 使用 Ambari 安装 Apache Ranger

在本节中，我们将使用 Apache Ambari 安装 Ranger。本节假设已经有一个运行的 Ambari 实例。

# Ambari 管理 UI

打开运行在主节点上的 Ambari Web 界面；然后点击“添加服务”，如图所示：

![图片](img/688e0308-9b80-4fd8-87a0-9c9240265967.png)

这将打开一个模态窗口，“添加服务向导”，它将引导我们完成 Apache Ambari 的完整安装。

# 添加服务

一旦模态窗口可见，从列表中选择 Apache Ranger 服务，并在屏幕上点击“下一步”。

这在以下截图中显示：

![图片](img/2be2832c-2753-4d1d-814c-5d4c8a883777.png)

# 服务放置

一旦选择了服务，我们就会在 UI 中看到下一步，我们需要选择该服务将要安装和运行的服务器。

我已选择 node-3 作为 Ranger（见绿色标签）：

![图片](img/87b67475-da9f-4b66-a0f9-6182ebf9b456.png)

屏幕截图显示如何选择将要安装和运行此服务的服务器

然后，选择页面底部的“下一步”。

# 服务客户端放置

在此步骤中，我们可以选择此服务的客户端可以安装的位置。使用复选框标记您的偏好。

它们看起来像这样：

![图片](img/65982a2c-18c6-4bd3-9665-fe8f2374398d.png)

在做出选择后，请点击“下一步”。

# 主节点上的数据库创建

我们已在主节点上安装了 MySQL 数据库服务器。在继续到 Ambari 向导的下一步之前，我们必须创建一个新的数据库并分配一些权限：

![图片](img/bfe9ba4f-aeca-4c63-b308-b9ce5518efe0.png)

我们还必须使用`ambari-server setup`命令注册 JDBC 驱动程序：

```py
bash-$ sudo ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar
Using python /usr/bin/python
Setup ambari-server
Copying /usr/share/java/mysql-connector-java.jar to /var/lib/ambari-server/resources
If you are updating existing jdbc driver jar for mysql with mysql-connector-java.jar. Please remove the old driver jar, from all hosts. Restarting services that need the driver, will automatically copy the new jar to the hosts.
JDBC driver was successfully initialized.
Ambari Server 'setup' completed successfully.
```

在此步骤之后，我们可以返回到 Ambari 向导。

# Ranger 数据库配置

在向导中，我们将被提示输入数据库名称、用户名和密码。请根据我们在上一步中做出的选择填写它们：

![图片](img/fbc737cb-de1e-4504-b1c5-54eb414da55b.png)

一旦添加了设置，请点击“测试连接”。这将节省很多时间。

如果有任何错误，请返回上一步；检查是否有拼写错误，并重新运行那些步骤。

完成更改后，请点击“下一步”。

# 配置更改

由于我们正在添加 Ranger 服务，Ambari 显示了 Ranger 正确工作所需的配置更改列表。大多数情况下，请保留这些为默认值。

这些更改看起来如下截图所示。一旦更改看起来不错，请点击“确定”继续：

![图片](img/2c1fa6d2-5325-419e-8540-7c7215002dec.png)

# 配置审查

在此步骤中，我们看到了在向导中迄今为止所做的更改列表，并显示了打印更改摘要和部署 Ranger 的选择。

只有当我们点击“部署”时，Ranger 软件才会被安装。在此之前，所有内容都保存在浏览器缓存中。

屏幕看起来像这样：

![图片](img/5ffb2278-f6c4-4dd6-b8a8-35c2e512af9c.png)

# 部署进度

一旦 Ranger 的安装开始，它应该看起来像截图中的那样。不应该有任何失败，因为我们已经正确设置了所有配置。如果有任何失败，请检查日志并通过点击“后退”按钮检查配置：

![图片](img/dd1a49f5-1b3c-47b2-aeb4-fc93c599e8c7.png)

# 应用程序重启

一旦部署完成，我们需要重启所有受影响的 Hadoop 组件，如下面的截图所示：

![图片](img/faa744da-eb8f-4af1-98e2-e1698c0d0d7a.png)

一旦所有组件都已重启，Ambari 仪表板看起来相当健康，我们就完成了 Apache Ranger 的安装。

在下一步中，我们将看到如何使用 Apache Ranger 来处理我们的数据安全。

# Apache Ranger 用户指南

一旦 Apache Ranger 的部署完成，我们可以使用 Apache Ranger 提供的 Web 界面管理整个 Hadoop 基础设施的安全。

# 登录到 UI

如果您没有更改默认设置，Ranger 默认在非 SSL 模式下运行在端口`6080`。在安装了它的服务器上打开端口`6080`的 Web 浏览器（`http://<服务器 IP>:6080`），您将看到一个类似这样的屏幕：

![图片](img/3475861b-dcd2-49b7-a19f-3b8f96b964eb.png)

使用默认用户名`admin`和密码`admin`登录（请登录后首次更改密码，出于安全原因）。

一旦登录成功，我们将被带到*访问管理器*部分。

# 访问管理器

访问管理器允许我们根据服务和标签定义策略。此截图显示了默认的服务列表和配置的策略：

![图片](img/c8e951c5-5dc0-493b-a140-bc425ed0a609.png)

如您所见，由于它们已经在 Ambari 设置中安装，因此已经为 HDFS 服务和 KAFKA 服务定义了策略。

当我们想要定义一个新的服务时，我们可以点击加号图标并定义服务详情。

# 服务详情

在我们开始为服务定义授权规则之前，我们需要定义一个服务，然后向服务添加授权策略。这些是从 UI 定义服务所需的必填属性：

| **UI 元素名称** | **描述** |
| --- | --- |
| 服务名称 | 在代理配置中定义的服务名称 |
| 用户名 | 服务用户的名称 |
| 密码 | 服务用户的密码 |
| Namenode URL | Namenode 的 URL |

通过点击应用程序下方的加号图标（例如，`HDFS`、`Kafka`等）可以定义新的服务。

之后，服务定义屏幕看起来像这样：

![图片](img/58f05368-f783-4656-83a1-21cf9fdcea1e.png)

定义新服务后服务定义屏幕的截图

我们需要填写服务定义所需的所有必要值并保存。稍后，我们需要向此服务添加策略以进行访问控制和审计。

# HDFS 的策略定义和审计

对于 Ranger 中的每个服务，我们都可以将不同的策略与服务中的资源关联起来。在 HDFS 的情况下，资源将是文件/目录路径。

在本节中，我们将为三个用户定义一个新的针对 HDFS 路径名为 projects 的策略：`hdfs-alice`、`hdfs-bob` 和 `hdfs-tom`。其中只有 `hdfs-alice` 被允许所有权限，其余用户只有读取权限。

一旦策略实施，我们将看到 Ranger 如何执行访问限制。

让我们看看策略创建的屏幕：

![图片](img/e5aa0907-08ff-4ed9-87c2-d1ab9cca6a94.png)

屏幕截图显示 Ranger 如何执行访问限制

一旦我们点击添加按钮，此策略就会被注册并添加到当前服务下。

现在，让我们回到 Unix 终端，看看 Ranger 如何执行策略。

此屏幕显示 `hdfs` 和 `hdfs-alice` 用户被允许创建 `/projects` 和 `/projects/1` 目录，但对于 `hdfs-tom` 来说这是被拒绝的：

![图片](img/45c42baf-d515-49e0-8d9c-b93f6498473a.png)

Apache Ranger 在网页界面中还有一个审计部分，我们可以看到这些访问模式。

此屏幕显示 `hdfs-tom` 被拒绝访问，而 `hdfs-alice` 被策略允许访问：

![图片](img/eeb6ddc0-5791-44e6-ad9a-3ea16271fbb3.png)

屏幕截图显示策略拒绝 `hdfs-tom` 访问并允许 `hdfs-alice` 访问

就像这样，我们可以定义自己的策略并自定义 `hdfs` 应该允许/拒绝访问哪些资源。

Ranger 的强大和灵活性来自于其可配置性。访问控制要发挥重要作用，无需任何配置文件和应用重启。

# 摘要

在本章中，我们学习了关于不同数据生命周期阶段的内容，包括数据创建、共享、维护、归档、保留和删除。

本章详细介绍了大数据的管理方法，考虑到它要么是非结构化的，要么是半结构化的，并且具有快速到达率和大量数据。

随着业务组织中生成和使用数据的基础设施复杂性急剧增加，确保数据安全变得至关重要。本章进一步介绍了数据安全工具，如 Apache Ranger，以及帮助我们了解如何控制数据访问模式的模式。

在下一章中，我们将探讨 Hadoop 的安装、其架构和关键组件。
