- en: Practical Sentiment Analysis
  prefs: []
  type: TYPE_NORMAL
- en: This is going to be a fun chapter. In this chapter, we will explore and demonstrate
    some practical examples of using **Natural Language Processing** (**NLP**) concepts
    to understand how unstructured text can be turned into insights. In [Chapter 10](84b19b06-81f4-460f-8c4c-a776f4e66c24.xhtml),
    *Exploring Text Data and Unstructured Data*, we explored the **Natural Language
    Toolkit** (**NLTK**) library and some fundamental features of working with identifying
    words, phrases, and sentences. In that process of tokenizing, we learned how to
    work with data and classify text, but did not go beyond that. In this chapter,
    we will learn about sentiment analysis, which predicts the underlying tone of
    text that's input into an algorithm. We will break down the elements that make
    up an NLP model and the packages used for sentiment analysis before walking through
    an example together.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why sentiment analysis is important
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elements of an NLP model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis in action
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the GitHub repository for this book at [https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter11](https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter11).
  prefs: []
  type: TYPE_NORMAL
- en: You can download and install the required software for this chapter from the
    following link: [https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual).
  prefs: []
  type: TYPE_NORMAL
- en: Why sentiment analysis is important
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Today, we are all living in a digital age where data is entangled in our daily
    lives. However, since most of this data is unstructured and the volume of it is
    large, it requires statistical libraries and **machine learning** (**ML**) to
    apply it to technology solutions. The NLTK libraries serve as a framework for
    us to work with unstructured data, and sentiment analysis serves as a practical
    use case in NLP. **Sentiment analysis**, or opinion mining, is a type of supervised
    ML that requires a training dataset to accurately predict an input sentence, phrase,
    headline, or even tweet is positive, negative, or neutral. Once the model has
    been trained, you can pass unstructured data into it, like a function, and it
    will return a value between negative one and positive one. The number will output
    decimals, and the closer it is to an integer, the more confident the model's accuracy
    will be. Sentiment analysis is an evolving science, so our focus will be on using
    the NLTK corpus libraries. As with any NLP model, you will find inaccuracies in
    the predicted output if you don't have a good sample for the input training data.
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that NLP and sentiment analysis is a deep subject and should be validated
    by a data scientist or ML engineering team if you plan on implementing your own
    models using internal company data sources. That being said, you will notice sentiment
    analysis in many different applications today, and the exercises in this chapter
    provide you with another tool for data analysis. Another benefit of learning about
    how to use sentiment analysis is that it allows you to argue about the data that's
    output from a model. The ability to defend the accuracy and predictive nature
    of working with unstructured data will increase your data literacy skills. For
    example, let's say you are analyzing a population of tweets about a restaurant
    for a marketing campaign that had a mix of positive and negative reviews in the
    past. If the results of your analysis come back as 100% positive, you should start
    questioning the training data, the source of the data, and the model itself. Of
    course, it's possible for all the tweets to be positive, especially against a
    small population of data, but is it likely that every single one has a positive
    sentiment?
  prefs: []
  type: TYPE_NORMAL
- en: This is why **Knowing Your Data** (**KYD**) remains important, as covered in
    [Chapter 1](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml), *Fundamentals of Data
    Analysis*, regardless of the technology and tools being used to analyze it. However,
    why sentiment analysis is important today needs to be stated. First, the accuracy
    of the models has significantly improved because the more training data there
    is, the better the prediction's output. The second point is that NLP models can
    scale beyond what a human can process in the same amount of time. Finally, the
    alternatives to sentiment analysis available today, such as expert systems, are
    more costly because of the time and resources required to implement them. Expert
    system development using text-based logic and wildcard keyword searches is rigid
    and difficult to maintain.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's explore what makes up the elements of NLP and the process of how
    it is used in sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Elements of an NLP model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To summarize the process required to use an NLP supervised ML model for sentiment
    analysis, I have created the following diagram, which shows the elements in a
    logical progression indicated by the letters **A** through **E**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ed89905-db73-4ed0-af73-182b11d8d644.png)'
  prefs: []
  type: TYPE_IMG
- en: The process begins with our source **Unstructured Input Data**, which is represented
    in the preceding diagram with the letter A. Since unstructured data has different
    formats, structures, and forms such as a tweet, sentence, or paragraph, we need
    to perform extra steps to work with the data to gain any insights.
  prefs: []
  type: TYPE_NORMAL
- en: The next element is titled Text Normalization and is represented by the letter
    B in the preceding diagram, and involves concepts such as tokenization, n-grams,
    and **bag-of-words** (**BoW**), which were introduced in [Chapter 10](84b19b06-81f4-460f-8c4c-a776f4e66c24.xhtml),
    *Exploring Text Data and Unstructured Data*. Let's explore them in more detail
    so that we can learn how they are applied in sentiment analysis. BoW is when a
    string of text such as a sentence or paragraph is broken down to determine how
    many times a word occurs. In the process of **tokenizing** to create the bag-of-words
    representation, the location of where the word appears in a sentence, tweet, or
    paragraph becomes less relevant. How each word is classified, categorized, and
    defined using a classifier will serve as input to the next process.
  prefs: []
  type: TYPE_NORMAL
- en: Think of tokens and bag-of-words as raw ingredients to the sentiment analysis
    recipe; as in cooking, the ingredients take additional steps of refinement. Hence,
    the concept of classification becomes important. This is considered a **Features** and
    is represented by the letter C in the preceding diagram. Because tokens are nothing
    more than ASCII characters to a computer, word embedding and tagging is the process
    of converting the words into an input for an ML model. An example would be to
    classify each word with a pair value such as a one or zero to represent true or
    false. This process also includes finding similar words or groupings in order
    to interpret the context.
  prefs: []
  type: TYPE_NORMAL
- en: Creating **Features** is known as feature engineering, which is the foundation
    of supervised ML. Feature engineering is the process of transforming unstructured
    data elements into specific inputs for the prediction model. Models are abstractions
    where the output is only as accurate as the input data behind it. This means models
    need training data with extracted features to improve their accuracy. Without
    feature engineering, the results of a model would be random guesses.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a prediction output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To see how **features** can be extracted from unstructured data, let's walk
    through the NLTK gender feature, which includes some minor modifications from
    the original example. You can find the original source in the *Further reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Launch a new Jupyter Notebook and name it `ch_11_exercises`. Now, follow these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the following libraries by adding the following command to your Jupyter
    Notebook and run the cell. Feel free to follow along by creating your own Notebook.
    I have placed a copy in this book''s GitHub repository for reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The library should already be available using Anaconda. Refer to [Chapter 2](e0fe6eb2-8f38-41f7-9dea-2b177578fd3c.xhtml),
    *Overview of Python and Installing Jupyter Notebook*, for help with setting up
    your environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to download the specific corpus we want to use. Alternatively,
    you can download all the packages using the `all` parameter. If you are behind
    a firewall, there is an `nltk.set_proxy` option available. Check the documentation
    at [nltk.org](http://www.nltk.org/) for more details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the package download is confirmed and
    the output is verified as `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9da572a5-f9b3-4b94-b621-5581c7bf8427.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use the following command to reference the corpus:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To explore the data available in this corpus, let''s run the `print` command
    against the two input sources, `male.txt` and `female.txt`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where a count of the number of words found
    in each source file is printed in the Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82feb653-4599-4e2e-b0ff-08fddf8a04f8.png)'
  prefs: []
  type: TYPE_IMG
- en: We now have a better understanding of the size of the data due to counting the
    number of words found in each source file. Let's continue by looking at the contents
    within each source, taking a look at a few samples from each gender file.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see a list of the first few words found in each source, let''s run the `print`
    command against the two input sources, `male.txt` and `female.txt`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where a list of words found in each source
    file is printed in the Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52eeafdf-75a9-4646-8ed6-1205b9853fdf.png)'
  prefs: []
  type: TYPE_IMG
- en: Remember that the computer has no idea if a name actually returns a value of
    `male` or `female`. The corpus has defined them as two different source files
    as a list of values that the NLTK library has identified as words because they
    have been defined as such. With thousands of names defined as either male or female,
    you can use this data as input for sentiment analysis. However, identifying gender
    alone will not determine whether the sentiment is positive or negative, so additional
    elements are required.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next element, labeled D in the first diagram, is the actual **NLP supervised
    ML** algorithm. Remember, building an accurate model involves using feature engineering, along
    with NLTK libraries and classifier models. When used correctly, the output will
    be based on the input **training** and **test** data. Models should always be
    validated and the accuracy should be measured. For our example, which is building
    a basic gender determination model, we are going to use `NaiveBayesClassifier`,
    which is available in the NLKT libraries. The Naïve Bayes Classifier is an ML
    model created from Bayes theorem that is used to determine the probability of
    an event happening based on how often another similar event has occurred. A classifier
    is a process that chooses the correct tag value or label based on an inputted
    feature dataset. The mathematical concepts behind these models and libraries are
    vast, so I have added some links in the *Further reading* section for additional
    reference. To complete the elements of sentiment analysis summarized in the first
    diagram, we will create a prediction output, so let''s continue in our Jupyter
    Notebook session:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `gender_features` function that returns the last letter of any input
    word. The model will use this classifier feature as input to predict the output,
    which, based on the concept that first names that end in the letters **A**, **E**,
    and **I** are more likely to be female, while first names ending in **K**, **O**,
    **R**, **S**, or **T** are more likely to be male. There will be no output after
    you run the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Remember to indent the second line in your cell so that Python can process the
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'To confirm the function will return a value, enter the following command, which
    prints the last character of any inputted name or word:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the last character from the inputted
    word `Debra` is printed in the Notebook with `Out[]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4e4784f-f863-4a4c-907f-d8cbedd7dd21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Create a new variable named `labeled_names` that loops through both source
    gender files and assigns a **name-value pair** so that it can be identified as
    either male or female to be input into the model. To see the results after the
    loop has completed, we print the first few values to verify that the `labeled_names`
    variable contains data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where each name value from the source file
    will be combined with a tag of `male` or `female`, depending on which text file
    source it came from:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b2b946b-a7a4-483d-b66a-bc5ee82a7279.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the model should be trained using a random list of values to avoid any
    bias, we will input the random function and shuffle all the name and gender combinations,
    which will change the sequence of how they are stored in the `labeled_names` variable.
    I added a `print()` statement so that you can see the difference from the output
    created in the prior step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where each name value from the source file
    will be combined with a tag of `male` or `female`, depending on which text file
    source it came from:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9600fe81-df59-43a7-a7dc-4a984d8f9352.png)'
  prefs: []
  type: TYPE_IMG
- en: Note because the `random()` function is used, the results of the `print()` function
    will always change each time you run the cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we are going to train the model by creating features for each gender
    using the last letter from each name in the `labeled_names` variable. We will
    print the new variable called `featuresets` so that you can see how the feature
    will be used in the next step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where each combination of the last letter
    from the names is assigned to a gender value, thereby creating a list of name-value
    pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1fe1f7d-2d16-4ba0-8b35-aa25762dcfa6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we are going to slice the data from the `featuresets` variable list into
    two input datasets called `train_set` and `test_set`. Once we have those datasets
    separated, we can use `train_set` as an input for the classifier. We use the `len()`
    function to give us a sense of the size of each dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the results of the `len()` function
    provide context as to how large each dataset is compared to the others:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da01ffe1-b381-45c6-954a-e078559ef4dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now pass the `train_set` variable as input to the NLTK Naïve Bayes
    classifier. The model is assigned the name `classifier`, so you can call it like
    a function in the next step. There will be no output once you run the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will validate the results of the model by sending random names into
    the model using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the gender values of either `male` or
    `female` will be displayed after each name is passed as a parameter in the `classifier` model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6af5b410-9cd6-4b42-be43-a175cebe5b51.png)'
  prefs: []
  type: TYPE_IMG
- en: Congratulations – you have successfully created your first supervised ML model!
    As you can see, the classifier model has some accuracy issues and returns incorrect
    values in some cases. For example, when you pass in the values of `Aaron`, `Marc`,
    or `Debra`, the gender results are predicted correctly. The name `Aaron` was found
    in the training data, so that was no surprise. However, the model shows signs
    of being incomplete or requiring additional features because it returns the incorrect
    gender when using the nickname of `Deb` for `Debra` and for the name `Seth`, who
    is male.
  prefs: []
  type: TYPE_NORMAL
- en: How do we solve this problem? There are a few approaches that can be used, all
    of which we will explore next.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NLTK libraries include a few packages to help solve the issues we experienced
    in the gender classifier model. The first is the `SentimentAnalyzer` module, which
    allows you to include additional features using built-in functions. What's special
    about these packages is that they go beyond traditional functions where defined
    parameters are passed in. In Python, arguments (`args`) and keyword arguments (`kwargs`)
    allow us to pass name-value pairs and multiple argument values into a function.
    These are represented with asterisks; for example, `*args` or `**kwargs`. The
    NLTK `SentimentAnalyzer` module is a useful utility for teaching purposes, so
    let's continue by walking through the features that are available within it.
  prefs: []
  type: TYPE_NORMAL
- en: The second is called **VADER**, which stands for **Valence Aware Dictionary
    and Sentiment Reasoner**. It was built to handle social media data. The VADER
    sentiment library has a dictionary known as a **lexicon** and includes a rule-based
    algorithm specifically built to process acronyms, emoticons, and slang. A nice
    feature available from VADER is that it already includes training data and we
    can use a built-in function called `polarity_scores()` that returns key insights
    in the output that's displayed. The first is a compound score that is between
    negative one and positive one. This provides you with a normalized sum of VADER's
    lexicon ratings in a single score. For example, if the output returns `0.703`,
    this would be an extremely positive sentence, while a compound score of `-0.5719`
    would be interpreted as negative. The next output from the VADER tool is a distribution
    score in terms of how positive, negative, or neutral the input is from zero to
    one.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the sentence `I HATE my school!` would return the results shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f61dda82-d3c8-4142-b5f2-e6d096eda89c.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, a compound value of `-0.6932` is returned, which validates the
    VADER model is accurately predicting the sentiment as very negative. On the same
    output line, you can see `'neg'`, `'neu'`, and `'pos'`, which are short for negative,
    neutral, and positive, respectively. Each metric next to the values provides a
    little more detail about how the compound score was derived. In the preceding
    screenshot, we can see a value of `0.703`, which means that the model prediction
    is 70.3% negative, with the remaining 29.7% being neutral. The model returned
    a value of `0.0` next to `pos`, so there is a `0%` positive sentiment based on
    the built-in VADER training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the VADER sentiment analysis scoring methodology has been trained
    to handle social media data and informal proper grammar. For example, if a tweet
    includes multiple exclamation points for emphasis, the compound score will increase.
    Capitalization, the use of conjunctions, and the use of swear words will all be
    accounted for in the output from the model. So, the main benefit of using VADER
    is that it already includes those extra steps required to feature and train the
    model, but you lose the ability to customize it with additional features.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a better understanding of the VADER tool, let's walk through
    an example of using it.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis in action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's continue with our Jupyter Notebook session and walk through how to install
    and use the VADER sentiment analysis library. First, we will walk through an example
    of using manual input and then learn how to load data from a file.
  prefs: []
  type: TYPE_NORMAL
- en: Manual input
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to learn how to use manual input in VADER:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the NLTK library and download the `vader_lexicon` library so that all
    the necessary functions and features will be available:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the package download will be confirmed
    and the output is verified as `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce376473-cec8-4d0b-b699-311c081e7aa0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Import `SentimentIntensityAnalyzer` from the NLTK Vader library. There will
    be no output when you run the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To make it easier, we will assign a variable object called `my_ analyzer` and
    assign it to the `SentimentIntensityAnalyzer()` model. There will be no output
    after you run the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will create a variable named `my_input_sentence` and assign it a string
    value of `I HATE my school!`. On the second line, we will call the model and pass
    the variable as an argument to the `polarity_scores()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where we can see the result of the VADER sentiment
    analysis model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15c382c5-8032-48d1-b0f5-90170521b9a3.png)'
  prefs: []
  type: TYPE_IMG
- en: Excellent—you have now utilized the VADER sentiment analysis model and returned
    results to determine whether a sentence is positive or negative. Now that we understand
    how the model works with individual input sentences, let's demonstrate how to
    work with a sample social media file and combine it with what we have learned
    using the `pandas` and `matplotlib` libraries.
  prefs: []
  type: TYPE_NORMAL
- en: In the next exercise, we are going to work with a text file source that you
    will need to import into your Jupyter Notebook. This is a small sample CSV file
    containing example social media type free text, including a hashtag, informal
    grammar, and extra punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: 'It has 2 columns and 10 rows of content, with a header row for easy reference,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f74ab551-8bad-452a-b028-648fee1e3293.png)'
  prefs: []
  type: TYPE_IMG
- en: Social media file input
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s continue working with our Jupyter Notebook session and walk through
    how to work with this source file so that it includes a VADER sentiment and then
    analyze the results:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to import some additional libraries so that we can work with and
    analyze the results, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We also have to install a new library named `twython`. Use the following command
    to install it in your Notebook session. The `twython` library includes features
    to make it easier to read social media data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the resulting installation will be displayed.
    If you need to upgrade `pip`, you may need to run additional commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f344ef7a-acd1-458c-b462-413a7dc5e532.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If required, re-import the NLTK library and import the `SentimentIntensityAnalyzer`
    module. No output will be displayed after you run the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a variable as `analyzer` to make it easier to reference later in the
    code. No output will be displayed after you run the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If required, redownload the NLTK `vader_lexicon`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the download result will be displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7fdb4ce1-5635-46ce-b0d1-6ce5e7f0ba6f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will read in the `.csv` file using the `pandas` library and assign
    the result to a variable named `sentences`. To validate the results, you can run
    the `len()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Be sure to upload the source CSV file in the correct file location so that you
    can reference it in your Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will look as follows, where the value of `10` will be displayed.
    This matches the number of records in the source CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2354dda-409f-4918-931f-98122dd0d13a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To preview the data and verify that your DataFrame is loaded correctly, you
    can run the `head()` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the results of the `head()` function
    are displayed to verify that the source file is now a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69ed741e-9d6d-4e4d-af6c-619ec7d5beea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following block of code includes a few steps that look through the DataFrame,
    analyze the text source, apply the VADER sentiment metrics, and assign the results
    to a `numpy` array for easier usage. No output will be displayed after you run
    the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Be sure to double-check your indentations when entering multiple commands in
    the Jupyter Notebook input cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can extend the source DataFrame so that it includes the results from
    the VADER sentiment model. This will create four new columns. No output will be
    displayed after you run the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To see the changes, run the `head()` function again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the results of the `head()` function
    are displayed to verify that the DataFrame now includes the new columns that were
    created from the loop in the previous step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c60b59c6-3d09-4bde-bed8-5772519b0f1c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'While this information is useful, it still requires the user to scan through
    the results row by row. Let''s make it easier to analyze and summarize the results
    by creating a new column that categorizes the compound score results. No output
    will be displayed after you run the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to before, we will take the results and add a new column to our DataFrame
    called `my prediction sentiment`. No output will be displayed after you run the
    cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'To see the changes, run the `head()` function again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the results of the `head()` function
    are displayed to verify that the DataFrame now includes the new column that was
    created from the loop in the previous step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b8e0078-f6f3-4dd9-89d7-d7bd8ffdfd7b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To make it easier to interpret the results, let''s create a data visualization
    against the DataFrame by summarizing the results using an aggregate `groupby`.
    We''ll use the `plot()` function from the `matplotlib` library to display a horizontal
    bar chart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where a horizontal bar chart will be displayed
    showing a summary of the count of the text by sentiment in terms of positive,
    negative, and neutral:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd8ba209-d076-46b9-902f-58fc141d195c.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we have more positive opinions in our data source. It was much
    faster to interpret the results like this because we visualized the results to
    make it easier to consume them visually. We now have a reusable workflow to analyze
    much larger volumes of unstructured data by looking at a source data file and
    applying the VADER sentiment analysis model to each record. If you replace the
    sample CSV file with any social media source, you can rerun the same steps and
    see how the analysis changes.
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy score for VADER models is around 96%, which has been proven to
    be more accurate than a human interpretation according to research on the subject.
  prefs: []
  type: TYPE_NORMAL
- en: There is some bias in the analysis since the bins of **positive**, **negative**,
    and **neutral** can be adjusted in the code. As a good data analyst, understanding
    the bias can help you either adjust it for your specific needs or be able to communicate
    the challenges of working with free text data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations—you have successfully walked through the foundations of NLP and
    should have a high-level understanding of supervised ML using the NLTK libraries!
    Sentiment analysis is a fascinating and evolving science that has many different
    moving parts. I hope this introduction is a good start to your continued research
    so that you can utilize it in your data analysis. In this chapter, we learned
    about the various elements of sentiment analysis, such as feature engineering,
    along with the process of how an NLP ML algorithm works. We also learned how to
    install NLP libraries in Jupyter to work with unstructured data, along with how
    to analyze the results created by a classifier model. With this knowledge, we
    walked through an example of how to use the VADER sentiment analysis model and
    visualized the results for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In our last chapter, [Chapter 12](4a24a1e7-aff4-4812-ad21-20e5b8737bd9.xhtml),
    *Bringing it all Together*, we will bring together all the concepts we've covered
    in this book and walk through some real-world examples.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLTK sentiment analysis example: [https://www.nltk.org/howto/sentiment.html](https://www.nltk.org/howto/sentiment.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The source code for VADER and its documentation: [https://github.com/cjhutto/vaderSentiment](https://github.com/cjhutto/vaderSentiment)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayes theorem explained: [https://plato.stanford.edu/entries/bayes-theorem/](https://plato.stanford.edu/entries/bayes-theorem/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VADER sentiment analysis research: [http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
