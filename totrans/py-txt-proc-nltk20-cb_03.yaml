- en: Chapter 3. Creating Custom Corpora
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a custom corpus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a word list corpus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a part-of-speech tagged word corpus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a chunked phrase corpus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a categorized text corpus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a categorized chunk corpus reader
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lazy corpus loading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a custom corpus view
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a MongoDB backed corpus reader
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Corpus editing with file locking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll cover how to use corpus readers and create custom corpora.
    At the same time, you'll learn how to use the existing corpus data that comes
    with NLTK. This information is essential for future chapters when we'll need to
    access the corpora as training data. We'll also cover creating custom corpus readers,
    which can be used when your corpus is not in a file format that NLTK already recognizes,
    or if your corpus is not in files at all, but instead is located in a database
    such as MongoDB.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a custom corpus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **corpus** is a collection of text documents, and **corpora** is the plural
    of corpus. So a *custom corpus* is really just a bunch of text files in a directory,
    often alongside many other directories of text files.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You should already have the NLTK data package installed, following the instructions
    at [http://www.nltk.org/data](http://www.nltk.org/data). We'll assume that the
    data is installed to `C:\nltk_data` on Windows, and `/usr/share/nltk_data` on
    Linux, Unix, or Mac OS X.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'NLTK defines a list of data directories, or **paths**, in `nltk.data.path`.
    Our custom corpora must be within one of these paths so it can be found by NLTK.
    So as not to conflict with the official data package, we''ll create a custom `nltk_data`
    directory in our home directory. Here''s some Python code to create this directory
    and verify that it is in the list of known paths specified by `nltk.data.path`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If the last line, `path in nltk.data.path`, is `True`, then you should now have
    a `nltk_data` directory in your home directory. The path should be `%UserProfile%\nltk_data`
    on Windows, or `~/nltk_data` on Unix, Linux, or Mac OS X. For simplicity, I'll
    refer to the directory as `~/nltk_data`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the last line does not return `True`, try creating the `nltk_data` directory
    manually in your home directory, then verify that the absolute path is in `nltk.data.path`.
    It's essential to ensure that this directory exists and is in `nltk.data.path`
    before continuing. Once you have your `nltk_data` directory, the convention is
    that corpora reside in a `corpora` subdirectory. Create this `corpora` directory
    within the `nltk_data` directory, so that the path is `~/nltk_data/corpora`. Finally,
    we'll create a subdirectory in `corpora` to hold our custom corpus. Let's call
    it `cookbook`, giving us the full path of `~/nltk_data/corpora/cookbook`.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can create a simple *word list* file and make sure it loads. In [Chapter
    2](ch02.html "Chapter 2. Replacing and Correcting Words"), *Replacing and Correcting
    Words*, *Spelling correction with Enchant* recipe, we created a word list file
    called `mywords.txt`. Put this file into `~/nltk_data/corpora/cookbook/`. Now
    we can use `nltk.data.load()` to load the file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We need to specify `format='raw'` since `nltk.data.load()` doesn't know how
    to interpret `.txt` files. As we'll see, it does know how to interpret a number
    of other file formats.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `nltk.data.load()` function recognizes a number of formats, such as `'raw'`,
    `'pickle'`, and `'yaml'`. If no format is specified, then it tries to guess the
    format based on the file's extension. In the previous case, we have a `.txt` file,
    which is not a recognized extension, so we have to specify the `'raw'` format.
    But if we used a file that ended in `.yaml`, then we would not need to specify
    the format.
  prefs: []
  type: TYPE_NORMAL
- en: Filenames passed in to `nltk.data.load()` can be *absolute* or *relative* paths.
    Relative paths must be relative to one of the paths specified in `nltk.data.path`.
    The file is found using `nltk.data.find(path)`, which searches all known paths
    combined with the relative path. Absolute paths do not require a search, and are
    used as is.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For most corpora access, you won't actually need to use `nltk.data.load`, as
    that will be handled by the `CorpusReader` classes covered in the following recipes.
    But it's a good function to be familiar with for loading `.pickle` files and `.yaml`
    files, plus it introduces the idea of putting all of your data files into a path
    known by NLTK.
  prefs: []
  type: TYPE_NORMAL
- en: Loading a YAML file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you put the `synonyms.yaml` file from the [Chapter 2](ch02.html "Chapter 2. Replacing
    and Correcting Words"), *Replacing and Correcting Words*, *Replacing synonyms*
    recipe, into `~/nltk_data/corpora/cookbook` (next to `mywords.txt`), you can use
    `nltk.data.load()` to load it without specifying a format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This assumes that PyYAML is installed. If not, you can find download and installation
    instructions at [http://pyyaml.org/wiki/PyYAML](http://pyyaml.org/wiki/PyYAML).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next recipes, we'll cover various corpus readers, and then in the *Lazy
    corpus loading* recipe, we'll use the `LazyCorpusLoader`, which expects corpus
    data to be in a `corpora` subdirectory of one of the paths specified by `nltk.data.path`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a word list corpus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `WordListCorpusReader` is one of the simplest `CorpusReader` classes. It
    provides access to a file containing a list of words, one word per line. In fact,
    you've already used it when we used the `stopwords` corpus in the *Filtering stopwords
    in a tokenized sentence* and *Discovering word collocations* recipes in [Chapter
    1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing Text
    and WordNet Basics*.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need to start by creating a word list file. This could be a single column
    CSV file, or just a normal text file with one word per line. Let''s create a file
    named `wordlist` that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we can instantiate a `WordListCorpusReader` that will produce a list of
    words from our file. It takes two arguments: the directory path containing the
    files, and a list of filenames. If you open the Python console in the same directory
    as the files, then `''.''` can be used as the directory path. Otherwise, you must
    use a directory path such as: `''nltk_data/corpora/cookbook''`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`WordListCorpusReader` inherits from `CorpusReader`, which is a common base
    class for all corpus readers. `CorpusReader` does all the work of identifying
    which files to read, while `WordListCorpus` reads the files and tokenizes each
    line to produce a list of words. Here''s an inheritance diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/3609OS_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When you call the `words()` function, it calls `nltk.tokenize.line_tokenize()`
    on the raw file data, which you can access using the `raw()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `stopwords` corpus is a good example of a multi-file `WordListCorpusReader`.
    In [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing
    Text and WordNet Basics*, in the *Filtering stopwords in a tokenized sentence*
    recipe, we saw that it had one word list file for each language, and you could
    access the words for that language by calling `stopwords.words(fileid)`. If you
    want to create your own multi-file word list corpus, this is a great example to
    follow.
  prefs: []
  type: TYPE_NORMAL
- en: Names corpus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another word list corpus that comes with NLTK is the `names` corpus. It contains
    two files: `female.txt` and `male.txt`, each containing a list of a few thousand
    common first names organized by gender.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: English words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NLTK also comes with a large list of English words. There's one file with 850
    `basic` words, and another list with over 200,000 known English words.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing
    Text and WordNet Basics*, the *Filtering stopwords in a tokenized sentence* recipe,
    has more details on using the `stopwords` corpus. In the following recipes, we'll
    cover more advanced corpus file formats and corpus reader classes.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a part-of-speech tagged word corpus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Part-of-speech tagging** is the process of identifying the part-of-speech
    tag for a word. Most of the time, a *tagger* must first be trained on a *training
    corpus*. How to train and use a tagger is covered in detail in [Chapter 4](ch04.html
    "Chapter 4. Part-of-Speech Tagging"), *Part-of-Speech Tagging*, but first we must
    know how to create and use a training corpus of part-of-speech tagged words.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The simplest format for a tagged corpus is of the form "word/tag". Following
    is an excerpt from the `brown` corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Each word has a *tag* denoting its part-of-speech. For example, `nn` refers
    to a noun, while a tag that starts with `vb` is a verb.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you were to put the previous excerpt into a file called `brown.pos`, you
    could then create a `TaggedCorpusReader` and do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This time, instead of naming the file explicitly, we use a regular expression,
    `r'.*\.pos'`, to match all files whose name ends with `.pos`. We could have done
    the same thing as we did with the `WordListCorpusReader`, and pass `['brown.pos']`
    as the second argument, but this way you can see how to include multiple files
    in a corpus without naming each one explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: '`TaggedCorpusReader` provides a number of methods for extracting text from
    a corpus. First, you can get a list of all words, or a list of tagged tokens.
    A **tagged token** is simply a tuple of `(word, tag)`. Next, you can get a list
    of every sentence, and also every tagged sentence, where the sentence is itself
    a list of words or tagged tokens. Finally, you can get a list of paragraphs, where
    each paragraph is a list of sentences, and each sentence is a list of words or
    tagged tokens. Here''s an inheritance diagram listing all the major methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/3609OS_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The functions demonstrated in the previous diagram all depend on *tokenizers*
    for splitting the text. `TaggedCorpusReader` tries to have good defaults, but
    you can customize them by passing in your own tokenizers at initialization time.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing the word tokenizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The default word tokenizer is an instance of `nltk.tokenize.WhitespaceTokenizer`.
    If you want to use a different tokenizer, you can pass that in as `word_tokenizer`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Customizing the sentence tokenizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The default sentence tokenizer is an instance of `nltk.tokenize.RegexpTokenize`
    with `'\n'` to identify the gaps. It assumes that each sentence is on a line all
    by itself, and individual sentences do not have line breaks. To customize this,
    you can pass in your own tokenizer as `sent_tokenizer`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Customizing the paragraph block reader
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Paragraphs are assumed to be split by blank lines. This is done with the default
    `para_block_reader`, which is `nltk.corpus.reader.util.read_blankline_block`.
    There are a number of other block reader functions in `nltk.corpus.reader.util`,
    whose purpose is to read blocks of text from a *stream*. Their usage will be covered
    in more detail in the later recipe, *Creating a custom corpus view*, where we'll
    create a custom corpus reader.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing the tag separator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you don't want to use `'/'` as the word/tag separator, you can pass an alternative
    string to `TaggedCorpusReader` for `sep`. The default is `sep='/'`, but if you
    want to split words and tags with `'|'`, such as 'word|tag', then you should pass
    in `sep='|'`.
  prefs: []
  type: TYPE_NORMAL
- en: Simplifying tags with a tag mapping function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you''d like to somehow transform the part-of-speech tags, you can pass in
    a `tag_mapping_function` at initialization, then call one of the `tagged_*` functions
    with `simplify_tags=True`. Here''s an example where we lowercase each tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Calling `tagged_words()` without `simplify_tags=True` would produce the same
    result as if you did not pass in a `tag_mapping_function`.
  prefs: []
  type: TYPE_NORMAL
- en: There are also a number of tag simplification functions defined in `nltk.tag.simplify`.
    These can be useful for reducing the number of different part-of-speech tags.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Chapter 4](ch04.html "Chapter 4. Part-of-Speech Tagging"), *Part-of-Speech
    Tagging* will cover part-of-speech tags and tagging in much more detail. And for
    more on tokenizers, see the first three recipes of [Chapter 1](ch01.html "Chapter 1. Tokenizing
    Text and WordNet Basics"), *Tokenizing Text and WordNet Basics*.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next recipe, we'll create a *chunked phrase* corpus, where each phrase
    is also part-of-speech tagged.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a chunked phrase corpus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **chunk** is a short phrase within a sentence. If you remember sentence diagrams
    from grade school, they were a tree-like representation of phrases within a sentence.
    This is exactly what chunks are: *sub-trees within a sentence tree*, and they
    will be covered in much more detail in [Chapter 5](ch05.html "Chapter 5. Extracting
    Chunks"), *Extracting Chunks*. Following is a sample sentence tree with three
    noun phrase (**NP**) chunks shown as sub-trees.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a chunked phrase corpus](img/3609OS_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This recipe will cover how to create a corpus with sentences that contain chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is an excerpt from the tagged `treebank` corpus. It has part-of-speech
    tags, as in the previous recipe, but it also has square brackets for denoting
    chunks. This is the same sentence as in the previous tree diagram, but in text
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In this format, every chunk is a *noun phrase*. Words that are not within brackets
    are part of the sentence tree, but are not part of any noun phrase sub-tree.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Put this excerpt into a file called `treebank.chunk`, and then do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ChunkedCorpusReader` provides the same methods as the `TaggedCorpusReader`
    for getting tagged tokens, along with three new methods for getting chunks. Each
    chunk is represented as an instance of `nltk.tree.Tree`. Sentence level trees
    look like `Tree(''S'', [...])` while noun phrase trees look like `Tree(''NP'',
    [...])`. In `chunked_sents()`, you get a list of sentence trees, with each noun-phrase
    as a sub-tree of the sentence. In `chunked_words()`, you get a list of noun phrase
    trees alongside tagged tokens of words that were not in a chunk. Here''s an inheritance
    diagram listing the major methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/3609OS_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can draw a `Tree` by calling the `draw()` method. Using the corpus reader
    defined earlier, you could do `reader.chunked_sents()[0].draw()` to get the same
    sentence tree diagram shown at the beginning of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`ChunkedCorpusReader` is similar to the `TaggedCorpusReader` from the last
    recipe. It has the same default `sent_tokenizer` and `para_block_reader`, but
    instead of a `word_tokenizer`, it uses a `str2chunktree()` function. The default
    is `nltk.chunk.util.tagstr2tree()`, which parses a sentence string containing
    bracketed chunks into a sentence tree, with each chunk as a noun phrase sub-tree.
    Words are split by whitespace, and the default word/tag separator is `''/''`.
    If you want to customize the chunk parsing, then you can pass in your own function
    for `str2chunktree()`.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An alternative format for denoting chunks is called IOB tags. **IOB** tags are
    similar to part-of-speech tags, but provide a way to denote the inside, outside,
    and beginning of a chunk. They also have the benefit of allowing multiple different
    chunk phrase types, not just noun phrases. Here is an excerpt from the `conll2000`
    corpus. Each word is on its own line with a part-of-speech tag followed by an
    IOB tag.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '`B-NP` denotes the beginning of a noun phrase, while `I-NP` denotes that the
    word is inside of the current noun phrase. `B-VP` and `I-VP` denote the beginning
    and inside of a verb phrase. `O` ends the sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: To read a corpus using the IOB format, you must use the `ConllChunkCorpusReader`.
    Each sentence is separated by a blank line, but there is no separation for paragraphs.
    This means that the `para_*` methods are not available. If you put the previous
    IOB example text into a file named `conll.iob`, you can create and use a `ConllChunkCorpusReader`
    with the code we are about to see. The third argument to `ConllChunkCorpusReader`
    should be a tuple or list specifying the types of chunks in the file, which in
    this case is `('NP', 'VP', 'PP')`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous code also shows the `iob_words()` and `iob_sents()` methods, which
    return lists of three tuples of `(word, pos, iob)`. The inheritance diagram for
    `ConllChunkCorpusReader` looks like the following, with most of the methods implemented
    by its superclass, `ConllCorpusReader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more...](img/3609OS_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tree leaves
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When it comes to chunk trees, the leaves of a tree are the tagged tokens. So
    if you want to get a list of all the tagged tokens in a tree, call the `leaves()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Treebank chunk corpus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `nltk.corpus.treebank_chunk` corpus uses `ChunkedCorpusReader` to provide
    part-of-speech tagged words and noun phrase chunks of Wall Street Journal headlines.
    NLTK comes with a 5% sample from the Penn Treebank Project. You can find out more
    at [http://www.cis.upenn.edu/~treebank/home.html](http://www.cis.upenn.edu/~treebank/home.html).
  prefs: []
  type: TYPE_NORMAL
- en: CoNLL2000 corpus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**CoNLL** stands for the **Conference on Computational Natural Language Learning**.
    For the year 2000 conference, a shared task was undertaken to produce a corpus
    of chunks based on the Wall Street Journal corpus. In addition to noun phrases
    (`NP`), it also contains verb phrases (`VP`) and prepositional phrases (`PP`).
    This chunked corpus is available as `nltk.corpus.conll2000`, which is an instance
    of `ConllChunkCorpusReader`. You can read more at [http://www.cnts.ua.ac.be/conll2000/chunking/](http://www.cnts.ua.ac.be/conll2000/chunking/).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Chapter 5](ch05.html "Chapter 5. Extracting Chunks"), *Extracting Chunks*
    will cover chunk extraction in detail. Also see the previous recipe for details
    on getting tagged tokens from a corpus reader.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a categorized text corpus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have a large corpus of text, you may want to categorize it into separate
    sections. The brown corpus, for example, has a number of different categories.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In this recipe, we'll learn how to create our own categorized text corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The easiest way to categorize a corpus is to have one file for each category.
    Following are two excerpts from the `movie_reviews` corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '`movie_pos.txt`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '`movie_neg.txt`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'With these two files, we''ll have two categories: `pos` and `neg`.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll use the `CategorizedPlaintextCorpusReader`, which inherits from both
    `PlaintextCorpusReader` and `CategorizedCorpusReader`. These two superclasses
    require three arguments: the root directory, the `fileids`, and a category specification.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first two arguments to `CategorizedPlaintextCorpusReader` are the root directory
    and `fileids`, which are passed on to the `PlaintextCorpusReader` to read in the
    files. The `cat_pattern` keyword argument is a regular expression for extracting
    the category names from the `fileids`. In our case, the category is the part of
    the `fileid` after `movie_` and before `.txt`. **The category must be surrounded
    by grouping parenthesis**.
  prefs: []
  type: TYPE_NORMAL
- en: '`cat_pattern` is passed to `CategorizedCorpusReader`, which overrides the common
    corpus reader functions such as `fileids()`, `words()`, `sents()`, and `paras()`
    to accept a `categories` keyword argument. This way, you could get all the `pos`
    sentences by calling `reader.sents(categories=[''pos''])`. `CategorizedCorpusReader`
    also provides the `categories()` function, which returns a list of all known categories
    in the corpus.'
  prefs: []
  type: TYPE_NORMAL
- en: '`CategorizedPlaintextCorpusReader` is an example of using multiple-inheritance
    to join methods from multiple superclasses, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/3609OS_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of `cat_pattern`, you could pass in a `cat_map`, which is a dictionary
    mapping a `fileid` to a list of category labels.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Category file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A third way of specifying categories is to use the `cat_file` keyword argument
    to specify a filename containing a mapping of `fileid` to category. For example,
    the `brown` corpus has a file called `cats.txt` that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `reuters` corpus has files in multiple categories, and its `cats.txt` looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Categorized tagged corpus reader
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `brown` corpus reader is actually an instance of `CategorizedTaggedCorpusReader`,
    which inherits from `CategorizedCorpusReader` and `TaggedCorpusReader`. Just like
    in `CategorizedPlaintextCorpusReader`, it overrides all the methods of `TaggedCorpusReader`
    to allow a `categories` argument, so you can call `brown.tagged_sents(categories=['news'])`
    to get all the tagged sentences from the `news` category. You can use the `CategorizedTaggedCorpusReader`
    just like `CategorizedPlaintextCorpusReader` for your own categorized and tagged
    text corpora.
  prefs: []
  type: TYPE_NORMAL
- en: Categorized corpora
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `movie_reviews` corpus reader is an instance of `CategorizedPlaintextCorpusReader`,
    as is the `reuters` corpus reader. But where the `movie_reviews` corpus only has
    two categories (`neg` and `pos`), `reuters` has 90 categories. These corpora are
    often used for training and evaluating classifiers, which will be covered in [Chapter
    7](ch07.html "Chapter 7. Text Classification"), *Text Classification*.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next recipe, we'll create a subclass of `CategorizedCorpusReader` and
    `ChunkedCorpusReader` for reading a categorized chunk corpus. Also see [Chapter
    7](ch07.html "Chapter 7. Text Classification"), *Text Classification* in which
    we use categorized text for classification.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a categorized chunk corpus reader
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLTK provides a `CategorizedPlaintextCorpusReader` and `CategorizedTaggedCorpusReader`,
    but there's no categorized corpus reader for chunked corpora. So in this recipe,
    we're going to make one.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the earlier recipe, *Creating a chunked phrase corpus*, for an explanation
    of `ChunkedCorpusReader`, and to the previous recipe for details on `CategorizedPlaintextCorpusReader`
    and `CategorizedTaggedCorpusReader`, both of which inherit from `CategorizedCorpusReader`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll create a class called `CategorizedChunkedCorpusReader` that inherits
    from both `CategorizedCorpusReader` and `ChunkedCorpusReader`. It is heavily based
    on the `CategorizedTaggedCorpusReader`, and also provides three additional methods
    for getting categorized chunks. The following code is found in `catchunked.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: All of the following methods call the corresponding function in `ChunkedCorpusReader`
    with the value returned from `_resolve()`. We'll start with the plain text methods.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Next comes the tagged text methods.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: And finally, the chunked methods, which is what we've really been after.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: All these methods together give us a complete `CategorizedChunkedCorpusReader`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`CategorizedChunkedCorpusReader` overrides all the `ChunkedCorpusReader` methods
    to take a `categories` argument for locating `fileids`. These `fileids` are found
    with the internal `_resolve()` function. This `_resolve()` function makes use
    of `CategorizedCorpusReader.fileids()` to return `fileids` for a given list of
    `categories`. If no `categories` are given, `_resolve()` just returns the given
    `fileids`, which could be `None`, in which case all files are read. The initialization
    of both `CategorizedCorpusReader` and `ChunkedCorpusReader` is what makes this
    all possible. If you look at the code for `CategorizedTaggedCorpusReader`, you''ll
    see it''s very similar. The inheritance diagram looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/3609OS_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here's some example code for using the `treebank` corpus. All we're doing is
    making categories out of the `fileids`, but the point is that you could use the
    same techniques to create your own categorized chunk corpus.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We use `nltk.data.find()` to search the data directories to get a `FileSystemPathPointer`
    to the `treebank` corpus. All the `treebank` tagged files start with `wsj_` followed
    by a number, and end with `.pos`. The previous code turns that file number into
    a category.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As covered in the *Creating a chunked phrase corpus* recipe, there's an alternative
    format and reader for a chunk corpus using IOB tags. To have a categorized corpus
    of IOB chunks, we have to make a new corpus reader.
  prefs: []
  type: TYPE_NORMAL
- en: Categorized Conll chunk corpus reader
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here's a subclass of `CategorizedCorpusReader` and `ConllChunkReader` called
    `CategorizedConllChunkCorpusReader`. It overrides all methods of `ConllCorpusReader`
    that take a `fileids` argument, so the methods can also take a `categories` argument.
    The `ConllChunkCorpusReader` is just a small subclass of `ConllCorpusReader` that
    handles initialization; most of the work is done in `ConllCorpusReader`. This
    code can also be found in `catchunked.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: All the following methods call the corresponding method of `ConllCorpusReader`
    with the value returned from `_resolve()`. We'll start with the plain text methods.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The `ConllCorpusReader` does not recognize paragraphs, so there are no `*_paras()`
    methods. Next are the tagged and chunked methods.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'For completeness, we must override the following methods of the `ConllCorpusReader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The inheritance diagram for this class is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Categorized Conll chunk corpus reader](img/3609OS_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Following is some example code using the `conll2000` corpus. Like with `treebank`,
    we''re using the `fileids` for categories. The `ConllChunkCorpusReader` requires
    a third argument to specify the `chunk_types`. These `chunk_types` are used to
    parse the IOB tags. As you learned in the *Creating a chunked phrase corpus* recipe,
    the `conll2000` corpus recognizes three chunk types:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NP` for noun phrases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VP` for verb phrases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PP` for prepositional phrases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the *Creating a chunked phrase corpus* recipe in this chapter, we covered
    both the `ChunkedCorpusReader` and `ConllChunkCorpusReader`. And in the previous
    recipe, we covered `CategorizedPlaintextCorpusReader` and `CategorizedTaggedCorpusReader`,
    which share the same superclass used by `CategorizedChunkedCorpusReader` and `CategorizedConllChunkReader`—`CategorizedCorpusReader`.
  prefs: []
  type: TYPE_NORMAL
- en: Lazy corpus loading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Loading a corpus reader can be an expensive operation due to the number of files,
    file sizes, and various initialization tasks. And while you'll often want to specify
    a corpus reader in a common module, you don't always need to access it right away.
    To speed up module import time when a corpus reader is defined, NLTK provides
    a `LazyCorpusLoader` class that can transform itself into your actual corpus reader
    as soon as you need it. This way, you can define a corpus reader in a common module
    without it slowing down module loading.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`LazyCorpusLoader` requires two arguments: the `name` of the corpus and the
    corpus reader class, plus any other arguments needed to initialize the corpus
    reader class.'
  prefs: []
  type: TYPE_NORMAL
- en: The `name` argument specifies the root directory name of the corpus, which must
    be within a `corpora` subdirectory of one of the paths in `nltk.data.path`. See
    the first recipe of this chapter, *Setting up a custom corpus*, for more details
    on `nltk.data.path`.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you have a custom corpora named `cookbook` in your local `nltk_data`
    directory, its path would be `~/nltk_data/corpora/cookbook`. You'd then pass `'cookbook'`
    to `LazyCorpusLoader` as the `name`, and `LazyCorpusLoader` will look in `~/nltk_data/corpora`
    for a directory named `'cookbook'`.
  prefs: []
  type: TYPE_NORMAL
- en: The second argument to `LazyCorpusLoader` is `reader_cls`, which should be the
    name of a subclass of `CorpusReader`, such as `WordListCorpusReader`. You will
    also need to pass in any other arguments required by the `reader_cls` for initialization.
    This will be demonstrated as follows, using the same `wordlist` file we created
    in the earlier recipe, *Creating a word list corpus*. The third argument to `LazyCorpusLoader`
    is the list of filenames and `fileids` that will be passed in to `WordListCorpusReader`
    at initialization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`LazyCorpusLoader` stores all the arguments given, but otherwise does nothing
    until you try to access an attribute or method. This way initialization is very
    fast, eliminating the overhead of loading the corpus reader immediately. As soon
    as you do access an attribute or method, it does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Calls `nltk.data.find('corpora/%s' % name)` to find the corpus data root directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instantiate the corpus reader class with the root directory and any other arguments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transforms itself into the corpus reader class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So in the previous example code, before we call `reader.fileids()`, `reader`
    is an instance of `LazyCorpusLoader`, but after the call, `reader` is an instance
    of `WordListCorpusReader`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All of the corpora included with NLTK and defined in `nltk.corpus` are initially
    an instance of `LazyCorpusLoader`. Here's some code from `nltk.corpus` defining
    the `treebank` corpora.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, any number of additional arguments can be passed through by
    `LazyCorpusLoader` to its `reader_cls`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom corpus view
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **corpus view** is a class wrapper around a corpus file that reads in blocks
    of tokens as needed. Its purpose is to provide a *view* into a file without reading
    the whole file at once (since corpus files can often be quite large). If the corpus
    readers included by NLTK already meet all your needs, then you do not have to
    know anything about corpus views. But, if you have a custom file format that needs
    special handling, this recipe will show you how to create and use a custom corpus
    view. The main corpus view class is `StreamBackedCorpusView`, which opens a single
    file as a *stream*, and maintains an internal cache of blocks it has read.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blocks of tokens are read in with a *block reader* function. A **block** can
    be any piece of text, such as a paragraph or a line, and **tokens** are parts
    of a block, such as individual words. In the *Creating a part-of-speech tagged
    word corpus* recipe, we discussed the default `para_block_reader` function of
    the `TaggedCorpusReader`, which reads lines from a file until it finds a blank
    line, then returns those lines as a single paragraph token. The actual block reader
    function is: `nltk.corpus.reader.util.read_blankline_block`. `TaggedCorpusReader`
    passes this block reader function into a `TaggedCorpusView` whenever it needs
    to read blocks from a file. `TaggedCorpusView` is a subclass of `StreamBackedCorpusView`
    that knows to split paragraphs of "word/tag" into `(word, tag)` tuples.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll start with the simple case of a plain text file with a heading that
    should be ignored by the corpus reader. Let''s make a file called `heading_text.txt`
    that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Normally we'd use the `PlaintextCorpusReader` but, by default, it will treat
    `A simple heading` as the first paragraph. To ignore this heading, we need to
    subclass the `PlaintextCorpusReader` so we can override its `CorpusView` class
    variable with our own `StreamBackedCorpusView` subclass. This code is found in
    `corpus.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: To demonstrate that this works as expected, here's the code showing that the
    default `PlaintextCorpusReader` finds four paragraphs, while our `IgnoreHeadingCorpusReader`
    only has three paragraphs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `PlaintextCorpusReader` by design has a `CorpusView` class variable that
    can be overridden by subclasses. So we do just that, and make our `IgnoreHeadingCorpusView`
    the `CorpusView`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most corpus readers do not have a `CorpusView` class variable because they require
    very specific corpus views.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `IgnoreHeadingCorpusView` is a subclass of `StreamBackedCorpusView` that
    does the following on initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the file using `self._open()`. This function is defined by `StreamBackedCorpusView`,
    and sets the internal instance variable `self._stream` to the opened file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read one block with `read_blankline_block()`, which will read the heading as
    a paragraph, and move the stream's file position forward to the next block.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reset the start file position to the current position of `self._stream`. `self._filepos`
    is an internal index of where each block is in the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s a diagram illustrating the relationships between the classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/3609OS_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Corpus views can get a lot fancier and more complicated, but the core concept
    is the same: read *blocks* from a `stream` to return a list of *tokens*. There
    are a number of block readers provided in `nltk.corpus.reader.util`, but you can
    always create your own. If you do want to define your own block reader function,
    then you have two choices on how to implement it:'
  prefs: []
  type: TYPE_NORMAL
- en: Define it as a separate function and pass it in to `StreamBackedCorpusView`
    as `block_reader`. This is a good option if your block reader is fairly simple,
    reusable, and doesn't require any outside variables or configuration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subclass `StreamBackedCorpusView` and override the `read_block()` method. This
    is what many custom corpus views do because the block reading is highly specialized
    and requires additional functions and configuration, usually provided by the corpus
    reader when the corpus view is initialized.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Block reader functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Following is a survey of most of the included block readers in `nltk.corpus.reader.util`.
    Unless otherwise noted, each block reader function takes a single argument: the
    `stream` to read from.'
  prefs: []
  type: TYPE_NORMAL
- en: '`read_whitespace_block()` will read 20 lines from the stream, splitting each
    line into tokens by whitespace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`read_wordpunct_block()` reads 20 lines from the stream, splitting each line
    using `nltk.tokenize.wordpunct_tokenize()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`read_line_block()` reads 20 lines from the stream and returns them as a list,
    with each line as a token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`read_blankline_block()` will read lines from the stream until it finds a blank
    line. It will then return a single token of all lines found combined into a single
    string.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`read_regexp_block()` takes two additional arguments, which must be regular
    expressions that can be passed to `re.match()`: a `start_re` and `end_re`. `start_re`
    matches the starting line of a block, and `end_re` matches the ending line of
    the block. `end_re` defaults to `None`, in which case the block will end as soon
    as a new `start_re` match is found. The return value is a single token of all
    lines in the block joined into a single string.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pickle corpus view
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to have a corpus of pickled objects, you can use the `PickleCorpusView`,
    a subclass of `StreamBackedCorpusView` found in `nltk.corpus.reader.util`. A file
    consists of blocks of pickled objects, and can be created with the `PickleCorpusView.write()`
    class method, which takes a sequence of objects and an output file, then pickles
    each object using `pickle.dump()` and writes it to the file. It overrides the
    `read_block()` method to return a list of unpickled objects from the stream, using
    `pickle.load()`.
  prefs: []
  type: TYPE_NORMAL
- en: Concatenated corpus view
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Also found in `nltk.corpus.reader.util` is the `ConcatenatedCorpusView`. This
    class is useful if you have multiple files that you want a corpus reader to treat
    as a single file. A `ConcatenatedCorpusView` is created by giving it a list of
    `corpus_views`, which are then iterated over as if they were a single view.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of block readers was introduced in the *Creating a part-of-speech
    tagged word corpus* recipe in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a MongoDB backed corpus reader
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the corpus readers we've dealt with so far have been file-based. That is
    in part due to the design of the `CorpusReader` base class, and also the assumption
    that most corpus data will be in text files. But sometimes you'll have a bunch
    of data stored in a database that you want to access and use just like a text
    file corpus. In this recipe, we'll cover the case where you have documents in
    MongoDB, and you want to use a particular field of each document as your block
    of text.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MongoDB is a document-oriented database that has become a popular alternative
    to relational databases such as MySQL. The installation and setup of MongoDB is
    outside the scope of this book, but you can find instructions at [http://www.mongodb.org/display/DOCS/Quickstart](http://www.mongodb.org/display/DOCS/Quickstart).
  prefs: []
  type: TYPE_NORMAL
- en: You'll also need to install PyMongo, a Python driver for MongoDB. You should
    be able to do this with either `easy_install` or `pip`, by doing `sudo easy_install
    pymongo` or `sudo pip install pymongo`.
  prefs: []
  type: TYPE_NORMAL
- en: The code in the *How to do it...* section assumes that your database is on `localhost`
    port `27017`, which is the MongoDB default configuration, and that you'll be using
    the `test` database with a collection named `corpus` that contains documents with
    a `text` field. Explanations for these arguments are available in the PyMongo
    documentation at [http://api.mongodb.org/python/](http://api.mongodb.org/python/).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the `CorpusReader` class assumes you have a file-based corpus, we can't
    directly subclass it. Instead, we're going to emulate both the `StreamBackedCorpusView`
    and `PlaintextCorpusReader`. `StreamBackedCorpusView` is a subclass of `nltk.util.AbstractLazySequence`,
    so we'll subclass `AbstractLazySequence` to create a MongoDB view, and then create
    a new class that will use the view to provide functionality similar to the `PlaintextCorpusReader`.
    This code is found in `mongoreader.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`AbstractLazySequence` is an abstract class that provides read-only, on-demand
    iteration. Subclasses must implement the `__len__()` and `iterate_from(start)`
    methods, while it provides the rest of the list and iterator emulation methods.
    By creating the `MongoDBLazySequence` subclass as our view, we can iterate over
    documents in the MongoDB collection on-demand, without keeping all the documents
    in memory. `LazyMap` is a lazy version of Python''s built-in `map()` function,
    and is used in `iterate_from()` to transform the document into the specific field
    that we''re interested in. It''s also a subclass of `AbstractLazySequence`.'
  prefs: []
  type: TYPE_NORMAL
- en: The `MongoDBCorpusReader` creates an internal instance of `MongoDBLazySequence`
    for iteration, then defines the word and sentence tokenization methods. The `text()`
    method simply returns the instance of `MongoDBLazySequence`, which results in
    a lazily evaluated list of each text field. The `words()` method uses `LazyMap`
    and `LazyConcatenation` to return a lazily evaluated list of all words, while
    the `sents()` method does the same for sentences. The `sent_tokenizer` is loaded
    on demand with `LazyLoader`, which is a wrapper around `nltk.data.load()`, analogous
    to `LazyCorpusLoader`. `LazyConcatentation` is a subclass of `AbstractLazySequence`
    too, and produces a flat list from a given list of lists (each list may also be
    lazy). In our case, we're concatenating the results of `LazyMap` to ensure we
    don't return nested lists.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All of the parameters are configurable. For example, if you had a `db` named
    `website`, with a `collection` named `comments`, whose documents had a `field`
    called `comment`, you could create a `MongoDBCorpusReader` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: You can also pass in custom instances for `word_tokenizer` and `sent_tokenizer`,
    as long as the objects implement the `nltk.tokenize.TokenizerI` interface by providing
    a `tokenize(text)` method.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Corpus views were covered in the previous recipe, and tokenization was covered
    in [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing
    Text and WordNet Basics*.
  prefs: []
  type: TYPE_NORMAL
- en: Corpus editing with file locking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Corpus readers and views are all read-only, but there may be times when you
    want to add to or edit the corpus files. However, modifying a corpus file while
    other processes are using it, such as through a corpus reader, can lead to dangerous
    undefined behavior. This is where file locking comes in handy.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You must install the `lockfile` library using `sudo easy_install lockfile` or
    `sudo pip install lockfile`. This library provides cross-platform file locking,
    and so will work on Windows, Unix/Linux, Mac OX, and more. You can find detailed
    documentation on `lockfile` at `http://packages.python.or` [g/lockfile/](http://g/lockfile/).
  prefs: []
  type: TYPE_NORMAL
- en: For the following code to work, you must also have Python 2.6\. Versions 2.4
    and earlier do not support the `with` keyword.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are two file editing functions: `append_line()` and `remove_line()`. Both
    try to acquire an *exclusive lock* on the file before updating it. An **exclusive
    lock** means that these functions will wait until no other process is reading
    from or writing to the file. Once the lock is acquired, any other process that
    tries to access the file will have to wait until the lock is released. This way,
    modifying the file will be safe and not cause any undefined behavior in other
    processes. These functions can be found in `corpus.py`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The lock acquiring and releasing happens transparently when you do `with lockfile.FileLock(fname)`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of using `with lockfile.FileLock(fname)`, you can also get a lock by
    calling `lock = lockfile.FileLock(fname)`, then call `lock.acquire()` to acquire
    the lock, and `lock.release()` to release the lock. This alternative usage is
    compatible with Python 2.4.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can use these functions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: In `append_line()`, a lock is acquired, the file is opened in *append mode*,
    the text is written along with an end-of-line character, and then the file is
    closed, releasing the lock.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A lock acquired by `lockfile` only protects the file from other processes that
    also use `lockfile`. In other words, just because your Python process has a lock
    with `lockfile`, doesn't mean a non-Python process can't modify the file. For
    this reason, it's best to only use `lockfile` with files that will not be edited
    by any non-Python processes, or Python processes that do not use `lockfile`.
  prefs: []
  type: TYPE_NORMAL
- en: '`The remove_line()` function is a bit more complicated. Because we''re removing
    a line and not a specific section of the file, we need to iterate over the file
    to find each instance of the line to remove. The easiest way to do this while
    writing the changes back to the file, is to use a `TemporaryFile` to hold the
    changes, then copy that file back into the original file using `shutil.copyfileobj()`.'
  prefs: []
  type: TYPE_NORMAL
- en: These functions are best suited for a word list corpus, or some other corpus
    type with presumably unique lines, that may be edited by multiple people at about
    the same time, such as through a web interface. Using these functions with a more
    document-oriented corpus such as `brown`, `treebank`, or `conll2000`, is probably
    a bad idea.
  prefs: []
  type: TYPE_NORMAL
