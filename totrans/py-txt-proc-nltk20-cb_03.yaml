- en: Chapter 3. Creating Custom Corpora
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章 创建自定义语料库
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍：
- en: Setting up a custom corpus
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置自定义语料库
- en: Creating a word list corpus
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建单词列表语料库
- en: Creating a part-of-speech tagged word corpus
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建词性标注的单词语料库
- en: Creating a chunked phrase corpus
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建分块短语语料库
- en: Creating a categorized text corpus
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建分类文本语料库
- en: Creating a categorized chunk corpus reader
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建分类分块语料库读取器
- en: Lazy corpus loading
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 懒加载语料库
- en: Creating a custom corpus view
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建自定义语料库视图
- en: Creating a MongoDB backed corpus reader
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建基于MongoDB的语料库读取器
- en: Corpus editing with file locking
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用文件锁定进行语料库编辑
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: In this chapter, we'll cover how to use corpus readers and create custom corpora.
    At the same time, you'll learn how to use the existing corpus data that comes
    with NLTK. This information is essential for future chapters when we'll need to
    access the corpora as training data. We'll also cover creating custom corpus readers,
    which can be used when your corpus is not in a file format that NLTK already recognizes,
    or if your corpus is not in files at all, but instead is located in a database
    such as MongoDB.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍如何使用语料库读取器创建自定义语料库。同时，你将学习如何使用NLTK附带的存在语料库数据。这些信息对于后续章节至关重要，届时我们需要将语料库作为训练数据来访问。我们还将介绍创建自定义语料库读取器，这可以在你的语料库不是NLTK已识别的文件格式时使用，或者如果你的语料库根本不在文件中，而是位于数据库（如MongoDB）中。
- en: Setting up a custom corpus
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置自定义语料库
- en: A **corpus** is a collection of text documents, and **corpora** is the plural
    of corpus. So a *custom corpus* is really just a bunch of text files in a directory,
    often alongside many other directories of text files.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**语料库**是一组文本文档的集合，**corpora**是语料库的复数形式。因此，*自定义语料库*实际上只是目录中的一些文本文件，通常与许多其他文本文件目录并列。'
- en: Getting ready
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You should already have the NLTK data package installed, following the instructions
    at [http://www.nltk.org/data](http://www.nltk.org/data). We'll assume that the
    data is installed to `C:\nltk_data` on Windows, and `/usr/share/nltk_data` on
    Linux, Unix, or Mac OS X.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该已经按照[http://www.nltk.org/data](http://www.nltk.org/data)上的说明安装了NLTK数据包。我们将假设数据安装在了Windows上的`C:\nltk_data`，Linux、Unix或Mac
    OS X上的`/usr/share/nltk_data`。
- en: How to do it...
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'NLTK defines a list of data directories, or **paths**, in `nltk.data.path`.
    Our custom corpora must be within one of these paths so it can be found by NLTK.
    So as not to conflict with the official data package, we''ll create a custom `nltk_data`
    directory in our home directory. Here''s some Python code to create this directory
    and verify that it is in the list of known paths specified by `nltk.data.path`:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK定义了一个数据目录列表，或**路径**，在`nltk.data.path`中。我们的自定义语料库必须位于这些路径之一，以便NLTK可以找到它。为了避免与官方数据包冲突，我们将在主目录中创建一个自定义的`nltk_data`目录。以下是一些Python代码，用于创建此目录并验证它是否在由`nltk.data.path`指定的已知路径列表中：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If the last line, `path in nltk.data.path`, is `True`, then you should now have
    a `nltk_data` directory in your home directory. The path should be `%UserProfile%\nltk_data`
    on Windows, or `~/nltk_data` on Unix, Linux, or Mac OS X. For simplicity, I'll
    refer to the directory as `~/nltk_data`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果最后一行`path in nltk.data.path`是`True`，那么你现在应该在主目录中有一个`nltk_data`目录。在Windows上，路径应该是`%UserProfile%\nltk_data`，在Unix、Linux或Mac
    OS X上，路径应该是`~/nltk_data`。为了简化，我将把这个目录称为`~/nltk_data`。
- en: Note
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If the last line does not return `True`, try creating the `nltk_data` directory
    manually in your home directory, then verify that the absolute path is in `nltk.data.path`.
    It's essential to ensure that this directory exists and is in `nltk.data.path`
    before continuing. Once you have your `nltk_data` directory, the convention is
    that corpora reside in a `corpora` subdirectory. Create this `corpora` directory
    within the `nltk_data` directory, so that the path is `~/nltk_data/corpora`. Finally,
    we'll create a subdirectory in `corpora` to hold our custom corpus. Let's call
    it `cookbook`, giving us the full path of `~/nltk_data/corpora/cookbook`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果最后一行没有返回`True`，请尝试在你的主目录中手动创建`nltk_data`目录，然后验证绝对路径是否在`nltk.data.path`中。在继续之前，确保此目录存在并且位于`nltk.data.path`中是至关重要的。一旦你有了`nltk_data`目录，惯例是语料库位于一个`corpora`子目录中。在`nltk_data`目录中创建此`corpora`目录，以便路径为`~/nltk_data/corpora`。最后，我们将在`corpora`中创建一个子目录来存放我们的自定义语料库。让我们称它为`cookbook`，完整的路径为`~/nltk_data/corpora/cookbook`。
- en: Now we can create a simple *word list* file and make sure it loads. In [Chapter
    2](ch02.html "Chapter 2. Replacing and Correcting Words"), *Replacing and Correcting
    Words*, *Spelling correction with Enchant* recipe, we created a word list file
    called `mywords.txt`. Put this file into `~/nltk_data/corpora/cookbook/`. Now
    we can use `nltk.data.load()` to load the file.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建一个简单的 *单词列表* 文件并确保它被加载。在 [第2章](ch02.html "第2章。替换和纠正单词")，*替换和纠正单词*，*使用
    Enchant 进行拼写纠正* 菜谱中，我们创建了一个名为 `mywords.txt` 的单词列表文件。将此文件放入 `~/nltk_data/corpora/cookbook/`。现在我们可以使用
    `nltk.data.load()` 来加载该文件。
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We need to specify `format='raw'` since `nltk.data.load()` doesn't know how
    to interpret `.txt` files. As we'll see, it does know how to interpret a number
    of other file formats.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要指定 `format='raw'`，因为 `nltk.data.load()` 不知道如何解释 `.txt` 文件。正如我们将看到的，它确实知道如何解释许多其他文件格式。
- en: How it works...
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `nltk.data.load()` function recognizes a number of formats, such as `'raw'`,
    `'pickle'`, and `'yaml'`. If no format is specified, then it tries to guess the
    format based on the file's extension. In the previous case, we have a `.txt` file,
    which is not a recognized extension, so we have to specify the `'raw'` format.
    But if we used a file that ended in `.yaml`, then we would not need to specify
    the format.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`nltk.data.load()` 函数识别多种格式，例如 `''raw''`、`''pickle''` 和 `''yaml''`。如果没有指定格式，它将尝试根据文件的扩展名猜测格式。在前一个例子中，我们有一个
    `.txt` 文件，这不是一个已识别的扩展名，因此我们必须指定 `''raw''` 格式。但如果我们使用以 `.yaml` 结尾的文件，则不需要指定格式。'
- en: Filenames passed in to `nltk.data.load()` can be *absolute* or *relative* paths.
    Relative paths must be relative to one of the paths specified in `nltk.data.path`.
    The file is found using `nltk.data.find(path)`, which searches all known paths
    combined with the relative path. Absolute paths do not require a search, and are
    used as is.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给 `nltk.data.load()` 的文件名可以是 *绝对* 路径或 *相对* 路径。相对路径必须是 `nltk.data.path` 中指定的路径之一。文件是通过
    `nltk.data.find(path)` 找到的，它搜索所有已知路径与相对路径的组合。绝对路径不需要搜索，可以直接使用。
- en: There's more...
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: For most corpora access, you won't actually need to use `nltk.data.load`, as
    that will be handled by the `CorpusReader` classes covered in the following recipes.
    But it's a good function to be familiar with for loading `.pickle` files and `.yaml`
    files, plus it introduces the idea of putting all of your data files into a path
    known by NLTK.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数语料库访问，实际上你不需要使用 `nltk.data.load`，因为这将由以下菜谱中介绍的 `CorpusReader` 类处理。但了解这个函数对于加载
    `.pickle` 文件和 `.yaml` 文件是很有帮助的，同时它也引入了将所有数据文件放入NLTK已知路径的概念。
- en: Loading a YAML file
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载 YAML 文件
- en: If you put the `synonyms.yaml` file from the [Chapter 2](ch02.html "Chapter 2. Replacing
    and Correcting Words"), *Replacing and Correcting Words*, *Replacing synonyms*
    recipe, into `~/nltk_data/corpora/cookbook` (next to `mywords.txt`), you can use
    `nltk.data.load()` to load it without specifying a format.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将 [第2章](ch02.html "第2章。替换和纠正单词")，*替换和纠正单词*，*替换同义词* 菜谱中的 `synonyms.yaml` 文件放入
    `~/nltk_data/corpora/cookbook`（在 `mywords.txt` 旁边），你可以使用 `nltk.data.load()` 来加载它，无需指定格式。
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This assumes that PyYAML is installed. If not, you can find download and installation
    instructions at [http://pyyaml.org/wiki/PyYAML](http://pyyaml.org/wiki/PyYAML).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这假设PyYAML已经安装。如果没有，你可以在[http://pyyaml.org/wiki/PyYAML](http://pyyaml.org/wiki/PyYAML)找到下载和安装说明。
- en: See also
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关内容
- en: In the next recipes, we'll cover various corpus readers, and then in the *Lazy
    corpus loading* recipe, we'll use the `LazyCorpusLoader`, which expects corpus
    data to be in a `corpora` subdirectory of one of the paths specified by `nltk.data.path`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的菜谱中，我们将介绍各种语料库读取器，然后在 *Lazy corpus loading* 菜谱中，我们将使用 `LazyCorpusLoader`，它期望语料库数据位于
    `nltk.data.path` 指定路径之一的 `corpora` 子目录中。
- en: Creating a word list corpus
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建单词列表语料库
- en: The `WordListCorpusReader` is one of the simplest `CorpusReader` classes. It
    provides access to a file containing a list of words, one word per line. In fact,
    you've already used it when we used the `stopwords` corpus in the *Filtering stopwords
    in a tokenized sentence* and *Discovering word collocations* recipes in [Chapter
    1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing Text
    and WordNet Basics*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`WordListCorpusReader` 是最简单的 `CorpusReader` 类之一。它提供对包含单词列表的文件的访问，每行一个单词。实际上，当我们在
    [第1章](ch01.html "第1章。文本分词和 WordNet 基础")，*文本分词和 WordNet 基础*，*在分词句子中过滤停用词* 和 *发现词搭配*
    菜谱中使用 `stopwords` 语料库时，你已经使用过它了。'
- en: Getting ready
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We need to start by creating a word list file. This could be a single column
    CSV file, or just a normal text file with one word per line. Let''s create a file
    named `wordlist` that looks like this:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要首先创建一个单词列表文件。这可以是一个单列CSV文件，或者只是一个每行一个单词的普通文本文件。让我们创建一个名为`wordlist`的文件，如下所示：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: How to do it...
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Now we can instantiate a `WordListCorpusReader` that will produce a list of
    words from our file. It takes two arguments: the directory path containing the
    files, and a list of filenames. If you open the Python console in the same directory
    as the files, then `''.''` can be used as the directory path. Otherwise, you must
    use a directory path such as: `''nltk_data/corpora/cookbook''`.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以实例化一个`WordListCorpusReader`，它将从我们的文件中生成单词列表。它需要两个参数：包含文件的目录路径和文件名列表。如果你在包含文件的同一目录中打开Python控制台，那么`'.'`可以用作目录路径。否则，你必须使用一个目录路径，例如：`'nltk_data/corpora/cookbook'`。
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How it works...
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: '`WordListCorpusReader` inherits from `CorpusReader`, which is a common base
    class for all corpus readers. `CorpusReader` does all the work of identifying
    which files to read, while `WordListCorpus` reads the files and tokenizes each
    line to produce a list of words. Here''s an inheritance diagram:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`WordListCorpusReader`类继承自`CorpusReader`，这是所有语料库读取器的公共基类。`CorpusReader`负责确定要读取哪些文件，而`WordListCorpus`读取文件并将每一行分词以生成单词列表。下面是一个继承关系图：'
- en: '![How it works...](img/3609OS_03_01.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/3609OS_03_01.jpg)'
- en: When you call the `words()` function, it calls `nltk.tokenize.line_tokenize()`
    on the raw file data, which you can access using the `raw()` function.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当你调用`words()`函数时，它会使用`nltk.tokenize.line_tokenize()`对原始文件数据进行分词，你可以通过`raw()`函数访问这些数据。
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: There's more...
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The `stopwords` corpus is a good example of a multi-file `WordListCorpusReader`.
    In [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing
    Text and WordNet Basics*, in the *Filtering stopwords in a tokenized sentence*
    recipe, we saw that it had one word list file for each language, and you could
    access the words for that language by calling `stopwords.words(fileid)`. If you
    want to create your own multi-file word list corpus, this is a great example to
    follow.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`stopwords`语料库是多文件`WordListCorpusReader`的一个很好的例子。在第1章[第1章.文本分词和WordNet基础知识](ch01.html
    "第1章.文本分词和WordNet基础知识")中，*在分词句子中过滤停用词*的配方中，我们看到了它为每种语言有一个单词列表文件，并且你可以通过调用`stopwords.words(fileid)`来访问该语言的单词。如果你想创建自己的多文件单词列表语料库，这是一个很好的例子。'
- en: Names corpus
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 名字语料库
- en: 'Another word list corpus that comes with NLTK is the `names` corpus. It contains
    two files: `female.txt` and `male.txt`, each containing a list of a few thousand
    common first names organized by gender.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK附带的其他单词列表语料库是`names`语料库。它包含两个文件：`female.txt`和`male.txt`，每个文件都包含按性别组织的几千个常见名字列表。
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: English words
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 英语单词
- en: NLTK also comes with a large list of English words. There's one file with 850
    `basic` words, and another list with over 200,000 known English words.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK还附带了一个大量的英语单词列表。有一个包含850个**基本**单词的文件，还有一个包含超过20万个已知英语单词的列表。
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: See also
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考信息
- en: In [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing
    Text and WordNet Basics*, the *Filtering stopwords in a tokenized sentence* recipe,
    has more details on using the `stopwords` corpus. In the following recipes, we'll
    cover more advanced corpus file formats and corpus reader classes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1章[第1章.文本分词和WordNet基础知识](ch01.html "第1章.文本分词和WordNet基础知识")中，*在分词句子中过滤停用词*的配方中，对使用`stopwords`语料库有更多细节。在接下来的配方中，我们将介绍更高级的语料库文件格式和语料库读取器类。
- en: Creating a part-of-speech tagged word corpus
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建词性标注的语料库
- en: '**Part-of-speech tagging** is the process of identifying the part-of-speech
    tag for a word. Most of the time, a *tagger* must first be trained on a *training
    corpus*. How to train and use a tagger is covered in detail in [Chapter 4](ch04.html
    "Chapter 4. Part-of-Speech Tagging"), *Part-of-Speech Tagging*, but first we must
    know how to create and use a training corpus of part-of-speech tagged words.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**词性标注**是识别单词的词性标签的过程。大多数情况下，一个**标记器**必须首先在一个**训练语料库**上训练。如何训练和使用标记器将在第4章[第4章.词性标注](ch04.html
    "第4章.词性标注")中详细说明，但首先我们必须知道如何创建和使用词性标注单词的训练语料库。'
- en: Getting ready
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The simplest format for a tagged corpus is of the form "word/tag". Following
    is an excerpt from the `brown` corpus:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 标注语料库的最简单格式是“word/tag”的形式。以下是从`brown`语料库中摘录的内容：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Each word has a *tag* denoting its part-of-speech. For example, `nn` refers
    to a noun, while a tag that starts with `vb` is a verb.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词都有一个表示其词性的**标记**。例如，`nn` 指名词，而以 `vb` 开头的标记是动词。
- en: How to do it...
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'If you were to put the previous excerpt into a file called `brown.pos`, you
    could then create a `TaggedCorpusReader` and do the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将前面的摘录放入一个名为 `brown.pos` 的文件中，然后你可以创建一个 `TaggedCorpusReader` 并执行以下操作：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: How it works...
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This time, instead of naming the file explicitly, we use a regular expression,
    `r'.*\.pos'`, to match all files whose name ends with `.pos`. We could have done
    the same thing as we did with the `WordListCorpusReader`, and pass `['brown.pos']`
    as the second argument, but this way you can see how to include multiple files
    in a corpus without naming each one explicitly.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们不是明确命名文件，而是使用正则表达式 `r'.*\.pos'` 来匹配所有以 `.pos` 结尾的文件。我们本来可以像对 `WordListCorpusReader`
    做的那样，将 `['brown.pos']` 作为第二个参数传递，但这样你可以看到如何在语料库中包含多个文件而不需要明确命名每个文件。
- en: '`TaggedCorpusReader` provides a number of methods for extracting text from
    a corpus. First, you can get a list of all words, or a list of tagged tokens.
    A **tagged token** is simply a tuple of `(word, tag)`. Next, you can get a list
    of every sentence, and also every tagged sentence, where the sentence is itself
    a list of words or tagged tokens. Finally, you can get a list of paragraphs, where
    each paragraph is a list of sentences, and each sentence is a list of words or
    tagged tokens. Here''s an inheritance diagram listing all the major methods:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`TaggedCorpusReader` 提供了多种从语料库中提取文本的方法。首先，你可以获取所有单词的列表，或者标记化标记的列表。一个**标记化标记**就是一个
    `(word, tag)` 的元组。接下来，你可以获取每个句子的列表，以及每个标记化句子的列表，其中句子本身是一个单词或标记化标记的列表。最后，你可以获取段落列表，其中每个段落是一个句子列表，每个句子是一个单词或标记化标记的列表。以下是一个列出所有主要方法的继承图：'
- en: '![How it works...](img/3609OS_03_02.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/3609OS_03_02.jpg)'
- en: There's more...
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The functions demonstrated in the previous diagram all depend on *tokenizers*
    for splitting the text. `TaggedCorpusReader` tries to have good defaults, but
    you can customize them by passing in your own tokenizers at initialization time.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 之前图中展示的函数都依赖于 *tokenizers* 来分割文本。`TaggedCorpusReader` 尝试使用良好的默认值，但你可以在初始化时传递自己的标记化器来自定义它们。
- en: Customizing the word tokenizer
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义词标记器
- en: The default word tokenizer is an instance of `nltk.tokenize.WhitespaceTokenizer`.
    If you want to use a different tokenizer, you can pass that in as `word_tokenizer`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 默认单词标记器是 `nltk.tokenize.WhitespaceTokenizer` 的一个实例。如果你想使用不同的标记器，你可以将其作为 `word_tokenizer`
    传递。
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Customizing the sentence tokenizer
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义句子标记器
- en: The default sentence tokenizer is an instance of `nltk.tokenize.RegexpTokenize`
    with `'\n'` to identify the gaps. It assumes that each sentence is on a line all
    by itself, and individual sentences do not have line breaks. To customize this,
    you can pass in your own tokenizer as `sent_tokenizer`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 默认句子标记器是 `nltk.tokenize.RegexpTokenize` 的一个实例，使用 `'\n'` 来识别间隔。它假设每个句子都单独在一行上，并且单个句子没有换行符。要自定义这一点，你可以传递自己的标记器作为
    `sent_tokenizer`。
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Customizing the paragraph block reader
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义段落块读取器
- en: Paragraphs are assumed to be split by blank lines. This is done with the default
    `para_block_reader`, which is `nltk.corpus.reader.util.read_blankline_block`.
    There are a number of other block reader functions in `nltk.corpus.reader.util`,
    whose purpose is to read blocks of text from a *stream*. Their usage will be covered
    in more detail in the later recipe, *Creating a custom corpus view*, where we'll
    create a custom corpus reader.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 假设段落是通过空白行分隔的。这是通过默认的 `para_block_reader` 实现的，即 `nltk.corpus.reader.util.read_blankline_block`。`nltk.corpus.reader.util`
    中有其他许多块读取函数，其目的是从 *stream* 中读取文本块。它们的用法将在后面的食谱中更详细地介绍，即 *创建自定义语料库视图*，我们将创建一个自定义语料库读取器。
- en: Customizing the tag separator
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义标记分隔符
- en: If you don't want to use `'/'` as the word/tag separator, you can pass an alternative
    string to `TaggedCorpusReader` for `sep`. The default is `sep='/'`, but if you
    want to split words and tags with `'|'`, such as 'word|tag', then you should pass
    in `sep='|'`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想使用 `'/'` 作为单词/标记分隔符，你可以传递一个替代字符串给 `TaggedCorpusReader` 的 `sep` 参数。默认是 `sep='/'`，但如果你想用
    `'|'` 分隔单词和标记，例如 'word|tag'，那么你应该传递 `sep='|'`。
- en: Simplifying tags with a tag mapping function
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用标记映射函数简化标记
- en: 'If you''d like to somehow transform the part-of-speech tags, you can pass in
    a `tag_mapping_function` at initialization, then call one of the `tagged_*` functions
    with `simplify_tags=True`. Here''s an example where we lowercase each tag:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要以某种方式转换词性标签，你可以在初始化时传入一个 `tag_mapping_function`，然后使用 `simplify_tags=True`
    调用一个 `tagged_*` 函数。以下是一个将每个标签转换为小写的示例：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Calling `tagged_words()` without `simplify_tags=True` would produce the same
    result as if you did not pass in a `tag_mapping_function`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 不带 `simplify_tags=True` 调用 `tagged_words()` 将产生与未传入 `tag_mapping_function` 相同的结果。
- en: There are also a number of tag simplification functions defined in `nltk.tag.simplify`.
    These can be useful for reducing the number of different part-of-speech tags.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `nltk.tag.simplify` 中定义了多个标签简化函数。这些函数可以用于减少不同词性标签的数量。
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: See also
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关内容
- en: '[Chapter 4](ch04.html "Chapter 4. Part-of-Speech Tagging"), *Part-of-Speech
    Tagging* will cover part-of-speech tags and tagging in much more detail. And for
    more on tokenizers, see the first three recipes of [Chapter 1](ch01.html "Chapter 1. Tokenizing
    Text and WordNet Basics"), *Tokenizing Text and WordNet Basics*.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[第4章](ch04.html "第4章。词性标注")，*词性标注* 将更详细地介绍词性标注和标注。有关分词器的更多信息，请参阅 [第1章](ch01.html
    "第1章。文本分词和WordNet基础知识") 的前三个食谱，*文本分词和WordNet基础知识*。'
- en: In the next recipe, we'll create a *chunked phrase* corpus, where each phrase
    is also part-of-speech tagged.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个食谱中，我们将创建一个 *语块短语* 语料库，其中每个短语也被标注了词性。
- en: Creating a chunked phrase corpus
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建语块短语语料库
- en: 'A **chunk** is a short phrase within a sentence. If you remember sentence diagrams
    from grade school, they were a tree-like representation of phrases within a sentence.
    This is exactly what chunks are: *sub-trees within a sentence tree*, and they
    will be covered in much more detail in [Chapter 5](ch05.html "Chapter 5. Extracting
    Chunks"), *Extracting Chunks*. Following is a sample sentence tree with three
    noun phrase (**NP**) chunks shown as sub-trees.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**语块** 是句子中的一个短短语。如果你还记得小学时的句子图，它们是句子中短语的树形表示。这正是语块：*句子树中的子树*，它们将在 [第5章](ch05.html
    "第5章。提取语块")，*提取语块* 中更详细地介绍。以下是一个包含三个名词短语（**NP**）语块作为子树的示例句子树形图。'
- en: '![Creating a chunked phrase corpus](img/3609OS_03_03.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![创建语块短语语料库](img/3609OS_03_03.jpg)'
- en: This recipe will cover how to create a corpus with sentences that contain chunks.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱将介绍如何创建包含语块的句子语料库。
- en: Getting ready
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Here is an excerpt from the tagged `treebank` corpus. It has part-of-speech
    tags, as in the previous recipe, but it also has square brackets for denoting
    chunks. This is the same sentence as in the previous tree diagram, but in text
    form:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是标注过的 `treebank` 语料库的摘录。它有词性标注，就像之前的食谱一样，但它还有方括号来表示语块。这与之前的树形图中的句子相同，但以文本形式呈现：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this format, every chunk is a *noun phrase*. Words that are not within brackets
    are part of the sentence tree, but are not part of any noun phrase sub-tree.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种格式中，每个语块都是一个 *名词短语*。不在括号内的单词是句子树的一部分，但不属于任何名词短语子树。
- en: How to do it...
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Put this excerpt into a file called `treebank.chunk`, and then do the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 将此摘录放入一个名为 `treebank.chunk` 的文件中，然后执行以下操作：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `ChunkedCorpusReader` provides the same methods as the `TaggedCorpusReader`
    for getting tagged tokens, along with three new methods for getting chunks. Each
    chunk is represented as an instance of `nltk.tree.Tree`. Sentence level trees
    look like `Tree(''S'', [...])` while noun phrase trees look like `Tree(''NP'',
    [...])`. In `chunked_sents()`, you get a list of sentence trees, with each noun-phrase
    as a sub-tree of the sentence. In `chunked_words()`, you get a list of noun phrase
    trees alongside tagged tokens of words that were not in a chunk. Here''s an inheritance
    diagram listing the major methods:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`ChunkedCorpusReader` 提供了与 `TaggedCorpusReader` 相同的方法来获取标注的标记，同时提供了三个新方法来获取语块。每个语块都表示为
    `nltk.tree.Tree` 的一个实例。句子级树形看起来像 `Tree(''S'', [...])`，而名词短语树形看起来像 `Tree(''NP'',
    [...])`。在 `chunked_sents()` 中，你得到一个句子树的列表，其中每个名词短语作为句子的子树。在 `chunked_words()`
    中，你得到一个名词短语树的列表，以及不在语块中的单词的标注标记。以下是一个列出主要方法的继承图：'
- en: '![How to do it...](img/3609OS_03_04.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/3609OS_03_04.jpg)'
- en: Note
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: You can draw a `Tree` by calling the `draw()` method. Using the corpus reader
    defined earlier, you could do `reader.chunked_sents()[0].draw()` to get the same
    sentence tree diagram shown at the beginning of this recipe.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过调用 `draw()` 方法来绘制一个 `Tree`。使用前面定义的语料库读取器，你可以执行 `reader.chunked_sents()[0].draw()`
    来获取与该食谱开头所示相同的句子树形图。
- en: How it works...
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: '`ChunkedCorpusReader` is similar to the `TaggedCorpusReader` from the last
    recipe. It has the same default `sent_tokenizer` and `para_block_reader`, but
    instead of a `word_tokenizer`, it uses a `str2chunktree()` function. The default
    is `nltk.chunk.util.tagstr2tree()`, which parses a sentence string containing
    bracketed chunks into a sentence tree, with each chunk as a noun phrase sub-tree.
    Words are split by whitespace, and the default word/tag separator is `''/''`.
    If you want to customize the chunk parsing, then you can pass in your own function
    for `str2chunktree()`.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`ChunkedCorpusReader` 与上一道菜谱中的 `TaggedCorpusReader` 类似。它具有相同的默认 `sent_tokenizer`
    和 `para_block_reader`，但使用 `str2chunktree()` 函数代替 `word_tokenizer`。默认为 `nltk.chunk.util.tagstr2tree()`，它将包含括号内短语的句子字符串解析为句子树，每个短语作为一个名词短语子树。单词通过空格分隔，默认的单词/标签分隔符是
    `''/''`。如果您想自定义短语解析，则可以为 `str2chunktree()` 传递自己的函数。'
- en: There's more...
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: An alternative format for denoting chunks is called IOB tags. **IOB** tags are
    similar to part-of-speech tags, but provide a way to denote the inside, outside,
    and beginning of a chunk. They also have the benefit of allowing multiple different
    chunk phrase types, not just noun phrases. Here is an excerpt from the `conll2000`
    corpus. Each word is on its own line with a part-of-speech tag followed by an
    IOB tag.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表示短语的另一种格式称为 IOB 标签。**IOB** 标签与词性标签类似，但提供了一种表示短语内部、外部和开始的方法。它们还有允许表示多种不同的短语类型（而不仅仅是名词短语）的优点。以下是
    `conll2000` 语料库的一个摘录。每个单词都在单独的一行上，后面跟着一个词性标签和一个 IOB 标签。
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`B-NP` denotes the beginning of a noun phrase, while `I-NP` denotes that the
    word is inside of the current noun phrase. `B-VP` and `I-VP` denote the beginning
    and inside of a verb phrase. `O` ends the sentence.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`B-NP` 表示名词短语的开始，而 `I-NP` 表示该词位于当前名词短语内部。`B-VP` 和 `I-VP` 表示动词短语的开始和内部。`O` 表示句子的结束。'
- en: To read a corpus using the IOB format, you must use the `ConllChunkCorpusReader`.
    Each sentence is separated by a blank line, but there is no separation for paragraphs.
    This means that the `para_*` methods are not available. If you put the previous
    IOB example text into a file named `conll.iob`, you can create and use a `ConllChunkCorpusReader`
    with the code we are about to see. The third argument to `ConllChunkCorpusReader`
    should be a tuple or list specifying the types of chunks in the file, which in
    this case is `('NP', 'VP', 'PP')`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 IOB 格式读取语料库，您必须使用 `ConllChunkCorpusReader`。每个句子由一个空行分隔，但段落之间没有分隔。这意味着 `para_*`
    方法不可用。如果您将之前的 IOB 示例文本放入名为 `conll.iob` 的文件中，您可以使用我们即将看到的代码创建并使用 `ConllChunkCorpusReader`。`ConllChunkCorpusReader`
    的第三个参数应该是一个元组或列表，指定文件中的短语类型，在这种情况下是 `('NP', 'VP', 'PP')`。
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The previous code also shows the `iob_words()` and `iob_sents()` methods, which
    return lists of three tuples of `(word, pos, iob)`. The inheritance diagram for
    `ConllChunkCorpusReader` looks like the following, with most of the methods implemented
    by its superclass, `ConllCorpusReader`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码还展示了 `iob_words()` 和 `iob_sents()` 方法，它们返回包含三个元组的列表 `(word, pos, iob)`。`ConllChunkCorpusReader`
    的继承图如下，其中大多数方法由其超类 `ConllCorpusReader` 实现：
- en: '![There''s more...](img/3609OS_03_05.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![更多内容...](img/3609OS_03_05.jpg)'
- en: Tree leaves
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 树的叶子
- en: When it comes to chunk trees, the leaves of a tree are the tagged tokens. So
    if you want to get a list of all the tagged tokens in a tree, call the `leaves()`
    method.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到短语树时，树的叶子是标记过的标记。因此，如果您想获取树中所有标记过的标记的列表，请调用 `leaves()` 方法。
- en: '[PRE18]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Treebank chunk corpus
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 树库短语语料库
- en: The `nltk.corpus.treebank_chunk` corpus uses `ChunkedCorpusReader` to provide
    part-of-speech tagged words and noun phrase chunks of Wall Street Journal headlines.
    NLTK comes with a 5% sample from the Penn Treebank Project. You can find out more
    at [http://www.cis.upenn.edu/~treebank/home.html](http://www.cis.upenn.edu/~treebank/home.html).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`nltk.corpus.treebank_chunk` 语料库使用 `ChunkedCorpusReader` 来提供华尔街日报标题的词性标注单词和名词短语短语。NLTK
    包含了宾夕法尼亚树库项目的 5% 样本。您可以在 [http://www.cis.upenn.edu/~treebank/home.html](http://www.cis.upenn.edu/~treebank/home.html)
    获取更多信息。'
- en: CoNLL2000 corpus
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CoNLL2000 语料库
- en: '**CoNLL** stands for the **Conference on Computational Natural Language Learning**.
    For the year 2000 conference, a shared task was undertaken to produce a corpus
    of chunks based on the Wall Street Journal corpus. In addition to noun phrases
    (`NP`), it also contains verb phrases (`VP`) and prepositional phrases (`PP`).
    This chunked corpus is available as `nltk.corpus.conll2000`, which is an instance
    of `ConllChunkCorpusReader`. You can read more at [http://www.cnts.ua.ac.be/conll2000/chunking/](http://www.cnts.ua.ac.be/conll2000/chunking/).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**CoNLL**代表**计算自然语言学习会议**。对于2000年的会议，一个共享任务被承担，基于《华尔街日报》语料库生成一个基于块的语料库。除了名词短语（`NP`）之外，它还包含动词短语（`VP`）和介词短语（`PP`）。这个块语料库作为`nltk.corpus.conll2000`提供，它是`ConllChunkCorpusReader`的一个实例。你可以了解更多信息[在这里](http://www.cnts.ua.ac.be/conll2000/chunking/)。'
- en: See also
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关内容
- en: '[Chapter 5](ch05.html "Chapter 5. Extracting Chunks"), *Extracting Chunks*
    will cover chunk extraction in detail. Also see the previous recipe for details
    on getting tagged tokens from a corpus reader.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[第五章](ch05.html "第五章。提取块")，*提取块*将详细介绍块提取。也可以查看之前的配方，了解从语料库读取器获取标记化标记的详细信息。'
- en: Creating a categorized text corpus
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建分类文本语料库
- en: If you have a large corpus of text, you may want to categorize it into separate
    sections. The brown corpus, for example, has a number of different categories.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个大量的文本语料库，你可能想要将其分类到不同的部分。例如，布朗语料库就有许多不同的类别。
- en: '[PRE19]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In this recipe, we'll learn how to create our own categorized text corpus.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将学习如何创建自己的分类文本语料库。
- en: Getting ready
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The easiest way to categorize a corpus is to have one file for each category.
    Following are two excerpts from the `movie_reviews` corpus:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 将语料库分类的最简单方法是每个类别一个文件。以下是`movie_reviews`语料库的两个摘录：
- en: '`movie_pos.txt`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`movie_pos.txt`'
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`movie_neg.txt`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`movie_neg.txt`'
- en: '[PRE21]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'With these two files, we''ll have two categories: `pos` and `neg`.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这两个文件，我们将有两个类别：`pos`和`neg`。
- en: How to do it...
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'We''ll use the `CategorizedPlaintextCorpusReader`, which inherits from both
    `PlaintextCorpusReader` and `CategorizedCorpusReader`. These two superclasses
    require three arguments: the root directory, the `fileids`, and a category specification.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用继承自`PlaintextCorpusReader`和`CategorizedCorpusReader`的`CategorizedPlaintextCorpusReader`。这两个超类需要三个参数：根目录、`fileids`和类别指定。
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: How it works...
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The first two arguments to `CategorizedPlaintextCorpusReader` are the root directory
    and `fileids`, which are passed on to the `PlaintextCorpusReader` to read in the
    files. The `cat_pattern` keyword argument is a regular expression for extracting
    the category names from the `fileids`. In our case, the category is the part of
    the `fileid` after `movie_` and before `.txt`. **The category must be surrounded
    by grouping parenthesis**.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`CategorizedPlaintextCorpusReader`的前两个参数是根目录和`fileids`，它们被传递给`PlaintextCorpusReader`以读取文件。`cat_pattern`关键字参数是从`fileids`中提取类别名称的正则表达式。在我们的例子中，类别是`fileid`中`movie_`之后和`.txt`之前的部分。**类别必须被分组括号包围**。'
- en: '`cat_pattern` is passed to `CategorizedCorpusReader`, which overrides the common
    corpus reader functions such as `fileids()`, `words()`, `sents()`, and `paras()`
    to accept a `categories` keyword argument. This way, you could get all the `pos`
    sentences by calling `reader.sents(categories=[''pos''])`. `CategorizedCorpusReader`
    also provides the `categories()` function, which returns a list of all known categories
    in the corpus.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`cat_pattern`被传递给`CategorizedCorpusReader`，它覆盖了常见的语料库读取器函数，如`fileids()`、`words()`、`sents()`和`paras()`，以接受一个`categories`关键字参数。这样，你可以通过调用`reader.sents(categories=[''pos''])`来获取所有`pos`句子。`CategorizedCorpusReader`还提供了一个`categories()`函数，它返回语料库中所有已知类别的列表。'
- en: '`CategorizedPlaintextCorpusReader` is an example of using multiple-inheritance
    to join methods from multiple superclasses, as shown in the following diagram:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`CategorizedPlaintextCorpusReader`是使用多继承将多个超类的方法结合起来的一个例子，如下面的图所示：'
- en: '![How it works...](img/3609OS_03_06.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/3609OS_03_06.jpg)'
- en: There's more...
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: Instead of `cat_pattern`, you could pass in a `cat_map`, which is a dictionary
    mapping a `fileid` to a list of category labels.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`cat_pattern`，你也可以传递一个`cat_map`，它是一个将`fileid`映射到类别标签列表的字典。
- en: '[PRE23]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Category file
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 类别文件
- en: 'A third way of specifying categories is to use the `cat_file` keyword argument
    to specify a filename containing a mapping of `fileid` to category. For example,
    the `brown` corpus has a file called `cats.txt` that looks like this:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 指定类别的第三种方式是使用 `cat_file` 关键字参数来指定一个包含 `fileid` 到类别映射的文件名。例如，`brown` 语料库有一个名为
    `cats.txt` 的文件，看起来像这样：
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `reuters` corpus has files in multiple categories, and its `cats.txt` looks
    like this:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`reuters` 语料库有多个类别的文件，其 `cats.txt` 看起来像这样：'
- en: '[PRE25]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Categorized tagged corpus reader
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类标记语料库读取器
- en: The `brown` corpus reader is actually an instance of `CategorizedTaggedCorpusReader`,
    which inherits from `CategorizedCorpusReader` and `TaggedCorpusReader`. Just like
    in `CategorizedPlaintextCorpusReader`, it overrides all the methods of `TaggedCorpusReader`
    to allow a `categories` argument, so you can call `brown.tagged_sents(categories=['news'])`
    to get all the tagged sentences from the `news` category. You can use the `CategorizedTaggedCorpusReader`
    just like `CategorizedPlaintextCorpusReader` for your own categorized and tagged
    text corpora.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`brown` 语料库读取器实际上是一个 `CategorizedTaggedCorpusReader` 的实例，它继承自 `CategorizedCorpusReader`
    和 `TaggedCorpusReader`。就像在 `CategorizedPlaintextCorpusReader` 中一样，它覆盖了 `TaggedCorpusReader`
    的所有方法，以允许一个 `categories` 参数，因此你可以调用 `brown.tagged_sents(categories=[''news''])`
    来获取 `news` 类别中的所有标记句子。你可以像使用 `CategorizedPlaintextCorpusReader` 一样使用 `CategorizedTaggedCorpusReader`
    来处理你自己的分类和标记文本语料库。'
- en: Categorized corpora
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类语料库
- en: The `movie_reviews` corpus reader is an instance of `CategorizedPlaintextCorpusReader`,
    as is the `reuters` corpus reader. But where the `movie_reviews` corpus only has
    two categories (`neg` and `pos`), `reuters` has 90 categories. These corpora are
    often used for training and evaluating classifiers, which will be covered in [Chapter
    7](ch07.html "Chapter 7. Text Classification"), *Text Classification*.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`movie_reviews` 语料库读取器是 `CategorizedPlaintextCorpusReader` 的一个实例，同样 `reuters`
    语料库读取器也是如此。但是，`movie_reviews` 语料库只有两个类别（`neg` 和 `pos`），而 `reuters` 有 90 个类别。这些语料库通常用于训练和评估分类器，这将在
    [第 7 章](ch07.html "第 7 章。文本分类") *文本分类* 中介绍。'
- en: See also
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: In the next recipe, we'll create a subclass of `CategorizedCorpusReader` and
    `ChunkedCorpusReader` for reading a categorized chunk corpus. Also see [Chapter
    7](ch07.html "Chapter 7. Text Classification"), *Text Classification* in which
    we use categorized text for classification.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个配方中，我们将创建一个 `CategorizedCorpusReader` 和 `ChunkedCorpusReader` 的子类，用于读取分类块语料库。也请参阅
    [第 7 章](ch07.html "第 7 章。文本分类") *文本分类*，其中我们使用分类文本进行分类。
- en: Creating a categorized chunk corpus reader
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建分类块语料库读取器
- en: NLTK provides a `CategorizedPlaintextCorpusReader` and `CategorizedTaggedCorpusReader`,
    but there's no categorized corpus reader for chunked corpora. So in this recipe,
    we're going to make one.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 提供了 `CategorizedPlaintextCorpusReader` 和 `CategorizedTaggedCorpusReader`，但没有为块语料库提供分类语料库读取器。因此，在这个配方中，我们将创建一个。
- en: Getting ready
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the earlier recipe, *Creating a chunked phrase corpus*, for an explanation
    of `ChunkedCorpusReader`, and to the previous recipe for details on `CategorizedPlaintextCorpusReader`
    and `CategorizedTaggedCorpusReader`, both of which inherit from `CategorizedCorpusReader`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 参考前面的配方，*创建块短语语料库*，以了解 `ChunkedCorpusReader` 的解释，以及前面的配方，以了解 `CategorizedPlaintextCorpusReader`
    和 `CategorizedTaggedCorpusReader` 的详细信息，这两个类都继承自 `CategorizedCorpusReader`。
- en: How to do it...
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We''ll create a class called `CategorizedChunkedCorpusReader` that inherits
    from both `CategorizedCorpusReader` and `ChunkedCorpusReader`. It is heavily based
    on the `CategorizedTaggedCorpusReader`, and also provides three additional methods
    for getting categorized chunks. The following code is found in `catchunked.py`:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个名为 `CategorizedChunkedCorpusReader` 的类，它继承自 `CategorizedCorpusReader`
    和 `ChunkedCorpusReader`。它主要基于 `CategorizedTaggedCorpusReader`，并提供了三个额外的获取分类块的方法。以下代码位于
    `catchunked.py` 中：
- en: '[PRE26]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: All of the following methods call the corresponding function in `ChunkedCorpusReader`
    with the value returned from `_resolve()`. We'll start with the plain text methods.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 所有以下方法都调用 `ChunkedCorpusReader` 中的相应函数，并使用 `_resolve()` 返回的值。我们将从纯文本方法开始。
- en: '[PRE27]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Next comes the tagged text methods.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是标记文本方法。
- en: '[PRE28]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: And finally, the chunked methods, which is what we've really been after.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，是块方法，这是我们真正追求的。
- en: '[PRE29]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: All these methods together give us a complete `CategorizedChunkedCorpusReader`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些方法共同构成了一个完整的 `CategorizedChunkedCorpusReader`。
- en: How it works...
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '`CategorizedChunkedCorpusReader` overrides all the `ChunkedCorpusReader` methods
    to take a `categories` argument for locating `fileids`. These `fileids` are found
    with the internal `_resolve()` function. This `_resolve()` function makes use
    of `CategorizedCorpusReader.fileids()` to return `fileids` for a given list of
    `categories`. If no `categories` are given, `_resolve()` just returns the given
    `fileids`, which could be `None`, in which case all files are read. The initialization
    of both `CategorizedCorpusReader` and `ChunkedCorpusReader` is what makes this
    all possible. If you look at the code for `CategorizedTaggedCorpusReader`, you''ll
    see it''s very similar. The inheritance diagram looks like this:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`CategorizedChunkedCorpusReader`覆盖了所有`ChunkedCorpusReader`方法，以接受一个`categories`参数来定位`fileids`。这些`fileids`通过内部`_resolve()`函数找到。这个`_resolve()`函数利用`CategorizedCorpusReader.fileids()`返回给定`categories`列表的`fileids`。如果没有提供`categories`，则`_resolve()`只返回给定的`fileids`，这可能是`None`，在这种情况下，将读取所有文件。`CategorizedCorpusReader`和`ChunkedCorpusReader`的初始化使得这一切成为可能。如果你查看`CategorizedTaggedCorpusReader`的代码，你会看到它与它非常相似。继承图如下：'
- en: '![How it works...](img/3609OS_03_07.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/3609OS_03_07.jpg)'
- en: Here's some example code for using the `treebank` corpus. All we're doing is
    making categories out of the `fileids`, but the point is that you could use the
    same techniques to create your own categorized chunk corpus.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用`treebank`语料库的一些示例代码。我们只是将`fileids`制作成类别，但重点是你可以使用相同的技巧来创建自己的分类分块语料库。
- en: '[PRE30]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We use `nltk.data.find()` to search the data directories to get a `FileSystemPathPointer`
    to the `treebank` corpus. All the `treebank` tagged files start with `wsj_` followed
    by a number, and end with `.pos`. The previous code turns that file number into
    a category.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`nltk.data.find()`在数据目录中搜索以获取`FileSystemPathPointer`到`treebank`语料库。所有以`wsj_`开头，后跟数字，并以`.pos`结尾的`treebank`标记文件。前面的代码将文件编号转换为类别。
- en: There's more...
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多...
- en: As covered in the *Creating a chunked phrase corpus* recipe, there's an alternative
    format and reader for a chunk corpus using IOB tags. To have a categorized corpus
    of IOB chunks, we have to make a new corpus reader.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如*创建分块短语语料库*配方中所述，有一个使用IOB标签的块语料库的替代格式和读取器。为了有一个分类的IOB块语料库，我们必须创建一个新的语料库读取器。
- en: Categorized Conll chunk corpus reader
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类Conll语料库分块读取器
- en: Here's a subclass of `CategorizedCorpusReader` and `ConllChunkReader` called
    `CategorizedConllChunkCorpusReader`. It overrides all methods of `ConllCorpusReader`
    that take a `fileids` argument, so the methods can also take a `categories` argument.
    The `ConllChunkCorpusReader` is just a small subclass of `ConllCorpusReader` that
    handles initialization; most of the work is done in `ConllCorpusReader`. This
    code can also be found in `catchunked.py`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个名为`CategorizedConllChunkCorpusReader`的`CategorizedCorpusReader`和`ConllChunkReader`的子类。它覆盖了所有接受`fileids`参数的`ConllCorpusReader`方法，因此这些方法也可以接受`categories`参数。`ConllChunkReader`只是`ConllCorpusReader`的一个小子类，用于处理初始化；大部分工作都在`ConllCorpusReader`中完成。此代码也可在`catchunked.py`中找到。
- en: '[PRE31]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: All the following methods call the corresponding method of `ConllCorpusReader`
    with the value returned from `_resolve()`. We'll start with the plain text methods.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 所有以下方法都调用`ConllCorpusReader`的相应方法，并使用从`_resolve()`返回的值。我们将从纯文本方法开始。
- en: '[PRE32]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The `ConllCorpusReader` does not recognize paragraphs, so there are no `*_paras()`
    methods. Next are the tagged and chunked methods.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConllCorpusReader`不识别段落，因此没有`*_paras()`方法。接下来是标记和分块的方法。'
- en: '[PRE33]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'For completeness, we must override the following methods of the `ConllCorpusReader`:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我们必须覆盖`ConllCorpusReader`的以下方法：
- en: '[PRE34]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The inheritance diagram for this class is as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 该类的继承图如下：
- en: '![Categorized Conll chunk corpus reader](img/3609OS_03_08.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![分类Conll分块语料库读取器](img/3609OS_03_08.jpg)'
- en: 'Following is some example code using the `conll2000` corpus. Like with `treebank`,
    we''re using the `fileids` for categories. The `ConllChunkCorpusReader` requires
    a third argument to specify the `chunk_types`. These `chunk_types` are used to
    parse the IOB tags. As you learned in the *Creating a chunked phrase corpus* recipe,
    the `conll2000` corpus recognizes three chunk types:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用`conll2000`语料库的一些示例代码。与`treebank`一样，我们使用`fileids`作为类别。`ConllChunkCorpusReader`需要一个第三个参数来指定`chunk_types`。这些`chunk_types`用于解析IOB标签。正如你在*创建分块短语语料库*配方中学到的，`conll2000`语料库识别三种分块类型：
- en: '`NP` for noun phrases'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NP`表示名词短语'
- en: '`VP` for verb phrases'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VP`表示动词短语'
- en: '`PP` for prepositional phrases'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PP`表示介词短语'
- en: '[PRE35]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: See also
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: In the *Creating a chunked phrase corpus* recipe in this chapter, we covered
    both the `ChunkedCorpusReader` and `ConllChunkCorpusReader`. And in the previous
    recipe, we covered `CategorizedPlaintextCorpusReader` and `CategorizedTaggedCorpusReader`,
    which share the same superclass used by `CategorizedChunkedCorpusReader` and `CategorizedConllChunkReader`—`CategorizedCorpusReader`.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的 *创建分块短语语料库* 食谱中，我们介绍了 `ChunkedCorpusReader` 和 `ConllChunkCorpusReader`。在前一个食谱中，我们介绍了
    `CategorizedPlaintextCorpusReader` 和 `CategorizedTaggedCorpusReader`，它们与 `CategorizedChunkedCorpusReader`
    和 `CategorizedConllChunkReader` 使用的相同超类 `CategorizedCorpusReader` 具有相同的超类。
- en: Lazy corpus loading
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 懒惰语料库加载
- en: Loading a corpus reader can be an expensive operation due to the number of files,
    file sizes, and various initialization tasks. And while you'll often want to specify
    a corpus reader in a common module, you don't always need to access it right away.
    To speed up module import time when a corpus reader is defined, NLTK provides
    a `LazyCorpusLoader` class that can transform itself into your actual corpus reader
    as soon as you need it. This way, you can define a corpus reader in a common module
    without it slowing down module loading.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文件数量、文件大小和多种初始化任务，加载语料库读取器可能是一个昂贵的操作。虽然你通常会想在公共模块中指定语料库读取器，但你并不总是需要立即访问它。为了在定义语料库读取器时加快模块导入时间，NLTK
    提供了一个 `LazyCorpusLoader` 类，它可以在你需要时立即将自身转换为实际的语料库读取器。这样，你可以在公共模块中定义语料库读取器，而不会减慢模块加载速度。
- en: How to do it...
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: '`LazyCorpusLoader` requires two arguments: the `name` of the corpus and the
    corpus reader class, plus any other arguments needed to initialize the corpus
    reader class.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`LazyCorpusLoader` 需要两个参数：语料库的 `name` 和语料库读取器类，以及初始化语料库读取器类所需的任何其他参数。'
- en: The `name` argument specifies the root directory name of the corpus, which must
    be within a `corpora` subdirectory of one of the paths in `nltk.data.path`. See
    the first recipe of this chapter, *Setting up a custom corpus*, for more details
    on `nltk.data.path`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`name` 参数指定语料库的根目录名称，它必须位于 `nltk.data.path` 中某个路径的 `corpora` 子目录内。有关 `nltk.data.path`
    的更多详细信息，请参阅本章的第一个食谱，*设置自定义语料库*。'
- en: For example, if you have a custom corpora named `cookbook` in your local `nltk_data`
    directory, its path would be `~/nltk_data/corpora/cookbook`. You'd then pass `'cookbook'`
    to `LazyCorpusLoader` as the `name`, and `LazyCorpusLoader` will look in `~/nltk_data/corpora`
    for a directory named `'cookbook'`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你在你的本地 `nltk_data` 目录中有一个名为 `cookbook` 的自定义语料库，它的路径将是 `~/nltk_data/corpora/cookbook`。然后，你会将
    `'cookbook'` 传递给 `LazyCorpusLoader` 作为 `name`，`LazyCorpusLoader` 将在 `~/nltk_data/corpora`
    中查找名为 `'cookbook'` 的目录。
- en: The second argument to `LazyCorpusLoader` is `reader_cls`, which should be the
    name of a subclass of `CorpusReader`, such as `WordListCorpusReader`. You will
    also need to pass in any other arguments required by the `reader_cls` for initialization.
    This will be demonstrated as follows, using the same `wordlist` file we created
    in the earlier recipe, *Creating a word list corpus*. The third argument to `LazyCorpusLoader`
    is the list of filenames and `fileids` that will be passed in to `WordListCorpusReader`
    at initialization.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`LazyCorpusLoader` 的第二个参数是 `reader_cls`，它应该是 `CorpusReader` 的子类的名称，例如 `WordListCorpusReader`。你还需要传递
    `reader_cls` 初始化所需的任何其他参数。以下将演示如何使用我们之前在 *创建单词列表语料库* 食谱中创建的相同 `wordlist` 文件，这将展示
    `LazyCorpusLoader` 的第三个参数，即将在初始化时传递给 `WordListCorpusReader` 的文件名和 `fileids` 列表。'
- en: '[PRE36]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: How it works...
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '`LazyCorpusLoader` stores all the arguments given, but otherwise does nothing
    until you try to access an attribute or method. This way initialization is very
    fast, eliminating the overhead of loading the corpus reader immediately. As soon
    as you do access an attribute or method, it does the following:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`LazyCorpusLoader` 存储了所有给出的参数，但在你尝试访问属性或方法之前，它不会做任何事情。这样初始化非常快，消除了立即加载语料库读取器的开销。一旦你访问了属性或方法，它就会执行以下操作：'
- en: Calls `nltk.data.find('corpora/%s' % name)` to find the corpus data root directory.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 `nltk.data.find('corpora/%s' % name)` 来查找语料库数据根目录。
- en: Instantiate the corpus reader class with the root directory and any other arguments.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用根目录和任何其他参数实例化语料库读取器类。
- en: Transforms itself into the corpus reader class.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将自身转换为语料库读取类。
- en: So in the previous example code, before we call `reader.fileids()`, `reader`
    is an instance of `LazyCorpusLoader`, but after the call, `reader` is an instance
    of `WordListCorpusReader`.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在之前的示例代码中，在我们调用 `reader.fileids()` 之前，`reader` 是 `LazyCorpusLoader` 的一个实例，但在调用之后，`reader`
    是 `WordListCorpusReader` 的一个实例。
- en: There's more...
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: All of the corpora included with NLTK and defined in `nltk.corpus` are initially
    an instance of `LazyCorpusLoader`. Here's some code from `nltk.corpus` defining
    the `treebank` corpora.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK中包含的所有语料库和定义在`nltk.corpus`中的语料库最初都是`LazyCorpusLoader`的一个实例。以下是从`nltk.corpus`定义`treebank`语料库的代码。
- en: '[PRE37]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As you can see, any number of additional arguments can be passed through by
    `LazyCorpusLoader` to its `reader_cls`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，可以通过`LazyCorpusLoader`通过`reader_cls`传递任意数量的附加参数。
- en: Creating a custom corpus view
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建自定义语料库视图
- en: A **corpus view** is a class wrapper around a corpus file that reads in blocks
    of tokens as needed. Its purpose is to provide a *view* into a file without reading
    the whole file at once (since corpus files can often be quite large). If the corpus
    readers included by NLTK already meet all your needs, then you do not have to
    know anything about corpus views. But, if you have a custom file format that needs
    special handling, this recipe will show you how to create and use a custom corpus
    view. The main corpus view class is `StreamBackedCorpusView`, which opens a single
    file as a *stream*, and maintains an internal cache of blocks it has read.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '**语料库视图**是一个围绕语料库文件的类包装器，它按需读取标记块。其目的是在不一次性读取整个文件的情况下（因为语料库文件通常相当大）提供对文件的“视图”。如果NLTK中包含的语料库读取器已经满足您的所有需求，那么您不需要了解任何关于语料库视图的知识。但是，如果您有一个需要特殊处理的自定义文件格式，这个菜谱将向您展示如何创建和使用自定义语料库视图。主要的语料库视图类是`StreamBackedCorpusView`，它将单个文件作为*流*打开，并维护它已读取的块的内缓存。'
- en: 'Blocks of tokens are read in with a *block reader* function. A **block** can
    be any piece of text, such as a paragraph or a line, and **tokens** are parts
    of a block, such as individual words. In the *Creating a part-of-speech tagged
    word corpus* recipe, we discussed the default `para_block_reader` function of
    the `TaggedCorpusReader`, which reads lines from a file until it finds a blank
    line, then returns those lines as a single paragraph token. The actual block reader
    function is: `nltk.corpus.reader.util.read_blankline_block`. `TaggedCorpusReader`
    passes this block reader function into a `TaggedCorpusView` whenever it needs
    to read blocks from a file. `TaggedCorpusView` is a subclass of `StreamBackedCorpusView`
    that knows to split paragraphs of "word/tag" into `(word, tag)` tuples.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*块读取器*函数读取标记块。一个**块**可以是任何文本片段，例如一个段落或一行，而**标记**是块的一部分，例如单个单词。在*创建词性标注词语料库*菜谱中，我们讨论了`TaggedCorpusReader`的默认`para_block_reader`函数，该函数从文件中读取行，直到找到空白行，然后返回这些行作为单个段落标记。实际的块读取器函数是：`nltk.corpus.reader.util.read_blankline_block`。`TaggedCorpusReader`在需要从文件中读取块时将此块读取器函数传递给`TaggedCorpusView`。`TaggedCorpusView`是`StreamBackedCorpusView`的一个子类，知道如何将“word/tag”段落分割成`(word,
    tag)`元组。
- en: How to do it...
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'We''ll start with the simple case of a plain text file with a heading that
    should be ignored by the corpus reader. Let''s make a file called `heading_text.txt`
    that looks like this:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从需要被语料库读取器忽略的标题的纯文本文件开始。让我们创建一个名为`heading_text.txt`的文件，其外观如下：
- en: '[PRE38]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Normally we'd use the `PlaintextCorpusReader` but, by default, it will treat
    `A simple heading` as the first paragraph. To ignore this heading, we need to
    subclass the `PlaintextCorpusReader` so we can override its `CorpusView` class
    variable with our own `StreamBackedCorpusView` subclass. This code is found in
    `corpus.py`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们会使用`PlaintextCorpusReader`，但默认情况下，它将`A simple heading`视为第一段。为了忽略这个标题，我们需要对`PlaintextCorpusReader`进行子类化，这样我们就可以用我们自己的`StreamBackedCorpusView`子类覆盖其`CorpusView`类变量。这段代码位于`corpus.py`中。
- en: '[PRE39]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: To demonstrate that this works as expected, here's the code showing that the
    default `PlaintextCorpusReader` finds four paragraphs, while our `IgnoreHeadingCorpusReader`
    only has three paragraphs.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明这按预期工作，以下代码显示了默认的`PlaintextCorpusReader`找到了四个段落，而我们的`IgnoreHeadingCorpusReader`只有三个段落。
- en: '[PRE40]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: How it works...
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `PlaintextCorpusReader` by design has a `CorpusView` class variable that
    can be overridden by subclasses. So we do just that, and make our `IgnoreHeadingCorpusView`
    the `CorpusView`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`PlaintextCorpusReader`设计上有一个可以被子类覆盖的`CorpusView`类变量。所以我们就是这样做的，并使我们的`IgnoreHeadingCorpusView`成为`CorpusView`。'
- en: Note
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Most corpus readers do not have a `CorpusView` class variable because they require
    very specific corpus views.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数语料库读取器没有`CorpusView`类变量，因为它们需要非常特定的语料库视图。
- en: 'The `IgnoreHeadingCorpusView` is a subclass of `StreamBackedCorpusView` that
    does the following on initialization:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`IgnoreHeadingCorpusView`是`StreamBackedCorpusView`的一个子类，在初始化时执行以下操作：'
- en: Open the file using `self._open()`. This function is defined by `StreamBackedCorpusView`,
    and sets the internal instance variable `self._stream` to the opened file.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`self._open()`打开文件。此函数由`StreamBackedCorpusView`定义，并将内部实例变量`self._stream`设置为打开的文件。
- en: Read one block with `read_blankline_block()`, which will read the heading as
    a paragraph, and move the stream's file position forward to the next block.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`read_blankline_block()`读取一个块，它将读取标题作为段落，并将流的文件位置向前移动到下一个块。
- en: Reset the start file position to the current position of `self._stream`. `self._filepos`
    is an internal index of where each block is in the file.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件起始位置重置为`self._stream`的当前位置。`self._filepos`是文件中每个块的内部索引。
- en: 'Here''s a diagram illustrating the relationships between the classes:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个说明类之间关系的图示：
- en: '![How it works...](img/3609OS_03_09.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/3609OS_03_09.jpg)'
- en: There's more...
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Corpus views can get a lot fancier and more complicated, but the core concept
    is the same: read *blocks* from a `stream` to return a list of *tokens*. There
    are a number of block readers provided in `nltk.corpus.reader.util`, but you can
    always create your own. If you do want to define your own block reader function,
    then you have two choices on how to implement it:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库视图可以变得更加复杂和花哨，但其核心概念是相同的：从`stream`中读取*块*以返回一个*标记*列表。`nltk.corpus.reader.util`中提供了一些块读取器，但您始终可以创建自己的。如果您确实想定义自己的块读取器函数，那么您有两种实现方式：
- en: Define it as a separate function and pass it in to `StreamBackedCorpusView`
    as `block_reader`. This is a good option if your block reader is fairly simple,
    reusable, and doesn't require any outside variables or configuration.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其定义为单独的函数，并将其作为`block_reader`传递给`StreamBackedCorpusView`。如果您的块读取器相对简单、可重用且不需要任何外部变量或配置，这是一个不错的选择。
- en: Subclass `StreamBackedCorpusView` and override the `read_block()` method. This
    is what many custom corpus views do because the block reading is highly specialized
    and requires additional functions and configuration, usually provided by the corpus
    reader when the corpus view is initialized.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继承`StreamBackedCorpusView`类并重写`read_block()`方法。这是因为块读取非常专业化，需要额外的函数和配置，通常在初始化语料库视图时由语料库读取器提供。
- en: Block reader functions
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 块读取器函数
- en: 'Following is a survey of most of the included block readers in `nltk.corpus.reader.util`.
    Unless otherwise noted, each block reader function takes a single argument: the
    `stream` to read from.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是对`nltk.corpus.reader.util`中包含的大多数块读取器的一个概述。除非另有说明，否则每个块读取器函数只接受一个参数：要从中读取的`stream`。
- en: '`read_whitespace_block()` will read 20 lines from the stream, splitting each
    line into tokens by whitespace.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`read_whitespace_block()`将从流中读取20行，通过空白字符将每行分割成标记。'
- en: '`read_wordpunct_block()` reads 20 lines from the stream, splitting each line
    using `nltk.tokenize.wordpunct_tokenize()`.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`read_wordpunct_block()`从流中读取20行，使用`nltk.tokenize.wordpunct_tokenize()`分割每行。'
- en: '`read_line_block()` reads 20 lines from the stream and returns them as a list,
    with each line as a token.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`read_line_block()`从流中读取20行，并将它们作为列表返回，每行作为一个标记。'
- en: '`read_blankline_block()` will read lines from the stream until it finds a blank
    line. It will then return a single token of all lines found combined into a single
    string.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`read_blankline_block()`将从流中读取行，直到找到空白行。然后，它将返回所有找到的行合并成一个字符串的单个标记。'
- en: '`read_regexp_block()` takes two additional arguments, which must be regular
    expressions that can be passed to `re.match()`: a `start_re` and `end_re`. `start_re`
    matches the starting line of a block, and `end_re` matches the ending line of
    the block. `end_re` defaults to `None`, in which case the block will end as soon
    as a new `start_re` match is found. The return value is a single token of all
    lines in the block joined into a single string.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`read_regexp_block()`需要两个额外的参数，这些参数必须是可以通过`re.match()`传递的正则表达式：`start_re`和`end_re`。`start_re`匹配块的起始行，而`end_re`匹配块的结束行。`end_re`默认为`None`，在这种情况下，块将在找到新的`start_re`匹配时结束。返回值是所有行合并成一个字符串的单个标记。'
- en: Pickle corpus view
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pickle语料库视图
- en: If you want to have a corpus of pickled objects, you can use the `PickleCorpusView`,
    a subclass of `StreamBackedCorpusView` found in `nltk.corpus.reader.util`. A file
    consists of blocks of pickled objects, and can be created with the `PickleCorpusView.write()`
    class method, which takes a sequence of objects and an output file, then pickles
    each object using `pickle.dump()` and writes it to the file. It overrides the
    `read_block()` method to return a list of unpickled objects from the stream, using
    `pickle.load()`.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要一个包含序列化对象的语料库，你可以使用`PickleCorpusView`，它是位于`nltk.corpus.reader.util`中的`StreamBackedCorpusView`的子类。一个文件由序列化对象的块组成，可以使用`PickleCorpusView.write()`类方法创建，该方法接受一个对象序列和一个输出文件，然后使用`pickle.dump()`将每个对象序列化并写入文件。它覆盖了`read_block()`方法，使用`pickle.load()`从流中返回一个未序列化对象的列表。
- en: Concatenated corpus view
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连接语料库视图
- en: Also found in `nltk.corpus.reader.util` is the `ConcatenatedCorpusView`. This
    class is useful if you have multiple files that you want a corpus reader to treat
    as a single file. A `ConcatenatedCorpusView` is created by giving it a list of
    `corpus_views`, which are then iterated over as if they were a single view.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在`nltk.corpus.reader.util`中还可以找到`ConcatenatedCorpusView`。如果你有多个文件，希望语料库读取器将其视为单个文件，这个类很有用。`ConcatenatedCorpusView`是通过提供一个`corpus_views`列表来创建的，然后像单个视图一样迭代这些视图。
- en: See also
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The concept of block readers was introduced in the *Creating a part-of-speech
    tagged word corpus* recipe in this chapter.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 块读取器的概念在本章的*创建词性标注词语料库*食谱中引入。
- en: Creating a MongoDB backed corpus reader
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建MongoDB支持的语料库读取器
- en: All the corpus readers we've dealt with so far have been file-based. That is
    in part due to the design of the `CorpusReader` base class, and also the assumption
    that most corpus data will be in text files. But sometimes you'll have a bunch
    of data stored in a database that you want to access and use just like a text
    file corpus. In this recipe, we'll cover the case where you have documents in
    MongoDB, and you want to use a particular field of each document as your block
    of text.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们处理的所有语料库读取器都是基于文件的。这在一定程度上是由于`CorpusReader`基类的设计，以及大多数语料库数据将存储在文本文件中的假设。但有时你会有大量存储在数据库中的数据，你希望像访问和使用文本文件语料库一样访问和使用这些数据。在本食谱中，我们将介绍你有存储在MongoDB中的文档，并且希望使用每个文档的特定字段作为你的文本块的情况。
- en: Getting ready
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: MongoDB is a document-oriented database that has become a popular alternative
    to relational databases such as MySQL. The installation and setup of MongoDB is
    outside the scope of this book, but you can find instructions at [http://www.mongodb.org/display/DOCS/Quickstart](http://www.mongodb.org/display/DOCS/Quickstart).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB是一个面向文档的数据库，它已成为MySQL等关系数据库的流行替代品。MongoDB的安装和设置超出了本书的范围，但你可以在[http://www.mongodb.org/display/DOCS/Quickstart](http://www.mongodb.org/display/DOCS/Quickstart)找到说明。
- en: You'll also need to install PyMongo, a Python driver for MongoDB. You should
    be able to do this with either `easy_install` or `pip`, by doing `sudo easy_install
    pymongo` or `sudo pip install pymongo`.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要安装PyMongo，它是MongoDB的Python驱动程序。你可以通过`easy_install`或`pip`来完成此操作，方法是执行`sudo
    easy_install pymongo`或`sudo pip install pymongo`。
- en: The code in the *How to do it...* section assumes that your database is on `localhost`
    port `27017`, which is the MongoDB default configuration, and that you'll be using
    the `test` database with a collection named `corpus` that contains documents with
    a `text` field. Explanations for these arguments are available in the PyMongo
    documentation at [http://api.mongodb.org/python/](http://api.mongodb.org/python/).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在*如何做到...*部分中的代码假设你的数据库位于`localhost`端口`27017`，这是MongoDB的默认配置，并且你将使用名为`corpus`的集合，该集合包含具有`text`字段的文档。这些参数的解释可以在PyMongo文档中找到，网址为[http://api.mongodb.org/python/](http://api.mongodb.org/python/)。
- en: How to do it...
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到...
- en: Since the `CorpusReader` class assumes you have a file-based corpus, we can't
    directly subclass it. Instead, we're going to emulate both the `StreamBackedCorpusView`
    and `PlaintextCorpusReader`. `StreamBackedCorpusView` is a subclass of `nltk.util.AbstractLazySequence`,
    so we'll subclass `AbstractLazySequence` to create a MongoDB view, and then create
    a new class that will use the view to provide functionality similar to the `PlaintextCorpusReader`.
    This code is found in `mongoreader.py`.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`CorpusReader`类假设你有一个基于文件的语料库，所以我们不能直接继承它。相反，我们将模拟`StreamBackedCorpusView`和`PlaintextCorpusReader`。`StreamBackedCorpusView`是`nltk.util.AbstractLazySequence`的子类，因此我们将继承`AbstractLazySequence`来创建一个MongoDB视图，然后创建一个新的类，该类将使用视图提供类似于`PlaintextCorpusReader`的功能。此代码位于`mongoreader.py`中。
- en: '[PRE41]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: How it works...
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '`AbstractLazySequence` is an abstract class that provides read-only, on-demand
    iteration. Subclasses must implement the `__len__()` and `iterate_from(start)`
    methods, while it provides the rest of the list and iterator emulation methods.
    By creating the `MongoDBLazySequence` subclass as our view, we can iterate over
    documents in the MongoDB collection on-demand, without keeping all the documents
    in memory. `LazyMap` is a lazy version of Python''s built-in `map()` function,
    and is used in `iterate_from()` to transform the document into the specific field
    that we''re interested in. It''s also a subclass of `AbstractLazySequence`.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`AbstractLazySequence`是一个抽象类，它提供了只读的按需迭代。子类必须实现`__len__()`和`iterate_from(start)`方法，同时它提供了列表和迭代器模拟的其余方法。通过创建`MongoDBLazySequence`子类作为我们的视图，我们可以在按需迭代MongoDB集合中的文档，而不需要将所有文档都保存在内存中。`LazyMap`是Python内置的`map()`函数的懒加载版本，并在`iterate_from()`中使用，以将文档转换为感兴趣的特定字段。它也是一个`AbstractLazySequence`的子类。'
- en: The `MongoDBCorpusReader` creates an internal instance of `MongoDBLazySequence`
    for iteration, then defines the word and sentence tokenization methods. The `text()`
    method simply returns the instance of `MongoDBLazySequence`, which results in
    a lazily evaluated list of each text field. The `words()` method uses `LazyMap`
    and `LazyConcatenation` to return a lazily evaluated list of all words, while
    the `sents()` method does the same for sentences. The `sent_tokenizer` is loaded
    on demand with `LazyLoader`, which is a wrapper around `nltk.data.load()`, analogous
    to `LazyCorpusLoader`. `LazyConcatentation` is a subclass of `AbstractLazySequence`
    too, and produces a flat list from a given list of lists (each list may also be
    lazy). In our case, we're concatenating the results of `LazyMap` to ensure we
    don't return nested lists.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`MongoDBCorpusReader`为迭代创建了一个`MongoDBLazySequence`的内部实例，然后定义了单词和句子分词方法。`text()`方法简单地返回`MongoDBLazySequence`的实例，从而得到每个文本字段的懒加载列表。`words()`方法使用`LazyMap`和`LazyConcatenation`返回所有单词的懒加载列表，而`sents()`方法对句子执行相同的操作。`sent_tokenizer`通过`LazyLoader`按需加载，`LazyLoader`是`nltk.data.load()`的包装器，类似于`LazyCorpusLoader`。`LazyConcatentation`也是一个`AbstractLazySequence`的子类，它从给定的列表列表（每个列表也可以是懒加载的）中生成一个扁平列表。在我们的情况下，我们通过连接`LazyMap`的结果来确保我们不返回嵌套列表。'
- en: There's more...
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'All of the parameters are configurable. For example, if you had a `db` named
    `website`, with a `collection` named `comments`, whose documents had a `field`
    called `comment`, you could create a `MongoDBCorpusReader` as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 所有参数都是可配置的。例如，如果您有一个名为`db`的数据库，其`collection`名为`comments`，文档中有一个名为`comment`的字段，您可以创建一个`MongoDBCorpusReader`如下：
- en: '[PRE42]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: You can also pass in custom instances for `word_tokenizer` and `sent_tokenizer`,
    as long as the objects implement the `nltk.tokenize.TokenizerI` interface by providing
    a `tokenize(text)` method.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以为`word_tokenizer`和`sent_tokenizer`传递自定义实例，只要这些对象通过提供`tokenize(text)`方法实现了`nltk.tokenize.TokenizerI`接口。
- en: See also
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考以下内容
- en: Corpus views were covered in the previous recipe, and tokenization was covered
    in [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing
    Text and WordNet Basics*.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的菜谱中已经介绍了语料库视图，而在[第1章](ch01.html "第1章. 文本分词和WordNet基础知识")中介绍了分词，*文本分词和WordNet基础知识*。
- en: Corpus editing with file locking
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用文件锁定进行语料库编辑
- en: Corpus readers and views are all read-only, but there may be times when you
    want to add to or edit the corpus files. However, modifying a corpus file while
    other processes are using it, such as through a corpus reader, can lead to dangerous
    undefined behavior. This is where file locking comes in handy.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库读取器和视图都是只读的，但有时您可能想要添加或编辑语料库文件。然而，当其他进程正在使用它时，例如通过语料库读取器，修改语料库文件可能会导致危险的不确定行为。这时文件锁定就派上用场了。
- en: Getting ready
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You must install the `lockfile` library using `sudo easy_install lockfile` or
    `sudo pip install lockfile`. This library provides cross-platform file locking,
    and so will work on Windows, Unix/Linux, Mac OX, and more. You can find detailed
    documentation on `lockfile` at `http://packages.python.or` [g/lockfile/](http://g/lockfile/).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须使用`sudo easy_install lockfile`或`sudo pip install lockfile`命令安装`lockfile`库。这个库提供跨平台的文件锁定功能，因此可以在Windows、Unix/Linux、Mac
    OS X等操作系统上工作。您可以在`http://packages.python.org/lockfile/` [g/lockfile/](http://g/lockfile/)找到关于`lockfile`的详细文档。
- en: For the following code to work, you must also have Python 2.6\. Versions 2.4
    and earlier do not support the `with` keyword.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使以下代码能够正常工作，您还必须安装Python 2.6。2.4版本及更早的版本不支持`with`关键字。
- en: How to do it...
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Here are two file editing functions: `append_line()` and `remove_line()`. Both
    try to acquire an *exclusive lock* on the file before updating it. An **exclusive
    lock** means that these functions will wait until no other process is reading
    from or writing to the file. Once the lock is acquired, any other process that
    tries to access the file will have to wait until the lock is released. This way,
    modifying the file will be safe and not cause any undefined behavior in other
    processes. These functions can be found in `corpus.py`.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个文件编辑功能：`append_line()` 和 `remove_line()`。这两个函数在更新文件之前都会尝试获取一个 *独占锁*。独占锁意味着这些函数将等待直到没有其他进程正在读取或写入文件。一旦获取了锁，任何尝试访问文件的进程都必须等待直到锁被释放。这样，修改文件将是安全的，并且不会在其他进程中引起任何未定义的行为。这些函数可以在
    `corpus.py` 中找到。
- en: '[PRE43]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The lock acquiring and releasing happens transparently when you do `with lockfile.FileLock(fname)`.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用 `with lockfile.FileLock(fname)` 时，锁的获取和释放是透明发生的。
- en: Note
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Instead of using `with lockfile.FileLock(fname)`, you can also get a lock by
    calling `lock = lockfile.FileLock(fname)`, then call `lock.acquire()` to acquire
    the lock, and `lock.release()` to release the lock. This alternative usage is
    compatible with Python 2.4.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用 `with lockfile.FileLock(fname)`，您还可以通过调用 `lock = lockfile.FileLock(fname)`
    来获取锁，然后调用 `lock.acquire()` 来获取锁，以及调用 `lock.release()` 来释放锁。这种替代用法与 Python 2.4
    兼容。
- en: How it works...
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'You can use these functions as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下功能：
- en: '[PRE44]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: In `append_line()`, a lock is acquired, the file is opened in *append mode*,
    the text is written along with an end-of-line character, and then the file is
    closed, releasing the lock.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `append_line()` 中，获取锁后，文件以 *追加模式* 打开，文本与换行符一起写入，然后关闭文件，释放锁。
- en: Tip
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: A lock acquired by `lockfile` only protects the file from other processes that
    also use `lockfile`. In other words, just because your Python process has a lock
    with `lockfile`, doesn't mean a non-Python process can't modify the file. For
    this reason, it's best to only use `lockfile` with files that will not be edited
    by any non-Python processes, or Python processes that do not use `lockfile`.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 由 `lockfile` 获取的锁只能保护文件免受其他也使用 `lockfile` 的进程的影响。换句话说，仅仅因为您的 Python 进程使用 `lockfile`
    有锁，并不意味着非 Python 进程不能修改文件。因此，最好只使用 `lockfile` 与那些不会被任何非 Python 进程编辑的文件，或者不使用 `lockfile`
    的 Python 进程。
- en: '`The remove_line()` function is a bit more complicated. Because we''re removing
    a line and not a specific section of the file, we need to iterate over the file
    to find each instance of the line to remove. The easiest way to do this while
    writing the changes back to the file, is to use a `TemporaryFile` to hold the
    changes, then copy that file back into the original file using `shutil.copyfileobj()`.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`remove_line()` 函数稍微复杂一些。因为我们是在删除一行而不是文件的具体部分，所以我们需要遍历文件以找到要删除的行的每个实例。在将更改写回文件时，最简单的方法是使用
    `TemporaryFile` 来保存更改，然后使用 `shutil.copyfileobj()` 将该文件复制回原始文件。'
- en: These functions are best suited for a word list corpus, or some other corpus
    type with presumably unique lines, that may be edited by multiple people at about
    the same time, such as through a web interface. Using these functions with a more
    document-oriented corpus such as `brown`, `treebank`, or `conll2000`, is probably
    a bad idea.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数最适合用于单词列表语料库，或者某些其他具有可能唯一行的语料库类型，这些语料库可能由多人同时编辑，例如通过网页界面。使用这些函数与更面向文档的语料库，如
    `brown`、`treebank` 或 `conll2000`，可能是一个糟糕的主意。
