<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch03"/>Chapter 3. SciPy for Linear Algebra</h1></div></div></div><p>In this chapter, we will continue exploring the different SciPy modules through meaningful examples. We will start with the treatment of matrices (whether normal or sparse) with the modules on Linear Algebra—<code class="literal">linalg</code> and <code class="literal">sparse</code>. Note that <code class="literal">linalg</code> expands on the NumPy module with the same name.</p><p>This discipline of mathematics studies vector spaces and linear mappings between them. Matrices represent objects in this field in such a way that any property of the underlying objects may be obtained by performing adequate operations on the representing matrices. In this chapter, we assume that you are familiar with at least the basics of linear algebra, in particular with the notion of matrix multiplication, finding the determinant and inverse of a matrix, as<a id="id109" class="indexterm"/> well as their immediate applications in <strong>vector calculus</strong>.</p><p>Accordingly, in this chapter, we will explore how vectors and matrices are handled in Numpy/SciPy, how to create them, how to program standard mathematical operations between them, and how to represent this on a functional form. Next, we will solve linear system of equations expressed in the matrix form involving dense or sparse matrices. The corresponding IPython Notebook will help you test the functionality of the modules involved and modify each illustrative example according to your specific needs.</p><div><div><div><div><h1 class="title"><a id="ch03lvl1sec23"/>Vector creation</h1></div></div></div><p>As <a id="id110" class="indexterm"/>mentioned in <a class="link" href="ch02.html" title="Chapter 2. Working with the NumPy Array As a First Step to SciPy">Chapter 2</a>, <em>Working with the NumPy Array As a First Step to SciPy</em>, SciPy depends on NumPy's main object's <code class="literal">ndarray</code> data structure. You can look at one-dimensional arrays as vectors and vice versa (oriented points in an n-dimensional space). Consequently, a vector can be created via Numpy as follows:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; import numpy</strong>
<strong>&gt;&gt;&gt; vectorA = numpy.array([1,2,3,4,5,6,7])</strong>
<strong>&gt;&gt;&gt; vectorA</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>array([1, 2, 3, 4, 5, 6, 7])</strong>
</pre></div><p>We can also use already defined arrays to create a new candidate. Some examples were presented in the previous chapter. Here we can reverse the already created vector and assign it to a new one:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; vectorB = vectorA[::-1].copy()</strong>
<strong>&gt;&gt;&gt; vectorB</strong>
</pre></div><p>The <a id="id111" class="indexterm"/>output is shown as follows:</p><div><pre class="programlisting"><strong>array([7, 6, 5, 4, 3, 2, 1])</strong>
</pre></div><p>Notice that in this example, we have to make a copy of the reverse of the elements of <code class="literal">vectorA</code> and assign it to <code class="literal">vectorB</code>. This way, by changing elements of <code class="literal">vectorB</code>, the elements of <code class="literal">vectorA</code> remain unchanged, as shown here:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; vectorB[0]=123</strong>
<strong>&gt;&gt;&gt; vectorB</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>array([123,   6,   5,   4,   3,   2,   1])</strong>
</pre></div><p>Let's look at <code class="literal">vectorA</code>:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; vectorA</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>array([1, 2, 3, 4, 5, 6, 7])</strong>
</pre></div><p>Let's make a copy of <code class="literal">vectorA</code> by reversing its elements and assigning it to <code class="literal">vectorB</code>:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; vectorB = vectorA[::-1].copy()</strong>
<strong>&gt;&gt;&gt; vectorB</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>array([7, 6, 5, 4, 3, 2, 1])</strong>
</pre></div><p>In the last code statement, we repeated the previous assignment to <code class="literal">vectorB</code>, bringing it back to its initial values taking the reverse of <code class="literal">vectorA</code>, once again.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec24"/>Vector operations</h1></div></div></div><p>In <a id="id112" class="indexterm"/>addition to being mathematical entities studied in linear algebra, Vectors are widely used in physics and engineering as a convenient way to represent physical quantities as <strong>displacement</strong>, <strong>velocity</strong>, <strong>acceleration</strong>, force, and so on. Accordingly, basic operations between vectors can be performed via Numpy/SciPy operations as follows:</p><div><div><div><div><h2 class="title"><a id="ch03lvl2sec19"/>Addition/subtraction</h2></div></div></div><p>Addition/subtraction of vectors does not require any explicit loop to perform them. Let's take <a id="id113" class="indexterm"/>a look at addition of two vectors:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; vectorC = vectorA + vectorB</strong>
<strong>&gt;&gt;&gt; vectorC</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>array([8, 8, 8, 8, 8, 8, 8])</strong>
</pre></div><p>Further, we <a id="id114" class="indexterm"/>perform subtraction on two vectors:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; vectorD = vectorB - vectorA</strong>
<strong>&gt;&gt;&gt; vectorD</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>array([ 6,  4,  2,  0, -2, -4, -6])</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec20"/>Scalar/Dot product</h2></div></div></div><p>Numpy has<a id="id115" class="indexterm"/> the built-in function dot to compute the<a id="id116" class="indexterm"/> scalar (<code class="literal">dot</code>) product between two vectors. We show you its use computing the <code class="literal">dot</code> product of <code class="literal">vectorA</code> and <code class="literal">vectorB</code> from the previous code snippet:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; dotProduct1 = numpy.dot(vectorA,vectorB)</strong>
<strong>&gt;&gt;&gt; dotProduct1</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>84</strong>
</pre></div><p>Alternatively, to compute this product we could perform the element-wise product between the components of the vectors and then add the respective results. This is implemented in the following lines of code:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; dotProduct2 = (vectorA*vectorB).sum()</strong>
<strong>&gt;&gt;&gt; dotProduct2</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>84</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec21"/>Cross/Vector product – on three-dimensional space vectors</h2></div></div></div><p>First, two<a id="id117" class="indexterm"/> vectors in 3 dimensions are created before<a id="id118" class="indexterm"/> applying the built-in function from NumPy to compute the cross product between the vectors:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; vectorA = numpy.array([5, 6, 7])</strong>
<strong>&gt;&gt;&gt; vectorB = numpy.array([7, 6, 5])</strong>
<strong>&gt;&gt;&gt; crossProduct = numpy.cross(vectorA,vectorB)</strong>
<strong>&gt;&gt;&gt; crossProduct</strong>
</pre></div><p>The <a id="id119" class="indexterm"/>output is shown as follows:</p><div><pre class="programlisting"><strong>array([-12,  24, -12])</strong>
</pre></div><p>Further, we<a id="id120" class="indexterm"/> perform a <code class="literal">cross</code> operation of <code class="literal">vectorB</code> over <code class="literal">vectorA</code>:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; crossProduct = numpy.cross(vectorB,vectorA)</strong>
<strong>&gt;&gt;&gt; crossProduct</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>array([ 12, -24,  12])</strong>
</pre></div><p>Notice that the last expression shows the expected result that <code class="literal">vectorA</code> cross <code class="literal">vectorB</code> is the negative of <code class="literal">vectorB</code> cross <code class="literal">vectorA</code>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec25"/>Creating a matrix</h1></div></div></div><p>In SciPy, a<a id="id121" class="indexterm"/> matrix structure is given to any one- or two-dimensional <code class="literal">ndarray</code>, with either the <code class="literal">matrix</code> or <code class="literal">mat</code> command. The complete syntax is as follows:</p><div><pre class="programlisting">numpy.matrix(data=object, dtype=None, copy=True)</pre></div><p>Creating matrices, the data may be given as <code class="literal">ndarray</code>, a string or a Python list (as the second example below), which is very convenient. When using strings, the semicolon denotes change of row and the comma, change of column:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; A=numpy.matrix("1,2,3;4,5,6")</strong>
<strong>&gt;&gt;&gt; A</strong>
</pre></div><p>The output is shown a follows s:</p><div><pre class="programlisting"><strong>matrix([[1, 2, 3],</strong>
<strong>        [4, 5, 6]])</strong>
</pre></div><p>Let's look at another example:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; A=numpy.matrix([[1,2,3],[4,5,6]])</strong>
<strong>&gt;&gt;&gt; A</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>matrix([[1, 2, 3],</strong>
<strong>        [4, 5, 6]])</strong>
</pre></div><p>Another technique to create a matrix from a two-dimensional array is to enforce the matrix structure on a new object, copying the data of the former with the <code class="literal">asmatrix</code> routine.</p><p>A matrix<a id="id122" class="indexterm"/> is said to be sparse (<a class="ulink" href="http://en.wikipedia.org/wiki/Sparse_matrix">http://en.wikipedia.org/wiki/Sparse_matrix</a>) if most of its entries are zeros. It is a waste of <a id="id123" class="indexterm"/>memory to input such matrices in the usual way, especially if the dimensions are large. SciPy provides different procedures to store such matrices effectively in memory. Most of the usual methods to input sparse matrices are contemplated in SciPy as routines in the <code class="literal">scipy.sparse</code> module. Some of those methods are <strong>block sparse row</strong> (<code class="literal">bsr_matrix</code>), <strong>coordinate format</strong> (<code class="literal">coo_matrix</code>), compressed sparse column or row (<code class="literal">csc_matrix</code>, <code class="literal">csr_matrix</code>), sparse matrix with diagonal storage (<code class="literal">dia_matrix</code>), dictionary with <strong>Keys-based sorting</strong> (<code class="literal">dok_matrix</code>), and <strong>Row-based linked list</strong> (<code class="literal">lil_matrix</code>).</p><p>At this point, we would like to present one of these: the coordinate format. In this format, and given a sparse matrix <code class="literal">A</code>, we identify the coordinates of the nonzero elements, say <em>n</em> of them, and we create two n-dimensional <code class="literal">ndarray</code> arrays containing the columns and the rows of those entries, and a third <code class="literal">ndarray</code> containing the values of the corresponding entries. For instance, notice the following sparse matrix:</p><div><img src="img/7702OS_03_01.jpg" alt="Creating a matrix"/></div><p>The standard form of creating such matrices is as follows:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; A=numpy.matrix([ [0,10,0,0,0], [0,0,20,0,0], [0,0,0,30,0], [0,0,0,0,40], [0,0,0,0,0] ])</strong>
<strong>&gt;&gt;&gt; A</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>matrix([[ 0, 10,  0,  0,  0],</strong>
<strong>        [ 0,  0, 20,  0,  0],</strong>
<strong>        [ 0,  0,  0, 30,  0],</strong>
<strong>        [ 0,  0,  0,  0, 40],</strong>
<strong>        [ 0,  0,  0,  0,  0]])</strong>
</pre></div><p>A more memory-efficient way to create these matrices would be to properly store the nonzero elements. In this case, one of the nonzero entries is at the 1<sup>st</sup> row and 2<sup>nd</sup> column (or location <code class="literal">(0, 1)</code> in Python) with value, <code class="literal">10</code>. Another nonzero entry is at <code class="literal">(1, 2)</code> with value, <code class="literal">20</code>. A 3<sup>rd</sup> nonzero entry, with the value <code class="literal">30</code>, is located at <code class="literal">(2, 3)</code>. The last nonzero entry of <code class="literal">A</code> is located at <code class="literal">(3, 4)</code>, and has the value, <code class="literal">40</code>.</p><p>We then<a id="id124" class="indexterm"/> have <code class="literal">ndarray</code> of rows, <code class="literal">ndarray</code> of columns, and another <code class="literal">ndarray</code> of values:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; import numpy</strong>
<strong>&gt;&gt;&gt; rows=numpy.array([0,1,2,3])</strong>
<strong>&gt;&gt;&gt; cols=numpy.array([1,2,3,4])</strong>
<strong>&gt;&gt;&gt; vals=numpy.array([10,20,30,40])</strong>
</pre></div><p>We create the matrix <code class="literal">A</code> as follows:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; import scipy.sparse</strong>
<strong>&gt;&gt;&gt; A=scipy.sparse.coo_matrix( (vals,(rows,cols)) )</strong>
<strong>&gt;&gt;&gt; print (A); print (A.todense())</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>  (0, 1)  10</strong>
<strong>  (1, 2)  20</strong>
<strong>  (2, 3)  30</strong>
<strong>  (3, 4)  40</strong>
<strong>[[  0.  10   0.   0.   0.]</strong>
<strong> [  0.   0.  20   0.   0.]</strong>
<strong> [  0.   0.   0.  30   0.]</strong>
<strong> [  0.   0.   0.   0.  40]]</strong>
</pre></div><p>Notice how the <code class="literal">todense</code> method turns sparse matrices into full matrices. Also note that it obviates any row or column of full zeros following the last nonzero element.</p><p>Associated to each input method, we have functions that identify sparse matrices of each kind. For instance, if we suspect that <code class="literal">A</code> is a sparse matrix in the <code class="literal">coo_matrix</code> format, we may use the following command:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; scipy.sparse.isspmatrix_coo(A)</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>True</strong>
</pre></div><p>All the array routines are cast to matrices, provided the input is a matrix. This is very convenient for matrix creation, especially thanks to stacking commands (<code class="literal">hstack</code>, <code class="literal">vstack</code>, <code class="literal">tile</code>). Besides these, matrices enjoy one more amazing stacking command, <code class="literal">bmat</code>. This routine allows the stacking of matrices by means of strings, making use of the convention: semicolon for change of row and comma for change of column. Also, it allows matrix names inside of the string to be evaluated. The following example is enlightening:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; B=numpy.mat(numpy.ones((3,3)))</strong>
<strong>&gt;&gt;&gt; W=numpy.mat(numpy.zeros((3,3)))</strong>
<strong>&gt;&gt;&gt; print (numpy.bmat('B,W;W,B'))</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>[[ 1.  1.  1.  0.  0.  0.]</strong>
<strong> [ 1.  1.  1.  0.  0.  0.]</strong>
<strong> [ 1.  1.  1.  0.  0.  0.]</strong>
<strong> [ 0.  0.  0.  1.  1.  1.]</strong>
<strong> [ 0.  0.  0.  1.  1.  1.]</strong>
<strong> [ 0.  0.  0.  1.  1.  1.]]</strong>
</pre></div><p>The main <a id="id125" class="indexterm"/>difference between arrays and matrices is in regards to the behavior of the product of two objects of the same type. For example, multiplication between two arrays means <em>element-wise multiplication of the entries of the two arrays</em> and requires two objects of the same shape. The following code snippet is an example of multiplication between two arrays:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; a=numpy.array([[1,2],[3,4]])</strong>
<strong>&gt;&gt;&gt; a*a</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>array([[ 1,  4],</strong>
<strong>       [ 9, 16]])</strong>
</pre></div><p>On the other hand, matrix multiplication requires a first matrix with shape (<em>m</em>, <em>n</em>), and a second matrix with shape (<em>n</em>, <em>p</em>)—the number of columns in the first matrix must be the same as the number of rows in the second matrix. This operation offers a new matrix of shape (<em>m</em>, <em>p</em>), as shown in the following diagram:</p><div><img src="img/7702OS_03_02.jpg" alt="Creating a matrix"/></div><p>The following is the code snippet:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; A=numpy.mat(a)</strong>
<strong>&gt;&gt;&gt; A*A</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>matrix([[ 7, 10],</strong>
<strong>        [15, 22]])</strong>
</pre></div><p>Alternatively, to obtain the matrix product between two conforming matrices as <code class="literal">ndarray</code> objects, we don't really need to transform the <code class="literal">ndarray</code> object to a matrix object if not needed. The matrix product could be obtained directly via the <code class="literal">numpy.dot</code> function introduced earlier in the <em>Scalar/Dot product</em> section of this chapter. Let's take a look at the following <code class="literal">numpy.dot</code> command example: </p><div><pre class="programlisting"><strong>&gt;&gt;&gt; b=numpy.array([[1,2,3],[3,4,5]])</strong>
<strong>&gt;&gt;&gt; numpy.dot(a,b)</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>array([[ 7, 10, 13],</strong>
<strong>       [15, 22, 29]])</strong>
</pre></div><p>If we<a id="id126" class="indexterm"/> desire to perform an element-wise multiplication of the elements of two matrices, we can do so with the versatile <code class="literal">numpy.multiply</code> command, as follows:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; numpy.multiply(A,A)</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>matrix([[ 1,  4],</strong>
<strong>        [ 9, 16]])</strong>
</pre></div><p>The other difference between arrays and matrices worth noticing is in regard to their shapes. While we allow arrays to have one dimension; their corresponding matrices must have at least two. This is very important to have in mind when we transpose either object. Let's take a look at the following code snippet implementing <code class="literal">shape()</code> and <code class="literal">transpose()</code> commands:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; a=numpy.arange(5); A=numpy.mat(a)</strong>
<strong>&gt;&gt;&gt; a.shape, A.shape, a.transpose().shape, A.transpose().shape</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>((5,), (1, 5), (5,), (5, 1))</strong>
</pre></div><p>As it has been shown, SciPy offers quite a number of basic tools to instantiate and manipulate matrices, with many related methods to follow. This also allows us to speed up computations in the cases where special matrices are used.</p><p>The <code class="literal">scipy.linalg</code> module provides commands to create special matrices such as block diagonal matrices from provided arrays (<code class="literal">block_diag</code>), <strong>circulant matrices</strong> (circulant), companion matrices (<code class="literal">companion</code>), <strong>Hadamard matrices</strong> (<code class="literal">hadamard</code>), <strong>Hankel matrices</strong> (<code class="literal">hankel</code>), <strong>Hilbert</strong> and <strong>inverse Hilbert matrices</strong> (<code class="literal">hilbert</code>, <code class="literal">invhilbert</code>), <strong>Leslie matrices</strong> (<code class="literal">leslie</code>), <strong>square Pascal matrices</strong> (<code class="literal">pascal</code>), <strong>Toeplitz matrices</strong> (<code class="literal">toeplitz</code>), <strong>lower-triangular matrices</strong> (<code class="literal">tril</code>), and <strong>upper-triangular matrices</strong> (<code class="literal">triu</code>).</p><p>Let's see an example on <strong>optimal weighings</strong>.</p><p>Suppose <a id="id127" class="indexterm"/>we are given <em>p</em> objects to be weighed in <em>n</em> weighings with a two-pan balance. We create an <em>n</em> x <em>p</em> matrix of plus and minus one, where a positive value in the <em>(i, j)</em> position indicates that the <em>j<sup>th</sup></em> object is placed in the left pan of the balance in the <em>i<sup>th</sup></em> weighing and a negative value that the <em>j<sup>th</sup></em> object corresponding is in the right pan.</p><p>It is known that optimal weighings are designed by submatrices of Hadamard matrices. For the problem of designing an optimal weighing for eight objects with three weighings, we could then explore different choices of three rows of a Hadamard matrix of order eight. The only requirement is that the sum of the elements on the row of the matrix is zero (so that the same number of objects are placed on each pan). Through slicing, we can<a id="id128" class="indexterm"/> accomplish just that:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; import scipy.linalg</strong>
<strong>&gt;&gt;&gt; A=scipy.linalg.hadamard(8)</strong>
<strong>&gt;&gt;&gt; zero_sum_rows = (numpy.sum(A,0)==0)</strong>
<strong>&gt;&gt;&gt; B=A[zero_sum_rows,:]</strong>
<strong>&gt;&gt;&gt; print (B[0:3,:])</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>[[ 1 -1  1 -1  1 -1  1 -1]</strong>
<strong> [ 1  1 -1 -1  1  1 -1 -1]</strong>
<strong> [ 1 -1 -1  1  1 -1 -1  1]]</strong>
</pre></div><p>The <code class="literal">scipy.sparse</code> module has its own set of special matrices. The most common are matrices of those along diagonals (<code class="literal">eye</code>), identity matrices (<code class="literal">identity</code>), matrices from diagonals (<code class="literal">diags</code>, <code class="literal">spdiags</code>), block diagonal matrices from sparse matrices (<code class="literal">block_diag</code>), matrices from sparse sub-blocks (<code class="literal">bmat</code>), column-wise and row-wise stacks (<code class="literal">hstack</code>, <code class="literal">vstack</code>), and random matrices of a given shape and density with uniformly distributed values (<code class="literal">rand</code>).</p></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec26"/>Matrix methods</h1></div></div></div><p>Besides<a id="id129" class="indexterm"/> inheriting all the array methods, matrices enjoy four extra attributes: <code class="literal">T</code> for transpose, <code class="literal">H</code> for conjugate transpose, <code class="literal">I</code> for inverse, and <code class="literal">A</code> to cast as <code class="literal">ndarray</code>:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; A = numpy.matrix("1+1j, 2-1j; 3-1j, 4+1j")</strong>
<strong>&gt;&gt;&gt; print (A.T); print (A.H)</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>[[ 1.+1.j  3.-1.j]</strong>
<strong> [ 2.-1.j  4.+1.j]]</strong>
<strong>[[ 1.-1.j  3.+1.j]</strong>
<strong> [ 2.+1.j  4.-1.j]]</strong>
</pre></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec22"/>Operations between matrices</h2></div></div></div><p>We have<a id="id130" class="indexterm"/> briefly covered the most basic operation between two matrices; the matrix product. For any other kind of product, we resort to the basic utilities in the NumPy libraries, as: dot product for arrays or vectors (<code class="literal">dot</code>, <code class="literal">vdot</code>), inner and outer products of two arrays (<code class="literal">inner</code>, <code class="literal">outer</code>), <strong>tensor dot product</strong> along specified axes (<code class="literal">tensordot</code>), or the <strong>Kronecker product</strong> of two arrays (<code class="literal">kron</code>).</p><p>Let's see an <a id="id131" class="indexterm"/>example of creating an <strong>orthonormal</strong> basis.</p><p>Create <a id="id132" class="indexterm"/>an orthonormal basis in the nine-dimensional real space from an orthonormal basis of the three-dimensional real space.</p><p>Let's choose, for example, the orthonormal basis formed by the vectors as shown in following diagram:</p><div><img src="img/7702OS_03_03.jpg" alt="Operations between matrices"/></div><p>We compute the desired basis by collecting these vectors in a matrix and using a Kronecker product, as follows:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; import numpy</strong>
<strong>&gt;&gt;&gt;</strong><strong> import scipy.linalg</strong>
<strong>&gt;&gt;&gt; mu = 1/numpy.sqrt(2)</strong>
<strong>&gt;&gt;&gt; A = numpy.matrix([[mu,0,mu],[0,1,0],[mu,0,-mu]])</strong>
<strong>&gt;&gt;&gt; B = scipy.linalg.kron(A,A)</strong>
</pre></div><p>The columns of matrix <code class="literal">B</code> shown previously, give us an orthonormal basis directly. For instance, the vectors with odd indices would be the columns of the following submatrix:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; print (B[:,0:-1:2])</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>[[ 0.5  0.5  0.   0.5]</strong>
<strong> [ 0.   0.   0.   0. ]</strong>
<strong> [ 0.5 -0.5  0.   0.5]</strong>
<strong> [ 0.   0.   0.   0. ]</strong>
<strong> [ 0.   0.   1.   0. ]</strong>
<strong> [ 0.  -0.   0.   0. ]</strong>
<strong> [ 0.5  0.5  0.  -0.5]</strong>
<strong> [ 0.   0.   0.  -0. ]</strong>
<strong> [ 0.5 -0.5  0.  -0.5]]</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec23"/>Functions on matrices</h2></div></div></div><p>The <code class="literal">scipy.linalg</code> module offers a useful set of functions on matrices. The basic two commands <a id="id133" class="indexterm"/>on square matrices are <code class="literal">inv</code> (for the inverse of a matrix) and <code class="literal">det</code> (for the determinant). The power of a square matrix is given by the standard exponentiation; that is, if <code class="literal">A</code> is a square matrix, then <code class="literal">A**2</code> indicates the matrix product <code class="literal">A*A</code>, which is shown in the following code snippet:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; A=numpy.matrix("1,1j;21,3")</strong>
<strong>&gt;&gt;&gt; A; A*A; A**2</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>matrix([[  1.+0.j,   0.+1.j],</strong>
<strong>        [ 21.+0.j,   3.+0.j]])</strong>
<strong>matrix([[  1.+21.j,   0. +4.j],</strong>
<strong>        [ 84. +0.j,   9.+21.j]])</strong>
<strong>matrix([[  1.+21.j,   0. +4.j],</strong>
<strong>        [ 84. +0.j,   9.+21.j]])</strong>
</pre></div><p>It should be pointed out that as a type array, the product of <code class="literal">A*A</code> (or <code class="literal">A**2</code>) is calculated by squaring each element of the array:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; numpy.asarray(A); numpy.asarray(A)*numpy.asarray(A); numpy.asarray(A)**2</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>array([[  1.+0.j,   0.+1.j],</strong>
<strong>       [ 21.+0.j,   3.+0.j]])</strong>
<strong>array([[   1.+0.j,   -1.+0.j],</strong>
<strong>       [ 441.+0.j,    9.+0.j]])</strong>
<strong>array([[   1.+0.j,   -1.+0.j],</strong>
<strong>       [ 441.+0.j,    9.+0.j]])</strong>
</pre></div><p>More advanced commands compute matrix functions that rely on the power series representation of expressions involving matrix powers, such as the matrix <strong>exponential</strong> (for which there are three possibilities—<code class="literal">expm</code>, <code class="literal">expm2</code>, and <code class="literal">expm3</code>), the matrix <strong>logarithm</strong> (<code class="literal">logm</code>), matrix <strong>trigonometric functions</strong> (<code class="literal">cosm</code>, <code class="literal">sinm</code>, <code class="literal">tanm</code>), matrix <strong>hyperbolic trigonometric functions</strong> (<code class="literal">coshm</code>, <code class="literal">sinhm</code>, <code class="literal">tanhm</code>), the <strong>matrix sign function</strong> (<code class="literal">signm</code>), or the matrix <strong>square root</strong> (<code class="literal">sqrtm</code>).</p><p>Notice the difference between the application of the normal exponential function on a matrix, and the result of a matrix exponential function. </p><p>In the former case, we obtain the application of <code class="literal">numpy.exp</code> to each entry of the matrix; in the latter, we actually compute the exponential of the matrix following the power series representation:</p><div><img src="img/7702OS_03_04.jpg" alt="Functions on matrices"/></div><p>The preceding formula is illustrated in this code snippet:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; import numpy</strong>
<strong>&gt;&gt;&gt; import scipy.linalg</strong>
<strong>&gt;&gt;&gt; a=numpy.arange(0,2*numpy.pi,1.6)</strong>
<strong>&gt;&gt;&gt; A = scipy.linalg.toeplitz(a)</strong>
<strong>&gt;&gt;&gt; print (A)</strong>
</pre></div><p>The output<a id="id134" class="indexterm"/> is shown as follows:</p><div><pre class="programlisting"><strong>[[ 0.   1.6  3.2  4.8]</strong>
<strong> [ 1.6  0.   1.6  3.2]</strong>
<strong> [ 3.2  1.6  0.   1.6]</strong>
<strong> [ 4.8  3.2  1.6  0. ]]</strong>
</pre></div><p>Let's perform the <code class="literal">exp()</code> operation on <code class="literal">A</code>:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; print (numpy.exp(A))</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>[[   1.            4.95303242   24.5325302   121.51041752]</strong>
<strong> [   4.95303242    1.            4.95303242   24.5325302 ]</strong>
<strong> [  24.5325302     4.95303242    1.            4.95303242]</strong>
<strong> [ 121.51041752   24.5325302     4.95303242    1.        ]]</strong>
</pre></div><p>Let's perform the <code class="literal">expm()</code> operation on <code class="literal">A</code>:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; print (scipy.linalg.expm(A))</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>[[ 1271.76972856   916.49316549   916.63015271  1271.70874469]</strong>
<strong> [  916.49316549   660.86560972   660.5306514    916.63015271]</strong>
<strong> [  916.63015271   660.5306514    660.86560972   916.49316549]</strong>
<strong> [ 1271.70874469   916.63015271   916.49316549  1271.76972856]]</strong>
</pre></div><p>For sparse square matrices, we have an optimized inverse function, as well as a matrix exponential—<code class="literal">scipy.sparse.linalg.inv</code>, <code class="literal">scipy.sparse.linalg.expm</code>.</p><p>For general matrices, we have the basic norm function (norm), as well as two versions of the <strong>Moore-Penrose pseudoinverse</strong> (<code class="literal">pinv</code> and <code class="literal">pinv2</code>).</p><p>Once again, we need to emphasize how important it is to rely on these functions, rather than coding their equivalent expressions manually. For instance, note the <code class="literal">norm</code> computation of vectors or matrices, <code class="literal">scipy.linalg.norm</code>. Let's show you, by example, the 2-norm of a two-dimensional vector <code class="literal">v=numpy.matrix([x,y])</code>, where at least one of the <code class="literal">x</code> and <code class="literal">y</code> values is extremely large—large enough so that <code class="literal">x*x</code> overflows:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; import numpy</strong>
<strong>&gt;&gt;&gt; import scipy.linalg</strong>
<strong>&gt;&gt;&gt; x=10**100; y=9; v=numpy.matrix([x,y])</strong>
<strong>&gt;&gt;&gt; scipy.linalg.norm(v,2)</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>1e+100</strong>
</pre></div><p>Now, let's<a id="id135" class="indexterm"/> perform the <code class="literal">sqrt()</code> operation:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; numpy.sqrt(x*x+y*y)</strong>
</pre></div><p>The output is an error which is shown as follows:</p><div><pre class="programlisting"><strong>Traceback (most recent call last)</strong>
<strong>  File "&lt;stdin&gt;", line 1, in &lt;module&gt;</strong>
<strong>AttributeError: 'long' object has no attribute 'sqrt'</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec24"/>Eigenvalue problems and matrix decompositions</h2></div></div></div><p>Another <a id="id136" class="indexterm"/>set of operations heavily used on matrices<a id="id137" class="indexterm"/> is to compute and handle eigenvalues and eigenvectors<a id="id138" class="indexterm"/> of square matrices. These two <a id="id139" class="indexterm"/>problems rank among the most complex operations that we can perform on square matrices, and extensive research has been put in place to obtain good algorithms with low complexity and optimal usage of memory resources. SciPy has state-of-the-art code to implement these ideas.</p><p>For the computation of eigenvalues, the <code class="literal">scipy.linalg</code> module provides three routines: <code class="literal">eigvals</code> (for any ordinary or general eigenvalue problem), <code class="literal">eigvalsh</code> (if the matrix is symmetric of complex <strong>Hermitian</strong>), and <code class="literal">eigvals_banded</code> (if the matrix is banded). To compute the eigenvectors, we similarly have three corresponding choices: <code class="literal">eig</code>, <code class="literal">eigh</code>, and <code class="literal">eigh_banded</code>.</p><p>The syntax used in all cases is very similar. For example, for the general case of eigenvalues, we use the following line of code where matrix <code class="literal">A</code> must be square:</p><div><pre class="programlisting">eigvals(A, B=None, overwrite_a=False)</pre></div><p>This should be the only parameter passed to the routine if we wish to solve an ordinary eigenvalue problem. If we wish to generalize this, we may provide an extra square matrix (of the same dimensions as matrix <code class="literal">A</code>). This is passed in the <code class="literal">B</code> parameter.</p><p>The module also offers an extensive collection of functions that compute different decompositions of matrices, as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Pivoted LU decomposition</strong>: This <a id="id140" class="indexterm"/>function allows us to use the <code class="literal">lu</code> and <code class="literal">lufactor</code> commands.</li><li class="listitem" style="list-style-type: disc"><strong>Singular value decomposition</strong>: This <a id="id141" class="indexterm"/>function allows us to use the <code class="literal">svd</code> command. To compute the singular values, we issue <code class="literal">svdvals</code>. If we wish to compose the sigma matrix in the singular value decomposition from its singular values, we do so with the <code class="literal">diagsvd</code> routine. If we wish to compute an orthogonal basis for the range of a matrix using SVD, we can accomplish this with the <code class="literal">orth</code> command.</li><li class="listitem" style="list-style-type: disc"><strong>Cholesky decomposition</strong>: This function<a id="id142" class="indexterm"/> allows us to use the <code class="literal">cholesky</code>, <code class="literal">cholesky_banded</code>, and <code class="literal">cho_factor</code> commands.</li><li class="listitem" style="list-style-type: disc"><strong>QR and QZ decompositions</strong>: This function allows us to use the <code class="literal">qr</code> and <code class="literal">qz</code> commands. If we wish to multiply a matrix with the matrix Q of a decomposition, we<a id="id143" class="indexterm"/> use<a id="id144" class="indexterm"/> the syntactic sugar <code class="literal">qr_multiply</code>, rather than performing this procedure in two steps.</li><li class="listitem" style="list-style-type: disc"><strong>Schur and Hessenberg decompositions</strong>: This<a id="id145" class="indexterm"/> function<a id="id146" class="indexterm"/> allows us to use the <code class="literal">schur</code> and <code class="literal">Hessenberg</code> commands. If we wish to convert a real Schur form to complex, we have the <code class="literal">rsf2csf</code> routine.</li></ul></div><p>At this <a id="id147" class="indexterm"/>point, we have an interesting application—image<a id="id148" class="indexterm"/> compression, which makes use of some <a id="id149" class="indexterm"/>of the routines explained so far.</p></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec25"/>Image compression via the singular value decomposition</h2></div></div></div><p>This is a very simple application where a square image <code class="literal">A</code> of size <em>n </em>x<em> n</em>, and stored as <code class="literal">ndarray</code> is regarded as a matrix, and where a <a id="id150" class="indexterm"/>singular<a id="id151" class="indexterm"/> value decomposition (SVD) is<a id="id152" class="indexterm"/> performed on it. This operation is visible in the following diagram:</p><div><img src="img/7702OS_03_05.jpg" alt="Image compression via the singular value decomposition"/></div><p>From all the singular values of <code class="literal">s</code> we choose a fraction, together with their corresponding left and right singular vectors <code class="literal">u</code>, <code class="literal">v</code>. We compute a new matrix by collecting them according to the formula given in the following diagram:</p><div><img src="img/7702OS_03_06.jpg" alt="Image compression via the singular value decomposition"/></div><p>Note, for example, the similarity between the original (512 singular values) and an approximation using only 32 singular values:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; import numpy</strong>
<strong>&gt;&gt;&gt; import scipy.misc</strong>
<strong>&gt;&gt;&gt; from scipy.linalg import svd</strong>
<strong>&gt;&gt;&gt; import matplotlib.pyplot as plt</strong>
<strong>&gt;&gt;&gt; img=scipy.misc.lena()</strong>
<strong>&gt;&gt;&gt; U,s,Vh=svd(img)      # Singular Value Decomposition</strong>
<strong>&gt;&gt;&gt; A = numpy.dot( U[:,0:32],  # use only 32 singular values</strong>
<strong>        numpy.dot( numpy.diag(s[0:32]),</strong>
<strong>                   Vh[0:32,:]))</strong>
<strong>&gt;&gt;&gt; plt.subplot(121,aspect='equal'); plt.imshow(img); plt.gray()</strong>
<strong>&gt;&gt;&gt; plt.subplot(122,aspect='equal'); plt.imshow(A)</strong>
<strong>&gt;&gt;&gt; plt.show()</strong>
</pre></div><p>This <a id="id153" class="indexterm"/>produces<a id="id154" class="indexterm"/> the<a id="id155" class="indexterm"/> following images, of which the picture to the left is the original image and the picture to the right, the approximation using 32 singular values:</p><div><img src="img/7702OS_03_07.jpg" alt="Image compression via the singular value decomposition"/></div><p>Using the <code class="literal">svd</code> approximation we managed to compress the original image of 262,144 coefficients (512 * 512)to only 32,800 coefficients ((2 * 32 * 512) + 32), or to one-eighth of the original information.</p></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec26"/>Solvers</h2></div></div></div><p>One of the<a id="id156" class="indexterm"/> fundamental applications of linear algebra is to solve large systems of linear equations. For the basic systems of the form <em>Ax=b</em>, for any square matrix <code class="literal">A</code> and general matrix <code class="literal">b</code> (with as many rows as columns in <code class="literal">A</code>), we have two generic methods to find <em>x</em> (<code class="literal">solve</code> for dense matrices and <code class="literal">spsolve</code> for sparse matrices), using the following syntax:</p><div><pre class="programlisting">solve(A, b, sym_pos=False, lower=False, overwrite_a=False, overwrite_b=False, debug=False)
spsolve(A, b[, permc_spec, use_umfpack])</pre></div><p>There are solvers that are even more sophisticated in SciPy, with enhanced performance for situations in which the structure of the matrix <code class="literal">A</code> is known. For dense matrices we have three commands in the <code class="literal">scipy.linalg</code> module: <code class="literal">solve_banded</code> (for banded matrices), <code class="literal">solveh_banded</code> (if besides banded, <code class="literal">A</code> is Hermitian), and <code class="literal">solve_triangular</code> (for triangular matrices).</p><p>When a solution is not possible (for example, if <code class="literal">A</code> is a singular matrix), it is still possible to obtain a matrix <em>x</em> that minimizes the <code class="literal">norm</code> of <em>b-Ax</em> in a least-squares sense. We can compute such a matrix with the <code class="literal">lstsq</code> command, which has the following syntax:</p><div><pre class="programlisting">lstsq(A, b, cond=None, overwrite_a=False, overwrite_b=False)</pre></div><p>The output of this function is a tuple that contains the following:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The solution found (as <code class="literal">ndarray</code>)</li><li class="listitem" style="list-style-type: disc">The sum of residues (as another <code class="literal">ndarray</code>)</li><li class="listitem" style="list-style-type: disc">The effective rank of the matrix <code class="literal">A</code></li><li class="listitem" style="list-style-type: disc">The singular values of the matrix <code class="literal">A</code> (as another <code class="literal">ndarray</code>)</li></ul></div><p>Let's illustrate this routine with a simple example, to solve the following system:</p><div><img src="img/7702OS_03_08.jpg" alt="Solvers"/></div><p>The following is the code snippet:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; import numpy</strong>
<strong>&gt;&gt;&gt; import scipy.linalg</strong>
<strong>&gt;&gt;&gt; A=numpy.mat(numpy.eye(3,k=1))</strong>
<strong>&gt;&gt;&gt; print(A)</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>[[ 0.  1.  0.]</strong>
<strong> [ 0.  0.  1.]</strong>
<strong> [ 0.  0.  0.]]</strong>
</pre></div><p>Let's move further into the code and perform the following operations on <code class="literal">b</code>:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; b=numpy.mat(numpy.arange(3) + 1).T</strong>
<strong>&gt;&gt;&gt; print(b)</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>[[1]</strong>
<strong> [2]</strong>
<strong> [3]]</strong>
</pre></div><p>Further, let's perform the <code class="literal">lstsq</code> operation:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; xinfo=scipy.linalg.lstsq(A,b)</strong>
<strong>&gt;&gt;&gt; print (xinfo[0].T)      # output the solution</strong>
</pre></div><p>The<a id="id157" class="indexterm"/> output is shown as follows:
 </p><div><pre class="programlisting"><strong>[[ 0.  1.  2.]]</strong>
</pre></div><p>The <code class="literal">overwrite_</code> options are designed to enhance performance of the algorithms, and should be used carefully, since they destroy the original data.</p><p>The truly fastest solvers in SciPy are based upon decomposition of matrices. Reducing the system into something simpler easily solves huge and really complicated systems of linear equations. We may accomplish this using decomposition techniques presented in the <em>Eigenvalue problems and matrix decompositions</em> and <em>Image compression via the singular value decomposition</em> subsections under the <em>Matrix methods</em> section of this chapter, but of course, the SciPy philosophy is to help us deal with all nuisances of memory and resources internally. To this end, the module also has the <code class="literal">lu_solve</code> (for solutions based on LU decompositions), and <code class="literal">cho_solve</code>, <code class="literal">cho_solve_banded</code> (for solutions based on Cholesky decompositions).</p><p>Finally, you will also find solvers for very complex matrix equations—the <strong>Sylvester</strong> equation (<code class="literal">solve_sylvester</code>), both the continuous and discrete algebraic <strong>Riccati</strong> equations (<code class="literal">solve_continuous_are</code>, <code class="literal">solve_discrete_are</code>) and both the continuous and discrete <strong>Lyapunov</strong> equations (<code class="literal">solve_discrete_lyapunov</code>, <code class="literal">solve_lyapunov</code>).</p><p>Most of the matrix decompositions and solutions to eigenvalue problems are contemplated for sparse matrices in the <code class="literal">scipy.sparse.linalg</code> module with a similar naming convention, but with much more robust use of computer resources and error control.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec27"/>Summary</h1></div></div></div><p>This chapter explored the treatment of vectors, matrices (whether normal or sparse) with the modules on linear algebra—<code class="literal">linalg</code> and <code class="literal">sparse.linalg</code>, which expand and improve the NumPy module with the same name.</p><p>In <a class="link" href="ch04.html" title="Chapter 4. SciPy for Numerical Analysis">Chapter 4</a>, <em>SciPy for Numerical Analysis</em>, we will continue discussing details of the options available in SciPy to perform numerical computations efficiently, will cover how to evaluate special functions found in applied mathematics and mathematical physics problems. This will be discussed in details of doing regression, interpolation and optimization via SciPy.</p></div></body></html>