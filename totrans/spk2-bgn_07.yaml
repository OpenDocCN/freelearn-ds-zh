- en: Chapter 7.  Spark Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Calculations based on formulas or algorithms have been used commonly since ancient
    times to find the output for a given input. But without knowing the formulas or
    algorithms, computer scientists and mathematicians devised methods to generate
    formulas or algorithms based on an existing input/output dataset and predicted
    the output of a new input data based on the generated formulas or algorithms.
    Generally, this process of learning from a dataset and doing predictions based
    on the learning is known as machine learning. Machine learning originates from
    the study of artificial intelligence in computer science.
  prefs: []
  type: TYPE_NORMAL
- en: Practical machine learning has numerous applications that are being consumed
    by laymen on a daily basis. YouTube users now get suggestions for the next items
    to be played in the playlist based on the video they are currently viewing. Popular
    movie rating sites give ratings and recommendations based on the user preferences
    of movie genres. Social media web sites such as Facebook suggest a list of names
    of the users' friends for easy tagging of pictures. What Facebook is doing here
    is classifying pictures by the names that are already available in the existing
    albums and checking whether the newly added picture has any similarities with
    the existing ones. If it finds a similarity, it suggests the name. The applications
    of this kind of picture identification are manifold. The way all these applications
    work is based on the huge amount of input/output datasets that have already been
    collected and the learning that has been done based on those datasets. When a
    new input dataset arrives, a prediction is made by making use of the learning
    that the computer or machine has already done.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark for machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model persistence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spam filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding synonyms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In traditional computing, input data is fed to a program to generate output.
    But in machine learning, input data and output data are fed to a machine learning
    algorithm to generate a function or program that can be used to predict the output
    of an input according to the learning done on the input/output dataset fed to
    the machine learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The data available in the wild may be classified into groups, it may form clusters,
    or it may fit into certain relationships. These are different kinds of machine
    learning problem. For example, if there is a databank of pre-owned car sale prices
    with its associated attributes or features, it is possible to predict the price
    of a car just by knowing the associated attributes or features. Regression algorithms
    are used to solve these kinds of problem. If there is a databank of spam and non-spam
    e-mails, then when a new e-mail comes, it is possible to predict whether the new
    e-mail is spam or non-spam. Classification algorithms are used to solve these
    kinds of problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are just a few machine learning algorithm types. But in general, when
    using a bank of data, if it is necessary to apply a machine learning algorithm
    and use that model to make predictions, then the data should be divided into features
    and outputs. For example, in the case of the car price prediction problem, price
    is the output, and here are some of the possible features of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: Car make
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Car model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Year of manufacture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mileage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fuel type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gearbox type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So whichever machine learning algorithm is being used, there will be a set of
    features and one or more outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many books and publications use the term *label* for output. In other words,
    *features* are the input and *label* is the output.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 1* depicts the way a machine learning algorithm works on the underlying
    data to enable predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding machine learning](img/image_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1
  prefs: []
  type: TYPE_NORMAL
- en: Data comes in various shapes and forms. Depending on the machine learning algorithm
    used, the training data has to be pre-processed to have the features and labels
    in the right format to be fed to the machine learning algorithm. That in turn
    generates the appropriate hypothesis function, which takes the features as the
    input and produces the predicted label.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dictionary definition of the word hypothesis is a supposition or proposed
    explanation made on the basis of limited evidence as a starting point for further
    investigation. Here, the function or program that is generated by the machine
    learning algorithm is based on the limited evidence that is the training data
    fed to the machine learning algorithm, and hence it is widely known as hypothesis
    function.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, this hypothesis function is not a definitive function that produces
    consistent results all the time with all types of input data. It is rather a function
    based on the training data. When a new piece of data is added to the training
    dataset, re-learning is required, and at that time even the hypothesis function
    generated will change accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: In reality, the flow given in *Figure 1* is not as simple as it seems. Once
    the model is trained, a lot of testing has to be done on the model to test predictions
    with known labels. The chain of train and test processes is an iterative process,
    and in each iteration the parameters of the algorithm are tweaked to make the
    prediction quality better. Once an acceptable test result is produced by the model,
    the model can be moved to production for doing the live prediction needs. Spark
    comes with a machine learning librarythat is rich with capabilities to make practical
    machine learning a reality.
  prefs: []
  type: TYPE_NORMAL
- en: Why Spark for machine learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous chapters covered various data processing functionalities of Spark
    in detail. Spark's machine learning library uses many Spark core functionalities
    as well as Spark libraries such as Spark SQL. The Spark machine learning library
    makes machine learning application development easy by combining data processing
    and machine learning algorithm implementations in a unified framework with the
    ability to do data processing on a cluster of nodes, combined with ability to
    read and write data to a variety of data formats.
  prefs: []
  type: TYPE_NORMAL
- en: Spark comes with two flavors of the machine learning library. They are `spark.mllib`
    and `spark.ml`. The first one is developed on top of Spark's RDD abstraction,
    and the second one is developed on top of Spark's DataFrame abstraction. It is
    recommended to use the spark.ml library for any future machine learning application
    developments.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is going to focus only on the spark.ml machine learning library.
    The following list explains terminology and concepts that are used again and again
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Estimator**: This is an algorithm that works on top of a Spark DataFrame
    containing features and labels. It trains on the data provided in the Spark DataFrame
    and creates a model. This model is used to do future predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformer**: This converts a Spark DataFrame containing features and transforms
    it to another Spark DataFrame containing predictions. The model created by an
    Estimator is a Transformer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parameter**: This is to be used by the Estimators and Transformers. Often,
    this is specific to the machine learning algorithm. Spark machine learning library
    comes with a uniform API for specifying the right parameters to the algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline**: This is a chain of Estimators and Transformers working together
    forming a machine learning workflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these new terms are slightly difficult to understand in a theoretical perspective
    but if an example is given, the concepts will become much clearer.
  prefs: []
  type: TYPE_NORMAL
- en: Wine quality prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The University of California Irvine Machine Learning Repository ([http://archive.ics.uci.edu/ml/index.html](http://archive.ics.uci.edu/ml/index.html))
    provides a lot of datasets as a service to those who are interested in learning
    about machine learning. The Wine Quality Dataset ([http://archive.ics.uci.edu/ml/datasets/Wine+Quality](http://archive.ics.uci.edu/ml/datasets/Wine+Quality))
    is being used here to demonstrate some machine learning applications. It contains
    two datasets with various features of white and red wines from Portugal.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Wine Quality Dataset download link lets you download the datasets for red
    wine and white wine as two separate CSV files. Once those files are downloaded,
    edit the two datasets to remove the first header line containing the column names.
    This is to let the programs parse the numerical data without errors. Detailed
    error handling and excluding the header record are avoided on purpose to focus
    on the machine learning functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset containing various features of red wine is used in this wine quality
    prediction use case. The following are the features of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Fixed acidity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Volatile acidity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Citric acid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Residual sugar
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chlorides
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Free sulfur dioxide
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total sulfur dioxide
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pH
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sulphates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alcohol
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on these features, the quality (score between 0 and 10) is determined.
    Here, quality is the label of this dataset. Using this dataset, a model is going
    to be trained and then, using the trained model, testing is done and predictions
    are made. This is a regression problem. The Linear Regression algorithm is used
    to train the model. The Linear Regression algorithm generates a linear hypothesis
    function. In mathematical terms, a linear function is a polynomial of degree one
    or less. In this machine learning application use case, it deals with modeling
    the relationship between a dependent variable (wine quality) and a set of independent
    variables (the features of the wine).
  prefs: []
  type: TYPE_NORMAL
- en: 'At the Scala REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code does a lot of things. It performs the following chain of
    activities in a pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: It reads the wine data from the data file to form a training DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then it creates a `LinearRegression` object and sets the parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It fits the model with the training data and this completes the estimator pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It creates a DataFrame containing test data. Typically, the test data will have
    both features and labels. This is to make sure that the model is right and used
    for comparing the predicted label and actual label.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the model created, it does a transformation with the test data, and from
    the DataFrame produced, extracts the features, input labels, and predictions.
    Note that while doing the transformation using the model, the labels are not required.
    In other words, the labels will not be used at all.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the model created, it does a transformation with the prediction data and
    from the DataFrame produced, extracts the features and predictions. Note that
    while doing the transformation using the model, the labels are not used. In other
    words, the labels are not used while doing the predictions. This completes a transformer
    pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The pipelines in the preceding code snippet are single stage pipelines, and
    for this reason there is no need to use the Pipeline object. Multiple stage pipelines
    are going to be discussed in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fitting/testing phases are repeated iteratively in real-world use cases
    until the model is giving the desired results when doing predictions. Figure 2
    elucidates the pipeline concept that is demonstrated through the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Wine quality prediction](img/image_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code demonstrates the same use case using Python. At the Python
    REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned earlier, linear regression is statistical model and an approach
    for modeling the relationship between two types of variable. One is an independent
    variable, and the other is a dependent variable. The dependent variable is computed
    from the independent variables. In many cases, if there is only one independent
    variable, then the regression will be a simple linear regression. But in reality,
    in practical real-world use cases, there will be multitude of independent variables,
    just like in the wine dataset. This falls into the case of multiple linear regressions.
    This should not be confused with multivariate linear regression. In multivariate
    regression, multiple and correlated dependent variables are predicted.
  prefs: []
  type: TYPE_NORMAL
- en: In the use case that is being discussed here, the prediction is only done for
    one variable, which is the quality of the wine and hence it is a multiple linear
    regression and not a multivariate linear regression problem. Some schools even
    use multiple linear regression as univariate linear regression. In other words,
    irrespective of the number of independent variables, if there is only one dependent
    variable, it is termed as univariate linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Model persistence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark 2.0 comes with the ability to save and load machine learning models across
    programming languages with ease. In other words, you can create a machine learning
    model in Scala and load it in Python. This allows us to create a model in one
    system, save it, copy it, and use it in other systems. Continuing with the same
    Scala REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the loaded model can be used for testing or prediction, just like the original
    model. Continuing with the same Python REPL prompt, try the following statements
    to load the model saved using the Scala program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Wine classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset containing various features of white wine is used in this wine
    quality classification use case. The following are the features of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Fixed acidity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Volatile acidity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Citric acid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Residual sugar
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chlorides
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Free sulfur dioxide
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total sulfur dioxide
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pH
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sulphates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alcohol
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on these features, the quality (score between 0 and 10) is determined.
    If the quality is less than 7, then it is classified as bad and a value of 0 is
    assigned to the label. If the quality is 7 or above, then it is classified as
    good and a value of 1 is assigned to the label. In other words, the classification
    value is the label of this dataset. Using this dataset, a model is going to be
    trained and then using the trained model, testing is done and predictions are
    made. This is a classification problem. The Logistic Regression algorithm is used
    to train the model. In this machine learning application use case, it deals with
    modeling the relationship between a dependent variable (wine quality) and a set
    of independent variables (the features of the wine). At the Scala REPL prompt,
    try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code snippet works exactly like a linear regression use case,
    except for the model used here. The model used here is Logistic Regression and
    its label takes only two values, 0 and 1\. Creating the model, testing the model,
    and then the predictions are all similar here. In other words, the pipelines look
    very similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code demonstrates the same use case using Python. At the Python
    REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Logistic regression is very similar to linear regression. The major difference
    in Logistic regression is that its dependent variable is a categorical variable.
    In other words, the dependent variable takes only a selected set of values. In
    this use case the values are 0 or 1\. The value 0 means that the wine quality
    is bad and the value 1 means that the wine quality is good. To be more precise,
    here, the dependent variable used is a binary dependent variable.
  prefs: []
  type: TYPE_NORMAL
- en: The use cases covered so far have only a handful of features. But in real-world
    use cases the number of features is going to be really huge, especially in machine
    learning use cases where lots of text processing is done. The next section is
    going to discuss one such use case.
  prefs: []
  type: TYPE_NORMAL
- en: Spam filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spam filtering is a very common use case that is used in many applications.
    It is ubiquitous in e-mail applications. It is one of the most widely used classification
    problems. In a typical mail server, a huge number of e-mails are processed. The
    spam filtering is done on the e-mails received before they are delivered to the
    recipient's mailboxes. For any machine learning algorithm, a model has to be trained
    before making a prediction. To train the model, training data is required. How
    is training data collected? A trivial way is that the users themselves mark some
    of the e-mails received as spam. Use all the e-mails in the mail server as training
    data and keep refreshing the model on a regular basis. This includes spam and
    non-spam e-mails. When the model has a good sample of both kinds of e-mail, the
    prediction is going to be good.
  prefs: []
  type: TYPE_NORMAL
- en: The spam filtering use case covered here is not a full-blown production-ready
    application, but it gives a good insight into how one can be built. Here, instead
    of using the entire text of an e-mail, only one line is used for simplicity. If
    this is to be extended to process real e-mails, instead of the single strings,
    read the contents of a full e-mail to one string and proceed as per the logic
    given in this application.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the numeric features that are covered in the earlier use cases of this
    chapter, the input here is pure text and selecting features is not as easy as
    those use cases. The lines are split into words to have a bag of words and the
    words are chosen as features. Since it is easy to process numerical features,
    these words are transformed to hashed term frequency vectors. In other words,
    the series of words or terms in the lines are converted to their term frequencies
    using a hashing method. So even in small-scale text processing use cases, there
    will be thousands of features. That is why they need to be hashed for easy comparison.
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed earlier, in typical machine learning applications, the input data
    needs to undergo lots of pre-processing to get it in the right form of features
    and labels in order to build the model. This typically forms a pipeline of transformations
    and estimations. In this use case, the incoming lines are split into words, and
    those words are transformed using HashingTF algorithm and then a LogisticRegression
    model is trained before doing the prediction. This is done using the Pipeline
    abstraction in the Spark machine learning library. At the Scala REPL prompt, try
    the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code snippet does the typical chain of activities: preparing
    training data, creating the model using the Pipeline abstraction, and then predicting
    using the test data. It doesn''t reveal how the features are created and processed.
    In an application development perspective, Spark machine learning library does
    the heavy lifting and does everything under the hood using the Pipeline abstraction.
    If the Pipeline methodology is not used, then tokenisation and then hashing are
    to be done as a separate DataFrame transformation. The following code snippet
    executed as the continuation of the preceding commands will give an insight into
    how that can be done as simple transformations to see the features using the naked
    eyes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The same use case implemented in Python is as follows. At the Python REPL prompt,
    try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As discussed in earlier, the transformations abstracted by the Pipeline is
    elucidated as follows using Python explicitly. The following code snippet executed
    as the continuation of the preceding commands will give an insight into how that
    can be done as simple transformations to see the features using the naked eyes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Based on the insight provided in the preceding use case, lots of text processing
    machine learning applications can be developed by abstracting away lots of transformations
    using the Spark machine learning library Pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just like the way the machine learning models are persisted to the media, all
    of the Spark machine learning library Pipelines can also be persisted to the media
    and reloaded by other programs.
  prefs: []
  type: TYPE_NORMAL
- en: Feature algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In real-world use cases, it is not very easy to get the raw data in the appropriate
    form of features and labels in order to train the model. Doing lots of pre-processing
    is very common. Unlike other data processing paradigms, Spark in conjunction with
    the Spark machine learning library provides a comprehensive set of tools and algorithms
    for this purpose. This pre-processing algorithms can be put into three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process of extracting the features from the raw data is feature extraction.
    The HashingTF that was used in the preceding use case is a good example of an
    algorithm that converts terms of text data to feature vectors. The process of
    transforming features into different formats is feature transformation. The process
    of selecting a subset of features from a super set is feature selection. Covering
    all these is beyond the scope of this chapter, but the next section is going to
    discuss an Estimator, which is an algorithm that is used to extract features,
    that is used to find synonyms of words in documents,. These are not the word's
    actual synonyms, but the words that are related to a given word in a context.
  prefs: []
  type: TYPE_NORMAL
- en: Finding synonyms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A synonym is a word or phrase that has exactly the same meaning or very close
    meaning to another word. In a purely literature perspective this explanation is
    correct, but in a much wider perspective, in a given context, some of the words
    will have a very close relationship, and that is also called synonymous in this
    context. For example, Roger Federer is *synonymous* with Tennis. Finding this
    kind of synonym in context is a very common requirement in entity recognition,
    machine translation, and so on. The **Word2Vec** algorithm computes a distributed
    vector representation of words from the words of a given document or collection
    of words. If this vector space is taken, the words that have similarity or synonymity
    will be close to each other.
  prefs: []
  type: TYPE_NORMAL
- en: The University of California Irvine Machine Learning Repository ([http://archive.ics.uci.edu/ml/index.html](http://archive.ics.uci.edu/ml/index.html))
    provides a lot of datasets as a service to those who are interested to learn machine
    learning. The Twenty Newsgroups Dataset ([http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups](http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups))
    is being used here to find synonyms of words in context. It contains a dataset
    consists of 20,000 messages taken from 20 newsgroups.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Twenty Newsgroups Dataset download link lets you download the dataset discussed
    here. The file `20_newsgroups.tar.gz` is to be downloaded and unzipped. The data
    directory used in the following code snippets should point to the directory where
    the data is available in unzipped form. If the Spark Driver is giving out of memory
    error because of the huge size of the data, remove some of the newsgroups data
    that is of not interest and experiment with a subset of the data. Here, to train
    the model, only the following news group data is used: talk.politics.guns, talk.politics.mideast,
    talk.politics.misc, and talk.religion.misc.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the Scala REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code snippet is loaded with a lot of functionality. The dataset
    is read from the filesystem into a DataFrame as one sentence of text from a given
    file. Then tokenisation is done to convert the sentences into words using regular
    expressions and removing the gaps. Then, from those words, the stop words are
    removed so that we only have relevant words. Finally, using the **Word2Vec** estimator,
    a model is trained with the data prepared. From the trained model, synonyms are
    determined.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code demonstrates the same use case using Python. At the Python
    REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The major difference between the Scala implementation and Python implementation
    is that in the Python implementation, the stop words have not been removed. That
    is because that functionality is not available in Python API of the Spark Machine
    Library. Because of this difference, the list of synonyms generated by Scala program
    and Python program are different.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information refer the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://archive.ics.uci.edu/ml/index.html](http://archive.ics.uci.edu/ml/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://archive.ics.uci.edu/ml/datasets/Wine+Quality](http://archive.ics.uci.edu/ml/datasets/Wine+Quality)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups](http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark provides a very powerful core data processing framework and the Spark
    machine learning library makes use of all the core features of Spark and Spark
    libraries such as Spark SQL, in addition to its rich set of machine learning algorithms.
    This chapter covered some of the very common prediction use cases and classification
    use cases with Scala and Python implementations using the Spark machine learning
    library with a few lines of code. These wine quality prediction, wine classification,
    spam filter, and synonym finder machine learning use cases have great potential
    to be developed into full-blown real-world use cases. Spark 2.0 brings flexibility
    to model creation, pipeline creation, and their usage in different programs written
    in a different languages by enabling the model and pipeline persistence.
  prefs: []
  type: TYPE_NORMAL
- en: Pair-wise relationships are very common in real-world use cases. Backed by a
    strong mathematical theoretical base, computer scientists have developed many
    data structures and the algorithms that are going with it falling under the subject
    of Graph Theory. These data structures and algorithms have huge applicability
    in applications such as social networking websites, scheduling problems, and many
    other applications. Graph processing is very computationally intensive and distributed
    data processing paradigms such as Spark are ideal for doing such computations.
    The Spark GraphX library built on top of Spark is a collection of graph processing
    APIs. The next chapter is going to take a look at Spark GraphX.
  prefs: []
  type: TYPE_NORMAL
