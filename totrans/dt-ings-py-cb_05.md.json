["```py\n$ jupyter notebook\n```", "```py\n    from pyspark.conf import SparkConf\n    from pyspark.sql import SparkSession\n    ```", "```py\n    conf = SparkConf()\n    conf.set('spark.jars', /path/to/your/postgresql-42.5.1.jar')\n    ```", "```py\n    spark = SparkSession.builder \\\n            .config(conf=conf) \\\n            .master(\"local\") \\\n            .appName(\"Postgres Connection Test\") \\\n            .getOrCreate()\n    ```", "```py\n    df= spark.read.format(\"jdbc\") \\\n        .options(url=\"jdbc:postgresql://localhost:5432/postgres\",\n                 dbtable=\"world_population\",\n                 user=\"root\",\n                 password=\"root\",\n                 driver=\"org.postgresql.Driver\") \\\n        .load()\n    ```", "```py\n    df.printSchema()\n    ```", "```py\nconf = SparkConf()\nconf.set('spark.jars', '/path/to/your/postgresql-42.5.1.jar')\n```", "```py\nspark = SparkSession.builder \\\n        .config(conf=conf) \\\n        .master(\"local\") \\\n    (...)\n```", "```py\ndf= spark.read.format(\"jdbc\") \\\n    .options(url=\"jdbc:postgresql://localhost:5432/postgres\",\n             dbtable=\"world_population\",\n             user=\"root\",\n             password=\"root\",\n             driver=\"org.postgresql.Driver\") \\\n    .load()\n```", "```py\ndf= spark.read.format(\"jdbc\") \\\n    .options(url=\"jdbc:postgresql://localhost:5432/postgres\",\n             dbtable=\"world_population\",\n             user=\"root\",\n             password=\"root\") \\\n    .load()\n```", "```py\ndf= spark.read.format(\"jdbc\") \\\n    .options(url=\"jdbc:postgresql://localhost:5432/postgres\",\n             dbtable=\"world_population\",\n             user=\"root\",\n             password=\"root\",\n             driver=\"org.postgresql.Driver\") \\\n    .load()\n```", "```py\n    df= spark.read.format(\"jdbc\") \\\n        .options(url=\"jdbc:postgresql://localhost:5432/postgres\",\n                 dbtable=\"world_population\",\n                 user=\"root\",\n                 password=\"root\",\n                 driver=\"org.postgresql.Driver\") \\\n        .load()\n    ```", "```py\n    df.createOrReplaceTempView(\"world_population\")\n    ```", "```py\n    spark.sql(\"select * from world_population\").show(3)\n    ```", "```py\n    south_america = spark.sql(\"select * from world_population where continent = 'South America' \")\n    ```", "```py\n    south_america.toPandas()\n    ```", "```py\n    south_america.write.parquet('south_america_population')\n    ```", "```py\ndf.createOrReplaceTempView(\"world_population\")\n```", "```py\nspark.sql(\"select * from world_population\").show(3)\n```", "```py\nsouth_america.write.parquet('south_america_population')\n```", "```py\ndf.filter(df['continent'] == 'South America').show(10)\n```", "```py\nmy-project/mongo-local$ docker run \\\n--name mongodb-local \\\n-p 27017:27017 \\\n-e MONGO_INITDB_ROOT_USERNAME=<your_username> \\\n-e MONGO_INITDB_ROOT_PASSWORD=<your_password>\\\n-d mongo:latest\n```", "```py\nmy-project/mongo-local$ docker run \\\n--name mongodb-local \\\n-p 27017:27017 \\\n-e MONGO_INITDB_ROOT_USERNAME=<your_username> \\\n-e MONGO_INITDB_ROOT_PASSWORD=<your_password>\\\n-d mongo:latest\n```", "```py\nmongodb://mongo-server-user:some_password@mongo-host01.example.com:27017/?authSource=admin\n```", "```py\nmongodb://mongo-server-user:some_password@mongo-host01.example.com:27017, mongo-host02.example.com:27017, mongo-hosta03.example.com:27017/?authSource=admin\n```", "```py\nMongoError: Authentication failed\n```", "```py\nmongodb+srv://my-server.example.com/\n```", "```py\n$ pip3 install pymongo\n```", "```py\n    import json\n    import os\n    from pymongo import MongoClient, InsertOne\n    mongo_client = pymongo.MongoClient(\"mongodb://root:root@localhost:27017/\")\n    ```", "```py\ndb_cookbook = mongo_client.db_airbnb\ncollection = db_cookbook.reviews\njson_collection = \"sample_airbnb/listingsAndReviews.json\"\n```", "```py\n    requesting_collection = []\n    with open(json_collection) as f:\n        for object in f:\n            my_dict = json.loads(object)\n            requesting.append(InsertOne(my_dict))\n    result = collection.bulk_write(requesting_collection)\n    mongo_client.close()\n    ```", "```py\nmongo_client = pymongo.MongoClient(\"mongodb://root:root@localhost:27017/\")\n```", "```py\ndb_cookbook = mongo_client.db_airbnb\ncollection = db_cookbook.reviews\n```", "```py\nrequesting_collection = []\nwith open(json_collection) as f:\n    for object in f:\n        my_dict = json.loads(object)\n        requesting_collection.append(InsertOne(my_dict))\n```", "```py\njson.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 190)\n```", "```py\nresult = collection.bulk_write(requesting_collection)\n```", "```py\nmongoimport --host localhost --port 27017-d db_name -c collection_name --file path/to/file.json\n```", "```py\nlocalhost:27017: $oid is not valid for storage.\n```", "```py\n    from pyspark.sql import SparkSession\n    spark = SparkSession.builder \\\n          .master(\"local[1]\") \\\n          .appName(\"MongoDB Ingest\") \\\n          .config(\"spark.executor.memory\", '3g') \\\n          .config(\"spark.executor.cores\", '1') \\\n          .config(\"spark.cores.max\", '1') \\\n          .config(\"spark.mongodb.input.uri\", \"mongodb://root:root@127.0.0.1/db_airbnb?authSource=admin&readPreference=primaryPreferred\") \\\n          .config(\"spark.mongodb.input.collection\", \"reviews\") \\\n          .config(\"spark.jars.packages\",\"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n          .getOrCreate()\n    ```", "```py\n    df = spark.read.format(\"mongo\").load()\n    ```", "```py\n    df.printSchema()\n    ```", "```py\nconfig(\"spark.mongodb.input.uri\", \"mongodb://root:root@127.0.0.1/db_airbnb?authSource=admin) \\\n```", "```py\n.config(\"spark.mongodb.input.collection\", \"reviews\")\\\n```", "```py\n.config(\"spark.jars.packages\",\"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\")\n```", "```py\ndf = spark.read.format(\"mongo\").load()\n```", "```py\nspark = SparkSession.builder \\\n      (...)\n      .config(\"spark.mongodb.input.uri\", \"mongodb://root:root@127.0.0.1/db_aibnb?authSource=admin\") \\\n      .config(\"spark.mongodb.input.collection\", \"reviews\")\n     .getOrCreate()\ndf = spark.read.format(\"mongo\").load()\n```"]