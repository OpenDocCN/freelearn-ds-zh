- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Structured Streaming in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The world of data processing has evolved rapidly as data volume and data velocity
    increase every day. With that, the need to analyze and derive insights from real-time
    data is becoming increasingly crucial. Structured Streaming, a component of Apache
    Spark, has emerged as a powerful framework to process and analyze data streams
    in real time. This chapter delves into the realm of Structured Streaming, exploring
    its capabilities, features, and real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Real-time data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fundamentals of streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark Streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structured Streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming sources and sinks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced topics in Structured Streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joins in Structured Streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will understand Spark Streaming and the power
    of real-time data insights.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by looking at what real-time data processing means.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time data processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Real-time data processing has become increasingly critical in today’s fast-paced
    and data-driven world. Organizations need to analyze and derive insights from
    data as it arrives, enabling them to make timely decisions and take immediate
    action. Spark Streaming, a powerful component of Apache Spark, addresses this
    need by providing a scalable and fault-tolerant framework to process real-time
    data streams.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time data processing has gained immense importance in various industries,
    ranging from finance and e-commerce to the Internet of Things (IoT) and social
    media. Traditional batch processing approaches, while suitable for many scenarios,
    fall short when immediate insights and actions are required. Real-time data processing
    fills this gap by enabling the analysis and processing of data as it arrives,
    allowing organizations to make timely decisions and respond quickly to changing
    conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time data processing involves the continuous ingestion, processing, and
    analysis of streaming data. Unlike **batch processing**, which operates on static
    datasets, real-time data processing systems handle data that is generated and
    updated in real time. This data can be sourced from various channels, including
    sensors, logs, social media feeds, and financial transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key characteristics of real-time data processing are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Low latency**: Real-time data processing aims to minimize the time delay
    between data generation and processing. It requires fast and efficient processing
    capabilities to provide near-instantaneous insights and responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Real-time data processing systems must be able to handle high-volume
    and high-velocity data streams. The ability to scale horizontally and distribute
    processing across multiple nodes is essential to accommodate the increasing data
    load.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance**: Given the continuous nature of streaming data, real-time
    processing systems need to be resilient to failures. They should have mechanisms
    in place to recover from failures and ensure uninterrupted processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A streaming data model**: Real-time data processing systems operate on **streaming
    data**, which is an unbounded sequence of events or records. Streaming data models
    are designed to handle the continuous flow of data and provide mechanisms for
    event-time and window-based computations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These characteristics of real-time data processing lead to several advantages,
    including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A rapid response**: Real-time processing enables organizations to respond
    quickly to changing conditions, events, or opportunities. It allows for timely
    actions, such as fraud detection, anomaly detection, real-time monitoring, and
    alerting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personalization**: Real-time processing enables personalized experiences
    by analyzing and acting on user behavior in real time. It powers real-time recommendations,
    dynamic pricing, targeted advertising, and content personalization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operational efficiency**: Real-time processing provides insights into operational
    processes, allowing organizations to optimize their operations, identify bottlenecks,
    and improve efficiency in real time. It facilitates predictive maintenance, supply
    chain optimization, and real-time resource allocation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Situational awareness**: Real-time data processing helps organizations gain
    situational awareness by continuously analyzing and aggregating data from various
    sources. It enables real-time analytics, monitoring, and decision making in domains
    such as cybersecurity, financial markets, and emergency response systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, real-time streaming involves the continuous transmission and processing
    of data, enabling immediate insights and rapid decision-making. It has a wide
    range of applications across different industries and utilizes various technologies
    to facilitate efficient and reliable streaming.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore the basics of streaming and understand
    how streaming is useful for real-time operations.
  prefs: []
  type: TYPE_NORMAL
- en: What is streaming?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Streaming refers to the continuous and real-time processing of data as it is
    generated or received. Unlike batch processing, where data is processed in chunks
    or batches at fixed intervals, streaming enables the processing of data continuously
    and incrementally. It allows applications to ingest, process, and analyze data
    in real time, enabling timely decision-making and immediate responses to events.
  prefs: []
  type: TYPE_NORMAL
- en: Different types of streaming architectures are available to handle streaming
    data. We will look at them next.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Streaming architectures are designed to handle the continuous and high-velocity
    nature of streaming data. They typically consist of three key components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Streaming sources**: These are the origins of the streaming data, such as
    IoT devices, sensors, logs, social media feeds, or messaging systems. Streaming
    sources continuously produce and emit data in real time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A streaming processing engine**: The streaming processing engine is responsible
    for ingesting, processing, and analyzing streaming data. It provides the necessary
    infrastructure and computational capabilities to handle the continuous and incremental
    nature of streaming data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streaming sinks**: Streaming sinks are destinations where the processed data
    is stored, visualized, or acted upon. They can be databases, data warehouses,
    dashboards, or external systems that consume the processed data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are various streaming architectures, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Event-driven architecture**: In event-driven architecture, events are generated
    by sources and then captured and processed by the engine, leading to immediate
    reactions and triggering actions or updates in real time. This framework facilitates
    real-time event processing, supports the development of event-driven microservices,
    and contributes to the creation of reactive systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Event-driven architecture’s advantages lie in its ability to provide responsiveness,
    scalability, and flexibility. This allows for prompt reactions to events as they
    unfold, fostering agility in system responses.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Lambda architecture**: The lambda architecture seamlessly integrates batch
    and stream processing to effectively manage both historical and real-time data.
    This involves parallel processing of data streams to enable real-time analysis,
    coupled with offline batch processing for in-depth and comprehensive analytics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach is particularly well-suited for applications that require a balance
    of real-time insights and thorough historical analysis. The lambda architecture’s
    strengths lie in its provision of fault tolerance, scalability, and capacity to
    handle substantial data volumes. This is achieved by harnessing the combined power
    of both the batch and stream processing techniques.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Unified streaming architecture**: Unified streaming architectures, such as
    Apache Spark’s Structured Streaming, aim to provide a unified API and processing
    model for both batch and stream processing. They simplify the development and
    deployment of real-time applications by abstracting away the complexities of managing
    separate batch and stream processing systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This architecture abstracts complexities by offering a simplified approach to
    developing and deploying real-time applications. This is ideal for scenarios where
    simplicity and ease of development are crucial, allowing developers to focus more
    on business logic than intricate technicalities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The advantages are that it simplifies development, reduces operational overhead,
    and ensures consistency across batch and stream processing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These architectures cater to different needs based on the specific requirements
    of a given application. Event-driven is ideal for real-time reactions, lambda
    for a balance between real-time and historical data, and unified streaming for
    a streamlined, unified approach to both batch and stream processing. Each approach
    has its strengths and trade-offs, making them suitable for various scenarios based
    on the specific needs of a system.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will delve into the specifics of Structured Streaming,
    its key concepts, and how it compares to Spark Streaming. We will also explore
    stateless and stateful streaming, streaming sources, and sinks, providing code
    examples and practical illustrations to enhance understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Spark Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you’ve seen so far, Spark Streaming is a powerful real-time data processing
    framework built on Apache Spark. It extends the capabilities of the Spark engine
    to support high-throughput, fault-tolerant, and scalable stream processing. Spark
    Streaming enables developers to process real-time data streams using the same
    programming model as batch processing, making it easy to transition from batch
    to streaming workloads.
  prefs: []
  type: TYPE_NORMAL
- en: At its core, Spark Streaming divides the real-time data stream into small batches
    or micro-batches, which are then processed using Spark’s distributed computing
    capabilities. Each micro-batch is treated as a **Resilient Distributed Dataset**
    (**RDD**), Spark’s fundamental abstraction for distributed data processing. This
    approach allows developers to leverage Spark’s extensive ecosystem of libraries,
    such as Spark SQL, MLlib, and GraphX, for real-time analytics and machine learning
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the architecture of Spark Streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark Streaming follows a **master-worker architecture**, where the driver
    program serves as the master and the worker nodes process the data. The high-level
    architecture consists of the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The driver program**: The driver program runs the main application and manages
    the overall execution of the Spark Streaming application. It divides the data
    stream into batches, schedules tasks on the worker nodes, and coordinates the
    processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Receivers**: Receivers are responsible for connecting to the streaming data
    sources and receiving the data. They run on worker nodes and pull the data from
    sources such as Kafka, Flume, or TCP sockets. The received data is then stored
    in the memory of the worker nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discretized Stream (DStream)**: DStream is the basic abstraction in Spark
    Streaming. It represents a continuous stream of data divided into small, discrete
    RDDs. DStream provides a high-level API to perform transformations and actions
    on the streaming data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`map`, `filter`, and `reduceByKey`, are applied to each RDD in the DStream.
    Actions, such as `count`, `saveAsTextFiles`, and `foreachRDD`, trigger the execution
    of the streaming computation and produce results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output operations**: Output operations allow the processed data to be written
    to external systems or storage. Spark Streaming supports various output operations,
    such as writing to files, databases, or sending to dashboards for visualization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To effectively use Spark Streaming, it is important to understand some key
    concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DStreams**: As mentioned earlier, DStreams represent the continuous stream
    of data in Spark Streaming. They are a sequence of RDDs, where each RDD contains
    data from a specific time interval. DStreams support various transformations and
    actions, enabling complex computations on the stream.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Window operations**: Window operations allow you to apply transformations
    on a sliding window of data in the stream. It enables computations over a fixed
    window size or based on time durations, enabling tasks such as windowed aggregations
    or time-based joins.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stateful operations**: Spark Streaming gives you the ability to maintain
    stateful information across batches. It enables operations that require maintaining
    and updating state, such as cumulative counts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Checkpointing**: Checkpointing is a crucial mechanism in Spark Streaming
    to ensure fault tolerance and recovery. It periodically saves the metadata about
    the streaming application, including the configuration, DStream operations, and
    the processed data. It enables the recovery of the application if there are failures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we will see the different advantages of using Spark Streaming in real-time
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A unified processing model**: One of the significant advantages of Spark
    Streaming is its integration with the larger Spark ecosystem. It leverages the
    same programming model as batch processing, allowing users to seamlessly transition
    between batch and real-time processing. This unified processing model simplifies
    development and reduces the learning curve for users familiar with Spark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High-level abstractions**: Spark Streaming provides high-level abstractions
    such as DStreams to represent streaming data. DStreams are designed to handle
    continuous data streams and enable easy integration with existing Spark APIs,
    libraries, and data sources. These abstractions provide a familiar and expressive
    programming interface to process real-time data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance and scalability**: Spark Streaming offers fault-tolerant
    processing by leveraging Spark’s RDD abstraction. It automatically recovers from
    failures by recomputing lost data, ensuring the processing pipeline remains resilient
    and robust. Additionally, Spark Streaming can scale horizontally by distributing
    a workload across a cluster of machines, allowing it to handle large-scale data
    streams effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Windowed computations**: Spark Streaming supports windowed computations,
    which enable time-based analysis over sliding or tumbling windows of data. Window
    operations provide flexibility in performing aggregations, time-series analysis,
    and window-level transformations. This capability is particularly useful when
    analyzing streaming data based on temporal characteristics or patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A wide range of data sources**: Spark Streaming seamlessly integrates with
    various data sources, including Kafka, Flume, Hadoop Distributed File System (HDFS),
    and Amazon S3\. This broad range of data sources allows users to ingest data from
    multiple streams and integrate it with existing data pipelines. Spark Streaming
    also supports custom data sources, enabling integration with proprietary or specialized
    streaming platforms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While Spark Streaming offers powerful real-time data processing capabilities,
    there are certain challenges to consider.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Building streaming architectures for applications comes with its set of challenges,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**End-to-end latency**: Latency is introduced when Spark Streaming processes
    data in micro-batches. The end-to-end latency can vary based on factors such as
    the batch interval, the data source, and the complexity of the computations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance**: Spark Streaming provides fault tolerance through RDD lineage
    and checkpointing. However, failures in receivers or the driver program can still
    disrupt the stream processing. Handling and recovering from failures is an important
    consideration to ensure the reliability of Spark Streaming applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Scaling Spark Streaming applications to handle large volumes
    of data and meet high throughput requirements can be a challenge. Proper resource
    allocation, tuning, and cluster management are crucial to achieve scalability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data ordering**: Spark Streaming processes data in parallel across multiple
    worker nodes, which can affect the order of events. Ensuring the correctness of
    the order of events becomes important in certain use cases, and developers need
    to consider this when designing their applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, Spark Streaming brings the power of Apache Spark to real-time data
    processing. Its integration with the Spark ecosystem, high-level abstractions,
    fault tolerance, scalability, and support for windowed computations make it a
    compelling choice for processing streaming data. By harnessing the advantages
    of Spark Streaming, organizations can unlock valuable insights and make informed
    decisions in real time.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore Structured Streaming, a newer and more
    expressive streaming API in Apache Spark that overcomes some of the limitations
    and challenges of Spark Streaming. We will discuss its core concepts, the differences
    from Spark Streaming, and its benefits for real-time data processing.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Structured Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Structured Streaming is a revolutionary addition to Apache Spark that brings
    a new paradigm for real-time data processing. It introduces a high-level API that
    seamlessly integrates batch and streaming processing, providing a unified programming
    model. Structured Streaming treats streaming data as an unbounded table or DataFrame,
    enabling developers to express complex computations using familiar SQL-like queries
    and transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the micro-batch processing model of Spark Streaming, Structured Streaming
    follows a continuous processing model. It processes data incrementally as it arrives,
    providing low-latency and near-real-time results. This shift toward continuous
    processing opens up new possibilities for interactive analytics, dynamic visualizations,
    and real-time decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Key features and advantages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Structured Streaming offers several key features and advantages over traditional
    stream processing frameworks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**An expressive API**: Structured Streaming provides a declarative API that
    allows developers to express complex streaming computations using SQL queries,
    DataFrame operations, and Spark SQL functions. This enables developers with SQL
    or DataFrame expertise to easily transition to real-time data processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance and exactly-once semantics**: Structured Streaming guarantees
    end-to-end fault tolerance and exactly-once semantics by maintaining the necessary
    metadata and state information. It handles failures gracefully and ensures that
    data is processed exactly once, even in the presence of failures or retries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Structured Streaming leverages the scalability of the Spark
    engine, enabling horizontal scaling by adding more worker nodes to the cluster.
    It can handle high-throughput data streams and scale seamlessly as the data volume
    increases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unified batch and streaming**: With Structured Streaming, developers can
    use the same API and programming model for both batch and streaming processing.
    This unification simplifies the development and maintenance of applications, as
    there is no need to manage separate batch and stream processing code bases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ecosystem integration**: Structured Streaming seamlessly integrates with
    the broader Spark ecosystem, enabling the use of libraries such as Spark SQL,
    MLlib, and GraphX for real-time analytics, machine learning, and graph processing
    on streaming data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s take a look at some of the differences between Structured Streaming
    and Spark Streaming.
  prefs: []
  type: TYPE_NORMAL
- en: Structured Streaming versus Spark Streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Structured Streaming differs from Spark Streaming in several fundamental ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The processing model**: Spark Streaming processes data in micro-batches,
    where each batch is treated as a discrete RDD. In contrast, Structured Streaming
    processes data incrementally in a continuous manner, treating the stream as an
    unbounded table or DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The API and query language**: Spark Streaming primarily offers a low-level
    API based on RDD transformations and actions. Structured Streaming, on the other
    hand, provides a higher-level API with SQL-like queries, DataFrame operations,
    and Spark SQL functions. This makes it easier to express complex computations
    and leverage the power of SQL for real-time analytics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance**: Both Spark Streaming and Structured Streaming provide
    fault tolerance. However, Structured Streaming’s fault tolerance is achieved by
    maintaining the necessary metadata and state information, whereas Spark Streaming
    relies on RDD lineage and checkpointing for fault recovery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data processing guarantees**: Spark Streaming provides at-least-once processing
    guarantees by default, where some duplicates may be processed if there are failures.
    Structured Streaming, on the other hand, provides exactly-once processing semantics,
    ensuring that each event is processed exactly once, even in the presence of failures
    or retries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations and considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While Structured Streaming offers significant advantages, there are certain
    limitations and considerations to keep in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Event time handling**: Proper handling of event time, such as timestamp extraction,
    watermarking, and late data handling, is essential in Structured Streaming. Care
    should be taken to ensure the correct processing and handling of out-of-order
    events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State management**: Structured Streaming allows you to maintain stateful
    information across batches, which can introduce challenges related to state management
    and scalability. Monitoring memory usage and configuring appropriate state retention
    policies are crucial for optimal performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ecosystem compatibility**: While Structured Streaming integrates well with
    the Spark ecosystem, certain libraries and features might not be fully compatible
    with real-time streaming use cases. It is important to evaluate the compatibility
    of specific libraries and functionalities before using them in a Structured Streaming
    application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance considerations**: Structured Streaming’s continuous processing
    model introduces different performance considerations compared to micro-batch
    processing. Factors such as the event rate, processing time, and resource allocation
    need to be carefully monitored and optimized for efficient real-time data processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will delve deeper into the concepts of stateless and
    stateful streaming, exploring their differences and use cases in the context of
    Structured Streaming.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s start by looking at some of the fundamental concepts in streaming that
    will help us get familiarized with different paradigms in streaming.
  prefs: []
  type: TYPE_NORMAL
- en: Stateless streaming – processing one event at a time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stateless streaming refers to the processing of each event in isolation, without
    considering any context or history from previous events. In this approach, each
    event is treated independently, and the processing logic does not rely on any
    accumulated state or information from past events.
  prefs: []
  type: TYPE_NORMAL
- en: Stateless streaming is well-suited for scenarios where each event can be processed
    independently, and the output is solely determined by the content of the event
    itself. This approach is often used for simple filtering, transformation, or enrichment
    operations that do not require you to maintain any contextual information across
    events.
  prefs: []
  type: TYPE_NORMAL
- en: Stateful streaming – maintaining stateful information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stateful streaming involves maintaining and utilizing contextual information
    or state across multiple events during processing. The processing logic considers
    the history of events and uses accumulated information to make decisions or perform
    computations. Stateful streaming enables more sophisticated analysis and complex
    computations that rely on context or accumulated knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Stateful streaming requires you to maintain and update state information as
    new events arrive. The state can be as simple as a running count or more complex,
    involving aggregations, windowed computations, or maintaining session information.
    Proper management of state is essential to ensure correctness, scalability, and
    fault tolerance in stateful streaming applications.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand the differences between stateless and stateful streaming.
  prefs: []
  type: TYPE_NORMAL
- en: The differences between stateless and stateful streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main differences between stateless and stateful streaming can be summarized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Stateless streaming processes events independently, while stateful streaming
    maintains and uses accumulated state information across events.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stateless streaming is suitable for simple operations that don’t rely on past
    events, while stateful streaming enables complex computations that require context
    or accumulated knowledge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stateless streaming is generally simpler to implement and reason about, while
    stateful streaming introduces additional challenges in managing state, fault tolerance,
    and scalability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stateless streaming is often used for real-time filtering, transformation, or
    basic aggregations, while stateful streaming is necessary for windowed computations,
    sessionization, and stateful joins.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the distinction between stateless and stateful streaming is crucial
    when designing real-time data processing systems, as it helps determine the appropriate
    processing model and requirements for a given use case.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at some of the fundamentals of Structured Streaming.
  prefs: []
  type: TYPE_NORMAL
- en: Structured Streaming concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand Structured Streaming, it’s important for us to understand the
    different operations that take place in a near-real-time scenario when data arrives.
    We will understand them in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Event time and processing time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Structured Streaming, there are two important notions of time – event time
    and processing time:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Event time**: Event time refers to the time when an event occurred or was
    generated. It is typically embedded within the data itself, representing the timestamp
    or a field indicating when the event occurred in the real world. Event time is
    crucial for analyzing data based on its temporal order or performing window-based
    computations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Processing time**: Processing time, on the other hand, refers to the time
    when an event is processed by the streaming application. It is determined by the
    system clock or the time at which the event is ingested by the processing engine.
    Processing time is useful for tasks that require low latency or an immediate response
    but may not accurately reflect the actual event order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on these different time concepts, we can determine which one works best
    for a given use case. It’s important to understand the difference between the
    two. Based on that, the strategy for data processing can be determined.
  prefs: []
  type: TYPE_NORMAL
- en: Watermarking and late data handling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we will discuss how to handle data that doesn’t arrive at the defined
    time in real-time applications. There are different ways to handle that situation.
    Structured Streaming has a built-in mechanism to handle this type of data. These
    mechanisms include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Watermarking**: Watermarking is a mechanism in Structured Streaming used
    to deal with event time and handle delayed or late-arriving data. A watermark
    is a threshold timestamp that indicates the maximum event time seen by a system
    up to a certain point. It allows the system to track the progress of event time
    and determine when it is safe to emit results for a specific window.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Late data handling**: Late-arriving data refers to events that have timestamps
    beyond the watermark threshold. Structured Streaming provides options to handle
    late data, such as discarding it, updating existing results, or storing it separately
    for further analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These built-in mechanisms save users a lot of time and efficiently handle late-arriving
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will see, once the data arrives, how we start the operations on it
    in streaming.
  prefs: []
  type: TYPE_NORMAL
- en: Triggers and output modes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Triggers determine when a streaming application should emit results or trigger
    the execution of the computation. Structured Streaming supports different types
    of triggers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Event time triggers**: Event time triggers operate based on the arrival of
    new events or when a watermark advances beyond a certain threshold. They enable
    more accurate and efficient processing, based on event time semantics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Processing time triggers**: These triggers operate based on processing time,
    allowing you to specify time intervals or durations at which the computation should
    be executed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Structured Streaming also offers different output modes. The output modes determine
    how data is updated in the sink. A sink is where we would write the output after
    the streaming operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Complete mode**: In this mode, the entire updated result, including all the
    rows in the output, is written to the sink. This mode provides the most comprehensive
    view of data but can be memory-intensive for large result sets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Append mode**: In append mode, only the new rows appended to the result table
    since the last trigger are written to the sink. This mode is suitable for cases
    where the result is an append-only stream.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Update mode**: Update mode only writes the changed rows to the sink, preserving
    the existing rows that haven’t changed since the last trigger. This mode is useful
    for cases where the result table is updated incrementally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s take a look at the different types of aggregate operations we can
    do on streaming data.
  prefs: []
  type: TYPE_NORMAL
- en: Windowing operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Windowing operations in Structured Streaming allow you to group and aggregate
    data over specific time windows. Windows for these operations can be defined based
    on either event time or processing time, and they provide a way to perform computations
    over a subset of events within a given time range.
  prefs: []
  type: TYPE_NORMAL
- en: 'The common types of windowing operations include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tumbling windows**: Tumbling windows divide a stream into non-overlapping
    fixed-size windows. Each event falls into exactly one window, and computations
    are performed independently for each window.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sliding windows**: Sliding windows create overlapping windows that slide
    or move over a stream at regular intervals. Each event can contribute to multiple
    windows, and computations can be performed on the overlapping parts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Session windows**: Session windows group events that are close in time or
    belong to the same session, based on a specified session timeout. A session is
    defined as a series of events within a certain time threshold of each other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next operation that we frequently use in streaming is the join operation.
    Now, we will see how we can use joins with streaming data.
  prefs: []
  type: TYPE_NORMAL
- en: Joins and aggregations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Structured Streaming supports joins and aggregations on streaming data, enabling
    complex analytics and data transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Joins**: Streaming joins allow you to combine two or more streams or a stream
    with static/reference data, based on a common key or condition. The join operation
    can be performed using event time or processing time, and it supports different
    join types such as inner join, outer join, and left/right join.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count`, `sum`, `average`, `min`, and `max`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structured Streaming’s flexible and expressive API for handling event time,
    triggers, output modes, windowing operations, joins, and aggregations allows developers
    to perform comprehensive real-time analytics and computations on streaming data.
    By understanding these concepts, developers can build sophisticated streaming
    applications with ease and precision.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore how to read and write data with streaming
    sources and sinks.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming sources and sinks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Streaming sources and sinks are essential components in a streaming system that
    enable the ingestion of data from external systems and the output of processed
    data to external destinations. They form the connectors between the streaming
    application and the data sources or sinks.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming sources retrieve data from various input systems, such as message
    queues, filesystems, databases, or external APIs, and make it available for processing
    in a streaming application. On the other hand, streaming sinks receive processed
    data from the application and write it to external storage, databases, filesystems,
    or other systems for further analysis or consumption.
  prefs: []
  type: TYPE_NORMAL
- en: There are different types of streaming sources and sinks. We will explore some
    of them next.
  prefs: []
  type: TYPE_NORMAL
- en: Built-in streaming sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Structured Streaming provides built-in support for a variety of streaming sources,
    making it easy to integrate with popular data systems. Some of the commonly used
    built-in streaming sources include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The file source**: The file source allows you to read data from files in
    a directory or a file stream from a file-based system, such as HDFS or Amazon
    S3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**KafkaSource**: KafkaSource enables the consumption of data from Apache Kafka,
    a distributed streaming platform. It provides fault-tolerant, scalable, and high-throughput
    ingestion of data streams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The socket source**: The socket source allows a streaming application to
    read data from a **Transmission Control Protocol** (**TCP**) socket. It is useful
    for scenarios where data is sent through network connections, such as log streaming
    or data sent by external systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Structured Streaming source**: The Structured Streaming source allows
    developers to define their own streaming sources by extending the built-in source
    interfaces. It provides the flexibility to integrate with custom or proprietary
    data sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom streaming sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the built-in streaming sources, Structured Streaming allows developers
    to create custom streaming sources to ingest data from any system that can be
    accessed programmatically. Custom streaming sources can be implemented by extending
    the `Source` interface provided by the Structured Streaming API.
  prefs: []
  type: TYPE_NORMAL
- en: When implementing a custom streaming source, developers need to consider aspects
    such as data ingestion, event-time management, fault tolerance, and scalability.
    They must define how data is fetched, how it is partitioned and distributed among
    workers, and how to handle late-arriving data and schema evolution.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to different streaming sources, we have streaming sinks as well. Let’s
    explore them next.
  prefs: []
  type: TYPE_NORMAL
- en: Built-in streaming sinks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Structured Streaming provides built-in support for various streaming sinks,
    enabling the output of processed data to different systems. Some of the commonly
    used built-in streaming sinks include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The console sink**: The console sink writes the output data to the console
    or standard output. It is useful for debugging and quick prototyping but not suitable
    for production use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The file sink**: The file sink writes the output data to files in a directory
    or a file-based system such as HDFS or Amazon S3\. It allows data to be stored
    and consumed later for batch processing or archival purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Kafka sink**: The Kafka sink enables you to write data to Apache Kafka
    topics. It provides fault-tolerant, scalable, and high-throughput output to Kafka
    for consumption by other systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ForeachWriter` interface. It provides the flexibility to write data to external
    systems or perform custom operations on the output data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom streaming sinks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to custom streaming sources, developers can implement custom streaming
    sinks in Structured Streaming by extending the `Sink` interface. There are instances
    when you would need to write the data back to a system that might not support
    streaming. It could be a database or a file-based storage system. Custom streaming
    sinks enable integration with external systems or databases that are not supported
    by the built-in sinks.
  prefs: []
  type: TYPE_NORMAL
- en: When implementing a custom streaming sink, developers need to define how output
    data is written or processed by the external system. This may involve establishing
    connections, handling batching or buffering, and ensuring fault tolerance and
    exactly-once semantics.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will talk about advanced techniques in Structured Streaming.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced techniques in Structured Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are certain built-in capabilities of Structured Streaming that makes it
    the default choice for even some batch operations. Instead of architecting things
    yourself, Structured Streaming handles these properties for you. Some of them
    are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Handling fault tolerance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fault tolerance is crucial in streaming systems to ensure data integrity and
    reliability. Structured Streaming provides built-in fault tolerance mechanisms
    to handle failures in both streaming sources and sinks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Source fault tolerance**: Structured Streaming ensures end-to-end fault tolerance
    in sources, by tracking the progress of event time using watermarks and checkpointing
    the metadata related to the stream. If there are failures, the system can recover
    and resume processing from the last consistent state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sink fault tolerance**: Fault tolerance in sinks depends on the guarantees
    provided by the specific sink implementation. Some sinks may inherently provide
    exactly-once semantics, while others may rely on idempotent writes or deduplication
    techniques to achieve at-least-once semantics. Sink implementations should be
    carefully chosen to ensure data consistency and reliability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developers should consider the fault tolerance characteristics of the streaming
    sources and sinks they use and configure appropriate checkpointing intervals,
    retention policies, and recovery mechanisms to ensure the reliability of their
    streaming applications.
  prefs: []
  type: TYPE_NORMAL
- en: Structured Streaming has built-in support for schema evolution as well. Let’s
    explore that in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Handling schema evolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Structured Streaming provides support for handling schema evolution in streaming
    data sources. Schema evolution refers to changes in the structure or schema of
    incoming data over time.
  prefs: []
  type: TYPE_NORMAL
- en: Structured Streaming can handle schema evolution by applying the concept of
    schema inference or schema merging. When reading from streaming sources, the initial
    schema is inferred from the incoming data. As the data evolves, subsequent DataFrames
    are merged with the initial schema, accommodating any new or changed fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet demonstrates handling schema evolution in Structured
    Streaming:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the initial schema is provided explicitly using the `schema`
    method. As new data arrives with additional fields, such as `new_col`, it can
    be selected and merged into the stream using the `selectExpr` method.
  prefs: []
  type: TYPE_NORMAL
- en: Handling schema evolution is crucial to ensure compatibility and flexibility
    in streaming applications where the data schema may change or evolve over time.
  prefs: []
  type: TYPE_NORMAL
- en: Different joins in Structured Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the key features of Structured Streaming is its ability to join different
    types of data streams together in one sink.
  prefs: []
  type: TYPE_NORMAL
- en: Stream-stream joins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stream-stream joins, also known as **stream-stream co-grouping** or **stream-stream
    correlation**, involve joining two or more streaming data sources based on a common
    key or condition. In this type of join, each incoming event from the streams is
    matched with events from other streams that share the same key or satisfy the
    specified condition.
  prefs: []
  type: TYPE_NORMAL
- en: Stream-stream joins enable real-time data correlation and enrichment, making
    it possible to combine multiple streams of data to gain deeper insights and perform
    complex analytics. However, stream-stream joins present unique challenges compared
    to batch or stream-static joins, due to the unbounded nature of streaming data
    and potential event-time skew.
  prefs: []
  type: TYPE_NORMAL
- en: One common approach to stream-stream joins is the use of windowing operations.
    By defining overlapping or tumbling windows on the streams, events within the
    same window can be joined based on their keys. Careful consideration of window
    size, watermarking, and event time characteristics is necessary to ensure accurate
    and meaningful joins.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of a stream-stream join using Structured Streaming:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this example, two Kafka streams, `stream1` and `stream2`, are read from different
    topics. The `join` method is then applied to perform the join operation, based
    on the `common_key` field shared by both streams.
  prefs: []
  type: TYPE_NORMAL
- en: Stream-static joins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stream-static joins, also known as **stream-batch joins**, involve joining a
    streaming data source to a static or reference dataset. The static dataset typically
    represents reference data, such as configuration data or dimension tables, that
    remains constant over time.
  prefs: []
  type: TYPE_NORMAL
- en: Stream-static joins are useful for enriching streaming data with additional
    information or attributes from the static dataset. For example, you might join
    a stream of user activity events with a static user profile table to enrich each
    event with user-related details.
  prefs: []
  type: TYPE_NORMAL
- en: To perform a stream-static join in Structured Streaming, you can load the static
    dataset as a static DataFrame and then use the join method to perform the join
    with the streaming DataFrame. Since the static dataset does not change, the join
    operation can be performed using the default “right outer join” mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of a stream-static join in Structured Streaming:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the streaming data is read from a Kafka source, and the static
    dataset is loaded from a CSV file. The join method is then used to perform the
    stream-static join based on the “`common_key`” field.
  prefs: []
  type: TYPE_NORMAL
- en: Both stream-stream and stream-static joins provide powerful capabilities for
    real-time data analysis and enrichment. When using these join operations, it is
    essential to carefully manage event time characteristics, windowing options, and
    data consistency to ensure accurate and reliable results. Additionally, performance
    considerations should be considered to handle large volumes of data and meet low-latency
    requirements in real-time streaming applications.
  prefs: []
  type: TYPE_NORMAL
- en: Final thoughts and future developments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Structured Streaming has emerged as a powerful framework for real-time data
    processing in Apache Spark. Its unified programming model, fault tolerance, and
    seamless integration with the Spark ecosystem make it an attractive choice for
    building scalable and robust streaming applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'As Structured Streaming continues to evolve, there are several areas that hold
    promise for future developments. These include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Enhanced support for streaming sources and sinks**: Providing more built-in
    connectors for popular streaming systems and databases, as well as improving the
    integration and compatibility with custom sources and sinks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advanced event time handling**: Introducing more advanced features for event
    time handling, including support for event-time skew detection and handling, event
    deduplication, and watermark optimizations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance optimization**: Continuously improving the performance of Structured
    Streaming, especially in scenarios with high data volumes and complex computations.
    This could involve optimizations in memory management, query planning, and query
    optimization techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with AI and machine learning**: Further integrating Structured
    Streaming with AI and machine learning libraries in Spark, such as MLlib and TensorFlow,
    to enable real-time machine learning and predictive analytics on streaming data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seamless integration with streaming data warehouses**: Providing better integration
    with streaming data warehouses or data lakes, such as Apache Iceberg or Delta
    Lake, to enable scalable and efficient storage and the querying of streaming data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conclusion, Structured Streaming offers a modern and expressive approach
    to real-time data processing in Apache Spark. Its ease of use, scalability, fault
    tolerance, and integration with the Spark ecosystem make it a valuable tool for
    building robust and scalable streaming applications. By leveraging the concepts
    and techniques covered in this chapter, developers can unlock the full potential
    of real-time data processing with Structured Streaming.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this chapter, we have explored the fundamental concepts and advanced
    techniques in Structured Streaming.
  prefs: []
  type: TYPE_NORMAL
- en: We started by understanding the fundamentals of Structured Streaming, its advantages,
    and the core concepts that underpin its operation. Then, we talked about Spark
    Streaming and what it has to offer.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we dived into the core functionalities of Structured Streaming.
    Then, we further delved into advanced topics, such as windowed operations in Structured
    Streaming. We explored sliding and tumbling windows, which enable us to perform
    aggregations and computations over a specified time window, allowing for time-based
    analysis of the streaming data. Additionally, we explored stateful streaming processing,
    which involves maintaining and updating state in streaming applications and integrating
    external libraries and APIs to enhance the capabilities of Structured Streaming.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we explored emerging trends in real-time data processing and concluded
    the chapter by summarizing the key takeaways and insights gained.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look into machine learning techniques and how to
    use Spark with machine learning.
  prefs: []
  type: TYPE_NORMAL
