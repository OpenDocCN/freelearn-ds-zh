<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Learning the Hidden Markov Model Using Mahout"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Learning the Hidden Markov Model Using Mahout</h1></div></div></div><p>In this chapter, we will cover one of the most interesting topics of classification techniques: the <span class="strong"><strong>Hidden Markov Model</strong></span> (<span class="strong"><strong>HMM</strong></span>). To understand the HMM, we will cover the following topics in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Deterministic and nondeterministic patterns</li><li class="listitem" style="list-style-type: disc">The Markov process</li><li class="listitem" style="list-style-type: disc">Introducing the HMM</li><li class="listitem" style="list-style-type: disc">Using Mahout for the HMM</li></ul></div><div class="section" title="Deterministic and nondeterministic patterns"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec24"/>Deterministic and nondeterministic patterns</h1></div></div></div><p>In a <a id="id174" class="indexterm"/>deterministic system, each state is solely dependent on the state it was previously in. For example, let's take the case of a set of traffic lights. The sequence of lights is red → green → amber → red. So, here we know what state will follow after the current state. Once the transitions are known, deterministic systems are easy to understand.</p><p>For <a id="id175" class="indexterm"/>nondeterministic patterns, consider an example of a person named Bob who has his snacks at 4:00 P.M. every day. Let's say he has any one of the three items from the menu: ice cream, juice, or cake. We cannot say for sure what item he will have the next day, even if we know what he had today. This is an example of a nondeterministic pattern.</p></div></div>
<div class="section" title="The Markov process"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec25"/>The Markov process</h1></div></div></div><p>In the Markov <a id="id176" class="indexterm"/>process, the next state is dependent on the previous states. If we assume that we have an <span class="emphasis"><em>n</em></span> state system, then the next state is dependent on the previous <span class="emphasis"><em>n</em></span> states. This process is called an <span class="emphasis"><em>n</em></span> model order. In the Markov process, we make the choice for the next state probabilistically. So, considering our previous example, if Bob had juice today, he can have juice, ice cream, or cake the next day. In the same way, we can reach any state in the system from the previous state. The Markov process is shown in the following diagram:</p><div class="mediaobject"><img src="graphics/4959OS_05_01.jpg" alt="The Markov process"/></div><p>If we have <span class="emphasis"><em>n</em></span> states in a process, then we can reach any state with n2 transitions. We have a probability of moving to any state, and hence, we will have n2 probabilities of doing this. For a Markov <a id="id177" class="indexterm"/>process, we will have the following three items:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>States</strong></span>: This refers to the <a id="id178" class="indexterm"/>states in the system. In our <a id="id179" class="indexterm"/>example, let's say there are three states: state 1, state 2, and state 3.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Transition matrix</strong></span>: This <a id="id180" class="indexterm"/>will have the <a id="id181" class="indexterm"/>probabilities of moving from one state to any other state. An example of the transition matrix is shown in the following screenshot:<div class="mediaobject"><img src="graphics/4959OS_05_02.jpg" alt="The Markov process"/></div><p>This matrix shows that if the system was in state 1 yesterday, then the probability of <a id="id182" class="indexterm"/>it to remain in the same <a id="id183" class="indexterm"/>state today will be 0.1.</p></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Initial state vector</strong></span>: This <a id="id184" class="indexterm"/>is the vector of the initial state of the system. (Any one of the states will have a probability <a id="id185" class="indexterm"/>of 1 and the rest will have a probability of 0 in this vector.)<div class="mediaobject"><img src="graphics/4959OS_05_03.jpg" alt="The Markov process"/></div></li></ul></div></div>
<div class="section" title="Introducing the Hidden Markov Model"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec26"/>Introducing the Hidden Markov Model</h1></div></div></div><p>The <span class="strong"><strong>Hidden Markov Model</strong></span> (<span class="strong"><strong>HMM</strong></span>) is a classification technique to predict the states of a system by <a id="id186" class="indexterm"/>observing the outcomes without having access to the actual states themselves. It is a Markov model in which the states are hidden.</p><p>Let's continue with Bob's snack example we saw earlier. Now <a id="id187" class="indexterm"/>assume we have one more set of events in place that is directly observable. We know what Bob has eaten for lunch and his snacks intake is related to his lunch. So, we have an observation state, which is Bob's lunch, and hidden states, which are his snacks intake. We want to build an algorithm that can forecast what would be Bob's choice of snack based on his lunch.</p><div class="mediaobject"><img src="graphics/4959OS_05_04.jpg" alt="Introducing the Hidden Markov Model"/></div><p>In addition to the transition probability matrix in the Hidden Markov Model, we have one more matrix that is called an <span class="strong"><strong>emission matrix</strong></span>. This matrix contains the probability of the observable state, provided <a id="id188" class="indexterm"/>it is assigned a hidden state. The emission matrix is as follows:</p><p>P (observable state | one state)</p><p>So, a Hidden Markov Model <a id="id189" class="indexterm"/>has the following properties:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>State vector</strong></span>: This <a id="id190" class="indexterm"/>contains the probability of the hidden <a id="id191" class="indexterm"/>model to be in a particular state at the start</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Transition matrix</strong></span>: This <a id="id192" class="indexterm"/>has the probabilities <a id="id193" class="indexterm"/>of a hidden state, given the previous hidden state</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Emission matrix</strong></span>: Given <a id="id194" class="indexterm"/>that the hidden model is <a id="id195" class="indexterm"/>in a particular hidden state, this has the probabilities of observing a particular observable state</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Hidden states</strong></span>: This <a id="id196" class="indexterm"/>refers to the states of the system that <a id="id197" class="indexterm"/>can be defined by the Hidden Markov Model</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Observable state</strong></span>: The <a id="id198" class="indexterm"/>states that are visible in the <a id="id199" class="indexterm"/>process</li></ul></div><p>Using the Hidden Markov Model, three types of problems can be solved. The first two are related to the pattern recognition problem and the third type of problem generates a Hidden Markov Model, given a sequence of observations. Let's look at these three types of problems:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Evaluation</strong></span>: This is finding out the probability of an observed sequence, given an HMM. From the <a id="id200" class="indexterm"/>number of different HMMs that describe different systems and a sequence of observations, our goal will be to find out which HMM will most probably generate the required sequence. We use the forward algorithm to calculate the probability of an observation sequence when a particular HMM is given and find out the most probable HMM.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Decoding</strong></span>: This is <a id="id201" class="indexterm"/>finding the most probable sequence of hidden states from some observations. We use the Viterbi algorithm to determine the most probable sequence of hidden states when you have a sequence of observations and an HMM.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Learning</strong></span>: Learning is <a id="id202" class="indexterm"/>generating the HMM from a sequence of observations. So, if we have such a sequence, we may wonder which is the most likely model to generate this sequence. The forward-backward algorithms are useful in solving this problem.</li></ul></div><p>The Hidden Markov Model is used in different applications such as speech recognition, handwritten letter recognition, genome analysis, parts of speech tagging, customer behavior modeling, and so on.</p></div>
<div class="section" title="Using Mahout for the Hidden Markov Model"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec27"/>Using Mahout for the Hidden Markov Model</h1></div></div></div><p>Apache <a id="id203" class="indexterm"/>Mahout has the implementation of the Hidden Markov Model. It is available in the <code class="literal">org.apache.mahout.classifier.sequencelearning.hmm</code> package.</p><p>The overall <a id="id204" class="indexterm"/>implementation is provided by eight different classes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">HMMModel</code>: This <a id="id205" class="indexterm"/>is the main class that defines the <a id="id206" class="indexterm"/>Hidden Markov Model.</li><li class="listitem" style="list-style-type: disc"><code class="literal">HmmTrainer</code>: This <a id="id207" class="indexterm"/>class has algorithms that are <a id="id208" class="indexterm"/>used to train the Hidden Markov Model. The main algorithms are supervised learning, unsupervised learning, and unsupervised Baum-Welch.</li><li class="listitem" style="list-style-type: disc"><code class="literal">HmmEvaluator</code>: This <a id="id209" class="indexterm"/>class provides different methods <a id="id210" class="indexterm"/>to evaluate an HMM model. The following use cases are covered in this class:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Generating a sequence of output states from a model (prediction)</li><li class="listitem" style="list-style-type: disc">Computing the likelihood that a given model will generate the given sequence of output states (model likelihood)</li><li class="listitem" style="list-style-type: disc">Computing the most likely hidden sequence for a given model and a given observed sequence (decoding)</li></ul></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">HmmAlgorithms</code>: This <a id="id211" class="indexterm"/>class contains implementations <a id="id212" class="indexterm"/>of the three major HMM algorithms: forward, backward, and Viterbi.</li><li class="listitem" style="list-style-type: disc"><code class="literal">HmmUtils</code>: This is a <a id="id213" class="indexterm"/>utility class and provides <a id="id214" class="indexterm"/>methods to handle HMM model objects.</li><li class="listitem" style="list-style-type: disc"><code class="literal">RandomSequenceGenerator</code>: This <a id="id215" class="indexterm"/>is a command-line tool to generate a sequence <a id="id216" class="indexterm"/>by the given HMM.</li><li class="listitem" style="list-style-type: disc"><code class="literal">BaumWelchTrainer</code>: This <a id="id217" class="indexterm"/>is the class to <a id="id218" class="indexterm"/>train HMM from the console.</li><li class="listitem" style="list-style-type: disc"><code class="literal">ViterbiEvaluator</code>: This <a id="id219" class="indexterm"/>is also a command-line <a id="id220" class="indexterm"/>tool for Viterbi evaluation.</li></ul></div><p>Now, let's <a id="id221" class="indexterm"/>work with Bob's example.</p><p>The following is the <a id="id222" class="indexterm"/>given matrix and the initial probability vector:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Ice cream</p>
</th><th style="text-align: left" valign="bottom">
<p>Cake</p>
</th><th style="text-align: left" valign="bottom">
<p>Juice</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>0.36</p>
</td><td style="text-align: left" valign="top">
<p>0.51</p>
</td><td style="text-align: left" valign="top">
<p>0.13</p>
</td></tr></tbody></table></div><p>The following will be the state transition matrix:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom">
<p>Ice cream</p>
</th><th style="text-align: left" valign="bottom">
<p>Cake</p>
</th><th style="text-align: left" valign="bottom">
<p>Juice</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Ice cream</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.365</p>
</td><td style="text-align: left" valign="top">
<p>0.500</p>
</td><td style="text-align: left" valign="top">
<p>0.135</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Cake</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.250</p>
</td><td style="text-align: left" valign="top">
<p>0.125</p>
</td><td style="text-align: left" valign="top">
<p>0.625</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Juice</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.365</p>
</td><td style="text-align: left" valign="top">
<p>0.265</p>
</td><td style="text-align: left" valign="top">
<p>0.370</p>
</td></tr></tbody></table></div><p>The following will be the emission matrix:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom">
<p>Spicy food</p>
</th><th style="text-align: left" valign="bottom">
<p>Normal food</p>
</th><th style="text-align: left" valign="bottom">
<p>No food</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Ice cream</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.1</p>
</td><td style="text-align: left" valign="top">
<p>0.2</p>
</td><td style="text-align: left" valign="top">
<p>0.7</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Cake</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.5</p>
</td><td style="text-align: left" valign="top">
<p>0.25</p>
</td><td style="text-align: left" valign="top">
<p>0.25</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Juice</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0.80</p>
</td><td style="text-align: left" valign="top">
<p>0.10</p>
</td><td style="text-align: left" valign="top">
<p>0.10</p>
</td></tr></tbody></table></div><p>Now we will execute a command-line-based example of this problem. We have three hidden states of what Bob's eaten for snacks: ice-cream, cake, or juice. Then, we have three observable states of what he is having at lunch: spicy food, normal food, or no food at all. Now, the following are the steps to execute from the command line:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create a directory with the name <code class="literal">hmm: mkdir /tmp/hmm</code>. Go to this directory and create the sample input file of the observed states. This will include a sequence of Bob's lunch habit: spicy food (state 0), normal food (state 1), and no food (state 2). Execute the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>echo "0 1 2 2 2 1 1 0 0 2 1 2 1 1 1 1 2 2 2 0 0 0 0 0 0 2 2 2 0 0 0 0 0 0 1 1 1 1 2 2 2 2 2 0 2 1 2 0 2 1 2 1 1 0 0 0 1 0 1 0 2 1 2 1 2 1 2 1 1 0 0 2 2 0 2 1 1 0" &gt; hmm-input</strong></span>
</pre></div></li><li class="listitem">Run the BaumWelch algorithm to train the model using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mahout baumwelch -i /tmp/hmm/hmm-input -o /tmp/hmm/hmm-model -nh 3 -no 3 -e .0001 -m 1000</strong></span>
</pre></div><p>The parameters <a id="id223" class="indexterm"/>used in the preceding command are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">i</code>: This is the input file location</li><li class="listitem" style="list-style-type: disc"><code class="literal">o</code>: This is the output location for the model</li><li class="listitem" style="list-style-type: disc"><code class="literal">nh</code>: This is the number of hidden states. In our example, it is three (ice cream, juice, or cake)</li><li class="listitem" style="list-style-type: disc"><code class="literal">no</code>: This is the number of observable states. In our example, it is three (spicy, normal, or no food)</li><li class="listitem" style="list-style-type: disc"><code class="literal">e</code>: This is the epsilon number. This is the convergence threshold value</li><li class="listitem" style="list-style-type: disc"><code class="literal">m</code>: This is the maximum iteration number</li></ul></div><p>The <a id="id224" class="indexterm"/>following screenshot shows the output on executing the previous command:</p><div class="mediaobject"><img src="graphics/4959OS_05_05.jpg" alt="Using Mahout for the Hidden Markov Model"/></div></li><li class="listitem">Now we have an HMM model that can be used to build a predicted sequence. We will run the model to predict the next 15 states of the observable sequence using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mahout hmmpredict -m /tmp/hmm/hmm-model -o /tmp/hmm/hmm-predictions -l 10</strong></span>
</pre></div><p>The parameters <a id="id225" class="indexterm"/>used in the preceding command are as follows:</p><p>
<code class="literal">m</code>: This is the path for the HMM model</p><p>
<code class="literal">o</code>: This is the output directory path</p><p>
<code class="literal">l</code>: This is the length of the generated sequence</p></li><li class="listitem">To view the <a id="id226" class="indexterm"/>prediction for the next 10 observable states, use the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mahout hmmpredict -m /tmp/hmm/hmm-model -o /tmp/hmm/hmm-predictions -l 10</strong></span>
</pre></div><p>The output of the previous command is shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/4959OS_05_06.jpg" alt="Using Mahout for the Hidden Markov Model"/></div><p>From the output, we can say that the next observable states for Bob's lunch will be spicy, spicy, spicy, normal, normal, no food, no food, no food, no food, and no food.</p></li><li class="listitem">Now, we will use one more algorithm to predict the hidden state. We will use the Viterbi algorithm to predict the hidden states for a given observational state's sequence. We will first create the sequence of the observational state using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>echo "0 1 2 0 2 1 1 0 0 1 1 2" &gt; /tmp/hmm/hmm-viterbi-input</strong></span>
</pre></div></li><li class="listitem">We will use the Viterbi command-line option to generate the output with the likelihood of generating this sequence:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mahout viterbi --input /tmp/hmm/hmm-viterbi-input --output tmp/hmm/hmm-viterbi-output --model /tmp/hmm/hmm-model --likelihood</strong></span>
</pre></div><p>The parameters used in the preceding command are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">input</code>: This <a id="id227" class="indexterm"/>is the input location of the file</li><li class="listitem" style="list-style-type: disc"><code class="literal">output</code>: This is the <a id="id228" class="indexterm"/>output location of the Viterbi algorithm's output</li><li class="listitem" style="list-style-type: disc"><code class="literal">model</code>: This is the HMM <a id="id229" class="indexterm"/>model location that we created earlier</li><li class="listitem" style="list-style-type: disc"><code class="literal">likelihood</code>: This <a id="id230" class="indexterm"/>is the computed likelihood of the observed sequence</li></ul></div><p>The following <a id="id231" class="indexterm"/>screenshot shows the output on <a id="id232" class="indexterm"/>executing the previous command:</p><div class="mediaobject"><img src="graphics/4959OS_05_07.jpg" alt="Using Mahout for the Hidden Markov Model"/></div></li><li class="listitem">Predictions <a id="id233" class="indexterm"/>from the Viterbi are saved in the output file and can be seen using the <code class="literal">cat</code> command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cat /tmp/hmm/hmm-viterbi-output</strong></span>
</pre></div><p>The following output shows the predictions for the hidden state:</p><div class="mediaobject"><img src="graphics/4959OS_05_08.jpg" alt="Using Mahout for the Hidden Markov Model"/></div></li></ol></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec28"/>Summary</h1></div></div></div><p>In this chapter, we discussed another classification technique: the Hidden Markov Model. You learned about deterministic and nondeterministic patterns. We also touched upon the Markov process and Hidden Markov process in general. We checked the classes implemented inside Mahout to support the Hidden Markov Model. We took up an example to create the HMM model and further used this model to predict the observational state's sequence. We used the Viterbi algorithm implemented in Mahout to predict the hidden states in the system.</p><p>Now, in the next chapter, we will cover one more interesting algorithm used in classification area: Random forest.</p></div></body></html>