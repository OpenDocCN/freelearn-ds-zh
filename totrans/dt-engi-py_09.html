<html><head></head><body>
		<div id="_idContainer119">
			<h1 id="_idParaDest-84"><em class="italic"><a id="_idTextAnchor086"/>Chapter 7</em>: Features of a Production Pipeline</h1>
			<p>In this chapter, you will learn several features that make a data pipeline ready for production. You will learn about building data pipelines that can be run multiple times without changing the results (idempotent). You will also learn what to do if transactions fail (atomicity). And you will learn about validating data in a staging environment. This chapter will use a sample data pipeline that I currently run in production.</p>
			<p>For me, this pipeline is a bonus, and I am not concerned with errors, or missing data. Because of this, there are elements missing in this pipeline that should be present in a mission critical, or production, pipeline. Every data pipeline will have different acceptable rates of errors – missing data – but in production, your pipelines should have some extra features that you have yet to learn.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Staging and validating data</li>
				<li>Building idempotent data pipelines</li>
				<li>Building atomic data pipelines</li>
			</ul>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor087"/>Staging and validating data</h1>
			<p>When building production data pipelines, staging and validating data become extremely important. While you have seen basic data validation and cleaning in <a href="B15739_05_ePub_AM.xhtml#_idTextAnchor063"><em class="italic">Chapter 5</em></a><em class="italic">, Cleaning, Transforming, and Enriching Data</em>, in production, you will need a more formal and <a id="_idIndexMarker399"/>automated way of performing these tasks. The next two sections will walk you through how to accomplish staging and validating data in production.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor088"/>Staging data</h2>
			<p>In the NiFi data pipeline examples, data was<a id="_idIndexMarker400"/> extracted, and then passed along a series of connected processors. These processors performed some tasks on the data and sent the results to the next processor. But what happens if a processor fails? Do you start all over from the beginning? Depending on the source data, that may be impossible. This is where staging comes in to play. We will divide staging in to two different types: the staging of files or <a id="_idIndexMarker401"/>database dumps, and the staging of data in a database that is ready to be loaded into a warehouse.</p>
			<h3>Staging of files</h3>
			<p>The first type of staging we will discuss is the<a id="_idIndexMarker402"/> staging of data in files following extraction from a source, usually a transactional database. Let's walk through a common scenario to see why we would need this type of staging.</p>
			<p>You are a data<a id="_idIndexMarker403"/> engineering at Widget Co – a company that has disrupted widget making and is the only online retailer of widgets. Every day, people from all over the world order widgets on the company website. Your boss has instructed you to build a data pipeline that takes sales from the website and puts them in a data warehouse every hour so that analysts can query the data and create reports.</p>
			<p>Since sales are worldwide, let's assume the only data transformation required is the conversion of the local sales date and time to be in GMT. This data pipeline should be straightforward and is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/Figure_7.1_B15739.jpg" alt="Figure 7.1 – A data pipeline to load widget sales into a warehouse&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – A data pipeline to load widget sales into a warehouse</p>
			<p>The preceding data pipeline queries the widget database. It passes the records as a single flowfile to the <strong class="source-inline">SplitText</strong> processor, which sends each record to the processor, which will convert the date and time to GMT. Lastly, it loads the results in the data warehouse. </p>
			<p>But what happens when you <a id="_idIndexMarker404"/>split the records, and then a date conversion fails? You can just re-query the database, right? No, you can't, because transactions are happening every minute and the transaction that failed was canceled and is no longer in the database, or they changed their order and now want a red widget and not the five blue widgets they initially ordered. Your <a id="_idIndexMarker405"/>marketing team will not be happy because they no longer know about these changes and cannot plan for how to convert these sales.</p>
			<p>The point of the example is to demonstrate that in a transactional database, transactions are constantly happening, and data is being modified. Running a query produces a set of results that may be completely different if you run the same query 5 minutes later, and you have now lost that original data. This is why you need to stage your extracts.</p>
			<p>If the preceding pipeline example is used for staging, you will end up with a pipeline like the example shown in the following screenshot:</p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/Figure_7.2_B15739.jpg" alt="Figure 7.2 – A data pipeline to load widget sales into a warehouse using staging&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – A data pipeline to load widget sales into a warehouse using staging</p>
			<p>The preceding data pipeline is displayed as two<a id="_idIndexMarker406"/> graphs. The first graph queries the widget database and puts the results in a file on disk. This is the staging step. From here, the next graph will load the data from the staging file, split the records into flowfiles, convert the dates and<a id="_idIndexMarker407"/> times, and finally, load it into the warehouse. If this portion of the pipeline crashes, or you need to replay your pipeline for any reason, you can then just reload the CSV by restarting the second half of the data pipeline. You have a copy of the database at the time of the original query. If, 3 months from now, your warehouse is corrupted, you could replay your data pipeline with the data at every query, even though the database is completely different.</p>
			<p>Another benefit of having copies of database extracts in CSV files is that it reduces the load in terms of replaying your pipeline. If your queries are resource intensive, perhaps they can only be run at night, or if the systems you query belong to another department, agency, or company. Instead of having to use their resources again to fix a mistake, you can just use the copy. </p>
			<p>In the Airflow data pipelines you have built up to this point, you have staged your queries. The way Airflow works <a id="_idIndexMarker408"/>encourages good practices. Each task has saved the results to a file, and then you have loaded that file in the next task. In NiFi, however, your queries have been sent, usually to the <strong class="source-inline">SplitRecords</strong> or <strong class="source-inline">Text</strong> processor, to the next<a id="_idIndexMarker409"/> processor in the pipeline. This is not good practice for running pipelines in production and will no longer be the case in examples from here on in.</p>
			<h3>Staging in databases</h3>
			<p>Staging data in files is helpful <a id="_idIndexMarker410"/>during the extract phase of a data pipeline. On the other end<a id="_idIndexMarker411"/> of the pipeline, the load stage, it is better to stage your data in a database, and preferably, the same database as the warehouse. Let's walk through another example to see why.</p>
			<p>You have queried your data widget database and staged the data. The next data pipeline picks up the data, transforms it, and then loads it into the warehouse. But now what happens if loading does not work properly? Perhaps records went in and everything looks successful, but the mapping is wrong, and dates are strings. Notice I didn't say the load failed. You will learn about handling load failures later in this chapter. </p>
			<p>Without actually loading the data into a database, you will only be able to guess what issues you may experience. By staging, you will load the data into a replica of your data warehouse. Then you can run validation suites and queries to see whether you get the results you expect – for example, you could run a <strong class="source-inline">select count(*)</strong> query from the table to see whether you get the correct number of records back. This will allow you to know exactly what issues you may have, or don't have, if all went well.</p>
			<p>A data pipeline for Widget Co that uses staging at both ends of the pipeline should look like the pipeline in the following screenshot:</p>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/Figure_7.3_B15739.jpg" alt="Figure 7.3 – A production using staging at both ends of the pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – A production using staging at both ends of the pipeline</p>
			<p>The data pipeline in the <a id="_idIndexMarker412"/>preceding screenshot queries the widget database and stages the results in a file. The next stage picks up the file and converts the dates and times. The point of departure from the earlier example is that the data pipeline now loads the <a id="_idIndexMarker413"/>data into a replica of the data warehouse. The new segment of the data pipeline then queries this replica, performs some validation, and then loads it into the final database or warehouse.</p>
			<p class="callout-heading">ETL versus ELT</p>
			<p class="callout">So far, you <a id="_idIndexMarker414"/>have seen Extract, Transform, and Load. However, there is a growing shift toward an Extract, Load, and Transform process. In the ELT process, data is staged in a database immediately after the extract ion without any transformations. You handle all of the transformations in the database. This is very helpful if you are using SQL-based transformation tools. There is no right or wrong way, only preferences and use cases.</p>
			<p>By staging data at the front and end of your data pipeline, you are now better suited for handling errors and for validating the data as it moves through your pipeline. Do not think that these are the only two places where data can be staged, or that data must be staged in files. You can stage your data after every transformation in your data pipeline. Doing so will make debugging errors <a id="_idIndexMarker415"/>easier and allow you to pick up at any point in the data pipeline after an error. As your transformations become more time consuming, this may become more helpful. </p>
			<p>You staged the extraction from the widget database in a file, but there is no reason to prevent you from extracting the data to a relational or noSQL database. Dumping data to files is slightly less complicated than loading it into a database – you don't need to handle schemas or build any additional infrastructure. </p>
			<p>While staging data is helpful for replaying pipelines, handling errors, and debugging your pipeline, it is also helpful in the validation stages of your pipeline. In the next section, you will learn how to use Great Expectations to build validation suites on both file and database staged data.</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor089"/>Validating data with Great Expectations</h2>
			<p>With your data staged in either a file or<a id="_idIndexMarker416"/> a database, you have the perfect opportunity to validate it. In <a href="B15739_05_ePub_AM.xhtml#_idTextAnchor063"><em class="italic">Chapter 5</em></a><em class="italic">, Cleaning, Transforming, and Enriching Data</em>, you used pandas to perform exploratory data analysis and gain insight into what columns existed, find counts of null values, look at ranges of values within columns, and examine the data types in each column. Pandas is powerful and, by using methods such as <strong class="source-inline">value_counts</strong> and <strong class="source-inline">describe</strong>, you can gain a lot of insight into your data, but there are tools that make validation much cleaner and make your <a id="_idIndexMarker417"/>expectations of the data much more obvious. </p>
			<p>The library you will learn about in this section is <strong class="bold">Great Expectations</strong>. The following is a screenshot of the Great Expectations home page, where you can join and get involved with it:</p>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/Figure_7.4_B15739.jpg" alt="Figure 7.4 – Great Expectations Python library for validating your data, and more&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – Great Expectations Python library for validating your data, and more</p>
			<p>Why Great Expectations? Because <a id="_idIndexMarker418"/>with Great Expectations, you can specify human-readable expectations and let the library handle the implementation. For example, you can specify that the <strong class="source-inline">age</strong> column should not have null values, in your code, with the following line:</p>
			<p class="source-code">expect_column_values_to_not_be_null('age')</p>
			<p>Great Expectations will handle the logic behind doing this irrespective of whether your data is in a DataFrame or in a database. The same expectation will run on either data context.</p>
			<h3>Getting started with Great Expectations</h3>
			<p>Installing Great Expectations<a id="_idIndexMarker419"/> can be done with <strong class="source-inline">pip3</strong> as shown:</p>
			<p class="source-code">pip3 install great_expectations </p>
			<p>To view the documents that Great Expectations generates, you will also need to have Jupyter Notebook available on your machine. You can install Notebook with <strong class="source-inline">pip3</strong> as well:</p>
			<p class="source-code">pip3 install jupyter</p>
			<p>With the requirements installed, you can<a id="_idIndexMarker420"/> now set up a project. Create a directory at <strong class="source-inline">$HOME/peoplepipeline</strong> and press <em class="italic">Enter</em>. You can do this on Linux using the following commands:</p>
			<p class="source-code">mkdir $HOME/peoplepipeline</p>
			<p class="source-code">cd $HOME/peoplepipeline</p>
			<p>Now that you are in the project directory, before you set up Great Expectations, we will dump a sample of the data we will be working with. Using the code from <a href="B15739_03_ePub_AM.xhtml#_idTextAnchor039"><em class="italic">Chapter 3</em></a><em class="italic">, Reading and Writing Files</em>, we will generate 1,000 records relating to people. The code is as follows:</p>
			<p class="source-code">from faker import Faker</p>
			<p class="source-code">import csv</p>
			<p class="source-code">output=open('people.csv','w')</p>
			<p class="source-code">fake=Faker()</p>
			<p class="source-code">header=['name','age','street','city','state','zip','lng','lat']</p>
			<p class="source-code">mywriter=csv.writer(output)</p>
			<p class="source-code">mywriter.writerow(header)</p>
			<p class="source-code">for r in range(1000):</p>
			<p class="source-code">    mywriter.writerow([fake.name(),fake.random_int(min=18,</p>
			<p class="source-code">    max=80, step=1), fake.street_address(), fake.city(),fake.</p>
			<p class="source-code">    state(),fake.zipcode(),fake.longitude(),fake.latitude()])</p>
			<p class="source-code">output.close()</p>
			<p>The preceding code creates a CSV file with records about people. We will put this CSV file into the project directory.</p>
			<p>Now you can set up Great Expectations on this project by using the command-line interface. The following line will initialize your project:</p>
			<p class="source-code">great_expectations init</p>
			<p>You will now walk through a series of steps to configure Great Expectations. First, Great Expectations will ask you whether you are ready to proceed. Your terminal should look like the following screenshot:</p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/Figure_7.5_B15739.jpg" alt="Figure 7.5 – Initializing Great Expectations on a project&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – Initializing Great Expectations on a project</p>
			<p>Having entered <em class="italic">Y</em> and pressed <em class="italic">Enter</em>, you will be <a id="_idIndexMarker421"/>prompted with a series of questions: </p>
			<p class="source-code">What data would you like Great Expectations to connect to?</p>
			<p class="source-code">What are you processing your files with?</p>
			<p class="source-code">Enter the path (relative or absolute) of a data file.</p>
			<p class="source-code">Name the new expectation suite [people.warning].</p>
			<p>The answers to the questions are shown in the following screenshot, but it should be <strong class="source-inline">Files</strong>, <strong class="source-inline">Pandas</strong>, where you put <a id="_idIndexMarker422"/>your file, and whatever you would like to name it:</p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/Figure_7.6_B15739.jpg" alt="Figure 7.6 – Initializing Great Expectations by answering questions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – Initializing Great Expectations by answering questions</p>
			<p>When Great Expectations has finished running, it will tell you it's done, give you a path to the document it has generated, and open the document in your browser. The documents will look like the<a id="_idIndexMarker423"/> following screenshot:</p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/Figure_7.7_B15739.jpg" alt="Figure 7.7 – Documentation generated by Great Expectations &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7 – Documentation generated by Great Expectations </p>
			<p>The preceding screenshot shows the documentation generated for the Great Expectations Suite. You can see there are <strong class="bold">11</strong> expectations and we have passed all of them. The expectations are very basic, specifying how many records should exist and what columns should exist in what order. Also, in the code I specified an age range. So, <strong class="bold">age</strong> has a minimum and maximum value. Ages have to be greater than 17 and less than 81 to pass the validation. You can see a sample of the expectations generated by scrolling. I have shown some of mine in the following screenshot:</p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/Figure_7.8_B15739.jpg" alt="Figure 7.8 – Sample generated expectations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8 – Sample generated expectations</p>
			<p>As you can see, the expectations are <a id="_idIndexMarker424"/>very rigid – age must never be null, for example. Let's edit the expectations. You have installed Jupyter Notebook, so you can run the following command to launch your expectation suite in a single step:</p>
			<p class="source-code">great_expectations suite edit people.validate</p>
			<p>Your browser will open a Jupyter notebook and should look like the following screenshot:</p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/Figure_7.9_B15739.jpg" alt="Figure 7.9 – Your expectation suite in a Jupyter notebook&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.9 – Your expectation suite in a Jupyter notebook</p>
			<p>Some items should stand out in the code – the <a id="_idIndexMarker425"/>expectation suite name, and the path to your data file in the <strong class="source-inline">batch_kwargs</strong> variable. As you scroll through, you will see the expectations with headers for their type. If you scroll to the <strong class="source-inline">Table_Expectation(s)</strong> header, I will remove the row count expectation by deleting the cell, or by deleting the code in the cell, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/Figure_7.10_B15739.jpg" alt="Figure 7.10 – Table Expectation(s)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.10 – Table Expectation(s)</p>
			<p>The other expectation to edit is under the <strong class="source-inline">age</strong> header. I will remove an expectation, specifically, the <strong class="source-inline">expect_quantile_values_to_be_between</strong> expectation. The exact line is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/Figure_7.11_B15739.jpg" alt="Figure 7.11 – Age expectations with the quantile expectations to be removed&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.11 – Age expectations with the quantile expectations to be removed</p>
			<p>You can continue to<a id="_idIndexMarker426"/> remove expectations, or you can add new ones, or even just modify the values of existing expectations. You can find a glossary of available expectations at <a href="https://docs.greatexpectations.io/en/latest/reference/glossary_of_expectations.html">https://docs.greatexpectations.io/en/latest/reference/glossary_of_expectations.html</a>.</p>
			<p>Once you have made all of the changes and are satisfied, you can run the entire notebook to save the changes to your expectation suite. The following screenshot shows how to do that – select <strong class="bold">Cell</strong> | <strong class="bold">Run All</strong>:</p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/Figure_7.12_B15739.jpg" alt="Figure 7.12 – Saving the changes to your expectation suite by running the notebook&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.12 – Saving the changes to your expectation suite by running the notebook</p>
			<p>Now that you have an <a id="_idIndexMarker427"/>expectation suite, it is time to add it to your pipeline. In the next two sections, you will learn how to add it alongside your pipeline for use with NiFi or embed the code into your pipeline for use with Airflow.</p>
			<h3>Great Expectations outside the pipeline</h3>
			<p>So far, you have <a id="_idIndexMarker428"/>validated data while you edited the expectation suite inside a Jupyter notebook. You could continue to do that using a library such as Papermill, but that is beyond the scope of this book. In this section, however, you will create a tap and run it from NiFi.</p>
			<p class="callout-heading">Papermill</p>
			<p class="callout">Papermill is a library created at Netflix that allows <a id="_idIndexMarker429"/>you to create parameterized Jupyter notebooks and run them from the command line. You can change parameters and specify an output directory for the resultant notebook. It pairs well with another Netflix library, Scrapbook. Find them both, along with other interesting projects, including Hydrogen, at <a href="https://github.com/nteract">https://github.com/nteract</a>.</p>
			<p>A tap is how Great Expectations creates executable Python files to run against your expectation suite. You can create a new tap using the command-line interface, as shown:</p>
			<p class="source-code">great_expectations tap new people.validate peoplevalidatescript.py</p>
			<p>The preceding command takes an <a id="_idIndexMarker430"/>expectation suite and the name of a Python file to create. When it runs, it will ask you for a data file. I have pointed it to the <strong class="source-inline">people.csv</strong> file that you used in the preceding section when creating the suite. This is the file that the data pipeline will overwrite as it stages data:</p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/Figure_7.13_B15739.jpg" alt="Figure 7.13 – Result of the Python file at the specified location&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.13 – Result of the Python file at the specified location</p>
			<p>If you run the tap, you should see that it succeeded, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/Figure_7.14_B15739.jpg" alt="Figure 7.14 – Great Expectation tap run&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.14 – Great Expectation tap run</p>
			<p>You are now ready to build a pipeline in NiFi and validate your data using Great Expectations. The next section will <a id="_idIndexMarker431"/>walk you through the process.</p>
			<h4>Great Expectations in NiFi</h4>
			<p>Combining NiFi and Great Expectations <a id="_idIndexMarker432"/>requires a few modifications to the tap<a id="_idIndexMarker433"/> you created in the previous section. First, you will need to change all the exits to be <strong class="source-inline">0</strong>. If you have a <strong class="source-inline">system.exit(1)</strong> exit, NiFi processors will crash because the script failed. We want the script to close successfully, even if the results are not, because the second thing you will change are the <strong class="source-inline">print</strong> statements. Change the <strong class="source-inline">print</strong> statements to be a JSON string with a result key and a pass or fail value. Now, even though the script exits successfully, we will know in NiFi whether it actually passed or failed. The code of the tap is shown in the following code block, with the modifications in bold:</p>
			<p class="source-code">import sys</p>
			<p class="source-code">from great_expectations import DataContext</p>
			<p class="source-code">context = DataContext("/home/paulcrickard/peoplepipeline/great_expectations")</p>
			<p class="source-code">suite = context.get_expectation_suite("people.validate")</p>
			<p class="source-code">batch_kwargs = {</p>
			<p class="source-code">    "path": "/home/paulcrickard/peoplepipeline/people.csv",</p>
			<p class="source-code">    "datasource": "files_datasource",</p>
			<p class="source-code">    "reader_method": "read_csv",</p>
			<p class="source-code">}</p>
			<p class="source-code">batch = context.get_batch(batch_kwargs, suite)</p>
			<p class="source-code">results = context.run_validation_operator(</p>
			<p class="source-code">                               "action_list_operator", [batch])</p>
			<p class="source-code">if not results["success"]:</p>
			<p class="source-code">    <strong class="bold">print('{"result":"fail"}')</strong></p>
			<p class="source-code"><strong class="bold">    sys.exit(0)</strong></p>
			<p class="source-code"><strong class="bold">print('{"result":"pass"}')</strong></p>
			<p class="source-code">sys.exit(0)</p>
			<p>With the changes<a id="_idIndexMarker434"/> to the tap complete, you can now build a data <a id="_idIndexMarker435"/>pipeline in NiFi. The following screenshot is the start of a data pipeline using the tap:</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/Figure_7.15_B15739.jpg" alt="Figure 7.15 – A NiFi data pipeline using Great Expectations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.15 – A NiFi data pipeline using Great Expectations</p>
			<p>The preceding data pipeline creates 1,000 records and saves it as a CSV file. It then runs the tap on the data and reads in the result — the pass or fail JSON from the script. Lastly, it extracts the result and routes the flowfile to either a pass or fail processor. From there, your data pipeline can continue, or it can log the error. You will walk through the<a id="_idIndexMarker436"/> pipeline in the following steps:</p>
			<ol>
				<li>The data pipeline<a id="_idIndexMarker437"/> starts by generating a fake flowfile without any data to trigger the next processor. You could replace this processor with one that queries your transactional database, or that reads files from your data lake. I have scheduled this processor to run every hour.</li>
				<li>Once the empty flowfile is received, the <strong class="source-inline">ExecuteStreamCommand</strong> processor calls the <strong class="source-inline">loadcsv.py</strong> Python script. This file is from <a href="B15739_03_ePub_AM.xhtml#_idTextAnchor039"><em class="italic">Chapter 3</em></a><em class="italic">, Reading and Writing Files</em>, and uses <strong class="source-inline">Faker</strong> to create 1,000 fake people records. The <strong class="source-inline">ExecuteStreamCommand</strong> processor will read the output from the script. If you had print statements, each line would become a flowfile. The script has one output, and that is <strong class="source-inline">{"status":"Complete"}</strong>. </li>
				<li>To configure the processor to run the script, you can set the <strong class="bold">Working Directory</strong> to the path of your Python script. Set <strong class="bold">Command Path</strong> to <strong class="source-inline">python3</strong> – if you can run the command with the full path, you do not need to enter it all. Lastly, set <strong class="bold">Command Arguments</strong> to the name of the Python file – <strong class="source-inline">loadcsv.py</strong>. When the processor runs, the output flowfile is shown in the following screenshot:<div id="_idContainer114" class="IMG---Figure"><img src="image/Figure_7.16_B15739.jpg" alt="Figure 7.16 – The flowfile shows the JSON string&#13;&#10;"/></div><p class="figure-caption">Figure 7.16 – The flowfile shows the JSON string</p></li>
				<li>The next processor is also an <strong class="source-inline">ExecuteStreamCommand</strong> processor. This time, the script will be <a id="_idIndexMarker438"/>your tap. The configuration should be the same as in the previous step, except <strong class="bold">Command Argument</strong> will be <strong class="source-inline">peoplevalidatescript.py</strong>. Once the processor completes, the flowfile will<a id="_idIndexMarker439"/> contain JSON with a result of pass or fail. The <strong class="source-inline">pass</strong> flowfile is shown in the following screenshot:<div id="_idContainer115" class="IMG---Figure"><img src="image/Figure_7.17_B15739.jpg" alt="Figure 7.17 – Result of the tap, validation passed&#13;&#10;"/></div><p class="figure-caption">Figure 7.17 – Result of the tap, validation passed</p></li>
				<li>The value of the result is extracted in the next processor – <strong class="source-inline">EvaluateJsonPath</strong>. Adding a new property with the plus button, name it <strong class="source-inline">result</strong> and set the value to <strong class="source-inline">$.result</strong>. This will extract the <strong class="source-inline">pass</strong> or <strong class="source-inline">fail</strong> value and send it as a flowfile attribute.</li>
				<li>The next process is <strong class="source-inline">RouteOnAttribute</strong>. This processor allows you to create properties that can be used as a relationship in a connection to another processor, meaning you can send each property to a different path. Creating two new properties – <strong class="source-inline">pass</strong> and <strong class="source-inline">fail</strong>, the values are shown in the following code snippet:<p class="source-code"><strong class="bold">${result:startsWith('pass')}</strong></p><p class="source-code"><strong class="bold">${result:startsWith('fail')}</strong></p></li>
				<li>The preceding command uses the NiFi expression language to read the value of the result attribute in the flowfile.</li>
				<li>From here, I have terminated the data pipeline at a <strong class="source-inline">PutFile</strong> processor. But you would now be<a id="_idIndexMarker440"/> able to continue by connecting a <strong class="source-inline">pass</strong> and <strong class="source-inline">fail</strong> path to their respective relationships in the previous processor. If it passed, you could read the staged file and insert the data into the warehouse.</li>
			</ol>
			<p>In this section, you<a id="_idIndexMarker441"/> connected Great Expectations to your data pipeline. The tap was generated using your data, and because of this, the test passed. The pipeline ended with the file being written to disk. However, you could continue the data pipeline to route success to a data warehouse. In the real world, your tests will fail on occasion. In the next section, you will learn how to handle failed tests.</p>
			<h3>Failing the validation</h3>
			<p>The validation will always pass because the<a id="_idIndexMarker442"/> script we are using generates records that meet the validations rules. What if we changed the script? If you edit the <strong class="source-inline">loadcsv.py</strong> script and change the minimum and maximum age, we can make the validation fail. The edit is shown as follows:</p>
			<p class="source-code">fake.random_int(min=1, max=100, step=1)</p>
			<p>This will create records that are below the minimum and above the maximum—hopefully, because it is random, but 1,000 records should get us there. Once you have edited the script, you can rerun the data pipeline. The final flowfile should have been routed to the <strong class="source-inline">fail</strong> path. Great Expectations creates documents for your validations. If you remember, you saw them initially when you created the validation suite. Now you will have a record of both the passed and failed runs. Using your browser, open the documents. The path is within your project folder. For example, my docs are at the following path:</p>
			<p><strong class="source-inline">file:///home/paulcrickard/peoplepipeline/great_expectations/uncommitted/data_docs/local_site/validations/people/validate/20200505T145722.862661Z/6f1eb7a06079eb9cab8de404c6faa b62.html</strong></p>
			<p>The documents should show all your validations runs. The documents will look like the following screenshot:</p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/Figure_7.18_B15739.jpg" alt="Figure 7.18 – Results of multiple validation runs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.18 – Results of multiple validation runs</p>
			<p>The preceding screenshot shows <a id="_idIndexMarker443"/>all of the validation runs. You can see the red <strong class="bold">x</strong> indicating failures. Click on one of the failed runs to see which expectations were not met. The results should be that both the minimum and maximum age were not met. You should see that this is the case, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/Figure_7.19_B15739.jpg" alt="Figure 7.19 – Age expectations have not been met&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.19 – Age expectations have not been met</p>
			<p>In this section, you have created<a id="_idIndexMarker444"/> a Great Expectations suite and specified expectations for your data. Previously, you would have had to do this manually using DataFrames and a significant amount of code. Now you can use human-readable statements and allow Great Expectations to do the work. You have created a tap that you can run inside your NiFi data pipeline — or that you can schedule using Cron or any other tool.</p>
			<h3>A quick note on Airflow</h3>
			<p>In the preceding example, you<a id="_idIndexMarker445"/> ran the validation suite outside of your pipeline – the script ran in the pipeline, but was called by a processor. You can also run the code inside the pipeline without having to call it. In Apache Airflow, you can create a validation task that has the code from the tap. To handle the failure, you would need to raise an exception. To do that, import the library in your Airflow code. I have included the libraries that you need to include on top of your standard boilerplate in the following code block:</p>
			<p class="source-code"><strong class="bold">import sys</strong></p>
			<p class="source-code"><strong class="bold">from great_expectations import DataContext</strong></p>
			<p class="source-code"><strong class="bold">from airflow.exceptions import AirflowException</strong></p>
			<p class="source-code">from airflow import DAG</p>
			<p class="source-code">from airflow.operators.bash_operator import BashOperator</p>
			<p class="source-code">from airflow.operators.python_operator import PythonOperator</p>
			<p>After importing all of the libraries, you can <a id="_idIndexMarker446"/>write your task, as shown in the following code block:</p>
			<p class="source-code">def validateData():</p>
			<p class="source-code">	context = DataContext("/home/paulcrickard/peoplepipeline/great_expectations")</p>
			<p class="source-code">	suite = context.get_expectation_suite("people.validate")</p>
			<p class="source-code">	batch_kwargs = {</p>
			<p class="source-code">    	"path": "/home/paulcrickard/peoplepipeline/people.csv",</p>
			<p class="source-code">    	"datasource": "files_datasource",</p>
			<p class="source-code">    	"reader_method": "read_csv",</p>
			<p class="source-code">}</p>
			<p class="source-code">	batch = context.get_batch(batch_kwargs, suite)</p>
			<p class="source-code">	results = context.run_validation_operator(</p>
			<p class="source-code">                               "action_list_operator", [batch])</p>
			<p class="source-code">	if not results["success"]:</p>
			<p class="source-code">    		raise AirflowException("Validation Failed")</p>
			<p>The preceding code will throw an error, or it will end if the validation succeeded. However, choosing to handle the failure is up to you. All you need to do is check whether <strong class="source-inline">results["success"]</strong> is <strong class="source-inline">True</strong>. You can now code the other functions, create the tasks using <strong class="source-inline">PythonOperator</strong>, and then <a id="_idIndexMarker447"/>set the downstream relationships as you have in all the other Airflow examples.</p>
			<p>The following sections will discuss two other features of a production data pipeline – idempotence and atomicity.</p>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor090"/>Building idempotent data pipelines</h1>
			<p>A crucial feature of a production data pipeline is that it is idempotent. Idempotent is defined as <em class="italic">denoting an element of a set that is unchanged in value when multiplied or otherwise operated on by itself</em>.</p>
			<p>In data science, this means that<a id="_idIndexMarker448"/> when your pipeline fails, which is not a matter of <em class="italic">if</em>, but <em class="italic">when</em>, it can be rerun and the results are the same. Or, if you accidently click run on your pipeline three times in a row by mistake, there are not duplicate records – even if you accidently click run multiple times in a row.</p>
			<p>In <a href="B15739_03_ePub_AM.xhtml#_idTextAnchor039"><em class="italic">Chapter 3</em></a><em class="italic">, Reading and Writing Files</em>, you created a data pipeline that generated 1,000 records of people and put that data in an Elasticsearch database. If you let that pipeline run every 5 minutes, you would have 2,000 records after 10 minutes. In this example, the records are all random and you may be OK. But what if the records were rows queried from another system?</p>
			<p>Every time the pipeline runs, it would insert the same records over and over again. How you create idempotent data pipelines depends on what systems you are using and how you want to store your data.</p>
			<p>In the SeeClickFix data pipeline from the previous chapter, you queried the SeeClickFix API. You did not specify any rolling time frame that would only grab the most recent records, and your backfill code grabbed all the archived issues. If you run this data pipeline every 8 hours, as it was scheduled, you will grab new issues, but also issues you already have. </p>
			<p>The SeeClickFix data pipeline used the <strong class="source-inline">upsert</strong> method in Elasticsearch to make the pipeline idempotent. Using the <strong class="source-inline">EvaluteJsonPath</strong> processor, you extracted the issue ID and then used that as the <strong class="source-inline">Identifier Attribute</strong> in the <strong class="source-inline">PutElasticsearchHttp</strong> processor. You also set the <strong class="bold">Index Operation</strong> to <strong class="source-inline">upsert</strong>. This is the equivalent of using an update in SQL. No records will be duplicated, and records will only be modified if there have been changes. </p>
			<p>Another way to make the<a id="_idIndexMarker449"/> data pipeline idempotent, and one that is advocated by some functional data engineering advocates, is to create a new index or partition every time your data pipeline is run. If you named your index with the datetime stamped as a suffix, you would get a new index with distinct records every time the pipeline runs. This not only makes the data pipeline idempotent; it creates an immutable object out of your database indexes. An index will never change; just new indexes will be added.</p>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor091"/>Building atomic data pipelines</h1>
			<p>The final feature of a production data <a id="_idIndexMarker450"/>pipeline that we will discuss in this chapter is atomicity. Atomicity means that if a single operation in a transaction fails, then all of the operations fail. If you are inserting 1,000 records into the database, as you did in <a href="B15739_03_ePub_AM.xhtml#_idTextAnchor039"><em class="italic">Chapter 3</em></a><em class="italic">, Reading and Writing Files</em>, if one record fails, then all 1,000 fail. </p>
			<p>In SQL databases, the database will roll back all the changes if record number 500 fails, and it will no longer attempt to continue. You are now free to retry the transaction. Failures can occur for many reasons, some of which are beyond your control. If the power or the network goes down while you are inserting records, do you want those records to be saved to the database? You would then need to determine which records in a transaction succeeded and which failed and then retry only the failed records. This would be much easier than retrying the entire transaction. </p>
			<p>In the NiFi data pipelines you have built, there was no atomicity. In the SeeClickFix example, each issue was sent as a flowfile and upserted in Elasticsearch. The only atomicity that existed is that every field in the document (issue) succeeded or failed. But we could have had a situation where all the issues failed except one, and that would have resulted in the data pipeline succeeding.</p>
			<p>Elasticsearch does not have atomic transactions, so any data pipeline that implements Elasticsearch would need to handle that within the logic. For example, you could track every record that is indexed in Elasticsearch as well as every failure relationship. If there is a failure relationship during the run, you would then delete all the successfully indexed issues. An example data<a id="_idIndexMarker451"/> pipeline is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/Figure_7.20_B15739.jpg" alt="Figure 7.20 – Building atomicity into a data pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.20 – Building atomicity into a data pipeline</p>
			<p>The preceding data pipeline created two flowfiles; one succeeded and one failed. The contents of both are put in files on disk. From here, your data pipeline could list the files in the failed directory. If there was one or more, it could then read the success files and remove them from Elasticsearch.</p>
			<p>This is not elegant, but atomicity is important. Debugging data pipeline failures when the failure is only partial is extremely difficult and time consuming. The extra work required to incorporate atomicity is well worth it.</p>
			<p>SQL databases have atomicity built into the transactions. Using a library such as <strong class="source-inline">psycopg2</strong>, you can roll multiple inserts, updates, or deletes into a single transaction and guarantee that the results will either be that all operations were successful, or the transaction failed.</p>
			<p>Creating data pipelines that <a id="_idIndexMarker452"/>are idempotent and atomic requires additional work when creating your data pipeline. But without these two features, you will have data pipelines that will make changes to your results if accidently run multiple times (not idempotent) or if there are records that are missing (not atomic). Debugging these issues is difficult, so the time spent on making your data pipelines idempotent and atomic is well spent.</p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor092"/>Summary</h1>
			<p>In this chapter, you learned three key features of production data pipelines: staging and validation, idempotency, and atomicity. You learned how to use Great Expectations to add production-grade validation to your data pipeline staged data. You also learned how you could incorporate idempotency and atomicity into your pipelines. With these skills, you can build more robust, production-ready pipelines.</p>
			<p>In the next chapter, you will learn how to use version control with the NiFi registry.</p>
		</div>
	</body></html>