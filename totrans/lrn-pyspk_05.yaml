- en: Chapter 5. Introducing MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to prepare the data for modeling. In
    this chapter, we will actually use some of that learning to build a classification
    model using the MLlib package of PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: MLlib stands for Machine Learning Library. Even though MLlib is now in a maintenance
    mode, that is, it is not actively being developed (and will most likely be deprecated
    later), it is warranted that we cover at least some of the features of the library.
    In addition, MLlib is currently the only library that supports training models
    for streaming.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Starting with Spark 2.0, ML is the main machine learning library that operates
    on DataFrames instead of RDDs as is the case for MLlib.
  prefs: []
  type: TYPE_NORMAL
- en: 'The documentation for `MLlib` can be found here: [http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn how to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the data for modeling with MLlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform statistical testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict survival chances of infants using logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select the most predictable features and train a random forest model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of the package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the high level, MLlib exposes three core machine learning functionalities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data preparation**: Feature extraction, transformation, selection, hashing
    of categorical features, and some natural language processing methods'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine learning algorithms**: Some popular and advanced regression, classification,
    and clustering algorithms are implemented'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Utilities**: Statistical methods such as descriptive statistics, chi-square
    testing, linear algebra (sparse and dense matrices and vectors), and model evaluation
    methods'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, the palette of available functionalities allows you to perform
    almost all of the fundamental data science tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will build two classification models: a linear regression
    and a random forest. We will use a portion of the US 2014 and 2015 birth data
    we downloaded from [http://www.cdc.gov/nchs/data_access/vitalstatsonline.htm](http://www.cdc.gov/nchs/data_access/vitalstatsonline.htm);
    from the total of 300 variables we selected 85 features that we will use to build
    our models. Also, out of the total of almost 7.99 million records, we selected
    a balanced sample of 45,429 records: 22,080 records where infants were reported
    dead and 23,349 records with infants alive.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dataset we will use in this chapter can be downloaded from [http://www.tomdrabas.com/data/LearningPySpark/births_train.csv.gz](http://www.tomdrabas.com/data/LearningPySpark/births_train.csv.gz).
  prefs: []
  type: TYPE_NORMAL
- en: Loading and transforming the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though MLlib is designed with RDDs and DStreams in focus, for ease of transforming
    the data we will read the data and convert it to a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The DStreams are the basic data abstraction for Spark Streaming (see [http://bit.ly/2jIDT2A](http://bit.ly/2jIDT2A))
  prefs: []
  type: TYPE_NORMAL
- en: Just like in the previous chapter, we first specify the schema of our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Note that here (for brevity), we only present a handful of features. You should
    always check our GitHub account for this book for the latest version of the code:
    [https://github.com/drabastomek/learningPySpark](https://github.com/drabastomek/learningPySpark).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we load the data. The `.read.csv(...)` method can read either uncompressed
    or (as in our case) GZipped comma-separated values. The `header` parameter set
    to `True` indicates that the first row contains the header, and we use the `schema`
    to specify the correct data types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: There are plenty of features in our dataset that are strings. These are mostly
    categorical variables that we need to somehow convert to a numeric form.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can glimpse over the original file schema specification here: [ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Dataset_Documentation/DVS/natality/UserGuide2015.pdf](ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Dataset_Documentation/DVS/natality/UserGuide2015.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first specify our recode dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Our goal in this chapter is to predict whether the `''INFANT_ALIVE_AT_REPORT''`
    is either `1` or `0`. Thus, we will drop all of the features that relate to the
    infant and will try to predict the infant''s chances of surviving only based on
    the features related to its mother, father, and the place of birth:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In our dataset, there are plenty of features with Yes/No/Unknown values; we
    will only code `Yes` to `1`; everything else will be set to `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also a small problem with how the number of cigarettes smoked by the
    mother was coded: as 0 means the mother smoked no cigarettes before or during
    the pregnancy, between 1-97 states the actual number of cigarette smoked, 98 indicates
    either 98 or more, whereas 99 identifies the unknown; we will assume the unknown
    is 0 and recode accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So next we will specify our recoding methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `recode` method looks up the correct key from the `recode_dictionary` (given
    the `key`) and returns the corrected value. The `correct_cig` method checks when
    the value of the feature `feat` is not equal to 99 and (for that situation) returns
    the value of the feature; if the value is equal to 99, we get 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'We cannot use the `recode` function directly on a `DataFrame`; it needs to
    be converted to a UDF that Spark will understand. The `rec_integer` is such a
    function: by passing our specified `recode` function and specifying the return
    value data type, we can use it then to encode our Yes/No/Unknown features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s get to it. First, we''ll correct the features related to the number
    of cigarettes smoked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `.withColumn(...)` method takes the name of the column as its first parameter
    and the transformation as the second one. In the previous cases, we do not create
    new columns, but reuse the same ones instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will focus on correcting the Yes/No/Unknown features. First, we will
    figure out which these are with the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: First, we created a list of tuples (`cols`) that hold column names and corresponding
    data types. Next, we loop through all of these and calculate distinct values of
    all string columns; if a `'Y'` is within the returned list, we append the column
    name to the `YNU_cols` list.
  prefs: []
  type: TYPE_NORMAL
- en: 'DataFrames can transform the features in bulk while selecting features. To
    present the idea, consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what we get in return:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading and transforming the data](img/B05793_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We select the `'INFANT_NICU_ADMISSION'` column and we pass the name of the feature
    to the `rec_integer` method. We also alias the newly transformed column as `'INFANT_NICU_ADMISSION_RECODE'`.
    This way we will also confirm that our UDF works as intended.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to transform all the `YNU_cols` in one go, we will create a list of such
    transformations, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check if we got it correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading and transforming the data](img/B05793_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Looks like everything worked as we wanted it to work, so let's get to know our
    data better.
  prefs: []
  type: TYPE_NORMAL
- en: Getting to know your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to build a statistical model in an informed way, an intimate knowledge
    of the dataset is necessary. Without knowing the data it is possible to build
    a successful model, but it is then a much more arduous task, or it would require
    more technical resources to test all the possible combinations of features. Therefore,
    after spending the required 80% of the time cleaning the data, we spend the next
    15% getting to know it!
  prefs: []
  type: TYPE_NORMAL
- en: Descriptive statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I normally start with descriptive statistics. Even though the DataFrames expose
    the `.describe()` method, since we are working with `MLlib`, we will use the `.colStats(...)`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A word of warning: the `.colStats(...)` calculates the descriptive statistics
    based on a sample. For real world datasets this should not really matter but if
    your dataset has less than 100 observations you might get some strange results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The method takes an `RDD` of data to calculate the descriptive statistics of
    and return a `MultivariateStatisticalSummary` object that contains the following
    descriptive statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '`count()`: This holds a row count'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max()`: This holds maximum value in the column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean():` This holds the value of the mean for the values in the column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min()`: This holds the minimum value in the column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normL1()`: This holds the value of the L1-Norm for the values in the column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normL2()`: This holds the value of the L2-Norm for the values in the column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numNonzeros()`: This holds the number of nonzero values in the column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`variance()`: This holds the value of the variance for the values in the column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can read more about the L1- and L2-norms here [http://bit.ly/2jJJPJ0](http://bit.ly/2jJJPJ0)
  prefs: []
  type: TYPE_NORMAL
- en: 'We recommend checking the documentation of Spark to learn more about these.
    The following is a snippet that calculates the descriptive statistics of the numeric
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Descriptive statistics](img/B05793_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, mothers, compared to fathers, are younger: the average age
    of mothers was 28 versus over 44 for fathers. A good indication (at least for
    some of the infants) was that many mothers quit smoking while being pregnant;
    it is horrifying, though, that there still were some that continued smoking.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the categorical variables, we will calculate the frequencies of their values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is what the results look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Descriptive statistics](img/B05793_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Most of the deliveries happened in hospital (`BIRTH_PLACE` equal to `1`). Around
    550 deliveries happened at home: some intentionally (`''BIRTH_PLACE''` equal to
    `3`), and some not (`''BIRTH_PLACE''` equal to `4`).'
  prefs: []
  type: TYPE_NORMAL
- en: Correlations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Correlations help to identify collinear numeric features and handle them appropriately.
    Let''s check the correlations between our features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will calculate the correlation matrix and will print only
    those features that have a correlation coefficient greater than `0.5`: the `corrs
    > 0.5` part takes care of that.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Correlations](img/B05793_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, the `''CIG_...''` features are highly correlated, so we can
    drop most of them. Since we want to predict the survival chances of an infant
    as soon as possible, we will keep only the `''CIG_1_TRI''`. Also, as expected,
    the weight features are also highly correlated and we will only keep the `''MOTHER_PRE_WEIGHT''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Statistical testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We cannot calculate correlations for the categorical features. However, we can
    run a Chi-square test to determine if there are significant differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how you can do it using the `.chiSqTest(...)` method of `MLlib`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We loop through all the categorical variables and pivot them by the `'INFANT_ALIVE_AT_REPORT'`
    feature to get the counts. Next, we transform them into an RDD, so we can then
    convert them into a matrix using the `pyspark.mllib.linalg` module. The first
    parameter to the `.Matrices.dense(...)` method specifies the number of rows in
    the matrix; in our case, it is the length of distinct values of the categorical
    feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second parameter specifies the number of columns: we have two as our `''INFANT_ALIVE_AT_REPORT''`
    target variable has only two values.'
  prefs: []
  type: TYPE_NORMAL
- en: The last parameter is a list of values to be transformed into a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example that shows this more clearly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Statistical testing](img/B05793_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Once we have our counts in a matrix form, we can use the `.chiSqTest(...)` to
    calculate our test.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s what we get in return:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Statistical testing](img/B05793_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Our tests reveal that all the features should be significantly different and
    should help us predict the chance of survival of an infant.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the final dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Therefore, it is time to create our final dataset that we will use to build
    our models. We will convert our DataFrame into an RDD of `LabeledPoints`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `LabeledPoint` is a MLlib structure that is used to train the machine learning
    models. It consists of two attributes: `label` and `features`.'
  prefs: []
  type: TYPE_NORMAL
- en: The `label` is our target variable and `features` can be a NumPy `array`, `list`,
    `pyspark.mllib.linalg.SparseVector`, `pyspark.mllib.linalg.DenseVector`, or `scipy.sparse`
    column matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an RDD of LabeledPoints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we build our final dataset, we first need to deal with one final obstacle:
    our `''BIRTH_PLACE''` feature is still a string. While any of the other categorical
    variables can be used as is (as they are now dummy variables), we will use a hashing
    trick to encode the `''BIRTH_PLACE''` feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: First, we create the hashing model. Our feature has seven levels, so we use
    as many features as that for the hashing trick. Next, we actually use the model
    to convert our `'BIRTH_PLACE'` feature into a `SparseVector`; such a data structure
    is preferred if your dataset has many columns but in a row only a few of them
    have non-zero values. We then combine all the features together and finally create
    a `LabeledPoint`.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting into training and testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we move to the modeling stage, we need to split our dataset into two
    sets: one we''ll use for training and the other for testing. Luckily, RDDs have
    a handy method to do just that: `.randomSplit(...)`. The method takes a list of
    proportions that are to be used to randomly split the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how it is done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: That's it! Nothing more needs to be done.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting infant survival
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we can move to predicting the infants'' survival chances. In this
    section, we will build two models: a linear classifier—the logistic regression,
    and a non-linear one—a random forest. For the former one, we will use all the
    features at our disposal, whereas for the latter one, we will employ a `ChiSqSelector(...)`
    method to select the top four features.'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression in MLlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logistic regression is somewhat a benchmark to build any classification model.
    MLlib used to provide a logistic regression model estimated using a **stochastic
    gradient descent** (**SGD**) algorithm. This model has been deprecated in Spark
    2.0 in favor of the `LogisticRegressionWithLBFGS` model.
  prefs: []
  type: TYPE_NORMAL
- en: The `LogisticRegressionWithLBFGS` model uses the **Limited-memory Broyden–Fletcher–Goldfarb–Shanno**
    (**BFGS**) optimization algorithm. It is a quasi-Newton method that approximates
    the BFGS algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For those of you who are mathematically adept and interested in this, we suggest
    perusing this blog post that is a nice walk-through of the optimization algorithms:
    [http://aria42.com/blog/2014/12/understanding-lbfgs](http://aria42.com/blog/2014/12/understanding-lbfgs).'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we train the model on our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Training the model is very simple: we just need to call the `.train(...)` method.
    The required parameters are the RDD with `LabeledPoints`; we also specified the
    number of `iterations` so it does not take too long to run.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Having trained the model using the `births_train` dataset, let''s use the model
    to predict the classes for our testing set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The preceding snippet creates an RDD where each element is a tuple, with the
    first element being the actual label and the second one, the model's prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'MLlib provides an evaluation metric for classification and regression. Let''s
    check how well or how bad our model performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what we got:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression in MLlib](img/B05793_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The model performed reasonably well! The 85% area under the Precision-Recall
    curve indicates a good fit. In this case, we might be getting slightly more predicted
    deaths (true and false positives). In this case, this is actually a good thing
    as it would allow doctors to put the expectant mother and the infant under special
    care.
  prefs: []
  type: TYPE_NORMAL
- en: The area under **Receiver-Operating Characteristic** (**ROC**) can be understood
    as a probability of the model ranking higher than a randomly chosen positive instance
    compared to a randomly chosen negative one. A 63% value can be thought of as acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more on these metrics, we point interested readers to [http://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves](http://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves)
    and [http://gim.unmc.edu/dxtests/roc3.htm](http://gim.unmc.edu/dxtests/roc3.htm).
  prefs: []
  type: TYPE_NORMAL
- en: Selecting only the most predictable features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Any model that uses less features to predict a class accurately should always
    be preferred to a more complex one. MLlib allows us to select the most predictable
    features using a Chi-Square selector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how you do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We asked the selector to return the four most predictive features from the dataset
    and train the selector using the `births_train` dataset. We then used the model
    to extract only those features from our training and testing datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The `.ChiSqSelector(...)` method can only be used for numerical features; categorical
    variables need to be either hashed or dummy coded before the selector can be used.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest in MLlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are now ready to build the random forest model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows you how to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The first parameter to the `.trainClassifier(...)` method specifies the training
    dataset. The `numClasses` one indicates how many classes our target variable has.
    As the third parameter, you can pass a dictionary where the key is the index of
    a categorical feature in our RDD and the value for the key indicates the number
    of levels that the categorical feature has. The `numTrees` specifies the number
    of trees to be in the forest. The next parameter tells the model to use all the
    features in our dataset instead of keeping only the most descriptive ones, while
    the last one specifies the seed for the stochastic part of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how well our model did:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Random forest in MLlib](img/B05793_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, the Random Forest model with fewer features performed even
    better than the logistic regression model. Let''s see how the logistic regression
    would perform with a reduced number of features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The results might surprise you:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Random forest in MLlib](img/B05793_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, both models can be simplified and still attain the same level
    of accuracy. Having said that, you should always opt for a model with fewer variables.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at the capabilities of the `MLlib` package of PySpark.
    Even though the package is currently in a maintenance mode and is not actively
    being worked on, it is still good to know how to use it. Also, for now it is the
    only package available to train models while streaming data. We used `MLlib` to
    clean up, transform, and get familiar with the dataset of infant deaths. Using
    that knowledge we then successfully built two models that aimed at predicting
    the chance of infant survival given the information about its mother, father,
    and place of birth.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will revisit the same problem, but using the newer package
    that is currently the Spark recommended package for machine learning.
  prefs: []
  type: TYPE_NORMAL
