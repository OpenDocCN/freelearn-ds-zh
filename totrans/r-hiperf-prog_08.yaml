- en: Chapter 8. Multiplying Performance with Parallel Computing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章. 使用并行计算提高性能
- en: In this chapter, we will learn how to write and execute a parallel R code, where
    different parts of the code run simultaneously. So far, we have learned various
    ways to optimize the performance of R programs running serially, that is in a
    single process. This does not take full advantage of the computing power of modern
    CPUs with multiple cores. Parallel computing allows us to tap into all the computational
    resources available and to speed up the execution of R programs by many times.
    We will examine the different types of parallelism and how to implement them in
    R, and we will take a closer look at a few performance considerations when designing
    the parallel architecture of R programs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何编写和执行并行R代码，其中代码的不同部分同时运行。到目前为止，我们已经学习了各种优化串行运行的R程序性能的方法，即在单个进程中运行。这并没有充分利用现代CPU的多核计算能力。并行计算使我们能够利用所有可用的计算资源，并通过许多倍的速度加快R程序的执行。我们将检查不同的并行类型以及如何在R中实现它们，并且我们将更详细地查看设计R程序并行架构时的几个性能考虑因素。
- en: 'This chapter covers the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: Data parallelism versus task parallelism
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据并行与任务并行
- en: Implementing data parallel algorithms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现数据并行算法
- en: Implementing task parallel algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现任务并行算法
- en: Executing tasks in parallel on a cluster of computers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在计算机集群上并行执行任务
- en: Shared memory versus distributed memory parallelism
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享内存与分布式内存并行
- en: Optimizing parallel performance
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化并行性能
- en: Data parallelism versus task parallelism
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据并行与任务并行
- en: Many modern software applications are designed to run computations in parallel
    in order to take advantage of the multiple CPU cores available on almost any computer
    today. Many R programs can similarly be written in order to run in parallel. However,
    the extent of possible parallelism depends on the computing task involved. On
    one side of the scale are **embarrassingly parallel** tasks, where there are no
    dependencies between the parallel subtasks; such tasks can be made to run in parallel
    very easily. An example of this is, building an ensemble of decision trees in
    a random forest algorithm—randomized decision trees can be built independently
    from one another and in parallel across tens or hundreds of CPUs, and can be combined
    to form the random forest. On the other end of the scale are tasks that cannot
    be parallelized, as each step of the task depends on the results of the previous
    step. One such example is a depth-first search of a tree, where the subtree to
    search at each step depends on the path taken in previous steps. Most algorithms
    fall somewhere in between with some steps that must run serially and some that
    can run in parallel. With this in mind, careful thought must be given when designing
    a parallel code that works correctly and efficiently.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现代软件应用程序被设计为在几乎任何计算机上并行运行计算，以利用今天几乎任何计算机上可用的多个CPU核心。许多R程序也可以类似地编写以并行运行。然而，可能的并行程度取决于涉及的计算任务。在衡量的一端是**令人尴尬的并行**任务，其中并行子任务之间没有依赖关系；这些任务可以非常容易地并行运行。这种例子之一是在随机森林算法中构建决策树集合——随机决策树可以独立于彼此并行构建，并在数十或数百个CPU上并行运行，然后可以组合成随机森林。在衡量的另一端是无法并行化的任务，因为任务的每一步都依赖于前一步的结果。一个这样的例子是树的深度优先搜索，其中每一步要搜索的子树取决于之前步骤中采取的路径。大多数算法在衡量之间，有些步骤必须串行运行，有些可以并行运行。考虑到这一点，在设计正确且高效的并行代码时，必须仔细思考。
- en: 'Often an R program has some parts that have to be run serially and other parts
    that can run in parallel. Before making the effort to parallelize any of the R
    code, it is useful to have an estimate of the potential performance gains that
    can be achieved. **Amdahl''s law** provides a way to estimate the best attainable
    performance gain when you convert a code from serial to parallel execution. It
    divides a computing task into its serial and potentially-parallel parts and states
    that the time needed to execute the task in parallel will be no less than this
    formula:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，R程序中的一些部分必须串行运行，而其他部分可以并行运行。在努力并行化任何R代码之前，估计可能实现的可实现性能增益是有用的。**阿姆达尔定律**提供了一种方法来估计将代码从串行转换为并行执行时可能实现的最佳性能增益。它将计算任务分为其串行和潜在并行部分，并指出并行执行任务所需的时间不会少于以下公式：
- en: '*T(n) = T(1)(P + (1-P)/n)*, where:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*T(n) = T(1)(P + (1-P)/n)*，其中：'
- en: '*T(n)* is the time taken to execute the task using *n* parallel processes'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*T(n)* 是使用 *n* 个并行进程执行任务所需的时间'
- en: '*P* is the proportion of the whole task that is strictly serial'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P* 是整个任务中严格串行的比例'
- en: 'The theoretical best possible speed up of the parallel algorithm is thus:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，并行算法的理论最佳加速比是：
- en: '*S(n) = T(1) / T(n) = 1 / (P + (1-P)/n)*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*S(n) = T(1) / T(n) = 1 / (P + (1-P)/n)*'
- en: For example, given a task that takes 10 seconds to execute on one processor,
    where half of the task can be run in parallel, then the best possible time to
    run it on four processors is *T(4) = 10(0.5 + (1-0.5)/4) = 6.25* seconds.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，给定一个在一个处理器上执行需要10秒的任务，其中一半的任务可以并行运行，那么在四个处理器上运行的最佳时间是 *T(4) = 10(0.5 + (1-0.5)/4)
    = 6.25* 秒。
- en: The theoretical best possible speed up of the parallel algorithm with four processors
    is *1 / (0.5 + (1-0.5)/4) = 1.6x*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用四个处理器的并行算法的理论最佳加速比是 *1 / (0.5 + (1-0.5)/4) = 1.6x*。
- en: The following figure shows you how the theoretical best possible execution time
    decreases as more CPU cores are added. Notice that the execution time reaches
    a limit that is just above five seconds. This corresponds to the half of the task
    that must be run serially, where parallelism does not help.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了随着CPU核心数的增加，理论最佳执行时间如何降低。请注意，执行时间达到一个略高于五秒的限制。这对应于必须串行运行的任务的一半，此时并行化没有帮助。
- en: '![Data parallelism versus task parallelism](img/9263OS_08_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![数据并行化与任务并行化](img/9263OS_08_01.jpg)'
- en: Best possible execution time versus number of CPU cores
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳执行时间与CPU核心数的关系
- en: In general, Amdahl's law means that the fastest execution time for any parallelized
    algorithm is limited by the time needed for the serial portions of the algorithm.
    Bear in mind that Amdahl's law provides only a theoretical estimate. It does not
    account for the overheads of parallel computing (such as starting and coordinating
    tasks) and assumes that the parallel portions of the algorithm are infinitely
    scalable. In practice, these factors might significantly limit the performance
    gains of parallelism, so use Amdahl's law only to get a rough estimate of the
    maximum speedup possible.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，Amdahl定律意味着任何并行化算法的最快执行时间都受算法串行部分所需时间的限制。请注意，Amdahl定律仅提供了一个理论估计。它不考虑并行计算的开销（如启动和协调任务），并假设算法的并行部分可以无限扩展。在实践中，这些因素可能会显著限制并行化的性能提升，因此仅使用Amdahl定律来获得最大加速比的大致估计。
- en: 'There are two main classes of parallelism: data parallelism and task parallelism.
    Understanding these concepts helps to determine what types of tasks can be modified
    to run in parallel.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化主要有两类：数据并行化和任务并行化。理解这些概念有助于确定哪些类型的任务可以被修改以并行运行。
- en: In **data parallelism**, a dataset is divided into multiple partitions. Different
    partitions are distributed to multiple processors, and the same task is executed
    on each partition of data. Take for example, the task of finding the maximum value
    in a vector dataset, say one that has one billion numeric data points. A serial
    algorithm to do this would look like the following code, which iterates over every
    element of the data in sequence to search for the largest value. (This code is
    intentionally verbose to illustrate how the algorithm works; in practice, the
    `max()` function in R, though also serial in nature, is much faster.)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **数据并行化** 中，数据集被分成多个分区。不同的分区被分配到多个处理器上，并在每个数据分区上执行相同的任务。以例如在向量数据集中找到最大值的任务为例，假设有一个包含十亿个数值数据点的数据集。执行此任务的串行算法如下代码所示，它按顺序遍历数据中的每个元素以搜索最大值。（此代码故意冗长，以说明算法的工作原理；在实践中，R中的`max()`函数虽然本质上也是串行的，但速度要快得多。）
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'One way to parallelize this algorithm is to split the data into partitions.
    If we have a computer with eight CPU cores, we can split the data into eight partitions
    of 125 million numbers each. Here is the pseudocode for how to perform the same
    task in parallel:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 将此算法并行化的一种方法是将数据分成分区。如果我们有一台具有八个CPU核心的计算机，我们可以将数据分成八个分区，每个分区包含1.25亿个数字。以下是并行执行相同任务的伪代码：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This pseudocode runs eight instances of `serialmax()`in parallel—one for each
    data partition—to find the local maximum value in each partition. Once all the
    partitions have been processed, the algorithm finds the global maximum value by
    finding the largest value among the local maxima. This parallel algorithm works
    because the global maximum of a dataset must be the largest of the local maxima
    from all the partitions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此伪代码并行运行了八个`serialmax()`实例——每个数据分区一个，以在每个分区中找到局部最大值。一旦所有分区都已被处理，算法通过在局部最大值中找到最大值来找到全局最大值。这个并行算法之所以有效，是因为数据集的全局最大值必须是所有分区局部最大值中的最大值。
- en: The following figure depicts data parallelism pictorially. The key behind data
    parallel algorithms is that each partition of data can be processed independently
    of the other partitions, and the results from all the partitions can be combined
    to compute the final results. This is similar to the mechanism of the MapReduce
    framework from Hadoop. Data parallelism allows algorithms to scale up easily as
    data volume increases—as more data is added to the dataset, more computing nodes
    can be added to a cluster to process new partitions of data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 下图以图示方式描述了数据并行。数据并行算法背后的关键是每个数据分区可以独立于其他分区进行处理，并且所有分区的结果可以合并来计算最终结果。这与Hadoop的MapReduce框架的机制相似。数据并行允许算法随着数据量的增加而轻松扩展——当数据集增加更多数据时，可以将更多计算节点添加到集群中，以处理新的数据分区。
- en: '![Data parallelism versus task parallelism](img/9263OS_08_02.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![数据并行与任务并行](img/9263OS_08_02.jpg)'
- en: Data parallelism
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行
- en: 'Other examples of computations and algorithms that can be run in a data parallel
    way include:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其他可以以数据并行方式运行的计算和算法示例包括：
- en: '**Element-wise matrix operations such as addition and subtraction**: The matrices
    can be partitioned and the operations are applied to each pair of partitions.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逐元素矩阵运算，如加法和减法**: 矩阵可以被分区，并且操作应用于每一对分区。'
- en: '**Means**: The sums and number of elements in each partition can be added to
    find the global sum and number of elements from which the mean can be computed.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均值**: 每个分区中的元素总和和元素数量可以相加，以找到全局总和和元素数量，从而可以计算均值。'
- en: '**K-means clustering**: After data partitioning, the K centroids are distributed
    to all the partitions. Finding the closest centroid is performed in parallel and
    independently across the partitions. The centroids are updated by first, calculating
    the sums and the counts of their respective members in parallel, and then consolidating
    them in a single process to get the global means.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K-means聚类**: 在数据分区之后，K个质心被分配到所有分区。在分区中并行且独立地执行寻找最近的质心操作。通过首先并行计算各自的成员的总和和计数，然后在一个单一过程中合并它们来更新质心。'
- en: '**Frequent itemset mining using the Partition algorithm**: In the first pass,
    the frequent itemsets are mined from each partition of data to generate a global
    set of candidate itemsets; in the second pass, the supports of the candidate itemsets
    are summed from each partition to filter out the globally infrequent ones.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用分区算法进行频繁项集挖掘**: 在第一次遍历中，从每个数据分区中挖掘频繁项集以生成全局候选项集集；在第二次遍历中，从每个分区中汇总候选项集的支持度以过滤掉全局不频繁的项集。'
- en: The other main class of parallelism is **task parallelism**, where tasks are
    distributed to and executed on different processors in parallel. The tasks on
    each processor might be the same or different, and the data that they act on might
    also be the same or different. The key difference between task parallelism and
    data parallelism is that the data is not divided into partitions. An example of
    a task parallel algorithm performing the same task on the same data is the training
    of a random forest model. A random forest is a collection of decision trees built
    independently on the same data. During the training process for a particular tree,
    a random subset of the data is chosen as the training set, and the variables to
    consider at each branch of the tree are also selected randomly. Hence, even though
    the same data is used, the trees are different from one another. In order to train
    a random forest of say 100 decision trees, the workload could be distributed to
    a computing cluster with 100 processors, with each processor building one tree.
    All the processors perform the same task on the same data (or exact copies of
    the data), but the data is not partitioned.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类主要的并行性是**任务并行性**，其中任务被分配到不同的处理器上并行执行。每个处理器上的任务可能是相同的或不同的，它们作用的数据也可能是相同的或不同的。任务并行性与数据并行性的关键区别在于数据没有被分成分区。一个任务并行算法的例子是在相同的数据上执行相同任务的随机森林模型的训练。随机森林是一组独立在同一数据上构建的决策树。在特定树的训练过程中，选择数据的一个随机子集作为训练集，并且在每个树的分支上考虑的变量也是随机选择的。因此，尽管使用了相同的数据，但树彼此不同。为了训练一个包含100个决策树的随机森林，可以将工作量分配给一个具有100个处理器的计算集群，每个处理器构建一棵树。所有处理器都在相同的数据（或数据的精确副本）上执行相同的任务，但数据没有被分区。
- en: The parallel tasks can also be different. For example, computing a set of summary
    statistics on the same set of data can be done in a task parallel way. Each process
    can be assigned to compute a different statistic—the mean, standard deviation,
    percentiles, and so on.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 并行任务也可以不同。例如，对同一组数据计算一组汇总统计量可以通过任务并行方式完成。每个进程可以分配计算不同的统计量——平均值、标准差、分位数等等。
- en: 'Pseudocode of a task parallel algorithm might look like this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个任务并行算法的伪代码可能看起来像这样：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Implementing data parallel algorithms
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现数据并行算法
- en: Several R packages allow code to be executed in parallel. The `parallel` package
    that comes with R provides the foundation for most parallel computing capabilities
    in other packages. Let's see how it works with an example.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 几个R包允许代码并行执行。R自带的`parallel`包为其他包中的大多数并行计算能力提供了基础。让我们通过一个例子看看它是如何工作的。
- en: This example involves finding documents that match a regular expression. Regular
    expression matching is a fairly computational expensive task, depending on the
    complexity of the regular expression. The corpus, or set of documents, for this
    example is a sample of the Reuters-21578 dataset for the topic corporate acquisitions
    (`acq`) from the `tm` package. Because this dataset contains only 50 documents,
    they are replicated 100,000 times to form a corpus of 5 million documents so that
    parallelizing the code will lead to meaningful savings in execution times.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子涉及到查找匹配正则表达式的文档。正则表达式匹配是一个相对计算密集型的任务，这取决于正则表达式的复杂性。这个语料库，或文档集合，是来自 `tm`
    包的 Reuters-21578 数据集关于公司收购（`acq`）主题的一个样本。因为这个数据集只包含50个文档，所以它们被复制了100,000次，形成一个包含500万个文档的语料库，这样并行化代码将导致执行时间的有意义节省。
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The task is to find documents that match the regular expression `\d+(,\d+)?
    mln dlrs`, which represents monetary amounts in millions of dollars. In this regular
    expression, `\d+` matches a string of one or more digits, and `(,\d+)?` optionally
    matches a comma followed by one more digits. For example, the strings `12 mln
    dlrs`, `1,234 mln dlrs` and `123,456,789 mln dlrs` will match the regular expression.
    First, we will measure the execution time to find these documents serially with
    `grepl()`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是找到匹配正则表达式 `\d+(,\d+)? mln dlrs` 的文档，该表达式表示百万美元的货币金额。在这个正则表达式中，`\d+` 匹配一个或多个数字的字符串，而
    `(,\d+)?` 可选地匹配一个逗号后跟一个或多个数字。例如，字符串 `12 mln dlrs`、`1,234 mln dlrs` 和 `123,456,789
    mln dlrs` 将匹配这个正则表达式。首先，我们将测量使用 `grepl()` 逐个查找这些文档的执行时间：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we will modify the code to run in parallel and measure the execution
    time on a computer with four CPU cores:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将修改代码以并行运行，并在具有四个CPU核心的计算机上测量执行时间：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this code, the `detectCores()` function reveals how many CPU cores are available
    on the machine, where this code is executed. Before running any parallel code,
    `makeCluster()` is called to create a local cluster of processing nodes with all
    four CPU cores. The corpus is then split into four partitions using the `clusterSplit()`
    function to determine the ideal split of the corpus such that each partition has
    roughly the same number of documents.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，`detectCores()`函数揭示了在执行此代码的机器上可用的CPU核心数量。在运行任何并行代码之前，调用`makeCluster()`来创建一个包含所有四个CPU核心的本地处理节点集群。然后使用`clusterSplit()`函数将语料库分成四个分区，以确定语料库的理想分割方式，使得每个分区具有大致相同的文档数量。
- en: The actual parallel execution of `grepl()` on each partition of the corpus is
    carried out by the `parSapply()` function. Each processing node in the cluster
    is given a copy of the partition of data that it is supposed to process along
    with the code to be executed and other variables that are needed to run the code
    (in this case, the `pattern` argument). When all four processing nodes have completed
    their tasks, the results are combined in a similar fashion to `sapply()`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在语料库的每个分区上实际执行`grepl()`的并行化是由`parSapply()`函数完成的。集群中的每个处理节点都会收到它应该处理的数据分区的副本，以及要执行的代码和其他运行代码所需的变量（在这种情况下，是`pattern`参数）。当所有四个处理节点完成其任务后，结果会以类似于`sapply()`的方式合并。
- en: Finally, the cluster is destroyed by calling `stopCluster()`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过调用`stopCluster()`来销毁集群。
- en: Tip
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'It is good practice to ensure that `stopCluster()` is always called in production
    code, even if an error occurs during execution. This can be done as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产代码中确保始终调用`stopCluster()`是一个好的实践，即使执行过程中发生错误也是如此。这可以通过以下方式完成：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In this example, running the task in parallel on four processors resulted in
    a 23 percent reduction in the execution time. This is not in proportion to the
    amount of compute resources used to perform the task; with four times as many
    CPU cores working on it, a perfectly parallelizable task might experience as much
    as a 75 percent runtime reduction. However, remember Amdahl's law—the speed of
    parallel code is limited by the serial parts, which includes the overheads of
    parallelization. In this case, calling `makeCluster()` with the default arguments
    creates a **socket-based cluster**. When such a cluster is created, additional
    copies of R are run as workers. The workers communicate with the master R process
    using network sockets, hence the name. The worker R processes are initialized
    with the relevant packages loaded, and data partitions are serialized and sent
    to each worker process. These overheads can be significant, especially in data
    parallel algorithms where large volumes of data needs to be transferred to the
    worker processes.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，在四个处理器上并行运行任务导致执行时间减少了23%。这并不与执行任务所使用的计算资源量成比例；如果有四个CPU核心在处理任务，一个完全可并行化的任务可能会体验到高达75%的运行时间减少。然而，请记住阿姆达尔定律——并行代码的速度受限于串行部分，这包括并行化的开销。在这种情况下，使用默认参数调用`makeCluster()`创建了一个基于套接字的**集群**。当创建这样的集群时，会运行R的额外副本作为工作进程。工作进程通过网络套接字与主R进程通信，因此得名。工作进程的R进程使用已加载的相关包初始化，并将数据分区序列化并发送到每个工作进程。这些开销可能相当大，尤其是在需要将大量数据传输到工作进程的数据并行算法中。
- en: Note
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Besides `parSapply()`, `parallel` also provides the `parApply()` and `parLapply()`
    functions; these functions are analogous to the standard `sapply()`, `apply()`,
    and `lapply()` functions, respectively. In addition, the `parLapplyLB()` and `parSapplyLB()`
    functions provide load balancing, which is useful when the execution of each parallel
    task takes variable amounts of time. Finally, `parRapply()` and `parCapply()`
    are parallel row and column `apply()` functions for matrices.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`parSapply()`之外，`parallel`还提供了`parApply()`和`parLapply()`函数；这些函数分别类似于标准的`sapply()`、`apply()`和`lapply()`函数。此外，`parLapplyLB()`和`parSapplyLB()`函数提供了负载均衡，这在每个并行任务执行时间可变时很有用。最后，`parRapply()`和`parCapply()`是矩阵的并行行和列`apply()`函数。
- en: On non-Windows systems, `parallel` supports another type of cluster that often
    incurs less overheads—**forked clusters**. In these clusters, new worker processes
    are forked from the parent R process with a copy of the data. However, the data
    is not actually copied in the memory unless it is modified by a child process.
    This means that, compared to socket-based clusters, initializing child processes
    is quicker and the memory usage is often lower.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在非 Windows 系统上，`parallel` 支持另一种类型的集群，这种集群通常产生的开销更少——**分叉集群**。在这些集群中，新的工作进程是从父
    R 进程中分叉出来的，并带有数据的副本。然而，除非子进程修改了数据，否则数据实际上并不会在内存中复制。这意味着，与基于套接字的集群相比，初始化子进程更快，内存使用率通常更低。
- en: 'Another advantage of using forked clusters is that `parallel` provides a convenient
    and concise way to run tasks on them via the `mclapply()`, `mcmapply()`, and `mcMap()`
    functions. (These functions start with `mc` because they were originally a part
    of the `multicore` package) There is no need to explicitly create and destroy
    the cluster, as these functions do this automatically. We can simply call `mclapply()`
    and state the number of worker processes to fork via the `mc.cores` argument:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分叉集群的另一个优点是，`parallel` 提供了一种方便且简洁的方式来通过 `mclapply()`、`mcmapply()` 和 `mcMap()`
    函数在这些集群上运行任务。 (这些函数以 `mc` 开头，因为它们最初是 `multicore` 包的一部分) 没有必要显式地创建和销毁集群，因为这些函数会自动完成这些操作。我们只需调用
    `mclapply()` 并通过 `mc.cores` 参数指定要分叉的工作进程数量：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This shows a 49 percent reduction in execution time compared to the serial version,
    and 35 percent reduction compared to parallelizing using a socket-based cluster.
    For this example, forked clusters provide the best performance.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了与串行版本相比，执行时间减少了 49%，与基于套接字的集群并行化相比减少了 35%。在这个例子中，分叉集群提供了最佳性能。
- en: Note
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Due to differences in system configuration, you might see very different results
    when you try the examples in this chapter in your own environment. When you develop
    parallel code, it is important to test the code in an environment that is similar
    to the one that it will eventually run in.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于系统配置的不同，当你在自己的环境中尝试本章的示例时，可能会看到非常不同的结果。当你开发并行代码时，在最终运行的环境中类似的环境中测试代码是非常重要的。
- en: Implementing task parallel algorithms
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现任务并行算法
- en: Let's now see how to implement a task parallel algorithm using both socket-based
    and forked clusters. We will look at how to run the same task and different tasks
    on workers in a cluster.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看如何使用基于套接字和分叉集群两种方式实现任务并行算法。我们将探讨如何在集群的工作进程中运行相同的任务和不同的任务。
- en: Running the same task on workers in a cluster
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在集群的工作进程中运行相同的任务
- en: To demonstrate how to run the same task on a cluster, the task for this example
    is to generate 500 million Poisson random numbers. We will do this by using L'Ecuyer's
    combined multiple-recursive generator, which is the only random number generator
    in base R that supports multiple streams to generate random numbers in parallel.
    The random number generator is selected by calling the `RNGkind()` function.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示如何在集群上运行相同的任务，这个例子中的任务是生成 5 亿个泊松随机数。我们将通过使用 L'Ecuyer 的组合多重递归生成器来完成这项工作，这是唯一支持在并行中生成随机数的基
    R 随机数生成器。随机数生成器是通过调用 `RNGkind()` 函数选择的。
- en: Note
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We cannot just use any random number generator in parallel because the randomness
    of the data depends on the algorithm used to generate random data and the seed
    value given to each parallel task. Most other algorithms were not designed to
    produce random numbers in multiple parallel streams, and might produce multiple
    highly correlated streams of numbers, or worse, multiple identical streams!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能在并行中使用任何随机的数生成器，因为数据的随机性取决于生成随机数据的算法以及每个并行任务给出的种子值。大多数其他算法都没有设计成在多个并行流中生成随机数，可能会产生多个高度相关的数字流，或者更糟，多个相同的数字流！
- en: 'First, we will measure the execution time of the serial algorithm:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将测量串行算法的执行时间：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To generate the random numbers on a cluster, we will first distribute the task
    evenly among the workers. In the following code, the integer vector `samples.per.process`
    contains the number of random numbers that each worker needs to generate on a
    four-core CPU. The `seq()` function produces `ncores+1` numbers evenly distributed
    between `0` and `nsamples`, with the first number being `0` and the next `ncores`
    numbers indicating the approximate cumulative number of samples across the worker
    processes. The `round()` function rounds off these numbers into integers and `diff()`
    computes the difference between them to give the number of random numbers that
    each worker process should generate.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要在集群上生成随机数，我们首先将任务均匀地分配给工作者。在下面的代码中，整数向量`samples.per.process`包含每个工作者在四核CPU上需要生成的随机数的数量。`seq()`函数产生`ncores+1`个在`0`和`nsamples`之间均匀分布的数字，第一个数字是`0`，接下来的`ncores`个数字表示工作者进程之间的近似累积样本数。`round()`函数将这些数字四舍五入为整数，`diff()`函数计算它们之间的差值，从而给出每个工作者进程应生成的随机数数量。
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Before we can generate the random numbers on a cluster, each worker needs a
    different seed from which it can generate a stream of random numbers. The seeds
    need to be set on all the workers before running the task, to ensure that all
    the workers generate different random numbers.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够在集群上生成随机数之前，每个工作者需要不同的种子，以便从中生成随机数流。在运行任务之前需要在所有工作者上设置种子，以确保所有工作者生成不同的随机数。
- en: 'For a socket-based cluster, we can call `clusterSetRNGStream()` to set the
    seeds for the workers, then run the random number generation task on the cluster.
    When the task is completed, we call `stopCluster()` to shut down the cluster:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于套接字的集群，我们可以调用`clusterSetRNGStream()`来设置工作者的种子，然后在集群上运行随机数生成任务。当任务完成后，我们调用`stopCluster()`来关闭集群：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Using four parallel processes in a socket-based cluster reduces the execution
    time by 48 percent. The performance of this type of cluster for this example is
    better than that of the data parallel example because there is less data to copy
    to the worker processes—only an integer that indicates how many random numbers
    to generate.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于套接字的集群中使用四个并行进程可以将执行时间减少48%。对于这个例子，这种类型集群的性能优于数据并行示例，因为需要复制到工作者进程中的数据更少——只有一个整数，表示要生成多少个随机数。
- en: 'Next, we run the same task on a forked cluster (again, this is not supported
    on Windows). The `mclapply()` function can set the random number seeds for each
    worker for us, when the `mc.set.seed` argument is set to `TRUE`; we do not need
    to call `clusterSetRNGStream()`. Otherwise, the code is similar to that of the
    socket-based cluster:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在派生集群上运行相同的任务（再次强调，在Windows上不支持此操作）。当`mc.set.seed`参数设置为`TRUE`时，`mclapply()`函数可以为我们设置每个工作者的随机数种子，我们不需要调用`clusterSetRNGStream()`。否则，代码与基于套接字的集群类似：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: On our test machine, the execution time of the forked cluster is slightly faster,
    but close to that of the socket-based cluster, indicating that the overheads for
    this task are similar for both types of clusters.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的测试机器上，派生集群的执行时间略快，但接近基于套接字的集群，这表明这两种类型集群的此任务开销相似。
- en: Running different tasks on workers in a cluster
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在集群的工作者上运行不同的任务
- en: 'So far, we have executed the same tasks on each parallel process. The *parallel*
    package also allows different tasks to be executed on different workers. For this
    example, the task is to generate not only Poisson random numbers, but also uniform,
    normal, and exponential random numbers. As before, we start by measuring the time
    to perform this task serially:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经在每个并行进程中执行了相同的任务。*并行*包还允许不同的任务在不同的工作者上执行。在这个例子中，任务不仅是要生成泊松随机数，还要生成均匀、正态和指数随机数。和之前一样，我们首先测量执行此任务串行所需的时间：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In order to run different tasks on different workers on socket-based clusters,
    a list of function calls and their associated arguments must be passed to `parLapply()`.
    This is a bit cumbersome, but `parallel` unfortunately does not provide an easier
    interface to run different tasks on a socket-based cluster. In the following code,
    the function calls are represented as a list of lists, where the first element
    of each sublist is the name of the function that runs on a worker, and the second
    element contains the function arguments. The function `do.call()` is used to call
    the given function with the given arguments.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在基于套接字的集群的不同工作者上运行不同的任务，必须将函数调用及其相关参数传递给`parLapply()`。这有点繁琐，但遗憾的是`parallel`包没有提供更简单的接口来在基于套接字的集群上运行不同的任务。在下面的代码中，函数调用以列表的形式表示，其中每个子列表的第一个元素是在工作者上运行的函数的名称，第二个元素包含函数参数。使用`do.call()`函数来调用给定参数的函数。
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: On forked clusters on non-Windows machines, the `mcparallel()` and `mccollect()`
    functions offer a more intuitive way to run different tasks on different workers.
    For each task, `mcparallel()` sends the given task to an available worker. Once
    all the workers have been assigned their tasks, `mccollect()` waits for the workers
    to complete their tasks and collects the results from all the workers.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在非Windows机器上的分叉集群中，`mcparallel()`和`mccollect()`函数提供了更直观的方式来在不同的工作者上运行不同的任务。对于每个任务，`mcparallel()`将给定的任务发送到可用的工作者。一旦所有工作者都分配了任务，`mccollect()`将等待工作者完成任务并从所有工作者收集结果。
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Notice that we also had to call `mc.reset.stream()` to set the seeds for random
    number generation in each worker. This was not necessary when we used `mclapply()`,
    which calls `mc.reset.stream()` for us. However, `mcparallel()` does not, so we
    need to call it ourselves.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们还需要调用`mc.reset.stream()`来设置每个工作者中随机数生成的种子。当我们使用`mclapply()`时，这并不是必需的，因为`mclapply()`会为我们调用`mc.reset.stream()`。然而，`mcparallel()`不会这样做，因此我们需要自己调用它。
- en: Executing tasks in parallel on a cluster of computers
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在计算机集群上并行执行任务
- en: By using the `parallel` package, we are not limited to running parallel code
    on a single computer; we can also do it on a cluster of computers. This allows
    much larger computational tasks to be performed, irrespective of whether we use
    data parallelism or task parallelism. Only socket-based clusters can be used for
    this purpose, as processes cannot be forked onto a different computer.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`parallel`包，我们不仅限于在单台计算机上运行并行代码；我们还可以在计算机集群上运行。这允许执行更大的计算任务，无论我们使用数据并行还是任务并行。只能使用基于套接字的集群来完成此目的，因为进程不能在不同的计算机上分叉。
- en: There are many ways to set up a cluster of computers to work with R. To keep
    things simple, all computers in the cluster should have the same configuration
    for R—the same version of R, installed in the same directories, installed with
    the same versions of any packages required, and running on the same operating
    system. The examples in this section have been tested on a cluster of three computers
    running Ubuntu 14.04—one master node and two worker nodes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以将计算机集群设置为与R一起工作。为了简化问题，集群中的所有计算机都应该有相同的R配置——相同的R版本、安装在同一目录下、使用相同版本的任何所需包，并且运行在相同的操作系统上。本节中的示例已在运行Ubuntu
    14.04的三个计算机集群上测试过——一个主节点和两个工作者节点。
- en: The master and worker nodes should be on the same network and able to communicate
    with each other via SSH (port 22) and one other port for exchanging data and code.
    This communications port can be set with the `R_PARALLEL_PORT` environment variable.
    If it is not set, R will randomly choose a port in the range 11000 to 11999.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点和工作节点应在同一网络中，并且能够通过SSH（端口22）和另一个端口（用于交换数据和代码）相互通信。此通信端口可以通过`R_PARALLEL_PORT`环境变量设置。如果未设置，R将随机选择11000到11999范围内的一个端口。
- en: By default, SSH is used to launch R on the workers. First, ensure that the SSH
    server is set up and running on all the worker nodes.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，SSH用于在工作者节点上启动R。首先，请确保SSH服务器已在所有工作者节点上设置并运行。
- en: For Windows worker nodes, download and install Cygwin from [http://www.cygwin.com](http://www.cygwin.com).
    When prompted to install additional packages, install the `openssh` package. Then,
    right-click on the **Cygwin** icon and select **Run as Administrator**. In the
    terminal window that opens, run the following code to set up the SSH server. The
    `ssh-host-config` command configures the SSH server with the default settings.
    The `chmod 400` command sets the permissions on the generated security keys so
    that only the user who owns the keys can read them, and `cygrunsrv -S sshd` starts
    the SSH server.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Windows工作节点，请从[http://www.cygwin.com](http://www.cygwin.com)下载并安装Cygwin。当提示安装附加包时，请安装`openssh`包。然后，右键单击**Cygwin**图标并选择**以管理员身份运行**。在打开的终端窗口中，运行以下代码以设置SSH服务器。`ssh-host-config`命令使用默认设置配置SSH服务器。`chmod
    400`命令设置生成的安全密钥的权限，使得只有拥有密钥的用户才能读取它们，而`cygrunsrv -S sshd`命令启动SSH服务器。
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Other operating systems like Linux normally come with an installed SSH server.
    Consult the documentation of your operating system for how to set it up.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 其他操作系统如Linux通常自带安装的SSH服务器。请查阅您操作系统的文档了解如何设置。
- en: The master node should be able to connect to the worker nodes using key-based
    authentication, as using password-based authentication to run a cluster might
    not always work. Use the following commands to set up key-based authentication.
    Windows users should run this from within a Cygwin terminal.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点应该能够通过基于密钥的认证连接到工作节点，因为使用基于密码的认证来运行集群可能并不总是有效。请使用以下命令设置基于密钥的认证。Windows用户应在Cygwin终端中运行此命令。
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Once all the computers are set up, we can run parallel tasks on the cluster
    just as before. The only change needed is in the call to `makeCluster()`, where
    the IP addresses or domain names of the worker nodes must be provided instead
    of the number of workers to create local workers. In the following example, replace
    the IP addresses with the IP addresses of your master and worker nodes.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有计算机都设置完毕，我们就可以像之前一样在集群上运行并行任务。唯一需要更改的是调用`makeCluster()`的地方，此时必须提供工作节点的IP地址或域名，而不是创建本地工作进程的数量。在以下示例中，将IP地址替换为您主节点和工作节点的IP地址。
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you are having trouble to start the cluster by automatically using `makeCluster()`,
    add the `manual=TRUE` argument to the call to `makeCluster()`, then follow the
    instructions given, to start the worker processes on each of the worker nodes.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在使用`makeCluster()`自动启动集群时遇到困难，请在调用`makeCluster()`时添加`manual=TRUE`参数，然后按照给出的说明操作，以在每个工作节点上启动工作进程。
- en: 'The code that sends the tasks to the workers for execution is then the same
    as before:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 将任务发送给工作进程执行的相关代码与之前相同：
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Because a cluster of computers has to communicate over a network, the bandwidth
    and latency of the network connections play a critical role in the performance
    of the whole cluster. It is best that the nodes are in the same location, connected
    by a high-speed network where data and code can be exchanged between the master
    and worker nodes speedily.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算机集群必须通过网络进行通信，因此网络连接的带宽和延迟在整个集群的性能中起着至关重要的作用。最好将节点放置在同一位置，通过高速网络连接，以便主节点和工作节点之间可以快速交换数据和代码。
- en: Shared memory versus distributed memory parallelism
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 共享内存与分布式内存并行处理
- en: In the examples that we have seen so far, data is copied from the master process
    or node to each worker. This is called **distributed memory** parallelism, where
    each process has its own memory space. In other words, each process needs to have
    its own copy of the data that it needs to work on, even if multiple processes
    are working on the same data. This is the typical way to distribute data in a
    cluster of computers because the workers in the cluster cannot access each other's
    RAM, so they need their own copy of the data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们迄今为止看到的示例中，数据是从主进程或节点复制到每个工作进程的。这被称为**分布式内存**并行处理，其中每个进程都有自己的内存空间。换句话说，每个进程都需要拥有它需要处理的数据的副本，即使多个进程正在处理相同的数据。这是在计算机集群中分配数据的典型方式，因为集群中的工作进程无法访问彼此的RAM，因此它们需要自己的数据副本。
- en: However, this can result in huge redundancies when you run a parallel code on
    multiple processes on a single computer. If a dataset takes up 5 GB of memory,
    then running four parallel processes could result in five copies of the data in
    memory—one for the master and four for the workers—occupying a total of 25 GB.
    Earlier, we saw that forked clusters might not suffer from this problem, as most
    operating systems do not make copies of the data in the memory unless it is modified
    by one of the workers. However, this is not guaranteed. On socket-based clusters,
    because new instances of R are created, new copies of the data are made for each
    worker.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当你在单台计算机上的多个进程中运行并行代码时，这可能会导致巨大的冗余。如果一个数据集占用5 GB的内存，那么运行四个并行进程可能会导致内存中有五个数据副本——一个用于主进程，四个用于工作进程——总共占用25
    GB。之前我们看到，派生的集群可能不会遇到这个问题，因为大多数操作系统只有在数据被工作进程修改时才会复制内存中的数据。然而，这并不保证。在基于套接字的集群中，因为创建了新的R实例，所以每个工作进程都会为数据创建新的副本。
- en: Contrast this with **shared memory** parallelism, where all the workers share
    a single copy of the data. This not only saves the memory, but also reduces the
    time needed to initialize and shut down the cluster, as the data does not need
    to be copied.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与**共享内存**并行处理进行对比，所有工作进程共享数据的一个副本。这不仅节省了内存，还减少了初始化和关闭集群所需的时间，因为不需要复制数据。
- en: Although the `parallel` package does not provide support for shared memory parallelism
    by default, we can achieve it by using the right data structures. One example
    for this is `big.matrix` from the `bigmemory` package that we learned about in
    the previous chapter (not available for Windows at the time of writing). In [Chapter
    7](ch07.html "Chapter 7. Processing Large Datasets with Limited RAM"), *Processing
    Large Datasets with Limited RAM*, we used `big.matrix` for its memory-mapped file
    capabilities; in this chapter, we will take advantage of it as a shared memory
    object for parallel workers. Besides taking the form of memory-mapped files on
    disk, `big.matrix` objects can also be fully in-memory objects that behave just
    like standard R matrices. The key difference is that `big.matrix` objects are
    not copied according to the usual R rules for copying objects that we examined
    in [Chapter 6](ch06.html "Chapter 6. Simple Tweaks to Use Less RAM"), *Simple
    Tweaks to Use Less RAM*. Instead, they are only copied when a call to `deepcopy()`
    is made. Let's see what this looks like in practice. First, we will create a `big.matrix`
    `a`, then a new variable `b` that points to `a`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管默认情况下`parallel`包不支持共享内存并行处理，但我们可以通过使用合适的数据结构来实现。一个例子是我们在上一章（当时在Windows上不可用）中了解到的`big.matrix`，它来自`bigmemory`包。在[第7章](ch07.html
    "第7章. 使用有限RAM处理大型数据集")《使用有限RAM处理大型数据集》中，我们使用`big.matrix`的内存映射文件功能；在本章中，我们将利用它作为并行工作者的共享内存对象。除了在磁盘上以内存映射文件的形式存在外，`big.matrix`对象还可以是完全内存对象，其行为与标准R矩阵相同。关键区别在于，`big.matrix`对象不是根据我们在[第6章](ch06.html
    "第6章. 简单调整以减少RAM使用")《简单调整以减少RAM使用》中考察的常规R复制对象规则进行复制。相反，只有在调用`deepcopy()`时才会进行复制。让我们看看这在实践中是什么样子。首先，我们将创建一个`big.matrix`对象`a`，然后创建一个新的变量`b`，它指向`a`。
- en: '[PRE19]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we will modify the contents of `b`. Under R''s normal data copying rules,
    the data should be copied in the memory so that the contents of `a` are not modified.
    However, that is not the case:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将修改`b`的内容。根据R的正常数据复制规则，数据应该在内存中复制，以便不修改`a`的内容。然而，情况并非如此：
- en: '[PRE20]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Clearly, `a` and `b` are the same object. A peek under their hoods at their
    pointers to the data confirms this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，`a`和`b`是同一个对象。查看它们指向数据指针的内部确认了这一点：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, let''s see how this impacts the performance of parallel code. This example
    uses a matrix with two variables and 50 million observations and the equivalent
    `big.matrix`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这如何影响并行代码的性能。此示例使用一个包含两个变量和5000万个观测值的矩阵以及等效的`big.matrix`：
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The task here is to compute the absolute difference for each pair of numbers.
    First, we will measure the execution time using a socket-based cluster on the
    matrix:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此处的任务是计算每对数字的绝对差值。首先，我们将使用基于套接字的集群在矩阵上测量执行时间：
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'It took 10.6 seconds on four CPU cores, and each thread consumed 1.01 GB of
    RAM, as shown in the following screenshot taken from Mac OS X''s Activity Monitor:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在四个CPU核心上耗时10.6秒，每个线程消耗了1.01 GB的RAM，如下面的截图所示，该截图来自Mac OS X的活动监视器：
- en: '![Shared memory versus distributed memory parallelism](img/9263OS_08_03.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![共享内存与分布式内存并行处理](img/9263OS_08_03.jpg)'
- en: Memory consumption of socket-based cluster using matrix data
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于套接字的集群使用矩阵数据的内存消耗
- en: Now, let's use `big.matrix` to see if there is a difference in speed and memory
    efficiency. In order to pass `big.matrix` to each worker process, we need to use
    `describe()` to pass the metadata of `big.matrix` to each process. Within each
    process, `attach.big.matrix()` must be called to access `big.matrix`. Also notice
    that `library(bigmemory)` is called within the function. This is required because
    each worker is a new R process so any packages required to run the task must be
    loaded on the workers as well.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用`big.matrix`来看看速度和内存效率是否有差异。为了将`big.matrix`传递给每个工进程，我们需要使用`describe()`将`big.matrix`的元数据传递给每个进程。在每个进程中，必须调用`attach.big.matrix()`来访问`big.matrix`。注意，在函数内部调用了`library(bigmemory)`。这是必需的，因为每个工进程都是一个新的R进程，因此运行任务所需的任何包都必须在工进程中加载。
- en: '[PRE24]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This version ran must faster with a 72 percent saving in the execution time
    from not making copies of the matrix! Furthermore, each R process took up only
    about 373 MB of memory, as shown in the following figure in the `Private Mem`
    column. 774 MB of the memory was shared from the parent process, most of which
    was the `big.matrix` object.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这个版本运行得更快，执行时间节省了72%，因为没有复制矩阵！此外，每个R进程仅占用大约373 MB的内存，如下图中`Private Mem`列所示。774
    MB的内存是从父进程共享的，其中大部分是`big.matrix`对象。
- en: '![Shared memory versus distributed memory parallelism](img/9263OS_08_04.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![共享内存与分布式内存并行比较](img/9263OS_08_04.jpg)'
- en: Memory consumption of forked cluster using big.matrix data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 使用big.matrix数据时派生集群的内存消耗。
- en: Shared memory parallelism worked in this case because the worker processed only
    read from the data but did not write to it. Designing parallel algorithms that
    write to shared memory is much trickier and outside the scope of this book. Much
    care must be take to avoid **race conditions**, which are conflicts and programming
    errors that arise when worker processes that read from and write to the same memory
    locations are not properly coordinated. This can lead to the data being corrupted.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例中，共享内存并行工作是因为工进程只从数据中读取，而没有写入。设计写入共享内存的并行算法要复杂得多，超出了本书的范围。必须非常小心，以避免**竞争条件**，这是当从和写入相同内存位置的工进程没有适当协调时出现的冲突和编程错误。这可能导致数据损坏。
- en: Optimizing parallel performance
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化并行性能
- en: Throughout the examples in this chapter, we saw various factors that affect
    the performance of parallel code.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的示例中，我们看到了影响并行代码性能的各种因素。
- en: One overhead in running a parallel R code is in setting up the cluster. By default,
    `makeCluster()` instructs the worker processes to load the `methods` package when
    they start. This can take a good amount of time, so if the task to be run does
    not require *methods*, this behavior can be disabled by passing `methods=FALSE`
    to `makeCluster()`.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 运行并行R代码的一个开销是在设置集群时。默认情况下，`makeCluster()`指示工进程在启动时加载`methods`包。这可能会花费相当多的时间，所以如果要运行的任务不需要*methods*，可以通过将`methods=FALSE`传递给`makeCluster()`来禁用此行为。
- en: One of the biggest obstacles to parallel performance is the copying and transmission
    of data between the master process and the worker process. This obstacle can be
    large when you run parallel tasks on a cluster of computers, as many factors such
    as limited network bandwidth, and data encryption slow down the transmission of
    data even before any computations can be done. Even on a single computer, unnecessary
    copying of data in memory takes up precious seconds that can multiply as the data
    grows. This can also happen the other way around, for example in the random number
    generation examples, where the input data is small but the output is large.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 并行性能的最大障碍之一是主进程和工进程之间数据的复制和传输。当你在计算机集群上运行并行任务时，这个障碍可能会很大，因为许多因素，如有限的网络带宽和数据加密，甚至在没有进行任何计算之前就会减慢数据的传输速度。即使在单台计算机上，内存中不必要的复制数据也会占用宝贵的秒数，随着数据的增长，这些秒数会成倍增加。这种情况也可能反过来发生，例如在随机数生成示例中，输入数据很小，但输出数据很大。
- en: One way to minimize these data communication overheads is to use shared memory
    objects, as we saw in the preceding section. Data compression can also help in
    some circumstances, provided the computational time to compress and decompress
    the data is relatively short. Another option is to store the data, including the
    results of any intermediate computations, at each worker node, and reserve internode
    data communication to only what is required to coordinate the tasks. An example
    of this is MapReduce from Hadoop, which we will explore in [Chapter 10](ch10.html
    "Chapter 10. R and Big Data"), *R and Big Data*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一种减少这些数据通信开销的方法是使用共享内存对象，正如我们在上一节所看到的。在某些情况下，数据压缩也有帮助，前提是压缩和解压缩数据所需的计算时间相对较短。另一种选择是在每个工作节点上存储数据，包括任何中间计算的结果，并将节点间数据通信仅限于协调任务所需的范围。这种方法的例子是Hadoop中的MapReduce，我们将在第10章[“第10章。R和大数据”](ch10.html
    "Chapter 10. R and Big Data")中探讨。
- en: Although there are ways to minimize the costs of data communication, sometimes
    these overheads far exceed the gains from parallelization, and we are better off
    running the code in series. It can be difficult to calculate the trade-offs between
    the performance gains and increased overheads of parallelizing code. When in doubt,
    conduct small experiments like we have done in this chapter.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有方法可以最小化数据通信的成本，但有时这些开销远远超过了并行化的收益，因此我们最好按顺序运行代码。计算并行化代码的性能提升和增加的开销之间的权衡可能很困难。当不确定时，可以像本章所做的那样进行小规模的实验。
- en: Summary
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we learned about two classes of parallelism: data parallelism
    and task parallelism. Data parallelism is good for tasks that can be performed
    in parallel on partitions of a dataset. The dataset to be processed is split into
    partitions and each partition is processed on a different worker processes. Task
    parallelism, on the other hand, divides a set of similar or different tasks to
    amongst the worker processes. In either case, Amdahl''s law states that the maximum
    improvement in speed that can be achieved by parallelizing code is limited by
    the proportion of that code that can be parallelized.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了两种并行处理类别：数据并行和任务并行。数据并行适用于可以在数据集的各个分区上并行执行的任务。要处理的数据集被分割成多个分区，每个分区在不同的工作进程中进行处理。另一方面，任务并行将一组相似或不同的任务分配给工作进程。在两种情况下，Amdahl定律指出，通过并行化代码所能实现的最大速度提升受限于可以并行化的代码比例。
- en: R supports both types of parallelism using the *parallel* package. We learned
    how to implement both data parallel and task parallel algorithms using socket-based
    clusters and forked clusters. We also learned how to run tasks in parallel on
    a cluster of computers using socket-based clusters.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: R使用*parallel*包支持这两种并行处理类型。我们学习了如何使用基于套接字的集群和派生集群实现数据并行和任务并行算法。我们还学习了如何在计算机集群上使用基于套接字的集群并行运行任务。
- en: The examples in this chapter demonstrated that the improvement in performance
    by parallelizing code depends on a great variety of factors—the type of cluster,
    whether the task is run on a single computer or on a cluster, the volume of data
    exchanged between nodes, the complexity of the individual subtasks, and so on.
    While techniques such as shared-memory parallelism can mitigate some of the bottlenecks,
    parallel computing is a complex discipline that takes much experience and skill
    to get well executed. Used correctly, the payoffs in speed and efficiency can
    be significant.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的示例表明，通过并行化代码所获得的性能提升取决于众多因素——集群的类型、任务是在单台计算机上运行还是在集群上运行、节点间交换的数据量、单个子任务的复杂性等等。虽然像共享内存并行这样的技术可以缓解一些瓶颈，但并行计算是一个复杂的学科，需要丰富的经验和技能才能有效执行。如果使用得当，速度和效率的回报可以非常显著。
- en: For a deeper look at parallel computing in R, see *Parallel R* by Q. Ethan McCallum
    and Stephen Weston.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解R中的并行计算，请参阅Q. Ethan McCallum和Stephen Weston合著的《Parallel R》。
- en: In the next chapter, we will look beyond the boundaries of R to tap on the processing
    power of specialized data processing platforms like analytical databases.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将超越R的边界，利用专门的数据处理平台（如分析数据库）的处理能力。
