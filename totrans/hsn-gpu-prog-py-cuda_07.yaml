- en: Using the CUDA Libraries with Scikit-CUDA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Scikit-CUDA 与 CUDA 库
- en: In this chapter, we will be taking a tour of three of the standard CUDA libraries
    intended for streamlined numerical and scientific computation. The first that
    we will look at is **cuBLAS**, which is NVIDIA's implementation of the **Basic
    Linear Algebra Subprograms** (**BLAS**) specification for CUDA. (cuBLAS is NVIDIA's
    answer to various optimized, CPU-based implementations of BLAS, such as the free/open
    source OpenBLAS or Intel's proprietary Math Kernel Library.) The next library
    that we will look at is **cuFFT**, which can perform virtually every variation
    of the **fast Fourier transform** (**FFT**) on the GPU. We'll look at how we can
    use cuFFT for filtering in image processing in particular. We will then look at
    **cuSolver**, which can perform more involved linear algebra operations than those
    featured in cuBLAS, such as **singular value decomposition** (**SVD**) or Cholesky
    factorization.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将游览三个旨在简化数值和科学计算的标准化 CUDA 库。我们将首先查看的是 **cuBLAS**，这是 NVIDIA 对 CUDA 的 **基本线性代数子程序**（**BLAS**）规范的实现。（cuBLAS
    是 NVIDIA 对各种基于 CPU 的优化 BLAS 实现的回应，例如免费/开源的 OpenBLAS 或英特尔专有的数学内核库。）我们将查看的下一个库是
    **cuFFT**，它可以在 GPU 上执行几乎所有的 **快速傅里叶变换**（**FFT**）变体。我们将特别探讨如何使用 cuFFT 进行图像处理中的滤波。然后我们将查看
    **cuSolver**，它可以执行比 cuBLAS 中提供的更复杂的线性代数操作，例如 **奇异值分解**（**SVD**）或 Cholesky 分解。
- en: So far, we have been primarily dealing with one single Python module that acted
    as our gateway to CUDA—PyCUDA. While PyCUDA is a very powerful and versatile Python
    library, its main purpose is to provide a gateway to program, compile, and launch
    CUDA kernels, rather than provide an interface to the CUDA libraries. To this
    end, fortunately, there is a free Python module available that provides a user-friendly
    wrapper interface to these libraries. This is called Scikit-CUDA.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要处理的是一个充当我们进入 CUDA 的门户的单个 Python 模块——PyCUDA。虽然 PyCUDA 是一个非常强大且多功能的
    Python 库，但其主要目的是提供进入程序、编译和启动 CUDA 内核的门户，而不是提供对 CUDA 库的接口。为此，幸运的是，有一个免费的 Python
    模块可用，它提供了一个用户友好的包装器接口来访问这些库。这被称为 Scikit-CUDA。
- en: While you don't have to know PyCUDA or even understand GPU programming to appreciate
    Scikit-CUDA, it is conveniently compatible with PyCUDA; Scikit-CUDA, for instance,
    can operate easily with PyCUDA's `gpuarray` class, and this allows you to easily
    pass data between our own CUDA kernel routines and Scikit-CUDA. Additionally,
    most routines will also work with PyCUDA's stream class, which will allow us to
    properly synchronize our own custom CUDA kernels with Scikit-CUDA's wrappers.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你不需要了解 PyCUDA 或甚至理解 GPU 编程就能欣赏 Scikit-CUDA，但它与 PyCUDA 的兼容性非常方便；例如，Scikit-CUDA
    可以轻松地与 PyCUDA 的 `gpuarray` 类一起操作，这允许你轻松地在我们的 CUDA 内核例程和 Scikit-CUDA 之间传递数据。此外，大多数例程也将与
    PyCUDA 的流类一起工作，这将使我们能够正确地同步我们的自定义 CUDA 内核与 Scikit-CUDA 的包装器。
- en: Please note that, besides these three listed libraries, Scikit-CUDA also provides
    wrappers for the proprietary CULA library, as well as for the open source MAGMA
    library. Both have a lot of overlap with the functionality provided by the official
    NVIDIA libraries. Since these libraries are not installed by default with a standard
    CUDA installation, we will opt to not cover them in this chapter. Interested readers
    can learn more about CULA and MAGMA at [http://www.culatools.com](http://www.culatools.com)
    and [http://icl.utk.edu/magma/](http://icl.utk.edu/magma/), respectively.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，除了这三个列出的库之外，Scikit-CUDA 还为专有的 CULA 库以及开源的 MAGMA 库提供了包装器。这两个库与官方 NVIDIA 库提供的功能有很多重叠。由于这些库不是默认与标准
    CUDA 安装一起安装的，因此我们选择在本章中不涵盖它们。感兴趣的读者可以在 [http://www.culatools.com](http://www.culatools.com)
    和 [http://icl.utk.edu/magma/](http://icl.utk.edu/magma/) 分别了解更多关于 CULA 和 MAGMA
    的信息。
- en: 'It is suggested that readers take a look at the official documentation for
    Scikit-CUDA, which is available here: [https://media.readthedocs.org/pdf/scikit-cuda/latest/scikit-cuda.pdf](https://media.readthedocs.org/pdf/scikit-cuda/latest/scikit-cuda.pdf).'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 建议读者查看 Scikit-CUDA 的官方文档，该文档可在以下位置找到：[https://media.readthedocs.org/pdf/scikit-cuda/latest/scikit-cuda.pdf](https://media.readthedocs.org/pdf/scikit-cuda/latest/scikit-cuda.pdf)。
- en: 'The learning outcomes for this chapter are as follows:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的学习成果如下：
- en: To learn how to install Scikit-CUDA
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何安装 Scikit-CUDA
- en: To understand the basic purposes and differences between the standard CUDA libraries
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解标准 CUDA 库的基本目的和区别
- en: To learn how to use low-level cuBLAS functions for basic linear algebra
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何使用低级 cuBLAS 函数进行基本线性代数
- en: To learn how to use the SGEMM and DGEMM operations to measure the performance
    of a GPU in FLOPS
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要学习如何使用SGEMM和DGEMM操作来衡量GPU在FLOPS中的性能
- en: To learn how to use cuFFT to perform 1D or 2D FFT operations on the GPU
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要学习如何使用cuFFT在GPU上执行1D或2D FFT操作
- en: To learn how to create a 2D convolutional filter using the FFT, and apply it
    to simple image processing
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要学习如何使用FFT创建2D卷积滤波器并将其应用于简单的图像处理
- en: To understand how to perform a Singular Value Decomposition (SVD) with cuSolver
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何使用cuSolver执行奇异值分解（SVD）
- en: To learn how to use cuSolver's SVD algorithm to perform basic principal component
    analysis
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要学习如何使用cuSolver的SVD算法进行基本的特征值分析
- en: Technical requirements
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: A Linux or Windows 10 PC with a modern NVIDIA GPU (2016—onward) is required
    for this chapter, with all of the necessary GPU drivers and the CUDA Toolkit (9.0–onward)
    installed. A suitable Python 2.7 installation (such as Anaconda Python 2.7) that
    includes the PyCUDA module is also required.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成本章内容，需要一个配备现代NVIDIA GPU（2016年及以后）的Linux或Windows 10 PC，并安装所有必要的GPU驱动程序和CUDA
    Toolkit（9.0及以后版本）。还需要一个合适的Python 2.7安装（例如Anaconda Python 2.7），其中包含PyCUDA模块。
- en: This chapter's code is also available on GitHub, and can be found at [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码也托管在GitHub上，可以在[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)找到。
- en: For more information about the prerequisites, check out the preface of this
    book. For more information about the software and hardware requirements, check
    out the README file at [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 关于先决条件的信息，请参阅本书的序言。有关软件和硬件要求的信息，请参阅[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)的README文件。
- en: Installing Scikit-CUDA
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Scikit-CUDA
- en: 'It is suggested that you install the latest stable version of Scikit-CUDA directly
    from GitHub: [https://github.com/lebedov/scikit-cuda](https://github.com/lebedov/scikit-cuda).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 建议您直接从GitHub安装Scikit-CUDA的最新稳定版本：[https://github.com/lebedov/scikit-cuda](https://github.com/lebedov/scikit-cuda)。
- en: Unzip the package into a directory, and then open up the command line here and
    install the module by typing `python setup.py install` into the command line.
    You may then run the unit tests to ensure that a correct installation has been
    performed with `python setup.py test`. (This method is suggested for both Windows
    and Linux users.) Alternatively, Scikit-CUDA can be installed directly from the
    PyPI repository with `pip install scikit-cuda`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 解压包到目录中，然后在此处打开命令行，通过在命令行中输入`python setup.py install`来安装模块。然后，您可以通过输入`python
    setup.py test`来运行单元测试，以确保已正确安装。（此方法适用于Windows和Linux用户。）或者，您可以直接从PyPI仓库使用`pip install
    scikit-cuda`来安装Scikit-CUDA。
- en: Basic linear algebra with cuBLAS
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用cuBLAS进行基本线性代数
- en: We will start this chapter by learning how to use Scikit-CUDA's cuBLAS wrappers.
    Let's spend a moment discussing BLAS. BLAS (Basic Linear Algebra Subroutines)
    is a specification for a basic linear algebra library that was first standardized
    in the 1970s. BLAS functions are broken down into several categories, which are
    referred to as *levels*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从这个章节开始，学习如何使用Scikit-CUDA的cuBLAS封装器。让我们花一点时间来讨论BLAS。BLAS（基本线性代数子例程）是一个关于基本线性代数库的规范，它最初在20世纪70年代被标准化。BLAS函数被分为几个类别，这些类别被称为*级别*。
- en: Level 1 BLAS functions consist of operations purely on vectors—vector-vector
    addition and scaling (also known as *ax+y* operations, or AXPY), dot products,
    and norms. Level 2 BLAS functions consist of general matrix-vector operations
    (GEMV), such as matrix multiplication of a vector, while level 3 BLAS functions
    consist of "general matrix-matrix" (GEMM) operations, such as matrix-matrix multiplication.
    Originally, these libraries were written entirely in FORTRAN in the 1970s, so
    you should take into account that there are some seemingly archaic holdovers in
    usage and naming that may seem cumbersome to new users today.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Level 1 BLAS函数包括仅对向量进行的操作——向量加法和缩放（也称为*ax+y*操作，或AXPY），点积和范数。Level 2 BLAS函数包括通用矩阵-向量操作（GEMV），例如向量的矩阵乘法，而level
    3 BLAS函数包括“通用矩阵-矩阵”操作（GEMM），例如矩阵-矩阵乘法。最初，这些库在20世纪70年代完全用FORTRAN编写，因此您应该考虑到在用法和命名中可能存在一些看似过时的遗留问题，这些可能对今天的初学者来说显得繁琐。
- en: cuBLAS is NVIDIA's own implementation of the BLAS specification, which is of
    course optimized to make full use of the GPU's parallelism. Scikit-CUDA provides
    wrappers for cuBLAS that are compatible with PyCUDA `gpuarray` objects, as well
    as with PyCUDA streams. This means that we can couple and interface these functions
    with our own custom CUDA-C kernels by way of PyCUDA, as well as synchronize these
    operations over multiple streams.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: cuBLAS 是 NVIDIA 自行实现的 BLAS 规范，当然是为了充分利用 GPU 的并行性而进行了优化。Scikit-CUDA 为 cuBLAS
    提供了包装器，这些包装器与 PyCUDA `gpuarray` 对象以及 PyCUDA 流兼容。这意味着我们可以通过 PyCUDA 将这些函数与我们的自定义
    CUDA-C 内核耦合和接口，同时还可以通过多个流同步这些操作。
- en: Level-1 AXPY with cuBLAS
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: cuBLAS 的 Level-1 AXPY
- en: Let's start with a basic level-1 *ax + y* (or AXPY) operation with cuBLAS. Let's
    stop for a moment and review a bit of linear algebra and think about what this
    means. Here, *a* is considered to be a scalar; that is, a real number, such as
    -10, 0, 1.345, or 100\. *x* and *y* are considered to be vectors in some vector
    space, ![](img/47a6873c-3e1b-4b3c-95e8-d1a3a4f796eb.png). This means that *x*
    and *y* are n-tuples of real numbers, so in the case of ![](img/d0e81dc7-0fa8-4bce-a264-941fee2e3ad7.png),
    these could be values such as `[1,2,3]` or `[-0.345, 8.15, -15.867]`. *ax* means
    the scaling of *x* by *a*, so if a is 10 and *x* is the first prior value, then
    *ax* is each individual value of *x* multiplied by *a;* that is, `[10, 20, 30]`.
    Finally, the sum *ax + y* means that we add each individual value in each slot
    of both vectors to produce a new vector, which would be as follows (assuming that
    *y* is the second vector given)—`[9.655, 28.15, 14.133]`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 cuBLAS 的基本 level-1 *ax + y*（或 AXPY）操作开始。让我们暂停一下，回顾一下线性代数，并思考这意味着什么。在这里，*a*
    被认为是标量；也就是说，一个实数，例如 -10、0、1.345 或 100。*x* 和 *y* 被认为是某个向量空间中的向量，![img/47a6873c-3e1b-4b3c-95e8-d1a3a4f796eb.png]。这意味着
    *x* 和 *y* 是由实数组成的 n-元组，所以在 ![img/d0e81dc7-0fa8-4bce-a264-941fee2e3ad7.png] 的情况下，这些可以是
    `[1,2,3]` 或 `[-0.345, 8.15, -15.867]` 这样的值。*ax* 表示 *x* 通过 *a* 的缩放，所以如果 *a* 是 10
    而 *x* 是先前的第一个值，那么 *ax* 就是 *x* 的每个单独值乘以 *a*；也就是说，`[10, 20, 30]`。最后，求和 *ax + y*
    意味着我们将两个向量中每个槽位的每个单独值相加，以产生一个新的向量，如下所示（假设 *y* 是给出的第二个向量）——`[9.655, 28.15, 14.133]`。
- en: 'Let''s do this in cuBLAS now. First, let''s import the appropriate modules:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 cuBLAS 中执行这个操作。首先，让我们导入适当的模块：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now let''s import cuBLAS:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们导入 cuBLAS：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can now set up our vector arrays and copy them to the GPU. Note that we
    are using 32-bit (single precision) floating point numbers:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以设置我们的向量数组并将它们复制到 GPU 上。请注意，我们正在使用 32 位（单精度）浮点数：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We now have to create a **cuBLAS context**. This is similar in nature to CUDA
    contexts, which we discussed in [Chapter 5](ea648e20-8c72-44a9-880d-11469d0e291f.xhtml),
    *Streams, Events, Contexts, and Concurrency*, only this time it is used explicitly
    for managing cuBLAS sessions. The `cublasCreate` function creates a cuBLAS context
    and gives a handle to it as its output. We will need to hold onto this handle
    for as long as we intend to use cuBLAS in this session:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在必须创建一个 **cuBLAS 上下文**。这与我们在 [第 5 章](ea648e20-8c72-44a9-880d-11469d0e291f.xhtml)
    中讨论的 CUDA 上下文在本质上相似，即 *流、事件、上下文和并发*，但这次它是专门用于管理 cuBLAS 会话的。`cublasCreate` 函数创建一个
    cuBLAS 上下文，并以句柄的形式返回它。我们将需要保留这个句柄，直到我们打算在这个会话中使用 cuBLAS：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can now use the `cublasSaxpy` function. The `S` stands for single precision,
    which is what we will need since we are working with 32-bit floating point arrays:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 `cublasSaxpy` 函数。其中的 `S` 代表单精度，这是我们需要的，因为我们正在处理 32 位浮点数组：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let's discuss what we just did. Also, let's keep in mind that this is a direct
    wrapper to a low-level C function, so the input may seem more like a C function
    than a true Python function. In short, this performed an "AXPY" operation, ultimately
    putting the output data into the `y_gpu` array. Let's go through each input parameter
    one by one.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论我们刚才做了什么。同时，请注意，这是一个对低级 C 函数的直接包装，因此输入可能看起来更像 C 函数而不是真正的 Python 函数。简而言之，这执行了一个
    "AXPY" 操作，最终将输出数据放入 `y_gpu` 数组中。让我们逐个检查每个输入参数。
- en: 'The first input is always the CUDA context handle. We then have to specify
    the size of the vectors, since this function will be ultimately operating on C
    pointers; we can do this by using the `size` parameter of a gpuarray. Having typecasted
    our scalar already to a NumPy `float32` variable, we can pass the `a` variable
    right over as the scalar parameter. We then hand the underlying C pointer of the
    `x_gpu` array to this function using the `gpudata` parameter. Then we specify
    the **stride** of the first array as 1: the stride specifies how many steps we
    should take between each input value. (In contrast, if you were using a vector
    from a column in a row-wise matrix, you would set the stride to the width of the
    matrix.) We then put in the pointer to the `y_gpu` array, and set its stride to
    1 as well.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个输入始终是CUDA上下文句柄。然后我们必须指定向量的大小，因为这个函数最终将在C指针上操作；我们可以通过使用gpuarray的`size`参数来完成这个操作。在我们已经将标量类型转换为NumPy的`float32`变量之后，我们可以直接将`a`变量作为标量参数传递。然后我们使用`gpudata`参数将`x_gpu`数组的底层C指针传递给这个函数。然后我们指定第一个数组的**步长**为1：步长指定了我们在每个输入值之间应该采取的步数。（相比之下，如果你正在使用来自行矩阵列的向量，你会将步长设置为矩阵的宽度。）然后我们放入`y_gpu`数组的指针，并将其步长也设置为1。
- en: 'We are done with our computation; now we have to explicitly destroy our cuBLAS
    context:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的计算已经完成；现在我们必须显式地销毁我们的cuBLAS上下文：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can now verify whether this is close with NumPy''s `allclose` function,
    like so:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用NumPy的`allclose`函数来验证这是否接近，如下所示：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Again, notice that the final output was put into the `y_gpu` array, which was
    also an input.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 再次注意，最终输出被放入了`y_gpu`数组，这同样也是一个输入。
- en: Always remember that BLAS and CuBLAS functions act in-place to save time and
    memory from a new allocation call. This means that an input array will also be
    used as an output!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 总是记住，BLAS和CuBLAS函数以原地方式操作以节省时间和内存，这意味着输入数组也将被用作输出！
- en: We just saw how to perform an `AXPY` operation using the `cublasSaxpy` function.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到了如何使用`cublasSaxpy`函数执行`AXPY`操作。
- en: Let's discuss the prominent upper case S. Like we mentioned previously, this
    stands for single precision that is, 32-bit real floating point values (`float32`).
    If we want to operate on arrays of 64-bit real floating point values, (`float64`
    in NumPy and PyCUDA), then we would use the `cublasDaxpy` function; for 64-bit
    single precision complex values (`complex64`), we would use `cublasCaxpy`, while
    for 128-bit double precision complex values (`complex128`), we would use `cublasZaxpy`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下突出的大写S。正如我们之前提到的，这代表单精度，即32位实浮点值（`float32`）。如果我们想操作64位实浮点值数组（NumPy和PyCUDA中的`float64`），那么我们会使用`cublasDaxpy`函数；对于64位单精度复数值（`complex64`），我们会使用`cublasCaxpy`，而对于128位双精度复数值（`complex128`），我们会使用`cublasZaxpy`。
- en: We can tell what type of data a BLAS or CuBLAS function operates on by checking
    the letter preceding the rest of the function name. Functions that use single
    precision reals are always preceded with S, double precision reals with D, single
    precision complex with C, and double precision complex with Z.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查函数名中其余部分之前字母来判断BLAS或CuBLAS函数操作的数据类型。使用单精度实数的函数总是以S开头，双精度实数以D开头，单精度复数以C开头，双精度复数以Z开头。
- en: Other level-1 cuBLAS functions
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他level-1 cuBLAS函数
- en: 'Let''s look at a few other level-1 functions. We won''t go over their operation
    in depth, but the steps are similar to the ones we just covered: create a cuBLAS
    context, call the function with the appropriate array pointers (which is accessed
    with the `gpudata` parameter from a PyCUDA `gpuarray`), and set the strides accordingly.
    Another thing to keep in mind is that if the output of a function is a single
    value as opposed to an array (for example, a dot product function), the function
    will directly output this value to the host rather than within an array of memory
    that has to be pulled off the GPU. (We will only cover the single precision real
    versions here, but the corresponding versions for other datatypes can be used
    by replacing the S with the appropriate letter.)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看几个其他的level-1函数。我们不会深入探讨它们的操作，但步骤与我们刚刚覆盖的类似：创建cuBLAS上下文，使用适当的数组指针调用函数（这些指针通过PyCUDA的`gpuarray`的`gpudata`参数访问），并相应地设置步长。另一件需要注意的事情是，如果一个函数的输出是一个单值而不是数组（例如，点积函数），该函数将直接将此值输出到主机，而不是在从GPU拉取的内存数组中。（我们在这里只涵盖单精度实数版本，但其他数据类型的相应版本可以通过将S替换为适当的字母来使用。）
- en: 'We can perform a dot product between two single precision real `gpuarray`s,
    `v_gpu`, and `w_gpu`. Again, the 1s are there to ensure that we are using stride-1
    in this calculation! Again, recall that a dot product is the sum of the point-wise
    multiple of two vectors:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在两个单精度浮点 `gpuarray`，`v_gpu` 和 `w_gpu` 之间执行点积。再次，1s 是为了确保我们在这种计算中使用步长-1！再次，回忆一下，点积是两个向量逐点乘积的和：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can also perform the L2-norm of a vector like so (recall that for a vector,
    *x*, this is its L2-norm, or length, which is calculated with the ![](img/839337d6-db29-481e-8467-bcd415a2ad7c.png)
    formula):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以这样计算向量的 L2-范数（回忆一下，对于向量 *x*，这是它的 L2-范数，或长度，使用 ![](img/839337d6-db29-481e-8467-bcd415a2ad7c.png)
    公式计算）：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Level-2 GEMV in cuBLAS
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: cuBLAS 中的 Level-2 GEMV
- en: 'Let''s look at how to do a `GEMV` matrix-vector multiplication. This is defined
    as the following operation for an *m* x *n* matrix *A*, an n-dimensional vector
    *x*, a *m*-dimensional vector *y*, and for the scalars *alpha* and *beta*:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何进行 `GEMV` 矩阵-向量乘法。这定义为以下操作，对于一个 *m* x *n* 矩阵 *A*，一个 n 维向量 *x*，一个 *m*
    维向量 *y*，以及标量 *alpha* 和 *beta*：
- en: '![](img/0b6277ff-e027-45fe-ad2e-d312ea1a38f5.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b6277ff-e027-45fe-ad2e-d312ea1a38f5.png)'
- en: 'Now let''s look at how the function is laid out before we continue:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们继续之前，让我们看看函数是如何布局的：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s go through these inputs one-by-one:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个检查这些输入：
- en: '`handle` refers to the cuBLAS context handle.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`handle` 指的是 cuBLAS 上下文句柄。'
- en: '`trans` refers to the structure of the matrix—we can specify whether we want
    to use the original matrix, a direct transpose, or a conjugate transpose (for
    complex matrices). This is important to keep in mind because this function will
    expect that the matrix `A` is stored in **column-major** format.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trans` 指的是矩阵的结构——我们可以指定我们是否想使用原始矩阵、直接转置或共轭转置（对于复数矩阵）。这一点很重要，因为这个函数将期望矩阵 `A`
    以 **列主序** 格式存储。'
- en: '`m` and `n` are the number of rows and columns of the matrix `A` that we want
    to use.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`m` 和 `n` 是我们想要使用的矩阵 `A` 的行数和列数。'
- en: '`alpha` is the floating-point value for *α.*'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha` 是 *α* 的浮点值。'
- en: '`A` is the *m x n* matrix *A.*'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`A` 是一个 *m x n* 矩阵 *A*。'
- en: '`lda` indicates the leading dimension of the matrix, where the total size of
    the matrix is actually `lda` x `n`. This is important in the column-major format
    because if `lda` is larger than `m`, this can cause problems for cuBLAS when it
    tries to access the values of `A` since its underlying structure of this matrix
    is a one-dimensional array.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lda` 表示矩阵的领先维度，其中矩阵的总大小实际上是 `lda` x `n`。这在列主序格式中很重要，因为如果 `lda` 大于 `m`，当 cuBLAS
    尝试访问 `A` 的值时，由于其矩阵的底层结构是一个一维数组，这可能会引起问题。'
- en: We then have `x` and its stride, `incx`; `x` is the underlying C pointer of
    the vector being multiplied by `A`. Remember, `x` will have to be of size `n`;
    that is, the number of columns of `A`.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们有 `x` 和它的步长，`incx`；`x` 是被 `A` 乘的向量的底层 C 指针。记住，`x` 将必须具有大小 `n`；即 `A` 的列数。
- en: '`beta`, which is the floating-point value for *β*.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta`，是 *β* 的浮点值。'
- en: Finally, we have `y` and its stride `incy` as the last parameters. We should
    remember that `y` should be of size `m`, or the number of rows of `A`.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们有 `y` 和它的步长 `incy` 作为最后两个参数。我们应该记住，`y` 应该是大小为 `m`，即 `A` 的行数。
- en: 'Let''s test this by generating a 10 x 100 matrix of random values `A`, and
    a vector `x` of 100 random values. We''ll initialize `y` as a matrix of 10 zeros.
    We will set alpha to 1 and beta to 0, just to get a direct matrix multiplication
    with no scaling:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过生成一个 10 x 100 的随机值矩阵 `A` 和一个 100 个随机值的向量 `x` 来测试这个方法。我们将 `y` 初始化为一个 10
    个零的矩阵。我们将 alpha 设置为 1，beta 设置为 0，只是为了得到没有缩放的直接矩阵乘法：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will now have to get `A` into **column-major** (or column-wise) format.
    NumPy stores matrices as **row-major** (or row-wise) by default, meaning that
    the underlying one-dimensional array that is used to store a matrix iterates through
    all of the values of the first row, then all of the values of the second row,
    and so on. You should remember that a transpose operation swaps the columns of
    a matrix with its rows. However, the result will be that the new one-dimensional
    array underlying the transposed matrix will represent the original matrix in a
    column-major format. We can make a copy of the transposed matrix of `A` with `A.T.copy()`
    like so, and copy this as well as `x` and `y` to the GPU:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须将 `A` 转换为**列主序**（或列式）格式。NumPy 默认将矩阵存储为**行主序**（或行式），这意味着用于存储矩阵的底层一维数组会遍历第一行的所有值，然后是第二行的所有值，依此类推。你应该记住，转置操作会交换矩阵的列和行。然而，结果将是，转置矩阵的底层一维数组将以列主序格式表示原始矩阵。我们可以使用
    `A.T.copy()` 如此复制 `A` 的转置矩阵，并将这个以及 `x` 和 `y` 复制到 GPU 上：
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Since we now have the column-wise matrix stored properly on the GPU, we can
    set the `trans` variable to not take the transpose by using the `_CUBLAS_OP` dictionary:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在已经将列矩阵正确存储在 GPU 上，我们可以通过使用 `_CUBLAS_OP` 字典来设置 `trans` 变量，使其不进行转置：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Since the size of the matrix is exactly the same as the number of rows that
    we want to use, we now set `lda` as `m`. The strides for the *x* and *y* vectors
    are, again, 1\. We now have all of the values we need set up, and can now create
    our CuBLAS context and store its handle, like so:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于矩阵的大小正好与我们想要使用的行数相同，我们现在将 `lda` 设置为 `m`。*x* 和 *y* 向量的步长再次为 1。我们现在已经设置了所有需要的值，现在可以创建我们的
    CuBLAS 上下文并存储其句柄，如下所示：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can now launch our function. Remember that `A`, `x`, and `y` are actually
    PyCUDA `gpuarray` objects, so we have to use the `gpudata` parameter to input
    into this function. Other than doing this, this is pretty straightforward:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以启动我们的函数。记住，`A`、`x` 和 `y` 实际上是 PyCUDA `gpuarray` 对象，因此我们必须使用 `gpudata`
    参数将其输入到这个函数中。除了这样做之外，这个过程相当直接：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can now destroy our cuBLAS context and check the return value to ensure
    that it is correct:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以销毁我们的 cuBLAS 上下文并检查返回值以确保它是正确的：
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Level-3 GEMM in cuBLAS for measuring GPU performance
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: cuBLAS 中的 Level-3 GEMM 用于测量 GPU 性能
- en: 'We will now look at how to perform a **general matrix-matrix multiplication**
    (**GEMM**) with CuBLAS. We will actually try to make something a little more utilitarian
    than the last few examples we saw in cuBLAS—we will use this as a performance
    metric for our GPU to determine the number of **Floating Point Operations Per
    Second** (**FLOPS**) it can perform, which will be two separate values: the case
    of single precision, and that of double precision. Using GEMM is a standard technique
    for evaluating the performance of computing hardware in FLOPS, as it gives a much
    better understanding of sheer computational power than using pure clock speed
    in MHz or GHz.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何使用 CuBLAS 执行一个**通用矩阵-矩阵乘法**（**GEMM**）。实际上，我们将尝试做一些比我们在 cuBLAS 中看到的最后几个例子更有实用性的东西——我们将使用这个作为我们
    GPU 性能的指标，以确定它每秒可以执行的**浮点运算次数**（**FLOPS**），这将有两个不同的值：单精度和双精度。使用 GEMM 是评估计算硬件性能的标准技术，因为它比使用纯时钟速度（MHz
    或 GHz）提供了对纯粹计算能力的更好理解。
- en: If you need a brief review, recall that we covered matrix-matrix multiplication
    in depth in the last chapter. If you forgot how this works, it's strongly suggested
    that you review this chapter before you move on to this section.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要简要回顾，请记住我们在上一章中深入探讨了矩阵-矩阵乘法。如果你忘记了它是如何工作的，强烈建议你在继续本节之前复习这一章。
- en: 'First, let''s see how a GEMM operation is defined:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看 GEMM 操作是如何定义的：
- en: '![](img/6732ed55-6eea-497a-adcb-95731cc211b9.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6732ed55-6eea-497a-adcb-95731cc211b9.png)'
- en: This means that we perform a matrix multiplication of *A* and *B*, scale the
    result by *alpha*, and then add this to the *C* matrix that we have scaled by
    *beta,* placing the final result in *C*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们执行了矩阵 *A* 和 *B* 的乘法，将结果按 *alpha* 缩放，然后将其添加到我们按 *beta* 缩放的 *C* 矩阵中，最终结果放在
    *C* 中。
- en: 'Let''s think about how many floating point operations are executed to get the
    final result of a real-valued GEMM operation, assuming that *A* is an *m* x *k*
    (where *m* is rows and *k* is columns)matrix, *B* is a *k* x *n* matrix, and C
    is an *m* x *n* matrix. First, let''s figure out how many operations are required
    for computing *AB*. Let''s take a single column of *A* and multiply it by *B*:
    this will amount to *k* multiplies and *k - 1* adds for each of the *m* rows in
    *A*, which means that this is *km + (k-1)m* total operations over *m* rows. There
    are *n* columns in *B*, so computing *AB* will total to *kmn + (k-1)mn = 2kmn
    - mn* operations. Now, we use *alpha* to scale *AB*, which will be *m**n* operations,
    since that is the size of the matrix *AB*; similarly, scaling *C* by *beta* is
    another *m**n* operation. Finally, we add these two resulting matrices, which
    is yet another *mn* operation. This means that we will have a total of *2kmn -
    mn + 3mn = 2kmn + 2mn = 2mn(k+1)* floating point operations in a given GEMM operation.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑执行实值GEMM操作的最终结果所需的浮点运算次数，假设*A*是一个*m* x *k*（其中*m*是行，*k*是列）的矩阵，*B*是一个*k*
    x *n*矩阵，而*C*是一个*m* x *n*矩阵。首先，让我们确定计算*AB*所需的操作次数。让我们取*A*的一个列并乘以*B*：这将涉及每行*m*个乘法和*k
    - 1*个加法，这意味着在*A*的*m*行中总共是*km + (k-1)m*个操作。*B*有*n*列，因此计算*AB*将总共是*kmn + (k-1)mn
    = 2kmn - mn*个操作。现在，我们使用*alpha*来缩放*AB*，这将涉及*m**n*个操作，因为这是矩阵*AB*的大小；同样，通过*beta*缩放*C*是另一个*m**n*操作。最后，我们将这两个结果矩阵相加，这又是另一个*m*个操作。这意味着在给定的GEMM操作中，我们将有*2kmn
    - mn + 3mn = 2kmn + 2mn = 2mn(k+1)*个浮点操作。
- en: Now the only thing we have to do is run a timed GEMM operation, taking note
    of the different sizes of the matrices, and divide *2kmn + 2mn* by the total time
    duration to calculate the FLOPS of our GPU. The resulting number will be very
    large, so we will represent this in terms of GFLOPS – that is, how many billions
    (10⁹) of operations that can be computed per second. We can compute this by multiplying
    the FLOPS value by 10^(-9).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们唯一要做的就是运行一个计时GEMM操作，注意矩阵的不同大小，并将`2kmn + 2mn`除以总时间来计算我们GPU的FLOPS。得到的数字将非常大，因此我们将以GFLOPS的形式表示它——即每秒可以计算多少十亿（10⁹）个操作。我们可以通过将FLOPS值乘以10^(-9)来计算这个值。
- en: 'Now we are ready to start coding this up. Let''s start with our import statements,
    as well as the `time` function:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备开始编写代码。让我们从导入语句以及`time`函数开始：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we will set the `m`, `n`, and `k` variables for our matrix sizes. We want
    our matrices to be relatively big so that the time duration is sufficiently large
    so as to avoid divide by 0 errors. The following values should be sufficient for
    any GPU released up to mid-2018 or earlier; users with newer cards may consider
    increasing these values:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将为我们的矩阵大小设置`m`、`n`和`k`变量。我们希望我们的矩阵相对较大，这样时间跨度就足够大，以避免除以0的错误。以下值对于截至2018年中或更早发布的任何GPU都应该是足够的；使用较新卡的用户可能需要增加这些值：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We will now write a function that computes the GFLOPS for both single and double
    precision. We will set the input value to `''D''` if we wish to use double precision,
    or `''S''` otherwise:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将编写一个函数来计算单精度和双精度下的GFLOPS。如果我们想使用双精度，我们将输入值设置为`'D'`，否则设置为`'S'`：
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now let's generate some random matrices that are of the appropriate precision
    that we will use for timing. The GEMM operations act similarly to the GEMV operation
    we saw before, so we will have to transpose these before we copy them to the GPU.
    (Since we are just doing timing, this step isn't necessary, but it's good practice
    to remember this.)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将生成一些随机矩阵，这些矩阵的精度与我们用于计时的精度相匹配。GEMM操作与之前看到的GEMV操作类似，因此在我们将它们复制到GPU之前，我们必须对这些矩阵进行转置。（由于我们只是在计时，这一步不是必需的，但记住这一点是个好习惯。）
- en: 'We will set up some other necessary variables for GEMM, whose purpose should
    be self-explanatory at this point (`transa`, `lda`, `ldb`, and so on):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为GEMM设置一些其他必要的变量，其目的在此点应该是显而易见的（`transa`、`lda`、`ldb`等）：
- en: '[PRE19]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can now start the timer! First, we will create a cuBLAS context:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始计时了！首先，我们将创建一个cuBLAS上下文：
- en: '[PRE20]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We will now launch GEMM. Keep in mind that there are two versions for the real
    case: `cublasSgemm` for single precision and `cublasDgemm` for double precision.
    We can execute the appropriate function using a little Python trick: we will write
    a string with `cublas%sgemm` with the appropriate parameters, and then replace
    the `%s` with D or S by appending `% precision` to the string. We will then execute
    this string as Python code with the `exec` function, like so:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将启动GEMM。请注意，对于实数情况有两个版本：`cublasSgemm`用于单精度，`cublasDgemm`用于双精度。我们可以使用一点Python技巧执行适当的函数：我们将写一个带有`cublas%sgemm`的字符串，并带有适当的参数，然后通过在字符串中追加`%
    precision`来将`%s`替换为D或S。然后我们将使用`exec`函数将此字符串作为Python代码执行，如下所示：
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can now destroy the cuBLAS context and get the final time for our computation:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以销毁cuBLAS上下文，并获取我们计算的最后时间：
- en: '[PRE22]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then we need to compute the GFLOPS using the equation we derived and return
    it as the output of this function:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要使用我们推导出的方程计算GFLOPS，并将其作为此函数的输出返回：
- en: '[PRE23]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now we can set up our main function. We will output the GFLOPS in both the
    single and double precision cases:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以设置我们的主函数。我们将输出单精度和双精度情况下的GFLOPS：
- en: '[PRE24]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now let''s do a little homework before we run this program—go to [https://www.techpowerup.com](https://www.techpowerup.com)
    and search for your GPU, and then take note of two things—the single precision
    floating point performance and the double precision floating point performance.
    I am using a GTX 1050 right now, and it''s listing claims that it has 1,862 GFLOPS
    performance in single precision, and 58.20 GFLOPS performance in double precision.
    Let''s run this program right now and see if this aligns with the truth:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们运行这个程序之前，先做一点作业——去[https://www.techpowerup.com](https://www.techpowerup.com)搜索你的GPU，并注意两件事——单精度浮点性能和双精度浮点性能。我现在使用的是GTX
    1050，它声称单精度性能为1,862 GFLOPS，双精度性能为58.20 GFLOPS。让我们现在运行这个程序，看看它是否与事实相符：
- en: '![](img/0d014970-b902-4cd8-804a-433bf0b83d77.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d014970-b902-4cd8-804a-433bf0b83d77.png)'
- en: Lo and behold, it does!
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 看看吧，它确实是这样！
- en: This program is also available as the `cublas_gemm_flops.py` file under the
    directory in this book's repository.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 此程序也作为`cublas_gemm_flops.py`文件包含在此书存储库的目录中。
- en: Fast Fourier transforms with cuFFT
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用cuFFT进行快速傅里叶变换
- en: 'Now let''s look at how we can do some basic **fast Fourier transforms** (**FFT**)
    with cuFFT. First, let''s briefly review what exactly a Fourier transform is.
    If you have taken an advanced Calculus or Analysis class, you might have seen
    the Fourier transform defined as an integral formula, like so:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看我们如何使用cuFFT进行一些基本的**快速傅里叶变换**（**FFT**）。首先，让我们简要回顾一下傅里叶变换究竟是什么。如果你已经上过高级微积分或分析课程，你可能已经看到傅里叶变换被定义为积分公式，如下所示：
- en: '![](img/d1c79c32-6eab-4a52-a2ef-4af0bc192f5c.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1c79c32-6eab-4a52-a2ef-4af0bc192f5c.png)'
- en: What this does is take *f* as a time domain function over *x*. This gives us
    a corresponding frequency domain function over "ξ". This turns out to be an incredibly
    useful tool that touches virtually all branches of science and engineering.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这所做的就是将*f*视为*x*上的时域函数。这给我们一个在"ξ"上的对应频域函数。这实际上是一个非常有用的工具，几乎触及了科学和工程的各个分支。
- en: 'Let''s remember that the integral can be thought of as a sum; likewise, there
    is a corresponding discrete, finite version of the Fourier Transform called the
    **discrete Fourier transform** (**DFT**). This operates on vectors of a finite
    length and allows them to be analyzed or modified in the frequency domain. The
    DFT of an *n*-dimensional vector *x* is defined as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们记住，积分可以被视为求和；同样，也存在傅里叶变换的对应离散、有限版本，称为**离散傅里叶变换**（**DFT**）。它作用于有限长度的向量，并允许在频域内分析或修改它们。一个*n*-维向量*x*的DFT定义如下：
- en: '![](img/8b60bac2-8488-4c5f-9d90-3ac7eb73bd62.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b60bac2-8488-4c5f-9d90-3ac7eb73bd62.png)'
- en: In other words, we can multiply a vector, *x*, by the complex *N* x *N* matrix
    ![](img/96b3a1fb-9202-44fa-8b1d-381398412504.png)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以将向量*x*乘以复数*N* x *N*矩阵![](img/96b3a1fb-9202-44fa-8b1d-381398412504.png)
- en: '(here, *k* corresponds to row number, while *n* corresponds to column number)
    to find its DFT. We should also note the inverse formula that lets us retrieve
    *x* from its DFT (replace *y* with the DFT of *x* here, and the output will be
    the original *x*):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: （在这里，*k*对应行号，而*n*对应列号）以找到其DFT。我们还应该注意逆公式，它允许我们从DFT中检索*x*（在这里用*x*的DFT替换*y*，输出将是原始的*x*）：
- en: '![](img/25229fe6-66c3-4ca5-b8d7-96ba5e639917.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/25229fe6-66c3-4ca5-b8d7-96ba5e639917.png)'
- en: Normally, computing a matrix-vector operation is of computational complexity
    O(*N²*) for a vector of length *N*. However, due to symmetries in the DFT matrix,
    this can always be reduced to O(*N log N*) by using an FFT. Let's look at how
    we can use an FFT with CuBLAS, and then we will move on to a more interesting
    example.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，计算矩阵-向量运算的计算复杂度为 O(*N²*)，其中 *N* 是向量的长度。然而，由于 DFT 矩阵中的对称性，这可以通过使用 FFT 总是减少到
    O(*N log N*)。让我们看看我们如何使用 FFT 与 CuBLAS，然后我们将转向一个更有趣的例子。
- en: A simple 1D FFT
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单的 1D FFT
- en: Let's start by looking at how we can use cuBLAS to compute a simple 1D FFT.
    First, we will briefly discuss the cuFFT interface in Scikit-CUDA.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看如何使用 cuBLAS 来计算一个简单的 1D FFT。首先，我们将简要讨论 Scikit-CUDA 中的 cuFFT 接口。
- en: There are two submodules here that we can access the cuFFT library with, `cufft`
    and `fft`. `cufft` consists of a collection of low-level wrappers for the cuFFT
    library, while `fft` provides a more user-friendly interface; we will be working
    solely with `fft` in this chapter.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个子模块，我们可以通过它们访问 cuFFT 库，分别是 `cufft` 和 `fft`。`cufft` 包含了 cuFFT 库的低级包装器集合，而
    `fft` 提供了一个更友好的用户界面；在本章中，我们将仅使用 `fft`。
- en: 'Let''s start with the appropriate imports, remembering to include the Scikit-CUDA
    `fft` submodule:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从适当的导入开始，记得要包含 Scikit-CUDA 的 `fft` 子模块：
- en: '[PRE25]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We now will set up some random array and copy it to the GPU. We will also set
    up an empty GPU array that will be used to store the FFT (notice that we are using
    a real float32 array as an input, but the output will be a complex64 array, since
    the Fourier transform is always complex-valued):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将设置一个随机数组并将其复制到 GPU。我们还将设置一个空的 GPU 数组，该数组将用于存储 FFT（注意我们使用的是实数 float32 数组作为输入，但输出将是
    complex64 数组，因为傅里叶变换总是复值）：
- en: '[PRE26]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We will now set up a cuFFT plan for the forward FFT transform. This is an object
    that cuFFT uses to determine the shape, as well as the input and output data types
    of the transform:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将设置一个 cuFFT 计划用于前向 FFT 变换。这是一个 cuFFT 用来确定变换的形状以及输入和输出数据类型的对象：
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We will also set up a plan for the inverse FFT plan object. Notice that this
    time we go from `complex64` to real `float32`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将为逆 FFT 计划对象设置一个计划。注意这次我们从 `complex64` 转换到实数 `float32`：
- en: '[PRE28]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, we must take the forward FFT from `x_gpu` into `x_hat`, and the inverse
    FFT from `x_hat` back into `x_gpu`. Notice that we set `scale=True` in the inverse
    FFT; we do this to indicate to cuFFT to scale the inverse FFT by 1/N:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须将 `x_gpu` 中的前向 FFT 转换到 `x_hat`，并将 `x_hat` 的逆 FFT 转换回 `x_gpu`。注意我们在逆 FFT
    中设置了 `scale=True`；我们这样做是为了指示 cuFFT 将逆 FFT 缩放为 1/N：
- en: '[PRE29]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We now will check `x_hat` against a NumPy FFT of `x`, and `x_gpu` against `x`
    itself:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将检查 `x_hat` 与 `x` 的 NumPy FFT 相比，以及 `x_gpu` 与 `x` 本身相比：
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If you run this, you will see that `x_hat` does not match `y`, yet, inexplicably,
    `x_gpu` matches `x`. How is this possible? Well, let's remember that `x` is real;
    if you look at how the Discrete Fourier Transform is computed, you can prove mathematically
    that the outputs of a real vector will repeat as their complex conjugates after
    N/2\. While the NumPy FFT fully computes these values anyway, cuFFT saves time
    by only computing the first half of the outputs when it sees that the input is
    real, and it sets the remaining outputs to `0`. You should verify that this is
    the case by checking the preceding variables.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个程序，你会看到 `x_hat` 与 `y` 不匹配，然而，不可思议的是，`x_gpu` 与 `x` 匹配。这是怎么可能的？好吧，让我们记住
    `x` 是实数；如果你看看离散傅里叶变换是如何计算的，你可以通过数学证明，实数向量的输出将在 N/2 后重复其复共轭。尽管 NumPy FFT 无论如何都会完全计算这些值，但
    cuFFT 通过仅在输入是实数时只计算输出的一半来节省时间，并将剩余的输出设置为 `0`。你应该通过检查前面的变量来验证这一点。
- en: 'Thus, if we change the first print statement in the preceding code to only
    compare the first N/2 outputs between CuFFT and NumPy, then this will return true:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们把前面代码中的第一个打印语句改为只比较 CuFFT 和 NumPy 之间的前 N/2 个输出，那么这将返回 true：
- en: '[PRE31]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Using an FFT for convolution
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 FFT 进行卷积
- en: 'We will now look at how we can use an FFT to perform **convolution**. Let''s
    review what exactly convolution is, first: given two one-dimensional vectors,
    *x* and *y*, their convolution is defined as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将探讨如何使用 FFT 来执行 **卷积**。首先，让我们回顾一下卷积的确切含义：给定两个一维向量，*x* 和 *y*，它们的卷积定义为以下：
- en: '![](img/34574397-830b-446f-8e5f-34b468e76b3e.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/34574397-830b-446f-8e5f-34b468e76b3e.png)'
- en: This is of interest to us because if *x* is some long, continuous signal, and
    *y* only has a small amount of localized non-zero values, then *y* will act as
    a filter on *x*; this has many applications in itself. First, we can use a filter
    to smooth the signal *x* (as is common in digital signal processing and image
    processing). We can also use it to collect samples of the signal *x* so as to
    represent the signal or compress it (as is common in the field of data compression
    or compressive sensing), or use filters to collect features for signal or image
    recognition in machine learning. This idea forms the basis for convolutional neural
    networks).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们来说很有趣，因为如果*x*是某个长而连续的信号，而*y*只有少量局部非零值，那么*y*将作为*x*的滤波器；这本身就有很多应用。首先，我们可以使用滤波器来平滑信号*x*（这在数字信号处理和图像处理中很常见）。我们还可以用它来收集信号*x*的样本，以表示或压缩信号（这在数据压缩或压缩感知领域很常见），或者使用滤波器收集信号或图像识别中的特征（这在机器学习中很常见）。这个想法是卷积神经网络的基础）。
- en: 'Of course, computers cannot handle infinitely long vectors (at least, not yet),
    so we will be considering **circular convolution**. In circular convolution, we
    are dealing with two length *n*-vectors whose indices below 0 or above n-1 will
    wrap around to the other end; that is to say, *x*[-1] = *x*[n-1], *x*[-2] = *x*[n-2],
    *x*[n] = *x*[0], *x*[n+1] = *x*[1], and so on. We define circular convolution
    of *x* and *y* like so:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，计算机无法处理无限长的向量（至少目前还不能），因此我们将考虑**圆卷积**。在圆卷积中，我们处理的是两个长度为*n*的向量，其索引小于0或大于n-1将环绕到另一端；也就是说，*x*[-1]
    = *x*[n-1]，*x*[-2] = *x*[n-2]，*x*[n] = *x*[0]，*x*[n+1] = *x*[1]，依此类推。我们这样定义*x*和*y*的圆卷积：
- en: '![](img/69ee6cc9-5c17-40f2-a4f5-675f5a0a9ee2.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/69ee6cc9-5c17-40f2-a4f5-675f5a0a9ee2.png)'
- en: 'It turns out that we can perform a circular convolution using an FFT quite
    easily; we can do this by performing an FFT on *x* and *y*, point-wise-multiplying
    the outputs, and then performing an inverse FFT on the final results. This result
    is known as the **convolution theorem**, which can also be expressed as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，我们可以非常容易地使用FFT执行圆卷积；我们可以通过在*x*和*y*上执行FFT，逐点相乘输出，然后在最终结果上执行逆FFT来实现这一点。这个结果被称为**卷积定理**，也可以如下表示：
- en: '![](img/d4a9eaac-c5fb-4ec2-8544-35b8e7388209.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d4a9eaac-c5fb-4ec2-8544-35b8e7388209.png)'
- en: We will be doing this over two dimensions, since we wish to apply the result
    to signal processing. While we have only seen the math for FFTs and convolution
    along one dimension, two-dimensional convolution and FFTs work very similarly
    to their one-dimensional counterparts, only with some more complex indexing. We
    will opt to skip over this, however, so that we can get directly into the application.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在两个维度上执行此操作，因为我们希望将结果应用于信号处理。虽然我们只看到了一维FFT和卷积的数学公式，但二维卷积和FFT与它们的一维对应物非常相似，只是索引更加复杂。然而，我们将选择跳过这部分，以便直接进入应用。
- en: Using cuFFT for 2D convolution
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用cuFFT进行2D卷积
- en: 'Now we are going to make a small program that performs **Gaussian filtering**
    on an image using cuFFT-based two-dimensional convolution. Gaussian filtering
    is an operation that smooths a rough image using what is known as a Gaussian filter.
    This is named as such because it is based on the Gaussian (normal) distribution
    in statistics. This is how the Gaussian filter is defined over two dimensions
    with a standard deviation of σ:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将编写一个小程序，使用基于cuFFT的二维卷积对图像进行**高斯滤波**。高斯滤波是一种使用高斯（正态）分布进行平滑的图像操作。之所以这样命名，是因为它基于统计学中的高斯（正态）分布。这是如何定义具有标准差σ的高斯滤波器在二维上的定义：
- en: '![](img/06f381b9-2e9d-48aa-95e7-265b988f144d.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/06f381b9-2e9d-48aa-95e7-265b988f144d.png)'
- en: When we convolve a discrete image with a filter, we sometimes refer to the filter
    as a **convolution kernel**. Oftentimes, image processing engineers will just
    call this a plain kernel, but since we don't want to confuse these with CUDA kernels,
    we will always use the full term, convolution kernel. We will be using a discrete
    version of the Gaussian filter as our convolution kernel here.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将一个离散图像与一个滤波器卷积时，我们有时将这个滤波器称为**卷积核**。通常，图像处理工程师会直接称其为普通核，但为了避免与CUDA核混淆，我们总是使用全称，即卷积核。在这里，我们将使用高斯滤波器的离散版本作为我们的卷积核。
- en: 'Let''s start with the appropriate imports; notice that we will use the Scikit-CUDA
    submodule `linalg` here. This will provide a higher-level interface for us than
    cuBLAS. Since we''re working with images here, we will also import Matplotlib''s
    `pyplot` submodule. Also note that we will use Python 3-style division here, from
    the first line; this means that if we divide two integers with the `/` operator,
    then the return value will be a float without typecasting (we perform integer
    division with the `//` operator):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从适当的导入开始；请注意，在这里我们将使用Scikit-CUDA子模块`linalg`。这将为我们的提供比cuBLAS更高层次的接口。由于我们在这里处理图像，我们还将导入Matplotlib的`pyplot`子模块。此外，请注意，我们将在这里使用Python
    3风格的除法，从第一行开始；这意味着如果我们用`/`运算符除以两个整数，则返回值将是一个浮点数，无需类型转换（我们使用`//`运算符执行整数除法）：
- en: '[PRE32]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let''s jump right in and start writing the convolution function. This will
    take in two NumPy arrays of the same size, `x` and `y`. We will typecast these
    to complex64 arrays, and then return `-1` if they are not of the same size:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接开始编写卷积函数。这将接受两个大小相同的NumPy数组`x`和`y`。我们将将这些数组类型转换为complex64数组，然后如果它们的大小不同，则返回`-1`：
- en: '[PRE33]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We will now set up our FFT plan and inverse FFT plan objects:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将设置FFT计划和逆FFT计划对象：
- en: '[PRE34]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now we can copy our arrays to the GPU. We will also set up some empty arrays
    of the appropriate sizes to hold the FFTs of these arrays, plus one additional
    array that will hold the output of the final convolution, `out_gpu`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将我们的数组复制到GPU。我们还将设置一些适当大小的空数组来存储这些数组的FFT，以及一个额外的数组，它将存储最终卷积的输出`out_gpu`：
- en: '[PRE35]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We now can perform our FFTs:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以执行我们的FFT：
- en: '[PRE36]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We will now perform pointwise (Hadamard) multiplication between `x_fft` and
    `y_fft` with the `linalg.multiply` function. We will set `overwrite=True` so as
    to write the final value into `y_fft`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用`linalg.multiply`函数在`x_fft`和`y_fft`之间执行逐点（Hadamard）乘法。我们将设置`overwrite=True`，以便将最终值写入`y_fft`：
- en: '[PRE37]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now we will call the inverse FFT, outputting the final result into `out_gpu`.
    We transfer this value to the host and return it:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将调用逆FFT，将最终结果输出到`out_gpu`。我们将此值传输到主机并返回：
- en: '[PRE38]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We are not done yet. Our convolution kernel will be much smaller than our input
    image, so we will have to adjust the sizes of our two 2D arrays (both the convolution
    kernel and the image) so that they are equal and perform the pointwise multiplication
    between them. Not only should we ensure that they are equal, but we also need
    to ensure that we perform **zero padding** on the arrays and that we appropriately
    center the convolution kernel. Zero padding means that we add a buffer of zeros
    on the sides of the images so as to prevent a wrap-around error. If we are using
    an FFT to perform our convolution, remember that it is a circular convolution,
    so the edges will literally always wrap-around. When we are done with our convolution,
    we can remove the buffer from the outside of the image to get the final output
    image.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有完成。我们的卷积核将比我们的输入图像小得多，因此我们必须调整我们两个二维数组（卷积核和图像）的大小，使它们相等，并在它们之间执行逐点乘法。我们不仅要确保它们相等，还需要确保我们在数组上执行**零填充**，并且适当地居中卷积核。零填充意味着我们在图像的两侧添加一个零缓冲区，以防止环绕错误。如果我们使用FFT来执行我们的卷积，请记住，它是一个循环卷积，因此边缘将始终实际环绕。当我们完成卷积后，我们可以从图像的外部移除缓冲区，以获得最终的输出图像。
- en: 'Let''s create a new function called `conv_2d` that takes in a convolution kernel,
    `ker`, and an image, `img`. The padded image size will be (`2*ker.shape[0] + img.shape[0]`,
    `2*ker.shape[1] + img.shape[1]`). Let''s set up the padded convolution kernel
    first. We will create a 2D array of zeros of this size, and then set the upper-left
    submatrix as our convolution kernel, like so:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个新的函数`conv_2d`，它接受一个卷积核`ker`和一个图像`img`。填充后的图像大小将是（`2*ker.shape[0] + img.shape[0]`，`2*ker.shape[1]
    + img.shape[1]`）。让我们首先设置填充后的卷积核。我们将创建一个大小为这个的零二维数组，然后设置左上角的子矩阵为我们自己的卷积核，如下所示：
- en: '[PRE39]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We will now have to shift our convolution kernel so that its center is precisely
    at the coordinate (0,0). We can do this with the NumPy `roll` command:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须将卷积核移动，使其中心精确地位于坐标（0,0）。我们可以使用NumPy的`roll`命令来完成此操作：
- en: '[PRE40]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now we need to pad the input image:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要填充输入图像：
- en: '[PRE41]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now we have two arrays of the same size that are appropriately formatted. We
    can now use our `cufft_conv` function that we just wrote here:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有两个大小相同且格式适当的数组。我们现在可以使用我们刚刚编写的`cufft_conv`函数：
- en: '[PRE42]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We now can remove the zero buffer outside of our image. We then return the
    result:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以移除图像外部的零缓冲区。然后我们返回结果：
- en: '[PRE43]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We are not yet done. Let''s write some small functions to set up our Gaussian
    filter, and then we can move on to applying this to an image. We can write the
    basic filter itself with a single line using a lambda function:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有完成。让我们编写一些小函数来设置我们的高斯滤波器，然后我们可以继续将其应用于图像。我们可以使用lambda函数在一行中编写基本的过滤器：
- en: '[PRE44]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can now write a function that uses this filter to output a discrete convolution
    kernel. The convolution kernel will be of height and length `2*sigma + 1`, which
    is fairly standard:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以编写一个函数，使用这个过滤器输出一个离散的卷积核。卷积核的高度和长度将是`2*sigma + 1`，这是相当标准的：
- en: Notice that we normalize the values of our Gaussian kernel by summing its values
    into `total_` and dividing it.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们通过将高斯核的值相加到`total_`中并将其除以来归一化高斯核的值。
- en: '[PRE45]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We are now ready to test this on an image! As our test case, we will use Gaussian
    filtering to blur a color JPEG image of this book's editor, *Akshada Iyer*. (This
    image is available under the `Chapter07` directory in the GitHub repository with
    the file name `akshada.jpg`.) We will use Matplotlib's `imread` function to read
    the image; this is stored as an array of unsigned 8-bit integers ranging from
    0 to 255 by default. We will typecast this to an array of floats and normalize
    it so that all of the values will range from 0 to 1\.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好在一个图像上测试这个操作了！作为我们的测试案例，我们将使用高斯滤波来模糊这本书的编辑者*Akshada Iyer*的彩色JPEG图像。（此图像存储在GitHub仓库的`Chapter07`目录中，文件名为`akshada.jpg`。）我们将使用Matplotlib的`imread`函数来读取图像；默认情况下，它存储为一个从0到255的无符号8位整数数组。我们将将其类型转换为浮点数数组并归一化，以便所有值都介于0到1之间。
- en: 'Note to the readers of the print edition of this text: although the print edition
    of this text is in greyscale, this a color image.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本书印刷版的读者：尽管本书的印刷版是灰度图像，但这是一个彩色图像。
- en: 'We will then set up an empty array of zeros that will store the blurred image:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将设置一个空的零数组来存储模糊后的图像：
- en: '[PRE46]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s set up our convolution kernel. Here, a standard deviation of 15 should
    be enough:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置我们的卷积核。在这里，标准差为15应该足够：
- en: '[PRE47]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We can now blur the image. Since this is a color image, we will have to apply
    Gaussian filtering to each color layer (red, green, and blue) individually; this
    is indexed by the third dimension in the image arrays:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以模糊图像了。由于这是一个彩色图像，我们必须对每个颜色层（红色、绿色和蓝色）分别应用高斯滤波；这在图像数组中由第三维索引：
- en: '[PRE48]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now let''s look at the Before and After images side-by-side by using some Matplotlib
    tricks:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用一些Matplotlib技巧来并排查看“之前”和“之后”的图像：
- en: '[PRE49]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We can now run the program and observe the effects of Gaussian filtering:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以运行程序并观察高斯滤波的效果：
- en: '![](img/ae723dc0-ccbd-4163-9c08-bb7caad3aa74.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae723dc0-ccbd-4163-9c08-bb7caad3aa74.png)'
- en: This program is available in the `Chapter07` directory in a file called `conv_2d.py`
    in the repository for this book.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 此程序存储在本书的`Chapter07`目录中，文件名为`conv_2d.py`。
- en: Using cuSolver from Scikit-CUDA
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scikit-CUDA的cuSolver
- en: We will now look at how we can use cuSolver from Scikit-CUDA's `linalg` submodule.
    Again, this provides a high-level interface for both cuBLAS and cuSolver, so we
    don't have to get caught up in the small details.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将查看如何使用Scikit-CUDA的`linalg`子模块中的cuSolver。同样，这为cuBLAS和cuSolver提供了一个高级接口，因此我们不必陷入细节。
- en: As we noted in the introduction, cuSolver is a library that's used for performing
    more advanced linear algebra operations than cuBLAS, such as the Singular Value
    Decomposition, LU/QR/Cholesky factorization, and eigenvalue computations. Since
    cuSolver, like cuBLAS and cuFFT, is another vast library, we will only take the
    time to look at one of the most fundamental operations in data science and machine
    learning—SVD.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在引言中提到的，cuSolver是一个库，用于执行比cuBLAS更高级的线性代数运算，例如奇异值分解、LU/QR/Cholesky分解和特征值计算。由于cuSolver，就像cuBLAS和cuFFT一样，也是一个庞大的库，我们将只花时间查看数据科学和机器学习中最基本的一个操作——奇异值分解（SVD）。
- en: 'Please refer to NVIDIA''s official documentation on cuSOLVER if you would like
    further information on this library: [https://docs.NVIDIA.com/cuda/cusolver/index.html](https://docs.nvidia.com/cuda/cusolver/index.html).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于这个库的信息，请参阅NVIDIA的官方cuSOLVER文档：[https://docs.NVIDIA.com/cuda/cusolver/index.html](https://docs.nvidia.com/cuda/cusolver/index.html)。
- en: Singular value decomposition (SVD)
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奇异值分解（SVD）
- en: SVD takes any *m* x *n* matrix *A*, and then returns three matrices in return—*U*,
    *Σ*, and *V*. Here, *U* is an *m* x *m* unitary matrix, *Σ* is an *m* x *n* diagonal
    matrix, and *V* is an *n* x *n* unitary matrix. By *unitary*, we mean that a matrix's
    columns form an orthonormal basis; by *diagonal*, we mean that all values in the
    matrix are zero, except for possibly the values along its diagonal.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: SVD 可以处理任何 *m* x *n* 矩阵 *A*，然后返回三个矩阵——*U*、*Σ* 和 *V*。在这里，*U* 是一个 *m* x *m* 的单位矩阵，*Σ*
    是一个 *m* x *n* 的对角矩阵，而 *V* 是一个 *n* x *n* 的单位矩阵。当我们说 *单位* 时，意味着矩阵的列构成了一个正交归一基；当我们说
    *对角* 时，意味着矩阵中的所有值都是零，除了可能的对角线上的值。
- en: The significance of the SVD is that this decomposes *A* into these matrices
    so that we have *A = UΣV^T* ; moreover, the values along the diagonal of *Σ* will
    all be positive or zero, and are known as the singular values. We will see some
    applications of this soon, but you should keep in mind that the computational
    complexity of SVD is of the order O(*mn²*)—for large matrices, it is definitely
    a good idea to use a GPU, since this algorithm is parallelizable.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: SVD 的意义在于，它将 *A* 分解成这些矩阵，使得我们有 *A = UΣV^T*；此外，*Σ* 对角线上的值都将为正或零，这些值被称为奇异值。我们很快将看到一些应用，但您应该记住，SVD
    的计算复杂度为 O(*mn²*)——对于大型矩阵，使用 GPU 确实是一个好主意，因为这个算法是可并行的。
- en: 'We''ll now look at how we can compute the SVD of a matrix. Let''s make the
    appropriate import statements:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看看如何计算矩阵的 SVD。让我们编写适当的导入语句：
- en: '[PRE50]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We will now generate a relatively large random matrix and transfer it to the
    GPU:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将生成一个相对较大的随机矩阵并将其传输到 GPU：
- en: '[PRE51]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We can now execute the SVD. This will have three outputs corresponding to the
    matrices that we just described. The first parameter will be the matrix array
    we just copied to the GPU. Then we need to specify that we want to use cuSolver
    as our backend for this operation:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以执行 SVD。这将产生三个输出，对应于我们刚才描述的矩阵。第一个参数将是刚刚复制到 GPU 的矩阵数组。然后我们需要指定我们想要使用 cuSolver
    作为此操作的底层后端：
- en: '[PRE52]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now let''s copy these arrays from the GPU to the host:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这些数组从 GPU 复制到主机：
- en: '[PRE53]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '`s` is actually stored as a one-dimensional array; we will have to create a
    zero matrix of size 1000 x 5000 and copy these values along the diagonal. We can
    do this with the NumPy `diag` function, coupled with some array slicing:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`s` 实际上存储为一个一维数组；我们需要创建一个大小为 1000 x 5000 的零矩阵，并将这些值沿对角线复制。我们可以使用 NumPy 的 `diag`
    函数和一些数组切片来完成这个操作：'
- en: '[PRE54]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We can now matrix-multiply these values on the host with the NumPy `dot` function
    to verify that they match up to our original array:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 NumPy 的 `dot` 函数在主机上对这些值进行矩阵乘法，以验证它们是否与我们的原始数组匹配：
- en: '[PRE55]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Since we are using only float32s and our matrix is relatively large, a bit of
    numerical error was introduced; we had to set the "tolerance" level (`atol`) a
    little higher than usual here, but it's still small enough to verify that the
    two arrays are sufficiently close.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只使用 float32 并且我们的矩阵相对较大，引入了一些数值误差；我们不得不将“容差”级别（`atol`）设置得比通常稍高，但它仍然足够小，可以验证这两个数组足够接近。
- en: Using SVD for Principal Component Analysis (PCA)
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SVD 进行主成分分析（PCA）
- en: '**Principal Component Analysis** (**PCA**) is a tool that''s used primarily
    for dimensionality reduction. We can use this to look at a dataset and find which
    dimensions and linear subspaces are the most salient. While there are several
    ways to implement this, we will show you how to perform PCA using SVD.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）是一种主要用于降维的工具。我们可以使用它来查看数据集，并找出哪些维度和线性子空间最为显著。虽然有多种实现方式，但我们将向您展示如何使用
    SVD 来执行 PCA。'
- en: 'We''ll do this as follows—we will work with a dataset that exists in 10 dimensions.
    We will start by creating two vectors that are heavily weighted in the front,
    and 0 otherwise:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这样做——我们将使用一个存在于 10 维的 dataset。我们首先将创建两个向量，它们在前面的权重很大，其他地方为 0：
- en: '[PRE56]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We will then add 9,000 additional vectors: 6,000 of these will be the same
    as the first two vectors, only with a little added random white noise, and the
    remaining 3,000 will just be random white noise:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将随后添加 9,000 个额外的向量：其中 6,000 个与最初的两个向量相同，只是增加了一点点随机白噪声，剩下的 3,000 个则完全是随机白噪声：
- en: '[PRE57]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We will now typecast the `vals` list to a `float32` NumPy array. We take the
    mean over the rows and subtract this value from each row. (This is a necessary
    step for PCA.) We then transpose this matrix, since cuSolver requires that input
    matrices have fewer or equal rows compared to the columns:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将`vals`列表强制转换为`float32` NumPy数组。我们对行取平均值，然后从每行减去这个值。（这是PCA的一个必要步骤。）然后我们转置这个矩阵，因为cuSolver要求输入矩阵的行数少于或等于列数：
- en: '[PRE58]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We will now run cuSolver, just like we did previously, and copy the output
    values off of the GPU:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将像之前一样运行cuSolver，并将输出值从GPU复制出来：
- en: '[PRE59]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Now we are ready to begin our investigative work. Let''s open up IPython and
    take a closer look at `u` and `s`. First, let''s look at s; its values are actually
    the square roots of the **principal values**, so we will square them and then
    take a look:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好开始我们的调查工作了。让我们打开IPython，更仔细地查看`u`和`s`。首先，让我们看看`s`；它的值实际上是**主值**的平方根，因此我们将它们平方然后查看：
- en: '![](img/28321a31-a6fb-49e8-974f-1b2caecfe01b.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/28321a31-a6fb-49e8-974f-1b2caecfe01b.png)'
- en: 'You will notice that the first two principal values are of the order 10⁵, while
    the remaining components are of the order 10^(-3). This tells us there is only
    really a two-dimensional subspace that is even relevant to this data at all, which
    shouldn''t be surprising. These are the first and second values, which will correspond
    to the first and second principal components that is, the corresponding vectors.
    Let''s take a look at these vectors, which will be stored in `U`:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到前两个主值是10^5的数量级，而其余分量是10^(-3)的数量级。这告诉我们，实际上只有一个二维子空间与这些数据相关，这并不令人惊讶。这些是第一个和第二个值，将对应于第一个和第二个主成分，即相应的向量。让我们看看这些向量，它们将存储在`U`中：
- en: '![](img/9d685877-5a4c-4449-8da4-68b1b21d1e66.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9d685877-5a4c-4449-8da4-68b1b21d1e66.png)'
- en: You will notice that these two vectors are very heavily weighted in the first
    two entries, which are of the order 10^(-1); the remaining entries are all of
    the order 10^(-6) or lower, and are comparably irrelevant. This is what we should
    have expected, considering how biased we made our data in the first two entries.
    That, in a nutshell, is the idea behind PCA.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到这两个向量在前两个条目中权重非常高，它们的数量级是10^(-1)；其余条目都是10^(-6)或更低的数量级，相对不相关。考虑到我们在前两个条目中如何偏置数据，这正是我们所预期的。简而言之，这就是PCA背后的想法。
- en: Summary
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We started this chapter by looking at how to use the wrappers for the cuBLAS
    library from Scikit-CUDA; we have to keep many details in mind here, such as when
    to use column-major storage, or if an input array will be overwritten in-place.
    We then look at how to perform one- and two-dimensional FFTs with cuFFT from Scikit-CUDA,
    and how to create a simple convolutional filter. We then showed you how to apply
    this for a simple Gaussian blurring effect on an image. Finally, we looked at
    how to perform a singular value decomposition (SVD) on the GPU with cuSolver,
    which is normally a very computationally onerous operation, but which parallelizes
    fairly well onto the GPU. We ended this chapter by looking at how to use the SVD
    for basic PCA.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本章从如何使用Scikit-CUDA库的包装器开始，这里我们需要记住许多细节，例如何时使用列主存储，或者一个输入数组是否会被就地覆盖。然后我们探讨了如何使用Scikit-CUDA的cuFFT执行一维和二维FFT，以及如何创建一个简单的卷积滤波器。接着我们展示了如何将此应用于图像的简单高斯模糊效果。最后，我们探讨了如何使用cuSolver在GPU上执行奇异值分解（SVD），这通常是一个非常计算密集的操作，但它在GPU上并行化得相当好。我们本章的结尾是探讨如何使用SVD进行基本的PCA。
- en: Questions
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Suppose you get a job translating some old legacy FORTRAN BLAS code to CUDA.
    You open a file and see a function called SBLAH, and another called ZBLEH. Can
    you tell what datatypes these two functions use without looking them up?
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你得到一份工作，将一些旧的遗留FORTRAN BLAS代码翻译成CUDA。你打开一个文件，看到一个名为SBLAH的函数，另一个名为ZBLEH。你能不查阅它们来告诉这两个函数使用的数据类型吗？
- en: Can you alter the cuBLAS level-2 GEMV example to work by directly copying the
    matrix `A` to the GPU, without taking the transpose on the host to set it column-wise?
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能否修改cuBLAS级别-2 GEMV示例，使其通过直接将矩阵`A`复制到GPU上工作，而不需要在主机上转置以设置列向？
- en: Use cuBLAS 32-bit real dot-product (`cublasSdot`) to implement matrix-vector
    multiplication using one row-wise matrix and one stride-1 vector.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用cuBLAS 32位实数点积(`cublasSdot`)，通过一个行矩阵和一个步长为1的向量实现矩阵-向量乘法。
- en: Implement matrix-matrix multiplication using `cublasSdot`.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`cublasSdot`实现矩阵-矩阵乘法。
- en: Can you implement a method to precisely measure the GEMM operations in the performance
    measurement example?
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能实现一种方法来精确测量性能测量示例中的GEMM操作吗？
- en: In the example of the 1D FFT, try typecasting `x` as a `complex64` array, and
    then switching the FFT and inverse FFT plans to be `complex64` valued in both
    directions. Then confirm whether `np.allclose(x, x_gpu.get())` is true without
    checking the first half of the array. Why do you think this works now?
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在1D FFT的示例中，尝试将`x`类型转换为`complex64`数组，然后切换FFT和逆FFT计划，使其在两个方向上都是`complex64`值。然后确认`np.allclose(x,
    x_gpu.get())`是否为真，而不检查数组的前半部分。你认为为什么现在这种方法会起作用？
- en: Notice that there is a dark edge around the blurred image in the convolution
    example. Why is this in the blurred image but not in the original? Can you think
    of a method that you can use to mitigate this?
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意到在卷积示例中，模糊图像周围有一个暗边。为什么这个暗边出现在模糊图像中，而不是原始图像中？你能想到一种可以用来减轻这种情况的方法吗？
