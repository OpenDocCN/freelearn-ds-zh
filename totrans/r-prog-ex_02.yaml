- en: Understanding Votes with Descriptive Statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter shows how to perform a descriptive statistics analysis to get a
    general idea about the data we're dealing with, which is usually the first step
    in data analysis projects and is a basic ability for data analysts in general.
    We will learn how to clean and transform data, summarize data in a useful way,
    find specific observations, create various kinds of plots that provide intuition
    for the data, use correlations to understand relations among numerical variables,
    use principal components to find optimal variable combinations, and put everything
    together into code that is reusable, understandable, and easily modifiable.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is a book about programming with R and not about doing statistics
    with R, our focus will be on the programming side of things, not the statistical
    side. Keep that in mind while reading it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the important topics covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning, transforming, and operating on data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating various kinds of graphs programmatically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing qualitative analysis with various tools in R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building new variables with Principal Components Analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing modular and flexible code that is easy to work with
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter's required packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During this chapter we will make use of the following R packages. If you don't
    already have them installed, you can look into [Appendix](part0296.html#8Q96G0-f494c932c729429fb734ce52cafce730)*,
    Required Packages *for instructions on how do so.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Package** | **Used for** |'
  prefs: []
  type: TYPE_TB
- en: '| `ggplot2` | High-quality graphs |'
  prefs: []
  type: TYPE_TB
- en: '| `viridis` | Color palette for graphs |'
  prefs: []
  type: TYPE_TB
- en: '| `corrplot` | Correlation plots |'
  prefs: []
  type: TYPE_TB
- en: '| `ggbiplot` | Principal components plots |'
  prefs: []
  type: TYPE_TB
- en: '| `progress` | Show progress for iterations |'
  prefs: []
  type: TYPE_TB
- en: The Brexit votes example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In June 2016, a referendum was held in the **United Kingdom** (**UK**) to decide
    whether or not to remain part of the **European Union** (**EU**). 72% of registered
    voters took part, and of those, 51.2% voted to leave the EU. In February 2017,
    Martin Rosenbaum, freedom of information specialist at BBC News, published the
    article, *Local Voting Figures Shed New Light on EU Referendum* ([http://www.bbc.co.uk/news/uk-politics-38762034](http://www.bbc.com/news/uk-politics-38762034)).
    He obtained data from 1,070 electoral wards (the smallest administrative division
    for electoral purposes in the UK), with numbers for **Leave** and **Remain** votes
    in each ward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Martin Rosenbaum calculated some statistical associations between the proportion
    of **Leave** votes in a ward and some of its social, economic, and demographic
    characteristics by making use of the most recent UK census, which was conducted
    in 2011\. He used his data for a university class, and that''s the data we will
    use in this example, with some variables removed. The data is provided in a CSV
    file (`data_brexit_referendum.csv`) which can be found in the accompanying code
    repository for this book ([https://github.com/PacktPublishing/R-Programming-By-Example](https://github.com/PacktPublishing/R-Programming-By-Example)).
    The table shows the variables included in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00009.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Data variable descriptions
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning and setting up the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Setting up the data for this example is straightforward. We will load the data,
    correctly label missing values, and create some new variables for our analysis.
    Before we start, make sure the `data.csv` file is in the same directory as the
    code you're working with, and that your *working directory* is properly setup.
    If you don't know how to do so, setting up your working directory is quite easy,
    you simply call the `setwd()` function passing the directory you want to use as
    such. For example, `setwd(/home/user/examples/)` would use the `/home/user/examples` directory
    to look for files, and save files to.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t know how to do so, setting up your working directory is quite easy,
    you simply call the `setwd()` function passing the directory you want to use as
    such. For example, `setwd(/home/user/examples/)` would use the /home/user/examples
    directory to look for files, and save files to.
  prefs: []
  type: TYPE_NORMAL
- en: We can load the contents of the `data.csv` file into a data frame (the most
    intuitive structure to use with data in CSV format) by using the `read.csv()`
    function. Note that the data has some missing values in the `Leave` variable.
    These values have a value of `-1` to identify them. However, the proper way to
    identify missing values in R is with `NA`, which is what we use to replace the
    `-1` values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To count the number of missing values in our data, we can use the `is.na()`
    function to get a logical (Boolean) vector that contains `TRUE` values to identify
    missing values and `FALSE` values to identify non-missing values. The length of
    such a vector will be equal to the length of the vector used as input, which is
    the `Leave` variable in our case. Then, we can use this logical vector as input
    for `sum()` while leverage the way R treats such `TRUE/FALSE` values to get the
    number of missing values. `TRUE` is treated as `1`, while `FALSE` is treated as
    `0`. We find that the number of missing values in the `Leave` variable is 267.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If we want to, we can use a mechanism to fill the missing values. A common and
    straightforward mechanism is to impute the variable's mean. In our case, in [Chapter
    3](part0076.html#28FAO0-f494c932c729429fb734ce52cafce730), *Predicting Votes with
    Linear Models*, we will use linear regression to estimate these missing values.
    However, we will keep things simple for now and just leave them as missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now proceed to defining a new variable, `Proportion`, which will contain
    the percentage of votes in favor of leaving the EU. To do so we divide the `Leave`
    variable (number of votes in favor of leaving) by the `NVotes` variable (number
    of votes in total), for each ward. Given the vectorized nature of R, this is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We are creating a new variable in the data frame by simply assigning to it.
    There's no difference between creating a new variable and modifying an existing
    one, which means that we need to be careful when doing so to make sure we're not
    overwriting an old variable by accident.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, create a new variable that contains a classification of whether most of
    the wards voted in favor of leaving or remaining in the EU. If more than 50 percent
    of each ward''s votes were in favor of leaving, then we will mark the ward as
    having voted for leaving, and vice versa for remaining. Again, R makes this very
    simple with the use of the `ifelse()` function. If the mentioned condition (first
    parameter) holds true, then the value assigned will be `"Leave"` (second parameter);
    otherwise it will be `"Remain"` (third parameter). This is a vectorized operation,
    so it will be done for each observation in the data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Sometimes, people like to use a different syntax for these types of operations;
    they will use a *subset-assign approach,* which is slightly different from what
    we used. We won''t go into the details of the differences among these approaches,
    but keep in mind that the latter approach may give you an error in our case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This happens because the `Proportion` variable contains some missing values
    that were consequences of the `Leave` variable having some `NA` values in the
    first place. Since we can't compute a `Proportion` value for observations with
    `NA` values in `Leave`, when we create it, the corresponding values also get an
    `NA` value assigned.
  prefs: []
  type: TYPE_NORMAL
- en: If we insist on using the *subset-assign approach,* we can make it work by using
    the `which()` function. It will ignore (returning as `FALSE`) those values that
    contain `NA` in the comparison. This way it won't give us an error, and we will
    get the same result as using the `ifelse()` function. We should use the `ifelse()`
    function when possible because it's simpler, easier to read, and more efficient
    (more about this in [Chapter 9](part0229.html#6QCGQ0-f494c932c729429fb734ce52cafce730), *Implementing
    an Efficient Simple Moving Average*).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Down the road we will want to create plots that include the `RegionName` information
    and having long names will most likely make them hard to read. To fix that we
    can shorten those names while we are in the process of cleaning the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that the first line in the previous code block is assigning a transformation
    of the `RegionName` into character type. Before we do this, the type of the variable
    is factor (which comes from the default way of reading data with `read.csv()`),
    and it prevents us from assigning a different value from the ones already contained
    in the variable. In such a case, we will get an error, `Invalid factor level,
    NA generated`. To avoid this problem, we need to perform the type transformation.
  prefs: []
  type: TYPE_NORMAL
- en: We now have clean data ready for analysis. We have created a new variable of
    interest for us (`Proportion`), which will be the focus of the rest of this chapter
    and the next one, since in this example, we're interested in finding out the relations
    among other variables and how people voted in the referendum.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing the data into a data frame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get a summary of the data, we may execute `summary(data)` and see the relevant
    summaries for each type of variable. The summary is tailored for each column's
    data type. As you can see, numerical variables such as `ID` and `NVotes` get a
    quantile summary, while factor (categorical) variables get a count for each different
    category, such as `AreaType` and `RegionName`. If there are many categories, the
    summary will show the categories that appear the most and group the rest into
    a (`Other`) group, as we can see at the bottom of `RegionName`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'From here, we can see that London is the region to which more wards belong,
    followed by the North West and West Midlands. We can also see that the ward with
    the least votes in all of the data had only 1,039 votes, the one with the most
    votes had 15,148, and the mean number of votes per ward was 5,703\. We will take
    a deeper look into these kinds of analyses later in the chapter. For now we''ll
    focus on making this summary data useful for further analysis. As you may have
    noticed, we can''t use the `summary()` results to make computations. We can try
    to save the summary into a variable, find out the variable type, and traverse
    it in an appropriate way. However, if we do that we will find that it''s text
    data, which means that we can''t use it for computations as it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Surely, there must be a way to get the `summary` data into a data frame for
    further analysis. This is R, so you can bet there is! The first thing we should
    note is that we can't directly translate the output of the `summary()` function
    into a data frame because of the non-numerical variables. These non-numerical
    variables contain a different summary structure which is not composed of the minimum,
    first quartile, median, mean, third quartile, and maximum values. This means that
    we first need to subset the data to get only the numerical variables. After all,
    a data frame is a rectangular structure with well defined rows and columns. If
    we tried to mix types (by including numerical and non-numerical summaries) into
    the data frame, we would have a hard time doing so.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find if a column is numeric or not, we can use the `is.numeric()` function.
    For example, we can see that the `Proportion` column is numeric and the `RegionName`
    is not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then apply `is.numeric()` to each column by using the `sapply()` function.
    This will give us a logical (Boolean) vector with a `TRUE` or `FALSE` value for
    each column, indicating whether or not it''s numeric. Then we can use this logical
    vector to subset our data and get only the numerical columns with `data[, numerical_variables]`.
    As you can see, there are no non-numerical columns in the `data_numerical` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Since it doesn''t make much sense to get the `summary` values for the `ID`
    variable, we can remove it from the logical vector, effectively treating it as
    a non-numerical variable. If we do, we must remember to recreate the `data_numeric`
    object to make sure it doesn''t include the `ID` variable also:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To create our numerical variables summary, we first will apply the `summary()`
    function we used before to each numerical column using the `lapply()` function.
    The `lapply()` function returns a named list, where each list member has the corresponding
    column name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now we need to put each member of this list together into a data frame. To do
    so, we will use the `cbind()` and `do.call()` functions. `do.call()` will consecutively
    apply `cbind()` to each member of the list generated by `lapply()` and return
    them all together. To get a reminder on how these vectorized operations work,
    take a look at [Chapter 1](part0022.html#KVCC0-f494c932c729429fb734ce52cafce730),
    *Introduction to R:*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We got our results, but not so fast! We got a warning, and it looks suspicious.
    What does this `number of rows of result is not a multiple of vector length` message
    mean? Aha! If we take a more detailed look at the list we previously got from
    our `lapply()` function, we can see that in the case of `Leave` (and `Proportion`)
    we get an extra column for `NAs` that we don't get for any other column. That
    means that when we try to use `cbind()` on these columns, the extra `NAs` column
    will create an extra space that needs to be filled. This is a problem we looked
    at in [Chapter 1](part0022.html#KVCC0-f494c932c729429fb734ce52cafce730), *Intro**duction
    to R*.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw, then, R deals with it by repeating the vectors in order until all
    spaces are filled. In our case this means that the first element, the one corresponding
    to the minimum value, will be repeated for the `NAs` space for each column that
    doesn't have an `NAs` space. You can verify this by comparing the numbers of the
    `Min` and `NAs` columns for variables other than `Leave` or `Proportion` (for
    these two, the values should actually be different).
  prefs: []
  type: TYPE_NORMAL
- en: 'To fix it we can just remove the extra `NA` value''s row from the resulting
    data frame, but this would not deal with the warning''s source, only the symptom.
    To deal with the source, we need to have the  same number of columns for each
    variable before we apply `cbind()`. Since we already know that we have 267 missing
    values for the `Leave` variable, which then affects the `Proportion` variable,
    we can easily fix this by just ignoring that information. To do so, we simply
    use the *complete cases*, meaning that we keep observations that don''t have any
    `NA` values in any of their variables; or, put another way, we drop every observation
    that contains at least one `NA`. Once we do that, we get our results back and
    we don''t get any warnings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to get the summary values as columns and the variables as rows,
    we can use the `rbind()` function instead of `cbind()`. The structure we actually
    end up using will depend on what we want to do with it. However, we can easily
    change between them later if we need to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have this `numerical_summary` object, we can use it to perform
    computations, such as finding the range between the wards with the least and most
    proportions of votes in favor of leaving (0.6681), which may be useful to interpret
    the big difference among the *types* of wards we may find in the UK. If we want
    to know which wards are being used to get to this result, we can search for the
    wards with the least and most proportion of votes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this analysis already shows some interesting results. The UK
    ward that voted to leave the EU the most is characterized by older people (`MeanAge`)
    with low education levels (`NoQuals`, `L4Quals_plus`). On the other hand, the
    UK ward that voted to remain in the EU the most is characterized by younger people
    with much higher education levels. Of course, this is not the full picture, but
    it's a hint about the direction in which we need to look to further understand
    what's going on. For now, we have found that education and age seem to be relevant
    variables for the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Getting intuition with graphs and correlations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have some clean data to work with, we will create lots of plots
    to build intuition about the data. In this chapter, we will work with plots that
    are easy to create and are used for exploratory purposes. In [Chapter 4](part0091.html#2MP360-f494c932c729429fb734ce52cafce730),
    *Simulating Sales Data and Working with Databases,* we will look into publication
    ready plots that are a little more verbose to create.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing variable distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our first plot is a simple one and shows the proportion of votes by each `RegionName`.
    As you can see in the plot shown below, the London, North West, and West Midlands
    regions account for around 55 percent of the observations in the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00010.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Vote Proportion by Region
  prefs: []
  type: TYPE_NORMAL
- en: To create the plot, we need to create a table for the frequencies of each region
    in `RegionName` with the `table()` function, then we feed that to the `prop.table()`
    function, which computes the corresponding proportions, which in turn are used
    as heights for each bar.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `barplot()` function to produce the plot, and we can specify some
    options, such as the title (`main`), the *y* axis label (`ylab`), and the color
    for the bars (`col`). As always, you can find out more about in the function''s
    parameters with `? barplot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Our next plot, shown below, is a little more eye-catching. Each point represents
    a ward observation, and it shows the `Proportion` of `Leave` votes for each ward,
    arranged in vertical lines corresponding to `RegionName` and colored by the proportion
    of white population for each ward. As you can see, we have another interesting
    finding; it seems that the more diversified a ward's population is (seen in the
    darker points), the more likely it is for the ward to vote in favor of remaining
    in the EU (a lower `Proportion` value).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00011.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Proportion by RegionName and White Population Percentage
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the plot, we need to load the `ggplot2` and `viridis` packages; the
    first one will be used to create the actual plot, while the second one will be
    used to color the points with a scientifically interesting color palette called
    **Viridis** (it comes from color perception research done by Nathaniel Smith and
    Stéfan van der Walt, [http://bids.github.io/colormap/](http://bids.github.io/colormap/)).
    The details of the `ggplot2` syntax will be explained in [Chapter 4](part0091.html#2MP360-f494c932c729429fb734ce52cafce730),
    *Simulating Sales Data and Working with Databases*, but for now, all you need
    to know is that the function receives as a first parameter the data frame with
    the data that will be used for the plot, and as a second parameter an aesthetics
    object (`aes`), created with the `aes()` function, which in turn can receive parameters
    for the variable that should be used in the *x* axis, *y* axis, and color. After
    that, we add a *points layer* with the `geom_points()` function, and the Viridis
    color palette with the `scale_color_viridis()` function. Notice how we are adding
    plot objects while we work with `ggplot2`. This is a very convenient feature that
    provides a lot of power and flexibility. Finally, we show the plot with the `print()`
    function (in R, some functions used for plotting immediately show the plot (for
    example, `barplot`), while others return a plot object (for example, `ggplot2`)
    and need to be printed explicitly):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The next set of plots, shown below, display histograms for the `NoQuals`, `L4Quals_plus`,
    and `AdultMeanAge` variables. As you can see, the `NoQuals` variable appears to
    be normally distributed, but the `L4Quals_plus` and `AdultMeanAge` variables seemed
    to be skewed towards the left and right, correspondingly. These tell us that most
    people in the sample don't have high education levels and are past 45 years of
    age.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00012.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Histogram for NoQuals, L4Quals_plus, and AdultMeanAge
  prefs: []
  type: TYPE_NORMAL
- en: Creating these plots is simple enough; you just need to pass the variable that
    will be used for the histogram into the `hist()` function, and optionally specify
    a title and *x* axis label for the plots (which we leave empty, as the information
    is already in the plot's title).
  prefs: []
  type: TYPE_NORMAL
- en: For the book, we arranged plots in such a way that their spacing and understanding
    is efficient, but when you create the plots using the code shown, you'll see them
    one by one. There are ways to group various plots together, but we'll look at
    them in [Chapter 4](part0091.html#2MP360-f494c932c729429fb734ce52cafce730), *Simulating
    Sales Data and Working with Databases*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Now that we understand a bit more about the distribution of the `NoQuals`, `L4Quals_plus`,
    and `AdultMeanAge` variables, we will see their joint-distribution in the scatter
    plots shown below. We can see how these scatter plots resemble the histograms
    by comparing the *x* axis and *y* axis in the scatter plots to the corresponding *x* axis
    in the histograms, and comparing the frequency (height) in the histograms with
    the point density in the scatter plots.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00013.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Scatter plots for NoQuals, L4Quals_plus vs AdultMeanAge
  prefs: []
  type: TYPE_NORMAL
- en: We find a slight relation that shows that the older the people, the lower the
    levels of education they have. This can be interpreted in a number of ways, but
    we'll leave that as an exercise, to keep focus on the programming, not the statistics.
    Creating these scatter plots is also very simple. Just send the `x` and `y` variables
    to the `plot()` function, and optionally specify labels for the axes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Using matrix scatter plots for a quick overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What happens if we want to visualize a lot of scatter plots in a single graph
    to quickly get a sense for the data? In that case, we need *matrix scatter plots*.
    We have various package options to create such matrix scatter plots (such as the
    `car` package). However, to keep things simple, we will use a built-in function
    instead of an external package.
  prefs: []
  type: TYPE_NORMAL
- en: By looking at the graph shown below, we can get a big-picture view of the interactions
    among variables. The purpose of this type of visualization is not to provide details,
    but to provide a general overview. To read this plot we need to look at any interesting
    scatter plot in the matrix, and move both horizontally and vertically until we
    find the name associated with its axis.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you look at the plot immediately to the right of `NoQuals` and
    simultaneously immediately on top of `L4Quals_plus`, what you're looking at is
    at the relation between those two variables (`NoQuals` in the *y* axis, `L4Quals_plus`
    in the *x* axis), and we find that it's an inverse relation; the higher the percentage
    of people in a ward with high levels of education, the lower the percentage of
    people with low levels of education. Another obvious relation is that the higher
    the education level (`L4Quals_plus`), the higher the occupation (`HigherOccup`).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00014.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Matrix scatter plot
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to space restrictions, we were not able to show all variable relations,
    since the scatter plots would be too small to make sense of. However, we encourage
    the reader to add more variables to the matrix. There are some non-obvious relations.
    Finding them is left as an exercise for the reader:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Getting a better look with detailed scatter plots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we know how to get a big-picture view of the scatter plots to get
    a general sense of the relations among variables, how can we get a more detailed
    look into each scatter plot? Well, I''m glad you asked! To achieve this, we''ll
    do it in two steps. First, we are going to work on producing a single, detailed
    scatter plot that we''re happy with. Second, we''re going to develop a simple
    algorithm that will traverse all variable combinations and create the corresponding
    plot for each of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00015.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Scatter plot for NoQuals vs AdultMeanAge vs Proportion with Regression Line
  prefs: []
  type: TYPE_NORMAL
- en: The graph shown above shows our prototype scatter plot. It has a combination
    of variables in the `x` and `y` axes, `NoQuals` and `AdultMeanAge` in our case,
    assigns a color according to the corresponding `Proportion`, and places a line
    corresponding to a linear regression on top to get a general sense of the relation
    among the variables in the axes. Compare this plot to the left scatter plot of
    previous pair of scatter plots. They are the same plot, but this one is more detailed
    and conveys more information. This plot seems good enough for now.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to develop the algorithm that will take all variable combinations
    and create the corresponding plots. We present the full algorithm and explain
    part by part. As you can see, we start defining the `create_graphs_iteratively`
    function, which receives two parameters: the `data` and the `plot_function`. The
    algorithm will get the variable names for the data and store them in the `vars`
    variables. It will then remove `Proportion` from such variables, because they
    will be used to create the combinations for the axis, and `Proportion` will never
    be used in the axis; it will be used exclusively for the colors.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we imagine all the variable combinations in a matrix like the one for
    the matrix scatter plot shown previously, then we need to traverse the upper triangle
    or the lower triangle to get all possible combinations (in fact, the upper and
    lower triangles from matrix of scatter plots are symmetrical because they convey
    the same information). To traverse these triangles, we can use a *known pattern,*
    which uses two for-loops, each for one axis, and where the inner loop need only
    start at the position of the outer loop (this is what forms a triangle). The `-1`
    and `+1` are there to make sure we start and finish in appropriate places in each
    loop without getting an error for array boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: Inside the inner loop is where we will create the name for the plot as a combination
    of the variable names and concatenate them using the `paste()` function, as well
    as create the plot with the `plot_function` we will send as a parameter (more
    on this ahead). The `png()` and `dev.off()` functions are there to save the plots
    to the computer's hard drive. Think of the `png()` function as the place where
    R starts looking for a graph, and `dev.off()` as the place where it stops the
    saving process. Feel free to look into their documentation or read more about *devices*
    in R.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re almost done; we just need to wrap the code we used to turn our plot
    prototype into a function and we will be all set. As you can see, we extracted
    the `x`, `y`, and `color` parameters for the `aes()` function as variables that
    are sent as parameters to the function (this is called **parametrizing arguments**),
    and we switched the `aes()` function with the `aes_string()` function, which is
    able to receive variables with strings for the parameters. We also added the option
    to send the `var_color` as `FALSE` to avoid using a color-version of the plot.
    Everything else is kept the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Since we will be checking in various places whether the `save_to` string is
    empty, we name the check and wrap it in the `not_empty()` function. Now it's a
    bit easier to read our code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: With this `prototype_scatter_plot()` function, we can re-create the right scatter
    plots shown previously, as well as any other variable combination, quite easily.
    This seems pretty powerful, doesn't it?
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00016.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Scatter plot for L4Quals_plus vs AdultMeanAge vs Proportion with Regression
    Line
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have done the hard work, we can create all possible combinations
    quite easily. We just need to call the `create_plots_iteratively()` function with
    our data and the `prototype_scatter_plot()` function. Using functions as parameters
    for other functions is known as the **strategy pattern**. The name comes from
    the fact that we can easily change our strategy for plotting for any other one
    we want that receives the same parameters (`data`, `var_x`, and `var_y`) to create
    plots, without having to change our algorithm to traverse the variable combinations.
    This kind of flexibility is very powerful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This will create all the plots for us and save them to our hard drive. Pretty
    cool, huh? Now we can look at each of them independently and do whatever we need
    with them, as we already have them as PNG files.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding interactions with correlations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The correlation is a measure of the linear relation among two variables. Its
    value ranges from `-1`, representing a perfect inverse relation, to `1`, representing
    a perfect direct relation. Just as we created a matrix of scatter plots, we will
    now create a matrix of correlations, and resulting graph is shown below. Large
    circles mean high absolute correlation. Blue circles mean positive correlation,
    while red circles mean negative correlation.
  prefs: []
  type: TYPE_NORMAL
- en: To create this plot we will use the `corrplot()` function from the `corrplot`
    package, and pass it the correlations data computed by the `cor()` function in
    R, and optionally some parameters for the text labels (`tl`), such as color (`color`)
    and size (`cex`).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00017.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Variable Correlations
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'If we look at the relation among the `Proportion` variable and the other variables,
    variables in large blue circles are positively correlated with it, meaning that
    the more that variable increases, the more likely it is for the `Proportion` variable
    to also increase. For examples of this type, look at the relations among `AdultMeanAge`
    and `NoQuals` with `Proportion`. If we find large red circles among `Proportion`
    and other variables, it means that the more that variable increases, the more
    `Proportion` is likely to decrease. For examples of this type, look at the relations
    among `Age_25to29`, `Age_30to44`, and `L4Quals_plus` with `Proportion`:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new dataset with what we've learned
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What we have learned so far in this chapter is that age, education, and ethnicity
    are important factors in understanding the way people voted in the Brexit Referendum.
    Younger people with higher education levels are related with votes in favor of
    remaining in the EU. Older white people are related with votes in favor of leaving
    the EU. We can now use this knowledge to make a more succinct data set that incorporates
    this knowledge. First we add relevant variables, and then we remove non-relevant
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our new relevant variables are two groups of age (adults below and above 45),
    two groups of ethnicity (whites and non-whites), and two groups of education (high
    and low education levels):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we remove the old variables that were used to create our newly added variables.
    To do so without having to manually specify a full list by leveraging the fact
    that all of them contain the word `"Age"`, we create the `age_variables` logical
    vector, which contains a `TRUE` value for those variables that contain the word
    `"Age"` inside (`FALSE` otherwise), and make sure we keep our newly created `Age_18to44`
    and `Age_45plus` variables. We remove the other ethnicity and education levels
    manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We save our created `data_adjusted` object by selecting the new columns, create
    our new numerical variables for the new data structure, and save it as a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Building new variables with principal components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Principal Component Analysis** (**PCA**) is a dimensionality reduction technique
    that is widely used in data analysis when there are many numerical variables,
    some of which may be correlated, and we would like to reduce the number of dimensions
    required to understand the data.'
  prefs: []
  type: TYPE_NORMAL
- en: It can be useful to help us understand the data, since thinking in more than
    three dimensions can be problematic, and to accelerate algorithms that are computationally
    intensive, especially with large numbers of variables. With PCA, we can extract
    most of the information into only one or two variables constructed in a very specific
    way, such that they capture the most variance while having the added benefit of
    being uncorrelated among them by construction.
  prefs: []
  type: TYPE_NORMAL
- en: The first principal component is a linear combination of the original variables
    which captures the maximum variance (information) in the dataset. No other component
    can have higher variability than the first principal component. Then, second principal
    component is orthogonal to the first one and is computed in such a way that it
    captures the maximum variance left in the data. And so on. The fact that all variables
    are linear combinations that are orthogonal among themselves is the key for them
    being uncorrelated among each other. Enough statistics talk; let's get on with
    the programming!
  prefs: []
  type: TYPE_NORMAL
- en: When performing PCA in R, we have a variety of functions which can do the task.
    To mention some of them, we have `prcomp()` and `princomp()` from the `stats`
    package (built-in), `PCA()` from the `FactoMineR` package, `dudi.pca()` from the
    `ade4` package, and `acp()` from the `amap` package. In our case, we'll use the
    `prcomp()` function that is built into R.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform our PCA, we will use the adjusted data from the previous section.
    First, we remove numerical variables which are correlated with `Proportion`. Then
    we perform the PCA by sending the numerical data to the `prcomp()` function, as
    well as some normalization parameters. `center = TRUE` will subtract each variable''s
    mean from itself, and `scale. = TRUE` will make each variable''s variance unitary,
    effectively normalizing the data. Normalizing the data is very important when
    performing PCA, as it''s a method sensitive to scales:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: When we print the `pca` object, we can see the standard deviations for each
    variable, but more importantly, we can see the weights used for each variable
    to create each principal component. As we can see, when we look at the full output
    in our computer, among the most important weights (the largest absolute values)
    we have the age and ethnicity variables, as well as others, such as home ownership.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to get the axis value for each observation in the new coordinate
    system composed of the principal components, you simply need to multiply each
    observation in your data (each row) with the corresponding weights from the rotation
    matrix from the `pca` object (`pca$rotation`). For example, to know where the
    first observation in the data should be placed in regards to the second principal
    component, you can use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In general, you can apply matrix operations to get coordinates for all the observations
    in your data in regards to all the principal components in your `pca` object by
    using the following line, which will perform a matrix multiplication. Note that
    you don't need to do this yourself since R will do it automatically for you when
    analyzing the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: When we look at the summary of `pca`, we can see the standard deviations for
    each principal component, as well as its proportion of the variance captured and
    its accumulation. This information is useful when deciding how many principal
    components we should keep for the rest of the analysis. In our case, we find that
    with just the first two principal components, we have captured approximately 70
    percent of the information in the data, which for our case may be good enough.
  prefs: []
  type: TYPE_NORMAL
- en: The 70% number can be arrived at by adding the `Proportion of variance` value
    for the principal components we want to consider (in order and starting at `PC1`).
    In this case, if we add the `Proportion of variance` for `PC1` and `PC2`, we get
    $0.411 + 0.280 = 0.691$, which is almost 70 percent. Note that you can simply
    look at the `Cumulative proportion` to find this number without having to perform
    the sum yourself, as it accumulates the `Proportion of variance` incrementally,
    starting at `PC1`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00018.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Principal Component's Variances
  prefs: []
  type: TYPE_NORMAL
- en: 'Take one moment to think about how powerful this technique is: with just two
    variables, we are able to capture 70 percent of the information contained in the
    original 40 variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In the graph shown above, we can see the variances (in the form of squared
    standard deviations) from the `summary(pca)` results. We can see how each subsequent
    principal component captures a lower amount of the total variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Finally, following graph shows a scatter plot of the ward observations (points)
    over a plane created by the two principal components from our analysis; it is
    called a **biplot**. Since these two principal components are formed as linear
    combinations of the original variables, we need some guidance when interpreting
    them. To make it easy, the arrows point towards the direction of that variable's
    association to the principal component axis. The further the arrow is from the
    center, the stronger the effect on the principal components.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00019.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: PCA Biplot
  prefs: []
  type: TYPE_NORMAL
- en: 'With this biplot, we can see that `Proportion` is strongly related to the wards
    that voted to leave the EU, which is obvious since that''s by construction. However,
    we can also see some other interesting relations. For example, other than the
    effects we have found so far (age, education, and ethnicity), people owning their
    own homes is also slightly associated with a higher tendency towards voting to
    leave the EU. On the other side, a previously unknown relation is the fact that
    the more dense a ward''s population is (think about highly populated cities),
    the more likely it is that they will vote to remain in the EU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Putting it all together into high-quality code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have the fundamentals about analyzing data with descriptive statistics,
    we're going to improve our code's structure and flexibility by breaking it up
    into functions. Even though this is common knowledge among efficient programmers,
    it's not a common practice among data analysts. Many data analysts would simply
    paste the code we have developed all together, as-is, into a single file, and
    run it every time they wanted to perform the analysis. We won't be adding new
    features to the analysis. All we'll do is reorder code into functions to encapsulate
    their inner-workings and communicate intention with function names (this substantially
    reduces the need for comments).
  prefs: []
  type: TYPE_NORMAL
- en: We'll focus on producing *high-quality* code that is easy to read, reuse, modify,
    and fix (in case of bugs). The way we actually do it is a matter of style, and
    different ways of arranging code are fit for different contexts. The method we'll
    work with here is one that has served me well for a variety of situations, but
    it may not be the best for yours. If it doesn't suit your needs, feel free to
    change it. Whichever style you prefer, making an investment in creating a habit
    of constantly producing high-quality code will make you a more efficient programmer
    in the long run, and a point will come where you will not want to program inefficiently
    any more.
  prefs: []
  type: TYPE_NORMAL
- en: Planning before programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Often, people start programming before having a general idea of what they want
    to accomplish. If you''re an experienced programmer, this may be a good way to
    get a feel for the problem, since you have already developed intuition, and you''ll
    probably end up throwing away the first couple of attempts anyway. However, if
    you''re a novice programmer, I recommend you make your objectives clear and explicit
    before writing any code (putting them into writing can help). It will help you
    make better decisions by asking yourself how a certain way of doing things will
    affect your objectives. So, before we set up anything, we need to understand and
    make our general objectives explicit:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand the big picture of the analysis quickly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reproduce our analysis automatically by executing a single file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save all the resulting objects, text, and images for the analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure the amount of time it takes to perform the full analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When working on iterative processes, know the completed percentage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Be able to find and change each part of the analysis easily.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To fulfill these general objectives, we need to develop modular code with well-managed
    dependencies that are flexible (easy to change) and friendly to side-effects (saving
    objects, texts, and images). Even if your explicit objectives don't require it,
    you should make a habit of programming this way, even when just doing data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the fundamentals of high-quality code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Code that is modular, flexible, and whose dependencies are well-managed, is
    said to be **highly-cohesive** and **loosely-coupled**. These terms are mostly
    used in object-oriented environments (more about these in [Chapter 8](part0178.html#59O440-f494c932c729429fb734ce52cafce730),
    *Object-Oriented System to Track Cryptocurrencies*), but apply generally to any
    system. **Highly-cohesive** means that things that are supposed to be together,
    are. **Loosely-coupled** means that things that are not supposed to be together,
    are not. The following image shows these characteristics, where each of the circles
    can be a function or an object in general. These are the basics of dependency
    management. Many books focused on these topics have been, and continue to be,
    published. For the interested reader, Steve McConnell's *Code Complete* (Microsoft
    Press, 2004) and Robert Martin's *Clean Code* (Prentice Hall, 2009) are excellent
    references. In this book, you'll see some of these techniques applied.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00020.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: High cohesion and low coupling (left) vs Low cohesion and high coupling (right)
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important principles for high-quality code are:'
  prefs: []
  type: TYPE_NORMAL
- en: Make things small and focused on a single responsibility.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make the concrete depend on the abstract (not vice versa).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make things that are highly-cohesive and loosely-coupled.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By *things,* I mean functions, methods, classes, and objects in general. We'll
    touch more on what these are in [Chapter 8](part0178.html#59O440-f494c932c729429fb734ce52cafce730),
    O*bject-Oriented System to Track Cryptocurrencies*.
  prefs: []
  type: TYPE_NORMAL
- en: We start by creating two files: `functions.R` and `main.R`. The `functions.R` file
    contains high-level functions (mainly called from the `main.R` file) as well as
    low-level functions (used within other functions). By reading the `main.R` file,
    we should have a clear idea of what the analysis does (this is the purpose of
    the high-level functions), and executing it should re-create our analysis for
    any data that fits our base assumptions (for this example, these are mainly data
    structures).
  prefs: []
  type: TYPE_NORMAL
- en: We should always keep related code at the same level of abstraction. This means
    that we don't want to program things at the big-picture level and implement it
    with mixed details, and separating our code into the `main.R` and `functions.R` is
    a first step in this direction. Furthermore, none of the code in the `main.R` file
    should depend on details of the implementation. This makes it modular in the sense
    that if we want to change the way something is implemented, we can do so without
    having to change the high-level code. However, the way we implement things depends
    on what we want the analysis to ultimately do, which means that concrete implementations
    should depend on the abstract implementations that in turn depend on our analysis'
    purpose (stated as code in the `main.R` file).
  prefs: []
  type: TYPE_NORMAL
- en: When we bring knowledge from one set of code to another, we're generating a
    dependency, because the code that knows about other code depends on it to function
    properly. We want to avoid these dependencies as much as possible, and most importantly,
    we want to manage their direction. As stated before, the abstract should not depend
    on the concrete, or put another way, the concrete should depend on the abstract.
    Since the analysis (`main.R`) is on the abstract side, it should not depend on
    the implementation details of the concrete functions. But, how can our analysis
    be performed without knowledge of the functions that implement it? Well, it can't.
    That's why we need an intermediary, the abstract functions. These functions are
    there to provide stable knowledge to `main.R` and guarantee that the analysis
    its looking for will be performed, and they remove the dependency of `main.R` on
    the implementation details by managing that knowledge themselves. This may seem
    a convoluted way of working and a tricky concept to grasp, but when you do, you'll
    find out that it's very simple, and you'll be able to create code that is pluggable,
    which is a big efficiency boost. You may want to take a look at the books referenced
    previously to get a deeper sense of these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00021.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: General code structure
  prefs: []
  type: TYPE_NORMAL
- en: The previous graph shows that our analysis depends on the abstract functions
    (interfaces), as well as the concrete code that implements those interfaces. These
    abstract functions let us invert the dependency between the concrete functions
    and the analysis. We'll go deeper into these concepts in [Chapter 8](part0178.html#59O440-f494c932c729429fb734ce52cafce730),
    *Object-Oriented System to Track Cryptocurrencies*.
  prefs: []
  type: TYPE_NORMAL
- en: Programming by visualizing the big picture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will work with a top-down approach, meaning that we'll start with abstract
    code first and gradually move into the implementation details. Generally I find
    this approach to be more efficient when you have a clear idea of what you want
    to do. In our case, we'll start by working with the `main.R` file.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing to note is that we will use the `proc.time()` function twice,
    once at the beginning and once at the end, and we will use the difference among
    these two values to measure how much time it took for the whole code to execute.
  prefs: []
  type: TYPE_NORMAL
- en: The second thing to note is that the `empty_directories()` function makes sure
    each of the specified directories exist, and deletes any files contained in them.
    We use it to clean up our directories at the beginning of each execution, to make
    sure we have the latest files, and only the files created in the last run. The
    actual code is shown below, and it simply iterates through each of the directories
    passed, removes any files inside recursively with the `unlink()` function, and
    makes sure the directory exists with the `dir.create()` function. It avoids showing
    any warnings due to the directory already existing, which is not a problem in
    our case, by using the `showWarnings = FALSE` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: From [Chapter 1](part0022.html#KVCC0-f494c932c729429fb734ce52cafce730), *Introduction
    to R*, we use of the `print_section()` and `empty_directories()` functions to
    print headers and delete directory contents (to re-create the results every time
    we run the function with empty directories), respectively, and we'll use the mechanism
    shown with `proc.time()` to measure execution time.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the previous two points are out of the way, we proceed to show the
    full contents of the `main.R` file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, with just this file, you get the big picture of the analysis,
    and are able to reproduce your analysis by running a single file, save the results
    to disk (note the `save_to` arguments), and measure the amount of time it takes
    to perform the full analysis. From our general objectives list, objectives one
    through four are fulfilled by this code. Fulfilling objectives five and six will
    be accomplished by working on the `functions.R` file, which contains lots of small
    functions. Having this `main.R` file gives us a map of what needs to be programmed,
    and even though right now it would not work because the functions it uses do not
    yet exist, by the time we finish programming them, this file will not require
    any changes and will produce the desired results.
  prefs: []
  type: TYPE_NORMAL
- en: Due to space restrictions, we won't look at the implementation of all the functions
    in the `main.R` file, just the representative ones: `prepare_data()`, `plot_scatter_plot()`,
    and `all_scatter_plots()`. The other functions use similar techniques to encapsulate
    the corresponding code. You can always go to this book's code repository ([https://github.com/PacktPublishing/R-Programming-By-Example](https://github.com/PacktPublishing/R-Programming-By-Example))
    to see the rest of the implementation details. After reading this book, you should
    be able to figure out exactly what's going on in every file in that repository.
  prefs: []
  type: TYPE_NORMAL
- en: We start with `prepare_data()`. This function is abstract and uses four different
    concrete functions to do its job, `read.csv()`, `clean_data()`, `transform_data()`,
    and, if required, `complete.cases()`. The first function, namely `read.csv()`,
    receives the path to a CSV file to read data from and loads into a data frame
    object named `data` in this case. The fourth function you have seen before in
    [Chapter 1](part0022.html#KVCC0-f494c932c729429fb734ce52cafce730), *Introduction
    to R*, so we won't explain it here. Functions two and three are created by us,
    and we'll explain them. Note that `main.R` doesn't know about how data is prepared,
    it only asks for data to be prepared, and delegates the job to the abstract function
    `prepare_data()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The `clean_data()` function simply encapsulates the re-coding of -1 for `NA` for
    now. If our cleaning procedure suddenly got more complex (for example, new data
    sources requiring more cleaning or realizing we missed something and we need to
    add it to the cleaning procedure), we would add those changes to this function
    and we would not have to modify anything else in the rest of our code. These are
    some of the advantages of encapsulating code into functions that communicate intention
    and isolate what needs to be done into small steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'To transform our data by adding the extra `Proportion` and `Vote` variables,
    and re-label the region names, we use the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: All of these lines of code you have seen before. All we are doing is encapsulating
    them into functions that communicate intention and allow us to find where certain
    procedures are taking place so that we can find them and change them easily if
    we need to do so later on.
  prefs: []
  type: TYPE_NORMAL
- en: Now we look into `plot_scatter_plot()`. This function is between being an abstract
    and a concrete function. We will use it directly in our `main.R` file, but we
    will also use it within other functions in the `functions.R` file. We know that
    most of the time we'll use `Proportion` as the color variable, so we add that
    as a default value, but we allow for the user to remove the color completely by
    checking if the argument was sent as `FALSE`, and since we will use this same
    function to create graphs that resemble all the scatter plots we have created
    up to this point, we will make the regression line optional.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the case of the former graphs, the *x* axis is a continuous variable, but
    in the case of the latter graph, it's a categorical (*factor*) variable. This
    kind of flexibility is very powerful and is available to us due to `ggplot2`'s
    capability to adapt to these changes. Formally, this is called **polymorphism**,
    and it's something we'll explain in [Chapter 8](part0178.html#59O440-f494c932c729429fb734ce52cafce730),
    *Object-Oriented System to Track Cryptocurrencies*.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, instead of assuming the user will always want to save the resulting
    graph to disk, we make the `save_to` argument optional by providing an empty string
    for it. When appropriate, we check to see if this string is empty with `not_empty()`,
    and if it's not empty, we set up the PNG saving mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Now we look into `all_scatter_plots()`. This function is an abstract function
    that hides from the user's knowledge the name of the function that will create
    graphs iteratively, conveniently named `create_graphs_iteratively()`, and the
    graphing function, the `plot_scatter_plot()` function we saw before. In case we
    want to improve the iterative mechanism or the graphing function, we can do so
    without requiring changes from people that use our code, because that knowledge
    is encapsulated here.
  prefs: []
  type: TYPE_NORMAL
- en: Encapsulate what changes frequently or is expected to change.
  prefs: []
  type: TYPE_NORMAL
- en: The `create_graphs_iteratively()` function is the same we have seen before,
    except for the progress bar code. The `progress` package provides the `progress_bar$new()` function
    that creates a progress bar in the terminal while an iterative process is being
    executed so that we see what percentage of the process has been completed and
    know how much time is remaining (see [Appendix](part0296.html#8Q96G0-f494c932c729429fb734ce52cafce730),
    *Required Packages* for more information).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note the change in the `save_to` argument from the functions `plot_scatter_plot()` and
    `all_scatter_plots()`. In the former, it''s a filename; in the latter, a directory
    name. The difference is small, but important. The incautious reader might not
    notice it and it may be a cause for confusion. The `plot_scatter_plot()` function
    produces a single plot, and thus receives a file name. However, the `all_scatter_plots()` will
    produce, by making use of `plot_scatter_plot()`, a lot of graphs, so it must know
    where all of them need to be saved, create the final image names dynamically,
    and send them one-by-one to `plot_scatter_plot()`. Finally, since we want the
    regression to be included in these graphs, we just send the `regression = TRUE` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The other functions that we have not looked at in detail follow similar techniques
    as the ones we showed, and the full implementation is available at this book's
    code repository ([https://github.com/PacktPublishing/R-Programming-By-Example](https://github.com/PacktPublishing/R-Programming-By-Example)).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter showed how to perform a qualitative analysis that is useful as
    a first step when doing data analysis. We showed some descriptive statistics techniques
    and how to implement them programmatically. With these skills, we are able to
    perform simple yet powerful analyses and save the results for later use. Specifically,
    we showed how to do basic data cleaning, how to create graphs programmatically,
    how to create matrix scatter plots and matrix correlations, how to perform Principal
    Component Analysis, and how to combine these tools to understand the data at hand.
    Finally, we touched on the basics of high-quality code and showed how to transform
    your initial data analysis code into programs that are modular, flexible, and
    easy to work with.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 3](part0076.html#28FAO0-f494c932c729429fb734ce52cafce730), *Predicting
    Votes with Linear Models,* we'll show how to extend the current analysis with
    qualitative tools. Specifically, we'll show how to use linear models to understand
    the quantitative effects of variables on the proportion of votes in favor of the
    UK leaving and remaining in the EU, how to make predictions for wards whose vote
    data we don't have, and how to measure the accuracy of those predictions with
    the data we do have. These are essential skills for any data analyst and, just
    as we did in this chapter, we'll see how to implement them programmatically.
  prefs: []
  type: TYPE_NORMAL
