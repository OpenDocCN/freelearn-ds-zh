- en: Production Hadoop Cluster Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hadoop itself started with a strong core and File System designed to handle
    the big data challenges. Later, many applications were developed on top of this,
    creating a big ecosystem of applications that play nicely with each other. As
    the number of applications started increasing, the challenges to create and manage
    the Hadoop environment increased as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Ambari
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Hadoop cluster with Ambari
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Ambari architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Ambari follows a master/slave architecture where the master node instructs
    the slave nodes to perform certain actions and report back the state of every
    action. The master node is responsible for keeping track of the state of the infrastructure.
    In order to do this, the master node uses a database server, which can be configured
    during setup time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to have a better understanding of how Ambari works, let''s take a
    look at the high level architecture of Ambari, in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9883b7a-2f6f-47b2-a18e-6e90f41f2f76.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At the core, we have the following applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Ambari server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ambari agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ambari web UI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Ambari server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Ambari server (`ambari-server`) is a shell script which is the entry point
    for all administrative activities on the master server. This script internally
    uses Python code, `ambari-server.py,` and routes all the requests to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Ambari server has the following entry points which are available when passed
    different parameters to the `ambari-server` program:'
  prefs: []
  type: TYPE_NORMAL
- en: Daemon management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software upgrade
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software setup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LDAP/PAM/Kerberos management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ambari backup and restore
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miscellaneous options
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Daemon management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The daemon management mode is activated when the script is invoked with `start`,
    `stop`, `reset`, `restart` arguments from the command line.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we want to start the Ambari background server, we can run the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Software upgrade
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once Ambari is installed, we can use this mode to upgrade the Ambari server
    itself. This is triggered when we call the `ambari-server` program with the `upgrade`
    flag. In case we want to upgrade the entire stack of Ambari, we can pass the `upgradestack`
    flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Software setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once Ambari is downloaded from the internet (or installed via YUM and APT),
    we need to do a preliminary setup of the software. This mode can be triggered
    when we pass the `setup` flag to the program. This mode will ask us several questions
    that we need to answer. Unless we finish this step, Ambari cannot be used for
    any kind of management of our servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: LDAP/PAM/Kerberos management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**T**he **Lightweight Directory Access Protocol** (**LDAP**) is used for identity
    management in enterprises. In order to use LDAP-based authentication, we need
    to use the following flags: `setup-ldap` (for setting up `ldap` properties with
    `ambari`) and `sync-ldap` (to perform a synchronization of the data from the `ldap`
    server):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Pluggable Authentication Module **(**PAM**) is at the core of the authentication
    and authorization in any UNIX or Linux operating systems. If we want to leverage
    the PAM-based access for Ambari then we need to run it with the `setup-pam` option.
    If we then want to move from LDAP to PAM-based authentication, we need to run
    it with `migrate-ldap-pam`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Kerberos** is another advanced authentication and authorization mechanism
    which is very helpful in networked environments. This simplifies **Authenticity,
    Authorisation and Auditing** (**AAA**) on large-scale servers. If we want to use
    Kerberos for Ambari, we can use the `setup-kerberos` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Ambari backup and restore
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we want to take a snapshot of the current installation of Ambari (excluding
    the database), we can enter this mode. This supports both backup and restore methods
    invoked via the `backup` and `restore` flags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Miscellaneous options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to these options, there are other options that are available with
    the Ambari server program which you can invoke with the `-h` (help) flag.
  prefs: []
  type: TYPE_NORMAL
- en: Ambari Agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ambari Agent is a program which runs on all the nodes that we want to be managed
    with Ambari. This program periodically heartbeats to the master node. Using this
    agent, `ambari-server` executes many of the tasks on the servers.
  prefs: []
  type: TYPE_NORMAL
- en: Ambari web interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is one of the powerful features of the Ambari application. This web application
    is exposed by the Ambari server program that is running on the master host; we
    can access this application on port `8080` and it is protected by authentication.
  prefs: []
  type: TYPE_NORMAL
- en: Once we log in to this web portal, we can control and view all aspects of our
    Hadoop clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ambari supports multiple RDBMS to keep track of the state of the entire Hadoop
    infrastructure. During the setup of the Ambari server for the first time, we can
    choose the database we want to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, Ambari supports the following databases:'
  prefs: []
  type: TYPE_NORMAL
- en: PostgreSQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oracle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MySQL or MariaDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedded PostgreSQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft SQL Server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SQL Anywhere
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Berkeley DB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a Hadoop cluster with Ambari
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to set up a brand new Hadoop cluster from
    scratch using Ambari. In order to do this, we are going to need four servers –
    one server for running the Ambari server and three other nodes for running the
    Hadoop components.
  prefs: []
  type: TYPE_NORMAL
- en: Server configurations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following table displays the configurations of the servers we are using
    as part of this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Server Type** | **Name** | **CPU** | **RAM** | **DISK** |'
  prefs: []
  type: TYPE_TB
- en: '| Ambari Server node | master | 1 | 3.7 GB | 100 GB |'
  prefs: []
  type: TYPE_TB
- en: '| Hadoop node 1 | node-1 | 2 | 13 GB | 250 GB |'
  prefs: []
  type: TYPE_TB
- en: '| Hadoop node 2 | node-2 | 2 | 13 GB | 250 GB |'
  prefs: []
  type: TYPE_TB
- en: '| Hadoop node 3 | node-3 | 2 | 13 GB | 250 GB |'
  prefs: []
  type: TYPE_TB
- en: Since this is a sample setup, we are good with this configuration. For real-world
    scenarios, please choose the configuration according to your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section and all further sections assume that you have a working internet
    connection on all the servers and are safely firewalled to prevent any intrusions.
  prefs: []
  type: TYPE_NORMAL
- en: All the servers are running the CentOS 7 operating system, as it's a system
    that uses RPM/YUM for package management. Don't get confused when you see `yum`
    in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we go ahead and start using the servers, we need to run basic utility
    programs which help us troubleshoot various issues with the servers. They are
    installed as part of the next command. Don''t worry if you are not sure what they
    are. Except for `mysql-connector-java` and `wget`, all other utilities are not
    mandatory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Installing the Ambari server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step in creating the Hadoop cluster is to get our Ambari server application
    up and running. So, log in to the master node with SSH and perform the following
    steps in order:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the Ambari YUM repository for CentOS 7 with this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'After this step, we need to move the `ambari.repo` file to the `/etc/yum.repos.d`
    directory using this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to install the `ambari-server` package with the help of this
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We are going to use a MySQL server for our Ambari server. So, let''s install
    the required packages as well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s configure the MySQL server (or MariaDB) before we touch the Ambari setup
    process. This is done with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, create a database called `ambari` and a user called `ambari` with the
    password, `ambari,` so that the Ambari server configuration is easy to set up
    in the following steps. This can be done with these SQL queries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Store these four lines into a text file called `ambari.sql` and execute with
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This will create a database, users and give necessary privileges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Please use a strong password for production setup, otherwise your system will
    be vulnerable to any attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have done the groundwork, let''s run the Ambari server setup. Note
    that we are required to answer a few questions that are highlighted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the setup is complete, we need to create the tables in the Ambari database
    by using the previous file that is generated during the setup process. This can
    be done with this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is for us to start the `ambari-server` daemon. This will start
    the web interface that we will use in the following steps to create the Hadoop
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the server setup is complete, configure the JDBC driver (which is helpful
    for all the other nodes as well):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Preparing the Hadoop cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a few more steps that we need to do before we go ahead and create
    the Hadoop cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Since we have the Ambari server up and running, let's generate an RSA key pair
    that we can use for communication between the Ambari server and the Ambari agent
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: This key pair lets the Ambari server node log in to all the Hadoop nodes and
    perform the installation in an automated way.
  prefs: []
  type: TYPE_NORMAL
- en: 'This step is optional if you have already done this as part of procuring the
    servers and infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate two files inside the `/home/user/.ssh` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '`~/.ssh/id_rsa`: This is the private key file which has to be kept in a secret
    place'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`~/.ssh/id_rsa.pub`: This is the public key file which allows any SSH login
    using the private key file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The contents of this `id_rsa.pub` file should be put in `~/.ssh/authorized_keys`
    on all the Hadoop nodes. In this case, they are node servers (1–3).
  prefs: []
  type: TYPE_NORMAL
- en: This step of propagating all the public SSH keys can be done during the server
    provisioning itself, so a manual step is avoided every time we acquire new servers.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will do all the work with only the Ambari web interface.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Hadoop cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will build a Hadoop cluster using the Ambari web interface.
    This section assumes the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: The nodes (1–3) are reachable over SSH from the master server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Admin can log in to the nodes (1–3) using the `id-rsa` private key from the
    master server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A UNIX user can run `sudo` and perform all administrative actions on the node
    (1–3) servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Ambari server setup is complete
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Ambari web interface is accessible to the browser without any firewall restrictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ambari web interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s open a web browser and connect to the Ambari server web interface using
    `http://<server-ip>:8080`. We are presented with a login screen like this. Please
    enter `admin` as the username and `admin` as the password to continue:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/342804dc-4adf-4c32-8c58-c81024b86a4f.png)'
  prefs: []
  type: TYPE_IMG
- en: Once the login is successful, we are taken to the home page.
  prefs: []
  type: TYPE_NORMAL
- en: The Ambari home page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the main page where there are multiple options on the UI. Since this
    is a brand new installation, there is no cluster data available yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the home page with this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e406237d-ad0e-4dba-8fc2-d82db7913b57.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From this place, we can carry out the following activities:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you may have guessed, this section is used to launch a wizard that will help
    us create a Hadoop cluster from the browser.
  prefs: []
  type: TYPE_NORMAL
- en: Managing users and groups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section is helpful to manage users and groups that can use and manage the
    Ambari web application.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying views
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This interface is helpful in creating views for different types of users and
    what actions they can perform via the Ambari web interface.
  prefs: []
  type: TYPE_NORMAL
- en: Since our objective is to create a new Hadoop cluster, we will click on the
    Launch Install Wizard button and start the process of creating a Hadoop cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The cluster install wizard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hadoop cluster creation is broken down into multiple steps. We will go through
    all these steps in the following sections. First, we are presented with a screen
    where we need to name our Hadoop cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Naming your cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I have chosen `packt` as the Hadoop cluster name. Click Next when the Hadoop
    name is entered in the screen. The screen looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6cae5fdd-9dbd-49e1-b16e-793bf355b979.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting the Hadoop version
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we name the Hadoop cluster, we are presented with a screen to select the
    version of Hadoop we want to run.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, Ambari supports the following Hadoop versions:'
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop 2.3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop 2.4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop 2.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop 2.6 (upto 2.6.3.0)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can choose any version for the installation. I have selected the default
    option which is version 2.6.3.0, which can be seen in this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf80801e-ac87-404a-b773-b28e24e41787.png)'
  prefs: []
  type: TYPE_IMG
- en: Click Next at the bottom of the screen to continue to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next logical step is to select the list of servers on which we are going
    to install the Hadoop-2.6.3.0 version. If you remember the original table, we
    named our node servers (1–3). We will enter those in the UI.
  prefs: []
  type: TYPE_NORMAL
- en: Since the installation is going to be completely automated, we also need to
    provide the RSA private key that we generated in the previous section in the UI.
    This will make sure that the master node can log in to the servers without any
    password over SSH.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we need to provide a UNIX username that's already been created on all
    the node (1–3) servers that can also accept RSA key for authentication.
  prefs: []
  type: TYPE_NORMAL
- en: Add `id_rsa.pub` to `~/.ssh/authorized_keys` on the node (1–3) servers.
  prefs: []
  type: TYPE_NORMAL
- en: Please keep in mind that these hostnames should have proper entries in the **DNS**
    (**Domain Name System**) Servers otherwise the installation won't be able to proceed
    from this step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The names that I have given can be seen in this following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f8538ec-bcee-4508-b32b-b6056f188dca.png)'
  prefs: []
  type: TYPE_IMG
- en: After the data is entered, click on Register and Confirm.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this step, the Ambari agent is automatically installed on the given nodes,
    provided the details are accurate. Success confirmation looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d657b897-b614-45c0-8735-e66399aac561.png)'
  prefs: []
  type: TYPE_IMG
- en: If we want to remove any nodes, this is the screen in which we can do it. Click
    Next when we are ready to go to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we need to select the list of applications/services that we want to install
    on the three servers we have selected.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, Ambari supports the following services:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Application/Service** | **Application Description** |'
  prefs: []
  type: TYPE_TB
- en: '| HDFS | Hadoop Distributed File System |'
  prefs: []
  type: TYPE_TB
- en: '| YARN + MapReduce2 | Next generation Map Reduce framework |'
  prefs: []
  type: TYPE_TB
- en: '| Tez | Hadoop query processing framework built on top of YARN |'
  prefs: []
  type: TYPE_TB
- en: '| Hive | Data warehouse system for ad hoc queries |'
  prefs: []
  type: TYPE_TB
- en: '| HBase | Non-relational distributed database |'
  prefs: []
  type: TYPE_TB
- en: '| Pig | Scripting platform to analyze datasets in HDFS |'
  prefs: []
  type: TYPE_TB
- en: '| Sqoop | Tool to transfer data between Hadoop and RDBMS |'
  prefs: []
  type: TYPE_TB
- en: '| Oozie | Workflow co-ordination for Hadoop jobs with a web UI |'
  prefs: []
  type: TYPE_TB
- en: '| ZooKeeper | Distributed system coordination providing service |'
  prefs: []
  type: TYPE_TB
- en: '| Falcon | Data processing and management platform |'
  prefs: []
  type: TYPE_TB
- en: '| Storm | Stream processing framework |'
  prefs: []
  type: TYPE_TB
- en: '| Flume | Distributed system to collect, aggregate, and move streaming data
    to HDFS |'
  prefs: []
  type: TYPE_TB
- en: '| Accumulo | Distributed key/value store |'
  prefs: []
  type: TYPE_TB
- en: '| Ambari Infra | Shared service used by Amari components |'
  prefs: []
  type: TYPE_TB
- en: '| Ambari Metrics | Grafana-based system for metric collection and storage |'
  prefs: []
  type: TYPE_TB
- en: '| Atlas | Metadata and governance platform |'
  prefs: []
  type: TYPE_TB
- en: '| Kafka | Distributed streaming platform |'
  prefs: []
  type: TYPE_TB
- en: '| Knox | Single-point authentication provider for all Hadoop components |'
  prefs: []
  type: TYPE_TB
- en: '| Log Search | Ambari-managed services log aggregator and viewer |'
  prefs: []
  type: TYPE_TB
- en: '| Ranger | Hadoop data security application |'
  prefs: []
  type: TYPE_TB
- en: '| Ranger KMS | Key management server |'
  prefs: []
  type: TYPE_TB
- en: '| SmartSense | Hortonworks Smart Sense tool to diagnose applications |'
  prefs: []
  type: TYPE_TB
- en: '| Spark | Large-scale data processing framework |'
  prefs: []
  type: TYPE_TB
- en: '| Zeppelin Notebook | Web-based notebook for data analytics |'
  prefs: []
  type: TYPE_TB
- en: '| Druid | Column-oriented data store |'
  prefs: []
  type: TYPE_TB
- en: '| Mahout | Machine learning algorithms |'
  prefs: []
  type: TYPE_TB
- en: '| Slider | Framework to monitor applications on YARN |'
  prefs: []
  type: TYPE_TB
- en: '| Superset | Browser-based data exploration platform for RDBMS and Druid |'
  prefs: []
  type: TYPE_TB
- en: 'As part of the current step, we have selected only HDFS and its dependencies.
    The screen is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d5ef787-a2a7-47b3-a56a-4f0726dc3793.png)'
  prefs: []
  type: TYPE_IMG
- en: Once you have made your choices, click the Next button at the bottom of the
    UI.
  prefs: []
  type: TYPE_NORMAL
- en: Service placement on nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this step, we are shown the automatic selection of services on the three
    nodes we have selected for installation. If we want to customize the placement
    of the services on the nodes, we can do so. The placement looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd7ee964-d0a2-40e5-a058-5d31a73a6c9d.png)'
  prefs: []
  type: TYPE_IMG
- en: Click Next when the changes look good.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting slave and client nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some applications support slaves and client utilities. In this screen, we need
    to select the nodes on which we want these applications to be installed. If you
    are unsure, click Next. The screen looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67a9dcf8-1d46-4bc4-9907-db153f82d489.png)'
  prefs: []
  type: TYPE_IMG
- en: Customizing services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even though Ambari automatically selects most of the properties and linkage
    between the applications, it provides us with some flexibility to choose values
    for some of the features, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usernames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Passwords
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And other properties that help the applications run smoothly. These are highlighted
    in the current screen in red.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to customize these, we need to go to the tab with the highlighted
    properties and choose the values according to our need. The screen looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0736a2bc-163a-4774-b4c1-347e8e2afea7.png)'
  prefs: []
  type: TYPE_IMG
- en: After all the service properties are configured correctly, we will not see anything
    in red in the UI and can click the Next button at the bottom of the page.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing the services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this step, we are shown a summary of the changes we have made so far. We
    are given an option to print the changes so that we will not forget them (don't
    worry, all these are available on the UI later). For now we can click Deploy.
    This is when the actual changes will be made to the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'No changes will be made to the servers if we cancel this process. The current
    state of the wizard looks like this now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a89754af-2fc1-4e5c-8b69-64cde207f49e.png)'
  prefs: []
  type: TYPE_IMG
- en: Installing the services on the nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After we've clicked Deploy in the previous step, a deployment plan is generated
    by the Ambari server and applications will be deployed on all the nodes in parallel,
    using the Ambari agents running on all the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: We are shown the progress of what is being deployed in real time in this step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once all the components have been installed, they will be automatically started
    and we can see the successful completion in this screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3026110-8cdf-4996-a162-cb94ea509f42.png)'
  prefs: []
  type: TYPE_IMG
- en: Click Next when everything is done successfully. In the case of any failures,
    we are shown what has failed and will be given an option to retry the installation.
    If there are any failures, we need to dig into the errors and fix the underlying
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: If you have followed the instructions given at the beginning of the section
    you should have everything running smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: Installation summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this step, we are shown the summary of what has been installed. The screen
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/324eceec-02e5-4514-b822-a2fb762a9caf.png)'
  prefs: []
  type: TYPE_IMG
- en: Click on the Complete button which marks the end of the Hadoop cluster setup.
    Next, we will be taken to the cluster dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: The cluster dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the home page of the Hadoop cluster we have just created where we can
    see the list of all the services that have been installed and the health sensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can manage all aspects of the Hadoop cluster in this interface. Feel free
    to explore the interface and play with it to understand more:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/653fc1b7-f43b-4cff-8613-447f54d147bb.png)'
  prefs: []
  type: TYPE_IMG
- en: This marks the end of the Hadoop cluster creation with Ambari.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen how to create a single Hadoop cluster using Ambari. But,
    is there ever a requirement for multiple Hadoop clusters?
  prefs: []
  type: TYPE_NORMAL
- en: The answer depends on the business requirements. There are trade-offs for both
    single versus multiple Hadoop clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Before we jump into the advantages and disadvantages of both of these, let's
    see in what scenarios we might use either.
  prefs: []
  type: TYPE_NORMAL
- en: A single cluster for the entire business
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the most straightforward approach and every business starts with one
    cluster, at least. As the diversity of the business increases, organizations tend
    to choose one cluster per department, or business unit.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ease of operability**: Since there is only one Hadoop cluster, managing it
    is very easy and the team sizes will also be optimal when administering it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One-stop shop**: Since all the company data is in a single place, it''s very
    easy to come up with innovative ways to use and generate analytics on top of the
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration cost**: Teams and departments within the enterprise can integrate
    with this single system very easily. They have less complex configurations to
    deal with when managing their applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost to serve**: Enterprises can have a better understanding of their entire
    big data usage and can also plan, in a less stringent way, on scaling their system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some disadvantages of employing this approach are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scale becomes a challenge**: Even though Hadoop can be run on hundreds and
    thousands of servers, it becomes a challenge to manage such big clusters, particularly
    during upgrades and other changes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Single point of failure**: Hadoop internally has replication built-in to
    it in the HDFS File System. When more nodes fail, the chances are that there is
    loss of data and it''s hard to recover from that.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Governance is a challenge**: As the scale of data, applications, and users
    increase, it is a challenge to keep track of the data without proper planning
    and implementation in place.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and confidential data management**: Enterprises deal with a variety
    of data that varies from highly sensitive to transient data. When all sorts of
    data is put in a big-data solution, we have to employ very strong authentication
    and authorization rules so that the data is visible only to the right audience.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With these thoughts, let's take a look at the other possibility of having Hadoop
    clusters in an enterprise.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Hadoop clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though having a single Hadoop cluster is easier to maintain within an organization,
    sometimes its important to have multiple Hadoop clusters to keep the business
    running smoothly and reduce dependency on a single point of failure system.
  prefs: []
  type: TYPE_NORMAL
- en: 'These multiple Hadoop clusters can be used for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Redundancy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cold backup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High availability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Business continuity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redundancy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we think of redundant Hadoop clusters, we should think about how much redundancy
    we can keep. As we already know, the **Hadoop Distributed File System** (**HDFS**)
    has internal data redundancy built in to it.
  prefs: []
  type: TYPE_NORMAL
- en: Given that a Hadoop cluster has lot of ecosystem built around it (services such
    as YARN, Kafka, and so on), we should think and plan carefully about whether to
    have the entire ecosystem made redundant or make only the data redundant by keeping
    it in a different cluster.
  prefs: []
  type: TYPE_NORMAL
- en: It's easier to make the HDFS portion of the Hadoop redundant as there are tools
    to copy the data from one HDFS to another HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at possible ways to achieve this via this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ac21035-486c-41d0-903a-c7b9e1a0b41a.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see here, the main Hadoop cluster runs a full stack of all its applications,
    and data is supplied to it via multiple sources.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have defined two types of redundant clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: A fully redundant Hadoop cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This cluster runs the exact set of applications as the primary cluster and the
    data is copied periodically from the main Hadoop cluster. Since this is a one-way
    copy from the main cluster to the second cluster, we can be 100% sure that the
    main cluster isn't impacted when we make any changes to this fully redundant cluster.
  prefs: []
  type: TYPE_NORMAL
- en: One important thing to understand is that we are running all other instances
    of applications in this cluster. Since every application maintains its state in
    its own predefined location, the application states are not replicated from the
    main Hadoop cluster to this cluster, which means that the jobs that were created
    in the main Hadoop cluster are not visible in this cluster. The same applies to
    the Kafka topics, zookeeper nodes, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: This type of cluster is helpful for running different environments such as QA,
    Staging, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: A data redundant Hadoop cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this type of cluster setup, we create a new Hadoop cluster and copy the data
    from the main cluster, like in the previous case; but here we are not worried
    about the other applications that are run in this cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'This type of setup is good for:'
  prefs: []
  type: TYPE_NORMAL
- en: Having data backup for Hadoop in a different geography
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing big data with other enterprises/organizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cold backup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cold backup is important for enterprises as the data gets older. Even though
    Hadoop is designed to store unlimited amounts of data, it's not always necessary
    to keep all the data available for processing.
  prefs: []
  type: TYPE_NORMAL
- en: It is sometimes necessary to preserve the data for auditing purposes and also
    for historical reasons. In such cases, we can create a dedicated Hadoop cluster
    with only the HDFS (File System) component and periodically sync all the data
    into this cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The design for this system is similar to the data redundant Hadoop cluster.
  prefs: []
  type: TYPE_NORMAL
- en: High availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though Hadoop has multiple components within the architecture, not all
    the components are highly available due to the internal design.
  prefs: []
  type: TYPE_NORMAL
- en: The core component of Hadoop is its distributed, fault-tolerant, filesystem
    HDFS. HDS has multiple components one of them is the NameNode which is the registry
    of where the files are located in the HDFS. In the earlier versions of HDS NameNode
    was Single point of Failure, In the recent versions Secondary NameNode has been
    added to assist with high availability requirements for Hadoop Cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In order to make every component of the Hadoop ecosystem a highly available
    system, we need to add multiple redundant nodes (they come with their own cost)
    which work together as a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: One more thing to note is that high availability with Hadoop is possible within
    a single geographical region, as the locality of the data with applications is
    one of the key things with Hadoop. The moment we have multiple data centers in
    play we need to think alternatively to achieve high availability across the data
    centers.
  prefs: []
  type: TYPE_NORMAL
- en: Business continuity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is part of **Business Continuity Planning **(**BCP**) where natural disasters
    can bring an end to the Hadoop system, if not planned correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the strategy would be to use multiple geographical regions as providers
    to run the big data systems. When we talk about multiple data centers, the obvious
    challenge is the network and the cost associated with managing both systems. One
    of the biggest challenges is how to keep multiple regions in sync.
  prefs: []
  type: TYPE_NORMAL
- en: One possible solution is to build a fully redundant Hadoop cluster in other
    geographical regions and keep the data in sync, periodically. In the case of any
    disaster/breakdown of one region, our businesses won't come to halt as we can
    smoothly run our operations.
  prefs: []
  type: TYPE_NORMAL
- en: Application environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many businesses internally follow different ways of releasing their software
    to production. As part of this, they follow several continuous integration methodologies,
    in order to have better control over the stability of the Hadoop environments.
    It's good to build multiple smaller Hadoop clusters with X% of the data from the
    main production environment and run all the applications here.
  prefs: []
  type: TYPE_NORMAL
- en: Applications can build their integration tests on these dedicated environments
    (QA, Staging, and so on) and can release their software to production once everything
    is good.
  prefs: []
  type: TYPE_NORMAL
- en: One practice that I have come across is that organizations tend to directly
    ship the code to production and end up facing outage of their applications because
    of an untested workflow or bug. It's good practice to have dedicated Hadoop application
    environments to test the software thoroughly and achieve higher uptime and happier
    customers.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop data copy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen in the previous sections that, having highly available data is
    very important for a business to succeed and stay up to date with its competition.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explore the possible ways to achieve highly available
    data setup.
  prefs: []
  type: TYPE_NORMAL
- en: HDFS data copy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hadoop uses HDFS as its core to store the files. HDFS is rack aware and is intelligent
    enough to reduce the network data transfer when applications are run on the data
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: One of the preferred ways of data copying in an HDFS environment is to use the
    DistCp. The official documentation for this is available at the following URL
    [http://hadoop.apache.org/docs/r1.2.1/distcp.html](http://hadoop.apache.org/docs/r1.2.1/distcp.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will see a few examples of copying data from one Hadoop cluster to another
    Hadoop cluster. But before that, let''s look at how the data is laid out:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a884670-9141-4abc-890e-e045b5280301.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to copy the data from the production Hadoop cluster to the backup
    Hadoop cluster, we can use `distcp`. Let''s see how to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: When we run the `distcp` command, a MapReduce job is created to automatically
    find out the list of files and then copy them to the destination.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full command syntax looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '`OPTIONS`: These are the multiple options the command takes which control the
    behavior of the execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`source path`: A source path can be any valid File System URI that''s supported
    by Hadoop. DistCp supports taking multiple source paths in one go.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`destination path`: This is a single path where all the source paths need to
    be copied.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s take a closer look at a few of the important options:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Flag/Option** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `append` | Incrementally writes the data to the destination files if they
    already exist (only `append` is performed, no block level check is performed to
    do incremental copy). |'
  prefs: []
  type: TYPE_TB
- en: '| `async` | Performs the copy in a non-blocking way. |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic` | Perform all the file copy or aborts even if one fails. |'
  prefs: []
  type: TYPE_TB
- en: '| `Tmp <path>` | Path to be used for atomic commit. |'
  prefs: []
  type: TYPE_TB
- en: '| `delete` | Deletes the files from the destination if they are not present
    in the source tree. |'
  prefs: []
  type: TYPE_TB
- en: '| `Bandwidth <arg>` | Limits how much network bandwidth to be used during the
    copy process. |'
  prefs: []
  type: TYPE_TB
- en: '| `f <file-path>` | Filename consisting of a list of all paths which need to
    be copied. |'
  prefs: []
  type: TYPE_TB
- en: '| `i` | Ignores any errors during file copy. |'
  prefs: []
  type: TYPE_TB
- en: '| `Log <file-path>` | Location where the execution log is saved. |'
  prefs: []
  type: TYPE_TB
- en: '| `M <number>` | Maximum number of concurrent maps to use for copying. |'
  prefs: []
  type: TYPE_TB
- en: '| `overwrite` | Overwrites the files even if they exist on destination. |'
  prefs: []
  type: TYPE_TB
- en: '| `update` | Copies only the missing files and directories. |'
  prefs: []
  type: TYPE_TB
- en: '| `skipcrccheck` | If passed, CRC checks are skipped during transfer. |'
  prefs: []
  type: TYPE_TB
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about Apache Ambari and studied its architecture
    in detail. We then understood how to prepare and create our own Hadoop cluster
    with Ambari. In order to do this, we also looked into configuring the Ambari server
    as per the requirement before preparing our cluster. We also learned about single
    and multiple Hadoop clusters and how they can be used, based on the business requirement.
  prefs: []
  type: TYPE_NORMAL
