["```py\n>>> from nltk.tag import DefaultTagger\n>>> tagger = DefaultTagger('NN')\n>>> tagger.tag(['Hello', 'World'])\n[('Hello', 'NN'), ('World', 'NN')]\n```", "```py\n>>> from nltk.corpus import treebank\n>>> test_sents = treebank.tagged_sents()[3000:]\n>>> tagger.evaluate(test_sents)\n0.14331966328512843\n```", "```py\n>>> tagger.batch_tag([['Hello', 'world', '.'], ['How', 'are', 'you', '?']])\n[[('Hello', 'NN'), ('world', 'NN'), ('.', 'NN')], [('How', 'NN'), ('are', 'NN'), ('you', 'NN'), ('?', 'NN')]]\n```", "```py\n>>> from nltk.tag import untag\n>>> untag([('Hello', 'NN'), ('World', 'NN')])\n['Hello', 'World']\n```", "```py\n>>> from nltk.tag import UnigramTagger\n>>> from nltk.corpus import treebank\n>>> train_sents = treebank.tagged_sents()[:3000]\n>>> tagger = UnigramTagger(train_sents)\n>>> treebank.sents()[0]\n['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n>>> tagger.tag(treebank.sents()[0])\n[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n```", "```py\n>>> tagger.evaluate(test_sents)\n0.85763004532700193\n```", "```py\n>>> tagger = UnigramTagger(model={'Pierre': 'NN'})\n>>> tagger.tag(treebank.sents()[0])\n[('Pierre', 'NN'), ('Vinken', None), (',', None), ('61', None), ('years', None), ('old', None), (',', None), ('will', None), ('join', None), ('the', None), ('board', None), ('as', None), ('a', None), ('nonexecutive', None),('director', None), ('Nov.', None), ('29', None), ('.', None)]\n```", "```py\n>>> tagger = UnigramTagger(train_sents, cutoff=3)\n>>> tagger.evaluate(test_sents)\n0.775350744657889\n```", "```py\n>>> tagger1 = DefaultTagger('NN')\n>>> tagger2 = UnigramTagger(train_sents, backoff=tagger1)\n>>> tagger2.evaluate(test_sents)\n0.87459529462551266\n```", "```py\n>>> tagger1._taggers == [tagger1]\nTrue\n>>> tagger2._taggers == [tagger2, tagger1]\nTrue\n```", "```py\n>>> import pickle\n>>> f = open('tagger.pickle', 'w')\n>>> pickle.dump(tagger, f)\n>>> f.close()\n>>> f = open('tagger.pickle', 'r')\n>>> tagger = pickle.load(f)\n```", "```py\n>>> from nltk.tag import BigramTagger, TrigramTagger\n>>> bitagger = BigramTagger(train_sents)\n>>> bitagger.evaluate(test_sents)\n0.11336067342974315\n>>> tritagger = TrigramTagger(train_sents)\n>>> tritagger.evaluate(test_sents)\n0.0688107058061731\n```", "```py\ndef backoff_tagger(train_sents, tagger_classes, backoff=None):\n  for cls in tagger_classes:\n    backoff = cls(train_sents, backoff=backoff)\n  return backoff\n```", "```py\n>>> from tag_util import backoff_tagger\n>>> backoff = DefaultTagger('NN')\n>>> tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff=backoff)\n>>> tagger.evaluate(test_sents)\n0.88163177206993304\n```", "```py\n>>> tagger._taggers[-1] == backoff\nTrue\n>>> isinstance(tagger._taggers[0], TrigramTagger)\nTrue\n>>> isinstance(tagger._taggers[1], BigramTagger)\nTrue\n```", "```py\n>>> from nltk.tag import NgramTagger\n>>> quadtagger = NgramTagger(4, train_sents)\n>>> quadtagger.evaluate(test_sents)\n0.058191236779624435\n```", "```py\nfrom nltk.tag import NgramTagger\n\nclass QuadgramTagger(NgramTagger):\n  def __init__(self, *args, **kwargs):\n    NgramTagger.__init__(self, 4, *args, **kwargs)\n```", "```py\n>>> from taggers import QuadgramTagger\n>>> quadtagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger, QuadgramTagger], backoff=backoff)\n>>> quadtagger.evaluate(test_sents)\n0.88111374919058927\n```", "```py\nfrom nltk.probability import FreqDist, ConditionalFreqDist\n\ndef word_tag_model(words, tagged_words, limit=200):\n  fd = FreqDist(words)\n  most_freq = fd.keys()[:limit]\n  cfd = ConditionalFreqDist(tagged_words)\n  return dict((word, cfd[word].max()) for word in most_freq)\n```", "```py\n>>> from tag_util import word_tag_model\n>>> from nltk.corpus import treebank\n>>> model = word_tag_model(treebank.words(), treebank.tagged_words())\n>>> tagger = UnigramTagger(model=model)\n>>> tagger.evaluate(test_sents)\n0.55972372113101665\n```", "```py\n>>> default_tagger = DefaultTagger('NN')\n>>> likely_tagger = UnigramTagger(model=model, backoff=default_tagger)\n>>> tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff=likely_tagger)\n>>> tagger.evaluate(test_sents)\n0.88163177206993304\n```", "```py\n>>> tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff=default_tagger)\n>>> likely_tagger = UnigramTagger(model=model, backoff=tagger)\n>>> likely_tagger.evaluate(test_sents)\n0.88245197496222749\n```", "```py\npatterns = [\n  (r'^\\d+$', 'CD'),\n  (r'.*ing$', 'VBG'), # gerunds, i.e. wondering\n  (r'.*ment$', 'NN'), # i.e. wonderment\n  (r'.*ful$', 'JJ') # i.e. wonderful\n]\n```", "```py\n>>> from tag_util import patterns\n>>> from nltk.tag import RegexpTagger\n>>> tagger = RegexpTagger(patterns)\n>>> tagger.evaluate(test_sents)\n0.037470321605870924\n```", "```py\n>>> from nltk.tag import AffixTagger\n>>> tagger = AffixTagger(train_sents)\n>>> tagger.evaluate(test_sents)\n0.27528599179797109\n```", "```py\n>>> prefix_tagger = AffixTagger(train_sents, affix_length=3)\n>>> prefix_tagger.evaluate(test_sents)\n0.23682279300669112\n```", "```py\n>>> suffix_tagger = AffixTagger(train_sents, affix_length=-2)\n>>> suffix_tagger.evaluate(test_sents)\n0.31953377940859057\n```", "```py\n>>> pre3_tagger = AffixTagger(train_sents, affix_length=3)\n>>> pre3_tagger.evaluate(test_sents)\n0.23682279300669112\n>>> pre2_tagger = AffixTagger(train_sents, affix_length=2, backoff=pre3_tagger)\n>>> pre2_tagger.evaluate(test_sents)\n0.29816533563565722\n>>> suf2_tagger = AffixTagger(train_sents, affix_length=-2, backoff=pre2_tagger)\n>>> suf2_tagger.evaluate(test_sents)\n0.32523203108137277\n>>> suf3_tagger = AffixTagger(train_sents, affix_length=-3, backoff=suf2_tagger)\n>>> suf3_tagger.evaluate(test_sents)\n0.35924886682495144\n```", "```py\nfrom nltk.tag import brill\n\ndef train_brill_tagger(initial_tagger, train_sents, **kwargs):\n  sym_bounds = [(1,1), (2,2), (1,2), (1,3)]\n  asym_bounds = [(-1,-1), (1,1)]\n  templates = [\n  brill.SymmetricProximateTokensTemplate(brill.ProximateTagsRule, *sym_bounds),\n    brill.SymmetricProximateTokensTemplate(brill.ProximateWordsRule, *sym_bounds),\n    brill.ProximateTokensTemplate(brill.ProximateTagsRule, *asym_bounds),\n    brill.ProximateTokensTemplate(brill.ProximateWordsRule, *asym_bounds)\n  ]\n  trainer = brill.FastBrillTaggerTrainer(initial_tagger, templates, deterministic=True)\n  return trainer.train(train_sents, **kwargs)\n```", "```py\n>>> default_tagger = DefaultTagger('NN')\n>>> initial_tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff=default_tagger)\n>>> initial_tagger.evaluate(test_sents)\n0.88163177206993304\n>>> from tag_util import train_brill_tagger\n>>> brill_tagger = train_brill_tagger(initial_tagger, train_sents)\n>>> brill_tagger.evaluate(test_sents)\n0.88327217785452194\n```", "```py\nTraining Brill tagger on 3000 sentences...\n\n    Finding initial useful rules...\n\n        Found 10709 useful rules.\n\n    Selecting rules...\n```", "```py\n>>> from nltk.tag import tnt\n>>> tnt_tagger = tnt.TnT()\n>>> tnt_tagger.train(train_sents)\n>>> tnt_tagger.evaluate(test_sents)\n0.87580401467731495\n```", "```py\n>>> from nltk.tag import DefaultTagger\n>>> unk = DefaultTagger('NN')\n>>> tnt_tagger = tnt.TnT(unk=unk, Trained=True)\n>>> tnt_tagger.train(train_sents)\n>>> tnt_tagger.evaluate(test_sents)\n0.89272609540254699\n```", "```py\n>>> tnt_tagger = tnt.TnT(N=100)\n>>> tnt_tagger.train(train_sents)\n>>> tnt_tagger.evaluate(test_sents)\n0.87580401467731495\n```", "```py\nfrom nltk.tag import SequentialBackoffTagger\nfrom nltk.corpus import wordnet\nfrom nltk.probability import FreqDist\n\nclass WordNetTagger(SequentialBackoffTagger):\n  '''\n  >>> wt = WordNetTagger()\n  >>> wt.tag(['food', 'is', 'great'])\n  [('food', 'NN'), ('is', 'VB'), ('great', 'JJ')]\n  '''\n  def __init__(self, *args, **kwargs):\n    SequentialBackoffTagger.__init__(self, *args, **kwargs)\n    self.wordnet_tag_map = {\n      'n': 'NN',\n      's': 'JJ',\n      'a': 'JJ',\n      'r': 'RB',\n      'v': 'VB'\n    }\n  def choose_tag(self, tokens, index, history):\n    word = tokens[index]\n    fd = FreqDist()\n    for synset in wordnet.synsets(word):\n      fd.inc(synset.pos)\n    return self.wordnet_tag_map.get(fd.max())\n```", "```py\n>>> from taggers import WordNetTagger\n>>> wn_tagger = WordNetTagger()\n>>> wn_tagger.evaluate(train_sents)\n0.18451574615215904\n```", "```py\n>>> from tag_util import backoff_tagger\n>>> from nltk.tag import UnigramTagger, BigramTagger, TrigramTagger\n>>> tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff=wn_tagger)\n>>> tagger.evaluate(test_sents)\n0.88564644938484782\n```", "```py\nfrom nltk.tag import SequentialBackoffTagger\nfrom nltk.corpus import names\n\nclass NamesTagger(SequentialBackoffTagger):\n  def __init__(self, *args, **kwargs):\n    SequentialBackoffTagger.__init__(self, *args, **kwargs)\n    self.name_set = set([n.lower() for n in names.words()])\n  def choose_tag(self, tokens, index, history):\n    word = tokens[index]\n    if word.lower() in self.name_set:\n      return 'NNP'\n    else:\n      return None\n```", "```py\n>>> from taggers import NamesTagger\n>>> nt = NamesTagger()\n>>> nt.tag(['Jacob'])\n[('Jacob', 'NNP')]\n```", "```py\n>>> from nltk.tag.sequential import ClassifierBasedPOSTagger\n>>> tagger = ClassifierBasedPOSTagger(train=train_sents)\n>>> tagger.evaluate(test_sents)\n0.93097345132743359\n```", "```py\n>>> from nltk.classify import MaxentClassifier\n>>> me_tagger = ClassifierBasedPOSTagger(train=train_sents, classifier_builder=MaxentClassifier.train)\n>>> me_tagger.evaluate(test_sents)\n0.93093028275415501\n```", "```py\ndef unigram_feature_detector(tokens, index, history):\n  return {'word': tokens[index]}\n```", "```py\n>>> from nltk.tag.sequential import ClassifierBasedTagger\n>>> from tag_util import unigram_feature_detector\n>>> tagger = ClassifierBasedTagger(train=train_sents, feature_detector=unigram_feature_detector)\n>>> tagger.evaluate(test_sents)\n0.87338657457371038\n```", "```py\n>>> default = DefaultTagger('NN')\n>>> tagger = ClassifierBasedPOSTagger(train=train_sents, backoff=default, cutoff_prob=0.3)\n>>> tagger.evaluate(test_sents)\n0.93110295704726964\n```"]