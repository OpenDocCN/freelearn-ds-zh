<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-113"><a id="_idTextAnchor113" class="pcalibre calibre6 pcalibre1"/>9</h1>
<h1 id="_idParaDest-114" class="calibre5"><a id="_idTextAnchor114" class="pcalibre calibre6 pcalibre1"/>Machine Learning for Networks</h1>
<p class="calibre3">In this chapter, we’ll <a id="_idIndexMarker400" class="pcalibre calibre6 pcalibre1"/>consider <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models typically used on relational data and their applications within network science. While many network-specific tools provide good insights into network structure and prediction of spread across a network, ML tools allow us to leverage additional information about individuals in the network to construct a more complete view of relationships, spreading processes, and key outcomes related to the network or its individuals. We’ll consider friendship networks and metadata associated with individuals and their connections to other individuals to explore ML on networks.</p>
<p class="calibre3">We’ll first return to network construction based on shared activities and traits of individuals, move on to clustering based on both network and metadata features, and finally predict individual and friendship network outcomes based on networks and their metadata. You’ll learn how to combine network metrics with metadata and how to build several types of ML models using network data, upon which we will build in the remaining chapters of this book. Let’s dive into some friendship networks and their metadata.</p>
<p class="calibre3">Specifically, we will cover the following topics in this chapter:</p>
<ul class="calibre10">
<li class="calibre11">Introduction to friendship networks and friendship relational datasets</li>
<li class="calibre11">ML on networks</li>
<li class="calibre11">SDL on networks</li>
</ul>
<h1 id="_idParaDest-115" class="calibre5"><a id="_idTextAnchor115" class="pcalibre calibre6 pcalibre1"/>Technical requirements</h1>
<p class="calibre3">The code for the practical examples presented in this chapter can be found here: <a href="https://github.com/PacktPublishing/Modern-Graph-Theory-Algorithms-with-Python" class="pcalibre calibre6 pcalibre1">https://github.com/PacktPublishing/Modern-Graph-Theory-Algorithms-with-Python</a></p>
<h1 id="_idParaDest-116" class="calibre5"><a id="_idTextAnchor116" class="pcalibre calibre6 pcalibre1"/>Introduction to friendship networks and friendship relational datasets</h1>
<p class="calibre3">In this section, we’ll consider a friendship network based on student behavior factors to form a network. We’ll then<a id="_idIndexMarker401" class="pcalibre calibre6 pcalibre1"/> apply <strong class="bold">unsupervised learning</strong> (<strong class="bold">UL</strong>) methods, namely<a id="_idIndexMarker402" class="pcalibre calibre6 pcalibre1"/> clustering, to group individuals into friendship groups to compare performance before and after adding extra network structural information.</p>
<h2 id="_idParaDest-117" class="calibre7"><a id="_idTextAnchor117" class="pcalibre calibre6 pcalibre1"/>Friendship network introduction</h2>
<p class="calibre3">Let’s consider a group<a id="_idIndexMarker403" class="pcalibre calibre6 pcalibre1"/> of classmates in a small school with enrollment based on age and geography, as is common in the United States. Classmates may participate in the same extracurricular activities, such as sports teams, the school paper, or a concert band. They may also study together, share meals, or get together to hang out on weekends. Some may form a core group of friends who take some of the same classes, participate in the same extracurriculars, study together, and hang out together outside of school-related activities. Strong social ties such as these often form an integral source of social support and lasting social relationships. These tend to be very important to individual life decisions and outcomes, particularly in adolescence and early adulthood, where peers play an important role in psychosocial development.</p>
<p class="calibre3">Other groups of friends may only study together or play on the same team, with few other shared interests or interactions. Weak social ties such as these also play an important role in society, connecting individuals with a wide range of resources across a community and exposing young people to a wider variety of viewpoints and new ideas. Social change often comes from weak ties across diverse communities, such as playing on the same sports teams, sharing classes in school, and participating in religious activities. While weak social ties often don’t provide strong social support, they serve a bridging function within networks and can introduce individuals to others who will become strong social ties.</p>
<p class="calibre3">In our first friendship network, we’ll consider both weak and strong social ties. Strong social ties mainly occur within a group of seven friends who mostly play on the same team, share some classes, study together, and play sports before school and at weekends:</p>
<div><div><img alt="Figure 9.1 – An illustration of a group of boys playing basketball before school" src="img/B21087_09_01.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 9.1 – An illustration of a group of boys playing basketball before school</p>
<p class="calibre3"><em class="italic">Figure 9</em><em class="italic">.1</em> shows three of the strong-social-tie boys playing basketball before school. We’d expect ideas, behaviors, and communicable diseases to spread quickly through this part of the network, as this group spends most of its time together.</p>
<p class="calibre3">In contrast, weak social ties within the network consist of occasional interactions that might include core courses or one shared interest that brings individuals together for short periods of time, such that they recognize each other and might know something about fellow students <a id="_idIndexMarker404" class="pcalibre calibre6 pcalibre1"/>but probably don’t know much about other students’ interests, home life, or aspirations:</p>
<div><div><img alt="Figure 9.2 – An illustration of students in the same classroom for a course who may not interact outside of the classroom" src="img/B21087_09_02.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 9.2 – An illustration of students in the same classroom for a course who may not interact outside of the classroom</p>
<p class="calibre3"><em class="italic">Figure 9</em><em class="italic">.2</em> shows students in the same classroom who may not interact outside of that single class. Weak social ties such as these expose students to different ideas, different interests, seasonal flu, and more but have less influence on an individual than on the group of individuals as a whole.</p>
<p class="calibre3">In this chapter, we’ll infer groups of students likely to share strong social ties through UL algorithms on both network metrics and metadata related to student demographics. We’ll also analyze social network risk on randomly generated networks to understand different epidemic risks for different types of networks through <strong class="bold">supervised learning</strong> (<strong class="bold">SL</strong>) with<a id="_idIndexMarker405" class="pcalibre calibre6 pcalibre1"/> GNNs. Let’s<a id="_idIndexMarker406" class="pcalibre calibre6 pcalibre1"/> explore our initial dataset a bit before diving into some analytics.</p>
<h2 id="_idParaDest-118" class="calibre7"><a id="_idTextAnchor118" class="pcalibre calibre6 pcalibre1"/>Friendship demographic and school factor dataset</h2>
<p class="calibre3">In this chapter, we’ll<a id="_idIndexMarker407" class="pcalibre calibre6 pcalibre1"/> mainly work with a dataset containing information<a id="_idIndexMarker408" class="pcalibre calibre6 pcalibre1"/> about a group of 25 students who are connected by many different lifestyle factors: team membership, casual workouts, weekend sports activities, game attendance, and homework study group membership. Demographic and socioeconomic factors, as well as class assignments, also connect these students by registration in four elective courses, gender, neighborhood of residence, and prior attendance at one of two local junior high schools.</p>
<p class="calibre3">This dataset was derived from Farrelly’s secondary school diary over the course of a month in her freshman year. Farrelly herself is individual <em class="italic">#7</em>. To create a weighted network, we’ll sum up connections across factors between pairs of students. This will give us an approximation of which students are most connected to each other. We’ll first explore clustering to discern friendship groups.</p>
<p class="calibre3">Let’s see how we can cluster this network based on metadata alone before we move into clustering on both metadata and network metrics.</p>
<h1 id="_idParaDest-119" class="calibre5"><a id="_idTextAnchor119" class="pcalibre calibre6 pcalibre1"/>ML on networks</h1>
<p class="calibre3">Now that we have<a id="_idIndexMarker409" class="pcalibre calibre6 pcalibre1"/> explored friendship data a bit, let’s see how<a id="_idIndexMarker410" class="pcalibre calibre6 pcalibre1"/> clustering algorithm performance varies depending on whether or not we include structural information about the network. We’ll start by considering just student factors.</p>
<h2 id="_idParaDest-120" class="calibre7"><a id="_idTextAnchor120" class="pcalibre calibre6 pcalibre1"/>Clustering based on student factors</h2>
<p class="calibre3">For our first attempt<a id="_idIndexMarker411" class="pcalibre calibre6 pcalibre1"/> at clustering, we’ll focus <a id="_idIndexMarker412" class="pcalibre calibre6 pcalibre1"/>on the dataset itself, which contains metadata regarding student demographics and social activities. One of the simplest clustering algorithms is <em class="italic">k-means clustering</em>, which partitions data iteratively to minimize within-cluster variance and maximize between-cluster variance. This means that students clustered together have more in common with students in that same cluster than with students in other clusters. K-means clustering is a simple algorithm that works well in most cases. However, one needs to specify the number of expected clusters, which is typically not known ahead of time. We’ll use a cluster size of <code>3</code> and assess model fit; in addition, we’ll restart the algorithm five times to ensure that we have an optimal three-cluster solution regardless of algorithm start point and random error.</p>
<p class="callout-heading">Important note</p>
<p class="calibre3">If you are on a Windows machine, you may get a warning that does not impact results; some of the packages on <code>scikit-learn</code> are not updated with the new Windows operating systems in mind. New releases of operating systems and updates to package dependencies tend to trigger these warnings.</p>
<p class="calibre3">Let’s dive into the k-means clustering code with <code>Script 9.1</code>:</p>
<pre class="source-code">
#import packages needed
import pandas as pd
from sklearn.cluster import KMeans
import igraph as ig
from igraph import Graph
import numpy as np
import os
#import file
File ="&lt;YourPath&gt;/Friendship_Factors.csv"
pwd = os.getcwd()
os.chdir(os.path.dirname(File))
mydata =
    pd.read_csv(os.path.basename(File),encoding='latin1')
#k-means model
X=mydata[mydata.columns.drop('Individual ID')]
km=KMeans(n_clusters=3,init='random',n_init=5)
km_model=km.fit_predict(X)
#explore k-means model
km_model
#add to dataset as first solution
km_1=np.array(km_model)+1
mydata['km_1']=km_1</pre> <p class="calibre3">The clustering results suggest that the three-cluster solution is a good fit. One cluster group (<em class="italic">#0</em>) includes individuals <code>1</code>-<code>7</code> and individual <code>10</code>; this group mostly does homework together, attends games, works out together on weekends, and plays on the same team. Cluster <em class="italic">#0</em> is characterized <a id="_idIndexMarker413" class="pcalibre calibre6 pcalibre1"/>by a tight-knit<a id="_idIndexMarker414" class="pcalibre calibre6 pcalibre1"/> group of friends who share many of the same activities and are near each other most of the week. We’d be concerned about an epidemic starting and spreading with this group. Likely, they share the same protective behaviors, such as healthy eating, regular physical activity, and social engagement. However, an infectious disease or risk behavior that might lead to physical injury (trying a dangerous take on a sports move, taking dares…) is a concern, as the behavior is likely to spread through the entire group of friends.</p>
<p class="calibre3">Another group (<em class="italic">#1</em>) includes individuals <code>8</code>-<code>9</code>, <code>12</code>-<code>14</code>, <code>16</code>, <code>19</code>, and <code>23</code>-<code>25</code>; these individuals usually share <em class="italic">class 2</em>, don’t work out or play sports together outside of school, don’t do homework together, and don’t share many other classes. Cluster <em class="italic">#1</em> is characterized by a lack of involvement and engagement with others in our sample. This group is low risk for both protective behavior and risk behavior spreading as they don’t have strong social ties to others in our sample. Likely, they wouldn’t be influenced or influence others with behavior.</p>
<p class="calibre3">The last group (<em class="italic">#2</em>) includes individuals <code>11</code>, <code>15</code>, <code>17</code>-<code>18</code>, and <code>20</code>-<code>22</code>; this group is heterogeneous and includes teammates who don’t have much else in common, individuals who share a few classes, and isolated individuals with few connections to others. In general, this group is low risk for epidemic or behavior spread like cluster <em class="italic">#1</em>; however, they are more active within the sample and may be influenced somewhat by teammates or <a id="_idIndexMarker415" class="pcalibre calibre6 pcalibre1"/>those with whom they share <a id="_idIndexMarker416" class="pcalibre calibre6 pcalibre1"/>multiple classes.</p>
<h2 id="_idParaDest-121" class="calibre7"><a id="_idTextAnchor121" class="pcalibre calibre6 pcalibre1"/>Clustering based on student factors and network metrics</h2>
<p class="calibre3">Now, let’s create a <a id="_idIndexMarker417" class="pcalibre calibre6 pcalibre1"/>network based on thresholded Pearson correlations, which represents the similarity of activities/classes across individuals by adding to <code>Script 9.1</code>:</p>
<pre class="source-code">
#create network via Pearson correlation
cor=np.corrcoef(X)
cor[cor&gt;=0.5]=1
cor[cor&lt;0.5]=0
X2=np.asmatrix(cor)
#create graph with self-loops removed
friends=Graph.Adjacency(X2)
edge_list=friends.get_edgelist()
self_loop=[]
for i in range(0,25):
    self=(i,i)
    self_loop.append(self)
to_remove=[]
for i in edge_list:
    for j in self_loop:
        if i==j:
            to_remove.append(i)
friends.delete_edges(to_remove)
ig.plot(friends)</pre> <p class="calibre3">Running this a<a id="_idIndexMarker418" class="pcalibre calibre6 pcalibre1"/>ddition to <code>Script 9.1</code> yields a plot of the friendship network, as shown in <em class="italic">Figure 9</em><em class="italic">.3</em>:</p>
<div><div><img alt="Figure 9.3 – A network plot of the thresholded friendship dataset" src="img/B21087_09_03.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 9.3 – A network plot of the thresholded friendship dataset</p>
<p class="calibre3"><em class="italic">Figure 9</em><em class="italic">.3</em> shows two<a id="_idIndexMarker419" class="pcalibre calibre6 pcalibre1"/> separate groups, with one very small group consisting of two individuals and a much larger group with sparse and dense connectivity among individuals in the group. We’d expect the degree and PageRank centralities to vary quite a bit among individuals, given the connectivity patterns of our friendship dataset. Let’s add to <code>Script 9.1</code> and append our feature matrix to rerun our k-means analysis, including both demographic factors and two scaled centrality metrics, to see how our clustering<a id="_idIndexMarker420" class="pcalibre calibre6 pcalibre1"/> changes:</p>
<p class="callout-heading">Note</p>
<p class="calibre3">You may find a warning about copying objects; this does not impact the analysis or object.</p>
<pre class="source-code">
#create scaled metrics and attach to X
d=np.array(Graph.degree(friends))/10
p=np.array(Graph.pagerank(friends))*20
X['degree']=d
X['pagerank']=p
#create new k-means model with graph metrics added
km2=KMeans(n_clusters=3,init='random',n_init=5)
km_model2=km2.fit_predict(X)
#explore new k-means model
km_model2
#add to dataset as first solution
km_2=np.array(km_model2)+1
mydata['km_2']=km_2</pre> <p class="calibre3">We can see some changes in our clustering results compared to our initial k-means model. In cluster <em class="italic">#0</em>, individual <code>19</code> is added (a teammate who does homework with the initial <em class="italic">#0</em> cluster and attends the game). Our initial cluster <em class="italic">#1</em> shows individuals <code>8</code>-<code>9</code>, <code>12</code>, <code>16</code>, and <code>23</code>-<code>25</code>; individuals <code>13</code>, <code>14</code>, and <code>19</code> are no longer assigned to this cluster but other individuals remain. In the remaining cluster, individuals <code>13</code> and <code>14</code> join our initial cluster, both of whom seem to have more connectivity than initial cluster <em class="italic">#1</em>, fitting better with cluster <em class="italic">#2</em> based on centrality metrics. It seems that adding network connectivity metrics improves k-means clustering results, as individuals who may not share every activity but show similar group connections are reassigned to groups that more closely fit their positions within the social network.</p>
<p class="calibre3">Let’s now see<a id="_idIndexMarker421" class="pcalibre calibre6 pcalibre1"/> how we can use a semi-supervised clustering algorithm that we first encountered in <a href="B21087_05.xhtml#_idTextAnchor066" class="pcalibre calibre6 pcalibre1"><em class="italic">Chapter 5</em></a>—spectral clustering—to obtain a semi-supervised solution to our friendship network clustering.</p>
<h2 id="_idParaDest-122" class="calibre7"><a id="_idTextAnchor122" class="pcalibre calibre6 pcalibre1"/>Spectral clustering on the friendship network</h2>
<p class="calibre3">As we <a id="_idIndexMarker422" class="pcalibre calibre6 pcalibre1"/>saw in <a href="B21087_05.xhtml#_idTextAnchor066" class="pcalibre calibre6 pcalibre1"><em class="italic">Chapter 5</em></a>, spectral <a id="_idIndexMarker423" class="pcalibre calibre6 pcalibre1"/>clustering offers a clustering option to partition either an adjacency matrix or a distance matrix; this can be done as a<a id="_idIndexMarker424" class="pcalibre calibre6 pcalibre1"/> UL or <code>Script 9.1</code> to run an unsupervised spectral clustering with three clusters and five initializations (similar to our k-means runs) on our friendship dataset to compare with our k-means results by adding to <code>Script 9.1</code>:</p>
<p class="callout-heading">Note</p>
<p class="calibre3">Again, you may encounter a Windows warning from scikit-learn or a warning about the graph not being fully connected (assessed via Laplacian, which results in a different approach to the clustering than would be run for a fully connected network). Neither of these warnings will impact the result.</p>
<pre class="source-code">
#import packages needed
from sklearn.cluster import SpectralClustering
from sklearn import metrics
#perform spectral clustering and attach to dataset
sc = SpectralClustering(3, affinity='precomputed',n_init=5)
sp_clust=sc.fit(cor)
mydata['sp']=sp_clust.labels_
sp_clust.labels_</pre> <p class="calibre3">These results differ significantly compared to the k-means solutions we obtained in the previous subsection. Given that both k-means models consider specific activities and course schedules rather than just a correlation summary, this difference makes sense. The spectral clustering solution focuses solely on network connectivity rather than the factors included in the friendship dataset or a combination of connectivity and factors. In this case, the k-means solutions make more sense given our data—particularly the second k-means solution, which includes network metrics and the original factors.</p>
<p class="calibre3">The selection of unsupervised versus semi-supervised clustering algorithms is highly specific to the task at hand. For very large networks, k-means algorithms have solutions that scale well, and adding network connectivity metrics that scale well may improve k-means solutions without sacrificing efficiency. For problems that involve a pure network connectivity solution, spectral clustering may be preferable, particularly if the factors used to construct the network were not collected or are unknown for a third-party network. However, spectral clustering can also take partially labeled data as input, allowing for SSL that can guide the learning process given what is known already about the data.</p>
<p class="calibre3">Now that we’ve<a id="_idIndexMarker425" class="pcalibre calibre6 pcalibre1"/> seen how UL and SSL <a id="_idIndexMarker426" class="pcalibre calibre6 pcalibre1"/>algorithms can be used on network datasets, let’s turn our attention to SL algorithms, focusing on an exciting new type<a id="_idIndexMarker427" class="pcalibre calibre6 pcalibre1"/> of <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) algorithm specifically designed to take network datasets as their input.</p>
<h1 id="_idParaDest-123" class="calibre5"><a id="_idTextAnchor123" class="pcalibre calibre6 pcalibre1"/>DL on networks</h1>
<p class="calibre3">In this section, we’ll<a id="_idIndexMarker428" class="pcalibre calibre6 pcalibre1"/> consider a new type of DL model called GNNs, which process and operate on networks by embedding vertex, edge, or global properties of the network to learn outcomes related to individual networks, vertex properties within a network, or edge properties within a network. Essentially, the DL architecture evolves the topology of these embeddings to find key topological features in the input data that are predictive of the outcome. This can be done in a fully supervised or semi-supervised fashion. In this example, we’ll focus on SSL, where only some of the labels are known; however, by providing all labels as input, this can be changed to an SL setting.</p>
<p class="calibre3">Before we dive into the technical details of GNNs, let’s explore their use cases in more depth. Classifying networks themselves often yields important insight into problems such as image features or type, molecular compound toxicity or potential use as a pharmaceutical agent, or potential for epidemic spread within a country of interest given travel routes and population hubs.</p>
<p class="calibre3">Typically, data such as molecules or images is transformed into network structure prior to the network embedding step of GNNs. Within the context of molecular compounds, atoms that share a covalent bound, for instance, are represented as vertices connected by an edge. Each compound, then, results in a unique network based on the molecular structure of that compound. For proteins, amino acids can serve as vertices, with connections existing between amino acids sharing a bond (such as a cysteine bridge resulting from a disulfide bond).</p>
<p class="calibre3">When screening<a id="_idIndexMarker429" class="pcalibre calibre6 pcalibre1"/> potential compounds for use in pharmaceutical development, we often want to predict if the compound might have toxic effects. Using known databases of toxic compounds and compounds with no toxic effects, we can develop a GNN to predict the toxic effects of new compounds in development based on the molecular structures of the new compounds, given what we know about molecules that are known to be or not to be toxic. This allows for quick screening of potential new drugs for toxicity prior to animal or human trials.</p>
<p class="calibre3">GNNs are also able to learn vertex labels given an input network, which is the focus of this chapter. For instance, within a crime or terrorism network, we may wish to identify potential leadership within the network given some knowledge of leaders and non-leaders from collected intelligence data. Incomplete information is common within intelligence data, and learning from what is known can be valuable in identifying key players in the network who are not known and who may be difficult to identify from informants or undercover agents. Since vertex prediction involves a network that has been constructed, we typically skip to the embedding steps of the GNN rather than wrangle the data. However, it might be necessary to add vertex labels to the graph to denote known information about leadership structure in the network.</p>
<p class="calibre3">Edge learning with GNNs mirrors vertex learning, typically through the use of an existing network with complete or incomplete information about edge properties (such as communication frequency or importance across members of a terrorist network that might involve coordinating a terrorist attack or recruiting new members in a geographic region). We embed the edges rather than the vertices in this case before proceeding with the GNN training.</p>
<p class="calibre3">Now that we know <a id="_idIndexMarker430" class="pcalibre calibre6 pcalibre1"/>a bit about problems we can tackle with GNNs, let’s learn more about the architecture and mathematical operations used to build a GNN.</p>
<h2 id="_idParaDest-124" class="calibre7"><a id="_idTextAnchor124" class="pcalibre calibre6 pcalibre1"/>GNN introduction</h2>
<p class="calibre3">GNN construction involves <a id="_idIndexMarker431" class="pcalibre calibre6 pcalibre1"/>a few key steps. In the prior subsection, we<a id="_idIndexMarker432" class="pcalibre calibre6 pcalibre1"/> mentioned data transformation as a potential first step. GNNs require a network or tensor of networks as input to the embedding step of the algorithm, so data must contain network-structured data and some outcome label associated with the networks themselves or edges/vertices in the network of interest. Some data engineering may be required to wrangle image(s), molecule(s), or other data sources into network structures. In the prior subsection, we overviewed how molecule or protein data can be transformed into a network structure. Many common types of data have standard transformation methods to transform them into network data; for example, in prior chapters, we’ve transformed spatial and time series data into network structures that could be used as input for a GNN.</p>
<p class="calibre3">Once our data exists in a network structure with a set of labels for networks, edges, or vertices, we’re ready to embed the relevant structures at a network, edge, or vertex level. Embeddings aim to find a low-dimensional representation of relevant network geometry at the level of embedding (network, edge, or vertex). They can also include other relevant information, such as other attributes of networks, edges, or vertices. Sometimes, it’s advantageous to create these embeddings manually to include both relevant network structure and attribute information. For instance, in our friendship network, we have data on many activities in which individuals participate; we may wish to create an embedding that captures not only the network centrality metrics but also the activity participation for individuals represented as a network vertex. In our k-means example, including both types of information (network structure and collected activity data) improved k-means performance in finding groups we hypothesized to exist.</p>
<p class="calibre3">Many GNN packages in Python, such as PyTorch (which we will use later in the section), have functions that summarize network properties at the network, edge, and vertex level to create an automatic embedding at a specified dimensionality. How we embed data prior to GNN training greatly impacts results, so this step is important to consider when building a GNN. Even with package functions such as the PyTorch one that we’ll use, specifying a dimensionality impacts algorithm performance. We don’t want too low of a dimensionality (missing key features relevant to the outcome of interest), but we also don’t want too high of a dimensionality (which might include a lot of noise). In practice, this parameter is often optimized through grid search.</p>
<p class="calibre3">Once we have the embedding, we can define the outcomes as target labels. We may need to employ one-hot encoding to transform text labels into a sequence of binary outcomes. Just as other DL algorithms can handle multiclass classification problems, continuous outcomes, or other types of distributions, GNNs can fit many different outcomes of<a id="_idIndexMarker433" class="pcalibre calibre6 pcalibre1"/> interest. This flexibility makes them ideal for modeling outcomes <a id="_idIndexMarker434" class="pcalibre calibre6 pcalibre1"/>across network classification/regression problems.</p>
<p class="calibre3">The DL architecture itself is not unique. Readers who are familiar with <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>) will<a id="_idIndexMarker435" class="pcalibre calibre6 pcalibre1"/> recognize many of the components and backfitting algorithms we’ll discuss, as they are identical within the context of GNNs. We start with an input layer with a dimension equal to the embedding dimension, and we end with an output layer with a dimension equal to the number of classes of our outcome (for classification problems, which we’ll consider in this chapter). When only the input and output layers exist, <strong class="bold">neural networks</strong> (<strong class="bold">NNs</strong>) approximate<a id="_idIndexMarker436" class="pcalibre calibre6 pcalibre1"/> linear regression, with a learned map between input matrices and output vectors. However, between these layers, we typically include hidden layers that further process features between the input and output layers, as shown in <em class="italic">Figure 9</em><em class="italic">.4</em>:</p>
<div><div><img alt="Figure 9.4 – A summary of the GNN life cycle, including data engineering and DL architecture steps" src="img/B21087_09_04.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 9.4 – A summary of the GNN life cycle, including data engineering and DL architecture steps</p>
<p class="calibre3">Hidden layers <a id="_idIndexMarker437" class="pcalibre calibre6 pcalibre1"/>refine the topological maps between the input and output<a id="_idIndexMarker438" class="pcalibre calibre6 pcalibre1"/> layers, often pooling topological features found during the training process to feed into the next hidden layer. For small networks and small samples of networks, the number of hidden layers should be small to maintain the stability of the solution and obtain good performance. For larger networks or sets of networks, more hidden layers can be added to improve performance without encountering instability of solutions and fit.</p>
<p class="calibre3">Hidden layers typically employ a non-linear mapping function, called an <em class="italic">activation function</em>, between <a id="_idIndexMarker439" class="pcalibre calibre6 pcalibre1"/>the input layer and output layer connected to that specific hidden layer. In practice, only a few activation functions are common, including<a id="_idIndexMarker440" class="pcalibre calibre6 pcalibre1"/> the <em class="italic">ReLU function</em>, which returns 0 for negative or zero input values and the input value for positive input values.</p>
<p class="calibre3"><em class="italic">Convolution layers</em>, also <a id="_idIndexMarker441" class="pcalibre calibre6 pcalibre1"/>commonly used in hidden layers, apply a filter function (typically a kernel) to the input layer to transform it through the defined kernel function. Typically, a convolution layer will reduce the dimensionality of the matrix or tensor, so zero padding to maintain dimensionality may be used to avoid the problem of transforming layers in a way that limits the number of layers possible given the dimensionality of the output. For small datasets, such as the one we consider, this is not necessarily a problem, as shallow networks tend to be the only stable GNNs that we can train given the limited data size.</p>
<p class="calibre3">The theory of building effective architectures is beyond the scope of this book, and readers interested in DL who do not have a background can obtain this knowledge by reading through the references provided at the end of this chapter.</p>
<p class="calibre3">Once an architecture is defined (either through expert guessing or, again, grid search to optimize architecture), we must fit the parameters connecting nodes in each layer and across<a id="_idIndexMarker442" class="pcalibre calibre6 pcalibre1"/> layers (called <em class="italic">weights</em>). There are many options to do this, and it’s possible to define custom fitting algorithms. However, we’ll focus on the two most common options within the PyTorch package used in our example: <strong class="bold">Adam optimizers</strong> and <strong class="bold">stochastic gradient </strong><strong class="bold">descent</strong> (<strong class="bold">SGD</strong>).</p>
<p class="calibre3"><em class="italic">SGD</em> fits <a id="_idIndexMarker443" class="pcalibre calibre6 pcalibre1"/>weights<a id="_idIndexMarker444" class="pcalibre calibre6 pcalibre1"/> between nodes within and across layers by exploring the<a id="_idIndexMarker445" class="pcalibre calibre6 pcalibre1"/> gradient function defined on the NN in much the way that gradient boosting fits a linear regression model. A <em class="italic">learning rate</em> is defined to control the<a id="_idIndexMarker446" class="pcalibre calibre6 pcalibre1"/> exploration of the gradient function. A steeper learning rate fits a model more quickly but may not find global minima or maxima. One caveat of SGD is that the algorithm can get stuck in local optima, resulting in lower accuracies of results than what is possible given the input data, mapping functions between layers, and the outcome data. It also tends to be slower, requiring more algorithm iterations and potentially more processing power to even fit the model.</p>
<p class="calibre3"><em class="italic">Adam optimizers</em> allow for <a id="_idIndexMarker447" class="pcalibre calibre6 pcalibre1"/>flexible learning rates for different weights between nodes, leading to faster model fits and avoiding local optima by allowing the learning rate to adjust to the local gradient landscape. Adam also allows for decay rates, further customizing local learning of weights. Many Adam optimizers have evolved since the initial Adam optimizer was developed, and it’s likely more will be developed for GNNs and other DL architectures. One drawback is that Adam optimizers tend to be memory-intensive. When training large GNNs, it may be preferable to use SGD to avoid memory issues during training.</p>
<p class="calibre3">In practice, it’s difficult to know which optimizer is best for fitting the weights of the defined architecture, and grid search, again, is typically employed for industry GNN models to optimize this choice. Once a fitting algorithm is selected, a predefined (again, usually optimized by grid search) number of iterations is run, or the algorithm runs until meeting a stopping criterion. Adam optimizers tend to converge more quickly than SGD optimizers, but performance can vary depending on the data and architecture.</p>
<p class="calibre3">Now that we understand a bit about the building blocks of GNNs, let’s explore an example using an <a id="_idIndexMarker448" class="pcalibre calibre6 pcalibre1"/>open source sports network consisting of students assigned<a id="_idIndexMarker449" class="pcalibre calibre6 pcalibre1"/> to two different teachers (our outcome of interest).</p>
<h2 id="_idParaDest-125" class="calibre7"><a id="_idTextAnchor125" class="pcalibre calibre6 pcalibre1"/>Example GNN classifying the Karate Network dataset</h2>
<p class="calibre3">For our<a id="_idIndexMarker450" class="pcalibre calibre6 pcalibre1"/> example, we’ll predict<a id="_idIndexMarker451" class="pcalibre calibre6 pcalibre1"/> vertex-level attributes<a id="_idIndexMarker452" class="pcalibre calibre6 pcalibre1"/> in a common open source network: Zachary’s <code>Karate Network</code> dataset. This dataset consists of 34 individuals connected by 78 edges in a karate training network who ended up splitting between an administrator and one of the instructors when a conflict between the administrator and instructor occurred. One of the primary tasks for vertex classification and learning problems on this network is to predict which individuals ended up siding with which person in the conflict (the administrator or the instructor). We will predict vertex labels through a semi-supervised GNN model approach.</p>
<p class="calibre3">We’ll first install the needed packages and import our dataset. If you don’t have the necessary packages installed, please install them on your machine prior to running our code. We’ve provided this step as an option in <code>Script 9.2</code>:</p>
<pre class="source-code">
#install packages if you have not installed them on your machine
#!pip install dgl
#!pip install torch
#import packages
import dgl
import dgl.data
import torch
import torch.nn as nn
import torch.nn.functional as F
import itertools
from dgl.nn import SAGEConv
#import Karate Club dataset with instructor/administrator labels
dataset = dgl.data.KarateClubDataset()
num_classes = dataset.num_classes
g = dataset[0]</pre> <p class="calibre3">To embed our vertex data, we’ll use PyTorch’s default embedding algorithm with a dimensionality of <code>6</code>. Anything that is in the <code>4</code>-<code>6</code> dimension range should work reasonably well, given the<a id="_idIndexMarker453" class="pcalibre calibre6 pcalibre1"/> size of <a id="_idIndexMarker454" class="pcalibre calibre6 pcalibre1"/>our <a id="_idIndexMarker455" class="pcalibre calibre6 pcalibre1"/>network. Let’s add to <code>Script 9.2</code> to embed our vertices:</p>
<pre class="source-code">
#embed vertices with a dimension of 6
vert_em = nn.Embedding(g.number_of_nodes(),6)
inputs = vert_em.weight
nn.init.xavier_uniform_(inputs)</pre> <p class="calibre3">This piece of the script should output embedding vectors for each vertex in our network. Now that we have our vertices embedded, we can create our labels. Given that we wish to demonstrate a semi-supervised approach, we’ll feed our network information on six vertices (<code>1</code>, <code>3</code>, <code>5</code>, <code>12</code>, <code>15</code>, and <code>32</code>). You can play around with this part of the script to see how fewer or more vertices impact the performance and stability of our chosen architecture. Let’s add the label information by adding to <code>Script 9.2</code>:</p>
<pre class="source-code">
#obtain labels and denote available labels for GNN learning
#(here: 1, 3, 5, 12, 15, 32)
labels = g.ndata['label']
labeled_nodes = [1, 3, 5, 12, 15, 32]</pre> <p class="calibre3">Next, we’ll need to build our GNN architecture and define training parameters. Many of the papers and tutorials on GNNs using this dataset employ a very shallow network architecture and Adam optimizers. For the sake of comparison and demonstration of other options for building GNNs, we’ll use two hidden layers instead of one (including convolution layers coupled with ReLU functions), employ small layers (eight and six nodes, respectively for hidden layers), an SGD fitting algorithm (with a learning rate of <code>0.01</code> and a momentum driving the algorithm of <code>0.8</code>, which is close to the default value), and <code>990</code> iterations. Many examples that exist online use Adam optimizers and a single hidden layer with more nodes than our architecture, allowing for fewer training iterations. However, for larger network vertex-label prediction problems, a more complex architecture is likely to perform better, so we will show a way to include more hidden layers and a way to use a different fitting algorithm than Adam. Let’s define our architecture <a id="_idIndexMarker456" class="pcalibre calibre6 pcalibre1"/>and fit our <a id="_idIndexMarker457" class="pcalibre calibre6 pcalibre1"/>weights<a id="_idIndexMarker458" class="pcalibre calibre6 pcalibre1"/> by adding to <code>Script 9.2</code>:</p>
<pre class="source-code">
#build a three-layer GraphSAGE model
class GraphSAGE(nn.Module):
    def __init__(self, in_feats, h_feats1, h_feats2,
        num_classes):
        super(GraphSAGE, self).__init__()
        self.conv1 = SAGEConv(in_feats, h_feats1, 'mean')
        self.conv2 = SAGEConv(h_feats1, h_feats2, 'mean')
        self.conv3 = SAGEConv(h_feats2, num_classes,'mean')
    def forward(self, g, in_feat):
        h = self.conv1(g, in_feat)
        h = F.relu(h)
        h = self.conv2(g, h)
        h = F.relu(h)
        h = self.conv3(g, h)
        return h
#6 embedding dimensions as input,
#a hidden layers of 8 and 6 nodes, and 2 classes to output
net = GraphSAGE(6,8,6,2)
#GNN training parameters
optimizer=torch.optim.SGD(
    itertools.chain(
        net.parameters(), vert_em.parameters()),
        lr=0.01, momentum=0.8)
all_logits = []
#train GNN
for e in range(990):
    logits = net(g, inputs)
    logp = F.log_softmax(logits, 1)
    loss = F.nll_loss(logp[labeled_nodes],labels[labeled_nodes])
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    all_logits.append(logits.detach())
    if e % 90 == 0:
        print('In epoch {}, loss: {}'.format(e, loss))</pre> <p class="calibre3">You should see <a id="_idIndexMarker459" class="pcalibre calibre6 pcalibre1"/>the loss function (logistic regression link function here) that decreases across iterations as your output. Typical accuracies from GNN architectures fall into the 95%-100% range. Because this dataset is small and our architecture is large, your output accuracies may vary quite a bit between runs of the algorithm. This has to do with random sampling within the fitting steps of the algorithm and the coarseness of the underlying gradient landscape. Let’s add to <code>Script 9.2</code> to find our accuracy:</p>
<pre class="source-code">
#obtain accuracy statistics
pred = torch.argmax(logits, axis=1)
print('Accuracy',(pred == labels).sum().item() / len(pred))</pre> <p class="calibre3">Our run of the algorithm gives an accuracy of ~97% in this run of the algorithm. That is on par with the performance of other GNN architectures. However, don’t be surprised if your accuracy is significantly lower in one or more runs of the script, as we don’t have a large enough sample size to fit this type of architecture. Changing the embedding dimension, architecture, and training algorithm parameters will impact accuracy, and interested readers are encouraged to revise <code>Script 9.2</code> as a way to see how different choices impact accuracy and stability of fit.</p>
<p class="calibre3">In general, GNN classifiers work much better and show better stability on larger networks and with more labels fed into semi-supervised usage. The Zachary Karate Network dataset<a id="_idIndexMarker460" class="pcalibre calibre6 pcalibre1"/> is <a id="_idIndexMarker461" class="pcalibre calibre6 pcalibre1"/>small <a id="_idIndexMarker462" class="pcalibre calibre6 pcalibre1"/>enough that other methods are recommended to classify the network. However, learning labels on a huge social network (such as those found on social media platforms) or a large geographic network (such as a United States city network with connections defined by roads connecting cities larger than 50,000 people) would result in a more stable GNN solution, and it would be possible to create a very deep architecture. However, to fit these large models, we often need a cloud computing platform, as the large datasets and large number of iterations can be difficult on a laptop.</p>
<p class="calibre3">GNNs have shown great promise in network-based classification problems in many different fields, and it is likely that they will continue to evolve and solve pressing problems with large networks and collections of networks. However, cloud computing solutions are often needed, and this requires expertise working with data and Python notebook solutions on the <a id="_idIndexMarker463" class="pcalibre calibre6 pcalibre1"/>cloud computing <a id="_idIndexMarker464" class="pcalibre calibre6 pcalibre1"/>platform used to store data and fit the GNN.</p>
<h1 id="_idParaDest-126" class="calibre5"><a id="_idTextAnchor126" class="pcalibre calibre6 pcalibre1"/>Summary</h1>
<p class="calibre3">In this chapter, we considered several use cases of ML algorithms on network datasets. This included UL on a friendship network through fitting k-means and spectral clustering. We considered k-means clustering on both the original dataset of activities in which individuals participated and the original dataset, with added network metrics to improve clustering accuracy. We then turned to SL and SSL on networks and collections of networks through a type of DL algorithm called GNNs. We accurately predicted the labels of individuals in Zachary’s Karate Network dataset through a shallow GNN and compared results with other existing solutions to this network classification problem. In <a href="B21087_10.xhtml#_idTextAnchor128" class="pcalibre calibre6 pcalibre1"><em class="italic">Chapter 10</em></a>, we'll mine educational data for causal relationships using network tools related to conditional probability.</p>
<h1 id="_idParaDest-127" class="calibre5"><a id="_idTextAnchor127" class="pcalibre calibre6 pcalibre1"/>References</h1>
<p class="calibre3">Acharya, D. B., &amp; Zhang, H. (2021). <em class="italic">Weighted Graph Nodes Clustering via Gumbel Softmax</em>. arXiv preprint arXiv:2102.10775.</p>
<p class="calibre3">Bongini, P., Bianchini, M., &amp; Scarselli, F. (2021). Molecular generative graph neural networks for drug discovery. <em class="italic">Neurocomputing, </em><em class="italic">450, 242-252.</em></p>
<p class="calibre3">Fan, W., Ma, Y., Li, Q., He, Y., Zhao, E., Tang, J., &amp; Yin, D. (2019, May). Graph neural networks for social recommendation. <em class="italic">In The World Wide Web Conference (</em><em class="italic">pp. 417-426).</em></p>
<p class="calibre3">Hartigan, J. A., &amp; Wong, M. A. (1979). Algorithm AS 136: A k-means clustering algorithm. <em class="italic">Journal of the Royal Statistical Society. Series C (Applied Statistics), </em><em class="italic">28(1), 100-108.</em></p>
<p class="calibre3">Imambi, S., Prakash, K. B., &amp; Kanagachidambaresan, G. R. (2021). PyTorch. <em class="italic">Programming with TensorFlow: Solution for Edge Computing </em><em class="italic">Applications, 87-104.</em></p>
<p class="calibre3">Kumar, V. (2020). <em class="italic">An Investigation Into Graph Neural Networks (Doctoral dissertation, Trinity College </em><em class="italic">Dublin, Ireland).</em></p>
<p class="calibre3">Labonne, M. (2023). <em class="italic">Hands-On Graph Neural Networks Using Python: Practical techniques and architectures for building powerful graph and deep learning apps with PyTorch. Packt </em><em class="italic">Publishing Ltd.</em></p>
<p class="calibre3">Liang, F., Qian, C., Yu, W., Griffith, D., &amp; Golmie, N. (2022). Survey of graph neural networks and applications. <em class="italic">Wireless Communications and Mobile </em><em class="italic">Computing, 2022.</em></p>
<p class="calibre3">Mantzaris, A. V., Chiodini, D., &amp; Ricketson, K. (2021). Utilizing the simple graph convolutional neural network as a model for simulating influence spread in networks. <em class="italic">Computational Social Networks, </em><em class="italic">8, 1-17.</em></p>
<p class="calibre3">Min, S., Gao, Z., Peng, J., Wang, L., Qin, K., &amp; Fang, B. (2021). STGSN—A Spatial–Temporal Graph Neural Network framework for time-evolving social networks. <em class="italic">Knowledge-Based Systems, </em><em class="italic">214, 106746.</em></p>
<p class="calibre3">Ng, A., Jordan, M., &amp; Weiss, Y. (2001). On spectral clustering: Analysis and an algorithm. <em class="italic">Advances in neural information processing </em><em class="italic">systems, 14.</em></p>
<p class="calibre3">Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., &amp; Monfardini, G. (2008). The graph neural network model. <em class="italic">IEEE transactions on neural networks, </em><em class="italic">20(1), 61-80.</em></p>
<p class="calibre3">Wieder, O., Kohlbacher, S., Kuenemann, M., Garon, A., Ducrot, P., Seidel, T., &amp; Langer, T. (2020). A compact review of molecular property prediction with graph neural networks. <em class="italic">Drug Discovery Today: Technologies, </em><em class="italic">37, 1-12.</em></p>
<p class="calibre3">Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., &amp; Philip, S. Y. (2020). A comprehensive survey on graph neural networks. <em class="italic">IEEE transactions on neural networks and learning systems, </em><em class="italic">32(1), 4-24.</em></p>
<p class="calibre3">Zachary, W. W. (1977). An information flow model for conflict and fission in small groups. <em class="italic">Journal of Anthropological Research, </em><em class="italic">33(4), 452-473.</em></p>
<p class="calibre3">Zhang, L., Xu, J., Pan, X., Ye, J., Wang, W., Liu, Y., &amp; Wei, Q. (2023). Visual analytics of route recommendation for tourist evacuation based on graph neural network. <em class="italic">Scientific Reports, </em><em class="italic">13(1), 17240.</em></p>
<p class="calibre3">Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., ... &amp; Sun, M. (2020). <em class="italic">Graph neural networks: A review of methods and applications. AI open, </em><em class="italic">1, 57-81.</em></p>
</div>
</body></html>