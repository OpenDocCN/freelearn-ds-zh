<html><head></head><body>
<div id="_idContainer121" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-113"><a id="_idTextAnchor113" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.1.1">9</span></h1>
<h1 id="_idParaDest-114" class="calibre5"><a id="_idTextAnchor114" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.2.1">Machine Learning for Networks</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.3.1">In this chapter, we’ll </span><a id="_idIndexMarker400" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.4.1">consider </span><strong class="bold"><span class="kobospan" id="kobo.5.1">machine learning</span></strong><span class="kobospan" id="kobo.6.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.7.1">ML</span></strong><span class="kobospan" id="kobo.8.1">) models typically used on relational data and their applications within network science. </span><span class="kobospan" id="kobo.8.2">While many network-specific tools provide good insights into network structure and prediction of spread across a network, ML tools allow us to leverage additional information about individuals in the network to construct a more complete view of relationships, spreading processes, and key outcomes related to the network or its individuals. </span><span class="kobospan" id="kobo.8.3">We’ll consider friendship networks and metadata associated with individuals and their connections to other individuals to explore ML </span><span><span class="kobospan" id="kobo.9.1">on networks.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.10.1">We’ll first return to network construction based on shared activities and traits of individuals, move on to clustering based on both network and metadata features, and finally predict individual and friendship network outcomes based on networks and their metadata. </span><span class="kobospan" id="kobo.10.2">You’ll learn how to combine network metrics with metadata and how to build several types of ML models using network data, upon which we will build in the remaining chapters of this book. </span><span class="kobospan" id="kobo.10.3">Let’s dive into some friendship networks and </span><span><span class="kobospan" id="kobo.11.1">their metadata.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.12.1">Specifically, we will cover the following topics in </span><span><span class="kobospan" id="kobo.13.1">this chapter:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span class="kobospan" id="kobo.14.1">Introduction to friendship networks and friendship </span><span><span class="kobospan" id="kobo.15.1">relational datasets</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.16.1">ML </span><span><span class="kobospan" id="kobo.17.1">on networks</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.18.1">SDL </span><span><span class="kobospan" id="kobo.19.1">on networks</span></span></li>
</ul>
<h1 id="_idParaDest-115" class="calibre5"><a id="_idTextAnchor115" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.20.1">Technical requirements</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.21.1">The code for the practical examples presented in this chapter can be found </span><span><span class="kobospan" id="kobo.22.1">here: </span></span><a href="https://github.com/PacktPublishing/Modern-Graph-Theory-Algorithms-with-Python" class="pcalibre calibre6 pcalibre1"><span><span class="kobospan" id="kobo.23.1">https://github.com/PacktPublishing/Modern-Graph-Theory-Algorithms-with-Python</span></span></a></p>
<h1 id="_idParaDest-116" class="calibre5"><a id="_idTextAnchor116" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.24.1">Introduction to friendship networks and friendship relational datasets</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.25.1">In this section, we’ll consider a friendship network based on student behavior factors to form a network. </span><span class="kobospan" id="kobo.25.2">We’ll then</span><a id="_idIndexMarker401" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.26.1"> apply </span><strong class="bold"><span class="kobospan" id="kobo.27.1">unsupervised learning</span></strong><span class="kobospan" id="kobo.28.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.29.1">UL</span></strong><span class="kobospan" id="kobo.30.1">) methods, namely</span><a id="_idIndexMarker402" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.31.1"> clustering, to group individuals into friendship groups to compare performance before and after adding extra network </span><span><span class="kobospan" id="kobo.32.1">structural information.</span></span></p>
<h2 id="_idParaDest-117" class="calibre7"><a id="_idTextAnchor117" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.33.1">Friendship network introduction</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.34.1">Let’s consider a group</span><a id="_idIndexMarker403" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.35.1"> of classmates in a small school with enrollment based on age and geography, as is common in the United States. </span><span class="kobospan" id="kobo.35.2">Classmates may participate in the same extracurricular activities, such as sports teams, the school paper, or a concert band. </span><span class="kobospan" id="kobo.35.3">They may also study together, share meals, or get together to hang out on weekends. </span><span class="kobospan" id="kobo.35.4">Some may form a core group of friends who take some of the same classes, participate in the same extracurriculars, study together, and hang out together outside of school-related activities. </span><span class="kobospan" id="kobo.35.5">Strong social ties such as these often form an integral source of social support and lasting social relationships. </span><span class="kobospan" id="kobo.35.6">These tend to be very important to individual life decisions and outcomes, particularly in adolescence and early adulthood, where peers play an important role in </span><span><span class="kobospan" id="kobo.36.1">psychosocial development.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.37.1">Other groups of friends may only study together or play on the same team, with few other shared interests or interactions. </span><span class="kobospan" id="kobo.37.2">Weak social ties such as these also play an important role in society, connecting individuals with a wide range of resources across a community and exposing young people to a wider variety of viewpoints and new ideas. </span><span class="kobospan" id="kobo.37.3">Social change often comes from weak ties across diverse communities, such as playing on the same sports teams, sharing classes in school, and participating in religious activities. </span><span class="kobospan" id="kobo.37.4">While weak social ties often don’t provide strong social support, they serve a bridging function within networks and can introduce individuals to others who will become strong </span><span><span class="kobospan" id="kobo.38.1">social ties.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.39.1">In our first friendship network, we’ll consider both weak and strong social ties. </span><span class="kobospan" id="kobo.39.2">Strong social ties mainly occur within a group of seven friends who mostly play on the same team, share some classes, study together, and play sports before school and </span><span><span class="kobospan" id="kobo.40.1">at weekends:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer117">
<span class="kobospan" id="kobo.41.1"><img alt="Figure 9.1 – An illustration of a group of boys playing basketball before school" src="image/B21087_09_01.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.42.1">Figure 9.1 – An illustration of a group of boys playing basketball before school</span></p>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.43.1">Figure 9</span></em></span><em class="italic"><span class="kobospan" id="kobo.44.1">.1</span></em><span class="kobospan" id="kobo.45.1"> shows three of the strong-social-tie boys playing basketball before school. </span><span class="kobospan" id="kobo.45.2">We’d expect ideas, behaviors, and communicable diseases to spread quickly through this part of the network, as this group spends most of its </span><span><span class="kobospan" id="kobo.46.1">time together.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.47.1">In contrast, weak social ties within the network consist of occasional interactions that might include core courses or one shared interest that brings individuals together for short periods of time, such that they recognize each other and might know something about fellow students </span><a id="_idIndexMarker404" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.48.1">but probably don’t know much about other students’ interests, home life, </span><span><span class="kobospan" id="kobo.49.1">or aspirations:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer118">
<span class="kobospan" id="kobo.50.1"><img alt="Figure 9.2 – An illustration of students in the same classroom for a course who may not interact outside of the classroom" src="image/B21087_09_02.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.51.1">Figure 9.2 – An illustration of students in the same classroom for a course who may not interact outside of the classroom</span></p>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.52.1">Figure 9</span></em></span><em class="italic"><span class="kobospan" id="kobo.53.1">.2</span></em><span class="kobospan" id="kobo.54.1"> shows students in the same classroom who may not interact outside of that single class. </span><span class="kobospan" id="kobo.54.2">Weak social ties such as these expose students to different ideas, different interests, seasonal flu, and more but have less influence on an individual than on the group of individuals as </span><span><span class="kobospan" id="kobo.55.1">a whole.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.56.1">In this chapter, we’ll infer groups of students likely to share strong social ties through UL algorithms on both network metrics and metadata related to student demographics. </span><span class="kobospan" id="kobo.56.2">We’ll also analyze social network risk on randomly generated networks to understand different epidemic risks for different types of networks through </span><strong class="bold"><span class="kobospan" id="kobo.57.1">supervised learning</span></strong><span class="kobospan" id="kobo.58.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.59.1">SL</span></strong><span class="kobospan" id="kobo.60.1">) with</span><a id="_idIndexMarker405" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.61.1"> GNNs. </span><span class="kobospan" id="kobo.61.2">Let’s</span><a id="_idIndexMarker406" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.62.1"> explore our initial dataset a bit before diving into </span><span><span class="kobospan" id="kobo.63.1">some analytics.</span></span></p>
<h2 id="_idParaDest-118" class="calibre7"><a id="_idTextAnchor118" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.64.1">Friendship demographic and school factor dataset</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.65.1">In this chapter, we’ll</span><a id="_idIndexMarker407" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.66.1"> mainly work with a dataset containing information</span><a id="_idIndexMarker408" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.67.1"> about a group of 25 students who are connected by many different lifestyle factors: team membership, casual workouts, weekend sports activities, game attendance, and homework study group membership. </span><span class="kobospan" id="kobo.67.2">Demographic and socioeconomic factors, as well as class assignments, also connect these students by registration in four elective courses, gender, neighborhood of residence, and prior attendance at one of two local junior </span><span><span class="kobospan" id="kobo.68.1">high schools.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.69.1">This dataset was derived from Farrelly’s secondary school diary over the course of a month in her freshman year. </span><span class="kobospan" id="kobo.69.2">Farrelly herself is individual </span><em class="italic"><span class="kobospan" id="kobo.70.1">#7</span></em><span class="kobospan" id="kobo.71.1">. </span><span class="kobospan" id="kobo.71.2">To create a weighted network, we’ll sum up connections across factors between pairs of students. </span><span class="kobospan" id="kobo.71.3">This will give us an approximation of which students are most connected to each other. </span><span class="kobospan" id="kobo.71.4">We’ll first explore clustering to discern </span><span><span class="kobospan" id="kobo.72.1">friendship groups.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.73.1">Let’s see how we can cluster this network based on metadata alone before we move into clustering on both metadata and </span><span><span class="kobospan" id="kobo.74.1">network metrics.</span></span></p>
<h1 id="_idParaDest-119" class="calibre5"><a id="_idTextAnchor119" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.75.1">ML on networks</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.76.1">Now that we have</span><a id="_idIndexMarker409" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.77.1"> explored friendship data a bit, let’s see how</span><a id="_idIndexMarker410" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.78.1"> clustering algorithm performance varies depending on whether or not we include structural information about the network. </span><span class="kobospan" id="kobo.78.2">We’ll start by considering just </span><span><span class="kobospan" id="kobo.79.1">student factors.</span></span></p>
<h2 id="_idParaDest-120" class="calibre7"><a id="_idTextAnchor120" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.80.1">Clustering based on student factors</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.81.1">For our first attempt</span><a id="_idIndexMarker411" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.82.1"> at clustering, we’ll focus </span><a id="_idIndexMarker412" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.83.1">on the dataset itself, which contains metadata regarding student demographics and social activities. </span><span class="kobospan" id="kobo.83.2">One of the simplest clustering algorithms is </span><em class="italic"><span class="kobospan" id="kobo.84.1">k-means clustering</span></em><span class="kobospan" id="kobo.85.1">, which partitions data iteratively to minimize within-cluster variance and maximize between-cluster variance. </span><span class="kobospan" id="kobo.85.2">This means that students clustered together have more in common with students in that same cluster than with students in other clusters. </span><span class="kobospan" id="kobo.85.3">K-means clustering is a simple algorithm that works well in most cases. </span><span class="kobospan" id="kobo.85.4">However, one needs to specify the number of expected clusters, which is typically not known ahead of time. </span><span class="kobospan" id="kobo.85.5">We’ll use a cluster size of </span><strong class="source-inline"><span class="kobospan" id="kobo.86.1">3</span></strong><span class="kobospan" id="kobo.87.1"> and assess model fit; in addition, we’ll restart the algorithm five times to ensure that we have an optimal three-cluster solution regardless of algorithm start point and </span><span><span class="kobospan" id="kobo.88.1">random error.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.89.1">Important note</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.90.1">If you are on a Windows machine, you may get a warning that does not impact results; some of the packages on </span><strong class="source-inline"><span class="kobospan" id="kobo.91.1">scikit-learn</span></strong><span class="kobospan" id="kobo.92.1"> are not updated with the new Windows operating systems in mind. </span><span class="kobospan" id="kobo.92.2">New releases of operating systems and updates to package dependencies tend to trigger </span><span><span class="kobospan" id="kobo.93.1">these warnings.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.94.1">Let’s dive into the k-means clustering code with </span><span><strong class="source-inline"><span class="kobospan" id="kobo.95.1">Script 9.1</span></strong></span><span><span class="kobospan" id="kobo.96.1">:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.97.1">
#import packages needed
import pandas as pd
from sklearn.cluster import KMeans
import igraph as ig
from igraph import Graph
import numpy as np
import os
#import file
File ="&lt;YourPath&gt;/Friendship_Factors.csv"
pwd = os.getcwd()
os.chdir(os.path.dirname(File))
mydata =
    pd.read_csv(os.path.basename(File),encoding='latin1')
#k-means model
X=mydata[mydata.columns.drop('Individual ID')]
km=KMeans(n_clusters=3,init='random',n_init=5)
km_model=km.fit_predict(X)
#explore k-means model
km_model
#add to dataset as first solution
km_1=np.array(km_model)+1
mydata['km_1']=km_1</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.98.1">The clustering results suggest that the three-cluster solution is a good fit. </span><span class="kobospan" id="kobo.98.2">One cluster group (</span><em class="italic"><span class="kobospan" id="kobo.99.1">#0</span></em><span class="kobospan" id="kobo.100.1">) includes individuals </span><strong class="source-inline"><span class="kobospan" id="kobo.101.1">1</span></strong><span class="kobospan" id="kobo.102.1">-</span><strong class="source-inline"><span class="kobospan" id="kobo.103.1">7</span></strong><span class="kobospan" id="kobo.104.1"> and individual </span><strong class="source-inline"><span class="kobospan" id="kobo.105.1">10</span></strong><span class="kobospan" id="kobo.106.1">; this group mostly does homework together, attends games, works out together on weekends, and plays on the same team. </span><span class="kobospan" id="kobo.106.2">Cluster </span><em class="italic"><span class="kobospan" id="kobo.107.1">#0</span></em><span class="kobospan" id="kobo.108.1"> is characterized </span><a id="_idIndexMarker413" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.109.1">by a tight-knit</span><a id="_idIndexMarker414" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.110.1"> group of friends who share many of the same activities and are near each other most of the week. </span><span class="kobospan" id="kobo.110.2">We’d be concerned about an epidemic starting and spreading with this group. </span><span class="kobospan" id="kobo.110.3">Likely, they share the same protective behaviors, such as healthy eating, regular physical activity, and social engagement. </span><span class="kobospan" id="kobo.110.4">However, an infectious disease or risk behavior that might lead to physical injury (trying a dangerous take on a sports move, taking dares…) is a concern, as the behavior is likely to spread through the entire group </span><span><span class="kobospan" id="kobo.111.1">of friends.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.112.1">Another group (</span><em class="italic"><span class="kobospan" id="kobo.113.1">#1</span></em><span class="kobospan" id="kobo.114.1">) includes individuals </span><strong class="source-inline"><span class="kobospan" id="kobo.115.1">8</span></strong><span class="kobospan" id="kobo.116.1">-</span><strong class="source-inline"><span class="kobospan" id="kobo.117.1">9</span></strong><span class="kobospan" id="kobo.118.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.119.1">12</span></strong><span class="kobospan" id="kobo.120.1">-</span><strong class="source-inline"><span class="kobospan" id="kobo.121.1">14</span></strong><span class="kobospan" id="kobo.122.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.123.1">16</span></strong><span class="kobospan" id="kobo.124.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.125.1">19</span></strong><span class="kobospan" id="kobo.126.1">, and </span><strong class="source-inline"><span class="kobospan" id="kobo.127.1">23</span></strong><span class="kobospan" id="kobo.128.1">-</span><strong class="source-inline"><span class="kobospan" id="kobo.129.1">25</span></strong><span class="kobospan" id="kobo.130.1">; these individuals usually share </span><em class="italic"><span class="kobospan" id="kobo.131.1">class 2</span></em><span class="kobospan" id="kobo.132.1">, don’t work out or play sports together outside of school, don’t do homework together, and don’t share many other classes. </span><span class="kobospan" id="kobo.132.2">Cluster </span><em class="italic"><span class="kobospan" id="kobo.133.1">#1</span></em><span class="kobospan" id="kobo.134.1"> is characterized by a lack of involvement and engagement with others in our sample. </span><span class="kobospan" id="kobo.134.2">This group is low risk for both protective behavior and risk behavior spreading as they don’t have strong social ties to others in our sample. </span><span class="kobospan" id="kobo.134.3">Likely, they wouldn’t be influenced or influence others </span><span><span class="kobospan" id="kobo.135.1">with behavior.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.136.1">The last group (</span><em class="italic"><span class="kobospan" id="kobo.137.1">#2</span></em><span class="kobospan" id="kobo.138.1">) includes individuals </span><strong class="source-inline"><span class="kobospan" id="kobo.139.1">11</span></strong><span class="kobospan" id="kobo.140.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.141.1">15</span></strong><span class="kobospan" id="kobo.142.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.143.1">17</span></strong><span class="kobospan" id="kobo.144.1">-</span><strong class="source-inline"><span class="kobospan" id="kobo.145.1">18</span></strong><span class="kobospan" id="kobo.146.1">, and </span><strong class="source-inline"><span class="kobospan" id="kobo.147.1">20</span></strong><span class="kobospan" id="kobo.148.1">-</span><strong class="source-inline"><span class="kobospan" id="kobo.149.1">22</span></strong><span class="kobospan" id="kobo.150.1">; this group is heterogeneous and includes teammates who don’t have much else in common, individuals who share a few classes, and isolated individuals with few connections to others. </span><span class="kobospan" id="kobo.150.2">In general, this group is low risk for epidemic or behavior spread like cluster </span><em class="italic"><span class="kobospan" id="kobo.151.1">#1</span></em><span class="kobospan" id="kobo.152.1">; however, they are more active within the sample and may be influenced somewhat by teammates or </span><a id="_idIndexMarker415" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.153.1">those with whom they share </span><a id="_idIndexMarker416" class="pcalibre calibre6 pcalibre1"/><span><span class="kobospan" id="kobo.154.1">multiple classes.</span></span></p>
<h2 id="_idParaDest-121" class="calibre7"><a id="_idTextAnchor121" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.155.1">Clustering based on student factors and network metrics</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.156.1">Now, let’s create a </span><a id="_idIndexMarker417" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.157.1">network based on thresholded Pearson correlations, which represents the similarity of activities/classes across individuals by adding to </span><span><strong class="source-inline"><span class="kobospan" id="kobo.158.1">Script 9.1</span></strong></span><span><span class="kobospan" id="kobo.159.1">:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.160.1">
#create network via Pearson correlation
cor=np.corrcoef(X)
cor[cor&gt;=0.5]=1
cor[cor&lt;0.5]=0
X2=np.asmatrix(cor)
#create graph with self-loops removed
friends=Graph.Adjacency(X2)
edge_list=friends.get_edgelist()
self_loop=[]
for i in range(0,25):
    self=(i,i)
    self_loop.append(self)
to_remove=[]
for i in edge_list:
    for j in self_loop:
        if i==j:
            to_remove.append(i)
friends.delete_edges(to_remove)
ig.plot(friends)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.161.1">Running this a</span><a id="_idIndexMarker418" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.162.1">ddition to </span><strong class="source-inline"><span class="kobospan" id="kobo.163.1">Script 9.1</span></strong><span class="kobospan" id="kobo.164.1"> yields a plot of the friendship network, as shown in </span><span><em class="italic"><span class="kobospan" id="kobo.165.1">Figure 9</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.166.1">.3</span></em></span><span><span class="kobospan" id="kobo.167.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer119">
<span class="kobospan" id="kobo.168.1"><img alt="Figure 9.3 – A network plot of the thresholded friendship dataset" src="image/B21087_09_03.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.169.1">Figure 9.3 – A network plot of the thresholded friendship dataset</span></p>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.170.1">Figure 9</span></em></span><em class="italic"><span class="kobospan" id="kobo.171.1">.3</span></em><span class="kobospan" id="kobo.172.1"> shows two</span><a id="_idIndexMarker419" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.173.1"> separate groups, with one very small group consisting of two individuals and a much larger group with sparse and dense connectivity among individuals in the group. </span><span class="kobospan" id="kobo.173.2">We’d expect the degree and PageRank centralities to vary quite a bit among individuals, given the connectivity patterns of our friendship dataset. </span><span class="kobospan" id="kobo.173.3">Let’s add to </span><strong class="source-inline"><span class="kobospan" id="kobo.174.1">Script 9.1</span></strong><span class="kobospan" id="kobo.175.1"> and append our feature matrix to rerun our k-means analysis, including both demographic factors and two scaled centrality metrics, to see how our </span><span><span class="kobospan" id="kobo.176.1">clustering</span></span><span><a id="_idIndexMarker420" class="pcalibre calibre6 pcalibre1"/></span><span><span class="kobospan" id="kobo.177.1"> changes:</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.178.1">Note</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.179.1">You may find a warning about copying objects; this does not impact the analysis </span><span><span class="kobospan" id="kobo.180.1">or object.</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.181.1">
#create scaled metrics and attach to X
d=np.array(Graph.degree(friends))/10
p=np.array(Graph.pagerank(friends))*20
X['degree']=d
X['pagerank']=p
#create new k-means model with graph metrics added
km2=KMeans(n_clusters=3,init='random',n_init=5)
km_model2=km2.fit_predict(X)
#explore new k-means model
km_model2
#add to dataset as first solution
km_2=np.array(km_model2)+1
mydata['km_2']=km_2</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.182.1">We can see some changes in our clustering results compared to our initial k-means model. </span><span class="kobospan" id="kobo.182.2">In cluster </span><em class="italic"><span class="kobospan" id="kobo.183.1">#0</span></em><span class="kobospan" id="kobo.184.1">, individual </span><strong class="source-inline"><span class="kobospan" id="kobo.185.1">19</span></strong><span class="kobospan" id="kobo.186.1"> is added (a teammate who does homework with the initial </span><em class="italic"><span class="kobospan" id="kobo.187.1">#0</span></em><span class="kobospan" id="kobo.188.1"> cluster and attends the game). </span><span class="kobospan" id="kobo.188.2">Our initial cluster </span><em class="italic"><span class="kobospan" id="kobo.189.1">#1</span></em><span class="kobospan" id="kobo.190.1"> shows individuals </span><strong class="source-inline"><span class="kobospan" id="kobo.191.1">8</span></strong><span class="kobospan" id="kobo.192.1">-</span><strong class="source-inline"><span class="kobospan" id="kobo.193.1">9</span></strong><span class="kobospan" id="kobo.194.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.195.1">12</span></strong><span class="kobospan" id="kobo.196.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.197.1">16</span></strong><span class="kobospan" id="kobo.198.1">, and </span><strong class="source-inline"><span class="kobospan" id="kobo.199.1">23</span></strong><span class="kobospan" id="kobo.200.1">-</span><strong class="source-inline"><span class="kobospan" id="kobo.201.1">25</span></strong><span class="kobospan" id="kobo.202.1">; individuals </span><strong class="source-inline"><span class="kobospan" id="kobo.203.1">13</span></strong><span class="kobospan" id="kobo.204.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.205.1">14</span></strong><span class="kobospan" id="kobo.206.1">, and </span><strong class="source-inline"><span class="kobospan" id="kobo.207.1">19</span></strong><span class="kobospan" id="kobo.208.1"> are no longer assigned to this cluster but other individuals remain. </span><span class="kobospan" id="kobo.208.2">In the remaining cluster, individuals </span><strong class="source-inline"><span class="kobospan" id="kobo.209.1">13</span></strong><span class="kobospan" id="kobo.210.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.211.1">14</span></strong><span class="kobospan" id="kobo.212.1"> join our initial cluster, both of whom seem to have more connectivity than initial cluster </span><em class="italic"><span class="kobospan" id="kobo.213.1">#1</span></em><span class="kobospan" id="kobo.214.1">, fitting better with cluster </span><em class="italic"><span class="kobospan" id="kobo.215.1">#2</span></em><span class="kobospan" id="kobo.216.1"> based on centrality metrics. </span><span class="kobospan" id="kobo.216.2">It seems that adding network connectivity metrics improves k-means clustering results, as individuals who may not share every activity but show similar group connections are reassigned to groups that more closely fit their positions within the </span><span><span class="kobospan" id="kobo.217.1">social network.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.218.1">Let’s now see</span><a id="_idIndexMarker421" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.219.1"> how we can use a semi-supervised clustering algorithm that we first encountered in </span><a href="B21087_05.xhtml#_idTextAnchor066" class="pcalibre calibre6 pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.220.1">Chapter 5</span></em></span></a><span class="kobospan" id="kobo.221.1">—spectral clustering—to obtain a semi-supervised solution to our friendship </span><span><span class="kobospan" id="kobo.222.1">network clustering.</span></span></p>
<h2 id="_idParaDest-122" class="calibre7"><a id="_idTextAnchor122" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.223.1">Spectral clustering on the friendship network</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.224.1">As we </span><a id="_idIndexMarker422" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.225.1">saw in </span><a href="B21087_05.xhtml#_idTextAnchor066" class="pcalibre calibre6 pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.226.1">Chapter 5</span></em></span></a><span class="kobospan" id="kobo.227.1">, spectral </span><a id="_idIndexMarker423" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.228.1">clustering offers a clustering option to partition either an adjacency matrix or a distance matrix; this can be done as a</span><a id="_idIndexMarker424" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.229.1"> UL or </span><strong class="bold"><span class="kobospan" id="kobo.230.1">semi-SL</span></strong><span class="kobospan" id="kobo.231.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.232.1">SSL</span></strong><span class="kobospan" id="kobo.233.1">) algorithm. </span><span class="kobospan" id="kobo.233.2">Here, we’ll use our correlation matrix from </span><strong class="source-inline"><span class="kobospan" id="kobo.234.1">Script 9.1</span></strong><span class="kobospan" id="kobo.235.1"> to run an unsupervised spectral clustering with three clusters and five initializations (similar to our k-means runs) on our friendship dataset to compare with our k-means results by adding to </span><span><strong class="source-inline"><span class="kobospan" id="kobo.236.1">Script 9.1</span></strong></span><span><span class="kobospan" id="kobo.237.1">:</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.238.1">Note</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.239.1">Again, you may encounter a Windows warning from scikit-learn or a warning about the graph not being fully connected (assessed via Laplacian, which results in a different approach to the clustering than would be run for a fully connected network). </span><span class="kobospan" id="kobo.239.2">Neither of these warnings will impact </span><span><span class="kobospan" id="kobo.240.1">the result.</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.241.1">
#import packages needed
from sklearn.cluster import SpectralClustering
from sklearn import metrics
#perform spectral clustering and attach to dataset
sc = SpectralClustering(3, affinity='precomputed',n_init=5)
sp_clust=sc.fit(cor)
mydata['sp']=sp_clust.labels_
sp_clust.labels_</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.242.1">These results differ significantly compared to the k-means solutions we obtained in the previous subsection. </span><span class="kobospan" id="kobo.242.2">Given that both k-means models consider specific activities and course schedules rather than just a correlation summary, this difference makes sense. </span><span class="kobospan" id="kobo.242.3">The spectral clustering solution focuses solely on network connectivity rather than the factors included in the friendship dataset or a combination of connectivity and factors. </span><span class="kobospan" id="kobo.242.4">In this case, the k-means solutions make more sense given our data—particularly the second k-means solution, which includes network metrics and the </span><span><span class="kobospan" id="kobo.243.1">original factors.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.244.1">The selection of unsupervised versus semi-supervised clustering algorithms is highly specific to the task at hand. </span><span class="kobospan" id="kobo.244.2">For very large networks, k-means algorithms have solutions that scale well, and adding network connectivity metrics that scale well may improve k-means solutions without sacrificing efficiency. </span><span class="kobospan" id="kobo.244.3">For problems that involve a pure network connectivity solution, spectral clustering may be preferable, particularly if the factors used to construct the network were not collected or are unknown for a third-party network. </span><span class="kobospan" id="kobo.244.4">However, spectral clustering can also take partially labeled data as input, allowing for SSL that can guide the learning process given what is known already about </span><span><span class="kobospan" id="kobo.245.1">the data.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.246.1">Now that we’ve</span><a id="_idIndexMarker425" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.247.1"> seen how UL and SSL </span><a id="_idIndexMarker426" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.248.1">algorithms can be used on network datasets, let’s turn our attention to SL algorithms, focusing on an exciting new type</span><a id="_idIndexMarker427" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.249.1"> of </span><strong class="bold"><span class="kobospan" id="kobo.250.1">deep learning</span></strong><span class="kobospan" id="kobo.251.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.252.1">DL</span></strong><span class="kobospan" id="kobo.253.1">) algorithm specifically designed to take network datasets as </span><span><span class="kobospan" id="kobo.254.1">their input.</span></span></p>
<h1 id="_idParaDest-123" class="calibre5"><a id="_idTextAnchor123" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.255.1">DL on networks</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.256.1">In this section, we’ll</span><a id="_idIndexMarker428" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.257.1"> consider a new type of DL model called GNNs, which process and operate on networks by embedding vertex, edge, or global properties of the network to learn outcomes related to individual networks, vertex properties within a network, or edge properties within a network. </span><span class="kobospan" id="kobo.257.2">Essentially, the DL architecture evolves the topology of these embeddings to find key topological features in the input data that are predictive of the outcome. </span><span class="kobospan" id="kobo.257.3">This can be done in a fully supervised or semi-supervised fashion. </span><span class="kobospan" id="kobo.257.4">In this example, we’ll focus on SSL, where only some of the labels are known; however, by providing all labels as input, this can be changed to an </span><span><span class="kobospan" id="kobo.258.1">SL setting.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.259.1">Before we dive into the technical details of GNNs, let’s explore their use cases in more depth. </span><span class="kobospan" id="kobo.259.2">Classifying networks themselves often yields important insight into problems such as image features or type, molecular compound toxicity or potential use as a pharmaceutical agent, or potential for epidemic spread within a country of interest given travel routes and </span><span><span class="kobospan" id="kobo.260.1">population hubs.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.261.1">Typically, data such as molecules or images is transformed into network structure prior to the network embedding step of GNNs. </span><span class="kobospan" id="kobo.261.2">Within the context of molecular compounds, atoms that share a covalent bound, for instance, are represented as vertices connected by an edge. </span><span class="kobospan" id="kobo.261.3">Each compound, then, results in a unique network based on the molecular structure of that compound. </span><span class="kobospan" id="kobo.261.4">For proteins, amino acids can serve as vertices, with connections existing between amino acids sharing a bond (such as a cysteine bridge resulting from a </span><span><span class="kobospan" id="kobo.262.1">disulfide bond).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.263.1">When screening</span><a id="_idIndexMarker429" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.264.1"> potential compounds for use in pharmaceutical development, we often want to predict if the compound might have toxic effects. </span><span class="kobospan" id="kobo.264.2">Using known databases of toxic compounds and compounds with no toxic effects, we can develop a GNN to predict the toxic effects of new compounds in development based on the molecular structures of the new compounds, given what we know about molecules that are known to be or not to be toxic. </span><span class="kobospan" id="kobo.264.3">This allows for quick screening of potential new drugs for toxicity prior to animal or </span><span><span class="kobospan" id="kobo.265.1">human trials.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.266.1">GNNs are also able to learn vertex labels given an input network, which is the focus of this chapter. </span><span class="kobospan" id="kobo.266.2">For instance, within a crime or terrorism network, we may wish to identify potential leadership within the network given some knowledge of leaders and non-leaders from collected intelligence data. </span><span class="kobospan" id="kobo.266.3">Incomplete information is common within intelligence data, and learning from what is known can be valuable in identifying key players in the network who are not known and who may be difficult to identify from informants or undercover agents. </span><span class="kobospan" id="kobo.266.4">Since vertex prediction involves a network that has been constructed, we typically skip to the embedding steps of the GNN rather than wrangle the data. </span><span class="kobospan" id="kobo.266.5">However, it might be necessary to add vertex labels to the graph to denote known information about leadership structure in </span><span><span class="kobospan" id="kobo.267.1">the network.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.268.1">Edge learning with GNNs mirrors vertex learning, typically through the use of an existing network with complete or incomplete information about edge properties (such as communication frequency or importance across members of a terrorist network that might involve coordinating a terrorist attack or recruiting new members in a geographic region). </span><span class="kobospan" id="kobo.268.2">We embed the edges rather than the vertices in this case before proceeding with the </span><span><span class="kobospan" id="kobo.269.1">GNN training.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.270.1">Now that we know </span><a id="_idIndexMarker430" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.271.1">a bit about problems we can tackle with GNNs, let’s learn more about the architecture and mathematical operations used to build </span><span><span class="kobospan" id="kobo.272.1">a GNN.</span></span></p>
<h2 id="_idParaDest-124" class="calibre7"><a id="_idTextAnchor124" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.273.1">GNN introduction</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.274.1">GNN construction involves </span><a id="_idIndexMarker431" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.275.1">a few key steps. </span><span class="kobospan" id="kobo.275.2">In the prior subsection, we</span><a id="_idIndexMarker432" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.276.1"> mentioned data transformation as a potential first step. </span><span class="kobospan" id="kobo.276.2">GNNs require a network or tensor of networks as input to the embedding step of the algorithm, so data must contain network-structured data and some outcome label associated with the networks themselves or edges/vertices in the network of interest. </span><span class="kobospan" id="kobo.276.3">Some data engineering may be required to wrangle image(s), molecule(s), or other data sources into network structures. </span><span class="kobospan" id="kobo.276.4">In the prior subsection, we overviewed how molecule or protein data can be transformed into a network structure. </span><span class="kobospan" id="kobo.276.5">Many common types of data have standard transformation methods to transform them into network data; for example, in prior chapters, we’ve transformed spatial and time series data into network structures that could be used as input for </span><span><span class="kobospan" id="kobo.277.1">a GNN.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.278.1">Once our data exists in a network structure with a set of labels for networks, edges, or vertices, we’re ready to embed the relevant structures at a network, edge, or vertex level. </span><span class="kobospan" id="kobo.278.2">Embeddings aim to find a low-dimensional representation of relevant network geometry at the level of embedding (network, edge, or vertex). </span><span class="kobospan" id="kobo.278.3">They can also include other relevant information, such as other attributes of networks, edges, or vertices. </span><span class="kobospan" id="kobo.278.4">Sometimes, it’s advantageous to create these embeddings manually to include both relevant network structure and attribute information. </span><span class="kobospan" id="kobo.278.5">For instance, in our friendship network, we have data on many activities in which individuals participate; we may wish to create an embedding that captures not only the network centrality metrics but also the activity participation for individuals represented as a network vertex. </span><span class="kobospan" id="kobo.278.6">In our k-means example, including both types of information (network structure and collected activity data) improved k-means performance in finding groups we hypothesized </span><span><span class="kobospan" id="kobo.279.1">to exist.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.280.1">Many GNN packages in Python, such as PyTorch (which we will use later in the section), have functions that summarize network properties at the network, edge, and vertex level to create an automatic embedding at a specified dimensionality. </span><span class="kobospan" id="kobo.280.2">How we embed data prior to GNN training greatly impacts results, so this step is important to consider when building a GNN. </span><span class="kobospan" id="kobo.280.3">Even with package functions such as the PyTorch one that we’ll use, specifying a dimensionality impacts algorithm performance. </span><span class="kobospan" id="kobo.280.4">We don’t want too low of a dimensionality (missing key features relevant to the outcome of interest), but we also don’t want too high of a dimensionality (which might include a lot of noise). </span><span class="kobospan" id="kobo.280.5">In practice, this parameter is often optimized through </span><span><span class="kobospan" id="kobo.281.1">grid search.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.282.1">Once we have the embedding, we can define the outcomes as target labels. </span><span class="kobospan" id="kobo.282.2">We may need to employ one-hot encoding to transform text labels into a sequence of binary outcomes. </span><span class="kobospan" id="kobo.282.3">Just as other DL algorithms can handle multiclass classification problems, continuous outcomes, or other types of distributions, GNNs can fit many different outcomes of</span><a id="_idIndexMarker433" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.283.1"> interest. </span><span class="kobospan" id="kobo.283.2">This flexibility makes them ideal for modeling outcomes </span><a id="_idIndexMarker434" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.284.1">across network </span><span><span class="kobospan" id="kobo.285.1">classification/regression problems.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.286.1">The DL architecture itself is not unique. </span><span class="kobospan" id="kobo.286.2">Readers who are familiar with </span><strong class="bold"><span class="kobospan" id="kobo.287.1">convolutional neural networks</span></strong><span class="kobospan" id="kobo.288.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.289.1">CNNs</span></strong><span class="kobospan" id="kobo.290.1">) will</span><a id="_idIndexMarker435" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.291.1"> recognize many of the components and backfitting algorithms we’ll discuss, as they are identical within the context of GNNs. </span><span class="kobospan" id="kobo.291.2">We start with an input layer with a dimension equal to the embedding dimension, and we end with an output layer with a dimension equal to the number of classes of our outcome (for classification problems, which we’ll consider in this chapter). </span><span class="kobospan" id="kobo.291.3">When only the input and output layers exist, </span><strong class="bold"><span class="kobospan" id="kobo.292.1">neural networks</span></strong><span class="kobospan" id="kobo.293.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.294.1">NNs</span></strong><span class="kobospan" id="kobo.295.1">) approximate</span><a id="_idIndexMarker436" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.296.1"> linear regression, with a learned map between input matrices and output vectors. </span><span class="kobospan" id="kobo.296.2">However, between these layers, we typically include hidden layers that further process features between the input and output layers, as shown in </span><span><em class="italic"><span class="kobospan" id="kobo.297.1">Figure 9</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.298.1">.4</span></em></span><span><span class="kobospan" id="kobo.299.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer120">
<span class="kobospan" id="kobo.300.1"><img alt="Figure 9.4 – A summary of the GNN life cycle, including data engineering and DL architecture steps" src="image/B21087_09_04.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.301.1">Figure 9.4 – A summary of the GNN life cycle, including data engineering and DL architecture steps</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.302.1">Hidden layers </span><a id="_idIndexMarker437" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.303.1">refine the topological maps between the input and output</span><a id="_idIndexMarker438" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.304.1"> layers, often pooling topological features found during the training process to feed into the next hidden layer. </span><span class="kobospan" id="kobo.304.2">For small networks and small samples of networks, the number of hidden layers should be small to maintain the stability of the solution and obtain good performance. </span><span class="kobospan" id="kobo.304.3">For larger networks or sets of networks, more hidden layers can be added to improve performance without encountering instability of solutions </span><span><span class="kobospan" id="kobo.305.1">and fit.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.306.1">Hidden layers typically employ a non-linear mapping function, called an </span><em class="italic"><span class="kobospan" id="kobo.307.1">activation function</span></em><span class="kobospan" id="kobo.308.1">, between </span><a id="_idIndexMarker439" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.309.1">the input layer and output layer connected to that specific hidden layer. </span><span class="kobospan" id="kobo.309.2">In practice, only a few activation functions are common, including</span><a id="_idIndexMarker440" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.310.1"> the </span><em class="italic"><span class="kobospan" id="kobo.311.1">ReLU function</span></em><span class="kobospan" id="kobo.312.1">, which returns 0 for negative or zero input values and the input value for positive </span><span><span class="kobospan" id="kobo.313.1">input values.</span></span></p>
<p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.314.1">Convolution layers</span></em><span class="kobospan" id="kobo.315.1">, also </span><a id="_idIndexMarker441" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.316.1">commonly used in hidden layers, apply a filter function (typically a kernel) to the input layer to transform it through the defined kernel function. </span><span class="kobospan" id="kobo.316.2">Typically, a convolution layer will reduce the dimensionality of the matrix or tensor, so zero padding to maintain dimensionality may be used to avoid the problem of transforming layers in a way that limits the number of layers possible given the dimensionality of the output. </span><span class="kobospan" id="kobo.316.3">For small datasets, such as the one we consider, this is not necessarily a problem, as shallow networks tend to be the only stable GNNs that we can train given the limited </span><span><span class="kobospan" id="kobo.317.1">data size.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.318.1">The theory of building effective architectures is beyond the scope of this book, and readers interested in DL who do not have a background can obtain this knowledge by reading through the references provided at the end of </span><span><span class="kobospan" id="kobo.319.1">this chapter.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.320.1">Once an architecture is defined (either through expert guessing or, again, grid search to optimize architecture), we must fit the parameters connecting nodes in each layer and across</span><a id="_idIndexMarker442" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.321.1"> layers (called </span><em class="italic"><span class="kobospan" id="kobo.322.1">weights</span></em><span class="kobospan" id="kobo.323.1">). </span><span class="kobospan" id="kobo.323.2">There are many options to do this, and it’s possible to define custom fitting algorithms. </span><span class="kobospan" id="kobo.323.3">However, we’ll focus on the two most common options within the PyTorch package used in our example: </span><strong class="bold"><span class="kobospan" id="kobo.324.1">Adam optimizers</span></strong><span class="kobospan" id="kobo.325.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.326.1">stochastic gradient </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.327.1">descent</span></strong></span><span><span class="kobospan" id="kobo.328.1"> (</span></span><span><strong class="bold"><span class="kobospan" id="kobo.329.1">SGD</span></strong></span><span><span class="kobospan" id="kobo.330.1">).</span></span></p>
<p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.331.1">SGD</span></em><span class="kobospan" id="kobo.332.1"> fits </span><a id="_idIndexMarker443" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.333.1">weights</span><a id="_idIndexMarker444" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.334.1"> between nodes within and across layers by exploring the</span><a id="_idIndexMarker445" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.335.1"> gradient function defined on the NN in much the way that gradient boosting fits a linear regression model. </span><span class="kobospan" id="kobo.335.2">A </span><em class="italic"><span class="kobospan" id="kobo.336.1">learning rate</span></em><span class="kobospan" id="kobo.337.1"> is defined to control the</span><a id="_idIndexMarker446" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.338.1"> exploration of the gradient function. </span><span class="kobospan" id="kobo.338.2">A steeper learning rate fits a model more quickly but may not find global minima or maxima. </span><span class="kobospan" id="kobo.338.3">One caveat of SGD is that the algorithm can get stuck in local optima, resulting in lower accuracies of results than what is possible given the input data, mapping functions between layers, and the outcome data. </span><span class="kobospan" id="kobo.338.4">It also tends to be slower, requiring more algorithm iterations and potentially more processing power to even fit </span><span><span class="kobospan" id="kobo.339.1">the model.</span></span></p>
<p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.340.1">Adam optimizers</span></em><span class="kobospan" id="kobo.341.1"> allow for </span><a id="_idIndexMarker447" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.342.1">flexible learning rates for different weights between nodes, leading to faster model fits and avoiding local optima by allowing the learning rate to adjust to the local gradient landscape. </span><span class="kobospan" id="kobo.342.2">Adam also allows for decay rates, further customizing local learning of weights. </span><span class="kobospan" id="kobo.342.3">Many Adam optimizers have evolved since the initial Adam optimizer was developed, and it’s likely more will be developed for GNNs and other DL architectures. </span><span class="kobospan" id="kobo.342.4">One drawback is that Adam optimizers tend to be memory-intensive. </span><span class="kobospan" id="kobo.342.5">When training large GNNs, it may be preferable to use SGD to avoid memory issues </span><span><span class="kobospan" id="kobo.343.1">during training.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.344.1">In practice, it’s difficult to know which optimizer is best for fitting the weights of the defined architecture, and grid search, again, is typically employed for industry GNN models to optimize this choice. </span><span class="kobospan" id="kobo.344.2">Once a fitting algorithm is selected, a predefined (again, usually optimized by grid search) number of iterations is run, or the algorithm runs until meeting a stopping criterion. </span><span class="kobospan" id="kobo.344.3">Adam optimizers tend to converge more quickly than SGD optimizers, but performance can vary depending on the data </span><span><span class="kobospan" id="kobo.345.1">and architecture.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.346.1">Now that we understand a bit about the building blocks of GNNs, let’s explore an example using an </span><a id="_idIndexMarker448" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.347.1">open source sports network consisting of students assigned</span><a id="_idIndexMarker449" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.348.1"> to two different teachers (our outcome </span><span><span class="kobospan" id="kobo.349.1">of interest).</span></span></p>
<h2 id="_idParaDest-125" class="calibre7"><a id="_idTextAnchor125" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.350.1">Example GNN classifying the Karate Network dataset</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.351.1">For our</span><a id="_idIndexMarker450" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.352.1"> example, we’ll predict</span><a id="_idIndexMarker451" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.353.1"> vertex-level attributes</span><a id="_idIndexMarker452" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.354.1"> in a common open source network: Zachary’s </span><strong class="source-inline"><span class="kobospan" id="kobo.355.1">Karate Network</span></strong><span class="kobospan" id="kobo.356.1"> dataset. </span><span class="kobospan" id="kobo.356.2">This dataset consists of 34 individuals connected by 78 edges in a karate training network who ended up splitting between an administrator and one of the instructors when a conflict between the administrator and instructor occurred. </span><span class="kobospan" id="kobo.356.3">One of the primary tasks for vertex classification and learning problems on this network is to predict which individuals ended up siding with which person in the conflict (the administrator or the instructor). </span><span class="kobospan" id="kobo.356.4">We will predict vertex labels through a semi-supervised GNN </span><span><span class="kobospan" id="kobo.357.1">model approach.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.358.1">We’ll first install the needed packages and import our dataset. </span><span class="kobospan" id="kobo.358.2">If you don’t have the necessary packages installed, please install them on your machine prior to running our code. </span><span class="kobospan" id="kobo.358.3">We’ve provided this step as an option in </span><span><strong class="source-inline"><span class="kobospan" id="kobo.359.1">Script 9.2</span></strong></span><span><span class="kobospan" id="kobo.360.1">:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.361.1">
#install packages if you have not installed them on your machine
#!pip install dgl
#!pip install torch
#import packages
import dgl
import dgl.data
import torch
import torch.nn as nn
import torch.nn.functional as F
import itertools
from dgl.nn import SAGEConv
#import Karate Club dataset with instructor/administrator labels
dataset = dgl.data.KarateClubDataset()
num_classes = dataset.num_classes
g = dataset[0]</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.362.1">To embed our vertex data, we’ll use PyTorch’s default embedding algorithm with a dimensionality of </span><strong class="source-inline"><span class="kobospan" id="kobo.363.1">6</span></strong><span class="kobospan" id="kobo.364.1">. </span><span class="kobospan" id="kobo.364.2">Anything that is in the </span><strong class="source-inline"><span class="kobospan" id="kobo.365.1">4</span></strong><span class="kobospan" id="kobo.366.1">-</span><strong class="source-inline"><span class="kobospan" id="kobo.367.1">6</span></strong><span class="kobospan" id="kobo.368.1"> dimension range should work reasonably well, given the</span><a id="_idIndexMarker453" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.369.1"> size of </span><a id="_idIndexMarker454" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.370.1">our </span><a id="_idIndexMarker455" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.371.1">network. </span><span class="kobospan" id="kobo.371.2">Let’s add to </span><strong class="source-inline"><span class="kobospan" id="kobo.372.1">Script 9.2</span></strong><span class="kobospan" id="kobo.373.1"> to embed </span><span><span class="kobospan" id="kobo.374.1">our vertices:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.375.1">
#embed vertices with a dimension of 6
vert_em = nn.Embedding(g.number_of_nodes(),6)
inputs = vert_em.weight
nn.init.xavier_uniform_(inputs)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.376.1">This piece of the script should output embedding vectors for each vertex in our network. </span><span class="kobospan" id="kobo.376.2">Now that we have our vertices embedded, we can create our labels. </span><span class="kobospan" id="kobo.376.3">Given that we wish to demonstrate a semi-supervised approach, we’ll feed our network information on six vertices (</span><strong class="source-inline"><span class="kobospan" id="kobo.377.1">1</span></strong><span class="kobospan" id="kobo.378.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.379.1">3</span></strong><span class="kobospan" id="kobo.380.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.381.1">5</span></strong><span class="kobospan" id="kobo.382.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.383.1">12</span></strong><span class="kobospan" id="kobo.384.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.385.1">15</span></strong><span class="kobospan" id="kobo.386.1">, and </span><strong class="source-inline"><span class="kobospan" id="kobo.387.1">32</span></strong><span class="kobospan" id="kobo.388.1">). </span><span class="kobospan" id="kobo.388.2">You can play around with this part of the script to see how fewer or more vertices impact the performance and stability of our chosen architecture. </span><span class="kobospan" id="kobo.388.3">Let’s add the label information by adding to </span><span><strong class="source-inline"><span class="kobospan" id="kobo.389.1">Script 9.2</span></strong></span><span><span class="kobospan" id="kobo.390.1">:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.391.1">
#obtain labels and denote available labels for GNN learning
#(here: 1, 3, 5, 12, 15, 32)
labels = g.ndata['label']
labeled_nodes = [1, 3, 5, 12, 15, 32]</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.392.1">Next, we’ll need to build our GNN architecture and define training parameters. </span><span class="kobospan" id="kobo.392.2">Many of the papers and tutorials on GNNs using this dataset employ a very shallow network architecture and Adam optimizers. </span><span class="kobospan" id="kobo.392.3">For the sake of comparison and demonstration of other options for building GNNs, we’ll use two hidden layers instead of one (including convolution layers coupled with ReLU functions), employ small layers (eight and six nodes, respectively for hidden layers), an SGD fitting algorithm (with a learning rate of </span><strong class="source-inline"><span class="kobospan" id="kobo.393.1">0.01</span></strong><span class="kobospan" id="kobo.394.1"> and a momentum driving the algorithm of </span><strong class="source-inline"><span class="kobospan" id="kobo.395.1">0.8</span></strong><span class="kobospan" id="kobo.396.1">, which is close to the default value), and </span><strong class="source-inline"><span class="kobospan" id="kobo.397.1">990</span></strong><span class="kobospan" id="kobo.398.1"> iterations. </span><span class="kobospan" id="kobo.398.2">Many examples that exist online use Adam optimizers and a single hidden layer with more nodes than our architecture, allowing for fewer training iterations. </span><span class="kobospan" id="kobo.398.3">However, for larger network vertex-label prediction problems, a more complex architecture is likely to perform better, so we will show a way to include more hidden layers and a way to use a different fitting algorithm than Adam. </span><span class="kobospan" id="kobo.398.4">Let’s define our architecture </span><a id="_idIndexMarker456" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.399.1">and fit our </span><a id="_idIndexMarker457" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.400.1">weights</span><a id="_idIndexMarker458" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.401.1"> by adding to </span><span><strong class="source-inline"><span class="kobospan" id="kobo.402.1">Script 9.2</span></strong></span><span><span class="kobospan" id="kobo.403.1">:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.404.1">
#build a three-layer GraphSAGE model
class GraphSAGE(nn.Module):
    def __init__(self, in_feats, h_feats1, h_feats2,
        num_classes):
        super(GraphSAGE, self).__init__()
        self.conv1 = SAGEConv(in_feats, h_feats1, 'mean')
        self.conv2 = SAGEConv(h_feats1, h_feats2, 'mean')
        self.conv3 = SAGEConv(h_feats2, num_classes,'mean')
    def forward(self, g, in_feat):
        h = self.conv1(g, in_feat)
        h = F.relu(h)
        h = self.conv2(g, h)
        h = F.relu(h)
        h = self.conv3(g, h)
        return h
#6 embedding dimensions as input,
#a hidden layers of 8 and 6 nodes, and 2 classes to output
net = GraphSAGE(6,8,6,2)
#GNN training parameters
optimizer=torch.optim.SGD(
    itertools.chain(
        net.parameters(), vert_em.parameters()),
        lr=0.01, momentum=0.8)
all_logits = []
#train GNN
for e in range(990):
    logits = net(g, inputs)
    logp = F.log_softmax(logits, 1)
    loss = F.nll_loss(logp[labeled_nodes],labels[labeled_nodes])
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    all_logits.append(logits.detach())
    if e % 90 == 0:
        print('In epoch {}, loss: {}'.format(e, loss))</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.405.1">You should see </span><a id="_idIndexMarker459" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.406.1">the loss function (logistic regression link function here) that decreases across iterations as your output. </span><span class="kobospan" id="kobo.406.2">Typical accuracies from GNN architectures fall into the 95%-100% range. </span><span class="kobospan" id="kobo.406.3">Because this dataset is small and our architecture is large, your output accuracies may vary quite a bit between runs of the algorithm. </span><span class="kobospan" id="kobo.406.4">This has to do with random sampling within the fitting steps of the algorithm and the coarseness of the underlying gradient landscape. </span><span class="kobospan" id="kobo.406.5">Let’s add to </span><strong class="source-inline"><span class="kobospan" id="kobo.407.1">Script 9.2</span></strong><span class="kobospan" id="kobo.408.1"> to find </span><span><span class="kobospan" id="kobo.409.1">our accuracy:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.410.1">
#obtain accuracy statistics
pred = torch.argmax(logits, axis=1)
print('Accuracy',(pred == labels).sum().item() / len(pred))</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.411.1">Our run of the algorithm gives an accuracy of ~97% in this run of the algorithm. </span><span class="kobospan" id="kobo.411.2">That is on par with the performance of other GNN architectures. </span><span class="kobospan" id="kobo.411.3">However, don’t be surprised if your accuracy is significantly lower in one or more runs of the script, as we don’t have a large enough sample size to fit this type of architecture. </span><span class="kobospan" id="kobo.411.4">Changing the embedding dimension, architecture, and training algorithm parameters will impact accuracy, and interested readers are encouraged to revise </span><strong class="source-inline"><span class="kobospan" id="kobo.412.1">Script 9.2</span></strong><span class="kobospan" id="kobo.413.1"> as a way to see how different choices impact accuracy and stability </span><span><span class="kobospan" id="kobo.414.1">of fit.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.415.1">In general, GNN classifiers work much better and show better stability on larger networks and with more labels fed into semi-supervised usage. </span><span class="kobospan" id="kobo.415.2">The Zachary Karate Network dataset</span><a id="_idIndexMarker460" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.416.1"> is </span><a id="_idIndexMarker461" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.417.1">small </span><a id="_idIndexMarker462" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.418.1">enough that other methods are recommended to classify the network. </span><span class="kobospan" id="kobo.418.2">However, learning labels on a huge social network (such as those found on social media platforms) or a large geographic network (such as a United States city network with connections defined by roads connecting cities larger than 50,000 people) would result in a more stable GNN solution, and it would be possible to create a very deep architecture. </span><span class="kobospan" id="kobo.418.3">However, to fit these large models, we often need a cloud computing platform, as the large datasets and large number of iterations can be difficult on </span><span><span class="kobospan" id="kobo.419.1">a laptop.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.420.1">GNNs have shown great promise in network-based classification problems in many different fields, and it is likely that they will continue to evolve and solve pressing problems with large networks and collections of networks. </span><span class="kobospan" id="kobo.420.2">However, cloud computing solutions are often needed, and this requires expertise working with data and Python notebook solutions on the </span><a id="_idIndexMarker463" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.421.1">cloud computing </span><a id="_idIndexMarker464" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.422.1">platform used to store data and fit </span><span><span class="kobospan" id="kobo.423.1">the GNN.</span></span></p>
<h1 id="_idParaDest-126" class="calibre5"><a id="_idTextAnchor126" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.424.1">Summary</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.425.1">In this chapter, we considered several use cases of ML algorithms on network datasets. </span><span class="kobospan" id="kobo.425.2">This included UL on a friendship network through fitting k-means and spectral clustering. </span><span class="kobospan" id="kobo.425.3">We considered k-means clustering on both the original dataset of activities in which individuals participated and the original dataset, with added network metrics to improve clustering accuracy. </span><span class="kobospan" id="kobo.425.4">We then turned to SL and SSL on networks and collections of networks through a type of DL algorithm called GNNs. </span><span class="kobospan" id="kobo.425.5">We accurately predicted the labels of individuals in Zachary’s Karate Network dataset through a shallow GNN and compared results with other existing solutions to this network classification problem. </span><span class="kobospan" id="kobo.425.6">In </span><a href="B21087_10.xhtml#_idTextAnchor128" class="pcalibre calibre6 pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.426.1">Chapter 10</span></em></span></a><span class="kobospan" id="kobo.427.1">, we'll mine educational data for causal relationships using network tools related to </span><span><span class="kobospan" id="kobo.428.1">conditional probability.</span></span></p>
<h1 id="_idParaDest-127" class="calibre5"><a id="_idTextAnchor127" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.429.1">References</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.430.1">Acharya, D. </span><span class="kobospan" id="kobo.430.2">B., &amp; Zhang, H. </span><span class="kobospan" id="kobo.430.3">(2021). </span><em class="italic"><span class="kobospan" id="kobo.431.1">Weighted Graph Nodes Clustering via Gumbel Softmax</span></em><span class="kobospan" id="kobo.432.1">. </span><span class="kobospan" id="kobo.432.2">arXiv </span><span><span class="kobospan" id="kobo.433.1">preprint arXiv:2102.10775.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.434.1">Bongini, P., Bianchini, M., &amp; Scarselli, F. </span><span class="kobospan" id="kobo.434.2">(2021). </span><span class="kobospan" id="kobo.434.3">Molecular generative graph neural networks for drug discovery. </span><em class="italic"><span class="kobospan" id="kobo.435.1">Neurocomputing, </span></em><span><em class="italic"><span class="kobospan" id="kobo.436.1">450, 242-252.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.437.1">Fan, W., Ma, Y., Li, Q., He, Y., Zhao, E., Tang, J., &amp; Yin, D. </span><span class="kobospan" id="kobo.437.2">(2019, May). </span><span class="kobospan" id="kobo.437.3">Graph neural networks for social recommendation. </span><em class="italic"><span class="kobospan" id="kobo.438.1">In The World Wide Web Conference (</span></em><span><em class="italic"><span class="kobospan" id="kobo.439.1">pp. </span><span class="kobospan" id="kobo.439.2">417-426).</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.440.1">Hartigan, J. </span><span class="kobospan" id="kobo.440.2">A., &amp; Wong, M. </span><span class="kobospan" id="kobo.440.3">A. </span><span class="kobospan" id="kobo.440.4">(1979). </span><span class="kobospan" id="kobo.440.5">Algorithm AS 136: A k-means clustering algorithm. </span><em class="italic"><span class="kobospan" id="kobo.441.1">Journal of the Royal Statistical Society. </span><span class="kobospan" id="kobo.441.2">Series C (Applied Statistics), </span></em><span><em class="italic"><span class="kobospan" id="kobo.442.1">28(1), 100-108.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.443.1">Imambi, S., Prakash, K. </span><span class="kobospan" id="kobo.443.2">B., &amp; Kanagachidambaresan, G. </span><span class="kobospan" id="kobo.443.3">R. </span><span class="kobospan" id="kobo.443.4">(2021). </span><span class="kobospan" id="kobo.443.5">PyTorch. </span><em class="italic"><span class="kobospan" id="kobo.444.1">Programming with TensorFlow: Solution for Edge Computing </span></em><span><em class="italic"><span class="kobospan" id="kobo.445.1">Applications, 87-104.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.446.1">Kumar, V. </span><span class="kobospan" id="kobo.446.2">(2020). </span><em class="italic"><span class="kobospan" id="kobo.447.1">An Investigation Into Graph Neural Networks (Doctoral dissertation, Trinity College </span></em><span><em class="italic"><span class="kobospan" id="kobo.448.1">Dublin, Ireland).</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.449.1">Labonne, M. </span><span class="kobospan" id="kobo.449.2">(2023). </span><em class="italic"><span class="kobospan" id="kobo.450.1">Hands-On Graph Neural Networks Using Python: Practical techniques and architectures for building powerful graph and deep learning apps with PyTorch. </span><span class="kobospan" id="kobo.450.2">Packt </span></em><span><em class="italic"><span class="kobospan" id="kobo.451.1">Publishing Ltd.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.452.1">Liang, F., Qian, C., Yu, W., Griffith, D., &amp; Golmie, N. </span><span class="kobospan" id="kobo.452.2">(2022). </span><span class="kobospan" id="kobo.452.3">Survey of graph neural networks and applications. </span><em class="italic"><span class="kobospan" id="kobo.453.1">Wireless Communications and Mobile </span></em><span><em class="italic"><span class="kobospan" id="kobo.454.1">Computing, 2022.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.455.1">Mantzaris, A. </span><span class="kobospan" id="kobo.455.2">V., Chiodini, D., &amp; Ricketson, K. </span><span class="kobospan" id="kobo.455.3">(2021). </span><span class="kobospan" id="kobo.455.4">Utilizing the simple graph convolutional neural network as a model for simulating influence spread in networks. </span><em class="italic"><span class="kobospan" id="kobo.456.1">Computational Social Networks, </span></em><span><em class="italic"><span class="kobospan" id="kobo.457.1">8, 1-17.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.458.1">Min, S., Gao, Z., Peng, J., Wang, L., Qin, K., &amp; Fang, B. </span><span class="kobospan" id="kobo.458.2">(2021). </span><span class="kobospan" id="kobo.458.3">STGSN—A Spatial–Temporal Graph Neural Network framework for time-evolving social networks. </span><em class="italic"><span class="kobospan" id="kobo.459.1">Knowledge-Based Systems, </span></em><span><em class="italic"><span class="kobospan" id="kobo.460.1">214, 106746.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.461.1">Ng, A., Jordan, M., &amp; Weiss, Y. </span><span class="kobospan" id="kobo.461.2">(2001). </span><span class="kobospan" id="kobo.461.3">On spectral clustering: Analysis and an algorithm. </span><em class="italic"><span class="kobospan" id="kobo.462.1">Advances in neural information processing </span></em><span><em class="italic"><span class="kobospan" id="kobo.463.1">systems, 14.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.464.1">Scarselli, F., Gori, M., Tsoi, A. </span><span class="kobospan" id="kobo.464.2">C., Hagenbuchner, M., &amp; Monfardini, G. </span><span class="kobospan" id="kobo.464.3">(2008). </span><span class="kobospan" id="kobo.464.4">The graph neural network model. </span><em class="italic"><span class="kobospan" id="kobo.465.1">IEEE transactions on neural networks, </span></em><span><em class="italic"><span class="kobospan" id="kobo.466.1">20(1), 61-80.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.467.1">Wieder, O., Kohlbacher, S., Kuenemann, M., Garon, A., Ducrot, P., Seidel, T., &amp; Langer, T. </span><span class="kobospan" id="kobo.467.2">(2020). </span><span class="kobospan" id="kobo.467.3">A compact review of molecular property prediction with graph neural networks. </span><em class="italic"><span class="kobospan" id="kobo.468.1">Drug Discovery Today: Technologies, </span></em><span><em class="italic"><span class="kobospan" id="kobo.469.1">37, 1-12.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.470.1">Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., &amp; Philip, S. </span><span class="kobospan" id="kobo.470.2">Y. </span><span class="kobospan" id="kobo.470.3">(2020). </span><span class="kobospan" id="kobo.470.4">A comprehensive survey on graph neural networks. </span><em class="italic"><span class="kobospan" id="kobo.471.1">IEEE transactions on neural networks and learning systems, </span></em><span><em class="italic"><span class="kobospan" id="kobo.472.1">32(1), 4-24.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.473.1">Zachary, W. </span><span class="kobospan" id="kobo.473.2">W. </span><span class="kobospan" id="kobo.473.3">(1977). </span><span class="kobospan" id="kobo.473.4">An information flow model for conflict and fission in small groups. </span><em class="italic"><span class="kobospan" id="kobo.474.1">Journal of Anthropological Research, </span></em><span><em class="italic"><span class="kobospan" id="kobo.475.1">33(4), 452-473.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.476.1">Zhang, L., Xu, J., Pan, X., Ye, J., Wang, W., Liu, Y., &amp; Wei, Q. </span><span class="kobospan" id="kobo.476.2">(2023). </span><span class="kobospan" id="kobo.476.3">Visual analytics of route recommendation for tourist evacuation based on graph neural network. </span><em class="italic"><span class="kobospan" id="kobo.477.1">Scientific Reports, </span></em><span><em class="italic"><span class="kobospan" id="kobo.478.1">13(1), 17240.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.479.1">Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., ... </span><span class="kobospan" id="kobo.479.2">&amp; Sun, M. </span><span class="kobospan" id="kobo.479.3">(2020). </span><em class="italic"><span class="kobospan" id="kobo.480.1">Graph neural networks: A review of methods and applications. </span><span class="kobospan" id="kobo.480.2">AI open, </span></em><span><em class="italic"><span class="kobospan" id="kobo.481.1">1, 57-81.</span></em></span></p>
</div>
</body></html>