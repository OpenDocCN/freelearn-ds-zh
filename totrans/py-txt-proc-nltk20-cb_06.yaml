- en: Chapter 6. Transforming Chunks and Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Filtering insignificant words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correcting verb forms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swapping verb phrases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swapping noun cardinals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swapping infinitive phrases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singularizing plural nouns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaining chunk transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting a chunk tree to text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flattening a deep tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a shallow tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting tree nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you know how to get chunks/phrases from a sentence, what do you do
    with them? This chapter will show you how to do various transforms on both chunks
    and trees. The chunk transforms are for grammatical correction and rearranging
    phrases without loss of meaning. The tree transforms give you ways to modify and
    flatten deep parse trees.
  prefs: []
  type: TYPE_NORMAL
- en: The functions detailed in these recipes modify data, as opposed to learning
    from it. That means it's not safe to apply them indiscriminately. A thorough knowledge
    of the data you want to transform, along with a few experiments, should help you
    decide which functions to apply and when.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever the term **chunk** is used in this chapter, it could refer to an actual
    chunk extracted by a chunker, or it could simply refer to a short phrase or sentence
    in the form of a list of tagged words. What's important in this chapter is what
    you can do with a chunk, not where it came from.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering insignificant words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many of the most commonly used words are insignificant when it comes to discerning
    the meaning of a phrase. For example, in the phrase "the movie was terrible",
    the most *significant* words are "movie" and "terrible", while "the" and "was"
    are almost useless. You could get the same meaning if you took them out, such
    as "movie terrible" or "terrible movie". Either way, the sentiment is the same.
    In this recipe, we'll learn how to remove the insignificant words, and keep the
    significant ones, by looking at their part-of-speech tags.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we need to decide which part-of-speech tags are significant and which
    are not. Looking through the `treebank` corpus for `stopwords` yields the following
    table of insignificant words and tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Word | Tag |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| a | DT |'
  prefs: []
  type: TYPE_TB
- en: '| all | PDT |'
  prefs: []
  type: TYPE_TB
- en: '| an | DT |'
  prefs: []
  type: TYPE_TB
- en: '| and | CC |'
  prefs: []
  type: TYPE_TB
- en: '| or | CC |'
  prefs: []
  type: TYPE_TB
- en: '| that | WDT |'
  prefs: []
  type: TYPE_TB
- en: '| the | DT |'
  prefs: []
  type: TYPE_TB
- en: Other than CC, all the tags end with DT. This means we can filter out insignificant
    words by looking at the tag's suffix.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In `transforms.py` there is a function called `filter_insignificant()`. It takes
    a single chunk, which should be a list of tagged words, and returns a new chunk
    without any insignificant tagged words. It defaults to filtering out any tags
    that end with DT or CC.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now we can use it on the part-of-speech tagged version of "the terrible movie".
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the word "the" is eliminated from the chunk.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`filter_insignificant()` iterates over the tagged words in the chunk. For each
    tag, it checks if that tag ends with any of the `tag_suffixes`. If it does, then
    the tagged word is skipped. However if the tag is ok, then the tagged word is
    appended to a new good chunk that is returned.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The way `filter_insignificant()` is defined, you can pass in your own tag suffixes
    if DT and CC are not enough, or are incorrect for your case. For example, you
    might decide that possessive words and pronouns such as "you", "your", "their",
    and "theirs" are no good but DT and CC words are ok. The tag suffixes would then
    be PRP and PRP$. Following is an example of this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Filtering insignificant words can be a good complement to stopword filtering
    for purposes such as search engine indexing, querying, and text classification.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe is analogous to the *Filtering stopwords in a tokenized sentence*
    recipe in [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"),
    *Tokenizing Text and WordNet Basics*.
  prefs: []
  type: TYPE_NORMAL
- en: Correcting verb forms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's fairly common to find incorrect verb forms in real-world language. For
    example, the correct form of "is our children learning?" is "are our children
    learning?". The verb "is" should only be used with singular nouns, while "are"
    is for plural nouns, such as "children". We can correct these mistakes by creating
    verb correction mappings that are used depending on whether there's a plural or
    singular noun in the chunk.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first need to define the verb correction mappings in `transforms.py`. We'll
    create two mappings, one for plural to singular, and another for singular to plural.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Each mapping has a tagged verb that maps to another tagged verb. These initial
    mappings cover the basics of mapping, is to are, was to were, and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In `transforms.py` there is a function called `correct_verbs()`. Pass it a chunk
    with incorrect verb forms, and you'll get a corrected chunk back. It uses a helper
    function `first_chunk_index()` to search the chunk for the position of the first
    tagged word where `pred` returns `True`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: When we call it on a part-of-speech tagged "is our children learning" chunk,
    we get back the correct form, "are our children learning".
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can also try this with a singular noun and an incorrect plural verb.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this case, "were" becomes "was" because "child" is a singular noun.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `correct_verbs()` function starts by looking for a verb in the chunk. If
    no verb is found, the chunk is returned with no changes. Once a verb is found,
    we keep the verb, its tag, and its index in the chunk. Then we look on either
    side of the verb to find the nearest noun, starting on the right, and only looking
    to the left if no noun is found on the right. If no noun is found at all, the
    chunk is returned as is. But if a noun is found, then we lookup the correct verb
    form depending on whether or not the noun is plural.
  prefs: []
  type: TYPE_NORMAL
- en: Recall from [Chapter 4](ch04.html "Chapter 4. Part-of-Speech Tagging"), *Part-of-Speech
    Tagging*, that plural nouns are tagged with NNS, while singular nouns are tagged
    with NN. This means we can check the plurality of a noun by seeing if its tag
    ends with S. Once we get the corrected verb form, it is inserted into the chunk
    to replace the original verb form.
  prefs: []
  type: TYPE_NORMAL
- en: To make searching through the chunk easier, we define a function called `first_chunk_index()`.
    It takes a chunk, a `lambda` predicate, the starting index, and a step increment.
    The predicate function is called with each tagged word until it returns `True`.
    If it never returns `True`, then `None` is returned. The starting index defaults
    to zero and the step increment to one. As you'll see in upcoming recipes, we can
    search backwards by overriding `start` and setting `step` to -1\. This small utility
    function will be a key part of subsequent transform functions.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next four recipes all make use of `first_chunk_index()` to perform chunk
    transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Swapping verb phrases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Swapping the words around a verb can eliminate the passive voice from particular
    phrases. For example, "the book was great" can be transformed into "the great
    book".
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In `transforms.py` there is a function called `swap_verb_phrase()`. It swaps
    the right-hand side of the chunk with the left-hand side, using the verb as the
    *pivot* point. It uses the `first_chunk_index()` function defined in the previous
    recipe to find the verb to pivot around.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now we can see how it works on the part-of-speech tagged phrase "the book was
    great".
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The result is "great the book". This phrase clearly isn't grammatically correct,
    so read on to learn how to fix it.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using `first_chunk_index()` from the previous recipe, we start by finding the
    first matching verb that is not a gerund (a word that ends in "ing") tagged with
    VBG. Once we've found the verb, we return the chunk with the right side before
    the left, and remove the verb.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason we don''t want to pivot around a gerund is that gerunds are commonly
    used to describe nouns, and pivoting around one would remove that description.
    Here''s an example where you can see how not pivoting around a gerund is a good
    thing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: If we had pivoted around the gerund, the result would be "book is fantastic
    this", and we'd lose the gerund "gripping".
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Filtering insignificant words makes the final result more readable. By filtering
    either before or after `swap_verb_phrase()`, we get "fantastic gripping book"
    instead of "fantastic this gripping book".
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Either way, we get a shorter grammatical chunk with no loss of meaning.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous recipe defines `first_chunk_index()`, which is used to find the
    verb in the chunk.
  prefs: []
  type: TYPE_NORMAL
- en: Swapping noun cardinals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a chunk, a **cardinal** word—tagged as CD—refers to a number, such as "10".
    These cardinals often occur before or after a noun. For normalization purposes,
    it can be useful to always put the cardinal before the noun.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The function `swap_noun_cardinal()` is defined in `transforms.py`. It swaps
    any cardinal that occurs immediately after a noun with the noun, so that the cardinal
    occurs immediately before the noun.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Let's try it on a date, such as "Dec 10", and another common phrase "the top
    10".
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The result is that the numbers are now in front of the noun, creating "10 Dec"
    and "the 10 top".
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We start by looking for a CD tag in the chunk. If no CD is found, or if the
    CD is at the beginning of the chunk, then the chunk is returned as is. There must
    also be a noun immediately before the CD. If we do find a CD with a noun preceding
    it, then we swap the noun and cardinal in place.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Correcting verb forms* recipe defines the `first_chunk_index()` function,
    used to find tagged words in a chunk.
  prefs: []
  type: TYPE_NORMAL
- en: Swapping infinitive phrases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An infinitive phrase has the form "A of B", such as "book of recipes". These
    can often be transformed into a new form while retaining the same meaning, such
    as "recipes book".
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An infinitive phrase can be found by looking for a word tagged with IN. The
    function `swap_infinitive_phrase()`, defined in `transforms.py`, will return a
    chunk that swaps the portion of the phrase after the IN word with the portion
    before the IN word.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The function can now be used to transform "book of recipes" into "recipes book".
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This function is similar to the `swap_verb_phrase()` function described in
    the *Swapping verb phrases* recipe. The `inpred lambda` is passed to `first_chunk_index()`
    to look for a word whose tag is IN. Next, `nnpred` is used to find the first noun
    that occurs before the IN word, so we can insert the portion of the chunk after
    the IN word between the noun and the beginning of the chunk. A more complicated
    example should demonstrate this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We don't want the result to be "recipes delicious book". Instead, we want to
    insert "recipes" before the noun "book", but after the adjective "delicious".
    Hence, the need to find the `nnidx` occurring before the `inidx`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You''ll notice that the `inpred lambda` checks to make sure the word is not
    "like". That''s because "like" phrases must be treated differently, as transforming
    them the same way will result in an ungrammatical phrase. For example, "tastes
    like chicken" should not be transformed into "chicken tastes":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next recipe, we'll learn how to transform "recipes book" into the more
    normal form "recipe book".
  prefs: []
  type: TYPE_NORMAL
- en: Singularizing plural nouns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in the previous recipe, the transformation process can result in phrases
    such as "recipes book". This is a NNS followed by an NN, when a more proper version
    of the phrase would be "recipe book", which is an NN followed by another NN. We
    can do another transform to correct these improper plural nouns.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`transforms.py` defines a function called `singularize_plural_noun()`, which
    will de-pluralize a plural noun (tagged with NNS) that is followed by another
    noun.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Using it on "recipes book", we get the more correct form, "recipe book".
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We start by looking for a plural noun with the tag NNS. If found, and if the
    next word is a noun (determined by making sure the tag starts with NN), then we
    de-pluralize the plural noun by removing an "s" from the right side of both the
    tag and the word.
  prefs: []
  type: TYPE_NORMAL
- en: The tag is assumed to be capitalized, so an uppercase "S" is removed from the
    right side of the tag, while a lowercase "s" is removed from the right side of
    the word.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous recipe shows how a transformation can result in a plural noun followed
    by a singular noun, though this could also occur naturally in real-world text.
  prefs: []
  type: TYPE_NORMAL
- en: Chaining chunk transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The transform functions defined in the previous recipes can be chained together
    to normalize chunks. The resulting chunks are often shorter with no loss of meaning.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In `transforms.py` is the function `transform_chunk()`. It takes a single chunk
    and an optional list of transform functions. It calls each transform function
    on the chunk, one at a time, and returns the final chunk.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Using it on the phrase "the book of recipes is delicious", we get "delicious
    recipe book":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `transform_chunk()` function defaults to chaining the following functions
    in order:'
  prefs: []
  type: TYPE_NORMAL
- en: '`filter_insignificant()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`swap_verb_phrase()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`swap_infinitive_phrase()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`singularize_plural_noun()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each function transforms the chunk that results from the previous function,
    starting with the original chunk.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The order in which you apply transform functions can be significant. Experiment
    with your own data to determine which transforms are best, and in which order
    they should be applied.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can pass `trace=1` into `transform_chunk()` to get an output at each step.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This shows you the result of each transform function, which is then passed in
    to the next transform function until a final chunk is returned.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The transform functions used were defined in the previous recipes of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Converting a chunk tree to text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At some point, you may want to convert a `Tree` or sub-tree back to a sentence
    or chunk string. This is mostly straightforward, except when it comes to properly
    outputting punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll use the first `Tree` of the `treebank_chunk` as our example. The obvious
    first step is to join all the words in the tree with a space.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the punctuation isn't quite right. The commas and period are
    treated as individual words, and so get the surrounding spaces as well. We can
    fix this using regular expression substitution. This is implemented in the `chunk_tree_to_sent()`
    function found in `transforms.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this function results in a much cleaner sentence, with no space before
    each punctuation mark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To correct the extra spaces in front of the punctuation, we create a regular
    expression `punct_re` that will match a space followed by any of the known punctuation
    characters. We have to escape both '.' and '?' with a '\' since they are special
    characters. The punctuation is surrounded by parenthesis so we can use the matched
    group for substitution.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have our regular expression, we define `chunk_tree_to_sent()`, whose
    first step is to join the words by a concatenation character that defaults to
    a space. Then we can call `re.sub()` to replace all the punctuation matches with
    just the punctuation group. This eliminates the space in front of the punctuation
    characters, resulting in a more correct string.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can simplify this function a little by using `nltk.tag.untag()` to get words
    from the tree's leaves, instead of using our own list comprehension.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `nltk.tag.untag()` function was covered at the end of the *Default tagging*
    recipe in [Chapter 4](ch04.html "Chapter 4. Part-of-Speech Tagging"), *Part-of-Speech
    Tagging*.
  prefs: []
  type: TYPE_NORMAL
- en: Flattening a deep tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some of the included corpora contain parsed sentences, which are often deep
    trees of nested phrases. Unfortunately, these trees are too deep to use for training
    a chunker, since IOB tag parsing is not designed for nested chunks. To make these
    trees usable for chunker training, we must flatten them.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''re going to use the first parsed sentence of the `treebank` corpus as our
    example. Here''s a diagram showing how deeply nested this tree is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/3609_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You may notice that the part-of-speech tags are part of the tree structure,
    instead of being included with the word. This will be handled next using the `Tree.pos()`
    method, which was designed specifically for combining words with pre-terminal
    `Tree` nodes such as part-of-speech tags.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In `transforms.py` there is a function named `flatten_deeptree()`. It takes
    a single `Tree` and will return a new `Tree` that keeps only the lowest level
    trees. It uses a helper function `flatten_childtrees()` to do most of the work.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use it on the first parsed sentence of the `treebank` corpus to get
    a flatter tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a much flatter `Tree` that only includes NP phrases. Words that
    are not part of a NP phrase are separated. This flatter tree is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/3609_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This `Tree` is quite similar to the first chunk `Tree` from the `treebank_chunk`
    corpus. The main difference is that the rightmost NP `Tree` is separated into
    two sub-trees in the previous diagram, one of them named NP-TMP.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first tree from `treebank_chunk` is shown as follows for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/3609_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The solution is composed of two functions: `flatten_deeptree()` returns a new
    `Tree` from the given tree by calling `flatten_childtrees()` on each of the given
    tree''s children.'
  prefs: []
  type: TYPE_NORMAL
- en: '`flatten_childtrees()` is a recursive function that drills down into the `Tree`
    until it finds child trees whose `height()` is equal to or less than three. A
    `Tree` whose `height()` is less than three looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![How it works...](img/3609_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: These short trees are converted into lists of tuples using the `pos()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Trees whose `height()` is equal to three are the lowest level trees that we''re
    interested in keeping. These trees look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![How it works...](img/3609_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When we call `pos()` on that tree, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The recursive nature of `flatten_childtrees()` eliminates all trees whose height
    is greater than three.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flattening a deep `Tree` allows us to call `nltk.chunk.util.tree2conlltags()`
    on the flattened `Tree`, a necessary step to train a chunker. If you try to call
    this function before flattening the `Tree`, you get a `ValueError` exception.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'However, after flattening there''s no problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Being able to flatten trees, opens up the possibility of training a chunker
    on corpora consisting of deep parse trees.
  prefs: []
  type: TYPE_NORMAL
- en: CESS-ESP and CESS-CAT treebank
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `cess_esp` and `cess_cat` corpora have parsed sentences, but no chunked
    sentences. In other words, they have deep trees that must be flattened in order
    to train a chunker. In fact, the trees are so deep that a diagram can't be shown,
    but the flattening can be demonstrated by showing the `height()` of the tree before
    and after flattening.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Training a tagger-based chunker* recipe in [Chapter 5](ch05.html "Chapter 5. Extracting
    Chunks"), *Extracting Chunks* covers training a chunker using IOB tags.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a shallow tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we flattened a deep `Tree` by only keeping the lowest
    level sub-trees. In this recipe, we'll keep only the highest level sub-trees instead.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll be using the first parsed sentence from the `treebank` corpus as our
    example. Recall from the previous recipe that the sentence `Tree` looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/3609_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `shallow_tree()` function defined in `transforms.py` eliminates all the
    nested sub-trees, keeping only the top tree nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Using it on the first parsed sentence in `treebank` results in a `Tree` with
    only two sub-trees.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visually and programmatically see the difference, as shown in the following
    diagram and code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/3609_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: As in the previous recipe, the height of the new tree is three so it can be
    used for training a chunker.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `shallow_tree()` function iterates over each of the top-level sub-trees
    in order to create new child trees. If the `height()` of a sub-tree is less than
    three, then that sub-tree is replaced by a list of its part-of-speech tagged children.
    All other sub-trees are replaced by a new `Tree` whose children are the part-of-speech
    tagged leaves. This eliminates all nested sub-trees while retaining the top-level
    sub-trees.
  prefs: []
  type: TYPE_NORMAL
- en: This function is an alternative to `flatten_deeptree()` from the previous recipe,
    for when you want to keep the higher level tree nodes and ignore the lower level
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous recipe covers how to flatten a `Tree` and keep the lowest level
    sub-trees, as opposed to keeping the highest level sub-trees.
  prefs: []
  type: TYPE_NORMAL
- en: Converting tree nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you've seen in previous recipes, parse trees often have a variety of `Tree`
    node types that are not present in chunk trees. If you want to use the parse trees
    to train a chunker, then you'll probably want to reduce this variety by converting
    some of these tree nodes to more common node types.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we have to decide what `Tree` nodes need to be converted. Let''s take
    a look at that first `Tree` again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/3609_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Immediately you can see that there are two alternative NP sub-trees: NP-SBJ
    and NP-TMP. Let''s convert both of those to NP. The mapping will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Original Node | New Node |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| NP-SBJ | NP |'
  prefs: []
  type: TYPE_TB
- en: '| NP-TMP | NP |'
  prefs: []
  type: TYPE_TB
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In `transforms.py` there is a function `convert_tree_nodes()`. It takes two
    arguments: the `Tree` to convert, and a node conversion `mapping`. It returns
    a new `Tree` with all matching nodes replaced based on the values in the `mapping`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Using the mapping table shown earlier, we can pass it in as a `dict` to `convert_tree_nodes()`
    and convert the first parsed sentence from `treebank`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following diagram, you can see that the NP-* sub-trees have been replaced
    with NP sub-trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/3609_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`convert_tree_nodes()` recursively converts every child sub-tree using the
    `mapping`. The `Tree` is then rebuilt with the converted nodes and children until
    the entire `Tree` has been converted.'
  prefs: []
  type: TYPE_NORMAL
- en: The result is a brand new `Tree` instance with new sub-trees whose nodes have
    been converted.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous two recipes cover different methods of flattening a parse `Tree`,
    both of which can produce sub-trees that may require mapping before using them
    to train a chunker. Chunker training is covered in the *Training a tagger-based
    chunker* recipe in [Chapter 5](ch05.html "Chapter 5. Extracting Chunks"), *Extracting
    Chunks*.
  prefs: []
  type: TYPE_NORMAL
