- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Spark DataFrames and their Operations
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark DataFrame及其操作
- en: In this chapter, we will learn about a few different APIs in Spark and talk
    about their features. We will also get started with Spark’s DataFrame operations
    and look at different data viewing and manipulation techniques such as filtering,
    adding, renaming, and dropping columns available in Spark.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习Spark中的一些不同API，并讨论它们的特性。我们还将开始学习Spark的DataFrame操作，并查看不同的数据查看和操作技术，例如过滤、添加、重命名和删除Spark中可用的列。
- en: 'We will cover these concepts under the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在以下主题下介绍这些概念：
- en: The Spark DataFrame API
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark DataFrame API
- en: Creating DataFrames
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建DataFrame
- en: Viewing DataFrames
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看DataFrame
- en: Manipulating DataFrames
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作DataFrame
- en: Aggregating DataFrames
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合DataFrame
- en: By the end of this chapter, you will know how to work with PySpark DataFrames.
    You’ll also discover various data manipulation techniques and see how you can
    view data after manipulating it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解如何使用PySpark DataFrame。你还将发现各种数据操作技术，并了解在操作数据后如何查看数据。
- en: Getting Started in PySpark
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark入门
- en: In the previous chapters, we discussed that Spark primarily uses four languages,
    which are Scala, Python, R, and SQL. When any of these languages are used, the
    underlying execution engine is the same. This provides the necessary unification
    we talked about in [*Chapter 2*](B19176_02.xhtml#_idTextAnchor030). This means
    that developers can use any language of their choice and can also switch between
    different APIs in applications.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了Spark主要使用四种语言，即Scala、Python、R和SQL。当使用这些语言中的任何一种时，底层的执行引擎是相同的。这为我们之前在[*第二章*](B19176_02.xhtml#_idTextAnchor030)中提到的必要统一性提供了支持。这意味着开发者可以使用他们选择的任何语言，也可以在应用程序中在不同API之间切换。
- en: For the context of this book, we’re going to focus on Python as the primary
    language. Spark used with Python is called **PySpark**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本书的上下文，我们将重点关注Python作为主要语言。与Python一起使用的Spark被称为**PySpark**。
- en: Let’s get started with the installation of Spark.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始安装Spark。
- en: Installing Spark
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Spark
- en: To get started with Spark, you would have to first install it on your computer.
    There are a few ways to install Spark. We will focus on just one in this section.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用Spark，你首先需要在你的电脑上安装它。安装Spark有几种方法。在本节中，我们将只关注其中一种。
- en: 'PySpark provides **pip** installation from **PyPI**. You can install it as
    follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark提供了从**PyPI**的**pip**安装。你可以按照以下方式安装它：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once Spark is installed, you will need to create a Spark session.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Spark安装完成，你需要创建一个Spark会话。
- en: Creating a Spark session
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建Spark会话
- en: 'Once you have installed Spark on your system, you can get started with creating
    a Spark session. A Spark session is the entry point of any Spark application.
    To create a Spark session, you will initialize it in the following way:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你在系统中安装了Spark，你就可以开始创建Spark会话。Spark会话是任何Spark应用程序的入口点。要创建Spark会话，你需要按照以下方式初始化它：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When you are running your code in the Spark shell, the Spark session is automatically
    created for you so you don’t have to manually execute this code to create a Spark
    session. This session is usually created in a variable called `spark`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在Spark shell中运行代码时，Spark会话会自动为你创建，因此你不需要手动执行此代码来创建Spark会话。这个会话通常在一个名为`spark`的变量中创建。
- en: It is important to note that we can only create a single spark session at any
    given time. Duplicating a Spark session is not possible in Spark.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，在任何给定时间，我们只能创建一个Spark会话。在Spark中无法复制Spark会话。
- en: Now, let’s take a look at different data APIs in Spark DataFrames.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看Spark DataFrames中的不同数据API。
- en: Dataset API
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dataset API
- en: Dataset is a newer interface added to *Spark 1.6*. It is a distributed collection
    of data. The Dataset API is available in Java and Scala, but not in Python and
    R. The Dataset API uses **Resilient Distributed Datasets** (**RDDs**) and hence
    provides additional features of RDDs, such as fixed typing. It also uses Spark
    SQL’s optimized engine for faster queries.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Dataset是添加到*Spark 1.6*的新接口。它是一个数据分布式集合。Dataset API在Java和Scala中可用，但在Python和R中不可用。Dataset
    API使用**Resilient Distributed Datasets**（**RDDs**），因此提供了RDD的附加功能，如固定类型。它还使用Spark
    SQL的优化引擎以实现更快的查询。
- en: Since a lot of the data engineering and data science community is already familiar
    with Python and uses it extensively for data architectures in production, PySpark
    also provides an equivalent API for DataFrames for this purpose. Let’s take a
    look at it in the next section.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多数据工程和数据科学社区已经熟悉 Python 并在生产中的数据架构中广泛使用它，PySpark 也为此提供了等效的 DataFrame API。让我们在下一节中看看它。
- en: DataFrame API
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrame API
- en: The motivation of Spark DataFrames comes from Pandas DataFrames in Python. A
    DataFrame is essentially a set of rows and columns. You can think of it like a
    table where you have table headers as column names and below these headers are
    data arranged accordingly. This table-like format has been part of computations
    for a long time in tools such as relational databases and comma-separated files.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Spark DataFrame 的动机来自 Python 中的 Pandas DataFrame。DataFrame 实质上是一组行和列。你可以将其想象成一个表格，其中表格头作为列名，在这些头下面是相应排列的数据。这种类似表格的格式已经在诸如关系数据库和逗号分隔文件等工具的计算中存在很长时间了。
- en: Spark’s DataFrame API is built on top of RDDs. The underlying structures to
    store the data are still RDDs but DataFrames create an abstraction on top of the
    RDDs to hide its complexity. Just as RDDs are lazily evaluated and are immutable,
    DataFrames are also evaluated lazily and are immutable. If you can remember from
    previous chapters, lazy evaluation gives Spark performance gains and optimization
    by running the computations only when needed. This also gives Spark a large number
    of optimizations in its DataFrames by planning how to best compute the operations.
    The computations start when an action is called on a DataFrame. There are a lot
    of different ways to create Spark DataFrames. We will learn about some of those
    in this chapter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的 DataFrame API 是建立在 RDD 之上的。存储数据的底层结构仍然是 RDD，但 DataFrame 在 RDD 上创建了一个抽象，以隐藏其复杂性。正如
    RDD 是惰性评估且不可变的一样，DataFrame 也是惰性评估且不可变的。如果你还记得前面的章节，惰性评估通过仅在需要时运行计算为 Spark 带来了性能提升和优化。这也使得
    Spark 在其 DataFrame 中通过规划如何最佳地计算操作而拥有大量的优化。计算是在对 DataFrame 调用动作时开始的。有无数种不同的方法可以创建
    Spark DataFrame。我们将在本章中学习其中的一些。
- en: Let’s take a look at what a DataFrame is in Spark.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在 Spark 中 DataFrame 是什么。
- en: Creating DataFrame operations
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 DataFrame 操作
- en: As we have already discussed, DataFrames are the main building blocks of Spark
    data. They consist of rows and column data structures.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经讨论过的，DataFrame 是 Spark 数据的主要构建块。它们由行和列数据结构组成。
- en: DataFrames in PySpark are created using the `pyspark.sql.SparkSession.createDataFrame`
    function. You can use lists, lists of lists, tuples, dictionaries, Pandas DataFrames,
    RDDs, and `pyspark.sql.Rows` to create DataFrames.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 中的 DataFrame 是通过 `pyspark.sql.SparkSession.createDataFrame` 函数创建的。你可以使用列表、列表的列表、元组、字典、Pandas
    DataFrame、RDD 和 `pyspark.sql.Rows` 来创建 DataFrame。
- en: Spark DataFrames also has an argument named **schema** that specifies the schema
    of the DataFrame. You can either choose to specify the schema explicitly or let
    Spark infer the schema from the DataFrame itself. If you don’t specify this argument
    in the code, Spark will infer the schema on its own.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Spark DataFrame 也有一个名为 **schema** 的参数，用于指定 DataFrame 的模式。你可以选择显式指定模式，或者让 Spark
    从 DataFrame 本身推断模式。如果你在代码中没有指定此参数，Spark 将会自行推断模式。
- en: There are different ways to create DataFrames in Spark. Some of them are explained
    in the following sections.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中创建 DataFrame 有不同的方法。其中一些将在以下章节中解释。
- en: Using a list of rows
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用数据行列表
- en: The first way to create DataFrames we see is by using rows of data. You can
    think of rows of data as lists. They would share common header values for each
    of the values in the list.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到创建 DataFrame 的第一种方法是通过使用数据行。你可以将数据行想象成列表。它们将为列表中的每个值共享共同的头值。
- en: 'Here’s the code to use when creating a new DataFrame using rows of data:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 创建新的 DataFrame 时使用数据行列表的代码如下：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As a result, you will see a DataFrame with our specified columns and their
    data types:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你会看到具有我们指定的列及其数据类型的 DataFrame：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, we’ll see how we can specify the schema for a Spark DataFrame explicitly.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看到如何可以显式指定 Spark DataFrame 的模式。
- en: Using a list of rows with schema
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用具有模式的行列表
- en: The schema of a DataFrame defines what would be the different data types present
    in each of the rows and columns of a DataFrame. Explicitly defining schema helps
    in cases where we want to enforce certain data types to our datasets.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 的模式定义了 DataFrame 的每一行和每一列中可能存在的不同数据类型。显式定义模式有助于我们想要强制某些数据类型到数据集的情况。
- en: 'Now, we will explicitly tell Spark which schema to use for the DataFrame that
    we’re creating. Notice that the majority of the code remains the same—we’re simply
    adding another argument named `schema` in the code for creating the DataFrame
    to explicitly tell which columns would have what kind of datatypes:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将明确告诉Spark我们创建的DataFrame应使用哪种模式。请注意，大部分代码保持不变——我们只是在创建DataFrame的代码中添加了一个名为`schema`的另一个参数，以明确指定哪些列将有什么样的数据类型：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As a result, you will see a DataFrame with our specified columns and their
    data types:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您将看到包含我们指定列及其数据类型的DataFrame：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, let’s take a look at how we can create DataFrames using Pandas DataFrames.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用Pandas DataFrame创建DataFrame。
- en: Using Pandas DataFrames
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Pandas DataFrame
- en: 'DataFrames can also be created using Pandas DataFrames. To achieve this, you
    would need to create a DataFrame in Pandas first. Once that is created, you would
    then convert that DataFrame to a PySpark DataFrame. The following code demonstrates
    this process:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame也可以使用Pandas DataFrame创建。为此，您首先需要在Pandas中创建一个DataFrame。一旦创建，然后将其转换为PySpark
    DataFrame。以下代码演示了此过程：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As a result, you will see a DataFrame with our specified columns and their
    data types:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您将看到包含我们指定列及其数据类型的DataFrame：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, let’s take a look at how we can create DataFrames using tuples.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用元组创建DataFrame。
- en: Using tuples
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用元组
- en: 'Another way to create DataFrames is through tuples. This means that we can
    create a tuple as a row and add each tuple as a separate row in the DataFrame.
    Each tuple would contain the data for each of the columns of the DataFrame. The
    following code demonstrates this:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 创建DataFrame的另一种方式是通过元组。这意味着我们可以创建一个元组作为一行，并将每个元组作为DataFrame中的单独一行添加。每个元组将包含DataFrame中每列的数据。以下代码演示了这一点：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As a result, you will see a DataFrame with our specified columns and their
    data types:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您将看到包含我们指定列及其数据类型的DataFrame：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, let’s take a look at different ways we can view the DataFrames in Spark
    and see the results of the DataFrames that we just created.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看在Spark中我们可以以不同的方式查看DataFrame，并查看我们刚刚创建的DataFrame的结果。
- en: How to view the DataFrames
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何查看DataFrame
- en: There are different statements in Spark to view data. The DataFrames that we
    created in the previous section through different methods all yield the same result
    as the DataFrame. Let’s look at a few different ways to view DataFrames.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中有不同的语句用于查看数据。我们在上一节中通过不同方法创建的DataFrame，其结果都与DataFrame相同。让我们看看几种不同的查看DataFrame的方法。
- en: Viewing DataFrames
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看DataFrame
- en: 'The first way to show a DataFrame is through the `DataFrame.show()` statement.
    Here’s an example:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 显示DataFrame的第一种方式是通过`DataFrame.show()`语句。以下是一个示例：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As a result, you will see a DataFrame with our specified columns and the data
    inside this DataFrame:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您将看到包含我们指定列及其数据类型的DataFrame：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can also select the total rows that can be viewed in a single statement.
    Let’s see how we can do that in the next topic.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在单个语句中选择要查看的总行数。让我们在下一个主题中看看如何做到这一点。
- en: Viewing top n rows
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看前n行
- en: We can also be selective in the number of rows that can be viewed in a single
    statement. We can control that using a parameter in `DataFrame.show()`. Here’s
    an example of looking at only the top two rows of the DataFrame.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在单个语句中指定要查看的行数。我们可以通过`DataFrame.show()`中的参数来控制这一点。以下是一个仅查看DataFrame前两行的示例。
- en: 'If you specify *n* to be a specific number, then only those sets of rows would
    be shown. Here’s an example:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您指定*n*为特定的数字，则只会显示那些行集。以下是一个示例：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As a result, you will see a DataFrame with its top two rows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您将看到包含其前两行的DataFrame：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Viewing DataFrame schema
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看DataFrame模式
- en: 'We can also choose to see the schema of the DataFrame using the `printSchema()`
    function:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以选择使用`printSchema()`函数查看DataFrame的模式：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As a result, you will see the schema of a DataFrame with our specified columns
    and their data types:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您将看到DataFrame的模式，包括我们指定的列及其数据类型：
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Viewing data vertically
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 垂直查看数据
- en: 'When the data becomes too long to fit into the screen, it’s sometimes useful
    to see the data in a vertical format instead of a horizontal table view. Here’s
    an example of how you can view the data in a vertical format:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据太长而无法适应屏幕时，以垂直格式查看数据而不是水平表格视图有时很有用。以下是一个示例，说明您如何以垂直格式查看数据：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As a result, you will see a DataFrame with our specified columns and their
    data but in a vertical format:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您将看到包含我们指定列及其数据但以垂直格式的DataFrame：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Viewing columns of data
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看数据列
- en: 'When we just need to view the columns that exist in a DataFrame, we would use
    the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们只需要查看DataFrame中存在的列时，我们会使用以下方法：
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As a result, you will see a list of the columns in the DataFrame:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您将看到一个DataFrame中列的列表：
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Viewing summary statistics
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看摘要统计信息
- en: 'Now, let’s take a look at how we can view the summary statistics of a DataFrame:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们如何查看DataFrame的摘要统计信息：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As a result, you will see a DataFrame with its summary statistics for each
    column defined:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您将看到一个DataFrame，其中定义了每列的摘要统计信息：
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now, let’s take a look at the collect statement.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下collect语句。
- en: Collecting the data
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集数据
- en: A collect statement is used when we want to get all the data that is being processed
    in different clusters back to the driver. When using a collect statement, we need
    to make sure that the driver has enough memory to hold the processed data. If
    the driver doesn’t have enough memory to hold the data, we will get out-of-memory
    errors.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要将不同集群中正在处理的所有数据收集回驱动器时，会使用collect语句。在使用collect语句时，我们需要确保驱动器有足够的内存来存储处理后的数据。如果驱动器没有足够的内存来存储数据，我们将遇到内存不足错误。
- en: 'This is how you show the collect statement:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是显示collect语句的方法：
- en: '[PRE22]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This statement will then show result as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此语句将显示如下结果：
- en: '[PRE23]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: There are a few ways to avoid out-of-memory errors. We will explore some of
    the options that avoid out-of-memory errors such as take, tail, and head statements.
    These statements return only a subset of the data and not all of the data in a
    DataFrame, therefore, they are very useful to inspect the data without having
    to lead all the data in driver memory.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以避免内存不足错误。我们将探讨一些避免内存不足错误的方法，例如take、tail和head语句。这些语句仅返回数据的一个子集，而不是DataFrame中的所有数据，因此，它们在无需将所有数据加载到驱动器内存中时非常有用。
- en: Now, let’s take a look at the take statement.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下take语句。
- en: Using take
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用take
- en: 'A take statement takes an argument for a number of elements to return from
    the top of a DataFrame. We will see how it is used in the following code example:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: take语句接受一个参数，用于从DataFrame顶部返回元素的数量。我们将在下面的代码示例中看到它是如何使用的：
- en: '[PRE24]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As a result, you will see a DataFrame with its top row:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您将看到一个包含其顶部行的DataFrame：
- en: '[PRE25]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In this example, we’re only returning the first element of the DataFrame by
    giving `1` as an argument value to the `take()` function. Therefore, only one
    row is returned in the result.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们通过将`1`作为`take()`函数的参数值来仅返回DataFrame的第一个元素。因此，结果中只返回了一行。
- en: Now, let’s take a look at the tail statement.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下tail语句。
- en: Using tail
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用tail
- en: 'The tail statement takes an argument for a number of elements to return from
    the bottom of a DataFrame. We will see how it is used in the following code example:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: tail语句接受一个参数，用于从DataFrame底部返回元素的数量。我们将在下面的代码示例中看到它是如何使用的：
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As a result, you will see a DataFrame with its last row of data:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您将看到一个包含其最后一行数据的DataFrame：
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In this example, we’re only returning the last element of the DataFrame by giving
    `1` as an argument value to the `tail()` function. Therefore, only one row is
    returned in the result.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们通过将`1`作为`tail()`函数的参数值来仅返回DataFrame的最后一个元素。因此，结果中只返回了一行。
- en: Now, let’s take a look at the head statement.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下head语句。
- en: Using head
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用head
- en: 'The head statement takes an argument for a number of elements to return from
    the top of a DataFrame. We will see how it is used in the following code example:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: head语句接受一个参数，用于从DataFrame顶部返回元素的数量。我们将在下面的代码示例中看到它是如何使用的：
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As a result, you will see a DataFrame with its top row of data:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您将看到一个包含其数据顶部行的DataFrame：
- en: '[PRE29]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In this example, we’re only returning the first element of the DataFrame by
    giving `1` as an argument value to the `head()` function. Therefore, only one
    row is returned in the result.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们通过将`1`作为`head()`函数的参数值来仅返回DataFrame的第一个元素。因此，结果中只返回了一行。
- en: Now, let’s take a look at how we can count the number of rows in a DataFrame.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们如何计算DataFrame中的行数。
- en: Counting the number of rows of data
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算数据行数
- en: 'When we just need to count the number of rows in a DataFrame, we would use
    the following:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们只需要计算DataFrame中的行数时，我们会使用以下方法：
- en: '[PRE30]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'As a result, you will see the total count of rows in a DataFrame:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您将看到一个DataFrame中的总行数：
- en: '[PRE31]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In PySpark, several methods are available for retrieving data from a DataFrame
    or RDD, each with its own characteristics and use cases. Here’s a summary of the
    major differences between take, collect, show, head, and tail that we used earlier
    in this section for data retrieval.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PySpark 中，有几种方法可以用于从 DataFrame 或 RDD 中检索数据，每种方法都有其自身的特性和用例。以下是我们在本节早期用于数据检索的
    `take`、`collect`、`show`、`head` 和 `tail` 之间主要差异的总结。
- en: take(n)
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: take(n)
- en: This function returns an array containing the first *n* elements from the DataFrame
    or RDD
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数返回一个包含 DataFrame 或 RDD 的前 *n* 个元素的数组
- en: It is useful for quickly inspecting a small subset of the data
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它用于快速检查数据的小子集
- en: It performs a lazy evaluation, meaning it only computes the required number
    of elements
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它执行惰性评估，这意味着它只计算所需数量的元素
- en: collect()
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: collect()
- en: This function retrieves all elements from the DataFrame or RDD and returns them
    as a list
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数从 DataFrame 或 RDD 中检索所有元素，并将它们作为列表返回
- en: It should be used with caution as it brings all data to the driver node, which
    can lead to out-of-memory errors for large datasets
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应谨慎使用，因为它会将所有数据带到驱动节点，这可能导致大型数据集出现内存不足错误
- en: It is suitable for small datasets or when working with aggregated results that
    fit into memory
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它适用于小型数据集或处理适合内存的聚合结果时
- en: show(n)
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: show(n)
- en: This function displays the first *n* rows of the DataFrame in a tabular format
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数以表格格式显示 DataFrame 的前 *n* 行
- en: It is primarily used for visual inspection of data during **exploratory data
    analysis** (**EDA**) or debugging
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它主要用于在 **探索性数据分析** （**EDA**） 或调试期间对数据进行视觉检查
- en: It provides a user-friendly display of data with column headers and formatting
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它以具有列标题和格式的用户友好的方式显示数据
- en: head(n)
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: head(n)
- en: This function returns the first *n* rows of the DataFrame as a list of `Row`
    objects
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数返回 DataFrame 的前 *n* 行，作为 `Row` 对象的列表
- en: It is similar to `take(n)` but it returns `Row` objects instead of simple values
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它与 `take(n)` 类似，但返回 `Row` 对象而不是简单值
- en: It is often used when you need access to specific column values while working
    with structured data
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当处理结构化数据时，需要访问特定列值时经常使用
- en: tail(n)
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: tail(n)
- en: This function returns the last *n* rows of the DataFrame
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数返回 DataFrame 的最后 *n* 行
- en: It is useful for examining the end of the dataset, especially in cases where
    data is sorted in descending order
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在检查数据集的末尾时很有用，尤其是在数据按降序排序的情况下
- en: It performs a more expensive operation compared to `head(n)` as it may involve
    scanning the entire dataset
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 `head(n)` 相比，它执行的操作更昂贵，因为它可能涉及扫描整个数据集
- en: In summary, `take` and `collect` are used to retrieve data elements, with `take`
    being more suitable for small subsets and `collect` for retrieving all data (with
    caution). `show` is used for visual inspection, `head` retrieves the first rows
    as `Row` objects, and `tail` retrieves the last rows of the dataset. Each method
    serves different purposes and should be chosen based on the specific requirements
    of the data analysis task.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，`take` 和 `collect` 用于检索数据元素，其中 `take` 更适合小子集，而 `collect` 用于检索所有数据（需谨慎）。`show`
    用于视觉检查，`head` 检索前几行作为 `Row` 对象，而 `tail` 检索数据集的最后几行。每种方法都有不同的用途，应根据数据分析任务的具体要求进行选择。
- en: When working with data in PySpark, sometimes, you will need to use some Python
    functions on the DataFrames. To achieve that, you will have to convert PySpark
    DataFrames to Pandas DataFrames. Now, let’s take a look at how we can convert
    a Pyspark DataFrame to a Pandas DataFrame.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当在 PySpark 中处理数据时，有时您需要在 DataFrames 上使用一些 Python 函数。为了实现这一点，您必须将 PySpark DataFrames
    转换为 Pandas DataFrames。现在，让我们看看如何将 PySpark DataFrame 转换为 Pandas DataFrame。
- en: Converting a PySpark DataFrame to a Pandas DataFrame
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 PySpark DataFrame 转换为 Pandas DataFrame
- en: At various times in your workflow, you will want to switch from a Pyspark DataFrame
    to a Pandas DataFrame. There are options to convert a PySpark DataFrame to a Pandas
    DataFrame. This option is `toPandas()`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的工作流程的各个阶段，您可能希望从 PySpark DataFrame 切换到 Pandas DataFrame。有选项可以将 PySpark DataFrame
    转换为 Pandas DataFrame。此选项是 `toPandas()`。
- en: One thing to note here is that Python inherently is not distributed. Therefore,
    when a PySpark DataFrame is converted to Pandas, the driver would need to collect
    all the data in its memory. We need to make sure that the driver’s memory is able
    to collect the data in itself. If the data is not able to fit in the driver’s
    memory, it will cause an out-of-memory error.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的是，Python 本身不是分布式的。因此，当 PySpark DataFrame 转换为 Pandas 时，驱动程序需要收集其内存中的所有数据。我们需要确保驱动程序的内存能够收集自身的数据。如果数据无法适应驱动程序的内存，将导致内存不足错误。
- en: 'Here’s an example to see how we can convert a PySpark DataFrame to a Pandas
    DataFrame:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例，说明如何将 PySpark DataFrame 转换为 Pandas DataFrame：
- en: '[PRE32]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As a result, you will see a DataFrame with our specified columns and their
    data types:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您将看到具有我们指定的列及其数据类型的 DataFrame：
- en: '|  | `col_1` | `col_2` | `col_3` | `col_4` | `col_5` |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | `col_1` | `col_2` | `col_3` | `col_4` | `col_5` |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| `0` | `100` | `200.0` | `String_test_1` | `2023-01-01` | `2023-01-01 12:00:00`
    |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| `0` | `100` | `200.0` | `String_test_1` | `2023-01-01` | `2023-01-01 12:00:00`
    |'
- en: '| `1` | `200` | `300.0` | `String_test_2` | `2023-02-01` | `2023-01-02 12:00:00`
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| `1` | `200` | `300.0` | `String_test_2` | `2023-02-01` | `2023-01-02 12:00:00`
    |'
- en: '| `2` | `300` | `400.0` | `String_test_3` | `2023-03-01` | `2023-01-03 12:00:00`
    |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| `2` | `300` | `400.0` | `String_test_3` | `2023-03-01` | `2023-01-03 12:00:00`
    |'
- en: 'Table 4.1: DataFrame with the columns and data types specified by us'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1：我们指定的列和数据类型的 DataFrame
- en: In the next section, we will learn about different data manipulation techniques.
    You will need to filter, slice, and dice the data based on different criteria
    for different purposes. Therefore, data manipulation is essential in working with
    data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将了解不同的数据操作技术。您将需要根据不同的目的根据不同的标准对数据进行筛选、切片和切块。因此，数据操作在处理数据时是必不可少的。
- en: How to manipulate data on rows and columns
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在行和列上操作数据
- en: In this section, we will learn how to do different data manipulation operations
    on Spark DataFrames rows and columns.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何在 Spark DataFrame 的行和列上执行不同的数据操作。
- en: We will start by looking at how we can select columns in a Spark DataFrame.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先看看如何在 Spark DataFrame 中选择列。
- en: Selecting columns
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择列
- en: 'We can use column functions for data manipulation at the column level in a
    Spark DataFrame. To select a column in a DataFrame, we would use the `select()`
    function like so:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 Spark DataFrame 中使用列函数在列级别进行数据操作。要选择 DataFrame 中的列，我们将使用 `select()` 函数如下：
- en: '[PRE33]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As a result, you will see only one column of the DataFrame with its data:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您将只看到 DataFrame 中的一个列及其数据：
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'There are some other ways to achieve the same result in PySpark as well. Some
    of those are demonstrated here:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PySpark 中也有其他一些方法可以达到相同的结果。其中一些在这里进行了演示：
- en: '[PRE35]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Once we select the required columns, there will be instances where you will
    need to add new columns to a DataFrame. We will now take a look at how we can
    create columns in a Spark DataFrame.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们选择了所需的列，就可能出现需要向 DataFrame 中添加新列的情况。现在我们将看看如何在 Spark DataFrame 中创建列。
- en: Creating columns
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建列
- en: 'We can use a `withColumn()` function to create a new column in a DataFrame.
    To create a new column, we would need to pass the column name and column values
    to fill the column with. In the following example, we’re creating a new column
    named `col_6` and putting a constant literal `A` in this column:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `withColumn()` 函数在 DataFrame 中创建新列。要创建新列，我们需要传递列名和列值以填充该列。在以下示例中，我们创建了一个名为
    `col_6` 的新列，并在该列中放置了一个常量字面量 `A`：
- en: '[PRE36]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'As a result, you will see a DataFrame with an additional column named `col_6`
    filled with multiple `A` instances:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您将看到具有一个名为 `col_6` 的附加列，该列填充了多个 `A` 实例：
- en: '![](img/B19176_04_01.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19176_04_01.jpg)'
- en: The `lit()` function is used to fill constant values in a column.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`lit()` 函数用于在列中填充常量值。'
- en: You can also delete columns that are no longer needed in a DataFrame. We will
    now take a look at how we can drop columns in a Spark DataFrame.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以删除 DataFrame 中不再需要的列。现在我们将看看如何在 Spark DataFrame 中删除列。
- en: Dropping columns
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除列
- en: 'If we need to drop a column from a Spark DataFrame, we would use the `drop()`
    function. We need to provide the name of the column to be dropped from the DataFrame.
    Here’s an example of how to use this function:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要从 Spark DataFrame 中删除列，我们将使用 `drop()` 函数。我们需要提供要删除的列的名称。以下是如何使用此函数的示例：
- en: '[PRE37]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'As a result, you will see that `col_5` is dropped from the DataFrame:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您将看到 DataFrame 中已删除 `col_5`：
- en: '[PRE38]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We have successfully dropped `col_5` from this DataFrame.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已成功从该 DataFrame 中删除了 `col_5`。
- en: 'You can also drop multiple columns in the same drop statement as well:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在同一个删除语句中删除多个列：
- en: '[PRE39]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Also note that if we drop a column that does not exist in the DataFrame, it
    will not result in any errors. The resulting DataFrame would remain as it is.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，如果我们删除 DataFrame 中不存在的列，不会产生任何错误。结果 DataFrame 将保持原样。
- en: Just like dropping columns, you can also update columns in Spark DataFrames.
    Now, we will take a look at how we can update columns in a Spark DataFrame.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 就像删除列一样，你还可以在 Spark DataFrame 中更新列。现在，我们将看看如何在 Spark DataFrame 中更新列。
- en: Updating columns
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新列
- en: 'Updating columns can also be done with the help of the `withColumn()` function
    in Spark. We need to provide the name of the column to be updated along with the
    updated value. Notice that we can also use this function to calculate some new
    values for the columns. Here’s an example:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 更新列也可以通过 Spark 中的 `withColumn()` 函数来完成。我们需要提供要更新的列的名称以及更新的值。注意，我们还可以使用此函数为列计算一些新值。以下是一个示例：
- en: '[PRE40]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This will give us the following updated frame:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下更新的框架：
- en: '[PRE41]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: One thing to note here is the use of the `col` function when updating the column.
    This function is used for column-wise operators. If we don’t use this function,
    our code will return an error.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的是，在更新列时使用 `col` 函数。此函数用于列操作。如果我们不使用此函数，我们的代码将返回错误。
- en: You don’t always have to update a column in a DataFrame if you only need to
    rename a column. Now, we will see how we can rename columns in a Spark DataFrame.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只需要重命名列，而不需要更新 DataFrame 中的列，你不必总是更新列。现在，我们将看看如何在 Spark DataFrame 中重命名列。
- en: Renaming columns
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重命名列
- en: 'For changing the name of a column, we would use the `withColumnRenamed()` function
    in Spark. We would need to provide the column name that needs to be changed along
    with the new column name. Here’s the code to illustrate this:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更改列名，我们会在 Spark 中使用 `withColumnRenamed()` 函数。我们需要提供需要更改的列名以及新的列名。以下是说明这一点的代码示例：
- en: '[PRE42]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'As a result, we’ll see the following change:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将看到以下变化：
- en: '[PRE43]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Notice that `col_3` is now called `string_col` after making the change.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，更改后 `col_3` 现在被称为 `string_col`。
- en: Now, let’s shift our focus to some data manipulation techniques in Spark DataFrames.
    You can have search-like functionality in Spark DataFrames for finding different
    values in a column. Now, let’s take a look at how we can find unique values in
    a column of a Spark DataFrame.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将注意力转向一些 Spark DataFrame 中的数据处理技术。你可以在 Spark DataFrame 中拥有类似搜索的功能，用于查找列中的不同值。现在，让我们看看如何在
    Spark DataFrame 的列中查找唯一值。
- en: Finding unique values in a column
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在列中查找唯一值
- en: 'Finding unique values is a very useful function that would give us distinct
    values in a column. For this purpose, we can use the `distinct()` function of
    the Spark DataFrame like so:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 查找唯一值是一个非常实用的函数，它将给我们列中的不同值。为此，我们可以使用 Spark DataFrame 的 `distinct()` 函数，如下所示：
- en: '[PRE44]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Here is the result:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE45]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We applied a `distinct` function on `col_6` to get all the unique values in
    this column. In our case, the column just had one distinct value, `A`, so that
    was shown.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `col_6` 上应用了 `distinct` 函数，以获取该列中所有唯一的值。在我们的例子中，该列只有一个不同的值，即 `A`，所以显示了它。
- en: 'We can also use it to find the count of distinct values in a given column.
    Here’s an example of how to use this function:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用它来查找给定列中不同值的数量。以下是如何使用此函数的示例：
- en: '[PRE46]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Here is the result:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE47]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In this example, we can see the total count of distinct values in `col_6`. Currently,
    it is the only type of distinct value present in this column, therefore, it returned
    `1`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以看到 `col_6` 中不同值的总数。目前，它是这个列中唯一的不同值类型，因此返回了 `1`。
- en: One other useful function in Spark data manipulation is changing the case of
    a column. Now, let’s take a look at how we can change the case of a column in
    a Spark DataFrame.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 数据操作中另一个有用的函数是更改列的大小写。现在，让我们看看如何在 Spark DataFrame 中更改列的大小写。
- en: Changing the case of a column
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更改列的大小写
- en: 'There is also a function that exists in Spark to change the case of a column.
    We don’t need to specify each value of the column separately to make use of the
    function. Once applied, the whole column’s values would change case. One such
    example is as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 中也存在一个用于更改列大小写的函数。我们不需要分别指定列的每个值来使用此函数。一旦应用，整个列的值都会更改大小写。以下是一个这样的例子：
- en: '[PRE48]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Here is the result:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '![](img/B19176_04_2.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19176_04_2.jpg)'
- en: In this example, we change the case of `string_col` to all caps. We need to
    assign this to a new column, so, we create a column called `upper_string_col`
    to store these upper-case values. Also, note that this column is not added to
    the original `data_df` because we did not save the results back in `data_df`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将 `string_col` 的字母大小写改为全部大写。我们需要将这个结果分配给一个新列，因此，我们创建了一个名为 `upper_string_col`
    的列来存储这些大写值。同时，请注意，这个列没有被添加到原始的 `data_df` 中，因为我们没有将结果保存回 `data_df`。
- en: A lot of times in data manipulation, we would need functions to filter DataFrames.
    We will now take a look at how we can filter data in a Spark DataFrame.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据处理中，很多时候我们需要函数来过滤 DataFrame。现在，我们将看看如何在 Spark DataFrame 中过滤数据。
- en: Filtering a DataFrame
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过滤 DataFrame
- en: 'Filtering a DataFrame means that we can get a subset of rows or columns from
    a DataFrame. There are different ways of filtering a Spark DataFrame. We will
    take a look at one example here:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤 DataFrame 意味着我们可以从 DataFrame 中获取行或列的子集。有几种不同的方法可以过滤 Spark DataFrame。在这里，我们将看看一个例子：
- en: '[PRE49]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Here is the result:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE50]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: In this example, we filter `data_df` to only include rows where the column value
    of `col_1` is equal to `100`. This criterion is met by only one row, therefore,
    a single row is returned in the resulting DataFrame.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们过滤 `data_df` 以仅包括 `col_1` 的列值等于 `100` 的行。只有一行满足这个标准，因此，结果 DataFrame
    中只返回一行。
- en: You can use this function to slice and dice your data in a number of different
    ways based on the requirements.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用此函数根据要求以多种方式对数据进行切片和切块。
- en: Since we are talking about data filtering, we can also filter data based on
    logical operators as well. We will now take a look at how we can use logical operators
    in DataFrames to filter data.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在讨论数据过滤，我们也可以根据逻辑运算符来过滤数据。现在，我们将看看如何在 DataFrames 中使用逻辑运算符来过滤数据。
- en: Logical operators in a DataFrame
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrame 中的逻辑运算符
- en: 'Another important set of operators that we can combine with filter operations
    in a DataFrame are the logical operators. These consist of AND and OR operators,
    amongst others. These are used to filter DataFrames based on complex conditions.
    Let’s take a look at how we use the AND operator here:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将逻辑运算符与 DataFrame 中的过滤操作结合使用，这是一组重要的运算符。这些包括 AND 和 OR 运算符等。它们用于根据复杂条件过滤
    DataFrame。让我们看看如何使用 AND 运算符：
- en: '[PRE51]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Here is the result:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE52]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: In this example, we’re trying to get the rows where the value of `col_1` is
    equal to `100` and the value of `col_6` is `A`. Currently, only one row fulfills
    this condition, therefore, one row is returned as a result.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们试图获取 `col_1` 的值为 `100` 且 `col_6` 的值为 `A` 的行。目前，只有一行满足这个条件，因此，只有一行作为结果返回。
- en: 'Now, let’s see how we can use the OR operator to combine conditions:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用 OR 运算符来组合条件：
- en: '[PRE53]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This statement will give the following result:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 此语句将给出以下结果：
- en: '[PRE54]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: In this example, we’re trying to get the rows where the value of `col_1` is
    equal to `100` or the value of `col_2` is equal to `300.0`. Currently, two rows
    fulfill this condition, therefore, they are returned as a result.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们试图获取 `col_1` 的值为 `100` 或 `col_2` 的值为 `300.0` 的行。目前，有两行满足这个条件，因此，它们被作为结果返回。
- en: In data filtering, there is another important function to find values in a list.
    Now, we will see how you use the `isin()` function in PySpark.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据过滤中，还有一个重要的函数用于在列表中查找值。现在，我们将看看如何在 PySpark 中使用 `isin()` 函数。
- en: Using isin()
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 isin()
- en: 'The `isin()` function is used to find values in a DataFrame column that exist
    in a list. To do this, we would create a list with some values in it. Once we
    have the list, then we would use the `isin()` function to see whether some of
    the values that are in the list exist in the DataFrame. Here’s an example to demonstrate
    this:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`isin()` 函数用于在 DataFrame 列中查找存在于列表中的值。为此，我们需要创建一个包含一些值的列表。一旦我们有了这个列表，我们就会使用
    `isin()` 函数来查看列表中的某些值是否存在于 DataFrame 中。以下是一个演示此功能的例子：'
- en: '[PRE55]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Here is the result:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE56]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: In this example, we see that the values `100` and `200` are present in the `data_df`
    DataFrame in two of its rows, therefore, both rows are returned.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以看到值 `100` 和 `200` 出现在 `data_df` DataFrame 的两行中，因此，这两行都被返回。
- en: We can also convert data types for different columns in Spark DataFrames. Now,
    let’s look at how we can convert different data types in Spark DataFrames.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在 Spark DataFrames 的不同列中转换数据类型。现在，让我们看看如何在 Spark DataFrames 中转换不同的数据类型。
- en: Datatype conversions
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据类型转换
- en: In this section, we’ll see different ways of converting data types in Spark
    DataFrame columns.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到在 Spark DataFrame 列中转换数据类型的不同方法。
- en: 'We will start by using the `cast` function in Spark. The following code illustrates
    this:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用 Spark 中的 `cast` 函数。以下代码说明了这一点：
- en: '[PRE57]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Here is the result:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE58]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: In the preceding code, we see that we’re changing the data types of two columns,
    namely `col_4` and `col_1`. First, we change `col_4` to string type. This column
    was previously a date column. Then, we change `col_1` to integer type from `long`.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们看到我们正在更改两个列的数据类型，即 `col_4` 和 `col_1`。首先，我们将 `col_4` 改为字符串类型。这个列之前是日期列。然后，我们将
    `col_1` 从 `long` 类型改为整数类型。
- en: 'Here is the schema of `data_df` for reference:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 `data_df` 的模式，仅供参考：
- en: '[PRE59]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We see that `col_1` and `col_4` were different data types.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到 `col_1` 和 `col_4` 是不同的数据类型。
- en: 'The next example of changing the data types of the columns is by using the
    `selectExpr()` function. The following code illustrates this:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例是通过使用 `selectExpr()` 函数来更改列的数据类型。以下代码说明了这一点：
- en: '[PRE60]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Here is the result:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE61]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: In the preceding code, we see that we’re changing the data types of two columns,
    namely `col_4` and `col_1`. First, we change `col_4` back to the `date` type.
    Then, we change `col_1` to the `long` type.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们看到我们正在更改两个列的数据类型，即 `col_4` 和 `col_1`。首先，我们将 `col_4` 改回 `date` 类型。然后，我们将
    `col_1` 改为 `long` 类型。
- en: 'The next example of changing the data types of the columns is by using SQL.
    The following code illustrates this:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例是通过使用 SQL 来更改列的数据类型。以下代码说明了这一点：
- en: '[PRE62]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Here is the result:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE63]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: In the preceding code, we see that we’re changing the data types of two columns,
    namely `col_4` and `col_1`. First, we use `createOrReplaceTempView()` to create
    a table named `CastExample`. Then, we use this table to change `col_4` back to
    the `date` type. Then, we change `col_1` to the `double` type.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们看到我们正在更改两个列的数据类型，即 `col_4` 和 `col_1`。首先，我们使用 `createOrReplaceTempView()`
    创建一个名为 `CastExample` 的表。然后，我们使用这个表将 `col_4` 改回 `date` 类型。然后，我们将 `col_1` 改为 `double`
    类型。
- en: In the data analysis world, working with null values is very valuable. Now,
    let’s take a look at how we can drop null values from a DataFrame.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析领域，处理空值非常有价值。现在，让我们看看我们如何从 DataFrame 中删除空值。
- en: Dropping null values from a DataFrame
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 DataFrame 中删除空值
- en: Sometimes, in the data, there exist null values that can make clean data messy.
    Dropping nulls is an essential exercise that a lot of data analysts and data engineers
    need to do. Pyspark provides us with relevant functions to do this.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，在数据中存在可能导致清洁数据变得混乱的空值。删除空值是许多数据分析师和数据工程师需要做的基本练习。PySpark 提供了相关的函数来完成这项工作。
- en: 'Let’s create another DataFrame called `salary_data` to show some of the next
    operations:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建另一个名为 `salary_data` 的 DataFrame 来展示一些后续操作：
- en: '[PRE64]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Here is the result:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE65]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now, let’s take a look at the `dropna()` function; this will help us drop null
    values from our DataFrame:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 `dropna()` 函数；这将帮助我们删除 DataFrame 中的空值：
- en: '[PRE66]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Here is the result:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE67]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: We see in the resulting DataFrame that rows with `Robert` and `Martin` are deleted
    from the new DataFrame when we use the `dropna()` function.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在结果 DataFrame 中，我们看到当使用 `dropna()` 函数时，带有 `Robert` 和 `Martin` 的行从新的 DataFrame
    中被删除。
- en: Deduplicating data is another useful technique that is often required in data
    analysis tasks. Now, let’s take a look at how we can drop duplicate values from
    a DataFrame.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 去重数据是数据分析任务中经常需要的一种有用技术。现在，让我们看看我们如何从 DataFrame 中删除重复值。
- en: Dropping duplicates from a DataFrame
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 DataFrame 中删除重复项
- en: 'Sometimes, in the data, there are redundant values present that would make
    clean data messy. Dropping these values might be needed in a lot of use cases.
    PySpark provides us with the `dropDuplicates()` function to do this. Here’s the
    code to illustrate this:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，在数据中存在冗余值，这会使清洁数据变得混乱。删除这些值可能在许多用例中是必要的。PySpark 提供了 `dropDuplicates()` 函数来执行此操作。以下是说明此操作的代码：
- en: '[PRE68]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Here is the result:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE69]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: We see in this example that employees named Michael are only shown once in the
    resulting DataFrame after we apply the `dropDuplicates()` function to the original
    DataFrame. This name and its corresponding values exist in the original DataFrame
    twice.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们看到在将 `dropDuplicates()` 函数应用于原始 DataFrame 后，名为 Michael 的员工在结果 DataFrame
    中只显示一次。这个名称及其对应值在原始 DataFrame 中出现了两次。
- en: Now that we have learned about different data filtering techniques, we will
    now see how we can aggregate data in Pyspark DataFrames.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了不同的数据过滤技术，接下来我们将看看如何在 PySpark DataFrame 中进行数据聚合。
- en: Using aggregates in a DataFrame
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 DataFrame 中使用聚合
- en: 'Some of the methods available in Spark for aggregating data are as follows:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 中用于聚合数据的一些方法如下：
- en: '`agg`'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`agg`'
- en: '`avg`'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`avg`'
- en: '`count`'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`count`'
- en: '`max`'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max`'
- en: '`mean`'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mean`'
- en: '`min`'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min`'
- en: '`pivot`'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pivot`'
- en: '`sum`'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sum`'
- en: We will see some of them in action in the following code examples.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下面的代码示例中看到一些实际操作。
- en: Average (avg)
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平均值（avg）
- en: 'In the following example, we see how to use aggregate functions in Spark. We
    will start by calculating the average of all the values in a column:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们看到如何在 Spark 中使用聚合函数。我们将从计算列中所有值的平均值开始：
- en: '[PRE70]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Here is the result:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE71]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: This example calculates the average of the salary column of the `salary_data`
    DataFrame. We have passed the `Salary` column to the `avg` function and it has
    calculated the average of that column for us.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例计算了 `salary_data` DataFrame 中薪水列的平均值。我们将 `Salary` 列传递给了 `avg` 函数，它为我们计算了该列的平均值。
- en: Now, let’s take a look at how to count different elements in a PySpark DataFrame.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何在 PySpark DataFrame 中计数不同的元素。
- en: Count
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计数
- en: 'In the following code example, we can see how you use aggregate functions in
    Spark:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码示例中，我们可以看到如何在 Spark 中使用聚合函数：
- en: '[PRE72]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Here is the result:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE73]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: This example calculates the total count of the values in the `Salary` column
    of the `salary_data` DataFrame. We have passed the `Salary` column to the `agg`
    function with `count` as its other parameter, and it has calculated the count
    of that column for us.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例计算了 `salary_data` DataFrame 中 `Salary` 列的值的总数。我们将 `Salary` 列传递给了 `agg` 函数，并将
    `count` 作为其另一个参数，它为我们计算了该列的计数。
- en: Now, let’s take a look at how to count distinct elements in a PySpark DataFrame.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何在 PySpark DataFrame 中计数不同的元素。
- en: Count distinct values
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计数独立值
- en: 'In the following example, we will look at how to count distinct elements in
    a PySpark DataFrame:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们将查看如何在 PySpark DataFrame 中计数不同的元素：
- en: '[PRE74]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Here is the result:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE75]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: This example calculates total distinct values in the salary column of the `salary_data`
    DataFrame. We have passed the `Salary` column to the `countDistinct` function
    and it has calculated the count of that column for us.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例计算了 `salary_data` DataFrame 中薪水列的总独立值。我们将 `Salary` 列传递给了 `countDistinct`
    函数，它为我们计算了该列的计数。
- en: Now, let’s take a look at how to find maximum values in a PySpark DataFrame.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何在 PySpark DataFrame 中查找最大值。
- en: Finding maximums (max)
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查找最大值（max）
- en: 'In the following code example, we will take a look at how to find maximum values
    in a column of a PySpark DataFrame:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码示例中，我们将查看如何在 PySpark DataFrame 的列中查找最大值：
- en: '[PRE76]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Here is the result:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE77]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: This example calculates the maximum value out of all the values in the `Salary`
    column of the `salary_data` DataFrame. We have passed the `Salary` column to the
    `agg` function with `max` as its other parameter, and it has calculated the maximum
    of that column for us.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例计算了 `salary_data` DataFrame 中 `Salary` 列的所有值的最大值。我们将 `Salary` 列传递给了 `agg`
    函数，并将 `max` 作为其另一个参数，它为我们计算了该列的最大值。
- en: Now, let’s take a look at how to get the sum of all elements in a PySpark DataFrame.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何在 PySpark DataFrame 中获取所有元素的总和。
- en: Sum
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总和
- en: 'In the following code example, we will look at how to sum all values in a PySpark
    DataFrames:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码示例中，我们将查看如何在 PySpark DataFrame 中求和所有值：
- en: '[PRE78]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Here is the result:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE79]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: This example calculates the sum of all the values in the `Salary` column of
    the `salary_data` DataFrame. We have passed the `Salary` column to the `agg` function
    with `sum` as its other parameter, and it has calculated the sum of that column
    for us.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例计算了 `salary_data` DataFrame 中 `Salary` 列所有值的总和。我们将 `Salary` 列传递给了 `agg`
    函数，并将 `sum` 作为其另一个参数，它为我们计算了该列的总和。
- en: Now, let’s take a look at how to sort data in a PySpark DataFrame.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何在 PySpark DataFrame 中排序数据。
- en: Sort data with OrderBy
  id: totrans-342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 OrderBy 排序数据
- en: 'In the following code example, we will look at how can we sort data in ascending
    order in a PySpark DataFrame:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码示例中，我们将查看如何在 PySpark DataFrame 中按升序排序数据：
- en: '[PRE80]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Here is the result:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE81]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: This example sorts the full DataFrame based on the values in the `Salary` column
    of the `salary_data` DataFrame. We have passed the `Salary` column to the `orderBy`
    function and it has sorted the DataFrame based on this column.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 本例根据`salary_data` DataFrame中`Salary`列的值对整个DataFrame进行排序。我们将`Salary`列传递给`orderBy`函数，它根据此列对DataFrame进行了排序。
- en: 'We can also sort the data in descending format by adding another function,
    `desc()`, to the original `orderBy` function. The following example illustrates
    this:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过向原始`orderBy`函数中添加另一个函数`desc()`来按降序格式对数据进行排序。以下示例说明了这一点：
- en: '[PRE82]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Here is the result:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE83]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: This example is sorting the full DataFrame in descending order based on the
    values in the `Salary` column of the `salary_data` DataFrame. We have passed the
    `Salary` column to the `orderBy` function with `desc()` as an additional function
    call and it has sorted the DataFrame in descending order based on this column.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 本例中，根据`salary_data` DataFrame中`Salary`列的值按降序对整个DataFrame进行排序。我们将`Salary`列传递给`orderBy`函数，并附加了`desc()`函数调用，它根据此列对DataFrame进行了降序排序。
- en: Summary
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Over the course of this chapter, we have learned how to manipulate data in Spark
    DataFrames.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何在Spark DataFrame中操作数据。
- en: We talked about the Spark DataFrame API and what different data types are in
    Spark. We also learned how to create DataFrames in Spark and how we can view these
    DataFrames once they’ve been created. Finally, we learned about different data
    manipulation and data aggregation functions.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了Spark DataFrame API以及Spark中的不同数据类型。我们还学习了如何在Spark中创建DataFrame以及如何创建后查看这些DataFrame。最后，我们学习了不同的数据操作和数据聚合函数。
- en: In the next chapter, we will cover some advanced operations in Spark with respect
    to data manipulation.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍Spark中与数据处理相关的一些高级操作。
- en: Sample question
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 样题
- en: 1\. Which of the following operations will trigger evaluation?
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 以下哪个操作会触发评估？
- en: '`DataFrame.filter()`'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame.filter()`'
- en: '`DataFrame.distinct()`'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame.distinct()`'
- en: '`DataFrame.intersect()`'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame.intersect()`'
- en: '`DataFrame.join()`'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame.join()`'
- en: '`DataFrame.count()`'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame.count()`'
- en: Answer
  id: totrans-364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 答案
- en: E
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: E
