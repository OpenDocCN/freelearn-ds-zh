- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark DataFrames and their Operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn about a few different APIs in Spark and talk
    about their features. We will also get started with Spark’s DataFrame operations
    and look at different data viewing and manipulation techniques such as filtering,
    adding, renaming, and dropping columns available in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover these concepts under the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The Spark DataFrame API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viewing DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulating DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregating DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will know how to work with PySpark DataFrames.
    You’ll also discover various data manipulation techniques and see how you can
    view data after manipulating it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started in PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we discussed that Spark primarily uses four languages,
    which are Scala, Python, R, and SQL. When any of these languages are used, the
    underlying execution engine is the same. This provides the necessary unification
    we talked about in [*Chapter 2*](B19176_02.xhtml#_idTextAnchor030). This means
    that developers can use any language of their choice and can also switch between
    different APIs in applications.
  prefs: []
  type: TYPE_NORMAL
- en: For the context of this book, we’re going to focus on Python as the primary
    language. Spark used with Python is called **PySpark**.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started with the installation of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get started with Spark, you would have to first install it on your computer.
    There are a few ways to install Spark. We will focus on just one in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'PySpark provides **pip** installation from **PyPI**. You can install it as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once Spark is installed, you will need to create a Spark session.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Spark session
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you have installed Spark on your system, you can get started with creating
    a Spark session. A Spark session is the entry point of any Spark application.
    To create a Spark session, you will initialize it in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: When you are running your code in the Spark shell, the Spark session is automatically
    created for you so you don’t have to manually execute this code to create a Spark
    session. This session is usually created in a variable called `spark`.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that we can only create a single spark session at any
    given time. Duplicating a Spark session is not possible in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at different data APIs in Spark DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dataset is a newer interface added to *Spark 1.6*. It is a distributed collection
    of data. The Dataset API is available in Java and Scala, but not in Python and
    R. The Dataset API uses **Resilient Distributed Datasets** (**RDDs**) and hence
    provides additional features of RDDs, such as fixed typing. It also uses Spark
    SQL’s optimized engine for faster queries.
  prefs: []
  type: TYPE_NORMAL
- en: Since a lot of the data engineering and data science community is already familiar
    with Python and uses it extensively for data architectures in production, PySpark
    also provides an equivalent API for DataFrames for this purpose. Let’s take a
    look at it in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The motivation of Spark DataFrames comes from Pandas DataFrames in Python. A
    DataFrame is essentially a set of rows and columns. You can think of it like a
    table where you have table headers as column names and below these headers are
    data arranged accordingly. This table-like format has been part of computations
    for a long time in tools such as relational databases and comma-separated files.
  prefs: []
  type: TYPE_NORMAL
- en: Spark’s DataFrame API is built on top of RDDs. The underlying structures to
    store the data are still RDDs but DataFrames create an abstraction on top of the
    RDDs to hide its complexity. Just as RDDs are lazily evaluated and are immutable,
    DataFrames are also evaluated lazily and are immutable. If you can remember from
    previous chapters, lazy evaluation gives Spark performance gains and optimization
    by running the computations only when needed. This also gives Spark a large number
    of optimizations in its DataFrames by planning how to best compute the operations.
    The computations start when an action is called on a DataFrame. There are a lot
    of different ways to create Spark DataFrames. We will learn about some of those
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at what a DataFrame is in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Creating DataFrame operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have already discussed, DataFrames are the main building blocks of Spark
    data. They consist of rows and column data structures.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames in PySpark are created using the `pyspark.sql.SparkSession.createDataFrame`
    function. You can use lists, lists of lists, tuples, dictionaries, Pandas DataFrames,
    RDDs, and `pyspark.sql.Rows` to create DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Spark DataFrames also has an argument named **schema** that specifies the schema
    of the DataFrame. You can either choose to specify the schema explicitly or let
    Spark infer the schema from the DataFrame itself. If you don’t specify this argument
    in the code, Spark will infer the schema on its own.
  prefs: []
  type: TYPE_NORMAL
- en: There are different ways to create DataFrames in Spark. Some of them are explained
    in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Using a list of rows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first way to create DataFrames we see is by using rows of data. You can
    think of rows of data as lists. They would share common header values for each
    of the values in the list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the code to use when creating a new DataFrame using rows of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, you will see a DataFrame with our specified columns and their
    data types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now, we’ll see how we can specify the schema for a Spark DataFrame explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Using a list of rows with schema
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The schema of a DataFrame defines what would be the different data types present
    in each of the rows and columns of a DataFrame. Explicitly defining schema helps
    in cases where we want to enforce certain data types to our datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will explicitly tell Spark which schema to use for the DataFrame that
    we’re creating. Notice that the majority of the code remains the same—we’re simply
    adding another argument named `schema` in the code for creating the DataFrame
    to explicitly tell which columns would have what kind of datatypes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, you will see a DataFrame with our specified columns and their
    data types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s take a look at how we can create DataFrames using Pandas DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Using Pandas DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DataFrames can also be created using Pandas DataFrames. To achieve this, you
    would need to create a DataFrame in Pandas first. Once that is created, you would
    then convert that DataFrame to a PySpark DataFrame. The following code demonstrates
    this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, you will see a DataFrame with our specified columns and their
    data types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s take a look at how we can create DataFrames using tuples.
  prefs: []
  type: TYPE_NORMAL
- en: Using tuples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another way to create DataFrames is through tuples. This means that we can
    create a tuple as a row and add each tuple as a separate row in the DataFrame.
    Each tuple would contain the data for each of the columns of the DataFrame. The
    following code demonstrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, you will see a DataFrame with our specified columns and their
    data types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s take a look at different ways we can view the DataFrames in Spark
    and see the results of the DataFrames that we just created.
  prefs: []
  type: TYPE_NORMAL
- en: How to view the DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are different statements in Spark to view data. The DataFrames that we
    created in the previous section through different methods all yield the same result
    as the DataFrame. Let’s look at a few different ways to view DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first way to show a DataFrame is through the `DataFrame.show()` statement.
    Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, you will see a DataFrame with our specified columns and the data
    inside this DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We can also select the total rows that can be viewed in a single statement.
    Let’s see how we can do that in the next topic.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing top n rows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can also be selective in the number of rows that can be viewed in a single
    statement. We can control that using a parameter in `DataFrame.show()`. Here’s
    an example of looking at only the top two rows of the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you specify *n* to be a specific number, then only those sets of rows would
    be shown. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, you will see a DataFrame with its top two rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Viewing DataFrame schema
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also choose to see the schema of the DataFrame using the `printSchema()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, you will see the schema of a DataFrame with our specified columns
    and their data types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Viewing data vertically
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When the data becomes too long to fit into the screen, it’s sometimes useful
    to see the data in a vertical format instead of a horizontal table view. Here’s
    an example of how you can view the data in a vertical format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, you will see a DataFrame with our specified columns and their
    data but in a vertical format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Viewing columns of data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we just need to view the columns that exist in a DataFrame, we would use
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, you will see a list of the columns in the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Viewing summary statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s take a look at how we can view the summary statistics of a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, you will see a DataFrame with its summary statistics for each
    column defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s take a look at the collect statement.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A collect statement is used when we want to get all the data that is being processed
    in different clusters back to the driver. When using a collect statement, we need
    to make sure that the driver has enough memory to hold the processed data. If
    the driver doesn’t have enough memory to hold the data, we will get out-of-memory
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how you show the collect statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This statement will then show result as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: There are a few ways to avoid out-of-memory errors. We will explore some of
    the options that avoid out-of-memory errors such as take, tail, and head statements.
    These statements return only a subset of the data and not all of the data in a
    DataFrame, therefore, they are very useful to inspect the data without having
    to lead all the data in driver memory.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at the take statement.
  prefs: []
  type: TYPE_NORMAL
- en: Using take
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A take statement takes an argument for a number of elements to return from
    the top of a DataFrame. We will see how it is used in the following code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, you will see a DataFrame with its top row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we’re only returning the first element of the DataFrame by
    giving `1` as an argument value to the `take()` function. Therefore, only one
    row is returned in the result.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at the tail statement.
  prefs: []
  type: TYPE_NORMAL
- en: Using tail
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The tail statement takes an argument for a number of elements to return from
    the bottom of a DataFrame. We will see how it is used in the following code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, you will see a DataFrame with its last row of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we’re only returning the last element of the DataFrame by giving
    `1` as an argument value to the `tail()` function. Therefore, only one row is
    returned in the result.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at the head statement.
  prefs: []
  type: TYPE_NORMAL
- en: Using head
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The head statement takes an argument for a number of elements to return from
    the top of a DataFrame. We will see how it is used in the following code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, you will see a DataFrame with its top row of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we’re only returning the first element of the DataFrame by
    giving `1` as an argument value to the `head()` function. Therefore, only one
    row is returned in the result.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at how we can count the number of rows in a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Counting the number of rows of data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we just need to count the number of rows in a DataFrame, we would use
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, you will see the total count of rows in a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: In PySpark, several methods are available for retrieving data from a DataFrame
    or RDD, each with its own characteristics and use cases. Here’s a summary of the
    major differences between take, collect, show, head, and tail that we used earlier
    in this section for data retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: take(n)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This function returns an array containing the first *n* elements from the DataFrame
    or RDD
  prefs: []
  type: TYPE_NORMAL
- en: It is useful for quickly inspecting a small subset of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It performs a lazy evaluation, meaning it only computes the required number
    of elements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: collect()
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This function retrieves all elements from the DataFrame or RDD and returns them
    as a list
  prefs: []
  type: TYPE_NORMAL
- en: It should be used with caution as it brings all data to the driver node, which
    can lead to out-of-memory errors for large datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is suitable for small datasets or when working with aggregated results that
    fit into memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: show(n)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This function displays the first *n* rows of the DataFrame in a tabular format
  prefs: []
  type: TYPE_NORMAL
- en: It is primarily used for visual inspection of data during **exploratory data
    analysis** (**EDA**) or debugging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides a user-friendly display of data with column headers and formatting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: head(n)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This function returns the first *n* rows of the DataFrame as a list of `Row`
    objects
  prefs: []
  type: TYPE_NORMAL
- en: It is similar to `take(n)` but it returns `Row` objects instead of simple values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is often used when you need access to specific column values while working
    with structured data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tail(n)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This function returns the last *n* rows of the DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: It is useful for examining the end of the dataset, especially in cases where
    data is sorted in descending order
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It performs a more expensive operation compared to `head(n)` as it may involve
    scanning the entire dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, `take` and `collect` are used to retrieve data elements, with `take`
    being more suitable for small subsets and `collect` for retrieving all data (with
    caution). `show` is used for visual inspection, `head` retrieves the first rows
    as `Row` objects, and `tail` retrieves the last rows of the dataset. Each method
    serves different purposes and should be chosen based on the specific requirements
    of the data analysis task.
  prefs: []
  type: TYPE_NORMAL
- en: When working with data in PySpark, sometimes, you will need to use some Python
    functions on the DataFrames. To achieve that, you will have to convert PySpark
    DataFrames to Pandas DataFrames. Now, let’s take a look at how we can convert
    a Pyspark DataFrame to a Pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Converting a PySpark DataFrame to a Pandas DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At various times in your workflow, you will want to switch from a Pyspark DataFrame
    to a Pandas DataFrame. There are options to convert a PySpark DataFrame to a Pandas
    DataFrame. This option is `toPandas()`.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to note here is that Python inherently is not distributed. Therefore,
    when a PySpark DataFrame is converted to Pandas, the driver would need to collect
    all the data in its memory. We need to make sure that the driver’s memory is able
    to collect the data in itself. If the data is not able to fit in the driver’s
    memory, it will cause an out-of-memory error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example to see how we can convert a PySpark DataFrame to a Pandas
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, you will see a DataFrame with our specified columns and their
    data types:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | `col_1` | `col_2` | `col_3` | `col_4` | `col_5` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | `100` | `200.0` | `String_test_1` | `2023-01-01` | `2023-01-01 12:00:00`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `200` | `300.0` | `String_test_2` | `2023-02-01` | `2023-01-02 12:00:00`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `2` | `300` | `400.0` | `String_test_3` | `2023-03-01` | `2023-01-03 12:00:00`
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.1: DataFrame with the columns and data types specified by us'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about different data manipulation techniques.
    You will need to filter, slice, and dice the data based on different criteria
    for different purposes. Therefore, data manipulation is essential in working with
    data.
  prefs: []
  type: TYPE_NORMAL
- en: How to manipulate data on rows and columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to do different data manipulation operations
    on Spark DataFrames rows and columns.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by looking at how we can select columns in a Spark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use column functions for data manipulation at the column level in a
    Spark DataFrame. To select a column in a DataFrame, we would use the `select()`
    function like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, you will see only one column of the DataFrame with its data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'There are some other ways to achieve the same result in PySpark as well. Some
    of those are demonstrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Once we select the required columns, there will be instances where you will
    need to add new columns to a DataFrame. We will now take a look at how we can
    create columns in a Spark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Creating columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use a `withColumn()` function to create a new column in a DataFrame.
    To create a new column, we would need to pass the column name and column values
    to fill the column with. In the following example, we’re creating a new column
    named `col_6` and putting a constant literal `A` in this column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, you will see a DataFrame with an additional column named `col_6`
    filled with multiple `A` instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19176_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `lit()` function is used to fill constant values in a column.
  prefs: []
  type: TYPE_NORMAL
- en: You can also delete columns that are no longer needed in a DataFrame. We will
    now take a look at how we can drop columns in a Spark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Dropping columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we need to drop a column from a Spark DataFrame, we would use the `drop()`
    function. We need to provide the name of the column to be dropped from the DataFrame.
    Here’s an example of how to use this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, you will see that `col_5` is dropped from the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We have successfully dropped `col_5` from this DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also drop multiple columns in the same drop statement as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Also note that if we drop a column that does not exist in the DataFrame, it
    will not result in any errors. The resulting DataFrame would remain as it is.
  prefs: []
  type: TYPE_NORMAL
- en: Just like dropping columns, you can also update columns in Spark DataFrames.
    Now, we will take a look at how we can update columns in a Spark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Updating columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Updating columns can also be done with the help of the `withColumn()` function
    in Spark. We need to provide the name of the column to be updated along with the
    updated value. Notice that we can also use this function to calculate some new
    values for the columns. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following updated frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: One thing to note here is the use of the `col` function when updating the column.
    This function is used for column-wise operators. If we don’t use this function,
    our code will return an error.
  prefs: []
  type: TYPE_NORMAL
- en: You don’t always have to update a column in a DataFrame if you only need to
    rename a column. Now, we will see how we can rename columns in a Spark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Renaming columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For changing the name of a column, we would use the `withColumnRenamed()` function
    in Spark. We would need to provide the column name that needs to be changed along
    with the new column name. Here’s the code to illustrate this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, we’ll see the following change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Notice that `col_3` is now called `string_col` after making the change.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s shift our focus to some data manipulation techniques in Spark DataFrames.
    You can have search-like functionality in Spark DataFrames for finding different
    values in a column. Now, let’s take a look at how we can find unique values in
    a column of a Spark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Finding unique values in a column
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finding unique values is a very useful function that would give us distinct
    values in a column. For this purpose, we can use the `distinct()` function of
    the Spark DataFrame like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: We applied a `distinct` function on `col_6` to get all the unique values in
    this column. In our case, the column just had one distinct value, `A`, so that
    was shown.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use it to find the count of distinct values in a given column.
    Here’s an example of how to use this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we can see the total count of distinct values in `col_6`. Currently,
    it is the only type of distinct value present in this column, therefore, it returned
    `1`.
  prefs: []
  type: TYPE_NORMAL
- en: One other useful function in Spark data manipulation is changing the case of
    a column. Now, let’s take a look at how we can change the case of a column in
    a Spark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the case of a column
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is also a function that exists in Spark to change the case of a column.
    We don’t need to specify each value of the column separately to make use of the
    function. Once applied, the whole column’s values would change case. One such
    example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19176_04_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this example, we change the case of `string_col` to all caps. We need to
    assign this to a new column, so, we create a column called `upper_string_col`
    to store these upper-case values. Also, note that this column is not added to
    the original `data_df` because we did not save the results back in `data_df`.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of times in data manipulation, we would need functions to filter DataFrames.
    We will now take a look at how we can filter data in a Spark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering a DataFrame
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Filtering a DataFrame means that we can get a subset of rows or columns from
    a DataFrame. There are different ways of filtering a Spark DataFrame. We will
    take a look at one example here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we filter `data_df` to only include rows where the column value
    of `col_1` is equal to `100`. This criterion is met by only one row, therefore,
    a single row is returned in the resulting DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: You can use this function to slice and dice your data in a number of different
    ways based on the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are talking about data filtering, we can also filter data based on
    logical operators as well. We will now take a look at how we can use logical operators
    in DataFrames to filter data.
  prefs: []
  type: TYPE_NORMAL
- en: Logical operators in a DataFrame
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another important set of operators that we can combine with filter operations
    in a DataFrame are the logical operators. These consist of AND and OR operators,
    amongst others. These are used to filter DataFrames based on complex conditions.
    Let’s take a look at how we use the AND operator here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we’re trying to get the rows where the value of `col_1` is
    equal to `100` and the value of `col_6` is `A`. Currently, only one row fulfills
    this condition, therefore, one row is returned as a result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see how we can use the OR operator to combine conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'This statement will give the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we’re trying to get the rows where the value of `col_1` is
    equal to `100` or the value of `col_2` is equal to `300.0`. Currently, two rows
    fulfill this condition, therefore, they are returned as a result.
  prefs: []
  type: TYPE_NORMAL
- en: In data filtering, there is another important function to find values in a list.
    Now, we will see how you use the `isin()` function in PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: Using isin()
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `isin()` function is used to find values in a DataFrame column that exist
    in a list. To do this, we would create a list with some values in it. Once we
    have the list, then we would use the `isin()` function to see whether some of
    the values that are in the list exist in the DataFrame. Here’s an example to demonstrate
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we see that the values `100` and `200` are present in the `data_df`
    DataFrame in two of its rows, therefore, both rows are returned.
  prefs: []
  type: TYPE_NORMAL
- en: We can also convert data types for different columns in Spark DataFrames. Now,
    let’s look at how we can convert different data types in Spark DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Datatype conversions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll see different ways of converting data types in Spark
    DataFrame columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by using the `cast` function in Spark. The following code illustrates
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we see that we’re changing the data types of two columns,
    namely `col_4` and `col_1`. First, we change `col_4` to string type. This column
    was previously a date column. Then, we change `col_1` to integer type from `long`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the schema of `data_df` for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: We see that `col_1` and `col_4` were different data types.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next example of changing the data types of the columns is by using the
    `selectExpr()` function. The following code illustrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we see that we’re changing the data types of two columns,
    namely `col_4` and `col_1`. First, we change `col_4` back to the `date` type.
    Then, we change `col_1` to the `long` type.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next example of changing the data types of the columns is by using SQL.
    The following code illustrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we see that we’re changing the data types of two columns,
    namely `col_4` and `col_1`. First, we use `createOrReplaceTempView()` to create
    a table named `CastExample`. Then, we use this table to change `col_4` back to
    the `date` type. Then, we change `col_1` to the `double` type.
  prefs: []
  type: TYPE_NORMAL
- en: In the data analysis world, working with null values is very valuable. Now,
    let’s take a look at how we can drop null values from a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Dropping null values from a DataFrame
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, in the data, there exist null values that can make clean data messy.
    Dropping nulls is an essential exercise that a lot of data analysts and data engineers
    need to do. Pyspark provides us with relevant functions to do this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create another DataFrame called `salary_data` to show some of the next
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s take a look at the `dropna()` function; this will help us drop null
    values from our DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: We see in the resulting DataFrame that rows with `Robert` and `Martin` are deleted
    from the new DataFrame when we use the `dropna()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Deduplicating data is another useful technique that is often required in data
    analysis tasks. Now, let’s take a look at how we can drop duplicate values from
    a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Dropping duplicates from a DataFrame
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sometimes, in the data, there are redundant values present that would make
    clean data messy. Dropping these values might be needed in a lot of use cases.
    PySpark provides us with the `dropDuplicates()` function to do this. Here’s the
    code to illustrate this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: We see in this example that employees named Michael are only shown once in the
    resulting DataFrame after we apply the `dropDuplicates()` function to the original
    DataFrame. This name and its corresponding values exist in the original DataFrame
    twice.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about different data filtering techniques, we will
    now see how we can aggregate data in Pyspark DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Using aggregates in a DataFrame
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some of the methods available in Spark for aggregating data are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`agg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`avg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pivot`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sum`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will see some of them in action in the following code examples.
  prefs: []
  type: TYPE_NORMAL
- en: Average (avg)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the following example, we see how to use aggregate functions in Spark. We
    will start by calculating the average of all the values in a column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: This example calculates the average of the salary column of the `salary_data`
    DataFrame. We have passed the `Salary` column to the `avg` function and it has
    calculated the average of that column for us.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at how to count different elements in a PySpark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Count
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the following code example, we can see how you use aggregate functions in
    Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: This example calculates the total count of the values in the `Salary` column
    of the `salary_data` DataFrame. We have passed the `Salary` column to the `agg`
    function with `count` as its other parameter, and it has calculated the count
    of that column for us.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at how to count distinct elements in a PySpark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Count distinct values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the following example, we will look at how to count distinct elements in
    a PySpark DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: This example calculates total distinct values in the salary column of the `salary_data`
    DataFrame. We have passed the `Salary` column to the `countDistinct` function
    and it has calculated the count of that column for us.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at how to find maximum values in a PySpark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Finding maximums (max)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the following code example, we will take a look at how to find maximum values
    in a column of a PySpark DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: This example calculates the maximum value out of all the values in the `Salary`
    column of the `salary_data` DataFrame. We have passed the `Salary` column to the
    `agg` function with `max` as its other parameter, and it has calculated the maximum
    of that column for us.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at how to get the sum of all elements in a PySpark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Sum
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the following code example, we will look at how to sum all values in a PySpark
    DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: This example calculates the sum of all the values in the `Salary` column of
    the `salary_data` DataFrame. We have passed the `Salary` column to the `agg` function
    with `sum` as its other parameter, and it has calculated the sum of that column
    for us.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at how to sort data in a PySpark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Sort data with OrderBy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the following code example, we will look at how can we sort data in ascending
    order in a PySpark DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: This example sorts the full DataFrame based on the values in the `Salary` column
    of the `salary_data` DataFrame. We have passed the `Salary` column to the `orderBy`
    function and it has sorted the DataFrame based on this column.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also sort the data in descending format by adding another function,
    `desc()`, to the original `orderBy` function. The following example illustrates
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: This example is sorting the full DataFrame in descending order based on the
    values in the `Salary` column of the `salary_data` DataFrame. We have passed the
    `Salary` column to the `orderBy` function with `desc()` as an additional function
    call and it has sorted the DataFrame in descending order based on this column.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the course of this chapter, we have learned how to manipulate data in Spark
    DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: We talked about the Spark DataFrame API and what different data types are in
    Spark. We also learned how to create DataFrames in Spark and how we can view these
    DataFrames once they’ve been created. Finally, we learned about different data
    manipulation and data aggregation functions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover some advanced operations in Spark with respect
    to data manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: Sample question
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1\. Which of the following operations will trigger evaluation?
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrame.filter()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`DataFrame.distinct()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`DataFrame.intersect()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`DataFrame.join()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`DataFrame.count()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: E
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
