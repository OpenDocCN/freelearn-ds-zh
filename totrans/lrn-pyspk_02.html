<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 2. Resilient Distributed Datasets"><div class="titlepage"><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Resilient Distributed Datasets</h1></div></div></div><p>Resilient Distributed Datasets (RDDs) are a distributed collection of immutable JVM objects that allow you to perform calculations very quickly, and they are the <span class="emphasis"><em>backbone</em></span> of Apache Spark.</p><p>As the name suggests, the dataset is distributed; it is split into chunks based on some key and distributed to executor nodes. Doing so allows for running calculations against such datasets very quickly. Also, as already mentioned in <a class="link" href="ch01.html" title="Chapter 1. Understanding Spark">Chapter 1</a>,
<span class="emphasis"><em>Understanding Spark</em></span>, RDDs keep track (log) of all the transformations applied to each chunk to speed up the computations and provide a fallback if things go wrong and that portion of the data is lost; in such cases, RDDs can recompute the data. This data lineage is another line of defense against data loss, a complement to data replication.</p><p>The following topics are covered in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Internal workings of an RDD</li><li class="listitem" style="list-style-type: disc">Creating RDDs</li><li class="listitem" style="list-style-type: disc">Global versus local scopes</li><li class="listitem" style="list-style-type: disc">Transformations</li><li class="listitem" style="list-style-type: disc">Actions</li></ul></div><div class="section" title="Internal workings of an RDD"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec11"/>Internal workings of an RDD</h1></div></div></div><p>RDDs operate<a id="id51" class="indexterm"/> in parallel. This is the strongest advantage of working in Spark: Each transformation is executed in parallel for enormous increase in speed.</p><p>The transformations to the dataset are lazy. This means that any transformation is only executed when an action on a dataset is called. This helps Spark to optimize the execution. For instance, consider the following very common steps that an analyst would normally do to get familiar with a dataset:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Count the occurrence of distinct values in a certain column.</li><li class="listitem">Select those that start with an <code class="literal">A</code>.</li><li class="listitem">Print the results to the screen.</li></ol></div><p>As simple as the previously mentioned steps sound, if only items that start with the letter <code class="literal">A</code> are of interest, there is no point in counting distinct values for all the other items. Thus, instead of following<a id="id52" class="indexterm"/> the execution as outlined in the preceding points, Spark could only count the items that start with <code class="literal">A</code>, and then print the results to the screen.</p><p>Let's break this example down in code. First, we order Spark to map the values of <code class="literal">A</code> using the <code class="literal">.map(lambda v: (v, 1))</code> method, and then select those records that start with an <code class="literal">'A'</code> (using the <code class="literal">.filter(lambda val: val.startswith('A'))</code> method). If we call the <code class="literal">.reduceByKey(operator.add)</code> method it will reduce the dataset and <span class="emphasis"><em>add</em></span> (in this example, count) the number<a id="id53" class="indexterm"/> of occurrences of each key. All of these steps <span class="strong"><strong>transform</strong></span> the dataset.</p><p>Second, we call the <code class="literal">.collect()</code> method to execute the steps. This step is an <span class="strong"><strong>action</strong></span> on our dataset - it finally<a id="id54" class="indexterm"/> counts the distinct elements of the dataset. In effect, the action might reverse the order of transformations and filter the data first before mapping, resulting in a smaller dataset being passed to the reducer.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note12"/>Note</h3><p>Do not worry if you do not understand the previous commands yet - we will explain them in detail later in this chapter.</p></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Creating RDDs"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec12"/>Creating RDDs</h1></div></div></div><p>There are<a id="id55" class="indexterm"/> two ways to create an RDD in PySpark: you can either <code class="literal">.parallelize(...)</code> a collection (<code class="literal">list</code> or an <code class="literal">array</code> of some elements):</p><div class="informalexample"><pre class="programlisting">data = sc.parallelize(
    [('Amber', 22), ('Alfred', 23), ('Skye',4), ('Albert', 12), 
     ('Amber', 9)])</pre></div><p>Or you can reference a file (or files) located either locally or somewhere externally:</p><div class="informalexample"><pre class="programlisting">data_from_file = sc.\    
    textFile(
        '/Users/drabast/Documents/PySpark_Data/VS14MORT.txt.gz',
        4)
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note13"/>Note</h3><p>We downloaded<a id="id56" class="indexterm"/> the Mortality dataset <code class="literal">VS14MORT.txt</code> file from (accessed on July 31, 2016) <a class="ulink" href="ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/DVS/mortality/mort2014us.zip">ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/DVS/mortality/mort2014us.zip</a>; the record<a id="id57" class="indexterm"/> schema is explained in this document <a class="ulink" href="http://www.cdc.gov/nchs/data/dvs/Record_Layout_2014.pdf">http://www.cdc.gov/nchs/data/dvs/Record_Layout_2014.pdf</a>. We selected this dataset on purpose: The encoding of the records will help us to explain how to use UDFs to transform your data later in this chapter. For your convenience, we also<a id="id58" class="indexterm"/> host the file here: <a class="ulink" href="http://tomdrabas.com/data/VS14MORT.txt.gz">http://tomdrabas.com/data/VS14MORT.txt.gz</a>
</p></div></div><p>The last parameter in <code class="literal">sc.textFile(..., n)</code> specifies the number of partitions the dataset is<a id="id59" class="indexterm"/> divided into.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip02"/>Tip</h3><p>A rule of thumb would be to break your dataset into two-four partitions for each in your cluster.</p></div></div><p>Spark can read from a multitude of filesystems: Local ones such as NTFS, FAT, or Mac OS Extended (HFS+), or distributed filesystems such as HDFS, S3, Cassandra, among many others.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip03"/>Tip</h3><p>Be wary where your datasets are read from or saved to: The path cannot contain special characters <code class="literal">[]</code>. Note, that this also applies to paths stored on Amazon S3 or Microsoft Azure Data Storage.</p></div></div><p>Multiple data formats are supported: Text, parquet, JSON, Hive tables, and data from relational databases can be read using a JDBC driver. Note that Spark can automatically work with compressed datasets (like the Gzipped one in our preceding example).</p><p>Depending on how the data is read, the object holding it will be represented slightly differently. The data read from a file is represented as <code class="literal">MapPartitionsRDD</code> instead of <code class="literal">ParallelCollectionRDD</code> when we <code class="literal">.paralellize(...) </code>a collection.</p><div class="section" title="Schema"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec17"/>Schema</h2></div></div></div><p>RDDs are <span class="emphasis"><em>schema-less</em></span> data structures (unlike DataFrames, which we will discuss in the next chapter). Thus, parallelizing<a id="id60" class="indexterm"/> a dataset, such as in the following code snippet, is perfectly fine with Spark when using RDDs:</p><div class="informalexample"><pre class="programlisting">data_heterogenous = sc.parallelize([
    ('Ferrari', 'fast'),
    {'Porsche': 100000},
    ['Spain','visited', 4504]
]).collect()</pre></div><p>So, we can mix<a id="id61" class="indexterm"/> almost anything: a <code class="literal">tuple</code>, a <code class="literal">dict</code>, or a <code class="literal">list</code> and Spark will not complain.</p><p>Once you <code class="literal">.collect()</code> the dataset (that is, run an action to bring it back to the driver) you can access the data in the object as you would normally do in Python:</p><div class="informalexample"><pre class="programlisting">data_heterogenous[1]['Porsche']</pre></div><p>It will produce the following:</p><div class="informalexample"><pre class="programlisting">100000</pre></div><p>The <code class="literal">.collect()</code> method returns all the elements of the RDD to the driver where it is serialized as a list.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note14"/>Note</h3><p>We will talk more about the caveats of using <code class="literal">.collect()</code> later in this chapter.</p></div></div></div><div class="section" title="Reading from files"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec18"/>Reading from files</h2></div></div></div><p>When you<a id="id62" class="indexterm"/> read from a text file, each row from the file forms an element of an RDD.</p><p>The <code class="literal">data_from_file.take(1)</code> command will produce the following (somewhat unreadable) output:</p><div class="mediaobject"><img src="images/B05793_02_01.jpg" alt="Reading from files"/></div><p>To make it<a id="id63" class="indexterm"/> more readable, let's create a list of elements so each line is represented as a list of values.</p></div><div class="section" title="Lambda expressions"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec19"/>Lambda expressions</h2></div></div></div><p>In this example, we will<a id="id64" class="indexterm"/> extract the<a id="id65" class="indexterm"/> useful information from the cryptic looking record of <code class="literal">data_from_file</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note15"/>Note</h3><p>Please refer to our GitHub repository for this book for the details of this method. Here, due to<a id="id66" class="indexterm"/> space constraints, we will only present an abbreviated version of the full method, especially where we create the Regex pattern. The code can be found here: <a class="ulink" href="https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/LearningPySpark_Chapter03.ipynb">https://github.com/drabastomek/learningPySpark/tree/master/Chapter03/LearningPySpark_Chapter03.ipynb</a>.</p></div></div><p>First, let's define the method with the help of the following code, which will parse the unreadable row into something that we can use:</p><div class="informalexample"><pre class="programlisting">def extractInformation(row):
    import re
    import numpy as np
    selected_indices = [
         2,4,5,6,7,9,10,11,12,13,14,15,16,17,18,
         ...
         77,78,79,81,82,83,84,85,87,89
    ]
    record_split = re\
        .compile(
            r'([\s]{19})([0-9]{1})([\s]{40})
            ...
            ([\s]{33})([0-9\s]{3})([0-9\s]{1})([0-9\s]{1})')
    try:
        rs = np.array(record_split.split(row))[selected_indices]
    except:
        rs = np.array(['-99'] * len(selected_indices))
    return rs</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip04"/>Tip</h3><p>A word of caution here is necessary. Defining pure Python methods can slow down your application as Spark needs to continuously switch back and forth between the Python interpreter and JVM. Whenever you can, you should use built-in Spark functions.</p></div></div><p>Next, we import<a id="id67" class="indexterm"/> the necessary modules: The <code class="literal">re</code> module as we will use regular expressions to parse the record, and <code class="literal">NumPy</code> for ease of<a id="id68" class="indexterm"/> selecting multiple elements at once.</p><p>Finally, we create a <code class="literal">Regex</code> object to extract the information as specified and parse the row through it.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note16"/>Note</h3><p>We will not<a id="id69" class="indexterm"/> be delving into details here describing Regular Expressions. A good compendium on the topic can be found here <a class="ulink" href="https://www.packtpub.com/application-development/mastering-python-regular-expressions">https://www.packtpub.com/application-development/mastering-python-regular-expressions</a>.</p></div></div><p>Once the record is parsed, we try to convert the list into a <code class="literal">NumPy</code> array and return it; if this fails we return a list of default values <code class="literal">-99</code> so we know this record did not parse properly.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip05"/>Tip</h3><p>We could<a id="id70" class="indexterm"/> implicitly filter out the malformed records by using <code class="literal">.flatMap(...)</code> and return an empty list <code class="literal">[]</code> instead of <code class="literal">-99</code> values. Check this for details: <a class="ulink" href="http://stackoverflow.com/questions/34090624/remove-elements-from-spark-rdd">http://stackoverflow.com/questions/34090624/remove-elements-from-spark-rdd</a>
</p></div></div><p>Now, we will use the <code class="literal">extractInformation(...)</code> method to split and convert our dataset. Note that we pass only the method signature to <code class="literal">.map(...)</code>: the method will <span class="emphasis"><em>hand over</em></span> one element of the RDD to the <code class="literal">extractInformation(...)</code> method at a time in each partition:</p><div class="informalexample"><pre class="programlisting">data_from_file_conv = data_from_file.map(extractInformation)</pre></div><p>Running <code class="literal">data_from_file_conv.take(1)</code> will produce the following result (abbreviated):</p><div class="mediaobject"><img src="images/B05793_02_02.jpg" alt="Lambda expressions"/></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Global versus local scope"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec13"/>Global versus local scope</h1></div></div></div><p>One of<a id="id71" class="indexterm"/> the things that you, as a prospective PySpark user, need to get used to is the inherent parallelism of Spark. Even if you are proficient in Python, executing scripts in PySpark requires shifting your thinking a bit.</p><p>Spark can be run in two modes: Local and cluster. When you run Spark locally your code might not differ to what you are currently used to with running Python: Changes would most likely be more syntactic than anything else but with an added twist that data and code can be copied between separate worker processes.</p><p>However, taking the same code and deploying it to a cluster might cause a lot of head-scratching if you are not careful. This requires understanding how Spark executes a job on the cluster.</p><p>In the cluster mode, when a job is submitted for execution, the job is sent to the driver (or a master) node. The driver node creates a DAG (see <a class="link" href="ch01.html" title="Chapter 1. Understanding Spark">Chapter 1</a>,
<span class="emphasis"><em>Understanding Spark</em></span>) for a job and decides which executor (or worker) nodes will run specific tasks.</p><p>The driver then instructs the workers to execute their tasks and return the results to the driver when done. Before that happens, however, the driver prepares each task's closure: A set of variables and methods present on the driver for the worker to execute its task on the RDD.</p><p>This set of variables and methods is inherently <span class="emphasis"><em>static</em></span> within the executors' context, that is, each executor gets a <span class="emphasis"><em>copy</em></span> of the variables and methods from the driver. If, when running the task, the executor alters these variables or overwrites the methods, it does so <span class="strong"><strong>without</strong></span> affecting either other executors' copies or the variables and methods of the driver. This might<a id="id72" class="indexterm"/> lead to some unexpected behavior and runtime bugs that can sometimes be really hard to track down.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note17"/>Note</h3><p>Check out this<a id="id73" class="indexterm"/> discussion in PySpark's documentation for a more hands-on example: <a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html#local-vs-cluster-modes">http://spark.apache.org/docs/latest/programming-guide.html#local-vs-cluster-modes</a>.</p></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Transformations"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec14"/>Transformations</h1></div></div></div><p>Transformations shape your dataset. These include mapping, filtering, joining, and transcoding the<a id="id74" class="indexterm"/> values in your dataset. In this section, we will showcase some of the transformations available on RDDs.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note18"/>Note</h3><p>Due to space<a id="id75" class="indexterm"/> constraints we include only the most often used transformations and actions here. For a full set of methods available we suggest you check PySpark's documentation on RDDs <a class="ulink" href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD">http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD</a>.</p></div></div><p>Since RDDs are schema-less, in this section we assume you know the schema of the produced dataset. If you cannot remember the positions of information in the parsed dataset we suggest you refer to the definition of the <code class="literal">extractInformation(...)</code> method on GitHub, code for <code class="literal">Chapter 03</code>.</p><div class="section" title="The .map(...) transformation"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec20"/>The .map(...) transformation</h2></div></div></div><p>It can be argued that you will use the <code class="literal">.map(...)</code> transformation most often. The method is applied to each<a id="id76" class="indexterm"/> element of the RDD: In the case of the <code class="literal">data_from_file_conv</code> dataset, you can<a id="id77" class="indexterm"/> think of this as a transformation of each row.</p><p>In this example, we will create a new dataset that will convert year of death into a numeric value:</p><div class="informalexample"><pre class="programlisting">data_2014 = data_from_file_conv.map(lambda row: int(row[16]))</pre></div><p>Running <code class="literal">data_2014.take(10)</code> will yield the following result:</p><div class="mediaobject"><img src="images/B05793_02_03.jpg" alt="The .map(...) transformation"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note19"/>Note</h3><p>If you are<a id="id78" class="indexterm"/> not familiar with <code class="literal">lambda</code> expressions, please refer to this resource: <a class="ulink" href="https://pythonconquerstheuniverse.wordpress.com/2011/08/29/lambda_tutorial/">https://pythonconquerstheuniverse.wordpress.com/2011/08/29/lambda_tutorial/</a>.</p></div></div><p>You can of course bring more columns over, but you would<a id="id79" class="indexterm"/> have to package them into a <code class="literal">tuple</code>, <code class="literal">dict,</code> or a <code class="literal">list</code>. Let's also include the 17th element of the row along so that we can confirm our <code class="literal">.map(...)</code> works as intended:</p><div class="informalexample"><pre class="programlisting">data_2014_2 = data_from_file_conv.map(
    lambda row: (row[16], int(row[16]):)
data_2014_2.take(5)</pre></div><p>The preceding code will produce the following result:</p><div class="mediaobject"><img src="images/B05793_02_04.jpg" alt="The .map(...) transformation"/></div></div><div class="section" title="The .filter(...) transformation"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec21"/>The .filter(...) transformation</h2></div></div></div><p>Another most<a id="id80" class="indexterm"/> often used transformation is the <code class="literal">.filter(...)</code> method, which allows<a id="id81" class="indexterm"/> you to select elements<a id="id82" class="indexterm"/> from your dataset that fit specified criteria. As an example, from the <code class="literal">data_from_file_conv</code> dataset, let's count how many people died in an accident in 2014:</p><div class="informalexample"><pre class="programlisting">data_filtered = data_from_file_conv.filter(
    lambda row: row[16] == '2014' and row[21] == '0')
data_filtered.count()</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip06"/>Tip</h3><p>Note that the preceding command might take a while depending on how fast your computer is. For us, it took a little over two minutes to return a result.</p></div></div></div><div class="section" title="The .flatMap(...) transformation"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec22"/>The .flatMap(...) transformation</h2></div></div></div><p>The<code class="literal"> .flatMap(...)</code> method works similarly to <code class="literal">.map(...)</code>, but it returns a flattened result instead<a id="id83" class="indexterm"/> of a list. If we execute the following code:</p><div class="informalexample"><pre class="programlisting">data_2014_flat = data_from_file_conv.flatMap(lambda row: (row[16], int(row[16]) + 1))
data_2014_flat.take(10)</pre></div><p>It will<a id="id84" class="indexterm"/> yield the following output:</p><div class="mediaobject"><img src="images/B05793_02_05.jpg" alt="The .flatMap(...) transformation"/></div><p>You can compare this result with the results of the command that generated <code class="literal">data_2014_2</code> previously. Note, also, as mentioned earlier, that the <code class="literal">.flatMap(...) </code>method can be used to filter out some malformed records when you need to parse your input. Under the hood, the <code class="literal">.flatMap(...)</code> method treats each row as a list and then simply <span class="emphasis"><em>adds</em></span> all the records together; by passing an empty list the malformed records is dropped.</p></div><div class="section" title="The .distinct(...) transformation"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec23"/>The .distinct(...) transformation</h2></div></div></div><p>This method returns<a id="id85" class="indexterm"/> a list of distinct values in a specified column. It is<a id="id86" class="indexterm"/> extremely useful if you want to get to know your dataset or validate it. Let's check if the <code class="literal">gender</code> column contains only males and females; that would verify that we parsed the dataset properly. Let's run the following code:</p><div class="informalexample"><pre class="programlisting">distinct_gender = data_from_file_conv.map(
    lambda row: row[5]).distinct()
distinct_gender.collect()</pre></div><p>This code will produce the following output:</p><div class="mediaobject"><img src="images/B05793_02_06.jpg" alt="The .distinct(...) transformation"/></div><p>First, we extract only the column that contains the gender. Next, we use the <code class="literal">.distinct()</code> method to select only the distinct values in the list. Lastly, we use the <code class="literal">.collect()</code> method to return the print of the values on the screen.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip07"/>Tip</h3><p>Note that this is an expensive method and should be used sparingly and only when necessary as it shuffles the data around.</p></div></div></div><div class="section" title="The .sample(...) transformation"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec24"/>The .sample(...) transformation</h2></div></div></div><p>The <code class="literal">.sample(...)</code> method returns a randomized sample from the dataset. The first parameter specifies<a id="id87" class="indexterm"/> whether the sampling should be with a replacement, the second<a id="id88" class="indexterm"/> parameter defines the fraction of the data to return, and the third is seed to the pseudo-random numbers generator:</p><div class="informalexample"><pre class="programlisting">fraction = 0.1
data_sample = data_from_file_conv.sample(False, fraction, 666)</pre></div><p>In this example, we selected a randomized sample of 10% from the original dataset. To confirm this, let's print the sizes of the datasets:</p><div class="informalexample"><pre class="programlisting">print('Original dataset: {0}, sample: {1}'\
.format(data_from_file_conv.count(), data_sample.count()))</pre></div><p>The preceding<a id="id89" class="indexterm"/> command produces the following output:</p><div class="mediaobject"><img src="images/B05793_02_07.jpg" alt="The .sample(...) transformation"/></div><p>We use the <code class="literal">.count()</code> action that<a id="id90" class="indexterm"/> counts all the records in the corresponding RDDs.</p></div><div class="section" title="The .leftOuterJoin(...) transformation"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec25"/>The .leftOuterJoin(...) transformation</h2></div></div></div><p>
<code class="literal">.leftOuterJoin(...)</code>, just like in<a id="id91" class="indexterm"/> the SQL world, joins two RDDs<a id="id92" class="indexterm"/> based on the values found in both datasets, and returns records from the left RDD with records from the right one appended in places where the two RDDs match:</p><div class="informalexample"><pre class="programlisting">rdd1 = sc.parallelize([('a', 1), ('b', 4), ('c',10)])
rdd2 = sc.parallelize([('a', 4), ('a', 1), ('b', '6'), ('d', 15)])
rdd3 = rdd1.leftOuterJoin(rdd2)</pre></div><p>Running <code class="literal">.collect(...)</code> on the <code class="literal">rdd3</code> will produce the following:</p><div class="mediaobject"><img src="images/B05793_02_08.jpg" alt="The .leftOuterJoin(...) transformation"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip08"/>Tip</h3><p>This is another expensive method and should be used sparingly and only when necessary as it shuffles the data around causing a performance hit.</p></div></div><p>What you can see here are all the elements from RDD <code class="literal">rdd1</code> and their corresponding values from RDD <code class="literal">rdd2</code>. As you can see, the value <code class="literal">'a'</code> shows up two times in <code class="literal">rdd3</code> and <code class="literal">'a'</code> appears twice in the RDD <code class="literal">rdd2</code>. The value <code class="literal">b</code> from the <code class="literal">rdd1</code> shows up only once and is joined with the value <code class="literal">'6'</code> from the <code class="literal">rdd2</code>. There are two things <span class="emphasis"><em>missing</em></span>: Value <code class="literal">'c'</code> from <code class="literal">rdd1</code> does not have a corresponding key in the <code class="literal">rdd2</code> so the value in the returned tuple shows as <code class="literal">None</code>, and, since we were performing a left outer join, the value <code class="literal">'d'</code> from the <code class="literal">rdd2</code> disappeared as expected.</p><p>If we used the <code class="literal">.join(...)</code> method instead we would have got only the values for <code class="literal">'a'</code> and <code class="literal">'b'</code> as these<a id="id93" class="indexterm"/> two values intersect between these two RDDs. Run the following code:</p><div class="informalexample"><pre class="programlisting">rdd4 = rdd1.join(rdd2)
rdd4.collect()</pre></div><p>It will result<a id="id94" class="indexterm"/> in the following output:</p><div class="mediaobject"><img src="images/B05793_02_09.jpg" alt="The .leftOuterJoin(...) transformation"/></div><p>Another useful method is <code class="literal">.intersection(...)</code>, which returns the records that are equal in both RDDs. Execute the following code:</p><div class="informalexample"><pre class="programlisting">rdd5 = rdd1.intersection(rdd2)
rdd5.collect()</pre></div><p>The output is as follows:</p><div class="mediaobject"><img src="images/B05793_02_10.jpg" alt="The .leftOuterJoin(...) transformation"/></div></div><div class="section" title="The .repartition(...) transformation"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec26"/>The .repartition(...) transformation</h2></div></div></div><p>Repartitioning the dataset<a id="id95" class="indexterm"/> changes the number of partitions that<a id="id96" class="indexterm"/> the dataset is divided into. This functionality should be used sparingly and only when really necessary as it shuffles the data around, which in effect results in a significant hit in terms of performance:</p><div class="informalexample"><pre class="programlisting">rdd1 = rdd1.repartition(4)
len(rdd1.glom().collect())</pre></div><p>The preceding code prints out <code class="literal">4</code> as the new number of partitions.</p><p>The <code class="literal">.glom()</code> method, in contrast to <code class="literal">.collect()</code>, produces a list where each element is another list of all elements of the dataset present in a specified partition; the main list returned has as many elements as the number of partitions.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Actions"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec15"/>Actions</h1></div></div></div><p>Actions, in contrast to transformations, execute the scheduled task on the dataset; once you have finished<a id="id97" class="indexterm"/> transforming your data you can execute your transformations. This might contain no transformations (for example, <code class="literal">.take(n)</code> will just return <code class="literal">n</code> records from an RDD even if you did not do any transformations to it) or execute the whole chain of transformations.</p><div class="section" title="The .take(...) method"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec27"/>The .take(...) method</h2></div></div></div><p>This is most<a id="id98" class="indexterm"/> arguably the most useful (and used, such as the <code class="literal">.map(...)</code> method). The method is preferred to <code class="literal">.collect(...)</code> as it only returns the <code class="literal">n</code> top rows from<a id="id99" class="indexterm"/> a single data partition in contrast to <code class="literal">.collect(...)</code>, which returns the whole RDD. This is especially important when you deal with large datasets:</p><div class="informalexample"><pre class="programlisting">data_first = data_from_file_conv.take(1)</pre></div><p>If you want somewhat randomized records you can use <code class="literal">.takeSample(...)</code> instead, which takes three arguments: First whether the sampling should be with replacement, the second specifies the number of records to return, and the third is a seed to the pseudo-random numbers generator:</p><div class="informalexample"><pre class="programlisting">data_take_sampled = data_from_file_conv.takeSample(False, 1, 667)</pre></div></div><div class="section" title="The .collect(...) method"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec28"/>The .collect(...) method</h2></div></div></div><p>This method<a id="id100" class="indexterm"/> returns all the elements of the RDD to the driver. As we have<a id="id101" class="indexterm"/> just provided a caution about it, we will not repeat ourselves here.</p></div><div class="section" title="The .reduce(...) method"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec29"/>The .reduce(...) method</h2></div></div></div><p>The <code class="literal">.reduce(...)</code> method reduces the elements of an RDD using a specified method.</p><p>You can use it<a id="id102" class="indexterm"/> to sum the elements of your RDD:</p><div class="informalexample"><pre class="programlisting">rdd1.map(lambda row: row[1]).reduce(lambda x, y: x + y)</pre></div><p>This will produce<a id="id103" class="indexterm"/> the sum of <code class="literal">15</code>.</p><p>We first create a list of all the values of the <code class="literal">rdd1</code> using the <code class="literal">.map(...)</code> transformation, and then use the <code class="literal">.reduce(...)</code> method to process the results. The <code class="literal">reduce(...)</code> method, on each partition, runs the summation method (here expressed as a <code class="literal">lambda</code>) and returns the<a id="id104" class="indexterm"/> sum to<a id="id105" class="indexterm"/> the driver node where the final aggregation takes place.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note20"/>Note</h3><p>A word of caution<a id="id106" class="indexterm"/> is necessary here. The functions passed as a reducer need to be <span class="strong"><strong>associative</strong></span>, that is, when the order of elements is changed the<a id="id107" class="indexterm"/> result does not, and <span class="strong"><strong>commutative</strong></span>, that is, changing the order of operands does not change the result either.</p><p>The example of the associativity rule is <span class="emphasis"><em>(5 + 2) + 3 = 5 + (2 + 3)</em></span>, and of the commutative is <span class="emphasis"><em>5 + 2 + 3 = 3 + 2 + 5</em></span>. Thus, you need to be careful about what functions you pass to the reducer.</p><p>If you ignore the preceding rule, you might run into trouble (assuming your code runs at all). For example, let's assume we have the following RDD (with one partition only!):</p><div class="informalexample"><pre class="programlisting">data_reduce = sc.parallelize([1, 2, .5, .1, 5, .2], 1)</pre></div><p>If we were to reduce the data in a manner that we would like to divide the current result by the subsequent one, we would expect a value of <code class="literal">10</code>:</p><div class="informalexample"><pre class="programlisting">works = data_reduce.reduce(lambda x, y: x / y)</pre></div><p>However, if you were to partition the data into three partitions, the result will be wrong:</p><div class="informalexample"><pre class="programlisting">data_reduce = sc.parallelize([1, 2, .5, .1, 5, .2], 3)
data_reduce.reduce(lambda x, y: x / y)</pre></div><p>It will produce <code class="literal">0.004</code>.</p></div></div><p>The <code class="literal">.reduceByKey(...)</code> method works in a similar way to the <code class="literal">.reduce(...)</code> method, but it performs a reduction on a key-by-key basis:</p><div class="informalexample"><pre class="programlisting">data_key = sc.parallelize(
    [('a', 4),('b', 3),('c', 2),('a', 8),('d', 2),('b', 1),
     ('d', 3)],4)
data_key.reduceByKey(lambda x, y: x + y).collect()</pre></div><p>The preceding code produces the following:</p><div class="mediaobject"><img src="images/B05793_02_11.jpg" alt="The .reduce(...) method"/></div></div><div class="section" title="The .count(...) method"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec30"/>The .count(...) method</h2></div></div></div><p>The <code class="literal">.count(...)</code> method counts<a id="id108" class="indexterm"/> the number of elements in the RDD. Use the following code:</p><div class="informalexample"><pre class="programlisting">data_reduce.count()</pre></div><p>This code<a id="id109" class="indexterm"/> will produce <code class="literal">6</code>, the exact number of elements in the <code class="literal">data_reduce</code> RDD.</p><p>The <code class="literal">.count(...)</code> method produces the same result as the following method, but it does not require moving the whole dataset to the driver:</p><div class="informalexample"><pre class="programlisting">len(data_reduce.collect()) # WRONG -- DON'T DO THIS!</pre></div><p>If your dataset is in a key-value form, you can use the <code class="literal">.countByKey() </code>method to get the counts of distinct keys. Run the following code:</p><div class="informalexample"><pre class="programlisting">data_key.countByKey().items()</pre></div><p>This code will produce the following output:</p><div class="mediaobject"><img src="images/B05793_02_12.jpg" alt="The .count(...) method"/></div></div><div class="section" title="The .saveAsTextFile(...) method"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec31"/>The .saveAsTextFile(...) method</h2></div></div></div><p>As the<a id="id110" class="indexterm"/> name suggests, the <code class="literal">.saveAsTextFile(...)</code> the RDD and saves it to text files: Each partition to a separate file:</p><div class="informalexample"><pre class="programlisting">data_key.saveAsTextFile(
'/Users/drabast/Documents/PySpark_Data/data_key.txt')</pre></div><p>To read it back, you need<a id="id111" class="indexterm"/> to parse it back as all the rows are treated as strings:</p><div class="informalexample"><pre class="programlisting">def parseInput(row):
    import re    
    pattern = re.compile(r'\(\'([a-z])\', ([0-9])\)')
    row_split = pattern.split(row)
    return (row_split[1], int(row_split[2]))
    
data_key_reread = sc \
    .textFile(
        '/Users/drabast/Documents/PySpark_Data/data_key.txt') \
    .map(parseInput)
data_key_reread.collect()</pre></div><p>The list<a id="id112" class="indexterm"/> of keys<a id="id113" class="indexterm"/> read matches what we had initially:</p><div class="mediaobject"><img src="images/B05793_02_13.jpg" alt="The .saveAsTextFile(...) method"/></div></div><div class="section" title="The .foreach(...) method"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec32"/>The .foreach(...) method</h2></div></div></div><p>This is a method<a id="id114" class="indexterm"/> that applies the same function to each element of the<a id="id115" class="indexterm"/> RDD in an iterative way; in contrast to <code class="literal">.map(..)</code>, the <code class="literal">.foreach(...)</code> method applies a defined function to each record in a one-by-one fashion. It is useful when you want to save the data to a database that is not natively supported by PySpark.</p><p>Here, we'll use it to print (to CLI - not the Jupyter Notebook) all the records that are stored in <code class="literal">data_key</code> RDD:</p><div class="informalexample"><pre class="programlisting">def f(x): 
    print(x)

data_key.foreach(f)</pre></div><p>If you now navigate to CLI you should see all the records printed out. Note, that every time the order will most likely be different.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec16"/>Summary</h1></div></div></div><p>RDDs are the backbone of Spark; these schema-less data structures are the most fundamental data structures that we will deal with within Spark.</p><p>In this chapter, we presented ways to create RDDs from text files, by means of the <code class="literal">.parallelize(...)</code> method as well as by reading data from text files. Also, some ways of processing unstructured data were shown.</p><p>Transformations in Spark are lazy - they are only applied when an action is called. In this chapter, we discussed and presented the most commonly used transformations and actions; the PySpark documentation contains many more <a class="ulink" href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD">http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD</a>.</p><p>One major distinction between Scala and Python RDDs is speed: Python RDDs can be much slower than their Scala counterparts.</p><p>In the next chapter we will walk you through a data structure that made PySpark applications perform <span class="emphasis"><em>on par</em></span> with those written in Scala - the DataFrames.</p></div></div>
</body></html>