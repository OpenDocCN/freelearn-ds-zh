<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Developing the Bitonic Sort with OpenCL</h1></div></div></div><p>In this chapter, we will cover the following recipes:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding sorting networks</li><li class="listitem" style="list-style-type: disc">Understanding bitonic sorting</li><li class="listitem" style="list-style-type: disc">Developing bitonic sorting in OpenCL</li></ul></div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec63"/>Introduction</h1></div></div></div><p>Sorting<a id="id629" class="indexterm"/> is one of the most important problems in computer science and the ability to sort large amounts of data efficiently is absolutely critical. Sorting algorithms <a id="id630" class="indexterm"/>were traditionally been implemented on CPUs and they work very well there, but on the flipside implementing them on GPUs can be challenging. In the OpenCL programming model, we have both task and data parallelism and getting a sorting algorithm to work on the OpenCL model can be challenging, but mostly from the algorithm point of view, that is, how to create an algorithm that takes advantage of the massive data and task parallelism that OpenCL offers.</p><p>Sorting methods can largely be categorized into two types: data-driven and data-independent. Data-driven sorting algorithms <a id="id631" class="indexterm"/>execute the<a id="id632" class="indexterm"/> next step of the algorithm depending on the value of the key under consideration, for example, the <a id="id633" class="indexterm"/>QuickSort. Data-independent sorting algorithms <a id="id634" class="indexterm"/>is rigid from <a id="id635" class="indexterm"/>this perspective because they do not change the order of processing according to the values of the key, so in that sense it doesn't behave like data-driven sorting algorithms. They can be implemented in GPUs to exploit the massive data and task parallelism it offers. Hence we are going to explore the bitonic sort, as it's a classic example of data-independent sorting algorithm and we'll see how it can be represented by sorting networks, and eventually how they can be implemented efficiently in OpenCL to execute on GPUs.</p><div><div><h3 class="title"><a id="note40"/>Note</h3><p>Ken Batcher invented<a id="id636" class="indexterm"/> bitonic sort in 1968. And for n items it would have a size of <img src="img/4520OT_09_16.jpg" alt="Introduction"/> and a depth of <img src="img/4520OT_09_17.jpg" alt="Introduction"/>.</p></div></div><p>The bitonic sort works effectively by comparing two elements at any point in time and what this means is that it consumes two inputs and decides whether a is equal to b, a is less than b, or a is greater than b, that is, the algorithm primarily operates on two elements, given an input. The bitonic sort is an example of a non-adaptive sorting algorithm.</p><div><div><h3 class="title"><a id="note41"/>Note</h3><p>A non-adaptive sorting algorithm<a id="id637" class="indexterm"/> is the one where the sequence of operations performed is independent of the order of the data also known as data-independent.</p></div></div><p>To give you a more concrete idea of what non-adaptive sorting methods are like, let's create a fictitious instruction <code class="literal">cmpxchg</code>, which has the semantics of comparing two elements and exchanging them when necessary. This is how it would look if we were to implement a compare-swap operation between two elements. In the following example, we illustrate the fact that non-adaptive methods are equivalent to straight line programs for sorting and they can be expressed as a list of compare-exchange operations to be performed.</p><div><pre class="programlisting">cmpxchg(a[0], a[1]);
cmpxchg(a[1], a[2]);
cmpxchg(a[0], a[1]);</pre></div><p>For example, the preceding sequence is a straight line program for sorting three elements; and quite often the goal of developing such an algorithm is to define for each <em>n</em>, a fixed sequence of the <code class="literal">cmpxchg</code> operations that can sort any set of <em>n</em> keys. To put it in another way, the algorithm doesn't take into account whether the data to be sorted is sorted prior or partially sorted.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec64"/>Understanding sorting networks</h1></div></div></div><p>In the previous section, we looked at a non-adaptive sorting algorithm and what it's nature is in its fundamental form. In this section, let's look at a model frequently used to study non-adaptive sorting algorithms. Technical literature has called this model, the sorting network. This form of sorting is also known as comparator networks, and is the idea behind the bitonic sort.</p><p>Sorting networks <a id="id638" class="indexterm"/>are the simplest model for this study, as they represent an abstract machine which accesses the data only through compare-exchange operations, and it comprises of atomic compare-exchanges also known as comparators which are wired together to implement the capability of general sorting.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec144"/>How to do it...</h2></div></div></div><p>The following is an illustration for sorting four keys. By convention, we draw a sorting network for <em>n</em> items as a sequence of <em>n</em> horizontal lines, with comparators connecting a pair of lines. We also imagine that the keys to be sorted pass from right to left through the network, with a pair of numbers exchanged if necessary to put the smaller on the top whenever the comparator is encountered:</p><div><img src="img/4520OT_09_01.jpg" alt="How to do it..."/></div><p>From the preceding diagram, you will notice that the keys move from left to right on the lines in the network. The comparators that they encounter would exchange the keys if necessary and continually push the smaller key towards the top of this network. An astute reader will notice that no exchanges were done on the fourth comparator. This sorting network will sort any permutation of four keys.</p><p>There are other sorting networks other than this and the following network also sorts the same input as before, but it takes two more compare-exchange operations as compared to the previous sorting network. It is interesting to study and that's why this is left as an exercise for you to research on your own.</p><div><img src="img/4520OT_09_02.jpg" alt="How to do it..."/></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec145"/>How it works...</h2></div></div></div><p>This <a id="id639" class="indexterm"/>sorting network exhibits a particular property and that is as long as the comparators do not overlap, then we can actually conduct the compare-exchange operations in parallel. Next, we need to understand how we can exact parallelism from this by grouping what can be done in parallel and needs to be performed in the next stage. Here's the sorting network that is optimal for sorting any four keys and we show the operations that can be conducted in parallel which are broken into three stages of sorting:</p><div><img src="img/4520OT_09_03.jpg" alt="How it works..."/></div><p>Although it is not <a id="id640" class="indexterm"/>the most efficient, the earlier diagram illustrates a possible parallel sorting network<a id="id641" class="indexterm"/> for any four keys. In this parallel sorting network, we could potentially launch threads where it will conduct the compare-exchange operations in three stages, and the result is that the input is sorted.</p><div><div><h3 class="title"><a id="tip32"/>Tip</h3><p>Notice that this sorting network for sorting four keys is optimal from a computational point of view, as it has only to perform five compare-exchange operations in three stages.</p></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec65"/>Understanding bitonic sorting</h1></div></div></div><p>Previously we have discussed sorting networks and it closely relates to bitonic sorting, because sorting networks are employed to implement non-adaptive sorting algorithms, for example, bitonic sort. In <a id="id642" class="indexterm"/>bitonic sorting, we basically have an input (defined elsewhere) that's a bitonic sequence. A bitonic sequence is one that monotonically increases (decreases), reaches a single maximum (minimum), and then monotonically decreases (increases). A sequence is considered bitonic if it can be made so by cyclically shifting the sequence.</p><p>In general, we consider a few scenarios for determining whether the input is suitable for sorting (after all processor cycles are precious and it is a good idea not to waste them doing needless work). In fact, when we wish to sort some input based on a particular sorting algorithm, we would always consider whether the input is already sorted based on our criteria. In the context of bitonic sorting, we could possibly receive a bitonic sequence, and what we do for that is apply what is known as a bitonic split sequence or an arbitrary sequence, in the case of an operation on the input sequence and keep doing this until we reach the final sorted state.</p><div><div><h3 class="title"><a id="note42"/>Note</h3><p>A bitonic split<a id="id643" class="indexterm"/> is an operation on a bitonic sequence, such that if <img src="img/4520OT_09_15.jpg" alt="Understanding bitonic sorting"/> the two elements are exchanged, <img src="img/4520OT_09_18.jpg" alt="Understanding bitonic sorting"/> and the operation produces two bitonic sequences A and B, such that the elements in A are less than the elements in B.</p></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec146"/>How to do it...</h2></div></div></div><p>The diagram shows how two bitonic sequences (at the top of the diagram) can be conceptually combined to a larger sequence (at the bottom of the diagram) by repeated application of this sorting algorithm:</p><div><img src="img/4520OT_09_04.jpg" alt="How to do it..."/></div><p>In the situation where we<a id="id644" class="indexterm"/> receive an arbitrary sequence, that is, unsorted and not in bitonic order, we have to basically produce a bitonic sequence from this unsorted input and then apply the same trick as before using the bitonic splits until we reach the final sorted state. The following diagram illustrates how a bitonic split or merge (as it's often called) operates on separate sequences and produces the final sorted sequence either in ascending or descending order:</p><div><img src="img/4520OT_09_05.jpg" alt="How to do it..."/></div><p>In either case, we will <a id="id645" class="indexterm"/>know when to terminate if the split sizes have reached two, because at this point, it's a comparison operation between a and b, where either a is greater than or equal to b or b is greater than or equal to a. And it holds and depending on the sorting order, we will place them into their appropriate position in the output.</p><p>Bitonic Sorting uses a principle created by Donald Knuth and it's known as the Knuth's 0/1 principle, which is: If a sorting algorithm that performs only element comparisons and exchanges on all sequences of zeros and ones, and then it sorts all sequences of arbitrary numbers.</p><p>Before we proceed to develop the bitonic sort algorithm using OpenCL, it's proper that we only introduce it through its sequential form from which we can begin to look for opportunities for parallelism.</p><p>The following code snippet is from <code class="literal">src/Ch9/BitonicSort_CPU_02/BitonicSort.c</code> and the relevant portions of the code are shown. This implementation is a translation of Batcher's algorithm, that for illustration purpose is a recursive one and looks like this:</p><div><pre class="programlisting">void merge(int a[], int l, int r) {
  int i, m = (l+r)/2;
  if (r == (l+1)) compareXchg(a, l, r);
  if (r &lt; (l+2)) return;

  unshuffle(a, l, r);
  merge(a, l, m);
  merge(a,  m+1, r);
  shuffle(a, l, r);
  // In the original algorithm the statement was the following:
  // for(i = l+1; i &lt; r; i+= 2) compareXchg(a, i, i+1);
  for(i = l; i &lt; r; i+= 2) compareXchg(a, i, i+1);
}</pre></div><p>This recursive <a id="id646" class="indexterm"/>program works is by repeatedly splitting its original input by half and it proceeds to sort each of the halves and merges those halves into bigger segments. This process is continued until the segment reaches the original size. Notice that it uses two other supporting functions to accomplish this and they're called <code class="literal">shuffle</code> and <code class="literal">unshuffle</code>. They work similarly to the same functions in OpenCL (which isn't a wonder because the same functions in OpenCL drew inspiration from them). Here are those functions:</p><div><pre class="programlisting">void shuffle(int a[], int l, int r) {
  int* aux = (int*)malloc(sizeof(int) * r);
  int i, j, m = (l+r)/2;
  for(i = l, j = 0; i &lt;= r; i += 2, j++ ) {
    aux[i] = a[l+j];
    aux[i+1] = a[m+1+j];
  }
  for(i = l; i &lt;= r; i++) a[i] = aux[i];
}

void unshuffle(int a[], int l, int r) {
  int* aux = (int*)malloc(sizeof(int) * r);
  int i, j, m = (l+r)/2;
  for(i = l, j = 0; i &lt;= r; i += 2, j++ ) {
    aux[l+j] = a[i];
    aux[m+1+j] = a[i+1];
  }
  for(i = l; i &lt;= r; i++) a[i] = aux[i];
}
void compareXchg(int* arr, int offset1, int offset2) {
  if (arr[offset1] &gt;= arr[offset2]) {
    int t = arr[offset1];
    arr[offset1] = arr[offset2];
    arr[offset2] = t;
  }
}</pre></div><p>And what they do<a id="id647" class="indexterm"/> is this: shuffling actually splits the input into halves again and picks each element from each half and place them side-by-side until it reaches the end of both halves. Unshuffling does exactly the opposite by removing those elements and placing them into their original positions and for those algorithm geeks in you, you would recognize that this is the program implementation of the top-down mergesort algorithm and belongs to the class of algorithms that uses the divide-and-conquer approach. As a refresher, an illustration is shown in the <em>How it works…</em> section of this recipe, which depicts how both shuffling and un-shuffling works in this algorithm.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec147"/>How it works...</h2></div></div></div><p>The concept of shuffling<a id="id648" class="indexterm"/> and unshuffling<a id="id649" class="indexterm"/> was explored in <a class="link" href="ch04.html" title="Chapter 4. Using OpenCL Functions">Chapter 4</a>, <em>Using OpenCL Functions</em> and we invite you to head back there and refresh yourself with the concepts. The following diagram illustrates how <code class="literal">shuffle</code> and <code class="literal">unshuffle</code> (as defined before) would work given an imaginary input: <strong>8</strong>, <strong>12</strong>, <strong>4</strong>, <strong>15</strong>, <strong>2</strong>, <strong>11</strong>, <strong>6</strong>, <strong>3</strong>, <strong>5</strong>, <strong>14</strong>, <strong>16</strong>, <strong>10</strong>, <strong>1</strong>, <strong>9</strong>, <strong>13</strong>, <strong>7</strong>:</p><div><img src="img/4520OT_09_06.jpg" alt="How it works..."/></div><p>Recursive<a id="id650" class="indexterm"/> algorithms similar to the one we have just presented are good for understanding the general flow of the algorithm, but it doesn't work well when you wish to run this algorithm on OpenCL GPUs because recursion isn't fully supported on GPUs. Even though you were to choose an implementation that runs on the CPU via OpenCL, it'll work but it won't be portable.</p><p>We need an iterative version of this algorithm we just discussed, and fortunately for us we can convert this recursive algorithm to an iterative one. We will look at the following solution from <code class="literal">src/Ch9/BitonicSort_CPU_02/BitonicSort.c</code>:</p><div><pre class="programlisting">void merge_iterative(int a[], int l, int r) {
  int i, j , k, p, N = r -l+1;
  for(p = 1; p &lt; N; p += p)
    for(k = p; k &gt; 0; k /= 2)
     for(j = k%p; j+k &lt; N; j += (k+k))
      for(i = 0; i &lt; k; i++)
        if(j+i+k &lt; N)
          if((j+i)/(p+p) == (j+i+k)/(p+p))
            compareXchg(a, l+j+i, l+j+i+k);
}</pre></div><p>This algorithm is divided into phases indexed by the <code class="literal">p</code> variable. The last phase, which is when <code class="literal">p</code> is <code class="literal">N</code>, and each phase applies the sorting and merging to segments of sizes <code class="literal">N / 2</code>, <code class="literal">N / 4</code>, <code class="literal">N / 8</code> to <img src="img/4520OT_09_20.jpg" alt="How it works..."/>. When <a id="id651" class="indexterm"/>examining this code deeper by tracing the execution flow, you would notice that it is actually computing the sorting network that accepts 32 inputs (corresponding to the number of inputs in our input buffer), and when you read the diagram from left to right, you will notice that it approaches solving this problem in a bottom-up manner:</p><div><img src="img/4520OT_09_07.jpg" alt="How it works..."/></div><p>What I meant by bottom-up approach is that figure should be read from left to right (that's also the flow of the data through this sorting network). When you draw columns around the first column, you'll notice that the algorithm creates segments of sizes two. Then the second and third columns form segments of sizes 4, then the fourth, fifth, and sixth columns form segments of size eight. They continue to form to sort/merge segments of sizes that are a power of two up to the point where it sorts and merges all the <code class="literal">N</code> elements in the input array. You will probably have realized that the algorithm doesn't create any temporary data structures to hold temporary values and it's actually sorting in-place. The immediate consequence of a sorting algorithm that sorts in-place is that it is memory efficient, since the output is written into the input and doesn't create any memory storage at all. The following is an illustration of the partition sizes that the algorithm works on while at every stage:</p><div><img src="img/4520OT_09_08.jpg" alt="How it works..."/></div><p>To develop our <a id="id652" class="indexterm"/>understanding of the bitonic sort and sorting networks, it is important to understand how parallelism can be subsequently extracted from.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec66"/>Developing bitonic sorting in OpenCL</h1></div></div></div><p>In this section, we will walk through an implementation of sorting an arbitrary input by using the <a id="id653" class="indexterm"/>bitonic sort in OpenCL which runs better<a id="id654" class="indexterm"/> on a GPU.</p><p>We recall that bitonic sorting recursively sorts elements in the input by building up sequences and merging those into bigger sized sequences and then repeats the cycle, and the two key operations performs it really does is to conduct: a pairwise comparison to determine the greater/smaller of the two elements in a sequence, and merging the two sequences by applying the bitonic sort between them.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec148"/>Getting ready</h2></div></div></div><p>So far we have seen how we can apply the bitonic sort to bitonic sequences. The question we need to address next is what do we do with an input that is entirely arbitrary? The answer to that question is to make it into a bitonic sequence and then apply a series of bitonic splits/merge. At the beginning, pairwise compare-exchange operations are conducted for elements in the input, and at the end of this stage we have sorted segments of size two. The next stage is to group two segments of size two and perform compare-exchange producing segments of size four. The cycle repeats itself and the algorithm keeps creating bigger segments of size <img src="img/4520OT_09_14.jpg" alt="Getting ready"/>.</p><p>Recall from the <a id="id655" class="indexterm"/>previous section, where we saw the iterative<a id="id656" class="indexterm"/> version of the bitonic sort (the algorithm is repeated here) which uses an array index, <code class="literal">p</code>, to denote the phases in which the sort will take place and with each phase of the algorithm, the algorithm sorts and merges segments of sizes two, four, eight, and so on. And building up on that idea, each phase of the sort is going to be parallel. Also remember that we need to do two things:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Build a comparator network (bitonic split/sort) that sorts two smaller bitonic sequences into a large one, remembering the fact that sizes are powers of two. This pairwise comparison between two elements will be conducted by a single executing thread/work item.</li><li class="listitem" style="list-style-type: disc">Build bitonic sequences on each half, such that one half is monotonically increasing and the other half is monotonically decreasing.</li></ul></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec149"/>How to do it...</h2></div></div></div><p>Our strategy focuses on using a single executable thread performing the compare-exchange operation, and following is the Bitonic Sort OpenCL kernel which uses this simple strategy.</p><p>The following code excerpt is taken from <code class="literal">Ch9/BitonicSort_GPU/BitonicSort.cl</code>:</p><div><pre class="programlisting">
<strong>__kernel</strong>
<strong>void bitonicSort(__global uint * data,</strong>
<strong>                 const uint stage,</strong>
<strong>                 const uint subStage,</strong>
<strong>                 const uint direction) {</strong>

<strong>    uint sortIncreasing = direction;</strong>
<strong>    uint threadId = get_global_id(0);</strong>

<strong>    // Determine where to conduct the bitonic split</strong>
<strong>    // by locating the middle-point of this 1D array</strong>
<strong>    uint distanceBetweenPairs = 1 &lt;&lt; (stage - subStage);</strong>
<strong>    uint blockWidth   = 2 * distanceBetweenPairs;</strong>

<strong>    // Determine the left and right indexes to data referencing</strong>
<strong>    uint leftId = (threadId % distanceBetweenPairs) + </strong>
<strong>                  (threadId / distanceBetweenPairs) * blockWidth;</strong>

<strong>    uint rightId = leftId + distanceBetweenPairs;</strong>

<strong>    uint leftElement = data[leftId];</strong>
<strong>    uint rightElement = data[rightId];</strong>

<strong>    // Threads are divided into blocks of size</strong>
<strong>    // 2^sameDirectionBlockWidth</strong>
<strong>    // and its used to build bitonic subsequences s.t the sorting is </strong>
<strong>    // monotically increasing on the left and decreasing on the right</strong>
<strong>    uint sameDirectionBlockWidth = 1 &lt;&lt; stage;</strong>

<strong>    if((threadId/sameDirectionBlockWidth) % 2 == 1)</strong>
<strong>        sortIncreasing = 1 - sortIncreasing;</strong>

<strong>    uint greater;</strong>
<strong>    uint lesser;</strong>
<strong>    // perform pairwise comparison between two elements and depending </strong>
<strong>    // whether its to build the bitonic that is monotically increasing</strong>
<strong>    // and decreasing.</strong>
<strong>    if(leftElement &gt; rightElement) {</strong>
<strong>        greater = leftElement;</strong>
<strong>        lesser  = rightElement;</strong>
<strong>    } else {</strong>
<strong>        greater = rightElement;</strong>
<strong>        lesser  = leftElement;</strong>
<strong>    }</strong>

<strong>    if(sortIncreasing) {</strong>
<strong>        input[leftId]  = lesser;</strong>
<strong>        input[rightId] = greater;</strong>
<strong>    } else {</strong>
<strong>        input[leftId]  = greater;</strong>
<strong>        input[rightId] = lesser;</strong>
<strong>    }</strong>
<strong>}</strong>
</pre></div><p>Using the preceding OpenCL kernel code we need to build an executable, so that it can execute on our platform. As before, the compilation will look familiar to you. On my setup with an Intel Core i7 CPU and AMD HD6870x2 GPU running Ubuntu 12.04 LTS, the compilation looks as follows, and it'll create an executable called <code class="literal">BitonicSort</code> into the working directory:</p><div><pre class="programlisting">
<strong>gcc -std=c99 -Wall -DUNIX -g -DDEBUG -arch i386 -o BitonicSort -framework OpenCL</strong>
</pre></div><p>At this point, you should have an executable deposited in that directory. All you need to do now is to run the program, simply execute the <code class="literal">BitonicSort</code> program in the directory and you should have noticed an output that resembles this:</p><div><pre class="programlisting">
<strong>Passed!</strong>
<strong>Execution of the Bitonic Sort took X.Xs</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec150"/>How it works...</h2></div></div></div><p>The algorithm <a id="id657" class="indexterm"/>starts from the basic strategy of using a thread to <a id="id658" class="indexterm"/>conduct the pairwise comparison-exchange operation. The details is that the host code will break down the original input into its respective phases, and for our testing purposes we have an input of 16 million elements which works out to 24 phases. In the host code, we use the <code class="literal">stage</code> variable to indicate that. Next at each phase, the algorithm will apply the bitonic split/sort and merge segments of sizes progressively from the least power of two to the greatest power of two, smaller or equal to the phases, for example if we are sorting for elements of size eight, then we would sort to produce segments of size two, then four, and finally we will sort and merge 4-by-4 sequences to get eight.</p><p>In detail when the kernel starts executing, it has to start building the bitonic subsequences by using the bitonic split. And to do that the kernel needs to know where to split the array, taking into account the current stage of the sort and it does this with the following code:</p><div><pre class="programlisting">
<strong>    uint distanceBetweenPairs = 1 &lt;&lt; (stage - subStage);</strong>
<strong>    uint blockWidth   = 2 * distanceBetweenPairs;</strong>

<strong>    // Determine the left and right indexes to data referencing</strong>
<strong>    uint leftId = (threadId % distanceBetweenPairs) + </strong>
<strong>                  (threadId / distanceBetweenPairs) * blockWidth;</strong>

<strong>    uint rightId = leftId + distanceBetweenPairs;</strong>
</pre></div><p>Next, the kernel loads the data values from the array by using the <code class="literal">leftId</code> and <code class="literal">rightId</code> indices and stores them in the thread's local register memory. The next part of the algorithm is to build bitonic sequences, such that one half is monotonically increasing and the other half is monotonically decreasing. And we use the variable, <code class="literal">sameDirectionBlockWidth</code>, as a heuristic to guide whether we are going to sort increasingly or decreasingly. The following code does that:</p><div><pre class="programlisting">
<strong>    uint sameDirectionBlockWidth = 1 &lt;&lt; stage;</strong>

<strong>    if((threadId/sameDirectionBlockWidth) % 2 == 1)</strong>
<strong>        sortIncreasing = 1 - sortIncreasing;</strong>
</pre></div><p>As an example, let's assume that stage is three which implies that <code class="literal">sameDirectionBlockWidth</code> is eight. The following figure demonstrates what will eventually happen when the <code class="literal">sortIncreasing</code> variable flips based on the (above) computation, and<a id="id659" class="indexterm"/> hence creates the desired effect of bitonic<a id="id660" class="indexterm"/> sequencing:</p><div><img src="img/4520OT_09_09.jpg" alt="How it works..."/></div><p>The rest of the kernel code is concerned with the pairwise comparison-exchange operation, which we are familiar with by now.</p><p>Another aspect of this implementation is that the algorithm is compute bound and it's executed iteratively on the OpenCL GPU via the CPU, and the kernel is notified of which stage it's at including its substages. This can be accomplished in the host code like this:</p><div><pre class="programlisting">for(cl_uint stage = 0; stage &lt; stages; ++stage) {
  clSetKernelArg(kernel, 1, sizeof(cl_uint),(void*)&amp;stage);

  for(cl_uint subStage = 0; subStage &lt; stage +1; subStage++) {
    clSetKernelArg(kernel, 2, sizeof(cl_uint),(void*)&amp;subStage);
                cl_event exeEvt;
                cl_ulong executionStart, executionEnd;
                error = clEnqueueNDRangeKernel(queue,
                                               kernel,
                                               1,
                                               NULL,
                                               globalThreads,
                                               threadsPerGroup,
                                               0,
                                               NULL,
                                               &amp;exeEvt);
                clWaitForEvents(1, &amp;exeEvt);</pre></div><p>The code basically<a id="id661" class="indexterm"/> iterates over all the stages and its substages, and<a id="id662" class="indexterm"/> invokes the GPU to work on the same input buffer notifying the kernel which stage and substage the kernel is executing by invoking <code class="literal">clSetKernelArg</code> for the appropriate parameter. And then waits until the sorting is done in that phase before starting work on another (this is critical, otherwise the input buffer would be corrupted). In order to make the input buffer be both readable and writeable by the algorithm, it was created like this:</p><div><pre class="programlisting">device_A_in = clCreateBuffer(context,
                             CL_MEM_READ_WRITE|CL_MEM_COPY_HOST_PTR,
                             LENGTH * sizeof(cl_int),
                             host_A_in,
                             &amp;error);</pre></div><p>The execution of this algorithm will see the execution flow entering the host, and then leaving for the GPU and continuing to do this until the stages run out. This process is illustrated in the following diagram, though it cannot be scaled:</p><div><img src="img/4520OT_09_10.jpg" alt="How it works..."/></div><p>We can actually <a id="id663" class="indexterm"/>apply an optimization on this kernel by employing <a id="id664" class="indexterm"/>a technique we have understood quite well so far, and that is using the shared memory. Shared memory, as you probably know by now, allows the developer to reduce global memory traffic since the program does not have to repeatedly request elements from the global memory space, but instead use what has been stored in its internal memory. Here's a refresher on how the memory model in OpenCL looks like:</p><div><img src="img/4520OT_09_11.jpg" alt="How it works..."/></div><p>Applying the <a id="id665" class="indexterm"/>techniques we have learnt so far, we actually<a id="id666" class="indexterm"/> have one possible point in which we can apply shared memory techniques by looking out for code that is fetching data from the global memory. We will develop a solution using shared memory and expanding it slightly to have our program load it in strides. We'll get into that in a short while. Let's start at a plausible point for reworking our <code class="literal">bitonicSort</code> program taking into account the presence of shared memory:</p><div><pre class="programlisting">uint leftElement = data[leftId];
uint rightElement = data[rightId];</pre></div><p>We present the following kernel that uses shared memory, we'll explain how it works, found in<code class="literal"> Ch9/BitonicSort_GPU/BitonicSort.cl</code>:</p><div><pre class="programlisting">__kernel
void bitonicSort_sharedmem(__global uint * data,
                           const uint stage,
                           const uint subStage,
                           const uint direction,
                           __local uint* sharedMem) {
    // more code omitted here
    // Copy data to shared memory on device
    if (threadId == 0) {
        sharedMem[threadId] = data[leftId];
        sharedMem[threadId+1] = data[rightId];
    } else {
        sharedMem[threadId+1] = data[leftId];
        sharedMem[threadId+2] = data[rightId];
    }
    barrier(CLK_LOCAL_MEM_FENCE);
    
    // more code omitted
    uint greater;
    uint lesser;

    if (threadId == 0) {
        if(sharedMem[threadId] &gt; sharedMem[threadId+1]) {
            greater = sharedMem[threadId];
            lesser  = sharedMem[threadId+1];
        } else {
            greater = sharedMem[threadId+1];
            lesser  = sharedMem[threadId];
        }
    } else {
        if(sharedMem[threadId+1] &gt; sharedMem[threadId+2]) {
            greater = sharedMem[threadId+1];
            lesser  = sharedMem[threadId+2];
        } else {
            greater = sharedMem[threadId+2];
            lesser  = sharedMem[threadId+1];
        }
    }</pre></div><p>What we did basically was to introduce a variable called <code class="literal">sharedMem</code> and the strategy for loading those values is <a id="id667" class="indexterm"/>simple: each thread will store two <a id="id668" class="indexterm"/>values (adjacent) in the shared memory data store, where it will be read out in the subsequent section and all reads which used to refer to the global memory is now conducted in the local/shared memory.</p><p>The host code that is responsible for allocating this memory space is the following code snippet from <code class="literal">Ch9/BitonicSort_GPU/BitonicSort.c</code> taking into account that each thread writes two adjacent values. And hence it requires twice the amount of memory for a work group of 256 threads:</p><div><pre class="programlisting">#ifdef USE_SHARED_MEM
clSetKernelArg(kernel, 4, (GROUP_SIZE &lt;&lt; 1) *sizeof(cl_uint),NULL);
#endif</pre></div><p>And to see it in action you can compile the program like this (invoking <code class="literal">gcc</code> directly):</p><div><pre class="programlisting">
<strong>gcc -DUSE_SHARED_MEM -Wall -std=c99 -lOpenCL ./BitonicSort.c -o BitonicSort_GPU</strong>
</pre></div><p>This deposits<a id="id669" class="indexterm"/> the <code class="literal">BitonicSort_GPU</code> program into that directory; another way is to invoke <code class="literal">cmake</code> at the root of this code base like this:</p><div><pre class="programlisting">
<strong>cmake –DUSE_SHARED_MEM=1 –DDEBUG .</strong>
</pre></div><p>And navigate to <code class="literal">Ch9/BitonicSort_GPU/</code> and invoke <code class="literal">make</code> like this:</p><div><pre class="programlisting">
<strong>make clean;make</strong>
</pre></div><p>The following is a <a id="id670" class="indexterm"/>diagram of how the writes to the shared memory are done with respect to the scheme we just described. Remember that all subsequent reads is through <code class="literal">sharedMem</code> instead of the global memory traffic, which means that a significant amount of bandwidth is saved:</p><div><img src="img/4520OT_09_12.jpg" alt="How it works..."/></div><p>We can explore the algorithm a little further by examining the original kernel, <code class="literal">bitonicSort</code>, where the last part of the algorithm involves essentially a comparison-exchange operation before writing that result back out to global memory. In this situation, we can extrapolate the shared memory concept further by applying it again and our strategy is rather simple here: we have each executing thread writing two pairs, where each pair is this [<img src="img/4520OT_09_19.jpg" alt="How it works..."/>], and referenced by a key and a value. And in our algorithm the key refers to the output index (that is, <code class="literal">leftId</code>, <code class="literal">rightId</code>) and the value refers to the sorted value (that is, <code class="literal">lesser</code>, <code class="literal">greater</code>) that will reside at that key. The following diagram illustrates how each thread would have written the two pairs into the <code class="literal">aux</code> shared memory, and how they could be laid out in memory:</p><div><img src="img/4520OT_09_13.jpg" alt="How it works..."/></div><p>The following <a id="id671" class="indexterm"/>kernel <a id="id672" class="indexterm"/>modifications are found at <code class="literal">Ch9/BitonicSort_GPU/BitonicSort.cl</code> in the kernel named <code class="literal">bitonicSort_sharedmem_2</code>. We will look at the portions where the changes were different relative to the <code class="literal">bitonicSort_sharedmem</code> kernel:</p><div><pre class="programlisting">    // Each thread will write the data elements to its own
    // partition of the shared storage without conflicts.
    const uint stride = 4;
    if(sortIncreasing) {
        aux[threadId*stride] = leftId;
        aux[threadId*stride+1] = lesser;
        aux[threadId*stride+2] = rightId;
        aux[threadId*stride+3] = greater;
    } else {
        aux[threadId*stride] = leftId;
        aux[threadId*stride+1] = greater;
        aux[threadId*stride+2] = rightId;
        aux[threadId*stride+3] = lesser;
    }
    barrier(CLK_LOCAL_MEM_FENCE);

    if(threadId == 0) {
        for(int i = 0; i &lt; GROUP_SIZE * stride; ++i) {
           data[aux[i*stride]] = aux[i*stride+1];
           data[aux[i*stride+2]] = aux[i*stride+3];
        }
    }</pre></div><p>The final section of <a id="id673" class="indexterm"/>the kernel illustrates how we allow only <a id="id674" class="indexterm"/>one executing thread, that is, the thread with ID zero, from each work group to conduct the actual write back to global memory from the shared memory, <code class="literal">aux</code>. Do note that the memory fence is necessary, since the memory in <code class="literal">aux</code> may not have been filled by the time the thread with ID zero has begun execution. Therefore, it's placed there to ensure memory coherency.</p></div></div></body></html>