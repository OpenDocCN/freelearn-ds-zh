- en: 'Chapter 3: Linguistic Features'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章：语言特征
- en: This chapter is a deep dive into the full power of spaCy. You will discover
    the linguistic features, including spaCy's most commonly used features such as
    the **part-of-speech (POS) tagger**, the **dependency parser**, the **named entity
    recognizer**, and **merging/splitting** features.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了spaCy的全部功能。你将发现语言特征，包括spaCy最常用的功能，如**词性（POS）标签器**、**依存句法分析器**、**命名实体识别器**以及**合并/拆分**功能。
- en: First, you'll learn the POS tag concept, how the spaCy POS tagger functions,
    and how to place POS tags into your **natural-language understanding** (**NLU**)
    applications. Next, you'll learn a structured way to represent the sentence syntax
    through the dependency parser. You'll learn about the dependency labels of spaCy
    and how to interpret the spaCy dependency labeler results with revealing examples.
    Then, you'll learn a very important NLU concept that lies at the heart of many
    **natural language processing** (**NLP**) applications—**named entity recognition**
    (**NER**). We'll go over examples of how to extract information from the text
    using NER. Finally, you'll learn how to merge/split the entities you extracted.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将学习POS标签的概念，了解spaCy POS标签器的功能，以及如何将POS标签放入你的**自然语言理解（NLU**）应用中。接下来，你将学习通过依存句法分析器以结构化的方式表示句子句法。你将了解spaCy的依存标签以及如何通过揭示性示例来解释spaCy依存标签器的结果。然后，你将学习一个非常重要的NLU概念，它是许多**自然语言处理（NLP**）应用的核心——**命名实体识别（NER**）。我们将通过NER从文本中提取信息的示例。最后，你将学习如何合并/拆分你提取的实体。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: What is POS tagging?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是POS标签？
- en: Introduction to dependency parsing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依存句法分析简介
- en: Introducing NER
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍NER
- en: Merging and splitting tokens
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合并和拆分标记
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The chapter code can be found at the book''s GitHub repository: [https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter03](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter03)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter03](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter03)
- en: What is POS tagging?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是POS标签？
- en: 'We saw the terms *POS tag* and *POS tagging* briefly in the previous chapter,
    while discussing the spaCy `Token` class features. As is obvious from the name,
    they refer to the process of tagging tokens with POS tags. One question remains
    here: *What is a POS tag?* In this section, we''ll discover in detail the concept
    of POS and how to make the most of it in our NLP applications.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章讨论spaCy `Token`类功能时，我们简要地看到了术语*POS标签*和*POS标签化*。从名称上看，它们指的是将POS标签标记到标记上的过程。这里仍有一个问题：*什么是POS标签？*在本节中，我们将详细探讨POS的概念以及如何在我们的NLP应用中充分利用它。
- en: 'The **POS tagging** acronym expands as **part-of-speech tagging**. A **part
    of speech** is a syntactic category in which every word falls into a category
    according to its function in a sentence. For example, English has nine main categories:
    verb, noun, pronoun, determiner, adjective, adverb, preposition, conjunction,
    and interjection. We can describe the functions of each category as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**POS标签**的缩写是**part-of-speech tagging**。**词性**是一个句法类别，每个词都根据其在句子中的功能落入一个类别。例如，英语有九个主要类别：动词、名词、代词、限定词、形容词、副词、介词、连词和感叹词。我们可以描述每个类别的功能如下：'
- en: '**Verb**: Expresses an action or a state of being'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动词**：表示动作或存在状态'
- en: '**Noun**: Identifies a person, a place, or a thing, or names a particular of
    one of these (a proper noun)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**名词**：识别人、地点或事物，或命名这些中的一个（专有名词）'
- en: '**Pronoun**: Can replace a noun or noun phrase'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代词**：可以替换名词或名词短语'
- en: '**Determiner**: Is placed in front of a noun to express a quantity or clarify
    what the noun refers to—briefly, a noun introducer'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**限定词**：放在名词前面以表达数量或阐明名词所指的内容——简而言之，名词引入者'
- en: '**Adjective**: Modifies a noun or a pronoun'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**形容词**：修饰名词或代词'
- en: '**Adverb**: Modifies a verb, an adjective, or another adverb'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**副词**：修饰动词、形容词或另一个副词'
- en: '**Preposition**: Connects a noun/pronoun to other parts of the sentence'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**介词**：将名词/代词与其他句子部分连接起来'
- en: '**Conjunction**: Glues words, clauses, and sentences together'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连词**：将单词、从句和句子粘合在一起'
- en: '**Interjection**: Expresses emotion in a sudden and exclamatory way'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**感叹词**：以突然和感叹的方式表达情感'
- en: 'This core set of categories, without any language-specific morphological or
    syntactic features, are called `pos_` feature and describes them with examples,
    as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个核心类别集，没有任何语言特定的形态学或句法特征，被称为`pos_`特征，并以下列示例描述它们：
- en: '![Figure 3.1 – spaCy universal tags explained with examples'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.1 – 使用示例解释spaCy通用标签]'
- en: '](img/B16570_03_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![图B16570_03_01.jpg]'
- en: Figure 3.1 – spaCy universal tags explained with examples
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.1 – 使用示例解释spaCy通用标签]'
- en: Throughout the book, we are providing examples with the English language and,
    in this section, we'll therefore focus on English. Different languages offer different
    tagsets, and spaCy supports different tagsets via `tag_map.py` under each language
    submodule. For example, the current English tagset lies under `lang/en/tag_map.py`
    and the German tagset lies under `lang/de/tag_map.py`. Also, the same language
    can support different tagsets; for this reason, spaCy and other NLP libraries
    always *specify* which tagset they use. The spaCy English POS tagger uses the
    `Ontonotes 5` tagset, and the German POS tagger uses the `TIGER Treebank` tagset.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们提供了使用英语的示例，因此在本节中，我们将专注于英语。不同的语言提供不同的标签集，spaCy通过每个语言子模块下的`tag_map.py`支持不同的标签集。例如，当前的英语标签集位于`lang/en/tag_map.py`下，德语标签集位于`lang/de/tag_map.py`下。此外，同一种语言可以支持不同的标签集；因此，spaCy和其他NLP库总是*指定*它们使用的标签集。spaCy的英语POS标签器使用`Ontonotes
    5`标签集，德语的POS标签器使用`TIGER Treebank`标签集。
- en: Each supported language of spaCy admits its own fine-grained tagset and tagging
    scheme, a specific tagging scheme that usually covers morphological features,
    tenses and aspects of verbs, number of nouns (singular/plural), person and number
    information of pronouns (first-, second-, third-person singular/plural), pronoun
    type (personal, demonstrative, interrogative), adjective type (comparative or
    superlative), and so on.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy支持的每种语言都有自己的精细粒度标签集和标记方案，这是一种特定的标记方案，通常涵盖形态学特征、动词的时态和语态、名词的数量（单数/复数）、代词的人称和数量信息（第一、第二、第三人称单数/复数）、代词类型（人称、指示、疑问）、形容词类型（比较级或最高级）等等。
- en: 'spaCy supports fine-grained POS tags to answer language-specific needs, and
    the `tag_` feature corresponds to the fine-grained tags. The following screenshot
    shows us a part of these fine-grained POS tags and their mappings to more universal
    POS tags for English:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy支持精细粒度的POS标签以满足语言特定的需求，`tag_`特征对应于精细粒度标签。以下截图展示了这些精细粒度POS标签及其对英语更通用POS标签的映射：
- en: '![Figure 3.2 – Fine-grained English tags and universal tag mappings'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.2 – 精细粒度的英语标签和通用标签映射]'
- en: '](img/B16570_03_02.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![图B16570_03_02.jpg]'
- en: Figure 3.2 – Fine-grained English tags and universal tag mappings
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 – 精细粒度的英语标签和通用标签映射
- en: 'Don''t worry if you haven''t worked with POS tags before, as you''ll become
    familiar by practicing with the help of our examples. We''ll always include explanations
    of the tags that we use. You can also call `spacy.explain()` on the tags. We usually
    call `spacy.explain()` in two ways, either directly on the tag name string or
    with `token.tag_`, as illustrated in the following code snippet:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您之前没有使用过POS标签，不要担心，因为通过我们的示例练习，您将变得熟悉。我们总是会包括对我们使用的标签的解释。您也可以在标签上调用`spacy.explain()`。我们通常以两种方式调用`spacy.explain()`，要么直接在标签名称字符串上，要么使用`token.tag_`，如下面的代码片段所示：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you want to know more about POS, you can read more about it at two excellent
    resources: *Part of Speech* at http://partofspeech.org/, and the *Eight Parts
    of Speech* at [http://www.butte.edu/departments/cas/tipsheets/grammar/parts_of_speech.html](http://www.butte.edu/departments/cas/tipsheets/grammar/parts_of_speech.html).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于POS的信息，你可以在两个优秀的资源中了解更多：http://partofspeech.org/上的*词性*，以及[http://www.butte.edu/departments/cas/tipsheets/grammar/parts_of_speech.html](http://www.butte.edu/departments/cas/tipsheets/grammar/parts_of_speech.html)上的*八种词性*。
- en: As you can see, POS tagging offers a very basic syntactic understanding of the
    sentence. POS tags are used in NLU extensively; we frequently want to find the
    verbs and the nouns in a sentence and better disambiguate some words for their
    meanings (more on this subject soon).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，POS标签提供了对句子非常基本的句法理解。POS标签在NLU中被广泛使用；我们经常想要找到句子中的动词和名词，并更好地区分一些词的意义（关于这个话题很快会有更多讨论）。
- en: Each word is tagged by a POS tag depending on its *context*—the other surrounding
    words and their POS tags. POS taggers are sequential statistical models, which
    means *that a tag of a word depends on the word-neighbor tokens, their tags, and
    the word itself*. POS tagging has always been done in different forms. **Sequence-to-sequence
    learning** (**Seq2seq**) started with **Hidden Markov Models** (**HMMs**) in the
    early days and evolved to neural network models—typically, **long short-term memory**
    (**LSTM**) variations (spaCy also uses an LSTM variation). You can witness the
    evolution of state-of-art POS tagging on the ACL website ([https://aclweb.org/aclwiki/POS_Tagging_(State_of_the_art](https://aclweb.org/aclwiki/POS_Tagging_(State_of_the_art))).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词都根据其 *上下文*（其他周围的单词及其 POS 标签）被标记为一个 POS 标签。POS 标签器是顺序统计模型，这意味着 *一个单词的标签取决于其词邻标记、它们的标签以及单词本身*。POS
    标记一直以不同的形式进行。**序列到序列学习**（**Seq2seq**）始于早期的**隐马尔可夫模型**（**HMMs**），并演变为神经网络模型——通常是**长短期记忆**（**LSTM**）变体（spaCy
    也使用 LSTM 变体）。您可以在 ACL 网站上见证最先进 POS 标记的发展（[https://aclweb.org/aclwiki/POS_Tagging_(State_of_the_art](https://aclweb.org/aclwiki/POS_Tagging_(State_of_the_art))）。
- en: 'It''s time for some code now. Again, spaCy offers universal POS tags via the
    `token.pos (int)` and `token.pos_ (unicode)` features. The fine-grained POS tags
    are available via the `token.tag (int)` and `token.tag_ (unicode)` features. Let''s
    learn more about tags that you''ll come across most, through some examples. The
    following example includes examples of noun, proper noun, pronoun, and verb tags:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看一些代码了。同样，spaCy 通过 `token.pos (int)` 和 `token.pos_ (unicode)` 特性提供通用 POS
    标签。细粒度 POS 标签可通过 `token.tag (int)` 和 `token.tag_ (unicode)` 特性获得。让我们通过一些例子来了解更多您将遇到最多的标签。以下例子包括名词、专有名词、代词和动词标签的例子：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We iterated over the tokens and printed the tokens'' text, universal tag, and
    fine-grained tag, together with the explanations, which are outlined here:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历了标记并打印了标记的文本、通用标签和细粒度标签，以及在此概述的解释：
- en: '`Alicia` is a proper noun, as expected, and `NNP` is a tag for proper nouns.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Alicia` 是一个专有名词，正如预期的那样，`NNP` 是专有名词的标签。'
- en: '`me` is a pronoun and `bus` is a noun. `NN` is a tag for singular nouns and
    `PRP` is a personal pronoun tag.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`me` 是一个代词，`bus` 是一个名词。`NN` 是单数名词的标签，`PRP` 是个人代词标签。'
- en: Verb tags start with `V`. Here, `VBD` is a tag for *went*, which is a past-tense
    verb.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动词标签以 `V` 开头。在这里，有三个动词，如下所示：
- en: 'Now, consider the following sentence:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑以下句子：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s start with the verbs. As we pointed out in the first example, verb tags
    start with `V`. Here, there are three verbs, as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从动词开始。正如我们在第一个例子中指出的那样，动词标签以 `V` 开头。在这里有三个动词，如下所示：
- en: '`fly`: a base form'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fly`：基本形式'
- en: '`staying`: an *-ing* form'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`staying`：一个 *-ing* 形式'
- en: '`is`: an auxiliary verb'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is`：一个助动词'
- en: The corresponding tags are `VB`, `VBG`, and `VBZ`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的标签是 `VB`、`VBG` 和 `VBZ`。
- en: Another detail is both `New` and `York` are tagged as proper nouns. If a proper
    noun consists of multiple tokens, then all the tokens admit the tag `NNP`. `My`
    is a possessive pronoun and is tagged as `PRP$`, in contrast to the preceding
    personal pronoun `me` and its tag `PRP`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个细节是 `New` 和 `York` 都被标记为专有名词。如果一个专有名词由多个标记组成，那么所有标记都接受 `NNP` 标签。`My` 是一个物主代词，被标记为
    `PRP$`，与前面的个人代词 `me` 及其标签 `PRP` 相比。
- en: 'Let''s continue with a word that can be a verb or noun, depending on the context:
    `ship`. In the following sentence, `ship` is used as a verb:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续一个可以根据上下文是动词或名词的单词：`ship`。在以下句子中，`ship` 被用作动词：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, `ship` is tagged as a verb, as we expected. Our next sentence also contains
    the word `ship`, but as a noun. Now, can the spaCy tagger tag it correctly? Have
    a look at the following code snippet to find out:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`ship` 被标记为动词，正如我们所预期的那样。我们的下一句话也包含了单词 `ship`，但作为名词。现在，spaCy 标签器能否正确地标记它？请看下面的代码片段以了解详情：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Et voilà*! This time, the word `ship` is now tagged as a noun, as we wanted
    to see. The tagger looked at the surrounding words; here, `ship` is used with
    a determiner and an adjective, and spaCy deduced that it should be a noun.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*Et voilà*！这次，单词 `ship` 现在被标记为名词，正如我们想要看到的那样。标签器检查了周围的单词；在这里，`ship` 与一个限定词和一个形容词一起使用，spaCy
    推断它应该是一个名词。'
- en: 'How about this tricky sentence:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个句子怎么样：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We wanted to fool the tagger with the different usages of the word `fish`,
    but the tagger is intelligent enough to distinguish the verb `fish`, the noun
    `fish`, and the adjective `fishy`. Here''s how it did it:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想用单词 `fish` 的不同用法来愚弄标签器，但标签器足够智能，能够区分动词 `fish`、名词 `fish` 和形容词 `fishy`。这是它如何做到的：
- en: Firstly, `fish` comes right after the modal verb `will`, so the tagger recognized
    it as a verb.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，`fish`紧随情态动词`will`之后，因此标注器将其识别为动词。
- en: Secondly, `fish` serves as the object of the sentence and is qualified by a
    determiner; the tag is most probably a noun.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，`fish`作为句子的宾语，并被限定词修饰；标签最有可能是名词。
- en: Finally, `fishy` ends in `y` and comes before a noun in the sentence, so it's
    clearly an adjective.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，`fishy`以`y`结尾，并在句子中位于名词之前，因此它显然是一个形容词。
- en: 'The spaCy tagger made a very smooth job here of predicting a tricky sentence.
    After examples of very accurate tagging, only one question is left in our minds:
    *Why do we need the POS tags?*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy标注器在这里对预测一个棘手的句子做了非常顺利的工作。在非常准确的标注示例之后，我们心中只剩下一个问题：*为什么我们需要词性标注？*
- en: 'What is the importance of POS tags in NLU, and why do we need to distinguish
    the class of the words anyway? The answer is simple: many applications need to
    know the word type for better accuracy. Consider machine translation systems for
    an example of this: the words for `fish (V)` and `fish (N)` correspond to different
    words in Spanish, as illustrated in the following code snippet:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注在自然语言理解中的重要性是什么，我们为什么需要区分词的类别呢？答案很简单：许多应用程序需要知道词的类型以获得更好的准确性。以机器翻译系统为例：`fish
    (V)`和`fish (N)`在西班牙语中对应不同的单词，如下面的代码片段所示：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Syntactic information can be used in many NLU tasks, and playing some POS tricks
    can help your NLU code a lot. Let''s continue with a traditional problem: **word-sense
    disambiguation** (**WSD**), and how to tackle it with the help of the spaCy tagger.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 句法信息可以在许多自然语言理解任务中使用，而玩一些词性标注的小技巧可以帮助你的自然语言理解代码。让我们继续一个传统问题：**词义消歧**（**WSD**），以及如何借助spaCy标注器来解决这个问题。
- en: WSD
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词义消歧
- en: '**WSD** is a classical NLU problem of deciding in which *sense* a particular
    word is used in a sentence. A word can have many senses—for instance, consider
    the word *bass*. Here are some senses we can think of:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**词义消歧**（**WSD**）是在句子中决定一个特定单词使用的哪个**词义**的经典自然语言理解问题。一个词可以有多个词义——例如，考虑单词*bass*。以下是我们能想到的一些词义：'
- en: Bass—seabass, fish (`N`))
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低音——海鲈鱼，鱼（`N`）
- en: Bass—lowest male voice (`N`)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低音——最低的男性声音（`N`）
- en: Bass—male singer with lowest voice range (`N`)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低音——音域最低的男性歌手（`N`）
- en: 'Determining the sense of the word can be crucial in search engines, machine
    translation, and question-answering systems. For the preceding example, *bass*,
    a POS tagger is unfortunately not much of help as the tagger labels all senses
    with a noun tag. We need more than a POS tagger. How about the word *beat*? Let''s
    have a look at this here:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 确定单词的词义在搜索引擎、机器翻译和问答系统中可能至关重要。对于先前的例子，不幸的是，词性标注器对*bass*的帮助不大，因为标注器将所有词义都标记为名词。我们需要比词性标注器更多的东西。那么，单词*beat*怎么样？让我们来看看这个例子：
- en: Beat—to strike violently (`V`))
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 击败——猛烈打击（`V`）
- en: Beat—to defeat someone else in a game or a competition (`V`)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 击败——在游戏或比赛中击败某人（`V`）
- en: Beat—rhythm in music or poetry (`N`)
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 击败——音乐或诗歌中的节奏（`N`）
- en: Beat—bird wing movement (`N`)
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 击败——鸟的翅膀动作（`N`）
- en: Beat—completely exhausted (`ADJ`))
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 击败——完全耗尽（`ADJ`）
- en: Here, POS tagging can help a lot indeed. The `ADJ` tag determines the word sense
    definitely; if the word *beat* is tagged as `ADJ`, it identifies the sense *completely
    exhausted*. This is not true for the `V` and `N` tags here; if the word *beat*
    is labeled with a `V` tag, its sense can be *to strike violently* or *to defeat
    someone else*. WSD is an open problem, and many complicated statistical models
    are proposed. However, if you need a quick prototype, you can tackle this problem
    in some cases (such as in the preceding example) with the help of the spaCy tagger.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，词性标注确实能帮上大忙。`ADJ`标签明确地决定了词义；如果单词*beat*被标注为`ADJ`，它识别的词义是*完全耗尽*。这里的`V`和`N`标签并非如此；如果单词*beat*被标记为`V`标签，其词义可以是*猛烈打击*或*击败某人*。词义消歧是一个开放性问题，已经提出了许多复杂的统计模型。然而，如果你需要一个快速的原型，你可以在某些情况下（例如在先前的例子中）借助spaCy标注器来解决这个问题。
- en: Verb tense and aspect in NLU applications
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言理解应用中的动词时态和语气
- en: In the previous chapter, we used the example of the travel agency application
    where we got the base forms (which are freed from verb tense and aspect) of the
    verbs by using **lemmatization**. In this subsection, we'll focus on how to use
    the verb tense and aspect information that we lost during the lemmatization process.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们通过使用**词形还原**的例子，从旅行社应用程序中获得了动词的基础形式（这些形式摆脱了时态和语气）。在本小节中，我们将关注如何使用在词形还原过程中丢失的动词时态和语气信息。
- en: '**Verb tense** and **aspect** are maybe the most interesting information that
    verbs provide us, telling us when the action happened in time and if the action
    of the verb is finished or ongoing. Tense and aspect together indicate a verb''s
    reference to the current time. English has three basic tenses: past, present,
    and future. A tense is accompanied by either simple, progressive/continuous, or
    perfect aspects. For instance, in the sentence *I''m eating*, the action *eat*
    happens in the present and is ongoing, hence we describe this verb as *present
    progressive/continuous*.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**动词时态**和**体**可能是动词提供给我们最有趣的信息，告诉我们动作何时发生以及动词的动作是已完成还是正在进行的。时态和体一起表明动词对当前时间的参照。英语有三种基本时态：过去、现在和未来。时态伴随着简单、进行/连续或完成体。例如，在句子“我在吃饭”中，动作“吃”发生在现在并且是持续的，因此我们描述这个动词为“现在进行/连续”。 '
- en: 'So far, so good. So, how do we use this information in our travel agency NLU,
    then? Consider the following customer sentences that can be directed to our NLU
    application:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利。那么，我们如何在我们的旅行社NLU中使用这些信息呢？考虑以下可以指向我们的NLU应用的客户句子：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In all the sentences, the action is *to fly*: however, only some sentences
    state intent to make a ticket booking. Let''s imagine these sentences with a surrounding
    context, as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有句子中，动作是“飞”：然而，只有一些句子表示有订票意图。让我们想象以下带有周围上下文的句子：
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'At a quick glance, past and perfect forms of the verb *fly* don''t indicate
    a booking intent at all. Rather, they direct to either a customer complaint or
    customer service issues. The infinitive and present progressive forms, on the
    other hand, point to booking intent. Let''s tag and lemmatize the verbs with the
    following code segment:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 快速浏览一下，动词“fly”的过去式和完成式根本不表示预订意图。相反，它们指向客户投诉或客户服务问题。另一方面，不定式和现在进行时形式则指向预订意图。让我们用以下代码段标记和词干化动词：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We iterated three `doc` objects one by one, and for each sentence we checked
    if the fine-grained tag of the token is `VBG` (a verb in present progressive form)
    or `VB` (a verb in base/infinitive form). Basically, we filtered out the present
    progressive and infinitive verbs. You can think of this process as a semantic
    representation of the verb in the form of `(word form, lemma, tag)` as illustrated
    in the following code snippet:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们逐个迭代了三个`doc`对象，并对每个句子检查标记的细粒度标签是否为`VBG`（现在进行时的动词）或`VB`（基本/不定式形式的动词）。基本上，我们过滤出了现在进行时和基本形式的动词。你可以将这个过程视为动词的语义表示，形式为`(词形，词干，标签)`，如下面的代码片段所示：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We have covered one semantic and one morphological task—WSD and tense/aspect
    of verbs. We''ll continue with a tricky subject: how to make the best of some
    special tags—namely, number, symbol, and punctuation tags.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经覆盖了一个语义任务和一个形态学任务——词义消歧和动词的时态/体。我们将继续探讨一个棘手的问题：如何充分利用一些特殊的标签——即数字、符号和标点符号标签。
- en: Understanding number, symbol, and punctuation tags
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解数字、符号和标点符号标签
- en: 'If you look at the English POS, you will notice the `NUM`, `SYM`, and `PUNCT`
    tags. These are the tags for numbers, symbols, and punctuation, respectively.
    These categories are divided into fine-grained categories: `$`, `SYM`, `''''`,
    `-LRB-`, and `-RRB-`. These are shown in the following screenshot:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看英语的POS（词性标注），你会注意到`NUM`、`SYM`和`PUNCT`标签。这些分别是数字、符号和标点的标签。这些类别被细分为细粒度类别：`$`、`SYM`、`''`、`-LRB-`和`-RRB-`。这些在下面的屏幕截图中有显示：
- en: '![Figure 3.3 – spaCy punctuation tags, general and fine-grained'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.3 – spaCy标点符号标签，一般和细粒度'
- en: '](img/B16570_03_03.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B16570_03_03.jpg)'
- en: Figure 3.3 – spaCy punctuation tags, general and fine-grained
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – spaCy标点符号标签，一般和细粒度
- en: 'Let''s tag some example sentences that contain numbers and symbols, as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们标记一些包含数字和符号的示例句子，如下所示：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We again iterated over the tokens and printed the fine-grained tags. The tagger
    was able to distinguish symbols, punctuation marks, and numbers. Even the word
    `million` is recognized as a number too!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次遍历了标记并打印了细粒度的标签。标记器能够区分符号、标点符号和数字。甚至单词“百万”也被识别为数字！
- en: Now, what to do with symbol tags? Currency symbols and numbers offer a way to
    systematically extract descriptions of money and are very handy in financial text
    such as financial reports. We'll see how to extract money entities in [*Chapter
    4*](B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069), *Rule-Based Matching*.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，关于符号标签怎么办？货币符号和数字提供了一种系统地提取货币描述的方法，在金融文本（如财务报告）中非常方便。我们将在[*第4章*](B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069)的*基于规则的匹配*中看到如何提取货币实体。
- en: That's it—you made it to the end of this exhaustive section! There's a lot to
    unpack and digest, but we assure you that you made a great investment for your
    industrial NLP work. We'll now continue with another syntactic concept—dependency
    parsing.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样——你已经到达了本节详尽内容的结尾！有很多东西需要分解和消化，但我们向你保证，你对工业NLP工作的投资是值得的。现在，我们将继续探讨另一个句法概念——依赖句法解析。
- en: Introduction to dependency parsing
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 依赖句法解析简介
- en: If you are already familiar with spaCy, you must have come across the spaCy
    dependency parser. Though many developers see *dependency parser* on the spaCy
    documentation, they're shy about using it or don't know how to use this feature
    to the fullest. In this part, you'll explore a systematic way of representing
    a sentence syntactically. Let's start with what dependency parsing actually is.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经熟悉spaCy，你肯定遇到过spaCy依赖句法解析器。尽管许多开发者看到spaCy文档中的**依赖句法解析器**，但他们可能对此有所顾虑，或者不知道如何充分利用这一功能。在本部分，你将探索一种系统性地表示句子句法的方法。让我们从依赖句法实际上是什么开始。
- en: What is dependency parsing?
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是依赖句法解析？
- en: In the previous section, we focused on POS tags—syntactic categories of words.
    Though POS tags provide information about neighbor words' tags as well, they do
    not give away any relations between words that are not neighbors in the given
    sentence.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们专注于词性标注——单词的句法类别。尽管词性标注提供了关于相邻单词标签的信息，但它们并没有给出给定句子中非相邻单词之间的任何关系。
- en: In this section, we'll focus on dependency parsing—a more structured way of
    exploring the sentence syntax. As the name suggests, **dependency parsing** is
    related to analyzing sentence structures via dependencies between the tokens.
    A **dependency parser** tags syntactic relations between tokens of the sentence
    and connects syntactically related pairs of tokens. A **dependency** or a **dependency
    relation** is a *directed link* between two tokens.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将关注依赖句法解析——一种更结构化的探索句子句法的方法。正如其名所示，**依赖句法解析**与通过标记之间的依赖关系分析句子结构相关。**依赖句法解析器**标记句子标记之间的句法关系，并连接句法相关的标记对。**依赖**或**依赖关系**是两个标记之间的**有向链接**。
- en: 'The result of the dependency parsing is always a **tree**, as illustrated in
    the following screenshot:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖句法解析的结果始终是一个**树**，如下截图所示：
- en: '![Figure 3.4 – An example of a dependency tree (taken from Wikipedia)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.4 – 依赖树的示例（摘自维基百科）]'
- en: '](img/B16570_03_04.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B16570_03_04.jpg)'
- en: Figure 3.4 – An example of a dependency tree (taken from Wikipedia)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 – 依赖树的示例（摘自维基百科）
- en: 'If you''re not familiar with a tree data structure, you can learn more about
    it at this excellent Computer Science resource:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不太熟悉树形数据结构，你可以在这个优秀的计算机科学资源中了解更多信息：
- en: https://www.cs.cmu.edu/~clo/www/CMU/DataStructures/Lessons/lesson4_1.htm
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: https://www.cs.cmu.edu/~clo/www/CMU/DataStructures/Lessons/lesson4_1.htm
- en: Dependency relations
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 依赖关系
- en: What is the use of **dependency relations**, then? Quite a number of statistical
    methods in NLP revolve around vector representations of words and treat a sentence
    as a sequence of words. As you can see in *Figure 3.4*, a sentence is more than
    a sequence of tokens—it has a structure; every word in a sentence has a well-defined
    role, such as verb, subject, object, and so on; hence, sentences definitely have
    a structure. This structure is used extensively in chatbots, question answering,
    and machine translation.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 那么**依赖关系**有什么用呢？在NLP中，许多统计方法都围绕单词的向量表示展开，并将句子视为单词的序列。正如你在*图3.4*中可以看到的，一个句子不仅仅是标记的序列——它有一个结构；句子中的每个单词都有一个明确的角色，例如动词、主语、宾语等等；因此，句子肯定是有结构的。这种结构在聊天机器人、问答系统和机器翻译中得到广泛的应用。
- en: 'The most useful application that first comes to mind is determining the sentence
    object and subject. Again, let''s go back to our travel agency application. Imagine
    a customer is complaining about the service. Compare the two sentences, `I forwarded
    you the email` and `You forwarded me the email`; if we eliminate the stopwords
    `I`, `you`, `me`, and `the`, this is what remains:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最有用的应用之一是确定句子中的宾语和主语。再次，让我们回到我们的旅行社应用程序。想象一下，一位客户正在抱怨服务。比较以下两个句子，“我把邮件转发给你”和“你把邮件转发给我”；如果我们消除了停用词“我”、“你”、“我”和“的”，剩下的就是：
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Though the remaining parts of the sentences are identical, sentences have very
    different meanings and require different answers. In the first sentence, the sentence
    subject is `I` (then, the answer most probably will start with `you`) and the
    second sentence's subject is `you` (which will end up in an `I` answer).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管句子的剩余部分相同，但句子具有非常不同的含义，需要不同的答案。在第一个句子中，句子主语是`I`（因此，答案很可能会以`你`开头）和第二个句子的主语是`you`（这将结束于一个`I`答案）。
- en: Obviously, the dependency parser helps us to go deeper into the sentence syntax
    and semantics. Let's explore more, starting from the dependency relations.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，依存分析器帮助我们更深入地了解句子的句法和语义。让我们从依存关系开始探索。
- en: Syntactic relations
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 句法关系
- en: 'spaCy assigns each token a dependency label, just as with other linguistic
    features such as a lemma or a POS tag. spaCy shows dependency relations with *directed
    arcs*. The following screenshot shows an example of a dependency relation between
    a noun and the adjective that qualifies the noun:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy为每个标记分配一个依存标签，就像其他语言特征（如词元或词性标签）一样。spaCy使用*有向弧*显示依存关系。以下截图显示了名词与其修饰名词的形容词之间的依存关系示例：
- en: '![Figure 3.5 – Dependency relation between a noun and its adjective'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.5 – 名词与其形容词之间的依存关系'
- en: '](img/B16570_03_05.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_03_05.jpg)'
- en: Figure 3.5 – Dependency relation between a noun and its adjective
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 – 名词与其形容词之间的依存关系
- en: 'A dependency label describes the type of syntactic relation between two tokens
    as follows: one of the tokens is the `flower` is the head and `blue` is its dependent/child.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 依存标签描述了两个标记之间的句法关系类型如下：其中一个标记是“flower”，它是“head”，而“blue”是其依赖/子节点。
- en: 'The dependency label is assigned to the child. Token objects have `dep (int)`
    and `dep_ (unicode)` properties that hold the dependency label, as illustrated
    in the following code snippet:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 依存标签分配给子节点。标记对象有`dep (int)`和`dep_ (unicode)`属性，它们包含依存标签，如下面的代码片段所示：
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In this example, we iterated over the tokens and printed their text and dependency
    label. Let''s go over what happened bit by bit, as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们遍历了标记并打印了它们的文本和依存标签。让我们一步一步地回顾一下发生了什么，如下所示：
- en: '`blue` admitted the `amod` label. `amod` is the dependency label for an adjective-noun
    relation. For more examples of the `amod` relation, please refer to *Figure 3.7*.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`blue`承认了`amod`标签。`amod`是形容词-名词关系的依存标签。有关`amod`关系的更多示例，请参阅*图3.7*。'
- en: '`flower` is the `ROOT`. `ROOT` is a special label in the dependency tree; it
    is assigned to the main verb of a sentence. If we''re processing a phrase (not
    a full sentence), the `ROOT` label is assigned to the root of the phrase, which
    is the head noun of the phrase. In the `blue flower` phrase, the head noun, `flower`,
    is the root of the phrase.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flower`是`ROOT`。`ROOT`是依存树中的一个特殊标签；它分配给句子的主要动词。如果我们处理的是一个短语（而不是完整的句子），则`ROOT`标签分配给短语的根，即短语的中心名词。在`blue
    flower`短语中，中心名词`flower`是短语的根。'
- en: Each sentence/phrase has exactly one root, and it's the root of the parse tree
    (remember, the dependency parsing result is a tree).
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个句子/短语都有一个根节点，它是分析树的根（记住，依存分析的结果是一个树）。
- en: Tree nodes can have more than one child, but each node can only have one parent
    (due to tree restrictions, and trees containing no cycles). In other words, every
    token has exactly one head, but a parent can have several children. This is the
    reason why the dependency label is assigned to the dependent node.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树节点可以有多个子节点，但每个节点只能有一个父节点（由于树的限制，以及不包含循环的树）。换句话说，每个标记恰好有一个头节点，但父节点可以有多个子节点。这就是为什么依存标签分配给依赖节点的原因。
- en: 'Here is a full list of spaCy''s English dependency labels:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是spaCy英语依存标签的完整列表：
- en: '![Figure 3.6 – List of some spaCy English dependency labels'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.6 – spaCy英语依存标签列表'
- en: '](img/B16570_03_06.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_03_06.jpg)'
- en: Figure 3.6 – List of spaCy English dependency labels
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 – spaCy英语依存标签列表
- en: 'That''s a long list! No worries—you don''t need to memorize every list item.
    Let''s first see a list of the most common and useful labels, then we''ll see
    how exactly they link tokens to each other. Here''s the list first:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很长的列表！不用担心——你不需要记住每个列表项。让我们首先看看最常见的和最有用的标签列表，然后我们将看到它们是如何将标记连接到一起的。首先是这个列表：
- en: '`amod`: Adjectival modifier'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`amod`：形容词修饰语'
- en: '`aux`: Auxiliary'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aux`：助动词'
- en: '`compound`: Compound'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compound`：复合'
- en: '`dative`: Dative object'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dative`：宾格宾语'
- en: '`det`: Determiner'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`det`：限定词'
- en: '`dobj`: Direct object'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dobj`：直接宾语'
- en: '`nsubj`: Nominal subject'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nsubj`：名词主语'
- en: '`nsubjpass`: Nominal subject, passive'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nsubjpass`：名词主语，被动'
- en: '`nummod`: Numeric modifier'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nummod`：数字修饰语'
- en: '`poss`: Possessive modifier'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`poss`：所有格修饰语'
- en: '`root`: The root'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`root`：根'
- en: 'Let''s see examples of how the aforementioned labels are used and what relation
    they express. `amod` is adjectival modifier. As understood from the name, this
    relation modifies the noun (or pronoun). In the following screenshot, we see **white**
    modifies **sheep**:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看上述标签的使用示例以及它们表达的关系。`amod` 是形容词修饰语。从名称上可以理解，这种关系修饰名词（或代词）。在下面的屏幕截图中，我们看到**白色**修饰**羊**：
- en: '![Figure 3.7 – amod relation'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.7 – amod 关系'
- en: '](img/B16570_03_07.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16570_03_07.jpg](img/B16570_03_07.jpg)'
- en: Figure 3.7 – amod relation
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.7 – amod 关系'
- en: '`aux` is what you might guess: it''s the dependency relation between an auxiliary
    verb and its main verb; the dependent is an auxiliary verb, and the head is the
    main verb. In the following screenshot, we see that **has** is the auxiliary verb
    of the main verb **gone**:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`aux` 是你可能猜到的：它是助动词和主要动词之间的依赖关系；依赖项是助动词，头是主要动词。在下面的屏幕截图中，我们看到**has** 是主要动词
    **gone** 的助动词：'
- en: '![Figure 3.8 – aux relation'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.8 – aux 关系'
- en: '](img/B16570_03_08.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16570_03_08.jpg](img/B16570_03_08.jpg)'
- en: Figure 3.8 – aux relation
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.8 – aux 关系'
- en: '`compound` is used for the noun compounds; the second noun is modified by the
    first noun. In the following screenshot, **phone book** is a noun compound and
    the **phone** noun modifies the **book** noun:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`compound` 用于名词复合结构；第二个名词由第一个名词修饰。在下面的屏幕截图中，**电话簿**是一个名词复合结构，**电话**名词修饰**书籍**名词：'
- en: '![Figure 3.9 – Compound relation between phone and book'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.9 – 电话和书籍的复合关系'
- en: '](img/B16570_03_09.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16570_03_09.jpg](img/B16570_03_09.jpg)'
- en: Figure 3.9 – Compound relation between phone and book
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.9 – 电话和书籍的复合关系'
- en: 'The `det` relation links a determiner (the dependent) to the noun it qualifies
    (its head). In the following screenshot, **the** is the determiner of the noun
    **girl** in this sentence:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`det` 关系将限定词（依赖项）与其所修饰的名词（其头）联系起来。在下面的屏幕截图中，**the** 是这个句子中名词 **girl** 的限定词：'
- en: '![Figure 3.10 – det relation on the right'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.10 – 右侧的 det 关系'
- en: '](img/B16570_03_10.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16570_03_10.jpg](img/B16570_03_10.jpg)'
- en: Figure 3.10 – det relation
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 – det 关系
- en: Next, we look into two object relations, `dative` and `dobj`. The `dobj` relation
    is between the verb and its direct object. A sentence can have more than one object
    (such as in the following example); a direct object is the object that the verb
    acts upon, and the others are called indirect objects.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看两种宾语关系，`dative` 和 `dobj`。`dobj` 关系存在于动词和它的直接宾语之间。一个句子可以有一个以上的宾语（如下面的例子所示）；直接宾语是动词所作用的对象，其他的是间接宾语。
- en: 'A direct object is generally marked with `dative` relation points to a `dative`
    object, which receives an indirect action from the verb. In the sentence shown
    in the following screenshot, the indirect object is **me** and the direct object
    is **book**:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 直接宾语通常用 `dative` 关系标记，指向 `dative` 宾语，它从动词那里接受间接动作。在下面的屏幕截图中，间接宾语是 **me**，直接宾语是
    **book**：
- en: '![Figure 3.11 – The direct and indirect objects of the sentence'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.11 – 句子的直接和间接宾语'
- en: '](img/B16570_03_11.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16570_03_11.jpg](img/B16570_03_11.jpg)'
- en: Figure 3.11 – The direct and indirect objects of the sentence
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.11 – 句子的直接和间接宾语'
- en: '`nsubj` and `nsubjposs` are two relations that are related to the nominal sentence
    subject. The subject of the sentence is the one that committed the action. A passive
    subject is still the subject, but we mark it with `nsubjposs`. In the following
    screenshot, **Mary** is the nominal subject of the first sentence:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`nsubj` 和 `nsubjposs` 是两种与名词性句子主语相关的关联。句子的主语是执行动作的人。被动主语仍然是主语，但我们用 `nsubjposs`
    来标记它。在下面的屏幕截图中，**Mary** 是第一句的名词主语：'
- en: '![Figure 3.12 – nsubj relation'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.12 – nsubj 关系'
- en: '](img/B16570_03_12.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16570_03_12.jpg](img/B16570_03_12.jpg)'
- en: Figure 3.12 – nsubj relation
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12 – nsubj 关系
- en: '**you** is the passive nominal subject of the sentence in the following screenshot:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的屏幕截图中，**you** 是句子的被动名词主语：
- en: '![Figure 3.13 – nsubjpass relation'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.13 – nsubjpass 关系'
- en: '](img/B16570_03_13.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_03_13.jpg)'
- en: Figure 3.13 – nsubjpass relation
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13 – nsubjpass 关系
- en: 'We have now covered sentence subject and object relations. Now, we''ll discover
    two modifier relations; one is the `nummod` `poss` `nummod` is easy to spot; it''s
    between **3** and **books**:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经涵盖了句子主语和宾语关系。现在，我们将发现两个修饰关系；一个是 `nummod` `poss` `nummod` 很容易找到；它在 **3**
    和 **books** 之间：
- en: '![Figure 3.14 – nummod relation'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.14 – nummod 关系'
- en: '](img/B16570_03_14.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_03_14.jpg)'
- en: Figure 3.14 – nummod relation
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.14 – nummod 关系
- en: 'A possessive modifier happens either between a *possessive pronoun* and a noun
    or a *possessive ''s* and a noun. In the sentence shown in the following screenshot,
    **my** is a possessive marker on the noun **book**:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 一个所有格修饰语发生在所有格代词和名词之间，或者所有格 `'s'` 和名词之间。在下面的屏幕截图所示的句子中，**my** 是名词 **book** 的所有格标记：
- en: '![Figure 3.15 – poss relation between “my” and “book”'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.15 – “my” 和 “book” 之间的 poss 关系'
- en: '](img/B16570_03_15.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_03_15.jpg)'
- en: Figure 3.15 – poss relation between "my" and "book"
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.15 – "my" 和 "book" 之间的 poss 关系
- en: Last, but not least, is the **root label**, which is not a real relation but
    is a marker for the sentence verb. A root word has no real parent in the syntactic
    tree; the root is the main verb of the sentence. In the preceding sentences, **took**
    and **given** are the corresponding roots. The main verbs of both sentences are
    the auxiliary verbs **is** and **are**. Notice that the root node has no incoming
    arc—that is, no parent.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，但同样重要的是，是 **root label**，它不是一个真实的关系，而是句子动词的标记。根词在句法树中没有真正的父节点；根是句子的主要动词。在前面的句子中，**took**
    和 **given** 是相应的根。两个句子的主要动词都是助动词 **is** 和 **are**。请注意，根节点没有进入弧线——也就是说，没有父节点。
- en: 'These are the most useful labels for our NLU purposes. You definitely don''t
    need to memorize all the labels, as you''ll become familiar as you practice in
    the next pages. Also, you can ask spaCy about a label any time you need, via `spacy.explain()`.
    The code to do this is shown in the following snippet:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们 NLU 目的最有用的标签。你绝对不需要记住所有的标签，因为你在下一页的练习中会变得熟悉。此外，你可以在需要时通过 `spacy.explain()`
    向 spaCy 询问任何标签。执行此操作的代码如下所示：
- en: '[PRE14]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Take a deep breath, since there is a lot to digest! Let's practice how we can
    make use of dependency labels.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 深吸一口气，因为有很多东西要消化！让我们练习如何利用依存标签。
- en: 'Again, `token.dep_` includes the dependency label of the dependent token. The
    `token.head` property points to the head/parent token. Only the root token does
    not have a parent; spaCy points to the token itself in this case. Let''s bisect
    the example sentence from *Figure 3.7*, as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，`token.dep_` 包含了从属标记的依存标签。`token.head` 属性指向头/父标记。只有根标记没有父标记；spaCy 在这种情况下指向标记本身。让我们将
    *图 3.7* 中的示例句子二分，如下所示：
- en: '[PRE15]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We iterated over the tokens and printed the fine-grained POS tag and the dependency
    label. `counted` is the main verb of the sentence and is labeled by the label
    `ROOT`. Now, `I` is the subject of the sentence, and `sheep` is the direct object.
    `white` is an adjective and modifies the noun `sheep`, hence its label is `amod`.
    We go one level deeper and print the token heads this time, as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历了标记并打印了细粒度的词性标注和依存标签。`counted` 是句子的主要动词，并被标记为 `ROOT`。现在，`I` 是句子的主语，而 `sheep`
    是直接宾语。`white` 是一个形容词，修饰名词 `sheep`，因此其标签是 `amod`。我们再深入一层，这次打印标记的头，如下所示：
- en: '[PRE16]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The visualization is as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化如下：
- en: '![Figure 3.16 – An example parse of a simple sentence'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.16 – 简单句的一个示例解析'
- en: '](img/B16570_03_16.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_03_16.jpg)'
- en: Figure 3.16 – An example parse of a simple sentence
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.16 – 简单句的一个示例解析
- en: 'When the `token.head` property is also involved, it''s a good idea to follow
    the code and the visual at the same time. Let''s go step by step in order to understand
    how the visual and the code match:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当 `token.head` 属性也参与其中时，同时遵循代码和视觉是一个好主意。让我们一步一步地来理解视觉和代码是如何匹配的：
- en: 'We start reading the parse tree from the root. It''s the main verb: `counted`.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从根开始阅读解析树。它是主要动词：`counted`。
- en: Next, we follow its arc on the left toward the pronoun `I`, which is the nominal
    subject of the sentence and is labeled by the label `nsubj`.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步，我们跟随其左侧的弧线指向代词 `I`，它是句子的名词主语，并被标记为 `nsubj`。
- en: Now, return back to the root, `counted`. This time, we navigate to the right.
    Follow the `dobj` arc to reach the noun `sheep`. `sheep` is modified by the adjective
    `white` with an `amod` relation, hence the direct object of this sentence is `white
    sheep`.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，回到根节点，`counted`。这次，我们向右导航。跟随`dobj`弧线到达名词`sheep`。`sheep`由形容词`white`修饰，具有`amod`关系，因此这个句子的直接宾语是`white
    sheep`。
- en: 'Even such a simple, flat sentence has a dependency parse tree that''s fancy
    to read, right? Don''t rush—you''ll get used to it by practicing. Let''s examine
    the dependency tree of a longer and more complicated sentence, as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这样简单、平铺直叙的句子也有一个复杂的依存句法分析树，读起来很复杂，对吧？不要急——通过练习你会习惯的。让我们检查一个更长、更复杂的句子的依存句法树，如下所示：
- en: '[PRE17]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now, this time things look a bit different, as we'll see in *Figure 3.17*. We
    locate the main verb and the root `trying` (it has no incoming arcs). The left
    side of the word `trying` looks manageable, but the right side has a chain of
    arcs. Let's start with the left side. The pronoun `we` is labeled by `nsubj`,
    hence this is the nominal subject of the sentence. The other left arc, labeled
    `aux`, points to the `trying` dependent `are`, which is the auxiliary verb of
    the main verb `trying`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这次情况看起来有些不同，正如我们在*图3.17*中将会看到的。我们定位到主要动词和根节点`trying`（它没有进入弧线）。`trying`这个词的左侧看起来可以管理，但右侧有一系列弧线。让我们从左侧开始。代词`we`被标记为`nsubj`，因此这是句子的名词主语。另一个左侧弧线，标记为`aux`，指向`trying`的从属动词`are`，它是主要动词`trying`的助动词。
- en: 'So far, so good. Now, what is happening on the right side? `trying` is attached
    to the second verb `understand` via an `xcomp` relation. The `xcomp` (or open
    complement) relation of a verb is a clause without its own subject. Here, the
    `to understand the difference` clause has no subject, so it''s an open complement.
    We follow the `dobj` arc from the second verb, `understand`, and land on the noun,
    `difference`, which is the direct object of the `to understand the difference`
    clause, and this is the result:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利。现在，右侧发生了什么？`trying`通过`xcomp`关系连接到第二个动词`understand`。动词的`xcomp`（或开放补语）关系是一个没有自己主语的从句。在这里，`to
    understand the difference`从句没有主语，因此它是一个开放补语。我们跟随从第二个动词`understand`开始的`dobj`弧线，到达名词`difference`，它是`to
    understand the difference`从句的直接宾语，结果是：
- en: '![Figure 3.17 – A complicated parsing example'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.17 – 一个复杂的解析示例'
- en: '](img/B16570_03_17.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_03_17.jpg]'
- en: Figure 3.17 – A complicated parsing example
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17 – 一个复杂的解析示例
- en: 'This was an in-depth analysis for this example sentence, which indeed does
    not look that complicated. Next, we process a sentence with a subsentence that
    owns its own nominal subject, as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对这个示例句子的深入分析，它确实看起来并不那么复杂。接下来，我们处理一个包含拥有自己名词主语的从句的句子，如下所示：
- en: '[PRE18]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In order to make the visuals big enough, I have split the visualization into
    two parts. First, let's find the root. The root lies in the right part. `died`
    is the main sentence of the verb and the root (again, it has no incoming arcs).
    The rest of the right side contains nothing tricky.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使视觉效果足够大，我已经将可视化分成了两部分。首先，让我们找到根节点。根节点位于右侧。`died`是动词的主句和根节点（再次强调，它没有进入弧线）。右侧的其他部分没有复杂的内容。
- en: 'On the other hand, the left side has some interesting stuff—actually, a relative
    clause. Let''s bisect the relative clause structure:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，左侧有一些有趣的内容——实际上，一个关系从句。让我们将关系从句结构一分为二：
- en: We start with the proper noun `Katherine`, which is attached to `died` with
    a `nsubj` relation, hence the subject of the sentence.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从专有名词`Katherine`开始，它与`died`通过`nsubj`关系连接，因此是句子的主语。
- en: We see a compound arc leaving `Katherine` toward the proper noun, `Queen`. Here,
    `Queen` is a title, so the relationship with `Katherine` is compound. The same
    relationship exists between `Mary` and `Tudor` on the right side, and the last
    names and first names are also tied with the compound relation.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们看到一个复合弧从`Katherine`指向专有名词`Queen`。在这里，`Queen`是一个头衔，所以它与`Katherine`的关系是复合的。右侧`Mary`和`Tudor`之间也存在相同的关系，姓氏和名字也通过复合关系联系在一起。
- en: 'It''s time to bisect the relative clause, `who was the mother of Mary Tudor`,
    as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候将关系从句`who was the mother of Mary Tudor`一分为二了，如下所示：
- en: First of all, it is `Katherine` who is mentioned in the relative clause, so
    we see a `relcl` (relative clause) arc from `Katherine` to `was` of the relative
    clause.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，关系从句中提到的是`Katherine`，所以我们看到从`Katherine`到关系从句中的`was`的`relcl`（关系从句）弧线。
- en: '`who` is the nominal subject of the clause and is linked to `was` via an `nsubj`
    relation. As you see in the following screenshot, the dependency tree is different
    from the previous example sentence, whose clause didn''t own a nominal subject:'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`who` 是从句的名词主语，通过 `nsubj` 关系与 `was` 相连。正如你在下面的截图中所见，依存句法树与之前例子句子中的不同，那个例子句子中的从句没有名词主语：'
- en: '![Figure 3.18 – A dependency tree with a relative clause, the left part'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.18 – 带有相对从句的依存句法树，左侧部分'
- en: '](img/B16570_03_18.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_03_18.jpg)'
- en: Figure 3.18 – A dependency tree with a relative clause, the left part
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.18 – 带有相对从句的依存句法树，左侧部分
- en: '![Figure 3.19 – Same sentence, the right part'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.19 – 相同句子的右侧部分'
- en: '](img/B16570_03_19.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_03_19.jpg)'
- en: Figure 3.19 – Same sentence, the right part
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.19 – 相同句子的右侧部分
- en: It's perfectly normal if you feel that you won't be able to keep all the relations
    in your mind. No worries—always find the root/main verb of the sentence, then
    follow the arcs from the root and go deeper, just as we did previously. You can
    always have a look at the spaCy documentation ([https://spacy.io/api/annotation#dependency-parsing](https://spacy.io/api/annotation#dependency-parsing))
    to see what the relation type means. Take your time until you warm up to the concept
    and the details.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得自己无法记住所有关系，这是很正常的。不用担心——总是找到句子的根/主动词，然后跟随从根出发的弧线深入，就像我们之前做的那样。你总是可以查看 spaCy
    文档([https://spacy.io/api/annotation#dependency-parsing](https://spacy.io/api/annotation#dependency-parsing))来了解关系类型意味着什么。直到你对这个概念和细节熟悉起来，请慢慢来。
- en: 'That was exhaustive! Dear reader—as we said before, please take your time to
    digest and practice on example sentences. The *displaCy* online demo is a great
    tool, so don''t be shy to try your own example sentences and see the parsing results.
    It''s perfectly normal for you to find this section heavy. However, this section
    is a solid foundation for general linguistics, and also for the information extraction
    and pattern-matching exercises in [*Chapter 4*](B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069),
    *Rule-Based Matching*. You will become even more comfortable after going through
    a case study in [*Chapter 6*](B16570_06_Final_JM_ePub.xhtml#_idTextAnchor103),
    *Putting Everything Together: Semantic Parsing with spaCy*. Give yourself time
    to digest dependency parsing with examples throughout the book.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经非常全面了！亲爱的读者——正如我们之前所说，请花些时间消化和实践例子句子。*displaCy* 在线演示是一个很好的工具，所以不要害羞，尝试你自己的例子句子并查看解析结果。你发现这一部分内容较多是很正常的。然而，这一部分是普通语言学以及[*第
    4 章*](B16570_04_Final_JM_ePub.xhtml#_idTextAnchor069)中信息提取和模式匹配练习的坚实基础，*基于规则的匹配*。在[*第
    6 章*](B16570_06_Final_JM_ePub.xhtml#_idTextAnchor103)中，*将一切整合：使用 spaCy 进行语义解析*的案例研究之后，你会更加得心应手。给自己一些时间，通过书中各个例子来消化依存句法分析。
- en: What comes after the dependency parser? Without any doubt, you must have heard
    NER frequently mentioned in the NLU world. Let's look into this very important
    NLU concept.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 依存句法分析之后是什么？毫无疑问，你一定经常在 NLU 世界中听到 NER（命名实体识别）这个词。让我们来探讨这个非常重要的 NLU 概念。
- en: Introducing NER
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 NER
- en: We opened this chapter with a tagger, and we'll see another very handy tagger—the
    NER tagger of spaCy. As NER's name suggests, we are interested in finding named
    entities.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以一个分词器开始了这一章，接下来我们将看到另一个非常实用的分词器——spaCy 的 NER 分词器。正如 NER 的名字所暗示的，我们感兴趣的是寻找命名实体。
- en: What is a **named entity**? A named entity is a real-world object that we can
    refer to by a proper name or a quantity of interest. It can be a person, a place
    (city, country, landmark, famous building), an organization, a company, a product,
    dates, times, percentages, monetary amounts, a drug, or a disease name. Some examples
    are Alicia Keys, Paris, France, Brandenburg Gate, WHO, Google, Porsche Cayenne,
    and so on.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是**命名实体**？命名实体是我们可以用一个专有名称或感兴趣的量来指代的现实世界中的对象。它可以是一个人、一个地方（城市、国家、地标、著名建筑）、一个组织、一家公司、一个产品、日期、时间、百分比、货币金额、一种药物或疾病名称。一些例子包括Alicia
    Keys、巴黎、法国、勃兰登堡门、世界卫生组织、谷歌、保时捷卡宴等等。
- en: A named entity always points to a *specific* object, and that object is distinguishable
    via the corresponding named entity. For instance, if we tag the sentence *Paris
    is the capital of France*, we parse *Paris* and *France* as named entities, but
    not the word *capital*. The reason is that *capital* does not point to a specific
    object; it's a general name for many objects.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体总是指向一个**特定**的对象，而这个对象可以通过相应的命名实体来区分。例如，如果我们标记句子 *巴黎是法国的首都*，我们将 *巴黎* 和 *法国*
    解析为命名实体，但不会标记单词 *首都*。原因是 *首都* 并不指向一个特定的对象；它是许多对象的通用名称。
- en: 'NER categorization is a bit different from POS categorization. Here, the number
    of categories is as high as we want. The most common categories are person, location,
    and organization and are supported by almost every usable NER tagger. In the following
    screenshot, we see the corresponding tags:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: NER 分类与 POS 分类略有不同。在这里，分类的数量可以高达我们想要的。最常见的分类是人物、地点和组织，几乎所有可用的 NER 标签器都支持这些分类。在下面的屏幕截图中，我们可以看到相应的标签：
- en: '![Figure 3.20 – Most common entity types'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.20 – 最常见的实体类型'
- en: '](img/B16570_03_20.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_03_20.jpg)'
- en: Figure 3.20 – Most common entity types
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.20 – 最常见的实体类型
- en: spaCy supports a wide range of entity types. Which ones you use depends on your
    corpus. If you process financial text, you most probably use `MONEY` and `PERCENTAGE`
    more often than `WORK_OF_ART`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 支持广泛的实体类型。您使用哪些类型取决于您的语料库。如果您处理财务文本，您最可能比 `WORK_OF_ART` 更频繁地使用 `MONEY`
    和 `PERCENTAGE`。
- en: 'Here is a list of the entity types supported by spaCy:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 spaCy 支持的实体类型列表：
- en: '![Figure 3.21 – Full list of entity types supported by spaCy'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.21 – spaCy 支持的实体类型完整列表'
- en: '](img/B16570_03_21.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_03_21.jpg)'
- en: Figure 3.21 – Full list of entity types supported by spaCy
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.21 – spaCy 支持的实体类型完整列表
- en: 'Just as with the POS tagger statistical models, NER models are also sequential
    models. The very first modern NER tagger model is a **conditional random field**
    (**CRF**). CRFs are sequence classifiers used for structured prediction problems
    such as labeling and parsing. If you want to learn more about the CRF implementation
    details, you can read more at this resource: https://homepages.inf.ed.ac.uk/csutton/publications/crftutv2.pdf.
    The current state-of-the-art NER tagging is achieved by neural network models,
    usually LSTM or LSTM+CRF architectures.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 POS 标签统计模型一样，NER 模型也是顺序模型。第一个现代 NER 标签模型是一个 **条件随机场**（**CRF**）。CRFs 是用于结构化预测问题（如标记和解析）的序列分类器。如果您想了解更多关于
    CRF 实现细节的信息，可以阅读此资源：https://homepages.inf.ed.ac.uk/csutton/publications/crftutv2.pdf。当前最先进的
    NER 标记是通过神经网络模型实现的，通常是 LSTM 或 LSTM+CRF 架构。
- en: 'Named entities in a doc are available via the `doc.ents` property. `doc.ents`
    is a list of `Span` objects, as illustrated in the following code snippet:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 文档中的命名实体可以通过 `doc.ents` 属性访问。`doc.ents` 是一个 `Span` 对象的列表，如下面的代码片段所示：
- en: '[PRE19]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: spaCy also tags each token with the entity type. The type of the named entity
    is available via `token.ent_type (int)` and `token.ent_type_ (unicode)`. If the
    token is not a named entity, then `token.ent_type_` is just an empty string.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 还会为每个标记分配实体类型。命名实体的类型可以通过 `token.ent_type (int)` 和 `token.ent_type_ (unicode)`
    获取。如果标记不是命名实体，则 `token.ent_type_` 只是一个空字符串。
- en: 'Just as for POS tags and dependency labels, we can call `spacy.explain()` on
    the tag string or on the `token.ent_type_`, as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 POS 标签和依存标签一样，我们可以对标签字符串或 `token.ent_type_` 调用 `spacy.explain()`，如下所示：
- en: '[PRE20]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s go over some examples to see the spaCy NER tagger in action, as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一些示例，看看 spaCy NER 标签器的实际应用，如下所示：
- en: '[PRE21]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We iterated over the tokens one by one and printed the token and its entity
    type. If the token is not tagged as an entity, then `token.ent_type_` is just
    an empty string, hence there is no explanation from `spacy.explain()`. For the
    tokens that are part of a NE, an appropriate tag is returned. In the preceding
    sentences, `Albert Einstein`, `Ulm`, `1879`, and `ETH Zurich` are correctly tagged
    as `PERSON`, `GPE`, `DATE`, and `ORG`, respectively.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们逐个遍历标记并打印标记及其实体类型。如果标记未被标记为实体，则 `token.ent_type_` 只是一个空字符串，因此没有 `spacy.explain()`
    的解释。对于属于 NE 的标记，返回适当的标记。在先前的句子中，`Albert Einstein`、`Ulm`、`1879` 和 `ETH Zurich`
    分别被正确标记为 `PERSON`、`GPE`、`DATE` 和 `ORG`。
- en: 'Let''s see a longer and more complicated sentence with a non-English entity
    and look at how spaCy tagged it, as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个更长且更复杂的句子，其中包含非英语实体，并查看 spaCy 如何标记它，如下所示：
- en: '[PRE22]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Looks good! The spaCy tagger picked up a person entity with a `-` smoothly.
    Overall, the tagger works quite well for different entity types, as we saw throughout
    the examples.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错！spaCy 标签器平滑地识别了一个带有 `-` 的人实体。总的来说，标签器在处理不同实体类型时表现相当好，正如我们在示例中看到的那样。
- en: After tagging tokens with different syntactical features, we sometimes want
    to merge/split entities into fewer/more tokens. In the next section, we will see
    how merging and splitting is done. Before that, we will see a real-world application
    of NER tagging.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记具有不同句法特征的标记之后，我们有时希望将实体合并/拆分到更少/更多的标记中。在下一节中，我们将看到合并和拆分是如何进行的。在此之前，我们将看到
    NER 标签的实际应用案例。
- en: A real-world example
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际案例
- en: NER is a popular and frequently used pipeline component of spaCy. NER is one
    of the key components of understanding the text topic, as named entities usually
    belong to a **semantic category**. For instance, *President Trump* invokes the
    *politics* subject in our minds, whereas *Leonardo DiCaprio* is more about *movies*.
    If you want to go deeper into resolving the text meaning and understanding who
    made what, you also need named entities.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: NER是spaCy中流行且经常使用的管道组件。NER是理解文本主题的关键组件之一，因为命名实体通常属于一个**语义类别**。例如，*特朗普总统*在我们的脑海中唤起了*政治*主题，而*莱昂纳多·迪卡普里奥*则更多关于*电影*。如果你想深入了解解决文本意义和理解谁做了什么，你也需要命名实体。
- en: 'This real-world example includes processing a *New York Times* article. Let''s
    go ahead and download the article first by running the following code:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这个现实世界的例子包括处理一篇*《纽约时报》*文章。让我们先运行以下代码来下载文章：
- en: '[PRE23]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We downloaded the article `BeautifulSoup` is a popular Python package for extracting
    text from HTML and `nlp` object, passed the article body to the `nlp` object,
    and created a `Doc` object.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下载了文章，`BeautifulSoup`是一个流行的Python包，用于从HTML中提取文本和`nlp`对象，将文章主体传递给`nlp`对象，并创建了一个`Doc`对象。
- en: 'Let''s start our analysis of the article by the entity type count, as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从文章的实体类型计数开始分析，如下所示：
- en: '[PRE24]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'That''s a totally normal number for a news article that includes many entities.
    Let''s go a bit further and group the entity types, as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 对于包含许多实体的新闻文章来说，这是一个完全正常的数字。让我们进一步对实体类型进行分组，如下所示：
- en: '[PRE25]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The most frequent entity type is `GPE`, which means a country, city, or state.
    The second one is `PERSON`, whereas the third most frequent entity label is `NORP`,
    which means a nationality/religious-political group. The next ones are organization,
    date, and cardinal number-type entities.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 最频繁的实体类型是`GPE`，表示国家、城市或州。其次是`PERSON`，而第三频繁的实体标签是`NORP`，表示国籍/宗教政治团体。接下来是组织、日期和基数类型实体。
- en: 'Can we summarize the text by looking at the entities or understanding the text
    topic? To answer this question, let''s start by counting the most frequent tokens
    that occur in the entities, as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否通过查看实体或理解文本主题来总结文本？为了回答这个问题，让我们首先统计实体中最频繁出现的标记，如下所示：
- en: '[PRE26]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Looks like a semantic group! Obviously, this article is about American politics,
    and possibly how America interacts with the rest of the world in politics. If
    we print all the entities of the article, we can see here that this guess is true:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来像是一个语义组！显然，这篇文章是关于美国政治的，可能还涉及美国在政治上如何与世界其他国家互动。如果我们打印出文章中的所有实体，我们可以看到这里的猜测是正确的：
- en: '[PRE27]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We made a visualization of the whole article by pasting the text into **displaCy
    Named Entity Visualizer** ([https://explosion.ai/demos/displacy-ent/](https://explosion.ai/demos/displacy-ent/)).
    The following screenshot is taken from the demo page that captured a part of the
    visual:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将文本粘贴到**displaCy命名实体可视化器**（[https://explosion.ai/demos/displacy-ent/](https://explosion.ai/demos/displacy-ent/)）来制作整篇文章的可视化。以下截图是从捕获了部分可视化的演示页面中获取的：
- en: '![Figure 3.22 – The New York Times article''s entities visualized by displaCy'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.22 – 使用displaCy可视化的《纽约时报》文章的实体'
- en: '](img/B16570_03_22.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_03_22.jpg)'
- en: Figure 3.22 – The New York Times article's entities visualized by displaCy
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.22 – 使用displaCy可视化的《纽约时报》文章的实体
- en: spaCy's NER offers great capabilities for understanding text, as well as presenting
    good-looking visuals to ourselves, colleagues, and stakeholders.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy的NER为我们理解文本以及向我们自己、同事和利益相关者展示美观的视觉提供了强大的功能。
- en: Merging and splitting tokens
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合并和拆分标记
- en: We extracted the name entities in the previous section, but how about if we
    want to unite or split multiword named entities? And what if the tokenizer performed
    this not so well on some exotic tokens and you want to split them by hand? In
    this subsection, we'll cover a very practical remedy for our multiword expressions,
    multiword named entities, and typos.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前一节中提取了命名实体，但如果我们想合并或拆分多词命名实体怎么办？还有，如果分词器在某些异国情调的标记上表现不佳，你想手动拆分它们怎么办？在本小节中，我们将介绍一种针对我们的多词表达式、多词命名实体和错别字的非常实用的补救措施。
- en: '`doc.retokenize` is the correct tool for merging and splitting the spans. Let''s
    see an example of retokenization by merging a multiword named entity, as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '`doc.retokenize`是合并和拆分跨度段的正确工具。让我们通过合并一个多词命名实体来查看重分词的示例，如下所示：'
- en: '[PRE28]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This is what we did in the preceding code:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们前面代码中所做的：
- en: First, we created a `doc` object from the sample sentence.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从样本句子创建了一个`doc`对象。
- en: Then, we printed its entities with `doc.ents`, and the result was `New Hampshire`,
    as expected.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用`doc.ents`打印了其实体，结果是预期的`New Hampshire`。
- en: In the next line, for each token, we printed `token.text` with token indices
    in the sentence (`token.i`).
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一行，对于每个标记，我们打印了`token.text`，包括句子中的标记索引（`token.i`）。
- en: Also, we examined length of the `doc` object by calling `len` on it, and the
    result was `6` (`"."` is a token too).
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，我们通过在`doc`对象上调用`len`来检查`doc`对象的长度，结果是`6`（`.`也是一个标记）。
- en: 'Now, we wanted to merge the tokens of position `3` until `5` (`3` is included;
    `5` is not), so we did the following:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想要合并位置`3`到`5`（`3`包含在内；`5`不包含），所以我们做了以下操作：
- en: First, we called the `retokenizer` method `merge(indices, attrs)`. `attrs` is
    a dictionary of token attributes we want to assign to the new token, such as `lemma`,
    `pos`, `tag`, `ent_type`, and so on.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们调用了`retokenizer`方法的`merge(indices, attrs)`。`attrs`是我们想要分配给新标记的标记属性字典，例如`lemma`、`pos`、`tag`、`ent_type`等。
- en: In the preceding example, we set the lemma of the new token; otherwise, the
    lemma would be `New` only (the starting token's lemma of the span we want to merge).
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们设置了新标记的词元；否则，词元只会是`New`（我们想要合并的跨度的起始标记的词元）。
- en: Then, we printed the tokens to see if the operation worked as we wished. When
    we print the new tokens, we see that the new `doc[3]` is the `New Hampshire` token.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们打印了标记以查看操作是否按我们的意愿进行。当我们打印新标记时，我们看到新的`doc[3]`是`New Hampshire`标记。
- en: Also, the `doc` object is of length `5` now, so we shrank the doc one less token.
    `doc.ents` remain the same and the new token's lemma is `new hampshire` because
    we set it with `attrs`.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，`doc`对象现在长度为`5`，所以我们减少了doc一个标记。`doc.ents`保持不变，新标记的词元是`new hampshire`，因为我们用`attrs`设置了它。
- en: Looks good, so how about splitting a multiword token into several tokens? In
    this setting, either there's a typo in the text you want to fix or the custom
    tokenization is not satisfactory for your specific sentence.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错，那么如何将多词标记分割成几个标记呢？在这种情况下，要么是你想要修复的文本中存在拼写错误，要么是自定义标记化对于你的特定句子来说不满意。
- en: 'Splitting a span is a bit more complicated than merging a span because of the
    following reasons:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 分割一个跨度比合并跨度要复杂一些，原因如下：
- en: We are changing the dependency tree.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在改变依赖树。
- en: We need to assign new POS tags, dependency labels, and necessary token attributes
    to the new tokens.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要为新标记分配新的词性标签、依赖标签和必要的标记属性。
- en: Basically, we need to think about how to assign linguistic features to the new
    tokens we created.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本上，我们需要考虑如何为新创建的标记分配语言特征。
- en: 'Let''s see how to deal with the new tokens with an example of how to fix a
    typo, as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下修复拼写错误的例子来看看如何处理新标记：
- en: '[PRE29]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here''s what the dependency tree looks like before the splitting operation:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在分割操作之前，依赖树看起来是这样的：
- en: '![Figure 3.23 – Sample sentence’s dependency tree before retokenization'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.23 – 样本句子在重新标记前的依赖树'
- en: '](img/B16570_03_23.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_03_23.jpg)'
- en: Figure 3.23 – Sample sentence's dependency tree before retokenization
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.23 – 样本句子在重新标记前的依赖树
- en: 'Now, we will split the `doc[3]`, `NewHampshire`, into two tokens: `New` and
    `Hampshire`. We will give fine-grained POS tags and dependency labels to the new
    tokens via the `attrs` dictionary. We will also rearrange the dependency tree
    by passing the new tokens'' parents via the `heads` parameter. While arranging
    the heads, there are two things to consider, as outlined here:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将`doc[3]`、`NewHampshire`分割成两个标记：`New`和`Hampshire`。我们将通过`attrs`字典为新标记分配细粒度的词性标签和依赖标签。我们还将通过`heads`参数传递新标记的父标记来重新排列依赖树。在排列头部时，有两个事情需要考虑，如下所述：
- en: Firstly, if you give a relative position (such as `(doc[3], 1)`) in the following
    code segment, this means that head of `doc[3]` will be the +1th position token—that
    is, `doc[4]` in the new setup (please see the following visualization ).
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，如果你在以下代码段中给出一个相对位置（例如`(doc[3], 1)`），这意味着`doc[3]`的头部将是+1位置的标记，即新设置中的`doc[4]`（请参见以下可视化）。
- en: Secondly, if you give an absolute position, it means the position in the *original*
    `Doc` object. In the following code snippet, the second item in the `heads` list
    means that the `Hampshire` token's head is the second token in the original Doc,
    which is the `in` token (please refer to *Figure 3.23*).
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，如果你给出一个绝对位置，这意味着在*原始* `Doc`对象中的位置。在以下代码片段中，`heads`列表中的第二项意味着`Hampshire`标记的头部是原始Doc中的第二个标记，即`in`标记（请参阅*图3.23*）。
- en: 'After the splitting, we printed the list of new tokens and the linguistic attributes.
    Also, we examined the new length of the `doc` object, which is `6` now. You can
    see the result here:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 分割后，我们打印了新标记的列表和语言属性。我们还检查了`doc`对象的新长度，现在为`6`。你可以在这里看到结果：
- en: '[PRE30]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here''s what the dependency tree looks like after the splitting operation (please
    compare this with *Figure 3.22*):'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这是分割操作后依赖树的模样（请与*图3.22*进行比较）：
- en: '![Figure 3.24 – Dependency tree after the splitting operation'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.24 – 分割操作后的依赖树'
- en: '](img/B16570_03_24.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_03_24.jpg](img/B16570_03_24.jpg)'
- en: Figure 3.24 – Dependency tree after the splitting operation
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.24 – 分割操作后的依赖树
- en: You can apply merging and splitting onto any span, not only the named entity
    spans. The most important part here is to correctly arrange the new dependency
    tree and the linguistic attributes.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以应用合并和分割到任何跨度，而不仅仅是命名实体跨度。这里最重要的部分是正确排列新的依赖树和语言属性。
- en: Summary
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: That was it—you made it to the end of this chapter! It was an exhaustive and
    long journey for sure, but we have unveiled the real linguistic power of spaCy
    to the fullest. This chapter gave you details of spaCy's linguistic features and
    how to use them.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样——你到达了本章的结尾！这确实是一次详尽且漫长的旅程，但我们已经完全揭示了spaCy的真实语言能力。本章为您提供了spaCy的语言特征及其使用方法的详细信息。
- en: You learned about POS tagging and applications, with many examples. You also
    learned about an important yet not so well-known and well-used feature of spaCy—the
    dependency labels. Then, we discovered a famous NLU tool and concept, NER. We
    saw how to do named entity extraction, again via examples. We finalized this chapter
    with a very handy tool for merging and splitting the spans that we calculated
    in the previous sections.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 你学习了关于词性标注和应用的知识，还有许多示例。你还了解了一个重要但不太为人所知且使用得很好的spaCy特性——依赖标签。然后，我们发现了一个著名的NLU工具和概念，NER。我们看到了如何通过示例进行命名实体提取。我们用一个非常实用的工具结束了本章，这个工具可以合并和分割我们在前几节计算出的跨度。
- en: What's next, then? In the next chapter, we will again be discovering a spaCy
    feature that you'll be using every day in your NLP application code—spaCy's `Matcher`
    class. We don't want to give a spoiler on this beautiful subject, so let's go
    onto our journey together!
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，接下来是什么？在下一章中，我们又将发现一个你将在日常NLP应用程序代码中每天都会使用的spaCy特性——spaCy的`Matcher`类。我们不想在这个美好的主题上给出剧透，所以让我们一起继续我们的旅程吧！
