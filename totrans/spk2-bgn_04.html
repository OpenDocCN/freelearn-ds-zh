<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Spark Programming with R"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Spark Programming with R</h1></div></div></div><p>R is a popular statistical computing programming language used by many and freely available under the <span class="strong"><strong>General Public License</strong></span> (<span class="strong"><strong>GNU</strong></span>). R originated from the programming language S, created by John Chambers. R was developed by Ross Ihaka and Robert Gentleman. Many data scientists use R for their computing needs. R has inherent support for many statistical functions and many scalar data types, and has composite data structures for vectors, matrices, data frames, and more, for statistical computation. R is highly extensible and for that, external packages can be created. Once an external package is created, it has to be installed and loaded for any program to use it. A collection of such packages under a directory forms an R library. In other words, R comes with a set of base packages and additional packages that can be installed on top of it to form the required library for the desired computing needs. In addition to functions, datasets can also be packaged in R packages.</p><p>We will cover the following topics in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The need for SparkR</li><li class="listitem" style="list-style-type: disc">Essentials of R</li><li class="listitem" style="list-style-type: disc">Dataframes</li><li class="listitem" style="list-style-type: disc">Aggregations</li><li class="listitem" style="list-style-type: disc">Multi-datasource joins with SparkR</li></ul></div><div class="section" title="The need for SparkR"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec32"/>The need for SparkR</h1></div></div></div><p>A plain base R installation cannot interact with Spark. The <span class="strong"><strong>SparkR</strong></span> package exposes all the required objects and functions for R to talk to the Spark ecosystem. Compared to Scala, Java, and Python, the Spark programming in R is different and the SparkR package mainly exposes R API for DataFrame-based Spark SQL programming. At the moment, R cannot be used to manipulate the RDDs of Spark directly. So for all practical purposes, the R API for Spark has access to only Spark SQL abstractions. The Spark <span class="strong"><strong>MLlib</strong></span> can also be programmed using R because Spark MLlib uses DataFrames.</p><p>How is SparkR going to help the data scientists to do better data processing? The base R installation mandates that all the data to be stored (or accessible) on the computer where R is installed. The data processing occurs on the single computer on which the R installation is available. Moreover, if the data size is more than the main memory available on the computer, R will not be able to do the required processing. With the SparkR package, there is access to a whole new world of a cluster of nodes for data storage and for doing data processing. With the help of SparkR package, R can be used to access the Spark DataFrames as well as R DataFrames.</p><p>It is very important to know the distinction between the two types of data frames, R Dataframes and Spark Dataframes. An R DataFrame is completely local and a data structure of the R language. A Spark DataFrame is a parallel collection of structured data managed by the Spark infrastructure.</p><p>An R DataFrame can be converted to a Spark DataFrame and a Spark DataFrame can be converted to an R DataFrame.</p><p>When a Spark DataFrame is converted to an R DataFrame, it should fit in the available memory of the computer. This conversion is a great feature and there is a need to do so. By converting an R DataFrame to a Spark DataFrame, the data can be distributed and processed in parallel. By converting a Spark DataFrame to an R DataFrame, a lot of computations, charting, and plotting that is done by other R functions can be done. In a nutshell, the SparkR package brings the power of distributed and parallel computing capabilities to R.</p><p>Often, when performing data processing with R, because of the sheer size of the data and the need to fit it into the main memory of the computer, the data processing is done in multiple batches and the results are consolidated to compute the final results. This kind of multi-batch processing can be completely avoided if Spark with R is used to process the data.</p><p>Often, reporting, charting, and plotting are done on the aggregated and summarized raw data. The raw data size can be huge and need not fit into one computer. In such cases, Spark with R can be used to process the entire raw data and finally, the aggregated and summarized data can be used to produce the reports, charts, or plots.</p><p>Because of the inability to process huge amounts of data and for doing data analysis with R, many times, ETL tools are made to use for doing the pre-processing or transformations on the raw data, and only in the final stage is the data analysis done using R. Because of Spark's ability to process data at scale, Spark with R can replace the entire ETL pipeline and do the desired data analysis with R.</p><p>Many R users use the <span class="strong"><strong>dplyr </strong></span>R package for manipulating datasets in R. This package provides fast data manipulation capabilities with R DataFrames. Just like manipulating local R DataFrames, it can access data from some of the RDBMS tables too. Apart from these primitive data manipulation capabilities, it lacks many of the data processing features available in Spark. So Spark with R is a good alternative to packages such as dplyr.</p><p>The SparkR package is yet another R package, but that is not stopping anybody from using any of the R packages that are already being used. At the same time, it supplements the data processing capability of R manifold by making use of the huge data processing capabilities of Spark.</p></div></div>
<div class="section" title="Basics of the R language"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec33"/>Basics of the R language</h1></div></div></div><p>This is not in any way a guide to R programming. But, it is important to touch upon the basics of R as a language very briefly for the benefit of those who are not familiar with R to appreciate what is being covered in this chapter. A very basic introduction to the language features is covered here.</p><p>R comes with a few built-in data types to hold numerical values, character values, and boolean values. There are composite data structures available and the most important ones are, namely, vectors, lists, matrices, and data frames. A vector consists of ordered collection of values of a given type. A list is an ordered collection of elements that can be of different types. For example, a list can hold two vectors, of which one is a vector containing numerical values and the the other is a vector containing boolean values. A matrix is a two-dimensional data structure holding numerical values in rows and columns. A data frame is a two-dimensional data structure containing rows and columns, where columns can have different data types but a single column cannot hold different data types.</p><p>Code samples of using a variable (a special case of vector), a numeric vector, a character vector, a list, a matrix, a data frame, and assigning column names to a data frame are as follows. The variable names are given as self-descriptive as possible for the reader to understand without the help of additional explanation. The following code snippet run on a regular R REPL gives an idea of the data structures of R:</p><pre class="programlisting">
<span class="strong"><strong>$ r &#13;
R version 3.2.2 (2015-08-14) -- "Fire Safety" &#13;
Copyright (C) 2015 The R Foundation for Statistical Computing &#13;
Platform: x86_64-apple-darwin13.4.0 (64-bit) &#13;
 &#13;
R is free software and comes with ABSOLUTELY NO WARRANTY. &#13;
You are welcome to redistribute it under certain conditions. &#13;
Type 'license()' or 'licence()' for distribution details. &#13;
 &#13;
  Natural language support but running in an English locale &#13;
 &#13;
R is a collaborative project with many contributors. &#13;
Type 'contributors()' for more information and &#13;
'citation()' on how to cite R or R packages in publications. &#13;
 &#13;
Type 'demo()' for some demos, 'help()' for on-line help, or &#13;
'help.start()' for an HTML browser interface to help. &#13;
Type 'q()' to quit R. &#13;
 &#13;
Warning: namespace 'SparkR' is not available and has been replaced &#13;
by .GlobalEnv when processing object 'goodTransRecords' &#13;
[Previously saved workspace restored] &#13;
&gt; &#13;
&gt; x &lt;- 5 &#13;
&gt; x &#13;
[1] 5 &#13;
&gt; aNumericVector &lt;- c(10,10.5,31.2,100) &#13;
&gt; aNumericVector &#13;
[1]  10.0  10.5  31.2 100.0 &#13;
&gt; aCharVector &lt;- c("apple", "orange", "mango") &#13;
&gt; aCharVector &#13;
[1] "apple"  "orange" "mango"  &#13;
&gt; aBooleanVector &lt;- c(TRUE, FALSE, TRUE, FALSE, FALSE) &#13;
&gt; aBooleanVector &#13;
[1]  TRUE FALSE  TRUE FALSE FALSE &#13;
&gt; aList &lt;- list(aNumericVector, aCharVector) &#13;
&gt; aList &#13;
[[1]] &#13;
[1]  10.0  10.5  31.2 100.0 &#13;
[[2]] &#13;
[1] "apple"  "orange" "mango" &#13;
&gt; aMatrix &lt;- matrix(c(100, 210, 76, 65, 34, 45),nrow=3,ncol=2,byrow = TRUE) &#13;
&gt; aMatrix &#13;
     [,1] [,2] &#13;
[1,]  100  210 &#13;
[2,]   76   65 &#13;
[3,]   34   45 &#13;
&gt; bMatrix &lt;- matrix(c(100, 210, 76, 65, 34, 45),nrow=3,ncol=2,byrow = FALSE) &#13;
&gt; bMatrix &#13;
     [,1] [,2] &#13;
[1,]  100   65 &#13;
[2,]  210   34 &#13;
[3,]   76   45 &#13;
&gt; ageVector &lt;- c(21, 35, 52)  &#13;
&gt; nameVector &lt;- c("Thomas", "Mathew", "John")  &#13;
&gt; marriedVector &lt;- c(FALSE, TRUE, TRUE)  &#13;
&gt; aDataFrame &lt;- data.frame(ageVector, nameVector, marriedVector)  &#13;
&gt; aDataFrame &#13;
  ageVector nameVector marriedVector &#13;
1        21     Thomas         FALSE &#13;
2        35     Mathew          TRUE &#13;
3        52       John          TRUE &#13;
&gt; colnames(aDataFrame) &lt;- c("Age","Name", "Married") &#13;
&gt; aDataFrame &#13;
  Age   Name Married &#13;
1  21 Thomas   FALSE &#13;
2  35 Mathew    TRUE &#13;
3  52   John    TRUE &#13;
</strong></span>
</pre><p>The main topic of discussion here is going to be revolving around data frames. Some of the functions that are commonly used with data frames are demonstrated here. All these commands are to be executed on the regular R REPL as a continuation of the session that executed the preceding code snippet:</p><pre class="programlisting">
<span class="strong"><strong>&gt; # Returns the first part of the data frame and return two rows &#13;
&gt; head(aDataFrame,2) &#13;
  Age   Name Married &#13;
1  21 Thomas   FALSE &#13;
2  35 Mathew    TRUE &#13;
 &#13;
&gt; # Returns the last part of the data frame and return two rows &#13;
&gt; tail(aDataFrame,2) &#13;
  Age   Name Married  &#13;
2  35 Mathew    TRUE &#13;
3  52   John    TRUE &#13;
&gt; # Number of rows in a data frame &#13;
&gt; nrow(aDataFrame) &#13;
[1] 3 &#13;
&gt; # Number of columns in a data frame &#13;
&gt; ncol(aDataFrame) &#13;
[1] 3 &#13;
&gt; # Returns the first column of the data frame. The return value is a data frame &#13;
&gt; aDataFrame[1] &#13;
  Age &#13;
1  21 &#13;
2  35 &#13;
3  52 &#13;
&gt; # Returns the second column of the data frame. The return value is a data frame &#13;
&gt; aDataFrame[2] &#13;
    Name &#13;
1 Thomas &#13;
2 Mathew &#13;
3   John &#13;
&gt; # Returns the named columns of the data frame. The return value is a data frame &#13;
&gt; aDataFrame[c("Age", "Name")] &#13;
  Age   Name &#13;
1  21 Thomas &#13;
2  35 Mathew &#13;
3  52   John &#13;
&gt; # Returns the contents of the second column of the data frame as a vector.  &#13;
&gt; aDataFrame[[2]] &#13;
[1] Thomas Mathew John   &#13;
Levels: John Mathew Thomas &#13;
&gt; # Returns the slice of the data frame by a row &#13;
&gt; aDataFrame[2,] &#13;
  Age   Name Married &#13;
2  35 Mathew    TRUE &#13;
&gt; # Returns the slice of the data frame by multiple rows &#13;
&gt; aDataFrame[c(1,2),] &#13;
  Age   Name Married &#13;
1  21 Thomas   FALSE &#13;
2  35 Mathew    TRUE &#13;
</strong></span>
</pre></div>
<div class="section" title="DataFrames in R and Spark"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec34"/>DataFrames in R and Spark</h1></div></div></div><p>When working with Spark using R, it is very easy to get confused with the DataFrame data structure. As mentioned earlier, it is there in R and in Spark SQL. The following code snippet deals with converting an R DataFrame to a Spark DataFrame and vice versa. This is going to be a very common operation when programming Spark with R. The following code snippet is to be executed in the R REPL of Spark. From now on, all the references to the R REPL are with respect to the R REPL of Spark:</p><pre class="programlisting">
<span class="strong"><strong>$ cd $SPARK_HOME &#13;
$ ./bin/sparkR &#13;
 &#13;
R version 3.2.2 (2015-08-14) -- "Fire Safety" &#13;
Copyright (C) 2015 The R Foundation for Statistical Computing &#13;
Platform: x86_64-apple-darwin13.4.0 (64-bit) &#13;
 &#13;
R is free software and comes with ABSOLUTELY NO WARRANTY. &#13;
You are welcome to redistribute it under certain conditions. &#13;
Type 'license()' or 'licence()' for distribution details. &#13;
 &#13;
  Natural language support but running in an English locale &#13;
 &#13;
R is a collaborative project with many contributors. &#13;
Type 'contributors()' for more information and &#13;
'citation()' on how to cite R or R packages in publications. &#13;
 &#13;
Type 'demo()' for some demos, 'help()' for on-line help, or &#13;
'help.start()' for an HTML browser interface to help. &#13;
Type 'q()' to quit R. &#13;
 &#13;
[Previously saved workspace restored] &#13;
 &#13;
Launching java with spark-submit command /Users/RajT/source-code/spark-source/spark-2.0/bin/spark-submit   "sparkr-shell" /var/folders/nf/trtmyt9534z03kq8p8zgbnxh0000gn/T//RtmpmuRsTC/backend_port2d121acef4  &#13;
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties &#13;
Setting default log level to "WARN". &#13;
To adjust logging level use sc.setLogLevel(newLevel). &#13;
16/07/16 21:08:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable &#13;
 &#13;
 Welcome to &#13;
    ____              __  &#13;
   / __/__  ___ _____/ /__  &#13;
  _\ \/ _ \/ _ `/ __/  '_/  &#13;
 /___/ .__/\_,_/_/ /_/\_\   version  2.0.1-SNAPSHOT  &#13;
    /_/  &#13;
 &#13;
 &#13;
 Spark context is available as sc, SQL context is available as sqlContext &#13;
During startup - Warning messages: &#13;
1: 'SparkR::sparkR.init' is deprecated. &#13;
Use 'sparkR.session' instead. &#13;
See help("Deprecated")  &#13;
2: 'SparkR::sparkRSQL.init' is deprecated. &#13;
Use 'sparkR.session' instead. &#13;
See help("Deprecated")  &#13;
&gt; &#13;
&gt; # faithful is a data set and the data frame that comes with base R &#13;
&gt; # Obviously it is an R DataFrame &#13;
&gt; head(faithful) &#13;
  eruptions waiting &#13;
1     3.600      79 &#13;
2     1.800      54 &#13;
3     3.333      74 &#13;
4     2.283      62 &#13;
5     4.533      85 &#13;
6     2.883      55 &#13;
&gt; tail(faithful) &#13;
    eruptions waiting &#13;
267     4.750      75 &#13;
268     4.117      81 &#13;
269     2.150      46 &#13;
270     4.417      90 &#13;
271     1.817      46 &#13;
272     4.467      74 &#13;
&gt; # Convert R DataFrame to Spark DataFrame  &#13;
&gt; sparkFaithful &lt;- createDataFrame(faithful) &#13;
&gt; head(sparkFaithful) &#13;
  eruptions waiting &#13;
1     3.600      79 &#13;
2     1.800      54 &#13;
3     3.333      74 &#13;
4     2.283      62 &#13;
5     4.533      85 &#13;
6     2.883      55 &#13;
&gt; showDF(sparkFaithful) &#13;
+---------+-------+ &#13;
|eruptions|waiting| &#13;
+---------+-------+ &#13;
|      3.6|   79.0| &#13;
|      1.8|   54.0| &#13;
|    3.333|   74.0| &#13;
|    2.283|   62.0| &#13;
|    4.533|   85.0| &#13;
|    2.883|   55.0| &#13;
|      4.7|   88.0| &#13;
|      3.6|   85.0| &#13;
|     1.95|   51.0| &#13;
|     4.35|   85.0| &#13;
|    1.833|   54.0| &#13;
|    3.917|   84.0| &#13;
|      4.2|   78.0| &#13;
|     1.75|   47.0| &#13;
|      4.7|   83.0| &#13;
|    2.167|   52.0| &#13;
|     1.75|   62.0| &#13;
|      4.8|   84.0| &#13;
|      1.6|   52.0| &#13;
|     4.25|   79.0| &#13;
+---------+-------+ &#13;
only showing top 20 rows &#13;
&gt; # Try calling a SparkR function showDF() on an R DataFrame. The following error message will be shown &#13;
&gt; showDF(faithful) &#13;
Error in (function (classes, fdef, mtable)  :  &#13;
  unable to find an inherited method for function 'showDF' for signature '"data.frame"' &#13;
&gt; # Convert the Spark DataFrame to an R DataFrame &#13;
&gt; rFaithful &lt;- collect(sparkFaithful) &#13;
&gt; head(rFaithful) &#13;
  eruptions waiting &#13;
1     3.600      79 &#13;
2     1.800      54 &#13;
3     3.333      74 &#13;
4     2.283      62 &#13;
5     4.533      85 &#13;
6     2.883      55 &#13;
 &#13;
</strong></span>
</pre><p>There is no complete compatibility and interoperability between an R DataFrame and a Spark DataFrame in terms of the supported functions.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip29"/>Tip</h3><p>As a good practice, it is better to name the R DataFrame and Spark DataFrame with agreed  conventions in R programs in order to have a distinction between the two different types. Not all the functions that are supported on R DataFrames are not supported on Spark DataFrames and vice versa. Always refer to the right version of the R API for Spark.</p></div></div><p>Those who use a lot of charting and plotting have to be extra careful while dealing with R DataFrames in conjunction with Spark DataFrames. The charting and plotting of R works with only R DataFrames. If there is a need to produce charts or plots with the data processed by Spark and available in Spark DataFrame, it has to be converted to an R DataFrame to proceed with the charting and plotting. The following code snippet will give an idea this. We will use the faithful dataset again for elucidation purposes in the R REPL of Spark:</p><pre class="programlisting">
<span class="strong"><strong>head(faithful) &#13;
  eruptions waiting &#13;
1     3.600      79 &#13;
2     1.800      54 &#13;
3     3.333      74 &#13;
4     2.283      62 &#13;
5     4.533      85 &#13;
6     2.883      55 &#13;
&gt; # Convert the faithful R DataFrame to Spark DataFrame   &#13;
&gt; sparkFaithful &lt;- createDataFrame(faithful) &#13;
&gt; # The Spark DataFrame sparkFaithful NOT producing a histogram &#13;
&gt; hist(sparkFaithful$eruptions,main="Distribution of Eruptions",xlab="Eruptions") &#13;
Error in hist.default(sparkFaithful$eruptions, main = "Distribution of Eruptions",  :  &#13;
  'x' must be numeric &#13;
&gt; # The R DataFrame faithful producing a histogram &#13;
&gt; hist(faithful$eruptions,main="Distribution of Eruptions",xlab="Eruptions")</strong></span>
</pre><p>The figure here is used jut to demonstrate that the Spark DataFrame cannot be used to do charting and R DataFrame has to be used for the same:</p><p>
</p><div class="mediaobject"><img alt="DataFrames in R and Spark" src="graphics/image_04_002.jpg"/><div class="caption"><p>Figure 1</p></div></div><p>
</p><p>The charting and plotting library, when used with Spark DataFrame, gave an error because of the incompatibility of the data types.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip30"/>Tip</h3><p>The most important aspect to have in mind is that an R DataFrame is an in-memory resident data structure, while a Spark DataFrame is a parallel collection of datasets distributed across a cluster of nodes. So, all the functions that use R DataFrames need not work with Spark DataFrames and vice versa.</p></div></div><p>Let's revisit the bigger picture again, as given in <span class="emphasis"><em>Figure 2</em></span>, to set the context and see what is being discussed here before getting into and taking up the use cases. In the previous chapter, the same subject was introduced by using the programming languages Scala and Python. In this chapter, the same set of use cases used in the Spark SQL programming will be implemented using R:</p><p>
</p><div class="mediaobject"><img alt="DataFrames in R and Spark" src="graphics/image_04_004.jpg"/><div class="caption"><p>Figure 2</p></div></div><p>
</p><p>The use cases that are going to be discussed here will be demonstrating the ability to mix SQL queries with Spark programs in R. Multiple data sources will be chosen, data will be read from those sources using DataFrame, and uniform data access will be demonstrated.</p></div>
<div class="section" title="Spark DataFrame programming with R"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec35"/>Spark DataFrame programming with R</h1></div></div></div><p>The use cases selected for elucidating the Spark SQL way of programming with DataFrame are given as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The transaction records are comma-separated values.</li><li class="listitem" style="list-style-type: disc">Filter out only the good transaction records from the list. The account number should start with <code class="literal">SB</code> and the transaction amount should be greater than zero.</li><li class="listitem" style="list-style-type: disc">Find all the high value transaction records with a transaction amount greater than 1000.</li><li class="listitem" style="list-style-type: disc">Find all the transaction records where the account number is bad.</li><li class="listitem" style="list-style-type: disc">Find all the transaction records where the transaction amount is less than or equal to zero.</li><li class="listitem" style="list-style-type: disc">Find a combined list of all the bad transaction records.</li><li class="listitem" style="list-style-type: disc">Find the total of all the transaction amounts.</li><li class="listitem" style="list-style-type: disc">Find the maximum of all the transaction amounts.</li><li class="listitem" style="list-style-type: disc">Find the minimum of all the transaction amounts.</li><li class="listitem" style="list-style-type: disc">Find all the good account numbers.</li></ul></div><p>This is exactly the same set of use cases that were used in the previous chapter, but here, the programming model is totally different. Here, the programming is done in R. Using this set of use cases, two types of programming model are demonstrated here. One is using the SQL queries and other is using DataFrame APIs.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip31"/>Tip</h3><p>The data files needed for running the following code snippets are available from the same directory where the R code is kept.</p></div></div><p>In the following code snippets, data is read from files located in the filesystem. Since all these code snippets are executed from the R REPL of Spark, all the data files are to be kept in the <code class="literal">$SPARK_HOME</code> directory.</p><div class="section" title="Programming with SQL"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec19"/>Programming with SQL</h2></div></div></div><p>At the R REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>&gt; # TODO - Change the data directory location to the right location in the system in which this program is being run &#13;
&gt; DATA_DIR &lt;- "/Users/RajT/Documents/CodeAndData/R/" &#13;
&gt; # Read data from a JSON file to create DataFrame &#13;
&gt;  &#13;
&gt; acTransDF &lt;- read.json(paste(DATA_DIR, "TransList1.json", sep = "")) &#13;
&gt; # Print the structure of the DataFrame &#13;
&gt; print(acTransDF) &#13;
SparkDataFrame[AccNo:string, TranAmount:bigint] &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(acTransDF) &#13;
+-------+----------+ &#13;
|  AccNo|TranAmount| &#13;
+-------+----------+ &#13;
|SB10001|      1000| &#13;
|SB10002|      1200| &#13;
|SB10003|      8000| &#13;
|SB10004|       400| &#13;
|SB10005|       300| &#13;
|SB10006|     10000| &#13;
|SB10007|       500| &#13;
|SB10008|        56| &#13;
|SB10009|        30| &#13;
|SB10010|      7000| &#13;
|CR10001|      7000| &#13;
|SB10002|       -10| &#13;
+-------+----------+ &#13;
&gt; # Register temporary view definition in the DataFrame for SQL queries &#13;
&gt; createOrReplaceTempView(acTransDF, "trans") &#13;
&gt; # DataFrame containing good transaction records using SQL &#13;
&gt; goodTransRecords &lt;- sql("SELECT AccNo, TranAmount FROM trans WHERE AccNo like 'SB%' AND TranAmount &gt; 0") &#13;
&gt; # Register temporary table definition in the DataFrame for SQL queries &#13;
 &#13;
&gt; createOrReplaceTempView(goodTransRecords, "goodtrans") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(goodTransRecords) &#13;
+-------+----------+ &#13;
|  AccNo|TranAmount| &#13;
+-------+----------+ &#13;
|SB10001|      1000| &#13;
|SB10002|      1200| &#13;
|SB10003|      8000| &#13;
|SB10004|       400| &#13;
|SB10005|       300| &#13;
|SB10006|     10000| &#13;
|SB10007|       500| &#13;
|SB10008|        56| &#13;
|SB10009|        30| &#13;
|SB10010|      7000| &#13;
+-------+----------+ &#13;
&gt; # DataFrame containing high value transaction records using SQL &#13;
&gt; highValueTransRecords &lt;- sql("SELECT AccNo, TranAmount FROM goodtrans WHERE TranAmount &gt; 1000") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(highValueTransRecords) &#13;
+-------+----------+ &#13;
|  AccNo|TranAmount| &#13;
+-------+----------+ &#13;
|SB10002|      1200| &#13;
|SB10003|      8000| &#13;
|SB10006|     10000| &#13;
|SB10010|      7000| &#13;
+-------+----------+ &#13;
&gt; # DataFrame containing bad account records using SQL &#13;
&gt; badAccountRecords &lt;- sql("SELECT AccNo, TranAmount FROM trans WHERE AccNo NOT like 'SB%'") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(badAccountRecords) &#13;
+-------+----------+ &#13;
|  AccNo|TranAmount| &#13;
+-------+----------+ &#13;
|CR10001|      7000| &#13;
+-------+----------+ &#13;
&gt; # DataFrame containing bad amount records using SQL &#13;
&gt; badAmountRecords &lt;- sql("SELECT AccNo, TranAmount FROM trans WHERE TranAmount &lt; 0") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(badAmountRecords) &#13;
+-------+----------+ &#13;
|  AccNo|TranAmount| &#13;
+-------+----------+ &#13;
|SB10002|       -10| &#13;
+-------+----------+ &#13;
&gt; # Create a DataFrame by taking the union of two DataFrames &#13;
&gt; badTransRecords &lt;- union(badAccountRecords, badAmountRecords) &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(badTransRecords) &#13;
+-------+----------+ &#13;
|  AccNo|TranAmount| &#13;
+-------+----------+ &#13;
|CR10001|      7000| &#13;
|SB10002|       -10| &#13;
+-------+----------+ &#13;
&gt; # DataFrame containing sum amount using SQL &#13;
&gt; sumAmount &lt;- sql("SELECT sum(TranAmount) as sum FROM goodtrans") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(sumAmount) &#13;
+-----+ &#13;
|  sum| &#13;
+-----+ &#13;
|28486| &#13;
+-----+ &#13;
&gt; # DataFrame containing maximum amount using SQL &#13;
&gt; maxAmount &lt;- sql("SELECT max(TranAmount) as max FROM goodtrans") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(maxAmount) &#13;
+-----+ &#13;
|  max| &#13;
+-----+ &#13;
|10000| &#13;
+-----+ &#13;
&gt; # DataFrame containing minimum amount using SQL &#13;
&gt; minAmount &lt;- sql("SELECT min(TranAmount)as min FROM goodtrans") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(minAmount) &#13;
+---+ &#13;
|min| &#13;
+---+ &#13;
| 30| &#13;
+---+ &#13;
&gt; # DataFrame containing good account number records using SQL &#13;
&gt; goodAccNos &lt;- sql("SELECT DISTINCT AccNo FROM trans WHERE AccNo like 'SB%' ORDER BY AccNo") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(goodAccNos) &#13;
+-------+ &#13;
|  AccNo| &#13;
+-------+ &#13;
|SB10001| &#13;
|SB10002| &#13;
|SB10003| &#13;
|SB10004| &#13;
|SB10005| &#13;
|SB10006| &#13;
|SB10007| &#13;
|SB10008| &#13;
|SB10009| &#13;
|SB10010| &#13;
+-------+</strong></span>
</pre><p>The retail banking transaction records come with account number, transaction amount are processed using SparkSQL to get the desired results of the use cases. Here is the summary of what the preceding script did:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Unlike other programming languages supported with Spark, R doesn't have an RDD programming capability. So, instead of going with the construction of RDD from collections, the data is read from the JSON file containing the transaction records.</li><li class="listitem" style="list-style-type: disc">A Spark DataFrame is created from the JSON file.</li><li class="listitem" style="list-style-type: disc">A table is registered with the DataFrame with a name. This registered name of the table can be used in SQL statements.</li><li class="listitem" style="list-style-type: disc">Then, all the other activities are issuing SQL statements using the SQL function from the SparkR package.</li><li class="listitem" style="list-style-type: disc">The result of all these SQL statements is stored as Spark DataFrames, and showDF function is used to extract the values to the calling R program.</li><li class="listitem" style="list-style-type: disc">The aggregate value calculations are also done through the SQL statements.</li><li class="listitem" style="list-style-type: disc">The DataFrame contents are displayed in table format using the the <code class="literal">showDF</code> function of SparkR.</li><li class="listitem" style="list-style-type: disc">A detailed view of the structure of the DataFrame is displayed using the print function. This is akin to the describe command of the database tables.</li></ul></div><p>In the preceding R code, the style of programming is different as compared to the Scala code. That is because it is an R program. Using the SparkR library, the Spark features are being used. But the functions and other abstractions are not in a really different style.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note32"/>Note</h3><p>Throughout this chapter, there will be instances where DataFrames are used. It is very easy to get confused by which is the R DataFrame and which is the Spark DataFrame. Hence, care is taken to specifically mention by qualifying the DataFrame, such as R DataFrame and Spark DataFrame.</p></div></div></div><div class="section" title="Programming with R DataFrame API"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec20"/>Programming with R DataFrame API</h2></div></div></div><p>In this section, the code snippets will be run in the same R REPL. Like the preceding code snippets, initially, some DataFrame-specific basic commands are given. These are used regularly to see the contents and for doing some sanity tests on the DataFrame and its contents. These are commands that are typically used in the exploratory stage of the data analysis quite often to get more insight into the structure and contents of the underlying data.</p><p>At the R REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>&gt; # Read data from a JSON file to create DataFrame &#13;
&gt; acTransDF &lt;- read.json(paste(DATA_DIR, "TransList1.json", sep = "")) &#13;
&gt; print(acTransDF) &#13;
SparkDataFrame[AccNo:string, TranAmount:bigint] &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(acTransDF) &#13;
+-------+----------+ &#13;
|  AccNo|TranAmount| &#13;
+-------+----------+ &#13;
|SB10001|      1000| &#13;
|SB10002|      1200| &#13;
|SB10003|      8000| &#13;
|SB10004|       400| &#13;
|SB10005|       300| &#13;
|SB10006|     10000| &#13;
|SB10007|       500| &#13;
|SB10008|        56| &#13;
|SB10009|        30| &#13;
|SB10010|      7000| &#13;
|CR10001|      7000| &#13;
|SB10002|       -10| &#13;
+-------+----------+ &#13;
&gt; # DataFrame containing good transaction records using API &#13;
&gt; goodTransRecordsFromAPI &lt;- filter(acTransDF, "AccNo like 'SB%' AND TranAmount &gt; 0") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(goodTransRecordsFromAPI) &#13;
+-------+----------+ &#13;
|  AccNo|TranAmount| &#13;
+-------+----------+ &#13;
|SB10001|      1000| &#13;
|SB10002|      1200| &#13;
|SB10003|      8000| &#13;
|SB10004|       400| &#13;
|SB10005|       300| &#13;
|SB10006|     10000| &#13;
|SB10007|       500| &#13;
|SB10008|        56| &#13;
|SB10009|        30| &#13;
|SB10010|      7000| &#13;
+-------+----------+ &#13;
&gt; # DataFrame containing high value transaction records using API &#13;
&gt; highValueTransRecordsFromAPI = filter(goodTransRecordsFromAPI, "TranAmount &gt; 1000") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(highValueTransRecordsFromAPI) &#13;
+-------+----------+ &#13;
|  AccNo|TranAmount| &#13;
+-------+----------+ &#13;
|SB10002|      1200| &#13;
|SB10003|      8000| &#13;
|SB10006|     10000| &#13;
|SB10010|      7000| &#13;
+-------+----------+ &#13;
&gt; # DataFrame containing bad account records using API &#13;
&gt; badAccountRecordsFromAPI &lt;- filter(acTransDF, "AccNo NOT like 'SB%'") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(badAccountRecordsFromAPI) &#13;
+-------+----------+ &#13;
|  AccNo|TranAmount| &#13;
+-------+----------+ &#13;
|CR10001|      7000| &#13;
+-------+----------+ &#13;
&gt; # DataFrame containing bad amount records using API &#13;
&gt; badAmountRecordsFromAPI &lt;- filter(acTransDF, "TranAmount &lt; 0") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(badAmountRecordsFromAPI) &#13;
+-------+----------+ &#13;
|  AccNo|TranAmount| &#13;
+-------+----------+ &#13;
|SB10002|       -10| &#13;
+-------+----------+ &#13;
&gt; # Create a DataFrame by taking the union of two DataFrames &#13;
&gt; badTransRecordsFromAPI &lt;- union(badAccountRecordsFromAPI, badAmountRecordsFromAPI) &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(badTransRecordsFromAPI) &#13;
+-------+----------+ &#13;
|  AccNo|TranAmount| &#13;
+-------+----------+ &#13;
|CR10001|      7000| &#13;
|SB10002|       -10| &#13;
+-------+----------+ &#13;
&gt; # DataFrame containing sum amount using API &#13;
&gt; sumAmountFromAPI &lt;- agg(goodTransRecordsFromAPI, sumAmount = sum(goodTransRecordsFromAPI$TranAmount)) &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(sumAmountFromAPI) &#13;
+---------+ &#13;
|sumAmount| &#13;
+---------+ &#13;
|    28486| &#13;
+---------+ &#13;
&gt; # DataFrame containing maximum amount using API &#13;
&gt; maxAmountFromAPI &lt;- agg(goodTransRecordsFromAPI, maxAmount = max(goodTransRecordsFromAPI$TranAmount)) &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(maxAmountFromAPI) &#13;
+---------+ &#13;
|maxAmount| &#13;
+---------+ &#13;
|    10000| &#13;
+---------+ &#13;
&gt; # DataFrame containing minimum amount using API &#13;
&gt; minAmountFromAPI &lt;- agg(goodTransRecordsFromAPI, minAmount = min(goodTransRecordsFromAPI$TranAmount))  &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(minAmountFromAPI) &#13;
+---------+ &#13;
|minAmount| &#13;
+---------+ &#13;
|       30| &#13;
+---------+ &#13;
&gt; # DataFrame containing good account number records using API &#13;
&gt; filteredTransRecordsFromAPI &lt;- filter(goodTransRecordsFromAPI, "AccNo like 'SB%'") &#13;
&gt; accNosFromAPI &lt;- select(filteredTransRecordsFromAPI, "AccNo") &#13;
&gt; distinctAccNoFromAPI &lt;- distinct(accNosFromAPI) &#13;
&gt; sortedAccNoFromAPI &lt;- arrange(distinctAccNoFromAPI, "AccNo") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(sortedAccNoFromAPI) &#13;
+-------+ &#13;
|  AccNo| &#13;
+-------+ &#13;
|SB10001| &#13;
|SB10002| &#13;
|SB10003| &#13;
|SB10004| &#13;
|SB10005| &#13;
|SB10006| &#13;
|SB10007| &#13;
|SB10008| &#13;
|SB10009| &#13;
|SB10010| &#13;
+-------+ &#13;
&gt; # Persist the DataFrame into a Parquet file  &#13;
&gt; write.parquet(acTransDF, "r.trans.parquet") &#13;
&gt; # Read the data from the Parquet file &#13;
&gt; acTransDFFromFile &lt;- read.parquet("r.trans.parquet")  &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(acTransDFFromFile) &#13;
+-------+----------+ &#13;
|  AccNo|TranAmount| &#13;
+-------+----------+ &#13;
|SB10007|       500| &#13;
|SB10008|        56| &#13;
|SB10009|        30| &#13;
|SB10010|      7000| &#13;
|CR10001|      7000| &#13;
|SB10002|       -10| &#13;
|SB10001|      1000| &#13;
|SB10002|      1200| &#13;
|SB10003|      8000| &#13;
|SB10004|       400| &#13;
|SB10005|       300| &#13;
|SB10006|     10000| &#13;
+-------+----------+ &#13;
</strong></span>
</pre><p>Here is the summary of what the preceding script did from a DataFrame API perspective:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The DataFrame containing the superset of data used in the preceding section is used here.</li><li class="listitem" style="list-style-type: disc">Filtering of the records is demonstrated next. Here, the most important aspect to notice is that the filter predicate is to be given exactly like the predicates in the SQL statements. Filters can not be chained.</li><li class="listitem" style="list-style-type: disc">The aggregation methods are calculated next.</li><li class="listitem" style="list-style-type: disc">The final statements in this set are doing the selection, filtering, choosing distinct records, and ordering.</li><li class="listitem" style="list-style-type: disc">Finally, the transaction records are persisted in Parquet format, read from the Parquet store, and created a Spark DataFrame. More details on the persistence formats have been covered in the previous chapter and the concepts remain the same. Only the DataFrame API syntax is different.</li><li class="listitem" style="list-style-type: disc">In this code snippet, the Parquet format data is stored in the current directory, from where the corresponding REPL is invoked. When it is run as a Spark program, the directory again will be the current directory from where the Spark submit is invoked.</li></ul></div><p>The last few statements are about the persisting of the DataFrame contents into the media. If this is compared with the persistence mechanisms in the previous chapter with Scala and Python, here also it is done in similar ways. </p></div></div>
<div class="section" title="Understanding aggregations in Spark R"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec36"/>Understanding aggregations in Spark R</h1></div></div></div><p>In SQL, aggregation of data is very flexible. The same thing is true in Spark SQL too. Instead of running SQL statements on a single data source located in a single machine, here, Spark SQL can do the same on distributed data sources. In the chapter where RDD-based programming is covered, a MapReduce use case was discussed to do data aggregation and the same is being used here to demonstrate the aggregation capabilities of Spark SQL. In this section also, the use cases are approached in the SQL query way as well as in the DataFrame API way. </p><p>The use cases selected for elucidating the MapReduce kind of data processing are given here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The retail banking transaction records come with account number and transaction amount in comma-separated strings</li><li class="listitem" style="list-style-type: disc">Find an account level summary of all the transactions to get the account balance </li></ul></div><p>At the R REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>&gt; # Read data from a JSON file to create DataFrame &#13;
&gt; acTransDFForAgg &lt;- read.json(paste(DATA_DIR, "TransList2.json", sep = "")) &#13;
&gt; # Register temporary view definition in the DataFrame for SQL queries &#13;
&gt; createOrReplaceTempView(acTransDFForAgg, "transnew") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(acTransDFForAgg) &#13;
+-------+----------+ &#13;
|  AccNo|TranAmount| &#13;
+-------+----------+ &#13;
|SB10001|      1000| &#13;
|SB10002|      1200| &#13;
|SB10001|      8000| &#13;
|SB10002|       400| &#13;
|SB10003|       300| &#13;
|SB10001|     10000| &#13;
|SB10004|       500| &#13;
|SB10005|        56| &#13;
|SB10003|        30| &#13;
|SB10002|      7000| &#13;
|SB10001|      -100| &#13;
|SB10002|       -10| &#13;
+-------+----------+ &#13;
&gt; # DataFrame containing account summary records using SQL &#13;
&gt; acSummary &lt;- sql("SELECT AccNo, sum(TranAmount) as TransTotal FROM transnew GROUP BY AccNo") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(acSummary) &#13;
+-------+----------+ &#13;
|  AccNo|TransTotal| &#13;
+-------+----------+ &#13;
|SB10001|     18900| &#13;
|SB10002|      8590| &#13;
|SB10003|       330| &#13;
|SB10004|       500| &#13;
|SB10005|        56| &#13;
+-------+----------+ &#13;
&gt; # DataFrame containing account summary records using API &#13;
&gt; acSummaryFromAPI &lt;- agg(groupBy(acTransDFForAgg, "AccNo"), TranAmount="sum") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(acSummaryFromAPI) &#13;
+-------+---------------+ &#13;
|  AccNo|sum(TranAmount)| &#13;
+-------+---------------+ &#13;
|SB10001|          18900| &#13;
|SB10002|           8590| &#13;
|SB10003|            330| &#13;
|SB10004|            500| &#13;
|SB10005|             56| &#13;
+-------+---------------+  &#13;
</strong></span>
</pre><p>In the R DataFrame API, there are some syntax differences as compared to its Scala or Python counterparts, mainly because this is a purely API-based programming model.</p></div>
<div class="section" title="Understanding multi-datasource joins with SparkR"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec37"/>Understanding multi-datasource joins with SparkR</h1></div></div></div><p>In the previous chapter, the joining of multiple DataFrames based on the key has been discussed. In this section, the same use case is implemented using R API of Spark SQL. The use cases selected for elucidating the join of multiple datasets using a key are given in the following section.</p><p>The first dataset contains a retail banking master records summary with the account number, first name, and last name. The second dataset contains the retail banking account balance with account number and balance amount. The key on both of the datasets is the account number. Join the two datasets and create one dataset containing the account number, first name, last name, and balance amount. From this report, pick up the top three accounts in terms of the balance amount.</p><p>The Spark DataFrames are created from persisted JSON files. Instead of the JSON files, it can be any supported data files. Then they are read from the disk to form the DataFrames and they are joined together.</p><p>At the R REPL prompt, try the following statements: </p><pre class="programlisting">
<span class="strong"><strong>&gt; # Read data from JSON file &#13;
&gt; acMasterDF &lt;- read.json(paste(DATA_DIR, "MasterList.json", sep = "")) &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(acMasterDF) &#13;
+-------+---------+--------+ &#13;
|  AccNo|FirstName|LastName| &#13;
+-------+---------+--------+ &#13;
|SB10001|    Roger| Federer| &#13;
|SB10002|     Pete| Sampras| &#13;
|SB10003|   Rafael|   Nadal| &#13;
|SB10004|    Boris|  Becker| &#13;
|SB10005|     Ivan|   Lendl| &#13;
+-------+---------+--------+ &#13;
&gt; # Register temporary view definition in the DataFrame for SQL queries &#13;
&gt; createOrReplaceTempView(acMasterDF, "master")  &#13;
&gt; acBalDF &lt;- read.json(paste(DATA_DIR, "BalList.json", sep = "")) &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(acBalDF) &#13;
+-------+---------+ &#13;
|  AccNo|BalAmount| &#13;
+-------+---------+ &#13;
|SB10001|    50000| &#13;
|SB10002|    12000| &#13;
|SB10003|     3000| &#13;
|SB10004|     8500| &#13;
|SB10005|     5000| &#13;
+-------+---------+ &#13;
 &#13;
&gt; # Register temporary view definition in the DataFrame for SQL queries &#13;
&gt; createOrReplaceTempView(acBalDF, "balance") &#13;
&gt; # DataFrame containing account detail records using SQL by joining multiple DataFrame contents &#13;
&gt; acDetail &lt;- sql("SELECT master.AccNo, FirstName, LastName, BalAmount FROM master, balance WHERE master.AccNo = balance.AccNo ORDER BY BalAmount DESC") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(acDetail) &#13;
+-------+---------+--------+---------+ &#13;
|  AccNo|FirstName|LastName|BalAmount| &#13;
+-------+---------+--------+---------+ &#13;
|SB10001|    Roger| Federer|    50000| &#13;
|SB10002|     Pete| Sampras|    12000| &#13;
|SB10004|    Boris|  Becker|     8500| &#13;
|SB10005|     Ivan|   Lendl|     5000| &#13;
|SB10003|   Rafael|   Nadal|     3000| &#13;
+-------+---------+--------+---------+ &#13;
 &#13;
&gt; # Persist data in the DataFrame into Parquet file &#13;
&gt; write.parquet(acDetail, "r.acdetails.parquet") &#13;
&gt; # Read data into a DataFrame by reading the contents from a Parquet file &#13;
 &#13;
&gt; acDetailFromFile &lt;- read.parquet("r.acdetails.parquet") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(acDetailFromFile) &#13;
+-------+---------+--------+---------+ &#13;
|  AccNo|FirstName|LastName|BalAmount| &#13;
+-------+---------+--------+---------+ &#13;
|SB10002|     Pete| Sampras|    12000| &#13;
|SB10003|   Rafael|   Nadal|     3000| &#13;
|SB10005|     Ivan|   Lendl|     5000| &#13;
|SB10001|    Roger| Federer|    50000| &#13;
|SB10004|    Boris|  Becker|     8500| &#13;
+-------+---------+--------+---------+ &#13;
 &#13;
</strong></span>
</pre><p>Continuing from the same R REPL session, the following lines of code get the same result through the DataFrame API:</p><pre class="programlisting">
<span class="strong"><strong>&gt; # Change the column names &#13;
&gt; acBalDFWithDiffColName &lt;- selectExpr(acBalDF, "AccNo as AccNoBal", "BalAmount") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(acBalDFWithDiffColName) &#13;
+--------+---------+ &#13;
|AccNoBal|BalAmount| &#13;
+--------+---------+ &#13;
| SB10001|    50000| &#13;
| SB10002|    12000| &#13;
| SB10003|     3000| &#13;
| SB10004|     8500| &#13;
| SB10005|     5000| &#13;
+--------+---------+ &#13;
&gt; # DataFrame containing account detail records using API by joining multiple DataFrame contents &#13;
&gt; acDetailFromAPI &lt;- join(acMasterDF, acBalDFWithDiffColName, acMasterDF$AccNo == acBalDFWithDiffColName$AccNoBal) &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(acDetailFromAPI) &#13;
+-------+---------+--------+--------+---------+ &#13;
|  AccNo|FirstName|LastName|AccNoBal|BalAmount| &#13;
+-------+---------+--------+--------+---------+ &#13;
|SB10001|    Roger| Federer| SB10001|    50000| &#13;
|SB10002|     Pete| Sampras| SB10002|    12000| &#13;
|SB10003|   Rafael|   Nadal| SB10003|     3000| &#13;
|SB10004|    Boris|  Becker| SB10004|     8500| &#13;
|SB10005|     Ivan|   Lendl| SB10005|     5000| &#13;
+-------+---------+--------+--------+---------+ &#13;
&gt; # DataFrame containing account detail records using SQL by selecting specific fields &#13;
&gt; acDetailFromAPIRequiredFields &lt;- select(acDetailFromAPI, "AccNo", "FirstName", "LastName", "BalAmount") &#13;
&gt; # Show sample records from the DataFrame &#13;
&gt; showDF(acDetailFromAPIRequiredFields) &#13;
+-------+---------+--------+---------+ &#13;
|  AccNo|FirstName|LastName|BalAmount| &#13;
+-------+---------+--------+---------+ &#13;
|SB10001|    Roger| Federer|    50000| &#13;
|SB10002|     Pete| Sampras|    12000| &#13;
|SB10003|   Rafael|   Nadal|     3000| &#13;
|SB10004|    Boris|  Becker|     8500| &#13;
|SB10005|     Ivan|   Lendl|     5000| &#13;
+-------+---------+--------+---------+ &#13;
</strong></span>
</pre><p>The join type selected in the preceding section of the code is inner join. Instead of that, any other type of join can be used, either through the SQL query way or through the DataFrame API way. One word of caution before using the join using DataFrame API is that the column names of both the Spark DataFrames have to be different to avoid ambiguity in the resultant Spark DataFrame. In this particular use case, it can be seen that the DataFrame API is becoming a bit difficult to deal with, while the SQL query way is looking very straightforward.</p><p>In the preceding sections, the R API for Spark SQL has been covered. In general, if possible, it is better to write the code using the SQL query way as much as possible. The DataFrame API is getting better, but it is not as flexible as in the other languages, such as Scala or Python.</p><p>Unlike the other chapters in this book, this is a self-contained one to introduce Spark to R programmers. All the use cases that are discussed in this chapter are run in the R REPL of Spark. But in real-world applications, this method is not ideal. The R commands have to be organized in script files and to be submitted to a Spark cluster to run. The easiest way is to use the already existing <code class="literal">$SPARK_HOME/bin/spark-submit &lt;path to the R script file&gt;</code> script, where the fully-qualified R filename is given with respect to the current directory from where the command is being invoked.</p></div>
<div class="section" title="References"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec38"/>References</h1></div></div></div><p>For more information refer to: <a class="ulink" href="https://spark.apache.org/docs/latest/api/R/index.html">https://spark.apache.org/docs/latest/api/R/index.html</a>
</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec39"/>Summary</h1></div></div></div><p>A whirlwind tour of the R language was covered in this chapter, followed by a special mention about the need to have a distinction of understanding the difference between an R DataFrame and a Spark DataFrame. Then, basic Spark programming with R was covered using the same use cases of the previous chapters. R API for Spark was covered, and the use cases have been implemented using the SQL query way and DataFrame API way. This chapter helps data scientists understand the power of Spark and use it in their R applications, using the SparkR package that comes with Spark. This opens up the door of big data processing, using Spark with R to process structured data.</p><p>The subject of Spark-based data processing in various languages has been discussed, and it is time to focus on some data analysis with charting and plotting. Python comes with a lot of charting and plotting libraries that produce publication quality pictures. The next chapter will discuss charting and plotting with the data processed by Spark.</p></div></body></html>