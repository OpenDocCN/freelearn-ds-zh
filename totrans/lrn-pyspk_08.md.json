["```py\n# Ubuntu/Linux 64-bit \n$ sudo apt-get install python-pip python-dev\n\n```", "```py\n# macOS \n$ sudo easy_install pip \n$ sudo easy_install --upgrade six\n\n```", "```py\n# Ubuntu/Linux pip upgrade\n$ pip install --upgrade pip \n\n```", "```py\n$ pip install tensorflow\n\n```", "```py\n$ pip install tensorflow-gpu\n\n```", "```py\n# macOS, GPU enabled, Python 2.7: \n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.0rc1-py2-none-any.whl\n# Python 2 \n$ sudo pip install --upgrade $TF_BINARY_URL\n\n```", "```py\n# Import TensorFlow\nimport tensorflow as tf\n# Setup the matrix\n#   c1: 1x3 matrix\n#   c2: 3x1 matrix\nc1 = tf.constant([[3., 2., 1.]])\nc2 = tf.constant([[-1.], [2.], [1.]])\n```", "```py\n# m3: matrix multiplication (m1 x m3)\nmp = tf.matmul(c1, c2)\n```", "```py\n# Launch the default graph\ns = tf.Session()\n\n# run: Execute the ops in graph\nr = s.run(mp)\nprint(r)\n```", "```py\n# [[ 2.]]\n```", "```py\n# Close the Session when completed\ns.close()\n```", "```py\n# Setup placeholder for your model\n#   t1: placeholder tensor\n#   t2: placeholder tensor\nt1 = tf.placeholder(tf.float32)\nt2 = tf.placeholder(tf.float32)\n\n# t3: matrix multiplication (m1 x m3)\ntp = tf.matmul(t1, t2)\n```", "```py\n# Define input matrices\nm1 = [[3., 2., 1.]]\nm2 = [[-1.], [2.], [1.]]\n\n# Execute the graph within a session\nwith tf.Session() as s:\n     print(s.run([tp], feed_dict={t1:m1, t2:m2}))\n```", "```py\n[array([[ 2.]], dtype=float32)]\n```", "```py\n# setup input matrices\nm1 = [[3., 2., 1., 0.]]\nm2 = [[-5.], [-4.], [-3.], [-2.]]\n# Execute the graph within a session\nwith tf.Session() as s:\n     print(s.run([tp], feed_dict={t1:m1, t2:m2}))\n```", "```py\n[array([[-26.]], dtype=float32)]\n```", "```py\n# The version we're using in this notebook\n$SPARK_HOME/bin/pyspark --packages tjhunter:tensorframes:0.2.2-s_2.10  \n\n# Or use the latest version \n$SPARK_HOME/bin/pyspark --packages databricks:tensorframes:0.2.3-s_2.10\n```", "```py\n    /databricks/python/bin/pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl\n    ```", "```py\n    /databricks/python/bin/pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl\n    ```", "```py\n%sh\n/databricks/python/bin/pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl\n```", "```py\nCollecting tensorflow==0.9.0rc0 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl Downloading https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl (27.6MB) Requirement already satisfied (use --upgrade to upgrade): numpy>=1.8.2 in /databricks/python/lib/python2.7/site-packages (from tensorflow==0.9.0rc0) Requirement already satisfied (use --upgrade to upgrade): six>=1.10.0 in /usr/lib/python2.7/dist-packages (from tensorflow==0.9.0rc0) Collecting protobuf==3.0.0b2 (from tensorflow==0.9.0rc0) Downloading protobuf-3.0.0b2-py2.py3-none-any.whl (326kB) Requirement already satisfied (use --upgrade to upgrade): wheel in /databricks/python/lib/python2.7/site-packages (from tensorflow==0.9.0rc0) Requirement already satisfied (use --upgrade to upgrade): setuptools in /databricks/python/lib/python2.7/site-packages (from protobuf==3.0.0b2->tensorflow==0.9.0rc0) Installing collected packages: protobuf, tensorflow Successfully installed protobuf-3.0.0b2 tensorflow-0.9.0rc0\n```", "```py\n# Import TensorFlow, TensorFrames, and Row\nimport tensorflow as tf\nimport tensorframes as tfs\nfrom pyspark.sql import Row\n\n# Create RDD of floats and convert into DataFrame `df`\nrdd = [Row(x=float(x)) for x in range(10)]\ndf = sqlContext.createDataFrame(rdd)\n```", "```py\ndf.show()\n```", "```py\n# Run TensorFlow program executes:\n#   The 'op' performs the addition (i.e. 'x' + '3')\n#   Place the data back into a DataFrame\nwith tf.Graph().as_default() as g:\n\n#   The placeholder that corresponds to column 'x'.\n#   The shape of the placeholder is automatically\n#   inferred from the DataFrame.\n    x = tfs.block(df, \"x\")\n\n    # The output that adds 3 to x\n    z = tf.add(x, 3, name='z')\n\n    # The resulting `df2` DataFrame\n    df2 = tfs.map_blocks(z, df)\n\n# Note that 'z' is the tensor output from the\n# 'tf.add' operation\nprint z\n\n## Output\nTensor(\"z:0\", shape=(?,), dtype=float64)\n```", "```py\n# Build a DataFrame of vectors\ndata = [Row(y=[float(y), float(-y)]) for y in range(10)]\ndf = sqlContext.createDataFrame(data)\ndf.show()\n```", "```py\n# Print the information gathered by TensorFlow to check the content of the DataFrame\ntfs.print_schema(df)\n\n## Output\nroot \n|-- y: array (nullable = true) double[?,?]\n```", "```py\n# Because the dataframe contains vectors, we need to analyze it\n# first to find the dimensions of the vectors.\ndf2 = tfs.analyze(df)\n\n# The information gathered by TF can be printed \n# to check the content:\ntfs.print_schema(df2)\n\n## Output\nroot \n|-- y: array (nullable = true) double[?,2] \n```", "```py\n# Note: First, let's make a copy of the 'y' column. \n# This is an inexpensive operation in Spark 2.0+\ndf3 = df2.select(df2.y, df2.y.alias(\"z\"))\n\n# Execute the Tensor Graph\nwith tf.Graph().as_default() as g:\n\n  # The placeholders. \n  # Note the special name that end with '_input':\n    y_input = tfs.block(df3, 'y', tf_name=\"y_input\")\n    z_input = tfs.block(df3, 'z', tf_name=\"z_input\")\n\n    # Perform elementwise sum and minimum \n    y = tf.reduce_sum(y_input, [0], name='y')\n    z = tf.reduce_min(z_input, [0], name='z')\n\n# The resulting dataframe\n(data_sum, data_min) = tfs.reduce_blocks([y, z], df3)\n\n# The finalresults are numpy arrays:\nprint \"Elementwise sum: %s and minimum: %s \" % (data_sum, data_min)\n\n## Output\nElementwise sum: [ 45\\. -45.] and minimum: [ 0\\. -9.] \n```"]