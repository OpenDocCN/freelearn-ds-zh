<html><head></head><body>
		<div><h1 id="_idParaDest-141" class="chapter-number"><a id="_idTextAnchor159"/>8</h1>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor160"/>Exploratory Data Analysis with R and Python</h1>
			<p><strong class="bold">Exploratory data analysis</strong> (<strong class="bold">EDA</strong>) is a crucial initial step in the data analysis process for data scientists. It involves the systematic examination and visualization of a dataset to uncover <a id="_idIndexMarker572"/>its underlying patterns, trends, and insights. The primary objectives of EDA are to gain a deeper understanding of the data, identify potential problems or anomalies, and inform subsequent analysis and modeling decisions.</p>
			<p>EDA typically starts with a series of data summarization techniques, such as calculating basic statistics (mean, median, and standard deviation), generating frequency distributions, and examining data types and missing values. These preliminary steps provide an overview of the dataset’s structure and quality.</p>
			<p>Visualization plays a central role in EDA. Data scientists create various charts and graphs, including histograms, box plots, scatter plots, and heat maps, to visualize the distribution and associations within the data. These visualizations help reveal outliers, skewness, correlations, and clusters within the data, aiding in the identification of interesting patterns.</p>
			<p>Exploring categorical variables involves generating bar charts, pie charts, or stacked bar plots to understand the distribution of different categories and their relationships. This is valuable for tasks such as customer segmentation or market analysis.</p>
			<p>EDA also involves assessing the relationships between variables. Data scientists use correlation matrices, scatter plots, and regression analysis to uncover connections and dependencies. Understanding these associations can guide feature selection for modeling and help identify potential multicollinearity issues.</p>
			<p>Data transformation and cleaning are often performed during EDA to address issues such as outliers, missing data, and skewness. Decisions about data imputation, scaling, or encoding categorical variables may be made based on the insights gained during exploration.</p>
			<p>Overall, EDA is a critical phase in the data science workflow, as it sets the stage for subsequent data modeling, hypothesis testing, and decision-making. It empowers data scientists to make informed choices about data preprocessing, feature engineering, and modeling techniques by providing a comprehensive understanding of the dataset’s characteristics and nuances. EDA helps ensure that data-driven insights and decisions are based on a solid foundation of data understanding and exploration.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Exploring data distributions</li>
				<li>Data structure and completeness</li>
				<li>EDA with various packages</li>
			</ul>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor161"/>Technical requirements</h1>
			<p>For this chapter, all scripts and files can be found on GitHub at the following link:<a href="https://github.com/PacktPublishing/Extending-Excel-with-Python-and-R/tree/main/Chapter%208"> https://github.com/PacktPublishing/Extending-Excel-with-Python-and-R/tree/main/Chapter%20</a>8.</p>
			<p>For the R section, we will cover the following libraries:</p>
			<ul>
				<li><code>skimr 2.1.5</code></li>
				<li><code>GGally 2.2.0</code></li>
				<li><code>DataExplorer 0.8.3</code></li>
			</ul>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor162"/>Understanding data with skimr</h1>
			<p>As an R programmer, the <code>skimr</code> package is a useful tool for providing summary statistics <a id="_idIndexMarker573"/>about variables that can come in a variety of forms such as data frames and vectors. The package provides a larger set of statistics in order to give the end user a more robust set of information as compared to the base R <code>summary()</code> function.</p>
			<p>To use the <code>skimr</code> package, it must first be installed from <code>CRAN</code> using the <code>install.packages("skimr")</code> command. Once installed, the package can be loaded using the <code>library(skimr)</code> command. The <code>skim()</code> function is then used to summarize a whole dataset. For example, <code>skim(iris)</code> would provide summary statistics for the <code>iris</code> dataset. The output of <code>skim()</code> is printed horizontally, with one section per variable type and one row per variable.</p>
			<p>The package <a id="_idIndexMarker574"/>also provides the <code>skim_to_wide()</code> function, which converts the output of <code>skim()</code> to a wide format. This can be useful for exporting the summary statistics to a spreadsheet or other external tool.</p>
			<p>Overall, the <code>skimr</code> package is a useful tool for quickly and easily obtaining summary statistics about variables in R. It provides a larger set of statistics than the <code>summary()</code> R base function and is easy to use and customize. The package is particularly useful for data exploration and data cleaning tasks, as it allows the user to quickly identify potential issues with the data. Now that we have a basic understanding of the <code>skimr</code> package, let’s see it in use.</p>
			<p>This R code is used to generate a summary of the <code>iris</code> dataset using the <code>skimr</code> package. The <code>skimr</code> package provides a convenient way to quickly summarize and visualize key statistics for a dataset.</p>
			<p>Here’s an explanation of each line of code along with the expected output:</p>
			<ul>
				<li><code>if(!require(skimr)){install.packages("skimr")}</code>: This line checks whether the <code>skimr</code> package is already installed. If it is not installed, it installs the package using <code>install.packages("skimr")</code>. This ensures that the <code>skimr</code> package is available for use in the subsequent code.</li>
				<li><code>library(skimr)</code>: This line loads the <code>skimr</code> package into the R session. Once the package is loaded, you can use its functions and features.</li>
				<li><code>skim(iris)</code>: This line calls the <code>skim()</code> function from the <code>skimr</code> package and applies it to the <code>iris</code> dataset. The <code>skim()</code> function generates a summary of the dataset, including statistics and information about each variable (column) in the dataset.</li>
			</ul>
			<p>Now, let’s discuss the expected output. When you run the <code>skim(iris)</code> command, you should see a summary of the <code>iris</code> dataset displayed in your R console. The output will include statistics and information such as the following:</p>
			<ul>
				<li><strong class="bold">Counts</strong>: The number of non-missing values for each variable</li>
				<li><strong class="bold">Missing</strong>: The number of missing values (if any) for each variable</li>
				<li><strong class="bold">Unique values</strong>: The number of unique values for each variable</li>
				<li><strong class="bold">Mean</strong>: The mean (average) value for each numeric variable</li>
				<li><strong class="bold">Min</strong>: The minimum value for each numeric variable</li>
				<li><strong class="bold">Max</strong>: The maximum value for each numeric variable</li>
				<li><strong class="bold">Standard deviation</strong>: The standard deviation for each numeric variable</li>
				<li>Other summary statistics</li>
			</ul>
			<p>The output <a id="_idIndexMarker575"/>will look something like the following but with more detailed statistics:</p>
			<pre class="console">
&gt; skim(iris)
── Data Summary ────────────────────────
                           Values
Name                       iris
Number of rows             150
Number of columns          5
_______________________
Column type frequency:
  factor                   1
  numeric                  4
________________________
Group variables            None
── Variable type: factor ─────────────────────────────────────────────────────────────────────────
  skim_variable n_missing complete_rate ordered n_unique top_counts
1 Species               0             1 FALSE          3 set: 50, ver: 50, vir: 50
── Variable type: numeric ────────────────────────────────────────────────────────────────────────
  skim_variable n_missing complete_rate mean    sd  p0 p25  p50 p75 p100 hist
1 Sepal.Length          0             1 5.84  0.828  4.3  5.1  5.8  6.4  7.9 ▆▇▇▅▂
2 Sepal.Width           0             1 3.06  0.436  2   2.8  3    3.3  4.4 ▁▆▇▂▁
3 Petal.Length          0             1 3.76  1.77  1   1.6  4.35  5.1  6.9  ▇▁▆▇▂
4 Petal.Width           0             1 1.20  0.762 0.1 0.3  1.3  1.8  2.5  ▇▁▇▅▃</pre>			<p>This output <a id="_idIndexMarker576"/>provides a comprehensive summary of the <code>iris</code> dataset, helping you quickly understand its structure and key statistics. You can also pass off a grouped <code>tibble</code> to the <code>skim()</code> function and obtain results that way as well.</p>
			<p>Now that we have gone over a simple example of using the <code>skimr</code> package to explore our data, we can now move on to the <code>GGally</code> package.</p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor163"/>Using the GGally package in R</h1>
			<p>At its core, <code>GGally</code> is an extension of the immensely popular <code>ggplot2</code> package in R. It takes the <a id="_idIndexMarker577"/>elegance and flexibility of <code>ggplot2</code> and supercharges it with a dazzling array of functions, unleashing your creativity <a id="_idIndexMarker578"/>to visualize data in stunning ways.</p>
			<p>With <code>GGally</code>, you can effortlessly create beautiful scatter plots, histograms, bar plots, and more. What makes it stand out? <code>GGally</code> simplifies the process of creating complex multivariate plots, saving you time and effort. Want to explore correlations, visualize regression models, or craft splendid survival curves? <code>GGally</code> has your back.</p>
			<p>But <code>GGally</code> is not just about aesthetics; it’s about insights. It empowers you to uncover hidden relationships within your data through visual exploration. The intuitive syntax and user-friendly interface make it accessible to both novices and seasoned data scientists.</p>
			<p>What’s even better is that <code>GGally</code> encourages collaboration. Its easy-to-share visualizations can be a powerful tool for communicating your findings to a wider audience, from colleagues to clients.</p>
			<p>So, if you’re looking to elevate your data visualization game, give <code>GGally</code> a try. It’s your trusted ally in the world of data visualization, helping you turn numbers into captivating stories. Unlock the true potential of your data and let <code>GGally</code> be your creative partner in crime. Your data has never looked this good! Now, let’s get into using it with what is a simple use case:</p>
			<pre class="source-code">
if(!require(GGally)){install.packages("GGally")}
If(!require(TidyDensity)){install.packages("TidyDensity")}
library(GGally)
library(TidyDensity)
tidy_normal(.n = 200) |&gt;
  ggpairs(columns = c("y","p","q","dx","dy"))</pre>			<p>Let’s break it down step by step:</p>
			<ul>
				<li><code>if(!require(GGally)){install.packages("GGally")}</code>: This line checks whether the <code>GGally</code> package is already installed in your R environment. If it’s not installed, it proceeds to install it using <code>install.packages("GGally")</code>.</li>
				<li><code>library(GGally)</code>: After ensuring that <code>Ggally</code> is installed, the code loads the <code>GGally</code> package into the current R session. This package provides tools for creating <a id="_idIndexMarker579"/>various types of plots and <a id="_idIndexMarker580"/>visualizations, including scatter plot matrices.</li>
				<li><code>library(TidyDensity)</code>: Similarly, this line loads the <code>TidyDensity</code> package, which is used for creating tidy density plots. Tidy density plots are a way to visualize the distribution of data in a neat and organized manner.</li>
				<li><code>tidy_normal(.n = 200)</code>: Here, the code generates a dataset with 200 random data points. These data points are assumed to follow a normal distribution (a bell-shaped curve). The <code>tidy_normal</code> function is used to create this dataset.</li>
				<li><code>ggpairs(columns = c("y","p","q","dx","dy"))</code>: This is where the magic happens. The <code>ggpairs</code> function from the <code>GGally</code> package is called with the dataset generated earlier. It creates a scatter plot matrix where each combination of variables is plotted against each other. The <code>y</code>, <code>p</code>, <code>q</code>, <code>dx</code>, and <code>dy</code> variables are the columns of the dataset to be used for creating the scatter plots.</li>
			</ul>
			<p>In summary, this code first installs and loads the necessary R packages (<code>GGally</code> and <code>TidyDensity</code>). Then, it generates a dataset of 200 random points following a normal distribution and creates a scatter plot matrix using the <code>ggpairs</code> function. The scatter plot matrix visualizes the relationships between the specified columns of the dataset, allowing you to explore the data’s patterns and correlations. Let’s take a look at the resulting plot:</p>
			<div><div><img src="img/B19142_08_1.jpg" alt="Figure 8.1 – Using GGally on 200 points generated from tidy_normal()"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Using GGally on 200 points generated from tidy_normal()</p>
			<p>The data <a id="_idIndexMarker581"/>here was randomly generated so you will <a id="_idIndexMarker582"/>most likely not get the exact same results. Now that we have covered the <code>GGally</code> package with a simple example, we can move on to the <code>DataExplorer</code> package and see how it compares.</p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor164"/>Using the DataExplorer package</h1>
			<p>The <code>DataExplorer</code> R package is created to streamline the majority of data management <a id="_idIndexMarker583"/>and visualization responsibilities during the EDA process. EDA is a critical and primary stage in data analysis, during which analysts take their initial glimpse at the data to formulate meaningful hypotheses and determine subsequent action.</p>
			<p><code>DataExplorer</code> provides a variety of functions to do the following:</p>
			<ul>
				<li><strong class="bold">Scan and analyze data variables</strong>: The package can automatically scan and analyze each variable in a dataset, identifying its type, data distribution, outliers, and missing values.</li>
				<li><code>DataExplorer</code> provides a variety of visualization functions to help analysts understand the relationships between variables and identify patterns in the data. These functions include histograms, scatter plots, box plots, heat maps, and correlation matrices.</li>
				<li><code>DataExplorer</code> also provides functions to transform data, such as converting categorical variables to numerical variables, imputing missing values, and scaling numerical variables.</li>
			</ul>
			<p><code>DataExplorer</code> <a id="_idIndexMarker584"/>can be used for a variety of EDA tasks, such as the following:</p>
			<ul>
				<li><code>DataExplorer</code> can be used to identify the different types of variables in a dataset, their distributions, and their relationships with each other</li>
				<li><code>DataExplorer</code> can help analysts identify outliers and missing values in their data, which can be important to address before building predictive models</li>
				<li><code>DataExplorer</code> can help analysts generate hypotheses about the data by identifying patterns and relationships between variables</li>
			</ul>
			<p>Here are some examples of how to use the package:</p>
			<pre class="source-code">
install.packages("DataExplorer")
library(DataExplorer)
library(TidyDensity)
library(dplyr)
df &lt;- tidy_normal(.n = 200)
df |&gt;
  introduce() |&gt;
  glimpse()</pre>			<p>First, we check to see whether the <code>DataExplorer</code> package is installed. If it’s not, we install it. Then, we load the <code>DataExplorer</code> package, as well as the <code>TidyDensity</code> and <code>dplyr</code> packages.</p>
			<p>Next, we create a normally distributed dataset with 200 observations. We use the <code>tidy_normal()</code> function for this because it’s a convenient way to create normally distributed datasets in R. Typically, one will most likely use the <em class="italic">y</em> column only from the <code>tidy_normal()</code> output.</p>
			<p>Once we <a id="_idIndexMarker585"/>have our dataset, we use the <code>introduce()</code> function from the <code>DataExplorer</code> package to generate a summary of the data. This summary includes information about the number of observations and the types of variables.</p>
			<p>Finally, we use the <code>glimpse()</code> function from the <code>dplyr</code> package to display the data transposed. This is a helpful way to get a quick overview of the data and to make sure that it looks like we expect it to.</p>
			<p>In other words, this code is a quick and easy way to explore a normally distributed dataset in R. It’s great for students and beginners, as well as for experienced data scientists who need to get up and running quickly. Now, let’s see the output:</p>
			<pre class="source-code">
&gt; df |&gt;
+   introduce() |&gt;
+   glimpse()
Rows: 1
Columns: 9
$ rows                 &lt;int&gt; 200
$ columns              &lt;int&gt; 7
$ discrete_columns     &lt;int&gt; 1
$ continuous_columns   &lt;int&gt; 6
$ all_missing_columns  &lt;int&gt; 0
$ total_missing_values &lt;int&gt; 0
$ complete_rows        &lt;int&gt; 200
$ total_observations   &lt;int&gt; 1400
$ memory_usage         &lt;dbl&gt; 12344</pre>			<p>Next, let’s take <a id="_idIndexMarker586"/>a look at the <code>plot_intro()</code> function and see its output on the same data with a simple call of <code>df |&gt; </code><code>plot_intro()</code>:</p>
			<div><div><img src="img/B19142_08_2.jpg" alt="Figure 8.2 – The plot_intro() function﻿"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – The plot_intro() function</p>
			<p>Lastly, we will view the output of the <code>plot_qq()</code> function:</p>
			<div><div><img src="img/B19142_08_3.jpg" alt="Figure 8.3 – The plot_qq() function with all data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – The plot_qq() function with all data</p>
			<p>We will <a id="_idIndexMarker587"/>now see a <strong class="bold">quantile-quantile</strong> (<strong class="bold">Q-Q</strong>) plot with only <a id="_idIndexMarker588"/>two variables, both the <em class="italic">q</em> and the <em class="italic">y</em> columns. Here is the code:</p>
			<pre class="source-code">
df[c("q","y")] |&gt;
  plot_qq()</pre>			<p>Here is the plot for it:</p>
			<div><div><img src="img/B19142_08_4.jpg" alt="Figure 8.4 –  The plot_qq() function with only the y and q columns"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 –  The plot_qq() function with only the y and q columns</p>
			<p>It is our intention for you to understand the Q-Q plot, but if that is not the case, a simple search on <a id="_idIndexMarker589"/>Google will yield many good results. Another item that was never discussed was how to handle missing data in R. There are a great many functions that can be used in capturing, cleaning, and otherwise understanding them such as <code>all()</code>, <code>any()</code>, <code>is.na()</code>, and <code>na.omit()</code>. Here, I advise you to explore these on the internet, all of which have been discussed extensively. Now that we have gone over several different examples of some functions from different packages in R, it’s time to explore the same for Python.</p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor165"/>Getting started with EDA for Python</h1>
			<p>As explained earlier, EDA is the process of visually and statistically exploring datasets to uncover <a id="_idIndexMarker590"/>patterns, relationships, and insights. It’s a critical step before diving into more complex data analysis tasks. In this section, we’ll <a id="_idIndexMarker591"/>introduce you to the fundamentals of EDA and show you how to prepare your Python environment for EDA.</p>
			<p>EDA is the initial phase of data analysis where you examine and summarize your dataset. The primary objectives of EDA are as follows:</p>
			<ul>
				<li><strong class="bold">Understand the data</strong>: Gain insights into the structure, content, and quality of your data</li>
				<li><strong class="bold">Identify patterns</strong>: Discover patterns, trends, and relationships within the data</li>
				<li><strong class="bold">Detect anomalies</strong>: Find outliers and anomalies that may require special attention</li>
				<li><strong class="bold">Generate hypotheses</strong>: Formulate initial hypotheses about your data</li>
				<li><strong class="bold">Prepare for modeling</strong>: Preprocess data for advanced modeling and analysis</li>
			</ul>
			<p>Before <a id="_idIndexMarker592"/>you can perform EDA, you’ll need to set <a id="_idIndexMarker593"/>up your Python environment to work with Excel data. We have covered the first steps in previous chapters: namely, installing necessary libraries and loading data from Excel.</p>
			<p>Next, we will cover the most important basics of EDA: data cleaning and data exploration.</p>
			<p>Data cleaning is an essential step in preparing your Excel data for EDA in Python. It involves identifying and rectifying various data quality issues that can affect the accuracy and reliability of your analysis.</p>
			<p>Let’s start with the intricacies of data cleaning, without which you cannot be confident in the results of your EDA. We will focus on both generic data cleaning challenges and those unique to data coming from Excel.</p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor166"/>Data cleaning in Python for Excel data</h1>
			<p>Data cleaning <a id="_idIndexMarker594"/>is a critical process when working <a id="_idIndexMarker595"/>with Excel data in Python. It ensures that your data is in the right format and free of errors, enabling you to perform accurate EDA.</p>
			<p>We will start with generating some dirty data as an example:</p>
			<pre class="source-code">
import pandas as pd
import numpy as np
# Create a DataFrame with missing data, duplicates, and mixed data types
data = {
    'ID': [1, 2, 3, 4, 5, 6],
    'Name': ['Alice', 'Bob', 'Charlie', 'Alice', 'Eva', 'Eva'],
    'Age': [25, np.nan, 30, 28, 22, 23],
    'Salary': ['$50,000', '$60,000', 'Missing', '$65,000', '$55,000',
    '$75,000']
}
df = pd.DataFrame(data)
# Introduce some missing data
df.loc[1, 'Age'] = np.nan
df.loc[3, 'Salary'] = np.nan
# Introduce duplicates
df = pd.concat([df, df.iloc[1:3]], ignore_index=True)
# Save the sample data in present working directory
df.to_excel('dirty_data.xlsx')</pre>			<p>The <a id="_idIndexMarker596"/>data <a id="_idIndexMarker597"/>frame created looks like this:</p>
			<div><div><img src="img/B19142_08_5.jpg" alt="Figure 8.5 – Using GGally on 200 points generated from tidy_normal()"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Using GGally on 200 points generated from tidy_normal()</p>
			<p>With the <a id="_idIndexMarker598"/>dirty data ready, we can start cleaning it, which <a id="_idIndexMarker599"/>includes handling missing data, duplicates, and data type conversion when cleaning Excel data. Let’s see how to do this.</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor167"/>Handling missing data</h2>
			<p>Begin by <a id="_idIndexMarker600"/>identifying cells or columns with missing data. In Python, missing <a id="_idIndexMarker601"/>values are typically represented as <strong class="bold">NaN</strong> (short for <strong class="bold">Not a Number</strong>) or <strong class="bold">None</strong>.</p>
			<p>Depending <a id="_idIndexMarker602"/>on the context, you can choose to replace missing values. Common techniques include the following:</p>
			<ul>
				<li>Filling missing numeric values with the mean, median, or mode. Do note that this will artificially reduce standard error measurements if, for example, a regression is performed. Keep this in mind when cleaning data for modeling purposes!</li>
				<li>Replacing missing categorical data with the mode (most frequent category).</li>
				<li>Using forward-fill or backward-fill to propagate the previous or next valid value.</li>
				<li>Interpolating missing values based on trends in the data.</li>
			</ul>
			<p>Python offers several solutions to do this efficiently and statistically robustly, ranging from basic <code>pandas</code> methods to dedicated packages with much more robust imputation methods such as <code>fancyimpute</code>. Imputation should never be applied blindly, though, as missing data can be information in its own right as well, and imputed values may lead to incorrect <a id="_idIndexMarker603"/>analysis results. Domain knowledge, as always, for the win.</p>
			<p>It is important <a id="_idIndexMarker604"/>to distinguish between the three types of missing data:</p>
			<ul>
				<li><strong class="bold">Missing completely at </strong><strong class="bold">random</strong> (<strong class="bold">MCAR</strong>):<ul><li>In this <a id="_idIndexMarker605"/>scenario, the missingness <a id="_idIndexMarker606"/>of data is completely random and unrelated to any observed or unobserved variables.</li><li>The probability of a data point being missing is the same for all observations.</li><li>There are no systematic differences between missing and non-missing values.</li><li>Here is an example: a survey where respondents accidentally skip some questions.</li></ul></li>
				<li><strong class="bold">Missing at </strong><strong class="bold">random</strong> (<strong class="bold">MAR</strong>):<ul><li>Missingness <a id="_idIndexMarker607"/>depends on observed data but not on unobserved data.</li><li>The <a id="_idIndexMarker608"/>probability of a data point being missing is related to other observed variables in the dataset.</li><li>Once other observed variables are accounted for, the missingness of data is random.</li><li>Here is an example: in a survey, men may be less likely to disclose their income compared to women. In this case, income data is missing at random conditional on gender.</li></ul></li>
				<li><strong class="bold">Missing not at </strong><strong class="bold">random</strong> (<strong class="bold">MNAR</strong>):<ul><li>Missingness depends on unobserved data or the missing values themselves.</li><li>The <a id="_idIndexMarker609"/>probability of a data point <a id="_idIndexMarker610"/>being missing is related to the missing values or other unobserved variables.</li><li>Missingness is not random even after accounting for observed variables.</li><li>Here is an example: in a survey about income, high-income earners may be less likely to disclose their income.</li></ul></li>
			</ul>
			<p>If a significant <a id="_idIndexMarker611"/>portion of a row or column contains missing data, you might <a id="_idIndexMarker612"/>consider removing that row or column entirely. Be cautious when doing this, as it should not result in a substantial loss of information:</p>
			<pre class="source-code">
import pandas as pd
import numpy as np
# Load Excel data into a pandas DataFrame
df = pd.read_excel('dirty_data.xlsx', index_col=0)
# Handling Missing Data
# Identify missing values
missing_values = df.isnull().sum()
# Replace missing values with the mean (for numeric columns)
df['Age'].fillna(df['Age'].mean(), inplace=True)
# Replace missing values with the mode (for categorical columns)
df['Salary'].fillna(df['Salary'].mode()[0], inplace=True)
# Forward-fill or backward-fill missing values
# This line is a placeholder to show you what's possible
# df['ColumnWithMissingValues'].fillna(method='ffill', inplace=True)
# Interpolate missing values based on trends
# This line is a placeholder to show you what's possible
# df['NumericColumn'].interpolate(method='linear', inplace=True)
# Remove rows or columns with missing data
df.dropna(axis=0, inplace=True)  # Remove rows with missing data
df.dropna(axis=1, inplace=True)  # Remove columns with missing data
df.to_excel('cleaned_data.xlsx')</pre>			<h2 id="_idParaDest-150"><a id="_idTextAnchor168"/>Dealing with duplicates</h2>
			<p>Start with detecting duplicate rows. Python libraries such as <code>pandas</code> provide functions to detect <a id="_idIndexMarker613"/>and handle duplicate rows. To identify duplicates, you can use the <code>duplicated()</code> method. If applicable, continue with removing <a id="_idIndexMarker614"/>duplicate rows: after detecting duplicates, you can choose to remove them using the <code>drop_duplicates()</code> method. Be cautious when removing duplicates, especially in cases where duplicates are expected:</p>
			<pre class="source-code">
# Handling Duplicates
# Detect and display duplicate rows
duplicate_rows = df[df.duplicated()]
print("Duplicate Rows:")
print(duplicate_rows)
# Remove duplicate rows
df.drop_duplicates(inplace=True)</pre>			<h2 id="_idParaDest-151"><a id="_idTextAnchor169"/>Handling data type conversion</h2>
			<p>Reading data from Excel, while highly automated, can lead to incorrectly identified data types. In Python, data types are often automatically assigned when you read Excel data using libraries <a id="_idIndexMarker615"/>such as <code>pandas</code>. However, it’s essential to verify that each column is assigned the correct data type (e.g., numeric, text, date, etc.).</p>
			<p>The effect <a id="_idIndexMarker616"/>of that ranges from a too-large memory footprint to actual semantic errors. If, for example, Boolean values are stored as floats, they will be handled correctly but will take up a lot more memory (instead of a single bit with 64 bytes), while trying to convert a string into a float may well break your code. To mitigate this risk, first identify data types in the loaded data. Then, convert data types to the appropriate types. To convert data types in Python, you can use the <code>astype()</code> method in <code>pandas</code>. For example, to convert a column to a numeric data type, you can use <code>df['Column Name'] = </code><code>df['Column Name'].astype(float)</code>.</p>
			<p>Here is an example of how that can be done:</p>
			<pre class="source-code">
# Handling Data Type Conversion
# Check data types
print(df.dtypes)
# Convert a column to a different data type (e.g., float)
df.loc[df['Salary']=='Missing', 'Salary'] = np.NaN
df.loc[:, 'Salary'] = df['Salary'].str.replace("$", "")
df.loc[:, 'Salary'] = df['Salary'].str.replace(",", "")
df['Salary'] = df['Salary'].astype(float)
print(df)
# Now that Salary is a numeric column, we can fill the missing values with mean
df['Salary'].fillna(df['Salary'].mean(), inplace=True)</pre>			<h2 id="_idParaDest-152"><a id="_idTextAnchor170"/>Excel-specific data issues</h2>
			<p>Let’s <a id="_idIndexMarker617"/>have a look at data issues that are <a id="_idIndexMarker618"/>specific to data loaded from Excel:</p>
			<ul>
				<li><strong class="bold">Merged cells</strong>: Merged cells in Excel files can cause irregularities in your dataset when imported into Python. It’s advisable to unmerge cells in Excel before importing. If unmerging isn’t feasible, consider preprocessing these cells within Python.</li>
				<li><code>pandas</code> <code>na_values</code> parameter when reading Excel files to specify values that should be treated as missing (NaN) during import</li><li>Manually adjusting the Excel file to remove unnecessary empty cells before import</li></ul></li>
			</ul>
			<p>By addressing these common data cleaning issues within Python for Excel data, you’ll ensure that your data is in a clean and usable format for your EDA. Clean data leads to more accurate and meaningful insights during the exploration phase, which we will cover next.</p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor171"/>Performing EDA in Python</h1>
			<p>With your data loaded and cleaned, you can embark on your initial data exploration journey. This phase <a id="_idIndexMarker619"/>is crucial for gaining a deep understanding of your dataset, revealing its underlying patterns, and identifying potential areas of interest or concern.</p>
			<p>These preliminary steps not only provide a solid foundation for your EDA but also help you uncover hidden patterns and relationships within your data. Armed with this initial understanding, you can proceed to more advanced data exploration techniques and dive deeper into the Excel dataset.</p>
			<p>In the subsequent sections, we’ll delve into specific data exploration and visualization techniques to <a id="_idIndexMarker620"/>further enhance your insights into the dataset. With this knowledge, let’s move on to the next section, where we’ll explore techniques for understanding data distributions and relationships in greater detail.</p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor172"/>Summary statistics</h2>
			<p>Begin by generating summary statistics for your dataset. This includes basic metrics such as mean, median, standard deviation, and percentiles for numerical features. For categorical <a id="_idIndexMarker621"/>features, you can calculate frequency counts and percentages. These statistics provide an initial overview of the central tendency and spread of your data.</p>
			<p>Summary statistics provide a concise and informative overview of your data’s central tendency, spread, and distribution. This step is essential for understanding the basic characteristics of your dataset, whether it contains numerical or categorical features. The following sections provide a closer look at generating summary statistics.</p>
			<h3>Numerical features</h3>
			<p>For numerical <a id="_idIndexMarker622"/>features, the following <a id="_idIndexMarker623"/>statistics are a good starting point:</p>
			<ul>
				<li><strong class="bold">Mean</strong>: The mean (average) is a measure of central tendency that represents the sum of all <a id="_idIndexMarker624"/>numerical values divided by the total number of data points. It provides insight into the data’s typical value.</li>
				<li><strong class="bold">Median</strong>: The median <a id="_idIndexMarker625"/>is the middle value when all data points are sorted in ascending or descending order. It’s a robust measure of central tendency, less affected by extreme values (outliers).</li>
				<li><strong class="bold">Standard deviation</strong>: The <a id="_idIndexMarker626"/>standard deviation quantifies the degree of variation or dispersion in your data. A higher standard deviation indicates a greater data spread.</li>
				<li><strong class="bold">Percentiles</strong>: Percentiles represent specific data values below which a given percentage <a id="_idIndexMarker627"/>of observations fall. For example, the 25th percentile (Q1) marks the value below which 25% of the data points lie. Percentiles help identify data distribution characteristics.</li>
			</ul>
			<h3>Categorical features</h3>
			<p>Categorical <a id="_idIndexMarker628"/>features have their own set of summary statistics <a id="_idIndexMarker629"/>that will give you insights:</p>
			<ul>
				<li><strong class="bold">Frequency</strong>: For <a id="_idIndexMarker630"/>categorical features, calculate the frequency count for each unique category. This shows how often each category appears in the dataset.</li>
				<li><strong class="bold">Percentages</strong>: Expressing <a id="_idIndexMarker631"/>frequency counts as percentages relative to the total number of observations provides insights into the relative prevalence of each category.</li>
			</ul>
			<p>Generating summary statistics serves several purposes:</p>
			<ul>
				<li><strong class="bold">Understanding your data</strong>: Summary statistics help you understand the typical values and spread of your numerical data. For categorical data, it shows the distribution of categories.</li>
				<li><strong class="bold">Identifying outliers</strong>: Outliers, which are data points significantly different from the norm, can often be detected through summary statistics. Unusually high or low mean values may indicate outliers.</li>
				<li><strong class="bold">Data quality assessment</strong>: Summary statistics can reveal missing values, which may appear as NaN in your dataset. Identifying missing data is crucial for data cleaning.</li>
				<li><strong class="bold">Initial insights</strong>: These statistics offer preliminary insights into your data’s structure, which can guide subsequent analysis and visualization choices.</li>
			</ul>
			<p>In Python, libraries such as <code>pandas</code> make it straightforward to calculate summary statistics. For numerical data, you can use functions such as <code>.mean()</code>, <code>.median()</code>, <code>.std()</code>, and <code>.describe()</code>. For categorical data, <code>.value_counts()</code> and custom functions can compute frequency counts and percentages.</p>
			<p>Here’s an <a id="_idIndexMarker632"/>example of how you can generate summary statistics <a id="_idIndexMarker633"/>for both numerical and categorical data using Python and <code>pandas</code>. We’ll create sample data for demonstration purposes:</p>
			<pre class="source-code">
import pandas as pd
import random
# Create a sample DataFrame
data = {
    'Age': [random.randint(18, 60) for _ in range(100)],
    'Gender': ['Male', 'Female'] * 50,
    'Income': [random.randint(20000, 100000) for _ in range(100)],
    'Region': ['North', 'South', 'East', 'West'] * 25
}
df = pd.DataFrame(data)
# Calculate summary statistics for numerical features
numerical_summary = df.describe()
# Calculate frequency counts and percentages for categorical features
categorical_summary = df['Gender'].value_counts(normalize=True)
print("Summary Statistics for Numerical Features:")
print(numerical_summary)
print("\nFrequency Counts and Percentages for Categorical Features (Gender):")
print(categorical_summary)</pre>			<p>In this <a id="_idIndexMarker634"/>example, we created a sample <code>DataFrame</code>, <code>df</code>, with columns for <code>Age</code>, <code>Gender</code>, <code>Income</code>, and <code>Region</code>. We used <code>df.describe()</code> to calculate summary statistics (mean, standard deviation, min, max, quartiles, and so on) for numerical features (<code>Age</code> and <code>Income</code>). We then used <code>df['Gender'].value_counts(normalize=True)</code> to calculate frequency counts <a id="_idIndexMarker635"/>and percentages for the <code>Gender</code> categorical feature. The <code>normalize=True</code> parameter expresses the counts as percentages. If it is set to <code>False</code>, the <code>value_counts()</code> command returns a series containing the raw counts of each unique value in the <code>Gender</code> column.</p>
			<p>You can adapt this code to your Excel dataset by loading your data into a <code>pandas</code> <code>DataFrame</code> and then applying these summary statistics functions.</p>
			<p>Once you’ve generated summary statistics, you’ll have a solid foundation for your data exploration journey. These statistics provide a starting point for further analysis and visualization, helping you uncover valuable insights within your Excel dataset.</p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor173"/>Data distribution</h2>
			<p>Understanding <a id="_idIndexMarker636"/>the distribution of your data is fundamental. Is it normally distributed or skewed, or does it follow another distribution? Identifying the data’s distribution informs subsequent statistical analysis and model selection.</p>
			<p>Before delving <a id="_idIndexMarker637"/>deeper into your data, it’s essential to understand its distribution. The distribution of data refers to the way data values are spread or arranged. Knowing the data distribution helps you make better decisions regarding statistical analysis and modeling. Here are some key concepts related to data distribution:</p>
			<ul>
				<li><strong class="bold">Normal distribution</strong>: In a normal distribution, data is symmetrically distributed <a id="_idIndexMarker638"/>around the mean, forming a bell-shaped curve. Many statistical techniques are simple to apply when the data follows a normal distribution. You can check for normality using visualizations such as histograms and Q-Q plots or statistical tests such as the Shapiro-Wilk test.</li>
				<li><strong class="bold">Skewness</strong>: Skewness measures the asymmetry of the data distribution. A positive <a id="_idIndexMarker639"/>skew indicates that the tail of the distribution extends to the right, while a negative skew means it extends to the left. Identifying skewness is important because it can affect the validity of some statistical tests.</li>
				<li><strong class="bold">Kurtosis</strong>: Kurtosis <a id="_idIndexMarker640"/>measures the “heavy-tailedness” or “peakedness” of a distribution. A high kurtosis value indicates heavy tails, while a low value suggests light tails. Understanding kurtosis helps in selecting appropriate statistical models.</li>
				<li><strong class="bold">Other distributions</strong>: Data can follow various distributions, such as exponential, log-normal, Poisson, or uniform. Counts often follow Poisson distribution, rainfall amounts are log-normal distributed, and these distributions are everywhere around us! Identifying the correct distribution is essential when choosing statistical models and making predictions.</li>
			</ul>
			<p>To analyze data distributions using Python, you can create visualizations such as histograms, kernel density plots, and box plots. Additionally, statistical tests and libraries such as SciPy can help you identify and quantify departures from normality.</p>
			<p>Let’s take a look at a sample code snippet that generates sample data from a <code>lognormal</code> distribution, performs the Shapiro-Wilk test, creates Q-Q plots with both a <code>Normal</code> and <code>lognormal</code> distribution, and calculates skewness and kurtosis statistics!</p>
			<p>First, let’s generate some sample data:</p>
			<pre class="source-code">
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import statsmodels.api as sm
# Generate sample data from a lognormal distribution
np.random.seed(0)
data = np.random.lognormal(mean=0, sigma=1, size=1000)
# Create a Pandas DataFrame
df = pd.DataFrame({'Data': data})
# Next, we can plot a histogram of the data to get visual insights into the distribution:
# Plot a histogram of the data
plt.hist(data, bins=30, color='skyblue', edgecolor='black')
plt.title('Histogram of Data')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.show()</pre>			<p>The resulting <a id="_idIndexMarker641"/>histogram shows a very clear skew, making a <code>Normal</code> distribution <a id="_idIndexMarker642"/>unlikely and a <code>lognormal</code> distribution the likely best choice (unsurprisingly, given how the sample data was generated):</p>
			<div><div><img src="img/B19142_08_6.jpg" alt="Figure 8.6 – Histogram of the data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Histogram of the data</p>
			<p>Next, we can <a id="_idIndexMarker643"/>perform some basic statistical analysis to <a id="_idIndexMarker644"/>confirm our suspicions from the histogram – starting with a Shapiro-Wilk test for normality, then plotting a Q-Q plot for the <code>Normal</code> distribution, and finally, a Q-Q plot against the suspected distribution of <code>lognormal</code>:</p>
			<pre class="source-code">
# Perform the Shapiro-Wilk test for normality
shapiro_stat, shapiro_p = stats.shapiro(data)
is_normal = shapiro_p &gt; 0.05  # Check if data is normally distributed
print(f'Shapiro-Wilk p-value: {shapiro_p}')
print(f'Is data normally distributed? {is_normal}')
# Create Q-Q plot with a Normal distribution
sm.qqplot(data, line='s', color='skyblue')
plt.title('Q-Q Plot (Normal)')
plt.xlabel('Theoretical Quantiles')
plt.ylabel('Sample Quantiles')
plt.show()
# Create Q-Q plot with a lognormal distribution
log_data = np.log(data)
sm.qqplot(log_data, line='s', color='skyblue')
plt.title('Q-Q Plot (Lognormal)')
plt.xlabel('Theoretical Quantiles')
plt.ylabel('Sample Quantiles')
plt.show()</pre>			<p>The first <a id="_idIndexMarker645"/>Q-Q plot shows that the <code>Normal</code> distribution is a very poor fit <a id="_idIndexMarker646"/>for the data:</p>
			<div><div><img src="img/B19142_08_7.jpg" alt="Figure 8.7 – Q-Q plot for the normal distribution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Q-Q plot for the normal distribution</p>
			<p>The next <a id="_idIndexMarker647"/>Q-Q plot, this time for the <code>lognormal</code> distribution, shows <a id="_idIndexMarker648"/>a near-perfect fit, on the other hand:</p>
			<div><div><img src="img/B19142_08_8.jpg" alt="Figure 8.8 –  Q-Q plot for the lognormal distribution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 –  Q-Q plot for the lognormal distribution</p>
			<p>Finally, we calculate <a id="_idIndexMarker649"/>skewness and kurtosis to be sure we are on the right <a id="_idIndexMarker650"/>track and print all the results:</p>
			<pre class="source-code">
# Calculate skewness and kurtosis
skewness = stats.skew(data)
kurtosis = stats.kurtosis(data)
print(f'Skewness: {skewness}')
print(f'Kurtosis: {kurtosis}')</pre>			<p>From the histogram, the Shapiro-Wilk test rejecting the hypothesis of a <code>normal</code> distribution, the two Q-Q plots, and the skewness and kurtosis estimators, we can conclude beyond reasonable doubt that the data was indeed generated from a <code>lognormal</code> distribution!</p>
			<p>It is now time to move on from single-variable analyses to the relationship between multiple variables.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor174"/>Associations between variables</h2>
			<p>Explore relationships between variables through scatter plots and correlation matrices. Identify any <a id="_idIndexMarker651"/>strong correlations or dependencies between features. This step <a id="_idIndexMarker652"/>can guide feature selection or engineering in later stages of analysis. In this section, we’ll delve into methods to investigate these relationships using Python.</p>
			<p>We will use the following libraries and sample dataset:</p>
			<pre class="source-code">
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import ppscore as pps
# Generate test data with three variables
np.random.seed(0)
data = {
    'Feature1': np.random.randn(100),
    'Feature2': np.random.randn(100) * 2,
}
# Create a linear Target variable based on Feature1 and a non-linear function of Feature2
data['Target'] = data['Feature1'] * 2 + np.sin(data['Feature2']) + np.random.randn(100) * 0.5
# Create a DataFrame
df = pd.DataFrame(data)</pre>			<p>You can <a id="_idIndexMarker653"/>adapt this code to analyze relationships <a id="_idIndexMarker654"/>between variables in your Excel data as you have learned in previous chapters.</p>
			<h3>Correlation heat map</h3>
			<p>One of the most common ways to assess relationships is by plotting a correlation heat map. This heat <a id="_idIndexMarker655"/>map provides a visual representation <a id="_idIndexMarker656"/>of the correlations between numerical variables in your dataset. Using libraries such as <code>Seaborn</code>, you can create an informative heat map that color-codes correlations, making it easy to identify strong and weak relationships.</p>
			<p>Let’s take a look at how to do that:</p>
			<pre class="source-code">
# Calculate and plot the correlation heatmap
corr_matrix = df.corr()
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f",
    linewidths=.5)
plt.title('Correlation Heatmap')
plt.show()</pre>			<p>In this code, we create a correlation heat map to visualize the relationships between these variables. The resulting heat map gives nice insights:</p>
			<div><div><img src="img/B19142_08_9.jpg" alt="Figure 8.9 – Correlation heat map"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – Correlation heat map</p>
			<h3>Predictive Power Score (PPS)</h3>
			<p>Beyond traditional correlation measures, the PPS offers insights into non-linear relationships <a id="_idIndexMarker657"/>and predictive capabilities between variables. PPS quantifies how well one variable can predict another.</p>
			<p>Now, we’ll <a id="_idIndexMarker658"/>demonstrate how to calculate and visualize PPS matrices using the <code>ppscore</code> library, giving you a deeper understanding of your data’s predictive potential:</p>
			<pre class="source-code">
# Calculate the Predictive Power Score (PPS)
plt.figure(figsize=(8, 6))
matrix_df = pps.matrix(df)[['x', 'y', 'ppscore']].pivot(columns='x',
    index='y', values='ppscore')
sns.heatmap(matrix_df, vmin=0, vmax=1, cmap="Blues", linewidths=0.5,
    annot=True)
plt.title("Predictive Power Score (PPS) Heatmap")
plt.show()
# Additional insights
correlation_target = df['Feature1'].corr(df['Target'])
pps_target = pps.score(df, 'Feature1', 'Target') ['ppscore']
print(f'Correlation between Feature1 and Target: {correlation_target:.2f}')
print(f'Predictive Power Score (PPS) between Feature1 and Target: {pps_target:.2f}')</pre>			<p>In this <a id="_idIndexMarker659"/>code snippet, we calculate the PPS matrix, which measures <a id="_idIndexMarker660"/>the predictive power of each feature with respect to the <code>Target</code> variable. Finally, we calculate the correlation and PPS specifically between <code>Feature1</code> and the <code>Target</code> variable. The results of the PPS heat map help gain further insights into the relationship between the variables:</p>
			<div><div><img src="img/B19142_08_10.jpg" alt="Figure 8.10 – PPS heat map"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – PPS heat map</p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor175"/>Scatter plots</h2>
			<p>Scatter plots <a id="_idIndexMarker661"/>are another valuable tool for visualizing relationships. They <a id="_idIndexMarker662"/>allow you to explore how two numerical variables interact by plotting data points on a 2D plane. We covered creating scatter plots in <a href="B19142_06.xhtml#_idTextAnchor119"><em class="italic">Chapter 6</em></a>.</p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor176"/>Visualizing key attributes</h2>
			<p>Visualization <a id="_idIndexMarker663"/>is a powerful tool for understanding your data. You can <a id="_idIndexMarker664"/>create various plots and charts to visualize key attributes of the dataset. For numerical data, histograms, box plots, and scatter plots can reveal data distributions, outliers, and potential correlations. Categorical data can be explored through bar charts and pie charts, displaying frequency distributions and proportions. These visualizations provide insights that may not be apparent in summary statistics alone. We covered these methods in <a href="B19142_06.xhtml#_idTextAnchor119"><em class="italic">Chapter 6</em></a>.</p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor177"/>Summary</h1>
			<p>In this chapter, we delved into two pivotal processes: data cleaning and EDA using R and Python, with a specific focus on Excel data.</p>
			<p>Data cleaning is a fundamental step. We learned how to address missing data, be it through imputation, removal, or interpolation. Dealing with duplicates was another key focus, as Excel data, often sourced from multiple places, can be plagued with redundancies. Ensuring the correct assignment of data types was emphasized to prevent analysis errors stemming from data type issues.</p>
			<p>In the realm of EDA, we started with summary statistics. These metrics, such as mean, median, standard deviation, and percentiles for numerical features, grant an initial grasp of data central tendencies and variability. We then explored data distribution, understanding which is critical for subsequent analysis and modeling decisions. Lastly, we delved into the relationships between variables, employing scatter plots and correlation matrices to unearth correlations and dependencies among features.</p>
			<p>By mastering these techniques, you’re equipped to navigate Excel data intricacies. You can ensure data quality and harness its potential for informed decision making. Subsequent chapters will build on this foundation, allowing you to perform advanced analysis and visualization to extract actionable insights from your Excel datasets.</p>
			<p>In the next chapter, we will put the cleaned and explored data to good use: we will start with <strong class="bold">statistical analysis</strong>, in particular, with linear and logistic regression.</p>
		</div>
	</body></html>