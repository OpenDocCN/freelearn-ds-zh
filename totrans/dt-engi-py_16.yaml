- en: '*Chapter 13*: Streaming Data with Apache Kafka'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第13章*：使用Apache Kafka进行流数据'
- en: Apache Kafka opens up the world of real-time data streams. While there are fundamental
    differences in stream processing and batch processing, how you build data pipelines
    will be very similar. Understanding the differences between streaming data and
    batch processing will allow you to build data pipelines that take these differences
    into account.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka打开了实时数据流的世界。虽然流处理和批处理之间存在基本差异，但你构建数据管道的方式将非常相似。理解流数据与批处理之间的差异将使你能够构建考虑这些差异的数据管道。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Understanding logs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解日志
- en: Understanding how Kafka uses logs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Kafka如何使用日志
- en: Building data pipelines with Kafka and NiFi
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kafka和NiFi构建数据管道
- en: Differentiating stream processing from batch processing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分流处理和批处理
- en: Producing and consuming with Python
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python进行生产和消费
- en: Understanding logs
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解日志
- en: 'If you have written code, you may be familiar with software logs. Software
    developers use logging to write output from applications to a text file to store
    different events that happen within the software. They then use these logs to
    help debug any issues that arise. In Python, you have probably implemented code
    similar to the following code:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你编写过代码，你可能熟悉软件日志。软件开发者使用日志将应用程序的输出写入文本文件以存储软件内部发生的事件。然后，他们使用这些日志来帮助调试出现的任何问题。在Python中，你可能实现了类似于以下代码的代码：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding code is a basic logging example that logs different levels –
    `debug`, `warning`, and `error` – to a file named `python-log.log`. The code will
    produce the following output:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码是一个基本的日志记录示例，它将不同的级别——`debug`、`warning`和`error`——记录到名为`python-log.log`的文件中。该代码将产生以下输出：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The messages are logged to the file in the order in which they occur. You do
    not, however, know the exact time the event happened. You can improve on this
    log by adding a timestamp, as shown in the following code:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 消息按照它们发生的顺序记录到文件中。然而，你并不知道事件发生的确切时间。你可以通过添加时间戳来改进这个日志，如下面的代码所示：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The results of the preceding code are shown in the following code block. Notice
    that now there is a timestamp. The log is ordered, as was the previous log. However,
    the exact time is known in this log:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的结果显示在下面的代码块中。注意现在有一个时间戳。日志是有序的，就像之前的日志一样。然而，在这个日志中，确切的时间是已知的：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the preceding logs, you should notice that they follow a very specific format,
    or schema, which is defined in `basicConfig`. Another common log that you are
    probably familiar with is the `web` log.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的日志中，你应该注意到它们遵循一个非常特定的格式，或者说是模式，这个模式在`basicConfig`中定义。另一个你可能熟悉的常见日志是`web`日志。
- en: Web server logs are like software logs; they report events – usually requests
    – in chronological order, with a timestamp and the event. These logs follow a
    very specific format and have many tools available for parsing them. Databases
    also use logs internally to help in replication and to record modifications in
    a transaction.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Web服务器日志类似于软件日志；它们按时间顺序报告事件——通常是请求——包括时间戳和事件。这些日志遵循一个非常特定的格式，并且有许多工具可用于解析它们。数据库也使用日志来帮助复制并记录事务中的修改。
- en: If applications, web servers, and databases all use logs, and they are all slightly
    different, what exactly is a log?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果应用程序、Web服务器和数据库都使用日志，并且它们都有所不同，那么日志究竟是什么？
- en: A **log** is an ordered collection of events, or records, that is append only.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**日志**是有序事件或记录的集合，是只追加的。'
- en: 'That is all there is to it. Simple. To the point. And yet an extremely powerful
    tool in software development and data processing. The following diagram shows
    a sample log:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是全部内容。简单。直接。然而，在软件开发和数据处理中却是一个极其强大的工具。以下图表显示了一个示例日志：
- en: '![Figure 13.1 – An example of a log'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.1 – 日志的一个示例'
- en: '](img/Figure_13.1_B15739.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.1 – 日志的一个示例](img/Figure_13.1_B15739.jpg)'
- en: Figure 13.1 – An example of a log
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1 – 日志的一个示例
- en: The preceding diagram shows individual records as blocks. The first record is
    on the left. Time is represented by the position of records in the log. The record
    to the right of another record is newer. So, record **3** is newer than record
    **2**. Records are not removed from the log but appended to the end. Record **9**
    is added to the far right of the log, as it is the newest record – until record
    10 comes along.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图显示了单个记录作为块。第一个记录在左侧。时间由记录在日志中的位置表示。另一个记录右侧的记录较新。因此，记录**3**比记录**2**新。记录不会被从日志中删除，而是追加到末尾。记录**9**被添加到日志的右侧，因为它是最新的记录——直到记录10出现。
- en: Understanding how Kafka uses logs
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Kafka如何使用日志
- en: Kafka maintains logs that are written to by producers and read by consumers.
    The following sections will explain topics, consumers, and producers.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka维护由生产者写入并由消费者读取的日志。以下章节将解释主题、消费者和生产者。
- en: Topics
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主题
- en: 'Apache Kafka uses logs to store data – records. Logs in Kafka are called `dataengineering`.
    The topic is saved to disk as a log file. Topics can be a single log, but usually
    they are scaled horizontally into partitions. Each partition is a log file that
    can be stored on another server. In a topic with partitions, the message order
    guarantee no longer applies to the topic, but only each partition. The following
    diagram shows a topic split into three partitions:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka使用日志来存储数据——记录。Kafka中的日志被称为`dataengineering`。主题被保存为日志文件存储在磁盘上。主题可以是一个单独的日志，但通常它们会水平扩展到分区。每个分区是一个日志文件，可以存储在另一台服务器上。在具有分区的主题中，消息顺序保证不再适用于主题，而只适用于每个分区。以下图显示了将主题分割成三个分区的情况：
- en: '![Figure 13.2 – A Kafka topic with three partitions'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.2 – 具有三个分区的Kafka主题'
- en: '](img/Figure_13.2_B15739.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_13.2_B15739.jpg)'
- en: Figure 13.2 – A Kafka topic with three partitions
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2 – 具有三个分区的Kafka主题
- en: The preceding topic – **Transactions** – has three partitions labeled **P1**,
    **P2**, and **P3**. Within each partition, the records are ordered, with the records
    to the left being older than the records to the right – the larger the number
    in the box, the newer the record. You will notice that the records have **K:A**
    in **P1** and **K:B** and **K:C** in **P2** and **P3**, respectively. Those are
    the keys associated with the records. By assigning a key, you guarantee that the
    records containing the same keys will go to the same partition. While the order
    of records in the topic may be out of order, each partition is in order.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上述主题——**事务**——有三个标记为**P1**、**P2**和**P3**的分区。在每个分区内部，记录是有序的，左侧的记录比右侧的记录旧——框中的数字越大，记录越新。你会注意到记录在**P1**中有**K:A**，在**P2**和**P3**中分别有**K:B**和**K:C**。那些是与记录关联的键。通过分配键，你可以保证包含相同键的记录将进入同一个分区。虽然主题中记录的顺序可能是不规则的，但每个分区是有序的。
- en: Kafka producers and consumers
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kafka生产者和消费者
- en: 'Kafka producers send data to a topic and a partition. Records can be sent round-robin
    to partitions or you can use a key to send data to specific partitions. When you
    send messages with a producer, you can do it in one of three ways:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka生产者将数据发送到主题和分区。记录可以轮询发送到分区，或者你可以使用键将数据发送到特定的分区。当你使用生产者发送消息时，你可以以三种方式之一进行：
- en: '**Fire and Forget**: You send a message and move on. You do not wait for an
    acknowledgment back from Kafka. In this method, records can get lost.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**发送后即忘**：你发送一条消息并继续操作。你不需要等待Kafka的确认。在此方法中，记录可能会丢失。'
- en: '**Synchronous**: Send a message and wait for a response before moving on.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同步**：发送一条消息，并在继续之前等待响应。'
- en: '**Asynchrous**: Send a message and a callback. You move on once the message
    is sent, but will get a response at some point that you can handle.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异步**：发送一条消息和一个回调。消息发送后，你可以继续操作，但会在某个时候收到一个你可以处理的响应。'
- en: Producers are fairly straightforward – they send messages to a topic and partition,
    maybe request an acknowledgment, retry if a message fails – or not – and continue.
    Consumers, however, can be a little more complicated.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 生产者相当直接——它们向主题和分区发送消息，可能请求确认，如果消息失败则重试——或者不重试——然后继续。然而，消费者可能更复杂一些。
- en: Consumers read messages from a topic. Consumers run in a poll loop that runs
    indefinitely waiting for messages. Consumers can read from the beginning – they
    will start at the first message in the topic and read the entire history. Once
    caught up, the consumer will wait for new messages.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者从主题中读取消息。消费者在一个无限循环的轮询中运行，等待消息。消费者可以从开始处读取——它们将从主题中的第一条消息开始读取整个历史记录。一旦追上，消费者将等待新消息。
- en: If a consumer reads five messages, the offset is five. The offset is the position
    of the consumer in the topic. It is a bookmark for where the consumer left off.
    A consumer can always start reading a topic from the offset, or a specified offset
    – which is stored by Zookeeper.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果消费者读取五条消息，偏移量是五。偏移量是消费者在主题中的位置。它是消费者停止读取的位置的标记。消费者可以从偏移量或指定的偏移量开始读取主题，该偏移量由
    Zookeeper 存储。
- en: 'What happens when you have a consumer on the `dataengineering` topic, but the
    topic has three partitions and is writing records faster than you can read it?
    The following diagram shows a single consumer trying to consume three partitions:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在 `dataengineering` 主题上有消费者，但主题有三个分区且写入记录的速度比你读取的速度快时，会发生什么？以下图显示了一个消费者试图消费三个分区：
- en: '![Figure 13.3 – Single consumer reading multiple partitions'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.3 – 单个消费者读取多个分区'
- en: '](img/Figure_13.3_B15739.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_13.3_B15739.jpg)'
- en: Figure 13.3 – Single consumer reading multiple partitions
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3 – 单个消费者读取多个分区
- en: 'Using consumer groups, you can scale the reading of Kafka topics. In the preceding
    diagram, the consumer **C1** is in a consumer group, but is the only consumer.
    By adding additional consumers, the topics can be distributed. The following diagram
    shows what that looks like:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用消费者组，您可以扩展 Kafka 主题的读取。在前面的图中，消费者 **C1** 在一个消费者组中，但它是唯一的消费者。通过添加额外的消费者，可以将主题进行分配。以下图显示了这种情况：
- en: '![Figure 13.4 – Two consumers in a consumer group consuming three partitions'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.4 – 消费者组中的两个消费者消费三个分区'
- en: '](img/Figure_13.4_B15739.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_13.4_B15739.jpg)'
- en: Figure 13.4 – Two consumers in a consumer group consuming three partitions
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4 – 消费者组中的两个消费者消费三个分区
- en: 'The preceding diagram shows that there are still more partitions than consumers
    in the group, meaning that one consumer will be handling multiple partitions.
    You can add more consumers, as shown in the following diagram:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图显示，组中的分区数仍然多于消费者数，这意味着一个消费者将处理多个分区。您可以根据以下图添加更多消费者：
- en: '![Figure 13.5 – More consumers than partitions leave one idle'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.5 – 消费者数量多于分区数导致一个空闲'
- en: '](img/Figure_13.5_B15739.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_13.5_B15739.jpg)'
- en: Figure 13.5 – More consumers than partitions leave one idle
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5 – 消费者数量多于分区数导致一个空闲
- en: In the preceding diagram, there are more consumers in the consumer group than
    partitions. When there are more consumers than partitions, consumers will sit
    idle. Therefore, it is not necessary to create more consumers than the number
    of partitions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，消费者组中的消费者数量多于分区数。当消费者数量多于分区数时，消费者将处于空闲状态。因此，没有必要创建多于分区数的消费者。
- en: 'You can, however, create more than one consumer group. The following diagram
    shows what this looks like:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，您可以创建多个消费者组。以下图显示了这种情况：
- en: '![Figure 13.6 – Multiple consumer groups reading from a single topic'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.6 – 从单个主题读取的多个消费者组'
- en: '](img/Figure_13.6_B15739.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_13.6_B15739.jpg)'
- en: Figure 13.6 – Multiple consumer groups reading from a single topic
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.6 – 多个消费者组从单个主题读取
- en: Multiple consumer groups can read from the same partition. It is good practice
    to create a consumer group for every application that needs access to the topic.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 多个消费者组可以读取同一个分区。为每个需要访问主题的应用程序创建一个消费者组是良好的实践。
- en: Now that you understand the basics of working with Kafka, the next section will
    show you how to build data pipelines using NiFi and Kafka.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了与 Kafka 一起工作的基础知识，下一节将向你展示如何使用 NiFi 和 Kafka 构建数据管道。
- en: Building data pipelines with Kafka and NiFi
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Kafka 和 NiFi 构建数据管道
- en: To build a data pipeline with Apache Kafka, you will need to create a producer
    since we do not have any production Kafka clusters to connect to. With the producer
    running, you can read the data like any other file or database.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Apache Kafka 构建数据管道，您需要创建一个生产者，因为我们没有可连接的生产 Kafka 集群。在生产者运行时，您可以像读取其他文件或数据库一样读取数据。
- en: The Kafka producer
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kafka 生产者
- en: 'The Kafka producer will take advantage of the production data pipeline from
    [*Chapter 11*](B15739_11_ePub_AM.xhtml#_idTextAnchor118)*, Project — Building
    a Production Data Pipeline*. The producer data pipeline will do little more than
    send the data to the Kafka topic. The following screenshot shows the completed
    producer data pipeline:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 生产者将利用来自[*第 11 章*](B15739_11_ePub_AM.xhtml#_idTextAnchor118)*，项目 — 构建生产数据管道*的生产数据管道。生产者数据管道所做的只是将数据发送到
    Kafka 主题。以下截图显示了完成的生产者数据管道：
- en: '![Figure 13.7 – The NiFi data pipeline'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.7 – NiFi 数据管道'
- en: '](img/Figure_13.7_B15739.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 13.7_B15739.jpg](img/Figure_13.7_B15739.jpg)'
- en: Figure 13.7 – The NiFi data pipeline
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 13.7 – NiFi 数据管道
- en: 'To create the data pipeline, perform the following steps:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建数据管道，请执行以下步骤：
- en: 'Open a terminal. You need to create the topic before you can send messages
    to it in NiFi. Enter the following command:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开终端。在 NiFi 中向其发送消息之前，您需要先创建主题。输入以下命令：
- en: '[PRE4]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Drag an input port and connect it to the output from the `ReadDataLake` processor
    group, as shown in the following screenshot:![Figure 13.8 – Connecting the input
    port to an output port
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下截图所示，将输入端口拖动并连接到 `ReadDataLake` 处理器组的输出：![Figure 13.8 – 连接输入端口到输出端口
- en: '](img/Figure_13.8_B15739.jpg)'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![Figure 13.8_B15739.jpg](img/Figure_13.8_B15739.jpg)'
- en: Figure 13.8 – Connecting the input port to an output port
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Figure 13.8 – 连接输入端口到输出端口
- en: Next, drag and drop the `ControlRate` processor to the canvas. The `ControlRate`
    processor will allow us to slow down the data flow with more control than just
    using backpressure in the queue. This will allow you to make it appear that data
    is streaming into the Kafka topic instead of just being there all at once. If
    you did write it all in one pass, once you had read it, the pipeline would stop
    until you added more data.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将 `ControlRate` 处理器拖放到画布上。`ControlRate` 处理器将允许我们比仅使用队列中的背压有更多控制地减慢数据流。这将使数据看起来是流式传输到
    Kafka 主题，而不是一次性全部存在。如果您一次性写入所有内容，一旦读取完毕，管道将停止，直到您添加更多数据。
- en: To configure the `ControlRate` processor, set the `flowfile count`. Set `1`.
    These two properties allow you to specify the amount of data passing through.
    Since you used the flowfile count, the maximum rate will be an integer of the
    number of flowfiles to let through. If you used the default option, you would
    set `1 MB`. Lastly, specify how frequently to allow the maximum rate through in
    the **Time Duration** property. I have left it at 1 minute. Every minute, one
    flowfile will be sent to the user's Kafka topic.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要配置 `ControlRate` 处理器，设置 `flowfile count`。设置为 `1`。这两个属性允许您指定通过的数据量。由于您使用了 flowfile
    count，最大速率将是一个整数，表示允许通过的 flowfile 数量。如果您使用了默认选项，则应设置为 `1 MB`。最后，在 **Time Duration**
    属性中指定允许最大速率通过的频率。我将其设置为 1 分钟。每分钟，将有一个 flowfile 发送到用户的 Kafka 主题。
- en: To send the data to Kafka, drag and drop the `PublishKafka_2_0` processor to
    the canvas. There are multiple Kafka processors for different versions of Kafka.
    To configure the processor, you will set the `localhost:9092`, `localhost:9093`,
    and `localhost:9094`. A Kafka broker is a Kafka server. Since you are running
    a cluster, you will enter all of the IPs as a comma-separated string – just as
    you did in the command-line example in the previous chapter. Enter **Topic Name**
    as users and the **Delivery Guarantee** property to **Guarantee Replication Delivery**.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要将数据发送到 Kafka，将 `PublishKafka_2_0` 处理器拖放到画布上。有多个 Kafka 处理器用于不同版本的 Kafka。要配置处理器，您将设置
    `localhost:9092`、`localhost:9093` 和 `localhost:9094`。Kafka broker 是 Kafka 服务器。由于您正在运行一个集群，您将输入所有
    IP 地址作为逗号分隔的字符串——就像您在前一章的命令行示例中所做的那样。输入 **Topic Name** 作为用户和 **Delivery Guarantee**
    属性为 **Guarantee Replication Delivery**。
- en: You now have a Kafka producer configured in NiFi that will take the output from
    `ReadDataLake` and send each record to Kafka at one-minute intervals. To read
    the topic, you will create a NiFi data pipeline.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已在 NiFi 中配置了 Kafka 生成器，它将从 `ReadDataLake` 获取输出，并以一分钟间隔将每条记录发送到 Kafka。要读取主题，您将创建一个
    NiFi 数据管道。
- en: The Kafka consumer
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kafka 消费者
- en: As a data engineer, you may or may not need to set up the Kafka cluster and
    producers. However, as you learned at the beginning of this book, the role of
    the data engineer varies widely, and building the Kafka infrastructure could very
    well be part of your job. With Kafka receiving messages on a topic, it is time
    to read those messages.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据工程师，您可能需要或不需设置 Kafka 集群和生成器。然而，正如您在本书的开始所学的，数据工程师的角色差异很大，构建 Kafka 基础设施可能是您工作的一部分。Kafka
    在主题上接收消息后，是时候读取这些消息了。
- en: 'The completed data pipeline is shown in the following screenshot:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后的数据管道如图所示：
- en: '![Figure 13.9 – Consuming a Kafka topic in NiFi'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 13.9 – Consuming a Kafka topic in NiFi](img/Figure_13.9.jpg)'
- en: '](img/Figure_13.9_B15739.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 13.9_B15739.jpg](img/Figure_13.9_B15739.jpg)'
- en: Figure 13.9 – Consuming a Kafka topic in NiFi
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 13.9 – Consuming a Kafka topic in NiFi
- en: 'To create the data pipeline, perform the following steps:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建数据管道，请执行以下步骤：
- en: Drag and drop the `ConsumeKafka_2_0` processor to the canvas. To configure the
    processor, set the Kafka brokers to your cluster – `localhost:9092`, `localhost:9093`,
    and `localhost:9094`. Set `users` and `Earliest`. Lastly, set `NiFi Consumer`.
    The **Group ID** property defines the consumer group that the consumer (processor)
    will be a part of.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`ConsumeKafka_2_0`处理器拖放到画布上。要配置处理器，将Kafka代理设置为您的集群——`localhost:9092`、`localhost:9093`和`localhost:9094`。设置`users`和`Earliest`。最后，设置`NiFi
    Consumer`。**Group ID**属性定义了消费者（处理器）将成为其一部分的消费者组。
- en: Next, I have added the `ControlRate` processor to the canvas. The `ControlRate`
    processor will slow down the reading of the records already on the topic. If the
    topic is not too large, you could just use backpressure on the queue so that once
    you have processed the historical data, the new records will move in real time.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我已经将`ControlRate`处理器添加到画布上。`ControlRate`处理器将减慢对主题上已有记录的读取速度。如果主题不是太大，您可以使用队列上的背压，这样一旦您处理了历史数据，新的记录将实时移动。
- en: To configure the `ControlRate` processor, set `flowfile count`, `1`, and `1`
    minute.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要配置`ControlRate`处理器，设置`flowfile count`为`1`，`1`分钟。
- en: Add an output port to the canvas and name it. I have named it `OutputKafkaConsumer`.
    This will allow you to connect this processor group to others to complete a data
    pipeline.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在画布上添加一个输出端口并命名它。我将其命名为`OutputKafkaConsumer`。这将允许您将此处理器组连接到其他处理器组以完成数据处理管道。
- en: Start the processor group, and you will see records processed every minute.
    You are reading a Kafka topic with a single consumer in a consumer group. If you
    recall, when you created the topic, you set the number of partitions to three.
    Because there are multiple partitions, you can add more consumers to the group.
    To do that, you just need to add another `ConsumeKafka_2_0` processor and configure
    it.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动处理器组，您将每分钟看到处理记录。您正在使用单个消费者在一个消费者组中读取Kafka主题。如果您还记得，当您创建主题时，您将分区数设置为三个。因为有多个分区，您可以向组中添加更多消费者。为此，您只需添加另一个`ConsumeKafka_2_0`处理器并对其进行配置。
- en: Drag another `ConsumeKafka_2_0` processor to the canvas. Configure it with the
    same Kafka brokers – `localhost:9092`, `localhost:9093`, and `localhost:9094`
    – and the same topic – `users`. Set `NiFi Consumer`.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将另一个`ConsumeKafka_2_0`处理器拖放到画布上。使用相同的Kafka代理进行配置——`localhost:9092`、`localhost:9093`和`localhost:9094`——以及相同的主题——`users`。设置`NiFi
    Consumer`。
- en: Now that both processors have the same group ID, they will be members of the
    same consumer group. With two consumer and three partitions, one of the consumers
    will read two partitions and the other will read one. You can add another `ConsumeKafka_2_0`
    processor if the topic is streaming large amounts of data, but any more than three
    would sit idle.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在两个处理器具有相同的组ID，它们将成为同一个消费者组的成员。在两个消费者和三个分区的情况下，一个消费者将读取两个分区，另一个将读取一个。如果主题正在流式传输大量数据，您可以添加另一个`ConsumeKafka_2_0`处理器，但超过三个将处于空闲状态。
- en: 'The new data pipeline is shown in the following screenshot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 新的数据管道在以下屏幕截图中显示：
- en: '![Figure 13.10 – Consuming a Kafka with multiple consumers in a consumer group'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.10 – 在消费者组中使用多个消费者消费Kafka'
- en: '](img/Figure_13.10_B15739.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_13.10_B15739.jpg]'
- en: Figure 13.10 – Consuming a Kafka with multiple consumers in a consumer group
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.10 – 在消费者组中使用多个消费者消费Kafka
- en: Running the processor group, you will start to see records flow through both
    `ConsumerKafka_2_0` processors. The configuration of the producer will determine
    which partition records are sent to and how many will flow through your consumer.
    Because of the default settings and the number of partitions to consumers, you
    will probably see two flowfiles processed by one consumer for every one flowfile
    processed by the other.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 运行处理器组，您将开始看到记录通过两个`ConsumerKafka_2_0`处理器流动。生产者的配置将决定哪些分区记录被发送以及有多少将通过您的消费者。由于默认设置和消费者数量，您可能会看到每个由另一个消费者处理的flowfile，只有一个消费者处理两个flowfile。
- en: Just as you can add more consumers to a consumer group, you can have more than
    one consumer group read a topic – the number of consumer groups is in no way related
    to the number of partitions.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可以向消费者组添加更多消费者一样，您可以让多个消费者组读取一个主题——消费者组的数量与分区数量没有任何关系。
- en: 'To add another consumer group to the data pipeline, drag and drop another `ConsumeKafka_2_0`
    processor. Set `localhost:9092`, `localhost:9093`, and `localhost:9094`, and set
    `users`. The group ID is the name of the consumer group. Set it to anything other
    than `NiFi Consumer` – since this consumer group already exists. I have set `NiFi
    Consumer2` – hardly creative or original, but it gets the job done. The data pipeline
    will now look like the following screenshot:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要将另一个消费者组添加到数据管道中，拖放另一个 `ConsumeKafka_2_0` 处理器。设置 `localhost:9092`、`localhost:9093`
    和 `localhost:9094`，并设置 `users`。组 ID 是消费者组的名称。将其设置为除 `NiFi Consumer` 之外的其他任何名称
    – 因为这个消费者组已经存在。我将其设置为 `NiFi Consumer2` – 几乎没有创意或原创性，但它完成了工作。现在数据管道将看起来像以下屏幕截图：
- en: '![Figure 13.11 – Two consumer groups in NiFi'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.11 – NiFi 中的两个消费者组'
- en: '](img/Figure_13.11_B15739.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_13.11_B15739.jpg)'
- en: Figure 13.11 – Two consumer groups in NiFi
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.11 – NiFi 中的两个消费者组
- en: In the preceding screenshot, you will notice that there is no `ControlRate`
    processor on the second consumer group. Once started, the processor will consume
    the entire history of the topic and send the records downstream. There were 17
    records in the topic. The other queues are much smaller because the topic is being
    throttled.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图中，您会注意到第二个消费者组上没有 `ControlRate` 处理器。一旦启动，处理器将消费主题的全部历史记录并将记录发送到下游。主题中有
    17 条记录。其他队列要小得多，因为主题正在被节流。
- en: 'You can now connect the processor group to any other processor group to create
    a data pipeline that reads from Apache Kafka. In the following screenshot, I have
    connected the `ReadKafka` processor group to the production data pipeline from
    [*Chapter 11*](B15739_11_ePub_AM.xhtml#_idTextAnchor118)*, Project – Building
    a Production Data Pipeline*:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以将处理器组连接到任何其他处理器组，以创建一个从 Apache Kafka 读取的数据管道。在下面的屏幕截图中，我已经将 `ReadKafka`
    处理器组连接到来自 [*第 11 章*](B15739_11_ePub_AM.xhtml#_idTextAnchor118) 的生产数据管道 – 项目 –
    构建生产数据管道：
- en: '![Figure 13.12 – The completed data pipeline'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.12 – 完成的数据管道'
- en: '](img/Figure_13.12_B15739.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_13.12_B15739.jpg)'
- en: Figure 13.12 – The completed data pipeline
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.12 – 完成的数据管道
- en: Instead of reading the data from the data lake, the new data pipeline reads
    the data from the Kafka topic users. The records are sent to the staging processor
    group to continue the data pipeline. The end result is that the PostgreSQL production
    table will have all of the records from the Kafka topic. The reading of the data
    lake is now a Kafka producer.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 与从数据湖读取数据不同，新的数据管道从 Kafka 主题 users 读取数据。记录被发送到临时处理器组以继续数据管道。最终结果是 PostgreSQL
    生产表将包含 Kafka 主题的所有记录。现在读取数据湖的操作变成了 Kafka 生产者。
- en: Creating producers and consumers in NiFi only requires the use of a single processor
    – `PublishKafka` or `ConsumeKafka`. The configuration is dependent on the Kafka
    cluster you will publish to or read from. In NiFi, Kafka is just another data
    input. How you process the data once received will be no different than if you
    ran a database query. There are some differences in the nature of the data that
    you must take into consideration, and the next section will discuss them.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NiFi 中创建生产者和消费者只需要使用单个处理器 – `PublishKafka` 或 `ConsumeKafka`。配置取决于您将发布到或从中读取的
    Kafka 集群。在 NiFi 中，Kafka 只是一个数据输入。一旦接收数据，您处理数据的方式将与运行数据库查询时没有区别。您必须考虑数据性质的一些差异，下一节将讨论它们。
- en: Differentiating stream processing from batch processing
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 区分流处理和批处理
- en: While the processing tools don't change whether you are processing streams or
    batches, there are two things you should keep in mind while processing streams
    – **unbounded** and **time**.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然处理工具不会改变您是处理流还是批处理，但在处理流时，您应该注意两点 – **无限** 和 **时间**。
- en: Data can be bounded or unbounded. Bounded data has an end, whereas unbounded
    data is constantly created and is possibly infinite. Bounded data is last year's
    sales of widgets. Unbounded data is a traffic sensor counting cars and recording
    their speeds on the highway.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以是有限或无限的。有限数据有一个终点，而无限数据是不断创建的，可能是无限的。有限数据是去年的小部件销售额。无限数据是高速公路上交通传感器的汽车计数和记录它们的速度。
- en: Why is this important in building data pipelines? Because with bounded data,
    you will know everything about the data. You can see it all at once. You can query
    it, put it in a staging environment, and then run Great Expectations on it to
    get a sense of the ranges, values, or other metrics to use in validation as you
    process your data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这在构建数据管道时很重要？因为对于有界数据，你会知道关于数据的一切。你可以一次性看到所有内容。你可以查询它，将其放入临时环境，然后运行 Great
    Expectations 来了解范围、值或其他指标，这些指标可以在处理数据时用于验证。
- en: With unbounded data, it is streaming in and you don't know what the next piece
    of data will look like. This doesn't mean you can't validate it – you know that
    the speed of a car must be within a certain range and can't have the value *h*
    – it will be an integer between 0 and 200-ish.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无界数据，它是流式进入的，你不知道下一份数据将是什么样子。这并不意味着你不能验证它 – 你知道一辆车的速度必须在某个范围内，并且不能有值 *h* –
    它将是一个介于 0 和 200 左右的整数。
- en: On bounded data, you can query the average or maximum of a field. On unbounded
    data, you will need to keep recalculating these values as data streams through
    the data pipeline. In the next chapter, you will learn about Apache Spark and
    how it can help in processing unbounded, or streaming, data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在有界数据上，你可以查询字段的平均值或最大值。在无界数据上，你需要随着数据流通过数据管道时不断重新计算这些值。在下一章中，你将学习 Apache Spark
    以及它是如何帮助处理无界或流数据的。
- en: You may be thinking that yes, last year's sales numbers are bounded, but this
    year's are not. The year is not over, and the data is still streaming in. This
    brings up the second thing you should keep in mind when dealing with streams and
    that is time. Bounded data is complete over a time period or a window. And windowing
    is a method of making unbounded data bounded.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，是的，去年的销售额是有界的，但今年的不是。一年还没有结束，数据仍在流式传输。这引出了你在处理流时应注意的第二件事，那就是时间。有界数据是在一个时间段或窗口内完成的。而窗口化是将无界数据有界化的方法。
- en: 'There are three common windows – **fixed**, **sliding**, and **session**:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的窗口有三种 – **固定**、**滑动**和**会话**：
- en: '**Fixed** – Sometimes called **tumbling windows, these**re windows that covers
    a fixed time and records do not overlap. If you specify a one-minute window, the
    records will fall within each interval, as shown in the following diagram:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**固定** – 有时称为**滚动窗口**，这些是覆盖固定时间且记录不重叠的窗口。如果你指定一个一分钟窗口，记录将落在每个间隔内，如下面的图表所示：'
- en: '![Figure 13.13 – Data in fixed windows'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.13 – 固定窗口中的数据'
- en: '](img/Figure_13.13_B15739.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_13.13_B15739.jpg)'
- en: Figure 13.13 – Data in fixed windows
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.13 – 固定窗口中的数据
- en: '**Sliding** – This is a window in which the window is defined, such as 1 minute,
    but the next window starts in less than the window length – say, every 30 seconds.
    This type of window will have duplicates and is good for rolling averages. A sliding
    window is shown in the following diagram:'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滑动** – 这是一个窗口，其中窗口被定义，例如 1 分钟，但下一个窗口在窗口长度内开始 – 比如，每 30 秒开始一次。这种窗口会有重复，适合滚动平均值。以下图表显示了滑动窗口：'
- en: '![Figure 13.14 – Data in sliding windows'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.14 – 滑动窗口中的数据'
- en: '](img/Figure_13.14_B15739.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_13.14_B15739.jpg)'
- en: Figure 13.14 – Data in sliding windows
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.14 – 滑动窗口中的数据
- en: The diagram shows two windows, one starting at 0 that is extended for 1 minute.
    The second window overlaps at 0:30 and extends for 1 minute until 1:30.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示了两个窗口，一个从 0 开始，持续 1 分钟。第二个窗口在 0:30 处重叠，持续 1 分钟直到 1:30。
- en: '**Session** – Sessions will not have the same window of time but are events.
    For example, a user logs in to shop, their data is streamed for that login session,
    and the session is defined by some piece of data in the records, called a session
    token.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**会话** – 会话不会有一个相同的时间窗口，但它们是事件。例如，一个用户登录购物，他们的数据会在这个登录会话中流式传输，会话由记录中的某些数据定义，称为会话令牌。'
- en: 'When discussing windows and time, you must also consider what time to use –
    **Event**, **Ingest**, or **Processing**. The three different times could have
    different values and whichever you choose depends on your use case:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当讨论窗口和时间时，你也必须考虑使用什么时间 – **事件**、**摄取**或**处理**。三个不同的时间可能具有不同的值，而你选择哪一个取决于你的用例：
- en: '**Event Time** is when the event happens. This may be recorded in the record
    before it is sent to Kafka. For example, at 1:05, a car was recorded travelling
    55 mph.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件时间**是事件发生的时间。这可能在发送到 Kafka 之前被记录在记录中。例如，在 1:05，一辆车被记录以 55 英里/小时的速度行驶。'
- en: '**Ingest Time** is the time that the data is recorded in a Kafka topic. The
    latency between the event and the recording could fluctuate depending on network
    latency.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**摄入时间**是数据记录在Kafka主题中的时间。事件与记录之间的延迟可能会根据网络延迟而波动。'
- en: '**Processing Time** is the time in which you read the data from the Kafka topic
    and did something with it – such as processed it through your data pipeline and
    put it in the warehouse.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理时间**是你从Kafka主题中读取数据并对其进行操作（例如，通过你的数据管道处理并将其放入仓库）的时间。'
- en: By recognizing that you may be working with unbounded data, you will avoid problems
    in your data pipelines by not trying to analyze all the data at once, but by choosing
    an appropriate windowing and also by using the correct time for your use case.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通过认识到你可能正在处理无界数据，你将通过一次不尝试分析所有数据，而是通过选择适当的窗口以及使用适合你用例的正确时间来避免你的数据管道中的问题。
- en: Producing and consuming with Python
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python进行生产和消费
- en: You can create producers and consumers for Kafka using Python. There are multiple
    Kafka Python libraries – Kafka-Python, PyKafka, and Confluent Python Kafka. In
    this section, I will use Confluent Python Kafka, but if you want to use an open
    source, community-based library, you can use Kafka-Python. The principles and
    structure of the Python programs will be the same no matter which library you
    choose.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用Python为Kafka创建生产者和消费者。有多个Kafka Python库 – Kafka-Python、PyKafka和Confluent
    Python Kafka。在本节中，我将使用Confluent Python Kafka，但如果你想要使用基于开源和社区的库，你可以使用Kafka-Python。无论你选择哪个库，Python程序的原理和结构都将相同。
- en: 'To install the library, you can use `pip`. The following command will install
    it:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装库，你可以使用`pip`。以下命令将安装它：
- en: '[PRE5]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Once the library has finished installing, you can use it by importing it into
    your applications. The following sections will walk through writing a producer
    and consumer.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦库安装完成，你就可以通过将其导入到你的应用程序中来使用它。接下来的部分将介绍如何编写生产者和消费者。
- en: Writing a Kafka producer in Python
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Python中编写Kafka生产者
- en: 'To write a producer in Python, you will create a producer, send data, and listen
    for acknowledgements. In the previous examples, you used `Faker` to create fake
    data about people. You will use it again to generate the data in this example.
    To write the producer, perform the following steps:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Python中编写生产者，你需要创建一个生产者、发送数据并监听确认。在之前的示例中，你使用了`Faker`来创建关于人的假数据。你将再次使用它来生成本例中的数据。要编写生产者，执行以下步骤：
- en: 'Import the required libraries and create a faker:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库并创建一个faker：
- en: '[PRE6]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, create the producer by specifying the IP addresses of your Kafka cluster:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，通过指定你的Kafka集群的IP地址来创建生产者：
- en: '[PRE7]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can list the topics available to publish to as follows:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以列出可用于发布的主题如下：
- en: '[PRE8]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'There are different settings for acknowledgments and how you handle them, but
    for now, create a callback that will receive an error (`err`) and an acknowledgment
    (`msg`). In every call, only one of them will be true and have data. Using an
    `if` statement, check whether there is an error, otherwise, you can print the
    message:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于确认及其处理有不同的设置，但就目前而言，创建一个将接收错误（`err`）和确认（`msg`）的回调函数。在每次调用中，只有其中一个将是真实的并且有数据。使用`if`语句检查是否存在错误，否则可以打印消息：
- en: '[PRE9]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, create the producer loop. The code loops through a range creating a fake
    data object. The object is the same as in [*Chapter 3*](B15739_03_ePub_AM.xhtml#_idTextAnchor039)*,
    Working with Files*. It then dumps the dictionary so that it can be sent to Kafka:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建生产者循环。代码通过一个范围创建一个假数据对象。该对象与[*第3章*](B15739_03_ePub_AM.xhtml#_idTextAnchor039)*，与文件一起工作*相同。然后它将字典导出，以便可以将其发送到Kafka：
- en: '[PRE10]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Before sending the data to Kafka, call `poll()` to get any acknowledgments
    for previous messages. Those will be sent to the callback (`receipt`). Now you
    can call `produce()` and pass the topic name, the data, and the function to send
    acknowledgments to:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在将数据发送到Kafka之前，调用`poll()`以获取之前消息的任何确认。这些确认将被发送到回调函数（`receipt`）。现在你可以调用`produce()`并传递主题名称、数据和发送确认的函数：
- en: '[PRE11]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To finish, flush the producer. This will also get any existing acknowledgements
    and send them to `receipt()`:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要完成，刷新生产者。这将获取任何现有的确认并将它们发送到`receipt()`：
- en: '[PRE12]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The results of the preceding code will be messages sent to the `user` topic
    on the Kafka cluster, and the terminal will print the acknowledgments, which will
    look like the following output:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的结果将是发送到Kafka集群中`user`主题的消息，终端将打印确认，如下所示：
- en: '[PRE13]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now that you can send data to a Kafka topic, the next section will show you
    how to consume it.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经可以向 Kafka 主题发送数据，下一节将展示如何消费它。
- en: Writing a Kafka consumer in Python
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Python 中编写 Kafka 消费者
- en: 'To create a consumer in Python, you create the consumer pointing to the Kafka
    cluster, select a topic to listen to, and then enter a loop that listens for new
    messages. The code that follows will walk you through how to write a Python consumer:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Python 中创建消费者，你需要创建指向 Kafka 集群的消费者，选择一个要监听的主题，然后进入一个循环，监听新消息。下面的代码将指导你如何编写
    Python 消费者：
- en: 'First, import the `Consumer` library and create the consumer. You will pass
    the IP addresses of your Kafka cluster, the consumer group name – this can be
    anything you want, but if you add multiple consumers to the group, they will need
    the same name, and Kafka will remember the offset where this consumer group stopped
    reading the topic – and lastly, you will pass the offset reset, or where you want
    to start reading:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入 `Consumer` 库并创建消费者。你需要传递 Kafka 集群的 IP 地址、消费者组名称——这可以是任何你想要的名字，但如果你要将多个消费者添加到该组，它们需要相同的名称，并且
    Kafka 会记住这个消费者组停止读取主题的位置——最后，你需要传递偏移量重置，或者你想从哪里开始读取：
- en: '[PRE14]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You can get a list of topics available to subscribe to as well as the number
    of partitions for a particular topic:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以获取可以订阅的主题列表以及特定主题的分区数：
- en: '[PRE15]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once you know which topic you want to consume, you can subscribe to it:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你知道你想要消费哪个主题，你就可以订阅它：
- en: '[PRE16]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To receive messages, create an infinite loop – if you want to listen forever.
    You can always start and stop using the offset to pick up where you left off.
    Call `poll()` to get the message. The result will be one of three things – nothing
    yet, an error, or a message. Using the `if` statements, check for a nothing, an
    error, or decode the message and do something with the data, which in this case
    is to print it. When you are done, close the connection:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要接收消息，创建一个无限循环——如果你想要永远监听。你可以始终使用偏移量来开始和停止，以便从上次离开的地方继续。调用 `poll()` 来获取消息。结果将是以下三种情况之一——还没有，错误，或消息。使用
    `if` 语句检查没有，错误，或解码消息并对数据进行处理，在这种情况下是打印它。当你完成时，关闭连接：
- en: '[PRE17]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The results will be several JSON objects scrolling through the terminal and
    will look like the following output:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将在终端中滚动显示几个 JSON 对象，如下所示：
- en: '[PRE18]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This is a basic example of consuming a topic with Python, but should give you
    an idea of the architecture and how to start building more complex consumers.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个使用 Python 消费主题的基本示例，但应该能给你一个架构和如何开始构建更复杂消费者的想法。
- en: Summary
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned the basics of Apache Kafka – from what is a log
    and how Kafka uses it, to partitions, producers, and consumers. You learned how
    Apache NiFi can create producers and consumers with a single processor. The chapter
    took a quick detour to explain how streaming data is unbounded and how time and
    windowing work with streams. These are important considerations when working with
    streaming data and can result in errors if you assume you have all the data at
    one time. Lastly, you learned how to use Confluent Python Kafka to write basic
    producers and consumers in Python.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了 Apache Kafka 的基础知识——从日志是什么以及 Kafka 如何使用它，到分区、生产者和消费者。你学习了 Apache NiFi
    如何使用单个处理器创建生产者和消费者。本章简要介绍了流数据的无界性以及时间和窗口如何与流一起工作。这些是在处理流数据时的重要考虑因素，如果你假设你一次就有所有数据，可能会导致错误。最后，你学习了如何使用
    Confluent Python Kafka 在 Python 中编写基本的生产者和消费者。
- en: Equipped with these skills, the next chapter will show you how to build a real-time
    data pipeline.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这些技能，下一章将展示如何构建实时数据管道。
