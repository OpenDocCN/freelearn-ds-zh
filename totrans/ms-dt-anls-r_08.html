<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Polishing Data</h1></div></div></div><p>When working with data, you will usually find that it may not always be perfect or clean in the means of missing values, outliers and similar anomalies. Handling and cleaning imperfect or so-called dirty data is part of every data scientist's daily life, and even more, it can take up to 80 percent of the time we actually deal with the data!</p><p>Dataset errors are often due to the inadequate data acquisition methods, but instead of repeating and tweaking the data collection process, it is usually better (in the means of saving money, time and other resources) or unavoidable to polish the data by a few simple functions and algorithms. In this chapter, we will cover:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Different use cases of the <code class="literal">na.rm</code> argument of various functions</li><li class="listitem" style="list-style-type: disc">The <code class="literal">na.action</code> and related functions to get rid of missing data</li><li class="listitem" style="list-style-type: disc">Several packages that offer a user-friendly way of data imputation</li><li class="listitem" style="list-style-type: disc">The <code class="literal">outliers</code> package with several statistical tests for extreme values</li><li class="listitem" style="list-style-type: disc">How to implement Lund's outlier test on our own as a brain teaser</li><li class="listitem" style="list-style-type: disc">Referring to some robust methods</li></ul></div><div><div><div><div><h1 class="title"><a id="ch08lvl1sec53"/>The types and origins of missing data</h1></div></div></div><p>First, we have to<a class="indexterm" id="id564"/> take a quick look at the possible different sources of missing data to identify why and how we usually get missing values. There are quite a few different reasons for data loss, which can be categorized into 3 different types.</p><p>For example, the<a class="indexterm" id="id565"/> main cause of missing data might be a malfunctioning device or the human factor of incorrectly entering data. <strong>Missing Completely at Random</strong> (<strong>MCAR</strong>) means that<a class="indexterm" id="id566"/> every value in the <a class="indexterm" id="id567"/>dataset has the same probability of being missed, so no systematic error or distortion is to be expected due to missing data, and nor can we explain the pattern of missing values. This is the best situation if we have <code class="literal">NA</code> (meaning: no answer, not applicable or not available) values in our data set.</p><p>But a much more frequent and unfortunate type of missing data is <strong>Missing at Random</strong> (<strong>MAR</strong>) compared to <a class="indexterm" id="id568"/>MCAR. In the case of MAR, the pattern of missing values is known or <a class="indexterm" id="id569"/>at least can be identified, although it has nothing to do with the actual missing values. For example, one <a class="indexterm" id="id570"/>might think of a population where males are more loners or lazier compared to females, thus they prefer not to answer all the questions in a survey – regardless of the actual question. So it's not that the males are not giving away their salary due to the fact that they make more or less compared to females, but they tend to skip a few questions in the questionnaire at random.</p><div><div><h3 class="title"><a id="note56"/>Note</h3><p>This classification and typology of missing data was first proposed by Donald B. Rubin in 1976 in his <em>Inference and Missing Data</em>, published in <em>Biometrika 63(3): 581—592</em>, later reviewed and extended in a book jointly written by <em>Roderick J. A. Little</em> (2002): <em>Statistical Analysis with Missing Data</em>, <em>Wiley</em> – which is well worth of reading for further details.</p></div></div><p>And the worst scenario would be<a class="indexterm" id="id571"/> <strong>Missing Not at Random</strong> (<strong>MNAR</strong>), where<a class="indexterm" id="id572"/> data is missing for a specific reason that is highly related to the actual question, which classifies missing values as<a class="indexterm" id="id573"/> nonignorable non-response.</p><p>This happens pretty often in surveys with sensitive questions or due to design flaws in the research preparation. In such cases, data is missing due to some latent process going on in the background, which is often the thing we wanted to come to know better with the help of the research – which can turn out to be a rather cumbersome situation.</p><p>So how can we resolve these problems? Sometimes it's relatively easy. For example, if we have lot of observations, MCAR is not a real problem at all due to the law of large numbers, as<a class="indexterm" id="id574"/> the probability of having missing value(s) is the same for each observation. We basically have two options to deal with unknown or missing data:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Removing missing values and/or observations</li><li class="listitem" style="list-style-type: disc">Replacing missing values with some estimates</li></ul></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec54"/>Identifying missing data</h1></div></div></div><p>The easiest way <a class="indexterm" id="id575"/>of dealing with missing values, especially with MCAR data, is simply removing all the observations with any missing values. If we want to exclude every row of a <code class="literal">matrix</code> or <code class="literal">data.frame</code> object which has at least one missing value, we can use<a class="indexterm" id="id576"/> the <code class="literal">complete.cases</code> function from the <code class="literal">stats</code> package to identify those.</p><p>For a quick start, let's see how many rows have at least one missing value:</p><div><pre class="programlisting"><strong>&gt; library(hflights)</strong>
<strong>&gt; table(complete.cases(hflights))</strong>
<strong> FALSE   TRUE </strong>
<strong>  3622 223874</strong>
</pre></div><p>This is around 1.5 percent of the quarter million rows:</p><div><pre class="programlisting"><strong>&gt; prop.table(table(complete.cases(hflights))) * 100</strong>
<strong>    FALSE      TRUE </strong>
<strong> 1.592116 98.407884</strong>
</pre></div><p>Let's see what the distribution of <code class="literal">NA</code> looks like within different columns:</p><div><pre class="programlisting"><strong>&gt; sort(sapply(hflights, function(x) sum(is.na(x))))</strong>
<strong>             Year             Month        DayofMonth </strong>
<strong>                0                 0                 0 </strong>
<strong>        DayOfWeek     UniqueCarrier         FlightNum </strong>
<strong>                0                 0                 0 </strong>
<strong>          TailNum            Origin              Dest </strong>
<strong>                0                 0                 0 </strong>
<strong>         Distance         Cancelled  CancellationCode </strong>
<strong>                0                 0                 0 </strong>
<strong>         Diverted           DepTime          DepDelay </strong>
<strong>                0              2905              2905 </strong>
<strong>          TaxiOut           ArrTime            TaxiIn </strong>
<strong>             2947              3066              3066 </strong>
<strong>ActualElapsedTime           AirTime          ArrDelay </strong>
<strong>             3622              3622              3622</strong>
</pre></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec55"/>By-passing missing values</h1></div></div></div><p>So it seems that <a class="indexterm" id="id577"/>missing data relatively frequently occurs with the time-related variables, but we have no missing values among the flight identifiers and dates. On the other hand, if one value is missing for a flight, the chances are rather high that some other variables are missing as well – out of the overall number of 3,622 cases with at least one missing value:</p><div><pre class="programlisting"><strong>&gt; mean(cor(apply(hflights, 2, function(x)</strong>
<strong>+    as.numeric(is.na(x)))), na.rm = TRUE)</strong>
<strong>[1] 0.9589153</strong>
<strong>Warning message:</strong>
<strong>In cor(apply(hflights, 2, function(x) as.numeric(is.na(x)))) :</strong>
<strong>  the standard deviation is zero</strong>
</pre></div><p>Okay, let's see what we have done here! First, we have called the <code class="literal">apply</code> function to transform the values of <code class="literal">data.frame</code> to <code class="literal">0</code> or <code class="literal">1</code>, where <code class="literal">0</code> stands for an observed, while <code class="literal">1</code> means a missing value. Then we computed the correlation coefficients of this newly created matrix, which of course returned a lot of missing values due to fact that some columns had only one unique value without any variability, as shown in the warning message. For this, we had to specify the <code class="literal">na.rm</code> parameter to be <code class="literal">TRUE</code>, so that the <code class="literal">mean</code> function would<a class="indexterm" id="id578"/> return a real value instead of an <code class="literal">NA</code>, by removing the missing values among the correlation coefficients returned by the <code class="literal">cor</code> function.</p><p>So one option is the heavy use of the <code class="literal">na.rm</code> argument, which is supported by most functions that are sensitive to missing data—to name a few from the <code class="literal">base</code> and <code class="literal">stats</code> packages: <code class="literal">mean</code>, <code class="literal">median</code>, <code class="literal">sum</code>, <code class="literal">max</code> and <code class="literal">min</code>.</p><p>To compile the complete list of functions that have the <code class="literal">na.rm</code> argument in the base package, we can follow the steps described in a very interesting SO answer located at <a class="ulink" href="http://stackoverflow.com/a/17423072/564164">http://stackoverflow.com/a/17423072/564164</a>. I found this answer motivating because I truly believe in the power of analyzing the tools we use for analysis, or in other words, spending some time on understanding how R works in the background.</p><p>First, let's make a list of all the functions found in <code class="literal">baseenv</code> (the environment of the <code class="literal">base</code> package) along with the complete function arguments and body:</p><div><pre class="programlisting"><strong>&gt; Funs &lt;- Filter(is.function, sapply(ls(baseenv()), get, baseenv()))</strong>
</pre></div><p>Then we can <code class="literal">Filter</code> all those functions from the returned list, which have <code class="literal">na.rm</code> among the formal arguments via the following:</p><div><pre class="programlisting"><strong>&gt; names(Filter(function(x)</strong>
<strong>+    any(names(formals(args(x))) %in% 'na.rm'), Funs))</strong>
<strong> [1] "all"                     "any"                    </strong>
<strong> [3] "colMeans"                "colSums"                </strong>
<strong> [5] "is.unsorted"             "max"                    </strong>
<strong> [7] "mean.default"            "min"                    </strong>
<strong> [9] "pmax"                    "pmax.int"               </strong>
<strong>[11] "pmin"                    "pmin.int"               </strong>
<strong>[13] "prod"                    "range"                  </strong>
<strong>[15] "range.default"           "rowMeans"               </strong>
<strong>[17] "rowsum.data.frame"       "rowsum.default"         </strong>
<strong>[19] "rowSums"                 "sum"                    </strong>
<strong>[21] "Summary.data.frame"      "Summary.Date"           </strong>
<strong>[23] "Summary.difftime"        "Summary.factor"         </strong>
<strong>[25] "Summary.numeric_version" "Summary.ordered"        </strong>
<strong>[27] "Summary.POSIXct"         "Summary.POSIXlt"  </strong>
</pre></div><p>This can be easily applied to any R package by changing the environment variable to for example <code class="literal">'package:stats'</code> in the case of the <code class="literal">stats</code> package:</p><div><pre class="programlisting"><strong>&gt; names(Filter(function(x)</strong>
<strong>+   any(names(formals(args(x))) %in% 'na.rm'),</strong>
<strong>+     Filter(is.function,</strong>
<strong>+       sapply(ls('package:stats'), get, 'package:stats'))))</strong>
<strong> [1] "density.default" "fivenum"         "heatmap"        </strong>
<strong> [4] "IQR"             "mad"             "median"         </strong>
<strong> [7] "median.default"  "medpolish"       "sd"             </strong>
<strong>[10] "var"                </strong>
</pre></div><p>So these are the<a class="indexterm" id="id579"/> functions that have the <code class="literal">na.rm</code> argument in the <code class="literal">base</code> and the <code class="literal">stats</code> packages, where we have seen that the fastest and easiest way of ignoring missing values in single function calls (without actually removing the <code class="literal">NA</code> values from the dataset) is setting <code class="literal">na.rm</code> to <code class="literal">TRUE</code>. But why doesn't <code class="literal">na.rm</code> default to <code class="literal">TRUE</code>?</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec48"/>Overriding the default arguments of a function</h2></div></div></div><p>If you are <a class="indexterm" id="id580"/>annoyed by the fact that most functions return <code class="literal">NA</code> if your R object includes missing values, then you can override those by using some custom wrapper functions, such as:</p><div><pre class="programlisting"><strong>&gt; myMean &lt;- function(...) mean(..., na.rm = TRUE)</strong>
<strong>&gt; mean(c(1:5, NA))</strong>
<strong>[1] NA</strong>
<strong>&gt; myMean(c(1:5, NA))</strong>
<strong>[1] 3</strong>
</pre></div><p>Another option might be to write a custom package which would override the factory defaults of the <code class="literal">base</code> and <code class="literal">stats</code> function, like in the<a class="indexterm" id="id581"/> <code class="literal">rapportools</code> package, which includes miscellaneous helper functions with sane defaults for reporting:</p><div><pre class="programlisting"><strong>&gt; library(rapportools)</strong>
<strong>Loading required package: reshape</strong>

<strong>Attaching package: 'rapportools'</strong>

<strong>The following objects are masked from 'package:stats':</strong>

<strong>    IQR, median, sd, var</strong>

<strong>The following objects are masked from 'package:base':</strong>

<strong>    max, mean, min, range, sum</strong>

<strong>&gt; mean(c(1:5, NA))</strong>
<strong>[1] 3</strong>
</pre></div><p>The problem with this<a class="indexterm" id="id582"/> approach is that you've just permanently overridden those functions listed, so you'll need to restart your R session or detach the <code class="literal">rapportools</code> package to reset to the standard arguments, like:</p><div><pre class="programlisting"><strong>&gt; detach('package:rapportools')</strong>
<strong>&gt; mean(c(1:5, NA))</strong>
<strong>[1] NA</strong>
</pre></div><p>A more general solution to override the default arguments of a function is to rely on some nifty features<a class="indexterm" id="id583"/> of the <code class="literal">Defaults</code> package, which is although not under active maintenance, but it does the job:</p><div><pre class="programlisting"><strong>&gt; library(Defaults)</strong>
<strong>&gt; setDefaults(mean.default, na.rm = TRUE)</strong>
<strong>&gt; mean(c(1:5, NA))</strong>
<strong>[1] 3</strong>
</pre></div><p>Please note that here we had to update the default argument value of <code class="literal">mean.default</code> instead of simply trying to tweak <code class="literal">mean</code>, as that latter would result in an error:</p><div><pre class="programlisting"><strong>&gt; setDefaults(mean, na.rm = TRUE)</strong>
<strong>Warning message:</strong>
<strong>In setDefaults(mean, na.rm = TRUE) :</strong>
<strong>  'na.rm' was not set, possibly not a formal arg for 'mean'</strong>
</pre></div><p>This is due to the fact that <code class="literal">mean</code> is an <code class="literal">S3</code> method without any formal arguments:</p><div><pre class="programlisting"><strong>&gt; mean</strong>
<strong>function (x, ...) </strong>
<strong>{</strong>
<strong>    if (exists(".importDefaults")) </strong>
<strong>        .importDefaults(calling.fun = "mean")</strong>
<strong>    UseMethod("mean")</strong>
<strong>}</strong>
<strong>&lt;environment: namespace:base&gt;</strong>
<strong>&gt; formals(mean)</strong>
<strong>$x</strong>

<strong>$...</strong>
</pre></div><p>Either methods you prefer, you can automatically call those functions when R starts by adding a few lines of code in your<a class="indexterm" id="id584"/> <code class="literal">Rprofile</code> file.</p><div><div><h3 class="title"><a id="note57"/>Note</h3><p>You can <a class="indexterm" id="id585"/>customize the R environment via a global or user-specific <code class="literal">Rprofile</code> file. This is a normal R script which is usually placed in the user's home directory with a leading dot in the file name, which is run every time a new R session is started. There you can call any R functions wrapped in the <code class="literal">.First</code> or <code class="literal">.Last</code> functions to be run at the start or at the end of the R session. Such useful additions might be loading some R packages, printing custom greetings or KPI metrics from a database, or for example installing the most recent versions of all R packages.</p></div></div><p>But it's probably better not to tweak your R environment in such a non-standard way, as you might soon experience some esoteric and unexpected errors or silent malfunctions in your analysis.</p><p>For example, I've got used to working in a temporary directory at all times by specifying <code class="literal">setwd('/tmp')</code> in my <code class="literal">Rprofile</code>, which is very useful if you start R sessions frequently for some quick jobs. On the other hand, it's really frustrating to spend 15 minutes of your life debugging why some random R function does not seem to do its job, and why it's returning some file not found error messages instead.</p><p>So please be warned: if you update the factory default arguments of R functions, do not ever think of ranting about some new bugs you have found in some major functions of base R on the R mailing lists, before trying<a class="indexterm" id="id586"/> to reproduce those errors in a vanilla R session with starting R with the --<code class="literal">vanilla</code> command line option.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec56"/>Getting rid of missing data</h1></div></div></div><p>An alternative way of<a class="indexterm" id="id587"/> using the <code class="literal">na.rm</code> argument in R functions is removing <code class="literal">NA</code> from the dataset before passing that to the analysis functions. This means that we are removing the missing values from the dataset permanently, so that they won't cause any problems at later stages in the analysis. For this, we could use either the <code class="literal">na.omit</code> or the <code class="literal">na.exclude</code> functions:</p><div><pre class="programlisting"><strong>&gt; na.omit(c(1:5, NA))</strong>
<strong>[1] 1 2 3 4 5</strong>
<strong>attr(,"na.action")</strong>
<strong>[1] 6</strong>
<strong>attr(,"class")</strong>
<strong>[1] "omit"</strong>
<strong>&gt; na.exclude(c(1:5, NA))</strong>
<strong>[1] 1 2 3 4 5</strong>
<strong>attr(,"na.action")</strong>
<strong>[1] 6</strong>
<strong>attr(,"class")</strong>
<strong>[1] "exclude"</strong>
</pre></div><p>The only difference between these two functions is the class of the <code class="literal">na.action</code> attribute of the returned R object, which are <code class="literal">omit</code> and <code class="literal">exclude</code> respectively. This minor difference is only important when modelling. The <code class="literal">na.exclude</code> function returns <code class="literal">NA</code> for residuals and predictions, while <code class="literal">na.omit</code> suppresses those elements of the vector:</p><div><pre class="programlisting"><strong>&gt; x &lt;- rnorm(10); y &lt;- rnorm(10)</strong>
<strong>&gt; x[1] &lt;- NA; y[2] &lt;- NA</strong>
<strong>&gt; exclude &lt;- lm(y ~ x, na.action = "na.exclude")</strong>
<strong>&gt; omit &lt;- lm(y ~ x, na.action = "na.omit")</strong>
<strong>&gt; residuals(exclude)</strong>
<strong>    1     2     3     4     5     6     7     8     9    10 </strong>
<strong>   NA    NA -0.89 -0.98  1.45 -0.23  3.11 -0.23 -1.04 -1.20 </strong>

<strong>&gt; residuals(omit)</strong>
<strong>    3     4     5     6     7     8     9    10 </strong>
<strong>-0.89 -0.98  1.45 -0.23  3.11 -0.23 -1.04 -1.20</strong>
</pre></div><p>Important thing to note in case of tabular data, like a <code class="literal">matrix</code> or <code class="literal">data.frame</code>, these functions remove the whole row if it contains at least one missing value. For a quick demo, let's create a matrix with 3 columns and 3 rows with values incrementing from 1 to 9, but replacing all values divisible by 4 with <code class="literal">NA</code>:</p><div><pre class="programlisting"><strong>&gt; m &lt;- matrix(1:9, 3)</strong>
<strong>&gt; m[which(m %% 4 == 0, arr.ind = TRUE)] &lt;- NA</strong>
<strong>&gt; m</strong>
<strong>     [,1] [,2] [,3]</strong>
<strong>[1,]    1   NA    7</strong>
<strong>[2,]    2    5   NA</strong>
<strong>[3,]    3    6    9</strong>
<strong>&gt; na.omit(m)</strong>
<strong>     [,1] [,2] [,3]</strong>
<strong>[1,]    3    6    9</strong>
<strong>attr(,"na.action")</strong>
<strong>[1] 1 2</strong>
<strong>attr(,"class")</strong>
<strong>[1] "omit"</strong>
</pre></div><p>As seen here, we can<a class="indexterm" id="id588"/> find the row numbers of the removed cases in the <code class="literal">na.action</code> attribute.</p></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec57"/>Filtering missing data before or during the actual analysis</h1></div></div></div><p>Let's suppose we want to calculate the <code class="literal">mean</code> of the actual length of flights:</p><div><pre class="programlisting"><strong>&gt; mean(hflights$ActualElapsedTime)</strong>
<strong>[1] NA</strong>
</pre></div><p>The result is <code class="literal">NA</code> of <a class="indexterm" id="id589"/>course, because as identified previously, this variable<a class="indexterm" id="id590"/> contains missing values, and almost every R operation with <code class="literal">NA</code> results in <code class="literal">NA</code>. So let's overcome this issue as follows:</p><div><pre class="programlisting"><strong>&gt; mean(hflights$ActualElapsedTime, na.rm = TRUE)</strong>
<strong>[1] 129.3237</strong>
<strong>&gt; mean(na.omit(hflights$ActualElapsedTime))</strong>
<strong>[1] 129.3237</strong>
</pre></div><p>Any performance issues there? Or other means of deciding which method to use?</p><div><pre class="programlisting"><strong>&gt; library(microbenchmark)</strong>
<strong>&gt; NA.RM   &lt;- function()</strong>
<strong>+              mean(hflights$ActualElapsedTime, na.rm = TRUE)</strong>
<strong>&gt; NA.OMIT &lt;- function()</strong>
<strong>+              mean(na.omit(hflights$ActualElapsedTime))</strong>
<strong>&gt; microbenchmark(NA.RM(), NA.OMIT())</strong>
<strong>Unit: milliseconds</strong>
<strong>      expr       min        lq    median        uq       max neval</strong>
<strong>   NA.RM()  7.105485  7.231737  7.500382  8.002941  9.850411   100</strong>
<strong> NA.OMIT() 12.268637 12.471294 12.905777 13.376717 16.008637   100</strong>
</pre></div><p>The first glance at the performance of these options computed with the help of the <code class="literal">microbenchmark</code> package (please see the <em>Loading text files of reasonable size</em> section in the <a class="link" href="ch01.html" title="Chapter 1. Hello, Data!">Chapter 1</a>, <em>Hello Data</em> for more details) suggests that using <code class="literal">na.rm</code> is the better solution in case of a single function call.</p><p>On the other hand, if we want<a class="indexterm" id="id591"/> to reuse the data at some later<a class="indexterm" id="id592"/> phase in the analysis, it is more viable and effective to omit the missing values and observations only once from the dataset, instead of always specifying <code class="literal">na.rm</code> to be <code class="literal">TRUE</code>.</p></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec58"/>Data imputation</h1></div></div></div><p>And sometimes omitting missing values is not reasonable or possible at all, for example due to the low number <a class="indexterm" id="id593"/>of observations or if it seems that missing data is not random. Data imputation is a real alternative in such situations, and this method can replace <code class="literal">NA</code> with some real values based on various algorithms, such as filling empty cells with:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A known scalar</li><li class="listitem" style="list-style-type: disc">The previous value appearing in the column (hot-deck)</li><li class="listitem" style="list-style-type: disc">A random element from the same column</li><li class="listitem" style="list-style-type: disc">The most frequent value in the column</li><li class="listitem" style="list-style-type: disc">Different values from the same column with given probability</li><li class="listitem" style="list-style-type: disc">Predicted values based on regression or machine learning models</li></ul></div><p>The hot-deck<a class="indexterm" id="id594"/> method is often used while joining multiple datasets together. In such a situation, the <code class="literal">roll</code> argument of <code class="literal">data.table</code> can be very useful and efficient, otherwise be sure to check out the <code class="literal">hotdeck</code> function in the<a class="indexterm" id="id595"/> <code class="literal">VIM</code> package, which offers some really useful ways of visualizing missing data. But when dealing with an already given column of a dataset, we have some other simple options as well.</p><p>For instance, imputing a known scalar is a pretty simple situation, where we know that all missing values are for example due to some research design patterns. Let's think of a database that stores the time you arrived to and left the office every weekday, and by computing the difference between those two, we can analyze the number of work hours spent in the office from day to day. If this variable returns <code class="literal">NA</code> for a time period, actually it means that we were<a class="indexterm" id="id596"/> outside of the office all day, so thus the computed value should be zero instead of <code class="literal">NA</code>.</p><p>And not just in theory, but this is pretty easy to implement in R as well (example is continued from the previous demo code where we defined <code class="literal">m</code> with two missing values):</p><div><pre class="programlisting"><strong>&gt; m[which(is.na(m), arr.ind = TRUE)] &lt;- 0</strong>
<strong>&gt; m</strong>
<strong>     [,1] [,2] [,3]</strong>
<strong>[1,]    1    0    7</strong>
<strong>[2,]    2    5    0</strong>
<strong>[3,]    3    6    9</strong>
</pre></div><p>Similarly, replacing missing values with a random number, a <code class="literal">sample</code> of other values or with the <code class="literal">mean</code> of a variable can be done relatively easily:</p><div><pre class="programlisting"><strong>&gt; ActualElapsedTime &lt;- hflights$ActualElapsedTime</strong>
<strong>&gt; mean(ActualElapsedTime, na.rm = TRUE)</strong>
<strong>[1] 129.3237</strong>
<strong>&gt; ActualElapsedTime[which(is.na(ActualElapsedTime))] &lt;-</strong>
<strong>+   mean(ActualElapsedTime, na.rm = TRUE)</strong>
<strong>&gt; mean(ActualElapsedTime)</strong>
<strong>[1] 129.3237</strong>
</pre></div><p>Which can be even easier with the <code class="literal">impute</code> function from the<a class="indexterm" id="id597"/> <code class="literal">Hmisc</code> package:</p><div><pre class="programlisting"><strong>&gt; library(Hmisc)</strong>
<strong>&gt; mean(impute(hflights$ActualElapsedTime, mean))</strong>
<strong>[1] 129.3237</strong>
</pre></div><p>It seems that we have preserved the value of the arithmetic mean of course, but you should be aware of some very serious side-effects:</p><div><pre class="programlisting"><strong>&gt; sd(hflights$ActualElapsedTime, na.rm = TRUE)</strong>
<strong>[1] 59.28584</strong>
<strong>&gt; sd(ActualElapsedTime)</strong>
<strong>[1] 58.81199</strong>
</pre></div><p>When replacing missing values with the mean, the variance of the transformed variable will be naturally lower <a class="indexterm" id="id598"/>compared to the original distribution. This can be extremely problematic in some situations, where some more sophisticated methods are needed.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec49"/>Modeling missing values</h2></div></div></div><p>Besides the previous<a class="indexterm" id="id599"/> mentioned univariate methods, you may also fit models on the complete cases in the dataset, rather than fitting those models on the remaining rows to estimate the missing values. Or in a nutshell, we are replacing the missing values with multivariate predictions.</p><p>There are a plethora of related functions and packages, for example you might be interested in checking the <code class="literal">transcan</code> function in the <code class="literal">Hmisc</code> package<a class="indexterm" id="id600"/>, or the <code class="literal">imputeR</code> package, which<a class="indexterm" id="id601"/> includes a wide variety of models for imputing categorical and continuous variables as well.</p><p>Most of the imputation methods and models are for one type of variable: either continuous or categorical. In case of mixed-type dataset, we typically use different algorithms to handle the different types of missing data. The problem with this approach is that some of the possible relations between different types of data might be ignored, resulting in some partial models.</p><p>To overcome this issue, and to save a few pages in the book on the description of the traditional regression and other related methods for data imputation (although you can find some related methods in the <a class="link" href="ch05.html" title="Chapter 5. Building Models (authored by Renata Nemeth and Gergely Toth)">Chapter 5</a>, <em>Buildings Models (authored by Renata Nemeth and Gergely Toth)</em> and the <a class="link" href="ch06.html" title="Chapter 6. Beyond the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)">Chapter 6</a>, <em>Beyond the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)</em>), we will concentrate on a non-parametric method that can handle categorical and continuous variables at the same time via a very user-friendly interface in the<a class="indexterm" id="id602"/> <code class="literal">missForest</code> package.</p><p>This iterative procedure fits a random forest model on the available data in order to predict the missing values. As our <code class="literal">hflights</code> data is relatively large for such a process and running the sample code would takes ages, we will rather use the standard <code class="literal">iris</code> dataset in the next examples.</p><p>First let's see the original structure of the dataset, which does not include any missing values:</p><div><pre class="programlisting"><strong>&gt; summary(iris)</strong>
<strong>  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   </strong>
<strong> Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  </strong>
<strong> 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  </strong>
<strong> Median :5.800   Median :3.000   Median :4.350   Median :1.300  </strong>
<strong> Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  </strong>
<strong> 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  </strong>
<strong> Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  </strong>
<strong>       Species  </strong>
<strong> setosa    :50  </strong>
<strong> versicolor:50  </strong>
<strong> virginica :50  </strong>
</pre></div><p>Now let's load the package<a class="indexterm" id="id603"/> and add some missing values (completely at random) to the dataset in the means of producing a reproducible minimal example for the forthcoming models:</p><div><pre class="programlisting"><strong>&gt; library(missForest)</strong>
<strong>&gt; set.seed(81)</strong>
<strong>&gt; miris &lt;- prodNA(iris, noNA = 0.2)</strong>
<strong>&gt; summary(miris)</strong>
<strong>  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   </strong>
<strong> Min.   :4.300   Min.   :2.000   Min.   :1.100   Min.   :0.100  </strong>
<strong> 1st Qu.:5.200   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  </strong>
<strong> Median :5.800   Median :3.000   Median :4.450   Median :1.300  </strong>
<strong> Mean   :5.878   Mean   :3.062   Mean   :3.905   Mean   :1.222  </strong>
<strong> 3rd Qu.:6.475   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.900  </strong>
<strong> Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  </strong>
<strong> NA's   :28      NA's   :29      NA's   :32      NA's   :33     </strong>
<strong>       Species  </strong>
<strong> setosa    :40  </strong>
<strong> versicolor:38  </strong>
<strong> virginica :44  </strong>
<strong> NA's      :28  </strong>
</pre></div><p>So now we have around 20 percent of missing values in each column, which is also stated in the bottom row of the preceding summary. The number of completely random missing values is between 28 and 33 cases per variable.</p><p>The next step should be building the random forest models to replace the missing values with real numbers and factor levels. As we also have the original dataset, we can use that complete matrix to test the performance of the method via the <code class="literal">xtrue</code> argument, which computes and returns the error rate when we call the function with <code class="literal">verbose</code>. This is useful in such didactical examples to show how the model and predictions improves from iteration to iteration:</p><div><pre class="programlisting"><strong>&gt; iiris &lt;- missForest(miris, xtrue = iris, verbose = TRUE)</strong>
<strong>  missForest iteration 1 in progress...done!</strong>
<strong>    error(s): 0.1512033 0.03571429 </strong>
<strong>    estimated error(s): 0.1541084 0.04098361 </strong>
<strong>    difference(s): 0.01449533 0.1533333 </strong>
<strong>    time: 0.124 seconds</strong>

<strong>  missForest iteration 2 in progress...done!</strong>
<strong>    error(s): 0.1482248 0.03571429 </strong>
<strong>    estimated error(s): 0.1402145 0.03278689 </strong>
<strong>    difference(s): 9.387853e-05 0 </strong>
<strong>    time: 0.114 seconds</strong>

<strong>  missForest iteration 3 in progress...done!</strong>
<strong>    error(s): 0.1567693 0.03571429 </strong>
<strong>    estimated error(s): 0.1384038 0.04098361 </strong>
<strong>    difference(s): 6.271654e-05 0 </strong>
<strong>    time: 0.152 seconds</strong>

<strong>  missForest iteration 4 in progress...done!</strong>
<strong>    error(s): 0.1586195 0.03571429 </strong>
<strong>    estimated error(s): 0.1419132 0.04918033 </strong>
<strong>    difference(s): 3.02275e-05 0 </strong>
<strong>    time: 0.116 seconds</strong>

<strong>  missForest iteration 5 in progress...done!</strong>
<strong>    error(s): 0.1574789 0.03571429 </strong>
<strong>    estimated error(s): 0.1397179 0.04098361 </strong>
<strong>    difference(s): 4.508345e-05 0 </strong>
<strong>    time: 0.114 seconds</strong>
</pre></div><p>The algorithm ran for 5<a class="indexterm" id="id604"/> iterations before stopping, when it seemed that the error rate was not improving any further. The returned <code class="literal">missForest</code> object includes a few other values besides the imputed dataset:</p><div><pre class="programlisting"><strong>&gt; str(iiris)</strong>
<strong>List of 3</strong>
<strong> $ ximp    :'data.frame':  150 obs. of  5 variables:</strong>
<strong>  ..$ Sepal.Length: num [1:150] 5.1 4.9 4.7 4.6 5 ...</strong>
<strong>  ..$ Sepal.Width : num [1:150] 3.5 3.3 3.2 3.29 3.6 ...</strong>
<strong>  ..$ Petal.Length: num [1:150] 1.4 1.4 1.3 1.42 1.4 ...</strong>
<strong>  ..$ Petal.Width : num [1:150] 0.2 0.218 0.2 0.2 0.2 ...</strong>
<strong>  ..$ Species     : Factor w/ 3 levels "setosa","versicolor",..: ...</strong>
<strong> $ OOBerror: Named num [1:2] 0.1419 0.0492</strong>
<strong>  ..- attr(*, "names")= chr [1:2] "NRMSE" "PFC"</strong>
<strong> $ error   : Named num [1:2] 0.1586 0.0357</strong>
<strong>  ..- attr(*, "names")= chr [1:2] "NRMSE" "PFC"</strong>
<strong> - attr(*, "class")= chr "missForest"</strong>
</pre></div><p>The Out of Box error is <a class="indexterm" id="id605"/>an estimate on how good our model<a class="indexterm" id="id606"/> was based on the <strong>normalized root mean squared error computed</strong> (<strong>NRMSE</strong>) for numeric values and the <strong>proportion of falsely classified</strong> (<strong>PFC</strong>) entries for factors. And as we also provided the complete dataset<a class="indexterm" id="id607"/> for the previously run model, we also get the true imputation error ratio – which is pretty close to the above estimates.</p><div><div><h3 class="title"><a id="note58"/>Note</h3><p>Please find more details on random forests and related machine learning topics in the <a class="link" href="ch10.html" title="Chapter 10. Classification and Clustering">Chapter 10</a>, <em>Classification and Clustering</em>.</p></div></div><p>But how does this approach compare to a much simpler imputation method, like replacing missing values with the mean?</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec50"/>Comparing different imputation methods</h2></div></div></div><p>In the comparison, only<a class="indexterm" id="id608"/> the first four columns of the <code class="literal">iris</code> dataset will be used, thus it is not dealing with the factor variable at the moment. Let's prepare this demo dataset:</p><div><pre class="programlisting"><strong>&gt; miris &lt;- miris[, 1:4]</strong>
</pre></div><p>In <code class="literal">iris_mean</code>, we replace all the missing values to the mean of the actual columns:</p><div><pre class="programlisting"><strong>&gt; iris_mean &lt;- impute(miris, fun = mean)</strong>
</pre></div><p>And in <code class="literal">iris_forest</code>, we predict the missing values by fitting random forest model:</p><div><pre class="programlisting"><strong>&gt; iris_forest &lt;- missForest(miris)</strong>
<strong>  missForest iteration 1 in progress...done!</strong>
<strong>  missForest iteration 2 in progress...done!</strong>
<strong>  missForest iteration 3 in progress...done!</strong>
<strong>  missForest iteration 4 in progress...done!</strong>
<strong>  missForest iteration 5 in progress...done!</strong>
</pre></div><p>Now let's simply check the accuracy of the two models by comparing the correlations of <code class="literal">iris_mean</code> and <code class="literal">iris_forest</code> with the complete <code class="literal">iris</code> dataset. For <code class="literal">iris_forest</code>, we will extract the actual imputed dataset from the <code class="literal">ximp</code> attribute, and we will silently ignore the factor variable of the original <code class="literal">iris</code> table:</p><div><pre class="programlisting"><strong>&gt; diag(cor(iris[, -5], iris_mean))</strong>
<strong>Sepal.Length  Sepal.Width Petal.Length  Petal.Width </strong>
<strong>   0.6633507    0.8140169    0.8924061    0.4763395 </strong>
<strong>&gt; diag(cor(iris[, -5], iris_forest$ximp))</strong>
<strong>Sepal.Length  Sepal.Width Petal.Length  Petal.Width </strong>
<strong>   0.9850253    0.9320711    0.9911754    0.9868851</strong>
</pre></div><p>These results<a class="indexterm" id="id609"/> suggest that the nonparametric random forest model did a lot better job compared to the simple univariate solution of replacing missing values with the mean.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec51"/>Not imputing missing values</h2></div></div></div><p>Please note that these methods have their drawbacks likewise. Replacing the missing values with a predicted one <a class="indexterm" id="id610"/>often lacks any error term and residual variance with most models.</p><p>This also means that we are lowering the variability, and overestimating some association in the dataset at the same time, which can seriously affect the results of our data analysis. For this, some simulation techniques were introduced in the past to overcome the problem of distorting the dataset and our hypothesis tests with some arbitrary models.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec52"/>Multiple imputation</h2></div></div></div><p>The basic idea behind <a class="indexterm" id="id611"/>multiple imputation is to fit models several times in a row on the missing values. This Monte Carlo method usually creates some (like 3 to 10) parallel versions of the simulated complete dataset, each of these is analyzed separately, and then we combine the results to produce the actual estimates and confidence intervals. See for example the <code class="literal">aregImpute</code> function from the <code class="literal">Hmisc</code> package for more details.</p><p>On the other hand, do we really have to remove or impute missing values in all cases? For more details on this question, please see the last section of this chapter. But before that, let's get to know some other requirements for polishing data.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec59"/>Extreme values and outliers</h1></div></div></div><p>An outlier or extreme value is defined as a data point that deviates so far from the other observations, that it <a class="indexterm" id="id612"/>becomes suspicious to be generated by a totally different mechanism or simply by error. Identifying outliers is important because those extreme values can:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Increase error variance</li><li class="listitem" style="list-style-type: disc">Influence estimates</li><li class="listitem" style="list-style-type: disc">Decrease normality</li></ul></div><p>Or in other words, let's say your raw dataset is a piece of rounded stone to be used as a perfect ball in some game, which has to be cleaned and polished before actually using it. The stone has some small holes on its surface, like missing values in the data, which should be filled – with data imputation.</p><p>On the other hand, the stone does not only has holes on its surface, but some mud also covers some parts of the item, which is to be removed. But how can we distinguish mud from the real stone? In this section, we will focus on what the<a class="indexterm" id="id613"/> <code class="literal">outliers</code> package and some related methods have to offer for identifying extreme values.</p><p>As this package has some conflicting function names with the <code class="literal">randomForest</code> package (automatically loaded by<a class="indexterm" id="id614"/> the <code class="literal">missForest</code> package), it's wise to detach the latter before heading to the following examples:</p><div><pre class="programlisting"><strong>&gt; detach('package:missForest')</strong>
<strong>&gt; detach('package:randomForest')</strong>
</pre></div><p>The <code class="literal">outlier</code> function returns the value with the largest difference from the mean, which, contrary to its name, not necessarily have to be an outlier. Instead, the function can be used to give the analyst an idea about which values can be outliers:</p><div><pre class="programlisting"><strong>&gt; library(outliers)</strong>
<strong>&gt; outlier(hflights$DepDelay)</strong>
<strong>[1] 981</strong>
</pre></div><p>So there was a flight with more than 16 hours of delay before actually taking off! This is impressive, isn't it? Let's see if it's normal to be so late:</p><div><pre class="programlisting"><strong>&gt; summary(hflights$DepDelay)</strong>
<strong>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's </strong>
<strong>-33.000  -3.000   0.000   9.445   9.000 981.000    2905</strong>
</pre></div><p>Well, <code class="literal">mean</code> is around 10 <a class="indexterm" id="id615"/>minutes, but as it's even larger than the third quarter and the <code class="literal">median</code> is zero, it's not that hard to guess that the relatively large mean is due to some extreme values:</p><div><pre class="programlisting"><strong>&gt; library(lattice)</strong>
<strong>&gt; bwplot(hflights$DepDelay)</strong>
</pre></div><div><img alt="Extreme values and outliers" src="img/2028OS_08_01.jpg"/></div><p>The preceding boxplot clearly shows that most flights were delayed by only a few minutes, and the interquartile range is around 10 minutes:</p><div><pre class="programlisting"><strong>&gt; IQR(hflights$DepDelay, na.rm = TRUE)</strong>
<strong>[1] 12</strong>
</pre></div><p>All the blue circles in the preceding image are the whiskers are possible extreme values, as being higher than the 1.5 IQR of the upper quartile. But how can we (statistically) test a value?</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec53"/>Testing extreme values</h2></div></div></div><p>The <code class="literal">outliers</code> package<a class="indexterm" id="id616"/> comes with several bundled extreme value detection <a class="indexterm" id="id617"/>algorithms, like:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Dixon's Q test (<code class="literal">dixon.test</code>)</li><li class="listitem" style="list-style-type: disc">Grubb's test (<code class="literal">grubbs.test</code>)</li><li class="listitem" style="list-style-type: disc">Outlying and inlying variance (<code class="literal">cochran.test</code>)</li><li class="listitem" style="list-style-type: disc">Chi-squared test (<code class="literal">chisq.out.test</code>)</li></ul></div><p>These functions are extremely easy to use. Just pass a vector to the statistical tests and the returning p-value of the significance test will clearly indicate if the data has any outliers. For example, let's test 10 random numbers between 0 and 1 against a relatively large number to verify it's an extreme value in this small sample:</p><div><pre class="programlisting"><strong>&gt; set.seed(83)</strong>
<strong>&gt; dixon.test(c(runif(10), pi))</strong>

<strong>  Dixon test for outliers</strong>

<strong>data:  c(runif(10), pi)</strong>
<strong>Q = 0.7795, p-value &lt; 2.2e-16</strong>
<strong>alternative hypothesis: highest value 3.14159265358979 is an outlier</strong>
</pre></div><p>But unfortunately, we <a class="indexterm" id="id618"/>cannot use these convenient functions in our live dataset, as the methods assume normal distribution, which is definitely not true in our cases as we all know from experience: flights tend to be late more often than arriving a lot sooner to their destinations.</p><p>For this, we should use some more robust methods, such as the <a class="indexterm" id="id619"/>
<code class="literal">mvoutlier</code> package, or some very simple approaches like Lund suggested around 40 years ago. This test basically computes the distance of each value from the mean with the help of a very simple linear regression:</p><div><pre class="programlisting"><strong>&gt; model &lt;- lm(hflights$DepDelay ~ 1)</strong>
</pre></div><p>Just to verify we are now indeed measuring the distance from the mean:</p><div><pre class="programlisting"><strong>&gt; model$coefficients</strong>
<strong>(Intercept) </strong>
<strong>   9.444951 </strong>
<strong>&gt; mean(hflights$DepDelay, na.rm = TRUE)</strong>
<strong>[1] 9.444951</strong>
</pre></div><p>Now let's compute the critical value based on the F distribution and two helper variables (where <code class="literal">a</code> stands for the alpha value and <code class="literal">n</code> represents the number of cases):</p><div><pre class="programlisting"><strong>&gt; a &lt;- 0.1</strong>
<strong>&gt; (n &lt;- length(hflights$DepDelay))</strong>
<strong>[1] 227496</strong>
<strong>&gt; (F &lt;- qf(1 - (a/n), 1, n-2, lower.tail = TRUE))</strong>
<strong>[1] 25.5138</strong>
</pre></div><p>Which can be passed to Lund's formula:</p><div><pre class="programlisting"><strong>&gt; (L &lt;- ((n - 1) * F / (n - 2 + F))^0.5)</strong>
<strong>[1] 5.050847</strong>
</pre></div><p>Now let's see how many values have a higher standardized residual than this computed critical value:</p><div><pre class="programlisting"><strong>&gt; sum(abs(rstandard(model)) &gt; L)</strong>
<strong>[1] 1684</strong>
</pre></div><p>But do we really have to remove these outliers from our data? Aren't extreme values normal? Sometimes these<a class="indexterm" id="id620"/> artificial edits in the raw data, like imputing missing values or removing outliers, makes more trouble than it's worth.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec60"/>Using robust methods</h1></div></div></div><p>Fortunately, there are<a class="indexterm" id="id621"/> some robust methods for analyzing datasets, which are generally less sensitive to extreme values. These robust statistical methods have been developed since 1960, but there are some well-known related methods from even earlier, like using the median instead of the mean as a central tendency. Robust methods are often used when the underlying distribution of our data is not considered to follow the Gaussian curve, so most good old regression models do not work (see more details in the <a class="link" href="ch05.html" title="Chapter 5. Building Models (authored by Renata Nemeth and Gergely Toth)">Chapter 5</a>, <em>Buildings Models (authored by Renata Nemeth and Gergely Toth)</em> and the <a class="link" href="ch06.html" title="Chapter 6. Beyond the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)">Chapter 6</a>, <em>Beyond the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)</em>).</p><p>Let's take the traditional linear regression example of predicting the sepal length of iris flowers based on the petal length with some missing data. For this, we will use the previously defined <code class="literal">miris</code> dataset:</p><div><pre class="programlisting"><strong>&gt; summary(lm(Sepal.Length ~ Petal.Length, data = miris))</strong>

<strong>Call:</strong>
<strong>lm(formula = Sepal.Length ~ Petal.Length, data = miris)</strong>

<strong>Residuals:</strong>
<strong>     Min       1Q   Median       3Q      Max </strong>
<strong>-1.26216 -0.36157  0.01461  0.35293  1.01933 </strong>

<strong>Coefficients:</strong>
<strong>             Estimate Std. Error t value Pr(&gt;|t|)    </strong>
<strong>(Intercept)   4.27831    0.11721   36.50   &lt;2e-16 ***</strong>
<strong>Petal.Length  0.41863    0.02683   15.61   &lt;2e-16 ***</strong>
<strong>---</strong>
<strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong>

<strong>Residual standard error: 0.4597 on 92 degrees of freedom</strong>
<strong>  (56 observations deleted due to missingness)</strong>
<strong>Multiple R-squared:  0.7258,  Adjusted R-squared:  0.7228 </strong>
<strong>F-statistic: 243.5 on 1 and 92 DF,  p-value: &lt; 2.2e-16</strong>
</pre></div><p>So it seems that our estimate for the sepal and petal length ratio is around <code class="literal">0.42</code>, which is not too far from the real value by the way:</p><div><pre class="programlisting"><strong>&gt; lm(Sepal.Length ~ Petal.Length, data = iris)$coefficients</strong>
<strong> (Intercept) Petal.Length </strong>
<strong>   4.3066034    0.4089223</strong>
</pre></div><p>The difference between the estimated and real coefficients is due to the artificially introduced missing values in a previous section. Can we produce even better estimates? We might impute the missing data with any of the previously mentioned methods, or instead we should rather fit a robust linear regression from the<a class="indexterm" id="id622"/> <code class="literal">MASS</code> package predicting <code class="literal">Sepal.Length</code> with the <code class="literal">Petal.Length</code> variable:</p><div><pre class="programlisting"><strong>&gt; library(MASS)</strong>
<strong>&gt; summary(rlm(Sepal.Length ~ Petal.Length, data = miris))</strong>

<strong>Call: rlm(formula = Sepal.Length ~ Petal.Length, data = miris)</strong>
<strong>Residuals:</strong>
<strong>     Min       1Q   Median       3Q      Max </strong>
<strong>-1.26184 -0.36098  0.01574  0.35253  1.02262 </strong>

<strong>Coefficients:</strong>
<strong>             Value   Std. Error t value</strong>
<strong>(Intercept)   4.2739  0.1205    35.4801</strong>
<strong>Petal.Length  0.4195  0.0276    15.2167</strong>

<strong>Residual standard error: 0.5393 on 92 degrees of freedom</strong>
<strong>  (56 observations deleted due to missingness)</strong>
</pre></div><p>Now let's compare the<a class="indexterm" id="id623"/> coefficients of the models run against the original (full) and the simulated data (with missing values):</p><div><pre class="programlisting"><strong>&gt; f &lt;- formula(Sepal.Length ~ Petal.Length)</strong>
<strong>&gt; cbind(</strong>
<strong>+     orig =  lm(f, data = iris)$coefficients,</strong>
<strong>+     lm   =  lm(f, data = miris)$coefficients,</strong>
<strong>+     rlm  = rlm(f, data = miris)$coefficients)</strong>
<strong>                  orig        lm       rlm</strong>
<strong>(Intercept)  4.3066034 4.2783066 4.2739350</strong>
<strong>Petal.Length 0.4089223 0.4186347 0.4195341</strong>
</pre></div><p>To be honest, there's not much difference between the standard linear regression and the robust version. Surprised? Well, the dataset included missing values completely at random, but what happens if the dataset includes other types of missing values or an outlier? Let's verify this by<a class="indexterm" id="id624"/> simulating some dirtier data issues (with updating the sepal length of the first observation from <code class="literal">1.4</code> to <code class="literal">14</code> – let's say due to a data input error) and rebuilding the models:</p><div><pre class="programlisting"><strong>&gt; miris$Sepal.Length[1] &lt;- 14</strong>
<strong>&gt; cbind(</strong>
<strong>+     orig = lm(f, data = iris)$coefficients,</strong>
<strong>+     lm   = lm(f, data = miris)$coefficients,</strong>
<strong>+     rlm  = rlm(f, data = miris)$coefficients)</strong>
<strong>                  orig        lm       rlm</strong>
<strong>(Intercept)  4.3066034 4.6873973 4.2989589</strong>
<strong>Petal.Length 0.4089223 0.3399485 0.4147676</strong>
</pre></div><p>It seems that the <code class="literal">lm</code> model's performance decreased a lot, while the coefficients of the robust model are almost identical to the original model regardless of the outlier in the data. We can conclude that robust methods are pretty impressive and powerful tools when it comes to extreme values! For more information on the related methods already implemented in R, visit the<a class="indexterm" id="id625"/> related CRAN Task View at <a class="ulink" href="http://cran.r-project.org/web/views/Robust.html">http://cran.r-project.org/web/views/Robust.html</a>.</p></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec61"/>Summary</h1></div></div></div><p>This chapter focused on some of the hardest challenges in data analysis in the means of cleansing data, and we covered the most important topics on missing and extreme values. Depending on your field of interest or industry you are working for, dirty data can be a rare or major issue (for example I've seen some projects in the past when regular expressions were applied to a <code class="literal">JSON</code> file to make that valid), but I am sure you will find the next chapter interesting and useful despite your background – where we will learn about multivariate statistical techniques.</p></div></body></html>