<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Data Modeling in Hadoop</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>So far, we've learned how to create a Hadoop cluster and how to load data into it. In the previous chapter, we learned about various data ingestion tools and techniques. As we know by now, there are various open source tools available in the market, but there is a single silver bullet tool that can take on all our use cases. Each data ingestion tool has certain unique features; they can prove to be very productive and useful in typical use cases. For example, Sqoop is more useful when used to import and export Hadoop data from and to an RDBMS.</span><br/></p>
<p class="chapter-content">In this chapter, we will learn how to store and model data in Hadoop clusters. Like data ingestion tools, there are various data stores available. These data stores support different data models—that is, columnar data storage, key value pairs, and so on; and they support various file formats, such as ORC, Parquet, and AVRO, and so on. There are very popular data stores<span>, widely</span> used in production these days, for example, Hive, HBase, Cassandra, and so on. We will learn more about the following two data stores and data modeling techniques:</p>
<ul>
<li>Apache Hive</li>
<li>Apache HBase</li>
</ul>
<p class="chapter-content">First, we will start with basic concepts and then we will learn how we can apply modern data modeling techniques for faster data access. In a nutshell, we will cover the following topics in this chapter:</p>
<ul>
<li>Apache Hive and RDBMS</li>
<li>Supported datatypes</li>
<li>Hive architecture and how it works</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache Hive</h1>
                </header>
            
            <article>
                
<p>Hive is a data processing tool in Hadoop. As we have learned in the previous chapter, data ingestion tools load data and generate HDFS files in Hadoop; we need to query that data based on our business requirements. We can access the data using MapReduce programming. But data access with MapReduce is extremely slow. To access a few lines of HDFS files, we have to write separate mapper, reducer, and driver code. So, in order to avoid this complexity, Apache introduced Hive. Hive supports an SQL-like interface that helps access the same lines of HDFS files using SQL commands. Hive was initially developed by Facebook but was later taken over by Apache.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache Hive and RDBMS</h1>
                </header>
            
            <article>
                
<p class="chapter-content">I mentioned that Hive provides an SQL-like interface. Bearing this in mind, the question that arises is: <em>is Hive the same as RDBMS on Hadoop?</em> The answer is <em>no</em>. Hive is not a database. Hive does not store any data. Hive stores table information as a part of metadata, which is called schema, and points to files on HDFS. Hive accesses data stored on HDFS files using an SQL-like interface called <strong>HiveQL</strong> (<strong>HQL</strong>). Hive supports SQL commands to access and modify data in HDFS. Hive is not a tool for OLTP. It does not provide any row-level insert, update, or delete. The current version of Hive (version 0.14), does support insert, update, and delete with full ACID properties, but that feature is not efficient. Also, this feature does not support all file formats. For example, the update supports only ORC file format. Basically, Hive is designed for batch processing and does not support transaction processing like RDBMS does. Hence, Hive is better suited for data warehouse applications for providing data summarization, query, and analysis. Internally, Hive SQL queries are converted into MapReduce by its compiler. Users need not worry about writing any complex mapper and reducer code. Hive supports query structured data only. It is very complex to access unstructured data using Hive SQL. You may have to write your own custom functions for that. Hive supports various file formats such as text files, sequence files, ORC, and Parquet, which provide significant data compression.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supported datatypes</h1>
                </header>
            
            <article>
                
<p class="chapter-content">The following datatypes are supported by Hive version 0.14:</p>
<table>
<tbody>
<tr>
<td><strong>Datatype group</strong></td>
<td><strong>Datatype</strong></td>
<td><strong>Format</strong></td>
</tr>
<tr>
<td rowspan="3"><span>String</span></td>
<td><kbd>STRING</kbd></td>
<td><kbd>column_name STRING</kbd></td>
</tr>
<tr>
<td><kbd>VARCHAR</kbd></td>
<td><kbd>column_name VARCHAR(max_length)</kbd></td>
</tr>
<tr>
<td><kbd>CHAR</kbd></td>
<td><kbd>column_name CHAR(length)</kbd></td>
</tr>
<tr>
<td rowspan="7"><span>Numeric</span></td>
<td><kbd>TINYINT</kbd></td>
<td><kbd>column_name TINYINT</kbd></td>
</tr>
<tr>
<td><kbd>SMALLINT</kbd></td>
<td><kbd>column_name SMALLINT</kbd></td>
</tr>
<tr>
<td><kbd>INT</kbd></td>
<td><kbd>column_name INT</kbd></td>
</tr>
<tr>
<td><kbd>BIGINT</kbd></td>
<td><kbd>column_name BIGINT</kbd></td>
</tr>
<tr>
<td><kbd>FLOAT</kbd></td>
<td><kbd>column_name FLOAT</kbd></td>
</tr>
<tr>
<td><kbd>DOUBLE</kbd></td>
<td><kbd>column_name DOUBLE</kbd></td>
</tr>
<tr>
<td><kbd>DECIMAL</kbd></td>
<td><kbd>column_name DECIMAL[(precision[,scale])]</kbd></td>
</tr>
<tr>
<td rowspan="3"><span>Date/time type</span></td>
<td><kbd>TIMESTAMP</kbd></td>
<td><kbd>column_name TIMESTAMP</kbd></td>
</tr>
<tr>
<td><kbd>DATE</kbd></td>
<td><kbd>column_name DATE</kbd></td>
</tr>
<tr>
<td><kbd>INTERVAL</kbd></td>
<td><kbd>column_name INTERVAL year to month</kbd></td>
</tr>
<tr>
<td rowspan="2"><span>Miscellaneous type</span></td>
<td><kbd>BOOLEAN</kbd></td>
<td><kbd>column_name BOOLEAN</kbd></td>
</tr>
<tr>
<td><kbd>BINARY</kbd></td>
<td><kbd>column_name BINARY</kbd></td>
</tr>
<tr>
<td rowspan="4"><span>Complex type</span></td>
<td><kbd>ARRAY</kbd></td>
<td><kbd>column_name ARRAY &lt; type &gt;</kbd></td>
</tr>
<tr>
<td><kbd>MAPS</kbd></td>
<td><kbd>column_name MAP &lt; primitive_type, type &gt;</kbd></td>
</tr>
<tr>
<td><kbd>STRUCT</kbd></td>
<td><kbd>column_name STRUCT &lt; name : type [COMMENT 'comment_string'] &gt;</kbd></td>
</tr>
<tr>
<td><kbd>UNION</kbd></td>
<td><kbd>column_name UNIONTYPE &lt;int, double, array, string&gt;</kbd></td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How Hive works</h1>
                </header>
            
            <article>
                
<p class="chapter-content">Hive databases are comprised of tables which are made up of partitions. Data can be accessed via a simple query language and Hive supports overwriting or appending of data. Within a particular database, data in tables is serialized and each table has a corresponding HDFS directory. Each table can be sub-divided into partitions that determine how data is distributed within subdirectories of the table directory. Data within partitions can be further broken down into buckets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hive architecture</h1>
                </header>
            
            <article>
                
<p class="chapter-content">The following is a representation of Hive architecture:</p>
<div class="chapter-content CDPAlignCenter CDPAlign"><img src="assets/7dcc7f8c-71f0-4b1d-b245-8e0e317aa65a.png"/></div>
<p class="chapter-content">The preceding diagram shows that Hive architecture is divided into three parts—that is, clients, services, and metastore. The Hive SQL is executed as follows:</p>
<ul>
<li><strong>Hive SQL query</strong>: A Hive query can be submitted to the Hive server using one of these ways: WebUI, JDBC/ODBC application, and Hive CLI. For a thrift-based application, it will provide a thrift client for communication.</li>
<li><strong>Query execution</strong>: Once the Hive server receives the query, it is compiled, converted into an optimized query plan for better performance, and converted into a MapReduce job. During this process, the Hive Server interacts with the metastore for query metadata.</li>
<li><strong>Job execution</strong>: The MapReduce job is executed on the Hadoop cluster.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hive data model management</h1>
                </header>
            
            <article>
                
<p class="chapter-content">Hive handles data in the following four ways:</p>
<ul>
<li>Hive tables</li>
<li>Hive table partition</li>
<li>Hive partition bucketing</li>
<li>Hive views</li>
</ul>
<p class="chapter-content">We will see each one of them in detail in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hive tables</h1>
                </header>
            
            <article>
                
<p class="chapter-content">A Hive table is very similar to any RDBMS table. The table is divided into rows and columns. Each column (field) is defined with a proper name and datatype. We have already seen all the available datatypes in Hive in the <em>Supported datatypes</em> section. A Hive table is divided into two types:</p>
<ul>
<li>Managed tables</li>
<li>External tables</li>
</ul>
<p class="chapter-content">We will learn about both of these types in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Managed tables</h1>
                </header>
            
            <article>
                
<p class="chapter-content">The following is a sample command to define a Hive managed table:</p>
<pre>Create Table &lt; managed_table_name&gt;  
   Column1 &lt;data type&gt;, 
   Column2 &lt;data type&gt;, 
   Column3 &lt;data type&gt; 
Row format delimited Fields Terminated by "t"; </pre>
<p class="chapter-content">When the preceding query is executed, Hive creates the table and the metadata is updated in the metastore accordingly. But the table is empty. So, data can be loaded into this table by executing the following command:</p>
<pre>Load data inpath &lt;hdfs_folder_name&gt; into table &lt;managed_table_name&gt;; </pre>
<p class="chapter-content">After executing the previous command, the data is moved from <kbd>&lt;hdfs_folder_name&gt;</kbd> to the Hive table's default location <kbd>/user/hive/warehouse/&lt;managed_table_name</kbd>. This default folder, <kbd>/user/hive/warehouse</kbd>, is defined in <kbd>hive-site.xml</kbd> and can be changed to any folder. Now, if we decide to drop the table, we can do so by issuing the following command:</p>
<pre>Drop table &lt;managed_table_name&gt;; </pre>
<p class="chapter-content">The <kbd>/user/hive/warehouse/&lt;managed_table_name</kbd> <span>folder</span><span> </span><span>will be dropped and the metadata stored in the metastore will be deleted.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">External tables</h1>
                </header>
            
            <article>
                
<p class="chapter-content">The following is a sample command to define a Hive external table:</p>
<pre>Create Table &lt; external_table_name&gt;  
   Column1 &lt;data type&gt;, 
   Column2 &lt;data type&gt;, 
   Column3 &lt;data type&gt; 
Row format delimited Fields Terminated by "t" 
Location &lt;hdfs_folder_name&gt;; </pre>
<p class="chapter-content">When the preceding query is executed, Hive creates the table and the metadata is updated in the metastore accordingly. But, again, the table is empty. So, data can be loaded into this table by executing the following command:</p>
<pre>Load data inpath &lt;hdfs_folder_name&gt; into table &lt;external_table_name&gt;; </pre>
<p class="chapter-content">This command will not move any file to any folder but, instead, creates a pointer to the folder location, and it is updated in the metadata in the metastore. The file remains at the same location (<kbd>&lt;hdfs_folder_name&gt;</kbd>) of the query. Now, if we decide to drop the table, we can do so by issuing the following command:</p>
<pre>Drop table &lt;managed_table_name&gt;;  </pre>
<p class="chapter-content">The folder <kbd>/user/hive/warehouse/&lt;managed_table_name</kbd> will not be dropped and only the metadata stored in the metastore will be deleted. The file remains in the same location—<kbd>&lt;hdfs_folder_name&gt;</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hive table partition</h1>
                </header>
            
            <article>
                
<p class="chapter-content">Partitioning a table means dividing a table into different parts based on a value of a partition key. A partition key can be any column, for example, date, department, country, and so on. As data is stored in parts, the query response time becomes faster. Instead of scanning the whole table, partition creates subfolders within the main table folders. Hive will scan only a specific part or parts of the table based on the query's <kbd>WHERE</kbd> clause. Hive table partition is similar to any RDBMS table partition. The purpose is also the same. As we keep inserting data into a table, the table becomes bigger in data size. Let's say we create an <kbd>ORDERS</kbd> table as follows:</p>
<pre>hive&gt; create database if not exists ORDERS; 
OK 
Time taken: 0.036 seconds 
 
hive&gt; use orders; 
OK 
Time taken: 0.262 seconds 
 
hive&gt; CREATE TABLE if not exists ORDEERS_DATA 
    &gt; (Ord_id INT, 
    &gt; Ord_month INT, 
    &gt; Ord_customer_id INT, 
    &gt; Ord_city  STRING, 
    &gt; Ord_zip   STRING, 
    &gt; ORD_amt   FLOAT 
    &gt; ) 
    &gt; ROW FORMAT DELIMITED 
    &gt; FIELDS TERMINATED BY  ',' 
    &gt; ; 
OK 
Time taken: 0.426 seconds 
hive&gt; </pre>
<p class="chapter-content">We will load the following sample file <kbd>ORDERS_DATA</kbd> table as follows:</p>
<pre>101,1,100,'Los Angeles','90001',1200 
102,2,200,'Los Angeles','90002',1800 
103,3,300,'Austin','78701',6500 
104,4,400,'Phoenix','85001',7800 
105,5,500,'Beverly Hills','90209',7822 
106,6,600,'Gaylord','49734',8900 
107,7,700,'Los Angeles','90001',7002 
108,8,800,'Los Angeles','90002',8088 
109,9,900,'Reno','89501',6700 
110,10,1000,'Los Angeles','90001',8500 
111,10,1000,'Logan','84321',2300 
112,10,1000,'Fremont','94539',9500 
113,10,1000,'Omaha','96245',7500 
114,11,2000,'New York','10001',6700 
115,12,3000,'Los Angeles','90003',1000 </pre>
<p class="chapter-content">Then we load <kbd>orders.txt</kbd> to the <kbd>/tmp</kbd> HDFS folder:</p>
<pre>[root@sandbox order_data]# hadoop fs -put /root/order_data/orders.txt /tmp 
 
[root@sandbox order_data]# hadoop fs -ls /tmp 
Found 3 items 
-rw-r--r--   1 root      hdfs        530 2017-09-02 18:06 /tmp/orders.txt </pre>
<p class="chapter-content">Load the <kbd>ORDERS_DATA</kbd> table as follows:</p>
<pre>hive&gt; load data inpath '/tmp/orders.txt' into table ORDERS_DATA; 
Loading data to table orders.orders_data 
Table orders.orders_data stats: [numFiles=1, numRows=0, totalSize=530, rawDataSize=0] 
OK 
Time taken: 0.913 seconds 
 
hive&gt; select * from ORDERS_DATA; 
OK 
101      1     100   'Los Angeles'     '90001'     1200.0 
102      2     200   'Los Angeles'     '90002'     1800.0 
103      3     300   'Austin'    '78701'     6500.0 
104      4     400   'Phoenix'   '85001'     7800.0 
105      5     500   'Beverly Hills'   '90209'     7822.0 
106      6     600   'Gaylord'   '49734'     8900.0 
107      7     700   'Los Angeles'     '90001'     7002.0 
108      8     800   'Los Angeles'     '90002'     8088.0 
109      9     900   'Reno'      '89501'     6700.0 
110      10    1000  'Los Angeles'     '90001'     8500.0 
111      10    1000  'Logan'     '84321'     2300.0 
112      10    1000  'Fremont'   '94539'     9500.0 
113      10    1000  'Omaha'     '96245'     7500.0 
114      11    2000  'New York'  '10001'     6700.0 
115      12    3000  'Los Angeles'     '90003'     1000.0 
Time taken: 0.331 seconds, Fetched: 15 row(s) </pre>
<p class="chapter-content">Let's assume we want to insert cities data in an <kbd>ORDERS_DATA</kbd> table. Each city orders data is of 1 TB in size. So the total data size of the <kbd>ORDERS_DATA</kbd> table will be 15 TB (there are 15 cities in the table). Now, if we write the following query to get all orders booked in <kbd>Los Angeles</kbd>:</p>
<pre>hive&gt;  select * from ORDERS where Ord_city = 'Los Angeles' ; 
 </pre>
<p class="chapter-content">The query will run very slowly as it has to scan the entire table. The obvious idea is that we can create 10 different <kbd>orders</kbd> tables for each city and store <kbd>orders</kbd> data in the corresponding city of the <kbd>ORDERS_DATA</kbd> table. But instead of that, we can partition the <kbd>ORDERS_PART</kbd> table as follows:</p>
<pre>hive&gt; use orders; 
 
hive&gt; CREATE TABLE orders_part 
    &gt; (Ord_id INT, 
    &gt; Ord_month INT, 
    &gt; Ord_customer_id INT, 
    &gt; Ord_zip   STRING, 
    &gt; ORD_amt   FLOAT 
    &gt; ) 
    &gt; PARTITIONED BY  (Ord_city INT) 
    &gt; ROW FORMAT DELIMITED 
    &gt; FIELDS TERMINATED BY  ',' 
    &gt; ; 
OK 
Time taken: 0.305 seconds 
hive&gt; </pre>
<p class="chapter-content">Now, Hive organizes the tables into partitions for grouping similar types of data together based on a column or partition key. Let's assume that we have 10 <kbd>orders</kbd> files for each city, that is, <kbd>Orders1.txt</kbd> to <kbd>Orders10.txt</kbd>. The following example shows how to load each monthly file to each corresponding partition:</p>
<pre>load data inpath '/tmp/orders.txt' into table orders_part partition(Ord_city='Los Angeles'); 
load data inpath '/tmp/orders.txt' into table orders_part partition(Ord_city='Austin'); 
load data inpath '/tmp/orders.txt' into table orders_part partition(Ord_city='Phoenix'); 
load data inpath '/tmp/orders.txt' into table orders_part partition(Ord_city='Beverly Hills'); 
load data inpath '/tmp/orders.txt' into table orders_part partition(Ord_city='Gaylord'); 
load data inpath '/tmp/orders.txt' into table orders_part partition(Ord_city=Reno'); 
load data inpath '/tmp/orders.txt' into table orders_part partition(Ord_city='Fremont'); 
load data inpath '/tmp/orders.txt' into table orders_part partition(Ord_city='Omaha'); 
load data inpath '/tmp/orders.txt' into table orders_part partition(Ord_city='New York'); 
load data inpath '/tmp/orders.txt' into table orders_part partition(Ord_city='Logan'); 
 
[root@sandbox order_data]# hadoop fs -ls /apps/hive/warehouse/orders.db/orders_part 
Found 10 items 
drwxrwxrwx   - root hdfs          0 2017-09-02 18:32 /apps/hive/warehouse/orders.db/orders_part/ord_city=Austin 
drwxrwxrwx   - root hdfs          0 2017-09-02 18:32 /apps/hive/warehouse/orders.db/orders_part/ord_city=Beverly Hills 
drwxrwxrwx   - root hdfs          0 2017-09-02 18:32 /apps/hive/warehouse/orders.db/orders_part/ord_city=Fremont 
drwxrwxrwx   - root hdfs          0 2017-09-02 18:32 /apps/hive/warehouse/orders.db/orders_part/ord_city=Gaylord 
drwxrwxrwx   - root hdfs          0 2017-09-02 18:33 /apps/hive/warehouse/orders.db/orders_part/ord_city=Logan 
drwxrwxrwx   - root hdfs          0 2017-09-02 18:32 /apps/hive/warehouse/orders.db/orders_part/ord_city=Los Angeles 
drwxrwxrwx   - root hdfs          0 2017-09-02 18:32 /apps/hive/warehouse/orders.db/orders_part/ord_city=New York 
drwxrwxrwx   - root hdfs          0 2017-09-02 18:32 /apps/hive/warehouse/orders.db/orders_part/ord_city=Omaha 
drwxrwxrwx   - root hdfs          0 2017-09-02 18:32 /apps/hive/warehouse/orders.db/orders_part/ord_city=Phoenix 
drwxrwxrwx   - root hdfs          0 2017-09-02 18:33 /apps/hive/warehouse/orders.db/orders_part/ord_city=Reno 
[root@sandbox order_data]  </pre>
<p class="chapter-content">Partitioning the data can greatly improve the performance of queries because the data is already separated into files based on the column value, which can decrease the number of mappers and greatly decrease the amount of shuffling and sorting of data in the resulting MapReduce job.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hive static partitions and dynamic partitions</h1>
                </header>
            
            <article>
                
<p class="chapter-content">If you want to use a static partition in Hive, you should set the property as follows:</p>
<pre>set hive.mapred.mode = strict;  </pre>
<p class="chapter-content">In the preceding example, we have seen that we have to insert each monthly order file to each static partition individually. Static partition saves time in loading data compared to dynamic partition. We have to individually add a partition to the table and move the file into the partition of the table. If we have a lot partitions, writing a query to load data in each partition may become cumbersome. We can overcome this with a dynamic partition. In dynamic partitions, we can insert data into a partition table with a single SQL statement but still load data in each partition. Dynamic partition takes more time in loading data compared to static partition. When you have large data stored in a table, dynamic partition is suitable. If you want to partition a number of columns but you don't know how many columns they are, then dynamic partition is also suitable. Here are the hive dynamic partition properties you should allow:</p>
<pre>SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;  </pre>
<p class="chapter-content">The following is an example of dynamic partition. Let's say we want to load data from the <kbd>ORDERS_PART</kbd> table to a new table called <kbd>ORDERS_NEW</kbd>:</p>
<pre>hive&gt; use orders; 
OK 
Time taken: 1.595 seconds 
hive&gt; drop table orders_New; 
OK 
Time taken: 0.05 seconds 
hive&gt; CREATE TABLE orders_New 
    &gt; (Ord_id INT, 
    &gt; Ord_month INT, 
    &gt; Ord_customer_id INT, 
    &gt; Ord_city  STRING, 
    &gt; Ord_zip   STRING, 
    &gt; ORD_amt   FLOAT 
    &gt; ) 
    &gt; ) 
    &gt; PARTITIONED BY  (Ord_city STRING) 
    &gt; ROW FORMAT DELIMITED 
    &gt; FIELDS TERMINATED BY  ',' 
    &gt; ; 
OK 
Time taken: 0.458 seconds 
hive&gt; </pre>
<p class="chapter-content">Load data into the <kbd>ORDER_NEW</kbd> table from the <kbd>ORDERS_PART</kbd> table. Here, Hive will load all partitions of the <kbd>ORDERS_NEW</kbd> table dynamically:</p>
<pre>hive&gt; SET hive.exec.dynamic.partition = true; 
hive&gt; SET hive.exec.dynamic.partition.mode = nonstrict; 
hive&gt;  
    &gt; insert into table orders_new  partition(Ord_city) select * from orders_part; 
Query ID = root_20170902184354_2d409a56-7bfc-416e-913a-2323ea3b339a 
Total jobs = 1 
Launching Job 1 out of 1 
Status: Running (Executing on YARN cluster with App id application_1504299625945_0013) 
 
-------------------------------------------------------------------------------- 
        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED 
-------------------------------------------------------------------------------- 
Map 1 ..........   SUCCEEDED      1          1        0        0       0       0 
-------------------------------------------------------------------------------- 
VERTICES: 01/01  [==========================&gt;&gt;] 100%  ELAPSED TIME: 3.66 s      
-------------------------------------------------------------------------------- 
Loading data to table orders.orders_new partition (ord_city=null) 
    Time taken to load dynamic partitions: 2.69 seconds 
   Loading partition {ord_city=Logan} 
   Loading partition {ord_city=Los Angeles} 
   Loading partition {ord_city=Beverly Hills} 
   Loading partition {ord_city=Reno} 
   Loading partition {ord_city=Fremont} 
   Loading partition {ord_city=Gaylord} 
   Loading partition {ord_city=Omaha} 
   Loading partition {ord_city=Austin} 
   Loading partition {ord_city=New York} 
   Loading partition {ord_city=Phoenix} 
    Time taken for adding to write entity : 3 
Partition orders.orders_new{ord_city=Austin} stats: [numFiles=1, numRows=1, totalSize=13, rawDataSize=12] 
Partition orders.orders_new{ord_city=Beverly Hills} stats: [numFiles=1, numRows=1, totalSize=13, rawDataSize=12] 
Partition orders.orders_new{ord_city=Fremont} stats: [numFiles=1, numRows=1, totalSize=15, rawDataSize=14] 
Partition orders.orders_new{ord_city=Gaylord} stats: [numFiles=1, numRows=1, totalSize=13, rawDataSize=12] 
Partition orders.orders_new{ord_city=Logan} stats: [numFiles=1, numRows=1, totalSize=15, rawDataSize=14] 
Partition orders.orders_new{ord_city=Los Angeles} stats: [numFiles=1, numRows=6, totalSize=82, rawDataSize=76] 
Partition orders.orders_new{ord_city=New York} stats: [numFiles=1, numRows=1, totalSize=15, rawDataSize=14] 
Partition orders.orders_new{ord_city=Omaha} stats: [numFiles=1, numRows=1, totalSize=15, rawDataSize=14] 
Partition orders.orders_new{ord_city=Phoenix} stats: [numFiles=1, numRows=1, totalSize=13, rawDataSize=12] 
Partition orders.orders_new{ord_city=Reno} stats: [numFiles=1, numRows=1, totalSize=13, rawDataSize=12] 
OK 
Time taken: 10.493 seconds 
hive&gt;  </pre>
<p class="chapter-content">Let's see how many partitions are created in <kbd>ORDERS_NEW</kbd>:</p>
<pre>hive&gt; show partitions ORDERS_NEW; 
OK 
ord_city=Austin 
ord_city=Beverly Hills 
ord_city=Fremont 
ord_city=Gaylord 
ord_city=Logan 
ord_city=Los Angeles 
ord_city=New York 
ord_city=Omaha 
ord_city=Phoenix 
ord_city=Reno 
Time taken: 0.59 seconds, Fetched: 10 row(s) 
hive&gt;  </pre>
<p class="chapter-content">Now it is very clear when to use static and dynamic partitions. Static partitioning can be used when the partition column values are known well in advance before loading data into a hive table. In the case of dynamic partitions, partition column values are known only during loading of the data into the hive table.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hive partition bucketing</h1>
                </header>
            
            <article>
                
<p class="chapter-content">Bucketing is a technique of decomposing a large dataset into more manageable groups. Bucketing is based on the hashing function. When a table is bucketed, all the table records with the same column value will go into the same bucket. Physically, each bucket is a file in a table folder just like a partition. In a partitioned table, Hive can group the data in multiple folders. But partitions prove effective when they are of a limited number and when the data is distributed equally among all of them. If there are a large number of partitions, then their use becomes less effective. So in that case, we can use bucketing. We can create a number of buckets explicitly during table creation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How Hive bucketing works</h1>
                </header>
            
            <article>
                
<p class="chapter-content">The following diagram shows the working of Hive bucketing in detail:</p>
<div class="chapter-content CDPAlignCenter CDPAlign"><img src="assets/9496a468-bbc4-4711-8027-95b31f9076d7.png"/></div>
<p class="chapter-content">If we decide to have three buckets in a table for a column, (<kbd>Ord_city</kbd>) in our example, then Hive will create three buckets with numbers 0-2 (<em>n-1</em>). During record insertion time, Hive will apply the Hash function to the <kbd>Ord_city</kbd> column of each record to decide the hash key. Then Hive will apply a modulo operator to each hash value. We can use bucketing in non-partitioned tables also. But we will get the best performance when the bucketing feature is used with a partitioned table. Bucketing has two key benefits:</p>
<ul>
<li><strong>Improved query performance</strong>: During joins on the same bucketed columns, we can specify the number of buckets explicitly. Since each bucket is of equal size of data, map-side joins perform better on a bucketed table than a non-bucketed table. In a map-side join, the left-hand side table bucket will exactly <span>know </span>the dataset in the right-<span>hand side </span>bucket to perform a table join efficiently.</li>
<li><strong>Improved sampling</strong>: Because the data is already split up into smaller chunks.</li>
</ul>
<p class="chapter-content">Let's consider our <kbd>ORDERS_DATA</kbd> table example. It is partitioned in the <kbd>CITY</kbd> column. It may be possible that all of the cities do not have an equal distribution of orders. Some cities may have more orders than others. In that case, we will have lopsided partitions. This will affect query performance. Queries with cities that have more orders will be slower than for cities with fewer orders. We can solve this problem by bucketing the table. Buckets in the table are defined by the <kbd>CLUSTER</kbd> clause in the table DDL. The following examples explain the bucketing feature in detail.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating buckets in a non-partitioned table</h1>
                </header>
            
            <article>
                
<p class="chapter-content">First, we will create a <kbd>ORDERS_BUCK_non_partition</kbd> table:</p>
<pre>SET hive.exec.dynamic.partition = true; 
SET hive.exec.dynamic.partition.mode = nonstrict; 
SET hive.exec.mx_dynamic.partition=20000; 
SET hive.exec.mx_dynamic.partition.pernode=20000; 
SET hive.enforce.bucketing = true; 
 
hive&gt; use orders; 
OK 
Time taken: 0.221 seconds 
hive&gt;  
    &gt; CREATE TABLE ORDERS_BUCKT_non_partition 
    &gt; (Ord_id INT, 
    &gt; Ord_month INT, 
    &gt; Ord_customer_id INT, 
    &gt; Ord_city  STRING, 
    &gt; Ord_zip   STRING, 
    &gt; ORD_amt   FLOAT 
    &gt; ) 
    &gt; CLUSTERED BY (Ord_city) into 4 buckets stored as textfile; 
OK 
Time taken: 0.269 seconds 
hive&gt;  
 </pre>
<div class="packt_infobox">To refer to all Hive <kbd>SET</kbd> configuration parameters, please use this URL:<br/>
<a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties">https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties</a>.</div>
<p class="chapter-content">Load the newly created non-partitioned bucket table:</p>
<pre>hive&gt; insert into ORDERS_BUCKT_non_partition select * from orders_data; 
Query ID = root_20170902190615_1f557644-48d6-4fa1-891d-2deb7729fa2a 
Total jobs = 1 
Launching Job 1 out of 1 
Tez session was closed. Reopening... 
Session re-established. 
Status: Running (Executing on YARN cluster with App id application_1504299625945_0014) 
 
-------------------------------------------------------------------------------- 
        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED 
-------------------------------------------------------------------------------- 
Map 1 ..........   SUCCEEDED      1          1        0        0       0       0 
Reducer 2 ......   SUCCEEDED      4          4        0        0       0       0 
-------------------------------------------------------------------------------- 
VERTICES: 02/02  [==========================&gt;&gt;] 100%  ELAPSED TIME: 9.58 s      
-------------------------------------------------------------------------------- 
Loading data to table orders.orders_buckt_non_partition 
Table orders.orders_buckt_non_partition stats: [numFiles=4, numRows=15, totalSize=560, rawDataSize=545] 
OK 
Time taken: 15.55 seconds 
hive&gt; 
  </pre>
<p class="chapter-content">The following command shows that Hive has created four buckets (folders), <kbd>00000[0-3]_0</kbd>, in the table:</p>
<pre> 
[root@sandbox order_data]# hadoop fs -ls /apps/hive/warehouse/orders.db/orders_buckt_non_partition 
Found 4 items 
-rwxrwxrwx   1 root hdfs         32 2017-09-02 19:06 /apps/hive/warehouse/orders.db/orders_buckt_non_partition/000000_0 
-rwxrwxrwx   1 root hdfs        110 2017-09-02 19:06 /apps/hive/warehouse/orders.db/orders_buckt_non_partition/000001_0 
-rwxrwxrwx   1 root hdfs        104 2017-09-02 19:06 /apps/hive/warehouse/orders.db/orders_buckt_non_partition/000002_0 
-rwxrwxrwx   1 root hdfs        314 2017-09-02 19:06 /apps/hive/warehouse/orders.db/orders_buckt_non_partition/000003_0 
[root@sandbox order_data]# </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating buckets in a partitioned table</h1>
                </header>
            
            <article>
                
<p class="chapter-content">First, we will create a bucketed partition table. Here, the table is partitioned into four buckets on the <kbd>Ord_city</kbd> column, but subdivided into <kbd>Ord_zip</kbd> columns:</p>
<pre>SET hive.exec.dynamic.partition = true; 
SET hive.exec.dynamic.partition.mode = nonstrict; 
SET hive.exec.mx_dynamic.partition=20000; 
SET hive.exec.mx_dynamic.partition.pernode=20000; 
SET hive.enforce.bucketing = true; 
 
hive&gt; CREATE TABLE ORDERS_BUCKT_partition 
    &gt; (Ord_id INT, 
    &gt; Ord_month INT, 
    &gt; Ord_customer_id INT, 
    &gt; Ord_zip   STRING, 
    &gt; ORD_amt   FLOAT 
    &gt; ) 
    &gt; PARTITIONED BY  (Ord_city STRING) 
    &gt; CLUSTERED BY (Ord_zip) into 4 buckets stored as textfile; 
OK 
Time taken: 0.379 seconds </pre>
<p class="chapter-content">Load the bucketed partitioned table with another partitioned table (<kbd>ORDERS_PART</kbd>) with a dynamic partition:</p>
<pre>hive&gt; SET hive.exec.dynamic.partition = true; 
hive&gt; SET hive.exec.dynamic.partition.mode = nonstrict; 
hive&gt; SET hive.exec.mx_dynamic.partition=20000; 
Query returned non-zero code: 1, cause: hive configuration hive.exec.mx_dynamic.partition does not exists. 
hive&gt; SET hive.exec.mx_dynamic.partition.pernode=20000; 
Query returned non-zero code: 1, cause: hive configuration hive.exec.mx_dynamic.partition.pernode does not exists. 
hive&gt; SET hive.enforce.bucketing = true; 
hive&gt; insert into ORDERS_BUCKT_partition partition(Ord_city) select * from orders_part; 
Query ID = root_20170902194343_dd6a2938-6aa1-49f8-a31e-54dafbe8d62b 
Total jobs = 1 
Launching Job 1 out of 1 
Status: Running (Executing on YARN cluster with App id application_1504299625945_0017) 
 
-------------------------------------------------------------------------------- 
        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED 
-------------------------------------------------------------------------------- 
Map 1 ..........   SUCCEEDED      1          1        0        0       0       0 
Reducer 2 ......   SUCCEEDED      4          4        0        0       0       0 
-------------------------------------------------------------------------------- 
VERTICES: 02/02  [==========================&gt;&gt;] 100%  ELAPSED TIME: 7.13 s      
-------------------------------------------------------------------------------- 
Loading data to table orders.orders_buckt_partition partition (ord_city=null) 
    Time taken to load dynamic partitions: 2.568 seconds 
   Loading partition {ord_city=Phoenix} 
   Loading partition {ord_city=Logan} 
   Loading partition {ord_city=Austin} 
   Loading partition {ord_city=Fremont} 
   Loading partition {ord_city=Beverly Hills} 
   Loading partition {ord_city=Los Angeles} 
   Loading partition {ord_city=New York} 
   Loading partition {ord_city=Omaha} 
   Loading partition {ord_city=Reno} 
   Loading partition {ord_city=Gaylord} 
    Time taken for adding to write entity : 3 
Partition orders.orders_buckt_partition{ord_city=Austin} stats: [numFiles=1, numRows=1, totalSize=22, rawDataSize=21] 
Partition orders.orders_buckt_partition{ord_city=Beverly Hills} stats: [numFiles=1, numRows=1, totalSize=29, rawDataSize=28] 
Partition orders.orders_buckt_partition{ord_city=Fremont} stats: [numFiles=1, numRows=1, totalSize=23, rawDataSize=22] 
Partition orders.orders_buckt_partition{ord_city=Gaylord} stats: [numFiles=1, numRows=1, totalSize=23, rawDataSize=22] 
Partition orders.orders_buckt_partition{ord_city=Logan} stats: [numFiles=1, numRows=1, totalSize=26, rawDataSize=25] 
Partition orders.orders_buckt_partition{ord_city=Los Angeles} stats: [numFiles=1, numRows=6, totalSize=166, rawDataSize=160] 
Partition orders.orders_buckt_partition{ord_city=New York} stats: [numFiles=1, numRows=1, totalSize=23, rawDataSize=22] 
Partition orders.orders_buckt_partition{ord_city=Omaha} stats: [numFiles=1, numRows=1, totalSize=25, rawDataSize=24] 
Partition orders.orders_buckt_partition{ord_city=Phoenix} stats: [numFiles=1, numRows=1, totalSize=23, rawDataSize=22] 
Partition orders.orders_buckt_partition{ord_city=Reno} stats: [numFiles=1, numRows=1, totalSize=20, rawDataSize=19] 
OK 
Time taken: 13.672 seconds 
hive&gt;  </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hive views</h1>
                </header>
            
            <article>
                
<p class="chapter-content">A Hive view is a logical table. It is just like any RDBMS view. The concept is the same. When a view is created, Hive will not store any data into it. When a view is created, Hive freezes the metadata. Hive does not support the materialized view concept of any RDBMS. The basic purpose of a view is to hide the query complexity. At times, HQL contains complex joins, subqueries, or filters. With the help of view, the entire query can be flattened out in a virtual table.</p>
<p class="chapter-content">When a view is created on an underlying table, any changes to that table, or even adding or deleting the table, are invalidated in the view. Also, when a view is created, it only changes the metadata. But when that view is accessed by a query, it triggers the MapReduce job. A view is a purely logical object with no associated storage (no support for materialized views is currently available in Hive). When a query references a view, the view's definition is evaluated in order to produce a set of rows for further processing by the query. (This is a conceptual description. In fact, as part of query optimization, Hive may combine the view's definition with the queries, for example, pushing filters from the query down into the view.)</p>
<p class="chapter-content">A view's schema is frozen at the time the view is created; subsequent changes to underlying tables (for example, adding a column) will not be reflected in the view's schema. If an underlying table is dropped or changed in an incompatible fashion, subsequent attempts to query the invalid view will fail. Views are read-only and may not be used as the target of <kbd>LOAD</kbd>/<kbd>INSERT</kbd>/<kbd>ALTER</kbd> for changing metadata. A view may contain <kbd>ORDER BY</kbd> and <kbd>LIMIT</kbd> clauses. If a referencing query also contains these clauses, the query-level clauses are evaluated after the view clauses (and after any other operations in the query). For example, if a view specifies <kbd>LIMIT 5</kbd> and a referencing query is executed as (<kbd>select * from v LIMIT 10</kbd>), then at most five rows will be returned.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Syntax of a view</h1>
                </header>
            
            <article>
                
<p class="chapter-content">Let's see a few examples of views:</p>
<pre>CREATE VIEW [IF NOT EXISTS] [db_name.]view_name [(column_name [COMMENT column_comment], ...) ] 
  [COMMENT view_comment] 
  [TBLPROPERTIES (property_name = property_value, ...)] 
  AS SELECT ...;</pre>
<p class="chapter-content">I will demonstrate the advantages of views using the following few examples. Let's assume we have two tables, <kbd>Table_X</kbd> and <kbd>Table_Y</kbd>, with the following schema: <kbd>Table_XXCol_1</kbd> string, <kbd>XCol_2</kbd> string, <kbd>XCol_3</kbd> string, <kbd>Table_YYCol_1</kbd> string, <kbd>YCol_2</kbd> string, <kbd>YCol_3</kbd> string, and <kbd>YCol_4</kbd> string. To create a view exactly like the base tables, use the following code:</p>
<pre>Create view table_x_view as select * from Table_X; </pre>
<p class="chapter-content">To create a view on selective columns of base tables, use the following:</p>
<pre>Create view table_x_view as select xcol_1,xcol_3  from Table_X; </pre>
<p class="chapter-content">To create a view to filter values of columns of base tables, we can use:</p>
<pre>Create view table_x_view as select * from Table_X where XCol_3 &gt; 40 and  XCol_2 is not null; </pre>
<p class="chapter-content">To create a view to hide query complexities:</p>
<pre>create view table_union_view  as select XCol_1, XCol_2, XCol_3,Null from Table_X 
   where XCol_2  = "AAA" 
   union all 
   select YCol_1, YCol_2, YCol_3, YCol_4 from Table_Y 
   where YCol_3 = "BBB"; 
     
   create view table_join_view as select * from Table_X 
   join Table_Y on Table_X. XCol_1 = Table_Y. YCol_1; </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hive indexes</h1>
                </header>
            
            <article>
                
<p class="chapter-content">The main purpose of the indexing is to search through the records easily and speed up the query. The goal of Hive indexing is to improve the speed of query lookup on certain columns of a table. Without an index, queries with predicates like <kbd>WHERE tab1.col1 = 10</kbd> load the entire table or partition and process all the rows. But if an index exists for <kbd>col1</kbd>, then only a portion of the file needs to be loaded and processed. The improvement in query speed that an index can provide comes at the cost of additional processing to create the index and disk space to store the index. There are two types of indexes:</p>
<ul>
<li>Compact index</li>
<li>Bitmap index</li>
</ul>
<p class="chapter-content">The main difference is in storing mapped values of the rows in the different blocks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compact index</h1>
                </header>
            
            <article>
                
<p class="chapter-content">In HDFS, the data is stored in blocks. But scanning which data is stored in which block is time consuming. Compact indexing stores the indexed column's value and its <kbd>blockId</kbd>. So the query will not go to the table. Instead, the query will directly go to the compact index, where the column value and <kbd>blockId</kbd> are stored. No need to scan all the blocks to find data! So, while performing a query, it will first check the index and then go directly into that block.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bitmap index</h1>
                </header>
            
            <article>
                
<p class="chapter-content">Bitmap indexing stores the combination of indexed column value and list of rows as a bitmap. Bitmap indexing is commonly used for columns with distinct values. Let's review a few examples: Base table, <kbd>Table_XXCol_1</kbd> Integer, <kbd>XCol_2</kbd> string, <kbd>XCol_3</kbd> integer, and <kbd>XCol_4</kbd> string. Create an index:</p>
<pre>CREATE INDEX table_x_idx_1 ON TABLE table_x (xcol_1) AS 'COMPACT';  
SHOW INDEX ON table_x_idx;  
DROP INDEX table_x_idx ON table_x; <br/><br/>CREATE INDEX table_x_idx_2 ON TABLE table_x (xcol_1) AS 'COMPACT' WITH DEFERRED REBUILD;  
ALTER INDEX table_x_idx_2 ON table_x REBUILD;  
SHOW FORMATTED INDEX ON table_x; </pre>
<p class="chapter-content">The preceding index is empty because it is created with the <kbd>DEFERRED REBUILD</kbd> clause, regardless of whether or not the table contains any data. After this index is created, the <kbd>REBUILD</kbd> command needs to be used to build the index structure. After creation of the index, if the data in the underlying table changes, the <kbd>REBUILD</kbd> command must be used to bring the index up to date. Create the index and store it in a text file:</p>
<pre>CREATE INDEX table_x_idx_3 ON TABLE table_x (table_x) AS 'COMPACT' ROW FORMAT DELIMITED  
FIELDS TERMINATED BY 't'  
STORED AS TEXTFILE; </pre>
<p class="chapter-content">Create a bitmap index:</p>
<pre>CREATE INDEX table_x_bitmap_idx_4 ON TABLE table_x (table_x) AS 'BITMAP' WITH DEFERRED REBUILD;  
ALTER INDEX table_x_bitmap_idx_4 ON table03 REBUILD;  
SHOW FORMATTED INDEX ON table_x; 
DROP INDEX table_x_bitmap_idx_4 ON table_x; </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">JSON documents using Hive</h1>
                </header>
            
            <article>
                
<p class="chapter-content">JSON, is a minimal readable format for structuring data. It is used primarily to transmit data between a server and web application as an alternative to XML. JSON is built on two structures:</p>
<ul>
<li>A collection of name/value pairs. In various languages, this is realized as an object, record, struct, dictionary, hash table, keyed list, or associative array.</li>
<li>An ordered list of values. In most languages, this is realized as an array, vector, list, or sequence.</li>
</ul>
<p class="chapter-content">Please read more on JSON at the following URL: <span class="URLPACKT"><a href="http://www.json.org/">http://www.json.org/</a>.</span><a href="http://www.json.org/"><br/></a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example 1 – Accessing simple JSON documents with Hive (Hive 0.14 and later versions)</h1>
                </header>
            
            <article>
                
<p class="chapter-content">In this example, we will see how to query simple JSON documents using HiveQL. Let's assume we want to access the following <kbd>Sample-Json-simple.json</kbd> file in <kbd>HiveSample-Json-simple.json</kbd>:</p>
<pre>{"username":"abc","tweet":"Sun shine is bright.","time1": "1366150681" } 
{"username":"xyz","tweet":"Moon light is mild .","time1": "1366154481" } </pre>
<p class="chapter-content">View the <kbd>Sample-Json-simple.json</kbd> file:</p>
<pre>[root@sandbox ~]# cat Sample-Json-simple.json 
{"username":"abc","tweet":"Sun shine is bright.","timestamp": 1366150681 } 
{"username":"xyz","tweet":"Moon light is mild .","timestamp": 1366154481 } 
[root@sandbox ~]#  </pre>
<p class="chapter-content">Load <kbd>Sample-Json-simple.json</kbd> into HDFS:</p>
<pre>[root@sandbox ~]# hadoop fs -mkdir  /user/hive-simple-data/ 
[root@sandbox ~]# hadoop fs -put Sample-Json-simple.json /user/hive-simple-data/ </pre>
<p class="chapter-content">Create an external Hive table, <kbd>simple_json_table</kbd>:</p>
<pre>hive&gt; use orders; 
OK 
Time taken: 1.147 seconds 
hive&gt;  
CREATE EXTERNAL TABLE simple_json_table ( 
username string, 
tweet string, 
time1 string) 
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe' 
LOCATION '/user/hive-simple-data/'; 
OK 
Time taken: 0.433 seconds 
hive&gt;  </pre>
<p class="chapter-content">Now verify the records:</p>
<pre>hive&gt; select * from simple_json_table ; 
OK 
abc      Sun shine is bright.    1366150681 
xyz      Moon light is mild .    1366154481 
Time taken: 0.146 seconds, Fetched: 2 row(s) 
hive&gt;  </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example 2 – Accessing nested JSON documents with Hive (Hive 0.14 and later versions)</h1>
                </header>
            
            <article>
                
<p class="chapter-content">We will see how to query Nested JSON documents using HiveQL. Let's assume we want to access the following <kbd>Sample-Json-complex.json</kbd> file in <kbd>HiveSample-Json-complex.json</kbd>:</p>
<pre>{"DocId":"Doc1","User1":{"Id":9192,"Username":"u2452","ShippingAddress":{"Address1":"6373 Sun Street","Address2":"apt 12","City":"Foster City","State":"CA"},"Orders":[{"ItemId":5343,"OrderDate":"12/23/2017"},{"ItemId":7362,"OrderDate":"12/24/2017"}]}} </pre>
<p class="chapter-content">Load <kbd>Sample-Json-simple.json</kbd> into HDFS:</p>
<pre>[root@sandbox ~]# hadoop fs -mkdir  /user/hive-complex-data/ 
[root@sandbox ~]# hadoop fs -put Sample-Json-complex.json /user/hive-complex-data/ </pre>
<p class="chapter-content">Create an external Hive table, <kbd>json_nested_table</kbd>:</p>
<pre>hive&gt;  
CREATE EXTERNAL TABLE json_nested_table( 
DocId string, 
user1 struct&lt;Id: int, username: string, shippingaddress:struct&lt;address1:string,address2:string,city:string,state:string&gt;, orders:array&lt;struct&lt;ItemId:int,orderdate:string&gt;&gt;&gt; 
) 
ROW FORMAT SERDE 
'org.apache.hive.hcatalog.data.JsonSerDe' 
LOCATION 
'/user/hive-complex-data/'; 
OK 
Time taken: 0.535 seconds 
hive&gt;  </pre>
<p class="chapter-content">Verify the records:</p>
<pre>hive&gt; select DocId,user1.username,user1.orders FROM json_nested_table; 
OK 
Doc1     u2452   [{"itemid":5343,"orderdate":"12/23/2017"},{"itemid":7362,"orderdate":"12/24/2017"}] 
Time taken: 0.598 seconds, Fetched: 1 row(s) 
hive&gt;  </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example 3 – Schema evolution with Hive and Avro (Hive 0.14 and later versions)</h1>
                </header>
            
            <article>
                
<p class="chapter-content">In production, we have to change the table structure to address new business requirements. The table schema has to change to add/delete/rename table columns. Any of these changes affect downstream ETL jobs adversely. In order avoid these, we have to make corresponding changes to ETL jobs and target tables.</p>
<p class="chapter-content">Schema evolution allows you to update the schema used to write new data while maintaining backwards compatibility with the schemas of your old data. Then you can read it all together as if all of the data has one schema. Please read more on Avro serialization at the following URL: <a href="https://avro.apache.org/">https://avro.apache.org/</a>. In the following example, I will demonstrate how Avro and Hive tables absorb the changes of source table's schema changes without ETL job failure. We will create a customer table in the MySQL database and load it to the target Hive external table using Avro files. Then we will add one more column to the source tables to see how a Hive table absorbs that change without any errors. Connect to MySQL to create a source table (<kbd>customer</kbd>):</p>
<pre>mysql -u root -p 
 
GRANT ALL PRIVILEGES ON *.* TO 'sales'@'localhost' IDENTIFIED BY 'xxx';  
 
mysql -u sales  -p 
 
mysql&gt; create database orders; 
 
mysql&gt; use orders; 
 
CREATE TABLE customer( 
cust_id INT , 
cust_name  VARCHAR(20) NOT NULL, 
cust_city VARCHAR(20) NOT NULL, 
PRIMARY KEY ( cust_id ) 
); </pre>
<p class="chapter-content">Insert records into the <kbd>customer</kbd> table:</p>
<pre>INSERT into customer (cust_id,cust_name,cust_city) values (1,'Sam James','Austin'); 
INSERT into customer (cust_id,cust_name,cust_city) values (2,'Peter Carter','Denver'); 
INSERT into customer (cust_id,cust_name,cust_city) values (3,'Doug Smith','Sunnyvale'); 
INSERT into customer (cust_id,cust_name,cust_city) values (4,'Harry Warner','Palo Alto'); 
 </pre>
<p class="chapter-content">On Hadoop, run the following <kbd>sqoop</kbd> command to import the <kbd>customer</kbd> table and store data in Avro files into HDFS:</p>
<pre> 
hadoop fs -rmr /user/sqoop_data/avro 
sqoop import -Dmapreduce.job.user.classpath.first=true  
--connect jdbc:mysql://localhost:3306/orders   
--driver com.mysql.jdbc.Driver  
--username sales --password xxx  
--target-dir /user/sqoop_data/avro  
--table customer  
--as-avrodatafile  
 </pre>
<p class="chapter-content">Verify the target HDFS folder:</p>
<pre>[root@sandbox ~]# hadoop fs -ls /user/sqoop_data/avro 
Found 7 items 
-rw-r--r--   1 root hdfs          0 2017-09-09 08:57 /user/sqoop_data/avro/_SUCCESS 
-rw-r--r--   1 root hdfs        472 2017-09-09 08:57 /user/sqoop_data/avro/part-m-00000.avro 
-rw-r--r--   1 root hdfs        475 2017-09-09 08:57 /user/sqoop_data/avro/part-m-00001.avro 
-rw-r--r--   1 root hdfs        476 2017-09-09 08:57 /user/sqoop_data/avro/part-m-00002.avro 
-rw-r--r--   1 root hdfs        478 2017-09-09 08:57 /user/sqoop_data/avro/part-m-00003.avro </pre>
<p class="chapter-content">Create a Hive external table to access Avro files:</p>
<pre>use orders; 
drop table customer ; 
CREATE EXTERNAL TABLE customer  
( 
cust_id INT , 
cust_name  STRING , 
cust_city STRING   
) 
STORED AS AVRO 
location '/user/sqoop_data/avro/'; </pre>
<p class="chapter-content">Verify the Hive <kbd>customer</kbd> table:</p>
<pre>hive&gt; select * from customer; 
OK 
1  Sam James   Austin 
2  Peter Carter      Denver 
3  Doug Smith  Sunnyvale 
4  Harry Warner      Palo Alto 
Time taken: 0.143 seconds, Fetched: 4 row(s) 
hive&gt;  
 </pre>
<p class="chapter-content">Perfect! We have no errors. We successfully imported the source <kbd>customer</kbd> table to the target Hive table using Avro serialization. Now, we add one column to the source table and import it again to verify that we can access the target Hive table without any schema changes. Connect to MySQL and add one more column:</p>
<pre>mysql -u sales  -p 
 
mysql&gt;  
ALTER TABLE customer 
ADD COLUMN cust_state VARCHAR(15) NOT NULL; 
 
mysql&gt; desc customer; 
+------------+-------------+------+-----+---------+-------+ 
| Field      | Type        | Null | Key | Default | Extra | 
+------------+-------------+------+-----+---------+-------+ 
| cust_id    | int(11)     | NO   | PRI | 0       |       | 
| cust_name  | varchar(20) | NO   |     | NULL    |       | 
| cust_city  | varchar(20) | NO   |     | NULL    |       | 
| CUST_STATE | varchar(15) | YES  |     | NULL    |       | 
+------------+-------------+------+-----+---------+-------+ 
4 rows in set (0.01 sec) 
 
mysql&gt;  </pre>
<p class="chapter-content">Now insert rows:</p>
<pre>INSERT into customer (cust_id,cust_name,cust_city,cust_state) values (5,'Mark Slogan','Huston','TX'); 
INSERT into customer (cust_id,cust_name,cust_city,cust_state) values (6,'Jane Miller','Foster City','CA'); </pre>
<p class="chapter-content">On Hadoop, run the following <kbd>sqoop</kbd> command to import the <kbd>customer</kbd> table so as to append the new address column and data. I have used the <kbd>append</kbd>  and <kbd>where "cust_id &gt; 4"</kbd> parameters to import only the new rows:</p>
<pre>sqoop import -Dmapreduce.job.user.classpath.first=true  
--connect jdbc:mysql://localhost:3306/orders   
--driver com.mysql.jdbc.Driver  
--username sales --password xxx  
--table customer  
--append  
--target-dir /user/sqoop_data/avro  
--as-avrodatafile  
--where "cust_id &gt; 4"  </pre>
<p class="chapter-content">Verify the HDFS folder:</p>
<pre>[root@sandbox ~]# hadoop fs -ls /user/sqoop_data/avro 
Found 7 items 
-rw-r--r--   1 root hdfs          0 2017-09-09 08:57 /user/sqoop_data/avro/_SUCCESS 
-rw-r--r--   1 root hdfs        472 2017-09-09 08:57 /user/sqoop_data/avro/part-m-00000.avro 
-rw-r--r--   1 root hdfs        475 2017-09-09 08:57 /user/sqoop_data/avro/part-m-00001.avro 
-rw-r--r--   1 root hdfs        476 2017-09-09 08:57 /user/sqoop_data/avro/part-m-00002.avro 
-rw-r--r--   1 root hdfs        478 2017-09-09 08:57 /user/sqoop_data/avro/part-m-00003.avro 
-rw-r--r--   1 root hdfs        581 2017-09-09 09:00 /user/sqoop_data/avro/part-m-00004.avro 
-rw-r--r--   1 root hdfs        586 2017-09-09 09:00 /user/sqoop_data/avro/part-m-00005.avro </pre>
<p class="chapter-content">Now, let's verify that our target Hive table is still able to access old and new Avro files:</p>
<pre>hive&gt; select * from customer; 
OK 
1  Sam James   Austin 
2  Peter Carter      Denver 
3  Doug Smith  Sunnyvale 
4  Harry Warner      Palo Alto 
Time taken: 0.143 seconds, Fetched: 4 row(s </pre>
<p class="chapter-content">Great! No errors. Still, it's business as usual; now we will add one new column to the Hive table to see the newly added Avro files:</p>
<pre> 
hive&gt; use orders; 
hive&gt; ALTER TABLE customer ADD COLUMNS (cust_state STRING); 
hive&gt; desc customer; 
OK 
cust_id              int                                          
cust_name            string                                       
cust_city            string                                       
cust_state           string                                       
Time taken: 0.488 seconds, Fetched: 4 row(s </pre>
<p class="chapter-content">Verify the Hive table for new data:</p>
<pre>hive&gt; select * from customer; 
OK 
1  Sam James   Austin      NULL 
2  Peter Carter      Denver      NULL 
3  Doug Smith  Sunnyvale   NULL 
4  Harry Warner      Palo Alto   NULL 
5  Mark Slogan Huston      TX 
6  Jane Miller Foster City CA 
Time taken: 0.144 seconds, Fetched: 6 row(s) 
hive&gt;  </pre>
<p class="chapter-content">Awesome! Take a look at customer IDs <kbd>5</kbd> and <kbd>6</kbd>. We can see the newly added column (<kbd>cust_state</kbd>) with values. You can experiment the delete column and replace column feature with the same technique. Now we have a fairly good idea about how to access data using Apache Hive. In the next section, we will learn about accessing data using HBase, which is a NoSQL data store.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache HBase</h1>
                </header>
            
            <article>
                
<p class="chapter-content">We have just learned about Hive, which is a database where users can access data using SQL commands. But there are certain databases where users cannot use SQL commands. Those databases are known as <strong>NoSQL data stores</strong>. HBase is a NoSQL database. So, what is actually meant by NoSQL? NoSQL means not only SQL. In NoSQL data stores like HBase, the main features of RDBMS, such as validation and consistency, are relaxed. Also, another important difference between RDBMS or SQL databases and NoSQL databases is schema on write versus schema on read. In schema on write, the data is validated at the time of writing to the table, whereas schema on read supports validation of data at the time of reading it. In this way, NoSQL data stores support storage of huge data velocity due to the relaxation of basic data validation at the time of writing data. There are about 150 NoSQL data stores in the market today. Each of these NoSQL data stores has some unique features to offer. Some popular NoSQL data stores are HBase, Cassandra, MongoDB, Druid, Apache Kudu, and Accumulo, and so on.</p>
<div class="packt_tip">You can get a detailed list of all types of NoSQL databases at <a href="http://nosql-database.org/">http://nosql-database.org/</a>.</div>
<p class="chapter-content">HBase is a popular NoSQL database used by many big companies such as Facebook, Google, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Differences between HDFS and HBase</h1>
                </header>
            
            <article>
                
<p class="chapter-content">The following explains the key difference between HDFS and HBase. Hadoop is built on top of HDFS, which has support for storing large volumes (petabytes) of datasets. These datasets are accessed using batch jobs, by using MapReduce algorithms. In order to find a data element in such a huge dataset, the entire dataset needs to be scanned. HBase, on the other hand, is built on top of HDFS and provides fast record lookups (and updates) for large tables. HBase internally puts your data in indexed StoreFiles that exist on HDFS for high-speed lookup.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Differences between Hive and HBase</h1>
                </header>
            
            <article>
                
<p class="chapter-content">HBase is a database management system; it supports both transaction processing and analytical processing. Hive is a data warehouse system, which can be used only for analytical processing. HBase supports low latency and random data access operations. Hive only supports batch processing, which leads to high latency. HBase does not support any SQL interface to interact with the table data. You may have to write Java code to read and write data to HBase tables. At times, Java code becomes very complex to process data sets involving joins of multiple data sets. But Hive supports very easy access with SQL, which makes it very easy to read and write data to its tables. In HBase, data modeling involves flexible data models and column-oriented data storage, which must support data denormalization. The columns of HBase tables are decided at the time of writing data into the tables. In Hive, the data model involves tables with a fixed schema like an RDBMS data model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Key features of HBase</h1>
                </header>
            
            <article>
                
<p class="chapter-content">The following are a few key features of HBase:</p>
<ul>
<li class="chapter-content"><strong>Sorted rowkeys</strong>: In HBase, data processing is down with three basic operations/APIs: get, put, and scan. All three of these APIs access data using rowkeys to ensure smooth data access. As scans are done over a range of rows, HBase lexicographically orders rows according to their rowkeys. Using these sorted rowkeys, a scan can be defined simply from its start and stop rowkeys. This is extremely powerful to get all relevant data in a single database call. The application developer can design a system to access recent datasets by querying recent rows based on their timestamp as all rows are stored in a table in sorted order based on the latest timestamp.</li>
<li class="chapter-content"><strong>Control data sharding</strong>: HBase Table rowkey strongly influences data sharding. Table data is sorted in ascending order by rowkey, column families, and column key. A solid rowkey design is very important to ensure data is evenly distributed across the Hadoop cluster. As rowkeys determine the sort order of a table's row, each region in the table ends up being responsible for the physical storage of a part of the row key space.</li>
<li class="chapter-content"><strong>Strong consistency</strong>: HBase favors consistency over availability. It also supports ACID-level semantics on a per row basis. It, of course, impacts the write performance, which will tend to be slower. Overall, the trade-off plays in favor of the application developer, who will have the guarantee that the data store always the right value of the data.</li>
<li class="chapter-content"><strong>Low latency processing</strong>: HBase supports fast, random reads and writes to all data stored.</li>
<li class="chapter-content"><strong>Flexibility</strong>: HBase supports any type—structured, semi-structured, unstructured.</li>
<li class="chapter-content"><strong>Reliability</strong>: HBase table data block is replicated multiple times to ensure protection against data loss. HBase also supports fault tolerance. The table data is always available for processing even in case of failure of any regional server.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">HBase data model</h1>
                </header>
            
            <article>
                
<p class="chapter-content">These are the key components of an HBase data model:</p>
<ul>
<li class="chapter-content"><strong>Table</strong>: In HBase, data is stored in a logical object, called <strong>table</strong>, that has multiple rows.</li>
<li class="chapter-content"><strong>Row</strong>: <span>A row in HBase consists of a row key and one or more columns. The row key sorts rows. The goal is to store data in such a way that related rows are near each other. The row key can be a combination of one of more columns. The row key is like the primary key of the table, which must be unique. HBase uses row keys to find data in a column. For example, <kbd>customer_id</kbd> can be a row key for the <kbd>customer</kbd> table.</span></li>
<li class="chapter-content"><strong>Column</strong>: A<span> </span><span>column in HBase consists of a<span> </span>column<span> </span>family and a column qualifier.</span></li>
<li class="chapter-content"><strong>Column qualifier</strong>: It<span> </span><span>is the<span> </span>column<span> </span>name of a table.</span></li>
<li class="chapter-content"><strong>Cell</strong>: This <span>is a combination of row, column family, and<span> </span>column<span> </span>qualifier, and contains a value and a timestamp which represents the value's version.</span></li>
<li class="chapter-content"><strong>Column family</strong>: It <span>is a collection of columns that are co-located and stored together, often for performance reasons. Each column family has a set of storage properties, such as cached, compression, and data encodation.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Difference between RDBMS table and column - oriented data store</h1>
                </header>
            
            <article>
                
<p class="chapter-content">We all know how data is stored in any RDBMS table. It looks like this:</p>
<table class="table">
<tbody>
<tr>
<td>
<p><strong>ID</strong></p>
</td>
<td>
<p><kbd>Column_1</kbd></p>
</td>
<td>
<p><kbd>Column_2</kbd></p>
</td>
<td>
<p><kbd>Column_3</kbd></p>
</td>
<td>
<p><kbd>Column_4</kbd></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>A</p>
</td>
<td>
<p>11</p>
</td>
<td>
<p>P</p>
</td>
<td>
<p>XX</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>B</p>
</td>
<td>
<p>12</p>
</td>
<td>
<p>Q</p>
</td>
<td>
<p>YY</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>C</p>
</td>
<td>
<p>13</p>
</td>
<td>
<p>R</p>
</td>
<td>
<p>ZZ</p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p>D</p>
</td>
<td>
<p>14</p>
</td>
<td>
<p>S</p>
</td>
<td>
<p>XX1</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="chapter-content">The column ID is used as a unique/primary key of the table to access data from other columns of the table. But in a column-oriented data store like HBase, the same table is divided into key and value and is stored like this:</p>
<table class="table">
<tbody>
<tr>
<td colspan="2"><strong>Key</strong></td>
<td><strong>Value</strong></td>
</tr>
<tr>
<td><strong>Row</strong></td>
<td><strong>Column</strong></td>
<td><strong>Column Value</strong></td>
</tr>
<tr>
<td>1</td>
<td><kbd>Column_1</kbd></td>
<td><kbd>A</kbd></td>
</tr>
<tr>
<td>1</td>
<td><kbd>Column_2</kbd></td>
<td><kbd>11</kbd></td>
</tr>
<tr>
<td>1</td>
<td><kbd>Column_3</kbd></td>
<td><kbd>P</kbd></td>
</tr>
<tr>
<td>1</td>
<td><kbd>Column_4</kbd></td>
<td><kbd>XX</kbd></td>
</tr>
<tr>
<td>2</td>
<td><kbd>Column_1</kbd></td>
<td><kbd>B</kbd></td>
</tr>
<tr>
<td>2</td>
<td><kbd>Column_2</kbd></td>
<td><kbd>12</kbd></td>
</tr>
<tr>
<td>2</td>
<td><kbd>Column_3</kbd></td>
<td><kbd>Q</kbd></td>
</tr>
<tr>
<td>2</td>
<td><kbd>Column_4</kbd></td>
<td><kbd>YY</kbd></td>
</tr>
<tr>
<td>3</td>
<td><kbd>Column_1</kbd></td>
<td><kbd>C</kbd></td>
</tr>
<tr>
<td>3</td>
<td><kbd>Column_2</kbd></td>
<td><kbd>13</kbd></td>
</tr>
<tr>
<td>3</td>
<td><kbd>Column_3</kbd></td>
<td><kbd>R</kbd></td>
</tr>
<tr>
<td>3</td>
<td><kbd>Column_4</kbd></td>
<td><kbd>ZZ</kbd></td>
</tr>
</tbody>
</table>
<p class="chapter-content">In HBase, each table is a sorted map format, where each key is sorted in ascending order. Internally, each key and value is serialized and stored on the disk in byte array format. Each column value is accessed by its corresponding key. So, in the preceding table, we define a key, which is a combination of two columns, <em>row + column</em>. For example, in order to access the <kbd>Column_1</kbd> data element of row 1, we have to use a key, row 1 + <kbd>column_1</kbd>. That's the reason the row key design is very crucial in HBase. Before creating the HBase table, we have to decide a column family for each column. A column family is a collection of columns, which are co-located and stored together, often for performance reasons. Each column family has a set of storage properties, such as cached, compression, and data encodation. For example, in a typical <kbd>CUSTOMER</kbd> table, we can define two column families, namely <kbd>cust_profile</kbd> and <kbd>cust_address</kbd>. All columns related to the customer address are assigned to the column family <kbd>cust_address</kbd>; all other columns, namely <kbd>cust_id</kbd>, <kbd>cust_name</kbd>, and <kbd>cust_age</kbd>, are assigned to the column family <kbd>cust_profile</kbd>. After assigning the column families, our sample table will look like the following:</p>
<table class="table">
<tbody>
<tr>
<td colspan="2"><strong>Key</strong></td>
<td colspan="3"><strong>Value</strong></td>
</tr>
<tr>
<td><strong>Row</strong></td>
<td><strong>Column</strong></td>
<td><strong>Column family</strong></td>
<td><strong>Value</strong></td>
<td><strong>Timestamp</strong></td>
</tr>
<tr>
<td>1</td>
<td><kbd>Column_1</kbd></td>
<td><kbd>cf_1</kbd></td>
<td><kbd>A</kbd></td>
<td><kbd>1407755430</kbd></td>
</tr>
<tr>
<td>1</td>
<td><kbd>Column_2</kbd></td>
<td><kbd>cf_1</kbd></td>
<td><kbd>11</kbd></td>
<td><kbd>1407755430</kbd></td>
</tr>
<tr>
<td>1</td>
<td><kbd>Column_3</kbd></td>
<td><kbd>cf_1</kbd></td>
<td><kbd>P</kbd></td>
<td><kbd>1407755430</kbd></td>
</tr>
<tr>
<td>1</td>
<td><kbd>Column_4</kbd></td>
<td><kbd>cf_2</kbd></td>
<td><kbd>XX</kbd></td>
<td><kbd>1407755432</kbd></td>
</tr>
<tr>
<td>2</td>
<td><kbd>Column_1</kbd></td>
<td><kbd>cf_1</kbd></td>
<td><kbd>B</kbd></td>
<td><kbd>1407755430</kbd></td>
</tr>
<tr>
<td>2</td>
<td><kbd>Column_2</kbd></td>
<td><kbd>cf_1</kbd></td>
<td><kbd>12</kbd></td>
<td><kbd>1407755430</kbd></td>
</tr>
<tr>
<td>2</td>
<td><kbd>Column_3</kbd></td>
<td><kbd>cf_1</kbd></td>
<td><kbd>Q</kbd></td>
<td><kbd>1407755430</kbd></td>
</tr>
<tr>
<td>2</td>
<td><kbd>Column_4</kbd></td>
<td><kbd>cf_2</kbd></td>
<td><kbd>YY</kbd></td>
<td><kbd>1407755432</kbd></td>
</tr>
<tr>
<td>3</td>
<td><kbd>Column_1</kbd></td>
<td><kbd>cf_1</kbd></td>
<td><kbd>C</kbd></td>
<td><kbd>1407755430</kbd></td>
</tr>
<tr>
<td>3</td>
<td><kbd>Column_2</kbd></td>
<td><kbd>cf_1</kbd></td>
<td><kbd>13</kbd></td>
<td><kbd>1407755430</kbd></td>
</tr>
<tr>
<td>3</td>
<td><kbd>Column_3</kbd></td>
<td><kbd>cf_1</kbd></td>
<td><kbd>R</kbd></td>
<td><kbd>1407755430</kbd></td>
</tr>
<tr>
<td>3</td>
<td><kbd>Column_4</kbd></td>
<td><kbd>cf_2</kbd></td>
<td><kbd>ZZ</kbd></td>
<td><kbd>1407755432</kbd></td>
</tr>
</tbody>
</table>
<p class="chapter-content">While inserting data into a table, HBase will automatically <span>add </span>a timestamp for each version of the cell.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">HBase architecture</h1>
                </header>
            
            <article>
                
<p class="chapter-content">If we want to read data from an HBase table, we have to give an appropriate row ID, and HBase will perform a lookup based on the given row ID. HBase uses the following sorted nested map to return the column value of the row ID: row ID a column family, a column at timestamp, and value. HBase is always deployed on Hadoop. The following is a typical installation:</p>
<div class="chapter-content CDPAlignCenter CDPAlign"><img src="assets/2b1fd55d-59df-48b3-b0a5-d5aacf0c836e.png"/></div>
<p class="chapter-content">It is a master server of the HBase cluster and is responsible for the administration, monitoring, and management of RegionServers, such as assignment of regions to RegionServer, region splits, and so on. In a distributed cluster, the HMaster typically runs on the Hadoop NameNode. </p>
<p class="chapter-content">ZooKeeper is a coordinator of HBase cluster. HBase uses ZooKeeper as a distributed coordination service to maintain server state in the cluster. ZooKeeper maintains which servers are alive and available, and provides server failure notification. RegionServer is responsible for management of regions. RegionServer is deployed on DataNode. It serves data for reads and writes. RegionServer is comprised of the following additional components:</p>
<ul>
<li class="chapter-content">Regions: HBase tables are divided horizontally by row key range into regions. A region contains all rows in the table between the region's start key and end key. <strong>Write-ahead logging</strong> (<strong>WAL</strong>) is used to store new data that has not yet stored on disk.</li>
<li class="chapter-content">MemStore is a write cache. It stores new data that has not yet been written to disk. It is sorted before writing to disk. There is one MemStore per column family per region. Hfile stores the rows as sorted key/values on disk/HDFS:</li>
</ul>
<div class="chapter-content CDPAlignCenter CDPAlign"><img src="assets/79ed20fa-ef32-4b86-9fc6-d0cb11127b8f.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">HBase architecture in a nutshell</h1>
                </header>
            
            <article>
                
<ul>
<li>The HBase cluster is comprised of one active master and one or more backup master servers</li>
<li>The cluster has multiple RegionServers</li>
<li>The HBase table is always large and rows are divided into partitions/shards called <strong>regions</strong></li>
<li>Each RegionServer hosts one or many regions</li>
<li>The HBase catalog is known as META table, which stores the locations of table regions</li>
<li>ZooKeeper stores the locations of the META table</li>
<li>During a write, the client sends the put request to the HRegionServer</li>
<li>Data is written to WAL</li>
<li>Then data is pushed into MemStore and an acknowledgement is sent to the client</li>
<li>Once enough data is accumulated in MemStore, it flushes data to the Hfile on HDFS</li>
<li>The HBase compaction process activates periodically to merge multiple HFiles into one Hfile (called <strong>compaction</strong>)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">HBase rowkey design</h1>
                </header>
            
            <article>
                
<p class="chapter-content">Rowkey design is a very crucial part of HBase table design. During key design, proper care must be taken to avoid hotspotting. In case of poorly designed keys, all of the data will be ingested into just a few nodes, leading to cluster imbalance. Then, all the reads have to be pointed to those few nodes, resulting in slower data reads. We have to design a key that will help load data equally to all nodes of the cluster. Hotspotting can be avoided by the following techniques:</p>
<ul>
<li class="chapter-content"><strong>Key salting</strong>: It means adding an arbitrary value at the beginning of the key to make sure that the rows are distributed equally among all the table regions. Examples are <kbd>aa-customer_id</kbd>, <kbd>bb-customer_id</kbd>, and so on.</li>
<li class="chapter-content"><strong>Key hashing</strong><span>: The k</span><span>ey can be hashed and the hashing value can be used as a rowkey, for example, <kbd>HASH(customer_id)</kbd>.</span></li>
<li class="chapter-content"><strong>Key with reverse timestamp</strong><span>: In this technique, you have to define a regular key and then attach a reverse timestamp to it. The timestamp has to be reversed by subtracting it from any arbitrary maximum value and then attached to the key. For example, if <kbd>customer_id</kbd> is your row ID, the new key will be <kbd>customer_id</kbd> + reverse timestamp.</span></li>
</ul>
<p><span>The following are the guidelines while designing a HBase table:</span></p>
<ul>
<li>Define no more than two column families per table</li>
<li>Keep column family names as small as possible</li>
<li>Keep column names as small as possible</li>
<li>Keep the rowkey length as small as possible</li>
<li>Do not set the row version at a high level</li>
<li>The table should not have more than 100 regions</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example 4 – loading data from MySQL table to HBase table</h1>
                </header>
            
            <article>
                
<p class="chapter-content">We will use the same <kbd>customer</kbd> table that we created earlier:</p>
<pre class="chapter-content">mysql -u sales -p<br/>mysql&gt; use orders;<br/>mysql&gt; select * from customer;<br/>+---------+--------------+--------------+------------+<br/>| cust_id | cust_name | cust_city | InsUpd_on |<br/>+---------+--------------+--------------+------------+<br/>| 1 | Sam James | Austin | 1505095030 |<br/>| 2 | Peter Carter | Denver | 1505095030 |<br/>| 3 | Doug Smith | Sunnyvale | 1505095030 |<br/>| 4 | Harry Warner | Palo Alto | 1505095032 |<br/>| 5 | Jen Turner | Cupertino | 1505095036 |<br/>| 6 | Emily Stone | Walnut Creek | 1505095038 |<br/>| 7 | Bill Carter | Greenville | 1505095040 |<br/>| 8 | Jeff Holder | Dallas | 1505095042 |<br/>| 10 | Mark Fisher | Mil Valley | 1505095044 |<br/>| 11 | Mark Fisher | Mil Valley | 1505095044 |<br/>+---------+--------------+--------------+------------+<br/>10 rows in set (0.00 sec)</pre>
<p>Start HBase and create a <kbd>customer</kbd> table in HBase:</p>
<pre>hbase shell<br/>create 'customer','cf1'</pre>
<p>Load MySQL <kbd>customer</kbd> table data in HBase using Sqoop:</p>
<pre>hbase<br/>sqoop import --connect jdbc:mysql://localhost:3306/orders --driver com.mysql.jdbc.Driver --username sales --password sales1 --table customer --hbase-table customer --column-family cf1 --hbase-row-key cust_id</pre>
<p>Verify the HBase table:</p>
<pre>hbase shell<br/>scan 'customer'</pre>
<p>You must see all 11 rows in the HBase table.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example 5 – incrementally loading data from MySQL table to HBase table</h1>
                </header>
            
            <article>
                
<pre class="chapter-content">mysql -u sales -p<br/>mysql&gt; use orders;</pre>
<p>Insert a new customer and update the existing one:</p>
<pre class="chapter-content">mysql&gt; Update customer set cust_city = 'Dublin', InsUpd_on = '1505095065' where cust_id = 4;<br/>Query OK, 1 row affected (0.00 sec)<br/>Rows matched: 1 Changed: 1 Warnings: 0<br/><br/>mysql&gt; INSERT into customer (cust_id,cust_name,cust_city,InsUpd_on) values (12,'Jane Turner','Glen Park',1505095075);<br/>Query OK, 1 row affected (0.00 sec)<br/><br/>mysql&gt; commit;<br/>Query OK, 0 rows affected (0.00 sec)<br/><br/>mysql&gt; select * from customer;<br/>+---------+--------------+--------------+------------+<br/>| cust_id | cust_name | cust_city | InsUpd_on |<br/>+---------+--------------+--------------+------------+<br/>| 1 | Sam James | Austin | 1505095030 |<br/>| 2 | Peter Carter | Denver | 1505095030 |<br/>| 3 | Doug Smith | Sunnyvale | 1505095030 |<br/>| 4 | Harry Warner | <strong>Dublin</strong> | 1505095065 |<br/>| 5 | Jen Turner | Cupertino | 1505095036 |<br/>| 6 | Emily Stone | Walnut Creek | 1505095038 |<br/>| 7 | Bill Carter | Greenville | 1505095040 |<br/>| 8 | Jeff Holder | Dallas | 1505095042 |<br/>| 10 | Mark Fisher | Mil Valley | 1505095044 |<br/>| 11 | Mark Fisher | Mil Valley | 1505095044 |<br/><strong>| 12 | Jane Turner | Glen Park | 1505095075 |<br/></strong>+---------+--------------+--------------+------------+<br/>11 rows in set (0.00 sec)<br/>mysql&gt;</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example 6 – Load the MySQL customer changed data into the HBase table</h1>
                </header>
            
            <article>
                
<p class="chapter-content">Here, we have used the <kbd>InsUpd_on</kbd> column as our ETL date:</p>
<pre class="chapter-content">sqoop import --connect jdbc:mysql://localhost:3306/orders --driver com.mysql.jdbc.Driver --username sales --password sales1 --table customer --hbase-table customer --column-family cf1 --hbase-row-key cust_id --append -- -m 1 --<strong>where "InsUpd_on &gt; 1505095060</strong>"<br/><br/>hbase shell<br/>hbase(main):010:0&gt; get 'customer', '4'<br/>COLUMN          CELL<br/>cf1:InsUpd_on   timestamp=1511509774123, value=1505095065<br/>cf1:cust_city   timestamp=1511509774123<strong>, value=Dublin<br/></strong>cf1:cust_name   timestamp=1511509774123, value=Harry Warner<br/>3 row(s) in 0.0200 seconds<br/><br/>hbase(main):011:0&gt; get 'customer', '12'<br/>COLUMN           CELL<br/>cf1:InsUpd_on    timestamp=1511509776158, value=1505095075<br/>cf1:cust_city    timestamp=1511509776158, value=Glen Park<br/>cf1:cust_name    timestamp=1511509776158, value=Jane Turner<br/>3 row(s) in 0.0050 seconds<br/><br/>hbase(main):012:0&gt;</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example 7 – Hive HBase integration</h1>
                </header>
            
            <article>
                
<p class="chapter-content">Now, we will access the HBase <kbd>customer</kbd> table using the Hive external table:</p>
<pre class="chapter-content">create external table customer_hbase(cust_id string, cust_name string, cust_city string, InsUpd_on string)<br/>STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'with serdeproperties ("hbase.columns.mapping"=":key,cf1:cust_name,cf1:cust_city,cf1:InsUpd_on")tblproperties("hbase.table.name"="customer");<br/><br/>hive&gt; select * from customer_hbase;<br/>OK<br/>1 Sam James Austin 1505095030<br/>10 Mark Fisher Mil Valley 1505095044<br/>11 Mark Fisher Mil Valley 1505095044<br/>12 Jane Turner Glen Park 1505095075<br/>2 Peter Carter Denver 1505095030<br/>3 Doug Smith Sunnyvale 1505095030<br/>4 Harry Warner Dublin 1505095065<br/>5 Jen Turner Cupertino 1505095036<br/>6 Emily Stone Walnut Creek 1505095038<br/>7 Bill Carter Greenville 1505095040<br/>8 Jeff Holder Dallas 1505095042<br/>Time taken: 0.159 seconds, Fetched: 11 row(s)<br/>hive&gt;</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="chapter-content">In this chapter, we saw how data is stored and accessed using a Hadoop SQL interface called Hive. We studied various partitioning and indexing strategies in Hive. The working examples helped us to understand JSON data access and schema evolution using Avro in Hive. In the second section of the chapter, we studied a NoSQL data store called HBase and its difference with respect to RDBMS. The row design of the HBase table is very crucial to balancing reads and writes to avoid region hotspots. One has to keep in mind the HBase table design best practices discussed in this chapter. The working example shows the easier paths of data ingestions into an HBase table and its integration with Hive.</p>
<p class="chapter-content">In the next chapter, we will take a look at tools and techniques for designing real-time data analytics.</p>


            </article>

            
        </section>
    </body></html>