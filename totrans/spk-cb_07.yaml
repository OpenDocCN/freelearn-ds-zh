- en: Chapter 7. Supervised Learning with MLlib – Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章. 使用MLlib进行监督学习 – 回归
- en: 'This chapter is divided into the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章分为以下几个部分：
- en: Using linear regression
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用线性回归
- en: Understanding the cost function
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解成本函数
- en: Doing linear regression with lasso
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用lasso进行线性回归
- en: Doing ridge regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行岭回归
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: 'The following is Wikipedia''s definition of supervised learning:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是维基百科对监督学习的定义：
- en: '*"Supervised learning is the machine learning task of inferring a function
    from labeled training data."*'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*"监督学习是从标记的训练数据中推断函数的机器学习任务。"*'
- en: 'Supervised learning has two steps:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习有两个步骤：
- en: Train the algorithm with training dataset; it is like giving questions and their
    answers first
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练数据集训练算法；这就像先给出问题和答案。
- en: Use test dataset to ask another set of questions to the trained algorithm
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用测试数据集向训练好的算法提出另一组问题
- en: 'There are two types of supervised learning algorithms:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习有两种类型的算法：
- en: '**Regression**: This predicts continuous value output, such as house price.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：这预测的是连续值输出，例如房价。'
- en: '**Classification**: This predicts discreet valued output (0 or 1) called label,
    such as whether an e-mail is a spam or not. Classification is not limited to two
    values; it can have multiple values such as marking an e-mail important, not important,
    urgent, and so on (0, 1, 2…).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：这预测的是离散值输出（0或1）称为标签，例如一封电子邮件是否是垃圾邮件。分类不仅限于两个值；它可以有多个值，例如标记一封电子邮件为重要、不重要、紧急等（0，1，2…）。'
- en: Note
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We are going to cover regression in this chapter and classification in the next.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章介绍回归，在下一章介绍分类。
- en: 'As an example dataset for regression, we will use the recently sold house data
    of the City of Saratoga, CA, as a training set to train the algorithm. Once the
    algorithm is trained, we will ask it to predict the house price by the given size
    of that house. The following figure illustrates the workflow:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为回归的示例数据集，我们将使用加利福尼亚州萨拉托加市最近售出的房屋数据作为训练集来训练算法。一旦算法被训练，我们将要求它根据该房屋的大小预测房价。以下图示了工作流程：
- en: '![Introduction](img/3056_07_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![简介](img/3056_07_01.jpg)'
- en: Hypothesis, for what it does, may sound like a misnomer here, and you may think
    that prediction function may be a better name, but the word hypothesis is used
    for historic reasons.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，就其功能而言，可能在这里听起来像是一个误称，你可能认为预测函数可能是一个更好的名称，但“假设”这个词是历史原因而使用的。
- en: If we use only one feature to predict the outcome, it is called **bivariate
    analysis**. When we have multiple features, it is called **multivariate analysis**.
    In fact, we can have as many features as we like. One such algorithm, **support
    vector machines** (**SVM**), which we will cover in the next chapter, in fact,
    allows you to have an infinite number of features.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只使用一个特征来预测结果，这被称为**双变量分析**。当我们有多个特征时，这被称为**多元分析**。实际上，我们可以有我们喜欢的任意数量的特征。其中一种算法是**支持向量机**（**SVM**），我们将在下一章介绍，实际上，它允许你拥有无限数量的特征。
- en: This chapter will cover how we can do supervised learning using MLlib, Spark's
    machine learning library.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍如何使用MLlib进行监督学习，MLlib是Spark的机器学习库。
- en: Note
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Mathematical explanations have been provided in as simple a way as possible,
    but you can feel free to skip math and directly go to *How to do it...* section.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 数学解释已经尽可能地简化，但你可以自由地跳过数学部分，直接进入*如何操作…*部分。
- en: Using linear regression
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用线性回归
- en: Linear regression is the approach to model the value of a response variable
    *y*, based on one or more predictor variables or feature *x*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是建模响应变量*y*的值的方法，基于一个或多个预测变量或特征*x*。
- en: Getting ready
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Let''s use some housing data to predict the price of a house based on its size.
    The following are the sizes and prices of houses in the City of Saratoga, CA,
    in early 2014:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一些住房数据来预测房屋的价格，基于其大小。以下是在2014年初加利福尼亚州萨拉托加市房屋的大小和价格：
- en: '| House size (sq ft) | Price |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 房屋面积（平方英尺） | 价格 |'
- en: '| --- | --- |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 2100 | $ 1,620,000 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 2100 | $ 1,620,000 |'
- en: '| 2300 | $ 1,690,000 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 2300 | $ 1,690,000 |'
- en: '| 2046 | $ 1,400,000 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 2046 | $ 1,400,000 |'
- en: '| 4314 | $ 2,000,000 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 4314 | $ 2,000,000 |'
- en: '| 1244 | $ 1,060,000 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 1244 | $ 1,060,000 |'
- en: '| 4608 | $ 3,830,000 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 4608 | $ 3,830,000 |'
- en: '| 2173 | $ 1,230,000 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 2173 | $ 1,230,000 |'
- en: '| 2750 | $ 2,400,000 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 2750 | $ 2,400,000 |'
- en: '| 4010 | $ 3,380,000 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 4010 | $ 3,380,000 |'
- en: '| 1959 | $ 1,480,000 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 1959 | $ 1,480,000 |'
- en: 'Here''s a graphical representation of the same:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是同样的图形表示：
- en: '![Getting ready](img/3056_07_04.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![准备工作](img/3056_07_04.jpg)'
- en: How to do it…
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Start the Spark shell:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 Spark shell：
- en: '[PRE0]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the statistics and related classes:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入统计和相关类：
- en: '[PRE1]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create the `LabeledPoint` array with the house price as the label:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建带有房价作为标签的 `LabeledPoint` 数组：
- en: '[PRE2]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create an RDD of the preceding data:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建前面数据的 RDD：
- en: '[PRE3]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Train a model using this data using 100 iterations. Here, step size has been
    kept small to account for very large values of response variables, that is, the
    house price. The fourth parameter is a fraction of the dataset to use per iteration,
    and the last parameter is the initial set of weights to be used (weights of different
    features):'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用100次迭代训练这个数据集的模型。在这里，步长被保持得很小，以考虑到响应变量（即房价）的非常大的值。第四个参数是每次迭代要使用的数据集的分数，最后一个参数是要使用的初始权重集（不同特征的权重）：
- en: '[PRE4]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Predict the price for a house with 2,500 sq ft:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测一栋2500平方英尺房子的价格：
- en: '[PRE5]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: House size is just one predictor variable. House price depends upon other variables,
    such as lot size, age of the house, and so on. The more variables you have, the
    better your prediction will be.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 房屋面积只是一个预测变量。房价取决于其他变量，如地块大小、房屋年龄等。你拥有的变量越多，你的预测就会越好。
- en: Understanding cost function
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解成本函数
- en: Cost function or loss function is a very important function in machine learning
    algorithms. Most algorithms have some form of cost function and the goal is to
    minimize that. Parameters, which affect cost function, such as `stepSize` in the
    last recipe, need to be set by hand. Therefore, understanding the whole concept
    of cost function is very important.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数或损失函数是机器学习算法中一个非常重要的函数。大多数算法都有某种形式的成本函数，目标是使其最小化。影响成本函数的参数，如上一个菜谱中的 `stepSize`，需要手动设置。因此，理解成本函数的整个概念非常重要。
- en: In this recipe, we are going to analyze cost function for linear regression.
    Linear regression is a simple algorithm to understand and it will help readers
    understand the role of cost functions for even complex algorithms.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将分析线性回归的成本函数。线性回归是一个简单的算法，它将帮助读者理解成本函数在复杂算法中的作用。
- en: Let's go back to linear regression. The goal is to find the best-fitting line
    so that the mean square of error is minimum. Here, we are referring error as the
    difference between the value as per the best-fitting line and the actual value
    of the response variable for the training dataset.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到线性回归。目标是找到最佳拟合线，使得均方误差最小。在这里，我们将误差定义为最佳拟合线的值与训练数据集中响应变量的实际值之间的差异。
- en: 'For a simple case of single predicate variable, the best-fit line can be written
    as:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单变量谓词的简单情况，最佳拟合线可以写成：
- en: '![Understanding cost function](img/3056_07_02.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_02.jpg)'
- en: 'This function is also called **hypothesis function**, and can be written as:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数也称为**假设函数**，可以写成：
- en: '![Understanding cost function](img/3056_07_03.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_03.jpg)'
- en: 'The goal of the linear regression is to find the best-fit line. On this line,
    θ[0] represents intercept on the *y* axis and θ[1] represents the slope of the
    line as obvious from the following equation:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的目标是找到最佳拟合线。在这条线上，θ[0] 代表 *y* 轴上的截距，θ[1] 代表线的斜率，从以下方程中可以明显看出：
- en: '![Understanding cost function](img/3056_07_05.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_05.jpg)'
- en: 'We have to choose θ[0] and θ[1] in a way that *h(x)* is closest to *y* for
    the training dataset. So, for the *i* ^(th) data point, the square of distance
    between the line and data point is:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须选择 θ[0] 和 θ[1]，使得 *h(x)* 最接近训练数据集中的 *y*。因此，对于 *i*^(th) 个数据点，直线和数据点之间的距离平方是：
- en: '![Understanding cost function](img/3056_07_06.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_06.jpg)'
- en: 'To put it in words, this is the square of the difference between the predicted
    house price and the actual price the house got sold for. Now, let''s take average
    of this value across the training dataset:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 用话来说，这是预测房价和实际售价之间差异的平方。现在，让我们取训练数据集中这个值的平均值：
- en: '![Understanding cost function](img/3056_07_07.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_07.jpg)'
- en: The preceding equation is called the cost function *J* for linear regression.
    The goal is to minimize this cost function.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程称为线性回归的成本函数 *J*。目标是使这个成本函数最小化。
- en: '![Understanding cost function](img/3056_07_08.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![理解成本函数](img/3056_07_08.jpg)'
- en: This cost function is also called **squared error function**. Both θ[0] and
    theta θ[1] follow convex curve independently if they are plotted against *J*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个成本函数也称为**平方误差函数**。如果将 θ[0] 和 θ[1] 分别与 *J* 绘制，它们各自遵循凸曲线。
- en: 'Let''s take a very simple example of dataset of three values, (1,1), (2,2),
    and (3,3), to make the calculation easy:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个包含三个值的数据集为例，(1,1)，(2,2)，和 (3,3)，以简化计算：
- en: '![Understanding cost function](img/3056_07_09.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![理解损失函数](img/3056_07_09.jpg)'
- en: 'Let''s assume θ[1] is 0, that is, the best-fit line parallel to the *x* axis.
    In the first case, assume that the best-fit line is the *x* axis, that is, *y=0*.
    The following will be the value of the cost function:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 θ[1] 为 0，即最佳拟合线平行于 *x* 轴。在第一种情况下，假设最佳拟合线是 *x* 轴，即 *y=0*。以下将是损失函数的值：
- en: '![Understanding cost function](img/3056_07_10.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![理解损失函数](img/3056_07_10.jpg)'
- en: 'Now, let''s move this line slightly up to *y=1*. The following will be the
    value of the cost function:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这条线稍微向上移动到 *y=1*。以下将是损失函数的值：
- en: '![Understanding cost function](img/3056_07_11.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![理解损失函数](img/3056_07_11.jpg)'
- en: 'Now, let''s move this line further up to *y=2*. Then, the following will be
    the value of the cost function:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这条线进一步向上移动到 *y=2*。然后，以下将是损失函数的值：
- en: '![Understanding cost function](img/3056_07_12.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![理解损失函数](img/3056_07_12.jpg)'
- en: 'Now, when we move this line further up to *y=3*, the following will be the
    value of the cost function:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们将这条线进一步向上移动到 *y=3* 时，以下将是损失函数的值：
- en: '![Understanding cost function](img/3056_07_13.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![理解损失函数](img/3056_07_13.jpg)'
- en: 'Now, let''s move this line further up to *y=4*. The following will be the value
    of the cost function:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这条线进一步向上移动到 *y=4*。以下将是损失函数的值：
- en: '![Understanding cost function](img/3056_07_14.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![理解损失函数](img/3056_07_14.jpg)'
- en: 'So, you saw that the cost function value first decreased, and then increased
    again like this:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您可以看到损失函数的值首先下降，然后再次像这样上升：
- en: '![Understanding cost function](img/3056_07_15.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![理解损失函数](img/3056_07_15.jpg)'
- en: Now, let's repeat the exercise by making θ[0] 0 and using different values of
    θ[1].
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过将 θ[0] 设置为 0 并使用不同的 θ[1] 值来重复这个练习：
- en: 'In the first case, assume the best-fit line is the *x* axis, that is, *y=0*.
    The following will be the value of the cost function:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，假设最佳拟合线是 *x* 轴，即 *y=0*。以下将是损失函数的值：
- en: '![Understanding cost function](img/3056_07_16.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![理解损失函数](img/3056_07_16.jpg)'
- en: 'Now, let''s use a slope of 0.5\. The following will be the value of the cost
    function:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用斜率为 0.5。以下将是损失函数的值：
- en: '![Understanding cost function](img/3056_07_17.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![理解损失函数](img/3056_07_17.jpg)'
- en: 'Now, let''s use a slope of 1\. The following will be the value of the cost
    function:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用斜率为 1。以下将是损失函数的值：
- en: '![Understanding cost function](img/3056_07_18.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![理解损失函数](img/3056_07_18.jpg)'
- en: 'Now, when we use a slope of 1.5, the following will be the value of the cost
    function:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们使用斜率为 1.5 时，以下将是损失函数的值：
- en: '![Understanding cost function](img/3056_07_19.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![理解损失函数](img/3056_07_19.jpg)'
- en: 'Now, let''s use a slope of 2.0\. The following will be the value of the cost
    function:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用斜率为 2.0。以下将是损失函数的值：
- en: '![Understanding cost function](img/3056_07_20.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![理解损失函数](img/3056_07_20.jpg)'
- en: As you can see in both the graphs, the minimum value of *J* is when slope or
    gradient of curve is 0.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如图中所示，*J* 的最小值出现在曲线的斜率或梯度为 0 时。
- en: '![Understanding cost function](img/3056_07_21.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![理解损失函数](img/3056_07_21.jpg)'
- en: When both θ[0] and θ[1] are mapped to a 3D space, it becomes like the shape
    of a bowl with the minimum value of the cost function being at the bottom of it.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当 θ[0] 和 θ[1] 都映射到三维空间时，它变成了一个碗的形状，损失函数的最小值位于其底部。
- en: This approach to arrive at this minimum is called **gradient descent**. In Spark,
    the implementation is stochastic gradient descent.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这种达到最小值的方法称为 **梯度下降**。在 Spark 中，其实现是随机梯度下降。
- en: Doing linear regression with lasso
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Lasso 进行线性回归
- en: The lasso is a shrinkage and selection method for linear regression. It minimizes
    the usual sum of squared errors, with a bound on the sum of the absolute values
    of the coefficients. It is based on the original lasso paper found at [http://statweb.stanford.edu/~tibs/lasso/lasso.pdf](http://statweb.stanford.edu/~tibs/lasso/lasso.pdf).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso 是线性回归的一种收缩和选择方法。它最小化通常的平方误差和，同时限制系数绝对值之和。它基于可在 [http://statweb.stanford.edu/~tibs/lasso/lasso.pdf](http://statweb.stanford.edu/~tibs/lasso/lasso.pdf)
    找到的原始 Lasso 论文。
- en: 'The least square method we used in the last recipe is also called **ordinary
    least squares** (**OLS**). OLS has two challenges:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个菜谱中我们使用的最小二乘法也称为 **普通最小二乘法**（**OLS**）。OLS 有两个挑战：
- en: '**Prediction accuracy**: Predictions made using OLS usually have low forecast
    bias and high variance. Prediction accuracy can be improved by shrinking some
    coefficients (or even making them zero). There will be some increase in bias,
    but overall prediction accuracy will improve.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测精度**：使用OLS进行的预测通常具有低预测偏差和高方差。可以通过缩小一些系数（甚至将它们设置为0）来提高预测精度。虽然会有一些偏差增加，但总体预测精度将提高。'
- en: '**Interpretation**: With a large number of predictors, it is desirable to find
    a subset of them that exhibits the strongest effect (correlation).'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解释**：在具有大量预测变量的情况下，找到其中表现出最强效应（相关性）的子集是可取的。'
- en: Note
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Bias versus variance
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差与方差
- en: 'There are two primary reasons behind prediction error: bias and variance. The
    best way to understand bias and variance is to look at a case where we are doing
    predictions on the same dataset multiple times.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 预测误差背后的两个主要原因是偏差和方差。理解偏差和方差的最佳方式是查看我们在同一数据集上进行多次预测的情况。
- en: Bias is an estimate of how far the predicted results are from the actual values,
    and variance is an estimate of the difference in predicted values among different
    predictions.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差是预测结果与实际值之间距离的估计，方差是不同预测中预测值差异的估计。
- en: Generally, adding more features helps to reduce bias, as can easily be understood.
    If, in building a prediction model, we have left out some features with significant
    correlation, it would lead to significant error.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，添加更多特征有助于减少偏差，这很容易理解。如果在构建预测模型时遗漏了一些具有显著相关性的特征，会导致显著的误差。
- en: If your model has high variance, you can remove features to reduce it. A bigger
    dataset also helps to reduce variance.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的模型具有高方差，你可以移除特征来减少它。更大的数据集也有助于减少方差。
- en: Here, we are going to use a simple dataset, which is ill-posed. An ill-posed
    dataset is a dataset where the sample data size is smaller than the number of
    predictors.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们将使用一个简单的数据集，它是不良设定的。不良设定的数据集是指样本数据量小于预测变量数量的数据集。
- en: '| y | x0 | x1 | x2 | x3 | x4 | x5 | x6 | x7 | x8 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| y | x0 | x1 | x2 | x3 | x4 | x5 | x6 | x7 | x8 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 5 | 3 | 1 | 2 | 1 | 3 | 2 | 2 | 1 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 5 | 3 | 1 | 2 | 1 | 3 | 2 | 2 | 1 |'
- en: '| 2 | 9 | 8 | 8 | 9 | 7 | 9 | 8 | 7 | 9 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 9 | 8 | 8 | 9 | 7 | 9 | 8 | 7 | 9 |'
- en: You can easily guess that, here, out of nine predictors, only two have a strong
    correlation with *y*, that is, *x0* and *x1*. We will use this dataset with the
    lasso algorithm to see its validity.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以很容易地猜出，在这里，九个预测变量中只有两个与*y*有强相关性，即*x0*和*x1*。我们将使用这个数据集和lasso算法来验证其有效性。
- en: How to do it…
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Start the Spark shell:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell：
- en: '[PRE6]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Import the statistics and related classes:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入统计和相关类：
- en: '[PRE7]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Create the `LabeledPoint` array with the house price as the label:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用房价作为标签创建`LabeledPoint`数组：
- en: '[PRE8]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Create an RDD of the preceding data:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建前述数据的RDD：
- en: '[PRE9]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Train a model using this data using 100 iterations. Here, the step size and
    regularization parameter have been set by hand:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用此数据通过100次迭代训练模型。在这里，步长和正则化参数已经手动设置：
- en: '[PRE10]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Check how many predictors have their coefficients set to zero:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查有多少预测变量的系数被设置为0：
- en: '[PRE11]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As you can see, six out of nine predictors have got their coefficients set
    to zero. This is the primary feature of lasso: any predictor it thinks is not
    useful, it literally moves them out of equation by setting their coefficients
    to zero.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，九个预测变量中有六个将其系数设置为0。这是lasso的主要特征：任何它认为无用的预测变量，它实际上通过将其系数设置为0将其从方程中移除。
- en: Doing ridge regression
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行岭回归
- en: An alternate way to lasso to improve prediction quality is ridge regression.
    While in lasso, a lot of features get their coefficients set to zero and, therefore,
    eliminated from an equation, in ridge, predictors or features are penalized, but
    are never set to zero.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高预测质量，lasso的另一种方法是岭回归。在lasso中，许多特征将其系数设置为0并被从方程中消除，而在岭回归中，预测变量或特征会受到惩罚，但永远不会被设置为0。
- en: How to do it…
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Start the Spark shell:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell：
- en: '[PRE12]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Import the statistics and related classes:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入统计和相关类：
- en: '[PRE13]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Create the `LabeledPoint` array with the house price as the label:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用房价作为标签创建`LabeledPoint`数组：
- en: '[PRE14]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Create an RDD of the preceding data:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建前述数据的RDD：
- en: '[PRE15]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Train a model using this data using 100 iterations. Here, the step size and
    regularization parameter have been set by hand :'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用此数据通过100次迭代训练模型。在这里，步长和正则化参数已经手动设置：
- en: '[PRE16]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Check how many predictors have their coefficients set to zero:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查有多少预测变量的系数被设置为零：
- en: '[PRE17]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you can see, unlike lasso, ridge regression does not assign any predictor
    coefficients zero, but it does make some very close to zero.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，与lasso回归不同，岭回归不会将任何预测变量的系数设置为零，但它确实会让一些系数非常接近零。
