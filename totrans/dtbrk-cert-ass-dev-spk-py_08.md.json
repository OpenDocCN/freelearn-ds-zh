["```py\nhousing_data = spark.read.csv(\"HousePricePrediction.csv\")\n# Printing first 5 records of the dataset\nhousing_data.show(5)\n```", "```py\nhousing_data.printSchema\n```", "```py\n<bound method DataFrame.printSchema of DataFrame[Id: bigint, MSSubClass: bigint, MSZoning: string, LotArea: bigint, LotConfig: string, BldgType: string, OverallCond: bigint, YearBuilt: bigint, YearRemodAdd: bigint, Exterior1st: string, BsmtFinSF2: bigint, TotalBsmtSF: bigint, SalePrice: bigint]>\n```", "```py\nhousing_data.count()\n```", "```py\n2919\n```", "```py\n# Remove rows with missing values\ncleaned_data = housing_data.dropna()\ncleaned_data.count()\n```", "```py\n1460\n```", "```py\n#import required libraries\nfrom pyspark.ml.feature import StringIndexer\nmszoning_indexer = StringIndexer(inputCol=\"MSZoning\", outputCol=\"MSZoningIndex\")\n#Fits a model to the input dataset with optional parameters.\ndf_mszoning = mszoning_indexer.fit(cleaned_data).transform(cleaned_data)\ndf_mszoning.show()\n```", "```py\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml import Pipeline\nmszoning_indexer = StringIndexer(inputCol=\"MSZoning\", outputCol=\"MSZoningIndex\")\nlotconfig_indexer = StringIndexer(inputCol=\"LotConfig\", outputCol=\"LotConfigIndex\")\nbldgtype_indexer = StringIndexer(inputCol=\"BldgType\", outputCol=\"BldgTypeIndex\")\nexterior1st_indexer = StringIndexer(inputCol=\"Exterior1st\", outputCol=\"Exterior1stIndex\")\nonehotencoder_mszoning_vector = OneHotEncoder(inputCol=\"MSZoningIndex\", outputCol=\"MSZoningVector\")\nonehotencoder_lotconfig_vector = OneHotEncoder(inputCol=\"LotConfigIndex\", outputCol=\"LotConfigVector\")\nonehotencoder_bldgtype_vector = OneHotEncoder(inputCol=\"BldgTypeIndex\", outputCol=\"BldgTypeVector\")\nonehotencoder_exterior1st_vector = OneHotEncoder(inputCol=\"Exterior1stIndex\", outputCol=\"Exterior1stVector\")\n#Create pipeline and pass all stages\npipeline = Pipeline(stages=[mszoning_indexer,\n                            lotconfig_indexer,\n                            bldgtype_indexer,\n                            exterior1st_indexer,\n                            onehotencoder_mszoning_vector,\n                            onehotencoder_lotconfig_vector,\n                            onehotencoder_bldgtype_vector,\n                            onehotencoder_exterior1st_vector])\n```", "```py\ndf_transformed = pipeline.fit(cleaned_data).transform(cleaned_data)\ndf_transformed.show(5)\n```", "```py\ndrop_column_list = [\"Id\", \"MSZoning\",\"LotConfig\",\"BldgType\", \"Exterior1st\"]\ndf_dropped_cols = df_transformed.select([column for column in df_transformed.columns if column not in drop_column_list])\ndf_dropped_cols.columns\n```", "```py\n['MSSubClass',\n 'LotArea',\n 'OverallCond',\n 'YearBuilt',\n 'YearRemodAdd',\n 'BsmtFinSF2',\n 'TotalBsmtSF',\n 'SalePrice',\n 'MSZoningIndex',\n 'LotConfigIndex',\n 'BldgTypeIndex',\n 'Exterior1stIndex',\n 'MSZoningVector',\n 'LotConfigVector',\n 'BldgTypeVector',\n 'Exterior1stVector']\n```", "```py\nfrom pyspark.ml.feature import VectorAssembler\n#Assembling features\nfeature_assembly = VectorAssembler(inputCols = ['MSSubClass',\n 'LotArea',\n 'OverallCond',\n 'YearBuilt',\n 'YearRemodAdd',\n 'BsmtFinSF2',\n 'TotalBsmtSF',\n 'MSZoningIndex',\n 'LotConfigIndex',\n 'BldgTypeIndex',\n 'Exterior1stIndex',\n 'MSZoningVector',\n 'LotConfigVector',\n 'BldgTypeVector',\n 'Exterior1stVector'], outputCol = 'features')\noutput = feature_assembly.transform(df_dropped_cols)\noutput.show(3)\n```", "```py\n#Normalizing the features\nfrom pyspark.ml.feature import StandardScaler\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",withStd=True, withMean=False)\n# Compute summary statistics by fitting the StandardScaler\nscalerModel = scaler.fit(output)\n# Normalize each feature to have unit standard deviation.\nscaledOutput = scalerModel.transform(output)\nscaledOutput.show(3)\n```", "```py\n#Selecting input and output column from output\ndf_model_final = scaledOutput.select(['SalePrice', 'scaledFeatures'])\ndf_model_final.show(3)\n```", "```py\n+---------+--------------------+\n|SalePrice|   scaledFeatures   |\n+---------+--------------------+\n|   208500|(37,[0,1,2,3,4,6,...|\n|   181500|(37,[0,1,2,3,4,6,...|\n|   223500|(37,[0,1,2,3,4,6,...|\n+---------+--------------------+\n```", "```py\n#test train split\ndf_train, df_test = df_model_final.randomSplit([0.75, 0.25])\n```", "```py\nfrom pyspark.ml.regression import LinearRegression\n# Instantiate the linear regression model\nregressor = LinearRegression(featuresCol = 'scaledFeatures', labelCol = 'SalePrice')\n# Fit the model on the training data\nregressor = regressor.fit(df_train)\n```", "```py\n#MSE for the train data\npred_results = regressor.evaluate(df_train)\nprint(\"The train MSE for the model is: %2f\"% pred_results.meanAbsoluteError)\nprint(\"The train r2 for the model is: %2f\"% pred_results.r2)\n```", "```py\nThe MSE for the model is: 32287.153682\nThe r2 for the model is: 0.614926\n```", "```py\n#Checking test performance\npred_results = regressor.evaluate(df_test)\nprint(\"The test MSE for the model is: %2f\"% pred_results.meanAbsoluteError)\nprint(\"The test r2 for the model is: %2f\"% pred_results.r2)\n```", "```py\nThe MSE for the model is: 31668.331218\nThe r2 for the model is: 0.613300\n```"]