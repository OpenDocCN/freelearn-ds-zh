- en: 'Chapter 6: Putting Everything Together: Semantic Parsing with spaCy'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a purely hands-on section. In this chapter, we will apply what we have
    learned hitherto to **Airline Travel Information System** (**ATIS**), a well-known
    airplane ticket reservation system dataset. First of all, we will get to know
    our dataset and make the basic statistics. As the first **natural language understanding**
    (**NLU**) task, we will extract the named entities with two different methods,
    with spaCy Matcher, and by walking on the dependency tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next task is to determine the intent of the user utterance. We will explore
    intent recognition in different ways, too: by extracting the verbs and their direct
    objects, by using wordlists, and by walking on the dependency tree to recognize
    multiple intents. Then you will match your keywords to synonyms from a synonyms
    list to detect semantic similarity.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, you'll do keyword matching with word vector-based semantic similarity
    methods. Finally, we will combine all this information to generate a semantic
    representation for the dataset utterances.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you'll learn how to semantically process a real-world
    dataset completely. You'll learn how to extract entities, recognize intents, and
    perform semantic similarity calculations. The tools of this chapter are really
    what you'll build for a real-world **natural language processing** (**NLP**) pipeline,
    including an NLU chatbot and an NLU customer support application.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting named entities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using dependency relations for intent recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic similarity methods for semantic parsing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll process a dataset. The dataset and the chapter code can
    be found at [https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter06](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter06).
  prefs: []
  type: TYPE_NORMAL
- en: We used the pandas library of Python to manipulate our dataset, besides using
    spaCy. We also used the awk command-line tool. pandas can be installed via pip
    and awk is preinstalled in many Linux distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting named entities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many NLP applications, including semantic parsing, we start looking for meaning
    in a text by examining the entity types and placing an entity extraction component
    into our NLP pipelines. **Named entities** play a key role in understanding the
    meaning of user text.
  prefs: []
  type: TYPE_NORMAL
- en: We'll also start a semantic parsing pipeline by extracting the named entities
    from our corpus. To understand what sort of entities we want to extract, first,
    we'll get to know the ATIS dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting to know the ATIS dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout this chapter, we'll work with the ATIS corpus. ATIS is a well-known
    dataset; it's one of the standard benchmark datasets for intent classification.
    The dataset consists of customer utterances who want to book a flight, get information
    about the flights, including flight costs, flight destinations, and timetables.
  prefs: []
  type: TYPE_NORMAL
- en: 'No matter what the NLP task is, you should always go over your corpus with
    a naked eye. We want to get to know our corpus so that we integrate our observations
    of corpus into our code. While viewing our text data, we usually keep an eye on
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What kind of utterances are there? Is it a short text corpus or does the corpus
    consist of long documents or medium-length paragraphs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What sort of entities does the corpus include? People's names, city names, country
    names, organization names, and so on. Which ones do we want to extract?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How is punctuation used? Is the text correctly punctuated, or is no punctuation
    used at all?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How are the grammatical rules followed? Is the capitalization correct? Did users
    follow the grammatical rules? Are there misspelled words?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before starting any processing, we''ll examine our corpus. Let''s go ahead
    and download the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset is a two-column CSV file. First, we''ll get some insights into
    the dataset statistics with **pandas**. pandas is a popular data manipulation
    library that is frequently used by data scientists. You can read more at [https://pandas.pydata.org/pandas-docs/version/0.15/tutorials.html](https://pandas.pydata.org/pandas-docs/version/0.15/tutorials.html):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by reading the CSV file into Python. We''ll use the `read_csv`
    method of pandas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The dataset variable holds the CSV as an object for us.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we''ll call `head()` on the dataset object. `head()` outputs the first
    10 columns of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result looks as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Overview of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16570_6_1.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.1 – Overview of the dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you see, the dataset object contains rows and columns. It is indeed a CSV
    object. The first column contains the intent, and the second column contains the
    user utterance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now we can print some example utterances:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we can see, the first user wants to book a flight; they included the destination,
    the source cities, and the flight time. The third user is asking about the arrival
    time of a specific flight and the fifth user made a query with a price limit.
    The utterances are not capitalized or punctuated. This is because these utterances
    are an output of a speech-to-text engine.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Last, we can see the distribution of the number of utterances by intent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can find the dataset exploration code at the book's GitHub repository under
    [https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter06/ATIS_dataset_exploration.ipynb](https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter06/ATIS_dataset_exploration.ipynb).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After this point, we''ll process just the utterance text. Hence, we can drop
    the first column. To do so, we''ll play a small trick with the Unix tool awk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we printed the second column of the input CSV file (where the filed separator
    is a `,`) and directed the output into a text file called `atis_utterances.txt`.
    Now that our utterances are ready to process, we can go ahead and extract the
    entities.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting named entities with Matcher
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have already seen, this is a flights dataset. Hence, we expect to see
    city/country names, airport names, and airline names:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Also, the `atis_abbreviation` intent contains utterances that are inquiries
    about some abbreviations. Flight abbreviations can be fare codes (for example,
    M = Economy), airline name codes (for example, United Airlines = UA), and airport
    codes (for example, Berlin Airport = BER), and so on. Examples include the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let's visualize some utterances from the dataset. The following screenshot shows
    the entities with their types:![Figure 6.2 – Example corpus sentences with entities
    and entity types highlighted; generated by the displaCy online demo](img/B16570_6_2.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 6.2 – Example corpus sentences with entities and entity types highlighted;
    generated by the displaCy online demo
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can see all the entity types and their frequencies more systematically.
    The following code segment makes the following actions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) It reads the text file of utterances that we created in the preceding dataset
    exploration subsection.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) It iterates over each utterance and creates a Doc object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) It extracts entities from the current doc object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) It updates the global entity label list with the labels of the entities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: e) It finally calculates the frequency of each label with a counter object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here is the code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll first extract the location entities by spaCy Matcher by searching for
    a pattern of the `preposition location_name` form. The following code extracts
    location entities preceded with a preposition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We already saw how to initialize a Matcher object and add patterns to it. Still,
    we''ll recall how to use a Matcher object to extract the matches now. Here''s
    what we did in this code segment:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) We started by importing `spacy` and the `spacy.matcher` class in *lines 1-2*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) We created a language pipeline object, `nlp`, at *line 3*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) In *line 4*, we initialized the Matcher object with the language vocabulary.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) In line 5, we created a pattern matching two tokens, a preposition (`POS`
    tag `ADP` means an *adposition = preposition + postposition*) and a location entity
    (label `GPE` means a **location entity**).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: e) We added this pattern to the Matcher object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: f) Finally, we asked for the matches in an example corpus sentence and printed
    the matches.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Although the `from` and `to` prepositions dominate in this dataset, verbs about
    leaving and arriving can be used with a variety of prepositions. Here are some
    more example sentences from the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The second example sentence is a question sentence:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Another example sentence from the dataset contains an abbreviation in a destination
    entity:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our last example sentence is again a question sentence:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we see some phrasal verbs such as `arrive in`, as well as preposition
    and verb combinations such as `stop in` and `fly out of`. `By the way of Dallas`
    does not include a verb at all. The user indicated that they want to make a stop
    at Dallas. `to`, `from`, `in`, `out`, and `of` are common prepositions that are
    used in a traveling context.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After extracting the locations, we can now extract the airline information.
    The `ORG` entity label means an organization and it corresponds to airline company
    names in our dataset. The following code segment extracts the organization names,
    possibly multi-worded names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extracting dates and times are not very different; you can replicate the preceding
    with code with `ENT_TYPE: DATE` and `ENT_TYPE: TIME`. We encourage you to try
    it yourself. The following screenshot exhibits how the date and time entities
    look in detail:![Figure 6.3 – Example dataset sentences with date and time entities
    highlighted. The image is generated by the displaCy online demo](img/B16570_6_3.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 6.3 – Example dataset sentences with date and time entities highlighted.
    The image is generated by the displaCy online demo
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we''ll extract abbreviation type entities. Extracting the abbreviation
    entities is a bit trickier. First, we will have a look at how the abbreviations
    appear:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Only one of these sentences includes an entity. The first example sentence
    includes an `AMOUNT` entity, which is `57`. Other than that, abbreviations are
    not marked with any entity type at all. In this case, we have to provide some
    custom rules to the Matcher. Let''s make some observations first, and then form
    a Matcher pattern:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) An abbreviation can be broken into two parts – letters, and digits.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The letter part can be 1-2 characters long.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) The digit part is also 1-2 characters long.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) The presence of digits indicates an abbreviation entity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'e) The presence of the following words indicates an abbreviation entity: class,
    code, abbreviation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: f) The `POS` tag of an abbreviation is a noun. If the candidate word is a 1-letter
    or 2-letter word, then we can look at the `POS` tag and see whether it's a noun.
    This approach eliminates the false positives, such as *us* (pronoun), *me* (pronoun),
    *a* (determiner), and *an* (determiner).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s now put these observations into Matcher patterns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we create a Matcher object with the patterns we defined:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''re now ready to feed our sentences into the matcher:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the preceding code, we defined four patterns:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) The first pattern matches to a single token, which consists of 1-2 letters
    and 1-2 digits. For example, `d1`, `d10`, `ad1`, and `ad21` will match this pattern.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The second pattern matches to 2-token abbreviations where the first token
    is 1-2 letters and the second token 1-2 digits. The abbreviations `ap 5`, `ap
    57`, `a 5`, and `a 57` will match this pattern.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) The third pattern matches to two tokens too. The first token is a context
    clue word, such as `class` or `code`, and the second token should be a 1-2 letter
    token. Some example matches are `code f`, `code y`, and `class c`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) The fourth pattern extracts 1-2 letter short words whose `POS` tag is `NOUN`.
    Some example matches from the preceding sentences are `c` and `co`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: spaCy Matcher makes life easy for us by allowing us to make use of token shape,
    context clues, and a token `POS` tag. We made a very successful entity extraction
    in this subsection by extracting locations, airline names, dates, times, and abbreviations.
    In the next subsection, we'll go deeper into the sentence syntax and extract entities
    from the sentences where context does not offer many clues.
  prefs: []
  type: TYPE_NORMAL
- en: Using dependency trees for extracting entities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous subsection, we extracted entities where the context provides
    obvious clues. Extracting the destination city from the following sentence is
    easy. We can look for the `to + GPE` pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'But suppose the user provides one of the following sentences instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the preposition `to` refers to `conference`, not `Munich`, in the first
    sentence. In this sentence, we need a pattern such as `to + .... + GPE`. Then,
    we have to be careful what words can come in between "to" and the city name, as
    well as what words should not come. For instance, this sentence carries a completely
    different meaning and shouldn''t match:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In the second sentence, there's no `to` at all. Here, as we see from these examples,
    we need to examine the syntactic relations between words. In [*Chapter 3*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)*,
    Linguistic Features*, we already saw how to interpret dependency trees to understand
    the relations between words. In this subsection, we'll walk the dependency trees.
  prefs: []
  type: TYPE_NORMAL
- en: Walking a dependency tree means visiting the tokens in a custom order, not necessarily
    from left to right. Usually, we stop iterating over the dependency tree once we
    find what we're looking for. Again, a dependency tree shows the syntactic relations
    between its words. Recall from [*Chapter 3*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)*,
    Linguistic Features*, that the relations are represented with directed arrows,
    connecting the head and the child of a relation. Every word in a sentence has
    to involve at least one relation. This fact guarantees that we'll visit each word
    while walking through the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs: []
  type: TYPE_NORMAL
- en: Before proceeding to code, first, let's remember some concepts about dependency
    trees. ROOT is a special dependency label and is always assigned to the main verb
    of the sentence. spaCy shows syntactic relations with arcs. One of the tokens
    is the syntactic parent (called the HEAD) and the other is dependent (called the
    CHILD). By way of an example, in *Figure 6.3*, **going** has 3 syntactic children
    – **I**, **m**, and **to**. Equivalently, the syntactic head of **to** is **going**
    (the same applies to **I** and **m**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Coming back to our examples, we''ll iterate the utterance dependency trees
    to find out whether the preposition **to** is syntactically related to the location
    entity, **Munich**. First of all, let''s see the dependency parse of our example
    sentence *I''m going to a conference in Munich* and also remember what a dependency
    tree looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Dependency parse of the example sentence'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_6_4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 – Dependency parse of the example sentence
  prefs: []
  type: TYPE_NORMAL
- en: There are no incoming arcs into the verb **going**, so **going** is the root
    of the dependency tree (when we examine the code, we'll see that the dependency
    label is **ROOT**). This is supposed to happen because **going** is the main verb
    of the sentence. If we follow the arc to its immediate right, we encounter **to**;
    jumping over the arcs to the right we reach **Munich**. This shows that there's
    a syntactic relation between **to** and **Munich**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now iterate over the dependency tree with code. There are two possible
    ways to connect **to** and **Munich**:'
  prefs: []
  type: TYPE_NORMAL
- en: Left to right. We start from **to** and try to reach **Munich** by visiting
    "to"'s syntactic children. This approach may not be a very good idea, because
    if "to" has more than one child, then we need to check each child and keep track
    of all the possible paths.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right to left. We start from **Munich**, jump onto its head, and follow the
    head's head, and so on. Since each word has exactly one head, it's guaranteed
    that there will be only one path. Then we determine whether **to** is on this
    path or not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code segments implement the second approach, start the dependency
    tree walk from `Munich`, and look for `to`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `reach_parent` function, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: We start from a source token and try to reach the destination token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the `while` loop, we iterate over the head of each token, starting from the
    source token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loop stops when we reach either the source token or the root of the sentence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We test for reaching the root via the `source_token == source_token.head` line.
    Because the root token always refers to itself, its head is itself (remember that
    in the dependency tree, there are no incoming arcs into the root).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we tested our function on two different test cases. In the first one,
    the source and destination are related, whereas in the second test, there's no
    relation, hence the function returns `None`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach is very different from the previous subsection's rather straightforward
    approach. Natural language is complicated and challenging to process, and it's
    important to know the approaches available and have the necessary tools in your
    toolbox when you need to use them.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll dive into a popular subject – intent recognition with a syntactic
    approach. Let's see some key points of designing a good intent recognition component,
    including recognizing multiple intents as well.
  prefs: []
  type: TYPE_NORMAL
- en: Using dependency relations for intent recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After extracting the entities, we want to find out what sort of intent the user
    carries – to book a flight, to purchase a meal on their already booked flight,
    cancel their flight, and so on. If you look at the intents list again, you will
    see that every intent includes a verb (to book) and an object that the verb acts
    on (flight, hotel, meal).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll extract transitive verbs and their direct objects from
    utterances. We'll begin our intent recognition section by extracting the transitive
    verb and the direct object of the verb. Then, we'll explore how to understand
    a user's intent by recognizing synonyms of verbs and nouns. Finally, we'll see
    how to determine a user's intent with semantic similarity methods. Before we move
    on to extracting transitive verbs and their direct objects, let's first quickly
    go over the concepts of transitive verbs and direct/indirect objects.
  prefs: []
  type: TYPE_NORMAL
- en: Linguistic primer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we''ll explore some linguistic concepts related to sentence
    structure, including verbs and verb-object relations. A verb is a very important
    component of the sentence as it indicates the action in the sentence. The object
    of the sentence is the thing/person that is affected by the action of the verb.
    Hence, there''s a natural connection between the sentence verb and objects. The
    concept of transitivity captures verb-object relations. A transitive verb is a
    verb that needs an object to act upon. Let''s see some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In these example sentences, `bought`, `loved`, and `borrowed` are transitive
    verbs. In the first sentence, `bought` is the transitive verb and `flowers` is
    its object, the thing that has been bought by the sentence subject, `I`. `Loved`
    – `his cat` and `borrowed` – `my book` are transitive verb-object examples. We'll
    focus on the first sentence again - what happens if we erase the `flowers` object?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Bought **what**? Without an object, this sentence doesn't carry any meaning
    at all. In the preceding sentences, each of the objects completes the meaning
    of the verb. This is a way of understanding whether a verb is transitive or not
    – erase the object and check whether the sentence remains semantically intact.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some verbs are transitive and some verbs are intransitive. An **intransitive
    verb** is the opposite of a transitive verb; it doesn''t need an object to act
    upon. Let''s see some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In all the preceding sentences, the verbs make sense without an object. If
    we erase all the words other than the subject and object, these sentences are
    still meaningful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Pairing an intransitive verb with an object doesn't make sense. You can't run
    someone/something, you can't shine something/someone, and you certainly cannot
    die something/someone.
  prefs: []
  type: TYPE_NORMAL
- en: Sentence object
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we remarked before, the object is the thing/person that is affected by the
    verb's action. The action stated by the verb is committed by the sentence subject
    and the sentence object gets affected.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sentence can be direct or indirect. A **direct object** answers the questions
    **whom?** / **what?** You can find the direct object by asking **The subject {verb}
    what/who?**. Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'An **indirect object** answers the questions **for what?**/**for whom?**/**to
    whom?**. Let''s see some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Indirect objects are often preceded by the prepositions to, for, from, and so
    on. As you can see from these examples, an indirect object is also an object and
    is affected by the verb's action, but its role in the sentence is a bit different.
    An indirect object is sometimes viewed as the recipient of the direct object.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is all you need to know about transitive/intransitive verbs and direct/indirect
    objects to digest this chapter''s material. If you want to learn more about sentence
    syntax, you can read the great book *Linguistic Fundamentals for Natural Language
    Processing* by **Emily Bender**: ([https://dl.acm.org/doi/book/10.5555/2534456](https://dl.acm.org/doi/book/10.5555/2534456)).
    We have covered the basics of sentence syntax, but this is still a great resource
    to learn about syntax in depth.'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting transitive verbs and their direct objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While recognizing the intent, usually we apply these steps to a user utterance:'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the sentence into tokens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dependency parsing is performed by spaCy. We walk the dependency tree to extract
    the tokens and relations that we''re interested in, which are the verb and the
    direct object, as shown in the following diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Dependency parse of an example sentence from the corpus'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_6_5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.5 – Dependency parse of an example sentence from the corpus
  prefs: []
  type: TYPE_NORMAL
- en: In this example sentence, the transitive verb is **find** and the direct object
    is **a flight**. The relation **dobj** connects a transitive verb to its direct
    object. If we follow the arc, semantically, we see that the user wants to commit
    the action of finding and the object they want to find is a flight. We can merge
    **find** and **a flight** into a single word, **findAflight** or **findFlight**,
    which can be this intent's name. Other intents can be **bookFlight**, **cancelFlight**,
    **bookMeal**, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s extract the verb and the direct object in a more systematic way. We''ll
    first spot the direct object by looking for the `dobj` label in the sentence.
    To locate the transitive verb, we look at the direct object''s syntactic head.
    A sentence can include more than one verb, hence we''re careful while processing
    the verbs. Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code segment, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: We applied the pipeline to our sample sentence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we spotted the direct object by looking for a token whose dependency label
    is `dobj`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we located a direct object, we spotted the corresponding transitive verb
    by obtaining the direct object's syntactic head.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we printed the verb and the object to generate this intent's name.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Great! Intent recognition was successful! Here, we recognized a single intent.
    Some utterances may carry multiple intents. In the next section, we'll learn how
    to recognize multiple intents based on the techniques of this section.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting multiple intents with conjunction relation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some utterances carry multiple intents. For example, consider the following
    utterance from the corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the user wants to list all the flights and, at the same time, wants to
    see the fare info. One way of processing is considering these intents as a single
    and complex intent. In that case, we can express this complex intent as `action:
    show, objects: flights, fares`. Another and more common way of processing this
    sort of utterance is to label the utterance with multiple intents. In the dataset,
    this example utterance is marked with two intents as `atis_flight#atis_airfare`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Dependency tree of the example dataset utterance. The conj relation
    is between "flight" and "fares" ](img/B16570_6_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Dependency tree of the example dataset utterance. The conj relation
    is between "flight" and "fares"
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, we see that the **dobj** arc connects **show** and
    **flights**. The **conj** arc connects **flights** and **fares** to indicate the
    conjunction relation between them. The conjunction relation is built by a conjunction
    such as **and** or **or** and indicates that a noun is joined to another noun
    by this conjunction. In this situation, we extract the direct object and its conjuncts.
    Let''s now see how we can turn this process into code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Here, we looped over all tokens to locate the direct object of the sentence.
    When we found the direct object, we obtained its conjuncts. After that, finding
    the transitive verb is the same as we did in the previous code segment, we extracted
    the direct object's head. After extracting the verb and two objects, if we want,
    we can combine the two to create two intent names – `showFlights` and `showFares`.
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing the intent using wordlists
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some cases, tokens other than the transitive verb and the direct object contain
    the semantics of the user intent. In that case, you need to go further down in
    the syntactic relations and explore the sentence structure deeper.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, consider the following utterance from our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In this sentence, the **verb-object** pair that best describes the user intent
    is **want-flight**. However, if we look at the parse tree in *Figure 6.7*, we
    see that **want** and **flight** are not directly related in the parse tree. **want**
    is related to the transitive verb **make**, and **flight** is related to the direct
    object **reservation**, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Parse tree of the example sentence from the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_6_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.7 – Parse tree of the example sentence from the dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'What will we do then? We can play a trick and keep a list of helper verbs such
    as **would like**, **want**, **make**, and **need**. Here''s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what we did step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: We started by locating the direct object and its transitive verb.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we found them, we compared them against our predefined lists of words.
    For this example, we used two shortened lists, `verbList` contains a list of helper
    verbs, and `objList` contains a list of the possible object words we want to extract.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We checked the transitive verb. If it's not in the list of helper verbs, then
    we checked the main verb of the sentence (marked by `ROOT`), which is the head
    of the transitive verb. If the transitive verb is the main verb of the sentence,
    then the syntactic head of this verb is itself (`tVerb.head` is `tVerb)`. Hence,
    the line `if tVerb.head.dep_ == "ROOT"` evaluates to `True` and this implementation
    works.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we checked the direct object. If it's not in the list of possible objects,
    then we check its syntactic children. For each child, we check whether the child
    is a preposition of the direct object. If so, we pick up the child's child (it
    can have only one child).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we printed the string that represents the intent name, which is `wantFlight`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, take a deep breath. It takes time to digest and process information,
    especially when it's about sentence syntax. You can try different sentences from
    the corpus and see what the script does by checkpointing/putting prints into the
    code.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll explore a very handy tool, using synonym lists. Let's
    move ahead to the next section and learn how to make the best of semantic similarity.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic similarity methods for semantic parsing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Natural language allows us to express the same concept in different ways and
    with different words. Every language has synonyms and semantically related words.
  prefs: []
  type: TYPE_NORMAL
- en: As an NLP developer, while developing a semantic parser for a chatbot application,
    text classification, or any other semantic application, you should keep in my
    mind that users use a fairly wide set of phrases and expressions for each intent.
    In fact, if you're building a chatbot by using a platform such as RASA ([https://rasa.com/](https://rasa.com/))
    or on a platform such as Dialogflow ([https://dialogflow.cloud.google.com/](https://dialogflow.cloud.google.com/)),
    you're asked to provide as many utterance examples as you can provide for each
    intent. Then, these utterances are used to train the intent classifier behind
    the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: There are usually two ways to recognize semantic similarity, either with a synonyms
    dictionary or with word vector-based semantic similarity methods. In this section,
    we will discuss both approaches. Let's start with how to use a synonyms dictionary
    to detect semantic similarity.
  prefs: []
  type: TYPE_NORMAL
- en: Using synonyms lists for semantic similarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We already went through our dataset and saw that different verbs are used to
    express the same actions. For instance, **landing**, **arriving**, and **flying
    to** verbs carry the same meaning, whereas **leaving**, **departing**, and **flying
    from** verbs form another semantic group.
  prefs: []
  type: TYPE_NORMAL
- en: We already saw that in most cases, the transitive verbs and direct objects express
    the intent. An easy way to determine whether two utterances represent the same
    intent is to check whether the verbs and the direct objects are synonyms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example and compare two example utterances from the dataset.
    First, we prepare a small synonyms dictionary. We include only the base forms
    of the verbs and nouns. While doing the comparison, we also use the base form
    of the words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Each **synonym set** (**synset**) includes the set of synonyms for our domain.
    We usually include the language-general synonyms (airplane-plane) and the domain-specific
    synonyms (book-buy).
  prefs: []
  type: TYPE_NORMAL
- en: 'The synsets are ready to use, and we''re ready to move onto the spaCy code.
    Let''s go step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we construct two doc objects corresponding to the two utterances we
    want to compare, `doc` and `doc2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we extract the transitive verb and direct object of the first utterance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we do the same for the second utterance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We obtained a synset of the first verb shown. Then, we checked whether the
    second verb list is in this synset, which returns `True`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, we obtain the synset of the first direct object – aircraft. Then
    we check whether the second direct object meal is in this synset, which is obviously
    not true:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We deduce that the preceding two utterances do not refer to the same intent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Synonym lists are great for semantic similarity calculations, and many real-world
    NLP applications benefit from such precompiled lists. Using synonyms is not always
    applicable though. Making a dictionary look up each word in a sentence can become
    inefficient for big synsets. In the next section, we'll introduce a more efficient
    way of calculating semantic similarity with word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Using word vectors to recognize semantic similarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087), *Working with
    Word Vectors and Semantic Similarity*, we already saw that the word vector carries
    semantics, including synonymity information. Synonym lists are handy if you work
    in a very specific domain and the number of synonyms is rather low. Working with
    big synsets can become inefficient at some point because we have to make a dictionary
    look up the verbs and direct objects each time. However, word vectors offer us
    a very convenient and vector-based way to calculate semantic similarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go over the code from the previous subsection again. This time, we''ll
    calculate the semantic distance between words with spaCy word vectors. Let''s
    go step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we construct two `doc` objects that we want to compare:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we extract the verb and object of the first sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We repeat the same procedure on the second sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we calculate the semantic similarity between two direct objects using
    the word vector-based similarity method of spaCy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we calculate the similarity between the verbs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code is different from the previous code. This time, we used the
    token objects directly; no lemma extraction is required. Then we called the `token.similarity(token2)`
    method of spaCy to calculate the semantic distance between the direct objects.
    The resulting score is very low. At this point, we deduce that these two utterances
    do not represent the same intent.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is an easy and efficient way of calculating semantic similarity. We remarked
    in the very first chapter that spaCy provides easy-to-use and efficient tools
    for NLP developers, and now we can see why.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We already extracted the entities and recognized the intent in several ways.
    We're now ready to put it all together to calculate a semantic representation
    for a user utterance!
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll process the example dataset utterance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We'll hold a dictionary object to hold the result. The result will include the
    entities and the intent.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s extract the entities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With this information, we can generate the following semantic representation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll perform intent recognition to generate a complete semantic parsing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After determining the intent, our semantic parse for this utterance now looks
    like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The final result is that the complete semantic representation of this utterance,
    intent, and entities is extracted. This is a machine-readable and usable output.
    We pass this result to the system component that made the call to the NLP application
    to generate a response action.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! You have made it to the end of a very intense chapter!
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned how to generate a complete semantic parse of utterances.
    First, you made a discovery on your dataset to get insights about the dataset
    analytics. Then, you learned to extract entities with two different techniques
    – with spaCy Matcher and by walking on the dependency tree. Next, you learned
    different ways of performing intent recognition by analyzing the sentence structure.
    Finally, you put all the information together to generate a semantic parse.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapters, we will shift toward more machine learning methods. The
    next section concerns how to train spaCy NLP pipeline components on your own data.
    Let's move ahead and customize spaCy for ourselves!
  prefs: []
  type: TYPE_NORMAL
