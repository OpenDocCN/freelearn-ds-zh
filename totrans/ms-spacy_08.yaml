- en: 'Chapter 6: Putting Everything Together: Semantic Parsing with spaCy'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章：整合一切：使用 spaCy 进行语义解析
- en: This is a purely hands-on section. In this chapter, we will apply what we have
    learned hitherto to **Airline Travel Information System** (**ATIS**), a well-known
    airplane ticket reservation system dataset. First of all, we will get to know
    our dataset and make the basic statistics. As the first **natural language understanding**
    (**NLU**) task, we will extract the named entities with two different methods,
    with spaCy Matcher, and by walking on the dependency tree.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个纯实践的部分。在本章中，我们将应用我们迄今为止所学的内容，应用到**航空旅行信息系统**（**ATIS**），一个著名的机票预订系统数据集。首先，我们将了解我们的数据集并做出基本统计。作为第一个**自然语言理解**（**NLU**）任务，我们将使用两种不同的方法提取命名实体，使用
    spaCy 匹配器和通过遍历依存树。
- en: 'The next task is to determine the intent of the user utterance. We will explore
    intent recognition in different ways, too: by extracting the verbs and their direct
    objects, by using wordlists, and by walking on the dependency tree to recognize
    multiple intents. Then you will match your keywords to synonyms from a synonyms
    list to detect semantic similarity.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个任务是确定用户话语的意图。我们也将以不同的方式探索意图识别：通过提取动词及其直接宾语，使用词表，以及通过遍历依存树来识别多个意图。然后你将匹配你的关键词到同义词列表中，以检测语义相似性。
- en: Also, you'll do keyword matching with word vector-based semantic similarity
    methods. Finally, we will combine all this information to generate a semantic
    representation for the dataset utterances.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还将使用基于词向量语义相似性的方法进行关键词匹配。最后，我们将结合所有这些信息为数据集话语生成语义表示。
- en: By the end of this chapter, you'll learn how to semantically process a real-world
    dataset completely. You'll learn how to extract entities, recognize intents, and
    perform semantic similarity calculations. The tools of this chapter are really
    what you'll build for a real-world **natural language processing** (**NLP**) pipeline,
    including an NLU chatbot and an NLU customer support application.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将学会如何完全语义处理一个真实世界的数据集。你将学会如何提取实体、识别意图以及执行语义相似性计算。本章的工具正是你将为真实世界的**自然语言处理**（**NLP**）管道所构建的工具，包括一个
    NLU 聊天机器人和一个 NLU 客户支持应用。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Extracting named entities
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取命名实体
- en: Using dependency relations for intent recognition
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用依存关系进行意图识别
- en: Semantic similarity methods for semantic parsing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义解析的语义相似性方法
- en: Putting it all together
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整合一切
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we'll process a dataset. The dataset and the chapter code can
    be found at [https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter06](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter06).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将处理一个数据集。数据集和本章的代码可以在[https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter06](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter06)找到。
- en: We used the pandas library of Python to manipulate our dataset, besides using
    spaCy. We also used the awk command-line tool. pandas can be installed via pip
    and awk is preinstalled in many Linux distributions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们除了使用 spaCy 之外，还使用了 Python 的 pandas 库来操作我们的数据集。我们还使用了 awk 命令行工具。pandas 可以通过
    pip 安装，而 awk 在许多 Linux 发行版中是预安装的。
- en: Extracting named entities
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取命名实体
- en: In many NLP applications, including semantic parsing, we start looking for meaning
    in a text by examining the entity types and placing an entity extraction component
    into our NLP pipelines. **Named entities** play a key role in understanding the
    meaning of user text.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多 NLP 应用中，包括语义解析，我们通过检查实体类型并将实体提取组件放入我们的 NLP 管道中来开始寻找文本的意义。**命名实体**在理解用户文本的意义中起着关键作用。
- en: We'll also start a semantic parsing pipeline by extracting the named entities
    from our corpus. To understand what sort of entities we want to extract, first,
    we'll get to know the ATIS dataset.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将通过从我们的语料库中提取命名实体来启动一个语义解析管道。为了了解我们想要提取哪种类型的实体，首先，我们将了解 ATIS 数据集。
- en: Getting to know the ATIS dataset
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解 ATIS 数据集
- en: Throughout this chapter, we'll work with the ATIS corpus. ATIS is a well-known
    dataset; it's one of the standard benchmark datasets for intent classification.
    The dataset consists of customer utterances who want to book a flight, get information
    about the flights, including flight costs, flight destinations, and timetables.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 ATIS 语料库。ATIS 是一个知名的数据集；它是意图分类的标准基准数据集之一。该数据集包括想要预订航班、获取航班信息（包括航班费用、目的地和时刻表）的客户话语。
- en: 'No matter what the NLP task is, you should always go over your corpus with
    a naked eye. We want to get to know our corpus so that we integrate our observations
    of corpus into our code. While viewing our text data, we usually keep an eye on
    the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 无论 NLP 任务是什么，您都应该用肉眼检查您的语料库。我们想要了解我们的语料库，以便我们将对语料库的观察整合到我们的代码中。在查看我们的文本数据时，我们通常会关注以下方面：
- en: What kind of utterances are there? Is it a short text corpus or does the corpus
    consist of long documents or medium-length paragraphs?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有哪些类型的话语？是短文本语料库，还是语料库由长文档或中等长度的段落组成？
- en: What sort of entities does the corpus include? People's names, city names, country
    names, organization names, and so on. Which ones do we want to extract?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语料库包含哪些类型的实体？人名、城市名、国家名、组织名等等。我们想要提取哪些？
- en: How is punctuation used? Is the text correctly punctuated, or is no punctuation
    used at all?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标点符号是如何使用的？文本是否正确标点，或者根本不使用标点？
- en: How are the grammatical rules followed? Is the capitalization correct? Did users
    follow the grammatical rules? Are there misspelled words?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语法规则是如何遵循的？大写是否正确？用户是否遵循了语法规则？是否有拼写错误？
- en: 'Before starting any processing, we''ll examine our corpus. Let''s go ahead
    and download the dataset:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始任何处理之前，我们将检查我们的语料库。让我们先下载数据集：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The dataset is a two-column CSV file. First, we''ll get some insights into
    the dataset statistics with **pandas**. pandas is a popular data manipulation
    library that is frequently used by data scientists. You can read more at [https://pandas.pydata.org/pandas-docs/version/0.15/tutorials.html](https://pandas.pydata.org/pandas-docs/version/0.15/tutorials.html):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是一个两列的 CSV 文件。首先，我们将使用 **pandas** 对数据集统计进行一些洞察。pandas 是一个流行的数据处理库，常被数据科学家使用。您可以在
    [https://pandas.pydata.org/pandas-docs/version/0.15/tutorials.html](https://pandas.pydata.org/pandas-docs/version/0.15/tutorials.html)
    上了解更多信息：
- en: 'Let''s begin by reading the CSV file into Python. We''ll use the `read_csv`
    method of pandas:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从将 CSV 文件读入 Python 开始。我们将使用 pandas 的 `read_csv` 方法：
- en: '[PRE1]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The dataset variable holds the CSV as an object for us.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集变量为我们保存了 CSV 对象。
- en: 'Next, we''ll call `head()` on the dataset object. `head()` outputs the first
    10 columns of the dataset:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将对数据集对象调用 `head()`。`head()` 输出数据集的前 10 列：
- en: '[PRE2]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The result looks as follows:'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '![Figure 6.1 – Overview of the dataset'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.1 – 数据集概览'
- en: '](img/B16570_6_1.jpg)'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](img/B16570_6_1.jpg)'
- en: Figure 6.1 – Overview of the dataset
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.1 – 数据集概览
- en: As you see, the dataset object contains rows and columns. It is indeed a CSV
    object. The first column contains the intent, and the second column contains the
    user utterance.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如您所见，数据集对象包含行和列。它确实是一个 CSV 对象。第一列包含意图，第二列包含用户话语。
- en: 'Now we can print some example utterances:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以打印一些示例话语：
- en: '[PRE3]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we can see, the first user wants to book a flight; they included the destination,
    the source cities, and the flight time. The third user is asking about the arrival
    time of a specific flight and the fifth user made a query with a price limit.
    The utterances are not capitalized or punctuated. This is because these utterances
    are an output of a speech-to-text engine.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如我们所见，第一个用户想要预订航班；他们包括了目的地、出发城市和航班时间。第三个用户询问特定航班的到达时间，第五个用户提出了一个价格限制的查询。这些话语没有大写或标点符号。这是因为这些话语是语音识别引擎的输出。
- en: 'Last, we can see the distribution of the number of utterances by intent:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以看到按意图划分的话语数量分布：
- en: '[PRE4]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can find the dataset exploration code at the book's GitHub repository under
    [https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter06/ATIS_dataset_exploration.ipynb](https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter06/ATIS_dataset_exploration.ipynb).
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以在本书的 GitHub 仓库中找到数据集探索代码，链接为 [https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter06/ATIS_dataset_exploration.ipynb](https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter06/ATIS_dataset_exploration.ipynb)。
- en: 'After this point, we''ll process just the utterance text. Hence, we can drop
    the first column. To do so, we''ll play a small trick with the Unix tool awk:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一点之后，我们只处理话语文本。因此，我们可以删除第一列。为此，我们将使用 Unix 工具 awk 玩一个小技巧：
- en: '[PRE5]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we printed the second column of the input CSV file (where the filed separator
    is a `,`) and directed the output into a text file called `atis_utterances.txt`.
    Now that our utterances are ready to process, we can go ahead and extract the
    entities.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们打印了输入 CSV 文件的第二列（其中字段分隔符是 `,`），并将输出重定向到名为 `atis_utterances.txt` 的文本文件。现在，我们的话语已经准备好处理，我们可以继续提取实体。
- en: Extracting named entities with Matcher
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Matcher 提取命名实体
- en: 'As we have already seen, this is a flights dataset. Hence, we expect to see
    city/country names, airport names, and airline names:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这是一个航班数据集。因此，我们期望看到城市/国家名称、机场名称和航空公司名称：
- en: 'Here are some examples:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里有一些示例：
- en: '[PRE6]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Also, the `atis_abbreviation` intent contains utterances that are inquiries
    about some abbreviations. Flight abbreviations can be fare codes (for example,
    M = Economy), airline name codes (for example, United Airlines = UA), and airport
    codes (for example, Berlin Airport = BER), and so on. Examples include the following:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，`atis_abbreviation` 意图包含关于某些缩写的询问。航班缩写可以是票价代码（例如，M = 经济舱），航空公司名称代码（例如，联合航空公司
    = UA），以及机场代码（例如，柏林机场 = BER），等等。以下是一些示例：
- en: '[PRE7]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let's visualize some utterances from the dataset. The following screenshot shows
    the entities with their types:![Figure 6.2 – Example corpus sentences with entities
    and entity types highlighted; generated by the displaCy online demo](img/B16570_6_2.jpg)
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们可视化一些数据集中的话语。以下截图显示了带有其类型的实体：![图6.2 – 带有实体和实体类型高亮的示例语料库句子；由 displaCy 在线演示生成](img/B16570_6_2.jpg)
- en: Figure 6.2 – Example corpus sentences with entities and entity types highlighted;
    generated by the displaCy online demo
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.2 – 带有实体和实体类型高亮的示例语料库句子；由 displaCy 在线演示生成
- en: 'We can see all the entity types and their frequencies more systematically.
    The following code segment makes the following actions:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以更系统地看到所有实体类型及其频率。以下代码段执行以下操作：
- en: a) It reads the text file of utterances that we created in the preceding dataset
    exploration subsection.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 它读取了我们之前在数据集探索子节中创建的话语文本文件。
- en: b) It iterates over each utterance and creates a Doc object.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 它遍历每个话语并创建一个 Doc 对象。
- en: c) It extracts entities from the current doc object.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 它从当前的文档对象中提取实体。
- en: d) It updates the global entity label list with the labels of the entities.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 它使用实体的标签更新全局实体标签列表。
- en: e) It finally calculates the frequency of each label with a counter object.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) 它最终使用计数器对象计算每个标签的频率。
- en: 'Here is the code:'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里是代码：
- en: '[PRE8]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We''ll first extract the location entities by spaCy Matcher by searching for
    a pattern of the `preposition location_name` form. The following code extracts
    location entities preceded with a preposition:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先通过 spaCy Matcher 提取位置实体，通过搜索 `preposition location_name` 形式的模式。以下代码提取了由介词引导的位置实体：
- en: '[PRE9]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We already saw how to initialize a Matcher object and add patterns to it. Still,
    we''ll recall how to use a Matcher object to extract the matches now. Here''s
    what we did in this code segment:'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们已经看到了如何初始化 Matcher 对象并向其中添加模式。尽管如此，我们还是回顾一下如何使用 Matcher 对象提取匹配项。以下是这段代码所做的事情：
- en: a) We started by importing `spacy` and the `spacy.matcher` class in *lines 1-2*.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 我们从导入 `spacy` 和 `spacy.matcher` 类在第 1-2 行开始。
- en: b) We created a language pipeline object, `nlp`, at *line 3*.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 我们在第 3 行创建了一个语言管道对象，`nlp`。
- en: c) In *line 4*, we initialized the Matcher object with the language vocabulary.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 在第 4 行，我们使用语言词汇初始化了 Matcher 对象。
- en: d) In line 5, we created a pattern matching two tokens, a preposition (`POS`
    tag `ADP` means an *adposition = preposition + postposition*) and a location entity
    (label `GPE` means a **location entity**).
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 在第 5 行，我们创建了一个匹配两个标记的模式，一个介词（`POS` 标签 `ADP` 表示 *adposition = preposition
    + postposition*）和一个位置实体（标签 `GPE` 表示 **location entity**）。
- en: e) We added this pattern to the Matcher object.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) 我们将此模式添加到 Matcher 对象中。
- en: f) Finally, we asked for the matches in an example corpus sentence and printed
    the matches.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: f) 最后，我们在一个示例语料库句子中请求匹配项并打印了匹配项。
- en: 'Although the `from` and `to` prepositions dominate in this dataset, verbs about
    leaving and arriving can be used with a variety of prepositions. Here are some
    more example sentences from the dataset:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然在这个数据集中 `from` 和 `to` 介词占主导地位，但关于离开和到达的动词可以与各种介词一起使用。以下是数据集中的一些更多示例句子：
- en: '[PRE10]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The second example sentence is a question sentence:'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二个示例句子是一个疑问句：
- en: '[PRE11]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Another example sentence from the dataset contains an abbreviation in a destination
    entity:'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集中另一个示例句子包含一个目的地实体中的缩写：
- en: '[PRE12]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Our last example sentence is again a question sentence:'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们最后的示例句子又是一个疑问句：
- en: '[PRE13]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, we see some phrasal verbs such as `arrive in`, as well as preposition
    and verb combinations such as `stop in` and `fly out of`. `By the way of Dallas`
    does not include a verb at all. The user indicated that they want to make a stop
    at Dallas. `to`, `from`, `in`, `out`, and `of` are common prepositions that are
    used in a traveling context.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到一些短语动词，例如 `arrive in`，以及介词和动词的组合，例如 `stop in` 和 `fly out of`。`By the
    way of Dallas` 完全没有动词。用户表示他们想在达拉斯停留。`to`、`from`、`in`、`out` 和 `of` 是在旅行语境中常用的介词。
- en: 'After extracting the locations, we can now extract the airline information.
    The `ORG` entity label means an organization and it corresponds to airline company
    names in our dataset. The following code segment extracts the organization names,
    possibly multi-worded names:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在提取了位置信息之后，我们现在可以提取航空公司信息。`ORG` 实体标签表示一个组织，在我们的数据集中对应于航空公司名称。以下代码段提取了组织名称，可能是多词名称：
- en: '[PRE14]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Extracting dates and times are not very different; you can replicate the preceding
    with code with `ENT_TYPE: DATE` and `ENT_TYPE: TIME`. We encourage you to try
    it yourself. The following screenshot exhibits how the date and time entities
    look in detail:![Figure 6.3 – Example dataset sentences with date and time entities
    highlighted. The image is generated by the displaCy online demo](img/B16570_6_3.jpg)'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '提取日期和时间并不非常不同；你可以用 `ENT_TYPE: DATE` 和 `ENT_TYPE: TIME` 代码复制前面的操作。我们鼓励你自己尝试。以下截图展示了日期和时间实体在细节上的样子：![图6.3
    – 带有日期和时间实体的高亮示例数据集句子。该图像由 displaCy 在线演示生成](img/B16570_6_3.jpg)'
- en: Figure 6.3 – Example dataset sentences with date and time entities highlighted.
    The image is generated by the displaCy online demo
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.3 – 带有日期和时间实体的高亮示例数据集句子。该图像由 displaCy 在线演示生成
- en: 'Next, we''ll extract abbreviation type entities. Extracting the abbreviation
    entities is a bit trickier. First, we will have a look at how the abbreviations
    appear:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将提取缩写类型实体。提取缩写实体稍微有些复杂。首先，我们将看看缩写是如何出现的：
- en: '[PRE15]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Only one of these sentences includes an entity. The first example sentence
    includes an `AMOUNT` entity, which is `57`. Other than that, abbreviations are
    not marked with any entity type at all. In this case, we have to provide some
    custom rules to the Matcher. Let''s make some observations first, and then form
    a Matcher pattern:'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有这些句子中的一个包含实体。第一个示例句子包含一个 `AMOUNT` 实体，其值为 `57`。除此之外，缩写根本没有任何实体类型标记。在这种情况下，我们必须向匹配器提供一些自定义规则。让我们先做一些观察，然后形成一个匹配器模式：
- en: a) An abbreviation can be broken into two parts – letters, and digits.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 缩写可以分成两部分 – 字母和数字。
- en: b) The letter part can be 1-2 characters long.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 字母部分可以是1-2个字符长。
- en: c) The digit part is also 1-2 characters long.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 数字部分也是1-2个字符长。
- en: d) The presence of digits indicates an abbreviation entity.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 数字的存在表明了一个缩写实体。
- en: 'e) The presence of the following words indicates an abbreviation entity: class,
    code, abbreviation.'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) 以下单词的存在表明了一个缩写实体：class、code、abbreviation。
- en: f) The `POS` tag of an abbreviation is a noun. If the candidate word is a 1-letter
    or 2-letter word, then we can look at the `POS` tag and see whether it's a noun.
    This approach eliminates the false positives, such as *us* (pronoun), *me* (pronoun),
    *a* (determiner), and *an* (determiner).
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: f) 缩写的 `POS` 标签是名词。如果候选词是1个或2个字母的词，那么我们可以查看 `POS` 标签，看它是否是名词。这种方法消除了假阳性，例如 *us*（代词）、*me*（代词）、*a*（限定词）和
    *an*（限定词）。
- en: 'Let''s now put these observations into Matcher patterns:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将这些观察结果放入匹配器模式中：
- en: '[PRE16]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then we create a Matcher object with the patterns we defined:'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后我们使用我们定义的模式创建一个匹配器对象：
- en: '[PRE17]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We''re now ready to feed our sentences into the matcher:'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们现在已经准备好将我们的句子输入到匹配器中：
- en: '[PRE18]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the preceding code, we defined four patterns:'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们定义了四个模式：
- en: a) The first pattern matches to a single token, which consists of 1-2 letters
    and 1-2 digits. For example, `d1`, `d10`, `ad1`, and `ad21` will match this pattern.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 第一个模式匹配单个标记，该标记由1-2个字母和1-2个数字组成。例如，`d1`、`d10`、`ad1` 和 `ad21` 将匹配此模式。
- en: b) The second pattern matches to 2-token abbreviations where the first token
    is 1-2 letters and the second token 1-2 digits. The abbreviations `ap 5`, `ap
    57`, `a 5`, and `a 57` will match this pattern.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 第二种模式匹配两个标记的缩写，第一个标记是 1-2 个字母，第二个标记是 1-2 个数字。缩写 `ap 5`、`ap 57`、`a 5` 和 `a
    57` 将匹配此模式。
- en: c) The third pattern matches to two tokens too. The first token is a context
    clue word, such as `class` or `code`, and the second token should be a 1-2 letter
    token. Some example matches are `code f`, `code y`, and `class c`.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 第三种模式也匹配两个标记。第一个标记是一个上下文线索词，例如 `class` 或 `code`，第二个标记应该是一个 1-2 个字母的标记。一些匹配示例是
    `code f`、`code y` 和 `class c`。
- en: d) The fourth pattern extracts 1-2 letter short words whose `POS` tag is `NOUN`.
    Some example matches from the preceding sentences are `c` and `co`.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 第四种模式提取了 `POS` 标签为 `NOUN` 的 1-2 个字母的短词。前文句子中的某些匹配示例是 `c` 和 `co`。
- en: spaCy Matcher makes life easy for us by allowing us to make use of token shape,
    context clues, and a token `POS` tag. We made a very successful entity extraction
    in this subsection by extracting locations, airline names, dates, times, and abbreviations.
    In the next subsection, we'll go deeper into the sentence syntax and extract entities
    from the sentences where context does not offer many clues.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 匹配器通过允许我们利用标记形状、上下文线索和标记 `POS` 标签来简化我们的工作。在本小节中，我们通过提取地点、航空公司名称、日期、时间和缩写词，成功地进行了实体提取。在下一小节中，我们将更深入地研究句子语法，并从上下文提供很少线索的句子中提取实体。
- en: Using dependency trees for extracting entities
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用依存树提取实体
- en: 'In the previous subsection, we extracted entities where the context provides
    obvious clues. Extracting the destination city from the following sentence is
    easy. We can look for the `to + GPE` pattern:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一小节中，我们提取了上下文提供明显线索的实体。从以下句子中提取目的地城市很容易。我们可以寻找 `to + GPE` 模式：
- en: '[PRE19]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'But suppose the user provides one of the following sentences instead:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 但假设用户提供了以下句子之一：
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here, the preposition `to` refers to `conference`, not `Munich`, in the first
    sentence. In this sentence, we need a pattern such as `to + .... + GPE`. Then,
    we have to be careful what words can come in between "to" and the city name, as
    well as what words should not come. For instance, this sentence carries a completely
    different meaning and shouldn''t match:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，介词 `to` 在第一句话中指的是 `conference`，而不是 `Munich`。在这句话中，我们需要一个类似于 `to + .... +
    GPE` 的模式。然后，我们必须小心哪些词可以放在 "to" 和城市名称之间，以及哪些词不应该出现。例如，这个句子有完全不同的含义，不应该匹配：
- en: '[PRE21]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the second sentence, there's no `to` at all. Here, as we see from these examples,
    we need to examine the syntactic relations between words. In [*Chapter 3*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)*,
    Linguistic Features*, we already saw how to interpret dependency trees to understand
    the relations between words. In this subsection, we'll walk the dependency trees.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二句话中，根本就没有 `to`。从这些例子中我们可以看到，我们需要检查词语之间的句法关系。在 [*第3章*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)*，语言特征*
    中，我们已经看到了如何解释依存树来理解词语之间的关系。在本小节中，我们将遍历依存树。
- en: Walking a dependency tree means visiting the tokens in a custom order, not necessarily
    from left to right. Usually, we stop iterating over the dependency tree once we
    find what we're looking for. Again, a dependency tree shows the syntactic relations
    between its words. Recall from [*Chapter 3*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)*,
    Linguistic Features*, that the relations are represented with directed arrows,
    connecting the head and the child of a relation. Every word in a sentence has
    to involve at least one relation. This fact guarantees that we'll visit each word
    while walking through the sentence.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历依存树意味着以自定义的顺序访问标记，而不一定是从左到右。通常，一旦找到我们想要的东西，我们就停止遍历依存树。再次强调，依存树显示了其词语之间的句法关系。回想一下
    [*第3章*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)*，语言特征* 中，这些关系是用有向箭头表示的，连接关系的头部和子节点。句子中的每个词都必须至少涉及一个关系。这个事实保证了我们在遍历句子时会访问每个词。
- en: Recall
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 回想
- en: Before proceeding to code, first, let's remember some concepts about dependency
    trees. ROOT is a special dependency label and is always assigned to the main verb
    of the sentence. spaCy shows syntactic relations with arcs. One of the tokens
    is the syntactic parent (called the HEAD) and the other is dependent (called the
    CHILD). By way of an example, in *Figure 6.3*, **going** has 3 syntactic children
    – **I**, **m**, and **to**. Equivalently, the syntactic head of **to** is **going**
    (the same applies to **I** and **m**).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续编写代码之前，首先让我们回顾一下关于依赖树的一些概念。ROOT 是一个特殊的依赖标签，总是分配给句子的主谓。spaCy 使用弧来显示句法关系。其中一个标记是句法父节点（称为
    HEAD），另一个是依赖节点（称为 CHILD）。以一个例子来说明，在 *图 6.3* 中，**going** 有 3 个句法子节点 – **I**，**m**
    和 **to**。同样，**to** 的句法头是 **going**（这对 **I** 和 **m** 也适用）。
- en: 'Coming back to our examples, we''ll iterate the utterance dependency trees
    to find out whether the preposition **to** is syntactically related to the location
    entity, **Munich**. First of all, let''s see the dependency parse of our example
    sentence *I''m going to a conference in Munich* and also remember what a dependency
    tree looks like:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的例子，我们将迭代话语依赖树，以找出介词 **to** 是否与地点实体 **Munich** 有句法关系。首先，让我们看看我们的例子句子 *I'm
    going to a conference in Munich* 的依赖分析，并记住依赖树的样子：
- en: '![Figure 6.4 – Dependency parse of the example sentence'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.4 – 示例句子的依赖分析'
- en: '](img/B16570_6_4.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_6_4.jpg)'
- en: Figure 6.4 – Dependency parse of the example sentence
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 示例句子的依赖分析
- en: There are no incoming arcs into the verb **going**, so **going** is the root
    of the dependency tree (when we examine the code, we'll see that the dependency
    label is **ROOT**). This is supposed to happen because **going** is the main verb
    of the sentence. If we follow the arc to its immediate right, we encounter **to**;
    jumping over the arcs to the right we reach **Munich**. This shows that there's
    a syntactic relation between **to** and **Munich**.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 动词 **going** 没有进入的弧，所以 **going** 是依赖树的根（当我们查看代码时，我们会看到依赖标签是 **ROOT**）。这是应该发生的，因为
    **going** 是句子的主谓。如果我们跟随右边的弧，我们会遇到 **to**；跳过右边的弧，我们会到达 **Munich**。这表明 **to** 和
    **Munich** 之间存在句法关系。
- en: 'Let''s now iterate over the dependency tree with code. There are two possible
    ways to connect **to** and **Munich**:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们用代码遍历依赖树。有两种可能的方式将 **to** 和 **Munich** 连接起来：
- en: Left to right. We start from **to** and try to reach **Munich** by visiting
    "to"'s syntactic children. This approach may not be a very good idea, because
    if "to" has more than one child, then we need to check each child and keep track
    of all the possible paths.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从左到右。我们从 **to** 开始，通过访问 "to" 的句法子节点来尝试到达 **Munich**。这种方法可能不是一个很好的主意，因为如果 "to"
    有多个子节点，那么我们需要检查每个子节点并跟踪所有可能的路径。
- en: Right to left. We start from **Munich**, jump onto its head, and follow the
    head's head, and so on. Since each word has exactly one head, it's guaranteed
    that there will be only one path. Then we determine whether **to** is on this
    path or not.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从右到左。我们从 **Munich** 开始，跳到其头，然后跟随头的头，依此类推。由于每个词只有一个头，所以可以保证只有一条路径。然后我们确定 **to**
    是否在这条路径上。
- en: 'The following code segments implement the second approach, start the dependency
    tree walk from `Munich`, and look for `to`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码段实现了第二种方法，从 `Munich` 开始依赖树遍历，并查找 `to`：
- en: '[PRE22]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the `reach_parent` function, the following applies:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `reach_parent` 函数中，以下适用：
- en: We start from a source token and try to reach the destination token.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从一个源标记开始，尝试到达目标标记。
- en: In the `while` loop, we iterate over the head of each token, starting from the
    source token.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `while` 循环中，我们从源标记开始迭代每个标记的头。
- en: The loop stops when we reach either the source token or the root of the sentence.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们到达源标记或句子的根时，循环停止。
- en: We test for reaching the root via the `source_token == source_token.head` line.
    Because the root token always refers to itself, its head is itself (remember that
    in the dependency tree, there are no incoming arcs into the root).
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过 `source_token == source_token.head` 这一行测试是否到达了根。因为根标记总是指代自身，其头也是自身（记住在依赖树中，根没有进入的弧）。
- en: Finally, we tested our function on two different test cases. In the first one,
    the source and destination are related, whereas in the second test, there's no
    relation, hence the function returns `None`.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们在两个不同的测试用例上测试了我们的函数。在第一个测试用例中，源和目的地相关联，而在第二个测试用例中，没有关系，因此函数返回 `None`。
- en: This approach is very different from the previous subsection's rather straightforward
    approach. Natural language is complicated and challenging to process, and it's
    important to know the approaches available and have the necessary tools in your
    toolbox when you need to use them.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法与前面小节的相当直接的方法非常不同。自然语言很复杂，难以处理，当你需要使用它们时，了解可用的方法和拥有必要的工具箱是很重要的。
- en: Next, we'll dive into a popular subject – intent recognition with a syntactic
    approach. Let's see some key points of designing a good intent recognition component,
    including recognizing multiple intents as well.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入一个热门主题——使用句法方法进行意图识别。让我们看看设计一个好的意图识别组件的一些关键点，包括识别多个意图。
- en: Using dependency relations for intent recognition
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用依存关系进行意图识别
- en: After extracting the entities, we want to find out what sort of intent the user
    carries – to book a flight, to purchase a meal on their already booked flight,
    cancel their flight, and so on. If you look at the intents list again, you will
    see that every intent includes a verb (to book) and an object that the verb acts
    on (flight, hotel, meal).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在提取实体之后，我们想要找出用户携带的意图类型——预订航班、在他们已预订的航班上购买餐点、取消航班等等。如果你再次查看意图列表，你会看到每个意图都包括一个动词（预订）以及动词所作用的宾语（航班、酒店、餐点）。
- en: In this section, we'll extract transitive verbs and their direct objects from
    utterances. We'll begin our intent recognition section by extracting the transitive
    verb and the direct object of the verb. Then, we'll explore how to understand
    a user's intent by recognizing synonyms of verbs and nouns. Finally, we'll see
    how to determine a user's intent with semantic similarity methods. Before we move
    on to extracting transitive verbs and their direct objects, let's first quickly
    go over the concepts of transitive verbs and direct/indirect objects.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从话语中提取及物动词及其直接宾语。我们将通过提取及物动词及其直接宾语来开始我们的意图识别部分。然后，我们将探讨如何通过识别动词和名词的同义词来理解用户的意图。最后，我们将看到如何使用语义相似度方法来确定用户的意图。在我们继续提取及物动词及其直接宾语之前，让我们首先快速回顾一下及物动词和直接/间接宾语的概念。
- en: Linguistic primer
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言学入门
- en: 'In this section, we''ll explore some linguistic concepts related to sentence
    structure, including verbs and verb-object relations. A verb is a very important
    component of the sentence as it indicates the action in the sentence. The object
    of the sentence is the thing/person that is affected by the action of the verb.
    Hence, there''s a natural connection between the sentence verb and objects. The
    concept of transitivity captures verb-object relations. A transitive verb is a
    verb that needs an object to act upon. Let''s see some examples:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨与句子结构相关的某些语言学概念，包括动词和动词-宾语关系。动词是句子的一个非常重要的组成部分，因为它表明了句子中的动作。句子的宾语是受到动词动作影响的物体/人。因此，句子动词和宾语之间存在自然联系。及物性概念捕捉了动词-宾语关系。及物动词是需要一个宾语来对其施加动作的动词。让我们看看一些例子：
- en: '[PRE23]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In these example sentences, `bought`, `loved`, and `borrowed` are transitive
    verbs. In the first sentence, `bought` is the transitive verb and `flowers` is
    its object, the thing that has been bought by the sentence subject, `I`. `Loved`
    – `his cat` and `borrowed` – `my book` are transitive verb-object examples. We'll
    focus on the first sentence again - what happens if we erase the `flowers` object?
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些示例句子中，“bought”、“loved”和“borrowed”是及物动词。在第一个句子中，“bought”是及物动词，而“flowers”是其宾语，即句子主语“我”所购买的物体。而“loved”
    – “他的猫”和“borrowed” – “我的书”是及物动词-宾语例子。我们将再次关注第一个句子——如果我们擦除“flowers”宾语会发生什么？
- en: '[PRE24]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Bought **what**? Without an object, this sentence doesn't carry any meaning
    at all. In the preceding sentences, each of the objects completes the meaning
    of the verb. This is a way of understanding whether a verb is transitive or not
    – erase the object and check whether the sentence remains semantically intact.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 买了**什么**？没有宾语，这个句子根本没有任何意义。在前面的句子中，每个宾语都完成了动词的意义。这是理解动词是否及物的一种方式——擦除宾语并检查句子是否在语义上保持完整。
- en: 'Some verbs are transitive and some verbs are intransitive. An **intransitive
    verb** is the opposite of a transitive verb; it doesn''t need an object to act
    upon. Let''s see some examples:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 有些动词是及物的，有些动词是不及物的。**不及物动词**是与及物动词相反的；它不需要宾语来对其施加动作。让我们看看一些例子：
- en: '[PRE25]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In all the preceding sentences, the verbs make sense without an object. If
    we erase all the words other than the subject and object, these sentences are
    still meaningful:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有前面的句子中，动词在没有宾语的情况下是有意义的。如果我们删除除了主语和宾语之外的所有单词，这些句子仍然是有效的：
- en: '[PRE26]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Pairing an intransitive verb with an object doesn't make sense. You can't run
    someone/something, you can't shine something/someone, and you certainly cannot
    die something/someone.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 将不及物动词与宾语搭配是没有意义的。你不能让某人/某物跑，你不能让某人/某物发光，你当然也不能让某人/某物死亡。
- en: Sentence object
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 句子宾语
- en: As we remarked before, the object is the thing/person that is affected by the
    verb's action. The action stated by the verb is committed by the sentence subject
    and the sentence object gets affected.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，宾语是受到动词动作影响的物体/人。动词所陈述的动作是由句子主语执行的，而句子宾语受到影响。
- en: 'A sentence can be direct or indirect. A **direct object** answers the questions
    **whom?** / **what?** You can find the direct object by asking **The subject {verb}
    what/who?**. Here are some examples:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一个句子可以是直接的或间接的。**直接宾语**回答问题**谁？** / **什么？** 你可以通过问**主语{动词}什么/谁？**来找到直接宾语。以下是一些例子：
- en: '[PRE27]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'An **indirect object** answers the questions **for what?**/**for whom?**/**to
    whom?**. Let''s see some examples:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**间接宾语**回答问题**为了什么？**/**为了谁？**/**给谁？**。让我们看看一些例子：'
- en: '[PRE28]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Indirect objects are often preceded by the prepositions to, for, from, and so
    on. As you can see from these examples, an indirect object is also an object and
    is affected by the verb's action, but its role in the sentence is a bit different.
    An indirect object is sometimes viewed as the recipient of the direct object.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 间接宾语通常由介词to、for、from等引导。正如你从这些例子中看到的，间接宾语也是一个宾语，并且受到动词动作的影响，但它在句子中的角色略有不同。间接宾语有时被视为直接宾语的接受者。
- en: 'This is all you need to know about transitive/intransitive verbs and direct/indirect
    objects to digest this chapter''s material. If you want to learn more about sentence
    syntax, you can read the great book *Linguistic Fundamentals for Natural Language
    Processing* by **Emily Bender**: ([https://dl.acm.org/doi/book/10.5555/2534456](https://dl.acm.org/doi/book/10.5555/2534456)).
    We have covered the basics of sentence syntax, but this is still a great resource
    to learn about syntax in depth.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你需要了解的关于及物/不及物动词和直接/间接宾语的知识，以便消化本章的内容。如果你想了解更多关于句子语法的知识，你可以阅读Emily Bender的杰出著作《自然语言处理的语言学基础》**Emily
    Bender**：([https://dl.acm.org/doi/book/10.5555/2534456](https://dl.acm.org/doi/book/10.5555/2534456))。我们已经涵盖了句子语法的基础知识，但这仍然是一个深入了解语法的极好资源。
- en: Extracting transitive verbs and their direct objects
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取及物动词及其直接宾语
- en: 'While recognizing the intent, usually we apply these steps to a user utterance:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在识别意图的同时，我们通常将这些步骤应用于用户的说话：
- en: Splitting the sentence into tokens.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将句子拆分为标记。
- en: 'Dependency parsing is performed by spaCy. We walk the dependency tree to extract
    the tokens and relations that we''re interested in, which are the verb and the
    direct object, as shown in the following diagram:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 依存句法分析由spaCy执行。我们遍历依存树以提取我们感兴趣的标记和关系，这些标记和关系是动词和直接宾语，如下面的图所示：
- en: '![Figure 6.5 – Dependency parse of an example sentence from the corpus'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.5 – 来自语料库的示例句子的依存句法分析'
- en: '](img/B16570_6_5.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_6_5.jpg)'
- en: Figure 6.5 – Dependency parse of an example sentence from the corpus
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 来自语料库的示例句子的依存句法分析
- en: In this example sentence, the transitive verb is **find** and the direct object
    is **a flight**. The relation **dobj** connects a transitive verb to its direct
    object. If we follow the arc, semantically, we see that the user wants to commit
    the action of finding and the object they want to find is a flight. We can merge
    **find** and **a flight** into a single word, **findAflight** or **findFlight**,
    which can be this intent's name. Other intents can be **bookFlight**, **cancelFlight**,
    **bookMeal**, and so on.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子句子中，及物动词是**找到**，直接宾语是**航班**。关系**dobj**将及物动词与其直接宾语连接起来。如果我们沿着弧线追踪，从语义上看，我们可以看到用户想要执行的动作是找到，他们想要找到的物体是一个航班。我们可以将**找到**和**航班**合并成一个单词，**findAflight**或**findFlight**，这可以成为这个意图的名称。其他意图可以是**bookFlight**（预订航班）、**cancelFlight**（取消航班）、**bookMeal**（预订餐点）等等。
- en: 'Let''s extract the verb and the direct object in a more systematic way. We''ll
    first spot the direct object by looking for the `dobj` label in the sentence.
    To locate the transitive verb, we look at the direct object''s syntactic head.
    A sentence can include more than one verb, hence we''re careful while processing
    the verbs. Here is the code:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以更系统的方式提取动词和直接宾语。我们首先通过在句子中寻找 `dobj` 标签来定位直接宾语。为了定位及物动词，我们查看直接宾语的句法主语。一个句子可以包含多个动词，因此在处理动词时要小心。以下是代码：
- en: '[PRE29]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In this code segment, the following applies:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，以下内容适用：
- en: We applied the pipeline to our sample sentence.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将管道应用于我们的样本句子。
- en: Next, we spotted the direct object by looking for a token whose dependency label
    is `dobj`.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们通过寻找依赖标签为 `dobj` 的标记来定位直接宾语。
- en: When we located a direct object, we spotted the corresponding transitive verb
    by obtaining the direct object's syntactic head.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们定位到直接宾语时，通过获取直接宾语的句法主语来定位相应的及物动词。
- en: Finally, we printed the verb and the object to generate this intent's name.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们打印出动词和宾语以生成这个意图的名称。
- en: Great! Intent recognition was successful! Here, we recognized a single intent.
    Some utterances may carry multiple intents. In the next section, we'll learn how
    to recognize multiple intents based on the techniques of this section.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！意图识别成功！在这里，我们识别出一个单一意图。有些语句可能包含多个意图。在下一节中，我们将学习如何根据本节的技巧识别多个意图。
- en: Extracting multiple intents with conjunction relation
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取具有联合关系的多个意图
- en: 'Some utterances carry multiple intents. For example, consider the following
    utterance from the corpus:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 有些语句包含多个意图。例如，考虑以下来自语料库的语句：
- en: '[PRE30]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here, the user wants to list all the flights and, at the same time, wants to
    see the fare info. One way of processing is considering these intents as a single
    and complex intent. In that case, we can express this complex intent as `action:
    show, objects: flights, fares`. Another and more common way of processing this
    sort of utterance is to label the utterance with multiple intents. In the dataset,
    this example utterance is marked with two intents as `atis_flight#atis_airfare`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，用户想要列出所有航班，同时还想查看票价信息。一种处理方式是将这些意图视为一个单一且复杂的意图。在这种情况下，我们可以将这个复杂意图表示为 `action:
    show, objects: flights, fares`。另一种更常见的方式是给这种语句标注多个意图。在数据集中，这个示例语句被标记为两个意图 `atis_flight#atis_airfare`：'
- en: '![Figure 6.6 – Dependency tree of the example dataset utterance. The conj relation
    is between "flight" and "fares" ](img/B16570_6_6.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6 – 示例数据集语句的依存树。conj 关系在 "flight" 和 "fares" 之间](img/B16570_6_6.jpg)'
- en: Figure 6.6 – Dependency tree of the example dataset utterance. The conj relation
    is between "flight" and "fares"
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – 示例数据集语句的依存树。conj 关系在 "flight" 和 "fares" 之间
- en: 'In the preceding diagram, we see that the **dobj** arc connects **show** and
    **flights**. The **conj** arc connects **flights** and **fares** to indicate the
    conjunction relation between them. The conjunction relation is built by a conjunction
    such as **and** or **or** and indicates that a noun is joined to another noun
    by this conjunction. In this situation, we extract the direct object and its conjuncts.
    Let''s now see how we can turn this process into code:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们看到 **dobj** 弧将 **show** 和 **flights** 连接起来。**conj** 弧将 **flights**
    和 **fares** 连接起来，表示它们之间的联合关系。联合关系由诸如 **and** 或 **or** 这样的连词构建，表示一个名词通过这个连词与另一个名词相连。在这种情况下，我们提取直接宾语及其并列成分。现在让我们看看我们如何将这个过程转化为代码：
- en: '[PRE31]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Here, we looped over all tokens to locate the direct object of the sentence.
    When we found the direct object, we obtained its conjuncts. After that, finding
    the transitive verb is the same as we did in the previous code segment, we extracted
    the direct object's head. After extracting the verb and two objects, if we want,
    we can combine the two to create two intent names – `showFlights` and `showFares`.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们遍历所有标记以定位句子的直接宾语。当我们找到直接宾语时，我们获取其并列成分。之后，找到及物动词的方式与之前的代码段相同，我们提取直接宾语的主语。在提取动词和两个宾语之后，如果我们愿意，可以将它们组合起来创建两个意图名称
    – `showFlights` 和 `showFares`。
- en: Recognizing the intent using wordlists
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用词表识别意图
- en: In some cases, tokens other than the transitive verb and the direct object contain
    the semantics of the user intent. In that case, you need to go further down in
    the syntactic relations and explore the sentence structure deeper.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，除了及物动词和直接宾语之外的其他标记可能包含用户意图的语义。在这种情况下，你需要进一步深入句法关系并更深入地探索句子结构。
- en: 'As an example, consider the following utterance from our dataset:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 以我们的数据集中的一个以下语句为例：
- en: '[PRE32]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In this sentence, the **verb-object** pair that best describes the user intent
    is **want-flight**. However, if we look at the parse tree in *Figure 6.7*, we
    see that **want** and **flight** are not directly related in the parse tree. **want**
    is related to the transitive verb **make**, and **flight** is related to the direct
    object **reservation**, respectively:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个句子中，最能描述用户意图的**动词-宾语**对是**想要-航班**。然而，如果我们查看*图6.7*中的分析树，我们会看到**想要**和**航班**在分析树中并没有直接关联。**想要**与及物动词**制作**相关，而**航班**与直接宾语**预订**分别相关：
- en: '![Figure 6.7 – Parse tree of the example sentence from the dataset'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.7 – 数据集中示例句子的分析树'
- en: '](img/B16570_6_7.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_6_7.jpg]'
- en: Figure 6.7 – Parse tree of the example sentence from the dataset
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 6.7 – 数据集中示例句子的分析树
- en: 'What will we do then? We can play a trick and keep a list of helper verbs such
    as **would like**, **want**, **make**, and **need**. Here''s the code:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们将做什么呢？我们可以玩一个技巧，保留一个包含辅助动词的列表，例如**想要**、**需要**、**制作**和**需要**。以下是代码：
- en: '[PRE33]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here''s what we did step by step:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的逐步操作：
- en: We started by locating the direct object and its transitive verb.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定位直接宾语及其及物动词。
- en: Once we found them, we compared them against our predefined lists of words.
    For this example, we used two shortened lists, `verbList` contains a list of helper
    verbs, and `objList` contains a list of the possible object words we want to extract.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦找到它们，我们就将它们与我们的预定义词汇列表进行比较。对于这个例子，我们使用了两个简化的列表，`verbList`包含辅助动词列表，而`objList`包含我们想要提取的可能宾语词汇列表。
- en: We checked the transitive verb. If it's not in the list of helper verbs, then
    we checked the main verb of the sentence (marked by `ROOT`), which is the head
    of the transitive verb. If the transitive verb is the main verb of the sentence,
    then the syntactic head of this verb is itself (`tVerb.head` is `tVerb)`. Hence,
    the line `if tVerb.head.dep_ == "ROOT"` evaluates to `True` and this implementation
    works.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们检查了及物动词。如果它不在辅助动词列表中，那么我们就检查句子的主要动词（由`ROOT`标记），它是及物动词的头部。如果及物动词是句子的主要动词，那么这个动词的句法头部就是它自己（`tVerb.head`是`tVerb`）。因此，`if
    tVerb.head.dep_ == "ROOT"`这一行评估为`True`，这种实现是有效的。
- en: Next, we checked the direct object. If it's not in the list of possible objects,
    then we check its syntactic children. For each child, we check whether the child
    is a preposition of the direct object. If so, we pick up the child's child (it
    can have only one child).
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们检查直接宾语。如果它不在可能宾语列表中，那么我们就检查它的句法子节点。对于每个子节点，我们检查该子节点是否是直接宾语的介词。如果是，我们就选择该子节点的子节点（它只能有一个子节点）。
- en: Finally, we printed the string that represents the intent name, which is `wantFlight`.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们打印出表示意图名称的字符串，它是`wantFlight`。
- en: At this point, take a deep breath. It takes time to digest and process information,
    especially when it's about sentence syntax. You can try different sentences from
    the corpus and see what the script does by checkpointing/putting prints into the
    code.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，深呼吸。消化和处理信息需要时间，尤其是当它涉及到句子句法时。你可以尝试从语料库中提取不同的句子，并通过检查点/在代码中添加打印语句来查看脚本的行为。
- en: In the next section, we'll explore a very handy tool, using synonym lists. Let's
    move ahead to the next section and learn how to make the best of semantic similarity.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨一个非常实用的工具，即使用同义词列表。让我们继续前进到下一节，学习如何充分利用语义相似性。
- en: Semantic similarity methods for semantic parsing
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语义解析的语义相似性方法
- en: Natural language allows us to express the same concept in different ways and
    with different words. Every language has synonyms and semantically related words.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言使我们能够用不同的方式和不同的词汇表达相同的概念。每种语言都有同义词和语义相关的词汇。
- en: As an NLP developer, while developing a semantic parser for a chatbot application,
    text classification, or any other semantic application, you should keep in my
    mind that users use a fairly wide set of phrases and expressions for each intent.
    In fact, if you're building a chatbot by using a platform such as RASA ([https://rasa.com/](https://rasa.com/))
    or on a platform such as Dialogflow ([https://dialogflow.cloud.google.com/](https://dialogflow.cloud.google.com/)),
    you're asked to provide as many utterance examples as you can provide for each
    intent. Then, these utterances are used to train the intent classifier behind
    the scenes.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名NLP开发者，在为聊天机器人应用、文本分类或任何其他语义应用开发语义解析器时，你应该记住，用户为每个意图使用相当广泛的短语和表达方式。实际上，如果你正在使用RASA（[https://rasa.com/](https://rasa.com/））或Dialogflow（[https://dialogflow.cloud.google.com/](https://dialogflow.cloud.google.com/））等平台构建聊天机器人，你会被要求为每个意图提供尽可能多的句子示例。然后，这些句子被用来在幕后训练意图分类器。
- en: There are usually two ways to recognize semantic similarity, either with a synonyms
    dictionary or with word vector-based semantic similarity methods. In this section,
    we will discuss both approaches. Let's start with how to use a synonyms dictionary
    to detect semantic similarity.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 通常有两种方法来识别语义相似性，要么使用同义词词典，要么使用基于词向量语义相似度方法。在本节中，我们将讨论这两种方法。让我们先从如何使用同义词词典来检测语义相似性开始。
- en: Using synonyms lists for semantic similarity
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用同义词列表进行语义相似度
- en: We already went through our dataset and saw that different verbs are used to
    express the same actions. For instance, **landing**, **arriving**, and **flying
    to** verbs carry the same meaning, whereas **leaving**, **departing**, and **flying
    from** verbs form another semantic group.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经遍历了我们的数据集，并看到不同的动词被用来表达相同的行为。例如，“降落”、“到达”和“飞往”动词具有相同的意义，而“离开”、“出发”和“从飞往”动词形成另一个语义组。
- en: We already saw that in most cases, the transitive verbs and direct objects express
    the intent. An easy way to determine whether two utterances represent the same
    intent is to check whether the verbs and the direct objects are synonyms.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，在大多数情况下，及物动词和直接宾语表达意图。判断两个句子是否代表相同意图的一个简单方法就是检查动词和直接宾语是否是同义词。
- en: 'Let''s take an example and compare two example utterances from the dataset.
    First, we prepare a small synonyms dictionary. We include only the base forms
    of the verbs and nouns. While doing the comparison, we also use the base form
    of the words:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一个例子，并比较数据集中的两个示例句子。首先，我们准备一个小型的同义词词典。我们只包括动词和名词的基本形式。在进行比较时，我们也使用单词的基本形式：
- en: '[PRE34]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Each **synonym set** (**synset**) includes the set of synonyms for our domain.
    We usually include the language-general synonyms (airplane-plane) and the domain-specific
    synonyms (book-buy).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 每个**同义词集**（**synset**）包括我们领域的同义词集合。我们通常包括语言通用的同义词（飞机-飞机）和领域特定的同义词（书-购买）。
- en: 'The synsets are ready to use, and we''re ready to move onto the spaCy code.
    Let''s go step by step:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: synsets已经准备好使用，我们准备开始使用spaCy代码。让我们一步一步来：
- en: 'First, we construct two doc objects corresponding to the two utterances we
    want to compare, `doc` and `doc2`:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们构建两个与我们要比较的两个句子对应的doc对象，`doc`和`doc2`：
- en: '[PRE35]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Then, we extract the transitive verb and direct object of the first utterance:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们提取第一句话的及物动词和直接宾语：
- en: '[PRE36]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then we do the same for the second utterance:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们对第二个句子做同样的处理：
- en: '[PRE37]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We obtained a synset of the first verb shown. Then, we checked whether the
    second verb list is in this synset, which returns `True`:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们获得了第一个动词的synset。然后，我们检查第二个动词列表是否在这个synset中，这返回`True`：
- en: '[PRE38]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Similarly, we obtain the synset of the first direct object – aircraft. Then
    we check whether the second direct object meal is in this synset, which is obviously
    not true:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样地，我们获得了第一个直接宾语“飞机”的synset。然后我们检查第二个直接宾语“餐”是否在这个synset中，这显然是不正确的：
- en: '[PRE39]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We deduce that the preceding two utterances do not refer to the same intent.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们推断出前两个句子不指代相同的意图。
- en: Synonym lists are great for semantic similarity calculations, and many real-world
    NLP applications benefit from such precompiled lists. Using synonyms is not always
    applicable though. Making a dictionary look up each word in a sentence can become
    inefficient for big synsets. In the next section, we'll introduce a more efficient
    way of calculating semantic similarity with word vectors.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 同义词列表对于语义相似度计算非常有用，许多现实世界的NLP应用都受益于这样的预编译列表。但使用同义词并不总是适用。对于大型同义词集，对句子中的每个词进行字典查找可能变得低效。在下一节中，我们将介绍一种更高效的用词向量计算语义相似度的方法。
- en: Using word vectors to recognize semantic similarity
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用词向量识别语义相似度
- en: In [*Chapter 5*](B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087), *Working with
    Word Vectors and Semantic Similarity*, we already saw that the word vector carries
    semantics, including synonymity information. Synonym lists are handy if you work
    in a very specific domain and the number of synonyms is rather low. Working with
    big synsets can become inefficient at some point because we have to make a dictionary
    look up the verbs and direct objects each time. However, word vectors offer us
    a very convenient and vector-based way to calculate semantic similarity.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第五章*](B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087) *使用词向量和语义相似度* 中，我们已经看到词向量携带语义信息，包括同义信息。在非常具体的领域工作且同义词数量较少时，同义词列表很有用。在某些时候，处理大型的同义词集可能变得低效，因为我们必须每次都进行字典查找以获取动词和直接宾语。然而，词向量为我们提供了一种非常方便且基于向量的计算语义相似度的方法。
- en: 'Let''s go over the code from the previous subsection again. This time, we''ll
    calculate the semantic distance between words with spaCy word vectors. Let''s
    go step by step:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次回顾前一小节中的代码。这次，我们将使用spaCy词向量来计算单词之间的语义距离。让我们一步一步来：
- en: 'First, we construct two `doc` objects that we want to compare:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们构建两个想要比较的`doc`对象：
- en: '[PRE40]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Then we extract the verb and object of the first sentence:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们提取第一句话的动词和宾语：
- en: '[PRE41]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We repeat the same procedure on the second sentence:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对第二句话重复相同的步骤：
- en: '[PRE42]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, we calculate the semantic similarity between two direct objects using
    the word vector-based similarity method of spaCy:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用基于词向量的spaCy相似度方法计算两个直接宾语之间的语义相似度：
- en: '[PRE43]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Finally, we calculate the similarity between the verbs:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们计算动词之间的相似度：
- en: '[PRE44]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The preceding code is different from the previous code. This time, we used the
    token objects directly; no lemma extraction is required. Then we called the `token.similarity(token2)`
    method of spaCy to calculate the semantic distance between the direct objects.
    The resulting score is very low. At this point, we deduce that these two utterances
    do not represent the same intent.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 之前的代码与之前的代码不同。这次，我们直接使用了标记对象；不需要进行词形还原。然后我们调用了spaCy的`token.similarity(token2)`方法来计算直接宾语之间的语义距离。得到的分数非常低。在这个时候，我们推断这两个语句并不代表相同的意图。
- en: This is an easy and efficient way of calculating semantic similarity. We remarked
    in the very first chapter that spaCy provides easy-to-use and efficient tools
    for NLP developers, and now we can see why.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个计算语义相似度的简单且高效的方法。我们在第一章中提到，spaCy为NLP开发者提供了易于使用且高效的工具，现在我们可以看到原因了。
- en: Putting it all together
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有这些放在一起
- en: We already extracted the entities and recognized the intent in several ways.
    We're now ready to put it all together to calculate a semantic representation
    for a user utterance!
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经以几种方式提取了实体并识别了意图。我们现在准备好将所有这些放在一起来计算用户语句的语义表示！
- en: 'We''ll process the example dataset utterance:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将处理示例数据集的语句：
- en: '[PRE45]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We'll hold a dictionary object to hold the result. The result will include the
    entities and the intent.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将使用一个字典对象来存储结果。结果将包括实体和意图。
- en: 'Let''s extract the entities:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们提取实体：
- en: '[PRE46]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'With this information, we can generate the following semantic representation:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于这些信息，我们可以生成以下语义表示：
- en: '[PRE47]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Next, we''ll perform intent recognition to generate a complete semantic parsing:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将执行意图识别以生成完整的语义解析：
- en: '[PRE48]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'After determining the intent, our semantic parse for this utterance now looks
    like this:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在确定意图后，这个语句的语义解析现在看起来是这样的：
- en: '[PRE49]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The final result is that the complete semantic representation of this utterance,
    intent, and entities is extracted. This is a machine-readable and usable output.
    We pass this result to the system component that made the call to the NLP application
    to generate a response action.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果是，提取了这一话语、意图和实体的完整语义表示。这是一个可机器读取和使用的输出。我们将此结果传递给系统组件，该组件调用了NLP应用程序以生成响应动作。
- en: Summary
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Congratulations! You have made it to the end of a very intense chapter!
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经到达了这一非常紧张章节的结尾！
- en: In this chapter, you learned how to generate a complete semantic parse of utterances.
    First, you made a discovery on your dataset to get insights about the dataset
    analytics. Then, you learned to extract entities with two different techniques
    – with spaCy Matcher and by walking on the dependency tree. Next, you learned
    different ways of performing intent recognition by analyzing the sentence structure.
    Finally, you put all the information together to generate a semantic parse.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何生成一个完整的语义解析。首先，你对你自己的数据集进行了发现，以获得关于数据集分析的认识。然后，你学习了使用两种不同的技术提取实体——使用spaCy
    Matcher和通过遍历依存树。接下来，你通过分析句子结构学习了执行意图识别的不同方法。最后，你将所有信息汇总起来以生成语义解析。
- en: In the next chapters, we will shift toward more machine learning methods. The
    next section concerns how to train spaCy NLP pipeline components on your own data.
    Let's move ahead and customize spaCy for ourselves!
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将转向更多机器学习方法。下一节将关注如何在你自己的数据上训练spaCy NLP管道组件。让我们继续前进，并为我们自己定制spaCy！
