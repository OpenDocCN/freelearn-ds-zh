["```py\n $ # Start the Zookeeper \n$ cd $KAFKA_HOME\n$ $KAFKA_HOME/bin/zookeeper-server-start.sh\n $KAFKA_HOME/config/zookeeper.properties\n      [2016-07-30 12:50:15,896] INFO binding to port 0.0.0.0/0.0.0.0:2181\n\t  (org.apache.zookeeper.server.NIOServerCnxnFactory)\n\n\t$ # Start the Kafka broker in a separate terminal window\n\t$ $KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.properties\n      [2016-07-30 12:51:39,206] INFO [Kafka Server 0], started \n\t  (kafka.server.KafkaServer)\n\n\t$ # Create the necessary Kafka topics. This is to be done in a separate terminal window\n\t$ $KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper localhost:2181\n\t--replication-factor 1 --partitions 1 --topic user\n      Created topic \"user\".\n    $ $KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper localhost:2181\n\t--replication-factor 1 --partitions 1 --topic follower\n      Created topic \"follower\".\n\n\t$ $KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper localhost:2181\n\t--replication-factor 1 --partitions 1 --topic message\n      Created topic \"message\".\n\n\t$ # Start producing messages and publish to the topic \"message\"\n\t$ $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list localhost:9092 \n\t--topic message\n\n```", "```py\n $ ./compile.sh\n\t$ ./submit.sh com.packtpub.sfb.DataIngestionApp 1\n\n```", "```py\n /**\n\tThe following program can be compiled and run using SBT\n\tWrapper scripts have been provided with thisThe following script can be run to compile the code\n\t./compile.sh\n\tThe following script can be used to run this application in Spark.\n\tThe second command line argument of value 1 is very important.\n\tThis is to flag the shipping of the kafka jar files to the Spark cluster\n\t./submit.sh com.packtpub.sfb.DataIngestionApp 1\n\t**/\n\tpackage com.packtpub.sfb\n\timport java.util.HashMap\n\timport org.apache.spark.streaming._\n\timport org.apache.spark.sql.{Row, SparkSession}\n\timport org.apache.spark.streaming.kafka._\n\timport org.apache.kafka.clients.producer.{ProducerConfig, KafkaProducer, ProducerRecord}\n\timport org.apache.spark.storage.StorageLevel\n\timport org.apache.log4j.{Level, Logger}\n\tobject DataIngestionApp {\n\tdef main(args: Array[String]) {\n\t// Log level settings\n\tLogSettings.setLogLevels()\n\t//Check point directory for the recovery\n\tval checkPointDir = \"/tmp\"\n    /**\n    * The following function has to be used to have checkpointing and driver recovery\n    * The way it should be used is to use the StreamingContext.getOrCreate with this function and do a start of that\n\t* This function example has been discussed but not used in the chapter covering Spark Streaming. But here it is being used    */\n    def sscCreateFn(): StreamingContext = {\n\t// Variables used for creating the Kafka stream\n\t// Zookeeper host\n\tval zooKeeperQuorum = \"localhost\"\n\t// Kaka message group\n\tval messageGroup = \"sfb-consumer-group\"\n\t// Kafka topic where the programming is listening for the data\n\t// Reader TODO: Here only one topic is included, it can take a comma separated string containing the list of topics. \n\t// Reader TODO: When using multiple topics, use your own logic to extract the right message and persist to its data store\n\tval topics = \"message\"\n\tval numThreads = 1     \n\t// Create the Spark Session, the spark context and the streaming context      \n\tval spark = SparkSession\n\t.builder\n\t.appName(getClass.getSimpleName)\n\t.getOrCreate()\n\tval sc = spark.sparkContext\n\tval ssc = new StreamingContext(sc, Seconds(10))\n\tval topicMap = topics.split(\",\").map((_, numThreads.toInt)).toMap\n\tval messageLines = KafkaUtils.createStream(ssc, zooKeeperQuorum, messageGroup, topicMap).map(_._2)\n\t// This is where the messages are printed to the console. \n\t// TODO - As an exercise to the reader, instead of printing messages to the console, implement your own persistence logic\n\tmessageLines.print()\n\t//Do checkpointing for the recovery\n\tssc.checkpoint(checkPointDir)\n\t// return the Spark Streaming Context\n\tssc\n    }\n\t// Note the function that is defined above for creating the Spark streaming context is being used here to create the Spark streaming context. \n\tval ssc = StreamingContext.getOrCreate(checkPointDir, sscCreateFn)\n\t// Start the streaming\n    ssc.start()\n\t// Wait till the application is terminated               \n    ssc.awaitTermination() \n\t}\n\t}\n\tobject LogSettings {\n\t/** \n\tNecessary log4j logging level settings are done \n\t*/\n\tdef setLogLevels() {\n    val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements\n    if (!log4jInitialized) {\n\t// This is to make sure that the console is clean from other INFO messages printed by Spark\n\tLogger.getRootLogger.setLevel(Level.INFO)\n    }\n\t}\n\t}\n\n```", "```py\n $ ./submitPy.sh DataIngestionApp.py 1\n\n```", "```py\n # The following script can be used to run this application in Spark\n# ./submitPy.sh DataIngestionApp.py 1\n  from __future__ import print_function\n  import sys\n  from pyspark import SparkContext\n  from pyspark.streaming import StreamingContext\n  from pyspark.streaming.kafka import KafkaUtils\n  if __name__ == \"__main__\":\n# Create the Spark context\n  sc = SparkContext(appName=\"DataIngestionApp\")\n  log4j = sc._jvm.org.apache.log4j\n  log4j.LogManager.getRootLogger().setLevel(log4j.Level.WARN)\n# Create the Spark Streaming Context with 10 seconds batch interval\n  ssc = StreamingContext(sc, 10)\n# Check point directory setting\n  ssc.checkpoint(\"\\tmp\")\n# Zookeeper host\n  zooKeeperQuorum=\"localhost\"\n# Kaka message group\n  messageGroup=\"sfb-consumer-group\"\n# Kafka topic where the programming is listening for the data\n# Reader TODO: Here only one topic is included, it can take a comma separated  string containing the list of topics. \n# Reader TODO: When using multiple topics, use your own logic to extract the right message and persist to its data store\ntopics = \"message\"\nnumThreads = 1    \n# Create a Kafka DStream\nkafkaStream = KafkaUtils.createStream(ssc, zooKeeperQuorum, messageGroup, {topics: numThreads})\nmessageLines = kafkaStream.map(lambda x: x[1])\n# This is where the messages are printed to the console. Instead of this, implement your own persistence logic\nmessageLines.pprint()\n# Start the streaming\nssc.start()\n# Wait till the application is terminated   \nssc.awaitTermination() \n\n```", "```py\n //TODO: Change the following directory to point to your data directory\nscala> val dataDir = \"/Users/RajT/Documents/Writing/SparkForBeginners/To-PACKTPUB/Contents/B05289-09-DesigningSparkApplications/Code/Data/\"\n      dataDir: String = /Users/RajT/Documents/Writing/SparkForBeginners/To-PACKTPUB/Contents/B05289-09-DesigningSparkApplications/Code/Data/\n    scala> //Define the case classes in Scala for the entities\n\tscala> case class User(Id: Long, UserName: String, FirstName: String, LastName: String, EMail: String, AlternateEmail: String, Phone: String)\n      defined class User\n    scala> case class Follow(Follower: String, Followed: String)\n      defined class Follow\n    scala> case class Message(UserName: String, MessageId: Long, ShortMessage: String, Timestamp: Long)\n      defined class Message\n    scala> case class MessageToUsers(FromUserName: String, ToUserName: String, MessageId: Long, ShortMessage: String, Timestamp: Long)\n      defined class MessageToUsers\n    scala> case class TaggedMessage(HashTag: String, UserName: String, MessageId: Long, ShortMessage: String, Timestamp: Long)\n      defined class TaggedMessage\n    scala> //Define the utility functions that are to be passed in the applications\n\tscala> def toUser =  (line: Seq[String]) => User(line(0).toLong, line(1), line(2),line(3), line(4), line(5), line(6))\n      toUser: Seq[String] => User\n    scala> def toFollow =  (line: Seq[String]) => Follow(line(0), line(1))\n      toFollow: Seq[String] => Follow\n    scala> def toMessage =  (line: Seq[String]) => Message(line(0), line(1).toLong, line(2), line(3).toLong)\n      toMessage: Seq[String] => Message\n    scala> //Load the user data into a Dataset\n\tscala> val userDataDS = sc.textFile(dataDir + \"user.txt\").map(_.split(\"\\\\|\")).map(toUser(_)).toDS()\n      userDataDS: org.apache.spark.sql.Dataset[User] = [Id: bigint, UserName: string ... 5 more fields]\n    scala> //Convert the Dataset into data frame\n\tscala> val userDataDF = userDataDS.toDF()\n      userDataDF: org.apache.spark.sql.DataFrame = [Id: bigint, UserName: string ... 5 more fields]\n    scala> userDataDF.createOrReplaceTempView(\"user\")\n\tscala> userDataDF.show()\n      +---+--------+---------+--------+--------------------+----------------+--------------+\n\n      | Id|UserName|FirstName|LastName|               EMail|  AlternateEmail|         Phone|\n\n      +---+--------+---------+--------+--------------------+----------------+--------------+\n\n      |  1| mthomas|     Mark|  Thomas| mthomas@example.com|mt12@example.com|+4411860297701|\n\n      |  2|mithomas|  Michael|  Thomas|mithomas@example.com| mit@example.com|+4411860297702|\n\n      |  3|  mtwain|     Mark|   Twain|  mtwain@example.com| mtw@example.com|+4411860297703|\n\n      |  4|  thardy|   Thomas|   Hardy|  thardy@example.com|  th@example.com|+4411860297704|\n\n      |  5| wbryson|  William|  Bryson| wbryson@example.com|  bb@example.com|+4411860297705|\n\n      |  6|   wbrad|  William|Bradford|   wbrad@example.com|  wb@example.com|+4411860297706|\n\n      |  7| eharris|       Ed|  Harris| eharris@example.com|  eh@example.com|+4411860297707|\n\n      |  8|   tcook|   Thomas|    Cook|   tcook@example.com|  tk@example.com|+4411860297708|\n\n      |  9| arobert|     Adam|  Robert| arobert@example.com|  ar@example.com|+4411860297709|\n\n      | 10|  jjames|    Jacob|   James|  jjames@example.com|  jj@example.com|+4411860297710|\n\n      +---+--------+---------+--------+--------------------+----------------+--------------+\n    scala> //Load the follower data into an Dataset\n\tscala> val followerDataDS = sc.textFile(dataDir + \"follower.txt\").map(_.split(\"\\\\|\")).map(toFollow(_)).toDS()\n      followerDataDS: org.apache.spark.sql.Dataset[Follow] = [Follower: string, Followed: string]\n    scala> //Convert the Dataset into data frame\n\tscala> val followerDataDF = followerDataDS.toDF()\n      followerDataDF: org.apache.spark.sql.DataFrame = [Follower: string, Followed: string]\n    scala> followerDataDF.createOrReplaceTempView(\"follow\")\n\tscala> followerDataDF.show()\n      +--------+--------+\n\n      |Follower|Followed|\n\n      +--------+--------+\n\n      | mthomas|mithomas|\n\n      | mthomas|  mtwain|\n\n      |  thardy| wbryson|\n\n      |   wbrad| wbryson|\n\n      | eharris| mthomas|\n\n      | eharris|   tcook|\n\n      | arobert|  jjames|\n\n      +--------+--------+\n    scala> //Load the message data into an Dataset\n\tscala> val messageDataDS = sc.textFile(dataDir + \"message.txt\").map(_.split(\"\\\\|\")).map(toMessage(_)).toDS()\n      messageDataDS: org.apache.spark.sql.Dataset[Message] = [UserName: string, MessageId: bigint ... 2 more fields]\n    scala> //Convert the Dataset into data frame\n\tscala> val messageDataDF = messageDataDS.toDF()\n      messageDataDF: org.apache.spark.sql.DataFrame = [UserName: string, MessageId: bigint ... 2 more fields]\n    scala> messageDataDF.createOrReplaceTempView(\"message\")\n\tscala> messageDataDF.show()\n      +--------+---------+--------------------+----------+\n\n      |UserName|MessageId|        ShortMessage| Timestamp|\n\n      +--------+---------+--------------------+----------+\n\n      | mthomas|        1|@mithomas Your po...|1459009608|\n\n      | mthomas|        2|Feeling awesome t...|1459010608|\n\n      |  mtwain|        3|My namesake in th...|1459010776|\n\n      |  mtwain|        4|Started the day w...|1459011016|\n\n      |  thardy|        5|It is just spring...|1459011199|\n\n      | wbryson|        6|Some days are rea...|1459011256|\n\n      |   wbrad|        7|@wbryson Stuff ha...|1459011333|\n\n      | eharris|        8|Anybody knows goo...|1459011426|\n\n      |   tcook|        9|Stock market is p...|1459011483|\n\n      |   tcook|       10|Dont do day tradi...|1459011539|\n\n      |   tcook|       11|I have never hear...|1459011622|\n\n      |   wbrad|       12|#Barcelona has pl...|1459157132|\n\n      |  mtwain|       13|@wbryson It is go...|1459164906|\n\n      +--------+---------+--------------------+----------+ \n\n```", "```py\n scala> //Create the purposed view of the message to users\n\tscala> val messagetoUsersDS = messageDataDS.filter(_.ShortMessage.contains(\"@\")).map(message => (message.ShortMessage.split(\" \").filter(_.contains(\"@\")).mkString(\" \").substring(1), message)).map(msgTuple => MessageToUsers(msgTuple._2.UserName, msgTuple._1, msgTuple._2.MessageId, msgTuple._2.ShortMessage, msgTuple._2.Timestamp))\n      messagetoUsersDS: org.apache.spark.sql.Dataset[MessageToUsers] = [FromUserName: string, ToUserName: string ... 3 more fields]\n\n\tscala> //Convert the Dataset into data frame\n\tscala> val messagetoUsersDF = messagetoUsersDS.toDF()\n      messagetoUsersDF: org.apache.spark.sql.DataFrame = [FromUserName: string, ToUserName: string ... 3 more fields]\n\n\tscala> messagetoUsersDF.createOrReplaceTempView(\"messageToUsers\")\n\tscala> messagetoUsersDF.show()\n      +------------+----------+---------+--------------------+----------+\n\n      |FromUserName|ToUserName|MessageId|        ShortMessage| Timestamp|\n\n      +------------+----------+---------+--------------------+----------+\n\n      |     mthomas|  mithomas|        1|@mithomas Your po...|1459009608|\n\n      |       wbrad|   wbryson|        7|@wbryson Stuff ha...|1459011333|\n\n      |      mtwain|   wbryson|       13|@wbryson It is go...|1459164906|\n\n      +------------+----------+---------+--------------------+----------+\n    scala> //Create the purposed view of tagged messages \n\tscala> val taggedMessageDS = messageDataDS.filter(_.ShortMessage.contains(\"#\")).map(message => (message.ShortMessage.split(\" \").filter(_.contains(\"#\")).mkString(\" \"), message)).map(msgTuple => TaggedMessage(msgTuple._1, msgTuple._2.UserName, msgTuple._2.MessageId, msgTuple._2.ShortMessage, msgTuple._2.Timestamp))\n      taggedMessageDS: org.apache.spark.sql.Dataset[TaggedMessage] = [HashTag: string, UserName: string ... 3 more fields]\n\n\tscala> //Convert the Dataset into data frame\n\tscala> val taggedMessageDF = taggedMessageDS.toDF()\n      taggedMessageDF: org.apache.spark.sql.DataFrame = [HashTag: string, UserName: string ... 3 more fields]\n\n\tscala> taggedMessageDF.createOrReplaceTempView(\"taggedMessages\")\n\tscala> taggedMessageDF.show()\n      +----------+--------+---------+--------------------+----------+\n\n      |   HashTag|UserName|MessageId|        ShortMessage| Timestamp|\n\n      +----------+--------+---------+--------------------+----------+\n\n      |#Barcelona| eharris|        8|Anybody knows goo...|1459011426|\n\n      |#Barcelona|   wbrad|       12|#Barcelona has pl...|1459157132|\n\n      +----------+--------+---------+--------------------+----------+\n\n\tscala> //The following are the queries given in the use cases\n\tscala> //Find the messages that are grouped by a given hash tag\n\tscala> val byHashTag = spark.sql(\"SELECT a.UserName, b.FirstName, b.LastName, a.MessageId, a.ShortMessage, a.Timestamp FROM taggedMessages a, user b WHERE a.UserName = b.UserName AND HashTag = '#Barcelona' ORDER BY a.Timestamp\")\n      byHashTag: org.apache.spark.sql.DataFrame = [UserName: string, FirstName: string ... 4 more fields]\n\n\tscala> byHashTag.show()\n      +--------+---------+--------+---------+--------------------+----------+\n\n      |UserName|FirstName|LastName|MessageId|        ShortMessage| Timestamp|\n\n      +--------+---------+--------+---------+--------------------+----------+\n\n      | eharris|       Ed|  Harris|        8|Anybody knows goo...|1459011426|\n\n      |   wbrad|  William|Bradford|       12|#Barcelona has pl...|1459157132|\n\n      +--------+---------+--------+---------+--------------------+----------+\n\n\tscala> //Find the messages that are addressed to a given user\n\tscala> val byToUser = spark.sql(\"SELECT FromUserName, ToUserName, MessageId, ShortMessage, Timestamp FROM messageToUsers WHERE ToUserName = 'wbryson' ORDER BY Timestamp\")\n      byToUser: org.apache.spark.sql.DataFrame = [FromUserName: string, ToUserName: string ... 3 more fields]\n\n\tscala> byToUser.show()\n      +------------+----------+---------+--------------------+----------+\n\n      |FromUserName|ToUserName|MessageId|        ShortMessage| Timestamp|\n\n      +------------+----------+---------+--------------------+----------+\n\n      |       wbrad|   wbryson|        7|@wbryson Stuff ha...|1459011333|\n\n      |      mtwain|   wbryson|       13|@wbryson It is go...|1459164906|\n\n      +------------+----------+---------+--------------------+----------+\n    scala> //Find the followers of a given user\n\tscala> val followers = spark.sql(\"SELECT b.FirstName as FollowerFirstName, b.LastName as FollowerLastName, a.Followed FROM follow a, user b WHERE a.Follower = b.UserName AND a.Followed = 'wbryson'\")\n      followers: org.apache.spark.sql.DataFrame = [FollowerFirstName: string, FollowerLastName: string ... 1 more field]\n    scala> followers.show()\n      +-----------------+----------------+--------+\n\n      |FollowerFirstName|FollowerLastName|Followed|\n\n      +-----------------+----------------+--------+\n\n      |          William|        Bradford| wbryson|\n\n      |           Thomas|           Hardy| wbryson|\n\n      +-----------------+----------------+--------+\n\n\tscala> //Find the followedUsers of a given user\n\tscala> val followedUsers = spark.sql(\"SELECT b.FirstName as FollowedFirstName, b.LastName as FollowedLastName, a.Follower FROM follow a, user b WHERE a.Followed = b.UserName AND a.Follower = 'eharris'\")\n      followedUsers: org.apache.spark.sql.DataFrame = [FollowedFirstName: string, FollowedLastName: string ... 1 more field]\n    scala> followedUsers.show()\n      +-----------------+----------------+--------+\n\n      |FollowedFirstName|FollowedLastName|Follower|\n\n      +-----------------+----------------+--------+\n\n      |           Thomas|            Cook| eharris|\n\n      |             Mark|          Thomas| eharris|\n\n      +-----------------+----------------+--------+ \n\n```", "```py\n >>> from pyspark.sql import Row\n\t>>> #TODO: Change the following directory to point to your data directory\n\t>>> dataDir = \"/Users/RajT/Documents/Writing/SparkForBeginners/To-PACKTPUB/Contents/B05289-09-DesigningSparkApplications/Code/Data/\"\n\t>>> #Load the user data into an RDD\n\t>>> userDataRDD = sc.textFile(dataDir + \"user.txt\").map(lambda line: line.split(\"|\")).map(lambda p: Row(Id=int(p[0]), UserName=p[1], FirstName=p[2], LastName=p[3], EMail=p[4], AlternateEmail=p[5], Phone=p[6]))\n\t>>> #Convert the RDD into data frame\n\t>>> userDataDF = userDataRDD.toDF()\n\t>>> userDataDF.createOrReplaceTempView(\"user\")\n\t>>> userDataDF.show()\n      +----------------+--------------------+---------+---+--------+--------------+--------+\n\n      |  AlternateEmail|               EMail|FirstName| Id|LastName|         Phone|UserName|\n\n      +----------------+--------------------+---------+---+--------+--------------+--------+\n\n      |mt12@example.com| mthomas@example.com|     Mark|  1|  Thomas|+4411860297701| mthomas|\n\n      | mit@example.com|mithomas@example.com|  Michael|  2|  Thomas|+4411860297702|mithomas|\n\n      | mtw@example.com|  mtwain@example.com|     Mark|  3|   Twain|+4411860297703|  mtwain|\n\n      |  th@example.com|  thardy@example.com|   Thomas|  4|   Hardy|+4411860297704|  thardy|\n\n      |  bb@example.com| wbryson@example.com|  William|  5|  Bryson|+4411860297705| wbryson|\n\n      |  wb@example.com|   wbrad@example.com|  William|  6|Bradford|+4411860297706|   wbrad|\n\n      |  eh@example.com| eharris@example.com|       Ed|  7|  Harris|+4411860297707| eharris|\n\n      |  tk@example.com|   tcook@example.com|   Thomas|  8|    Cook|+4411860297708|   tcook|\n\n      |  ar@example.com| arobert@example.com|     Adam|  9|  Robert|+4411860297709| arobert|\n\n      |  jj@example.com|  jjames@example.com|    Jacob| 10|   James|+4411860297710|  jjames|\n\n      +----------------+--------------------+---------+---+--------+--------------+--------+\n\n\t>>> #Load the follower data into an RDD\n\t>>> followerDataRDD = sc.textFile(dataDir + \"follower.txt\").map(lambda line: line.split(\"|\")).map(lambda p: Row(Follower=p[0], Followed=p[1]))\n\t>>> #Convert the RDD into data frame\n\t>>> followerDataDF = followerDataRDD.toDF()\n\t>>> followerDataDF.createOrReplaceTempView(\"follow\")\n\t>>> followerDataDF.show()\n      +--------+--------+\n\n      |Followed|Follower|\n\n      +--------+--------+\n\n      |mithomas| mthomas|\n\n      |  mtwain| mthomas|\n\n      | wbryson|  thardy|\n\n      | wbryson|   wbrad|\n\n      | mthomas| eharris|\n\n      |   tcook| eharris|\n\n      |  jjames| arobert|\n\n      +--------+--------+\n\n\t>>> #Load the message data into an RDD\n\t>>> messageDataRDD = sc.textFile(dataDir + \"message.txt\").map(lambda line: line.split(\"|\")).map(lambda p: Row(UserName=p[0], MessageId=int(p[1]), ShortMessage=p[2], Timestamp=int(p[3])))\n\t>>> #Convert the RDD into data frame\n\t>>> messageDataDF = messageDataRDD.toDF()\n\t>>> messageDataDF.createOrReplaceTempView(\"message\")\n\t>>> messageDataDF.show()\n      +---------+--------------------+----------+--------+\n\n      |MessageId|        ShortMessage| Timestamp|UserName|\n\n      +---------+--------------------+----------+--------+\n\n      |        1|@mithomas Your po...|1459009608| mthomas|\n\n      |        2|Feeling awesome t...|1459010608| mthomas|\n\n      |        3|My namesake in th...|1459010776|  mtwain|\n\n      |        4|Started the day w...|1459011016|  mtwain|\n\n      |        5|It is just spring...|1459011199|  thardy|\n\n      |        6|Some days are rea...|1459011256| wbryson|\n\n      |        7|@wbryson Stuff ha...|1459011333|   wbrad|\n\n      |        8|Anybody knows goo...|1459011426| eharris|\n\n      |        9|Stock market is p...|1459011483|   tcook|\n\n      |       10|Dont do day tradi...|1459011539|   tcook|\n\n      |       11|I have never hear...|1459011622|   tcook|\n\n      |       12|#Barcelona has pl...|1459157132|   wbrad|\n\n      |       13|@wbryson It is go...|1459164906|  mtwain|\n\n      +---------+--------------------+----------+--------+ \n\n```", "```py\n >>> #Create the purposed view of the message to users\n\t>>> messagetoUsersRDD = messageDataRDD.filter(lambda message: \"@\" in message.ShortMessage).map(lambda message : (message, \" \".join(filter(lambda s: s[0] == '@', message.ShortMessage.split(\" \"))))).map(lambda msgTuple: Row(FromUserName=msgTuple[0].UserName, ToUserName=msgTuple[1][1:], MessageId=msgTuple[0].MessageId, ShortMessage=msgTuple[0].ShortMessage, Timestamp=msgTuple[0].Timestamp))\n\t>>> #Convert the RDD into data frame\n\t>>> messagetoUsersDF = messagetoUsersRDD.toDF()\n\t>>> messagetoUsersDF.createOrReplaceTempView(\"messageToUsers\")\n\t>>> messagetoUsersDF.show()\n      +------------+---------+--------------------+----------+----------+\n\n      |FromUserName|MessageId|        ShortMessage| Timestamp|ToUserName|\n\n      +------------+---------+--------------------+----------+----------+\n\n      |     mthomas|        1|@mithomas Your po...|1459009608|  mithomas|\n\n      |       wbrad|        7|@wbryson Stuff ha...|1459011333|   wbryson|\n\n      |      mtwain|       13|@wbryson It is go...|1459164906|   wbryson|\n\n      +------------+---------+--------------------+----------+----------+\n\n\t>>> #Create the purposed view of tagged messages \n\t>>> taggedMessageRDD = messageDataRDD.filter(lambda message: \"#\" in message.ShortMessage).map(lambda message : (message, \" \".join(filter(lambda s: s[0] == '#', message.ShortMessage.split(\" \"))))).map(lambda msgTuple: Row(HashTag=msgTuple[1], UserName=msgTuple[0].UserName, MessageId=msgTuple[0].MessageId, ShortMessage=msgTuple[0].ShortMessage, Timestamp=msgTuple[0].Timestamp))\n\t>>> #Convert the RDD into data frame\n\t>>> taggedMessageDF = taggedMessageRDD.toDF()\n\t>>> taggedMessageDF.createOrReplaceTempView(\"taggedMessages\")\n\t>>> taggedMessageDF.show()\n      +----------+---------+--------------------+----------+--------+\n\n      |   HashTag|MessageId|        ShortMessage| Timestamp|UserName|\n\n      +----------+---------+--------------------+----------+--------+\n\n      |#Barcelona|        8|Anybody knows goo...|1459011426| eharris|\n\n      |#Barcelona|       12|#Barcelona has pl...|1459157132|   wbrad|\n\n      +----------+---------+--------------------+----------+--------+\n\n\t>>> #The following are the queries given in the use cases\n\t>>> #Find the messages that are grouped by a given hash tag\n\t>>> byHashTag = spark.sql(\"SELECT a.UserName, b.FirstName, b.LastName, a.MessageId, a.ShortMessage, a.Timestamp FROM taggedMessages a, user b WHERE a.UserName = b.UserName AND HashTag = '#Barcelona' ORDER BY a.Timestamp\")\n\t>>> byHashTag.show()\n      +--------+---------+--------+---------+--------------------+----------+\n\n      |UserName|FirstName|LastName|MessageId|        ShortMessage| Timestamp|\n\n      +--------+---------+--------+---------+--------------------+----------+\n\n      | eharris|       Ed|  Harris|        8|Anybody knows goo...|1459011426|\n\n      |   wbrad|  William|Bradford|       12|#Barcelona has pl...|1459157132|\n\n      +--------+---------+--------+---------+--------------------+----------+\n\n\t>>> #Find the messages that are addressed to a given user\n\t>>> byToUser = spark.sql(\"SELECT FromUserName, ToUserName, MessageId, ShortMessage, Timestamp FROM messageToUsers WHERE ToUserName = 'wbryson' ORDER BY Timestamp\")\n\t>>> byToUser.show()\n      +------------+----------+---------+--------------------+----------+\n\n      |FromUserName|ToUserName|MessageId|        ShortMessage| Timestamp|\n\n      +------------+----------+---------+--------------------+----------+\n\n      |       wbrad|   wbryson|        7|@wbryson Stuff ha...|1459011333|\n\n      |      mtwain|   wbryson|       13|@wbryson It is go...|1459164906|\n\n      +------------+----------+---------+--------------------+----------+\n\n\t>>> #Find the followers of a given user\n\t>>> followers = spark.sql(\"SELECT b.FirstName as FollowerFirstName, b.LastName as FollowerLastName, a.Followed FROM follow a, user b WHERE a.Follower = b.UserName AND a.Followed = 'wbryson'\")>>> followers.show()\n      +-----------------+----------------+--------+\n\n      |FollowerFirstName|FollowerLastName|Followed|\n\n      +-----------------+----------------+--------+\n\n      |          William|        Bradford| wbryson|\n\n      |           Thomas|           Hardy| wbryson|\n\n      +-----------------+----------------+--------+\n\n\t>>> #Find the followed users of a given user\n\t>>> followedUsers = spark.sql(\"SELECT b.FirstName as FollowedFirstName, b.LastName as FollowedLastName, a.Follower FROM follow a, user b WHERE a.Followed = b.UserName AND a.Follower = 'eharris'\")\n\t>>> followedUsers.show()\n      +-----------------+----------------+--------+\n\n      |FollowedFirstName|FollowedLastName|Follower|\n\n      +-----------------+----------------+--------+\n\n      |           Thomas|            Cook| eharris|\n\n      |             Mark|          Thomas| eharris| \n +-----------------+----------------+--------+ \n\n```", "```py\n scala> import org.apache.spark.rdd.RDD\n    import org.apache.spark.rdd.RDD    \n\tscala> import org.apache.spark.graphx._\n    import org.apache.spark.graphx._    \n\tscala> //TODO: Change the following directory to point to your data directory\n\tscala> val dataDir = \"/Users/RajT/Documents/Writing/SparkForBeginners/To-PACKTPUB/Contents/B05289-09-DesigningSparkApplications/Code/Data/\"\ndataDir: String = /Users/RajT/Documents/Writing/SparkForBeginners/To-PACKTPUB/Contents/B05289-09-DesigningSparkApplications/Code/Data/\n\n\tscala> //Define the case classes in Scala for the entities\n\tscala> case class User(Id: Long, UserName: String, FirstName: String, LastName: String, EMail: String, AlternateEmail: String, Phone: String)\n      defined class User\n\n\tscala> case class Follow(Follower: String, Followed: String)\n      defined class Follow\n\n\tscala> case class ConnectedUser(CCId: Long, UserName: String)\n      defined class ConnectedUser\n\n\tscala> //Define the utility functions that are to be passed in the applications\n\tscala> def toUser =  (line: Seq[String]) => User(line(0).toLong, line(1), line(2),line(3), line(4), line(5), line(6))\n      toUser: Seq[String] => User\n\n\tscala> def toFollow =  (line: Seq[String]) => Follow(line(0), line(1))\n      toFollow: Seq[String] => Follow\n\n\tscala> //Load the user data into an RDD\n\tscala> val userDataRDD = sc.textFile(dataDir + \"user.txt\").map(_.split(\"\\\\|\")).map(toUser(_))\nuserDataRDD: org.apache.spark.rdd.RDD[User] = MapPartitionsRDD[160] at map at <console>:34\n\n\tscala> //Convert the RDD into data frame\n\tscala> val userDataDF = userDataRDD.toDF()\nuserDataDF: org.apache.spark.sql.DataFrame = [Id: bigint, UserName: string ... 5 more fields]\n\n\tscala> userDataDF.createOrReplaceTempView(\"user\")\n\tscala> userDataDF.show()\n      +---+--------+---------+--------+-----------+----------------+--------------+\n\nId|UserName|FirstName|LastName| EMail|  AlternateEmail|   Phone|\n\n      +---+--------+---------+--------+----------+-------------+--------------+\n\n|  1| mthomas|     Mark|  Thomas| mthomas@example.com|mt12@example.com|\n+4411860297701|\n\n|  2|mithomas|  Michael|  Thomas|mithomas@example.com| mit@example.com|\n+4411860297702|\n\n|  3|  mtwain|     Mark|   Twain|  mtwain@example.com| mtw@example.com|\n+4411860297703|\n\n|  4|  thardy|   Thomas|   Hardy|  thardy@example.com|  th@example.com|\n+4411860297704|\n\n|  5| wbryson|  William|  Bryson| wbryson@example.com|  bb@example.com|\n+4411860297705|\n\n|  6|   wbrad|  William|Bradford|   wbrad@example.com|  wb@example.com|\n+4411860297706|\n\n|  7| eharris|       Ed|  Harris| eharris@example.com|  eh@example.com|\n+4411860297707|\n\n|  8|   tcook|   Thomas|    Cook|   tcook@example.com|  tk@example.com|\n+4411860297708|\n\n|  9| arobert|     Adam|  Robert| arobert@example.com|  ar@example.com|\n+4411860297709|\n\n| 10|  jjames|    Jacob|   James|  jjames@example.com|  jj@example.com|\n+4411860297710|    \n      +---+--------+---------+--------+-------------+--------------+--------------+\n\n\tscala> //Load the follower data into an RDD\n\tscala> val followerDataRDD = sc.textFile(dataDir + \"follower.txt\").map(_.split(\"\\\\|\")).map(toFollow(_))\nfollowerDataRDD: org.apache.spark.rdd.RDD[Follow] = MapPartitionsRDD[168] at map at <console>:34\n\n\tscala> //Convert the RDD into data frame\n\tscala> val followerDataDF = followerDataRDD.toDF()\nfollowerDataDF: org.apache.spark.sql.DataFrame = [Follower: string, Followed: string]\n\n\tscala> followerDataDF.createOrReplaceTempView(\"follow\")\n\tscala> followerDataDF.show()\n      +--------+--------+\n\n      |Follower|Followed|\n\n      +--------+--------+\n\n      | mthomas|mithomas|\n\n      | mthomas|  mtwain|\n\n      |  thardy| wbryson|\n\n      |   wbrad| wbryson|\n\n      | eharris| mthomas|\n\n      | eharris|   tcook|\n\n      | arobert|  jjames|\n\n      +--------+--------+\n\n\tscala> //By joining with the follower and followee users with the master user data frame for extracting the unique ids\n\tscala> val fullFollowerDetails = spark.sql(\"SELECT b.Id as FollowerId, c.Id as FollowedId, a.Follower, a.Followed FROM follow a, user b, user c WHERE a.Follower = b.UserName AND a.Followed = c.UserName\")\nfullFollowerDetails: org.apache.spark.sql.DataFrame = [FollowerId: bigint, FollowedId: bigint ... 2 more fields]\n\n\tscala> fullFollowerDetails.show()\n      +----------+----------+--------+--------+\n\n      |FollowerId|FollowedId|Follower|Followed|\n\n      +----------+----------+--------+--------+\n\n      |         9|        10| arobert|  jjames|\n\n      |         1|         2| mthomas|mithomas|\n\n      |         7|         8| eharris|   tcook|\n\n      |         7|         1| eharris| mthomas|\n\n      |         1|         3| mthomas|  mtwain|\n\n      |         6|         5|   wbrad| wbryson|\n\n      |         4|         5|  thardy| wbryson|\n\n      +----------+----------+--------+--------+\n\n\tscala> //Create the vertices of the connections graph\n\tscala> val userVertices: RDD[(Long, String)] = userDataRDD.map(user => (user.Id, user.UserName))\nuserVertices: org.apache.spark.rdd.RDD[(Long, String)] = MapPartitionsRDD[194] at map at <console>:36\n\n\tscala> userVertices.foreach(println)\n      (6,wbrad)\n\n      (7,eharris)\n\n      (8,tcook)\n\n      (9,arobert)\n\n      (10,jjames)\n\n      (1,mthomas)\n\n      (2,mithomas)\n\n      (3,mtwain)\n\n      (4,thardy)\n\n      (5,wbryson)\n\n\tscala> //Create the edges of the connections graph \n\tscala> val connections: RDD[Edge[String]] = fullFollowerDetails.rdd.map(conn => Edge(conn.getAs[Long](\"FollowerId\"), conn.getAs[Long](\"FollowedId\"), \"Follows\"))\n      connections: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] = MapPartitionsRDD[217] at map at <console>:29\n\n\tscala> connections.foreach(println)\n\tEdge(9,10,Follows)\n\tEdge(7,8,Follows)\n\tEdge(1,2,Follows)\n\tEdge(7,1,Follows)\n\tEdge(1,3,Follows)\n\tEdge(6,5,Follows)\n\tEdge(4,5,Follows)\n\tscala> //Create the graph using the vertices and the edges\n\tscala> val connectionGraph = Graph(userVertices, connections)\n      connectionGraph: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@3c207acd \n\n```", "```py\n scala> //Calculate the connected users\n\tscala> val cc = connectionGraph.connectedComponents()\n      cc: org.apache.spark.graphx.Graph[org.apache.spark.graphx.VertexId,String] = org.apache.spark.graphx.impl.GraphImpl@73f0bd11\n\n\tscala> // Extract the triplets of the connected users\n\tscala> val ccTriplets = cc.triplets\n      ccTriplets: org.apache.spark.rdd.RDD[org.apache.spark.graphx.EdgeTriplet[org.apache.spark.graphx.VertexId,String]] = MapPartitionsRDD[285] at mapPartitions at GraphImpl.scala:48\n\n\tscala> // Print the structure of the triplets\n\tscala> ccTriplets.foreach(println)\n      ((9,9),(10,9),Follows)\n\n      ((1,1),(2,1),Follows)\n\n      ((7,1),(8,1),Follows)\n\n      ((7,1),(1,1),Follows)\n\n      ((1,1),(3,1),Follows)\n\n      ((4,4),(5,4),Follows) \n ((6,4),(5,4),Follows) \n\n```", "```py\n scala> //Print the vertex numbers and the corresponding connected component id. The connected component id is generated by the system and it is to be taken only as a unique identifier for the connected component\n   scala> val ccProperties = ccTriplets.map(triplet => \"Vertex \" + triplet.srcId + \" and \" + triplet.dstId + \" are part of the CC with id \" + triplet.srcAttr)\n      ccProperties: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[288] at map at <console>:48\n\n\tscala> ccProperties.foreach(println)\n      Vertex 9 and 10 are part of the CC with id 9\n\n      Vertex 1 and 2 are part of the CC with id 1\n\n      Vertex 7 and 8 are part of the CC with id 1\n\n      Vertex 7 and 1 are part of the CC with id 1\n\n      Vertex 1 and 3 are part of the CC with id 1\n\n      Vertex 4 and 5 are part of the CC with id 4\n\n      Vertex 6 and 5 are part of the CC with id 4\n\n\tscala> //Find the users in the source vertex with their CC id\n\tscala> val srcUsersAndTheirCC = ccTriplets.map(triplet => (triplet.srcId, triplet.srcAttr))\n      srcUsersAndTheirCC: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = MapPartitionsRDD[289] at map at <console>:48\n\n\tscala> //Find the users in the destination vertex with their CC id\n\tscala> val dstUsersAndTheirCC = ccTriplets.map(triplet => (triplet.dstId, triplet.dstAttr))\n      dstUsersAndTheirCC: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = MapPartitionsRDD[290] at map at <console>:48\n\n\tscala> //Find the union\n\tscala> val usersAndTheirCC = srcUsersAndTheirCC.union(dstUsersAndTheirCC)\n      usersAndTheirCC: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = UnionRDD[291] at union at <console>:52\n\n\tscala> //Join with the name of the users\n\tscala> //Convert the RDD to DataFrame\n\tscala> val usersAndTheirCCWithName = usersAndTheirCC.join(userVertices).map{case (userId,(ccId,userName)) => (ccId, userName)}.distinct.sortByKey().map{case (ccId,userName) => ConnectedUser(ccId, userName)}.toDF()\n      usersAndTheirCCWithName: org.apache.spark.sql.DataFrame = [CCId: bigint, UserName: string]\n\n\tscala> usersAndTheirCCWithName.createOrReplaceTempView(\"connecteduser\")\n\tscala> val usersAndTheirCCWithDetails = spark.sql(\"SELECT a.CCId, a.UserName, b.FirstName, b.LastName FROM connecteduser a, user b WHERE a.UserName = b.UserName ORDER BY CCId\")\n      usersAndTheirCCWithDetails: org.apache.spark.sql.DataFrame = [CCId: bigint, UserName: string ... 2 more fields]\n\n\tscala> //Print the usernames with their CC component id. If two users share the same CC id, then they are connected\n\tscala> usersAndTheirCCWithDetails.show()\n      +----+--------+---------+--------+\n\n      |CCId|UserName|FirstName|LastName|\n\n      +----+--------+---------+--------+\n\n      |   1|mithomas|  Michael|  Thomas|\n\n      |   1|  mtwain|     Mark|   Twain|\n\n      |   1|   tcook|   Thomas|    Cook|\n\n      |   1| eharris|       Ed|  Harris|\n\n      |   1| mthomas|     Mark|  Thomas|\n\n      |   4|   wbrad|  William|Bradford|\n\n      |   4| wbryson|  William|  Bryson|\n\n      |   4|  thardy|   Thomas|   Hardy|\n\n      |   9|  jjames|    Jacob|   James|\n\n      |   9| arobert|     Adam|  Robert| \n +----+--------+---------+--------+ \n\n```"]