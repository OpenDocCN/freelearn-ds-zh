["```py\n# Generate a Random Time Series\n# Set seed to make results reproducible\nset.seed(123)\n# Generate Random Points using a gaussian distribution with mean 0 and sd = 1\nn <- 25\nx <- rnorm(n)\nhead(x)\n[1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499\n```", "```py\n# Make x a ts object\nts_obj <- ts(x)\n```", "```py\nclass(ts_obj)\n[1] \"ts\"\n```", "```py\nstr(ts_obj)\n Time-Series [1:25] from 1 to 25: -0.5605 -0.2302 1.5587 0.0705 0.1293 ...\nattributes(ts_obj)\n$tsp\n[1]  1 25  1\n$class\n[1] \"ts\"\n```", "```py\n# Change Start\nts(x, start = 1980)\nts(x, start = c(1980, 05))\nts(x, start = 1980, frequency = 12)\nts(x, start = 1980, frequency = 12/3)\n# Change End\nts(x, end = 2023)\nts(x, end = 2023, frequency = 12)\nts(x, end = 2023, frequency = 12/3)\n      Qtr1         Qtr2         Qtr3         Qtr4\n2017 -0.56047565  -0.23017749   1.55870831   0.07050839\n2018  0.12928774   1.71506499   0.46091621  -1.26506123\n2019 -0.68685285  -0.44566197   1.22408180   0.35981383\n2020  0.40077145   0.11068272  -0.55584113   1.78691314\n2021  0.49785048  -1.96661716   0.70135590  -0.47279141\n2022 -1.06782371  -0.21797491  -1.02600445  -0.72889123\n2023 -0.62503927\n```", "```py\n# Read the airpassengers.xlsx file in and convert to a ts object starting at 1949\nap_ts <- read_xlsx(\"./Chapter 10/airpassengers.xlsx\")  |>\n  ts(start = 1949, frequency = 12)\n# Plot the ts object\nplot(ap_ts)\n```", "```py\nplot(decompose(ap_ts))\n```", "```py\nacf(ap_ts)\n```", "```py\nlibrary(healthyR.ts)\nlibrary(dplyr)\nlibrary(timetk)\nap_tbl <- ts_to_tbl(ap_ts) |>\n  select(-index)\n> class(ap_tbl)\n[1] \"tbl_df\"      \"tbl\"           \"data.frame\"\n```", "```py\n# Time Series Split\nsplits <- time_series_split(\n  data = ap_tbl\n  , date_var = date_col\n  , assess = 12\n  , skip = 3\n  , cumulative = TRUE\n)\n> splits\n<Analysis/Assess/Total>\n<132/12/144>\n```", "```py\nLibrary(modeltime)\nts_auto_arima <- ts_auto_arima(\n  .data = ap_tbl,\n  .num_cores = 10,\n  .date_col = date_col,\n  .value_col = x,\n  .rsamp_obj = splits,\n  .formula = x ~ .,\n  .grid_size = 20,\n  .cv_slice_limit = 5,\n  .tune = TRUE\n)\n```", "```py\n> ts_auto_arima$recipe_info\n$recipe_call\nrecipe(.data = ap_tbl, .date_col = date_col, .value_col = x,\n     .formula = x ~ ., .rsamp_obj = splits, .tune = TRUE, .grid_size = 20,\n     .num_cores = 10, .cv_slice_limit = 5)\n$recipe_syntax\n[1] \"ts_arima_recipe <-\"\n[2] \"\\n  recipe(.data = ap_tbl, .date_col = date_col, .value_col = x, .formula = x ~ \\n     ., .rsamp_obj = splits, .tune = TRUE, .grid_size = 20, .num_cores = 10, \\n     .cv_slice_limit = 5)\"\n$rec_obj\n── Recipe ────────────────────────────────────────────────────────────────────────────────────────\n── Inputs\nNumber of variables by role\noutcome:   1\npredictor: 1\n```", "```py\nts_auto_arima\n> ts_auto_arima$model_info\n$model_spec\nARIMA Regression Model Specification (regression)\nMain Arguments:\n  seasonal_period = tune::tune()\n  non_seasonal_ar = tune::tune()\n  non_seasonal_differences = tune::tune()\n  non_seasonal_ma = tune::tune()\n  seasonal_ar = tune::tune()\n  seasonal_differences = tune::tune()\n  seasonal_ma = tune::tune()\nComputational engine: arima\n```", "```py\n$wflw\n══ Workflow\nPreprocessor: Recipe\nModel: arima_reg()\n── Preprocessor\n0 Recipe Steps\n── Model\nARIMA Regression Model Specification (regression)\nMain Arguments:\n  seasonal_period = tune::tune()\n  non_seasonal_ar = tune::tune()\n  non_seasonal_differences = tune::tune()\n  non_seasonal_ma = tune::tune()\n  seasonal_ar = tune::tune()\n  seasonal_differences = tune::tune()\n  seasonal_ma = tune::tune()\nComputational engine: arima\n```", "```py\n$fitted_wflw\n…\n── Model\nSeries: outcome\nARIMA(4,1,2)(1,0,1)[12]\nCoefficients:\n       ar1    ar2     ar3      ar4     ma1      ma2     sar1     sma1\n     -0.221  0.9020  0.0894  -0.2144  0.0477  -0.9523  0.9695  -0.0869\ns.e.  0.092  0.0996  0.0958   0.0875  0.0367   0.0365  0.0143   0.0927\nsigma^2 = 99.46:  log likelihood = -497.36\nAIC=1012.72   AICc=1014.21   BIC=1038.6\n$was_tuned\n[1] \"tuned\"\n```", "```py\n> ts_auto_arima$model_calibration\n$plot\n$calibration_tbl\n# Modeltime Table\n# A tibble: 1 × 5\n  .model_id .model.model_desc.type .calibration_data\n        <int> <list>      <chr>                           <chr> <list>\n1            1 <workflow> ARIMA(4,1,2)(1,0,1)[12] Test  <tibble [12 × 4]>\n$model_accuracy\n# A tibble: 1 × 9\n  .model_id .model_desc.type   mae  mape  mase smape  rmse   rsq\n        <int> <chr>                           <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1            1 ARIMA(4,1,2)(1,0,1)[12] Test   16.2  3.35 0.335  3.35  19.5 0.960\n```", "```py\n> ts_auto_arima$tuned_info\n$tuning_grid\n# A tibble: 20 × 7\n   seasonal_period non_seasonal_ar non_seasonal_differences non_seasonal_ma seasonal_ar\n   <chr>         <int>           <int>         <int>       <int>\n 1 weekly          3               0            1             2\n 2 yearly          5               1            4               0\n# ℹ 2 more variables: seasonal_differences <int>, seasonal_ma <int>\n$tscv\n# Time Series Cross Validation Plan\n# A tibble: 5 × 2\n  splits               id\n  <list>               <chr>\n1 <split [120/12]> Slice1\n2 <split [117/12]> Slice2\n3 <split [114/12]> Slice3\n4 <split [111/12]> Slice4\n5 <split [108/12]> Slice5\n$tuned_results\n# Tuning results\n# NA\n# A tibble: 5 × 4\n  splits               id      .metrics                 .notes\n  <list>               <chr>  <list>                    <list>\n1 <split [120/12]> Slice1 <tibble [120 × 11]> <tibble [1 × 3]>\n2 …\n$grid_size\n[1] 20\n$best_metric\n[1] \"rmse\"\n$best_result_set\n# A tibble: 1 × 13\n  seasonal_period non_seasonal_ar non_seasonal_differences non_seasonal_ma seasonal_ar\n  <chr>           <int>            <int>         <int>         <int>\n1 yearly             4                1              2               1\n# ℹ 8 more variables: seasonal_differences <int>, seasonal_ma <int>, .metric <chr>,\n#   .estimator <chr>, mean <dbl>, n <int>, std_err <dbl>, .config <chr>\n$tuning_grid_plot\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n$plotly_grid_plot\n```", "```py\nlibrary(healthyR.ts)\nts_brownian_motion() |>\n  ts_brownian_motion_plot(t, y)\n```", "```py\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n```", "```py\n# Create a date range\ndate_rng = pd.date_range(start='2022-01-01', end='2023-12-31',\n    freq='D')\n# Create a trend component\ntrend = 0.05 * np.arange(len(date_rng))\n# Create a seasonal component (cyclicality)\nseasonal = 2.5 * np.sin(2 * np.pi * np.arange(len(date_rng)) / 365)\n# Add some random noise\nnoise = np.random.normal(0, 0.5, len(date_rng))\n# Combine all components to create the time series\ntime_series = trend + seasonal + noise\n# Create a DataFrame\ndf = pd.DataFrame({'Date': date_rng, 'Value': time_series})\n# Save the data to an Excel file\ndf.to_excel('time_series_data.xlsx', index=False)\n```", "```py\n# Read the data back into pandas\nloaded_df = pd.read_excel('time_series_data.xlsx')\n# Display the first few rows\nprint(loaded_df.head())\n```", "```py\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n```", "```py\n# Load time series data (replace 'time_series_data.xlsx' with your data file)\ndata = pd.read_excel('time_series_data.xlsx')\n# Convert the 'Date' column to datetime format and set it as the index\ndata['Date'] = pd.to_datetime(data['Date'])\ndata.set_index('Date', inplace=True)\n```", "```py\n# Plot the time series\nplt.figure(figsize=(12, 6))\nplt.plot(data['Value'])\nplt.title('Time Series Plot')\nplt.xlabel('Date')\nplt.ylabel('Value')\nplt.grid(True)\nplt.show()\n```", "```py\n# ACF and PACF plots\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n# ACF plot\nplot_acf(data['Value'], lags=10, ax=ax1)\nax1.set_title('Autocorrelation Function (ACF)')\n# PACF plot\nplot_pacf(data['Value'], lags=40, ax=ax2)\nax2.set_title('Partial Autocorrelation Function (PACF)')\nplt.tight_layout()\nplt.show()\n```", "```py\nfrom statsmodels.tsa.stattools import adfuller\nimport pandas as pd\n# Read the data back into pandas\ndf = pd.read_excel('time_series_data.xlsx')\n# Augmented Dickey-Fuller Test\nadf_result = adfuller(df['Value'])\nprint(\"\\nAugmented Dickey-Fuller Test:\")\nprint(f\"ADF Statistic: {adf_result[0]}\")\nprint(f\"P-value: {adf_result[1]}\")\nprint(\"Null Hypothesis (H0): Data is non-stationary\")\nprint(\"Alternative Hypothesis (H1): Data is stationary\")\nif adf_result[1] <= 0.05:\n     print(\"Result: Reject the null hypothesis. Data is stationary.\")\nelse:\n     print(\"Result: Failed to reject the null hypothesis. Data is non-stationary.\")\n```", "```py\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport matplotlib.pyplot as plt\n# Time Series Decomposition\ndecomposition = seasonal_decompose(df['Value'],\n    model='additive',  period=365)\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n# Plot the decomposition components\nplt.figure(figsize=(12, 8))\nplt.subplot(411)\nplt.plot(df['Date'], df['Value'], label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(df['Date'], trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(df['Date'], seasonal, label='Seasonal')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(df['Date'], residual, label='Residual')\nplt.legend(loc='best')\nplt.suptitle(\"Time Series Decomposition\")\nplt.show()\n```", "```py\n    # Import necessary libraries\n    import pandas as pd\n    import numpy as np\n    import statsmodels.api as sm\n    from scipy.stats import norm\n    import matplotlib.pyplot as plt\n    # Load the time series data (replace with your data)\n    time_series_data = pd.read_excel('time_series_data.xlsx')['Value']\n    ```", "```py\n    # Perform the Augmented Dickey-Fuller test to check for stationarity\n    result = sm.tsa.adfuller(time_series_data, autolag='AIC')\n    # If the p-value is greater than a threshold (e.g., 0.05), perform differencing to make the data stationary\n    if result[1] > 0.05:\n            differenced_data = np.diff(time_series_data, n=1)\n    else:\n            differenced_data = time_series_data\n    ```", "```py\n    # Build an ARIMA model\n    order = (1, 1, 1)  # Values based on ACF and PACF analysis\n    model = sm.tsa.ARIMA(differenced_data, order=order)\n    # Fit the ARIMA model\n    model_fit = model.fit()\n    ```", "```py\n    # Make forecasts\n    forecast_steps = 50  # Adjust the number of forecast steps as needed\n    forecast = model_fit.forecast(steps=forecast_steps)\n    # If the p-value is greater than a threshold (e.g., 0.05), perform differencing to make the data stationary\n    if result[1] > 0.05:\n            # The model was trained on the differenced data so the forecasts have to be added to the last data point\n            cumsum_forecasts = np.cumsum(forecast)\n            # Add this cumulative sum to the last observed value in your raw data\n            real_forecasts = cumsum_forecasts + time_series_data[len(time_series_data)-1]\n    else:\n            real_forecasts = forecast\n    ```", "```py\n    # Retrieve ARIMA model parameters\n    params = model_fit.params\n    p, d, q = order\n    resid = model_fit.resid\n    # Compute the standard errors\n    stderr = np.std(resid)\n    # Calculate the confidence intervals\n    z_score = norm.ppf(0.975)  # For a 95% confidence interval\n    conf_int = np.column_stack((real_forecasts - z_score * stderr,\n            real_forecasts + z_score * stderr))\n    # Separate the forecasts into point forecasts and confidence intervals\n    point_forecasts = real_forecasts  # The point forecasts\n    forecast_stderr = stderr  # The standard errors of the forecasts\n    lower_bound = conf_int[:, 0]  # Lower confidence interval bounds\n    upper_bound = conf_int[:, 1]  # Upper confidence interval bounds\n    ```", "```py\n    # Visualize the original time series and forecasts\n    plt.figure(figsize=(12, 6))\n    plt.plot(time_series_data, label='Original Time Series', color='blue')\n    plt.plot(range(len(time_series_data),\n        len(time_series_data) + forecast_steps),\n        real_forecasts, label='Forecast', color='red')\n    plt.fill_between(range(len(time_series_data),\n        len(time_series_data) + forecast_steps),\n        lower_bound, upper_bound, color='pink', alpha=0.5)\n    plt.xlabel('Time Steps')\n    plt.ylabel('Value')\n    plt.title('ARIMA Time Series Forecast')\n    plt.legend()\n    plt.show()\n    ```", "```py\n    # Import necessary libraries\n    import pandas as pd\n    from prophet import Prophet\n    from prophet.plot import plot\n    # Load the time series data (replace with your data)\n    time_series_data = pd.read_excel('time_series_data.xlsx')\n    # Create a DataFrame with 'ds' and 'y' columns\n    df = pd.DataFrame({'ds': time_series_data['Date'], \n        'y': time_series_data['Value']})\n    ```", "```py\n    # Initialize and fit the Prophet model without weekly seasonality\n    model = Prophet(weekly_seasonality=False)\n    # Add custom seasonality obtained from domain knowledge (in this case: we generated the data so)\n    model.add_seasonality(name='custom_season', period=365, \n        fourier_order=5)\n    # Fit the customized model\n    model.fit(df)\n    ```", "```py\n    # Create a dataframe for future dates\n    forecast_steps = 150  # Adjust the number of forecast steps as needed\n    future = model.make_future_dataframe(periods=forecast_steps,\n        freq='D')\n    # Make predictions\n    forecast = model.predict(future)\n    # Plot the forecast\n    fig = model.plot(forecast)\n    fig.show()\n    # Plot components of the forecast (trend, yearly, and weekly seasonality)\n    fig2 = model.plot_components(forecast)\n    fig2.show()\n    ```", "```py\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from keras.models import Sequential\n    from keras.layers import LSTM, Dense\n    from sklearn.preprocessing import MinMaxScaler\n    # Load the time series data (replace with your data)\n    time_series_data = pd.read_excel('time_series_data.xlsx')\n    # Normalize the data to be in the range [0, 1]\n    scaler = MinMaxScaler()\n    data = scaler.fit_transform(\n        time_series_data['Value'].to_numpy().reshape(-1, 1))\n    ```", "```py\n    # Split the data into training and testing sets\n    train_size = int(len(data) * 0.67)\n    train, test = data[0:train_size, :], data[train_size:len(data), :]\n    # Create sequences and labels for training\n    def create_dataset(dataset, look_back=1):\n        X, Y = [], []\n        for i in range(len(dataset) - look_back):\n            a = dataset[i:(i + look_back), 0]\n            X.append(a)\n            Y.append(dataset[i + look_back, 0])\n        return np.array(X), np.array(Y)\n    look_back = 3\n    X_train, Y_train = create_dataset(train, look_back)\n    X_test, Y_test = create_dataset(test, look_back)\n    # Reshape the data for LSTM input\n    X_train = np.reshape(X_train, (X_train.shape[0], 1, \n        X_train.shape[1]))\n    X_test = np.reshape(X_test, (X_test.shape[0], 1, \n        X_test.shape[1]))\n    ```", "```py\n    model = Sequential()\n    model.add(LSTM(4, input_shape=(1, look_back)))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    model.fit(X_train, Y_train, epochs=100, batch_size=1, verbose=2)\n    ```", "```py\n    # Make predictions:\n    trainPredict = model.predict(X_train)\n    testPredict = model.predict(X_test)\n    # Inverse transform the predictions to the original scale\n    trainPredict = scaler.inverse_transform(trainPredict)\n    testPredict = scaler.inverse_transform(testPredict)\n    ```", "```py\n    # Plot the training predictions\n    trainPredictPlot = np.empty_like(data)\n    trainPredictPlot[:, :] = np.nan\n    trainPredictPlot[look_back:len(trainPredict) + look_back, :] =\\\n        trainPredict\n    # Plot the test predictions\n    testPredictPlot = np.empty_like(data)\n    testPredictPlot[:, :] = np.nan\n    testPredictPlot[len(trainPredict) + (look_back * 2):len(data),\n        :] = testPredict\n    # Plot the training data in blue\n    plt.plot(time_series_data['Value'], color='blue', label=\n        'Actual Data')\n    # Create shaded regions for the training and test data\n    plt.fill_between(range(len(data)), 0, \n        trainPredictPlot.reshape(-1),\n        color='lightgray', label='Training Data')\n    plt.fill_between(range(len(data)), 0, \n        testPredictPlot.reshape(-1),\n        color='lightcoral', label='Test Data')\n    # Overlay the predictions in green\n    plt.plot(testPredictPlot, color='green', label='Predictions')\n    plt.title('Time Series Analysis with LSTM')\n    plt.legend()\n    plt.show()\n    ```"]