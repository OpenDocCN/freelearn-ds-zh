- en: '*Chapter 1*: What is Data Engineering?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to *Data Engineering with Python*. While data engineering is not a new
    field, it seems to have stepped out from the background recently and started to
    take center stage. This book will introduce you to the field of data engineering.
    You will learn about the tools and techniques employed by data engineers and you
    will learn how to combine them to build data pipelines. After completing this
    book, you will be able to connect to multiple data sources, extract the data,
    transform it, and load it into new locations. You will be able to build your own
    data engineering infrastructure, including clustering applications to increase
    their capacity to process data.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn about the roles and responsibilities of data
    engineers and how data engineering works to support data science. You will be
    introduced to the tools used by data engineers, as well as the different areas
    of technology that you will need to be proficient in to become a data engineer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What data engineers do
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data engineering versus data science
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data engineering tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What data engineers do
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data engineering** is part of the big data ecosystem and is closely linked
    to data science. Data engineers work in the background and do not get the same
    level of attention as data scientists, but they are critical to the process of
    data science. The roles and responsibilities of a data engineer vary depending
    on an organization''s level of data maturity and staffing levels; however, there
    are some tasks, such as the extracting, loading, and transforming of data, that
    are foundational to the role of a data engineer.'
  prefs: []
  type: TYPE_NORMAL
- en: At the lowest level, data engineering involves the movement of data from one
    system or format to another system or format. Using more common terms, data engineers
    query data from a source (extract), they perform some modifications to the data
    (transform), and then they put that data in a location where users can access
    it and know that it is production quality (load). The terms **extract**, **transform**,
    and **load** will be used a lot throughout this book and will often be abbreviated
    to **ETL**. This definition of data engineering is broad and simplistic. With
    the help of an example, let's dig deeper into what data engineers do.
  prefs: []
  type: TYPE_NORMAL
- en: An online retailer has a website where you can purchase widgets in a variety
    of colors. The website is backed by a relational database. Every transaction is
    stored in the database. How many blue widgets did the retailer sell in the last
    quarter?
  prefs: []
  type: TYPE_NORMAL
- en: To answer this question, you could run a SQL query on the database. This doesn't
    rise to the level of needing a data engineer. But as the site grows, running queries
    on the production database is no longer practical. Furthermore, there may be more
    than one database that records transactions. There may be a database at different
    geographical locations – for example, the retailers in North America may have
    a different database than the retailers in Asia, Africa, and Europe.
  prefs: []
  type: TYPE_NORMAL
- en: Now you have entered the realm of data engineering. To answer the preceding
    question, a data engineer would create connections to all of the transactional
    databases for each region, extract the data, and load it into a data warehouse.
    From there, you could now count the number of all the blue widgets sold.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than finding the number of blue widgets sold, companies would prefer
    to find the answer to the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How do we find out which locations sell the most widgets?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we find out the peak times for selling widgets?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many users put widgets in their carts and remove them later?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we find out the combinations of widgets that are sold together?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answering these questions requires more than just extracting the data and loading
    it into a single system. There is a transformation required in between the extract
    and load. There is also the difference in times zones in different regions. For
    instance, the United States alone has four time zones. Because of this, you would
    need to transform time fields to a standard. You will also need a way to distinguish
    sales in each region. This could be accomplished by adding a location field to
    the data. Should this field be spatial – in coordinates or as well-known text
    – or will it just be text that could be transformed in a data engineering pipeline?
  prefs: []
  type: TYPE_NORMAL
- en: Here, the data engineer would need to extract the data from each database, then
    transform the data by adding an additional field for the location. To compare
    the time zones, the data engineer would need to be familiar with data standards.
    For the time, the **International Organization for Standardization** (**ISO**)
    has a standard – **ISO 8601**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now answer the questions in the preceding list one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract the data from each database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a field to tag the location for each transaction in the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transform the date from local time to ISO 8601.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load the data into the data warehouse.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The combination of extracting, loading, and transforming data is accomplished
    by the creation of a data pipeline. The data comes into the pipeline raw, or dirty
    in the sense that there may be missing data or typos in the data, which is then
    cleaned as it flows through the pipe. After that, it comes out the other side
    into a data warehouse, where it can be queried. The following diagram shows the
    pipeline required to accomplish the task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – A pipeline that adds a location and modifies the date'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15739_01_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.1 – A pipeline that adds a location and modifies the date
  prefs: []
  type: TYPE_NORMAL
- en: Knowing a little more about what data engineering is, and what data engineers
    do, you should start to get a sense of the responsibilities and skills that data
    engineers need to acquire. The following section will elaborate on these skills.
  prefs: []
  type: TYPE_NORMAL
- en: Required skills and knowledge to be a data engineer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding example, it should be clear that data engineers need to be
    familiar with many different technologies, and we haven't even mentioned the business
    processes or needs.
  prefs: []
  type: TYPE_NORMAL
- en: At the start of a data pipeline, data engineers need to know how to extract
    data from files in different formats or different types of databases. This means
    data engineers need to know several languages used to perform many different tasks,
    such as SQL and Python.
  prefs: []
  type: TYPE_NORMAL
- en: During the transformation phase of the data pipeline, data engineers need to
    be familiar with data modeling and structures. They will also need to understand
    the business and what knowledge and insight they are hoping to extract from the
    data because this will impact the design of the data models.
  prefs: []
  type: TYPE_NORMAL
- en: The loading of data into the data warehouse means there needs to be a data warehouse
    with a schema to hold the data. This is also usually the responsibility of the
    data engineer. Data engineers will need to know the basics of data warehouse design,
    as well as the types of databases used in their construction.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the entire infrastructure that the data pipeline runs on could be the
    responsibility of the data engineer. They need to know how to manage Linux servers,
    as well as how to install and configure software such as Apache Airflow or NiFi.
    As organizations move to the cloud, the data engineer now needs to be familiar
    with spinning up the infrastructure on the cloud platform used by the organization
    – Amazon, Google Cloud Platform, or Azure.
  prefs: []
  type: TYPE_NORMAL
- en: Having walked through an example of what data engineers do, we can now develop
    a broader definition of data engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Information
  prefs: []
  type: TYPE_NORMAL
- en: Data engineering is the development, operation, and maintenance of data infrastructure,
    either on-premises or in the cloud (or hybrid or multi-cloud), comprising databases
    and pipelines to extract, transform, and load data.
  prefs: []
  type: TYPE_NORMAL
- en: Data engineering versus data science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data engineering is what makes data science possible. Again, depending on the
    maturity of an organization, data scientists may be expected to clean and move
    the data required for analysis. This is not the best use of a data scientist's
    time. Data scientists and data engineers use similar tools (Python, for instance),
    but they specialize in different areas. Data engineers need to understand data
    formats, models, and structures to efficiently transport data, whereas data scientists
    utilize them for building statistical models and mathematical computation.
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists will connect to the data warehouses built by data engineers.
    From there, they can extract the data required for machine learning models and
    analysis. Data scientists may have their models incorporated into a data engineering
    pipeline. A close relationship should exist between data engineers and data scientists.
    Understanding what data scientists need in the data will only serve to help the
    data engineers deliver a better product.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn more about the most common tools used by
    data engineers.
  prefs: []
  type: TYPE_NORMAL
- en: Data engineering tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To build data pipelines, data engineers need to choose the right tools for
    the job. Data engineering is part of the overall big data ecosystem and has to
    account for the three Vs of big data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Volume**: The volume of data has grown substantially. Moving a thousand records
    from a database requires different tools and techniques than moving millions of
    rows or handling millions of transactions a minute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variety**: Data engineers need tools that handle a variety of data formats
    in different locations (databases, APIs, files).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Velocity**: The velocity of data is always increasing. Tracking the activity
    of millions of users on a social network or the purchases of users all over the
    world requires data engineers to operate often in near real time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Programming languages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The lingua franca of data engineering is **SQL**. Whether you use low-code tools
    or a specific programming language, there is almost no way to get around knowing
    SQL. A strong foundation in SQL allows the data engineer to optimize queries for
    speed and can assist in data transformations. SQL is so prevalent in data engineering
    that data lakes and non-SQL databases have tools to allow the data engineer to
    query them in SQL.
  prefs: []
  type: TYPE_NORMAL
- en: A large number of open source data engineering tools use **Java** and **Scala**
    (Apache projects). Java is a popular, mainstream, object-oriented programming
    language. While debatable, Java is slowly being replaced by other languages that
    run on the **Java Virtual Machine** (**JVM**). Scala is one of these languages.
    Other languages that run on the JVM include **Clojure** and **Groovy**. In the
    next chapter, you will be introduced to **Apache NiFi**. NiFi allows you to develop
    custom processers in Java, Clojure, Groovy, and **Jython**. While Java is an object-oriented
    language, there has been a movement toward functional programming languages, of
    which Clojure and Scala are members.
  prefs: []
  type: TYPE_NORMAL
- en: The focus of this book is on data engineering with Python. It is well-documented
    with a larger user base and cross-platform. Python has become the default language
    for data science and data engineering. Python has an extensive collection of standard
    libraries and third-party libraries. The data science environment in Python is
    unmatched in other languages. Libraries such as `pandas`, `matplotlib`, `numpy`,
    `scipy`, `scikit-learn`, `tensorflow`, `pytorch`, and `NLTK` make up an extremely
    powerful data engineering and data science environment.
  prefs: []
  type: TYPE_NORMAL
- en: Databases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In most production systems, data will be stored in **relational databases**.
    Most proprietary solutions will use either **Oracle** or **Microsoft SQL Server**,
    while open source solutions tend to use **MySQL** or **PostgreSQL**. These databases
    store data in rows and are well-suited to recording transactions. There are also
    relationships between tables, utilizing primary keys to join data from one table
    to another – thus making them relational. The following table diagram shows a
    simple data model and the relationships between the tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – Relational tables joined on Region = RegionID.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15739_01_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.2 – Relational tables joined on Region = RegionID.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common databases used in data warehousing are **Amazon Redshift**,
    **Google BigQuery**, **Apache Cassandra**, and other NoSQL databases, such as
    **Elasticsearch**. Amazon Redshift, Google BigQuery, and Cassandra deviate from
    the traditional rows of relational databases and store data in a columnar format,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – Rows stored in a columnar format'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15739_01_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.3 – Rows stored in a columnar format
  prefs: []
  type: TYPE_NORMAL
- en: Columnar databases are better suited for fast queries – therefore making them
    well-suited for data warehouses. All three of the columnar databases can be queried
    using SQL – although Cassandra uses the Cassandra Query Language, it is similar.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to columnar databases, there are document, or NoSQL, databases,
    such as Elasticsearch. Elasticsearch is actually a search engine based on **Apache
    Lucene**. It is similar to **Apache Solr** but is more user-friendly. Elasticsearch
    is open source, but it does have proprietary components – most notably, the X-Pack
    plugins for machine learning, graphs, security, and alerting/monitoring. Elasticsearch
    uses the Elastic Query **DSL** (**Domain-Specific Language**). It is not SQL,
    but rather a JSON query. Elasticsearch stores data as documents, and while it
    has parent-child documents, it is non-relational (like the columnar databases).
  prefs: []
  type: TYPE_NORMAL
- en: Once a data engineer extracts data from a database, they will need to transform
    or process it. With big data, it helps to use a data processing engine.
  prefs: []
  type: TYPE_NORMAL
- en: Data processing engines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data processing engines allow data engineers to transform data whether it is
    in batches or streams. These engines allow the parallel execution of transformation
    tasks. The most popular engine is **Apache Spark**. Apache Spark allows data engineers
    to write transformations in Python, Java, and Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark works with Python DataFrames, making it an ideal tool for Python
    programmers. Spark also has **Resilient Distributed Datasets** (**RDDs**). RDDs
    are an immutable and distributed collection of objects. You create them mainly
    by loading in an external data source. RDDs allow fast and distributed processing.
    The tasks in an RDD are run on different nodes within the cluster. Unlike DataFrames,
    they do not try to guess the schema in your data.
  prefs: []
  type: TYPE_NORMAL
- en: Other popular process engines include **Apache Storm**, which utilizes spouts
    to read data and bolts to perform transformations. By connecting them, you build
    a processing pipeline. **Apache Flink** and **Samza** are more modern stream and
    batch processing frameworks that allow you to process unbounded streams. An unbounded
    stream is data that comes in with no known end – a temperature sensor, for example,
    is an unbounded stream. It is constantly reporting temperatures. Flink and Samza
    are excellent choices if you are using Apache Kafka to stream data from a system.
    You will learn more about Apache Kafka later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Data pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Combining a transactional database, a programming language, a processing engine,
    and a data warehouse results in a pipeline. For example, if you select all the
    records of widget sales from the database, run it through Spark to reduce the
    data to widgets and counts, then dump the result to the data warehouse, you have
    a pipeline. But this pipeline is not very useful if you have to execute manually
    every time you want it to run. Data pipelines need a scheduler to allow them to
    run at specified intervals. The simplest way to accomplish this is by using **crontab**.
    Schedule a cron job for your Python file and sit back and watch it run every *X*
    number of hours.
  prefs: []
  type: TYPE_NORMAL
- en: Managing all the pipelines in crontab becomes difficult fast. How do you keep
    track of pipelines' successes and failures? How do you know what ran and what
    didn't? How do you handle backpressure – if one task runs faster than the next,
    how do you hold data back, so it doesn't overwhelm the task? As your pipelines
    become more advanced, you will quickly outgrow crontab and will need a better
    framework.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Airflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most popular framework for building data engineering pipelines in Python
    is **Apache Airflow**. Airflow is a workflow management platform built by Airbnb.
    Airflow is made up of a web server, a scheduler, a metastore, a queueing system,
    and executors. You can run Airflow as a single instance, or you can break it up
    into a cluster with many executor nodes – this is most likely how you would run
    it in production. Airflow uses **Directed Acyclic Graphs** (**DAGs**).
  prefs: []
  type: TYPE_NORMAL
- en: 'A DAG is Python code that specifies tasks. A graph is a series of nodes connected
    by a relationship or dependency. In Airflow, they are directed because they flow
    in a direction with each task coming after its dependency. Using the preceding
    example pipeline, the first node would be to execute a SQL statement grabbing
    all the widget sales. This node would connect downstream to another node, which
    would aggregate the widgets and counts. Lastly, this node would connect to the
    final node, which loads the data into the warehouse. The pipeline DAG would look
    as in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – A DAG showing the flow of data between nodes. The task follows
    the arrows (is directed) from left to right'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15739_01_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.4 – A DAG showing the flow of data between nodes. The task follows
    the arrows (is directed) from left to right
  prefs: []
  type: TYPE_NORMAL
- en: 'This book will cover the basics of Apache Airflow but will primarily use Apache
    NiFi to demonstrate the principles of data engineering. The following is a screenshot
    of a DAG in Airflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – The Airflow GUI showing the details of a DAG'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15739_01_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.5 – The Airflow GUI showing the details of a DAG
  prefs: []
  type: TYPE_NORMAL
- en: The GUI is not as polished as NiFi, which we will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Apache NiFi
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Apache NiFi is another framework for building data engineering pipelines, and
    it too utilizes DAGs. Apache NiFi was built by the National Security Agency and
    is used at several federal agencies. Apache NiFi is easier to set up and is useful
    for new data engineers. The GUI is excellent and while you can use Jython, Clojure,
    Scala, or Groovy to write processors, you can accomplish a lot with a simple configuration
    of existing processors. The following screenshot shows the NiFi GUI and a sample
    DAG:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – A sample NiFi flow extracting data from a database and sending
    it to Elasticsearch'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Image86923.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.6 – A sample NiFi flow extracting data from a database and sending
    it to Elasticsearch
  prefs: []
  type: TYPE_NORMAL
- en: Apache NiFi also allows clustering and the remote execution of pipelines. It
    has a built-in scheduler and provides the backpressure and monitoring of pipelines.
    Furthermore, Apache NiFi has version control using the NiFi Registry and can be
    used to collect data on the edge using MiNiFi.
  prefs: []
  type: TYPE_NORMAL
- en: Another Python-based tool for data engineering pipelines is Luigi – developed
    by Spotify. Luigi also uses a graph structure and allows you to connect tasks.
    It has a GUI much like Airflow. Luigi will not be covered in this book but is
    an excellent option for Python-based data engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned what data engineering is. Data engineering roles
    and responsibilities vary depending on the maturity of an organization's data
    infrastructure. But data engineering, at its simplest, is the creation of pipelines
    to move data from one source or format to another. This may or may not involve
    data transformations, processing engines, and the maintenance of infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Data engineers use a variety of programming languages, but most commonly Python,
    Java, or Scala, as well as proprietary and open source transactional databases
    and data warehouses, both on-premises and in the cloud, or a mixture. Data engineers
    need to be knowledgeable in many areas – programming, operations, data modeling,
    databases, and operating systems. The breadth of the field is part of what makes
    it fun, exciting, and challenging. To those willing to accept the challenge, data
    engineering is a rewarding career.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will begin by setting up an environment to start building
    data pipelines.
  prefs: []
  type: TYPE_NORMAL
