- en: Data Modeling in Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've learned how to create a Hadoop cluster and how to load data into
    it. In the previous chapter, we learned about various data ingestion tools and
    techniques. As we know by now, there are various open source tools available in
    the market, but there is a single silver bullet tool that can take on all our
    use cases. Each data ingestion tool has certain unique features; they can prove
    to be very productive and useful in typical use cases. For example, Sqoop is more
    useful when used to import and export Hadoop data from and to an RDBMS.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn how to store and model data in Hadoop clusters.
    Like data ingestion tools, there are various data stores available. These data
    stores support different data models—that is, columnar data storage, key value
    pairs, and so on; and they support various file formats, such as ORC, Parquet,
    and AVRO, and so on. There are very popular data stores, widely used in production
    these days, for example, Hive, HBase, Cassandra, and so on. We will learn more
    about the following two data stores and data modeling techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Hive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache HBase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, we will start with basic concepts and then we will learn how we can
    apply modern data modeling techniques for faster data access. In a nutshell, we
    will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Hive and RDBMS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supported datatypes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hive architecture and how it works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Hive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hive is a data processing tool in Hadoop. As we have learned in the previous
    chapter, data ingestion tools load data and generate HDFS files in Hadoop; we
    need to query that data based on our business requirements. We can access the
    data using MapReduce programming. But data access with MapReduce is extremely
    slow. To access a few lines of HDFS files, we have to write separate mapper, reducer,
    and driver code. So, in order to avoid this complexity, Apache introduced Hive.
    Hive supports an SQL-like interface that helps access the same lines of HDFS files
    using SQL commands. Hive was initially developed by Facebook but was later taken
    over by Apache.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Hive and RDBMS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I mentioned that Hive provides an SQL-like interface. Bearing this in mind,
    the question that arises is: *is Hive the same as RDBMS on Hadoop?* The answer
    is *no*. Hive is not a database. Hive does not store any data. Hive stores table
    information as a part of metadata, which is called schema, and points to files
    on HDFS. Hive accesses data stored on HDFS files using an SQL-like interface called
    **HiveQL** (**HQL**). Hive supports SQL commands to access and modify data in
    HDFS. Hive is not a tool for OLTP. It does not provide any row-level insert, update,
    or delete. The current version of Hive (version 0.14), does support insert, update,
    and delete with full ACID properties, but that feature is not efficient. Also,
    this feature does not support all file formats. For example, the update supports
    only ORC file format. Basically, Hive is designed for batch processing and does
    not support transaction processing like RDBMS does. Hence, Hive is better suited
    for data warehouse applications for providing data summarization, query, and analysis.
    Internally, Hive SQL queries are converted into MapReduce by its compiler. Users
    need not worry about writing any complex mapper and reducer code. Hive supports
    query structured data only. It is very complex to access unstructured data using
    Hive SQL. You may have to write your own custom functions for that. Hive supports
    various file formats such as text files, sequence files, ORC, and Parquet, which
    provide significant data compression.
  prefs: []
  type: TYPE_NORMAL
- en: Supported datatypes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following datatypes are supported by Hive version 0.14:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Datatype group** | **Datatype** | **Format** |'
  prefs: []
  type: TYPE_TB
- en: '| String | `STRING` | `column_name STRING` |'
  prefs: []
  type: TYPE_TB
- en: '| `VARCHAR` | `column_name VARCHAR(max_length)` |'
  prefs: []
  type: TYPE_TB
- en: '| `CHAR` | `column_name CHAR(length)` |'
  prefs: []
  type: TYPE_TB
- en: '| Numeric | `TINYINT` | `column_name TINYINT` |'
  prefs: []
  type: TYPE_TB
- en: '| `SMALLINT` | `column_name SMALLINT` |'
  prefs: []
  type: TYPE_TB
- en: '| `INT` | `column_name INT` |'
  prefs: []
  type: TYPE_TB
- en: '| `BIGINT` | `column_name BIGINT` |'
  prefs: []
  type: TYPE_TB
- en: '| `FLOAT` | `column_name FLOAT` |'
  prefs: []
  type: TYPE_TB
- en: '| `DOUBLE` | `column_name DOUBLE` |'
  prefs: []
  type: TYPE_TB
- en: '| `DECIMAL` | `column_name DECIMAL[(precision[,scale])]` |'
  prefs: []
  type: TYPE_TB
- en: '| Date/time type | `TIMESTAMP` | `column_name TIMESTAMP` |'
  prefs: []
  type: TYPE_TB
- en: '| `DATE` | `column_name DATE` |'
  prefs: []
  type: TYPE_TB
- en: '| `INTERVAL` | `column_name INTERVAL year to month` |'
  prefs: []
  type: TYPE_TB
- en: '| Miscellaneous type | `BOOLEAN` | `column_name BOOLEAN` |'
  prefs: []
  type: TYPE_TB
- en: '| `BINARY` | `column_name BINARY` |'
  prefs: []
  type: TYPE_TB
- en: '| Complex type | `ARRAY` | `column_name ARRAY < type >` |'
  prefs: []
  type: TYPE_TB
- en: '| `MAPS` | `column_name MAP < primitive_type, type >` |'
  prefs: []
  type: TYPE_TB
- en: '| `STRUCT` | `column_name STRUCT < name : type [COMMENT ''comment_string'']
    >` |'
  prefs: []
  type: TYPE_TB
- en: '| `UNION` | `column_name UNIONTYPE <int, double, array, string>` |'
  prefs: []
  type: TYPE_TB
- en: How Hive works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hive databases are comprised of tables which are made up of partitions. Data
    can be accessed via a simple query language and Hive supports overwriting or appending
    of data. Within a particular database, data in tables is serialized and each table
    has a corresponding HDFS directory. Each table can be sub-divided into partitions
    that determine how data is distributed within subdirectories of the table directory.
    Data within partitions can be further broken down into buckets.
  prefs: []
  type: TYPE_NORMAL
- en: Hive architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a representation of Hive architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7dcc7f8c-71f0-4b1d-b245-8e0e317aa65a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram shows that Hive architecture is divided into three parts—that
    is, clients, services, and metastore. The Hive SQL is executed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hive SQL query**: A Hive query can be submitted to the Hive server using
    one of these ways: WebUI, JDBC/ODBC application, and Hive CLI. For a thrift-based
    application, it will provide a thrift client for communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query execution**: Once the Hive server receives the query, it is compiled,
    converted into an optimized query plan for better performance, and converted into
    a MapReduce job. During this process, the Hive Server interacts with the metastore
    for query metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Job execution**: The MapReduce job is executed on the Hadoop cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hive data model management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hive handles data in the following four ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Hive tables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hive table partition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hive partition bucketing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hive views
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will see each one of them in detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Hive tables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Hive table is very similar to any RDBMS table. The table is divided into
    rows and columns. Each column (field) is defined with a proper name and datatype.
    We have already seen all the available datatypes in Hive in the *Supported datatypes*
    section. A Hive table is divided into two types:'
  prefs: []
  type: TYPE_NORMAL
- en: Managed tables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: External tables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will learn about both of these types in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Managed tables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a sample command to define a Hive managed table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When the preceding query is executed, Hive creates the table and the metadata
    is updated in the metastore accordingly. But the table is empty. So, data can
    be loaded into this table by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After executing the previous command, the data is moved from `<hdfs_folder_name>`
    to the Hive table''s default location `/user/hive/warehouse/<managed_table_name`.
    This default folder, `/user/hive/warehouse`, is defined in `hive-site.xml` and
    can be changed to any folder. Now, if we decide to drop the table, we can do so
    by issuing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `/user/hive/warehouse/<managed_table_name` folder will be dropped and the
    metadata stored in the metastore will be deleted.
  prefs: []
  type: TYPE_NORMAL
- en: External tables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a sample command to define a Hive external table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When the preceding query is executed, Hive creates the table and the metadata
    is updated in the metastore accordingly. But, again, the table is empty. So, data
    can be loaded into this table by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will not move any file to any folder but, instead, creates a pointer
    to the folder location, and it is updated in the metadata in the metastore. The
    file remains at the same location (`<hdfs_folder_name>`) of the query. Now, if
    we decide to drop the table, we can do so by issuing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The folder `/user/hive/warehouse/<managed_table_name` will not be dropped and
    only the metadata stored in the metastore will be deleted. The file remains in
    the same location—`<hdfs_folder_name>`.
  prefs: []
  type: TYPE_NORMAL
- en: Hive table partition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Partitioning a table means dividing a table into different parts based on a
    value of a partition key. A partition key can be any column, for example, date,
    department, country, and so on. As data is stored in parts, the query response
    time becomes faster. Instead of scanning the whole table, partition creates subfolders
    within the main table folders. Hive will scan only a specific part or parts of
    the table based on the query''s `WHERE` clause. Hive table partition is similar
    to any RDBMS table partition. The purpose is also the same. As we keep inserting
    data into a table, the table becomes bigger in data size. Let''s say we create
    an `ORDERS` table as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We will load the following sample file `ORDERS_DATA` table as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we load `orders.txt` to the `/tmp` HDFS folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the `ORDERS_DATA` table as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s assume we want to insert cities data in an `ORDERS_DATA` table. Each
    city orders data is of 1 TB in size. So the total data size of the `ORDERS_DATA`
    table will be 15 TB (there are 15 cities in the table). Now, if we write the following
    query to get all orders booked in `Los Angeles`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The query will run very slowly as it has to scan the entire table. The obvious
    idea is that we can create 10 different `orders` tables for each city and store
    `orders` data in the corresponding city of the `ORDERS_DATA` table. But instead
    of that, we can partition the `ORDERS_PART` table as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, Hive organizes the tables into partitions for grouping similar types of
    data together based on a column or partition key. Let''s assume that we have 10
    `orders` files for each city, that is, `Orders1.txt` to `Orders10.txt`. The following
    example shows how to load each monthly file to each corresponding partition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Partitioning the data can greatly improve the performance of queries because
    the data is already separated into files based on the column value, which can
    decrease the number of mappers and greatly decrease the amount of shuffling and
    sorting of data in the resulting MapReduce job.
  prefs: []
  type: TYPE_NORMAL
- en: Hive static partitions and dynamic partitions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want to use a static partition in Hive, you should set the property
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, we have seen that we have to insert each monthly
    order file to each static partition individually. Static partition saves time
    in loading data compared to dynamic partition. We have to individually add a partition
    to the table and move the file into the partition of the table. If we have a lot
    partitions, writing a query to load data in each partition may become cumbersome.
    We can overcome this with a dynamic partition. In dynamic partitions, we can insert
    data into a partition table with a single SQL statement but still load data in
    each partition. Dynamic partition takes more time in loading data compared to
    static partition. When you have large data stored in a table, dynamic partition
    is suitable. If you want to partition a number of columns but you don''t know
    how many columns they are, then dynamic partition is also suitable. Here are the
    hive dynamic partition properties you should allow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is an example of dynamic partition. Let''s say we want to load
    data from the `ORDERS_PART` table to a new table called `ORDERS_NEW`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Load data into the `ORDER_NEW` table from the `ORDERS_PART` table. Here, Hive
    will load all partitions of the `ORDERS_NEW` table dynamically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how many partitions are created in `ORDERS_NEW`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now it is very clear when to use static and dynamic partitions. Static partitioning
    can be used when the partition column values are known well in advance before
    loading data into a hive table. In the case of dynamic partitions, partition column
    values are known only during loading of the data into the hive table.
  prefs: []
  type: TYPE_NORMAL
- en: Hive partition bucketing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bucketing is a technique of decomposing a large dataset into more manageable
    groups. Bucketing is based on the hashing function. When a table is bucketed,
    all the table records with the same column value will go into the same bucket.
    Physically, each bucket is a file in a table folder just like a partition. In
    a partitioned table, Hive can group the data in multiple folders. But partitions
    prove effective when they are of a limited number and when the data is distributed
    equally among all of them. If there are a large number of partitions, then their
    use becomes less effective. So in that case, we can use bucketing. We can create
    a number of buckets explicitly during table creation.
  prefs: []
  type: TYPE_NORMAL
- en: How Hive bucketing works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following diagram shows the working of Hive bucketing in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9496a468-bbc4-4711-8027-95b31f9076d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we decide to have three buckets in a table for a column, (`Ord_city`) in
    our example, then Hive will create three buckets with numbers 0-2 (*n-1*). During
    record insertion time, Hive will apply the Hash function to the `Ord_city` column
    of each record to decide the hash key. Then Hive will apply a modulo operator
    to each hash value. We can use bucketing in non-partitioned tables also. But we
    will get the best performance when the bucketing feature is used with a partitioned
    table. Bucketing has two key benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved query performance**: During joins on the same bucketed columns,
    we can specify the number of buckets explicitly. Since each bucket is of equal
    size of data, map-side joins perform better on a bucketed table than a non-bucketed
    table. In a map-side join, the left-hand side table bucket will exactly know the
    dataset in the right-hand side bucket to perform a table join efficiently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved sampling**: Because the data is already split up into smaller chunks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's consider our `ORDERS_DATA` table example. It is partitioned in the `CITY`
    column. It may be possible that all of the cities do not have an equal distribution
    of orders. Some cities may have more orders than others. In that case, we will
    have lopsided partitions. This will affect query performance. Queries with cities
    that have more orders will be slower than for cities with fewer orders. We can
    solve this problem by bucketing the table. Buckets in the table are defined by
    the `CLUSTER` clause in the table DDL. The following examples explain the bucketing
    feature in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Creating buckets in a non-partitioned table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we will create a `ORDERS_BUCK_non_partition` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To refer to all Hive `SET` configuration parameters, please use this URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties](https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the newly created non-partitioned bucket table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command shows that Hive has created four buckets (folders), `00000[0-3]_0`,
    in the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Creating buckets in a partitioned table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we will create a bucketed partition table. Here, the table is partitioned into
    four buckets on the `Ord_city` column, but subdivided into `Ord_zip` columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the bucketed partitioned table with another partitioned table (`ORDERS_PART`)
    with a dynamic partition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Hive views
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Hive view is a logical table. It is just like any RDBMS view. The concept
    is the same. When a view is created, Hive will not store any data into it. When
    a view is created, Hive freezes the metadata. Hive does not support the materialized
    view concept of any RDBMS. The basic purpose of a view is to hide the query complexity.
    At times, HQL contains complex joins, subqueries, or filters. With the help of
    view, the entire query can be flattened out in a virtual table.
  prefs: []
  type: TYPE_NORMAL
- en: When a view is created on an underlying table, any changes to that table, or
    even adding or deleting the table, are invalidated in the view. Also, when a view
    is created, it only changes the metadata. But when that view is accessed by a
    query, it triggers the MapReduce job. A view is a purely logical object with no
    associated storage (no support for materialized views is currently available in
    Hive). When a query references a view, the view's definition is evaluated in order
    to produce a set of rows for further processing by the query. (This is a conceptual
    description. In fact, as part of query optimization, Hive may combine the view's
    definition with the queries, for example, pushing filters from the query down
    into the view.)
  prefs: []
  type: TYPE_NORMAL
- en: A view's schema is frozen at the time the view is created; subsequent changes
    to underlying tables (for example, adding a column) will not be reflected in the
    view's schema. If an underlying table is dropped or changed in an incompatible
    fashion, subsequent attempts to query the invalid view will fail. Views are read-only
    and may not be used as the target of `LOAD`/`INSERT`/`ALTER` for changing metadata.
    A view may contain `ORDER BY` and `LIMIT` clauses. If a referencing query also
    contains these clauses, the query-level clauses are evaluated after the view clauses
    (and after any other operations in the query). For example, if a view specifies
    `LIMIT 5` and a referencing query is executed as (`select * from v LIMIT 10`),
    then at most five rows will be returned.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax of a view
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see a few examples of views:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'I will demonstrate the advantages of views using the following few examples.
    Let''s assume we have two tables, `Table_X` and `Table_Y`, with the following
    schema: `Table_XXCol_1` string, `XCol_2` string, `XCol_3` string, `Table_YYCol_1` string,
    `YCol_2` string, `YCol_3` string, and `YCol_4` string. To create a view exactly
    like the base tables, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a view on selective columns of base tables, use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a view to filter values of columns of base tables, we can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a view to hide query complexities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Hive indexes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main purpose of the indexing is to search through the records easily and
    speed up the query. The goal of Hive indexing is to improve the speed of query
    lookup on certain columns of a table. Without an index, queries with predicates
    like `WHERE tab1.col1 = 10` load the entire table or partition and process all
    the rows. But if an index exists for `col1`, then only a portion of the file needs
    to be loaded and processed. The improvement in query speed that an index can provide
    comes at the cost of additional processing to create the index and disk space
    to store the index. There are two types of indexes:'
  prefs: []
  type: TYPE_NORMAL
- en: Compact index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bitmap index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main difference is in storing mapped values of the rows in the different
    blocks.
  prefs: []
  type: TYPE_NORMAL
- en: Compact index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In HDFS, the data is stored in blocks. But scanning which data is stored in
    which block is time consuming. Compact indexing stores the indexed column's value
    and its `blockId`. So the query will not go to the table. Instead, the query will
    directly go to the compact index, where the column value and `blockId` are stored.
    No need to scan all the blocks to find data! So, while performing a query, it
    will first check the index and then go directly into that block.
  prefs: []
  type: TYPE_NORMAL
- en: Bitmap index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bitmap indexing stores the combination of indexed column value and list of
    rows as a bitmap. Bitmap indexing is commonly used for columns with distinct values.
    Let''s review a few examples: Base table, `Table_XXCol_1` Integer, `XCol_2` string,
    `XCol_3` integer, and `XCol_4` string. Create an index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding index is empty because it is created with the `DEFERRED REBUILD`
    clause, regardless of whether or not the table contains any data. After this index
    is created, the `REBUILD` command needs to be used to build the index structure.
    After creation of the index, if the data in the underlying table changes, the
    `REBUILD` command must be used to bring the index up to date. Create the index
    and store it in a text file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a bitmap index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: JSON documents using Hive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'JSON, is a minimal readable format for structuring data. It is used primarily
    to transmit data between a server and web application as an alternative to XML.
    JSON is built on two structures:'
  prefs: []
  type: TYPE_NORMAL
- en: A collection of name/value pairs. In various languages, this is realized as
    an object, record, struct, dictionary, hash table, keyed list, or associative
    array.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ordered list of values. In most languages, this is realized as an array,
    vector, list, or sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please read more on JSON at the following URL: [http://www.json.org/](http://www.json.org/).
    [](http://www.json.org/)
  prefs: []
  type: TYPE_NORMAL
- en: Example 1 – Accessing simple JSON documents with Hive (Hive 0.14 and later versions)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will see how to query simple JSON documents using HiveQL.
    Let''s assume we want to access the following `Sample-Json-simple.json` file in
    `HiveSample-Json-simple.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'View the `Sample-Json-simple.json` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Load `Sample-Json-simple.json` into HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an external Hive table, `simple_json_table`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now verify the records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Example 2 – Accessing nested JSON documents with Hive (Hive 0.14 and later versions)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will see how to query Nested JSON documents using HiveQL. Let''s assume
    we want to access the following `Sample-Json-complex.json` file in `HiveSample-Json-complex.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Load `Sample-Json-simple.json` into HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an external Hive table, `json_nested_table`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify the records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Example 3 – Schema evolution with Hive and Avro (Hive 0.14 and later versions)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In production, we have to change the table structure to address new business
    requirements. The table schema has to change to add/delete/rename table columns.
    Any of these changes affect downstream ETL jobs adversely. In order avoid these,
    we have to make corresponding changes to ETL jobs and target tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Schema evolution allows you to update the schema used to write new data while
    maintaining backwards compatibility with the schemas of your old data. Then you
    can read it all together as if all of the data has one schema. Please read more
    on Avro serialization at the following URL: [https://avro.apache.org/](https://avro.apache.org/).
    In the following example, I will demonstrate how Avro and Hive tables absorb the
    changes of source table''s schema changes without ETL job failure. We will create
    a customer table in the MySQL database and load it to the target Hive external
    table using Avro files. Then we will add one more column to the source tables
    to see how a Hive table absorbs that change without any errors. Connect to MySQL
    to create a source table (`customer`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Insert records into the `customer` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'On Hadoop, run the following `sqoop` command to import the `customer` table
    and store data in Avro files into HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify the target HDFS folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a Hive external table to access Avro files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify the Hive `customer` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Perfect! We have no errors. We successfully imported the source `customer`
    table to the target Hive table using Avro serialization. Now, we add one column
    to the source table and import it again to verify that we can access the target
    Hive table without any schema changes. Connect to MySQL and add one more column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Now insert rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'On Hadoop, run the following `sqoop` command to import the `customer` table
    so as to append the new address column and data. I have used the `append`  and
    `where "cust_id > 4"` parameters to import only the new rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify the HDFS folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s verify that our target Hive table is still able to access old and
    new Avro files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Great! No errors. Still, it''s business as usual; now we will add one new column
    to the Hive table to see the newly added Avro files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify the Hive table for new data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Awesome! Take a look at customer IDs `5` and `6`. We can see the newly added
    column (`cust_state`) with values. You can experiment the delete column and replace
    column feature with the same technique. Now we have a fairly good idea about how
    to access data using Apache Hive. In the next section, we will learn about accessing
    data using HBase, which is a NoSQL data store.
  prefs: []
  type: TYPE_NORMAL
- en: Apache HBase
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have just learned about Hive, which is a database where users can access
    data using SQL commands. But there are certain databases where users cannot use
    SQL commands. Those databases are known as **NoSQL data stores**. HBase is a NoSQL
    database. So, what is actually meant by NoSQL? NoSQL means not only SQL. In NoSQL
    data stores like HBase, the main features of RDBMS, such as validation and consistency,
    are relaxed. Also, another important difference between RDBMS or SQL databases
    and NoSQL databases is schema on write versus schema on read. In schema on write,
    the data is validated at the time of writing to the table, whereas schema on read
    supports validation of data at the time of reading it. In this way, NoSQL data
    stores support storage of huge data velocity due to the relaxation of basic data
    validation at the time of writing data. There are about 150 NoSQL data stores
    in the market today. Each of these NoSQL data stores has some unique features
    to offer. Some popular NoSQL data stores are HBase, Cassandra, MongoDB, Druid,
    Apache Kudu, and Accumulo, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: You can get a detailed list of all types of NoSQL databases at [http://nosql-database.org/](http://nosql-database.org/).
  prefs: []
  type: TYPE_NORMAL
- en: HBase is a popular NoSQL database used by many big companies such as Facebook,
    Google, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Differences between HDFS and HBase
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following explains the key difference between HDFS and HBase. Hadoop is
    built on top of HDFS, which has support for storing large volumes (petabytes)
    of datasets. These datasets are accessed using batch jobs, by using MapReduce
    algorithms. In order to find a data element in such a huge dataset, the entire
    dataset needs to be scanned. HBase, on the other hand, is built on top of HDFS
    and provides fast record lookups (and updates) for large tables. HBase internally
    puts your data in indexed StoreFiles that exist on HDFS for high-speed lookup.
  prefs: []
  type: TYPE_NORMAL
- en: Differences between Hive and HBase
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HBase is a database management system; it supports both transaction processing
    and analytical processing. Hive is a data warehouse system, which can be used
    only for analytical processing. HBase supports low latency and random data access
    operations. Hive only supports batch processing, which leads to high latency.
    HBase does not support any SQL interface to interact with the table data. You
    may have to write Java code to read and write data to HBase tables. At times,
    Java code becomes very complex to process data sets involving joins of multiple
    data sets. But Hive supports very easy access with SQL, which makes it very easy
    to read and write data to its tables. In HBase, data modeling involves flexible
    data models and column-oriented data storage, which must support data denormalization.
    The columns of HBase tables are decided at the time of writing data into the tables.
    In Hive, the data model involves tables with a fixed schema like an RDBMS data
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Key features of HBase
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are a few key features of HBase:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sorted rowkeys**: In HBase, data processing is down with three basic operations/APIs:
    get, put, and scan. All three of these APIs access data using rowkeys to ensure
    smooth data access. As scans are done over a range of rows, HBase lexicographically
    orders rows according to their rowkeys. Using these sorted rowkeys, a scan can
    be defined simply from its start and stop rowkeys. This is extremely powerful
    to get all relevant data in a single database call. The application developer
    can design a system to access recent datasets by querying recent rows based on
    their timestamp as all rows are stored in a table in sorted order based on the
    latest timestamp.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Control data sharding**: HBase Table rowkey strongly influences data sharding.
    Table data is sorted in ascending order by rowkey, column families, and column
    key. A solid rowkey design is very important to ensure data is evenly distributed
    across the Hadoop cluster. As rowkeys determine the sort order of a table''s row,
    each region in the table ends up being responsible for the physical storage of
    a part of the row key space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strong consistency**: HBase favors consistency over availability. It also
    supports ACID-level semantics on a per row basis. It, of course, impacts the write
    performance, which will tend to be slower. Overall, the trade-off plays in favor
    of the application developer, who will have the guarantee that the data store
    always the right value of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low latency processing**: HBase supports fast, random reads and writes to
    all data stored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: HBase supports any type—structured, semi-structured, unstructured.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reliability**: HBase table data block is replicated multiple times to ensure
    protection against data loss. HBase also supports fault tolerance. The table data
    is always available for processing even in case of failure of any regional server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HBase data model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These are the key components of an HBase data model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table**: In HBase, data is stored in a logical object, called **table**,
    that has multiple rows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Row**: A row in HBase consists of a row key and one or more columns. The
    row key sorts rows. The goal is to store data in such a way that related rows
    are near each other. The row key can be a combination of one of more columns.
    The row key is like the primary key of the table, which must be unique. HBase
    uses row keys to find data in a column. For example, `customer_id` can be a row
    key for the `customer` table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Column**: A column in HBase consists of a column family and a column qualifier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Column qualifier**: It is the column name of a table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cell**: This is a combination of row, column family, and column qualifier,
    and contains a value and a timestamp which represents the value''s version.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Column family**: It is a collection of columns that are co-located and stored
    together, often for performance reasons. Each column family has a set of storage
    properties, such as cached, compression, and data encodation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difference between RDBMS table and column - oriented data store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We all know how data is stored in any RDBMS table. It looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ID** | `Column_1` | `Column_2` | `Column_3` | `Column_4` |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | A | 11 | P | XX |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | B | 12 | Q | YY |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | C | 13 | R | ZZ |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | D | 14 | S | XX1 |'
  prefs: []
  type: TYPE_TB
- en: 'The column ID is used as a unique/primary key of the table to access data from
    other columns of the table. But in a column-oriented data store like HBase, the
    same table is divided into key and value and is stored like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Key** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| **Row** | **Column** | **Column Value** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | `Column_1` | `A` |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | `Column_2` | `11` |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | `Column_3` | `P` |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | `Column_4` | `XX` |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | `Column_1` | `B` |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | `Column_2` | `12` |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | `Column_3` | `Q` |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | `Column_4` | `YY` |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | `Column_1` | `C` |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | `Column_2` | `13` |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | `Column_3` | `R` |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | `Column_4` | `ZZ` |'
  prefs: []
  type: TYPE_TB
- en: 'In HBase, each table is a sorted map format, where each key is sorted in ascending
    order. Internally, each key and value is serialized and stored on the disk in
    byte array format. Each column value is accessed by its corresponding key. So,
    in the preceding table, we define a key, which is a combination of two columns,
    *row + column*. For example, in order to access the `Column_1` data element of
    row 1, we have to use a key, row 1 + `column_1`. That''s the reason the row key
    design is very crucial in HBase. Before creating the HBase table, we have to decide
    a column family for each column. A column family is a collection of columns, which
    are co-located and stored together, often for performance reasons. Each column
    family has a set of storage properties, such as cached, compression, and data
    encodation. For example, in a typical `CUSTOMER` table, we can define two column
    families, namely `cust_profile` and `cust_address`. All columns related to the
    customer address are assigned to the column family `cust_address`; all other columns,
    namely `cust_id`, `cust_name`, and `cust_age`, are assigned to the column family
    `cust_profile`. After assigning the column families, our sample table will look
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Key** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| **Row** | **Column** | **Column family** | **Value** | **Timestamp** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | `Column_1` | `cf_1` | `A` | `1407755430` |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | `Column_2` | `cf_1` | `11` | `1407755430` |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | `Column_3` | `cf_1` | `P` | `1407755430` |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | `Column_4` | `cf_2` | `XX` | `1407755432` |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | `Column_1` | `cf_1` | `B` | `1407755430` |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | `Column_2` | `cf_1` | `12` | `1407755430` |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | `Column_3` | `cf_1` | `Q` | `1407755430` |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | `Column_4` | `cf_2` | `YY` | `1407755432` |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | `Column_1` | `cf_1` | `C` | `1407755430` |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | `Column_2` | `cf_1` | `13` | `1407755430` |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | `Column_3` | `cf_1` | `R` | `1407755430` |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | `Column_4` | `cf_2` | `ZZ` | `1407755432` |'
  prefs: []
  type: TYPE_TB
- en: While inserting data into a table, HBase will automatically add a timestamp
    for each version of the cell.
  prefs: []
  type: TYPE_NORMAL
- en: HBase architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we want to read data from an HBase table, we have to give an appropriate
    row ID, and HBase will perform a lookup based on the given row ID. HBase uses
    the following sorted nested map to return the column value of the row ID: row
    ID a column family, a column at timestamp, and value. HBase is always deployed
    on Hadoop. The following is a typical installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b1fd55d-59df-48b3-b0a5-d5aacf0c836e.png)'
  prefs: []
  type: TYPE_IMG
- en: It is a master server of the HBase cluster and is responsible for the administration,
    monitoring, and management of RegionServers, such as assignment of regions to
    RegionServer, region splits, and so on. In a distributed cluster, the HMaster
    typically runs on the Hadoop NameNode.
  prefs: []
  type: TYPE_NORMAL
- en: 'ZooKeeper is a coordinator of HBase cluster. HBase uses ZooKeeper as a distributed
    coordination service to maintain server state in the cluster. ZooKeeper maintains
    which servers are alive and available, and provides server failure notification. RegionServer
    is responsible for management of regions. RegionServer is deployed on DataNode.
    It serves data for reads and writes. RegionServer is comprised of the following
    additional components:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Regions: HBase tables are divided horizontally by row key range into regions.
    A region contains all rows in the table between the region''s start key and end
    key. **Write-ahead logging** (**WAL**) is used to store new data that has not
    yet stored on disk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MemStore is a write cache. It stores new data that has not yet been written
    to disk. It is sorted before writing to disk. There is one MemStore per column
    family per region. Hfile stores the rows as sorted key/values on disk/HDFS:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/79ed20fa-ef32-4b86-9fc6-d0cb11127b8f.png)'
  prefs: []
  type: TYPE_IMG
- en: HBase architecture in a nutshell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The HBase cluster is comprised of one active master and one or more backup master
    servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cluster has multiple RegionServers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The HBase table is always large and rows are divided into partitions/shards
    called **regions**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each RegionServer hosts one or many regions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The HBase catalog is known as META table, which stores the locations of table
    regions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ZooKeeper stores the locations of the META table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During a write, the client sends the put request to the HRegionServer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data is written to WAL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then data is pushed into MemStore and an acknowledgement is sent to the client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once enough data is accumulated in MemStore, it flushes data to the Hfile on
    HDFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The HBase compaction process activates periodically to merge multiple HFiles
    into one Hfile (called **compaction**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HBase rowkey design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Rowkey design is a very crucial part of HBase table design. During key design,
    proper care must be taken to avoid hotspotting. In case of poorly designed keys,
    all of the data will be ingested into just a few nodes, leading to cluster imbalance.
    Then, all the reads have to be pointed to those few nodes, resulting in slower
    data reads. We have to design a key that will help load data equally to all nodes
    of the cluster. Hotspotting can be avoided by the following techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Key salting**: It means adding an arbitrary value at the beginning of the
    key to make sure that the rows are distributed equally among all the table regions.
    Examples are `aa-customer_id`, `bb-customer_id`, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Key hashing**: The key can be hashed and the hashing value can be used as
    a rowkey, for example, `HASH(customer_id)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Key with reverse timestamp**: In this technique, you have to define a regular
    key and then attach a reverse timestamp to it. The timestamp has to be reversed
    by subtracting it from any arbitrary maximum value and then attached to the key.
    For example, if `customer_id` is your row ID, the new key will be `customer_id`
    + reverse timestamp.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the guidelines while designing a HBase table:'
  prefs: []
  type: TYPE_NORMAL
- en: Define no more than two column families per table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep column family names as small as possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep column names as small as possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep the rowkey length as small as possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not set the row version at a high level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The table should not have more than 100 regions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example 4 – loading data from MySQL table to HBase table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the same `customer` table that we created earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Start HBase and create a `customer` table in HBase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Load MySQL `customer` table data in HBase using Sqoop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify the HBase table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: You must see all 11 rows in the HBase table.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5 – incrementally loading data from MySQL table to HBase table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Insert a new customer and update the existing one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Example 6 – Load the MySQL customer changed data into the HBase table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we have used the `InsUpd_on` column as our ETL date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Example 7 – Hive HBase integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will access the HBase `customer` table using the Hive external table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how data is stored and accessed using a Hadoop SQL interface
    called Hive. We studied various partitioning and indexing strategies in Hive.
    The working examples helped us to understand JSON data access and schema evolution
    using Avro in Hive. In the second section of the chapter, we studied a NoSQL data
    store called HBase and its difference with respect to RDBMS. The row design of
    the HBase table is very crucial to balancing reads and writes to avoid region
    hotspots. One has to keep in mind the HBase table design best practices discussed
    in this chapter. The working example shows the easier paths of data ingestions
    into an HBase table and its integration with Hive.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take a look at tools and techniques for designing
    real-time data analytics.
  prefs: []
  type: TYPE_NORMAL
