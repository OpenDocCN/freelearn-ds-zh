- en: Debugging and Profiling Your CUDA Code
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调试和性能分析您的 CUDA 代码
- en: In this chapter, we will finally learn how to debug and profile our GPU code
    using several different methods and tools. While we can easily debug pure Python
    code using IDEs such as Spyder and PyCharm, we can't use these tools to debug
    the actual GPU code, remembering that the GPU code itself is written in CUDA-C
    with PyCUDA providing an interface. The first and easiest method for debugging
    a CUDA kernel is the usage of `printf` statements, which we can actually call
    directly in the middle of a CUDA kernel to print to the standard output. We will
    see how to use `printf` in the context of CUDA and how to apply it effectively
    for debugging.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将最终学习如何使用几种不同的方法和工具来调试和性能分析我们的 GPU 代码。虽然我们可以轻松地使用像 Spyder 和 PyCharm 这样的
    IDE 调试纯 Python 代码，但我们不能使用这些工具来调试实际的 GPU 代码，记住 GPU 代码本身是用 CUDA-C 编写的，而 PyCUDA 提供了一个接口。调试
    CUDA 内核的第一个也是最简单的方法是使用 `printf` 语句，我们实际上可以在 CUDA 内核的中间直接调用它以打印到标准输出。我们将看到如何在 CUDA
    的上下文中使用 `printf` 以及如何有效地应用它进行调试。
- en: Next, we will fill in some of the gaps in our CUDA-C programming so that we
    can directly write CUDA programs within the NVIDIA Nsight IDE, which will allow
    us to make test cases in CUDA-C for some of the code we have been writing. We
    will take a look at how to compile CUDA-C programs, both from the command line
    with `nvcc` and also with the Nsight IDE. We will then see how to debug within
    Nsight and use Nsight to understand the CUDA lockstep property. Finally, we will
    have an overview of the NVIDIA command line and Visual Profilers for profiling
    our code.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将填补我们 CUDA-C 编程中的一些空白，以便我们可以在 NVIDIA Nsight IDE 中直接编写 CUDA 程序，这将使我们能够为一些我们一直在编写的代码编写
    CUDA-C 测试用例。我们将探讨如何编译 CUDA-C 程序，无论是使用命令行中的 `nvcc` 还是使用 Nsight IDE。然后，我们将了解如何在
    Nsight 中进行调试并使用 Nsight 来理解 CUDA 锁步属性。最后，我们将概述 NVIDIA 命令行和 Visual Profilers，以分析我们的代码。
- en: 'The learning outcomes for this chapter include the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的学习成果包括以下内容：
- en: Using `printf` effectively as a debugging tool for CUDA kernels
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有效地使用 `printf` 作为 CUDA 内核的调试工具
- en: Writing complete CUDA-C programs outside of Python, especially for creating
    test cases for debugging
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 之外编写完整的 CUDA-C 程序，特别是为创建调试测试用例
- en: Compiling CUDA-C programs on the command line with the `nvcc` compiler
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `nvcc` 编译器在命令行上编译 CUDA-C 程序
- en: Developing and debugging CUDA programs with the NVIDIA Nsight IDE
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 NVIDIA Nsight IDE 开发和调试 CUDA 程序
- en: Understanding the CUDA warp lockstep property and why we should avoid branch
    divergence within a single CUDA warp
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 CUDA warp 锁步属性以及为什么我们应该避免单个 CUDA warp 中的分支发散
- en: Learn to effectively use the NVIDIA command line and Visual Profilers for GPU
    code
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习有效地使用 NVIDIA 命令行和 Visual Profilers 进行 GPU 代码性能分析
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: A Linux or Windows 10 PC with a modern NVIDIA GPU (2016—onward) is required
    for this chapter, with all necessary GPU drivers and the CUDA Toolkit (9.0–onward)
    installed. A suitable Python 2.7 installation (such as Anaconda Python 2.7) with
    the PyCUDA module is also required.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本章，需要一个配备现代 NVIDIA GPU（2016 年及以后）的 Linux 或 Windows 10 PC，并安装所有必要的 GPU 驱动程序和
    CUDA Toolkit（9.0 及以后版本）。还需要一个合适的 Python 2.7 安装（例如 Anaconda Python 2.7），并包含 PyCUDA
    模块。
- en: This chapter's code is also available on GitHub at [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码也可在 GitHub 上找到：[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)。
- en: For more information about the prerequisites, check the *Preface* of this book,
    and for the software and hardware requirements, check the README in [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 关于先决条件的更多信息，请参阅本书的**前言**，有关软件和硬件要求，请查看 [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)
    中的 README。
- en: Using printf from within CUDA kernels
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 CUDA 内核中使用 `printf`
- en: 'It may come as a surprise, but we can actually print text to the standard output
    from directly within a CUDA kernel; not only that, each individual thread can
    print its own output. This will come in particularly handy when we are debugging
    our kernels, as we may need to monitor the values of particular variables or computations
    at particular points in our code and it will also free us from the shackles of
    using a debugger to go through step by step. Printing output from a CUDA kernel
    is done with none other than the most fundamental function in all of C/C++ programming,
    the function that most people will learn when they write their first `Hello world`
    program in C: `printf`. Of course, `printf` is the standard function that prints
    a string to the standard output, and is really the equivalent in the C programming
    language of Python''s `print` function.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会让人惊讶，但我们可以实际上从CUDA内核中直接打印文本到标准输出；不仅如此，每个单独的线程都可以打印它自己的输出。这在我们调试内核时尤其有用，因为我们可能需要在代码的特定点监控特定变量或计算的值，这也会使我们摆脱使用调试器逐步进行的束缚。从CUDA内核打印输出是通过C/C++编程中最基本的功能完成的，这是大多数人当他们用C编写第一个`Hello
    world`程序时会学习到的功能：`printf`。当然，`printf`是打印字符串到标准输出的标准函数，在C编程语言中相当于Python的`print`函数。
- en: 'Let''s now briefly review how to use `printf` before we see how to use it in
    CUDA. The first thing to remember is that `printf` always takes a string as its
    first parameter; so printing "Hello world!" in C is done with `printf("Hello world!\n");`.
    (Of course, `\n` indicates "new line" or "return", which moves the output in the
    Terminal to the next line.) `printf` can also take a variable number of parameters
    in the case that we want to print any constants or variables from directly within
    C: if we want to print the `123` integers to the output, we do this with `printf("%d",
    123);` (where `%d` indicates that an integer follows the string.)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们了解如何在CUDA中使用`printf`之前，让我们简要回顾一下如何使用它。首先要注意的是，`printf`总是将其第一个参数作为字符串；所以用C打印"Hello
    world!"是通过`printf("Hello world!\n");`完成的。（当然，`\n`表示"新行"或"返回"，将输出在终端移动到下一行。）如果我们要从C中直接打印任何常数或变量，`printf`也可以接受可变数量的参数：如果我们想打印`123`个整数到输出，我们这样做`printf("%d",
    123);`（其中`%d`表示字符串后面跟着一个整数。）
- en: 'Similarly, we use `%f`, `%e`, or `%g` to print floating-point values (where
    `%f` is the decimal notation, `%e` is the scientific notation, and `%g` is the
    shortest representation whether decimal or scientific). We can even print several
    values in a row, remembering to place these specifiers in the correct order: `printf("%d
    is a prime number, %f is close to pi, and %d is even.\n", 17, 3.14, 4);` will
    print "17 is a prime number, 3.14 is close to pi, and 4 is even." on the Terminal.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们使用`%f`、`%e`或`%g`来打印浮点值（其中`%f`是十进制表示，`%e`是科学记数法，`%g`是最短的表示，无论是十进制还是科学记数法）。我们甚至可以一行打印多个值，记得将这些说明符按正确顺序放置：`printf("%d
    is a prime number, %f is close to pi, and %d is even.\n", 17, 3.14, 4);`将在终端上打印"17
    is a prime number, 3.14 is close to pi, and 4 is even."。
- en: 'Now, nearly halfway through this book, we will finally embark on creating our
    first parallel `Hello world` program in CUDA! We start by importing the appropriate
    modules into Python and then write our kernel. We will start out by printing the
    thread and grid identification of each individual thread (we will only launch
    this in one-dimensional blocks and grids, so we only need the `x` values):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这本书已经快读了一半，我们终于要开始创建我们的第一个CUDA并行`Hello world`程序了！我们首先将适当的模块导入Python，然后编写我们的内核。我们将从打印每个单独线程的线程和网格标识符开始（我们只在一个维度的块和网格中启动，所以我们只需要`x`值）：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let's stop for a second and note that we wrote `\\n` rather than `\n`. This
    is due to the fact that the triple quote in Python itself will interpret `\n`
    as a "new line", so we have to indicate that we mean this literally by using a
    double backslash so as to pass the `\n` directly into the CUDA compiler.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂停一下，注意我们写了`\\n`而不是`\n`。这是因为Python本身的三重引号会解释`\n`为"新行"，所以我们必须通过使用双反斜杠来表明我们字面意思是这个，以便将`\n`直接传递给CUDA编译器。
- en: We will now print some information about the block and grid dimensions, but
    we want to ensure that it is printed after every thread has already finished its
    initial `printf` command. We can do this by putting in `__syncthreads();` to ensure
    each individual thread will be synchronized after the first `printf` function
    is executed.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将打印一些关于块和网格维度信息，但我们想确保它在每个线程完成其初始`printf`命令之后打印。我们可以通过在`__syncthreads();`中添加来确保每个单独的线程在第一个`printf`函数执行后都会同步。
- en: 'Now, we only want to print the block and grid dimensions to the terminal only
    once; if we just place `printf` statements here, every single thread will print
    out the same information. We can do this by having only one specified thread print
    to the output; let''s go with the 0th thread of the 0th block, which is the only
    thread that is guaranteed to exist no matter the block and grid dimensionality
    we choose. We can do this with a C `if` statement:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只想将块和网格维度打印到终端一次；如果我们只是在这里放置`printf`语句，每个线程都会打印出相同的信息。我们可以通过只有一个指定的线程打印到输出来实现；让我们选择第0个块的第0个线程，这是唯一一个无论我们选择的块和网格维度如何都保证存在的线程。我们可以使用C的`if`语句来实现这一点：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will now print the dimensionality of our block and grid and close up the
    `if` statement, and that will be the end of our CUDA kernel:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将打印块和网格的维度，并关闭`if`语句，这样我们的CUDA内核就结束了：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will now extract the kernel and then launch it over a grid consisting of
    two blocks, where each block has five threads:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将提取内核并在由两个块组成的网格上启动它，其中每个块有五个线程：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s run this right now (this program is also available in `hello-world_gpu.py`
    under `6` in the repository):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们立即运行这个程序（此程序也在存储库中的`hello-world_gpu.py`文件下的`6`目录中可用）：
- en: '![](img/2c945416-8c8a-4eae-8d57-ef1479dd2798.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c945416-8c8a-4eae-8d57-ef1479dd2798.png)'
- en: Using printf for debugging
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用printf进行调试
- en: Let's go over an example to see how we can approach debugging a CUDA kernel
    with `printf` with an example before we move on. There is no exact science to
    this method, but it is a skill that can be learned through experience. We will
    start with a CUDA kernel that is for matrix-matrix multiplication, but that has
    several bugs in it. (The reader is encouraged to go through the code as we go
    along, which is available as the `broken_matrix_ker.py` file in the `6` directories
    within the repository.)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们通过一个例子来了解如何使用`printf`调试CUDA内核。这种方法没有精确的科学，但它是可以通过经验学会的技能。我们将从一个用于矩阵-矩阵乘法的CUDA内核开始，但其中存在几个错误。（鼓励读者在阅读过程中查看代码，该代码作为存储库中`6`目录下的`broken_matrix_ker.py`文件提供。）
- en: 'Let''s briefly review matrix-matrix multiplication before we continue. Suppose
    we have two matrices ![](img/0e03608a-7c0c-4629-b61b-ec90b15d0cf7.png), *A* and
    *B*, and we multiply these together to get another matrix, *C*, of the same size
    as follows: ![](img/60101910-7794-4484-b69f-7ddaad3994db.png). We do this by iterating
    over all tuples ![](img/f550491f-6b50-49dd-be38-817b363525b9.png) and setting
    the value of ![](img/54c310e8-02c5-4bbc-b645-9f33465d4e77.png) to the dot product
    of the *i*^(th) row of *A* and the *j*^(th) column of *B*: ![](img/615e6730-f5b3-48d2-9ac2-fb9a7dc535de.png).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们简要回顾一下矩阵-矩阵乘法。假设我们有两个矩阵 ![](img/0e03608a-7c0c-4629-b61b-ec90b15d0cf7.png)，*A*和*B*，我们将它们相乘得到另一个与它们大小相同的矩阵*C*，如下所示：![](img/60101910-7794-4484-b69f-7ddaad3994db.png)。我们通过遍历所有元组
    ![](img/f550491f-6b50-49dd-be38-817b363525b9.png)并将 ![](img/54c310e8-02c5-4bbc-b645-9f33465d4e77.png)的值设置为*A*的第*i*行与*B*的第*j*列的点积来实现这一点：![](img/615e6730-f5b3-48d2-9ac2-fb9a7dc535de.png)。
- en: 'In other words, we set each *i, j* element in the output matrix *C* as follows:
    ![](img/8870cb22-35f8-4612-aa78-b1cc92fd38f9.png)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们将输出矩阵*C*中的每个*i, j*元素设置为如下：![](img/8870cb22-35f8-4612-aa78-b1cc92fd38f9.png)
- en: Suppose we already wrote a kernel that is to perform matrix-matrix multiplication,
    which takes in two arrays representing the input matrices, an additional pre allocated
    float array that the output will be written to, and an integer that indicates
    the height and width of each matrix (we will assume that all matrices are the
    same size and square-shaped). These matrices are all to be represented as one-dimensional
    `float *` arrays in a row-wise one-dimensional layout. Furthermore, this will
    be implemented so that each CUDA thread will handle a single row/column tuple
    in the output matrix.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经编写了一个内核，该内核将执行矩阵-矩阵乘法，它接受两个表示输入矩阵的数组，一个额外的预分配的浮点数组，该数组将写入输出，以及一个整数，表示每个矩阵的高度和宽度（我们将假设所有矩阵的大小相同且为方形）。所有这些矩阵都应表示为一维`float
    *`数组，以行向一维布局。此外，这将实现为每个CUDA线程将处理输出矩阵中的一个行/列元组。
- en: 'We make a small test case and check it against the output of the matrix multiplication
    in CUDA, and it fails as an assertion check on two 4 x 4 matrices, as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们制作了一个小的测试用例，并将其与CUDA中矩阵乘法的输出进行对比，结果在两个4 x 4矩阵的断言检查中失败了，如下所示：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We will run this program right now, and unsurprisingly get the following output:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将立即运行这个程序，不出所料，得到以下输出：
- en: '![](img/7fb29cbf-af3a-4fd1-a75a-cd05d3f56a44.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7fb29cbf-af3a-4fd1-a75a-cd05d3f56a44.png)'
- en: 'Let''s now look at the CUDA C code, which consists of a kernel and a device
    function:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看CUDA C代码，它由一个内核和一个设备函数组成：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Our goal is to place `printf` invocations intelligently throughout our CUDA
    code so that we can monitor a number of appropriate values and variables in the
    kernel and device function; we should also be sure to print out the thread and
    block numbers alongside these values at every `printf` invocation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目的是在CUDA代码中智能地放置`printf`调用，以便我们可以监控内核和设备函数中的多个适当的值和变量；我们还应该确保在每个`printf`调用中打印出线程和块号。
- en: 'Let''s start at the entry point of our kernel. We see two variables, `row`
    and `col`, so we should check these right away. Let''s put the following line
    right after we set them (since this is parallelized over two dimensions, we should
    print the *x* and *y* values of `threadIdx` and `blockIdx`):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从内核的入口点开始。我们看到两个变量，`row`和`col`，因此我们应该立即检查这些。让我们在设置它们之后立即放置以下行（由于这是在两个维度上并行化的，我们应该打印`threadIdx`和`blockIdx`的*x*和*y*值）：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Running the code again, we get this output:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 再次运行代码，我们得到以下输出：
- en: '![](img/579c6bb8-3bee-4788-b606-aff8d5d15812.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/579c6bb8-3bee-4788-b606-aff8d5d15812.png)'
- en: 'There are two things that are immediately salient: that there are repeated
    values for row and column tuples (every individual tuple should be represented
    only once), and that the row and column values never exceed two, when they both
    should reach three (since this unit test is using 4 x 4 matrices). This should
    indicate to us that we are calculating the row and column values wrongly; indeed,
    we are forgetting to multiply the `blockIdx` values by the `blockDim` values to
    find the objective row/column values. We fix this as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个问题立即显得突出：行和列元组的值有重复（每个单独的元组应该只表示一次），并且当它们都应该达到三个时，行和列的值从未超过两个。这应该提示我们，我们在计算行和列的值时是错误的；确实，我们忘记将`blockIdx`值乘以`blockDim`值来找到目标行/列值。我们按照以下方式修复这个问题：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If we run the program again, though, we still get an assertion error. Let''s
    keep our original `printf` invocation in place, so we can monitor the values as
    we continue. We see that there is an invocation to a device function in the kernel,
    `rowcol_dot`, so we decide to look into there. Let''s first ensure that the variables
    are being passed into the device function correctly by putting this `printf` invocation
    at the beginning:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，如果我们再次运行程序，我们仍然得到一个断言错误。让我们保留原始的`printf`调用，这样我们可以在继续的过程中监控这些值。我们看到内核中有一个对设备函数的调用，`rowcol_dot`，因此我们决定调查那里。让我们首先通过在开始处放置这个`printf`调用来确保变量被正确地传递到设备函数中：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'When we run our program, even more lines will come out, however, we will see
    one that says—`threadIdx.x,y: 0,0 blockIdx.x,y: 1,0 -- row is 2, col is 0.` and
    yet another that says—`threadIdx.x,y: 0,0 blockIdx.x,y: 1,0 -- row is 0, col is
    2, N is 4`. By the `threadIdx` and `blockIdx` values, we see that this is the
    same thread in the same block, but with the `row` and `col` values reversed. Indeed,
    when we look at the invocation of the `rowcol_dot` device function, we see that
    `row` and `col` are indeed reversed from that in the declaration of the device
    function. We fix this, but when we run the program again, we get yet another assertion
    error.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '当我们运行我们的程序时，将输出更多行，然而，我们会看到一条写着——`threadIdx.x,y: 0,0 blockIdx.x,y: 1,0 -- row
    is 2, col is 0.`的行，还有另一条写着——`threadIdx.x,y: 0,0 blockIdx.x,y: 1,0 -- row is 0,
    col is 2, N is 4`的行。通过`threadIdx`和`blockIdx`的值，我们看到这是同一块中的同一线程，但`row`和`col`的值是相反的。确实，当我们查看`rowcol_dot`设备函数的调用时，我们看到`row`和`col`确实与设备函数声明中的相反。我们修复了这个问题，但当我们再次运行程序时，我们得到了另一个断言错误。'
- en: 'Let''s place another `printf` invocation in the device function, within the
    `for` loop; this, of course, is the *dot product* that is to perform a dot product
    between rows of matrix `A` with columns of matrix `B`. We will check the values
    of the matrices we are multiplying, as well as `k`; we will also only look at
    the values of the very first thread, or else we will get an incoherent mess of
    an output:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在设备函数中放置另一个`printf`调用，在`for`循环内；这当然是要执行矩阵`A`的行与矩阵`B`的列之间的点积。我们将检查我们正在乘的矩阵的值以及`k`的值；我们也将只查看第一个线程的值，否则我们将得到一个混乱的输出：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s look at the values of the `A` and `B` matrices that are set up for our
    unit tests before we continue:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们看看为我们的单元测试设置的`A`和`B`矩阵的值：
- en: '![](img/b2580d2a-f34e-4d83-abbd-43ebc163aba7.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2580d2a-f34e-4d83-abbd-43ebc163aba7.png)'
- en: 'We see that both matrices vary when we switch between columns but are constant
    when we change between rows. Therefore, by the nature of matrix multiplication,
    the values of matrix `A` should vary across `k` in our `for` loop, while the values
    of `B` should remain constant. Let''s run the program again and check the pertinent
    output:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，当我们切换列时，两个矩阵都会变化，但当我们改变行时，它们是恒定的。因此，根据矩阵乘法的性质，矩阵`A`的值应该在`for`循环中的`k`上变化，而矩阵`B`的值应该保持不变。让我们再次运行程序并检查相关的输出：
- en: '![](img/47f4367a-3282-4718-9ec9-e818e0a16ffb.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47f4367a-3282-4718-9ec9-e818e0a16ffb.png)'
- en: 'So, it appears that we are not accessing the elements of the matrices in a
    correct way; remembering that these matrices are stored in a row-wise format,
    we modify the indices so that their values are accessed in the proper manner:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，看起来我们没有以正确的方式访问矩阵的元素；记住这些矩阵是以行为主序存储的，我们修改索引以便以正确的方式访问它们的值：
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Running the program again will yield no assertion errors. Congratulations, you
    just debugged a CUDA kernel using the only `printf`!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 再次运行程序将不会产生断言错误。恭喜你，你刚刚使用唯一的`printf`调试了一个CUDA内核！
- en: Filling in the gaps with CUDA-C
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用CUDA-C填补空白
- en: We will now go through the very basics of how to write a full-on CUDA-C program.
    We'll start small and just translate the *fixed* version of the little matrix
    multiplication test program we just debugged in the last section to a pure CUDA-C
    program, which we will then compile from the command line with NVIDIA's `nvcc`
    compiler into a native Windows or Linux executable file (we will see how to use
    the Nsight IDE in the next section, so we will just be doing this with only a
    text editor and the command line for now). Again, the reader is encouraged to
    look at the code we are translating from Python as we go along, which is available
    as the `matrix_ker.py` file in the repository.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '我们现在将介绍如何编写一个完整的CUDA-C程序的基础知识。我们将从小处着手，将上一节中调试的小矩阵乘法测试程序的*固定*版本翻译成纯CUDA-C程序，然后我们将使用NVIDIA的`nvcc`编译器从命令行编译成本机Windows或Linux可执行文件（我们将在下一节中看到如何使用Nsight
    IDE，所以现在我们只使用文本编辑器和命令行）。再次鼓励读者在翻译过程中查看我们从Python翻译的代码，这些代码作为存储库中的`matrix_ker.py`文件提供。 '
- en: Now, let's open our favorite text editor and create a new file entitled `matrix_ker.cu`.
    The extension will indicate that this is a CUDA-C program, which can be compiled
    with the `nvcc` compiler.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打开我们最喜欢的文本编辑器，创建一个名为`matrix_ker.cu`的新文件。扩展名将表明这是一个CUDA-C程序，可以使用`nvcc`编译器进行编译。
- en: CUDA-C program and library source code filenames always use the `.cu` file extension.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA-C程序和库源代码文件名总是使用`.cu`文件扩展名。
- en: Let's start at the beginning—as Python uses the `import` keyword at the beginning
    of a program for libraries, we recall the C language uses `#include`. We will
    need to include a few import libraries before we continue.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一开始——就像Python在程序开始时使用`import`关键字导入库一样，我们回忆C语言使用`#include`。在我们继续之前，我们需要包含一些导入库。
- en: 'Let''s start with these:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从这些开始：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let''s briefly think about what we need these for: `cuda_runtime.h` is the
    header file that has the declarations of all of the particular CUDA datatypes,
    functions, and structures that we will need for our program. We will need to include
    this for any pure CUDA-C program that we write. `stdio.h`, of course, gives us
    all of the standard I/O functions for the host such as `printf`, and we need `stdlib.h`
    for using the `malloc` and `free` dynamic memory allocation functions on the host.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要思考一下我们需要这些做什么：`cuda_runtime.h`是包含所有特定CUDA数据类型、函数和结构的声明头文件，我们将在我们的程序中需要这些。我们将需要包含这个头文件来编写任何纯CUDA-C程序。`stdio.h`当然提供了所有标准I/O函数，如`printf`，我们需要`stdlib.h`来在主机上使用`malloc`和`free`动态内存分配函数。
- en: Remember to always put `#include <cuda_runtime.h>` at the beginning of every
    pure CUDA-C program!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在每一个纯CUDA-C程序的开头都要包含`#include <cuda_runtime.h>`！
- en: 'Now, before we continue, we remember that we will ultimately have to check
    the output of our kernel with a correct known output, as we did with NumPy''s
    `allclose` function. Unfortunately, we don''t have a standard or easy-to-use numerical
    math library in C as Python has with NumPy. More often than not, it''s just easier
    to write your own equivalent function if it''s something simple, as in this case.
    This means that we will now explicitly have to make our own equivalent to NumPy''s
    `allclose`. We will do so as such: we will use the `#define` macro in C to set
    up a value called `_EPSILON`, which will act as a constant to indicate the minimum
    value between the output and expected output to be considered the same, and we
    will also set up a macro called `_ABS`, which will tell us the absolute difference
    between two numbers. We do so as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们继续之前，我们记得我们最终必须用正确的已知输出检查内核的输出，就像我们使用NumPy的`allclose`函数那样。不幸的是，C语言中没有像Python中的NumPy那样的标准或易于使用的数值数学库。通常情况下，如果它是简单的东西，就像这个例子一样，自己编写等效函数会更容易。这意味着我们现在必须显式地创建NumPy的`allclose`的等效函数。我们将这样做：我们将使用C语言的`#define`宏设置一个名为`_EPSILON`的值，它将作为一个常量来指示输出和预期输出之间的最小值，我们将设置一个名为`_ABS`的宏，它将告诉我们两个数字之间的绝对差值。我们这样做如下：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can now create our own version of `allclose`. This will take in two float
    pointers and an integer value, `len`. We loop through both arrays and check them:
    if any points differ by more than `_EPSILON`, we return -1, otherwise we return
    0 to indicate that the two arrays do indeed match.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以创建自己的`allclose`版本。这将接受两个浮点指针和一个整数值`len`。我们遍历两个数组并检查它们：如果任何点之间的差异超过`_EPSILON`，我们返回-1，否则我们返回0以指示两个数组确实匹配。
- en: 'We note one thing: since we are using CUDA-C, we precede the definition of
    the function with `__host__`, to indicate that this function is intended to be
    run on the CPU rather than on the GPU:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到一点：由于我们使用CUDA-C，我们在函数定义前加上`__host__`，以表明这个函数是打算在CPU上而不是在GPU上运行：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We now can cut and paste the device and kernel functions exactly as they appear
    in our Python version here:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将设备和内核函数直接复制粘贴，就像它们在我们的Python版本中显示的那样：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Again, in contrast with `__host__`, notice that the CUDA device function is
    preceded by `__device__`, while the CUDA kernel is preceded by `__global__`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，与`__host__`相比，请注意，CUDA设备函数前面是`__device__`，而CUDA内核前面是`__global__`。
- en: 'Now, as in any C program, we will need to write the `main` function, which
    will run on the host, where we will set up our test case and from which we explicitly
    launch our CUDA kernel onto GPU. Again, in contrast to vanilla C, we will have
    explicitly to specify that this is also to be run on the CPU with `__host__`:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，就像在任何一个C程序中一样，我们需要编写`main`函数，该函数将在主机上运行，在那里我们将设置我们的测试用例，并从那里显式地将我们的CUDA内核启动到GPU上。再次强调，与普通的C语言相比，我们必须显式地指定这也将在CPU上运行，使用`__host__`：
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The first thing we will have to do is select and initialize our GPU. We do
    so with `cudaSetDevice` as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先必须做的是选择和初始化我们的GPU。我们使用`cudaSetDevice`如下：
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`cudaSetDevice(0)` will select the default GPU. If you have multiple GPUs installed
    in your system, you can select and use them instead with `cudaSetDevice(1)`, `cudaSetDevice(2)`,
    and so on.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaSetDevice(0)`将选择默认的GPU。如果你在你的系统中安装了多个GPU，你可以使用`cudaSetDevice(1)`、`cudaSetDevice(2)`等来选择并使用它们。'
- en: 'We will now set up `N` as in Python to indicate the height/width of our matrix.
    Since our test case will consist only of 4 x 4 matrices, we set it to `4`. Since
    we will be working with dynamically allocated arrays and pointers, we will also
    have to set up a value that will indicate the number of bytes our test matrices
    will require. The matrices will consist of *N* x *N* floats, and we can determine
    the number of bytes required by a float with the `sizeof` keyword in C:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将`N`设置为Python中的值，以表示我们矩阵的高度/宽度。由于我们的测试用例将只包含4 x 4矩阵，我们将它设置为`4`。由于我们将使用动态分配的数组和指针，我们还将设置一个值来指示我们的测试矩阵所需的字节数。矩阵将包含*N*
    x *N*个浮点数，我们可以使用C中的`sizeof`关键字来确定所需的字节数：
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We now set up our test matrices as such; these will correspond exactly to the
    `test_a` and `test_b` matrices that we saw in our Python test program (notice
    how we use the `h_` prefix to indicate that these arrays are stored on the host,
    rather than on the device):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在以这种方式设置我们的测试矩阵；这些将正好对应于我们在Python测试程序中看到的`test_a`和`test_b`矩阵（注意我们如何使用`h_`前缀来表示这些数组存储在主机上，而不是在设备上）：
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We now set up another array, which will indicate the expected output of the
    matrix multiplication of the prior test matrices. We will have to calculate this
    explicitly and put these values into our C code. Ultimately, we will compare this
    to the GPU output at the end of the program, but let''s just set it up and get
    it out of the way:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在设置另一个数组，它将指示先前测试矩阵的矩阵乘法预期输出。我们将必须显式地计算这些值并将它们放入我们的C代码中。最终，我们将在程序结束时将其与GPU输出进行比较，但让我们先设置它并把它处理掉：
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We now declare some pointers for arrays that will live on the GPU, and for
    that we will copy the values of `h_A` and `h_B` and pointer to the GPU''s output.
    Notice how we just use standard float pointers for this. Also, notice the prefix
    `d_`— this is another standard CUDA-C convention that indicates that these will
    exist on the device:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在声明一些指向将存在于GPU上的数组的指针，为此我们将复制`h_A`和`h_B`的值以及指向GPU输出的指针。注意我们只是使用标准的浮点指针来做这件事。注意前缀`d_`——这是另一个标准的CUDA-C约定，表示这些将存在于设备上：
- en: '[PRE20]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we will allocate some memory on the device for `d_A` and `d_B` with `cudaMalloc`,
    which is almost the same as `malloc` in C; this is what PyCUDA `gpuarray` functions
    such as `empty` or `to_gpu` have been calling us invisibly to allocate memory
    arrays on the GPU throughout this book:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用`cudaMalloc`在设备上为`d_A`和`d_B`分配一些内存，这几乎与C中的`malloc`相同；这正是PyCUDA `gpuarray`函数如`empty`或`to_gpu`在整本书中无形中调用我们以在GPU上分配内存数组的方式：
- en: '[PRE21]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let''s think a bit about how this works: in C functions, we can get the address
    of a variable by preceding it with an ampersand (`&`); if you have an integer,
    `x`, we can get its address with `&x`. `&x` will be a pointer to an integer, so
    its type will be `int *`. We can use this to set values of parameters into a C
    function, rather than use only pure return values.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微思考一下它是如何工作的：在C函数中，我们可以通过在变量前加上一个与号（`&`）来获取变量的地址；如果你有一个整数`x`，我们可以用`&x`来获取它的地址。`&x`将是一个指向整数的指针，所以它的类型将是`int
    *`。我们可以使用这个来设置参数的值到一个C函数中，而不是只使用纯返回值。
- en: Since `cudaMalloc` sets the pointer through a parameter rather than with the
    return value (in contrast to the regular `malloc`), we have to use the ampersand
    operator, which will be a pointer to a pointer, as it is a pointer to a float
    pointer as here (`float **`). We have to typecast this value explicitly with the
    parenthesis since `cudaMalloc` can allocate arrays of any type. Finally, in the
    second parameter, we have to indicate how many bytes to allocate on the GPU; we
    already set up `num_bytes` previously to be the number of bytes we will need to
    hold a 4 x 4 matrix consisting of floats, so we plug this in and continue.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`cudaMalloc`通过参数设置指针而不是通过返回值（与常规的`malloc`不同），我们必须使用与号操作符，这将是一个指向指针的指针，因为它是一个指向浮点指针的指针，就像这里一样（`float
    **`）。由于`cudaMalloc`可以分配任何类型的数组，我们必须显式地使用括号来转换这个值。最后，在第二个参数中，我们必须指明在GPU上分配多少字节；我们之前已经设置了`num_bytes`，表示我们将需要多少字节来存储一个由浮点数组成的4
    x 4矩阵，所以我们将其插入并继续。
- en: 'We can now copy the values from `h_A` and `h_B` to `d_A` and `d_B` respectively
    with two invocations of the function `cudaMemcpy`, as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过两次调用 `cudaMemcpy` 函数，将 `h_A` 和 `h_B` 中的值分别复制到 `d_A` 和 `d_B` 中，如下所示：
- en: '[PRE22]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`cudaMemcpy` always takes a destination pointer as the first argument, a source
    pointer as the second, the number of bytes to copy as the third argument, and
    a final parameter. The last parameter will indicate if we are copying from the
    host to the GPU with `cudaMemcpyHostToDevice` , from the GPU to the host with
    `cudaMemcpyDeviceToHost`, or between two arrays on the GPU with `cudaMemcpyDeviceToDevice`.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaMemcpy` 总是接受一个目标指针作为第一个参数，一个源指针作为第二个参数，要复制的字节数作为第三个参数，以及一个最终参数。最后一个参数将指示我们是否使用
    `cudaMemcpyHostToDevice` 从主机复制到 GPU，使用 `cudaMemcpyDeviceToHost` 从 GPU 复制到主机，或者使用
    `cudaMemcpyDeviceToDevice` 在 GPU 上的两个数组之间复制。'
- en: 'We will now allocate an array to hold the output of our matrix multiplication
    on the GPU with another invocation of `cudaMalloc`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用另一个 `cudaMalloc` 调用来分配一个数组，用于在 GPU 上存储矩阵乘法的结果：
- en: '[PRE23]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we will have to have some memory set up on the host that will store
    the output of the GPU when we want to check the output of our kernel. Let''s set
    up a regular C float pointer and allocate memory with `malloc` as we would normally:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当我们想要检查内核的输出时，我们将在主机上设置一些内存来存储 GPU 的输出。让我们设置一个常规的 C float 指针，并使用 `malloc`
    分配内存，就像我们通常做的那样：
- en: '[PRE24]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, we are almost ready to launch our kernel. CUDA uses a data structure called
    `dim3` to indicate block and grid sizes for kernel launches; we will set these
    up as such, since we want a grid with a dimension of 2 x 2 and blocks that are
    also of a dimension of 2 x 2:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们几乎准备好启动我们的内核。CUDA 使用一个名为 `dim3` 的数据结构来指示内核启动的块和网格大小；我们将这样设置它们，因为我们想要一个
    2x2 维度的网格和同样维度的块：
- en: '[PRE25]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We are now ready to launch our kernel; we use the triple-triangle brackets
    to indicate to the CUDA-C compiler the block and grid sizes that the kernel should
    be launched over:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好启动我们的内核；我们使用三重三角形括号来指示 CUDA-C 编译器内核应该启动的块和网格大小：
- en: '[PRE26]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, of course, before we can copy the output of the kernel back to the host,
    we have to ensure that the kernel has finished executing. We do this by calling
    `cudaDeviceSynchronize`, which will block the host from issuing any more commands
    to the GPU until the kernel has finished execution:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在我们可以将内核的输出复制回主机之前，我们必须确保内核已经执行完成。我们通过调用 `cudaDeviceSynchronize` 来完成这个操作，这将阻止主机在内核执行完成之前向
    GPU 发送任何更多命令：
- en: '[PRE27]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We now can copy the output of our kernel to the array we''ve allocated on the
    host:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将内核的输出复制到我们在主机上分配的数组中：
- en: '[PRE28]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Again, we synchronize:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们进行同步：
- en: '[PRE29]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Before we check the output, we realize that we no longer need any of the arrays
    we allocated on the GPU. We free this memory by calling `cudaFree` on each array:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们检查输出之前，我们意识到我们不再需要我们在 GPU 上分配的任何数组。我们通过在每个数组上调用 `cudaFree` 来释放这些内存：
- en: '[PRE30]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We''re done with the GPU, so we call `cudaDeviceReset`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了 GPU 的工作，因此我们调用 `cudaDeviceReset`：
- en: '[PRE31]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, we finally check the output we copied onto the host with the `allclose`
    function we wrote at the beginning of this chapter. If the actual output doesn''t
    match the expected output, we print an error and return `-1`, otherwise, we print
    that it does match and we return 0\. We then put a closing bracket on our program''s
    `main` function:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用本章开头编写的 `allclose` 函数检查我们复制到主机上的输出。如果实际输出与预期输出不匹配，我们打印一个错误并返回 `-1`，否则，我们打印它们匹配，并返回
    `0`。然后，我们在程序的 `main` 函数上放置一个闭合括号：
- en: '[PRE32]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Notice that we make one final invocation to the standard C free function since
    we have allocated memory to `h_output` , in both cases.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于我们在 `h_output` 上分配了内存，我们在这里进行了最后一次调用标准 C 的 `free` 函数。
- en: 'We now save our file, and compile it into a Windows or Linux executable file
    from the command line with `nvcc matrix_ker.cu -o matrix_ker`. This should output
    a binary executable file, `matrix_ker.exe` (in Windows) or `matrix_ker` (in Linux).
    Let''s try compiling and running it right now:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们保存我们的文件，并使用命令行中的 `nvcc matrix_ker.cu -o matrix_ker` 将其编译成 Windows 或 Linux
    可执行文件。这应该输出一个二进制可执行文件，`matrix_ker.exe`（在 Windows 上）或 `matrix_ker`（在 Linux 上）。让我们现在尝试编译和运行它：
- en: '![](img/ada8e5c8-9443-40ce-81d9-03100f570855.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ada8e5c8-9443-40ce-81d9-03100f570855.png)'
- en: Congratulations, you've just created your first pure CUDA-C program! (This example
    is available as `matrix_ker.cu` in the repository, under `7`.)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你，你刚刚创建了你第一个纯 CUDA-C 程序！（此示例作为 `matrix_ker.cu` 存储在存储库中，位于 `7` 目录下。）
- en: Using the Nsight IDE for CUDA-C development and debugging
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Nsight IDE 进行 CUDA-C 开发和调试
- en: Let's now learn how to use the Nsight IDE for developing CUDA-C programs. We
    will see how to import the program we just wrote, and compile and debug it from
    within Nsight. Note that there are differences between the Windows and Linux versions
    of Nsight, since it is effectively a plugin of the Visual Studio IDE under Windows
    and in the Eclipse IDE under Linux. We will cover both in the following two subsections;
    feel free to skip whatever operating system does not apply to you here.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来学习如何使用 Nsight IDE 开发 CUDA-C 程序。我们将看到如何导入我们刚刚编写的程序，并在 Nsight 中编译和调试它。请注意，Nsight
    的 Windows 和 Linux 版本之间存在差异，因为它在 Windows 下是 Visual Studio IDE 的插件，在 Linux 下是 Eclipse
    IDE 的插件。我们将在以下两个小节中介绍这两个版本；如果你不适用某个操作系统，可以自由跳过。
- en: Using Nsight with Visual Studio in Windows
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Windows 中使用 Nsight 与 Visual Studio
- en: 'Open up Visual Studio, and click on File, then choose New | Project.... A window
    will pop up where you set the type of project: choose the NVIDIA drop-down item,
    and then choose CUDA 9.2:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 Visual Studio，然后点击文件，然后选择新建 | 项目.... 将弹出一个窗口，你可以设置项目的类型：选择 NVIDIA 下拉菜单项，然后选择
    CUDA 9.2：
- en: '![](img/e5b3c450-388f-44ac-9d77-8f7cf574bf69.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5b3c450-388f-44ac-9d77-8f7cf574bf69.png)'
- en: Give the project some appropriate name and then click OK. A project should appear
    in the solution explorer window with a simple premade CUDA test program, consisting
    of one source file, `kernel.cu`, which consists of a simple parallel add kernel
    with test code. If you want to see whether this compiles and runs, click the green
    right-pointing arrow at the top marked Local Windows Debugger. A Terminal should
    pop up with some text output from the kernel and then close immediately.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 给项目起一个合适的名字，然后点击确定。解决方案资源管理器窗口中应该会出现一个简单的预置 CUDA 测试程序，包含一个源文件 `kernel.cu`，它包含一个简单的并行加法内核和测试代码。如果你想查看它是否可以编译和运行，请点击
    IDE 顶部标记为本地 Windows 调试器的绿色右箭头。应该会弹出一个终端，显示内核的一些文本输出，然后立即关闭。
- en: If you have problems with a Windows Terminal-based application closing after
    you run it from Visual Studio, try adding `getchar();` to the end of the main
    function, which will keep the Terminal open until you press a key. (Alternatively,
    you can also use a debugger breakpoint at the end of the program.)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从 Visual Studio 运行基于 Windows Terminal 的应用程序后遇到问题，尝试在主函数的末尾添加 `getchar();`，这将使终端保持打开状态，直到你按下键。
    (或者，你还可以在程序末尾使用调试器断点。)
- en: Now, let's add the CUDA-C program we just wrote. In the Solution Explorer window,
    right-click `kernel.cu`, and click Remove on `kernel.cu`. Now, right-click on
    the project name, and choose Add, and then choose Existing item. We will now be
    able to select an existing file, so find where the path is to `matrix_ker.cu`
    and add it to the project. Click on the green arrow marked Local Windows Debugger
    at the top of the IDE and the program should compile and run, again in a Windows
    Terminal. So, that's it—we can set up and compile a complete CUDA program in Visual
    Studio now, just from those few steps.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们添加我们刚刚编写的 CUDA-C 程序。在解决方案资源管理器窗口中，右键单击 `kernel.cu`，然后点击 `kernel.cu` 上的移除。现在，右键单击项目名称，然后选择添加，然后选择现有项。现在我们将能够选择一个现有文件，所以找到
    `matrix_ker.cu` 的路径并将其添加到项目中。点击 IDE 顶部标记为本地 Windows 调试器的绿色箭头，程序应该会编译并运行，再次在 Windows
    Terminal 中。所以，就是这样——我们只需这些步骤就可以在 Visual Studio 中设置和编译完整的 CUDA 程序了。
- en: 'Let''s now see how to debug our CUDA kernel. Let''s start by adding one breakpoint
    to our code at the entry point of the kernel `matrix_mult_ker`, where we set the
    value of `row` and `col`. We can add this breakpoint by clicking on the gray column
    left of the line numbers on the window; a red dot should appear there for every
    breakpoint we add. (You can ignore any red squiggly lines that the Visual Studio
    editor may place under your code; this is due to the fact that CUDA is not a *native*
    language to Visual Studio):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '现在我们来看如何调试我们的 CUDA 内核。让我们首先在我们的代码中添加一个断点，在内核 `matrix_mult_ker` 的入口点，我们设置 `row`
    和 `col` 的值。我们可以通过点击窗口左侧行号左侧的灰色列来添加这个断点；对于每个添加的断点，那里应该会出现一个红色圆点。（你可以忽略 Visual Studio
    编辑器可能在你的代码下放置的任何红色波浪线；这是由于 CUDA 不是 Visual Studio 的 *原生* 语言）： '
- en: '![](img/b4408001-ab3d-42fb-9741-1e1a3c896491.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4408001-ab3d-42fb-9741-1e1a3c896491.png)'
- en: We can now start debugging. From the top menu, choose the Nsight drop-down menu
    and choose Start CUDA Debugging. There may be two options here, Start CUDA Debugging
    (Next-Gen) and Start CUDA Debugging (Legacy). It doesn't matter which one, but
    you may have issues with Next-Gen depending on your GPU; in that case, choose
    Legacy.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始调试。从顶部菜单中选择Nsight下拉菜单，然后选择开始CUDA调试。这里可能有两个选项，开始CUDA调试（下一代）和开始CUDA调试（旧版）。哪个都无所谓，但您可能根据您的GPU遇到Next-Gen的问题；在这种情况下，请选择Legacy。
- en: 'Your program should start up, and the debugger should halt at the breakpoint
    in our kernel that we just set. Let''s press *F10* to step over the line, and
    now see if the `row` variable gets set correctly. Let''s look at the Locals window
    in the Variable Explorer:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 您的程序应该启动，并且调试器应该在我们刚刚设置的内核断点处停止。让我们按*F10*来跳过这一行，现在看看`row`变量是否被正确设置。让我们查看变量探索器中的局部变量窗口：
- en: '![](img/4bbfaa46-46cb-44cc-ac0b-055dc9ce89ca.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4bbfaa46-46cb-44cc-ac0b-055dc9ce89ca.png)'
- en: 'We can see that we are currently in the very first thread in the very first
    block in the grid by checking the values of `threadIdx` and `blockIdx`; `row`
    is set to `0`, which does indeed correspond to the correct value. Now, let''s
    check the value of row for some different thread. To do this, we have to switch
    the **thread focus** in the IDE; we do this by clicking the Nsight drop-down menu
    above, then choosing Windows|CUDA Debug Focus.... A new menu should appear allowing
    you to choose a new thread and block. Change thread from 0, 0, 0 to 1, 0, 0 in
    the menu, and click OK:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们目前位于网格中第一个块中的第一个线程，通过检查`threadIdx`和`blockIdx`的值；`row`被设置为`0`，这确实对应于正确的值。现在，让我们检查不同线程的`row`值。为此，我们必须在IDE中切换**线程焦点**；我们通过点击Nsight下拉菜单上面的Nsight，然后选择窗口|CUDA调试焦点...来完成此操作。应该出现一个新菜单，允许您选择新的线程和块。在菜单中将线程从0,
    0, 0更改为1, 0, 0，然后点击确定：
- en: '![](img/c5f2b196-8ce2-45b6-807a-11f8d86fe86b.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c5f2b196-8ce2-45b6-807a-11f8d86fe86b.png)'
- en: 'When you check the variables again, you should see the correct value is set
    for `row` for this thread:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当您再次检查变量时，应该看到`row`变量为这个线程设置了正确的值：
- en: '![](img/f2f27821-5df5-4ea7-a847-385967ba54a0.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f2f27821-5df5-4ea7-a847-385967ba54a0.png)'
- en: In a nutshell, that is how you debug with Nsight in Visual Studio. We now have
    the basics of how to debug a CUDA program from Nsight/Visual Studio in Windows,
    and we can use all of the regular conventions as we would for debugging a regular
    Windows program as with any other IDE (setting breakpoints, starting the debugger,
    continue/resume, step over, step in, and step out). Namely, the main difference
    is you have to know how to switch between CUDA threads and blocks to check variables,
    otherwise, it's pretty much the same.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这就是在Visual Studio中使用Nsight进行调试的方法。我们现在有了如何在Windows中的Nsight/Visual Studio中调试CUDA程序的基础，我们可以像在常规Windows程序中一样使用所有常规约定进行调试（设置断点、启动调试器、继续/恢复、跳过、进入和退出）。主要区别在于您必须知道如何在不同CUDA线程和块之间切换以检查变量，否则基本上是相同的。
- en: Using Nsight with Eclipse in Linux
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Linux中使用Eclipse的Nsight
- en: 'We will now see how to use Nsight in Linux. You can open Nsight from either
    your desktop by selecting it or you can run it from a command line with the `nsight`
    command. The Nsight IDE will open. From the top of the IDE, click on File, then
    choose New... from the drop-down menu, and from there choose New CUDA C/C++ Project.
    A new window will appear, and from here choose CUDA Runtime Project. Give the
    project some appropriate name, and then click Next. You''ll be prompted to give
    further settings options, but the defaults will work fine for our purposes for
    now. (Be sure to note where the source folder and project paths will be located
    in the third and fourth screens here.) You''ll get to a final screen, where you
    can press Finish to create the project:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看到如何在Linux中使用Nsight。您可以通过选择它或使用`nsight`命令从命令行运行它来打开Nsight。Nsight IDE将打开。从IDE的顶部，点击文件，然后从下拉菜单中选择新建...，然后从那里选择新建CUDA
    C/C++项目。将出现一个新窗口，从这里选择CUDA运行时项目。给项目起一个合适的名字，然后点击下一步。您将需要提供进一步的设置选项，但默认设置对我们目前的目的来说已经足够好。（请注意，在第三和第四屏幕中，这里将位于源文件夹和项目路径的位置。）您将到达一个最终屏幕，在那里您可以按完成来创建项目：
- en: '![](img/e2f72961-a2fe-4c06-a4c2-2ab6653c140b.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e2f72961-a2fe-4c06-a4c2-2ab6653c140b.png)'
- en: Finally, you'll end up at a project view with your new project and some placeholder
    code open; as of CUDA 9.2, this will consist of a reciprocal kernel example.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您将到达一个项目视图，其中包含您的新项目和一些占位符代码打开；截至CUDA 9.2，这将是倒数第二个内核示例。
- en: 'We can now import our code. Either you can just use the editor in Nsight to
    delete all of the code in the default source file and cut and paste it in, or
    you can manually delete the file from the project''s source directory, manually
    copy the `matrix_ker.cu` file into the source directory, and then choose to refresh
    the source directory view in Nsight by selecting it and then pressing *F5*. You
    can now build the project with *Ctrl* + *B*, and run it with *F11*. The output
    of our program should appear within the IDE itself within the Console subwindow,
    as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以导入我们的代码。你可以只是使用 Nsight 中的编辑器删除默认源文件中的所有代码，并将其剪切粘贴进来，或者你可以手动从项目的源目录中删除文件，手动将
    `matrix_ker.cu` 文件复制到源目录中，然后通过选择它并按 *F5* 来在 Nsight 中刷新源目录视图。你现在可以通过按 *Ctrl* +
    *B* 构建项目，并通过按 *F11* 运行它。程序输出应该出现在 IDE 的控制台子窗口中，如下所示：
- en: '![](img/1e287f83-ed1a-416e-aca7-511b375c1568.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1e287f83-ed1a-416e-aca7-511b375c1568.png)'
- en: We can now set a breakpoint within our CUDA code; let's set it at the entry
    point of our kernel where the row value is set. We set the cursor onto that row
    in the Eclipse editor, and then press *Ctrl* + *Shift* + *B* to set it.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在我们的 CUDA 代码中设置一个断点；让我们将其设置在内核入口点，此时行值被设置。我们在 Eclipse 编辑器中将光标置于该行上，然后按
    *Ctrl* + *Shift* + *B* 来设置它。
- en: We can now begin debugging by pressing *F11* (or clicking the bug icon). The
    program should be paused at the very beginning of the `main` function, so press
    *F8* to *resume* to the first breakpoint. You should see the first line in our
    CUDA kernel highlighted with an arrow pointing to it in the IDE; let's step over
    the current line by pressing *F6*, which will ensure that the row has been set.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过按 *F11*（或点击错误图标）开始调试。程序应该在 `main` 函数的非常开始处暂停，因此按 *F8* 来 *恢复* 到第一个断点。你应该在
    IDE 中看到我们的 CUDA 内核的第一行被高亮显示，并有一个箭头指向它；让我们通过按 *F6* 跳过当前行，这将确保行值已被设置。
- en: 'Now, we can easily switch between different threads and blocks in our CUDA
    grid to check the current values that they hold as follows: from the top of the
    IDE, click on the Window drop-down menu, then click Show view, and then choose
    CUDA. A window with the currently running kernel should open, and from here you
    can see a list of all of the blocks that this kernel is running over.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以轻松地在我们的 CUDA 网格中的不同线程和块之间切换，以检查它们当前持有的值，如下所示：从 IDE 的顶部，点击窗口下拉菜单，然后点击显示视图，然后选择
    CUDA。应该会打开一个包含当前正在运行的内核的窗口，从这里你可以看到该内核正在运行的块列表。
- en: 'Click on the first one and from here you will be able to see all of the individual
    threads that are running within the block:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 点击第一个，从这里你将能够看到块内运行的各个单独的线程：
- en: '![](img/429490aa-3a80-48af-8833-9cc6a4988a7f.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/429490aa-3a80-48af-8833-9cc6a4988a7f.png)'
- en: 'Now, we can look at the variable corresponding to the very first thread in
    the very first block by clicking on the Variables tab—here, row should be 0, as
    expected:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过单击变量选项卡来查看第一个块中第一个线程对应的变量——在这里，行应该是 0，正如我们所预期的：
- en: '![](img/2d42ba20-c3bb-4f76-acc9-054fa1cc3960.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2d42ba20-c3bb-4f76-acc9-054fa1cc3960.png)'
- en: 'Now, we can check the values for a different thread by again going to the CUDA
    tab, choosing the appropriate thread, and switching back. Let''s stay in the same
    block, but choose thread (1, 0, 0) this time, and check the value of row again:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过再次转到 CUDA 选项卡，选择适当的线程，并切换回来来检查不同线程的值。让我们保持在同一个块中，但这次选择线程 (1, 0, 0)，再次检查行值：
- en: '![](img/ca99eb7a-04e5-40a0-982e-8a605b3cc697.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ca99eb7a-04e5-40a0-982e-8a605b3cc697.png)'
- en: We see that the value of row is now 1, as we expect.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到行的值现在是 1，正如我们所预期的。
- en: We now have the basics of how to debug a CUDA program from Nsight/Eclipse in
    Linux, and we can use all of the regular conventions as you would for debugging
    a regular Linux program as with any other IDE (setting breakpoints, starting the
    debugger, continue/resume, step over, step in, and step out). Namely, the main
    difference here is we have to know how to switch between CUDA threads and blocks
    to check variables, otherwise, it's pretty much the same.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何在 Linux 中的 Nsight/Eclipse 上调试 CUDA 程序的基础，我们可以使用所有常规约定，就像在任何一个其他 IDE
    中调试常规 Linux 程序一样（设置断点、启动调试器、继续/恢复、跳过、进入和退出）。主要区别在于，我们必须知道如何在不同 CUDA 线程和块之间切换以检查变量，否则基本上是相同的。
- en: Using Nsight to understand the warp lockstep property in CUDA
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Nsight 理解 CUDA 中的 warp lockstep 属性
- en: 'We will now use Nsight to step through some code to help us better understand
    some of the CUDA GPU architecture, and how **branching** within a kernel is handled.
    This will give us some insight about how to write more efficient CUDA kernels.
    By branching, we mean how the GPU handles control flow statements such as `if`,
    `else`, or `switch` within a CUDA kernel. In particular, we are interested in
    how **branch divergence** is handled within a kernel, which is what happens when
    one thread in a kernel satisfies the conditions to be an `if` statement, while
    another doesn''t and is an `else` statement: they are divergent because they are
    executing different pieces of code.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用Nsight逐步执行一些代码，以帮助我们更好地理解一些CUDA GPU架构，以及内核内部的**分支**是如何处理的。这将让我们对如何编写更高效的CUDA内核有所了解。当我们提到**分支**时，我们指的是GPU如何处理CUDA内核内部的控制流语句，如`if`、`else`或`switch`。特别是，我们感兴趣的是内核内部如何处理**分支发散**，这发生在内核中的一个线程满足成为`if`语句的条件，而另一个线程不满足且是`else`语句时：它们是发散的，因为它们正在执行不同的代码片段。
- en: 'Let''s write a small CUDA-C program as an experiment: we will start with a
    small kernel that prints one output if its `threadIdx.x` value is even and another
    if it is odd. We then write a `main` function that will launch this kernel over
    one single block consisting of 32 different threads:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个小型的CUDA-C程序作为实验：我们将从一个打印一个输出，如果其`threadIdx.x`值是偶数，另一个如果它是奇数的内核开始。然后我们编写一个`main`函数，该函数将在这个由32个不同线程组成的单个块上启动这个内核：
- en: '[PRE33]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: (This code is also available as `divergence_test.cu` in the repository.)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: （此代码也作为`divergence_test.cu`存储库中的文件提供。）
- en: If we compile and run this from the command line, we might naively expect there
    to be an interleaved sequence of strings from even and odd threads; or maybe they
    will be randomly interleaved—since all of the threads run concurrently and branch
    about the same time, this would make sense.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从命令行编译并运行这个程序，我们可能会天真地期望看到来自偶数和奇数线程的字符串交错序列；或者它们可能会随机交错——因为所有线程都是并发运行的，并且大约在同一时间分支，这听起来是有道理的。
- en: 'Instead, every single time we run this, we always get this output:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，每次我们运行这个程序时，我们总是得到这个输出：
- en: '![](img/38c855ca-4bd8-4ab6-9e6b-371a9c9c0bb1.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/38c855ca-4bd8-4ab6-9e6b-371a9c9c0bb1.png)'
- en: All of the strings corresponding to even threads are printed first, while all
    of the odd strings are printed second. Perhaps the Nsight debugger can shed some
    light on this; let's import this little program into an Nsight project as we did
    in the last section, putting a breakpoint at the first `if` statement in our kernel.
    We will then do a *step over*, so that the debugger stops where the first `printf`
    statement is. Since the default thread in Nsight is (0,0,0), this should have
    satisfied the first `if` statement so it will be stuck there until the debugger
    continues.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 所有对应于偶数线程的字符串首先打印出来，而所有奇数字符串随后打印。也许Nsight调试器可以提供一些线索；让我们像上一节那样将这个小程序导入到Nsight项目中，在内核中的第一个`if`语句处设置一个断点。然后我们将执行`step
    over`，这样调试器就会在第一个`printf`语句处停止。由于Nsight中的默认线程是（0,0,0），这应该满足了第一个`if`语句，所以它会卡在那里，直到调试器继续。
- en: 'Let''s switch over to an odd thread, say (1,0,0), and see where it is in our
    program now:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们切换到一个奇数线程，比如（1,0,0），看看它在我们的程序中的位置：
- en: '![](img/58816167-07da-44ae-954a-4b6c3e911bbd.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58816167-07da-44ae-954a-4b6c3e911bbd.png)'
- en: Very strange! Thread (1,0,0) is also at the same place in execution as thread
    (0,0,0). Indeed, if we check every single other odd thread here, it will be stuck
    in the same place—at a `printf` statement that all of the odd threads should have
    skipped right past.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 非常奇怪！线程（1,0,0）在执行过程中也处于与线程（0,0,0）相同的位置。实际上，如果我们检查这里所有的其他奇数线程，它们都会卡在相同的位置——在一个所有奇数线程都应该直接跳过的`printf`语句上。
- en: What gives? This is known as the **warp lockstep property**. A **warp** in the
    CUDA architecture is a unit of 32 "lanes" within which our GPU executes kernels
    and grids over, where each lane will execute a single thread. A major limitation
    of warps is that all threads executing on a single warp must step through the
    same exact code in **lockstep**; this means that not every thread does indeed
    run the same code, but just ignores steps that are not applicable to it. (This
    is called lockstep because it's like a group of soldiers marching *lockstep* in
    unison—whether they want to march, or not!)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 发生了什么？这被称为** warp 步调一致属性**。在 CUDA 架构中，**warp** 是一个包含 32 个 "lanes" 的单元，我们的 GPU
    在其中执行内核和网格，每个 lane 将执行一个线程。warp 的一个主要限制是，在单个 warp 上执行的所有线程都必须以**步调一致**的方式执行相同的精确代码；这意味着并非每个线程都确实运行相同的代码，而是忽略对其不适用的一步。
    (这被称为步调一致，因为它就像一群士兵以**步调一致**的方式齐步前进——无论他们是否想前进！)
- en: The lockstep property implies that if one single thread running on a warp diverges
    from all 31 other threads in a single `if` statement, all 31 other threads have
    their execution delayed until this single anomalous thread finishes and returns
    from its solitary `if` divergence. This is a property that you should always keep
    in mind when writing kernels, and why branch divergence should be minimized as
    much as possible as a general rule in CUDA programming.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 步调一致属性意味着如果一个 warp 上运行的单个线程在单个 `if` 语句中与其他 31 个线程发生分歧，那么其他 31 个线程的执行将被延迟，直到这个异常的单个线程完成其孤立的
    `if` 分支并返回。这是你在编写内核时应该始终牢记的一个属性，也是为什么在 CUDA 编程中应该尽可能减少分支分歧的一般规则。
- en: Using the NVIDIA nvprof profiler and Visual Profiler
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 NVIDIA nvprof 分析器和 Visual Profiler
- en: We will end with a brief overview of the command-line Nvidia `nvprof` profiler.
    In contrast to the Nsight IDE, we can freely use any Python code that we have
    written—we won't be compelled here to write full-on, pure CUDA-C test function
    code.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以对命令行 Nvidia `nvprof` 分析器的一个简要概述结束。与 Nsight IDE 相比，我们可以自由使用我们编写的任何 Python
    代码——我们在这里不会被迫编写完整的、纯 CUDA-C 测试函数代码。
- en: 'We can do a basic profiling of a binary executable program with the `nvprof
    program` command; we can likewise profile a Python script by using the `python`
    command as the first argument, and the script as the second as follows: `nvprof
    python program.py`. Let''s profile the simple matrix-multiplication CUDA-C executable
    program that we wrote earlier, with `nvprof matrix_ker`:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `nvprof program` 命令对二进制可执行程序进行基本分析；我们同样可以使用 `python` 命令作为第一个参数，脚本作为第二个参数进行
    Python 脚本的分析，如下所示：`nvprof python program.py`。让我们使用 `nvprof matrix_ker` 对我们之前编写的简单矩阵乘法
    CUDA-C 可执行程序进行分析：
- en: '![](img/41fb00d0-582b-4b22-81ea-b9ae1988ad84.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/41fb00d0-582b-4b22-81ea-b9ae1988ad84.png)'
- en: We see that this is very similar to the output of the Python cProfiler module
    that we first used to analyze a Mandelbrot algorithm way back in [Chapter 1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml),
    *Why GPU Programming?*—only now, this exclusively tells us only about all of the
    CUDA operations that were executed. So, we can use this when we specifically want
    to optimize on the GPU, rather than concern ourselves with any of the Python or
    other commands that executed on the host. (We can further analyze each individual
    CUDA kernel operation with block and grid size launch parameters if we add the
    command-line option, `--print-gpu-trace`.)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到这与我们最初在 [第 1 章](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml) 中分析 Mandelbrot
    算法的 Python cProfiler 模块的输出非常相似，*为什么进行 GPU 编程？*——但现在，这仅告诉我们所有已执行的 CUDA 操作。因此，当我们特别想要在
    GPU 上进行优化时，我们可以使用这个工具，而不是关心主机上执行的任何 Python 或其他命令。 (如果我们添加命令行选项 `--print-gpu-trace`，我们还可以进一步分析每个单独的
    CUDA 内核操作及其块和网格大小启动参数。)
- en: 'Let''s look at one more trick to help us *visualize* the execution time of
    all of the operations of a program; we will use `nvprof` to dump a file that can
    then be read by the NVIDIA Visual Profiler, which will show this to us graphically.
    Let''s do this using an example from the last chapter, `multi-kernel_streams.py`
    (this is available in the repository under `5`). Let''s recall that this was one
    of our introductory examples to the idea of CUDA streams, which allow us to execute
    and organize multiple GPU operations concurrently. Let''s dump the output to a
    file with the `.nvvp` file suffix with the `-o` command-line option as follows:
    `nvprof -o m.nvvp python multi-kernel_streams.py`. We can now load this file into
    the NVIDIA Visual Profiler with the `nvvp m.nvvp` command.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再来看一个技巧，帮助我们*可视化*程序中所有操作的执行时间；我们将使用`nvprof`来导出一个文件，然后可以被NVIDIA Visual Profiler读取，并以图形方式展示给我们。让我们用一个来自上一章的例子来演示，`multi-kernel_streams.py`（这个文件在仓库的`5`目录下有提供）。让我们回顾一下，这是我们介绍CUDA流概念的入门示例之一，CUDA流允许我们并发执行和组织多个GPU操作。让我们使用以下命令将输出保存到以`.nvvp`为后缀的文件中：`nvprof
    -o m.nvvp python multi-kernel_streams.py`。现在我们可以使用`nvvp m.nvvp`命令将这个文件加载到NVIDIA
    Visual Profiler中。
- en: 'We should see a timeline across all CUDA streams as such (remembering that
    the name of the kernel used in this program is called `mult_ker`):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到所有CUDA流的时间线如下（记住，这个程序中使用的内核名称叫做`mult_ker`）：
- en: '![](img/37df8c10-9b93-4d86-bb74-ff00920e7470.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/37df8c10-9b93-4d86-bb74-ff00920e7470.png)'
- en: Not only can we see all kernel launches, but also memory allocations, memory
    copies, and other operations. This can be useful for getting an intuitive and
    visual understanding of how your program is using your GPU over time.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅可以看到所有内核的启动，还可以看到内存分配、内存复制和其他操作。这有助于直观和可视化地理解程序随时间如何使用GPU。
- en: Summary
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'We started out in this chapter by seeing how `printf` can be used within a
    CUDA kernel to output data from individual threads; we saw in particular how useful
    this can be for debugging code. We then covered some of the gaps in our knowledge
    in CUDA-C, so that we can write full test programs that we can compile into proper
    executable binary files: there is a lot of overhead here that was hidden from
    us before that we have to be meticulous about. Next, we saw how to create and
    compile a project in the Nsight IDE and how to use it for debugging. We saw how
    to stop at any breakpoint we set in a CUDA kernel and switch between individual
    threads to see the different local variables. We also used the Nsight debugger
    to learn about the warp lockstep property and why it is important to avoid branch
    divergence in CUDA kernels. Finally, we had a very brief overview of the NVIDIA
    command-line `nvprof` profiler and Visual Profiler for analyzing our GPU code.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章开始时看到如何在CUDA内核中使用`printf`输出单个线程的数据；我们特别看到这对于调试代码非常有用。然后我们填补了CUDA-C知识中的某些空白，这样我们就可以编写完整的测试程序，我们可以将其编译成正确的可执行二进制文件：这里有很多我们之前没有注意到的开销，我们现在必须非常细致。接下来，我们看到了如何在Nsight
    IDE中创建和编译一个项目，以及如何使用它进行调试。我们看到了如何在CUDA内核中设置任何断点并切换到单个线程以查看不同的局部变量。我们还使用了Nsight调试器来了解warp同步属性以及为什么在CUDA内核中避免分支发散很重要。最后，我们简要概述了NVIDIA命令行`nvprof`分析器和Visual
    Profiler，用于分析我们的GPU代码。
- en: Questions
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'In the first CUDA-C program that we wrote, we didn''t use a `cudaDeviceSynchronize`
    command after the calls we made to allocate memory arrays on the GPU with `cudaMalloc`.
    Why was this not necessary? (Hint: Review the last chapter.)'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们编写的第一个CUDA-C程序中，我们在使用`cudaMalloc`在GPU上分配内存数组后没有使用`cudaDeviceSynchronize`命令。为什么这并不是必要的？（提示：回顾上一章。）
- en: Suppose we have a single kernel that is launched over a grid consisting of two
    blocks, where each block has 32 threads. Suppose all of the threads in the first
    block execute an `if` statement, while all of the threads in the second block
    execute the corresponding `else` statement. Will all of the threads in the second
    block have to "lockstep" through the commands in the `if` statement as the threads
    in the first block are actually executing them?
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们有一个内核被启动在一个由两个块组成的网格上，每个块有32个线程。假设第一个块中的所有线程都执行一个`if`语句，而第二个块中的所有线程都执行相应的`else`语句。第二个块中的所有线程是否都需要像第一个块中的线程实际执行它们一样“同步”通过`if`语句中的命令？
- en: What if we executed a similar piece of code, only over a grid consisting of
    one single block executed over 64 threads, where the first 32 threads execute
    an `if` and the second 32 execute an `else` statement?
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们执行一段类似的代码，但只是在由一个包含64个线程的单个块组成的网格上执行，其中前32个线程执行一个`if`语句，而接下来的32个线程执行一个`else`语句，会怎样？
- en: What can the `nvprof` profiler measure for us that Python's cProfiler cannot?
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`nvprof`分析器能为我们测量到Python的cProfiler无法测量的内容有哪些？'
- en: Name some contexts where we might prefer to use `printf` to debug a CUDA kernel
    and other contexts where it might be easier to use Nsight to debug a CUDA kernel.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请列举一些我们可能更倾向于使用`printf`来调试CUDA内核的上下文，以及一些可能更容易使用Nsight来调试CUDA内核的上下文。
- en: What is the purpose of the `cudaSetDevice` command in CUDA-C?
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在CUDA-C中，`cudaSetDevice`命令的目的是什么？
- en: Why do we have to use `cudaDeviceSynchronize` after every kernel launch or memory
    copy in CUDA-C?
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在CUDA-C中，每次内核启动或内存复制后我们都必须使用`cudaDeviceSynchronize`？
