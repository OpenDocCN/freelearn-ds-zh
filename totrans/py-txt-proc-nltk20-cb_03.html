<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;Creating Custom Corpora"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Creating Custom Corpora</h1></div></div></div><p>In this chapter, we will cover:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Setting up a custom corpus</li><li class="listitem" style="list-style-type: disc">Creating a word list corpus</li><li class="listitem" style="list-style-type: disc">Creating a part-of-speech tagged word corpus</li><li class="listitem" style="list-style-type: disc">Creating a chunked phrase corpus</li><li class="listitem" style="list-style-type: disc">Creating a categorized text corpus</li><li class="listitem" style="list-style-type: disc">Creating a categorized chunk corpus reader</li><li class="listitem" style="list-style-type: disc">Lazy corpus loading</li><li class="listitem" style="list-style-type: disc">Creating a custom corpus view</li><li class="listitem" style="list-style-type: disc">Creating a MongoDB backed corpus reader</li><li class="listitem" style="list-style-type: disc">Corpus editing with file locking</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec25"/>Introduction</h1></div></div></div><p>In this chapter, we'll cover how to use corpus readers and create custom corpora. At the same time, you'll learn how to use the existing corpus data that comes with NLTK. This information is essential for future chapters when we'll need to access the corpora as training data. We'll also cover creating custom corpus readers, which can be used when your corpus is not in a file format that NLTK already recognizes, or if your corpus is not in files at all, but instead is located in a database such as MongoDB.</p></div></div>
<div class="section" title="Setting up a custom corpus"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec26"/>Setting up a custom corpus</h1></div></div></div><a id="id136" class="indexterm"/><a id="id137" class="indexterm"/><p>A <span class="strong"><strong>corpus</strong></span> is a collection of text documents, and <span class="strong"><strong>corpora</strong></span>
<a id="id138" class="indexterm"/> is the plural of corpus. So a <span class="emphasis"><em>custom corpus</em></span> is really just a bunch of text files in a directory, often alongside many other directories of text files.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec78"/>Getting ready</h2></div></div></div><a id="id139" class="indexterm"/><p>You should already have the NLTK data package installed, following the instructions at <a class="ulink" href="http://www.nltk.org/data">http://www.nltk.org/data</a>. We'll assume that the data is installed to <code class="literal">C:\nltk_data</code> on Windows, and <code class="literal">/usr/share/nltk_data</code> on Linux, Unix, or Mac OS X.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec79"/>How to do it...</h2></div></div></div><a id="id140" class="indexterm"/><p>NLTK defines a list of data directories, or <span class="strong"><strong>paths</strong></span>, in <code class="literal">nltk.data.path</code>. Our custom corpora must be within one of these paths so it can be found by NLTK. So as not to conflict with the official data package, we'll create a custom <code class="literal">nltk_data</code> directory in our home directory. Here's some Python code to create this directory and verify that it is in the list of known paths specified by <code class="literal">nltk.data.path</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; import os, os.path
&gt;&gt;&gt; path = os.path.expanduser('~/nltk_data')
&gt;&gt;&gt; if not os.path.exists(path):
...    os.mkdir(path)
&gt;&gt;&gt; os.path.exists(path)
True
&gt;&gt;&gt; import nltk.data
&gt;&gt;&gt; path in nltk.data.path
True</pre></div><p>If the last line, <code class="literal">path in nltk.data.path</code>, is <code class="literal">True</code>, then you should now have a <code class="literal">nltk_data</code> directory in your home directory. The path should be <code class="literal">%UserProfile%\nltk_data</code> on Windows, or <code class="literal">~/nltk_data</code> on Unix, Linux, or Mac OS X. For simplicity, I'll refer to the directory as <code class="literal">~/nltk_data</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note06"/>Note</h3><p>If the last line does not return <code class="literal">True</code>, try creating the <code class="literal">nltk_data</code> directory manually in your home directory, then verify that the absolute path is in <code class="literal">nltk.data.path</code>. It's essential to ensure that this directory exists and is in <code class="literal">nltk.data.path</code> before continuing. Once you have your <code class="literal">nltk_data</code> directory, the convention is that corpora reside in a <code class="literal">corpora</code> subdirectory. Create this <code class="literal">corpora</code> directory within the <code class="literal">nltk_data</code> directory, so that the path is <code class="literal">~/nltk_data/corpora</code>. Finally, we'll create a subdirectory in <code class="literal">corpora</code> to hold our custom corpus. Let's call it <code class="literal">cookbook</code>, giving us the full path of <code class="literal">~/nltk_data/corpora/cookbook</code>.</p></div></div><p>Now we can create a simple <span class="emphasis"><em>word list</em></span> file and make sure it loads. In <a class="link" href="ch02.html" title="Chapter 2. Replacing and Correcting Words">Chapter 2</a>, <span class="emphasis"><em>Replacing and Correcting Words</em></span>, <span class="emphasis"><em>Spelling correction with Enchant</em></span> recipe, we created a word list file called <code class="literal">mywords.txt</code>. Put this file into <code class="literal">~/nltk_data/corpora/cookbook/</code>. Now we can use <code class="literal">nltk.data.load()</code> to load the file.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; import nltk.data
&gt;&gt;&gt; nltk.data.load('corpora/cookbook/mywords.txt', format='raw')
'nltk\n'</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note07"/>Note</h3><p>We need to specify <code class="literal">format='raw'</code> since <code class="literal">nltk.data.load()</code> doesn't know how to interpret <code class="literal">.txt</code> files. As we'll see, it does know how to interpret a number of other file formats.</p></div></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec80"/>How it works...</h2></div></div></div><a id="id141" class="indexterm"/><a id="id142" class="indexterm"/><p>The <code class="literal">nltk.data.load()</code> function recognizes a number of formats, such as <code class="literal">'raw'</code>, <code class="literal">'pickle'</code>, and <code class="literal">'yaml'</code>. If no format is specified, then it tries to guess the format based on the file's extension. In the previous case, we have a <code class="literal">.txt</code> file, which is not a recognized extension, so we have to specify the <code class="literal">'raw'</code> format. But if we used a file that ended in <code class="literal">.yaml</code>, then we would not need to specify the format.</p><p>Filenames passed in to <code class="literal">nltk.data.load()</code> can be <span class="emphasis"><em>absolute</em></span> or <span class="emphasis"><em>relative</em></span> paths. Relative paths must be relative to one of the paths specified in <code class="literal">nltk.data.path</code>. The file is found using <code class="literal">nltk.data.find(path)</code>, which searches all known paths combined with the relative path. Absolute paths do not require a search, and are used as is.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec81"/>There's more...</h2></div></div></div><p>For most corpora access, you won't actually need to use <code class="literal">nltk.data.load</code>, as that will be handled by the <code class="literal">CorpusReader</code> classes covered in the following recipes. But it's a good function to be familiar with for loading <code class="literal">.pickle</code> files and <code class="literal">.yaml</code> files, plus it introduces the idea of putting all of your data files into a path known by NLTK.</p><div class="section" title="Loading a YAML file"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec24"/>Loading a YAML file</h3></div></div></div><a id="id143" class="indexterm"/><p>If you put the <code class="literal">synonyms.yaml</code> file from the <a class="link" href="ch02.html" title="Chapter 2. Replacing and Correcting Words">Chapter 2</a>, <span class="emphasis"><em>Replacing and Correcting Words</em></span>, <span class="emphasis"><em>Replacing synonyms</em></span> recipe, into <code class="literal">~/nltk_data/corpora/cookbook</code> (next to <code class="literal">mywords.txt</code>), you can use <code class="literal">nltk.data.load()</code> to load it without specifying a format.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; import nltk.data
&gt;&gt;&gt; nltk.data.load('corpora/cookbook/synonyms.yaml')
{'bday': 'birthday'}</pre></div><p>This assumes that PyYAML is installed. If not, you can find download and installation instructions at <a class="ulink" href="http://pyyaml.org/wiki/PyYAML">http://pyyaml.org/wiki/PyYAML</a>.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec82"/>See also</h2></div></div></div><p>In the next recipes, we'll cover various corpus readers, and then in the <span class="emphasis"><em>Lazy corpus loading</em></span> recipe, we'll use the <code class="literal">LazyCorpusLoader</code>, which expects corpus data to be in a <code class="literal">corpora</code> subdirectory of one of the paths specified by <code class="literal">nltk.data.path</code>.</p></div></div>
<div class="section" title="Creating a word list corpus"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec27"/>Creating a word list corpus</h1></div></div></div><a id="id144" class="indexterm"/><a id="id145" class="indexterm"/><p>The <code class="literal">WordListCorpusReader</code> is one of the simplest <code class="literal">CorpusReader</code> classes. It provides access to a file containing a list of words, one word per line. In fact, you've already used it when we used the <code class="literal">stopwords</code> corpus in the <span class="emphasis"><em>Filtering stopwords in a tokenized sentence</em></span> and <span class="emphasis"><em>Discovering word collocations </em></span>recipes in <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <span class="emphasis"><em>Tokenizing Text and WordNet Basics</em></span>.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec83"/>Getting ready</h2></div></div></div><p>We need to start by creating a word list file. This could be a single column CSV file, or just a normal text file with one word per line. Let's create a file named <code class="literal">wordlist</code> that looks like this:</p><div class="informalexample"><pre class="programlisting">nltk
corpus
corpora
wordnet</pre></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec84"/>How to do it...</h2></div></div></div><p>Now we can instantiate a <code class="literal">WordListCorpusReader</code> that will produce a list of words from our file. It takes two arguments: the directory path containing the files, and a list of filenames. If you open the Python console in the same directory as the files, then <code class="literal">'.'</code> can be used as the directory path. Otherwise, you must use a directory path such as: <code class="literal">'nltk_data/corpora/cookbook'</code>.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus.reader import WordListCorpusReader
&gt;&gt;&gt; reader = WordListCorpusReader('.', ['wordlist'])
&gt;&gt;&gt; reader.words()
['nltk', 'corpus', 'corpora', 'wordnet']
&gt;&gt;&gt; reader.fileids()
['wordlist']</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec85"/>How it works...</h2></div></div></div><a id="id146" class="indexterm"/><p>
<code class="literal">WordListCorpusReader</code> inherits from <code class="literal">CorpusReader</code>, which is a common base class for all corpus readers. <code class="literal">CorpusReader</code> does all the work of identifying which files to read, while <code class="literal">WordListCorpus</code> reads the files and tokenizes each line to produce a list of words. Here's an inheritance diagram:</p><div class="mediaobject"><img src="graphics/3609OS_03_01.jpg" alt="How it works..."/></div><a id="id147" class="indexterm"/><a id="id148" class="indexterm"/><p>When you call the <code class="literal">words()</code> function, it calls <code class="literal">nltk.tokenize.line_tokenize()</code> on the raw file data, which you can access using the <code class="literal">raw()</code> function.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; reader.raw()
'nltk\ncorpus\ncorpora\nwordnet\n'
&gt;&gt;&gt; from nltk.tokenize import line_tokenize
&gt;&gt;&gt; line_tokenize(reader.raw())
['nltk', 'corpus', 'corpora', 'wordnet']</pre></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec86"/>There's more...</h2></div></div></div><p>The <code class="literal">stopwords</code> corpus is a good example of a multi-file <code class="literal">WordListCorpusReader</code>. In <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <span class="emphasis"><em>Tokenizing Text and WordNet Basics</em></span>, in the <span class="emphasis"><em>Filtering stopwords in a tokenized sentence</em></span> recipe, we saw that it had one word list file for each language, and you could access the words for that language by calling <code class="literal">stopwords.words(fileid)</code>. If you want to create your own multi-file word list corpus, this is a great example to follow.</p><div class="section" title="Names corpus"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec25"/>Names corpus</h3></div></div></div><a id="id149" class="indexterm"/><p>Another word list corpus that comes with NLTK is the <code class="literal">names</code> corpus. It contains two files: <code class="literal">female.txt</code> and <code class="literal">male.txt</code>, each containing a list of a few thousand common first names organized by gender.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus import names
&gt;&gt;&gt; names.fileids()
['female.txt', 'male.txt']
&gt;&gt;&gt; len(names.words('female.txt'))
5001
&gt;&gt;&gt; len(names.words('male.txt'))
2943</pre></div></div><div class="section" title="English words"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec26"/>English words</h3></div></div></div><p>NLTK also comes with a large list of English words. There's one file with 850 <code class="literal">basic</code> words, and another list with over 200,000 known English words.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus import words
&gt;&gt;&gt; words.fileids()
['en', 'en-basic']
&gt;&gt;&gt; len(words.words('en-basic'))
850
&gt;&gt;&gt; len(words.words('en'))
234936</pre></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec87"/>See also</h2></div></div></div><p>In <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <span class="emphasis"><em>Tokenizing Text and WordNet Basics</em></span>, the <span class="emphasis"><em>Filtering stopwords in a tokenized sentence</em></span> recipe, has more details on using the <code class="literal">stopwords</code> corpus. In the following recipes, we'll cover more advanced corpus file formats and corpus reader classes.</p></div></div>
<div class="section" title="Creating a part-of-speech tagged word corpus"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec28"/>Creating a part-of-speech tagged word corpus</h1></div></div></div><a id="id150" class="indexterm"/><a id="id151" class="indexterm"/><p>
<span class="strong"><strong>Part-of-speech tagging</strong></span> is the process of identifying the part-of-speech tag for a word. Most of the time, a <span class="emphasis"><em>tagger</em></span> must first be trained on a <span class="emphasis"><em>training corpus</em></span>. How to train and use a tagger is covered in detail in <a class="link" href="ch04.html" title="Chapter 4. Part-of-Speech Tagging">Chapter 4</a>, <span class="emphasis"><em>Part-of-Speech Tagging</em></span>, but first we must know how to create and use a training corpus of part-of-speech tagged words.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec88"/>Getting ready</h2></div></div></div><p>The simplest format for a tagged corpus is of the form "word/tag". Following is an excerpt from the <code class="literal">brown</code> corpus:</p><div class="informalexample"><pre class="programlisting">The/at-tl expense/nn and/cc time/nn involved/vbn are/ber astronomical/jj ./.</pre></div><p>Each word has a <span class="emphasis"><em>tag</em></span> denoting its part-of-speech. For example, <code class="literal">nn</code> refers to a noun, while a tag that starts with <code class="literal">vb</code> is a verb.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec89"/>How to do it...</h2></div></div></div><a id="id152" class="indexterm"/><p>If you were to put the previous excerpt into a file called <code class="literal">brown.pos</code>, you could then create a <code class="literal">TaggedCorpusReader</code> and do the following:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus.reader import TaggedCorpusReader
&gt;&gt;&gt; reader = TaggedCorpusReader('.', r'.*\.pos')
&gt;&gt;&gt; reader.words()
['The', 'expense', 'and', 'time', 'involved', 'are', ...]
&gt;&gt;&gt; reader.tagged_words()
[('The', 'AT-TL'), ('expense', 'NN'), ('and', 'CC'), …]
&gt;&gt;&gt; reader.sents()
[['The', 'expense', 'and', 'time', 'involved', 'are', 'astronomical', '.']]
&gt;&gt;&gt; reader.tagged_sents()
[[('The', 'AT-TL'), ('expense', 'NN'), ('and', 'CC'), ('time', 'NN'), ('involved', 'VBN'), ('are', 'BER'), ('astronomical', 'JJ'), ('.', '.')]]
&gt;&gt;&gt; reader.paras()
[[['The', 'expense', 'and', 'time', 'involved', 'are', 'astronomical', '.']]]
&gt;&gt;&gt; reader.tagged_paras()
[[[('The', 'AT-TL'), ('expense', 'NN'), ('and', 'CC'), ('time', 'NN'), ('involved', 'VBN'), ('are', 'BER'), ('astronomical', 'JJ'), ('.', '.')]]]</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec90"/>How it works...</h2></div></div></div><a id="id153" class="indexterm"/><p>This time, instead of naming the file explicitly, we use a regular expression, <code class="literal">r'.*\.pos'</code>, to match all files whose name ends with <code class="literal">.pos</code>. We could have done the same thing as we did with the <code class="literal">WordListCorpusReader</code>, and pass <code class="literal">['brown.pos']</code> as the second argument, but this way you can see how to include multiple files in a corpus without naming each one explicitly.</p><a id="id154" class="indexterm"/><p>
<code class="literal">TaggedCorpusReader</code> provides a number of methods for extracting text from a corpus. First, you can get a list of all words, or a list of tagged tokens. A <a id="id155" class="indexterm"/>
<span class="strong"><strong>tagged token</strong></span> is simply a tuple of <code class="literal">(word, tag)</code>. Next, you can get a list of every sentence, and also every tagged sentence, where the sentence is itself a list of words or tagged tokens. Finally, you can get a list of paragraphs, where each paragraph is a list of sentences, and each sentence is a list of words or tagged tokens. Here's an inheritance diagram listing all the major methods:</p><div class="mediaobject"><img src="graphics/3609OS_03_02.jpg" alt="How it works..."/></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec91"/>There's more...</h2></div></div></div><p>The functions demonstrated in the previous diagram all depend on <span class="emphasis"><em>tokenizers</em></span> for splitting the text. <code class="literal">TaggedCorpusReader</code> tries to have good defaults, but you can customize them by passing in your own tokenizers at initialization time.</p><div class="section" title="Customizing the word tokenizer"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec27"/>Customizing the word tokenizer</h3></div></div></div><a id="id156" class="indexterm"/><p>The default word tokenizer is an instance of <code class="literal">nltk.tokenize.WhitespaceTokenizer</code>. If you want to use a different tokenizer, you can pass that in as <code class="literal">word_tokenizer</code>.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.tokenize import SpaceTokenizer
&gt;&gt;&gt; reader = TaggedCorpusReader('.', r'.*\.pos', word_tokenizer=SpaceTokenizer())
&gt;&gt;&gt; reader.words()
['The', 'expense', 'and', 'time', 'involved', 'are', ...]</pre></div></div><div class="section" title="Customizing the sentence tokenizer"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec28"/>Customizing the sentence tokenizer</h3></div></div></div><a id="id157" class="indexterm"/><p>The default sentence tokenizer is an instance of <code class="literal">nltk.tokenize.RegexpTokenize</code> with <code class="literal">'\n'</code> to identify the gaps. It assumes that each sentence is on a line all by itself, and individual sentences do not have line breaks. To customize this, you can pass in your own tokenizer as <code class="literal">sent_tokenizer</code>.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.tokenize import LineTokenizer
&gt;&gt;&gt; reader = TaggedCorpusReader('.', r'.*\.pos', sent_tokenizer=LineTokenizer())
&gt;&gt;&gt; reader.sents()
[['The', 'expense', 'and', 'time', 'involved', 'are', 'astronomical', '.']]</pre></div></div><div class="section" title="Customizing the paragraph block reader"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec29"/>Customizing the paragraph block reader</h3></div></div></div><a id="id158" class="indexterm"/><p>Paragraphs are assumed to be split by blank lines. This is done with the default <code class="literal">para_block_reader</code>, which is <code class="literal">nltk.corpus.reader.util.read_blankline_block</code>. There are a number of other block reader functions in <code class="literal">nltk.corpus.reader.util</code>, whose purpose is to read blocks of text from a <span class="emphasis"><em>stream</em></span>. Their usage will be covered in more detail in the later recipe, <span class="emphasis"><em>Creating a custom corpus view</em></span>, where we'll create a custom corpus reader.</p></div><div class="section" title="Customizing the tag separator"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec30"/>Customizing the tag separator</h3></div></div></div><a id="id159" class="indexterm"/><p>If you don't want to use <code class="literal">'/'</code> as the word/tag separator, you can pass an alternative string to <code class="literal">TaggedCorpusReader</code> for <code class="literal">sep</code>. The default is <code class="literal">sep='/'</code>, but if you want to split words and tags with <code class="literal">'|'</code>, such as 'word|tag', then you should pass in <code class="literal">sep='|'</code>.</p></div><div class="section" title="Simplifying tags with a tag mapping function"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec31"/>Simplifying tags with a tag mapping function</h3></div></div></div><a id="id160" class="indexterm"/><p>If you'd like to somehow transform the part-of-speech tags, you can pass in a <code class="literal">tag_mapping_function</code> at initialization, then call one of the <code class="literal">tagged_*</code> functions with <code class="literal">simplify_tags=True</code>. Here's an example where we lowercase each tag:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; reader = TaggedCorpusReader('.', r'.*\.pos', tag_mapping_function=lambda t: t.lower())
&gt;&gt;&gt; reader.tagged_words(simplify_tags=True)
[('The', 'at-tl'), ('expense', 'nn'), ('and', 'cc'), …]</pre></div><p>Calling <code class="literal">tagged_words()</code> without <code class="literal">simplify_tags=True</code> would produce the same result as if you did not pass in a <code class="literal">tag_mapping_function</code>.</p><p>There are also a number of tag simplification functions defined in <code class="literal">nltk.tag.simplify</code>. These can be useful for reducing the number of different part-of-speech tags.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.tag import simplify
&gt;&gt;&gt; reader = TaggedCorpusReader('.', r'.*\.pos', tag_mapping_function=simplify.simplify_brown_tag)
&gt;&gt;&gt; reader.tagged_words(simplify_tags=True)
[('The', 'DET'), ('expense', 'N'), ('and', 'CNJ'), ...]
&gt;&gt;&gt; reader = TaggedCorpusReader('.', r'.*\.pos', tag_mapping_function=simplify.simplify_tag)
&gt;&gt;&gt; reader.tagged_words(simplify_tags=True)
[('The', 'A'), ('expense', 'N'), ('and', 'C'), ...]</pre></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec92"/>See also</h2></div></div></div><p>
<a class="link" href="ch04.html" title="Chapter 4. Part-of-Speech Tagging">Chapter 4</a>, <span class="emphasis"><em>Part-of-Speech Tagging</em></span> will cover part-of-speech tags and tagging in much more detail. And for more on tokenizers, see the first three recipes of <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <span class="emphasis"><em>Tokenizing Text and WordNet Basics</em></span>.</p><p>In the next recipe, we'll create a <span class="emphasis"><em>chunked phrase</em></span> corpus, where each phrase is also part-of-speech tagged.</p></div></div>
<div class="section" title="Creating a chunked phrase corpus"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec29"/>Creating a chunked phrase corpus</h1></div></div></div><a id="id161" class="indexterm"/><a id="id162" class="indexterm"/><p>A <span class="strong"><strong>chunk</strong></span> is a short phrase within a sentence. If you remember sentence diagrams from grade school, they were a tree-like representation of phrases within a sentence. This is exactly what chunks are: <span class="emphasis"><em>sub-trees within a sentence tree</em></span>, and they will be covered in much more detail in <a class="link" href="ch05.html" title="Chapter 5. Extracting Chunks">Chapter 5</a>, <span class="emphasis"><em>Extracting Chunks</em></span>. Following is a sample sentence tree with three noun phrase (<span class="strong"><strong>NP</strong></span>) chunks shown as sub-trees.</p><div class="mediaobject"><img src="graphics/3609OS_03_03.jpg" alt="Creating a chunked phrase corpus"/></div><p>This recipe will cover how to create a corpus with sentences that contain chunks.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec93"/>Getting ready</h2></div></div></div><a id="id163" class="indexterm"/><p>Here is an excerpt from the tagged <code class="literal">treebank</code> corpus. It has part-of-speech tags, as in the previous recipe, but it also has square brackets for denoting chunks. This is the same sentence as in the previous tree diagram, but in text form:</p><div class="informalexample"><pre class="programlisting">[Earlier/JJR staff-reduction/NN moves/NNS] have/VBP trimmed/VBN about/IN [300/CD jobs/NNS] ,/, [the/DT spokesman/NN] said/VBD ./.</pre></div><p>In this format, every chunk is a <span class="emphasis"><em>noun phrase</em></span>. Words that are not within brackets are part of the sentence tree, but are not part of any noun phrase sub-tree.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec94"/>How to do it...</h2></div></div></div><p>Put this excerpt into a file called <code class="literal">treebank.chunk</code>, and then do the following:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus.reader import ChunkedCorpusReader
&gt;&gt;&gt; reader = ChunkedCorpusReader('.', r'.*\.chunk')
&gt;&gt;&gt; reader.chunked_words()
[Tree('NP', [('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS')]), ('have', 'VBP'), ...]
&gt;&gt;&gt; reader.chunked_sents()
[Tree('S', [Tree('NP', [('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS')]), ('have', 'VBP'), ('trimmed', 'VBN'), ('about', 'IN'), Tree('NP', [('300', 'CD'), ('jobs', 'NNS')]), (',', ','), Tree('NP', [('the', 'DT'), ('spokesman', 'NN')]), ('said', 'VBD'), ('.', '.')])]
&gt;&gt;&gt; reader.chunked_paras()
[[Tree('S', [Tree('NP', [('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS')]), ('have', 'VBP'), ('trimmed', 'VBN'), ('about', 'IN'), Tree('NP', [('300', 'CD'), ('jobs', 'NNS')]), (',', ','), Tree('NP', [('the', 'DT'), ('spokesman', 'NN')]), ('said', 'VBD'), ('.', '.')])]]</pre></div><a id="id164" class="indexterm"/><p>The <code class="literal">ChunkedCorpusReader</code> provides the same methods as the <code class="literal">TaggedCorpusReader</code> for getting tagged tokens, along with three new methods for getting chunks. Each chunk is represented as an instance of <code class="literal">nltk.tree.Tree</code>. Sentence level trees look like <code class="literal">Tree('S', [...])</code> while noun phrase trees look like <code class="literal">Tree('NP', [...])</code>. In <code class="literal">chunked_sents()</code>, you get a list of sentence trees, with each noun-phrase as a sub-tree of the sentence. In <code class="literal">chunked_words()</code>, you get a list of noun phrase trees alongside tagged tokens of words that were not in a chunk. Here's an inheritance diagram listing the major methods:</p><div class="mediaobject"><img src="graphics/3609OS_03_04.jpg" alt="How to do it..."/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note08"/>Note</h3><p>You can draw a <code class="literal">Tree</code> by calling the <code class="literal">draw()</code> method. Using the corpus reader defined earlier, you could do <code class="literal">reader.chunked_sents()[0].draw()</code> to get the same sentence tree diagram shown at the beginning of this recipe.</p></div></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec95"/>How it works...</h2></div></div></div><a id="id165" class="indexterm"/><p>
<code class="literal">ChunkedCorpusReader</code> is similar to the <code class="literal">TaggedCorpusReader</code> from the last recipe. It has the same default <code class="literal">sent_tokenizer</code> and <code class="literal">para_block_reader</code>, but instead of a <code class="literal">word_tokenizer</code>, it uses a <code class="literal">str2chunktree()</code> function. The default is <code class="literal">nltk.chunk.util.tagstr2tree()</code>, which parses a sentence string containing bracketed chunks into a sentence tree, with each chunk as a noun phrase sub-tree. Words are split by whitespace, and the default word/tag separator is <code class="literal">'/'</code>. If you want to customize the chunk parsing, then you can pass in your own function for <code class="literal">str2chunktree()</code>.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec96"/>There's more...</h2></div></div></div><a id="id166" class="indexterm"/><p>An alternative format for denoting chunks is called IOB tags. <span class="strong"><strong>IOB</strong></span> tags are similar to part-of-speech tags, but provide a way to denote the inside, outside, and beginning of a chunk. They also have the benefit of allowing multiple different chunk phrase types, not just noun phrases. Here is an excerpt from the <code class="literal">conll2000</code> corpus. Each word is on its own line with a part-of-speech tag followed by an IOB tag.</p><div class="informalexample"><pre class="programlisting">Mr. NNP B-NP
Meador NNP I-NP
had VBD B-VP
been VBN I-VP
executive JJ B-NP
vice NN I-NP
president NN I-NP
of IN B-PP
Balcor NNP B-NP
. . O</pre></div><p>
<code class="literal">B-NP</code> denotes the beginning of a noun phrase, while <code class="literal">I-NP</code> denotes that the word is inside of the current noun phrase. <code class="literal">B-VP</code> and <code class="literal">I-VP</code> denote the beginning and inside of a verb phrase. <code class="literal">O</code> ends the sentence.</p><p>To read a corpus using the IOB format, you must use the <code class="literal">ConllChunkCorpusReader</code>. Each sentence is separated by a blank line, but there is no separation for paragraphs. This means that the <code class="literal">para_*</code> methods are not available. If you put the previous IOB example text into a file named <code class="literal">conll.iob</code>, you can create and use a <code class="literal">ConllChunkCorpusReader</code> with the code we are about to see. The third argument to <code class="literal">ConllChunkCorpusReader</code> should be a tuple or list specifying the types of chunks in the file, which in this case is <code class="literal">('NP', 'VP', 'PP')</code>.</p><div class="informalexample"><pre class="programlisting">
&gt;&gt;&gt; from nltk.corpus.reader import ConllChunkCorpusReader
&gt;&gt;&gt; conllreader = ConllChunkCorpusReader('.', r'.*\.iob', ('NP', 'VP', 'PP'))
&gt;&gt;&gt; conllreader.chunked_words()
[Tree('NP', [('Mr.', 'NNP'), ('Meador', 'NNP')]), Tree('VP', [('had', 'VBD'), ('been', 'VBN')]), ...]
&gt;&gt;&gt; conllreader.chunked_sents()
[Tree('S', [Tree('NP', [('Mr.', 'NNP'), ('Meador', 'NNP')]), Tree('VP', [('had', 'VBD'), ('been', 'VBN')]), Tree('NP', [('executive', 'JJ'), ('vice', 'NN'), ('president', 'NN')]), Tree('PP', [('of', 'IN')]), Tree('NP', [('Balcor', 'NNP')]), ('.', '.')])]
&gt;&gt;&gt; conllreader.iob_words()
[('Mr.', 'NNP', 'B-NP'), ('Meador', 'NNP', 'I-NP'), ...]
&gt;&gt;&gt; conllreader.iob_sents()
[[('Mr.', 'NNP', 'B-NP'), ('Meador', 'NNP', 'I-NP'), ('had', 'VBD', 'B-VP'), ('been', 'VBN', 'I-VP'), ('executive', 'JJ', 'B-NP'), ('vice', 'NN', 'I-NP'), ('president', 'NN', 'I-NP'), ('of', 'IN', 'B-PP'), ('Balcor', 'NNP', 'B-NP'), ('.', '.', 'O')]]
</pre></div><p>The previous code also shows the <code class="literal">iob_words()</code> and <code class="literal">iob_sents()</code> methods, which return lists of three tuples of <code class="literal">(word, pos, iob)</code>. The inheritance diagram for <code class="literal">ConllChunkCorpusReader</code> looks like the following, with most of the methods implemented by its superclass, <code class="literal">ConllCorpusReader</code>:</p><div class="mediaobject"><img src="graphics/3609OS_03_05.jpg" alt="There's more..."/></div><div class="section" title="Tree leaves"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec32"/>Tree leaves</h3></div></div></div><p>When it comes to chunk trees, the leaves of a tree are the tagged tokens. So if you want to get a list of all the tagged tokens in a tree, call the <a id="id167" class="indexterm"/>
<code class="literal">leaves()</code> method.</p><div class="informalexample"><pre class="programlisting">
&gt;&gt;&gt; reader.chunked_words()[0].leaves()
[('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS')]
&gt;&gt;&gt; reader.chunked_sents()[0].leaves()
[('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS'), ('have', 'VBP'), ('trimmed', 'VBN'), ('about', 'IN'), ('300', 'CD'), ('jobs', 'NNS'), (',', ','), ('the', 'DT'), ('spokesman', 'NN'), ('said', 'VBD'), ('.', '.')]
&gt;&gt;&gt; reader.chunked_paras()[0][0].leaves()
[('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS'), ('have', 'VBP'), ('trimmed', 'VBN'), ('about', 'IN'), ('300', 'CD'), ('jobs', 'NNS'), (',', ','), ('the', 'DT'), ('spokesman', 'NN'), ('said', 'VBD'), ('.', '.')]
</pre></div></div><div class="section" title="Treebank chunk corpus"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec33"/>Treebank chunk corpus</h3></div></div></div><p>The <code class="literal">nltk.corpus.treebank_chunk</code> corpus uses <code class="literal">ChunkedCorpusReader</code> to provide part-of-speech tagged words and noun phrase chunks of Wall Street Journal headlines. NLTK comes with a 5% sample from the Penn Treebank Project. You can find out more at <a class="ulink" href="http://www.cis.upenn.edu/~treebank/home.html">http://www.cis.upenn.edu/~treebank/home.html</a>.</p></div><div class="section" title="CoNLL2000 corpus"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec34"/>CoNLL2000 corpus</h3></div></div></div><a id="id168" class="indexterm"/><p>
<span class="strong"><strong>CoNLL</strong></span> stands for the <span class="strong"><strong>Conference on Computational Natural Language Learning</strong></span>. For the year 2000 conference, a shared task was undertaken to produce a corpus of chunks based on the Wall Street Journal corpus. In addition to noun phrases (<code class="literal">NP</code>), it also contains verb phrases (<code class="literal">VP</code>) and prepositional phrases (<code class="literal">PP</code>). This chunked corpus is available as <code class="literal">nltk.corpus.conll2000</code>, which is an instance of <code class="literal">ConllChunkCorpusReader</code>. You can read more at <a class="ulink" href="http://www.cnts.ua.ac.be/conll2000/chunking/">http://www.cnts.ua.ac.be/conll2000/chunking/</a>.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec97"/>See also</h2></div></div></div><p>
<a class="link" href="ch05.html" title="Chapter 5. Extracting Chunks">Chapter 5</a>, <span class="emphasis"><em>Extracting Chunks</em></span> will cover chunk extraction in detail. Also see the previous recipe for details on getting tagged tokens from a corpus reader.</p></div></div>
<div class="section" title="Creating a categorized text corpus"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec30"/>Creating a categorized text corpus</h1></div></div></div><a id="id169" class="indexterm"/><p>If you have a large corpus of text, you may want to categorize it into separate sections. The brown corpus, for example, has a number of different categories.</p><div class="informalexample"><pre class="programlisting">
&gt;
&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; brown.categories()
['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']
</pre></div><p>In this recipe, we'll learn how to create our own categorized text corpus.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec98"/>Getting ready</h2></div></div></div><p>The easiest way to categorize a corpus is to have one file for each category. Following are two excerpts from the <code class="literal">movie_reviews</code> corpus:</p><p>
<code class="literal">movie_pos.txt</code>
</p><div class="informalexample"><pre class="programlisting">the thin red line is flawed but it provokes .</pre></div><p>
<code class="literal">movie_neg.txt</code>
</p><div class="informalexample"><pre class="programlisting">a big-budget and glossy production can not make up for a lack of spontaneity that permeates their tv show .</pre></div><p>With these two files, we'll have two categories: <code class="literal">pos</code> and <code class="literal">neg</code>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec99"/>How to do it...</h2></div></div></div><a id="id170" class="indexterm"/><p>We'll use the <code class="literal">CategorizedPlaintextCorpusReader</code>, which inherits from both <code class="literal">PlaintextCorpusReader</code> and <code class="literal">CategorizedCorpusReader</code>. These two superclasses require three arguments: the root directory, the <code class="literal">fileids</code>, and a category specification.</p><div class="informalexample"><pre class="programlisting">
&gt;&gt;&gt; from nltk.corpus.reader import CategorizedPlaintextCorpusReader
&gt;&gt;&gt; reader = CategorizedPlaintextCorpusReader('.', r'movie_.*\.txt', cat_pattern=r'movie_(\w+)\.txt')
&gt;&gt;&gt; reader.categories()
['neg', 'pos']
&gt;&gt;&gt; reader.fileids(categories=['neg'])
['movie_neg.txt']
&gt;&gt;&gt; reader.fileids(categories=['pos'])
['movie_pos.txt']
</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec100"/>How it works...</h2></div></div></div><p>The first two arguments to <code class="literal">CategorizedPlaintextCorpusReader</code> are the root directory and <code class="literal">fileids</code>, which are passed on to the <code class="literal">PlaintextCorpusReader</code> to read in the files. The <code class="literal">cat_pattern</code> keyword argument is a regular expression for extracting the category names from the <code class="literal">fileids</code>. In our case, the category is the part of the <code class="literal">fileid</code> after <code class="literal">movie_</code> and before <code class="literal">.txt</code>. <span class="strong"><strong>The category must be surrounded by grouping parenthesis</strong></span>.</p><p>
<code class="literal">cat_pattern</code> is passed to <code class="literal">CategorizedCorpusReader</code>, which overrides the common corpus reader functions such as <code class="literal">fileids()</code>, <code class="literal">words()</code>, <code class="literal">sents()</code>, and <code class="literal">paras()</code> to accept a <code class="literal">categories</code> keyword argument. This way, you could get all the <code class="literal">pos</code> sentences by calling <code class="literal">reader.sents(categories=['pos'])</code>. <code class="literal">CategorizedCorpusReader</code> also provides the <code class="literal">categories()</code> function, which returns a list of all known categories in the corpus.</p><p>
<code class="literal">CategorizedPlaintextCorpusReader</code> is an example of using multiple-inheritance to join methods from multiple superclasses, as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/3609OS_03_06.jpg" alt="How it works..."/></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec101"/>There's more...</h2></div></div></div><p>Instead of <code class="literal">cat_pattern</code>, you could pass in a <code class="literal">cat_map</code>, which is a dictionary mapping a <code class="literal">fileid</code> to a list of category labels.</p><div class="informalexample"><pre class="programlisting">
&gt;&gt;&gt; reader = CategorizedPlaintextCorpusReader('.', r'movie_.*\.txt', cat_map={'movie_pos.txt': ['pos'], 'movie_neg.txt': ['neg']})
&gt;&gt;&gt; reader.categories()
['neg', 'pos']
</pre></div><div class="section" title="Category file"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec35"/>Category file</h3></div></div></div><a id="id171" class="indexterm"/><p>A third way of specifying categories is to use the <code class="literal">cat_file</code> keyword argument to specify a filename containing a mapping of <code class="literal">fileid</code> to category. For example, the <code class="literal">brown</code> corpus has a file called <code class="literal">cats.txt</code> that looks like this:</p><div class="informalexample"><pre class="programlisting">
ca44 news
cb01 editorial
</pre></div><p>The <code class="literal">reuters</code> corpus has files in multiple categories, and its <code class="literal">cats.txt</code> looks like this:</p><div class="informalexample"><pre class="programlisting">
test/14840 rubber coffee lumber palm-oil veg-oil
test/14841 wheat grain
</pre></div></div><div class="section" title="Categorized tagged corpus reader"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec36"/>Categorized tagged corpus reader</h3></div></div></div><a id="id172" class="indexterm"/><p>The <code class="literal">brown</code> corpus reader is actually an instance of <code class="literal">CategorizedTaggedCorpusReader</code>, which inherits from <code class="literal">CategorizedCorpusReader</code> and <code class="literal">TaggedCorpusReader</code>. Just like in <code class="literal">CategorizedPlaintextCorpusReader</code>, it overrides all the methods of <code class="literal">TaggedCorpusReader</code> to allow a <code class="literal">categories</code> argument, so you can call <code class="literal">brown.tagged_sents(categories=['news'])</code> to get all the tagged sentences from the <code class="literal">news</code> category. You can use the <code class="literal">CategorizedTaggedCorpusReader</code> just like <code class="literal">CategorizedPlaintextCorpusReader</code> for your own categorized and tagged text corpora.</p></div><div class="section" title="Categorized corpora"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec37"/>Categorized corpora</h3></div></div></div><a id="id173" class="indexterm"/><p>The <code class="literal">movie_reviews</code> corpus reader is an instance of <code class="literal">CategorizedPlaintextCorpusReader</code>, as is the <code class="literal">reuters</code> corpus reader. But where the <code class="literal">movie_reviews</code> corpus only has two categories (<code class="literal">neg</code> and <code class="literal">pos</code>), <code class="literal">reuters</code> has 90 categories. These corpora are often used for training and evaluating classifiers, which will be covered in <a class="link" href="ch07.html" title="Chapter 7. Text Classification">Chapter 7</a>, <span class="emphasis"><em>Text Classification</em></span>.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec102"/>See also</h2></div></div></div><p>In the next recipe, we'll create a subclass of <code class="literal">CategorizedCorpusReader</code> and <code class="literal">ChunkedCorpusReader</code> for reading a categorized chunk corpus. Also see <a class="link" href="ch07.html" title="Chapter 7. Text Classification">Chapter 7</a>, <span class="emphasis"><em>Text Classification</em></span> in which we use categorized text for classification.</p></div></div>
<div class="section" title="Creating a categorized chunk corpus reader"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec31"/>Creating a categorized chunk corpus reader</h1></div></div></div><a id="id174" class="indexterm"/><p>NLTK provides a <code class="literal">CategorizedPlaintextCorpusReader</code> and <code class="literal">CategorizedTaggedCorpusReader</code>, but there's no categorized corpus reader for chunked corpora. So in this recipe, we're going to make one.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec103"/>Getting ready</h2></div></div></div><p>Refer to the earlier recipe, <span class="emphasis"><em>Creating a chunked phrase corpus</em></span>, for an explanation of <code class="literal">ChunkedCorpusReader</code>, and to the previous recipe for details on <code class="literal">CategorizedPlaintextCorpusReader</code> and <code class="literal">CategorizedTaggedCorpusReader</code>, both of which inherit from <code class="literal">CategorizedCorpusReader</code>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec104"/>How to do it...</h2></div></div></div><a id="id175" class="indexterm"/><p>We'll create a class called <code class="literal">CategorizedChunkedCorpusReader</code> that inherits from both <code class="literal">CategorizedCorpusReader</code> and <code class="literal">ChunkedCorpusReader</code>. It is heavily based on the <code class="literal">CategorizedTaggedCorpusReader</code>, and also provides three additional methods for getting categorized chunks. The following code is found in <code class="literal">catchunked.py</code>:</p><div class="informalexample"><pre class="programlisting">
from nltk.corpus.reader import CategorizedCorpusReader, ChunkedCorpusReader

class CategorizedChunkedCorpusReader(CategorizedCorpusReader, ChunkedCorpusReader):
  def __init__(self, *args, **kwargs):
    CategorizedCorpusReader.__init__(self, kwargs)
    ChunkedCorpusReader.__init__(self, *args, **kwargs)

  def _resolve(self, fileids, categories):
    if fileids is not None and categories is not None:
      raise ValueError('Specify fileids or categories, not both')
    if categories is not None:
      return self.fileids(categories)
    else:
      return fileids
</pre></div><p>All of the following methods call the corresponding function in <code class="literal">ChunkedCorpusReader</code> with the value returned from <code class="literal">_resolve()</code>. We'll start with the plain text methods.</p><div class="informalexample"><pre class="programlisting">
  def raw(self, fileids=None, categories=None):
    return ChunkedCorpusReader.raw(self, self._resolve(fileids, categories))
  def words(self, fileids=None, categories=None):
    return ChunkedCorpusReader.words(self, self._resolve(fileids, categories))
  
  def sents(self, fileids=None, categories=None):
    return ChunkedCorpusReader.sents(self, self._resolve(fileids, categories))
  
  def paras(self, fileids=None, categories=None):
    return ChunkedCorpusReader.paras(self, self._resolve(fileids, categories))
</pre></div><p>Next comes the tagged text methods.</p><div class="informalexample"><pre class="programlisting">
  def tagged_words(self, fileids=None, categories=None, simplify_tags=False):
    return ChunkedCorpusReader.tagged_words(
      self, self._resolve(fileids, categories), simplify_tags)
  
  def tagged_sents(self, fileids=None, categories=None, simplify_tags=False):
    return ChunkedCorpusReader.tagged_sents(
      self, self._resolve(fileids, categories), simplify_tags)
    
  def tagged_paras(self, fileids=None, categories=None, simplify_tags=False):
    return ChunkedCorpusReader.tagged_paras(
      self, self._resolve(fileids, categories), simplify_tags)
</pre></div><p>And finally, the chunked methods, which is what we've really been after.</p><div class="informalexample"><pre class="programlisting">
  def chunked_words(self, fileids=None, categories=None):
    return ChunkedCorpusReader.chunked_words(
      self, self._resolve(fileids, categories))
  
  def chunked_sents(self, fileids=None, categories=None):
    return ChunkedCorpusReader.chunked_sents(
      self, self._resolve(fileids, categories))
  
  def chunked_paras(self, fileids=None, categories=None):
    return ChunkedCorpusReader.chunked_paras(
      self, self._resolve(fileids, categories))
</pre></div><p>All these methods together give us a complete <code class="literal">CategorizedChunkedCorpusReader</code>.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec105"/>How it works...</h2></div></div></div><a id="id176" class="indexterm"/><p>
<code class="literal">CategorizedChunkedCorpusReader</code> overrides all the <code class="literal">ChunkedCorpusReader</code> methods to take a <code class="literal">categories</code> argument for locating <code class="literal">fileids</code>. These <code class="literal">fileids</code> are found with the internal <code class="literal">_resolve()</code> function. This <code class="literal">_resolve()</code> function makes use of <code class="literal">CategorizedCorpusReader.fileids()</code> to return <code class="literal">fileids</code> for a given list of <code class="literal">categories</code>. If no <code class="literal">categories</code> are given, <code class="literal">_resolve()</code> just returns the given <code class="literal">fileids</code>, which could be <code class="literal">None</code>, in which case all files are read. The initialization of both <code class="literal">CategorizedCorpusReader</code> and <code class="literal">ChunkedCorpusReader</code> is what makes this all possible. If you look at the code for <code class="literal">CategorizedTaggedCorpusReader</code>, you'll see it's very similar. The inheritance diagram looks like this:</p><div class="mediaobject"><img src="graphics/3609OS_03_07.jpg" alt="How it works..."/></div><p>Here's some example code for using the <code class="literal">treebank</code> corpus. All we're doing is making categories out of the <code class="literal">fileids</code>, but the point is that you could use the same techniques to create your own categorized chunk corpus.</p><div class="informalexample"><pre class="programlisting">
&gt;&gt;&gt; import nltk.data
&gt;&gt;&gt; from catchunked import CategorizedChunkedCorpusReader
&gt;&gt;&gt; path = nltk.data.find('corpora/treebank/tagged')
&gt;&gt;&gt; reader = CategorizedChunkedCorpusReader(path, r'wsj_.*\.pos', cat_pattern=r'wsj_(.*)\.pos')
&gt;&gt;&gt; len(reader.categories()) == len(reader.fileids())
True
&gt;&gt;&gt; len(reader.chunked_sents(categories=['0001']))
16
</pre></div><a id="id177" class="indexterm"/><p>We use <code class="literal">nltk.data.find()</code> to search the data directories to get a <code class="literal">FileSystemPathPointer</code> to the <code class="literal">treebank</code> corpus. All the <code class="literal">treebank</code> tagged files start with <code class="literal">wsj_</code> followed by a number, and end with <code class="literal">.pos</code>. The previous code turns that file number into a category.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec106"/>There's more...</h2></div></div></div><p>As covered in the <span class="emphasis"><em>Creating a chunked phrase corpus</em></span> recipe, there's an alternative format and reader for a chunk corpus using IOB tags. To have a categorized corpus of IOB chunks, we have to make a new corpus reader.</p><div class="section" title="Categorized Conll chunk corpus reader"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec38"/>Categorized Conll chunk corpus reader</h3></div></div></div><a id="id178" class="indexterm"/><p>Here's a subclass of <code class="literal">CategorizedCorpusReader</code> and <code class="literal">ConllChunkReader</code> called <code class="literal">CategorizedConllChunkCorpusReader</code>. It overrides all methods of <code class="literal">ConllCorpusReader</code> that take a <code class="literal">fileids</code> argument, so the methods can also take a <code class="literal">categories</code> argument. The <code class="literal">ConllChunkCorpusReader</code> is just a small subclass of <code class="literal">ConllCorpusReader</code> that handles initialization; most of the work is done in <code class="literal">ConllCorpusReader</code>. This code can also be found in <code class="literal">catchunked.py</code>.</p><div class="informalexample"><pre class="programlisting">
from nltk.corpus.reader import CategorizedCorpusReader, ConllCorpusReader, ConllChunkCorpusReader

class CategorizedConllChunkCorpusReader(CategorizedCorpusReader, ConllChunkCorpusReader):
  def __init__(self, *args, **kwargs):
    CategorizedCorpusReader.__init__(self, kwargs)
    ConllChunkCorpusReader.__init__(self, *args, **kwargs)
  
  def _resolve(self, fileids, categories):
    if fileids is not None and categories is not None:
      raise ValueError('Specify fileids or categories, not both')
    if categories is not None:
      return self.fileids(categories)
    else:
      return fileids
</pre></div><p>All the following methods call the corresponding method of <code class="literal">ConllCorpusReader</code> with the value returned from <code class="literal">_resolve()</code>. We'll start with the plain text methods.</p><div class="informalexample"><pre class="programlisting">
  def raw(self, fileids=None, categories=None):
    return ConllCorpusReader.raw(self, self._resolve(fileids, categories))
  
  def words(self, fileids=None, categories=None):
    return ConllCorpusReader.words(self, self._resolve(fileids, categories))
  
  def sents(self, fileids=None, categories=None):
    return ConllCorpusReader.sents(self, self._resolve(fileids, categories))
</pre></div><p>The <code class="literal">ConllCorpusReader</code> does not recognize paragraphs, so there are no <code class="literal">*_paras()</code> methods. Next are the tagged and chunked methods.</p><div class="informalexample"><pre class="programlisting">
  def tagged_words(self, fileids=None, categories=None):
    return ConllCorpusReader.tagged_words(self, self._resolve(fileids, categories))
  
  def tagged_sents(self, fileids=None, categories=None):
    return ConllCorpusReader.tagged_sents(self, self._resolve(fileids, categories))
  
  def chunked_words(self, fileids=None, categories=None, chunk_types=None):
    return ConllCorpusReader.chunked_words(
      self, self._resolve(fileids, categories), chunk_types)
  
  def chunked_sents(self, fileids=None, categories=None, chunk_types=None):
    return ConllCorpusReader.chunked_sents(
      self, self._resolve(fileids, categories), chunk_types)
</pre></div><a id="id179" class="indexterm"/><p>For completeness, we must override the following methods of the <code class="literal">ConllCorpusReader</code>:</p><div class="informalexample"><pre class="programlisting">
  def parsed_sents(self, fileids=None, categories=None, pos_in_tree=None):
    return ConllCorpusReader.parsed_sents(
      self, self._resolve(fileids, categories), pos_in_tree)
  
  def srl_spans(self, fileids=None, categories=None):
    return ConllCorpusReader.srl_spans(self, self._resolve(fileids, categories))
  
  def srl_instances(self, fileids=None, categories=None, pos_in_tree=None, flatten=True):
    return ConllCorpusReader.srl_instances(
      self, self._resolve(fileids, categories), pos_in_tree, flatten)
  
  def iob_words(self, fileids=None, categories=None):
    return ConllCorpusReader.iob_words(self, self._resolve(fileids, categories))
  
  def iob_sents(self, fileids=None, categories=None):
    return ConllCorpusReader.iob_sents(self, self._resolve(fileids, categories))
</pre></div><p>The inheritance diagram for this class is as follows:</p><div class="mediaobject"><img src="graphics/3609OS_03_08.jpg" alt="Categorized Conll chunk corpus reader"/></div><p>Following is some example code using the <code class="literal">conll2000</code> corpus. Like with <code class="literal">treebank</code>, we're using the <code class="literal">fileids</code> for categories. The <code class="literal">ConllChunkCorpusReader</code> requires a third argument to specify the <code class="literal">chunk_types</code>. These <code class="literal">chunk_types</code> are used to parse the IOB tags. As you learned in the <span class="emphasis"><em>Creating a chunked phrase corpus</em></span> recipe, the <code class="literal">conll2000</code> corpus recognizes three chunk types:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">NP</code> for noun phrases</li><li class="listitem" style="list-style-type: disc"><code class="literal">VP</code> for verb phrases</li><li class="listitem" style="list-style-type: disc"><code class="literal">PP</code> for prepositional phrases</li></ul></div><div class="informalexample"><pre class="programlisting">
&gt;&gt;&gt; import nltk.data
&gt;&gt;&gt; from catchunked import CategorizedConllChunkCorpusReader
&gt;&gt;&gt; path = nltk.data.find('corpora/conll2000')
&gt;&gt;&gt; reader = CategorizedConllChunkCorpusReader(path, r'.*\.txt', ('NP','VP','PP'), cat_pattern=r'(.*)\.txt')
&gt;&gt;&gt; reader.categories()
['test', 'train']
&gt;&gt;&gt; reader.fileids()
['test.txt', 'train.txt']
&gt;&gt;&gt; len(reader.chunked_sents(categories=['test']))
2012
</pre></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec107"/>See also</h2></div></div></div><p>In the <span class="emphasis"><em>Creating a chunked phrase corpus</em></span> recipe in this chapter, we covered both the <code class="literal">ChunkedCorpusReader</code> and <code class="literal">ConllChunkCorpusReader</code>. And in the previous recipe, we covered <code class="literal">CategorizedPlaintextCorpusReader</code> and <code class="literal">CategorizedTaggedCorpusReader</code>, which share the same superclass used by <code class="literal">CategorizedChunkedCorpusReader</code> and <code class="literal">CategorizedConllChunkReader</code>—<code class="literal">CategorizedCorpusReader</code>.</p></div></div>
<div class="section" title="Lazy corpus loading"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec32"/>Lazy corpus loading</h1></div></div></div><p>Loading a corpus reader can be an expensive operation due to the number of files, file sizes, and various initialization tasks. And while you'll often want to specify a corpus reader in a common module, you don't always need to access it right away. To speed up module import time when a corpus reader is defined, NLTK provides a <a id="id180" class="indexterm"/>
<code class="literal">LazyCorpusLoader</code> class that can transform itself into your actual corpus reader as soon as you need it. This way, you can define a corpus reader in a common module without it slowing down module loading.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec108"/>How to do it...</h2></div></div></div><a id="id181" class="indexterm"/><p>
<code class="literal">LazyCorpusLoader</code> requires two arguments: the <code class="literal">name</code> of the corpus and the corpus reader class, plus any other arguments needed to initialize the corpus reader class.</p><p>The <code class="literal">name</code> argument specifies the root directory name of the corpus, which must be within a <code class="literal">corpora</code> subdirectory of one of the paths in <code class="literal">nltk.data.path</code>. See the first recipe of this chapter, <span class="emphasis"><em>Setting up a custom corpus</em></span>, for more details on <code class="literal">nltk.data.path</code>.</p><p>For example, if you have a custom corpora named <code class="literal">cookbook</code> in your local <code class="literal">nltk_data</code> directory, its path would be <code class="literal">~/nltk_data/corpora/cookbook</code>. You'd then pass <code class="literal">'cookbook'</code> to <code class="literal">LazyCorpusLoader</code> as the <code class="literal">name</code>, and <code class="literal">LazyCorpusLoader</code> will look in <code class="literal">~/nltk_data/corpora</code> for a directory named <code class="literal">'cookbook'</code>.</p><p>The second argument to <code class="literal">LazyCorpusLoader</code> is <code class="literal">reader_cls</code>, which should be the name of a subclass of <code class="literal">CorpusReader</code>, such as <code class="literal">WordListCorpusReader</code>. You will also need to pass in any other arguments required by the <code class="literal">reader_cls</code> for initialization. This will be demonstrated as follows, using the same <code class="literal">wordlist</code> file we created in the earlier recipe, <span class="emphasis"><em>Creating a word list corpus</em></span>. The third argument to <code class="literal">LazyCorpusLoader</code> is the list of filenames and <code class="literal">fileids</code> that will be passed in to <code class="literal">WordListCorpusReader</code> at initialization.</p><div class="informalexample"><pre class="programlisting">
&gt;&gt;&gt; from nltk.corpus.util import LazyCorpusLoader
&gt;&gt;&gt; from nltk.corpus.reader import WordListCorpusReader
&gt;&gt;&gt; reader = LazyCorpusLoader('cookbook', WordListCorpusReader, ['wordlist'])
&gt;&gt;&gt; isinstance(reader, LazyCorpusLoader)
True
&gt;&gt;&gt; reader.fileids()
['wordlist']
&gt;&gt;&gt; isinstance(reader, LazyCorpusLoader)
False
&gt;&gt;&gt; isinstance(reader, WordListCorpusReader)
True
</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec109"/>How it works...</h2></div></div></div><a id="id182" class="indexterm"/><p>
<code class="literal">LazyCorpusLoader</code> stores all the arguments given, but otherwise does nothing until you try to access an attribute or method. This way initialization is very fast, eliminating the overhead of loading the corpus reader immediately. As soon as you do access an attribute or method, it does the following:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Calls <code class="literal">nltk.data.find('corpora/%s' % name)</code> to find the corpus data root directory.</li><li class="listitem">Instantiate the corpus reader class with the root directory and any other arguments.</li><li class="listitem">Transforms itself into the corpus reader class.</li></ol></div><p>So in the previous example code, before we call <code class="literal">reader.fileids()</code>, <code class="literal">reader</code> is an instance of <code class="literal">LazyCorpusLoader</code>, but after the call, <code class="literal">reader</code> is an instance of <code class="literal">WordListCorpusReader</code>.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec110"/>There's more...</h2></div></div></div><p>All of the corpora included with NLTK and defined in <code class="literal">nltk.corpus</code> are initially an instance of <code class="literal">LazyCorpusLoader</code>. Here's some code from <code class="literal">nltk.corpus</code> defining the <code class="literal">treebank</code> corpora.</p><div class="informalexample"><pre class="programlisting">
treebank = LazyCorpusLoader(

    'treebank/combined', BracketParseCorpusReader, r'wsj_.*\.mrg',

    tag_mapping_function=simplify_wsj_tag)

treebank_chunk = LazyCorpusLoader(

    'treebank/tagged', ChunkedCorpusReader, r'wsj_.*\.pos',

    sent_tokenizer=RegexpTokenizer(r'(?&lt;=/\.)\s*(?![^\[]*\])', gaps=True),

    para_block_reader=tagged_treebank_para_block_reader)

treebank_raw = LazyCorpusLoader(

    'treebank/raw', PlaintextCorpusReader, r'wsj_.*')
</pre></div><p>As you can see, any number of additional arguments can be passed through by <code class="literal">LazyCorpusLoader</code> to its <code class="literal">reader_cls</code>.</p></div></div>
<div class="section" title="Creating a custom corpus view"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec33"/>Creating a custom corpus view</h1></div></div></div><a id="id183" class="indexterm"/><p>A <span class="strong"><strong>corpus view</strong></span> is a class wrapper around a corpus file that reads in blocks of tokens as needed. Its purpose is to provide a <span class="emphasis"><em>view</em></span> into a file without reading the whole file at once (since corpus files can often be quite large). If the corpus readers included by NLTK already meet all your needs, then you do not have to know anything about corpus views. But, if you have a custom file format that needs special handling, this recipe will show you how to create and use a custom corpus view. The main corpus view class is <code class="literal">StreamBackedCorpusView</code>, which opens a single file as a <span class="emphasis"><em>stream</em></span>, and maintains an internal cache of blocks it has read.</p><p>Blocks of tokens are read in with a <span class="emphasis"><em>block reader</em></span> function. <a id="id184" class="indexterm"/>A <span class="strong"><strong>block</strong></span> can be any piece of text, such as a paragraph or a line, and <a id="id185" class="indexterm"/>
<span class="strong"><strong>tokens</strong></span> are parts of a block, such as individual words. In the <span class="emphasis"><em>Creating a part-of-speech tagged word corpus</em></span> recipe, we discussed the default <code class="literal">para_block_reader</code> function of the <code class="literal">TaggedCorpusReader</code>, which reads lines from a file until it finds a blank line, then returns those lines as a single paragraph token. The actual block reader function is: <code class="literal">nltk.corpus.reader.util.read_blankline_block</code>. <code class="literal">TaggedCorpusReader</code> passes this block reader function into a <code class="literal">TaggedCorpusView</code> whenever it needs to read blocks from a file. <code class="literal">TaggedCorpusView</code> is a subclass of <code class="literal">StreamBackedCorpusView</code> that knows to split paragraphs of "word/tag" into <code class="literal">(word, tag)</code> tuples.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec111"/>How to do it...</h2></div></div></div><a id="id186" class="indexterm"/><p>We'll start with the simple case of a plain text file with a heading that should be ignored by the corpus reader. Let's make a file called <code class="literal">heading_text.txt</code> that looks like this:</p><div class="informalexample"><pre class="programlisting">
<code class="literal">A simple heading</code>

<code class="literal">Here is the actual text for the corpus.</code>

<code class="literal">Paragraphs are split by blanklines.</code>

<code class="literal">This is the 3rd paragraph.</code>
</pre></div><p>Normally we'd use the <code class="literal">PlaintextCorpusReader</code> but, by default, it will treat <code class="literal">A simple heading</code> as the first paragraph. To ignore this heading, we need to subclass the <code class="literal">PlaintextCorpusReader</code> so we can override its <code class="literal">CorpusView</code> class variable with our own <code class="literal">StreamBackedCorpusView</code> subclass. This code is found in <code class="literal">corpus.py</code>.</p><div class="informalexample"><pre class="programlisting">
from nltk.corpus.reader import PlaintextCorpusReader
from nltk.corpus.reader.util import StreamBackedCorpusView

class IgnoreHeadingCorpusView(StreamBackedCorpusView):
  def __init__(self, *args, **kwargs):
    StreamBackedCorpusView.__init__(self, *args, **kwargs)
    # open self._stream
    self._open()
    # skip the heading block
    self.read_block(self._stream)
    # reset the start position to the current position in the stream
    self._filepos = [self._stream.tell()]

class IgnoreHeadingCorpusReader(PlaintextCorpusReader):
  CorpusView = IgnoreHeadingCorpusView
</pre></div><p>To demonstrate that this works as expected, here's the code showing that the default <code class="literal">PlaintextCorpusReader</code> finds four paragraphs, while our <code class="literal">IgnoreHeadingCorpusReader</code> only has three paragraphs.</p><div class="informalexample"><pre class="programlisting">
&gt;&gt;&gt; from nltk.corpus.reader import PlaintextCorpusReader
&gt;&gt;&gt; plain = PlaintextCorpusReader('.', ['heading_text.txt'])
&gt;&gt;&gt; len(plain.paras())
4
&gt;&gt;&gt; from corpus import IgnoreHeadingCorpusReader
&gt;&gt;&gt; reader = IgnoreHeadingCorpusReader('.', ['heading_text.txt'])
&gt;&gt;&gt; len(reader.paras())
3
</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec112"/>How it works...</h2></div></div></div><p>The <code class="literal">PlaintextCorpusReader</code> by design has a <code class="literal">CorpusView</code> class variable that can be overridden by subclasses. So we do just that, and make our <code class="literal">IgnoreHeadingCorpusView</code> the <code class="literal">CorpusView</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note09"/>Note</h3><p>Most corpus readers do not have a <code class="literal">CorpusView</code> class variable because they require very specific corpus views.</p></div></div><p>The <code class="literal">IgnoreHeadingCorpusView</code> is a subclass of <code class="literal">StreamBackedCorpusView</code> that does the following on initialization:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Open the file using <code class="literal">self._open()</code>. This function is defined by <code class="literal">StreamBackedCorpusView</code>, and sets the internal instance variable <code class="literal">self._stream</code> to the opened file.</li><li class="listitem">Read one block with <code class="literal">read_blankline_block()</code>, which will read the heading as a paragraph, and move the stream's file position forward to the next block.</li><li class="listitem">Reset the start file position to the current position of <code class="literal">self._stream</code>. <code class="literal">self._filepos</code> is an internal index of where each block is in the file.</li></ol></div><p>Here's a diagram illustrating the relationships between the classes:</p><div class="mediaobject"><img src="graphics/3609OS_03_09.jpg" alt="How it works..."/></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec113"/>There's more...</h2></div></div></div><p>Corpus views can get a lot fancier and more complicated, but the core concept is the same: read <span class="emphasis"><em>blocks</em></span> from a <code class="literal">stream</code> to return a list of <span class="emphasis"><em>tokens</em></span>. There are a number of block readers provided in <code class="literal">nltk.corpus.reader.util</code>, but you can always create your own. If you do want to define your own block reader function, then you have two choices on how to implement it:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Define it as a separate function and pass it in to <code class="literal">StreamBackedCorpusView</code> as <code class="literal">block_reader</code>. This is a good option if your block reader is fairly simple, reusable, and doesn't require any outside variables or configuration.</li><li class="listitem">Subclass <code class="literal">StreamBackedCorpusView</code> and override the <code class="literal">read_block()</code> method. This is what many custom corpus views do because the block reading is highly specialized and requires additional functions and configuration, usually provided by the corpus reader when the corpus view is initialized.</li></ol></div><div class="section" title="Block reader functions"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec39"/>Block reader functions</h3></div></div></div><p>Following is a survey of most of the included block readers in <code class="literal">nltk.corpus.reader.util</code>. Unless otherwise noted, each block reader function takes a single argument: the <code class="literal">stream</code> to read from.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">read_whitespace_block()</code><a id="id187" class="indexterm"/> <a id="id188" class="indexterm"/> will read 20 lines from the stream, splitting each line into tokens by whitespace.</li><li class="listitem" style="list-style-type: disc"><code class="literal">read_wordpunct_block()</code><a id="id189" class="indexterm"/> <a id="id190" class="indexterm"/> reads 20 lines from the stream, splitting each line using <code class="literal">nltk.tokenize.wordpunct_tokenize()</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">read_line_block()</code> <a id="id191" class="indexterm"/><a id="id192" class="indexterm"/> reads 20 lines from the stream and returns them as a list, with each line as a token.</li><li class="listitem" style="list-style-type: disc"><code class="literal">read_blankline_block()</code><a id="id193" class="indexterm"/><a id="id194" class="indexterm"/> will read lines from the stream until it finds a blank line. It will then return a single token of all lines found combined into a single string.</li><li class="listitem" style="list-style-type: disc"><code class="literal">read_regexp_block()</code><a id="id195" class="indexterm"/><a id="id196" class="indexterm"/> takes two additional arguments, which must be regular expressions that can be passed to <code class="literal">re.match()</code>: a <code class="literal">start_re</code> and <code class="literal">end_re</code>. <code class="literal">start_re</code> matches the starting line of a block, and <code class="literal">end_re</code> matches the ending line of the block. <code class="literal">end_re</code> defaults to <code class="literal">None</code>, in which case the block will end as soon as a new <code class="literal">start_re</code> match is found. The return value is a single token of all lines in the block joined into a single string.</li></ul></div></div><div class="section" title="Pickle corpus view"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec40"/>Pickle corpus view</h3></div></div></div><a id="id197" class="indexterm"/><p>If you want to have a corpus of pickled objects, you can use the <code class="literal">PickleCorpusView</code>, a subclass of <code class="literal">StreamBackedCorpusView</code> found in <code class="literal">nltk.corpus.reader.util</code>. A file consists of blocks of pickled objects, and can be created with the <code class="literal">PickleCorpusView.write()</code> class method, which takes a sequence of objects and an output file, then pickles each object using <code class="literal">pickle.dump()</code> and writes it to the file. It overrides the <code class="literal">read_block()</code> method to return a list of unpickled objects from the stream, using <code class="literal">pickle.load()</code>.</p></div><div class="section" title="Concatenated corpus view"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec41"/>Concatenated corpus view</h3></div></div></div><p>Also found in <code class="literal">nltk.corpus.reader.util</code> is the <code class="literal">ConcatenatedCorpusView</code>. This class is useful if you have multiple files that you want a corpus reader to treat as a single file. A <code class="literal">ConcatenatedCorpusView</code> is created by giving it a list of <code class="literal">corpus_views</code>, which are then iterated over as if they were a single view.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec114"/>See also</h2></div></div></div><p>The concept of block readers was introduced in the <span class="emphasis"><em>Creating a part-of-speech tagged word corpus</em></span> recipe in this chapter.</p></div></div>
<div class="section" title="Creating a MongoDB backed corpus reader"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec34"/>Creating a MongoDB backed corpus reader</h1></div></div></div><p>All the corpus readers we've dealt with so far have been file-based. That is in part due to the design of the <code class="literal">CorpusReader</code> base class, and also the assumption that most corpus data will be in text files. But sometimes you'll have a bunch of data stored in a database that you want to access and use just like a text file corpus. In this recipe, we'll cover the case where you have documents in MongoDB, and you want to use a particular field of each document as your block of text.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec115"/>Getting ready</h2></div></div></div><a id="id198" class="indexterm"/><p>MongoDB is a document-oriented database that has become a popular alternative to relational databases such as MySQL. The installation and setup of MongoDB is outside the scope of this book, but you can find instructions at <a class="ulink" href="http://www.mongodb.org/display/DOCS/Quickstart">http://www.mongodb.org/display/DOCS/Quickstart</a>.</p><p>You'll also need to install PyMongo, a Python driver for MongoDB. You should be able to do this with either <code class="literal">easy_install</code> or <code class="literal">pip</code>, by doing <code class="literal">sudo easy_install pymongo</code> or <code class="literal">sudo pip install pymongo</code>.</p><p>The code in the <span class="emphasis"><em>How to do it...</em></span> section assumes that your database is on <code class="literal">localhost</code> port <code class="literal">27017</code>, which is the MongoDB default configuration, and that you'll be using the <code class="literal">test</code> database with a collection named <code class="literal">corpus</code> that contains documents with a <code class="literal">text</code> field. Explanations for these arguments are available in the PyMongo documentation at <a class="ulink" href="http://api.mongodb.org/python/">http://api.mongodb.org/python/</a>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec116"/>How to do it...</h2></div></div></div><p>Since the <code class="literal">CorpusReader</code> class assumes you have a file-based corpus, we can't directly subclass it. Instead, we're going to emulate both the <code class="literal">StreamBackedCorpusView</code> and <code class="literal">PlaintextCorpusReader</code>. <code class="literal">StreamBackedCorpusView</code> is a subclass of <code class="literal">nltk.util.AbstractLazySequence</code>, so we'll subclass <code class="literal">AbstractLazySequence</code> to create a MongoDB view, and then create a new class that will use the view to provide functionality similar to the <code class="literal">PlaintextCorpusReader</code>. This code is found in <code class="literal">mongoreader.py</code>.</p><div class="informalexample"><pre class="programlisting">
import pymongo
from nltk.data import LazyLoader
from nltk.tokenize import TreebankWordTokenizer
from nltk.util import AbstractLazySequence, LazyMap, LazyConcatenation

class MongoDBLazySequence(AbstractLazySequence):
  def __init__(self, host='localhost', port=27017, db='test', collection='corpus', field='text'):
    self.conn = pymongo.Connection(host, port)
    self.collection = self.conn[db][collection]
    self.field = field

  def __len__(self):
    return self.collection.count()

  def iterate_from(self, start):
    f = lambda d: d.get(self.field, '')
    return iter(LazyMap(f, self.collection.find(fields=[self.field], skip=start)))

class MongoDBCorpusReader(object):
  def __init__(self, word_tokenizer=TreebankWordTokenizer(),
         sent_tokenizer=LazyLoader('tokenizers/punkt/english.pickle'),
         **kwargs):
    self._seq = MongoDBLazySequence(**kwargs)
    self._word_tokenize = word_tokenizer.tokenize
    self._sent_tokenize = sent_tokenizer.tokenize

  def text(self):
    return self._seq

  def words(self):
    return LazyConcatenation(LazyMap(self._word_tokenize, self.text()))

  def sents(self):
    return LazyConcatenation(LazyMap(self._sent_tokenize, self.text()))
</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec117"/>How it works...</h2></div></div></div><a id="id199" class="indexterm"/><p>
<code class="literal">AbstractLazySequence</code> is an abstract class that provides read-only, on-demand iteration. Subclasses must implement the <code class="literal">__len__()</code> and <code class="literal">iterate_from(start)</code> methods, while it provides the rest of the list and iterator emulation methods. By creating the <code class="literal">MongoDBLazySequence</code> subclass as our view, we can iterate over documents in the MongoDB collection on-demand, without keeping all the documents in memory. <code class="literal">LazyMap</code> is a lazy version of Python's built-in <code class="literal">map()</code> function, and is used in <code class="literal">iterate_from()</code> to transform the document into the specific field that we're interested in. It's also a subclass of <code class="literal">AbstractLazySequence</code>.</p><a id="id200" class="indexterm"/><p>The <code class="literal">MongoDBCorpusReader</code> creates an internal instance of <code class="literal">MongoDBLazySequence</code> for iteration, then defines the word and sentence tokenization methods. The <code class="literal">text()</code> method simply returns the instance of <code class="literal">MongoDBLazySequence</code>, which results in a lazily evaluated list of each text field. The <code class="literal">words()</code> method uses <code class="literal">LazyMap</code> and <code class="literal">LazyConcatenation</code> to return a lazily evaluated list of all words, while the <code class="literal">sents()</code> method does the same for sentences. The <code class="literal">sent_tokenizer</code> is loaded on demand with <code class="literal">LazyLoader</code>, which is a wrapper around <code class="literal">nltk.data.load()</code>, analogous to <code class="literal">LazyCorpusLoader</code>. <code class="literal">LazyConcatentation</code> is a subclass of <code class="literal">AbstractLazySequence</code> too, and produces a flat list from a given list of lists (each list may also be lazy). In our case, we're concatenating the results of <code class="literal">LazyMap</code> to ensure we don't return nested lists.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec118"/>There's more...</h2></div></div></div><p>All of the parameters are configurable. For example, if you had a <code class="literal">db</code> named <code class="literal">website</code>, with a <code class="literal">collection</code> named <code class="literal">comments</code>, whose documents had a <code class="literal">field</code> called <code class="literal">comment</code>, you could create a <code class="literal">MongoDBCorpusReader</code> as follows:</p><div class="informalexample"><pre class="programlisting">
&gt;&gt;&gt; reader = MongoDBCorpusReader(db='website', collection='comments', field='comment')
</pre></div><p>You can also pass in custom instances for <code class="literal">word_tokenizer</code> and <code class="literal">sent_tokenizer</code>, as long as the objects implement the <code class="literal">nltk.tokenize.TokenizerI</code> interface by providing a <code class="literal">tokenize(text)</code> method.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec119"/>See also</h2></div></div></div><p>Corpus views were covered in the previous recipe, and tokenization was covered in <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <span class="emphasis"><em>Tokenizing Text and WordNet Basics</em></span>.</p></div></div>
<div class="section" title="Corpus editing with file locking"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec35"/>Corpus editing with file locking</h1></div></div></div><a id="id201" class="indexterm"/><p>Corpus readers and views are all read-only, but there may be times when you want to add to or edit the corpus files. However, modifying a corpus file while other processes are using it, such as through a corpus reader, can lead to dangerous undefined behavior. This is where file locking comes in handy.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec120"/>Getting ready</h2></div></div></div><p>You must install the <code class="literal">lockfile</code> library using <code class="literal">sudo easy_install lockfile</code> or <code class="literal">sudo pip install lockfile</code>. This library provides cross-platform file locking, and so will work on Windows, Unix/Linux, Mac OX, and more. You can find detailed documentation on <code class="literal">lockfile </code>at <code class="literal">http://packages.python.or</code>
<a class="ulink" href="http://g/lockfile/">g/lockfile/</a>.</p><p>For the following code to work, you must also have Python 2.6. Versions 2.4 and earlier do not support the <code class="literal">with</code> keyword.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec121"/>How to do it...</h2></div></div></div><p>Here are two file editing functions: <code class="literal">append_line()</code> and <code class="literal">remove_line()</code>. Both try to acquire an <span class="emphasis"><em>exclusive lock</em></span> on the file before updating it. <a id="id202" class="indexterm"/>An <span class="strong"><strong>exclusive lock</strong></span> means that these functions will wait until no other process is reading from or writing to the file. Once the lock is acquired, any other process that tries to access the file will have to wait until the lock is released. This way, modifying the file will be safe and not cause any undefined behavior in other processes. These functions can be found in <code class="literal">corpus.py</code>.</p><div class="informalexample"><pre class="programlisting">
import lockfile, tempfile, shutil

def append_line(fname, line):
  with lockfile.FileLock(fname):
    fp = open(fname, 'a+')
    fp.write(line)
    fp.write('\n')
    fp.close()

def remove_line(fname, line):

  with lockfile.FileLock(fname):
    tmp = tempfile.TemporaryFile()
    fp = open(fname, 'r+')
    # write all lines from orig file, except if matches given line
    for l in fp:
      if l.strip() != line:
        tmp.write(l)
    
    # reset file pointers so entire files are copied
    fp.seek(0)
    tmp.seek(0)
    # copy tmp into fp, then truncate to remove trailing line(s)
    shutil.copyfileobj(tmp, fp)
    fp.truncate()
    fp.close()
    tmp.close()
</pre></div><a id="id203" class="indexterm"/><p>The lock acquiring and releasing happens transparently when you do <code class="literal">with lockfile.FileLock(fname)</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note10"/>Note</h3><p>Instead of using <code class="literal">with lockfile.FileLock(fname)</code>, you can also get a lock by calling <code class="literal">lock = lockfile.FileLock(fname)</code>, then call <code class="literal">lock.acquire()</code> to acquire the lock, and <code class="literal">lock.release()</code> to release the lock. This alternative usage is compatible with Python 2.4.</p></div></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec122"/>How it works...</h2></div></div></div><p>You can use these functions as follows:</p><div class="informalexample"><pre class="programlisting">
&gt;&gt;&gt; from corpus import append_line, remove_line
&gt;&gt;&gt; append_line('test.txt', 'foo')
&gt;&gt;&gt; remove_line('test.txt', 'foo')
</pre></div><p>In <code class="literal">append_line()</code>,<a id="id204" class="indexterm"/> a lock is acquired, the file is opened in <span class="emphasis"><em>append mode</em></span>, the text is written along with an end-of-line character, and then the file is closed, releasing the lock.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip02"/>Tip</h3><p>A lock acquired by <code class="literal">lockfile</code> only protects the file from other processes that also use <code class="literal">lockfile</code>. In other words, just because your Python process has a lock with <code class="literal">lockfile</code>, doesn't mean a non-Python process can't modify the file. For this reason, it's best to only use <code class="literal">lockfile</code> with files that will not be edited by any non-Python processes, or Python processes that do not use <code class="literal">lockfile</code>.</p></div></div><a id="id205" class="indexterm"/><a id="id206" class="indexterm"/><p>
<code class="literal">The remove_line()</code> function is a bit more complicated. Because we're removing a line and not a specific section of the file, we need to iterate over the file to find each instance of the line to remove. The easiest way to do this while writing the changes back to the file, is to use a <code class="literal">TemporaryFile</code> to hold the changes, then copy that file back into the original file using <code class="literal">shutil.copyfileobj()</code>.</p><p>These functions are best suited for a word list corpus, or some other corpus type with presumably unique lines, that may be edited by multiple people at about the same time, such as through a web interface. Using these functions with a more document-oriented corpus such as <code class="literal">brown</code>, <code class="literal">treebank</code>, or <code class="literal">conll2000</code>, is probably a bad idea.</p></div></div></body></html>