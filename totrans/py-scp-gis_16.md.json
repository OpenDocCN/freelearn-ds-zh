["```py\ncurl -fsSL https://get.docker.com/ | sh\n```", "```py\nsudo systemctl start docker\n```", "```py\nyour username  ALL=(ALL) ALL\n```", "```py\ndocker load -i <image name>\n```", "```py\ndocker images\n```", "```py\ndocker ps docker inspect <container ID> \n```", "```py\ndocker inspect $(docker ps --format \"{{.ID}}\") --format=\"{{json .NetworkSettings.IPAddress}}\"\n```", "```py\necho '172.17.0.2 sandbox.hortonworks.com sandbox-hdp.hortonworks.com sandbox-hdf.hortonworks.com' | sudo tee -a /etc/hosts\n```", "```py\nsudo sh start_sandbox-hdp.sh\n```", "```py\nssh raj_ops@127.0.0.1 -p2222\n```", "```py\n[raj_ops@sandbox-hdp ~]$\n```", "```py\n40, Paul\n23, Fred\n72, Mary\n16, Helen\n16, Steve \n```", "```py\nSELECT * FROM sample_07\n```", "```py\nunzip gis-tools-for-hadoop-master.zip\nunzip gis-tools-for-hadoop-master.zip -d /home/pcrickard\n```", "```py\nadd jar hdfs:///esri-geometry-api-2.0.0.jar;\nadd jar hdfs:///spatial-sdk-json-2.0.0.jar;\nadd jar hdfs:///spatial-sdk-hive-2.0.0.jar;\n\ncreate temporary function ST_Point as 'com.esri.hadoop.hive.ST_Point';\ncreate temporary function ST_Contains as 'com.esri.hadoop.hive.ST_Contains';\n\ndrop table earthquakes;\ndrop table counties;\n\nCREATE TABLE earthquakes (earthquake_date STRING, latitude DOUBLE, longitude DOUBLE, depth DOUBLE, magnitude DOUBLE,magtype string, mbstations string, gap string, distance string, rms string, source string, eventid string)\nROW FORMAT DELIMITED FIELDS TERMINATED BY ','\nSTORED AS TEXTFILE;\n\nCREATE TABLE counties (Area string, Perimeter string, State string, County string, Name string, BoundaryShape binary)                  \nROW FORMAT SERDE 'com.esri.hadoop.hive.serde.EsriJsonSerDe'\nSTORED AS INPUTFORMAT 'com.esri.json.hadoop.EnclosedEsriJsonInputFormat'\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat';\n\nLOAD DATA LOCAL INPATH '/gis-tools-for-hadoop-master/samples/data/earthquake-data/earthquakes.csv' OVERWRITE INTO TABLE earthquakes;\n\nLOAD DATA LOCAL INPATH '/gis-tools-for-hadoop-master/samples/data/counties-data/california-counties.json' OVERWRITE INTO TABLE counties;\n\nSELECT counties.name, count(*) cnt FROM counties\nJOIN earthquakes\nWHERE ST_Contains(counties.boundaryshape, ST_Point(earthquakes.longitude, earthquakes.latitude))\nGROUP BY counties.name\nORDER BY cnt desc;\n```", "```py\nadd jar hdfs:///esri-geometry-api-2.0.0.jar;\nadd jar hdfs:///spatial-sdk-json-2.0.0.jar;\nadd jar hdfs:///spatial-sdk-hive-2.0.0.jar;\n\ncreate temporary function ST_Point as 'com.esri.hadoop.hive.ST_Point';\ncreate temporary function ST_Contains as 'com.esri.hadoop.hive.ST_Contains';\n\n```", "```py\ndrop table earthquakes;\ndrop table counties; \n```", "```py\nCREATE TABLE earthquakes (earthquake_date STRING, latitude DOUBLE, longitude DOUBLE, depth DOUBLE, magnitude DOUBLE,magtype string, mbstations string, gap string, distance string, rms string, source \nstring, eventid string)\nROW FORMAT DELIMITED FIELDS TERMINATED BY ','\nSTORED AS TEXTFILE;\n```", "```py\nCREATE TABLE counties (Area string, Perimeter string, State string, County string, Name string, BoundaryShape binary)                  \nROW FORMAT SERDE 'com.esri.hadoop.hive.serde.EsriJsonSerDe'\nSTORED AS INPUTFORMAT 'com.esri.json.hadoop.EnclosedEsriJsonInputFormat'\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat';\n```", "```py\nLOAD DATA LOCAL INPATH '/gis-tools-for-hadoop-master/samples/data/earthquake-data/earthquakes.csv' OVERWRITE INTO TABLE earthquakes;\n\nLOAD DATA LOCAL INPATH '/gis-tools-for-hadoop-master/samples/data/counties-data/california-counties.json' OVERWRITE INTO TABLE counties;\n\n```", "```py\n SELECT counties.name, count(*) cnt FROM counties\n JOIN earthquakes\n WHERE ST_Contains(counties.boundaryshape, \n ST_Point(earthquakes.longitude, earthquakes.latitude))\n GROUP BY counties.name\n ORDER BY cnt desc;\n```", "```py\nconda install -c blaze pyhive\n```", "```py\nconda install -c blaze sasl\n```", "```py\nconda install -c conda-forge pywebhdfs\n```", "```py\nfrom pywebhdfs.webhdfs import PyWebHdfsClient as h\nhdfs=h(host='sandbox.hortonworks.com',port='50070',user_name='raj_ops')\n```", "```py\nls=hdfs.list_dir('/')\n```", "```py\nls['FileStatuses']['FileStatus'][0]\n```", "```py\n{'accessTime': 0, 'blockSize': 0, 'childrenNum': 1, 'fileId': 16404, 'group': 'hadoop', 'length': 0, 'modificationTime': 1510325976603, 'owner': 'yarn', 'pathSuffix': 'app-logs', 'permission': '777', 'replication': 0, 'storagePolicy': 0, 'type': 'DIRECTORY'}\n```", "```py\nhdfs.make_dir('/samples',permission=755)\nf=open('/home/pcrickard/sample.csv')\nd=f.read()\nhdfs.create_file('/samples/sample.csv',d)\n```", "```py\nhdfs.read_file('/samples/sample.csv')\n```", "```py\nhdfs.get_file_dir_status('/samples/sample.csv')\n```", "```py\n{'FileStatus': {'accessTime': 1517929744092, 'blockSize': 134217728, 'childrenNum': 0, 'fileId': 22842, 'group': 'hdfs', 'length': 47, 'modificationTime': 1517929744461, 'owner': 'raj_ops', 'pathSuffix': '', 'permission': '755', 'replication': 1, 'storagePolicy': 0, 'type': 'FILE'}}\n```", "```py\nfrom pyhive import hive\nc=hive.connect('sandbox.hortonworks.com').cursor()\nc.execute('CREATE TABLE FromPython (age int, name string)  ROW FORMAT DELIMITED FIELDS TERMINATED BY \",\"')\n```", "```py\nc.execute(\"LOAD DATA INPATH '/samples/sample.csv' OVERWRITE INTO TABLE FromPython\")\nc.execute(\"SELECT * FROM FromPython\")\nresult=c.fetchall()\n```", "```py\n[(40, ' Paul'), (23, ' Fred'), (72, ' Mary'), (16, ' Helen'), (16, ' Steve')]\n```"]