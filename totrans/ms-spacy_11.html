<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer129">
			<h1 id="_idParaDest-135"><a id="_idTextAnchor137"/>Chapter <a id="_idTextAnchor138"/>8: Text Classification with spaCy</h1>
			<p>This chapter is devoted to a very basic and popular task of NLP: text classification. You will first learn how to train spaCy's text classifier component, <strong class="source-inline">TextCategorizer</strong>. For this, you will learn how to prepare data and feed the data to the classifier; then we'll proceed to train the classifier. You'll also practice your new <strong class="source-inline">TextCategorizer</strong> skills on a popular dataset for sentiment analysis. </p>
			<p>Next, you will also do text classification with the popular framework TensorFlow's Keras API together with spaCy. You will learn the basics of neural networks, sequential data modeling with LSTMs, and how to prepare text for machine learning tasks with Keras's text preprocessing module. You will also learn how to design a neural network with <strong class="source-inline">tf.keras</strong>.</p>
			<p>Following that, we will then make an end-to-end text classification experiment, from data preparation to preprocessing text with Keras <strong class="source-inline">Tokenizer</strong>, neural network designing, model training, and interpreting the classification results. That's a whole package of machine learning!</p>
			<p>In this chapter, we're going to cover the following main topics: </p>
			<ul>
				<li>Understanding the basics of text classification</li>
				<li>Training the spaCy text classifier</li>
				<li>Sentiment analysis with spaCy</li>
				<li>Text classification with spaCy and Keras</li>
			</ul>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor139"/>Technical requirements</h1>
			<p>The code in the sections <em class="italic">Training the spaCy text classifier</em> and <em class="italic">Sentiment analysis with spaCy</em> is spaCy v3.0 compatible. </p>
			<p>The section <em class="italic">Text classification with spaCy and Keras</em> requires the following Python libraries:</p>
			<ul>
				<li>TensorFlow &gt;=2.2.0 </li>
				<li>NumPy</li>
				<li>pandas</li>
				<li>Matplotlib</li>
			</ul>
			<p>You can install the latest version of these libraries with <strong class="source-inline">pip</strong> as follows:</p>
			<p class="source-code">pip install tensorflow</p>
			<p class="source-code">pip install numpy</p>
			<p class="source-code">pip install pandas</p>
			<p class="source-code">pip install matplotlib</p>
			<p>We also use Jupyter notebooks in the last two sections. You can follow the instructions on the Jupyter website (<a href="https://jupyter.org/install">https://jupyter.org/install</a>) to install the Jupyter notebook onto your system. If you don't want to use notebooks, you can copy-paste code as Python code as well.</p>
			<p>You can find the chapter code and data files in the book's GitHub repository at <a href="https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter08">https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter08</a>.</p>
			<p>Let's get started with spaCy's text classifier component first, then we'll transition to designing our own neural network.</p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor140"/>Understanding the basics of text classification </h1>
			<p>Text classification is <a id="_idIndexMarker489"/>the task of assigning a set of predefined labels to text. Given a set of predefined classes and some text, you want to understand which predefined class this text falls into. We have to determine the classes ourselves by the nature of our data before starting the classification task. For example, a customer review can be positive, negative, or neutral. </p>
			<p>Text classifiers are used for detecting spam emails in your mailbox, determining the sentiment of customer's reviews, understanding customer's intent, sorting customer's complaint tickets, and so on.</p>
			<p>Text classification is a fundamental task of NLP. It is gaining importance in the business world, as it enables businesses to automate their processes. One immediate example is spam filters. Every day, users receive many spam emails but most of the time never see these emails and don't get any notifications because spam filters save the users from bothering about irrelevant emails and from spending time deleting these emails.</p>
			<p>Text classifiers can come in different flavors. Some classifiers focus on the overall emotion of the text, some classifiers focus on detecting the language of the text, and some classifiers <a id="_idIndexMarker490"/>focus on only some words of the text, such as verbs. The following are some of the most common types of text classification and their use cases:</p>
			<ul>
				<li><strong class="bold">Topic detection</strong>: Topic detection is the task of understanding the topic of a given text. For <a id="_idIndexMarker491"/>example, the text in a customer email could be asking about a refund, asking for a past bill, or simply complaining about the customer service.</li>
				<li><strong class="bold">Sentiment analysis</strong>: Sentiment analysis is the task of understanding whether the text <a id="_idIndexMarker492"/>contains positive or negative emotions about a given subject. Sentiment analysis is used often to analyze customer reviews about products and services.</li>
				<li><strong class="bold">Language detection</strong>: Language <a id="_idIndexMarker493"/>detection is the first step of many NLP systems, such as machine translation. </li>
			</ul>
			<p>The following figure shows a text classifier for a customer service automation system: </p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="Images/B16570_8_1.jpg" alt="Figure 8.1 – Topic detection is used to label a customer complaint with a predefined label &#13;&#10;" width="1299" height="419"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Topic detection is used to label a customer complaint with a predefined label </p>
			<p>Coming to the technical details, text classification is a <em class="italic">supervised</em> machine learning task. It means that the classifier can predict the class label of a text based on <em class="italic">example</em><strong class="bold"> </strong>input text-class <a id="_idIndexMarker494"/>label pairs. Hence, to train a text classifier, we need a <em class="italic">labeled dataset</em>. A labeled dataset is basically a list of text-label pairs. Here is an example dataset of five training sentences with their labels:</p>
			<p class="source-code">This shampoo is great for hair.</p>
			<p class="source-code">                        POSITIVE</p>
			<p class="source-code">I loved this shampoo, best product ever!</p>
			<p class="source-code">         POSITIVE</p>
			<p class="source-code">My hair has never been better, great product. POSITIVE</p>
			<p class="source-code">This product make my scalp itchy.</p>
			<p class="source-code">                    NEGATIVE</p>
			<p class="source-code">Not the best quality for this price.</p>
			<p class="source-code">                     NEGATIVE</p>
			<p>Then we train the classifier by showing the text and the corresponding class labels to the classifier. When the classifier sees new text that was not in the training text, it then predicts the class label of this unseen text based on the examples it saw during the training phase. The output of a text classifier is <em class="italic">always</em> a class label.</p>
			<p>Text classification <a id="_idIndexMarker495"/>can also be divided into three categories depending on the number of classes used: </p>
			<ul>
				<li><strong class="bold">Binary text classification</strong> means <a id="_idIndexMarker496"/>that we want to categorize our text into two classes. </li>
				<li><strong class="bold">Multiclass text classification</strong> means that there are more than two classes. Each class <a id="_idIndexMarker497"/>is mutually exclusive – one text can belong to one class only. Equivalently, a training instance can be labeled with only one class label. An example is rating customer reviews. A review can have 1, 2, 3, 4, or 5 stars (each star category is a class).</li>
				<li><strong class="bold">Multilabel text classification</strong> is a generalization of multiclass classification, where multiple labels can be assigned to each example text. For example, classifying <a id="_idIndexMarker498"/>toxic social media messages is done with multiple labels. This way, our model can distinguish different levels of toxicity. Class labels are typically toxic, severe toxic, insult, threat, obscenity. A message can include both insults and threats, or be classed as insult, toxicity, and obscenity, and so on. Hence for this problem, using multiple classes is more suitable.</li>
			</ul>
			<p>Labels are the name of the classes we want to see as the output. A class label can be categorical (string) or numerical (a number). Here are some commonly used class labels:</p>
			<ul>
				<li>For sentiment analysis, we usually use the class labels positive and negative. Their abbreviations, pos and neg, are also commonly used. Binary class labels are popular as well – 0 means negative sentiment and 1 means positive sentiment.</li>
				<li>The same applies to binary classification problems. We usually use 0-1 for class labels.</li>
				<li>For multiclass and multilabel problems, we usually name the classes with a meaningful name. For a movie genre classifier, we can use the labels family, international, Sunday evening, Disney, action, and so on. Numbers are used as labels as well. For a five-class classification problem, we can use the labels 1, 2, 3, 4, and 5.</li>
			</ul>
			<p>Now we've <a id="_idIndexMarker499"/>covered the basic concepts of text classification, let's do some coding! In the next section, we'll explore how to train spaCy's text classifier component.</p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor141"/>Training the spaCy text classifier</h1>
			<p>In this section, we will learn about the details of spaCy's text classifier component <strong class="source-inline">TextCategorizer</strong>. In <a href="B16570_02_Final_JM_ePub.xhtml#_idTextAnchor037"><em class="italic">Chapter 2</em></a>, <em class="italic">Core Operations with spaCy</em>, we saw that the spaCy NLP pipeline consists of components. In <a href="B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055"><em class="italic">Chapter 3</em></a>, <em class="italic">Linguistic Features</em>, we learned about <a id="_idIndexMarker500"/>the essential components of the spaCy NLP pipeline, which are the sentence tokenizer, POS tagger, dependency parser, and <strong class="bold">named entity recogition</strong> (<strong class="bold">NER</strong>). </p>
			<p><strong class="source-inline">TextCategorizer</strong> is an optional and trainable pipeline component. In order to train it, we need to provide examples and their class labels. We first add <strong class="source-inline">TextCategorizer</strong> to the NLP pipeline and then do the training procedure. <em class="italic">Figure 8.2</em> shows where exactly the <strong class="source-inline">TextCategorizer</strong> component lies in the NLP pipeline; this component comes after the essential components. In the following diagram, <strong class="bold">textcat</strong> refers to the <strong class="source-inline">TextCategorizer</strong> component.</p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="Images/B16570_8_2.jpg" alt="Figure 8.2 – TextCategorizer in the nlp pipeline&#13;&#10;" width="1131" height="168"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – TextCategorizer in the nlp pipeline</p>
			<p>A neural network architecture lies behind spaCy's <strong class="source-inline">TextCategorizer</strong>. <strong class="source-inline">TextCategorizer</strong> provides us with user-friendly and end-to-end approaches to train the classifier, so we don't have to deal directly with the neural network architecture. We'll design our own neural network architecture in the upcoming <em class="italic">Text classification with spaCy and Keras</em> section. After looking at the architecture, we’re ready to dive into TextCategorizer code. Let’s get to know <strong class="source-inline">TextCategorizer</strong> class first.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor142"/>Getting to know TextCategorizer class</h2>
			<p>Now let's get <a id="_idIndexMarker501"/>to know the <strong class="source-inline">TextCategorizer</strong> class in detail. First of all, we import <strong class="source-inline">TextCategorizer</strong> from the pipeline components:</p>
			<p class="source-code">from spacy.pipeline.textcat import DEFAULT_SINGLE_TEXTCAT_MODEL</p>
			<p><strong class="source-inline">TextCategorizer</strong> <a id="_idIndexMarker502"/>is available in two flavors, single-label classifier and multilabel classifier. As we remarked in the previous section, a multilabel classifier <a id="_idIndexMarker503"/>can predict more than one class. A single-label classifier <a id="_idIndexMarker504"/>predicts only one class for each example <a id="_idIndexMarker505"/>and classes are mutually exclusive. The preceding <strong class="source-inline">import</strong> line imports the single-label classifier and the following code imports the multilabel classifier:</p>
			<p class="source-code">from spacy.pipeline.textcat_multilabel import DEFAULT_MULTI_TEXTCAT_MODEL</p>
			<p>Next, we need to <a id="_idIndexMarker506"/>provide a configuration to the <strong class="source-inline">TextCategorizer</strong> component. We provide two parameters here, a threshold value and a model name (either <strong class="source-inline">Single</strong> or <strong class="source-inline">Multi</strong> depending on the classification task). <strong class="source-inline">TextCategorizer</strong> internally generates a probability for each class and a class is assigned to the text if the probability of this class is higher than the threshold value. </p>
			<p>A traditional threshold value for text classification is <strong class="source-inline">0.5</strong>, however, if you want to make a prediction with higher confidence, you can make the threshold higher, such as 0.6, 0.7, or 0.8.</p>
			<p>Putting it altogether, we can add a single-label <strong class="source-inline">TextCategorizer</strong> component to the <strong class="source-inline">nlp</strong> pipeline as follows:</p>
			<p class="source-code">from spacy.pipeline.textcat import DEFAULT_SINGLE_TEXTCAT_MODEL</p>
			<p class="source-code">config = {</p>
			<p class="source-code">   "threshold": 0.5,  </p>
			<p class="source-code">   "model": DEFAULT_SINGLE_TEXTCAT_MODEL</p>
			<p class="source-code">}</p>
			<p class="source-code">textcat = nlp.add_pipe("textcat", config=config)</p>
			<p class="source-code">textcat</p>
			<p class="source-code">&lt;spacy.pipeline.textcat.TextCategorizer object at 0x7f0adf004e08&gt;</p>
			<p>Adding a multilabel component to the <strong class="source-inline">nlp</strong> pipeline is similar:</p>
			<p class="source-code">from spacy.pipeline.textcat_multilabel import</p>
			<p class="source-code">DEFAULT_MULTI_TEXTCAT_MODEL</p>
			<p class="source-code">config = {</p>
			<p class="source-code">   "threshold": 0.5,</p>
			<p class="source-code">   "model": DEFAULT_MULTI_TEXTCAT_MODEL</p>
			<p class="source-code">}</p>
			<p class="source-code">textcat = nlp.add_pipe("textcat_multilabel", config=config)</p>
			<p class="source-code">textcat</p>
			<p class="source-code">&lt;spacy.pipeline.textcat.TextCategorizer object at 0x7f0adf004e08&gt;</p>
			<p>In the last line <a id="_idIndexMarker507"/>of each of the preceding code blocks, we added a <strong class="source-inline">TextCategorizer</strong> pipeline component to the nlp pipeline object. The newly created <strong class="source-inline">TextCategorizer</strong> component is captured by the <strong class="source-inline">textcat</strong> variable. We're ready to train the <strong class="source-inline">TextCategorizer</strong> component now. The training code looks quite similar to the NER component training code from <a href="B16570_07_Final_JM_ePub.xhtml#_idTextAnchor120"><em class="italic">Chapter 7</em></a><em class="italic">, Customizing spaCy Models</em>, except for some minor details. </p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor143"/>Formatting training data for the TextCategorizer</h2>
			<p>Let's start our code by preparing a small training set. We'll prepare a customer sentiment dataset for binary text classification. The label will be called <strong class="source-inline">sentiment</strong> and can obtain two possible values, 0 and 1 corresponding to negative and positive sentiment. The following training set contains 6 examples, 3 being positive and 3 being negative:</p>
			<p class="source-code">train_data = [</p>
			<p class="source-code">    ("I loved this product, very easy to use.", {"cats": {"sentiment": 1}}),</p>
			<p class="source-code">    ("I'll definitely purchase again. I recommend this product.", {"cats": {"sentiment": 1}}),</p>
			<p class="source-code">    ("This is the best product ever. I loved the scent and the feel. Will buy again.", {"cats": {"sentiment": 1}}),</p>
			<p class="source-code">   ("Disappointed. This product didn't work for me at all", {"cats": {"sentiment": 0}}),</p>
			<p class="source-code">   ("I hated the scent. Won't buy again", {"cats": {"sentiment": 0}}),</p>
			<p class="source-code">   ("Truly horrible product. Very few amount of product for a high price. Don't recommend.", {"cats": {"sentiment": 0}})</p>
			<p class="source-code">]</p>
			<p>Each training example is a tuple of a text and a nested dictionary. The dictionary contains the class label in a format that spaCy recognizes. The <strong class="source-inline">cts</strong> field means the categories. Then we <a id="_idIndexMarker508"/>include the class label sentiment and its value. The value should always be a floating-point number.</p>
			<p>In the code, we will introduce the class label we choose to the <strong class="source-inline">TextCategorizer</strong> component. Let's see the complete code. First, we do the necessary imports: </p>
			<p class="source-code">import random</p>
			<p class="source-code">import spacy   </p>
			<p class="source-code">from spacy.training import Example</p>
			<p class="source-code">from spacy.pipeline.textcat import DEFAULT_SINGLE_TEXTCAT_MODEL</p>
			<p>We imported the built-in library <strong class="source-inline">random</strong> for shuffling our dataset. We imported <strong class="source-inline">spacy</strong> as usual, and we imported <strong class="source-inline">Example</strong> to prepare the training examples in spaCy format. In the last line of the code block, we imported a text categorizer model. </p>
			<p>Next, we'll do the pipeline and <strong class="source-inline">TextCategorizer</strong> component initialization:</p>
			<p class="source-code">nlp = spacy.load("en_core_web_md")</p>
			<p class="source-code">config = {</p>
			<p class="source-code">   "threshold": 0.5,</p>
			<p class="source-code">   "model": DEFAULT_SINGLE_TEXTCAT_MODEL</p>
			<p class="source-code">}</p>
			<p class="source-code">textcat = nlp.add_pipe("textcat", config=config)</p>
			<p>Now, we'll do some work on the newly created <strong class="source-inline">TextCategorizer</strong> component, <strong class="source-inline">textcat</strong>. We'll introduce our label <strong class="source-inline">sentiment</strong> to the <strong class="source-inline">TextCategorizer</strong> componenet by calling <strong class="source-inline">add_label</strong>. Then, we need to initialize this component with our examples. This step is different than what we did in the NER training code in <a href="B16570_07_Final_JM_ePub.xhtml#_idTextAnchor120"><em class="italic">Chapter 7</em></a><em class="italic">, Customizing spaCy Models</em>. </p>
			<p>The reason is NER is an essential component, hence it's initialized by the pipeline always. TextCategorizer <a id="_idIndexMarker509"/>is an optional component, and it comes as a blank statistical model. The following code adds our label to the <strong class="source-inline">TextCategorizer</strong> component and then initializes the <strong class="source-inline">TextCategorizer</strong> model's weights with the training examples: </p>
			<p class="source-code">textcat.add_label("sentiment")</p>
			<p class="source-code">train_examples = [Example.from_dict(nlp.make_doc(text), label) for text,label in train_data]</p>
			<p class="source-code">textcat.initialize(lambda: train_examples, nlp=nlp)</p>
			<p>Note that we feed the examples to <strong class="source-inline">textcat.initialize</strong> as <strong class="source-inline">Example</strong> objects. Recall from <a href="B16570_07_Final_JM_ePub.xhtml#_idTextAnchor120"><em class="italic">Chapter 7</em></a>, <em class="italic">Customizing spaCy Models</em>, that spaCy training methods always work with <strong class="source-inline">Example</strong> objects.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor144"/>Defining the training loop</h2>
			<p>We're ready to define the training loop. First of all, we'll disable other pipe components so that only <strong class="source-inline">textcat</strong> will be trained. Second, we will create an optimizer object by calling <strong class="source-inline">resume_training</strong>, keeping the weights of the existing statistical models. For each epoch, we go over training examples one by one and update the weights of <strong class="source-inline">textcat</strong>. We go over the data for 20 epochs. The following code defines the training loop:</p>
			<p class="source-code">epochs=20</p>
			<p class="source-code">with nlp.select_pipes(enable="textcat"):</p>
			<p class="source-code">  optimizer = nlp.resume_training()</p>
			<p class="source-code">  for i in range(epochs):</p>
			<p class="source-code">    random.shuffle(train_data)</p>
			<p class="source-code">    for text, label in train_data:</p>
			<p class="source-code">      doc = nlp.make_doc(text)</p>
			<p class="source-code">      example = Example.from_dict(doc, label)</p>
			<p class="source-code">      nlp.update([example], sgd=optimizer)</p>
			<p>That's it! With this relatively short code segment, we trained a text classifier! Here's the output on my machine (your loss values might be different):</p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="Images/B16570_8_3.jpg" alt="Figure 8.3 – Loss values at each epoch&#13;&#10;" width="566" height="655"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – Loss values at each epoch</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor145"/>Testing the new component</h2>
			<p>Let's test the <a id="_idIndexMarker510"/>new text categorizer component. The <strong class="source-inline">doc.cats</strong> property holds the class labels:</p>
			<p class="source-code">doc2 = nlp("This product sucks")</p>
			<p class="source-code">doc2.cats</p>
			<p class="source-code">{'sentiment': 0.09907063841819763}</p>
			<p class="source-code">doc3 = nlp("This product is great")</p>
			<p class="source-code">doc3.cats</p>
			<p class="source-code">{'sentiment': 0.9740120000120339}</p>
			<p>Great! Our small dataset successfully trained the spaCy text classifier for a binary text classification <a id="_idIndexMarker511"/>problem, indeed a sentiment analysis task. Now, we'll see how to do multilabel classification with spaCy's <strong class="source-inline">TextCategorizer</strong>.</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor146"/>Training TextCategorizer for multilabel classification </h2>
			<p>Recall from the first section that multilabel classification means the classifier can predict more than <a id="_idIndexMarker512"/>one label for an example text. Naturally, the classes are not mutually exclusive at all. In order to train a multilabel classifier, we need to provide a dataset that contains examples that have more than one label. </p>
			<p>To train <a id="_idIndexMarker513"/>spaCy's <strong class="source-inline">TextCategorizer</strong> for multilabel classification, we'll again start by building a small training set. This time, we'll form a set of movie reviews, where the labels are <strong class="source-inline">FAMILY,</strong> <strong class="source-inline">THRILLER</strong>, and <strong class="source-inline">SUNDAY_EVENING</strong>. Here is our small dataset:</p>
			<p class="source-code">train_data = [</p>
			<p class="source-code">    ("It's the perfect movie for a Sunday evening.", {"cats": {"SUNDAY_EVENING": True}}),</p>
			<p class="source-code">    ("Very good thriller", {"cats": {"THRILLER": True}}),</p>
			<p class="source-code">    ("A great movie for the kids and all the family"  , {"cats": {"FAMILY": True}}),</p>
			<p class="source-code">    ("An ideal movie for Sunday night with all the family. My kids loved the movie.", {"cats": {"FAMILY": True, "SUNDAY_EVENING":True}}),</p>
			<p class="source-code">    ("A perfect thriller for all the family. No violence, no drugs, pure action.", {"cats": {"FAMILY": True, "THRILLER": True}})</p>
			<p class="source-code">]</p>
			<p>We provided some examples with one label, such as the first example (the first sentence of <strong class="source-inline">train_data</strong>, the second line of the preceding code block), and we also provided examples with more than one label, such as the fourth example of the <strong class="source-inline">train_data</strong>. </p>
			<p>We'll make the imports after we've formed the training set:</p>
			<p class="source-code">import random</p>
			<p class="source-code">import spacy   </p>
			<p class="source-code">from spacy.training import Example</p>
			<p class="source-code">from spacy.pipeline.textcat_multilabel import</p>
			<p class="source-code">DEFAULT_MULTI_TEXTCAT_MODEL</p>
			<p>Here, the last line is different than the code of the previous section. We imported the multilabel model instead of the single-label model. </p>
			<p>Next, we add the multilabel classifier component to the nlp pipeline. Again, pay attention <a id="_idIndexMarker514"/>to the pipeline component <a id="_idIndexMarker515"/>name – this time, it is <strong class="source-inline">textcat_multilabel</strong>, compared to the previous section's <strong class="source-inline">textcat</strong>:</p>
			<p class="source-code">config = {</p>
			<p class="source-code">   "threshold": 0.5,</p>
			<p class="source-code">   "model": DEFAULT_MULTI_TEXTCAT_MODEL</p>
			<p class="source-code">}</p>
			<p class="source-code">textcat = nlp.add_pipe("textcat_multilabel", config=config)</p>
			<p>Adding the labels to the <strong class="source-inline">TextCategorizer</strong> component and initializing the model is similar to the <em class="italic">Training the spaCy text classifier</em> section. This time, we'll add three labels instead of one:</p>
			<p class="source-code">labels = ["FAMILY", "THRILLER", "SUNDAY_EVENING"]</p>
			<p class="source-code">for label in labels:</p>
			<p class="source-code">  textcat.add_label(label)</p>
			<p class="source-code">train_examples = [Example.from_dict(nlp.make_doc(text), label) for text,label in train_data]</p>
			<p class="source-code">textcat.initialize(lambda: train_examples, nlp=nlp)</p>
			<p>We're ready to define the training loop. The code functions are similar to the previous section's code. The only difference is the component name in the first line. Now it's <strong class="source-inline">textcat_multilabel</strong>:</p>
			<p class="source-code">epochs=20</p>
			<p class="source-code">with nlp.select_pipes(enable="textcat_multilabel"): </p>
			<p class="source-code">  optimizer = nlp.resume_training() </p>
			<p class="source-code">  for i in range(epochs): </p>
			<p class="source-code">     random.shuffle(train_data) </p>
			<p class="source-code">     for text, label in train_data: </p>
			<p class="source-code">        doc = nlp.make_doc(text) </p>
			<p class="source-code">        example = Example.from_dict(doc, label) </p>
			<p class="source-code">        nlp.update([example], sgd=optimizer)</p>
			<p>The output <a id="_idIndexMarker516"/>should look similar to <a id="_idIndexMarker517"/>the output of the previous section, a loss value per epoch. Now, let's test our brand new multilabel classifier:</p>
			<p class="source-code">doc2 = nlp("Definitely in my Sunday movie night list")</p>
			<p class="source-code">doc2.cats</p>
			<p class="source-code">{'FAMILY': 0.9044250249862671, 'THRILLER': 0.34271398186683655, 'SUNDAY_EVENING': 0.9801468253135681}</p>
			<p>Notice that each label admitted a positive probability at the output. Also, the probabilities do not sum up to 1, because they're not mutually exclusive. For this example, the <strong class="source-inline">SUNDAY_EVENING</strong> and <strong class="source-inline">THRILLER</strong> label probabilities are predicted correctly, but the <strong class="source-inline">FAMILY</strong> label probability does not look ideal. This is mainly due to the fact that we didn't provide enough examples. Usually, for multilabel classification problems, the classifier needs more examples than binary classification since the classifier needs to learn more labels.</p>
			<p>We've learned how to train spaCy's <strong class="source-inline">TextCategorizer</strong> component for binary text classification and multilabel text classification. Now, we'll train <strong class="source-inline">TextCategorizer</strong> on a real-world dataset for a sentiment analysis problem.</p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor147"/>Sentiment analysis with spaCy</h1>
			<p>In this section, we'll work on a real-world dataset and train spaCy's <strong class="source-inline">TextCategorizer</strong> on this dataset. We'll be working on the Amazon Fine Food Reviews dataset (<a href="https://www.kaggle.com/snap/amazon-fine-food-reviews">https://www.kaggle.com/snap/amazon-fine-food-reviews</a>) from Kaggle in this chapter. The original dataset is huge, with 100,000 rows. We sampled 4,000 rows. This dataset <a id="_idIndexMarker518"/>contains customer reviews about fine food sold on Amazon. Reviews include user and product information, user rating, and text. </p>
			<p>You can download the dataset from the book's GitHub repository. Type the following command into your terminal:</p>
			<p class="source-code">wget  https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter08/data/Reviews.zip</p>
			<p>Alternatively, you can click on the URL in the preceding command and the download will start. You can unzip the zip file with the following:</p>
			<p class="source-code">unzip Reviews.zip</p>
			<p>Alternatively, you can right-click on the ZIP file and choose <strong class="bold">Extract here</strong> to inflate the ZIP file. </p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor148"/>Exploring the dataset</h2>
			<p>Now, we're ready to explore the dataset. In this section, we'll be using a Jupyter notebook. If you <a id="_idIndexMarker519"/>already have Jupyter installed, you can execute the notebook cells directly. If you don't have the Jupyter Notebook <a id="_idIndexMarker520"/>on your system, you can follow the instructions on the Jupyter website (<a href="https://jupyter.org/install">https://jupyter.org/install</a>).</p>
			<p>Let's do our dataset exploration step by step:</p>
			<ol>
				<li>First, we'll do the imports for reading and visualizing the dataset:<p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">%matplotlib inline</p></li>
				<li>We'll read the CSV file into a pandas DataFrame and output the shape of the DataFrame:<p class="source-code">reviews_df=pd.read_csv('data/Reviews.csv')</p><p class="source-code">reviews_df.shape</p><p class="source-code">(3999, 10)</p></li>
				<li>Next, we examine the rows and the columns of the dataset by printing the first 10 rows:<p class="source-code">reviews_df.head()</p><p>The resulting <a id="_idIndexMarker521"/>view tells us there are 10 rows including the review text and the review score:</p><div id="_idContainer120" class="IMG---Figure"><img src="Images/B16570_8_4.jpg" alt=" Figure 8.4 – First 10 rows of the reviews dataframe&#13;&#10;" width="1235" height="696"/></div><p class="figure-caption"> Figure 8.4 – First 10 rows of the reviews dataframe</p></li>
				<li>We'll be using the <strong class="source-inline">Text</strong> and <strong class="source-inline">Score</strong> columns; hence, we'll drop the other columns that we won't use. We’ll also call the dropna() method to drop the rows with missing values::<p class="source-code">reviews_df = reviews_df[['Text','Score']].dropna()</p></li>
				<li>We can have a quick look at how the review scores are distributed:<p class="source-code">ax=reviews_df.Score.value_counts().plot(kind='bar', colormap='Paired')</p><p class="source-code">plt.show()</p></li>
				<li>This piece <a id="_idIndexMarker522"/>of code calls the <strong class="source-inline">plot</strong> method of <strong class="source-inline">dataframe reviews_df</strong> and exhibits a bar plot:<div id="_idContainer121" class="IMG---Figure"><img src="Images/B16570_8_5.jpg" alt="Figure 8.5 – Distribution of review scores&#13;&#10;" width="555" height="358"/></div><p class="figure-caption">Figure 8.5 – Distribution of review scores</p><p>The number of 5-star ratings is quite high; it looks like customers are happy with the food they purchased. However, it might create an imbalance in the training data if a class has significantly more weight than the others.</p><p><strong class="bold">Class imbalance</strong> creates trouble for classification algorithms in general. For example, it is considered as imbalance when a class has significantly more training examples than the other classes (usually a ratio of 1:5 between examples). There are different ways to handle imbalance, one way is <strong class="bold">up</strong>-<strong class="bold">sampling</strong>/<strong class="bold">down</strong>-<strong class="bold">sampling</strong>. In down-sampling, we randomly remove training examples from the majority class. In up-sampling, we randomly replicate training example from the minority class. Both methods aim to balance the number of training examples of majority and minority classes. </p><p>Here we’ll apply another method. We’ll combine 1,2,3 star reviews and 4,5 star reviews to get a more balanced dataset.</p></li>
				<li>In order to prevent this, we'll treat 1-, 2-, and 3-star ratings as negative and ratings that have more than 4 stars as positive. The following code segment assigns a negative label to all the reviews that have a rating of fewer than 4 stars and a positive label to all the reviews that have a higher rating than 4 stars:<p class="source-code">reviews_df.Score[reviews_df.Score&lt;=3]=0</p><p class="source-code">reviews_df.Score[reviews_df.Score&gt;=4]=1</p></li>
				<li>Let's plot the distribution of the ratings again:<p class="source-code">ax=reviews_df.Score.value_counts().plot(kind='bar', colormap='Paired')</p><p class="source-code">plt.show()</p><p>The resulting rating distribution looks much better than <em class="italic">Figure 8.5</em>. Still, the number of <a id="_idIndexMarker523"/>positive reviews is greater, but the number of negative reviews is significant as well, as can be seen from the following graph:</p></li>
			</ol>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="Images/B16570_8_6.jpg" alt="Figure 8.6 – Distribution of positive and negative ratings&#13;&#10;" width="567" height="368"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.6 – Distribution of positive and negative ratings</p>
			<p>After processing the dataset, we reduced it to a two-column dataset with negative and positive ratings. We call <strong class="source-inline">reviews_df.head()</strong> once again and the following is the result we get:</p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="Images/B16570_8_7.jpg" alt="Figure 8.7 – The DataFrame’s first four rows&#13;&#10;" width="487" height="218"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.7 – The DataFrame's first four rows</p>
			<p>We'll finish <a id="_idIndexMarker524"/>our dataset exploration here. We saw the distribution of the review scores and the class labels. The dataset is now ready to be processed. We dropped the unused columns and converted review scores to binary class labels. Let's go ahead and start the training procedure!</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor149"/>Training the TextClassifier component</h2>
			<p>Now, we're ready to start the training procedure. We'll train a binary text classifier with the <a id="_idIndexMarker525"/>multilabel classifier this time. Again, let's go step by step:</p>
			<ol>
				<li value="1">We start by importing the spaCy classes as follows:<p class="source-code">import spacy</p><p class="source-code">import random</p><p class="source-code">from spacy.training import Example </p><p class="source-code">from spacy.pipeline.textcat_multilabel import DEFAULT_MULTI_TEXTCAT_MODEL</p></li>
				<li>Next, we'll create a pipeline object, <strong class="source-inline">nlp</strong>, define the classifier configuration, and add the <strong class="source-inline">TextCategorizer</strong> component to <strong class="source-inline">nlp</strong> with the following configuration:<p class="source-code">nlp = spacy.load("en_core_web_md") </p><p class="source-code"> config = { </p><p class="source-code">   "threshold": 0.5, </p><p class="source-code">   "model": DEFAULT_MULTI_TEXTCAT_MODEL </p><p class="source-code">} </p><p class="source-code">textcat = nlp.add_pipe("textcat_multilabel", config=config)</p></li>
				<li>After creating the text classifier component, we'll convert training sentences and <a id="_idIndexMarker526"/>ratings into a spaCy usable format. We'll iterate every row of the DataFrame with <strong class="source-inline">iterrows()</strong>, and for each row we'll extract the <strong class="source-inline">Text</strong> and <strong class="source-inline">Score</strong> fields. Then, we'll create a spaCy <strong class="source-inline">Doc</strong> object from the review text and make a dictionary of the class labels as well. Finally, we will create an <strong class="source-inline">Example</strong> object and append it to the list of training examples:<p class="source-code">train_examples = []</p><p class="source-code">for index, row in reviews_df.iterrows():</p><p class="source-code">    text = row["Text"]</p><p class="source-code">    rating = row["Score"]</p><p class="source-code">    label = {"POS": True, "NEG": False} if rating == 1 else {"NEG": True, "POS": False}</p><p class="source-code">    train_examples.append(Example.from_dict(nlp.make_doc(text), {"cats": label}))    </p></li>
				<li>We'll use <strong class="source-inline">POS</strong> and <strong class="source-inline">NEG</strong> labels for positive and negative sentiment, respectively. We'll introduce these labels to the new component and also initialize the component with examples:<p class="source-code">textcat.add_label("POS")</p><p class="source-code">textcat.add_label("NEG")</p><p class="source-code">textcat.initialize(lambda: train_examples, nlp=nlp)</p></li>
				<li>We're <a id="_idIndexMarker527"/>ready to define the training loop! We went over the training set for two epochs, but you can go over more if you like. The following code snippet will train the new text categorizer component:<p class="source-code">epochs = 2</p><p class="source-code">with nlp.select_pipes(enable="textcat_multilabel"): </p><p class="source-code">  optimizer = nlp.resume_training()</p><p class="source-code">  for i in range(epochs): </p><p class="source-code">    random.shuffle(train_examples) </p><p class="source-code">    for example in train_examples: </p><p class="source-code">      nlp.update([example], sgd=optimizer)</p></li>
				<li>Finally, we'll test how the text classifier component works for two example sentences:<p class="source-code">doc2 = nlp("This is the best food I ever ate")</p><p class="source-code">doc2.cats</p><p class="source-code">{'POS': 0.9553419947624207, 'NEG': 0.061326123774051666}</p><p class="source-code">doc3 = nlp("This food is so bad")</p><p class="source-code">doc3.cats</p><p class="source-code">{'POS': 0.21204468607902527, 'NEG': 0.8010350465774536}</p></li>
			</ol>
			<p>Both the <strong class="source-inline">NEG</strong> and <strong class="source-inline">POS</strong> labels appear in the prediction result because we used the multilabel classifier. The results look good. The first sentence outputs a very high positive probability, and the second sentence is predicted as negative with a high probability. </p>
			<p>We've completed training spaCy's text classifier component. In the next section, we'll dive into <a id="_idIndexMarker528"/>the world of a very popular deep learning library, Keras. We'll explore how to write Keras code to do text classification by using another popular machine learning library – TensorFlow's Keras API. Let's go ahead and explore Keras and TensorFlow!  </p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor150"/>Text classification with spaCy and Keras</h1>
			<p>In this <a id="_idIndexMarker529"/>section, we will learn about methods for blending <a id="_idIndexMarker530"/>spaCy with neural networks using <a id="_idIndexMarker531"/>another very popular Python deep learning library, <strong class="bold">TensorFlow</strong>, and <a id="_idIndexMarker532"/>its high-level API, <strong class="bold">Keras</strong>. </p>
			<p><strong class="bold">Deep learning</strong> is a broad <a id="_idIndexMarker533"/>family of machine learning algorithms that are <a id="_idIndexMarker534"/>based on neural networks. <strong class="bold">Neural networks</strong> are human brain-inspired <a id="_idIndexMarker535"/>algorithms that contain connected layers, which are made from neurons. Each neuron is a mathematical operation that takes its input, multiplies it by its weights, and then passes the sum through the activation function to the other neurons. The following diagram shows a neural network architecture with three layers -- the <strong class="bold">input layer</strong>, <strong class="bold">hidden layer</strong>, and <strong class="bold">output layer</strong>:</p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="Images/B16570_8_8.jpg" alt="Figure 8.8 – A neural network architecture with three layers &#13;&#10;" width="545" height="389"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.8 – A neural network architecture with three layers </p>
			<p><strong class="bold">TensorFlow</strong> is an <a id="_idIndexMarker536"/>end-to-end open source platform for machine learning. TensorFlow might be the most popular deep learning library among research engineers and scientists. It has huge community support and great documentation, available at <a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a>. </p>
			<p><strong class="bold">Keras </strong>is a high-level <a id="_idIndexMarker537"/>deep learning API that can run on top of popular machine learning libraries such as TensorFlow, Theano, and CNTK. Keras is very popular in the research and development world because it supports rapid prototyping and provides a user-friendly API to neural network architectures. </p>
			<p><strong class="bold">TensorFlow 2</strong> introduced <a id="_idIndexMarker538"/>great changes in machine learning methods by tightly integrating <a id="_idIndexMarker539"/>with Keras and providing a high-level API, <strong class="bold">tf.keras</strong>. TensorFlow 1 was a bit ugly with symbolic graph computations and other low-level computations. With TensorFlow 2, developers can take advantage of Keras' user-friendliness as well as TensorFlow's low-level methods.</p>
			<p>Neural networks <a id="_idIndexMarker540"/>are commonly used for computer <a id="_idIndexMarker541"/>vision and NLP tasks, including object detection, image classification, and scene understanding as well as text classification, POS tagging, text summarization, and natural language generation. </p>
			<p>In the following <a id="_idIndexMarker542"/>sections, we'll go through the details of a neural network architecture for text classification implemented with <strong class="source-inline">tf.keras</strong>. Throughout this section, we'll use TensorFlow 2 as we stated in the <em class="italic">Technical requirements</em> section. Let's warm up to neural networks with some neural network basics, and then start building our Keras code.</p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor151"/>What is a layer?</h2>
			<p>A neural network is formed by connecting layers. <strong class="bold">Layers</strong> are basically the building blocks of the <a id="_idIndexMarker543"/>neural network. A layer consists of several <strong class="bold">neurons</strong>, as in <em class="italic">Figure 8.8</em>. </p>
			<p>In <em class="italic">Figure 8.8</em>, the first layer <a id="_idIndexMarker544"/>of this neural network has two layers, and the second layer has six neurons. Each neuron in each layer is connected to all neurons in the next layer. Each layer might have different functionalities; some layers can lower the dimensions of their input, some layers can flatten their input (flattening means collapsing a multidimensional vector into one dimension), and so on. At each layer, we transform the input vectors and feed them to the next layer to get a final vector.   </p>
			<p>Keras provides different sorts of layers, such as input layers, dense layers, dropout layers, embedding layers, activation layers, recurrent layers, and so on. Let's get to know some useful layers one by one:</p>
			<ul>
				<li><strong class="bold">Input layer</strong>: The input layer <a id="_idIndexMarker545"/>is responsible for sending our input data <a id="_idIndexMarker546"/>to the rest of the network. While initializing an input layer, we provide the input data shape. </li>
				<li><strong class="bold">Dense layers</strong>: Dense layers <a id="_idIndexMarker547"/>transform the input of a given shape to <a id="_idIndexMarker548"/>the output shape we want. Layer 2 in <em class="italic">Figure 8.8</em> represents a dense layer, which collapses a 5-dimensional input into a 1-dimensional output. </li>
				<li><strong class="bold">Recurrent layers</strong>: Keras provides strong support for RNN, GRU, and LSTM cells. If you're <a id="_idIndexMarker549"/>not familiar with RNN variations at all, please <a id="_idIndexMarker550"/>refer to the resources in the <em class="italic">Technical requirements</em> section. We'll use an LSTM layer in our code. The <em class="italic">LSTM layer</em> subsection contains the input and output shape information. In the next subsection, <em class="italic">Sequential modeling with LSTMs</em>, we'll get into details of modeling with LSTMs.   </li>
				<li><strong class="bold">Dropout layers</strong>: Dropout is <a id="_idIndexMarker551"/>a technique to prevent overfitting. Overfitting <a id="_idIndexMarker552"/>happens when neural networks memorize data instead of learning it. Dropout layers randomly select a given number of neurons and set their weights to zero for the forward and backward passes, that is, for one iteration. We usually place dropout layers after dense layers.</li>
			</ul>
			<p>These are the basic layers that are used in NLP models. The next subsection is devoted to modeling sequential data with LSTMs, which is the core of statistical modeling for NLP. </p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor152"/>Sequential modeling with LSTMs</h2>
			<p><strong class="bold">LSTM</strong> is an <a id="_idIndexMarker553"/>RNN variation. <strong class="bold">RNNs</strong> are special neural <a id="_idIndexMarker554"/>networks <a id="_idIndexMarker555"/>that can process sequential data in steps. In usual neural networks, we assume that all the inputs and outputs are independent of each other. Of course, it's not true for text data. Every word's presence depends on the neighbor words. </p>
			<p>For example, during a machine translation task, we predict a word by considering all the words we predicted before. RNNs capture information about the past sequence elements by <a id="_idIndexMarker556"/>holding a <strong class="bold">memory</strong> (called <strong class="bold">hidden state</strong>). <em class="italic">Figure 8.9</em> shows a well-known illustration of RNNs. The loop on the left-hand side of the figure explains that an RNN feeds the output of the previous step to itself as the current input. The right-hand <a id="_idIndexMarker557"/>side of the figure shows the unrolled <a id="_idIndexMarker558"/>version of the RNN diagram. At each time step, <strong class="source-inline">i</strong>, we feed the input word <strong class="source-inline">xi</strong>, and RNN outputs a value, <strong class="source-inline">hi</strong>, for this time step:</p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="Images/B16570_8_9.jpg" alt="Figure 8.9 – RNN illustration, taken from Colah’s notable blog on LSTMs&#13;&#10;" width="559" height="166"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.9 – RNN illustration, taken from Colah's notable blog on LSTMs</p>
			<p>LSTMs were invented to fix some computational problems of RNNs. RNNs have the problem of forgetting some data back in the sequence, as well as some numerical stability <a id="_idIndexMarker559"/>issues due to chain multiplications called <strong class="bold">vanishing and exploding gradients</strong>. If you are interested, you can refer to Colah's blog, the link to which you will find in the <em class="italic">References</em> section. </p>
			<p>An LSTM cell is slightly more complicated than an RNN cell, but the logic of computation is the same: we feed one input word at each time step and LSTM outputs an output value at each time step. The following diagram shows what's inside an LSTM cell. Note that the input steps and output steps are identical to the RNN counterparts:</p>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="Images/B16570_8_10.jpg" alt="Figure 8.10 – LSTM illustration from Colah’s LSTM blog article&#13;&#10;" width="1403" height="596"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.10 – LSTM illustration from Colah's LSTM blog article</p>
			<p>Keras has extensive support for the RNN variations GRU and LSTM, as well as a simple API for training RNNs. RNN variations are crucial for NLP tasks, as language data's nature is sequential: text is a sequence of words, speech is a sequence of sounds, and so on. </p>
			<p>Now that we <a id="_idIndexMarker560"/>have learned what type of statistical <a id="_idIndexMarker561"/>model to use in our design, we can switch to a more practical subject: how to represent a sequence of words. In the next section, we'll learn how to transform a sequence of words into a sequence of word IDs and build vocabularies at the same time with Keras's preprocessing module.</p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor153"/>Keras Tokenizer</h2>
			<p>As we remarked in the previous section, text is sequential data (a sequence of words or characters). We'll feed <a id="_idIndexMarker562"/>a sentence as a sequence of words. Neural networks can work only with vectors, so we need a way to vectorize the words. In <a href="B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087"><em class="italic">Chapter 5</em></a>, <em class="italic">Working with Word Vectors and Semantic Similarity</em>, we saw how to vectorize words with word vectors. A word vector is a continuous representation of a word. In order to vectorize a word, we follow these steps:</p>
			<ol>
				<li value="1">We tokenize each sentence and turn sentences into a sequence of words.</li>
				<li>We create a vocabulary from the set of words present in <em class="italic">step 1</em>. These are words that are supposed to be recognized by our neural network design.</li>
				<li>Creating a vocabulary should assign an ID to each word. </li>
				<li>Then word IDs are mapped to word vectors.</li>
			</ol>
			<p>Let's look at a short example. We can work on a small corpus of three sentences:</p>
			<p class="source-code">data = [</p>
			<p class="source-code">"Tomorrow I will visit the hospital.",</p>
			<p class="source-code">"Yesterday I took a flight to Athens.",</p>
			<p class="source-code">"Sally visited Harry and his dog."</p>
			<p class="source-code">]</p>
			<p>Let's first <a id="_idIndexMarker563"/>tokenize the words into sentences:</p>
			<p class="source-code">import spacy</p>
			<p class="source-code">nlp = spacy.load("en_core_web_md")</p>
			<p class="source-code">sentences = [[token.text for token in nlp(sentence)] for sentence in data]</p>
			<p class="source-code">for sentence in sentences:</p>
			<p class="source-code">    sentence</p>
			<p class="source-code">... </p>
			<p class="source-code">['Tomorrow', 'I', 'will', 'visit', 'the', 'hospital', '.']</p>
			<p class="source-code">['Yesterday', 'I', 'took', 'a', 'flight', 'to', 'Athens', '.']</p>
			<p class="source-code">['Sally', 'visited', 'Harry', 'and', 'his', 'dog', '.']</p>
			<p>In the preceding code, we iterated over all tokens of the Doc object generated by calling <strong class="source-inline">nlp(sentence)</strong>. Notice that we didn't filter out the punctuation marks. Filtering punctuation depends on the task. For instance, in sentiment analysis, punctuation marks such as "!" correlate to the result. In this example, we'll keep the punctuation marks as well.</p>
			<p>Keras's text preprocessing module is used to create vocabularies and <strong class="source-inline">trn</strong> word sequences into word-ID sequences with the <strong class="source-inline">Tokenizer</strong> class. The following code segment exhibits how to use a <strong class="source-inline">Tokenizer</strong> object:</p>
			<p class="source-code">from tensorflow.keras.preprocessing.text import Tokenizer</p>
			<p class="source-code">tokenizer = Tokenizer(lower=True)  </p>
			<p class="source-code">tokenizer.fit_on_texts(data)</p>
			<p class="source-code">tokenizer</p>
			<p class="source-code">&lt;keras_preprocessing.text.Tokenizer object at 0x7f89e9d2d9e8&gt;</p>
			<p class="source-code">tokenizer.word_index</p>
			<p class="source-code">{'i': 1, 'tomorrow': 2, 'will': 3, 'visit': 4, 'the': 5, 'hospital': 6, 'yesterday': 7, 'took': 8, 'a': 9, 'flight': 10, 'to': 11, 'athens': 12, 'sally': 13, 'visited': 14, 'harry': 15, 'and': 16, 'his': 17, 'dog': 18}</p>
			<p>We did the following:</p>
			<ol>
				<li value="1">We imported <strong class="source-inline">Tokenizer</strong> from the Keras text preprocessing module.</li>
				<li>We created a <strong class="source-inline">tokenizer</strong> object with the parameter <strong class="source-inline">lower=True</strong>, which means <strong class="source-inline">tokenizer</strong> should lower all words while building the vocabulary.</li>
				<li>We called <strong class="source-inline">tokenizer.fit_on_texts</strong> on <strong class="source-inline">data</strong> to build the vocabulary. <strong class="source-inline">fit_on_text</strong> works on a sequence of words; input should always be a list of words. </li>
				<li>We examined the vocabulary by printing <strong class="source-inline">tokenizer.word_index</strong>. <strong class="source-inline">Word_index</strong> is basically a dictionary where keys are vocabulary words and values are word-IDs. </li>
			</ol>
			<p>In order to <a id="_idIndexMarker564"/>get a word's word-ID, we call <strong class="source-inline">tokenizer.texts_to_sequences</strong>. Notice that the input to this method should always be a list, even if we want to feed only one word. In the following code segment, we feed one-word input as a list (notice the list brackets):</p>
			<p class="source-code">tokenizer.texts_to_sequences(["hospital"])</p>
			<p class="source-code">[[6]]</p>
			<p class="source-code">tokenizer.texts_to_sequences(["hospital", "took"])</p>
			<p class="source-code">[[6], [8]]</p>
			<p>The reverse of <strong class="source-inline">texts_to_sequences</strong> is the <strong class="source-inline">sequences_to_texts</strong>. <strong class="source-inline">sequences_to_texts</strong> method will input a list of lists and return the corresponding word sequences:</p>
			<p class="source-code">tokenizer.sequences_to_texts([[3,2,1]])</p>
			<p class="source-code">['will tomorrow i']</p>
			<p class="source-code">tokenizer.sequences_to_texts([[3,2,1], [5,6,10]])</p>
			<p class="source-code">['will tomorrow i', 'the hospital flight']</p>
			<p>We also notice that the word-IDs start from <strong class="source-inline">1</strong>, not <strong class="source-inline">0</strong>. <strong class="source-inline">0</strong> is a reserved value and has a special meaning, which means a padding value. Keras cannot process sentences of different lengths, hence we need to pad shorter sentences to reach the longest sentence's length. We pad each sentence of the dataset to a maximum length by adding padding words <a id="_idIndexMarker565"/>either to the start or end of the sentence. Keras inserts <strong class="source-inline">0</strong> for the padding, which means it's not a real word, but a padding value. Let's understand padding with a simple example:</p>
			<p class="source-code">from tensorflow.keras.preprocessing.sequence import pad_sequences</p>
			<p class="source-code">sequences = [[7], [8,1], [9,11,12,14]]</p>
			<p class="source-code">MAX_LEN=4</p>
			<p class="source-code">pad_sequences(sequences, MAX_LEN, padding="post")</p>
			<p class="source-code">array([[ 7,  0,  0,  0],</p>
			<p class="source-code">          [ 8,  1,  0,  0],</p>
			<p class="source-code">          [ 9, 11, 12, 14]], dtype=int32)</p>
			<p class="source-code">pad_sequences(sequences, MAX_LEN, padding="pre")</p>
			<p class="source-code">array([[ 0,  0,  0,  7],</p>
			<p class="source-code">          [ 0,  0,  8,  1],</p>
			<p class="source-code">          [ 9, 11, 12, 14]], dtype=int32)</p>
			<p>Our sequences are of length 1, 2, and 4. We called <strong class="source-inline">pad_sequences</strong> on this list of sequences and every sequence is padded with zeros such that its length reaches <strong class="source-inline">MAX_LEN=4</strong>, the length of the longest sequence. We can pad the sequences from the right or left with the <strong class="source-inline">post</strong> and <strong class="source-inline">pre</strong> options. In the preceding code, we padded our sentences with the <strong class="source-inline">post</strong> option, hence the sentences are padded from the right. </p>
			<p>If we put it all together, the complete text preprocessing steps are as follows:</p>
			<p class="source-code">from tensorflow.keras.preprocessing.text import Tokenizer</p>
			<p class="source-code">from tensorflow.keras.preprocessing.sequence import pad_sequences</p>
			<p class="source-code">tokenizer = Tokenizer(lower=True)</p>
			<p class="source-code">tokenizer.fit_on_texts(data)</p>
			<p class="source-code">seqs = tokenizer.texts_to_sequences(data)</p>
			<p class="source-code">MAX_LEN=7</p>
			<p class="source-code">padded_seqs = pad_sequences(seqs, MAX_LEN, padding="post")</p>
			<p class="source-code">padded_seqs</p>
			<p class="source-code">array([[ 2,  1,  3,  4,  5,  6,  0],</p>
			<p class="source-code">          [ 7,  1,  8,  9, 10, 11, 12],</p>
			<p class="source-code">          [13, 14, 15, 16, 17, 18,  0]], dtype=int32)</p>
			<p>Now, we've <a id="_idIndexMarker566"/>transformed sentences into a sequence of word-IDs. We've come one step closer to vectorizing the words. In the next subsection, we'll finally transform words into vectors. Then our sentences will be ready to be fed into the neural network.</p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor154"/>Embedding words </h2>
			<p>We're ready to transform words into word vectors. Embedding words into vectors happens via <a id="_idIndexMarker567"/>an embedding table. An embedding table is basically a lookup table. Each row holds the word vector of a word. We index the rows by word-IDs, hence the flow of obtaining a word's word vector is as follows:</p>
			<ol>
				<li value="1"><strong class="bold">word-&gt;word-ID</strong>: In the previous section, we obtained a word-ID for each word with Keras' <strong class="source-inline">Tokenizer</strong>. <strong class="source-inline">Tokenizer</strong> holds all the vocabulary and maps each vocabulary word to an ID, which is an integer.</li>
				<li><strong class="bold">word-ID-&gt;word vector</strong>: A word-ID is an integer and therefore can be used as an index to the embedding table's rows. Each word-ID corresponds to one row and when we want to get a word's word vector, we first obtain its word-ID and then do a lookup in the embedding table rows with this word-ID.</li>
			</ol>
			<p>The following diagram shows how embedding words into word vectors works:</p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="Images/B16570_8_11.jpg" alt="Figure 8.11 – Steps of transforming a word to its word vector with Keras&#13;&#10;" width="941" height="404"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.11 – Steps of transforming a word to its word vector with Keras</p>
			<p>Remember that <a id="_idIndexMarker568"/>in the previous section, we started with a list of sentences. Then we did the following: </p>
			<ol>
				<li value="1">We broke each sentence into words and built a vocabulary with Keras' <strong class="source-inline">Tokenizer</strong>. </li>
				<li>The <strong class="source-inline">Tokenizer</strong> object held a word index, which was a <strong class="bold">word-&gt;word-ID</strong> mapping. </li>
				<li>After obtaining the word-ID, we could do a lookup to the embedding table rows with this word-ID and got a word vector. </li>
				<li>Finally, we fed this word vector to the neural network. </li>
			</ol>
			<p>Training a neural network is not easy. We have to take several steps to transform sentences <a id="_idIndexMarker569"/>into vectors. After these preliminary steps, we're ready to design the neural network architecture and do the model training.</p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor155"/>Neural network architecture for text classification</h2>
			<p>In this section, we <a id="_idIndexMarker570"/>will design the neural <a id="_idIndexMarker571"/>network architecture for our text classifier. We'll follow these steps to train the classifier:</p>
			<ol>
				<li value="1">First, we'll preprocess, tokenize, and pad the review sentences. After this step, we'll obtain a list of sequences.</li>
				<li>We'll feed this list of sequences to the neural network through the input layer.</li>
				<li>Next, we'll vectorize each word by looking up its word ID in the embedding layer. At this point, a sentence is now a sequence of word vectors, each word vector corresponding to a word.</li>
				<li>After that, we'll feed the sequence of word vectors to LSTM.</li>
				<li>Finally, we'll squash the LSTM output with a sigmoid layer to obtain class probabilities.</li>
			</ol>
			<p>Let's get started by remembering the dataset again.</p>
			<h3>Dataset</h3>
			<p>We'll <a id="_idIndexMarker572"/>use the same Amazon fine food reviews dataset from the <em class="italic">Sentiment analysis with spaCy</em> section. We already processed the dataset with pandas in that section and reduced it to two columns and binary labels. Here is how the <strong class="source-inline">reviews_df</strong> dataset looks:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="Images/B16570_8_12.jpg" alt="Figure 8.12 - Result of the reviews_df.head()&#13;&#10;" width="489" height="222"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.12 – Result of the reviews_df.head()</p>
			<p>We'll <a id="_idIndexMarker573"/>transform our dataset a bit. We'll extract the review text and review label from each dataset row and append them into Python lists:</p>
			<p class="source-code">train_examples = []</p>
			<p class="source-code">labels = []</p>
			<p class="source-code">for index, row in reviews_df.iterrows():</p>
			<p class="source-code">    text = row["Text"]</p>
			<p class="source-code">    rating = row["Score"]</p>
			<p class="source-code">    labels.append(rating)</p>
			<p class="source-code">    tokens = [token.text for token in nlp(text)]</p>
			<p class="source-code">    train_examples.append(tokens)    </p>
			<p>Notice that we appended a list of words to <strong class="source-inline">train_examples</strong>, hence each element of this list is a list of words. Next, we'll invoke Keras' <strong class="source-inline">Tokenizer</strong> on this list of words to build our vocabulary.</p>
			<h3>Data and vocabulary preparation</h3>
			<p>We already processed our dataset, hence we are ready to tokenize the dataset sentences <a id="_idIndexMarker574"/>and create a vocabulary. Let's go step by step:</p>
			<ol>
				<li value="1">First, we'll do the necessary imports:<p class="source-code">from tensorflow.keras.preprocessing.text import Tokenizer</p><p class="source-code">from tensorflow.keras.preprocessing.sequence import pad_sequences</p><p class="source-code">import numpy as np</p></li>
				<li>We're ready to fit the <strong class="source-inline">Tokenizer</strong> object on our list of words. First, we'll fit the <strong class="source-inline">Tokenizer</strong>, then we'll convert words to their IDs by calling <strong class="source-inline">texts_to_sequences</strong>:<p class="source-code">tokenizer = Tokenizer(lower=True)</p><p class="source-code">tokenizer.fit_on_texts(train_examples)</p><p class="source-code">sequences = tokenizer.texts_to_sequences(train_examples)</p></li>
				<li>Then, we'll pad the short sequences to a maximum length of <strong class="source-inline">50</strong> (we picked this number). Also, this will truncate long reviews to a length of <strong class="source-inline">50</strong> words:<p class="source-code">MAX_LEN = 50</p><p class="source-code">X = pad_sequences(sequences, MAX_LEN, padding="post")</p></li>
				<li>Now <strong class="source-inline">X</strong> is a list of sequences of <strong class="source-inline">50</strong> words. Finally, we'll convert this list of reviews and the labels to <strong class="source-inline">numpy</strong> arrays:<p class="source-code">X = np.array(X)</p><p class="source-code">y = np.array(labels)</p></li>
			</ol>
			<p>At this point, we're ready to feed our data to our neural network. We'll feed our data to the input layer. For all the necessary imports, please follow the notebook of this section from our GitHub repository: <a href="https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter08/Keras_train.ipynb">https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter08/Keras_train.ipynb</a>.</p>
			<p>Here, notice that we didn’t do any lemmatization/stemming or stopwords removal. This is completely fine and indeed the standard way to go with neural network algorithms, because words that are variations of the same root word (liked, liking, like) will obtain similar word vectors (recall from <a href="B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087"><em class="italic">Chapter 5</em></a><em class="italic">, Working with Word Vectors and Semantic Similarity</em> that similar words obtain similar word vectors). Also, stopwords occur frequently in different contexts, hence neural network can deduce that these words are just common words of the language and don't carry much importance.</p>
			<h3>The input layer</h3>
			<p>The <a id="_idIndexMarker575"/>following piece of code defines our input layer:</p>
			<p class="source-code">sentence_input = Input(shape=(None,))</p>
			<p>Don't be confused by <strong class="source-inline">None</strong> as the input shape. Here, <strong class="source-inline">None</strong> means that this dimension can be any scalar number, hence, we use this expression when we want Keras to infer the input shape.</p>
			<h3>The embedding layer</h3>
			<p>We define the embedding layer as follows:</p>
			<p class="source-code">embedding =  Embedding(\</p>
			<p class="source-code">input_dim = len(tokenizer.word_index)+1,\</p>
			<p class="source-code">output_dim = 100)(sentence_input)</p>
			<p>While defining the embedding layer, the input dimension should always be the number of words in the vocabulary (here, there's a plus <strong class="source-inline">1</strong> because the indices start from <strong class="source-inline">1</strong>, not <strong class="source-inline">0</strong>. Index <strong class="source-inline">0</strong> is reserved for the padding value). </p>
			<p>Here, we chose the output shape to be 100, hence the word vectors for the vocabulary words will be 100-dimensional. Popular numbers for word vector dimensions are 50, 100, and 200 depending on the complexity of the task.</p>
			<h3>The LSTM layer</h3>
			<p>We'll <a id="_idIndexMarker576"/>feed the word vectors to our LSTM:</p>
			<p class="source-code">LSTM_layer = LSTM(units=256)(embedding)</p>
			<p>Here, the <strong class="source-inline">units</strong> parameter means the dimension of the hidden state. The LSTM output shape and hidden state shape are the same due to the LSTM architecture. Here, our LSTM layer will output a <strong class="source-inline">256</strong>-dimensional vector.</p>
			<h3>The output layer</h3>
			<p>We <a id="_idIndexMarker577"/>obtained a <strong class="source-inline">256</strong>-dimensional vector from the LSTM layer and we want to squash it to a <strong class="source-inline">1</strong>-dimensional vector (possible values of this vector are <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>, which are the class labels): </p>
			<p class="source-code">output_dense = Dense(1, activation='sigmoid')(LSTM_layer)</p>
			<p>We used the sigmoid function to squash the values. The sigmoid function is an S-shaped function and maps its input to a [0-1] range. You can find out more about this function at <a href="https://deepai.org/machine-learning-glossary-and-terms/sigmoid-function">https://deepai.org/machine-learning-glossary-and-terms/sigmoid-function</a>.</p>
			<h3>Compiling the model</h3>
			<p>After <a id="_idIndexMarker578"/>defining the model, we need to compile it with an optimizer, a loss function, and an evaluation metric:</p>
			<p class="source-code">model = \</p>
			<p class="source-code">Model(inputs=[sentence_input],outputs=[output_dense])</p>
			<p class="source-code">model.compile(optimizer="adam",loss="binary_crossentropy",\</p>
			<p class="source-code">metrics=["accuracy"])</p>
			<p><strong class="bold">Adaptive Moment Estimation</strong> (<strong class="bold">ADAM</strong>) is a popular optimizer in deep learning. It basically <a id="_idIndexMarker579"/>adapts how fast the neural network <a id="_idIndexMarker580"/>should learn. You can learn about different optimizers in this blog post: <a href="https://ruder.io/optimizing-gradient-descent/">https://ruder.io/optimizing-gradient-descent/</a>. Binary cross-entropy is a loss that is used in binary classification tasks. Keras <a id="_idIndexMarker581"/>supports different loss functions depending on the tasks. You can find the list on the Keras website at <a href="https://keras.io/api/losses/">https://keras.io/api/losses/</a>. </p>
			<p>A <strong class="bold">metric</strong> is a function <a id="_idIndexMarker582"/>that we use to evaluate our model's performance. The accuracy metric basically compares how many times the predicted label and the real label matches. A list of <a id="_idIndexMarker583"/>supported metrics can be found in Keras's documentation (<a href="https://keras.io/api/metrics/">https://keras.io/api/metrics/</a>).</p>
			<h3>Fitting the model and experiment evaluation</h3>
			<p>Finally, we'll <a id="_idIndexMarker584"/>fit the model on our data:</p>
			<p class="source-code">model.fit(x=X,</p>
			<p class="source-code">          y=y,</p>
			<p class="source-code">          batch_size=64,</p>
			<p class="source-code">          epochs=5,</p>
			<p class="source-code">          validation_split=0.2)</p>
			<p>Here, <strong class="source-inline">x</strong> is the list of training examples and <strong class="source-inline">y</strong> is the list of labels. We want to make 5 passes over the data, hence we set the <strong class="source-inline">epochs</strong> parameter to <strong class="source-inline">5</strong>.</p>
			<p>We went over the data <strong class="source-inline">5</strong> times in batch sizes of <strong class="source-inline">64</strong>. Usually, we don't fit all of the dataset into the memory at once (due to memory limitations), but we feed the dataset to the classifier in smaller chunks, each <a id="_idIndexMarker585"/>chunk being called a <strong class="bold">batch</strong>. Here, the parameter<strong class="source-inline"> batch_size=64</strong> means we want to feed a batch of 64 training sentences at once.</p>
			<p>Finally, the parameter <strong class="source-inline">validation_split</strong> is used to evaluate the experiment. This parameter simply will separate 20 percent of the data as the validation set and validate the <a id="_idIndexMarker586"/>model on this validation set. Our experiment results in 0.795 accuracy, which is quite good for such a basic neural network design.</p>
			<p>We encourage you to experiment more. You can experiment with the code more by placing dropout layers at different locations (such as after the embedding layer or after the LSTM layer). Another way of experimenting is to try different values for the embedding dimensions, such as 50, 150, and 200, and observe the change in the accuracy. The same applies to the LSTM layer's hidden dimension – you can experiment with different values instead of 256.</p>
			<p>We finished training with <strong class="source-inline">tf.keras</strong> in this section and also concluded the chapter. Keras is a great, efficient, and user-friendly deep learning API; the spaCy and Keras combination is especially powerful. Text classification is an essential task of NLP and we discovered how to do this task with spaCy.  </p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor156"/>Summary</h1>
			<p>We have finished this chapter about a very hot NLP topic – text classification. In this chapter, you first learned about text classification concepts such as binary classification, multilabel classification, and multiclass classification. Next, you learned how to train <strong class="source-inline">TextCategorizer</strong>, spaCy's text classifier component. You learned how to transform your data into spaCy training format and then train the <strong class="source-inline">TextCategorizer</strong> component with this data.</p>
			<p>After learning text classification with spaCy's <strong class="source-inline">TextCategorizer</strong>, in the final section, you learned how to combine spaCy code and Keras code. First, you learned the basics of neural networks, including some handy layers such as the dense layer, dropout layer, embedding layer, and recurrent layers. Then, you learned how to tokenize and preprocess the data with Keras' <strong class="source-inline">Tokenizer</strong>. </p>
			<p>You had a quick review of sequential modeling with LSTMs, as well as recalling word vectors from <a href="B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087"><em class="italic">Chapter 5</em></a>, <em class="italic">Working with Word Vectors and Semantic Similarity</em>, to understand the embedding layer better. Finally, you went through neural network design with <strong class="source-inline">tf.keras</strong> code. You learned how to design and evaluate a statistical experiment with LSTM. </p>
			<p>Looks like a lot! Indeed, it is a lot of material; no worries if it takes time to digest. Practicing text classification can be intense, but in the end, you earn crucial NLP skills.</p>
			<p>The next chapter is again devoted to a brand-new technology: <strong class="bold">transformers</strong>. In the next chapter, we'll explore how to design high-accuracy NLP pipelines in only a few lines. Let's move onto the next chapter and see what transformers offer for your NLP skills!</p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor157"/>References</h1>
			<p>It'd be good but not mandatory if you're familiar with neural networks, particularly RNN variations. Here is some great material for neural networks:</p>
			<ul>
				<li>Free online book: <em class="italic">Neural Networks and Deep Learning</em> (<a href="http://neuralnetworksanddeeplearning.com/">http://neuralnetworksanddeeplearning.com/</a>)</li>
				<li>Video tutorial at <a href="https://www.youtube.com/watch?v=ob1yS9g-Zcs">https://www.youtube.com/watch?v=ob1yS9g-Zcs</a></li>
			</ul>
			<p>RNN variations, especially LSTMs, have great tutorials too:</p>
			<ul>
				<li>RNN tutorial on the WildML blog: <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/</a></li>
				<li>RNN tutorial by the University of Toronto:  <a href="https://www.cs.toronto.edu/~tingwuwang/rnn_tutorial.pdf">https://www.cs.toronto.edu/~tingwuwang/rnn_tutorial.pdf</a></li>
				<li>Colah's blog: <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li>
				<li>Blog post by Michael Phi: <a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</a></li>
				<li>Video tutorial at <a href="https://www.youtube.com/watch?v=lWkFhVq9-nc">https://www.youtube.com/watch?v=lWkFhVq9-nc</a></li>
			</ul>
			<p>Although we have introduced neural networks in this chapter, you can read these references in order to learn more about how neural networks work. More explanations on neural network and LSTM concepts will follow in <a href="B16570_10_Final_JM_ePub.xhtml#_idTextAnchor173"><em class="italic">Chapter 10</em></a>, <em class="italic">Putting Everything Together: Designing Your Chatbot with spaCy</em>.</p>
		</div>
	</div></body></html>