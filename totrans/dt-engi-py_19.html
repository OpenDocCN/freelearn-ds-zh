<html><head></head><body>
		<div id="_idContainer240">
			<h1 id="_idParaDest-161"><a id="_idTextAnchor163"/><em class="italic">Appendix</em></h1>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor164"/>Building a NiFi cluster</h1>
			<p>In this book, you<a id="_idIndexMarker761"/> have built a Kafka cluster, a ZooKeeper cluster, and a Spark cluster. Instead of increasing the power of a single server, through clustering, you are able to add more machines to increase the processing power of a data pipeline. In this chapter, you will learn how to cluster NiFi so that your data pipelines can run across multiple machines.</p>
			<p>In this appendix, we're going to cover the following main topics:</p>
			<ul>
				<li>The basics of NiFi clustering</li>
				<li>Building a NiFi cluster</li>
				<li>Building a distributed data pipeline</li>
				<li>Managing the distributed data pipeline</li>
			</ul>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor165"/>The basics of NiFi clustering</h1>
			<p>Clustering<a id="_idIndexMarker762"/> in Apache NiFi follows a <strong class="bold">Zero-Master Clustering</strong> architecture. In this <a id="_idIndexMarker763"/>type of clustering, there is no pre-defined master. Every node can perform the same tasks, and the data is split between them. NiFi uses Zookeeper when deployed as a cluster. </p>
			<p>Zookeeper will elect a <strong class="bold">Cluster Coordinator</strong>. The Cluster Coordinator is<a id="_idIndexMarker764"/> responsible for deciding whether new nodes can join – the nodes will connect to the coordinator – and to provide the updated flows to the new nodes. </p>
			<p>While it sounds like the Cluster Coordinator is the master, it is not. You can make changes to the data pipelines on any node and they will be replicated to all the other nodes, meaning a non-Cluster Coordinator or a non-Primary Node can submit changes.</p>
			<p>The <strong class="bold">Primary Node</strong> is <a id="_idIndexMarker765"/>also elected by Zookeeper. On the Primary Node, you can run isolated processes. An isolated process is a NiFi processor that runs only on the Primary Node. This is important because think what would happen if you had three nodes all trying to read from a directory, or a single file, or a database. There would be a race condition or a lock. Processors that can result in these race conditions should be run on the Primary Node. The <strong class="source-inline">ExecuteSQL</strong> processor can run on the Primary Node, and then distribute the data to the other nodes downstream for processing. You will see how this is done later in this chapter.</p>
			<p>Clustering <a id="_idIndexMarker766"/>allows you to build data pipelines that can process larger amounts of data than on a single machine. Furthermore, it allows a single point to build and monitor data pipelines. If you had several single-node NiFi instances running, you would need to manage all of them. Changes to a data pipeline on one would need to be replicated on the others or at least checked to make sure it is not a duplicate. Which machine is running the data warehouse pipeline again? I forgot. Managing a cluster, from any node, makes it much easier and more efficient.</p>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor166"/>Building a NiFi cluster </h1>
			<p>In this section, you <a id="_idIndexMarker767"/>will build a two-node cluster on different machines. Just like with MiNiFi, however, there are some compatibility issues with the newest versions of NiFi and Zookeeper. To work around these issues and demonstrate the concepts, this chapter will use an older version of NiFi and the pre-bundled Zookeeper. To build the NiFi cluster, perform the following steps:</p>
			<ol>
				<li>As root, or using sudo, open your <strong class="source-inline">/etc/hosts</strong> file. You will need to assign names to the machines that you will use in your cluster. It is best practice to use a hostname instead of IP addresses. Your hosts file should look like the following example:<p class="source-code"><strong class="bold">127.0.0.1	localhost</strong></p><p class="source-code"><strong class="bold">::1		localhost</strong></p><p class="source-code"><strong class="bold">127.0.1.1	pop-os.localdomain	pop-os</strong></p><p class="source-code"><strong class="bold">10.0.0.63	nifi-node-2</strong></p><p class="source-code"><strong class="bold">10.0.0.148	nifi-node-1</strong></p></li>
				<li>In the preceding hosts file, I have added the last two lines. The nodes are <strong class="source-inline">nifi-node-1</strong> and <strong class="source-inline">nifi-node-2</strong> and you can see that they have different IP addresses. Make these changes in the hosts file for each machine. When you have finished, you can test that it works by using <strong class="source-inline">ping</strong>. From each machine, try to use <strong class="source-inline">ping</strong> to hit<a id="_idIndexMarker768"/> the other machine by hostname. The following is the command to hit <strong class="source-inline">nifi-node-2</strong> from the <strong class="source-inline">nifi-node-1</strong> machine:<p class="source-code"><strong class="bold">paulcrickard@pop-os:~$ ping nifi-node-2</strong></p><p class="source-code"><strong class="bold">PING nifi-node-2 (10.0.0.63) 56(84) bytes of data.</strong></p><p class="source-code"><strong class="bold">64 bytes from nifi-node-2 (10.0.0.63): icmp_seq=1 ttl=64 time=55.1 ms</strong></p><p class="source-code"><strong class="bold">64 bytes from nifi-node-2 (10.0.0.63): icmp_seq=2 ttl=64 time=77.1 ms</strong></p><p class="source-code"><strong class="bold">64 bytes from nifi-node-2 (10.0.0.63): icmp_seq=3 ttl=64 time=101 ms</strong></p><p class="source-code"><strong class="bold">64 bytes from nifi-node-2 (10.0.0.63): icmp_seq=4 ttl=64 time=32.8 ms</strong></p></li>
				<li>If you do the opposite from your other node, <strong class="source-inline">nifi-node-2</strong>, you should get the same results – <strong class="source-inline">nifi-node-1</strong> will return data. </li>
				<li>Next, download an older version of Apache NiFi, 1.0.0, at <a href="https://archive.apache.org/dist/nifi/1.0.0/">https://archive.apache.org/dist/nifi/1.0.0/</a>. Select the <strong class="source-inline">-bin.tar.gz</strong> file as it contains the binaries. Once the file has downloaded, extract the files using your file manager or with the following command:<p class="source-code"><strong class="bold">tar -xvzf nifi-1.0.0-bin.tar.gz</strong></p><p>Once you have extracted the files, you will edit the configuration files.</p></li>
				<li>To edit the Zookeeper configuration file, open <strong class="source-inline">zookeeper.properties</strong> in the <strong class="source-inline">$NIFI_HOME/conf</strong> directory. At the bottom of the file, add your servers as shown: <p class="source-code"><strong class="bold">server.1=nifi-node-1:2888:3888</strong></p><p class="source-code"><strong class="bold">server.2=nifi-node-2:2888:3888</strong></p></li>
				<li>At the top of the file, you will see <strong class="source-inline">clientPort</strong> and <strong class="source-inline">dataDir</strong>. It should look like the following example:<p class="source-code"><strong class="bold">clientPort=2181</strong></p><p class="source-code"><strong class="bold">initLimit=10</strong></p><p class="source-code"><strong class="bold">autopurge.purgeInterval=24</strong></p><p class="source-code"><strong class="bold">syncLimit=5</strong></p><p class="source-code"><strong class="bold">tickTime=2000</strong></p><p class="source-code"><strong class="bold">dataDir=./state/zookeeper</strong></p><p class="source-code"><strong class="bold">autopurge.snapRetainCount=30</strong></p></li>
				<li>In <strong class="source-inline">dataDir</strong>, you<a id="_idIndexMarker769"/> will need to add a file named <strong class="source-inline">myfile</strong> with the number of the server as the content. On <strong class="source-inline">server.1</strong> (<strong class="source-inline">nifi-node-1</strong>), you will create a <strong class="source-inline">myid</strong> ID with <strong class="source-inline">1</strong> as the content. To do that, from the <strong class="source-inline">$NIFI_HOME</strong> directory, use the following commands:<p class="source-code"><strong class="bold">mkdir state</strong></p><p class="source-code"><strong class="bold">mkdir state/zookeeper</strong></p><p class="source-code"><strong class="bold">echo 1 &gt;&gt; myid</strong></p></li>
				<li>On <strong class="source-inline">nifi-node-2</strong>, repeat the preceding steps, except change <strong class="source-inline">echo</strong> to the following line:<p class="source-code"><strong class="bold">echo 2 &gt;&gt; myid</strong></p><p>With Zookeeper configured, you will now edit the <strong class="source-inline">nifi.properties</strong> file.</p></li>
				<li>To edit <strong class="source-inline">nifi.properties</strong>, you will need to change several properties. The first property is <strong class="source-inline">nifi.state.management.embedded.zookeeper.start</strong>, which needs to be set to <strong class="source-inline">true</strong>. The section of the file is shown as follows:<p class="source-code"><strong class="bold">####################</strong></p><p class="source-code"><strong class="bold"># State Management #</strong></p><p class="source-code"><strong class="bold">####################</strong></p><p class="source-code"><strong class="bold">nifi.state.management.configuration.file=./conf/state-management.xml</strong></p><p class="source-code"><strong class="bold"># The ID of the local state provider</strong></p><p class="source-code"><strong class="bold">nifi.state.management.provider.local=local-provider</strong></p><p class="source-code"><strong class="bold"># The ID of the cluster-wide state provider. This will be ignored if NiFi is not clustered but must be populated if running in a cluster.</strong></p><p class="source-code"><strong class="bold">nifi.state.management.provider.cluster=zk-provider</strong></p><p class="source-code"><strong class="bold"># Specifies whether or not this instance of NiFi should run an embedded ZooKeeper server</strong></p><p class="source-code"><strong class="bold">nifi.state.management.embedded.zookeeper.start=true  </strong></p><p class="source-code"><strong class="bold"># Properties file that provides the ZooKeeper properties to use if &lt;nifi.state.management.embedded.zookeeper.start&gt; is set to true</strong></p><p class="source-code"><strong class="bold">nifi.state.management.embedded.zookeeper.properties=./conf/zookeeper.properties</strong></p><p>The preceding commands tells NiFi to use the embedded version of Zookeeper. </p></li>
				<li>You now need to <a id="_idIndexMarker770"/>tell NiFi how to connect to Zookeeper in <strong class="source-inline">nifi.zookeeper.connect.string</strong>. The string is a comma-separated list of the Zookeeper servers in the format of <strong class="source-inline">&lt;hostname&gt;:&lt;port&gt;</strong>, and the port is <strong class="source-inline">clientPort</strong> from the <strong class="source-inline">zookeeper.config</strong> file, which was <strong class="source-inline">2181</strong>. The section of the file is shown in the following code block:<p class="source-code"><strong class="bold"># zookeeper properties, used for cluster management #</strong></p><p class="source-code"><strong class="bold">nifi.zookeeper.connect.string=nifi.zookeeper.connect.string=nifi-node-1:2181,nifi-node-2:2181 </strong></p><p class="source-code"><strong class="bold">nifi.zookeeper.connect.timeout=3 secs</strong></p><p class="source-code"><strong class="bold">nifi.zookeeper.session.timeout=3 secs</strong></p><p class="source-code"><strong class="bold">nifi.zookeeper.root.node=/nifi</strong></p></li>
				<li>Next, you will configure the <strong class="source-inline">cluster</strong> properties of NiFi. Specifically, you will set <strong class="source-inline">nifi.cluster.node</strong> to <strong class="source-inline">true</strong>. You will add the hostname of the node to <strong class="source-inline">nifi.cluster.node.address</strong>, as well as adding the port at <strong class="source-inline">nifi.cluster.node.protocol.port</strong>. You can set this to anything available and high enough such that you do not need root to access it (over <strong class="source-inline">1024</strong>). Lastly, you can change <strong class="source-inline">nifi.cluster.flow.election.max.wait.time</strong> to something shorter than <a id="_idIndexMarker771"/>5 minutes and you can add a value for <strong class="source-inline">nifi.cluster.flow.election.max.candidates</strong>. I have changed the wait time to <strong class="source-inline">1</strong> minute and left the candidates blank. The section of the file is shown in the following code block:<p class="source-code"><strong class="bold"># cluster node properties (only configure for cluster nodes) #</strong></p><p class="source-code"><strong class="bold">nifi.cluster.is.node=true </strong></p><p class="source-code"><strong class="bold">nifi.cluster.node.address=nifi-node-1 </strong></p><p class="source-code"><strong class="bold">nifi.cluster.node.protocol.port=8881 </strong></p><p class="source-code"><strong class="bold">nifi.cluster.node.protocol.threads=10</strong></p><p class="source-code"><strong class="bold">nifi.cluster.node.protocol.max.threads=50</strong></p><p class="source-code"><strong class="bold">nifi.cluster.node.event.history.size=25</strong></p><p class="source-code"><strong class="bold">nifi.cluster.node.connection.timeout=5 sec</strong></p><p class="source-code"><strong class="bold">nifi.cluster.node.read.timeout=5 sec</strong></p><p class="source-code"><strong class="bold">nifi.cluster.node.max.concurrent.requests=100</strong></p><p class="source-code"><strong class="bold">nifi.cluster.firewall.file=</strong></p><p class="source-code"><strong class="bold">nifi.cluster.flow.election.max.wait.time=1 mins </strong></p><p class="source-code"><strong class="bold">nifi.cluster.flow.election.max.candidates=</strong></p></li>
				<li>The web properties require the hostname of the machine as well as the port. By default, <strong class="source-inline">nifi.web.http.port</strong> is <strong class="source-inline">8080</strong>, but if you have something running on that port already, you can change it. I have changed it to <strong class="source-inline">8888</strong>. The hostname is <strong class="source-inline">nifi-node-1</strong> or <strong class="source-inline">nifi-mode-2</strong>. The web properties are shown in the following code block:<p class="source-code"><strong class="bold"># web properties #</strong></p><p class="source-code"><strong class="bold">nifi.web.war.directory=./lib</strong></p><p class="source-code"><strong class="bold">nifi.web.http.host=nifi-node-1 &lt;----------------------</strong></p><p class="source-code"><strong class="bold">nifi.web.http.port=8888</strong></p></li>
				<li>Lastly, NiFi uses<a id="_idIndexMarker772"/> Site-to-Site to communicate. You will need to configure the <strong class="source-inline">nifi.remote.input.host</strong> property to the machine hostname, and <strong class="source-inline">nifi.remote.input.socket.port</strong> to an available port. The properties file is shown in the following code block:<p class="source-code"><strong class="bold"># Site to Site properties</strong></p><p class="source-code"><strong class="bold">nifi.remote.input.host=nifi-node-1 </strong></p><p class="source-code"><strong class="bold">nifi.remote.input.secure=false</strong></p><p class="source-code"><strong class="bold">nifi.remote.input.socket.port=8882 </strong></p><p class="source-code"><strong class="bold">nifi.remote.input.http.enabled=true</strong></p><p class="source-code"><strong class="bold">nifi.remote.input.http.transaction.ttl=30 sec</strong></p><p class="source-code"><strong class="bold">nifi.remote.contents.cache.expiration=30 secs</strong></p><p>Each node will have the same settings in the <strong class="source-inline">nifi.properties</strong> file, with the exception of changing the hostname to the appropriate number, <strong class="source-inline">nifi-node-#</strong>.</p></li>
			</ol>
			<p>Your cluster is now configured, and you are ready to launch the two nodes. From each machine, launch NiFi as normal using the following command:</p>
			<p class="source-code">./nifi.sh start</p>
			<p>You should now be <a id="_idIndexMarker773"/>able to browse to any node at <strong class="source-inline">http://nifi-node-1:8888/nifi</strong>. You will see NiFi as usual, shown in the following screenshot:</p>
			<div>
				<div id="_idContainer230" class="IMG---Figure">
					<img src="image/Figure_16.1_B15739.jpg" alt="Figure 16.1 – NiFi running as a cluster&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.1 – NiFi running as a cluster</p>
			<p>Everything looks exactly the same, except for the top-left corner of the status bar. You should now have a cloud with <strong class="bold">2/2</strong> next to it. This is telling you that NiFi is running as a cluster with 2 out of 2 nodes available and connected. You can see the events by hovering over the messages on the right of the status bar. The following screenshot shows the election and connection of nodes:</p>
			<div>
				<div id="_idContainer231" class="IMG---Figure">
					<img src="image/Figure_16.2_B15739.jpg" alt="Figure 16.2 – Messages showing events in the cluster"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.2 – Messages showing events in the cluster</p>
			<p>Lastly, you can <a id="_idIndexMarker774"/>open the cluster window by selecting <strong class="bold">Cluster</strong> from the waffle menu in the right corner of the NiFi window. The cluster is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer232" class="IMG---Figure">
					<img src="image/Figure_16.3_B15739.jpg" alt="Figure 16.3 – Cluster details&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.3 – Cluster details</p>
			<p>The preceding<a id="_idIndexMarker775"/> screenshot shows which node is the Primary Node, along with the Controller Node and a Regular Node. From here you can also see details about the queues and disconnect or reconnect the nodes. The cluster is working, and you can now build a distributed data pipeline.</p>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor167"/>Building a distributed data pipeline</h1>
			<p>Building a<a id="_idIndexMarker776"/> distributed data pipeline is almost exactly the same as building a data pipeline to run on a single machine. NiFi will handle the logistics of passing and recombining the data. A basic data pipeline is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer233" class="IMG---Figure">
					<img src="image/Figure_16.4_B15739.jpg" alt="Figure 16.4 – A basic data pipeline to generate data, extract attributes to json, and write to disk&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.4 – A basic data pipeline to generate data, extract attributes to json, and write to disk</p>
			<p>The preceding<a id="_idIndexMarker777"/> data pipeline uses the <strong class="source-inline">GenerateFlowFile</strong> processor to create unique flowfiles. This is passed downstream to the <strong class="source-inline">AttributesToJSON</strong> processor, which extracts the attributes and writes to the flowfile content. Lastly, the file is written to disk at <strong class="source-inline">/home/paulcrickard/output</strong>. </p>
			<p>Before running the data pipeline, you will need to make sure that you have the output directory for the <strong class="source-inline">PutFile</strong> processor on each node. Earlier, I said that data pipelines are no different when distributed, but there are some things you must keep in mind, one being that <strong class="source-inline">PutFile</strong> will write to disk on every node by default. You will need to configure your processors to be able to run on any node. We will fix this later in this section. </p>
			<p>One more thing before you run the data pipeline. Open the browser to your other node. You will see the exact same data pipeline in that node. Even the layout of the processors is the same. Changes to any node will be distributed to all the other nodes. You can work from any node.</p>
			<p>When you run<a id="_idIndexMarker778"/> the data pipeline, you will see files written to the output directory of both nodes. The data pipeline is running and distributing the load across the nodes. The following screenshot shows the output of the data pipeline:</p>
			<div>
				<div id="_idContainer234" class="IMG---Figure">
					<img src="image/Figure_16.5_B15739.jpg" alt="Figure 16.5 – Data pipeline writing flowfiles to a node&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.5 – Data pipeline writing flowfiles to a node</p>
			<p>If you are getting the same results as the preceding screenshot, congratulations, you have just built a distributed data pipeline. Next, you will learn some more features of the NiFi cluster.</p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor168"/>Managing the distributed data pipeline</h1>
			<p>The <a id="_idIndexMarker779"/>preceding data pipeline runs on each node. To compensate for that, you had to create the same path on both nodes for the <strong class="source-inline">PutFile</strong> processor to work. Earlier, you learned that there are several processors that can result in race conditions – trying to read the same file at the same time – which will cause problems. To resolve these issues, you can specify that a processor should only run on the Primary Node – as an isolated process. </p>
			<p>In the configuration for the <strong class="source-inline">PutFile</strong> processor, select the <strong class="bold">Scheduling</strong> tab. In the dropdown for <strong class="bold">Scheduling Strategy</strong>, choose <strong class="bold">On primary node</strong>, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer235" class="IMG---Figure">
					<img src="image/Figure_16.6_B15739.jpg" alt="Figure 16.6 – Running a processor on the Primary Node only&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.6 – Running a processor on the Primary Node only</p>
			<p>Now, when you run the data pipeline, the files will only be placed on the Primary Node. You can schedule processors such as <strong class="source-inline">GetFile</strong> or <strong class="source-inline">ExecuteSQL</strong> to do the same thing. </p>
			<p>To see the load of the data pipeline on each node, you can look at the cluster details from the waffle menu. As data moves through the data pipeline, you can see how many flowfiles are sitting in the queues of each node. The following screenshot shows the pipeline running on my cluster:</p>
			<div>
				<div id="_idContainer236" class="IMG---Figure">
					<img src="image/Figure_16.7_B15739.jpg" alt="Figure 16.7 – Viewing the queues of each node. Each node has four flowfiles&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.7 – Viewing the queues of each node. Each node has four flowfiles</p>
			<p>The <a id="_idIndexMarker780"/>data pipeline is distributing the flowfiles evenly across the nodes. In <strong class="bold">Zero-Master Clustering</strong>, the <a id="_idIndexMarker781"/>data is not copied or replicated. It exists only on the node that is processing it. If a node goes down, the data needs to be redistributed. This can only happen if the node is still connected to the network, otherwise, it will not happen until the node rejoins.</p>
			<p>You can manually disconnect a node by clicking the power icon on the right of the node's row. The following screenshot shows a node being disconnected:</p>
			<div>
				<div id="_idContainer237" class="IMG---Figure">
					<img src="image/Figure_16.9_B15739.jpg" alt="Figure 16.8 – nifi-node-1 has been disconnected from the cluster&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.8 – nifi-node-1 has been disconnected from the cluster</p>
			<p>In the<a id="_idIndexMarker782"/> preceding screenshot, you can see that <strong class="source-inline">nifi-node-1</strong> has a status of <strong class="bold">DISCONNECTED</strong>. But you should also notice that it has eight flowfiles that need to be redistributed. Since you disconnected the node, but did not drop it from the network, NiFi will redistribute the flowfiles. You can see the results when the screen is refreshed, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer238" class="IMG---Figure">
					<img src="image/Figure_16.8_B15739.jpg" alt="Figure 16.9 – Redistributed flowfiles from a disconnected node&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.9 – Redistributed flowfiles from a disconnected node</p>
			<p>You can also<a id="_idIndexMarker783"/> reconnect any disconnected nodes. You do this by clicking the plug icon. When you do, the node will rejoin the cluster and the flowfiles will be redistributed. The following screenshot shows the node rejoined to the cluster:</p>
			<div>
				<div id="_idContainer239" class="IMG---Figure">
					<img src="image/Figure_16.10_B15739.jpg" alt="Figure 16.10 – Reconnecting a node and flowfile redistribution"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.10 – Reconnecting a node and flowfile redistribution</p>
			<p>In the <a id="_idIndexMarker784"/>preceding screenshot, the flowfiles have accumulated since the node was disconnected evenly across the nodes.</p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor169"/>Summary</h1>
			<p>In this Appendix, you learned the basics of NiFi clustering, as well as how to build a cluster with the embedded Zookeeper and how to build distributed data pipelines. NiFi handles most of the distribution of data; you only need to keep in mind the gotchas – such as race conditions and the fact that processors need to be configured to run on any node. Using a NiFi cluster allows you to manage NiFi on several machines from a single instance. It also allows you to process large amounts of data and have some redundancy in case an instance crashes.</p>
		</div>
	</body></html>