<html><head></head><body>
		<div><h1 id="_idParaDest-161"><a id="_idTextAnchor163"/><em class="italic">Appendix</em></h1>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor164"/>Building a NiFi cluster</h1>
			<p>In this book, you<a id="_idIndexMarker761"/> have built a Kafka cluster, a ZooKeeper cluster, and a Spark cluster. Instead of increasing the power of a single server, through clustering, you are able to add more machines to increase the processing power of a data pipeline. In this chapter, you will learn how to cluster NiFi so that your data pipelines can run across multiple machines.</p>
			<p>In this appendix, we're going to cover the following main topics:</p>
			<ul>
				<li>The basics of NiFi clustering</li>
				<li>Building a NiFi cluster</li>
				<li>Building a distributed data pipeline</li>
				<li>Managing the distributed data pipeline</li>
			</ul>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor165"/>The basics of NiFi clustering</h1>
			<p>Clustering<a id="_idIndexMarker762"/> in Apache NiFi follows a <strong class="bold">Zero-Master Clustering</strong> architecture. In this <a id="_idIndexMarker763"/>type of clustering, there is no pre-defined master. Every node can perform the same tasks, and the data is split between them. NiFi uses Zookeeper when deployed as a cluster. </p>
			<p>Zookeeper will elect a <strong class="bold">Cluster Coordinator</strong>. The Cluster Coordinator is<a id="_idIndexMarker764"/> responsible for deciding whether new nodes can join – the nodes will connect to the coordinator – and to provide the updated flows to the new nodes. </p>
			<p>While it sounds like the Cluster Coordinator is the master, it is not. You can make changes to the data pipelines on any node and they will be replicated to all the other nodes, meaning a non-Cluster Coordinator or a non-Primary Node can submit changes.</p>
			<p>The <code>ExecuteSQL</code> processor can run on the Primary Node, and then distribute the data to the other nodes downstream for processing. You will see how this is done later in this chapter.</p>
			<p>Clustering <a id="_idIndexMarker766"/>allows you to build data pipelines that can process larger amounts of data than on a single machine. Furthermore, it allows a single point to build and monitor data pipelines. If you had several single-node NiFi instances running, you would need to manage all of them. Changes to a data pipeline on one would need to be replicated on the others or at least checked to make sure it is not a duplicate. Which machine is running the data warehouse pipeline again? I forgot. Managing a cluster, from any node, makes it much easier and more efficient.</p>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor166"/>Building a NiFi cluster </h1>
			<p>In this section, you <a id="_idIndexMarker767"/>will build a two-node cluster on different machines. Just like with MiNiFi, however, there are some compatibility issues with the newest versions of NiFi and Zookeeper. To work around these issues and demonstrate the concepts, this chapter will use an older version of NiFi and the pre-bundled Zookeeper. To build the NiFi cluster, perform the following steps:</p>
			<ol>
				<li>As root, or using sudo, open your <code>/etc/hosts</code> file. You will need to assign names to the machines that you will use in your cluster. It is best practice to use a hostname instead of IP addresses. Your hosts file should look like the following example:<pre><strong class="bold">127.0.0.1	localhost</strong>
<strong class="bold">::1		localhost</strong>
<strong class="bold">127.0.1.1	pop-os.localdomain	pop-os</strong>
<strong class="bold">10.0.0.63	nifi-node-2</strong>
<strong class="bold">10.0.0.148	nifi-node-1</strong></pre></li>
				<li>In the preceding hosts file, I have added the last two lines. The nodes are <code>nifi-node-1</code> and <code>nifi-node-2</code> and you can see that they have different IP addresses. Make these changes in the hosts file for each machine. When you have finished, you can test that it works by using <code>ping</code>. From each machine, try to use <code>ping</code> to hit<a id="_idIndexMarker768"/> the other machine by hostname. The following is the command to hit <code>nifi-node-2</code> from the <code>nifi-node-1</code> machine:<pre><strong class="bold">paulcrickard@pop-os:~$ ping nifi-node-2</strong>
<strong class="bold">PING nifi-node-2 (10.0.0.63) 56(84) bytes of data.</strong>
<strong class="bold">64 bytes from nifi-node-2 (10.0.0.63): icmp_seq=1 ttl=64 time=55.1 ms</strong>
<strong class="bold">64 bytes from nifi-node-2 (10.0.0.63): icmp_seq=2 ttl=64 time=77.1 ms</strong>
<strong class="bold">64 bytes from nifi-node-2 (10.0.0.63): icmp_seq=3 ttl=64 time=101 ms</strong>
<strong class="bold">64 bytes from nifi-node-2 (10.0.0.63): icmp_seq=4 ttl=64 time=32.8 ms</strong></pre></li>
				<li>If you do the opposite from your other node, <code>nifi-node-2</code>, you should get the same results – <code>nifi-node-1</code> will return data. </li>
				<li>Next, download an older version of Apache NiFi, 1.0.0, at <a href="https://archive.apache.org/dist/nifi/1.0.0/">https://archive.apache.org/dist/nifi/1.0.0/</a>. Select the <code>-bin.tar.gz</code> file as it contains the binaries. Once the file has downloaded, extract the files using your file manager or with the following command:<pre><strong class="bold">tar -xvzf nifi-1.0.0-bin.tar.gz</strong></pre><p>Once you have extracted the files, you will edit the configuration files.</p></li>
				<li>To edit the Zookeeper configuration file, open <code>zookeeper.properties</code> in the <code>$NIFI_HOME/conf</code> directory. At the bottom of the file, add your servers as shown: <pre><strong class="bold">server.1=nifi-node-1:2888:3888</strong>
<strong class="bold">server.2=nifi-node-2:2888:3888</strong></pre></li>
				<li>At the top of the file, you will see <code>clientPort</code> and <code>dataDir</code>. It should look like the following example:<pre><strong class="bold">clientPort=2181</strong>
<strong class="bold">initLimit=10</strong>
<strong class="bold">autopurge.purgeInterval=24</strong>
<strong class="bold">syncLimit=5</strong>
<strong class="bold">tickTime=2000</strong>
<strong class="bold">dataDir=./state/zookeeper</strong>
<strong class="bold">autopurge.snapRetainCount=30</strong></pre></li>
				<li>In <code>dataDir</code>, you<a id="_idIndexMarker769"/> will need to add a file named <code>myfile</code> with the number of the server as the content. On <code>server.1</code> (<code>nifi-node-1</code>), you will create a <code>myid</code> ID with <code>1</code> as the content. To do that, from the <code>$NIFI_HOME</code> directory, use the following commands:<pre><strong class="bold">mkdir state</strong>
<strong class="bold">mkdir state/zookeeper</strong>
<strong class="bold">echo 1 &gt;&gt; myid</strong></pre></li>
				<li>On <code>nifi-node-2</code>, repeat the preceding steps, except change <code>echo</code> to the following line:<pre><code>nifi.properties</code> file.</p></li>
				<li>To edit <code>nifi.properties</code>, you will need to change several properties. The first property is <code>nifi.state.management.embedded.zookeeper.start</code>, which needs to be set to <code>true</code>. The section of the file is shown as follows:<pre><strong class="bold">####################</strong>
<strong class="bold"># State Management #</strong>
<strong class="bold">####################</strong>
<strong class="bold">nifi.state.management.configuration.file=./conf/state-management.xml</strong>
<strong class="bold"># The ID of the local state provider</strong>
<strong class="bold">nifi.state.management.provider.local=local-provider</strong>
<strong class="bold"># The ID of the cluster-wide state provider. This will be ignored if NiFi is not clustered but must be populated if running in a cluster.</strong>
<strong class="bold">nifi.state.management.provider.cluster=zk-provider</strong>
<strong class="bold"># Specifies whether or not this instance of NiFi should run an embedded ZooKeeper server</strong>
<strong class="bold">nifi.state.management.embedded.zookeeper.start=true  </strong>
<strong class="bold"># Properties file that provides the ZooKeeper properties to use if &lt;nifi.state.management.embedded.zookeeper.start&gt; is set to true</strong>
<strong class="bold">nifi.state.management.embedded.zookeeper.properties=./conf/zookeeper.properties</strong></pre><p>The preceding commands tells NiFi to use the embedded version of Zookeeper. </p></li>
				<li>You now need to <a id="_idIndexMarker770"/>tell NiFi how to connect to Zookeeper in <code>nifi.zookeeper.connect.string</code>. The string is a comma-separated list of the Zookeeper servers in the format of <code>&lt;hostname&gt;:&lt;port&gt;</code>, and the port is <code>clientPort</code> from the <code>zookeeper.config</code> file, which was <code>2181</code>. The section of the file is shown in the following code block:<pre><strong class="bold"># zookeeper properties, used for cluster management #</strong>
<strong class="bold">nifi.zookeeper.connect.string=nifi.zookeeper.connect.string=nifi-node-1:2181,nifi-node-2:2181 </strong>
<strong class="bold">nifi.zookeeper.connect.timeout=3 secs</strong>
<strong class="bold">nifi.zookeeper.session.timeout=3 secs</strong>
<strong class="bold">nifi.zookeeper.root.node=/nifi</strong></pre></li>
				<li>Next, you will configure the <code>cluster</code> properties of NiFi. Specifically, you will set <code>nifi.cluster.node</code> to <code>true</code>. You will add the hostname of the node to <code>nifi.cluster.node.address</code>, as well as adding the port at <code>nifi.cluster.node.protocol.port</code>. You can set this to anything available and high enough such that you do not need root to access it (over <code>1024</code>). Lastly, you can change <code>nifi.cluster.flow.election.max.wait.time</code> to something shorter than <a id="_idIndexMarker771"/>5 minutes and you can add a value for <code>nifi.cluster.flow.election.max.candidates</code>. I have changed the wait time to <code>1</code> minute and left the candidates blank. The section of the file is shown in the following code block:<pre><strong class="bold"># cluster node properties (only configure for cluster nodes) #</strong>
<strong class="bold">nifi.cluster.is.node=true </strong>
<strong class="bold">nifi.cluster.node.address=nifi-node-1 </strong>
<strong class="bold">nifi.cluster.node.protocol.port=8881 </strong>
<strong class="bold">nifi.cluster.node.protocol.threads=10</strong>
<strong class="bold">nifi.cluster.node.protocol.max.threads=50</strong>
<strong class="bold">nifi.cluster.node.event.history.size=25</strong>
<strong class="bold">nifi.cluster.node.connection.timeout=5 sec</strong>
<strong class="bold">nifi.cluster.node.read.timeout=5 sec</strong>
<strong class="bold">nifi.cluster.node.max.concurrent.requests=100</strong>
<strong class="bold">nifi.cluster.firewall.file=</strong>
<strong class="bold">nifi.cluster.flow.election.max.wait.time=1 mins </strong>
<strong class="bold">nifi.cluster.flow.election.max.candidates=</strong></pre></li>
				<li>The web properties require the hostname of the machine as well as the port. By default, <code>nifi.web.http.port</code> is <code>8080</code>, but if you have something running on that port already, you can change it. I have changed it to <code>8888</code>. The hostname is <code>nifi-node-1</code> or <code>nifi-mode-2</code>. The web properties are shown in the following code block:<pre><strong class="bold"># web properties #</strong>
<strong class="bold">nifi.web.war.directory=./lib</strong>
<strong class="bold">nifi.web.http.host=nifi-node-1 &lt;----------------------</strong>
<strong class="bold">nifi.web.http.port=8888</strong></pre></li>
				<li>Lastly, NiFi uses<a id="_idIndexMarker772"/> Site-to-Site to communicate. You will need to configure the <code>nifi.remote.input.host</code> property to the machine hostname, and <code>nifi.remote.input.socket.port</code> to an available port. The properties file is shown in the following code block:<pre><code>nifi.properties</code> file, with the exception of changing the hostname to the appropriate number, <code>nifi-node-#</code>.</p></li>
			</ol>
			<p>Your cluster is now configured, and you are ready to launch the two nodes. From each machine, launch NiFi as normal using the following command:</p>
			<pre>./nifi.sh start</pre>
			<p>You should now be <a id="_idIndexMarker773"/>able to browse to any node at <code>http://nifi-node-1:8888/nifi</code>. You will see NiFi as usual, shown in the following screenshot:</p>
			<div><div><img src="img/Figure_16.1_B15739.jpg" alt="Figure 16.1 – NiFi running as a cluster&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.1 – NiFi running as a cluster</p>
			<p>Everything looks exactly the same, except for the top-left corner of the status bar. You should now have a cloud with <strong class="bold">2/2</strong> next to it. This is telling you that NiFi is running as a cluster with 2 out of 2 nodes available and connected. You can see the events by hovering over the messages on the right of the status bar. The following screenshot shows the election and connection of nodes:</p>
			<div><div><img src="img/Figure_16.2_B15739.jpg" alt="Figure 16.2 – Messages showing events in the cluster"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.2 – Messages showing events in the cluster</p>
			<p>Lastly, you can <a id="_idIndexMarker774"/>open the cluster window by selecting <strong class="bold">Cluster</strong> from the waffle menu in the right corner of the NiFi window. The cluster is shown in the following screenshot:</p>
			<div><div><img src="img/Figure_16.3_B15739.jpg" alt="Figure 16.3 – Cluster details&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.3 – Cluster details</p>
			<p>The preceding<a id="_idIndexMarker775"/> screenshot shows which node is the Primary Node, along with the Controller Node and a Regular Node. From here you can also see details about the queues and disconnect or reconnect the nodes. The cluster is working, and you can now build a distributed data pipeline.</p>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor167"/>Building a distributed data pipeline</h1>
			<p>Building a<a id="_idIndexMarker776"/> distributed data pipeline is almost exactly the same as building a data pipeline to run on a single machine. NiFi will handle the logistics of passing and recombining the data. A basic data pipeline is shown in the following screenshot:</p>
			<div><div><img src="img/Figure_16.4_B15739.jpg" alt="Figure 16.4 – A basic data pipeline to generate data, extract attributes to json, and write to disk&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.4 – A basic data pipeline to generate data, extract attributes to json, and write to disk</p>
			<p>The preceding<a id="_idIndexMarker777"/> data pipeline uses the <code>GenerateFlowFile</code> processor to create unique flowfiles. This is passed downstream to the <code>AttributesToJSON</code> processor, which extracts the attributes and writes to the flowfile content. Lastly, the file is written to disk at <code>/home/paulcrickard/output</code>. </p>
			<p>Before running the data pipeline, you will need to make sure that you have the output directory for the <code>PutFile</code> processor on each node. Earlier, I said that data pipelines are no different when distributed, but there are some things you must keep in mind, one being that <code>PutFile</code> will write to disk on every node by default. You will need to configure your processors to be able to run on any node. We will fix this later in this section. </p>
			<p>One more thing before you run the data pipeline. Open the browser to your other node. You will see the exact same data pipeline in that node. Even the layout of the processors is the same. Changes to any node will be distributed to all the other nodes. You can work from any node.</p>
			<p>When you run<a id="_idIndexMarker778"/> the data pipeline, you will see files written to the output directory of both nodes. The data pipeline is running and distributing the load across the nodes. The following screenshot shows the output of the data pipeline:</p>
			<div><div><img src="img/Figure_16.5_B15739.jpg" alt="Figure 16.5 – Data pipeline writing flowfiles to a node&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.5 – Data pipeline writing flowfiles to a node</p>
			<p>If you are getting the same results as the preceding screenshot, congratulations, you have just built a distributed data pipeline. Next, you will learn some more features of the NiFi cluster.</p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor168"/>Managing the distributed data pipeline</h1>
			<p>The <a id="_idIndexMarker779"/>preceding data pipeline runs on each node. To compensate for that, you had to create the same path on both nodes for the <code>PutFile</code> processor to work. Earlier, you learned that there are several processors that can result in race conditions – trying to read the same file at the same time – which will cause problems. To resolve these issues, you can specify that a processor should only run on the Primary Node – as an isolated process. </p>
			<p>In the configuration for the <code>PutFile</code> processor, select the <strong class="bold">Scheduling</strong> tab. In the dropdown for <strong class="bold">Scheduling Strategy</strong>, choose <strong class="bold">On primary node</strong>, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_16.6_B15739.jpg" alt="Figure 16.6 – Running a processor on the Primary Node only&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.6 – Running a processor on the Primary Node only</p>
			<p>Now, when you run the data pipeline, the files will only be placed on the Primary Node. You can schedule processors such as <code>GetFile</code> or <code>ExecuteSQL</code> to do the same thing. </p>
			<p>To see the load of the data pipeline on each node, you can look at the cluster details from the waffle menu. As data moves through the data pipeline, you can see how many flowfiles are sitting in the queues of each node. The following screenshot shows the pipeline running on my cluster:</p>
			<div><div><img src="img/Figure_16.7_B15739.jpg" alt="Figure 16.7 – Viewing the queues of each node. Each node has four flowfiles&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.7 – Viewing the queues of each node. Each node has four flowfiles</p>
			<p>The <a id="_idIndexMarker780"/>data pipeline is distributing the flowfiles evenly across the nodes. In <strong class="bold">Zero-Master Clustering</strong>, the <a id="_idIndexMarker781"/>data is not copied or replicated. It exists only on the node that is processing it. If a node goes down, the data needs to be redistributed. This can only happen if the node is still connected to the network, otherwise, it will not happen until the node rejoins.</p>
			<p>You can manually disconnect a node by clicking the power icon on the right of the node's row. The following screenshot shows a node being disconnected:</p>
			<div><div><img src="img/Figure_16.9_B15739.jpg" alt="Figure 16.8 – nifi-node-1 has been disconnected from the cluster&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.8 – nifi-node-1 has been disconnected from the cluster</p>
			<p>In the<a id="_idIndexMarker782"/> preceding screenshot, you can see that <code>nifi-node-1</code> has a status of <strong class="bold">DISCONNECTED</strong>. But you should also notice that it has eight flowfiles that need to be redistributed. Since you disconnected the node, but did not drop it from the network, NiFi will redistribute the flowfiles. You can see the results when the screen is refreshed, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_16.8_B15739.jpg" alt="Figure 16.9 – Redistributed flowfiles from a disconnected node&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.9 – Redistributed flowfiles from a disconnected node</p>
			<p>You can also<a id="_idIndexMarker783"/> reconnect any disconnected nodes. You do this by clicking the plug icon. When you do, the node will rejoin the cluster and the flowfiles will be redistributed. The following screenshot shows the node rejoined to the cluster:</p>
			<div><div><img src="img/Figure_16.10_B15739.jpg" alt="Figure 16.10 – Reconnecting a node and flowfile redistribution"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.10 – Reconnecting a node and flowfile redistribution</p>
			<p>In the <a id="_idIndexMarker784"/>preceding screenshot, the flowfiles have accumulated since the node was disconnected evenly across the nodes.</p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor169"/>Summary</h1>
			<p>In this Appendix, you learned the basics of NiFi clustering, as well as how to build a cluster with the embedded Zookeeper and how to build distributed data pipelines. NiFi handles most of the distribution of data; you only need to keep in mind the gotchas – such as race conditions and the fact that processors need to be configured to run on any node. Using a NiFi cluster allows you to manage NiFi on several machines from a single instance. It also allows you to process large amounts of data and have some redundancy in case an instance crashes.</p>
		</div>
	</body></html>