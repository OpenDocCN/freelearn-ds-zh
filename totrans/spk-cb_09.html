<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Unsupervised Learning with MLlib</h1></div></div></div><p>This chapter will cover how we can do unsupervised learning using MLlib, Spark's machine learning library.</p><p>This chapter is divided into the following recipes:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Clustering using k-means</li><li class="listitem" style="list-style-type: disc">Dimensionality reduction with principal component analysis</li><li class="listitem" style="list-style-type: disc">Dimensionality reduction with singular value decomposition</li></ul></div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec63"/>Introduction</h1></div></div></div><p>The following<a id="id498" class="indexterm"/> is Wikipedia's definition of unsupervised learning:</p><div><blockquote class="blockquote"><p><em>"In machine learning, the problem of unsupervised learning is that of trying to find hidden structure in unlabeled data."</em></p></blockquote></div><p>In contrast to <a id="id499" class="indexterm"/>supervised learning where we have labeled data to train an algorithm, in unsupervised learning we ask the algorithm to find a structure on its own. Let's take a look at the following sample dataset:</p><div><img src="img/3056_09_01.jpg" alt="Introduction"/></div><p>As you can see from the preceding graph, the data points are forming two clusters as follows:</p><div><img src="img/3056_09_02.jpg" alt="Introduction"/></div><p>In fact, clustering<a id="id500" class="indexterm"/> is the most common type of unsupervised learning algorithm.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec64"/>Clustering using k-means</h1></div></div></div><p>Cluster analysis or <a id="id501" class="indexterm"/>clustering is the process of grouping data into multiple groups <a id="id502" class="indexterm"/>so that the data in one group is similar to the data in other groups.</p><p>The following are<a id="id503" class="indexterm"/> a few examples where clustering is used:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Market segmentation</strong>: Dividing the target market into multiple segments so that the <a id="id504" class="indexterm"/>needs of each segment can be served better</li><li class="listitem" style="list-style-type: disc"><strong>Social network analysis</strong>: Finding a coherent group of people in the social network <a id="id505" class="indexterm"/>for ad targeting through a social networking site such as Facebook</li><li class="listitem" style="list-style-type: disc"><strong>Data center computing clusters</strong>: Putting a set of computers together to improve<a id="id506" class="indexterm"/> performance</li><li class="listitem" style="list-style-type: disc"><strong>Astronomical data analysis</strong>: Understanding astronomical data and events <a id="id507" class="indexterm"/>such as galaxy formations</li><li class="listitem" style="list-style-type: disc"><strong>Real estate</strong>: Identifying <a id="id508" class="indexterm"/>neighborhoods based on similar features</li><li class="listitem" style="list-style-type: disc"><strong>Text analysis</strong>: Dividing<a id="id509" class="indexterm"/> text documents, such as novels or essays, into genres</li></ul></div><p>The k-means <a id="id510" class="indexterm"/>algorithm is best illustrated using imagery, so let's<a id="id511" class="indexterm"/> look at our sample figure again:</p><div><img src="img/3056_09_01.jpg" alt="Clustering using k-means"/></div><p>The first step in k-means<a id="id512" class="indexterm"/> is to randomly select two points called <strong>cluster centroids</strong>:</p><div><img src="img/3056_09_03.jpg" alt="Clustering using k-means"/></div><p>The k-means algorithm<a id="id513" class="indexterm"/> is an iterative algorithm and<a id="id514" class="indexterm"/> works in two steps:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Cluster assignment step</strong>: This algorithm will go through each data point and, depending <a id="id515" class="indexterm"/>upon which centroid it is nearer to, it will be assigned that centroid and, in turn, the cluster it represents</li><li class="listitem" style="list-style-type: disc"><strong>Move centroid step</strong>: This algorithm will take each centroid and move it to the<a id="id516" class="indexterm"/> mean of the data points in the cluster</li></ul></div><p>Let's see how our data looks after the cluster assignment:</p><div><img src="img/3056_09_04.jpg" alt="Clustering using k-means"/></div><p>Now let's<a id="id517" class="indexterm"/> move the cluster centroids to the mean value of the<a id="id518" class="indexterm"/> data points in a cluster, as follows:</p><div><img src="img/3056_09_05.jpg" alt="Clustering using k-means"/></div><p>In this <a id="id519" class="indexterm"/>case, one iteration is enough and further iterations <a id="id520" class="indexterm"/>will not move the cluster centroids. For most real data, multiple iterations are required to move the centroid to the final position.</p><p>The k-means algorithm takes a number of clusters as input.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec93"/>Getting ready</h2></div></div></div><p>Let's use some different housing data from the City of Saratoga, CA. This time, we are going to take lot size and house price:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Lot size</p>
</th><th style="text-align: left" valign="bottom">
<p>House price (in $1,000)</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>12839</p>
</td><td style="text-align: left" valign="top">
<p>2405</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10000</p>
</td><td style="text-align: left" valign="top">
<p>2200</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>8040</p>
</td><td style="text-align: left" valign="top">
<p>1400</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>13104</p>
</td><td style="text-align: left" valign="top">
<p>1800</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10000</p>
</td><td style="text-align: left" valign="top">
<p>2351</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3049</p>
</td><td style="text-align: left" valign="top">
<p>795</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>38768</p>
</td><td style="text-align: left" valign="top">
<p>2725</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>16250</p>
</td><td style="text-align: left" valign="top">
<p>2150</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>43026</p>
</td><td style="text-align: left" valign="top">
<p>2724</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>44431</p>
</td><td style="text-align: left" valign="top">
<p>2675</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>40000</p>
</td><td style="text-align: left" valign="top">
<p>2930</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1260</p>
</td><td style="text-align: left" valign="top">
<p>870</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>15000</p>
</td><td style="text-align: left" valign="top">
<p>2210</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10032</p>
</td><td style="text-align: left" valign="top">
<p>1145</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>12420</p>
</td><td style="text-align: left" valign="top">
<p>2419</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>69696</p>
</td><td style="text-align: left" valign="top">
<p>2750</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>12600</p>
</td><td style="text-align: left" valign="top">
<p>2035</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10240</p>
</td><td style="text-align: left" valign="top">
<p>1150</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>876</p>
</td><td style="text-align: left" valign="top">
<p>665</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>8125</p>
</td><td style="text-align: left" valign="top">
<p>1430</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>11792</p>
</td><td style="text-align: left" valign="top">
<p>1920</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1512</p>
</td><td style="text-align: left" valign="top">
<p>1230</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1276</p>
</td><td style="text-align: left" valign="top">
<p>975</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>67518</p>
</td><td style="text-align: left" valign="top">
<p>2400</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>9810</p>
</td><td style="text-align: left" valign="top">
<p>1725</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>6324</p>
</td><td style="text-align: left" valign="top">
<p>2300</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>12510</p>
</td><td style="text-align: left" valign="top">
<p>1700</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>15616</p>
</td><td style="text-align: left" valign="top">
<p>1915</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>15476</p>
</td><td style="text-align: left" valign="top">
<p>2278</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>13390</p>
</td><td style="text-align: left" valign="top">
<p>2497.5</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1158</p>
</td><td style="text-align: left" valign="top">
<p>725</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2000</p>
</td><td style="text-align: left" valign="top">
<p>870</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2614</p>
</td><td style="text-align: left" valign="top">
<p>730</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>13433</p>
</td><td style="text-align: left" valign="top">
<p>2050</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>12500</p>
</td><td style="text-align: left" valign="top">
<p>3330</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>15750</p>
</td><td style="text-align: left" valign="top">
<p>1120</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>13996</p>
</td><td style="text-align: left" valign="top">
<p>4100</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10450</p>
</td><td style="text-align: left" valign="top">
<p>1655</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>7500</p>
</td><td style="text-align: left" valign="top">
<p>1550</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>12125</p>
</td><td style="text-align: left" valign="top">
<p>2100</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>14500</p>
</td><td style="text-align: left" valign="top">
<p>2100</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10000</p>
</td><td style="text-align: left" valign="top">
<p>1175</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10019</p>
</td><td style="text-align: left" valign="top">
<p>2047.5</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>48787</p>
</td><td style="text-align: left" valign="top">
<p>3998</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>53579</p>
</td><td style="text-align: left" valign="top">
<p>2688</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10788</p>
</td><td style="text-align: left" valign="top">
<p>2251</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>11865</p>
</td><td style="text-align: left" valign="top">
<p>1906</p>
</td></tr></tbody></table></div><p>Let's<a id="id521" class="indexterm"/> convert<a id="id522" class="indexterm"/> this data into a <strong>comma-separated value</strong> (<strong>CSV</strong>) file called <code class="literal">saratoga.c</code>
<code class="literal">sv</code> and<a id="id523" class="indexterm"/> draw it as a scatter plot:</p><div><img src="img/3056_09_06.jpg" alt="Getting ready"/></div><p>Finding a number of clusters is a tricky task. Here, we have the advantage of visual inspection, which is not available for data on hyperplanes (more than three dimensions). Let's roughly divide the data into four clusters as follows:</p><div><img src="img/3056_09_07.jpg" alt="Getting ready"/></div><p>We will<a id="id524" class="indexterm"/> run the k-means algorithm to do the same and see <a id="id525" class="indexterm"/>how close our results come.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec94"/>How to do it…</h2></div></div></div><div><ol class="orderedlist arabic"><li class="listitem">Load <code class="literal">sarataga.csv</code> to HDFS:<div><pre class="programlisting">
<strong>$ hdfs dfs -put saratoga.csv saratoga.csv</strong>
</pre></div></li><li class="listitem">Start the Spark shell:<div><pre class="programlisting">
<strong>$ spark-shell</strong>
</pre></div></li><li class="listitem">Import statistics and related classes:<div><pre class="programlisting">
<strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong>
<strong>scala&gt; import org.apache.spark.mllib.clustering.KMeans</strong>
</pre></div></li><li class="listitem">Load <code class="literal">saratoga.csv</code> as an RDD:<div><pre class="programlisting">
<strong>scala&gt; val data = sc.textFile("saratoga.csv")</strong>
</pre></div></li><li class="listitem">Transform the data into an RDD of dense vectors:<div><pre class="programlisting">
<strong>scala&gt; val parsedData = data.map( line =&gt; Vectors.dense(line.split(',').map(_.toDouble)))</strong>
</pre></div></li><li class="listitem">Train the model for four clusters and five iterations:<div><pre class="programlisting">
<strong>scala&gt; val kmmodel= KMeans.train(parsedData,4,5)</strong>
</pre></div></li><li class="listitem">Collect <code class="literal">parsedData</code> as a local scala collection:<div><pre class="programlisting">
<strong>scala&gt; val houses = parsedData.collect</strong>
</pre></div></li><li class="listitem">Predict the cluster for the 0th element:<div><pre class="programlisting">
<strong>scala&gt; val prediction = kmmodel.predict(houses(0))</strong>
</pre></div></li><li class="listitem">Now let's compare the cluster assignments by k-means versus the ones we have done individually. The k-means algorithm gives the cluster IDs starting from 0. Once you inspect the data, you find out the following mapping between the A to D cluster IDs we gave versus k-means: A=&gt;3, B=&gt;1, C=&gt;0, D=&gt;2.</li><li class="listitem">Now, let's pick some of the data from different parts of the chart and predict which <a id="id526" class="indexterm"/>cluster it belongs to.</li><li class="listitem">Let's look<a id="id527" class="indexterm"/> at the house (18) data, which has a lot size of 876 sq ft and is priced at $665K:<div><pre class="programlisting">
<strong>scala&gt; val prediction = kmmodel.predict(houses(18))</strong>
<strong>resxx: Int = 3</strong>
</pre></div></li><li class="listitem">Now, look at the data for house (35) with a lot size of 15,750 sq ft and a price of $1.12 million:<div><pre class="programlisting">
<strong>scala&gt; val prediction = kmmodel.predict(houses(35))</strong>
<strong>resxx: Int = 1</strong>
</pre></div></li><li class="listitem">Now look at the house (6) data, which has a lot size of 38,768 sq ft and is priced at $2.725 million:<div><pre class="programlisting">
<strong>scala&gt; val prediction = kmmodel.predict(houses(6))</strong>
<strong>resxx: Int = 0</strong>
</pre></div></li><li class="listitem">Now look at the house (15) data, which has a lot size of 69,696 sq ft and is priced at $2.75 million:<div><pre class="programlisting">
<strong>scala&gt;  val prediction = kmmodel.predict(houses(15))</strong>
<strong>resxx: Int = 2</strong>
</pre></div></li></ol></div><p>You can test the prediction capability with more data. Let's do some neighborhood analysis to see what meaning these clusters carry. Most of the houses in cluster 3 are near downtown. The cluster 2 houses are on hilly terrain.</p><p>In this example, we dealt with a very small set of features; common sense and visual inspection would also lead us to the same conclusions. The beauty of the k-means algorithm is that it does the clustering on the data with an unlimited number of features. It is a great tool to use when you have a raw data and would like to know the patterns in that data.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec65"/>Dimensionality reduction with principal component analysis</h1></div></div></div><p>Dimensionality reduction<a id="id528" class="indexterm"/> is the process of reducing the number of dimensions or features. A lot of real data contains a very high number of features. It is not uncommon to have thousands of features. Now, we need to drill down to features that matter.</p><p>Dimensionality reduction<a id="id529" class="indexterm"/> serves several purposes such as:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Data compression</li><li class="listitem" style="list-style-type: disc">Visualization</li></ul></div><p>When the number of dimensions is reduced, it reduces the disk footprint and memory footprint. Last but not least; it helps algorithms to run much faster. It also helps reduce highly<a id="id530" class="indexterm"/> correlated dimensions to one.</p><p>Humans can only visualize three dimensions, but data can have a much higher number of dimensions. Visualization can help find hidden patterns in the data. Dimensionality reduction helps visualization by compacting multiple features into one.</p><p>The most <a id="id531" class="indexterm"/>popular algorithm for dimensionality reduction is <strong>principal component analysis</strong> (<strong>PCA</strong>).</p><p>Let's look at the following dataset:</p><div><img src="img/3056_09_08.jpg" alt="Dimensionality reduction with principal component analysis"/></div><p>Let's say the <a id="id532" class="indexterm"/>goal is to<a id="id533" class="indexterm"/> divide this two-dimensional data into one dimension. The way to do that would be to find a line on which we can project this data. Let's find a line that is good for projecting this data on:</p><div><img src="img/3056_09_09.jpg" alt="Dimensionality reduction with principal component analysis"/></div><p>This is <a id="id534" class="indexterm"/>the line that has<a id="id535" class="indexterm"/> the shortest projected distance from the data points. Let's explain it further by dropping the shortest lines from each data point to this projected line:</p><div><img src="img/3056_09_10.jpg" alt="Dimensionality reduction with principal component analysis"/></div><p>Another <a id="id536" class="indexterm"/>way to look at it is<a id="id537" class="indexterm"/> that we have to find a line to project the data on so that the sum of the square distances of the data points from this line is minimized. These <a id="id538" class="indexterm"/>gray line segments are also called <strong>projection errors</strong>.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec95"/>Getting ready</h2></div></div></div><p>Let's look at the three features of the housing data of the City of Saratoga, CA—that is, house size, lot size, and price. Using PCA, we will merge the house size and lot size features into one feature— <em>z</em>. Let's <a id="id539" class="indexterm"/>call this feature <strong>z density of a house</strong>.</p><p>It is worth noting that it is not always possible to give meaning to the new feature created. In this case, it is easy as we have only two features to combine and we can use our common sense to combine the effect of the two. In a more practical case, you may have 1,000 features that you are trying to project to 100 features. It may not be possible to give real-life meaning to each of those 100 features.</p><p>In this exercise, we will derive the housing density using PCA and then we will do linear regression to see how this density affects the house price.</p><p>There is a preprocessing stage before we <a id="id540" class="indexterm"/>delve into PCA: <strong>feature scaling</strong>. Feature scaling comes into the picture when two features have ranges that are at very different scales. Here, house size varies in the range of 800 sq ft to 7,000 sq ft, while the lot size varies between 800 sq ft to a few acres.</p><p>Why <a id="id541" class="indexterm"/>did we not have to do<a id="id542" class="indexterm"/> feature scaling before? The answer is that we really did not have to put features on a level playing field. Gradient descent is another area where feature scaling is very useful.</p><p>There are <a id="id543" class="indexterm"/>different ways of doing feature scaling:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">
Dividing a feature value with a maximum value that will put every feature in the <img src="img/3056_09_22.jpg" alt="Getting ready"/> range
</li><li class="listitem" style="list-style-type: disc">Dividing a feature value with the range, that is, maximum value - minimum value</li><li class="listitem" style="list-style-type: disc">Subtracting a feature value by its mean and then dividing by the range</li><li class="listitem" style="list-style-type: disc">Subtracting a feature value by its mean and then dividing by the standard deviation</li></ul></div><p>We are going to use the fourth choice to scale in the best way possible. The following is the data we are going to use for this recipe:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>House size</p>
</th><th style="text-align: left" valign="bottom">
<p>Lot size</p>
</th><th style="text-align: left" valign="bottom">
<p>Scaled house size</p>
</th><th style="text-align: left" valign="bottom">
<p>Scaled lot size</p>
</th><th style="text-align: left" valign="bottom">
<p>House price (in $1,000)</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>2524</p>
</td><td style="text-align: left" valign="top">
<p>12839</p>
</td><td style="text-align: left" valign="top">
<p>-0.025</p>
</td><td style="text-align: left" valign="top">
<p>-0.231</p>
</td><td style="text-align: left" valign="top">
<p>2405</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2937</p>
</td><td style="text-align: left" valign="top">
<p>10000</p>
</td><td style="text-align: left" valign="top">
<p>0.323</p>
</td><td style="text-align: left" valign="top">
<p>-0.4</p>
</td><td style="text-align: left" valign="top">
<p>2200</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1778</p>
</td><td style="text-align: left" valign="top">
<p>8040</p>
</td><td style="text-align: left" valign="top">
<p>-0.654</p>
</td><td style="text-align: left" valign="top">
<p>-0.517</p>
</td><td style="text-align: left" valign="top">
<p>1400</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1242</p>
</td><td style="text-align: left" valign="top">
<p>13104</p>
</td><td style="text-align: left" valign="top">
<p>-1.105</p>
</td><td style="text-align: left" valign="top">
<p>-0.215</p>
</td><td style="text-align: left" valign="top">
<p>1800</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2900</p>
</td><td style="text-align: left" valign="top">
<p>10000</p>
</td><td style="text-align: left" valign="top">
<p>0.291</p>
</td><td style="text-align: left" valign="top">
<p>-0.4</p>
</td><td style="text-align: left" valign="top">
<p>2351</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1218</p>
</td><td style="text-align: left" valign="top">
<p>3049</p>
</td><td style="text-align: left" valign="top">
<p>-1.126</p>
</td><td style="text-align: left" valign="top">
<p>-0.814</p>
</td><td style="text-align: left" valign="top">
<p>795</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2722</p>
</td><td style="text-align: left" valign="top">
<p>38768</p>
</td><td style="text-align: left" valign="top">
<p>0.142</p>
</td><td style="text-align: left" valign="top">
<p>1.312</p>
</td><td style="text-align: left" valign="top">
<p>2725</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2553</p>
</td><td style="text-align: left" valign="top">
<p>16250</p>
</td><td style="text-align: left" valign="top">
<p>-0.001</p>
</td><td style="text-align: left" valign="top">
<p>-0.028</p>
</td><td style="text-align: left" valign="top">
<p>2150</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3681</p>
</td><td style="text-align: left" valign="top">
<p>43026</p>
</td><td style="text-align: left" valign="top">
<p>0.949</p>
</td><td style="text-align: left" valign="top">
<p>1.566</p>
</td><td style="text-align: left" valign="top">
<p>2724</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3032</p>
</td><td style="text-align: left" valign="top">
<p>44431</p>
</td><td style="text-align: left" valign="top">
<p>0.403</p>
</td><td style="text-align: left" valign="top">
<p>1.649</p>
</td><td style="text-align: left" valign="top">
<p>2675</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3437</p>
</td><td style="text-align: left" valign="top">
<p>40000</p>
</td><td style="text-align: left" valign="top">
<p>0.744</p>
</td><td style="text-align: left" valign="top">
<p>1.385</p>
</td><td style="text-align: left" valign="top">
<p>2930</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1680</p>
</td><td style="text-align: left" valign="top">
<p>1260</p>
</td><td style="text-align: left" valign="top">
<p>-0.736</p>
</td><td style="text-align: left" valign="top">
<p>-0.92</p>
</td><td style="text-align: left" valign="top">
<p>870</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2260</p>
</td><td style="text-align: left" valign="top">
<p>15000</p>
</td><td style="text-align: left" valign="top">
<p>-0.248</p>
</td><td style="text-align: left" valign="top">
<p>-0.103</p>
</td><td style="text-align: left" valign="top">
<p>2210</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1660</p>
</td><td style="text-align: left" valign="top">
<p>10032</p>
</td><td style="text-align: left" valign="top">
<p>-0.753</p>
</td><td style="text-align: left" valign="top">
<p>-0.398</p>
</td><td style="text-align: left" valign="top">
<p>1145</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3251</p>
</td><td style="text-align: left" valign="top">
<p>12420</p>
</td><td style="text-align: left" valign="top">
<p>0.587</p>
</td><td style="text-align: left" valign="top">
<p>-0.256</p>
</td><td style="text-align: left" valign="top">
<p>2419</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3039</p>
</td><td style="text-align: left" valign="top">
<p>69696</p>
</td><td style="text-align: left" valign="top">
<p>0.409</p>
</td><td style="text-align: left" valign="top">
<p>3.153</p>
</td><td style="text-align: left" valign="top">
<p>2750</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3401</p>
</td><td style="text-align: left" valign="top">
<p>12600</p>
</td><td style="text-align: left" valign="top">
<p>0.714</p>
</td><td style="text-align: left" valign="top">
<p>-0.245</p>
</td><td style="text-align: left" valign="top">
<p>2035</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1620</p>
</td><td style="text-align: left" valign="top">
<p>10240</p>
</td><td style="text-align: left" valign="top">
<p>-0.787</p>
</td><td style="text-align: left" valign="top">
<p>-0.386</p>
</td><td style="text-align: left" valign="top">
<p>1150</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>876</p>
</td><td style="text-align: left" valign="top">
<p>876</p>
</td><td style="text-align: left" valign="top">
<p>-1.414</p>
</td><td style="text-align: left" valign="top">
<p>-0.943</p>
</td><td style="text-align: left" valign="top">
<p>665</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1889</p>
</td><td style="text-align: left" valign="top">
<p>8125</p>
</td><td style="text-align: left" valign="top">
<p>-0.56</p>
</td><td style="text-align: left" valign="top">
<p>-0.512</p>
</td><td style="text-align: left" valign="top">
<p>1430</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4406</p>
</td><td style="text-align: left" valign="top">
<p>11792</p>
</td><td style="text-align: left" valign="top">
<p>1.56</p>
</td><td style="text-align: left" valign="top">
<p>-0.294</p>
</td><td style="text-align: left" valign="top">
<p>1920</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1885</p>
</td><td style="text-align: left" valign="top">
<p>1512</p>
</td><td style="text-align: left" valign="top">
<p>-0.564</p>
</td><td style="text-align: left" valign="top">
<p>-0.905</p>
</td><td style="text-align: left" valign="top">
<p>1230</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1276</p>
</td><td style="text-align: left" valign="top">
<p>1276</p>
</td><td style="text-align: left" valign="top">
<p>-1.077</p>
</td><td style="text-align: left" valign="top">
<p>-0.92</p>
</td><td style="text-align: left" valign="top">
<p>975</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3053</p>
</td><td style="text-align: left" valign="top">
<p>67518</p>
</td><td style="text-align: left" valign="top">
<p>0.42</p>
</td><td style="text-align: left" valign="top">
<p>3.023</p>
</td><td style="text-align: left" valign="top">
<p>2400</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2323</p>
</td><td style="text-align: left" valign="top">
<p>9810</p>
</td><td style="text-align: left" valign="top">
<p>-0.195</p>
</td><td style="text-align: left" valign="top">
<p>-0.412</p>
</td><td style="text-align: left" valign="top">
<p>1725</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3139</p>
</td><td style="text-align: left" valign="top">
<p>6324</p>
</td><td style="text-align: left" valign="top">
<p>0.493</p>
</td><td style="text-align: left" valign="top">
<p>-0.619</p>
</td><td style="text-align: left" valign="top">
<p>2300</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2293</p>
</td><td style="text-align: left" valign="top">
<p>12510</p>
</td><td style="text-align: left" valign="top">
<p>-0.22</p>
</td><td style="text-align: left" valign="top">
<p>-0.251</p>
</td><td style="text-align: left" valign="top">
<p>1700</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2635</p>
</td><td style="text-align: left" valign="top">
<p>15616</p>
</td><td style="text-align: left" valign="top">
<p>0.068</p>
</td><td style="text-align: left" valign="top">
<p>-0.066</p>
</td><td style="text-align: left" valign="top">
<p>1915</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2298</p>
</td><td style="text-align: left" valign="top">
<p>15476</p>
</td><td style="text-align: left" valign="top">
<p>-0.216</p>
</td><td style="text-align: left" valign="top">
<p>-0.074</p>
</td><td style="text-align: left" valign="top">
<p>2278</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2656</p>
</td><td style="text-align: left" valign="top">
<p>13390</p>
</td><td style="text-align: left" valign="top">
<p>0.086</p>
</td><td style="text-align: left" valign="top">
<p>-0.198</p>
</td><td style="text-align: left" valign="top">
<p>2497.5</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1158</p>
</td><td style="text-align: left" valign="top">
<p>1158</p>
</td><td style="text-align: left" valign="top">
<p>-1.176</p>
</td><td style="text-align: left" valign="top">
<p>-0.927</p>
</td><td style="text-align: left" valign="top">
<p>725</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1511</p>
</td><td style="text-align: left" valign="top">
<p>2000</p>
</td><td style="text-align: left" valign="top">
<p>-0.879</p>
</td><td style="text-align: left" valign="top">
<p>-0.876</p>
</td><td style="text-align: left" valign="top">
<p>870</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1252</p>
</td><td style="text-align: left" valign="top">
<p>2614</p>
</td><td style="text-align: left" valign="top">
<p>-1.097</p>
</td><td style="text-align: left" valign="top">
<p>-0.84</p>
</td><td style="text-align: left" valign="top">
<p>730</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2141</p>
</td><td style="text-align: left" valign="top">
<p>13433</p>
</td><td style="text-align: left" valign="top">
<p>-0.348</p>
</td><td style="text-align: left" valign="top">
<p>-0.196</p>
</td><td style="text-align: left" valign="top">
<p>2050</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3565</p>
</td><td style="text-align: left" valign="top">
<p>12500</p>
</td><td style="text-align: left" valign="top">
<p>0.852</p>
</td><td style="text-align: left" valign="top">
<p>-0.251</p>
</td><td style="text-align: left" valign="top">
<p>3330</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1368</p>
</td><td style="text-align: left" valign="top">
<p>15750</p>
</td><td style="text-align: left" valign="top">
<p>-0.999</p>
</td><td style="text-align: left" valign="top">
<p>-0.058</p>
</td><td style="text-align: left" valign="top">
<p>1120</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>5726</p>
</td><td style="text-align: left" valign="top">
<p>13996</p>
</td><td style="text-align: left" valign="top">
<p>2.672</p>
</td><td style="text-align: left" valign="top">
<p>-0.162</p>
</td><td style="text-align: left" valign="top">
<p>4100</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2563</p>
</td><td style="text-align: left" valign="top">
<p>10450</p>
</td><td style="text-align: left" valign="top">
<p>0.008</p>
</td><td style="text-align: left" valign="top">
<p>-0.373</p>
</td><td style="text-align: left" valign="top">
<p>1655</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1551</p>
</td><td style="text-align: left" valign="top">
<p>7500</p>
</td><td style="text-align: left" valign="top">
<p>-0.845</p>
</td><td style="text-align: left" valign="top">
<p>-0.549</p>
</td><td style="text-align: left" valign="top">
<p>1550</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1993</p>
</td><td style="text-align: left" valign="top">
<p>12125</p>
</td><td style="text-align: left" valign="top">
<p>-0.473</p>
</td><td style="text-align: left" valign="top">
<p>-0.274</p>
</td><td style="text-align: left" valign="top">
<p>2100</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2555</p>
</td><td style="text-align: left" valign="top">
<p>14500</p>
</td><td style="text-align: left" valign="top">
<p>0.001</p>
</td><td style="text-align: left" valign="top">
<p>-0.132</p>
</td><td style="text-align: left" valign="top">
<p>2100</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1572</p>
</td><td style="text-align: left" valign="top">
<p>10000</p>
</td><td style="text-align: left" valign="top">
<p>-0.827</p>
</td><td style="text-align: left" valign="top">
<p>-0.4</p>
</td><td style="text-align: left" valign="top">
<p>1175</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2764</p>
</td><td style="text-align: left" valign="top">
<p>10019</p>
</td><td style="text-align: left" valign="top">
<p>0.177</p>
</td><td style="text-align: left" valign="top">
<p>-0.399</p>
</td><td style="text-align: left" valign="top">
<p>2047.5</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>7168</p>
</td><td style="text-align: left" valign="top">
<p>48787</p>
</td><td style="text-align: left" valign="top">
<p>3.887</p>
</td><td style="text-align: left" valign="top">
<p>1.909</p>
</td><td style="text-align: left" valign="top">
<p>3998</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4392</p>
</td><td style="text-align: left" valign="top">
<p>53579</p>
</td><td style="text-align: left" valign="top">
<p>1.548</p>
</td><td style="text-align: left" valign="top">
<p>2.194</p>
</td><td style="text-align: left" valign="top">
<p>2688</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3096</p>
</td><td style="text-align: left" valign="top">
<p>10788</p>
</td><td style="text-align: left" valign="top">
<p>0.457</p>
</td><td style="text-align: left" valign="top">
<p>-0.353</p>
</td><td style="text-align: left" valign="top">
<p>2251</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2003</p>
</td><td style="text-align: left" valign="top">
<p>11865</p>
</td><td style="text-align: left" valign="top">
<p>-0.464</p>
</td><td style="text-align: left" valign="top">
<p>-0.289</p>
</td><td style="text-align: left" valign="top">
<p>1906</p>
</td></tr></tbody></table></div><p>Let's take the scaled house size and scaled house price data and save it as <code class="literal">scaledhousedata.csv</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec96"/>How to do it…</h2></div></div></div><div><ol class="orderedlist arabic"><li class="listitem">Load <code class="literal">scaledhousedata.csv</code> to HDFS:<div><pre class="programlisting">
<strong>$ hdfs dfs -put scaledhousedata.csv scaledhousedata.csv</strong>
</pre></div></li><li class="listitem">Start <a id="id544" class="indexterm"/>the <a id="id545" class="indexterm"/>Spark shell:<div><pre class="programlisting">
<strong>$ spark-shell</strong>
</pre></div></li><li class="listitem">Import statistics and related classes:<div><pre class="programlisting">
<strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong>
<strong>scala&gt; import org.apache.spark.mllib.linalg.distributed.RowMatrix</strong>
</pre></div></li><li class="listitem">Load <code class="literal">saratoga.csv</code> as an RDD:<div><pre class="programlisting">
<strong>scala&gt; val data = sc.textFile("scaledhousedata.csv")</strong>
</pre></div></li><li class="listitem">Transform the data into an RDD of dense vectors:<div><pre class="programlisting">
<strong>scala&gt; val parsedData = data.map( line =&gt; Vectors.dense(line.split(',').map(_.toDouble)))</strong>
</pre></div></li><li class="listitem">Create a <code class="literal">RowMatrix</code> from <code class="literal">parsedData</code>:<div><pre class="programlisting">
<strong>scala&gt; val mat = new RowMatrix(parsedData)</strong>
</pre></div></li><li class="listitem">Compute one principal component:<div><pre class="programlisting">
<strong>scala&gt; val pc= mat.computePrincipalComponents(1)</strong>
</pre></div></li><li class="listitem">Project the rows to the linear space spanned by the principal component:<div><pre class="programlisting">
<strong>scala&gt; val projected = mat.multiply(pc)</strong>
</pre></div></li><li class="listitem">Convert the projected <code class="literal">RowMatrix</code> back to the RDD:<div><pre class="programlisting">
<strong>scala&gt; val projectedRDD = projected.rows</strong>
</pre></div></li><li class="listitem">Save <code class="literal">projectedRDD</code> back to HDFS:<div><pre class="programlisting">
<strong>scala&gt; projectedRDD.saveAsTextFile("phdata")</strong>
</pre></div></li></ol></div><p>Now we will use this projected feature, which we decided to call housing density, plot it against the house price, and see whether any new pattern emerges:</p><div><ol class="orderedlist arabic"><li class="listitem">Download the HDFS directory <code class="literal">phdata</code> to the local directory <code class="literal">phdata</code>:<div><pre class="programlisting">
<strong>scala&gt; hdfs dfs -get phdata phdata</strong>
</pre></div></li><li class="listitem">Trim start and end brackets in the data and load the data into MS Excel, next to the house price.</li></ol></div><p>The following is the plot of the house price versus the housing density:</p><div><img src="img/3056_09_11.jpg" alt="How to do it…"/></div><p>Let's <a id="id546" class="indexterm"/>draw some <a id="id547" class="indexterm"/>patterns in this data as follows:</p><div><img src="img/3056_09_12.jpg" alt="How to do it…"/></div><p>What <a id="id548" class="indexterm"/>patterns do<a id="id549" class="indexterm"/> we see here? For moving from a very high-density to low-density housing, people are ready to pay a heavy premium. As the housing density reduces, this premium flattens out. For example, people will pay a heavy premium to move from condominiums and town-homes to a single-family home, but the premium on a single- family home with a 3-acre lot size is not going to be much different from a single-family house with a 2-acre lot size in a comparable built-up area.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec66"/>Dimensionality reduction with singular value decomposition</h1></div></div></div><p>Often, the original dimensions do not represent data in the best way possible. As we saw <a id="id550" class="indexterm"/>in PCA, you can, sometimes, project the data to fewer dimensions and still retain most of the useful<a id="id551" class="indexterm"/> information.</p><p>Sometimes, the best approach is to align dimensions along the features that exhibit most of the variations. This approach helps to eliminate dimensions that are not representative of the data.</p><p>Let's look at the following figure again, which shows the best-fit line on two dimensions:</p><div><img src="img/3056_09_10.jpg" alt="Dimensionality reduction with singular value decomposition"/></div><p>The<a id="id552" class="indexterm"/> projection<a id="id553" class="indexterm"/> line shows the best approximation of the original data with one dimension. If we take the points where the gray line is intersecting with the black line and isolates the black line, we will have a reduced representation of the original data with as much variation retained as possible, as shown in the following figure:</p><div><img src="img/3056_09_13.jpg" alt="Dimensionality reduction with singular value decomposition"/></div><p>Let's draw a line perpendicular to the first projection line, as shown in the following figure:</p><div><img src="img/3056_09_14.jpg" alt="Dimensionality reduction with singular value decomposition"/></div><p>This line<a id="id554" class="indexterm"/> captures<a id="id555" class="indexterm"/> as much variation as possible along the second dimension of the original dataset. It does a bad job at approximating the original data as this dimension exhibits less variation to start with. It is possible to use these projection lines to generate a set of uncorrelated data points that will show subgroupings in the original data, not visible at first glance.</p><p>This is the basic idea behind SVD. Take a high dimension, a highly variable set of data points, and reduce it to a lower dimensional space that exposes the structure of the original data more clearly and orders it from the most variation to the least. What makes SVD very useful, especially for NLP application, is that you can simply ignore variation below a certain threshold to massively reduce the original data, making sure that the original relationship interests are retained.</p><p>Let's get slightly into the theory now. SVD is based on a theorem from linear algebra that a rectangular matrix A can be broken down into a product of three matrices—an orthogonal matrix U, a diagonal matrix S, and the transpose of an orthogonal matrix V. We can show it as follows:</p><div><img src="img/3056_09_15.jpg" alt="Dimensionality reduction with singular value decomposition"/></div><p><em>U</em> and <em>V</em> are orthogonal matrices:</p><div><img src="img/3056_09_16.jpg" alt="Dimensionality reduction with singular value decomposition"/></div><div><img src="img/3056_09_17.jpg" alt="Dimensionality reduction with singular value decomposition"/></div><p>The <a id="id556" class="indexterm"/>columns of <em>U</em> are <a id="id557" class="indexterm"/>orthonormal eigenvectors of <img src="img/3056_09_18.jpg" alt="Dimensionality reduction with singular value decomposition"/> and the columns of <em>V</em> are orthonormal eigenvectors of <img src="img/3056_09_19.jpg" alt="Dimensionality reduction with singular value decomposition"/>. <em>S</em> is a diagonal matrix containing the square roots of eigenvalues from <em>U</em> or <em>V</em> in descending order.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec97"/>Getting ready</h2></div></div></div><p>Let's look at an example of a term-document matrix. We are going to look at two new items about the US presidential elections. The following are the links to the two documents:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Fox</strong>: <a class="ulink" href="http://www.foxnews.com/politics/2015/03/08/top-2016-gop-presidential-hopefuls-return-to-iowa-to-hone-message-including/">http://www.foxnews.com/politics/2015/03/08/top-2016-gop-presidential-hopefuls-return-to-iowa-to-hone-message-including/</a></li><li class="listitem" style="list-style-type: disc"><strong>Npr</strong>: <a class="ulink" href="http://www.npr.org/blogs/itsallpolitics/2015/03/09/391704815/in-iowa-2016-has-begun-at-least-for-the-republican-party">http://www.npr.org/blogs/itsallpolitics/2015/03/09/391704815/in-iowa-2016-has-begun-at-least-for-the-republican-party</a></li></ul></div><p>Let's build the presidential candidate matrix out of these two news items:</p><div><img src="img/3056_09_20.jpg" alt="Getting ready"/></div><div><img src="img/3056_09_21.jpg" alt="Getting ready"/></div><p>Let's put<a id="id558" class="indexterm"/> this matrix in a<a id="id559" class="indexterm"/> CSV file and then put it in HDFS. We will apply SVD to this matrix and analyze the results.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec98"/>How to do it…</h2></div></div></div><div><ol class="orderedlist arabic"><li class="listitem">Load <code class="literal">scaledhousedata.csv</code> to HDFS:<div><pre class="programlisting">
<strong>$ hdfs dfs -put pres.csv scaledhousedata.csv</strong>
</pre></div></li><li class="listitem">Start the Spark shell:<div><pre class="programlisting">
<strong>$ spark-shell</strong>
</pre></div></li><li class="listitem">Import statistics and related classes:<div><pre class="programlisting">
<strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong>
<strong>scala&gt; import org.apache.spark.mllib.linalg.distributed.RowMatrix</strong>
</pre></div></li><li class="listitem">Load <code class="literal">pres.csv</code> as an RDD:<div><pre class="programlisting">
<strong>scala&gt; val data = sc.textFile("pres.csv")</strong>
</pre></div></li><li class="listitem">Transform data into an RDD of dense vectors:<div><pre class="programlisting">
<strong>scala&gt; val parsedData = data.map( line =&gt; Vectors.dense(line.split(',').map(_.toDouble)))</strong>
</pre></div></li><li class="listitem">Create a <code class="literal">RowMatrix</code> from <code class="literal">parsedData</code>:<div><pre class="programlisting">
<strong>scala&gt; val mat = new RowMatrix(parsedData)</strong>
</pre></div></li><li class="listitem">Compute <code class="literal">svd</code>:<div><pre class="programlisting">
<strong>scala&gt; val svd = mat.computeSVD(2,true)</strong>
</pre></div></li><li class="listitem">Calculate the <code class="literal">U</code> factor (eigenvector):<div><pre class="programlisting">
<strong>scala&gt; val U = svd.U</strong>
</pre></div></li><li class="listitem">Calculate the matrix of singular values (eigenvalues):<div><pre class="programlisting">
<strong>scala&gt; val s = svd.s</strong>
</pre></div></li><li class="listitem">Calculate the <code class="literal">V</code> factor (eigenvector):<div><pre class="programlisting">
<strong>scala&gt; val s = svd.s</strong>
</pre></div></li></ol></div><p>If you<a id="id560" class="indexterm"/> look at <code class="literal">s</code>, you <a id="id561" class="indexterm"/>will realize that it gave a much higher score to the Npr article than to the Fox article.</p></div></div></body></html>