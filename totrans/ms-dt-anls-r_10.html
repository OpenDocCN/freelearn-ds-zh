<html><head></head><body><div class="chapter" title="Chapter&#xA0;10.&#xA0;Classification and Clustering"><div class="titlepage"><div><div><h1 class="title"><a id="ch10"/>Chapter 10. Classification and Clustering</h1></div></div></div><p>In the previous chapter, we concentrated on how to compress information found in a number of continuous variables into a smaller set of numbers, but these statistical methods are somewhat limited when we are dealing with categorized data, for example when analyzing surveys.</p><p>Although some methods try to convert discrete variables into numeric ones, such as by using a number of dummy or indicator variables, in most cases it's simply better to think about our research design goals instead of trying to forcibly use previously learned methods in the analysis.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note65"/>Note</h3><p>We can replace a categorical <a class="indexterm" id="id721"/>variable with a number of dummy variables by creating a new variable for each label of the original discrete variable, and then assign <span class="emphasis"><em>1</em></span> to the related column and <span class="emphasis"><em>0</em></span> to all the others. Such values can be used as numeric variables in statistical analysis, especially with regression models.</p></div></div><p>When we analyze a sample and target population via categorical variables, usually we are not interested in individual cases, but instead in similar elements and groups. Similar elements can be defined as rows in a dataset with similar values in the columns.</p><p>In this chapter, we will discuss <a class="indexterm" id="id722"/>different <span class="emphasis"><em>supervised</em></span> and <span class="emphasis"><em>unsupervised</em></span> ways to <a class="indexterm" id="id723"/>identify similar cases in a dataset, such as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Hierarchical clustering</li><li class="listitem" style="list-style-type: disc">K-means clustering</li><li class="listitem" style="list-style-type: disc">Some machine learning algorithms</li><li class="listitem" style="list-style-type: disc">Latent class model</li><li class="listitem" style="list-style-type: disc">Discriminant analysis</li><li class="listitem" style="list-style-type: disc">Logistic regression</li></ul></div><div class="section" title="Cluster analysis"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec68"/>Cluster analysis</h1></div></div></div><p>
<span class="strong"><strong>Clustering</strong></span>
<a class="indexterm" id="id724"/> is an <a class="indexterm" id="id725"/>unsupervised data analysis method that is used in diverse fields, such as pattern recognition, social sciences, and pharmacy. The aim of cluster analysis is to make homogeneous subgroups called clusters, where the objects in the same cluster are similar, and the clusters differ from each other.</p><div class="section" title="Hierarchical clustering"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec63"/>Hierarchical clustering</h2></div></div></div><p>Cluster analysis is one<a class="indexterm" id="id726"/> of the most well known and popular pattern recognition methods; thus, there are many clustering models and algorithms analyzing the distribution, density, possible center points, and so on in the dataset. In this section we are going to examine some hierarchical clustering methods.</p><p>
<span class="strong"><strong>Hierarchical clustering</strong></span>
<a class="indexterm" id="id727"/> can be either agglomerative or divisive. In agglomerative methods every case starts out as an individual cluster, then the closest clusters are merged together in an iterative manner, until finally they merge into one single cluster, which includes all elements of the original dataset. The biggest problem with this approach is that distances between clusters have to be recalculated at each iteration, which makes it extremely slow on large data. I'd rather not suggest trying to run the following commands on the <code class="literal">hflights</code> dataset.</p><p>Divisive methods on the other hand take a top-down approach. They start from a single cluster, which is then iteratively divided into smaller groups until they are all singletons.</p><p>The <code class="literal">stats</code> package<a class="indexterm" id="id728"/> contains the <code class="literal">hclust</code> function<a class="indexterm" id="id729"/> for hierarchical clustering that takes a distance matrix as an input. To see how it works, let's use the <code class="literal">mtcars</code> dataset that we already analyzed in <a class="link" href="ch03.html" title="Chapter 3. Filtering and Summarizing Data">Chapter 3</a>, <span class="emphasis"><em>Filtering and Summarizing Data</em></span> and <a class="link" href="ch09.html" title="Chapter 9. From Big to Small Data">Chapter 9</a>, <span class="emphasis"><em>From Big to Smaller Data</em></span>. The <code class="literal">dist</code> function<a class="indexterm" id="id730"/> is also familiar from the latter chapter:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; d &lt;- dist(mtcars)</strong></span>
<span class="strong"><strong>&gt; h &lt;- hclust(d)</strong></span>
<span class="strong"><strong>&gt; h</strong></span>

<span class="strong"><strong>Call:</strong></span>
<span class="strong"><strong>hclust(d = d)</strong></span>

<span class="strong"><strong>Cluster method   : complete </strong></span>
<span class="strong"><strong>Distance         : euclidean </strong></span>
<span class="strong"><strong>Number of objects: 32</strong></span>
</pre></div><p>Well, this is a way too brief output and only shows that our distance matrix included 32 elements and the clustering method. A visual representation of the results will be a lot more useful for<a class="indexterm" id="id731"/> such a small dataset:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; plot(h)</strong></span>
</pre></div><div class="mediaobject"><img alt="Hierarchical clustering" src="graphics/2028OS_10_01.jpg"/></div><p>By plotting this <code class="literal">hclust</code> object, we<a class="indexterm" id="id732"/> obtained a <span class="emphasis"><em>dendrogram</em></span>, which <a class="indexterm" id="id733"/>shows how the clusters are formed. It can be useful for determining the number of clusters, although in datasets with numerous cases it becomes difficult to interpret. A horizontal line can be drawn to any given height on the <span class="emphasis"><em>y</em></span> axis so that the <span class="emphasis"><em>n</em></span> number of intersections with the line provides a n-cluster solution.</p><p>R can provide very convenient ways of visualizing the clusters on the <span class="emphasis"><em>dendrogram</em></span>. In the following plot, the red boxes show the cluster membership of a three-cluster solution on top of the previous plot:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; plot(h)</strong></span>
<span class="strong"><strong>&gt; rect.hclust(h, k=3, border = "red")</strong></span>
</pre></div><div class="mediaobject"><img alt="Hierarchical clustering" src="graphics/2028OS_10_02.jpg"/></div><p>Although this <a class="indexterm" id="id734"/>graph looks nice and it is extremely useful to have<a class="indexterm" id="id735"/> similar elements grouped together, for bigger datasets, it becomes hard to see through. Instead, we might be rather interested in the actual cluster membership represented in a vector:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; (cn &lt;- cutree(h, k = 3))</strong></span>
<span class="strong"><strong>          Mazda RX4       Mazda RX4 Wag          Datsun 710 </strong></span>
<span class="strong"><strong>                  1                   1                   1 </strong></span>
<span class="strong"><strong>     Hornet 4 Drive   Hornet Sportabout             Valiant </strong></span>
<span class="strong"><strong>                  2                   3                   2 </strong></span>
<span class="strong"><strong>         Duster 360           Merc 240D            Merc 230 </strong></span>
<span class="strong"><strong>                  3                   1                   1 </strong></span>
<span class="strong"><strong>           Merc 280           Merc 280C          Merc 450SE </strong></span>
<span class="strong"><strong>                  1                   1                   2 </strong></span>
<span class="strong"><strong>         Merc 450SL         Merc 450SLC  Cadillac Fleetwood </strong></span>
<span class="strong"><strong>                  2                   2                   3 </strong></span>
<span class="strong"><strong>Lincoln Continental   Chrysler Imperial            Fiat 128 </strong></span>
<span class="strong"><strong>                  3                   3                   1 </strong></span>
<span class="strong"><strong>        Honda Civic      Toyota Corolla       Toyota Corona </strong></span>
<span class="strong"><strong>                  1                   1                   1 </strong></span>
<span class="strong"><strong>   Dodge Challenger         AMC Javelin          Camaro Z28 </strong></span>
<span class="strong"><strong>                  2                   2                   3 </strong></span>
<span class="strong"><strong>   Pontiac Firebird           Fiat X1-9       Porsche 914-2 </strong></span>
<span class="strong"><strong>                  3                   1                   1 </strong></span>
<span class="strong"><strong>       Lotus Europa      Ford Pantera L        Ferrari Dino </strong></span>
<span class="strong"><strong>                  1                   3                   1 </strong></span>
<span class="strong"><strong>      Maserati Bora          Volvo 142E </strong></span>
<span class="strong"><strong>                  3                   1</strong></span>
</pre></div><p>And the number of elements in the resulting clusters as a frequency table:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; table(cn)</strong></span>
<span class="strong"><strong> 1  2  3 </strong></span>
<span class="strong"><strong>16  7  9</strong></span>
</pre></div><p>It seems that <span class="emphasis"><em>Cluster 1</em></span>, the third cluster on the preceding plot, has the most elements. Can you guess how<a class="indexterm" id="id736"/> this group differs from the other two clusters? Well, those <a class="indexterm" id="id737"/>readers who are familiar with car names might be able to guess the answer, but let's see what the numbers actually show:</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note66"/>Note</h3><p>Please note that we use the <code class="literal">round</code> function<a class="indexterm" id="id738"/> in the following examples to suppress the number of decimal places to 1 or 4 in the code output to fit the page width.</p></div></div><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; round(aggregate(mtcars, FUN = mean, by = list(cn)), 1)</strong></span>
<span class="strong"><strong>  Group.1  mpg cyl  disp    hp drat  wt qsec  vs  am gear carb</strong></span>
<span class="strong"><strong>1       1 24.5 4.6 122.3  96.9  4.0 2.5 18.5 0.8 0.7  4.1  2.4</strong></span>
<span class="strong"><strong>2       2 17.0 7.4 276.1 150.7  3.0 3.6 18.1 0.3 0.0  3.0  2.1</strong></span>
<span class="strong"><strong>3       3 14.6 8.0 388.2 232.1  3.3 4.2 16.4 0.0 0.2  3.4  4.0</strong></span>
</pre></div><p>There's a really spectacular difference in the average performance and gas consumption between the clusters! What about the standard deviation inside the groups?</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; round(aggregate(mtcars, FUN = sd, by = list(cn)), 1)</strong></span>
<span class="strong"><strong>  Group.1 mpg cyl disp   hp drat  wt qsec  vs  am gear carb</strong></span>
<span class="strong"><strong>1       1 5.0   1 34.6 31.0  0.3 0.6  1.8 0.4 0.5  0.5  1.5</strong></span>
<span class="strong"><strong>2       2 2.2   1 30.2 32.5  0.2 0.3  1.2 0.5 0.0  0.0  0.9</strong></span>
<span class="strong"><strong>3       3 3.1   0 58.1 49.4  0.4 0.9  1.3 0.0 0.4  0.9  1.7</strong></span>
</pre></div><p>These values are pretty low compared to the standard deviations in the original dataset:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; round(sapply(mtcars, sd), 1)</strong></span>
<span class="strong"><strong>  mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb </strong></span>
<span class="strong"><strong>  6.0   1.8 123.9  68.6   0.5   1.0   1.8   0.5   0.5   0.7   1.6</strong></span>
</pre></div><p>And the same applies when compared to the standard deviation between the groups as well:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; round(apply(</strong></span>
<span class="strong"><strong>+   aggregate(mtcars, FUN = mean, by = list(cn)),</strong></span>
<span class="strong"><strong>+   2, sd), 1)</strong></span>
<span class="strong"><strong>Group.1     mpg     cyl    disp      hp    drat      wt    qsec </strong></span>
<span class="strong"><strong>    1.0     5.1     1.8   133.5    68.1     0.5     0.8     1.1 </strong></span>
<span class="strong"><strong>     vs      am    gear    carb </strong></span>
<span class="strong"><strong>    0.4     0.4     0.6     1.0</strong></span>
</pre></div><p>This means<a class="indexterm" id="id739"/> that we achieved our original goal to identify similar <a class="indexterm" id="id740"/>elements of our data and organize those in groups that differ from each other. But why did we split the original data into exactly three artificially defined groups? Why not two, four, or even more?</p></div><div class="section" title="Determining the ideal number of clusters"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec64"/>Determining the ideal number of clusters</h2></div></div></div><p>The <code class="literal">NbClust</code> package<a class="indexterm" id="id741"/> offers<a class="indexterm" id="id742"/> a very convenient way to do some exploratory data analysis on our data before running the actual cluster analysis. The main function of the package can compute 30 different indices, all designed to determine the ideal number of groups. These include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Single link</li><li class="listitem" style="list-style-type: disc">Average</li><li class="listitem" style="list-style-type: disc">Complete link</li><li class="listitem" style="list-style-type: disc">McQuitty</li><li class="listitem" style="list-style-type: disc">Centroid (cluster center)</li><li class="listitem" style="list-style-type: disc">Median</li><li class="listitem" style="list-style-type: disc">K-means</li><li class="listitem" style="list-style-type: disc">Ward</li></ul></div><p>After loading the package, let's start with a visual method representing the possible number of clusters in our data—on a knee plot, which might be familiar from <a class="link" href="ch09.html" title="Chapter 9. From Big to Small Data">Chapter 9</a>, <span class="emphasis"><em>From Big to Smaller Data</em></span>, where you can also find some more information about the following elbow-rule:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(NbClust)</strong></span>
<span class="strong"><strong>&gt; NbClust(mtcars, method = 'complete', index = 'dindex')</strong></span>
</pre></div><div class="mediaobject"><img alt="Determining the ideal number of clusters" src="graphics/2028OS_10_03.jpg"/></div><p>In the preceding <a class="indexterm" id="id743"/>plots, we traditionally look for the <span class="emphasis"><em>elbow</em></span>, but the second differences plot on the right might be more straightforward for most readers. There we are interested in where the most significant peak can be found, which suggests that choosing three groups would be ideal when clustering the <code class="literal">mtcars</code> dataset.</p><p>Unfortunately, running all <code class="literal">NbClust</code> methods fails on such a small dataset. Thus, for demonstrational purposes, we are now running only a few standard methods and filtering the results for the suggested number of clusters via the related list element:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; NbClust(mtcars, method = 'complete', index = 'hartigan')$Best.nc</strong></span>
<span class="strong"><strong>All 32 observations were used. </strong></span>
 
<span class="strong"><strong>Number_clusters     Value_Index </strong></span>
<span class="strong"><strong>         3.0000         34.1696 </strong></span>
<span class="strong"><strong>&gt; NbClust(mtcars, method = 'complete', index = 'kl')$Best.nc</strong></span>
<span class="strong"><strong>All 32 observations were used. </strong></span>
 
<span class="strong"><strong>Number_clusters     Value_Index </strong></span>
<span class="strong"><strong>         3.0000          6.8235</strong></span>
</pre></div><p>Both the Hartigan and Krzanowski-Lai indexes suggest sticking to three clusters. Let's view the<a class="indexterm" id="id744"/> <code class="literal">iris</code> dataset as well, which includes a lot more cases with fewer numeric columns, so we can run all available methods:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; NbClust(iris[, -5], method = 'complete', index = 'all')$Best.nc[1,]</strong></span>
<span class="strong"><strong>All 150 observations were used. </strong></span>
 
<span class="strong"><strong>******************************************************************* </strong></span>
<span class="strong"><strong>* Among all indices:                                                </strong></span>
<span class="strong"><strong>* 2 proposed 2 as the best number of clusters </strong></span>
<span class="strong"><strong>* 13 proposed 3 as the best number of clusters </strong></span>
<span class="strong"><strong>* 5 proposed 4 as the best number of clusters </strong></span>
<span class="strong"><strong>* 1 proposed 6 as the best number of clusters </strong></span>
<span class="strong"><strong>* 2 proposed 15 as the best number of clusters </strong></span>

<span class="strong"><strong>                   ***** Conclusion *****                            </strong></span>
 
<span class="strong"><strong>* According to the majority rule, the best number of clusters is  3 </strong></span>
 
<span class="strong"><strong> ******************************************************************* </strong></span>
<span class="strong"><strong>        KL         CH   Hartigan        CCC      Scott    Marriot </strong></span>
<span class="strong"><strong>         4          4          3          3          3          3 </strong></span>
<span class="strong"><strong>    TrCovW     TraceW   Friedman      Rubin     Cindex         DB </strong></span>
<span class="strong"><strong>         3          3          4          6          3          3 </strong></span>
<span class="strong"><strong>Silhouette       Duda   PseudoT2      Beale  Ratkowsky       Ball </strong></span>
<span class="strong"><strong>         2          4          4          3          3          3 </strong></span>
<span class="strong"><strong>PtBiserial       Frey    McClain       Dunn     Hubert    SDindex </strong></span>
<span class="strong"><strong>         3          1          2         15          0          3 </strong></span>
<span class="strong"><strong>    Dindex       SDbw </strong></span>
<span class="strong"><strong>         0         15</strong></span>
</pre></div><p>The output summarizes that the ideal number of clusters is three based on the 13 methods returning that number, five further methods suggest four clusters, and a few other cluster numbers were also computed by a much smaller number of methods.</p><p>These methods are not only useful with the previously discussed hierarchical clustering, but generally used with k-means clustering as well, where the number of clusters is to be defined before running the analysis—unlike the hierarchical method, where we cut the dendogram <a class="indexterm" id="id745"/>after the heavy computations have already been run.</p></div><div class="section" title="K-means clustering"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec65"/>K-means clustering</h2></div></div></div><p>
<span class="strong"><strong>K-means clustering</strong></span>
<a class="indexterm" id="id746"/> is a non-hierarchical method first described by MacQueen in 1967. Its big <a class="indexterm" id="id747"/>advantage over hierarchical clustering is its great performance.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note67"/>Note</h3><p>Unlike hierarchical cluster analysis, k-means clustering requires you to determine the number of clusters before running the actual analysis.</p></div></div><p>The algorithm runs the following steps in a nutshell:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Initialize a predefined (<span class="emphasis"><em>k</em></span>) number of randomly chosen centroids in space.</li><li class="listitem">Assign each object to the cluster with the closest centroid.</li><li class="listitem">Recalculate centroids.</li><li class="listitem">Repeat the second and third steps until convergence.</li></ol></div><p>We are going to use the <code class="literal">kmeans</code> function<a class="indexterm" id="id748"/> from the<a class="indexterm" id="id749"/> <code class="literal">stats</code> package. As k-means clustering requires a prior decision on the number of clusters, we can either use the <code class="literal">NbClust</code> function described previously, or we can come up with an arbitrary number that fits the goals of the analysis.</p><p>According to the previously defined optimal cluster number in the previous section, we are going to stick to three groups, where the within-cluster sum of squares ceases to drop significantly:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; (k &lt;- kmeans(mtcars, 3))</strong></span>
<span class="strong"><strong>K-means clustering with 3 clusters of sizes 16, 7, 9</strong></span>

<span class="strong"><strong>Cluster means:</strong></span>
<span class="strong"><strong>       mpg      cyl     disp       hp     drat       wt     qsec</strong></span>
<span class="strong"><strong>1 24.50000 4.625000 122.2937  96.8750 4.002500 2.518000 18.54312</strong></span>
<span class="strong"><strong>2 17.01429 7.428571 276.0571 150.7143 2.994286 3.601429 18.11857</strong></span>
<span class="strong"><strong>3 14.64444 8.000000 388.2222 232.1111 3.343333 4.161556 16.40444</strong></span>
<span class="strong"><strong>         vs        am     gear     carb</strong></span>
<span class="strong"><strong>1 0.7500000 0.6875000 4.125000 2.437500</strong></span>
<span class="strong"><strong>2 0.2857143 0.0000000 3.000000 2.142857</strong></span>
<span class="strong"><strong>3 0.0000000 0.2222222 3.444444 4.000000</strong></span>

<span class="strong"><strong>Clustering vector:</strong></span>
<span class="strong"><strong>          Mazda RX4       Mazda RX4 Wag          Datsun 710 </strong></span>
<span class="strong"><strong>                  1                   1                   1 </strong></span>
<span class="strong"><strong>     Hornet 4 Drive   Hornet Sportabout             Valiant </strong></span>
<span class="strong"><strong>                  2                   3                   2 </strong></span>
<span class="strong"><strong>         Duster 360           Merc 240D            Merc 230 </strong></span>
<span class="strong"><strong>                  3                   1                   1 </strong></span>
<span class="strong"><strong>           Merc 280           Merc 280C          Merc 450SE </strong></span>
<span class="strong"><strong>                  1                   1                   2 </strong></span>
<span class="strong"><strong>         Merc 450SL         Merc 450SLC  Cadillac Fleetwood </strong></span>
<span class="strong"><strong>                  2                   2                   3 </strong></span>
<span class="strong"><strong>Lincoln Continental   Chrysler Imperial            Fiat 128 </strong></span>
<span class="strong"><strong>                  3                   3                   1 </strong></span>
<span class="strong"><strong>        Honda Civic      Toyota Corolla       Toyota Corona </strong></span>
<span class="strong"><strong>                  1                   1                   1 </strong></span>
<span class="strong"><strong>   Dodge Challenger         AMC Javelin          Camaro Z28 </strong></span>
<span class="strong"><strong>                  2                   2                   3 </strong></span>
<span class="strong"><strong>   Pontiac Firebird           Fiat X1-9       Porsche 914-2 </strong></span>
<span class="strong"><strong>                  3                   1                   1 </strong></span>
<span class="strong"><strong>       Lotus Europa      Ford Pantera L        Ferrari Dino </strong></span>
<span class="strong"><strong>                  1                   3                   1 </strong></span>
<span class="strong"><strong>      Maserati Bora          Volvo 142E </strong></span>
<span class="strong"><strong>                  3                   1 </strong></span>

<span class="strong"><strong>Within cluster sum of squares by cluster:</strong></span>
<span class="strong"><strong>[1] 32838.00 11846.09 46659.32</strong></span>
<span class="strong"><strong> (between_SS / total_SS =  85.3 %)</strong></span>

<span class="strong"><strong>Available components:</strong></span>

<span class="strong"><strong>[1] "cluster"      "centers"      "totss"        "withinss"    </strong></span>
<span class="strong"><strong>[5] "tot.withinss" "betweenss"    "size"         "iter"        </strong></span>
<span class="strong"><strong>[9] "ifault"      </strong></span>
</pre></div><p>The cluster means show some really important characteristics for each cluster, which we generated manually for the hierarchical clusters in the previous section. We can see that, in the first cluster, the cars have high mpg (low gas consumption), on average four cylinders (in contrast to six or eight), rather low performance and so on. The output also automatically reveals the actual cluster numbers.</p><p>Let's compare <a class="indexterm" id="id750"/>these to the clusters defined by the hierarchical <a class="indexterm" id="id751"/>method:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; all(cn == k$cluster)</strong></span>
<span class="strong"><strong>[1] TRUE</strong></span>
</pre></div><p>The results seem to be pretty stable, right?</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip14"/>Tip</h3><p>The cluster numbers have no meaning and their order is arbitrary. In other words, the cluster membership is a nominal variable. Based on this, the preceding R command might return <code class="literal">FALSE</code> instead of <code class="literal">TRUE</code> when the cluster numbers were allocated in a different order, but comparing the actual cluster membership will verify that we have found the very same groups. See for example <code class="literal">cbind(cn, k$cluster)</code> to generate a table including both cluster memberships.</p></div></div></div><div class="section" title="Visualizing clusters"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec66"/>Visualizing clusters</h2></div></div></div><p>Plotting these clusters is also a<a class="indexterm" id="id752"/> great way to understand groupings. To this end, we will use the <code class="literal">clusplot</code> function<a class="indexterm" id="id753"/> from the<a class="indexterm" id="id754"/> <code class="literal">cluster</code> package. For easier understanding, this function reduces the number of dimensions to two, in a similar way to when we are conducting a PCA or MDS (described in <a class="link" href="ch09.html" title="Chapter 9. From Big to Small Data">Chapter 9</a>, <span class="emphasis"><em>From Big to Smaller Data</em></span>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(cluster) </strong></span>
<span class="strong"><strong>&gt; clusplot(mtcars, k$cluster, color = TRUE, shade = TRUE, labels = 2)</strong></span>
</pre></div><div class="mediaobject"><img alt="Visualizing clusters" src="graphics/2028OS_10_04.jpg"/></div><p>As you can see, after the <a class="indexterm" id="id755"/>dimension reduction, the two components explain 84.17 percent of variance, so this small information loss is a great trade-off in favor of an easier understanding of the clusters.</p><p>Visualizing the relative density of the ellipses with the <code class="literal">shade</code> parameter can also help us realize how similar the elements of the same groups are. And we used the labels argument to show both the <a class="indexterm" id="id756"/>points and cluster labels as well. Be sure to stick to the default of <span class="emphasis"><em>0</em></span> (no labels) or <span class="emphasis"><em>4</em></span> (only ellipse labels) when visualizing large number of elements.</p></div></div></div>
<div class="section" title="Latent class models"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec69"/>Latent class models</h1></div></div></div><p>
<span class="strong"><strong>Latent Class Analysis</strong></span> (<span class="strong"><strong>LCA</strong></span>)<a class="indexterm" id="id757"/> is a method for identifying latent variables among<a class="indexterm" id="id758"/> polychromous outcome variables. It is similar to factor analysis, but can be used with discrete/categorical data. To this end, LCA is mostly used when analyzing surveys.</p><p>In this section, we are going to use the <code class="literal">poLCA</code> function from the<a class="indexterm" id="id759"/> <code class="literal">poLCA</code> package. It uses expectation-maximization and Newton-Raphson algorithms for finding the maximum likelihood for the parameters.</p><p>The <code class="literal">poLCA</code> function requires the data to be coded as integers starting from one or as a factor, otherwise it will produce an error message. To this end, let's transform some of the variables in the <code class="literal">mtcars</code> dataset to factors:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; factors &lt;- c('cyl', 'vs', 'am', 'carb', 'gear')</strong></span>
<span class="strong"><strong>&gt; mtcars[, factors] &lt;- lapply(mtcars[, factors], factor)</strong></span>
</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip15"/>Tip</h3><p>The preceding command will overwrite the <code class="literal">mtcars</code> dataset in your current R session. To revert to the original dataset for other examples, please delete this updated dataset from the session by <code class="literal">rm(mtcars)</code> if needed.</p></div></div><div class="section" title="Latent Class Analysis"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec67"/>Latent Class Analysis</h2></div></div></div><p>Now that the <a class="indexterm" id="id760"/>data is in an appropriate format, we can conduct the LCA. The related function comes with a number of important arguments:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">First, we have to define a formula that describes the model. Depending on the formula, we can define LCA (similar to clustering but with discrete variables) or<a class="indexterm" id="id761"/> <span class="strong"><strong>Latent Class Regression</strong></span> (<span class="strong"><strong>LCR</strong></span>) model.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">nclass</code> argument specifies the number of latent classes assumed in the model, which is 2 by default. Based on the previous examples in this chapter, we will override this to 3.</li><li class="listitem" style="list-style-type: disc">We can use the <code class="literal">maxiter</code>, <code class="literal">tol</code>, <code class="literal">probs.start</code>, and <code class="literal">nrep</code> parameters to fine-tune the model.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">graphs</code> argument can display or suppress the parameter estimates.</li></ul></div><p>Let's start with basic <a class="indexterm" id="id762"/>LCA of three latent classes <a class="indexterm" id="id763"/>defined by all the available discrete variables:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(poLCA)</strong></span>
<span class="strong"><strong>&gt; p &lt;- poLCA(cbind(cyl, vs, am, carb, gear) ~ 1,</strong></span>
<span class="strong"><strong>+   data = mtcars, graphs = TRUE, nclass = 3)</strong></span>
</pre></div><p>The first part of the output (which can be also accessed via the <code class="literal">probs</code> element of the preceding saved <code class="literal">poLCA</code> list) summarizes the probabilities of the outcome variables by each latent class:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; p$probs</strong></span>
<span class="strong"><strong>Conditional item response (column) probabilities,</strong></span>
<span class="strong"><strong> by outcome variable, for each class (row) </strong></span>
 
<span class="strong"><strong>$cyl</strong></span>
<span class="strong"><strong>               4      6 8</strong></span>
<span class="strong"><strong>class 1:  0.3333 0.6667 0</strong></span>
<span class="strong"><strong>class 2:  0.6667 0.3333 0</strong></span>
<span class="strong"><strong>class 3:  0.0000 0.0000 1</strong></span>

<span class="strong"><strong>$vs</strong></span>
<span class="strong"><strong>               0      1</strong></span>
<span class="strong"><strong>class 1:  0.0000 1.0000</strong></span>
<span class="strong"><strong>class 2:  0.2667 0.7333</strong></span>
<span class="strong"><strong>class 3:  1.0000 0.0000</strong></span>

<span class="strong"><strong>$am</strong></span>
<span class="strong"><strong>               0      1</strong></span>
<span class="strong"><strong>class 1:  1.0000 0.0000</strong></span>
<span class="strong"><strong>class 2:  0.2667 0.7333</strong></span>
<span class="strong"><strong>class 3:  0.8571 0.1429</strong></span>

<span class="strong"><strong>$carb</strong></span>
<span class="strong"><strong>               1      2      3      4      6      8</strong></span>
<span class="strong"><strong>class 1:  1.0000 0.0000 0.0000 0.0000 0.0000 0.0000</strong></span>
<span class="strong"><strong>class 2:  0.2667 0.4000 0.0000 0.2667 0.0667 0.0000</strong></span>
<span class="strong"><strong>class 3:  0.0000 0.2857 0.2143 0.4286 0.0000 0.0714</strong></span>
<span class="strong"><strong>$gear</strong></span>
<span class="strong"><strong>               3   4      5</strong></span>
<span class="strong"><strong>class 1:  1.0000 0.0 0.0000</strong></span>
<span class="strong"><strong>class 2:  0.0000 0.8 0.2000</strong></span>
<span class="strong"><strong>class 3:  0.8571 0.0 0.1429</strong></span>
</pre></div><p>From these probabilities, <a class="indexterm" id="id764"/>we can see that all 8-cylinder cars <a class="indexterm" id="id765"/>belong to the third class, the first one only includes cars with automatic transmission, one carburetor, three gears, and so on. The exact same values can be plotted as well by setting the graph parameter to <code class="literal">TRUE</code> in the function call, or by calling the plot function directly afterwards:</p><div class="mediaobject"><img alt="Latent Class Analysis" src="graphics/2028OS_10_05.jpg"/></div><p>The plot is also useful in highlighting that the first latent class includes only a few elements compared to the other classes (also known as "Estimated class population shares"):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; p$P</strong></span>
<span class="strong"><strong>[1] 0.09375 0.46875 0.43750</strong></span>
</pre></div><p>The <code class="literal">poLCA</code> object can also reveal a bunch of other important information about the results. Just to name a few, let's see the named list parts of the object, which can be extracted via the <a class="indexterm" id="id766"/>standard <code class="literal">$</code> operator:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">predclass</code> returns <a class="indexterm" id="id767"/>the most likely class memberships</li><li class="listitem" style="list-style-type: disc">On the other hand, the posterior element is a matrix containing the class membership probabilities of each case</li><li class="listitem" style="list-style-type: disc">The <a class="indexterm" id="id768"/><span class="strong"><strong>Akaike Information Criterion</strong></span> (<code class="literal">aic</code>), <a class="indexterm" id="id769"/><span class="strong"><strong>Bayesian Information Criterion</strong></span> (<code class="literal">bic</code>), <a class="indexterm" id="id770"/><span class="strong"><strong>deviance</strong></span> (<code class="literal">Gsq</code>), and <code class="literal">Chisq</code> values<a class="indexterm" id="id771"/> represent different measures of goodness of fit</li></ul></div></div><div class="section" title="LCR models"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec68"/>LCR models</h2></div></div></div><p>On the other hand, the LCR model<a class="indexterm" id="id772"/> is a supervised method, where we are not mainly interested in <a class="indexterm" id="id773"/>the latent variables explaining our observations at the exploratory data analysis scale, but instead we are using training data from which one or more covariates predict the probability of the latent class membership.</p></div></div>
<div class="section" title="Discriminant analysis"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec70"/>Discriminant analysis</h1></div></div></div><p>
<span class="strong"><strong>Discriminant Function Analysis</strong></span> (<span class="strong"><strong>DA</strong></span>)<a class="indexterm" id="id774"/> refers to the process of <a class="indexterm" id="id775"/>determining which continuous independent (predictor) variables discriminate between a discrete dependent (response) variable's categories, which can be considered as a reversed <a class="indexterm" id="id776"/>
<span class="strong"><strong>Multivariate Analysis of Variance</strong></span> (<span class="strong"><strong>MANOVA</strong></span>).</p><p>This suggests that DA is very similar to logistic regression (see <a class="link" href="ch06.html" title="Chapter 6. Beyond the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)">Chapter 6</a>, <span class="emphasis"><em>Beyond the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)</em></span> and the following section), which is more generally used because of its flexibility. While logistic regression can handle both categorical and continuous data, DA requires numeric independent variables and has a few further requirements that logistic regression does <a class="indexterm" id="id777"/>not have:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Normal distribution is assumed</li><li class="listitem" style="list-style-type: disc">Outliers should be eliminated</li><li class="listitem" style="list-style-type: disc">No two variables should be highly correlated (multi-collinearity)</li><li class="listitem" style="list-style-type: disc">The sample size of the smallest category should be higher than the number of predictor values</li><li class="listitem" style="list-style-type: disc">The number of independent variables should not exceed the sample size</li></ul></div><p>There are two different types of DA, and we will use <code class="literal">lda</code> from <a class="indexterm" id="id778"/>the <code class="literal">MASS</code> package for the<a class="indexterm" id="id779"/> linear discriminant function, and <code class="literal">qda</code> for the<a class="indexterm" id="id780"/> quadratic discriminant function.</p><p>Let us start with the dependent variable being the number of gears, and we will use all the other numeric values as independent variables. To make sure that we start with a standard <code class="literal">mtcars</code> dataset not overwritten in the preceding examples, let's clear the namespace and update the gear column to include categories instead of the actual numeric values:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; rm(mtcars)</strong></span>
<span class="strong"><strong>&gt; mtcars$gear &lt;- factor(mtcars$gear)</strong></span>
</pre></div><p>Due to the low number of observations (and as we have already discussed the related options in <a class="link" href="ch09.html" title="Chapter 9. From Big to Small Data">Chapter 9</a>, <span class="emphasis"><em>From Big to Smaller Data</em></span>), we can now set aside conducting the normality and other tests. Let's proceed with the actual analysis.</p><p>We call the <code class="literal">lda</code> function, setting<a class="indexterm" id="id781"/> <span class="strong"><strong>cross validation</strong></span> (<span class="strong"><strong>CV</strong></span>) to <code class="literal">TRUE</code>, so that we can test the accuracy of the prediction. The dot in the formula refers to all variables except the explicitly mentioned gear:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(MASS)</strong></span>
<span class="strong"><strong>&gt; d &lt;- lda(gear ~ ., data = mtcars, CV =TRUE)</strong></span>
</pre></div><p>So now we can check the accuracy of the predictions by comparing them to the original values via the confusion matrix:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; (tab &lt;- table(mtcars$gear, d$class)) </strong></span>
<span class="strong"><strong>     3  4  5</strong></span>
<span class="strong"><strong>  3 14  1  0</strong></span>
<span class="strong"><strong>  4  2 10  0</strong></span>
<span class="strong"><strong>  5  1  1  3</strong></span>
</pre></div><p>To present relative percentages instead of the raw numbers, we can do some quick transformations:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; tab / rowSums(tab)</strong></span>
<span class="strong"><strong>             3          4          5</strong></span>
<span class="strong"><strong>  3 0.93333333 0.06666667 0.00000000</strong></span>
<span class="strong"><strong>  4 0.16666667 0.83333333 0.00000000</strong></span>
<span class="strong"><strong>  5 0.20000000 0.20000000 0.60000000</strong></span>
</pre></div><p>And we can also compute the percentage of missed predictions:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sum(diag(tab)) / sum(tab)</strong></span>
<span class="strong"><strong>[1] 0.84375</strong></span>
</pre></div><p>After all, around <a class="indexterm" id="id782"/>84 percent of the cases got classified into their most likely respective classes, which were made up from the actual probabilities that can be extracted by the <code class="literal">posterior</code> element of the list:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; round(d$posterior, 4)</strong></span>
<span class="strong"><strong>                         3      4      5</strong></span>
<span class="strong"><strong>Mazda RX4           0.0000 0.8220 0.1780</strong></span>
<span class="strong"><strong>Mazda RX4 Wag       0.0000 0.9905 0.0095</strong></span>
<span class="strong"><strong>Datsun 710          0.0018 0.6960 0.3022</strong></span>
<span class="strong"><strong>Hornet 4 Drive      0.9999 0.0001 0.0000</strong></span>
<span class="strong"><strong>Hornet Sportabout   1.0000 0.0000 0.0000</strong></span>
<span class="strong"><strong>Valiant             0.9999 0.0001 0.0000</strong></span>
<span class="strong"><strong>Duster 360          0.9993 0.0000 0.0007</strong></span>
<span class="strong"><strong>Merc 240D           0.6954 0.2990 0.0056</strong></span>
<span class="strong"><strong>Merc 230            1.0000 0.0000 0.0000</strong></span>
<span class="strong"><strong>Merc 280            0.0000 1.0000 0.0000</strong></span>
<span class="strong"><strong>Merc 280C           0.0000 1.0000 0.0000</strong></span>
<span class="strong"><strong>Merc 450SE          1.0000 0.0000 0.0000</strong></span>
<span class="strong"><strong>Merc 450SL          1.0000 0.0000 0.0000</strong></span>
<span class="strong"><strong>Merc 450SLC         1.0000 0.0000 0.0000</strong></span>
<span class="strong"><strong>Cadillac Fleetwood  1.0000 0.0000 0.0000</strong></span>
<span class="strong"><strong>Lincoln Continental 1.0000 0.0000 0.0000</strong></span>
<span class="strong"><strong>Chrysler Imperial   1.0000 0.0000 0.0000</strong></span>
<span class="strong"><strong>Fiat 128            0.0000 0.9993 0.0007</strong></span>
<span class="strong"><strong>Honda Civic         0.0000 1.0000 0.0000</strong></span>
<span class="strong"><strong>Toyota Corolla      0.0000 0.9995 0.0005</strong></span>
<span class="strong"><strong>Toyota Corona       0.0112 0.8302 0.1586</strong></span>
<span class="strong"><strong>Dodge Challenger    1.0000 0.0000 0.0000</strong></span>
<span class="strong"><strong>AMC Javelin         1.0000 0.0000 0.0000</strong></span>
<span class="strong"><strong>Camaro Z28          0.9955 0.0000 0.0044</strong></span>
<span class="strong"><strong>Pontiac Firebird    1.0000 0.0000 0.0000</strong></span>
<span class="strong"><strong>Fiat X1-9           0.0000 0.9991 0.0009</strong></span>
<span class="strong"><strong>Porsche 914-2       0.0000 1.0000 0.0000</strong></span>
<span class="strong"><strong>Lotus Europa        0.0000 0.0234 0.9766</strong></span>
<span class="strong"><strong>Ford Pantera L      0.9965 0.0035 0.0000</strong></span>
<span class="strong"><strong>Ferrari Dino        0.0000 0.0670 0.9330</strong></span>
<span class="strong"><strong>Maserati Bora       0.0000 0.0000 1.0000</strong></span>
<span class="strong"><strong>Volvo 142E          0.0000 0.9898 0.0102</strong></span>
</pre></div><p>Now we can <a class="indexterm" id="id783"/>run <code class="literal">lda</code> again without cross validation to see the actual discriminants and how the different categories of <code class="literal">gear</code> are structured:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; d &lt;- lda(gear ~ ., data = mtcars)</strong></span>
<span class="strong"><strong>&gt; plot(d)</strong></span>
</pre></div><div class="mediaobject"><img alt="Discriminant analysis" src="graphics/2028OS_10_06.jpg"/></div><p>The numbers<a class="indexterm" id="id784"/> in the preceding plot stand for the cars in the <code class="literal">mtcars</code> dataset presented by the actual number of gears. It is really straightforward that the elements rendered by the two discriminants highlight the similarity of cars with the same number of gears and the difference between those with unequal values in the <code class="literal">gear</code> column.</p><p>These discriminants <a class="indexterm" id="id785"/>can be also extracted from the <code class="literal">d</code> object by calling <code class="literal">predict</code>, or can directly be rendered on a histogram to see the distribution of this continuous variable by the categories of the independent variable:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; plot(d, dimen = 1, type = "both" )</strong></span>
</pre></div><div class="mediaobject"><img alt="Discriminant analysis" src="graphics/2028OS_10_07.jpg"/></div></div>
<div class="section" title="Logistic regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec71"/>Logistic regression</h1></div></div></div><p>Although logistic regression <a class="indexterm" id="id786"/>was partly covered in <a class="link" href="ch06.html" title="Chapter 6. Beyond the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)">Chapter 6</a>, <span class="emphasis"><em>Beyond the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)</em></span>, as it's often used to solve classification problems we will revisit this topic again with some related examples and some notes on—for example—the multinomial version of logistic regression, which was not introduced in the previous chapters.</p><p>Our data often does not meet the requirements of the <span class="emphasis"><em>discriminant analysis</em></span>. In such cases, using logistic, logit, or probit regression can be a reasonable choice, as these methods are not sensitive to non-normal distribution and unequal variances within each group; on the other hand, they require much larger sample sizes. For small sample sizes, discriminant analysis is much more reliable.</p><p>As a rule of thumb, you should have at least 50 observations for each independent variable, which means that, if we want to build a logistic regression model for the <code class="literal">mtcars</code> dataset as earlier, we will need at least 500 observations—but we have only 32.</p><p>To this end, we will <a class="indexterm" id="id787"/>restrict this section to one or two quick examples on how to conduct a logit regression—for example, to estimate whether a car has automatic or manual transmission based on the performance and weight of the automobile:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; lr &lt;- glm(am ~ hp + wt, data = mtcars, family = binomial)</strong></span>
<span class="strong"><strong>&gt; summary(lr)</strong></span>

<span class="strong"><strong>Call:</strong></span>
<span class="strong"><strong>glm(formula = am ~ hp + wt, family = binomial, data = mtcars)</strong></span>

<span class="strong"><strong>Deviance Residuals: </strong></span>
<span class="strong"><strong>    Min       1Q   Median       3Q      Max  </strong></span>
<span class="strong"><strong>-2.2537  -0.1568  -0.0168   0.1543   1.3449  </strong></span>

<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>            Estimate Std. Error z value Pr(&gt;|z|)   </strong></span>
<span class="strong"><strong>(Intercept) 18.86630    7.44356   2.535  0.01126 * </strong></span>
<span class="strong"><strong>hp           0.03626    0.01773   2.044  0.04091 * </strong></span>
<span class="strong"><strong>wt          -8.08348    3.06868  -2.634  0.00843 **</strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>

<span class="strong"><strong>(Dispersion parameter for binomial family taken to be 1)</strong></span>

<span class="strong"><strong>    Null deviance: 43.230  on 31  degrees of freedom</strong></span>
<span class="strong"><strong>Residual deviance: 10.059  on 29  degrees of freedom</strong></span>
<span class="strong"><strong>AIC: 16.059</strong></span>

<span class="strong"><strong>Number of Fisher Scoring iterations: 8</strong></span>
</pre></div><p>The most important table from the preceding output is the coefficients table, which describes whether the model and the independent variables significantly contribute to the value of the independent variable. We can conclude that:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A 1-unit increase of horsepower increases the log odds of having a manual transmission (at least back in 1974, when the data was collected)</li><li class="listitem" style="list-style-type: disc">A 1-unit increase of weight (in pounds), on the other hand, decreases the same log odds by 8</li></ul></div><p>It seems that, despite (or rather due to) the low sample size, the model fits the data very well, and the horsepower and weight of the cars can explain whether a car has an automatic transmission or manual shift:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; table(mtcars$am, round(predict(lr, type = 'response')))</strong></span>
<span class="strong"><strong>     0  1</strong></span>
<span class="strong"><strong>  0 18  1</strong></span>
<span class="strong"><strong>  1  1 12</strong></span>
</pre></div><p>But running the preceding <a class="indexterm" id="id788"/>command on the number of gears instead of transmission would fail, as logit regression by default expects a dichotomous variable. We can overcome this by fitting multiple models on the data, such as verifying whether a car has 3/4/5 gears or not with <a class="indexterm" id="id789"/>dummy variables, or by fitting a multinomial logistic regression. The <code class="literal">nnet</code> package <a class="indexterm" id="id790"/>has a very convenient function to do so:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(nnet) </strong></span>
<span class="strong"><strong>&gt; (mlr &lt;- multinom(factor(gear) ~ ., data = mtcars)) </strong></span>
<span class="strong"><strong># weights:  36 (22 variable)</strong></span>
<span class="strong"><strong>initial  value 35.155593 </strong></span>
<span class="strong"><strong>iter  10 value 5.461542</strong></span>
<span class="strong"><strong>iter  20 value 0.035178</strong></span>
<span class="strong"><strong>iter  30 value 0.000631</strong></span>
<span class="strong"><strong>final  value 0.000000 </strong></span>
<span class="strong"><strong>converged</strong></span>
<span class="strong"><strong>Call:</strong></span>
<span class="strong"><strong>multinom(formula = factor(gear) ~ ., data = mtcars)</strong></span>

<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>  (Intercept)       mpg       cyl      disp         hp     drat</strong></span>
<span class="strong"><strong>4  -12.282953 -1.332149 -10.29517 0.2115914 -1.7284924 15.30648</strong></span>
<span class="strong"><strong>5    7.344934  4.934189 -38.21153 0.3972777 -0.3730133 45.33284</strong></span>
<span class="strong"><strong>         wt        qsec        vs       am     carb</strong></span>
<span class="strong"><strong>4 21.670472   0.1851711  26.46396 67.39928 45.79318</strong></span>
<span class="strong"><strong>5 -4.126207 -11.3692290 -38.43033 32.15899 44.28841</strong></span>

<span class="strong"><strong>Residual Deviance: 4.300374e-08 </strong></span>
<span class="strong"><strong>AIC: 44</strong></span>
</pre></div><p>As expected, it returns a highly fitted model to our small dataset:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; table(mtcars$gear, predict(mlr))</strong></span>
<span class="strong"><strong>     3  4  5</strong></span>
<span class="strong"><strong>  3 15  0  0</strong></span>
<span class="strong"><strong>  4  0 12  0</strong></span>
<span class="strong"><strong>  5  0  0  5</strong></span>
</pre></div><p>However, due to the<a class="indexterm" id="id791"/> small sample size, this model is extremely limited. Before proceeding to the next examples, please remove the updated <code class="literal">mtcars</code> dataset from the current R session to avoid unexpected errors:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; rm(mtcars)</strong></span>
</pre></div></div>
<div class="section" title="Machine learning algorithms"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec72"/>Machine learning algorithms</h1></div></div></div><p>
<span class="strong"><strong>Machine learning</strong></span> (<span class="strong"><strong>ML</strong></span>)<a class="indexterm" id="id792"/> is a collection of data-driven algorithms that work without being explicitly programmed for a specific task. Unlike non-ML algorithms, they require (and learn by) the training data. ML algorithms are classified into supervised and unsupervised types.</p><p>
<span class="strong"><strong>Supervised learning</strong></span>
<a class="indexterm" id="id793"/> means that the training data consists of input vectors and their corresponding output value as well. This means that the task is to establish relationships between inputs and outputs in a historical database, called the training set, and thus make it possible to predict outputs for future input values.</p><p>For example, banks have vast databases on previous loan transaction details. The input vector is comprised of personal information—such as age, salary, marital status and so on—while the output (target) variable shows whether the payment deadlines were kept or not. In this case, a supervised algorithm may detect different groups of people who may be prone to not being able to keep the deadlines, which may serve as a screening of applicants.</p><p>Unsupervised learning<a class="indexterm" id="id794"/> has different goals. As the output values are not available in the historical dataset, the aim is to identify underlying correlations between the inputs, and define arbitrary groups of cases.</p><div class="section" title="The K-Nearest Neighbors algorithm"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec69"/>The K-Nearest Neighbors algorithm</h2></div></div></div><p>
<span class="strong"><strong>K-Nearest Neighbors</strong></span> (<span class="strong"><strong>k-NN</strong></span>), unlike<a class="indexterm" id="id795"/> the hierarchical or k-means clustering, is a <a class="indexterm" id="id796"/>supervised classification algorithm. Although it is often confused with k-means clustering, k-NN classification is a completely different method. It is mostly used in pattern recognition and business analytics. A big advantage of k-NN is that it is not sensitive to outliers, and the usage is extremely straightforward—just like with most machine learning algorithms.</p><p>The main idea of k-NN is that it identifies the <span class="emphasis"><em>k</em></span> number of nearest neighbors of the observation in the historical dataset, then it defines the class of the observation to match the majority of the neighbors mentioned earlier.</p><p>As a sample analysis, we are going to use the <code class="literal">knn</code> function from<a class="indexterm" id="id797"/> the <code class="literal">class</code> package. The <code class="literal">knn</code> function<a class="indexterm" id="id798"/> takes 4 parameters, where <code class="literal">train</code> and <code class="literal">test</code> are the training and test datasets respectively, <code class="literal">cl</code> is the class membership of the training data, and <code class="literal">k</code> is the number of neighbors to take into account when classifying the elements of the test dataset.</p><p>The default value of <code class="literal">k</code> is <code class="literal">1</code>, which always works without a problem—although usually with a rather low accuracy. When defining a higher number of neighbors to be used in the analysis for improved accuracy, it's wise to select an integer that is not a multiple of the number of classes.</p><p>Let's split the <code class="literal">mtcars</code> dataset into two parts: training and test data. For the sake of simplicity, half of the cars will belong to the training set, and the other half to the test set:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; set.seed(42)</strong></span>
<span class="strong"><strong>&gt; n     &lt;- nrow(mtcars)</strong></span>
<span class="strong"><strong>&gt; train &lt;- mtcars[sample(n, n/2), ]</strong></span>
</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip16"/>Tip</h3><p>We used <code class="literal">set.seed</code> to configure the random generator's state to a (well) known number for the sake of reproducibility: so that the exact same <span class="emphasis"><em>random</em></span> numbers will be generated on all machines.</p></div></div><p>So we sampled 16 integers between 1 and 32 to select 50 percent of the rows from the <code class="literal">mtcars</code> dataset. Some might consider the following <code class="literal">dplyr</code> (discussed in <a class="link" href="ch03.html" title="Chapter 3. Filtering and Summarizing Data">Chapter 3</a>, <span class="emphasis"><em>Filtering and Summarizing Data</em></span> and in <a class="link" href="ch04.html" title="Chapter 4. Restructuring Data">Chapter 4</a>, <span class="emphasis"><em>Restructuring Data</em></span>) code snippet more appealing for the task:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(dplyr)</strong></span>
<span class="strong"><strong>&gt; train &lt;- sample_n(mtcars, n / 2)</strong></span>
</pre></div><p>Then let's select the rest of the rows with the difference of the newly created <code class="literal">data.frame</code> compared to the original data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; test &lt;- mtcars[setdiff(row.names(mtcars), row.names(train)), ]</strong></span>
</pre></div><p>Now we have to<a class="indexterm" id="id799"/> define the class memberships of the <a class="indexterm" id="id800"/>observations in the training data, what we would like to predict in the test dataset in the means of classification. To this end, we might use what we have learned in the previous section and, instead of an already known characteristic of the cars, we could run a clustering method to define the class membership of each element in the training data—but that's not something we should do for instructional purposes. You could also run the clustering algorithm on your test data as well, right? The major difference between the supervised and unsupervised methods is that we have empirical data with the former methods to feed the classification models.</p><p>So, instead, let's use the number of gears in the cars as the class membership and, based on the information found in the training data, let's predict the number of gears in the test dataset:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(class)</strong></span>
<span class="strong"><strong>&gt; (cm &lt;- knn(</strong></span>
<span class="strong"><strong>+     train = subset(train, select = -gear),</strong></span>
<span class="strong"><strong>+     test  = subset(test, select = -gear),</strong></span>
<span class="strong"><strong>+     cl    = train$gear,</strong></span>
<span class="strong"><strong>+     k     = 5))</strong></span>
<span class="strong"><strong>[1] 4 4 4 4 3 4 4 3 3 3 3 3 4 4 4 3</strong></span>
<span class="strong"><strong>Levels: 3 4 5</strong></span>
</pre></div><p>The test cases have just got classified into the preceding classes. We can check the accuracy of the classification, for example, by calculating the correlation coefficient between the real and predicted number of gears:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; cor(test$gear, as.numeric(as.character(cm)))</strong></span>
<span class="strong"><strong>[1] 0.5459487</strong></span>
</pre></div><p>Well, this might have been a lot better, especially if the training data had been a lot larger. Machine learning algorithms typically ­use millions of rows from historical databases, as opposed to our meager dataset with only 16 cases. But let's see where the model failed to provide accurate predictions by computing the confusion matrix:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; table(test$gear, as.numeric(as.character(cm)))</strong></span>
<span class="strong"><strong>    3 4</strong></span>
<span class="strong"><strong>  3 6 1</strong></span>
<span class="strong"><strong>  4 0 6</strong></span>
<span class="strong"><strong>  5 1 2</strong></span>
</pre></div><p>So it seems that the <a class="indexterm" id="id801"/>k-NN classification algorithm could predict the<a class="indexterm" id="id802"/> number of gears very accurately (one miss out of 13) for all those cars with three or four gears, but it ultimately failed with the ones with five gears. This can be explained by the number of related cars in the original dataset:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; table(train$gear)</strong></span>
<span class="strong"><strong>3 4 5 </strong></span>
<span class="strong"><strong>8 6 2</strong></span>
</pre></div><p>Well, the training data had only two cars with 5 gears, which is indeed really tight when it comes to building a model providing accurate predictions.</p></div><div class="section" title="Classification trees"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec70"/>Classification trees</h2></div></div></div><p>An alternative ML method for <a class="indexterm" id="id803"/>supervised classification is the<a class="indexterm" id="id804"/> use of recursive partitioning via decision trees. The great advantage of this method is that visualizing decision rules can significantly improve understanding of the underlying data, and running the algorithm can be extremely easy in most cases.</p><p>Let's load the <code class="literal">rpart</code> package and<a class="indexterm" id="id805"/> build a classification tree with the response variable being the <code class="literal">gear</code> function again:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(rpart)</strong></span>
<span class="strong"><strong>&gt; ct &lt;- rpart(factor(gear) ~ ., data = train, minsplit = 3)</strong></span>
<span class="strong"><strong>&gt; summary(ct)</strong></span>
<span class="strong"><strong>Call:</strong></span>
<span class="strong"><strong>rpart(formula = factor(gear) ~ ., data = train, minsplit = 3)</strong></span>
<span class="strong"><strong>  n= 16 </strong></span>

<span class="strong"><strong>    CP nsplit rel error xerror      xstd</strong></span>
<span class="strong"><strong>1 0.75      0      1.00  1.000 0.2500000</strong></span>
<span class="strong"><strong>2 0.25      1      0.25  0.250 0.1653595</strong></span>
<span class="strong"><strong>3 0.01      2      0.00  0.125 0.1210307</strong></span>

<span class="strong"><strong>Variable importance</strong></span>
<span class="strong"><strong>drat qsec  cyl disp   hp  mpg   am carb </strong></span>
<span class="strong"><strong>  18   16   12   12   12   12    9    9 </strong></span>

<span class="strong"><strong>Node number 1: 16 observations,    complexity param=0.75</strong></span>
<span class="strong"><strong>  predicted class=3  expected loss=0.5  P(node) =1</strong></span>
<span class="strong"><strong>    class counts:     8     6     2</strong></span>
<span class="strong"><strong>   probabilities: 0.500 0.375 0.125 </strong></span>
<span class="strong"><strong>  left son=2 (10 obs) right son=3 (6 obs)</strong></span>
<span class="strong"><strong>  Primary splits:</strong></span>
<span class="strong"><strong>      drat &lt; 3.825 to the left,  improve=6.300000, (0 missing)</strong></span>
<span class="strong"><strong>      disp &lt; 212.8 to the right, improve=4.500000, (0 missing)</strong></span>
<span class="strong"><strong>      am   &lt; 0.5   to the left,  improve=3.633333, (0 missing)</strong></span>
<span class="strong"><strong>      hp   &lt; 149   to the right, improve=3.500000, (0 missing)</strong></span>
<span class="strong"><strong>      qsec &lt; 18.25 to the left,  improve=3.500000, (0 missing)</strong></span>
<span class="strong"><strong>  Surrogate splits:</strong></span>
<span class="strong"><strong>      mpg  &lt; 22.15 to the left,  agree=0.875, adj=0.667, (0 split)</strong></span>
<span class="strong"><strong>      cyl  &lt; 5     to the right, agree=0.875, adj=0.667, (0 split)</strong></span>
<span class="strong"><strong>      disp &lt; 142.9 to the right, agree=0.875, adj=0.667, (0 split)</strong></span>
<span class="strong"><strong>      hp   &lt; 96    to the right, agree=0.875, adj=0.667, (0 split)</strong></span>
<span class="strong"><strong>      qsec &lt; 18.25 to the left,  agree=0.875, adj=0.667, (0 split)</strong></span>

<span class="strong"><strong>Node number 2: 10 observations,    complexity param=0.25</strong></span>
<span class="strong"><strong>  predicted class=3  expected loss=0.2  P(node) =0.625</strong></span>
<span class="strong"><strong>    class counts:     8     0     2</strong></span>
<span class="strong"><strong>   probabilities: 0.800 0.000 0.200 </strong></span>
<span class="strong"><strong>  left son=4 (8 obs) right son=5 (2 obs)</strong></span>
<span class="strong"><strong>  Primary splits:</strong></span>
<span class="strong"><strong>      am   &lt; 0.5   to the left,  improve=3.200000, (0 missing)</strong></span>
<span class="strong"><strong>      carb &lt; 5     to the left,  improve=3.200000, (0 missing)</strong></span>
<span class="strong"><strong>      qsec &lt; 16.26 to the right, improve=1.866667, (0 missing)</strong></span>
<span class="strong"><strong>      hp   &lt; 290   to the left,  improve=1.422222, (0 missing)</strong></span>
<span class="strong"><strong>      disp &lt; 325.5 to the right, improve=1.200000, (0 missing)</strong></span>
<span class="strong"><strong>  Surrogate splits:</strong></span>
<span class="strong"><strong>      carb &lt; 5     to the left,  agree=1.0, adj=1.0, (0 split)</strong></span>
<span class="strong"><strong>      qsec &lt; 16.26 to the right, agree=0.9, adj=0.5, (0 split)</strong></span>

<span class="strong"><strong>Node number 3: 6 observations</strong></span>
<span class="strong"><strong>  predicted class=4  expected loss=0  P(node) =0.375</strong></span>
<span class="strong"><strong>    class counts:     0     6     0</strong></span>
<span class="strong"><strong>   probabilities: 0.000 1.000 0.000 </strong></span>

<span class="strong"><strong>Node number 4: 8 observations</strong></span>
<span class="strong"><strong>  predicted class=3  expected loss=0  P(node) =0.5</strong></span>
<span class="strong"><strong>    class counts:     8     0     0</strong></span>
<span class="strong"><strong>   probabilities: 1.000 0.000 0.000 </strong></span>

<span class="strong"><strong>Node number 5: 2 observations</strong></span>
<span class="strong"><strong>  predicted class=5  expected loss=0  P(node) =0.125</strong></span>
<span class="strong"><strong>    class counts:     0     0     2</strong></span>
<span class="strong"><strong>   probabilities: 0.000 0.000 1.000 </strong></span>
</pre></div><p>The resulting object is a <a class="indexterm" id="id806"/>rather simple <span class="emphasis"><em>decision tree</em></span>—despite the fact that we have<a class="indexterm" id="id807"/> specified an extremely low <code class="literal">minsplit</code> parameter, to be able to generate more than one node. Running the preceding call <a class="indexterm" id="id808"/>without this argument would not even result in a decision tree, as the 16 cases of our train data would fit in a single node due to the default minimum value of 20 elements per node.</p><p>But we have built a decision tree where the most important rule to determine the number of gears is the rear axle ratio and whether the car has automatic or manual transmission:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; plot(ct); text(ct)</strong></span>
</pre></div><div class="mediaobject"><img alt="Classification trees" src="graphics/2028OS_10_08.jpg"/></div><p>To translate this into plain and simple English:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A car with a high rear axle ratio has four gears</li><li class="listitem" style="list-style-type: disc">All other cars with automatic transmission have three gears</li><li class="listitem" style="list-style-type: disc">Cars with manual shift have five gears</li></ul></div><p>Well, this rule is indeed very basic due to the low number of cases and the confusion matrix also reveals the serious limitation of the model, namely that it cannot successfully identify cars with 5 gears:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; table(test$gear, predict(ct, newdata = test, type = 'class'))</strong></span>
<span class="strong"><strong>    3 4 5</strong></span>
<span class="strong"><strong>  3 7 0 0</strong></span>
<span class="strong"><strong>  4 1 5 0</strong></span>
<span class="strong"><strong>  5 0 2 1</strong></span>
</pre></div><p>But 13 out of 16 cars were classified perfectly, which is quite impressive and a bit better than the previous k-NN example!</p><p>Let's improve the preceding code, rather minimalist graph a bit by either calling the <code class="literal">main</code> function from the <code class="literal">rpart.plot</code> package<a class="indexterm" id="id809"/> on the preceding object, or loading <a class="indexterm" id="id810"/>the <code class="literal">party</code> package, which provides a very neat plotting function for <code class="literal">party</code> objects. One option might be to call <code class="literal">as.party</code> on the previously computed <code class="literal">ct</code> object via<a class="indexterm" id="id811"/> the <code class="literal">partykit</code> package; alternatively, we can recreate the classification tree with its <code class="literal">ctree</code> function. Based on the <a class="indexterm" id="id812"/>previous experiences, let's pass only<a class="indexterm" id="id813"/> the preceding highlighted variables to the model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(party)</strong></span>
<span class="strong"><strong>&gt; ct &lt;- ctree(factor(gear) ~ drat, data = train,</strong></span>
<span class="strong"><strong>+   controls = ctree_control(minsplit = 3)) </strong></span>
<span class="strong"><strong>&gt; plot(ct, main = "Conditional Inference Tree")</strong></span>
</pre></div><div class="mediaobject"><img alt="Classification trees" src="graphics/2028OS_10_09.jpg"/></div><p>It seems that this model decides on the number of gears solely based on the rear axle ratio with a lot lower accuracy:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; table(test$gear, predict(ct, newdata = test, type = 'node'))</strong></span>
<span class="strong"><strong>    2 3</strong></span>
<span class="strong"><strong>  3 7 0</strong></span>
<span class="strong"><strong>  4 1 5</strong></span>
<span class="strong"><strong>  5 0 3</strong></span>
</pre></div><p>Now let's see which<a class="indexterm" id="id814"/> additional ML algorithms can provide <a class="indexterm" id="id815"/>more accurate and/or reliable models!</p></div><div class="section" title="Random forest"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec71"/>Random forest</h2></div></div></div><p>The main idea behind<a class="indexterm" id="id816"/> <span class="strong"><strong>random forest</strong></span>
<a class="indexterm" id="id817"/> is that, instead of building a deep <a class="indexterm" id="id818"/>decision tree with an ever-growing number of nodes that might risk overfitting the data, we instead generate multiple trees to minimize the variance instead of maximizing the accuracy. This way the results are expected to be noisier compared to a well-trained decision tree, but on average these results are more reliable.</p><p>This can be achieved in a similar way to the preceding examples in R, via for example the <code class="literal">randomForest</code> package, which<a class="indexterm" id="id819"/> provides very user-friendly access to the classical random forest algorithm:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(randomForest)</strong></span>
<span class="strong"><strong>&gt; (rf &lt;- randomForest(factor(gear) ~ ., data = train, ntree = 250))</strong></span>
<span class="strong"><strong>Call:</strong></span>
<span class="strong"><strong> randomForest(formula = factor(gear) ~ ., data = train, ntree = 250) </strong></span>
<span class="strong"><strong>               Type of random forest: classification</strong></span>
<span class="strong"><strong>                     Number of trees: 250</strong></span>
<span class="strong"><strong>No. of variables tried at each split: 3</strong></span>

<span class="strong"><strong>        OOB estimate of  error rate: 25%</strong></span>
<span class="strong"><strong>Confusion matrix:</strong></span>
<span class="strong"><strong>  3 4 5 class.error</strong></span>
<span class="strong"><strong>3 7 1 0   0.1250000</strong></span>
<span class="strong"><strong>4 1 5 0   0.1666667</strong></span>
<span class="strong"><strong>5 2 0 0   1.0000000</strong></span>
</pre></div><p>This function is very convenient to use: it automatically returns the confusion matrix and also computes the estimated error rate—although we can of course, generate our own based on the other subset of <code class="literal">mtcars</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; table(test$gear, predict(rf, test))   </strong></span>
<span class="strong"><strong>    3 4 5</strong></span>
<span class="strong"><strong>  3 7 0 0</strong></span>
<span class="strong"><strong>  4 1 5 0</strong></span>
<span class="strong"><strong>  5 1 2 0</strong></span>
</pre></div><p>But this time, the plotting <a class="indexterm" id="id820"/>function returns something new:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; plot(rf)</strong></span>
<span class="strong"><strong>&gt; legend('topright',</strong></span>
<span class="strong"><strong>+   legend = colnames(rf$err.rate),</strong></span>
<span class="strong"><strong>+   col    = 1:4,</strong></span>
<span class="strong"><strong>+   fill   = 1:4,</strong></span>
<span class="strong"><strong>+   bty    = 'n')</strong></span>
</pre></div><div class="mediaobject"><img alt="Random forest" src="graphics/2028OS_10_10.jpg"/></div><p>We see how the mean squared error of the<a class="indexterm" id="id821"/> model changes over time as we generate more and more <a class="indexterm" id="id822"/>decision trees on random subsamples of the training data, where the error rate does not seem to change after a while, and there's not much sense in generating more than a given number of random samples.</p><p>Well, this is really straightforward for such small example, as the combination of the possible subsamples is limited. It's also worth mentioning that the error rate of cars with five gears (blue line) did not change at all over time, which highlights again the main limitation of our training dataset.</p></div><div class="section" title="Other algorithms"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec72"/>Other algorithms</h2></div></div></div><p>Although it would be great to continue discussing the wide variety of related ML algorithms (for example, the ID3 <a class="indexterm" id="id823"/>and Gradient Boosting<a class="indexterm" id="id824"/> algorithms from the<a class="indexterm" id="id825"/> <code class="literal">gbm</code> or <code class="literal">xgboost</code> packages) and<a class="indexterm" id="id826"/> how to call, say, Weka <a class="indexterm" id="id827"/>from the R console to use <a class="indexterm" id="id828"/>C4.5, in this chapter I can focus on only one last practical example on how to use a general interface for all these algorithms via <a class="indexterm" id="id829"/>the <code class="literal">caret</code> package:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(caret)</strong></span>
</pre></div><p>This package bundles some really useful functions and methods, which can be used as general, algorithm-independent tools for predictive models. This means that all the previous models could be run without actually calling the <code class="literal">rpart</code>, <code class="literal">ctree</code>, or <code class="literal">randomForest</code> functions, and we can simply rely on the <code class="literal">train</code> function of caret, which takes the algorithm definition as an argument.</p><p>For a quick example, let's see how the improved version and open-source implementation of C4.5 performs with our training data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(C50)</strong></span>
<span class="strong"><strong>&gt; C50 &lt;- train(factor(gear) ~ ., data = train, method = 'C5.0')</strong></span>
<span class="strong"><strong>&gt; summary(C50)</strong></span>

<span class="strong"><strong>C5.0 [Release 2.07 GPL Edition]    Fri Mar 20 23:22:10 2015</strong></span>
<span class="strong"><strong>-------------------------------</strong></span>

<span class="strong"><strong>Class specified by attribute `outcome'</strong></span>

<span class="strong"><strong>Read 16 cases (11 attributes) from undefined.data</strong></span>

<span class="strong"><strong>-----  Trial 0:  -----</strong></span>

<span class="strong"><strong>Rules:</strong></span>

<span class="strong"><strong>Rule 0/1: (8, lift 1.8)</strong></span>
<span class="strong"><strong>  drat &lt;= 3.73</strong></span>
<span class="strong"><strong>  am &lt;= 0</strong></span>
<span class="strong"><strong>  -&gt;  class 3  [0.900]</strong></span>

<span class="strong"><strong>Rule 0/2: (6, lift 2.3)</strong></span>
<span class="strong"><strong>  drat &gt; 3.73</strong></span>
<span class="strong"><strong>  -&gt;  class 4  [0.875]</strong></span>

<span class="strong"><strong>Rule 0/3: (2, lift 6.0)</strong></span>
<span class="strong"><strong>  drat &lt;= 3.73</strong></span>
<span class="strong"><strong>  am &gt; 0</strong></span>
<span class="strong"><strong>  -&gt;  class 5  [0.750]</strong></span>

<span class="strong"><strong>Default class: 3</strong></span>

<span class="strong"><strong>*** boosting reduced to 1 trial since last classifier is very accurate</strong></span>

<span class="strong"><strong>*** boosting abandoned (too few classifiers)</strong></span>


<span class="strong"><strong>Evaluation on training data (16 cases):</strong></span>

<span class="strong"><strong>          Rules     </strong></span>
<span class="strong"><strong>    ----------------</strong></span>
<span class="strong"><strong>      No      Errors</strong></span>

<span class="strong"><strong>       3    0( 0.0%)   &lt;&lt;</strong></span>


<span class="strong"><strong>     (a)   (b)   (c)    &lt;-classified as</strong></span>
<span class="strong"><strong>    ----  ----  ----</strong></span>
<span class="strong"><strong>       8                (a): class 3</strong></span>
<span class="strong"><strong>             6          (b): class 4</strong></span>
<span class="strong"><strong>                   2    (c): class 5</strong></span>


<span class="strong"><strong>  Attribute usage:</strong></span>

<span class="strong"><strong>  100.00%  drat</strong></span>
<span class="strong"><strong>   62.50%  am</strong></span>
</pre></div><p>This output seems extremely compelling as the error rate is exactly zero, which means that we have just created a model that perfectly fits out training data with three simple rules:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Cars with a large rear axle ratio have four gears</li><li class="listitem" style="list-style-type: disc">The others have either three (manual shift) or five (automatic transmission)</li></ul></div><p>Well, a second look at the results reveals that we have not found the Holy Grail yet:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; table(test$gear, predict(C50, test))</strong></span>
<span class="strong"><strong>    3 4 5</strong></span>
<span class="strong"><strong>  3 7 0 0</strong></span>
<span class="strong"><strong>  4 1 5 0</strong></span>
<span class="strong"><strong>  5 0 3 0</strong></span>
</pre></div><p>So the overall performance of this algorithm with our test dataset resulted in 12 hits out of the 16 cars, which is a good example of how a single decision tree might over-fit the training data.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec73"/>Summary</h1></div></div></div><p>This chapter introduced a wide variety of ways to cluster and classify data, discussed which analysis procedures and models are very important, and generally used elements of a data scientist's toolbox. In the next chapter, we will focus on a less general, but still important, field— how to analyze graphs and network data.</p></div></body></html>