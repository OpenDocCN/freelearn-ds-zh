<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Using GPUs to Run R Even Faster"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Using GPUs to Run R Even Faster</h1></div></div></div><p>In this chapter, we will look at another means to speed up the execution of an R code using a technology that is often untapped, although it is part of most computers—the <span class="strong"><strong>Graphics Processing Unit</strong></span> (<span class="strong"><strong>GPU</strong></span>), otherwise known as a graphics card. When we think of a <a id="id140" class="indexterm"/>GPU, we often think of the amazing graphics it can produce. In fact, GPUs are powered by technologies with highly parallel processing capabilities that are like the top supercomputers in the world. In the past, programming with GPUs was very difficult. However, in the last few years, this barrier has been removed with GPU programming platforms like CUDA and OpenCL that make programming with GPUs accessible for many programmers. Better still, the R community has developed a few packages for R users to leverage the computing power of GPUs.</p><p>To run the examples in this chapter, you will need an NVIDIA GPU with CUDA capabilities.</p><p>This chapter covers:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">General purpose computing on GPUs</li><li class="listitem" style="list-style-type: disc">R and GPUs</li><li class="listitem" style="list-style-type: disc">Fast statistical modeling in R with <code class="literal">gputools</code></li></ul></div><div class="section" title="General purpose computing on GPUs"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec30"/>General purpose computing on GPUs</h1></div></div></div><p>Historically, GPUs were designed and used to render high-resolution graphics such as for <a id="id141" class="indexterm"/>video games. To be able to render millions of pixels every second, GPUs utilize a highly parallel architecture that specializes in the types of computations required to render graphics. At a high level, the architecture of a GPU is similar to that of a CPU—it has its own multi-core processor and memory. However, because GPUs are not designed for general computation, individual cores are much simpler with slower clock speeds and limited support for complex instructions, compared to CPUs. In addition, they typically have less RAM than CPUs. To achieve real-time rendering, most GPU computations are done in a highly parallel manner, with many more cores than CPUs—a modern GPU might have more than 2,000 cores. Given that one core can run multiple threads, it is possible to run tens of thousands of parallel threads on a GPU.</p><p>In 1990s, programmers began to realize that certain computations outside of graphics rendering can benefit from the highly parallel architecture of GPUs. Remember the embarrassingly parallel nature of vectorized operations in R from <a class="link" href="ch03.html" title="Chapter 3. Simple Tweaks to Make R Run Faster">Chapter 3</a>, <span class="emphasis"><em>Simple Tweaks to Make R Run Faster</em></span>; imagine the potential speedup if they were done simultaneously by thousands of GPU cores. This awareness gave rise to general purpose computing on GPUs (GPGPU).</p><p>But it was challenging to program GPUs. Using low-level interfaces provided by standards like DirectX and OpenGL, programmers had to trick the GPUs to compute on numbers as if they were rendering graphics. Realizing this challenge, efforts sprung up to develop proper programming languages and the supporting architectures for GPGPU. The chief outcomes from these efforts are two technologies called CUDA, developed by NVIDIA, and OpenCL, developed by Apple and now maintained by Khronos. While CUDA is proprietary and works only on NVIDIA GPUs, OpenCL is brand <a id="id142" class="indexterm"/>agnostic and is even able to support other accelerators like <span class="strong"><strong>Field Programmable Gate Arrays</strong></span> (<span class="strong"><strong>FPGAs</strong></span>).</p></div></div>
<div class="section" title="R and GPUs"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec31"/>R and GPUs</h1></div></div></div><p>The R community<a id="id143" class="indexterm"/> has developed a few packages for R programmers to leverage GPUs. The <a id="id144" class="indexterm"/>vectorized nature of R makes the use of GPUs a natural fit. The packages vary in the level of encapsulation and hence the required familiarity with the native CUDA or OpenCL languages. A selection of R packages for GPU programming are listed here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">gputools</code>: This <a id="id145" class="indexterm"/>provides R functions that wrap around GPU-based algorithms <a id="id146" class="indexterm"/>for common operations, such as linear models and matrix algebra. It requires CUDA, and hence an NVIDIA GPU.</li><li class="listitem" style="list-style-type: disc"><code class="literal">gmatrix</code>: This <a id="id147" class="indexterm"/>provides<a id="id148" class="indexterm"/> the <code class="literal">gmatrix</code> and <code class="literal">gvector</code> classes to represent matrices and vectors respectively in NVIDIA GPUs. It also provides functions for common matrix operations such as matrix algebra, and random number generation and sorting.</li><li class="listitem" style="list-style-type: disc"><code class="literal">RCUDA</code>: This <a id="id149" class="indexterm"/>provides <a id="id150" class="indexterm"/>a low-level interface to load and call a CUDA kernel from an R session. Using RCUDA requires a good understanding of the CUDA language, but allows more flexibility and code optimization. More information about t can be found at <a class="ulink" href="http://www.omegahat.org/RCUDA/">http://www.omegahat.org/RCUDA/</a>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">OpenCL</code>: This is <a id="id151" class="indexterm"/>similar <a id="id152" class="indexterm"/>to RCUDA, but interfaces with OpenCL. It caters to users that have non-NVIDIA GPUs like ATI, Intel, or AMD GPUs.</li></ul></div><p>Other CRAN packages are available for more specialized functions on GPUs, such as linear regression. For a list of these packages, see the GPUs section of <span class="emphasis"><em>CRAN Task View: High-Performance and Parallel Computing with R</em></span>, maintained by Dirk Eddelbuettel on the CRAN website at <a class="ulink" href="http://cran.r-project.org/web/views/HighPerformanceComputing.html">http://cran.r-project.org/web/views/HighPerformanceComputing.html</a>.</p><p>In this chapter, we will focus only on <code class="literal">gputools</code> and use a few examples from this package to illustrate how GPUs can speed up computations in R.</p><div class="section" title="Installing gputools"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec18"/>Installing gputools</h2></div></div></div><p>These <a id="id153" class="indexterm"/>are the <a id="id154" class="indexterm"/>steps <a id="id155" class="indexterm"/>to install <code class="literal">gputools</code>:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Make sure that your computer has a CUDA-enabled GPU card. For the list <a id="id156" class="indexterm"/>of CUDA-enabled GPUs, refer to <a class="ulink" href="https://developer.nvidia.com/cuda-gpus">https://developer.nvidia.com/cuda-gpus</a>.</li><li class="listitem">Download and install <a id="id157" class="indexterm"/>CUDA toolkit from <a class="ulink" href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a>.</li><li class="listitem">Set a few <a id="id158" class="indexterm"/>environment variables as specified in the <code class="literal">gputools</code> installation note <a class="ulink" href="http://cran.r-project.org/web/packages/gputools/INSTALL">http://cran.r-project.org/web/packages/gputools/INSTALL</a>.</li><li class="listitem">Open an R session and run <code class="literal">install.packages("gputools")</code>.</li></ol></div><p>If you do not have an NVIDIA GPU with CUDA capabilities, <span class="strong"><strong>Amazon Web Services</strong></span> (<span class="strong"><strong>AWS</strong></span>) offers<a id="id159" class="indexterm"/> GPU instances, called <code class="literal">g2.2xlarge</code> instances, that come <a id="id160" class="indexterm"/>with (at the time of writing) NVIDIA GRID K520 GPUs with 1,536 CUDA cores and <a id="id161" class="indexterm"/>4 GB of <a id="id162" class="indexterm"/>video memory. You can use these instances together with <span class="strong"><strong>Amazon Machine Images</strong></span> (<span class="strong"><strong>AMIs</strong></span>) provided by NVIDIA that are preloaded with the <a id="id163" class="indexterm"/>CUDA toolkit and drivers. Both Windows and Linux AMIs are available at <a class="ulink" href="https://aws.amazon.com/marketplace/seller-profile/ref=sp_mpg_product_vendor?ie=UTF8&amp;id=c568fe05-e33b-411c-b0ab-047218431da9">https://aws.amazon.com/marketplace/seller-profile/ref=sp_mpg_product_vendor?ie=UTF8&amp;id=c568fe05-e33b-411c-b0ab-047218431da9</a>. For this chapter, we <a id="id164" class="indexterm"/>used the Linux AMI version 2014.03.2.</p></div></div>
<div class="section" title="Fast statistical modeling in R with gputools"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec32"/>Fast statistical modeling in R with gputools</h1></div></div></div><p><code class="literal">gputools</code> provides a convenient way to execute statistical functions on a GPU, without CUDA programming. All the heavy lifting, including copying data from RAM to <a id="id165" class="indexterm"/>GPU memory and setting the number of cores to use have been encapsulated within the functions (in fact, <code class="literal">gputools</code> relies on the well-encapsulated <code class="literal">CUBLAS</code> library, which provides<a id="id166" class="indexterm"/> linear algebra functions for GPUs). For example, to perform linear modeling on the <code class="literal">mtcars</code> dataset on a CPU, we use the <code class="literal">lm()</code>: <code class="literal">lm(mpg~cyl+disp+hp, data=mtcars)</code> function. To run it on a GPU, we call the <code class="literal">gpuLm()</code> function from <code class="literal">gputools</code>: <code class="literal">gpuLm(mpg~cyl+disp+hp, data=mtcars)</code>. The output of <code class="literal">gpuLm()</code> <code class="literal">
</code>follows the same format as <code class="literal">lm()</code>.</p><p>To demonstrate the speedup that we can expect from a GPU, we will calculate Kendall correlations on random datasets having 100 variables. We will use a varying number of observations from 100, 200, … to 500 records in order to observe the speedup in comparison to the CPU version. The code is as follows:</p><div class="informalexample"><pre class="programlisting">library(gputools)
A &lt;- lapply(c(1:5), function(x) {
    matrix(rnorm((x*1e2) * 1e2), 1e2, (x*1e2))})
cpu_k_time &lt;- sapply(A, function(x) {
    system.time(cor(x=x, method="kendall"))[[3]]})
gpu_k_time &lt;- sapply(A, function(x) {
    system.time(gpuCor(x=x, method="kendall"))[[3]]})
K &lt;- data.frame(cpu=cpu_k_time, gpu=gpu_k_time)</pre></div><p>We tested this code on an NVIDIA GRID K520 GPU from AWS; the performance you get depends on your GPU. The computational times are plotted on the following figure. We see that the CPU version of the <code class="literal">cor()</code> correlation function scales super linearly with the number of records. On the other hand, the GPU version shows a very small increase in computation time as the number of records increases, as evident from the almost flat red line.</p><div class="mediaobject"><img src="graphics/9263OS_05_01.jpg" alt="Fast statistical modeling in R with gputools"/><div class="caption"><p>Computational times of calculating Kendall correlations in GPU versus CPU</p></div></div><p>Next, we will <a id="id167" class="indexterm"/>run timing comparisons for a few other functions available in <code class="literal">gputools</code>: linear model (<code class="literal">gpuLm()</code>), generalized linear model (<code class="literal">gpuGlm()</code>), distance matrix <a id="id168" class="indexterm"/>calculation (<code class="literal">gpuDist()</code>), and matrix multiplication (<code class="literal">gpuMatMult()</code>). The datasets used for these tests have 1,000 observations and 1,000 variables, except for <code class="literal">gpuLm</code>, where a dataset with 10,000 observations and 1,000 variables is used. The <code class="literal">microbenchmark()</code> function is used to compare the execution times of the CPU and GPU versions of these algorithms:</p><div class="informalexample"><pre class="programlisting">library(microbenchmark)
A &lt;- matrix(rnorm(1e7), 1e4, 1e3)
A_df &lt;- data.frame(A)
A_df$label &lt;- rnorm(1e4)
microbenchmark(lm(label~., data=A_df),
               gpuLm(label~., data=A_df), 
               times=30L)
## Unit: seconds
##             expr       min        lq    median        uq
##    lm(formu,...) 18.153458 18.228136 18.264231 18.274046
## gpuLm(formu,...)  9.310136  9.424152  9.467559  9.507548
##      max
## 18.32938
## 10.25019

A &lt;- matrix(rnorm(1e6), 1e3, 1e3)
A_df &lt;- data.frame(A)
A_df$label &lt;- rbinom(1e3, size=1, prob=0.5)
microbenchmark(glm(label~., data=A_df, family=binomial), 
               gpuGlm(label~., data=A_df, family=binomial),
               times=30L)
## Unit: seconds
##              expr      min       lq   median       uq      max
##    glm(formu,...) 23.64777 23.68571 23.73135 23.82055 24.07102    
## gpuGlm(formu,...) 15.14166 15.30302 15.42091 15.50876 15.71143    

microbenchmark(dist(A), gpuDist(A), times=30L)
## Unit: milliseconds
##       expr        min         lq   median        uq        max 
##    dist(A) 11113.4842 11141.2138 11167.81 11194.852 11287.2603    
## gpuDist(A)   191.1447   203.6862   222.79   229.408   239.9834    

B &lt;- matrix(rnorm(1E6), 1E3, 1E3)
microbenchmark(A%*%B, gpuMatMult(A, B), times=30L)
## Unit: milliseconds
##           expr       min        lq    median        uq
##          A%*%B 921.68863 934.64234 945.74926 955.33485 
## gpuMatMult(A,B) 33.28274  33.59875  33.70138  37.35431 
##        max
## 1029.75887
##   38.29123</pre></div><p>The test results <a id="id169" class="indexterm"/>show the power of using GPU computations in R. However, just like any other parallel program, not all functions will enjoy faster performance when executed in a GPU. For <a id="id170" class="indexterm"/>example, running the correlation comparison for Pearson's correlations (by changing the <code class="literal">method</code> argument from <code class="literal">kendall</code> to <code class="literal">pearson</code>), the GPU performs slower than the CPU as shown in the upcoming figure. Due to the extra sorting operations required by the Kendall correlation, it is known to be much more computationally intensive than the Pearson correlation (our benchmark here shows that computing the Kendall correlation is hundreds of times slower than computing the Pearson correlation). However, it seems that this implementation of the Kendall correlation algorithm is well suited for the highly parallel architecture of the GPU, resulting in the performance gains we saw in the first example of this chapter. The algorithm for computing the Pearson correlation, on the other hand, suffers when we switch from CPU to GPU suggesting that it is not suited for the GPU's architecture. It is difficult to pinpoint exactly the reason for the differences in performance between the two algorithms without studying the details of the underlying CUDA code and the GPU's architecture. Before deciding to use GPUs for a specific task, it is best to benchmark the relative performance of GPUs versus CPUs, as we have done here:</p><div class="mediaobject"><img src="graphics/9263OS_05_02.jpg" alt="Fast statistical modeling in R with gputools"/><div class="caption"><p>Computation times of calculating Pearson correlations in GPU versus CPU</p></div></div><p>In general, these <a id="id171" class="indexterm"/>factors can affect the GPU's performance:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">GPUs work best <a id="id172" class="indexterm"/>for data parallel problems (see <a class="link" href="ch08.html" title="Chapter 8. Multiplying Performance with Parallel Computing">Chapter 8</a>, <span class="emphasis"><em>Multiplying Performance with Parallel Computing</em></span> for a definition of data parallelism). They are <a id="id173" class="indexterm"/>not suited for tasks that require large amounts of synchronization between threads.</li><li class="listitem" style="list-style-type: disc">GPU's performance depends on the amount of data transferred between the main memory (RAM) and the GPU's memory, because the connection between the RAM and GPU's memory has a low bandwidth. Good GPU programming should minimize this data transfer.</li></ul></div><p>Addressing these factors requires programming in the low-level GPU interfaces provided by RCUDA or OpenCL. Other efforts are being made to minimize the efforts required by programmers to optimize a CUDA or OpenCL code. For example, to address the RAM-GPU memory bottleneck, AMD has released a GPU that combines the RAM and GPU memories in a single card.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec33"/>Summary</h1></div></div></div><p>In this chapter, we learned how to speed certain computations in R by leveraging GPUs. Given that most computers today come with a GPU, this gives a quick opportunity to improve the performance of R programs. This is especially true with the growing number of packages that interface R with GPUs. Some, such as <code class="literal">gputools</code>, require no knowledge of CUDA or OpenCL at all. GPUs do not guarantee improved performance for all tasks.</p><p>In the next chapter, we will turn our attention to address RAM-related issues in R programs.</p></div></body></html>