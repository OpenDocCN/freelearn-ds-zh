<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch13"/>Chapter 13. Data Around Us</h1></div></div></div><p>Spatial data, also <a class="indexterm" id="id898"/>known as<a class="indexterm" id="id899"/> geospatial data, identifies geographic locations, such as natural or constructed features around us. Although all observations have some spatial content, such as the location of the observation, but this is out of most data analysis tools' range due to the complex nature of spatial information; alternatively, the spatiality might not be that interesting (at first sight) in the given research topic.</p><p>On the other hand, analyzing spatial data can reveal some very important underlying structures of the data, and it is well worth spending time visualizing the differences and similarities between close or far data points.</p><p>In this chapter, we are going to help with this and will use a variety of R packages to:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Retrieve geospatial information from the Internet</li><li class="listitem" style="list-style-type: disc">Visualize points and polygons on a map</li><li class="listitem" style="list-style-type: disc">Compute some spatial statistics</li></ul></div><div><div><div><div><h1 class="title"><a id="ch13lvl1sec88"/>Geocoding</h1></div></div></div><p>As in the previous <a class="indexterm" id="id900"/>chapters, we will use the <code class="literal">hflights</code> dataset to demonstrate how one can deal with data bearing spatial information. To this end, let's aggregate our dataset, just like we did in <a class="link" href="ch12.html" title="Chapter 12. Analyzing Time-series">Chapter 12</a>, <em>Analyzing Time-series</em>, but instead of generating daily data, let's view the aggregated characteristics of the airports. For the sake of performance, we will use the <code class="literal">data.table</code> package again as introduced in <a class="link" href="ch03.html" title="Chapter 3. Filtering and Summarizing Data">Chapter 3</a>, <em>Filtering and Summarizing Data</em> and <a class="link" href="ch04.html" title="Chapter 4. Restructuring Data">Chapter 4</a>, <em>Restructuring Data</em>:</p><div><pre class="programlisting">
<strong>&gt; library(hflights)</strong>
<strong>&gt; library(data.table)</strong>
<strong>&gt; dt &lt;- data.table(hflights)[, list(</strong>
<strong>+     N         = .N,</strong>
<strong>+     Cancelled = sum(Cancelled),</strong>
<strong>+     Distance  = Distance[1],</strong>
<strong>+     TimeVar   = sd(ActualElapsedTime, na.rm = TRUE),</strong>
<strong>+     ArrDelay  = mean(ArrDelay, na.rm = TRUE)) , by = Dest]</strong>
</pre></div><p>So we have loaded and then immediately transformed the <code class="literal">hfights</code> dataset to a <code class="literal">data.table</code> object. At the same time, we aggregated by the destination of the flights to compute:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The number of rows</li><li class="listitem" style="list-style-type: disc">The number of cancelled flights</li><li class="listitem" style="list-style-type: disc">The distance</li><li class="listitem" style="list-style-type: disc">The standard deviation of the elapsed time of the flights</li><li class="listitem" style="list-style-type: disc">The arithmetic mean of the delays</li></ul></div><p>The resulting <a class="indexterm" id="id901"/>R object looks like this:</p><div><pre class="programlisting">
<strong>&gt; str(dt)</strong>
<strong>Classes 'data.table' and 'data.frame': 116 obs. of 6 variables:</strong>
<strong> $ Dest     : chr  "DFW" "MIA" "SEA" "JFK" ...</strong>
<strong> $ N        : int  6653 2463 2615 695 402 6823 4893 5022 6064 ...</strong>
<strong> $ Cancelled: int  153 24 4 18 1 40 40 27 33 28 ...</strong>
<strong> $ Distance : int  224 964 1874 1428 3904 305 191 140 1379 862 ...</strong>
<strong> $ TimeVar  : num  10 12.4 16.5 19.2 15.3 ...</strong>
<strong> $ ArrDelay : num  5.961 0.649 9.652 9.859 10.927 ...</strong>
<strong> - attr(*, ".internal.selfref")=&lt;externalptr&gt;</strong>
</pre></div><p>So we have 116 observations all around the world and five variables describing those. Although this seems to be a spatial dataset, we have no geospatial identifiers that a computer can understand per se, so let's fetch <a class="indexterm" id="id902"/>the <em>geocodes</em> of these airports from the Google Maps API<a class="indexterm" id="id903"/> via the<a class="indexterm" id="id904"/> <code class="literal">ggmap</code> package. First, let's see how it works when we are looking for the geo-coordinates of Houston:</p><div><pre class="programlisting">
<strong>&gt; library(ggmap)</strong>
<strong>&gt; (h &lt;- geocode('Houston, TX'))</strong>
<strong>Information from URL : http://maps.googleapis.com/maps/api/geocode/json?address=Houston,+TX&amp;sensor=false</strong>
<strong>       lon      lat</strong>
<strong>1 -95.3698 29.76043</strong>
</pre></div><p>So the <code class="literal">geocode</code> function can return the matched <a class="indexterm" id="id905"/>latitude and longitude<a class="indexterm" id="id906"/> of the string we sent to Google. Now let's do the very same thing for all flight destinations:</p><div><pre class="programlisting">
<strong>&gt; dt[, c('lon', 'lat') := geocode(Dest)]</strong>
</pre></div><p>Well, this took some time as we had to make 116 separate queries to the Google Maps API. Please note that Google limits you to 2,500 queries a day without authentication, so do not run this on a large dataset. There is a helper function in the package, called <code class="literal">geocodeQueryCheck</code>, which can be used to check the remaining number of free queries for the day.</p><p>Some of the <a class="indexterm" id="id907"/>methods and functions that we plan to use in some later sections of this chapter do not support <code class="literal">data.table</code>, so let's fall back to the traditional <code class="literal">data.frame</code> format and also print the structure of the current object:</p><div><pre class="programlisting">
<strong>&gt; str(setDF(dt))</strong>
<strong>'data.frame':  116 obs. of  8 variables:</strong>
<strong> $ Dest     : chr  "DFW" "MIA" "SEA" "JFK" ...</strong>
<strong> $ N        : int  6653 2463 2615 695 402 6823 4893 5022 6064 ...</strong>
<strong> $ Cancelled: int  153 24 4 18 1 40 40 27 33 28 ...</strong>
<strong> $ Distance : int  224 964 1874 1428 3904 305 191 140 1379 862 ...</strong>
<strong> $ TimeVar  : num  10 12.4 16.5 19.2 15.3 ...</strong>
<strong> $ ArrDelay : num  5.961 0.649 9.652 9.859 10.927 ...</strong>
<strong> $ lon      : num  -97 136.5 -122.3 -73.8 -157.9 ...</strong>
<strong> $ lat      : num  32.9 34.7 47.5 40.6 21.3 ...</strong>
</pre></div><p>This was pretty quick and easy, wasn't it? Now that we have the longitude and latitude values of all the airports, we can try to show these points on a map.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch13lvl1sec89"/>Visualizing point data in space</h1></div></div></div><p>For the first time, let's<a class="indexterm" id="id908"/> keep it simple and load some package-bundled polygons<a class="indexterm" id="id909"/> as the base map. To this end, we will use the <code class="literal">maps</code> package. After loading it, we use the <code class="literal">map</code> function to render the polygons of the United States of America, add a title, and then some points for the airports and also for Houston with a slightly modified symbol:</p><div><pre class="programlisting">
<strong>&gt; library(maps)</strong>
<strong>&gt; map('state')</strong>
<strong>&gt; title('Flight destinations from Houston,TX')</strong>
<strong>&gt; points(h$lon, h$lat, col = 'blue', pch = 13)</strong>
<strong>&gt; points(dt$lon, dt$lat, col = 'red', pch = 19)</strong>
</pre></div><div><img alt="Visualizing point data in space" src="img/2028OS_13_01.jpg"/></div><p>And showing the<a class="indexterm" id="id910"/> airport names on the plot is pretty easy as well: we <a class="indexterm" id="id911"/>can use the well-known functions <a class="indexterm" id="id912"/>from the <a class="indexterm" id="id913"/>base <code class="literal">graphics</code> package. Let's pass the three character names as labels to the text function with a slightly increased <em>y</em> value to shift the preceding text the previously rendered data points:</p><div><pre class="programlisting">
<strong>&gt; text(dt$lon, dt$lat + 1, labels = dt$Dest, cex = 0.7)</strong>
</pre></div><div><img alt="Visualizing point data in space" src="img/2028OS_13_02.jpg"/></div><p>Now, we can also specify the<a class="indexterm" id="id914"/> color of the points to be rendered. This feature can be <a class="indexterm" id="id915"/>used to plot our first meaningful map to <a class="indexterm" id="id916"/>highlight the number of flights in 2011 to different parts of the USA:</p><div><pre class="programlisting">
<strong>&gt; map('state')</strong>
<strong>&gt; title('Frequent flight destinations from Houston,TX')</strong>
<strong>&gt; points(h$lon, h$lat, col = 'blue', pch = 13)</strong>
<strong>&gt; points(dt$lon, dt$lat, pch = 19,</strong>
<strong>+   col = rgb(1, 0, 0, dt$N / max(dt$N)))</strong>
<strong>&gt; legend('bottomright', legend = round(quantile(dt$N)), pch = 19, </strong>
<strong>+   col = rgb(1, 0, 0, quantile(dt$N) / max(dt$N)), box.col = NA)</strong>
</pre></div><div><img alt="Visualizing point data in space" src="img/2028OS_13_03.jpg"/></div><p>So the intensity of<a class="indexterm" id="id917"/> red shows the number of flights to the given<a class="indexterm" id="id918"/> points (airports); the values range from 1 to almost 10,000. Probably it would be more meaningful to compute these values on a state level as there are many airports, very close to each other, which might be better aggregated at a higher administrative area level. To this end, we load the polygon of the states, match the points of interest (airports) with the overlaying polygons (states), and render the polygons as a thematic map instead of points, like we did on the previous pages.</p></div>
<div><div><div><div><h1 class="title"><a id="ch13lvl1sec90"/>Finding polygon overlays of point data</h1></div></div></div><p>We already have all the<a class="indexterm" id="id919"/> data we need to identify the parent state <a class="indexterm" id="id920"/>of each airport. The <code class="literal">dt</code> dataset includes the geo-coordinates of the locations, and we managed to render the states as polygons with the <code class="literal">map</code> function. Actually, this latter function can return the underlying dataset without rendering a plot:</p><div><pre class="programlisting">
<strong>&gt; str(map_data &lt;- map('state', plot = FALSE, fill = TRUE))</strong>
<strong>List of 4</strong>
<strong> $ x    : num [1:15599] -87.5 -87.5 -87.5 -87.5 -87.6 ...</strong>
<strong> $ y    : num [1:15599] 30.4 30.4 30.4 30.3 30.3 ...</strong>
<strong> $ range: num [1:4] -124.7 -67 25.1 49.4</strong>
<strong> $ names: chr [1:63] "alabama" "arizona" "arkansas" "california" ...</strong>
<strong> - attr(*, "class")= chr "map"</strong>
</pre></div><p>So we have around 16,000 points describing the boundaries of the US states, but this map data is more detailed than we actually need (see for example the name of the polygons starting with Washington):</p><div><pre class="programlisting">
<strong>&gt; grep('^washington', map_data$names, value = TRUE)</strong>
<strong>[1] "washington:san juan island" "washington:lopez island"</strong>
<strong>[3] "washington:orcas island"    "washington:whidbey island"</strong>
<strong>[5] "washington:main"</strong>
</pre></div><p>In short, the <a class="indexterm" id="id921"/>non-connecting parts of a state are defined <a class="indexterm" id="id922"/>as separate polygons. To this end, let's save a list of the state names without the string after the colon:</p><div><pre class="programlisting">
<strong>&gt; states &lt;- sapply(strsplit(map_data$names, ':'), '[[', 1)</strong>
</pre></div><p>We will use this list as the basis of aggregation from now on. Let's transform this <code class="literal">map</code> dataset into another class of object, so that we can use the powerful features<a class="indexterm" id="id923"/> of the <code class="literal">sp</code> package. We will use <a class="indexterm" id="id924"/>the <code class="literal">maptools</code> package to do this transformation:</p><div><pre class="programlisting">
<strong>&gt; library(maptools)</strong>
<strong>&gt; us &lt;- map2SpatialPolygons(map_data, IDs = states,</strong>
<strong>+    proj4string = CRS("+proj=longlat +datum=WGS84"))</strong>
</pre></div><div><div><h3 class="title"><a id="note68"/>Note</h3><p>An alternative way of getting the state polygons might be to directly load those instead of transforming from other data formats as described earlier. To this end, you may find <a class="indexterm" id="id925"/>the <code class="literal">raster</code> package especially useful to download free map <strong>shapefiles</strong>
<a class="indexterm" id="id926"/> from <code class="literal">gadm.org</code> via the <code class="literal">getData</code> function. Although these maps are way too detailed for such a simple task, you can always simplify those—for example, with the <code class="literal">gSimplify</code> function of<a class="indexterm" id="id927"/> the <code class="literal">rgeos</code> package.</p></div></div><p>So we have just created an object called <code class="literal">us</code>, which includes the polygons of <code class="literal">map_data</code> for each state with the given<a class="indexterm" id="id928"/> <strong>projection</strong>. This object can be shown on a map just like we did previously, although you should use the general <code class="literal">plot</code> method instead of the <code class="literal">map</code> function:</p><div><pre class="programlisting">
<strong>&gt; plot(us)</strong>
</pre></div><div><img alt="Finding polygon overlays of point data" src="img/2028OS_13_04.jpg"/></div><p>Besides this, however, the <code class="literal">sp</code> package <a class="indexterm" id="id929"/>supports so many powerful features! For <a class="indexterm" id="id930"/>example, it's very easy to identify the overlay <a class="indexterm" id="id931"/>polygons of the provided points via the <code class="literal">over</code> function. As this function name conflicts with the one found in the <code class="literal">grDevices</code> package, it's better to refer to the function along with the namespace using a double colon:</p><div><pre class="programlisting">
<strong>&gt; library(sp)</strong>
<strong>&gt; dtp &lt;- SpatialPointsDataFrame(dt[, c('lon', 'lat')], dt,</strong>
<strong>+   proj4string = CRS("+proj=longlat +datum=WGS84"))</strong>
<strong>&gt; str(sp::over(us, dtp))</strong>
<strong>'data.frame':  49 obs. of  8 variables:</strong>
<strong> $ Dest     : chr  "BHM" "PHX" "XNA" "LAX" ...</strong>
<strong> $ N        : int  2736 5096 1172 6064 164 NA NA 2699 3085 7886 ...</strong>
<strong> $ Cancelled: int  39 29 34 33 1 NA NA 35 11 141 ...</strong>
<strong> $ Distance : int  562 1009 438 1379 926 NA NA 1208 787 689 ...</strong>
<strong> $ TimeVar  : num  10.1 13.61 9.47 15.16 13.82 ...</strong>
<strong> $ ArrDelay : num  8.696 2.166 6.896 8.321 -0.451 ...</strong>
<strong> $ lon      : num  -86.8 -112.1 -94.3 -118.4 -107.9 ...</strong>
<strong> $ lat      : num  33.6 33.4 36.3 33.9 38.5 ...</strong>
</pre></div><p>What happened here? First, we passed the coordinates and the whole dataset to the <code class="literal">SpatialPointsDataFrame</code> function, which stored our data as spatial points with the given longitude and latitude values. Next, we called the <code class="literal">over</code> function to left-join the values of <code class="literal">dtp</code> to the US states.</p><div><div><h3 class="title"><a id="note69"/>Note</h3><p>An alternative way of identifying the state of a given airport is to ask for more detailed information from the Google Maps API. By changing the default <code class="literal">output</code> argument of the <code class="literal">geocode</code> function, we can get all address components for the matched spatial object, which of course includes the state as well. Look for example at the following code snippet:</p><div><pre class="programlisting">
<strong>geocode('LAX','all')$results[[1]]$address_components</strong>
</pre></div><p>Based on this, you might want to get a similar output for all airports and filter the list for the short name of the state. The <code class="literal">rlist</code> package <a class="indexterm" id="id932"/>would be extremely useful in this task, as it offers some very convenient ways of manipulating lists in R.</p></div></div><p>The only problem <a class="indexterm" id="id933"/>here is that we matched only one<a class="indexterm" id="id934"/> airport to the states, which is definitely not okay. See for example the fourth column in the earlier output: it shows <code class="literal">LAX</code> as the matched airport for <code class="literal">California</code> (returned by <code class="literal">states[4]</code>), although there are many others there as well.</p><p>To overcome this issue, we can do at least two things. First, we can use the <code class="literal">returnList</code> argument of the <code class="literal">over</code> function to return all matched rows of <code class="literal">dtp</code>, and we will then post-process that data:</p><div><pre class="programlisting">
<strong>&gt; str(sapply(sp::over(us, dtp, returnList = TRUE),</strong>
<strong>+   function(x) sum(x$Cancelled)))</strong>
<strong> Named int [1:49] 51 44 34 97 23 0 0 35 66 149 ...</strong>
<strong> - attr(*, "names")= chr [1:49] "alabama" "arizona" "arkansas" ...</strong>
</pre></div><p>So we created and called an anonymous function that will <code class="literal">sum</code> up the <code class="literal">Cancelled</code> values of the <code class="literal">data.frame</code> in each element of the list returned by <code class="literal">over</code>.</p><p>Another, probably cleaner, approach is to redefine <code class="literal">dtp</code> to only include the related values and pass a function to <code class="literal">over</code> to do the summary:</p><div><pre class="programlisting">
<strong>&gt; dtp &lt;- SpatialPointsDataFrame(dt[, c('lon', 'lat')],</strong>
<strong>+    dt[, 'Cancelled', drop = FALSE],</strong>
<strong>+    proj4string = CRS("+proj=longlat +datum=WGS84"))</strong>
<strong>&gt; str(cancels &lt;- sp::over(us, dtp, fn = sum))</strong>
<strong>'data.frame':  49 obs. of  1 variable:</strong>
<strong> $ Cancelled: int  51 44 34 97 23 NA NA 35 66 149 ...</strong>
</pre></div><p>Either way, we have a vector to merge back to the US state names:</p><div><pre class="programlisting">
<strong>&gt; val &lt;- cancels$Cancelled[match(states, row.names(cancels))]</strong>
</pre></div><p>And to update<a class="indexterm" id="id935"/> all missing values to zero (as the number of<a class="indexterm" id="id936"/> cancelled flights in a state without any airport is not missing data, but exactly zero for sure):</p><div><pre class="programlisting">
<strong>&gt; val[is.na(val)] &lt;- 0</strong>
</pre></div></div>
<div><div><div><div><h1 class="title"><a id="ch13lvl1sec91"/>Plotting thematic maps</h1></div></div></div><p>Now we have <a class="indexterm" id="id937"/>everything to create our first <em>thematic</em> map. Let's pass the <code class="literal">val</code> vector to the previously used <code class="literal">map</code> function (or <code class="literal">plot</code> it using the <code class="literal">us</code> object), specify a plot title, add a blue point for Houston, and then create a <a class="indexterm" id="id938"/>legend, which shows the quantiles of the overall number of cancelled flights as a reference:</p><div><pre class="programlisting">
<strong>&gt; map("state", col = rgb(1, 0, 0, sqrt(val/max(val))), fill = TRUE)</strong>
<strong>&gt; title('Number of cancelled flights from Houston to US states')</strong>
<strong>&gt; points(h$lon, h$lat, col = 'blue', pch = 13)</strong>
<strong>&gt; legend('bottomright', legend = round(quantile(val)),</strong>
<strong>+   fill = rgb(1, 0, 0, sqrt(quantile(val)/max(val))), box.col = NA)</strong>
</pre></div><div><img alt="Plotting thematic maps" src="img/2028OS_13_05.jpg"/></div><p>Please note that, instead of a linear scale, we have decided to compute the square root of the relative values to define the intensity of the fill color, so that we can visually highlight the differences between the states. This was necessary as most flight cancellations happened in Texas (<code class="literal">748</code>), and there were no more than 150 cancelled flights in any other state (with the average being around 45).</p><div><div><h3 class="title"><a id="note71"/>Note</h3><p>You can also easily load ESRI shape files or other geospatial vector data formats into R as points or polygons with a bunch of packages already discussed and a few others as well, such as the <code class="literal">maptools</code>, <code class="literal">rgdal</code>, <code class="literal">dismo</code>, <code class="literal">raster</code>, or <code class="literal">shapefile</code> packages.</p></div></div><p>Another, probably <a class="indexterm" id="id939"/>easier, way to generate country-level thematic maps, especially<a class="indexterm" id="id940"/> choropleth maps, is to load the <code class="literal">rworldmap</code> package <a class="indexterm" id="id941"/>made by Andy South, and rely on the convenient <code class="literal">mapCountryData</code> function.</p></div>
<div><div><div><div><h1 class="title"><a id="ch13lvl1sec92"/>Rendering polygons around points</h1></div></div></div><p>Besides thematic <a class="indexterm" id="id942"/>maps, another really useful way of presenting<a class="indexterm" id="id943"/> spatial data is to draw artificial polygons around the data points based on the data values. This is especially useful if there is no available polygon shape file to be used to generate a thematic map.</p><p>A <a class="indexterm" id="id944"/>level plot, <a class="indexterm" id="id945"/>contour plot, or <a class="indexterm" id="id946"/>isopleths, might be an already familiar design from tourist maps, where the altitude of the mountains is represented by a line drawn around the center of the hill at the very same levels. This is a very smart approach having maps present the height of hills—projecting this third dimension onto a 2-dimensional image.</p><p>Now let's try to replicate this design by considering our data points as mountains on the otherwise flat map. We already know the heights and exact geo-coordinates of the geometric centers of these hills (airports); the only challenge here is to draw the actual shape of these objects. In other words:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Are these <em>mountains</em> connected?</li><li class="listitem" style="list-style-type: disc">How steep are the <em>hillsides</em>?</li><li class="listitem" style="list-style-type: disc">Should we consider any underlying spatial effects in the data? In other words, can we actually render these as <em>mountains</em> with a 3D shape instead of plotting independent points in space?</li></ul></div><p>If the answer for the last question is positive, then we can start trying to answer the other questions by fine-tuning the plot parameters. For now, let's simply suppose that there is a spatial effect in the underlying data, and it makes sense to visualize the data in such a way. Later, we will have the chance to disprove or support this statement either by analyzing the generated plots, or by building some geo-spatial models—some of these will be discussed later, in the <em>Spatial Statistics</em> section.</p><div><div><div><div><h2 class="title"><a id="ch13lvl2sec76"/>Contour lines</h2></div></div></div><p>First, let's expand our <a class="indexterm" id="id947"/>data points into a matrix with<a class="indexterm" id="id948"/> the <code class="literal">fields</code> package. The size of the resulting R object is defined arbitrarily but, for the given number of rows and columns, which should be a lot higher to generate higher resolution images, 256 is a good start:</p><div><pre class="programlisting">
<strong>&gt; library(fields)</strong>
<strong>&gt; out &lt;- as.image(dt$ArrDelay, x = dt[, c('lon', 'lat')],</strong>
<strong>+   nrow = 256, ncol = 256)</strong>
</pre></div><p>The <code class="literal">as.image</code> function generates a special R object, which in short includes a 3‑dimensional matrix-like data structure, where the <em>x</em> and <em>y</em> axes represent the longitude and latitude ranges of the original data respectively. To simplify this even more, we have a matrix with 256 rows and 256 columns, where each of those represents a discrete value evenly distributed between the lowest and highest values of the latitude and longitude. And on the <em>z</em> axis, we have the <code class="literal">ArrDelay</code> values—which are in most cases of course missing:</p><div><pre class="programlisting">
<strong>&gt; table(is.na(out$z))</strong>
<strong>FALSE  TRUE </strong>
<strong>  112 65424</strong>
</pre></div><p>What does this matrix <a class="indexterm" id="id949"/>look like? It's better to see what we have at the moment:</p><div><pre class="programlisting">
<strong>&gt; image(out)</strong>
</pre></div><div><img alt="Contour lines" src="img/2028OS_13_06.jpg"/></div><p>Well, this does not seem to be useful at all. What is shown there? We rendered the <em>x</em> and <em>y</em> dimensions of the matrix with <em>z</em> colors here, and most tiles of this map are empty due to the high amount of missing values in <em>z</em>. Also, it's pretty straightforward now that the dataset includes many airports outside the USA as well. How does it look if we focus only on the USA?</p><div><pre class="programlisting">
<strong>&gt; image(out, xlim = base::range(map_data$x, na.rm = TRUE),</strong>
<strong>+            ylim = base::range(map_data$y, na.rm = TRUE))</strong>
</pre></div><div><img alt="Contour lines" src="img/2028OS_13_07.jpg"/></div><div><div><h3 class="title"><a id="note72"/>Note</h3><p>An alternative and more elegant approach to rendering only the US part of the matrix would be to drop the non-US airports from the database before actually creating the <code class="literal">out</code> R object. Although we will continue with this example for didactic purposes, with real data make sure that you concentrate on the target subset of your data instead of trying to smooth and model unrelated data points as well.</p></div></div><p>A lot better! So <a class="indexterm" id="id950"/>we have our data points as a tile, now let's try to identify the slope of these mountain peaks, to be able to render them on a future map. This can be done by smoothing the matrix:</p><div><pre class="programlisting">
<strong>&gt; look &lt;- image.smooth(out, theta = .5)</strong>
<strong>&gt; table(is.na(look$z))</strong>
<strong>FALSE  TRUE </strong>
<strong>14470 51066</strong>
</pre></div><p>As can be seen in the<a class="indexterm" id="id951"/> preceding table, this algorithm successfully eliminated many missing values from the matrix. The <code class="literal">image.smooth</code> function basically reused our initial data point values in the neighboring tiles, and computed some kind of average for the conflicting overrides. This smoothing algorithm results in the following arbitrary map, which does not respect any political or geographical boundaries:</p><div><pre class="programlisting">
<strong>&gt; image(look)</strong>
</pre></div><div><img alt="Contour lines" src="img/2028OS_13_08.jpg"/></div><p>It would be really nice to plot these artificial polygons along with the administrative boundaries, so let's clear out all cells that do not belong to the territory of the USA. We will use the <code class="literal">point.in.polygon</code> function from the<a class="indexterm" id="id952"/> <code class="literal">sp</code> package to do so:</p><div><pre class="programlisting">
<strong>&gt; usa_data &lt;- map('usa', plot = FALSE, region = 'main')</strong>
<strong>&gt; p &lt;- expand.grid(look$x, look$y)</strong>
<strong>&gt; library(sp)</strong>
<strong>&gt; n &lt;- which(point.in.polygon(p$Var1, p$Var2,</strong>
<strong>+  usa_data$x, usa_data$y) == 0)</strong>
<strong>&gt; look$z[n] &lt;- NA</strong>
</pre></div><p>In a nutshell, we have loaded the main polygon of the USA without any sub-administrative areas, and verified our cells in the <code class="literal">look</code> object, if those are overlapping the polygon. Then we simply reset the value of the cell, if not.</p><p>The next step is to<a class="indexterm" id="id953"/> render the boundaries of the USA, plot our smoothed contour plot, then add some eye-candy in the means of the US states and, the main point of interest, the airport:</p><div><pre class="programlisting">
<strong>&gt; map("usa")</strong>
<strong>&gt; image(look, add = TRUE)</strong>
<strong>&gt; map("state", lwd = 3, add = TRUE)</strong>
<strong>&gt; title('Arrival delays of flights from Houston')</strong>
<strong>&gt; points(dt$lon, dt$lat, pch = 19, cex = .5)</strong>
<strong>&gt; points(h$lon, h$lat, pch = 13)</strong>
</pre></div><div><img alt="Contour lines" src="img/2028OS_13_09.jpg"/></div><p>Now this is pretty neat, isn't it?</p></div><div><div><div><div><h2 class="title"><a id="ch13lvl2sec77"/>Voronoi diagrams</h2></div></div></div><p>An alternative way of <a class="indexterm" id="id954"/>visualizing point data with polygons is to generate <em>Voronoi</em> cells between them. In short, the Voronoi map partitions the space into regions around the data points by aligning all parts of the map to one of the regions to minimize the distance from the central data points. This is extremely easy to interpret, and also to implement in R. The <code class="literal">deldir</code> package<a class="indexterm" id="id955"/> provides a function with the very same name for Delaunay triangulation:</p><div><pre class="programlisting">
<strong>&gt; library(deldir)</strong>
<strong>&gt; map("usa")</strong>
<strong>&gt; plot(deldir(dt$lon, dt$lat), wlines = "tess", lwd = 2,</strong>
<strong>+   pch = 19, col = c('red', 'darkgray'), add = TRUE)</strong>
</pre></div><div><img alt="Voronoi diagrams" src="img/2028OS_13_10.jpg"/></div><p>Here, we represented<a class="indexterm" id="id956"/> the airports with red dots, as we did before, but also added the Dirichlet tessellation (Voronoi cells) rendered as dark-gray dashed lines. For more options on how to fine-tune the results, see the <code class="literal">plot.deldir</code> method.</p><p>In the next section, let's see how to improve this plot by adding a more detailed background map to it.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch13lvl1sec93"/>Satellite maps</h1></div></div></div><p>There are many R packages on CRAN that can <a class="indexterm" id="id957"/>fetch data from Google Maps, Stamen, Bing, or OpenStreetMap—even some of the packages that we have previously used in this chapter, such as the<a class="indexterm" id="id958"/> <code class="literal">ggmap</code> package, can do this. Similarly, the <code class="literal">dismo</code> package also comes<a class="indexterm" id="id959"/> with both geo-coding and Google Maps API integration capabilities, and there are some other packages focused on that latter, such as <a class="indexterm" id="id960"/>the <code class="literal">RgoogleMaps</code> package.</p><p>Now we will use <a class="indexterm" id="id961"/>the <code class="literal">OpenStreetMap</code> package, mainly because it supports not only the awesome OpenStreetMap database back-end, but also a bunch of other formats as well. For example, we can render really nice terrain maps via Stamen:</p><div><pre class="programlisting">
<strong>&gt; library(OpenStreetMap)</strong>
<strong>&gt; map &lt;- openmap(c(max(map_data$y, na.rm = TRUE),</strong>
<strong>+                  min(map_data$x, na.rm = TRUE)),</strong>
<strong>+                c(min(map_data$y, na.rm = TRUE),</strong>
<strong>+                  max(map_data$x, na.rm = TRUE)),</strong>
<strong>+                type = 'stamen-terrain')</strong>
</pre></div><p>So we defined the left upper and right lower corners of the map we need, and also specified the map style to be a satellite map. As the data by default arrives from the remote servers with the Mercator projections, we first have to transform that to WGS84 (we used this previously), so that we can render the points and polygons on the top of the fetched map:</p><div><pre class="programlisting">
<strong>&gt; map &lt;- openproj(map,</strong>
<strong>+   projection = '+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs')</strong>
</pre></div><p>And showtime at last:</p><div><pre class="programlisting">
<strong>&gt; plot(map)</strong>
<strong>&gt; plot(deldir(dt$lon, dt$lat), wlines = "tess", lwd = 2,</strong>
<strong>+   col = c('red', 'black'), pch = 19, cex = 0.5, add = TRUE)</strong>
</pre></div><div><img alt="Satellite maps" src="img/2028OS_13_11.jpg"/></div><p>This seems to <a class="indexterm" id="id962"/>be a lot better compared to the outline map we created previously. Now you can try some other map styles as well, such as <code class="literal">mapquest-aerial</code>, or some of the really nice-looking <code class="literal">cloudMade</code> designs.</p></div>
<div><div><div><div><h1 class="title"><a id="ch13lvl1sec94"/>Interactive maps</h1></div></div></div><p>Besides being able to use <a class="indexterm" id="id963"/>Web-services to download map tiles for the background of the maps created in R, we can also rely on some of those to generate truly interactive maps. One of the best known related services is the <a class="indexterm" id="id964"/>Google Visualization API, which provides a platform for hosting visualizations made by the community; you can also use it to share maps you've created with others.</p><div><div><div><div><h2 class="title"><a id="ch13lvl2sec78"/>Querying Google Maps</h2></div></div></div><p>In R, you<a class="indexterm" id="id965"/> can access<a class="indexterm" id="id966"/> this API <a class="indexterm" id="id967"/>via the <code class="literal">googleVis</code> package written and maintained by Markus Gesmann and Diego de Castillo. Most functions of the package generate HTML and JavaScript code that we can directly view in a Web browser as an <code class="literal">SVG</code> object with the <code class="literal">base</code> plot function; alternatively, we can integrate them in a Web page, for example via the IFRAME HTML tag.</p><p>The <code class="literal">gvisIntensityMap</code> function takes a <code class="literal">data.frame</code> with country ISO or USA state codes and the actual data to create a simple intensity map. We will use the <code class="literal">cancels</code> dataset we created in the <em>Finding Polygon Overlays of Point Data</em> section, but before that, we have to do some data transformations. Let's add the state name as a new column to the <code class="literal">data.frame</code>, and replace the missing values with zero:</p><div><pre class="programlisting">
<strong>&gt; cancels$state &lt;- rownames(cancels)</strong>
<strong>&gt; cancels$Cancelled[is.na(cancels$Cancelled)] &lt;- 0</strong>
</pre></div><p>Now it's time to <a class="indexterm" id="id968"/>load the package and pass the data along with a few extra parameters, signifying<a class="indexterm" id="id969"/> that we want to generate a state-level US map:</p><div><pre class="programlisting">
<strong>&gt; library(googleVis)</strong>
<strong>&gt; plot(gvisGeoChart(cancels, 'state', 'Cancelled',</strong>
<strong>+                   options = list(</strong>
<strong>+                       region      = 'US',</strong>
<strong>+                       displayMode = 'regions', </strong>
<strong>+                       resolution  = 'provinces')))</strong>
</pre></div><div><img alt="Querying Google Maps" src="img/2028OS_13_12.jpg"/></div><p>The package also offers opportunities to query the Google Map API via the <code class="literal">gvisMap</code> function. We will use this feature to render the airports from the <code class="literal">dt</code> dataset as points on a Google Map with an auto-generated tooltip of the variables.</p><p>But first, as usual, we have to do some data transformations again. The location argument of the <code class="literal">gvisMap</code> function takes the latitude and longitude values separated by a colon:</p><div><pre class="programlisting">
<strong>&gt; dt$LatLong &lt;- paste(dt$lat, dt$lon, sep = ':')</strong>
</pre></div><p>We also have to<a class="indexterm" id="id970"/> generate the tooltips as a new variable, which can be done easily with<a class="indexterm" id="id971"/> an <code class="literal">apply</code> call. We will concatenate the variable names and actual values separated by a HTML line break:</p><div><pre class="programlisting">
<strong>&gt; dt$tip &lt;- apply(dt, 1, function(x)</strong>
<strong>+                  paste(names(dt), x, collapse = '&lt;br/ &gt;'))</strong>
</pre></div><p>And now we just pass these arguments to the function for an instant interactive map:</p><div><pre class="programlisting">
<strong>&gt; plot(gvisMap(dt, 'LatLong', tipvar = 'tip'))</strong>
</pre></div><div><img alt="Querying Google Maps" src="img/2028OS_13_13.jpg"/></div><p>Another nifty feature of <a class="indexterm" id="id972"/>the <code class="literal">googleVis</code> package is that you can easily merge the different visualizations into one by using the <code class="literal">gvisMerge</code> function. The use of this function is quite simple: specify any two <code class="literal">gvis</code> objects you want to merge, and also whether they are to be placed horizontally or vertically.</p></div><div><div><div><div><h2 class="title"><a id="ch13lvl2sec79"/>JavaScript mapping libraries</h2></div></div></div><p>The great success of the <a class="indexterm" id="id973"/>trending JavaScript data visualization libraries is only partly due to their great design. I suspect other factors also contribute to the general spread of such tools: it's very easy to create and deploy full-blown data models, especially since the release and on-going development of Mike Bostock's <a class="indexterm" id="id974"/>D3.js.</p><p>Although there are also many really useful and smart R packages to interact directly with D3 and topojson<a class="indexterm" id="id975"/> (see for example my R user activity compilation<a class="indexterm" id="id976"/> at <a class="ulink" href="http://bit.ly/countRies">http://bit.ly/countRies</a>). Now we will only focus on how to use Leaflet— probably the most used JavaScript library for interactive maps.</p><p>What I truly love in R is that there are many packages wrapping other tools, so that R users can rely on only one programming language, and we can easily use C++ programs and Hadoop MapReduce jobs or build JavaScript-powered dashboards without actually knowing anything about the underlying technology. This is especially true when it comes to Leaflet!</p><p>There are at least two very nice packages that can generate a Leaflet plot from the R console, without a single line of JavaScript. The <code class="literal">Leaflet</code> reference class of the <code class="literal">rCharts</code> package was<a class="indexterm" id="id977"/> developed by Ramnath Vaidyanathan, and includes some methods to create a new object, set the viewport and zoom level, add some points or polygons to the map, and then render or print the generated HTML and JavaScript code to the console or to a file.</p><p>Unfortunately, this package is not on CRAN yet, so you will have to install it from GitHub:</p><div><pre class="programlisting">
<strong>&gt; devtools::install_github('ramnathv/rCharts')</strong>
</pre></div><p>As a quick example, let's generate a Leaflet map of the airports with some tooltips, like we did with the Google Maps API in the previous section. As the <code class="literal">setView</code> method expects numeric geo-coordinates as the center of the map, we will use Kansas City's airport as a reference:</p><div><pre class="programlisting">
<strong>&gt; library(rCharts)</strong>
<strong>&gt; map &lt;- Leaflet$new()</strong>
<strong>&gt; map$setView(as.numeric(dt[which(dt$Dest == 'MCI'),</strong>
<strong>+   c('lat', 'lon')]), zoom = 4)</strong>
<strong>&gt; for (i in 1:nrow(dt))</strong>
<strong>+     map$marker(c(dt$lat[i], dt$lon[i]), bindPopup = dt$tip[i])</strong>
<strong>&gt; map$show()</strong>
</pre></div><div><img alt="JavaScript mapping libraries" src="img/2028OS_13_14.jpg"/></div><p>Similarly, RStudio's <code class="literal">leaflet</code> package<a class="indexterm" id="id978"/> and the more general <code class="literal">htmlwidgets</code> package <a class="indexterm" id="id979"/>also provide some easy ways to generate <a class="indexterm" id="id980"/>JavaScript-powered data visualizations. Let's load the library and define the steps one by one using the pipe operator<a class="indexterm" id="id981"/> from the <code class="literal">magrittr</code> package, which is pretty standard for all packages created or inspired by RStudio or Hadley Wickham:</p><div><pre class="programlisting">
<strong>&gt; library(leaflet)</strong>
<strong>&gt; leaflet(us) %&gt;%</strong>
<strong>+   addProviderTiles("Acetate.terrain") %&gt;%</strong>
<strong>+   addPolygons() %&gt;%</strong>
<strong>+   addMarkers(lng = dt$lon, lat = dt$lat, popup = dt$tip)</strong>
</pre></div><div><img alt="JavaScript mapping libraries" src="img/2028OS_13_15.jpg"/></div><p>I especially like this <a class="indexterm" id="id982"/>preceding map, as we can load a third-party satellite map in the background, then render the states as polygons; we also added the original data points along with some useful tooltips on the very same map with literally a one-line R command. We could even color the state polygons based on the aggregated results we computed in the previous sections! Ever tried to do the same in Java?</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch13lvl1sec95"/>Alternative map designs</h1></div></div></div><p>Besides being able to use<a class="indexterm" id="id983"/> third-party tools, another main reason why I tend to use R for all my data analysis tasks is that R is extremely powerful in creating custom data exploration, visualization, and modeling designs.</p><p>As an example, let's create a flow-map based on our data, where we will highlight the flights from Houston based on the number of actual and cancelled flights. We will use lines and circles to render these two variables on a 2-dimensional map, and we will also add a contour plot in the background based on the average time delay.</p><p>But, as usual, let's do some data transformations first! To keep the number of flows at a minimal level, let's get rid of the airports outside the USA at last:</p><div><pre class="programlisting">
<strong>&gt; dt &lt;- dt[point.in.polygon(dt$lon, dt$lat,</strong>
<strong>+                           usa_data$x, usa_data$y) == 1, ]</strong>
</pre></div><p>We will need<a class="indexterm" id="id984"/> the <code class="literal">diagram</code> package (to render curved arrows from Houston to the destination airports) and<a class="indexterm" id="id985"/> the <code class="literal">scales</code> package to create transparent colors:</p><div><pre class="programlisting">
<strong>&gt; library(diagram)</strong>
<strong>&gt; library(scales)</strong>
</pre></div><p>Then, let's render the contour map described in the <em>Contour Lines</em> section:</p><div><pre class="programlisting">
<strong>&gt; map("usa")</strong>
<strong>&gt; title('Number of flights, cancellations and delays from Houston')</strong>
<strong>&gt; image(look, add = TRUE)</strong>
<strong>&gt; map("state", lwd = 3, add = TRUE)</strong>
</pre></div><p>And then add a curved line from <a class="indexterm" id="id986"/>Houston to each of the destination airports, where the width of the line represents the number of cancelled flights and the diameter of the target circles shows the number of actual flights:</p><div><pre class="programlisting">
<strong>&gt; for (i in 1:nrow(dt)) {</strong>
<strong>+   curvedarrow(</strong>
<strong>+     from       = rev(as.numeric(h)),</strong>
<strong>+     to         = as.numeric(dt[i, c('lon', 'lat')]),</strong>
<strong>+     arr.pos    = 1,</strong>
<strong>+     arr.type   = 'circle',</strong>
<strong>+     curve      = 0.1,</strong>
<strong>+     arr.col    = alpha('black', dt$N[i] / max(dt$N)),</strong>
<strong>+     arr.length = dt$N[i] / max(dt$N),</strong>
<strong>+     lwd        = dt$Cancelled[i] / max(dt$Cancelled) * 25,</strong>
<strong>+     lcol       = alpha('black',</strong>
<strong>+                    dt$Cancelled[i] / max(dt$Cancelled)))</strong>
<strong>+ }</strong>
</pre></div><div><img alt="Alternative map designs" src="img/2028OS_13_16.jpg"/></div><p>Well, this chapter ended up <a class="indexterm" id="id987"/>being about visualizing spatial data, and not really about analyzing spatial data by fitting models, filtering raw data, and looking for spatial effects. In the last section of the chapter, let's see how one can start using analytical approaches with spatial data.</p></div>
<div><div><div><div><h1 class="title"><a id="ch13lvl1sec96"/>Spatial statistics</h1></div></div></div><p>Most exploratory data analysis<a class="indexterm" id="id988"/> projects dealing with spatial data start by looking for, and potentially filtering, spatial autocorrelation. In simple terms, this means that we are looking for spatial effects in the data—for instance, the similarities of some data points can be (partly) explained by the short distance between them; further points seem to differ a lot more. There is nothing surprising in this statement; probably all of you agree with this. But how can we test this on real data with analytical tools?</p><p>
<em>Moran's I index</em> is a <a class="indexterm" id="id989"/>well-known and generally used measure to test whether spatial autocorrelation is present or not in the variable of interest. This is a quite simple statistical test with the null hypothesis that there is no spatial autocorrelation in the dataset.</p><p>With the current data structure we have, probably the easiest way to compute Moran's I is to load the<a class="indexterm" id="id990"/> <code class="literal">ape</code> package, and pass the similarity matrix along with the variable of interest to the <code class="literal">Moran.I</code> function. First, let's compute this similarity matrix by the inverse of the Euclidian distance matrix:</p><div><pre class="programlisting">
<strong>&gt; dm &lt;- dist(dt[, c('lon', 'lat')])</strong>
<strong>&gt; dm &lt;- as.matrix(dm)</strong>
<strong>&gt; idm &lt;- 1 / dm</strong>
<strong>&gt; diag(idm) &lt;- 0</strong>
<strong>&gt; str(idm)</strong>
<strong> num [1:88, 1:88] 0 0.0343 0.1355 0.2733 0.0467 ...</strong>
<strong> - attr(*, "dimnames")=List of 2</strong>
<strong>  ..$ : chr [1:88] "1" "3" "6" "7" ...</strong>
<strong>  ..$ : chr [1:88] "1" "3" "6" "7" ...</strong>
</pre></div><p>Then let's replace all <a class="indexterm" id="id991"/>possible missing values (because the number of flights can be one as well, resulting in zero variance) in the <code class="literal">TimeVar</code> column, and let's see if there is any spatial autocorrelation in the variance of the actual elapsed time of the flights:</p><div><pre class="programlisting">
<strong>&gt; dt$TimeVar[is.na(dt$TimeVar)] &lt;- 0</strong>
<strong>&gt; library(ape)</strong>
<strong>&gt; Moran.I(dt$TimeVar, idm)</strong>
<strong>$observed</strong>
<strong>[1] 0.1895178</strong>

<strong>$expected</strong>
<strong>[1] -0.01149425</strong>

<strong>$sd</strong>
<strong>[1] 0.02689139</strong>

<strong>$p.value</strong>
<strong>[1] 7.727152e-14</strong>
</pre></div><p>This was pretty easy, wasn't it? Based on the returned <code class="literal">P</code> value, we can reject the null hypothesis, and the <code class="literal">0.19</code> Moran's I suggests that the variation in the elapsed flight time is affected by the location of the destination airports, probably due to the very different distances.</p><p>A reverse dependency <a class="indexterm" id="id992"/>of the previously mentioned <code class="literal">sp</code> package, the <code class="literal">spdep</code> package<a class="indexterm" id="id993"/> can also compute this index, although we have to first transform the similarity matrix into a list object:</p><div><pre class="programlisting">
<strong>&gt; library(spdep)</strong>
<strong>&gt; idml &lt;- mat2listw(idm)</strong>
<strong>&gt; moran.test(dt$TimeVar, idml)</strong>

<strong>  Moran's I test under randomisation</strong>

<strong>data:  dt$TimeVar  </strong>
<strong>weights: idml  </strong>

<strong>Moran I statistic standard deviate = 1.7157, p-value = 0.04311</strong>
<strong>alternative hypothesis: greater</strong>
<strong>sample estimates:</strong>
<strong>Moran I statistic       Expectation          Variance </strong>
<strong>      0.108750656      -0.011494253       0.004911818</strong>
</pre></div><p>Although the test results are <a class="indexterm" id="id994"/>similar to the previous run, and we can reject the null hypothesis of zero spatial autocorrelation in the data, the Moran's I index and the <code class="literal">P</code> values are not identical. This is mainly due to the fact that the <code class="literal">ape</code> package used weight matrix for the computation, while the <code class="literal">moran.test</code> function was intended to be used with polygon data, as it requires the neighbor lists of the data. Well, as our example included point data, this is not a clean-cut solution. Another main difference between the approaches is that the <code class="literal">ape</code> package <a class="indexterm" id="id995"/>uses normal approximation, while <code class="literal">spdep</code> implements randomization. But this difference is still way too high, isn't it?</p><p>Reading the function documentation reveals that we can improve the <code class="literal">spdep</code> approach: when converting the <code class="literal">matrix</code> into a <code class="literal">listw</code> object, we can specify the actual type of the originating matrix. In our case, as we are using the inverse distance matrix, a row-standardized style seems more appropriate:</p><div><pre class="programlisting">
<strong>&gt; idml &lt;- mat2listw(idm, style = "W")</strong>
<strong>&gt; moran.test(dt$TimeVar, idml)</strong>

<strong>  Moran's I test under randomisation</strong>

<strong>data:  dt$TimeVar  </strong>
<strong>weights: idml  </strong>
<strong>Moran I statistic standard deviate = 7.475, p-value = 3.861e-14</strong>
<strong>alternative hypothesis: greater</strong>
<strong>sample estimates:</strong>
<strong>Moran I statistic       Expectation          Variance </strong>
<strong>     0.1895177587     -0.0114942529      0.0007231471</strong>
</pre></div><p>Now the differences between this and the <code class="literal">ape</code> results are in an acceptable range, right?</p><p>Unfortunately, this <a class="indexterm" id="id996"/>section cannot cover related questions or other statistical methods dealing with spatial data, but there are many really useful books out there dedicated to the topic. Please be sure to check the <em>Appendix</em> at the end of the book for some suggested titles.</p></div>
<div><div><div><div><h1 class="title"><a id="ch13lvl1sec97"/>Summary</h1></div></div></div><p>Congratulations, you have just finished the last systematic chapter of the book! Here, we focused on how to analyze spatial data mainly with data visualization tools.</p><p>Now let's see how we can combine the methods learned in the previous chapters. In the final part of the book, we will analyze the R community with various data science tools. If you liked this chapter, I am sure you will enjoy the final one as well.</p></div></body></html>