<html><head></head><body><div><div><div></div>
		</div>
		<div><h1 id="_idParaDest-204"><a id="_idTextAnchor212"/>7. Advanced Web Scraping and Data Gathering</h1>
		</div>
		<div><p class="callout-heading">Overview</p>
			<p class="callout">This chapter will introduce you to the concepts of advanced web scraping and data gathering. It will enable you to use <code>requests</code> and <code>BeautifulSoup</code> to read various web pages and gather data from them. You can perform read operations on XML files and the web using an <strong class="bold">Application Program Interface</strong> (<strong class="bold">API</strong>). You can use regex techniques to scrape useful information from a large and messy text corpus. By the end of this chapter, you will have learned how to gather data from web pages, XML files, and APIs.</p>
			<h1 id="_idParaDest-205"><a id="_idTextAnchor213"/>Introduction</h1>
			<p>The previous chapter covered how to create a successful data wrangling pipeline. In this chapter, we will build a web scraper that can be used by a data wrangling professional in their daily tasks using all of the techniques that we have learned so far. This chapter builds on the foundation of <code>BeautifulSoup</code> and introduces various methods for scraping a web page and using an API to gather data.</p>
			<p>In today's connected world, one of the most valued and widely used skills for a data wrangling professional is the ability to extract and read data from web pages and databases hosted on the web. Most organizations host data on the cloud (public or private), and the majority of web microservices these days provide some kind of API for external users to access data. Let's take a look at the following diagram:</p>
			<div><div><img src="img/B15780_07_01.jpg" alt="Figure 7.1: Data wrangling HTTP request and an XML/JSON reply&#13;&#10;" width="660" height="364"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1: Data wrangling HTTP request and an XML/JSON reply</p>
			<p>As we can see in the diagram, to fetch data from a web server or a database, we initiate <code>XML/JSON</code>. It is necessary that, as a data wrangling engineer, you know about the structure of web pages and the Python libraries so that you are able to extract data from a web page. The <strong class="bold">World Wide Web</strong> (<strong class="bold">WWW</strong>) is an ever-growing, ever-changing universe, where different data exchange protocols and formats are used. A few of these are widely used and have become standard.</p>
			<p>Python comes equipped with built-in modules, such as <code>urllib 3</code>, which can initiate HTTP requests and receive data from the cloud. However, these modules operate at a low level and require a deep knowledge of HTTP protocols, encoding, and requests.</p>
			<h1 id="_idParaDest-206"><a id="_idTextAnchor214"/>The Requests and BeautifulSoup Libraries</h1>
			<p>We will take advantage of two Python libraries in this chapter: <code>requests</code> and <code>BeautifulSoup</code>. To avoid dealing with HTTP methods at a lower level, we will use the <code>requests</code> library. It is an API built on top of pure Python web utility libraries, which makes placing HTTP requests easy and intuitive.</p>
			<p><code>BeautifulSoup </code>is one of the most popular HTML parser packages. It parses the HTML content you pass on and builds a detailed tree of all the tags and markup within the page for easy and intuitive traversal. This tree can be used by a programmer to look for certain markup elements (for example, a table, a hyperlink, or a blob of text within a particular <code>div</code> ID) to scrape useful data.</p>
			<p>We are going to do a couple of exercises in order to demonstrate how to use the <code>requests</code> library and decode the contents of the response received when data is fetched from the server.</p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor215"/>Exercise 7.01: Using the Requests Library to Get a Response from the Wikipedia Home Page</h2>
			<p>In this exercise, we will use the <code>requests</code> library to extract data from a Wikipedia web page. The Wikipedia home page consists of many elements and scripts, all of which are a mix of HTML, CSS, and JavaScript code blocks. While reading from the home page of Wikipedia (<a href="https://en.wikipedia.org/wiki/Main_Page">https://en.wikipedia.org/wiki/Main_Page</a>), the code or markup elements/texts might not be very useful. Therefore, we will peel off the layers of HTML/CSS/JavaScript to pry away the information we are interested in. Let's follow these steps:</p>
			<ol>
				<li>Open a new Jupyter Notebook and import the <code>requests</code> library:<pre>import requests</pre></li>
				<li>Assign the home page URL to a variable, <code>wiki_home</code>:<pre>wiki_home = "https://en.wikipedia.org/wiki/Main_Page"</pre></li>
				<li>Use the <code>get</code> method from the <code>requests</code> library to get a response from this page:<pre>response = requests.get(wiki_home)
response</pre><p>The output is as follows:</p><pre>&lt;Response [200]&gt;</pre></li>
				<li>To find out more about the <code>response</code> object, enter the following code:<pre>type(response)</pre><p>The output is as follows:</p><pre>requests.models.Response</pre></li>
			</ol>
			<p>As we can see, the output is an object that models the data structure of an HTTP response. It is defined in the <code>requests</code> library.</p>
			<p>The web is an extremely dynamic place. For example, it is quite possible that the home page of Wikipedia will have changed by the time somebody uses your code, or that a particular web server will be not be running and your request will fail. If you proceed to write more complex and elaborate code without checking the status of your request, then all that subsequent work will be fruitless.</p>
			<p>A web page request generally comes back with various numeric codes. They are the standard HTTP response codes. The following table shows the common codes you may encounter:</p>
			<div><div><img src="img/B15780_07_02.jpg" alt="Figure 7.2: HTTP response codes&#13;&#10;" width="1553" height="706"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2: HTTP response codes</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3d7qmK0%20">https://packt.live/3d7qmK0.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3hEKbff">https://packt.live/3hEKbff</a>.</p>
			<p>In the next exercise, we are going to write a function to check the return code and print out messages as needed. These kinds of small helper/utility functions are incredibly useful for complex projects.</p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor216"/>Exercise 7.02: Checking the Status of the Web Request</h2>
			<p>In this exercise, we will write a small utility function called <code>status_check</code> to check the status of the response received from the server. Our goal here is to check the status code and flag an error/no-error situation by writing a function. We will start by getting into the habit of writing small functions to accomplish small modular tasks, instead of writing long scripts, which are hard to debug and track. Let's follow these steps:</p>
			<ol>
				<li value="1">Open a new Jupyter notebook and create a <code>status_check</code> function as follows:<pre>def status_check(r):
    if r.status_code==200:
        print("Success!")
        return 1
    else:
        print("Failed!")
        return -1</pre><p>Note that, along with printing the appropriate message, we are returning either <code>1</code> or <code>-1</code> from this function. This is important because in the code that utilizes this function, we will be able to examine this return value to find out whether the request was a success or a failure.</p></li>
				<li>Import the <code>requests</code> library:<pre>import requests</pre></li>
				<li>Assign the home page URL to a variable, <code>wiki_home</code>:<pre>wiki_home = "https://en.wikipedia.org/wiki/Main_Page"</pre></li>
				<li>Use the <code>get</code> method from the <code>requests</code> library to get a response from this page:<pre>response = requests.get(wiki_home)</pre></li>
				<li>Pass the response object to the <code>status_check</code> function to examine the status of the response: <pre>status_check(response)</pre><p>The output is as follows:</p><pre>Success!
1</pre><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3hHcf1k%20">https://packt.live/3hHcf1k.</a></p><p class="callout">You can also run this example online at <a href="https://packt.live/3hDUhNp">https://packt.live/3hDUhNp</a>.</p></li>
			</ol>
			<p>In this chapter, for more complex programming activity, we will proceed only if we get <code>1</code> as the return value of the <code>status_check</code> function, that is, we will write a conditional statement to check the return value and then execute the subsequent code based on it.</p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor217"/>Checking the Encoding of a Web Page</h2>
			<p>We can also write a utility function to check the encoding of a web page. Various encodings are possible with any HTML document, although the most popular is <code>UTF-8</code>. Some of the most popular encodings are <code>ASCII</code>, <code>Unicode</code>, and <code>UTF-8</code>. <code>ASCII</code> is the simplest, but it cannot capture the complex symbols used in various spoken and written languages all over the world, so <code>UTF-8</code> has become the almost universal standard in web development these days.</p>
			<p>When we run this function on the Wikipedia home page, we get back the particular encoding type that's used for that page. This function, like the previous one, takes the <code>response</code> object as an argument and returns a value:</p>
			<pre>def encoding_check(r):
    return (r.encoding)</pre>
			<p>Check the response:</p>
			<pre>response = requests.get("https://en.wikipedia.org/wiki/Main_Page")
encoding_check(response)</pre>
			<p>The output is as follows:</p>
			<pre>'UTF-8'</pre>
			<p>Here, <code>'UTF-8'</code> denotes the most popular character encoding scheme that's used in the digital medium and on the web today. It employs variable-length encoding with <code>1-4</code> bytes, thereby representing all Unicode characters in various languages around the world.</p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor218"/>Exercise 7.03: Decoding the Contents of a Response and Checking Its Length</h2>
			<p>In this exercise, we will create a function to get the Wikipedia page's contents as a blob of text or as a string object that Python can process afterward. We will first initiate a request to get the contents of a Wikipedia page and store the data in a <code>response</code> object. We will then decode this <code>response</code> object. To do this, follow these steps:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook and import the <code>requests</code> library:<pre>import requests</pre></li>
				<li>Assign the home page URL to a variable, <code>wiki_home</code>:<pre>wiki_home = "https://en.wikipedia.org/wiki/Main_Page"</pre></li>
				<li>Use the <code>get</code> method from the <code>requests</code> library to get a response from this page:<pre>response = requests.get(wiki_home)</pre></li>
				<li>Write a utility function to decode the contents of the response:<pre>def encoding_check(r):
    return (r.encoding)
def decode_content(r,encoding):
    return (r.content.decode(encoding))
contents = decode_content(response,encoding_check(response))</pre></li>
				<li>Check the type of the decoded object to see what type of data we are finally getting:<pre>type(contents)</pre><p>The output is as follows:</p><pre>str</pre><p>We finally got a string object by reading the HTML page.</p></li>
				<li>Check the length of the object using the <code>len</code> function:<pre>len(contents)</pre><p>The output is as follows:</p><pre>74182</pre><p class="callout-heading">Note</p><p class="callout">This output is variable and is susceptible to change depending on the updates made to the Wikipedia web page.</p></li>
				<li>Use the following code to print the first <code>10,000</code> characters of this string. It will look something similar to this:<pre>contents[:10000]</pre><p>The output is as follows:</p><div><img src="img/B15780_07_03.jpg" alt="Figure 7.3: Partial output showing a mixed blob of HTML markup tags, text, &#13;&#10;and element names and properties&#13;&#10;" width="818" height="199"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.3: Partial output showing a mixed blob of HTML markup tags, text, and element names and properties</p>
			<p>Obviously, this is a mixed blob of various HTML markup tags, text, and element names/properties. We cannot hope to extract meaningful information from this that could be used for efficient analysis without using sophisticated functions or methods. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2BfmUQq%20">https://packt.live/2BfmUQq.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2UW2L8L">https://packt.live/2UW2L8L</a>.</p>
			<p>Fortunately, the <code>BeautifulSoup</code> library or <code>bs4</code> library provides such methods, and we will see how to use them in the following exercise.</p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor219"/>Exercise 7.04: Extracting Readable Text from a BeautifulSoup Object</h2>
			<p>In this exercise, we will create a utility function, <code>decode_content</code>, to decode the response received after initiating a request to the Wikipedia web page. We will use the <code>BeautifulSoup</code> library on the <code>response</code> object to further process it so that it becomes easier for us to extract any meaningful information from it. <code>BeautifulSoup</code> has a <code>text</code> method, which can be used to extract text. Let's follow these steps:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook and import the <code>requests</code> library:<pre>import requests</pre></li>
				<li>Assign the home page URL to a variable, <code>wiki_home</code>:<pre>wiki_home = "https://en.wikipedia.org/wiki/Main_Page"</pre></li>
				<li>Use the <code>get</code> method from the <code>requests</code> library to get a response from this page:<pre>response = requests.get(wiki_home)</pre></li>
				<li>Write a utility function to decode the contents of the response:<pre>def encoding_check(r):
    return (r.encoding)
def decode_content(r,encoding):
    return (r.content.decode(encoding))
contents = decode_content(response,encoding_check(response))</pre></li>
				<li>Import the package and then pass on the whole string (HTML content) to a method for parsing:<pre>from bs4 import BeautifulSoup
soup = BeautifulSoup(contents, 'html.parser')</pre></li>
				<li>Execute the following code in your notebook:<pre>txt_dump=soup.text</pre></li>
				<li>Find the type of the <code>txt_dmp</code>:<pre>type(txt_dump)</pre><p>The output is as follows:</p><pre>str</pre></li>
				<li>Find the length of the <code>txt_dmp</code>:<pre>len(txt_dump)</pre><p>The output is as follows:</p><pre>15326</pre><p class="callout-heading">Note</p><p class="callout">This output is variable and is susceptible to change depending on the updates made to the Wikipedia web page.</p><p>Now, the length of the text dump is much smaller than the raw HTML string's length. This is because the <code>bs4</code> library has parsed through the HTML and extracted only human-readable text for further processing.</p></li>
				<li>Print the initial portion of this text:<pre>print(txt_dump[10000:11000])</pre><p>You will see something similar to the following:</p><div><img src="img/B15780_07_04.jpg" alt="Figure 7.4: Output showing the initial portion of text&#13;&#10;" width="798" height="258"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.4: Output showing the initial portion of text</p>
			<p>In this exercise, we were introduced to the main interface of <code>BeautifulSoup</code> or <code>bs4</code> and we also saw how we can parse a raw string containing HTML and other types of data using <code>bs4</code> and retain only HTML-related data.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Cky5rt%20">https://packt.live/2Cky5rt.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Bj2Xbr">https://packt.live/2Bj2Xbr</a>.</p>
			<p>Web pages are becoming more and more dynamic with more and more diverse types of elements and content in them. As a data wrangling engineer, you will have to deal with the growing complexity and the heterogeneous nature of data. So, knowing what we just saw will often give you a big advantage.</p>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor220"/>Extracting Text from a Section</h2>
			<p>Now, let's move on to more exciting data wrangling tasks. If you open the Wikipedia home page, <a href="https://en.wikipedia.org/wiki/Main_Page">https://en.wikipedia.org/wiki/Main_Page,</a> you are likely to see a section called <code>From today's featured article</code>. This is an excerpt from the day's featured article, which is randomly selected and promoted on the home page. This article can also change throughout the day:</p>
			<div><div><img src="img/B15780_07_05.jpg" alt="Figure 7.5: Sample Wikipedia page highlighting the &quot;From today's featured article&quot; section&#13;&#10;" width="1223" height="777"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5: Sample Wikipedia page highlighting the "From today's featured article" section</p>
			<p>You need to extract the text from this section. There are several ways to accomplish this task. We will go through a simple and intuitive method for doing so here.</p>
			<p>First, we try to identify two indices – the <em class="italic">start index</em> and <em class="italic">end index</em> of the line string – which demarcate the start and end of the text we are interested in extracting or reading. In the next screenshot, the indices are shown:</p>
			<div><div><img src="img/B15780_07_06.jpg" alt="Figure 7.6: Wikipedia page highlighting the text to be extracted&#13;&#10;" width="1277" height="816"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6: Wikipedia page highlighting the text to be extracted</p>
			<p>The following code accomplishes the extraction:</p>
			<pre>idx1=txt_dump.find("From today's featured article")
idx2=txt_dump.find("Recently featured")
print(txt_dump[idx1+len("From today's featured article"):idx2])</pre>
			<p>Note that we have to add the length of the <code>From today's featured article</code> string to <code>idx1</code> and then pass that as the starting index. This is because <code>idx1</code> finds where the <code>From today's featured article</code> string starts.</p>
			<p>It prints out something like this (this is a sample output):</p>
			<div><div><img src="img/B15780_07_07.jpg" alt="Figure 7.7: The extracted text&#13;&#10;" width="716" height="174"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7: The extracted text</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The output you get will vary based on the current featured article.</p>
			<p>As you can see, the <code>BeautifulSoup</code> library provides an efficient technique to read data from a source. It will also be interesting to know the events that occurred on a particular day.</p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor221"/>Extracting Important Historical Events that Happened on Today's Date</h2>
			<p>Next, we will try to extract the text corresponding to the important historical events that happened on today's date. This can generally be found in the bottom-right corner, as shown in the following screenshot:</p>
			<div><div><img src="img/B15780_07_08.jpg" alt="Figure 7.8: Wikipedia page highlighting the On this day section&#13;&#10;" width="1139" height="641"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8: Wikipedia page highlighting the On this day section</p>
			<p>So, can we apply the same technique as we did for <code>From today's featured article</code>? Apparently not, because there is text just below where we want our extraction to end, which is not fixed, unlike in the previous case. Note that, in the previous section, the fixed string <code>Recently featured</code> occurs at the exact place where we want the extraction to stop, so we could use it in our code. However, we cannot do that in this case, and the reason for this is illustrated in the following screenshot:</p>
			<div><div><img src="img/B15780_07_09.jpg" alt="Figure 7.9: Wikipedia page highlighting the text to be extracted&#13;&#10;" width="1212" height="594"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.9: Wikipedia page highlighting the text to be extracted</p>
			<p>So, in this section, we just want to find out what the text looks like around the main content we are interested in. For that, we must find out the start of the <code>On this day</code> string and print out the next 1,000 characters using the following code:</p>
			<pre>idx3=txt_dump.find("On this day")
print(txt_dump[idx3+len("On this day"):idx3+len("On this day")\
               +1000])</pre>
			<p>The output looks as follows:</p>
			<div><div><img src="img/B15780_07_10.jpg" alt="Figure 7.10: Output of the On this day section from Wikipedia&#13;&#10;" width="1223" height="586"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.10: Output of the On this day section from Wikipedia</p>
			<p>As we can see, there is a bit of unwanted data along with the relevant information that we are really interested in reading (as shown by the arrows). To address this issue, we need to think differently and use some other methods apart from <code>BeautifulSoup</code> (and write another utility function).</p>
			<p>HTML pages are made of many markup tags, such as <code>&lt;div&gt;</code>, which denotes a division of text/images, and <code>&lt;ul&gt;</code>, which denotes lists. In the following exercise, we'll use advanced techniques from the <code>BeautifulSoup</code> library to extract relevant information from a web page.</p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor222"/>Exercise 7.05: Using Advanced BS4 Techniques to Extract Relevant Text</h2>
			<p>In this exercise, we'll take advantage of <code>BeautifulSoup</code> library techniques and extract the element that contains the text we are interested in. Let's perform the following steps:</p>
			<ol>
				<li value="1">Open the Wikipedia page using this link: <a href="https://en.wikipedia.org/wiki/Main_Page">https://en.wikipedia.org/wiki/Main_Page</a>.</li>
				<li>In the Mozilla Firefox browser, right-click and select the <code>Inspect Element</code> option (in Chrome, we do the same, except the menu option is called <code>Inspect</code>) as shown in the following screenshot:<div><img src="img/B15780_07_11.jpg" alt="Figure 7.11: Inspecting elements on Wikipedia&#13;&#10;" width="1116" height="687"/></div><p class="figure-caption">Figure 7.11: Inspecting elements on Wikipedia</p><p>As you hover over this with the mouse, you will see different portions of the page being highlighted. By doing this, it is easy to find out which precise block of markup text is responsible for the textual information we are interested in. Here, we can see that a certain <code>&lt;ul&gt;</code> block contains the text:</p><div><img src="img/B15780_07_12.jpg" alt="Figure 7.12: Identifying the HTML block that contains the text we are interested in&#13;&#10;" width="1270" height="715"/></div><p class="figure-caption">Figure 7.12: Identifying the HTML block that contains the text we are interested in</p><p>Now, it is prudent to find the <code>&lt;div&gt;</code> tag that contains this <code>&lt;ul&gt;</code> block within it. By looking around the same screen as before, we can find the <code>&lt;div&gt;</code> and its <code>ID</code>:</p><div><img src="img/B15780_07_13.jpg" alt="Figure 7.13: The &lt;ul&gt; tag containing the text&#13;&#10;" width="1387" height="548"/></div><p class="figure-caption">Figure 7.13: The &lt;ul&gt; tag containing the text</p><p>We can do similar things using <code>bs4</code> functions. </p></li>
				<li>Start off by importing <code>requests</code> and <code>BeautifulSoup</code>. Also, retrieve the contents of the Wikipedia Main Page (highlighted).<pre>import requests 
wiki_home = "<strong class="bold">https://en.wikipedia.org/wiki/Main_Page</strong>"
response = requests.get(wiki_home) 
def encoding_check(r): 
    return (r.encoding) 
def decode_content(r,encoding):
    return (r.content.decode(encoding)) 
contents = decode_content(response,encoding_check(response))
from bs4 import BeautifulSoup 
soup = BeautifulSoup(contents, 'html.parser')</pre></li>
				<li>Use the <code>find_all</code> method from <code>BeautifulSoup</code>, which scans all the tags of the HTML page (and their sub-elements) to find and extract the text associated with this particular <code>&lt;div&gt;</code> element. Create an empty list and append the text from the <code>NavigableString</code> class to this list as we traverse the page:<pre>text_list=[] #Empty list
for d in soup.find_all('div'):
    if (d.get('id')=='mp-otd'):
        for i in d.find_all('ul'):
            text_list.append(i.text)</pre><p>The <code>find_all</code> method returns a <code>NavigableString</code> class, which has a useful <code>text</code> method associated with it for extraction. Note how we are utilizing the <code>mp-otd</code> ID of the <code>&lt;div&gt;</code> element to identify it among tens of other <code>&lt;div&gt;</code> elements. Now, if we examine the <code>text_list</code> list, we will see that it has three elements.</p></li>
				<li>Print the elements separated by a marker. We will see that the text we are interested in appears as the first element:<pre>for i in text_list:
    print(i)
    print('-'*100)</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/B15780_07_14.jpg" alt="Figure 7.14: The text highlighted&#13;&#10;" width="1432" height="510"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.14: The text highlighted</p>
			<p>As we can see, it is the first element of the list that we are interested in. However, the exact position will depend on the web page. In this exercise, we were introduced to some advanced uses of <code>BeautifulSoup</code> and saw how we can extract meaningful information using its APIs.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2USTDSg%20">https://packt.live/2USTDSg.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2zGIUTG">https://packt.live/2zGIUTG</a>.</p>
			<p>Next, we will create a compact function to encapsulate some of those. Creating such functions helps us to increase the reusability of code. </p>
			<p>As we discussed before, it is always good to try to functionalize specific tasks, particularly in a web-scraping application. In the following exercise, we are going to create a compact function. </p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor223"/>Exercise 7.06: Creating a Compact Function to Extract the On this day Text from the Wikipedia Home Page</h2>
			<p>In this exercise, we are going to create a function that will take the Wikipedia URL (as a string), <a href="https://en.wikipedia.org/wiki/Main_Page">https://en.wikipedia.org/wiki/Main_Page</a>, and return the text corresponding to the <code>On this day</code> section. The benefit of a functional approach is that you can call this function from any Python script and use it anywhere in another program as a standalone module. To do this, let's follow these steps:</p>
			<ol>
				<li value="1">Create the compact <code>def</code> function. Extract the text from the <code>On this day</code> section of the Wikipedia home page, <a href="https://en.wikipedia.org/wiki/Main_Page">https://en.wikipedia.org/wiki/Main_Page</a>. Accept the Wikipedia home page URL as a string. A default URL is provided:<pre>def wiki_on_this_day(url="https://en.wikipedia.org/"\
                         "wiki/Main_Page"):
    import requests
    from bs4 import BeautifulSoup
    wiki_home = str(url)
    response = requests.get(wiki_home)</pre></li>
				<li>Create a function that will check the status of the response received from the web page: <pre>    def status_check(r):
        if r.status_code==200:
            return 1
        else:
            return -1
    def encoding_check(r): 
        return (r.encoding)
    def decode_content(r,encoding): 
        return (r.content.decode(encoding))
    status = status_check(response)
    if status==1:
        contents = decode_content(response,\
                                  encoding_check(response))
    else:
        print("Sorry could not reach the web page!")
        return -1</pre></li>
				<li> Create a <code>BeautifulSoup</code> object and read the contents of the web page:<pre>soup = BeautifulSoup(contents, 'html.parser')
text_list=[]
for d in soup.find_all('div'):
    if (d.get('id')=='mp-otd'):
        for i in d.find_all('ul'):
            text_list.append(i.text)
return (text_list[0])</pre></li>
				<li>Let's see the function in action.<pre>print(wiki_on_this_day())</pre><p>The output will be:</p><div><img src="img/B15780_07_15.jpg" alt="Figure 7.15: Output of wiki_on_this_day&#13;&#10;" width="1451" height="346"/></div><p class="figure-caption">Figure 7.15: Output of wiki_on_this_day</p></li>
				<li>Note how this function utilizes the status check and prints out an error message if the request failed. When we test this function with an intentionally incorrect URL, it behaves as expected:<pre>print(wiki_on_this_day\
      ("https://en.wikipedia.org/wiki/Main_Page1"))</pre><p>The output is as follows:</p><pre>Sorry could not reach the web page!</pre></li>
			</ol>
			<p>In this exercise, we saw how to write a function to encapsulate a lot of important things that we have learned about <code>BeautifulSoup</code>. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2YcaEJm%20">https://packt.live/2YcaEJm.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3hBS2dn">https://packt.live/3hBS2dn</a>.</p>
			<h1 id="_idParaDest-216"><a id="_idTextAnchor224"/>Reading Data from XML</h1>
			<p><strong class="bold">XML</strong> or <strong class="bold">Extensible Markup Language</strong> is a web markup language that's similar to HTML but with significant flexibility (on the part of the user) built in, such as the ability to define your own tags. It was one of the most hyped technologies in the 1990s and early 2000s. It is a meta-language, that is, a language that allows us to define other languages using its mechanics, such as RSS and MathML (a mathematical markup language widely used for web publication and the display of math-heavy technical information). XML is also heavily used in regular data exchanges over the web, and as a data wrangling professional, you should have enough familiarity with its basic features to tap into the data flow pipeline whenever you need to extract data for your project.</p>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor225"/>Exercise 7.07: Creating an XML File and Reading XML Element Objects</h2>
			<p>In this exercise, we'll create some random data and store it in XML format. We'll then read from the XML file and examine the XML-formatted data string. Let's follow these steps:</p>
			<ol>
				<li value="1">Create an XML file using the following command:<pre>data = '''
&lt;person&gt;
  &lt;name&gt;Dave&lt;/name&gt;
  &lt;surname&gt;Piccardo&lt;/surname&gt;
  &lt;phone type="intl"&gt;
    +1 742 101 4456
  &lt;/phone&gt;
  &lt;email hide="yes"&gt;
    dave.p@gmail.com&lt;/email&gt;
&lt;/person&gt;'''</pre><p>As we can see, the <code>phone</code> type is a triple-quoted string or multiline string. If you print this object, you will get the following output. This is an XML-formatted data string in a tree structure, as we will see when we parse the structure and break apart the individual parts.</p></li>
				<li>To process and wrangle with the data, we have to read it as an <code>Element</code> object using the Python XML parser engine:<pre>import xml.etree.ElementTree as ET
tree = ET.fromstring(data)
type (tree)</pre><p>The output is as follows:</p><pre>xml.etree.ElementTree.Element</pre></li>
			</ol>
			<p>In this exercise, we saw how to create an XML file, how to read an XML file, and what kind of object we can expect when we read an XML file.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/37EDwgt%20">https://packt.live/37EDwgt.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3hDwUDv">https://packt.live/3hDwUDv</a>.</p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor226"/>Exercise 7.08: Finding Various Elements of Data within a Tree (Element)</h2>
			<p>In this exercise, we will use the <code>find</code> method to search for various pieces of useful data within an XML element object and print them using the <code>text</code> method. We will also use the <code>get</code> method to extract the specific attribute we want. To do so, let's follow these steps:</p>
			<ol>
				<li value="1">Create an XML file using the following code:<pre>data = '''
&lt;person&gt;
  &lt;name&gt;Dave&lt;/name&gt;
  &lt;surname&gt;Piccardo&lt;/surname&gt;
  &lt;phone type="intl"&gt;
    +1 742 101 4456
  &lt;/phone&gt;
  &lt;email hide="yes"&gt;
    dave.p@gmail.com
  &lt;/email&gt;
  &lt;/person&gt;'''</pre></li>
				<li>To process and wrangle with the data, we have to read it as an <code>Element</code> object using the Python XML parser engine:<pre>import xml.etree.ElementTree as ET
tree = ET.fromstring(data)</pre></li>
				<li>Use the <code>find</code> method to find <code>Name</code>:<pre>print('Name:', tree.find('name').text)</pre><p>The output is as follows:</p><pre>Name: Dave</pre></li>
				<li>Use the <code>find</code> method to find <code>Surname</code>:<pre>print('Surname:', tree.find('surname').text)</pre><p>The output is as follows:</p><pre>Surname: Piccardo</pre></li>
				<li>Use the <code>find</code> method to find <code>Phone</code>. Note the use of the <code>strip</code> method to strip away any trailing spaces/blanks:<pre>print('Phone:', tree.find('phone').text.strip())</pre><p>The output will be as follows:</p><pre>Phone: +1 742 101 4456</pre></li>
				<li>Use the <code>find</code> method to find <code>email status</code> and <code>actual email</code>. Note the use of the <code>get</code> method to extract the status:<pre>print('Email hidden:', tree.find('email').get('hide'))
print('Email:', tree.find('email').text.strip())</pre><p>The output will be as follows:</p><pre>Email hidden: yes
Email: dave.p@gmail.com</pre></li>
			</ol>
			<p>In this exercise, we saw how we can use the <code>find</code> method to read the relevant information from an XML file. XML is a very diverse format of expressing data. Apart from following some ground rules, everything else is customizable in an XML document. In this exercise, we saw how to access a custom XML element and extract data from it.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3dgSoTf%20">https://packt.live/3dgSoTf.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2CjDnU9">https://packt.live/2CjDnU9</a>.</p>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor227"/>Reading from a Local XML File into an ElementTree Object</h2>
			<p>We can also read from an XML file saved locally on disk. This is a fairly common situation where a frontend web scraping module has already downloaded a lot of XML files by reading a table of data on the web and the data wrangler needs to parse through this XML file to extract meaningful pieces of numerical and textual data.</p>
			<p>We have a file associated with this chapter called <code>xml1.xml</code>. The file can be found here: <a href="https://packt.live/3e8jM7n">https://packt.live/3e8jM7n</a>.</p>
			<p>Please make sure you have the file in the same directory that you are running your Jupyter notebook from:</p>
			<pre>tree2=ET.parse('../datasets/xml1.xml')
type(tree2)</pre>
			<p>The output will be as follows: </p>
			<pre>xml.etree.ElementTree.ElementTree</pre>
			<p>Note how we use the <code>parse</code> method to read this XML file. This is slightly different than using the <code>fromstring</code> method used in the previous exercise, where we were directly reading from a <code>string</code> object. This produces an <code>ElementTree</code> object instead of a simple <code>Element</code>.</p>
			<p>The idea of building a tree-like object is the same as in the domains of computer science and programming. Let's take a look at the following diagram:</p>
			<div><div><img src="img/B15780_07_16.jpg" alt="Figure 7.16: Tree-like children nodes&#13;&#10;" width="1140" height="502"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.16: Tree-like children nodes</p>
			<p>In the preceding diagram, we can see the following:</p>
			<ul>
				<li>There is a root.</li>
				<li>There are child objects attached to the root.</li>
				<li>There could be multiple levels, that is, children of children, recursively going down.</li>
				<li>All of the nodes of the tree (root and children alike) have attributes attached to them that contain data.</li>
			</ul>
			<p>Tree traversal algorithms can be used to search for a particular attribute. If provided, special methods can be used to probe a node more deeply.</p>
			<p>Every node in the XML tree has tags and attributes. The idea is as follows:</p>
			<div><div><img src="img/B15780_07_17.jpg" alt="Figure 7.17: Finding the root and child nodes of an XML tag&#13;&#10;" width="562" height="477"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.17: Finding the root and child nodes of an XML tag</p>
			<p>As the document is organized in a tree fashion, we can use a tree traversal algorithm to go through it and visit all the children, starting at the root.</p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor228"/>Exercise 7.09: Traversing the Tree, Finding the Root, and Exploring All the Child Nodes and Their Tags and Attributes</h2>
			<p>In this exercise, we will use the tree traversal algorithm to traverse a tree, find the root, and explore all the child nodes. We will first define a variable called <code>tree2</code>, which will contain the contents of the <code>xml1.xml</code> file. Then, we will use a <code>for</code> loop to traverse through this XML document tree.</p>
			<p>The XML file can be found here: <a href="https://packt.live/3e8jM7n">https://packt.live/3e8jM7n</a>. Follow these steps:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook and define the tree:<pre>import xml.etree.ElementTree as ET
tree2=ET.parse('<strong class="bold">../datasets/xml1.xml</strong>')
type(tree2)
xml.etree.ElementTree.ElementTree</pre></li>
				<li>Explore these tags and attributes using the following code:<pre>root=tree2.getroot()
for child in root:
    print("Child:",child.tag, "| Child attribute:",\
          child.attrib)</pre><p>The output will be as follows:</p></li>
			</ol>
			<div><div><img src="img/B15780_07_18.jpg" alt="Figure 7.18: The output showing the extracted XML tags&#13;&#10;" width="1235" height="143"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.18: The output showing the extracted XML tags</p>
			<p>In this exercise, we saw how to traverse an XML document tree.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2AEgqe1%20">https://packt.live/2AEgqe1.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3ebu5re">https://packt.live/3ebu5re</a>.</p>
			<p class="callout">Remember that every XML data file could follow a different naming or structural format, but using an element tree approach puts the data into a somewhat structured flow that can be explored systematically. Still, it is best to examine the raw XML file structure once and understand (even if at a high level) the data format before attempting automatic extractions.</p>
			<p>In the following exercise, we will see how to extract relevant information from a tree.</p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor229"/>Exercise 7.10: Using the text Method to Extract Meaningful Data</h2>
			<p>In this exercise, we will be using the <code>text</code> method from the <code>BeautifulSoup</code> library to extract different types of data from a particular node of the XML document tree. We can almost think of the XML tree as a <strong class="bold">list of lists</strong> and index it accordingly. Let's follow these steps:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook and define the tree:<pre>import xml.etree.ElementTree as ET
tree2=ET.parse('<strong class="bold">../datasets/xml1.xml</strong>')
type(tree2)
xml.etree.ElementTree.ElementTree</pre></li>
				<li>Explore these tags and attributes using the following code:<pre>root=tree2.getroot()</pre></li>
				<li>Access the <code>root[0][2]</code> element by using the following code:<pre>root[0][2]</pre><p>The output will be as follows:</p><pre>&lt;Element 'gdppc' at 0x00000000051FF278&gt;</pre><p>So, this points to the <code>gdppc</code> piece of data. Here, <code>gdppc</code> is the tag and the actual GDP/per capita data is attached to this tag.</p></li>
				<li>Use the <code>text</code> method to access the data:<pre>root[0][2].text</pre><p>The output will be as follows:</p><pre>'141100'</pre></li>
				<li>Use the <code>tag</code> method to access <code>gdppc</code>:<pre>root[0][2].tag</pre><p>The output will be as follows:</p><pre>'gdppc'</pre></li>
				<li>Check <code>root[0]</code>:<pre>root[0]</pre><p>The output will be as follows:</p><pre>&lt;Element 'country1' at 0x00000000050298B8&gt;</pre></li>
				<li>Check the tag:<pre>root[0].tag</pre><p>The output will be as follows:</p><pre>'country'</pre></li>
				<li>We can use the <code>attrib</code> method to access it:<pre>root[0].attrib</pre><p>The output will be as follows:</p><pre>{'name': ' Liechtenstein '}</pre><p>So, <code>root[0]</code> is again an element, but it has a different set of tags and attributes than <code>root[0][2]</code>. This is expected because they are all part of the tree as nodes, but each is associated with a different level of data.</p></li>
			</ol>
			<p>In this exercise, we saw how to access a particular node in an XML document and how to get the data, attributes, and other related things from it. This knowledge is very valuable as a lot of data is still presented and exchanged in XML format.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3ee0mhl%20">https://packt.live/3ee0mhl.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2YMqbyz">https://packt.live/2YMqbyz</a>.</p>
			<p>This last piece of code output is interesting because it returns a dictionary object. Therefore, we can just index it by its keys. We will do that in the next exercise.</p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor230"/>Extracting and Printing the GDP/Per Capita Information Using a Loop</h2>
			<p>Now that we know how to read the GDP/per capita data and how to get a dictionary back from the tree, we can easily construct a simple dataset by running a loop over the tree:</p>
			<pre>for c in root:
    country_name=c.attrib['name']
    gdppc = int(c[2].text)
    print("{}: {}".format(country_name,gdppc))</pre>
			<p>The output is as follows:</p>
			<pre>Liechtenstein: 141100
Singapore: 59900
Panama: 13600</pre>
			<p>We can put these in a DataFrame or a CSV file to be saved to a local disk for further processing, such as a simple plot.</p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor231"/>Finding All the Neighboring Countries for Each Country and Printing Them</h2>
			<p>There are efficient search algorithms for tree structures, and one such method for XML trees is <code>findall</code>. We can use this, for this example, to find all the neighbors a country has and print them out.</p>
			<p>Why do we need to use <code>findall</code> instead of <code>find</code>? Well, because not all countries have an equal number of neighbors and <code>findall</code> searches for all the data with that tag that is associated with a particular node, and we want to traverse all of them:</p>
			<pre>for c in root:
# Find all the neighbors
    ne=c.findall('neighbor') 
    print("Neighbors\n"+"-"*25)
# Iterate over the neighbors and print their 'name' attribute
    for i in ne: 
        print(i.attrib['name'])
    print('\n')</pre>
			<p>The output looks something like this:</p>
			<div><div><img src="img/B15780_07_19.jpg" alt="Figure 7.19: The output that's generated by using findall&#13;&#10;" width="819" height="328"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.19: The output that's generated by using findall</p>
			<p>In this section, we have looked into how to use specific search algorithms in the form of pre-defined functions to traverse through an XML document and get interesting data from the nodes we visit.</p>
			<p>In the previous topic of this chapter, we learned about simple web scraping using the <code>requests</code> library. So far, we have worked with static XML data, that is, data from a local file or a string object we've scripted. Now, it is time to combine our learning and read XML data directly over the internet (as you are expected to do almost all the time).</p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor232"/>Exercise 7.11: A Simple Demo of Using XML Data Obtained by Web Scraping</h2>
			<p>In this exercise, we will obtain XML data using web scraping. We will read a cooking recipe from a website called <a href="http://www.recipepuppy.com/">http://www.recipepuppy.com/</a>, which contains aggregates links of various other sites with the recipe. Next, we will use the <code>find</code> method to extract the appropriate attribute from the XML file and display the relevant content. Let's follow these steps:</p>
			<ol>
				<li value="1">Import the necessary libraries:<pre>import requests, urllib.parse</pre><p>Read from the <a href="http://www.recipepuppy.com/">http://www.recipepuppy.com/</a> website:</p><pre>serviceurl = 'http://www.recipepuppy.com/api/?'
item = str(input('Enter the name of a food item '\
                 '(enter\'quit\' to quit): '))
url = serviceurl + urllib.parse.urlencode({'q':item})\
      +'&amp;p=1&amp;format=xml'
uh = requests.get(url)
data = uh.text
print('Retrieved', len(data), 'characters')</pre><p>This code will ask the user for input. You have to enter the name of a food item: '<code>chicken tikka</code>'.</p><p>You will get the following output:</p><pre>Enter the name of a food item (enter 'quit' to quit): chicken tikka
Retrieved 2611 characters</pre><p>If we print the last variable, <code>data</code>, we may see that it is a mix of a legitimate XML document and some junk HTML appended to it. We need to first check if that is the case. </p></li>
				<li>Use the <code>find</code> method from Python. As <code>data</code> is a string, we can simply do the following:<pre>data.find("&lt;!DOCTYPE html PUBLIC") </pre><p>This should return an integer if that string is found in <code>data</code>. Otherwise, it will return <code>–1</code>. If we get a positive integer, then we know – thanks to Python's <code>find</code> method – it is the start index of the string we are searching.</p></li>
				<li>Get only the XML part using a piece of code like the following:<pre>end_marker = data.find("&lt;!DOCTYPE html PUBLIC")
xml_text = data[:end_marker]</pre><p>However, if we do not get a positive integer, then we assume that the whole return text is valid XML and we simply set the <code>end_marker</code> as the total length of the string. Although, it is always good practice to print the raw data and check whether it is pure XML or some junk added with it.</p></li>
				<li>Write the code to get back data in XML format and read and decode it before creating an XML tree out of it:<pre>import xml.etree.ElementTree as ET
end_marker = data.find("&lt;!DOCTYPE html PUBLIC") \
             if data.find("&lt;!DOCTYPE html PUBLIC") != \
             -1 else len(data)
xml_text = data[:end_marker]
tree3 = ET.fromstring(xml_text)</pre></li>
				<li>Now, we can use another useful method, called <code>iter</code>, which basically iterates over the nodes under an element. If we traverse the tree and extract the text, we get the following output:<pre>for elem in tree3.iter():
    print(elem.text)</pre><p>The output (partially shown) is as follows:</p><div><img src="img/B15780_07_20.jpg" alt="Figure 7.20: The output that's generated by using iter&#13;&#10;" width="1045" height="689"/></div><p class="figure-caption">Figure 7.20: The output that's generated by using iter</p><p>We can use the <code>find</code> method to search for the appropriate attribute and extract its content. This is the reason it is important to scan through the XML data manually and check what attributes are used. Remember, this means scanning the raw string data, not the tree structure.</p></li>
				<li>Print the raw string data:<pre>print(data)</pre><p>The output (partially shown) is as follows:</p><p> </p><div><img src="img/B15780_07_21.jpg" alt="Figure 7. 21: output of raw string data&#13;&#10;" width="1121" height="643"/></div><p class="figure-caption">Figure 7. 21: output of raw string data</p><p>Let's examine the XML data that we received, and let's locate the <code>&lt;title&gt;</code> and <code>&lt;href&gt;</code> tags:</p><div><img src="img/B15780_07_22.jpg" alt="Figure 7.22: The output showing the extracted href tags&#13;&#10;" width="1665" height="772"/></div><p class="figure-caption">Figure 7.22: The output showing the extracted href tags</p><p>Now we know what tags to search for.</p></li>
				<li>Print the <code>&lt;title&gt;</code> and <code>&lt;href&gt;</code> hyperlinks in the XML data:<pre>for e in tree3.iter():
    h=e.find('href')
    t=e.find('title')
    if h!=None and t!=None:
        print("Receipe Link for:",t.text)
        print(h.text)
        print("-"*100)</pre><p>The final output (partially shown) is as follows:</p></li>
			</ol>
			<p> </p>
			<div><div><img src="img/B15780_07_23.jpg" alt="Figure 7.23: The output showing the final output&#13;&#10;" width="1183" height="597"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.23: The output showing the final output</p>
			<p>Note the use of <code>h!=None</code> and <code>t!=None</code>. These are difficult to anticipate when you first run this kind of code. You may get an error because some of the tags may return a <code>None</code> object, that is, they were empty for some reason in this XML data stream. This kind of situation is fairly common and cannot be anticipated beforehand. You have to use your Python knowledge and programming intuition to get around it if you receive such an error. Here, we are just checking for the type of the object and if it is not <code>None</code>, then we need to extract the text associated with it.</p>
			<p>As we can see in the output of this exercise, we're getting a nice output with links to recipes relevant to the food item we searched for. And this concludes this exercise. We have used our knowledge of making HTTP requests and getting data from the internet and mixed it with our newly acquired knowledge of parsing and traversing XML documents to accomplish a small but functional data pipeline. This kind of data pipeline building is a fairly common task for a data wrangling engineer. Now you know how to approach that.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ALU6yZ%20">https://packt.live/2ALU6yZ.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3hBSMPH">https://packt.live/3hBSMPH</a>.</p>
			<h1 id="_idParaDest-225"><a id="_idTextAnchor233"/>Reading Data from an API</h1>
			<p>Fundamentally, an API or Application Programming Interface is an interface to a computing resource (for example, an operating system or database table), which has a set of exposed methods (function calls) that allow a programmer to access particular data or internal features of that resource.</p>
			<p>A web API is, as the name suggests, an API over the web. Note that it is not a specific technology or programming framework, but an architectural concept. Think of an API like a fast-food restaurant's customer service desk. Internally, there are many food items, raw materials, cooking resources, and recipe management systems, but all you see are fixed menu items on the board and you can only interact through those items. It is like a port that can be accessed using an HTTP protocol and that's able to deliver data and services if used properly.</p>
			<p>Web APIs are extremely popular these days for all kinds of data services. In the very first chapter, we talked about how UC San Diego's data science team pulls data from Twitter feeds to analyze the occurrence of forest fires. For this, they do not go to <a href="http://twitter.com">twitter.com</a> and scrape the data by looking at HTML pages and text. Instead, they use the Twitter API, which sends this data continuously in a streaming format.</p>
			<p>Therefore, it is very important for a data wrangling professional to understand the basics of data extraction from a web API as you are extremely likely to find yourself in a situation where large quantities of data must be read through an API for processing and wrangling. These days, most APIs stream data in JSON format. In this chapter, we will use a free API to read some information about various countries around the world in JSON format and process it.</p>
			<p>We will use Python's built-in <code>urllib</code> module for this topic, along with pandas to make a DataFrame. So, we can import them now. We will also import Python's <code>json</code> module:</p>
			<pre>import urllib.request, urllib.parse
from urllib.error import HTTPError,URLError
import json
import pandas as pd</pre>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor234"/>Defining the Base URL (or API Endpoint)</h2>
			<p>First, we need to set the base URL. When we are dealing with API microservices, this is often called the <strong class="bold">API endpoint</strong>. Therefore, look for such a phrase in the web service portal you are interested in and use the endpoint URL they give you:</p>
			<pre>serviceurl = 'https://restcountries.eu/rest/v2/name/'</pre>
			<p>API-based microservices are extremely dynamic in nature in terms of what and how they offer their services and data. It can change at any time. At the time of writing, we found this particular API to be a nice choice for extracting data easily and without using authorization keys (login or special API keys).</p>
			<p>For most APIs, however, you need to have your own API key. You get that by registering with their service. A basic usage (up to a fixed number of requests or a data flow limit) is often free, but after that, you will be charged. To register for an API key, you often need to enter credit card information.</p>
			<p>We wanted to avoid all that hassle to teach you the basics and that's why we chose this example, which does not require such authorization. But, depending on what kind of data you will encounter in your work, please be prepared to learn about using an API key.</p>
			<h2 id="_idParaDest-227"><a id="_idTextAnchor235"/>Exercise 7.12: Defining and Testing a Function to Pull Country Data from an API</h2>
			<p>In this exercise, we'll use a particular API, <code>https://restcountries.eu/rest/v2/name/</code>, that serves basic information about countries around the world. We will first connect with the API. Next, we will create a user-defined function to get the data for a specific country. Let's follow these steps:</p>
			<ol>
				<li value="1">Import the necessary libraries:<pre>import urllib.request, urllib.parse
from urllib.error import HTTPError,URLError
import json
import pandas as pd</pre></li>
				<li>Define the <code>service_url</code> variable:<pre>serviceurl = 'https://restcountries.eu/rest/v2/name/'</pre></li>
				<li>Define a function to pull out data when we pass the name of a country as an argument. The crux of the operation is contained in the following two lines of code:<pre>country_name = 'Switzerland' 
url = serviceurl + country_name
uh = urllib.request.urlopen(url)</pre><p>The first line of code appends the country name as a string to the base URL and the second line sends a <code>get</code> request to the API endpoint. If all goes well, we get back the data, decode it, and read it as a <code>JSON</code> file. This whole exercise is coded in the following function, along with some error-handling code wrapped around the basic actions we talked about previously.</p></li>
				<li>Define the <code>get_country_data</code> function:<pre>def get_country_data(country):
    """
    Function to get data about country
    from "https://restcountries.eu" API
    """
    country_name=str(country)
    url = serviceurl + country_name
    try: 
        uh = urllib.request.urlopen(url)
    except HTTPError as e:
        print("Sorry! Could not retrieve anything on {}"\
              .format(country_name))
        return None
    except URLError as e:
        print('Failed to reach a server.')
        print('Reason: ', e.reason)
        return None
    else:
        data = uh.read().decode()
        print("Retrieved data on {}. Total {} characters  read."\
              .format(country_name,len(data)))
        return data</pre><p>Test this function by passing some arguments. Note that we are using the <code>try..except</code> block here. The <code>try</code> block lets you test a block of code and see whether there are any errors; the <code>except</code> block lets you handle the errors.</p></li>
				<li>Type in the following command:<pre>data = get_country_data(country_name)</pre><p>The output is as follows:</p><pre>Retrieved data on Switzerland. Total 1090 characters read.</pre></li>
				<li>Feed erroneous data in <code>country_name1</code>:<pre>country_name1 = 'Switzerland1'
data1 = get_country_data(country_name1)</pre><p>We pass a correct name and an erroneous name. The response is as follows:</p><pre>Sorry! Could not retrieve anything on Switzerland1</pre></li>
			</ol>
			<p>This is an example of rudimentary error handling. You have to think about various possibilities and put in the right code to catch and gracefully respond to user input when you are building a real-life web or enterprise application.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/30QU3MY%20">https://packt.live/30QU3MY.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2UUPc9I">https://packt.live/2UUPc9I</a>.</p>
			<p>Now that we have written a function to get this data with some kind of error handling built into it, we are ready to move on to the next part, where we deal with the data that we just got.</p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor236"/>Using the Built-In JSON Library to Read and Examine Data</h2>
			<p>As we have already mentioned, JSON looks a lot like a Python dictionary.</p>
			<p>We will use Python's <code>requests</code> module to read raw data in that format and see what we can process further:</p>
			<pre>import json
x=json.loads(data)
# Load the only element
y=x[0]
type(y)</pre>
			<p>The output will be as follows:</p>
			<pre>dict</pre>
			<p>It reads a string datatype into a list of dictionaries. In this case, we get only one element in the list, so we extract that and check its type to make sure it is a dictionary.</p>
			<p>We can quickly check the keys of the dictionary by using the <code>keys()</code> method on the dictionary, that is, the JSON data (note that a full screenshot is not shown here).</p>
			<p>Let's try the following command: </p>
			<pre>y.keys()</pre>
			<p>The output (partially shown), will be:</p>
			<div><div><img src="img/B15780_07_24.jpg" alt="Figure 7.24: The output of dict_keys&#13;&#10;" width="851" height="82"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.24: The output of dict_keys</p>
			<p>We can see the relevant country data, such as calling codes, population, area, time zones, borders, and so on.</p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor237"/>Printing All the Data Elements</h2>
			<p>This task is extremely simple given that we have a dictionary at our disposal. All we have to do is iterate over the dictionary and print the key/item pairs one by one:</p>
			<pre>for k,v in y.items():
    print("{}: {}".format(k,v))</pre>
			<p>The output (partially shown) is as follows:</p>
			<div><div><img src="img/B15780_07_25.jpg" alt="Figure 7.25: The output using dict&#13;&#10;" width="758" height="252"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.25: The output using dict</p>
			<p>Note that the items in the dictionary are not of the same type, that is, they are not similar objects. Some are floating-point numbers, such as <code>area</code>, many are simple strings, but some are lists or even lists of dictionaries.</p>
			<p>This is fairly common with JSON data. The internal data structure of JSON can be arbitrarily complex and multilevel, that is, you can have a dictionary of lists of dictionaries of dictionaries of lists of lists… and so on.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">It is clear, therefore, that there is no universal method or processing function for the JSON data format, and you have to write custom loops and functions to extract data from such a dictionary object based on your particular needs.</p>
			<p>Now, we will write a small loop to extract the languages spoken in Switzerland. First, let's examine the dictionary closely and see where the language data is:</p>
			<div><div><img src="img/B15780_07_26.jpg" alt="Figure 7.26: The tags&#13;&#10;" width="1428" height="365"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.26: The tags</p>
			<p>So, the data is embedded inside a list of dictionaries, which is accessed by a particular key of the main dictionary.</p>
			<p>We can write two simple lines of code to extract this data:</p>
			<pre>for lang in y['languages']:
    print(lang['name'])</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/B15780_07_27.jpg" alt="Figure 7.27: The output showing the languages&#13;&#10;" width="586" height="147"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.27: The output showing the languages</p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor238"/>Using a Function that Extracts a DataFrame Containing Key Information</h2>
			<p>Here, we are interested in writing a function that can take a list of countries and return a <code>pandas</code> DataFrame with some key information:</p>
			<ul>
				<li>Capital</li>
				<li>Region</li>
				<li>Sub-region</li>
				<li>Population</li>
				<li>Latitude/longitude</li>
				<li>Area</li>
				<li>Gini index</li>
				<li>Time zones</li>
				<li>Currencies</li>
				<li>Languages<p class="callout-heading">Note</p><p class="callout">This is the kind of wrapper function you are generally expected to write in real-life data wrangling tasks, that is, a utility function that can take a user argument and output a useful data structure (or a mini database-type object) with key information extracted over the internet about the item the user is interested in.</p></li>
			</ul>
			<p>We will show you the whole function first and then discuss some key points about it. It is a slightly complex and long piece of code. However, with your Python data-wrangling knowledge, you should be able to examine this function closely and understand what it is doing:</p>
			<pre>Exercise 7.13.ipynb
import pandas as pd
import json
def build_country_database(list_country):
    """
    Takes a list of country names.
    Output a DataFrame with key information about those countries.
    """
    # Define an empty dictionary with keys
    country_dict={'Country':[],'Capital':[],'Region':[],\
                  'Sub-region':[],'Population':[], \
                  'Latitude':[],'Longitude':[], 'Area':[],\
                  'Gini':[],'Timezones':[], 'Currencies':[],\
                  'Languages':[]}
The code has been truncated here. You can find the entire code for this function at the following GitHub link: <a href="https://packt.live/2YeRDpP">https://packt.live/2YeRDpP</a>.</pre>
			<p>Here are some of the key points about this function:</p>
			<ul>
				<li>It starts by building an empty dictionary of lists. This is the chosen format for finally passing to the pandas <strong class="bold">DataFrame</strong> method, which accepts this format and returns a nice DataFrame with column names set to the dictionary keys' names.</li>
				<li>We use the previously defined <code>get_country_data</code> function to extract data for each country in the user-defined list. For this, we simply iterate over the list and call this function.</li>
				<li>We check the output of the <code>get_country_data</code> function. If for some reason it returns a <code>None</code> object, we will know that the API reading was not successful, and we will print out a suitable message. Again, this is an example of an error-handling mechanism and you must have them in your code. Without this small error-checking code, your application won't be robust enough for the occasional incorrect input or API malfunction.</li>
				<li>For many data types, we simply extract the data from the main JSON dictionary and append it to the corresponding list in our data dictionary.</li>
				<li>However, for special data types, such as time zones, currencies, and languages, we write a special loop to extract the data without error.</li>
				<li>We also take care of the fact that these special data types can have a variable length, that is, some countries may have multiple spoken languages, but most will have only one entry. So, we check whether the length of the list is greater than one and handle the data accordingly.</li>
			</ul>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor239"/>Exercise 7.13: Testing the Function by Building a Small Database of Country Information</h2>
			<p>In this exercise, we will use the example code used in the previous section and build a database of country information. We will test this function by passing a list of country names. </p>
			<p>Let's follow these steps:</p>
			<ol>
				<li value="1">Import the necessary libraries:<pre>import urllib.request, urllib.parse
from urllib.error import HTTPError,URLError
import pandas as pd</pre></li>
				<li>Define the <code>service_url</code> variable:<pre>serviceurl = 'https://restcountries.eu/rest/v2/name/'</pre></li>
				<li>Define the <code>get_country_data</code> function:<pre>Exercise 7.13.ipynb
def get_country_data(country):
    """
    Function to get data about a country
    from "https://restcountries.eu" API
    """
    country_name=str(country)
The complete code for this step can be found at <a href="https://packt.live/2YeRDpP">https://packt.live/2YeRDpP</a>.</pre></li>
				<li>Define the name of the country:<pre>country_name = 'Switzerland'</pre></li>
				<li>Type in the following command:<pre>data=get_country_data(country_name)</pre><p>The output is as follows:</p><pre>Retrieved data on Switzerland. Total 1090 characters read.</pre></li>
				<li>Feed erroneous data in <code>country_name1</code>:<pre>country_name1 = 'Switzerland1'
data1 = get_country_data(country_name1)</pre><p>On passing an erroneous name, the response is as follows:</p><pre>Sorry! Could not retrieve anything on Switzerland1</pre></li>
				<li>Now, import the <code>json</code> library:<pre>import json</pre></li>
				<li>Load from string <code>data</code> as follows:<pre>x=json.loads(data)</pre></li>
				<li>Load the only element as follows:<pre># Load the only element
y=x[0]</pre></li>
				<li>Check the type of <code>y</code> as follows:<pre>type(y)</pre><p>This will return <code>dict</code></p></li>
				<li>Print the keys of <code>y</code> as follows:<pre>y.keys()</pre><p>The output is as follows:</p><pre>dict_keys(['name', 'topLevelDomain', 'alpha2Code', 'alpha3Code', 'callingCodes', 'capital', 'altSpellings', 'region', 'subregion', 'population', 'latlng', 'demonym', 'area', 'gini', 'timezones', 'borders', 'nativeName', 'numericCode', 'currencies', 'languages', 'translations', 'flag', 'regionalBlocs', 'cioc'])</pre></li>
				<li>Iterate over the dictionary and print the key/item pairs one by one:<pre>for k,v in y.items():
    print("{}: {}".format(k,v))</pre><p>A section of output is as follows:</p><pre>name: Switzerland
topLevelDomain: ['.ch']
alpha2Code: CH
alpha3Code: CHE
callingCodes: ['41']
capital: Bern
altSpellings: ['CH', 'Swiss Confederation', 'Schweiz', 'Suisse', 'Svizzera', 'Svizra']
region: Europe
subregion: Western Europe
population: 8341600
latlng: [47.0, 8.0]
demonym: Swiss</pre></li>
				<li>Create a loop to extract the languages spoken in <code>Switzerland</code>:<pre>for lang in y['languages']:
    print(lang['name'])</pre><p>The output is as follows:</p><pre>German
French
Italian</pre></li>
				<li>Import the necessary libraries:<pre>import pandas as pd
import json</pre></li>
				<li>Define the <code>build_country_database</code>:<pre>Exercise 7.13.ipynb
def build_country_database(list_country):
    """
    Takes a list of country names.
    Output a DataFrame with key information about those countries.
    """
    # Define an empty dictionary with keys
    country_dict={'Country':[],'Capital':[],'Region':[],'Sub-      region':[],'Population':[],
The complete code for this step is available at: <a href="https://packt.live/2YFVYkM">https://packt.live/2YFVYkM</a>.</pre></li>
				<li>To test its robustness, we pass in an erroneous name, such as <code>Turmeric</code> in this case:<pre>df1=build_country_database(['Nigeria','Switzerland','France',\
                            'Turmeric','Russia',\
                            'Kenya','Singapore'])</pre><p>The output is as follows:</p><div><img src="img/B15780_07_28.jpg" alt="Figure 7.28: output of country database&#13;&#10;" width="1047" height="259"/></div><p class="figure-caption">Figure 7.28: output of country database</p><p>As we can see from the output, it detected that it did not get any data back for the incorrect entry and printed out a suitable message. The key thing is that if you do not have the error-checking and handling code in your function, then it will stop the execution on that entry and will not return the expected mini database. To avoid this behavior, error-handling code is invaluable. The following screenshot points at the incorrect entry:</p><div><img src="img/B15780_07_29.jpg" alt="Figure 7.29: The incorrect entry highlighted&#13;&#10;" width="1406" height="495"/></div><p class="figure-caption">Figure 7.29: The incorrect entry highlighted</p></li>
				<li>Print the <code>pandas</code> DataFrame:<pre>df1</pre><p>The output is as follows (only partial output is shown):</p><div><img src="img/B15780_07_30.jpg" alt="Figure 7.30: Partial output&#13;&#10;" width="1451" height="737"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.30: Partial output</p>
			<p>Let's analyze the data that has been extracted:</p>
			<div><div><img src="img/B15780_07_31.jpg" alt="Figure 7.31: The data extracted correctly&#13;&#10;" width="1432" height="628"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.31: The data extracted correctly</p>
			<p>As we can see from the output, single as well as multiple pieces of data have been extracted correctly.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2YeRDpP%20">https://packt.live/2YeRDpP.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3fvAY6U">https://packt.live/3fvAY6U</a>.</p>
			<h1 id="_idParaDest-232"><a id="_idTextAnchor240"/>Fundamentals of Regular Expressions (RegEx)</h1>
			<p><strong class="bold">Reg</strong>ular <strong class="bold">ex</strong>pressions or <strong class="bold">regex</strong> are used to identify whether a pattern exists in a given sequence of characters (a string) or not. They help with manipulating textual data, which is often a prerequisite for data science projects that involve text mining.</p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor241"/>RegEx in the Context of Web Scraping</h2>
			<p>Web pages are often full of text, and while there are some methods in <code>BeautifulSoup</code> or XML parsers to extract raw text, there is no method for the intelligent analysis of that text. If, as a data wrangler, you are looking for a particular piece of data (for example, email IDs or phone numbers in a special format), you have to do a lot of string manipulation on a large corpus to extract email IDs or phone numbers. <code>RegEx</code> is very powerful and can save a data wrangling professional a lot of time and effort with string manipulation because they can search for complex textual patterns with wildcards of an arbitrary length.</p>
			<p><code>RegEx</code> is like a mini-programming language in itself and common ideas are used not only in Python, but in all widely used web app languages, such as JavaScript, PHP, and Perl. The <code>regex</code> module is built into Python, and you can import it by using the following code:</p>
			<pre>import re</pre>
			<p>In the next exercise, we are going to use the <code>match</code> method to check whether a pattern matches a string or sequence.</p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor242"/>Exercise 7.14: Using the match Method to Check Whether a Pattern Matches a String/Sequence</h2>
			<p>In this exercise, we will use one of the most common regex methods, <code>match</code>, to check for an exact or partial match at the beginning of a string. Let's follow these steps:</p>
			<ol>
				<li value="1">Import the <code>regex</code> module:<pre>import re</pre></li>
				<li>Define a string and a pattern:<pre>string1 = 'Python'
pattern = r"Python"</pre></li>
				<li>Write a conditional expression to check for a match:<pre>if re.match(pattern,string1):
    print("Matches!")
else:
    print("Doesn't match.")</pre><p>The output should be as follows:</p><pre>Matches!</pre></li>
				<li>Test this with a string that only differs in the first letter by making it lowercase:<pre>string2 = 'python'
if re.match(pattern,string2):
       print("Matches!")
else:
      print("Doesn't match.")</pre><p>The output is as follows:</p><pre>Doesn't match.</pre><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2N8SKAW%20">https://packt.live/2N8SKAW.</a></p><p class="callout">You can also run this example online at <a href="https://packt.live/3hHJOAr">https://packt.live/3hHJOAr</a>.</p></li>
			</ol>
			<p>In this exercise, we just saw how to do the most basic regex operations. In itself, it may not look very impressive, but we will be building further complex logic on top of this basic idea in the forthcoming exercises.</p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor243"/>Using the compile Method to Create a RegEx Program</h2>
			<p>In a program or module, if we are making heavy use of a particular pattern, then it is better to use the <code>compile</code> method and create a regex program and then call methods on this program.</p>
			<p>Here is how you compile a regex program:</p>
			<pre>prog = re.compile(pattern)
prog.match(string1)</pre>
			<p>The output is as follows:</p>
			<pre>&lt;re.SRE_Match object; span=(0, 6), match='Python'&gt;</pre>
			<p>This code produced an <code>SRE.Match</code> object that has a <code>span</code> of (<code>0,6</code>) and the matched string of <code>Python</code>. The span here simply denotes the start and end indices of the pattern that was matched. These indices may come in handy in a text mining program where the subsequent code uses the indices for further search or decision-making purposes. </p>
			<p>Compiled objects act like functions in that they return <code>None</code> if the pattern does not match. This concept will come in handy later when we write a small utility function to check for the type of the returned object from regex-compiled programs and act accordingly. We cannot be sure whether a pattern will match a given string or whether it will appear in a corpus of text (if we are searching for the pattern anywhere within the text). Depending on the situation, we may encounter <code>Match</code> objects or <code>None</code> as the returned value, and we have to handle this gracefully. Let's practice this in the following exercise. </p>
			<h2 id="_idParaDest-236">Exercise 7.15: Compiling Progra<a id="_idTextAnchor244"/>ms to Match Objects</h2>
			<p>In this exercise, we will define two strings and a pattern. We will use the <code>compile</code> method to compile a regex program. Next, we will write a small conditional to test whether the compiled object matches the defined pattern. Let's follow these steps:</p>
			<ol>
				<li value="1">Use the <code>compile</code> function from the <code>regex</code> module:<pre>import re
def print_match(s):
    if prog.search(s)==None:
        print("No match")
    else:
        print(prog.search(s).group()) 
string1 = 'Python'
string2 = 'python'
pattern = r"Python"
prog = re.compile(pattern)</pre></li>
				<li>Match it with the first string:<pre>if prog.match(string1)!=None:
    print("Matches!")
else:
    print("Doesn't match.")</pre><p>The output is as follows:</p><pre>Matches!</pre></li>
				<li>Match it with the second string:<pre>if prog.match(string2)!=None:
    print("Matches!")
else:
    print("Doesn't match.")</pre><p>The output is as follows:</p><pre>Doesn't match.</pre></li>
			</ol>
			<p>So, the <code>compile</code> method returns special objects, such as <code>match</code> objects. But if they don't match, it will return <code>None</code>, so we can still run our conditional loop.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/30SJ4m9%20">https://packt.live/30SJ4m9.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3hIBkJE">https://packt.live/3hIBkJE</a>.</p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor245"/>Exercise 7.16: Using Additional Parameters in the match Method to Check for Positional Matching</h2>
			<p>In this exercise, we will use the <code>match</code> method to check whether there's a match at a specific location in the string. Let's follow these steps:</p>
			<ol>
				<li value="1">Match <code>y</code> in the second position:<pre>import re
def print_match(s):
    if prog.search(s)==None:
        print("No match")
    else:
        print(prog.search(s).group()) 
prog = re.compile(r'y')
prog.match('Python',pos=1)</pre><p>The output is as follows:</p><pre>&lt;re.Match object; span=(1, 2), match='y'&gt;</pre><p>This is the <code>match</code> object that we talked about before.</p></li>
				<li>Check for a pattern called <code>thon</code> starting from <code>pos=2</code>, that is, the third character:<pre>prog = re.compile(r'thon')
prog.match('Python',pos=2)</pre><p>The output is as follows:</p><pre>&lt;_re.SRE_Match object; span=(2, 6), match='thon'&gt;</pre></li>
				<li>Find a match in a different string by using the following command:<pre>prog.match('Marathon',pos=4)</pre><p>The output is as follows:</p><pre>&lt;_re.SRE_Match object; span=(4, 8), match='thon'&gt;</pre></li>
			</ol>
			<p>So, we have seen how can we use regex, and use it in various use cases.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2CmKc7z%20">https://packt.live/2CmKc7z.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/30OsDY6">https://packt.live/30OsDY6</a>.</p>
			<h2 id="_idParaDest-238"><a id="_idTextAnchor246"/>Finding the Number of Words in a List That End with "ing"</h2>
			<p>Suppose we want to find out whether a given string has the last three letters <code>ing</code>. This kind of query may come up in a text analytics/text mining program where somebody is interested in finding instances of present continuous tense words, which are highly likely to end with <code>ing</code>. However, nouns may also end with <code>ing</code> (as we will see in this example):</p>
			<pre>prog = re.compile(r'ing')
words = ['Spring','Cycling','Ringtone']</pre>
			<p>Create a <code>for</code> loop to find words ending with <code>ing</code>:</p>
			<pre>for w in words:
    if prog.match(w,pos=len(w)-3)!=None:
        print("{} has last three letters 'ing'".format(w))
    else:
        print("{} does not have last three letter as 'ing'"\
              .format(w))</pre>
			<p>The output is as follows:</p>
			<pre>Spring has last three letters 'ing'
Cycling has last three letters 'ing'
Ringtone does not have last three letter as 'ing'</pre>
			<h2 id="_idParaDest-239"><a id="_idTextAnchor247"/>The search Method in RegEx</h2>
			<p>It looks plain and simple, and you may well wonder what the purpose of using a special regex module for this is. A simple string method should have been sufficient. Yes, it would have been OK for this particular example, but the whole point of using regex is to be able to use very complex string patterns that are not at all obvious when it comes to how they are written using simple string methods. We will see the real power of regex compared to string methods shortly. But before that, let's explore another of the most commonly used methods, called <code>search</code>.</p>
			<p><code>search</code> and <code>match</code> are related concepts, and they both return the same <code>match</code> object. The real difference between them is that <strong class="bold">match works for only the first match</strong> (either at the beginning of the string or at a specified position, as we saw in the previous exercises), whereas <strong class="bold">search looks for the pattern anywhere in the string</strong> and returns the position if it finds a match.</p>
			<h2 id="_idParaDest-240"><a id="_idTextAnchor248"/>Exercise 7.17: The search Method in RegEx</h2>
			<p>In this exercise, we will use the <code>search</code> method to find the <code>ing</code> pattern in a regex structure. Let's follow these steps:</p>
			<ol>
				<li value="1">Use the <code>compile</code> method to find matching strings:<pre>import re
def print_match(s):
    if prog.search(s)==None:
        print("No match")
    else:
        print(prog.search(s).group())
prog = re.compile('ing')
if prog.match('Spring')==None:
    print("None")</pre><p>The output is as follows:</p><pre>None</pre></li>
				<li>Search the string by using the following command:<pre>prog.search('Spring')</pre><p>The output is as follows:</p><pre>&lt;_sre.SRE_Match object; span=(3, 6), match='ing'&gt;</pre></li>
				<li>Let's use <code>Ringtone</code> as the search parameter:<pre>prog.search('Ringtone')</pre><p>The output is as follows:</p><pre>&lt;re.Match object; span=(1, 4), match='ing'&gt;</pre></li>
			</ol>
			<p>As you can see, the <code>match</code> method returns <code>None</code> for the input <code>Spring</code>, and we had to write code to print that out explicitly (because in a Jupyter notebook, nothing will show up for a <code>None</code> object). But <code>search</code> returns a <code>match</code> object with <code>span=(3,6)</code> as it finds the <code>ing</code> pattern spanning those positions.</p>
			<p>Similarly, for the <code>Ringtone</code> string, it finds the correct position of the match and returns <code>span=(1,4)</code>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3fDRmme%20">https://packt.live/3fDRmme.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/30U2WFm">https://packt.live/30U2WFm</a>.</p>
			<h2 id="_idParaDest-241"><a id="_idTextAnchor249"/>Exercise 7.18: Using the span Method of the Match Object to Locate the Position of the Matched Pattern</h2>
			<p>In this exercise, we will use the <code>span</code> contained in the <code>Match</code> object to locating the exact position of the pattern as it appears in the string. Let's follow these steps:</p>
			<ol>
				<li value="1">Initialize <code>prog</code> with the <code>ing</code> pattern:<pre>import re
def print_match(s):
    if prog.search(s)==None:
        print("No match")
    else:
        print(prog.search(s).group()) 
prog = re.compile(r'ing')
words = ['Spring','Cycling','Ringtone']</pre></li>
				<li>Create a function to return a tuple of the start and end positions of the match:<pre>for w in words:
    mt = prog.search(w)
# Span returns a tuple of start and end positions of the match
# Starting position of the match
start_pos = mt.span()[0]
# Ending position of the match 
end_pos = mt.span()[1] </pre></li>
				<li>Print the word ending with <code>ing</code> and its start and end position:<pre>print("The word '{}' contains 'ing' in the position {}-{}"\
      .format(w,start_pos,end_pos))</pre><p>The output is as follows:</p><pre>The word 'Ringtone' contains 'ing' in the position 1-4</pre><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2YIZB9y%20">https://packt.live/2YIZB9y.</a></p><p class="callout">You can also run this example online at <a href="https://packt.live/37FXSG5">https://packt.live/37FXSG5</a>.</p></li>
			</ol>
			<p>Now, we will start getting into the real usage of regex with examples of various useful pattern matching. In the following exercise, we will explore single-character matching.</p>
			<h2 id="_idParaDest-242"><a id="_idTextAnchor250"/>Exercise 7.19: Examples of Single-Character Pattern Matching with search</h2>
			<p>In this exercise, we will use the <code>group</code> method, which will return the matched pattern in a string format so that we can print and process it easily. Let's follow these steps:</p>
			<ol>
				<li value="1">Pass a regex expression with a dot (<code>.</code>) inside the <code>compile</code> method. It matches any single character except a newline character:<pre>import re
prog = re.compile(r'py.')
print(prog.search('pygmy').group())
print(prog.search('Jupyter').group())</pre><p>The output is as follows:</p><pre>pyg
pyt</pre></li>
				<li>Pass a regex expression with <code>\w</code> (lowercase w) inside the <code>compile</code> method. It matches any single letter, digit, or underscore:<pre>prog = re.compile(r'c\wm')
print(prog.search('comedy').group())
print(prog.search('camera').group())
print(prog.search('pac_man').group())
print(prog.search('pac2man').group())</pre><p>The output is as follows:</p><pre>com
cam
c_m
c2m</pre></li>
				<li>Pass a regex expression with <code>\W</code> (uppercase W) inside the <code>compile</code> method. It matches anything not covered by <code>\w</code>:<pre>prog = re.compile(r'4\W1')
print(prog.search('4/1 was a wonderful day!').group())
print(prog.search('4-1 was a wonderful day!').group())
print(prog.search('4.1 was a wonderful day!').group())
print(prog.search('Remember the wonderful day 04/1?').group())</pre><p>The output is as follows:</p><pre>4/1
4-1
4.1
4/1</pre></li>
				<li>Pass a regex expression with <code>\s</code> (lowercase s) inside the <code>compile</code> method. It matches a single whitespace character, such as a space, newline, tab, or return:<pre>prog = re.compile(r'Data\swrangling')
print(prog.search("Data wrangling is cool").group())
print("-"*80)
print("Data\twrangling is the full string")
print(prog.search("Data\twrangling is the full string").group())
print("-"*80)
print("Data\nwrangling is the full string")
print(prog.search("Data\nwrangling").group())</pre><p>The output is as follows:</p><pre>Data wrangling
--------------------------------------------------------------
Data    wrangling is the full string
Data    wrangling
--------------------------------------------------------------
Data
wrangling is the full string
Data
wrangling</pre></li>
				<li>Pass a regex expression with <code>\d</code> inside the <code>compile</code> method. It matches numerical digits 0-9:<pre>prog = re.compile(r"score was \d\d")
print(prog.search("My score was 67").group())
print(prog.search("Your score was 73").group())</pre><p>The output is as follows:</p><pre>score was 67
score was 73</pre></li>
			</ol>
			<p>As we can see, we can use the <code>group</code> function to return a group of matched characters.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2YOJcAi%20">https://packt.live/2YOJcAi.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3edPMHj">https://packt.live/3edPMHj</a>.</p>
			<p>In the following exercise, we will manipulate the start or end of a string using pattern matching. </p>
			<h2 id="_idParaDest-243"><a id="_idTextAnchor251"/>Exercise 7.20: Handling Pattern Matching at the Start or End of a String</h2>
			<p>In this exercise, we will match patterns with strings using the <code>^</code> (caret) operator. The focus is to find out whether the pattern is present at the start or the end of the string. Let's follow these steps:</p>
			<ol>
				<li value="1">Write a function to handle cases where a match is not found, that is, to handle <code>None</code> objects that are returned:<pre>import re
def print_match(s):
    if prog.search(s)==None:
        print("No match")
    else:
        print(prog.search(s).group())</pre></li>
				<li>Use <code>^</code> (caret) to match a pattern at the start of the string:<pre>prog = re.compile(r'^India')
print_match("Russia implemented this law")
print_match("India implemented that law")
print_match("This law was implemented by India")</pre><p>The output is as follows: </p><pre>No match
India
No match</pre></li>
				<li>Use <code>$</code> (dollar sign) to match a pattern at the end of the string:<pre>prog = re.compile(r'Apple$')
print_match("Patent no 123456 belongs to Apple")
print_match("Patent no 345672 belongs to Samsung")
print_match("Patent no 987654 belongs to Apple")</pre><p>The output is as follows:</p><pre>Apple
No match
Apple</pre><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3ddku23%20">https://packt.live/3ddku23.</a></p><p class="callout">You can also run this example online at <a href="https://packt.live/3djOXeV">https://packt.live/3djOXeV</a>.</p><p class="callout">For these examples and exercises, also try to think about how you would implement them without regex, that is, by using simple string methods and any other logic that you can think of. Then, compare that solution to the ones implemented with regex for brevity and efficiency.</p></li>
			</ol>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor252"/>Exercise 7.21: Pattern Matching with Multiple Characters</h2>
			<p>In this exercise, we will use the <code>match</code> method for matching multiple characters. Let's perform the following steps:</p>
			<ol>
				<li value="1">Use <code>*</code> to match <code>0</code> or more repetitions of the preceding regular expression:<pre>import re
def print_match(s):
    if prog.search(s)==None:
        print("No match")
    else:
        print(prog.search(s).group())
prog = re.compile(r'ab*')
print_match("a")
print_match("ab")
print_match("abbb")
print_match("b")
print_match("bbab")
print_match("something_abb_something")</pre><p>The output is as follows:</p><pre>a
ab
abbb
No match
ab
abb</pre></li>
				<li>Using <code>+</code> causes the resulting <code>RE</code> to match <code>1</code> or more repetitions of the preceding regular expression:<pre>prog = re.compile(r'ab+')
print_match("a")
print_match("ab")
print_match("abbb")
print_match("b")
print_match("bbab")
print_match("something_abb_something")</pre><p>The output is as follows:</p><pre>No match
ab
abbb
No match
ab
abb</pre></li>
				<li>? causes the resulting <code>re</code> string to match precisely 0 or 1 repetitions of the preceding regular expression:<pre>prog = re.compile(r'ab?')
print_match("a")
print_match("ab")
print_match("abbb")
print_match("b")
print_match("bbab")
print_match("something_abb_something")</pre><p>The output is as follows:</p><pre>a
ab
ab
No match
ab
ab</pre></li>
			</ol>
			<p>Here, we saw how we can use regex to search for and match a set of characters in the same order as they occur in the search pattern.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/310l7Jw%20">https://packt.live/310l7Jw.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3hCdnDz">https://packt.live/3hCdnDz</a>.</p>
			<p>The standard (default) mode of pattern matching in regex is <strong class="bold">greedy</strong>, that is, the program tries to match as much as it can. Sometimes, this behavior is natural, but in some cases, you may want to match minimally. This is called <strong class="bold">non-greedy</strong> matching.</p>
			<h2 id="_idParaDest-245"><a id="_idTextAnchor253"/>Exercise 7.22: Greedy versus Non-Greedy Matching</h2>
			<p>In this exercise, we will perform greedy and non-greedy pattern matching. Let's go through the following steps:</p>
			<ol>
				<li value="1">Write the code to check the greedy way of matching a string, as follows:<pre>import re
def print_match(s):
    if prog.search(s)==None:
        print("No match")
    else:
        print(prog.search(s).group())
prog = re.compile(r'&lt;.*&gt;')
print_match('&lt;a&gt; b &lt;c&gt;')</pre><p>The output is as follows:</p><pre> &lt;a&gt; b &lt;c&gt;</pre><p>So, the preceding regex found both tags with the <code>&lt;&gt;</code> pattern, but what if we wanted to match the first tag only and stop there. </p></li>
				<li>Use <code>?</code> by inserting it after any regex expression to make it non-greedy:<pre>prog = re.compile(r'&lt;.*?&gt;')
print_match('&lt;a&gt; b &lt;c&gt;')</pre><p>The output is as follows:</p><pre>&lt;a&gt;</pre></li>
			</ol>
			<p>In the following exercise, we will be handling repetitions using <code>match</code>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/37Hz944%20">https://packt.live/37Hz944.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2UVlK3q">https://packt.live/2UVlK3q</a>.</p>
			<h2 id="_idParaDest-246"><a id="_idTextAnchor254"/>Exercise 7.23: Controlling Repetitions to Match in a Text</h2>
			<p>In this exercise, we will check the number of repetitions of the pattern we want to match in a text. Let's go through the following steps:</p>
			<ol>
				<li value="1"><code>{m}</code> specifies exactly <code>m</code> copies of <code>RE</code> to match. Fewer matches cause a non-match and return <code>None</code>:<pre>import re
def print_match(s):
    if prog.search(s)==None:
        print("No match")
    else:
        print(prog.search(s).group())
prog = re.compile(r'A{3}')
print_match("ccAAAdd")
print_match("ccAAAAdd")
print_match("ccAAdd")</pre><p>The output is as follows:</p><pre>AAA
AAA
No match</pre></li>
				<li><code>{m,n}</code> specifies exactly <code>m</code> to <code>n</code> copies of <code>RE</code> to match:<pre>prog = re.compile(r'A{2,4}B')
print_match("ccAAABdd")
print_match("ccABdd")
print_match("ccAABBBdd")
print_match("ccAAAAAAABdd")</pre><p>The output is as follows:</p><pre>AAAB
No match
AAB
AAAAB</pre></li>
				<li>Omitting <code>m</code> specifies a lower bound of zero:<pre>prog = re.compile(r'A{,3}B')
print_match("ccAAABdd")
print_match("ccABdd")
print_match("ccAABBBdd")
print_match("ccAAAAAAABdd")</pre><p>The output is as follows:</p><pre>AAAB
AB
AAB
AAAB</pre></li>
				<li>Omitting <code>n</code> specifies an infinite upper bound:<pre>prog = re.compile(r'A{3,}B')
print_match("ccAAABdd")
print_match("ccABdd")
print_match("ccAABBBdd")
print_match("ccAAAAAAABdd")</pre><p>The output is as follows:</p><pre>AAAB
No match
No match
AAAAAAAB</pre></li>
				<li><code>{m,n}?</code> specifies <code>m</code> to <code>n</code> copies of <code>RE</code> to match in a non-greedy fashion:<pre>prog = re.compile(r'A{2,4}')
print_match("AAAAAAA")
prog = re.compile(r'A{2,4}?')
print_match("AAAAAAA")</pre><p>The output is as follows:</p><pre>AAAA
AA</pre><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2YOzAWf%20">https://packt.live/2YOzAWf.</a></p><p class="callout">You can also run this example online at <a href="https://packt.live/2YKO7T4">https://packt.live/2YKO7T4</a>.</p></li>
			</ol>
			<p>Let's go over to the next section.</p>
			<h2 id="_idParaDest-247"><a id="_idTextAnchor255"/>Sets of Matching Characters</h2>
			<p>To match an arbitrarily complex pattern, we need to be able to include a logical combination of characters together as a bunch. Regex gives us that kind of capability.</p>
			<p>The following examples demonstrate such uses of regex. <code>[x,y,z]</code> matches <code>x</code>, <code>y</code>, or <code>z</code>:</p>
			<pre>prog = re.compile(r'[A,B]')
print_match("ccAd")
print_match("ccABd")
print_match("ccXdB")
print_match("ccXdZ")</pre>
			<p>The output will be as follows:</p>
			<pre>A
A
B
No match</pre>
			<p>A range of characters can be matched inside the set using <code>-</code>. This is one of the most widely used regex techniques.</p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor256"/>Exercise 7.24: Sets of Matching Characters</h2>
			<p>In this exercise, we will find the sets of matching characters from a defined string. We will look for an email address pattern, <code>&lt;some name&gt;@&lt;some domain name&gt;.&lt;some domain identifier&gt;</code>, from a string. Let's go through the following steps:</p>
			<ol>
				<li value="1">Suppose we want to pick out an email address from some text:<pre>import re
def print_match(s):
    if prog.search(s)==None:
        print("No match")
    else:
        print(prog.search(s).group())
prog = re.compile(r'[a-zA-Z]+@+[a-zA-Z]+\.com')
print_match("My email is coolguy@xyz.com")
print_match("My email is coolguy12@xyz.com")</pre><p>The output is as follows:</p><pre>coolguy@xyz.com
No match</pre><p>Look at the regex pattern inside [<code> … </code>]. It is <code>a-zA-Z</code>. This covers all letters, including lowercase and uppercase. With this one simple regex, you are able to match any (pure) alphabetical string for that part of the email. Now, the next pattern is <code>@</code>, which is added to the previous regex by the <code>+</code> character. This is the way to build up a complex regex: by adding/stacking up individual regex patterns. We also use the same <code>[a-zA-Z]</code> for the email domain name and add a <code>.com</code> at the end to complete the pattern as a valid email address. Why <code>\.</code>? Because, by itself, a dot (<code>.</code>) is used as a special modifier in regex but here we want to use a dot (<code>.</code>) just as a dot (<code>.</code>), not as a modifier. So, we need to precede it with <code>\</code>.</p><p>So, with this regex, we could extract the first email address perfectly but got <code>No match</code> with the second one. What happened with the second email ID?</p><p>The regex could not capture it because it had the number <code>12</code> in the name. That pattern is not captured by the expression [<code>a-zA-Z</code>].</p></li>
				<li>Let's change that and add the digits as well:<pre>prog = re.compile(r'[a-zA-Z0-9]+@+[a-zA-Z]+\.com')
print_match("My email is coolguy12@xyz.com")
print_match("My email is coolguy12@xyz.org")</pre><p>The output is as follows:</p><pre>coolguy12@xyz.com
No match</pre><p>We caught the first email ID perfectly. But what's going on with the second one? Again, we got a mismatch. The reason is that we changed the <code>.com</code> to <code>.org</code> in that email, and in our regex expression, that portion was hardcoded as <code>.com</code>, so it did not find a match.</p></li>
				<li>Let's try to address this in the following regex:<pre>prog = re.compile(r'[a-zA-Z0-9]+@+[a-zA-Z]+\.+[a-zA-Z]{2,3}')
print_match("My email is coolguy12@xyz.org")
print_match("My email is coolguy12[AT]xyz[DOT]org")</pre><p>The output is as follows:</p><pre>coolguy12@xyz.org
No match</pre></li>
			</ol>
			<p>In this regex, we used the fact that most domain identifiers have two or three characters, so we used <code>[a-zA-Z]{2,3}</code> to capture that.</p>
			<p>What happened with the second email ID? This is an example of the small tweaks that you can make to stay ahead of telemarketers who want to scrape online forums or any other corpus of text and extract your email ID. If you do not want your email to be found, you can change <code>@</code> to <code>[AT]</code> and <code>.</code> to <code>[DOT]</code>, and hopefully, that should beat some regex techniques (but not all of them).</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2UXv6eS%20">https://packt.live/2UXv6eS.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/315GaL9">https://packt.live/315GaL9</a>.</p>
			<h2 id="_idParaDest-249"><a id="_idTextAnchor257"/>Exercise 7.25: The Use of OR in RegEx Using the OR Operator</h2>
			<p>In this exercise, we will use the <code>OR</code> operator in a Regex expression. We will try to extract patterns of 10-digit numbers that could be phone numbers. We can do that by using the <code>|</code> operator. Let's go through the following steps:</p>
			<ol>
				<li value="1">Let's start with the <code>OR</code> operator:<pre>import re
def print_match(s):
    if prog.search(s)==None:
         print("No match")
    else:
         print(prog.search(s).group())
prog = re.compile(r'[0-9]{10}')
print_match("3124567897")
print_match("312-456-7897")</pre><p>The output is as follows:</p><pre>3124567897
No match</pre><p>Note the use of <code>{10}</code> to denote exactly <code>10</code>-digit numbers in the pattern. But the second number could not be matched for obvious reasons – it had <code>-</code> symbols inserted in between groups of numbers.</p></li>
				<li>Use multiple smaller regexes and logically combine them by using the following command:<pre>prog = re.compile(r'[0-9]{10}|[0-9]{3}-[0-9]{3}-[0-9]{4}')
print_match("3124567897")
print_match("312-456-7897")</pre><p>The output is as follows:</p><pre>3124567897
312-456-7897</pre><p>Phone numbers are written in a myriad of ways and if you search on the web, you will see examples of very complex regexes (written not only in Python but in other widely used languages for web apps such as JavaScript, C++, PHP, and Perl) for capturing phone numbers.</p></li>
				<li>Create four strings and execute <code>print_match</code> on them:<pre>p1= r'[0-9]{10}'
p2=r'[0-9]{3}-[0-9]{3}-[0-9]{4}'
p3 = r'\([0-9]{3}\)[0-9]{3}-[0-9]{4}'
p4 = r'[0-9]{3}\.[0-9]{3}\.[0-9]{4}'
pattern= p1+'|'+p2+'|'+p3+'|'+p4
prog = re.compile(pattern)
print_match("3124567897")
print_match("312-456-7897")
print_match("(312)456-7897")
print_match("312.456.7897")</pre><p>The output is as follows:</p><pre>3124567897
312-456-7897
(312)456-7897
312.456.7897</pre></li>
			</ol>
			<p>So, as you can see, thanks to all the different patterns we have added together using the <code>OR</code> operator, we are able to detect phone numbers even if they are written in very different ways.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3eeZc59%20">https://packt.live/3eeZc59.</a></p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2APMFH5">https://packt.live/2APMFH5</a>.</p>
			<h2 id="_idParaDest-250"><a id="_idTextAnchor258"/>The findall Method</h2>
			<p>The last regex method that we will cover in this chapter is <code>findall</code>. Essentially, it is a <strong class="bold">search-and-aggregate</strong> method, that is, it puts together all the instances that match the regex pattern in a given text and returns them in a list. This is extremely useful, as we can just count the length of the returned list to count the number of occurrences or pick and use the returned pattern-matched words one by one as we see fit.</p>
			<p>Note that although we are giving short examples of single sentences in this chapter, you will often deal with a large corpus of text when using a regex.</p>
			<p>In those cases, you are likely to get many matches from a single regex pattern search. For all of those cases, the <code>findall</code> method is going to be the most useful:</p>
			<pre>ph_numbers = """Here are some phone numbers.
Pick out the numbers with 312 area code: 
312-423-3456, 456-334-6721, 312-5478-9999, 
312-Not-a-Number,777.345.2317, 312.331.6789"""
print(ph_numbers)
re.findall('312+[-\.][0-9-\.]+',ph_numbers)</pre>
			<p>The output is as follows:</p>
			<pre> Here are some phone numbers.
Pick out the numbers with 312 area code: 
312-423-3456, 456-334-6721, 312-5478-9999, 
312-Not-a-Number,777.345.2317, 312.331.6789
 ['312-423-3456', '312-5478-9999', '312.331.6789']</pre>
			<p>With all this knowledge gained from the chapter, let's get started with solving the following activities. </p>
			<h2 id="_idParaDest-251">Activity 7.01: Extracting the T<a id="_idTextAnchor259"/>op 100 e-books from Gutenberg</h2>
			<p>Project Gutenberg encourages the creation and distribution of eBooks by encouraging volunteer efforts to digitize and archive cultural works. This activity aims to scrape the URL of Project Gutenberg's Top 100 eBooks to identify the eBooks' links. It uses <code>BeautifulSoup</code> to parse the HTML and regular expression code to identify the Top 100 eBook file numbers. You can use these numbers to download the book into your local drive if you want.</p>
			<p>These are the steps that will help you complete this activity:</p>
			<ol>
				<li value="1">Import the necessary libraries, including <code>regex</code> and <code>BeautifulSoup</code>.</li>
				<li>Read the HTML from the URL.</li>
				<li>Write a small function to check the status of the web request.</li>
				<li>Decode the response and pass this on to <code>BeautifulSoup</code> for HTML parsing.</li>
				<li>Find all the <code>href</code> tags and store them in the list of links. Check what the list looks like – print the first 30 elements.</li>
				<li>Use a regular expression to find the numeric digits in these links. These are the file numbers for the top 100 eBooks.</li>
				<li>Initialize the empty list to hold the file numbers over an appropriate range and use <code>regex</code> to find the numeric digits in the link <code>href</code> string. <code>findall</code> method.</li>
				<li>What does the <code>soup</code> object's text look like? Use the <code>.text</code> method and print only the first 2,000 characters (do not print the whole thing, as it is too long).</li>
				<li>Search in the extracted text (using a regular expression) from the <code>soup</code> object to find the names of the top 100 eBooks (yesterday's ranking).</li>
				<li>Create a starting index. It should point at the text <em class="italic">Top 100 Ebooks yesterday</em>. Use the <code>splitlines</code> method of <code>soup.text</code>. It splits the lines of text of the <code>soup</code> object.</li>
				<li>Run the <code>for</code> loop <code>1-100</code> to add the strings of the next <code>100</code> lines to this temporary list. <code>splitlines</code> method.</li>
				<li>Use a regular expression to extract only text from the name strings and append it to an empty list. Use <code>match</code> and <code>span</code> to find the indices and use them.</li>
				<li>Print the list of titles.</li>
			</ol>
			<p>The output (shown partially) should look like this:</p>
			<pre>Pride and Prejudice by Jane Austen 
Frankenstein
A Modest Proposal by Jonathan Swift 
A Christmas Carol in Prose
Heart of Darkness by Joseph Conrad 
Et dukkehjem
A Tale of Two Cities by Charles Dickens 
Dracula by Bram Stoker 
Moby Dick
The Importance of Being Earnest
Alice
Metamorphosis by Franz Kafka 
The Strange Case of Dr
Beowulf
…
The Russian Army and the Japanese War
Calculus Made Easy by Silvanus P
Beyond Good and Evil by Friedrich Wilhelm Nietzsche 
An Occurrence at Owl Creek Bridge by Ambrose Bierce 
Don Quixote by Miguel de Cervantes Saavedra 
Blue Jackets by Edward Greey 
The Life and Adventures of Robinson Crusoe by Daniel Defoe 
The Waterloo Campaign 
The War of the Worlds by H
Democracy in America 
Songs of Innocence
The Confessions of St
Modern French Masters by Marie Van Vorst 
Persuasion by Jane Austen 
The Works of Edgar Allan Poe 
The Fall of the House of Usher by Edgar Allan Poe 
The Masque of the Red Death by Edgar Allan Poe 
The Lady with the Dog and Other Stories by Anton Pavlovich Chekhov</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found via <a href="B15780_Solution_Final_RK.xhtml#_idTextAnchor323">this link</a>.</p>
			<h2 id="_idParaDest-252">Activity 7.02: Building Your Ow<a id="_idTextAnchor260"/>n Movie Database by Reading an API</h2>
			<p>In this activity, you will build a complete movie database by communicating and interfacing with a free API from the OMDb portal <a href="http://www.omdbapi.com/?">http://www.omdbapi.com/?</a>.You will obtain a unique user key from the OMDb website that must be used when your program tries to access the API. Then, you will need to store this key value in a <code>.json</code> file.</p>
			<p>The aims of this activity are as follows:</p>
			<ul>
				<li>To retrieve and print basic data about a movie (the title is entered by the user) from the web (the OMDb database).</li>
				<li>If a poster of the movie can be found, download the file and save it in a user-specified location.</li>
			</ul>
			<p>These are the steps that will help you complete this activity:</p>
			<ol>
				<li value="1">Import <code>urllib.request</code>, <code>urllib.parse</code>, <code>urllib.error</code>, and <code>json</code>.</li>
				<li>Load the secret API key (you have to get one from the OMDb website and use that; it has a daily limit of 1,000 API keys) from a JSON file, stored in the same folder, in a variable.<p><code>json.loads()</code>.</p><p>Students/users will need to obtain a key and store it in a JSON file.</p></li>
				<li>Obtain a key and store it in a JSON file as <code>APIkeys.json</code>.</li>
				<li>Open the <code>APIkeys.json</code> file.</li>
				<li>Assign the OMDb portal (<a href="http://www.omdbapi.com/?">http://www.omdbapi.com/?</a>) as a string to a variable.</li>
				<li>Create a variable called <code>apikey</code> with the last portion of the URL (<code>&amp;apikey=secretapikey</code>), where <code>secretapikey</code> is your own API key.</li>
				<li>Write a utility function called <code>print_json</code> to print the movie data from a JSON file (which we will get from the portal).</li>
				<li>Write a utility function to download a poster of the movie based on the information from the JSON dataset and save it in your local folder. Use the <code>os</code> module. The poster data is stored in a JSON key called <code>Poster</code>. Use the <code>open</code> Python command to open a file and write the poster data. Close the file after you're done. This function will save the poster data as an image file.</li>
				<li>Write a utility function called <code>search_movie</code> to search for a movie by its name, print the downloaded <code>JSON</code> data, and save the movie poster in the local folder. Use a <code>try-except</code> loop for this. Use the previously created <code>serviceurl</code> and <code>apikey</code> variables. You have to pass on a dictionary with a key, <code>t</code>, and the movie name as the corresponding value to the <code>urllib.parse.urlencode()</code> function and then add the <code>serviceurl</code> and <code>apikey</code> variables to the output of the function to construct the full URL. This URL will be used to access the data. The <code>JSON</code> data has a key called <code>Response</code>. If it is <code>True</code>, that means the read was successful. Check this before processing the data. If it's not successful, then print the <code>JSON</code> key <code>Error</code>, which will contain the appropriate error message returned by the movie database.</li>
				<li>Test the <code>search_movie</code> function by entering <code>Titanic</code>. The output should look like this:<pre>http://www.omdbapi.com/?t=Titanic&amp;apikey=&lt;your API key&gt;
--------------------------------------------------
Title: Titanic
Year: 1997
Rated: PG-13
Released: 19 Dec 1997
Runtime: 194 min
Genre: Drama, Romance
Director: James Cameron
Writer: James Cameron
Actors: Leonardo DiCaprio, Kate Winslet, Billy Zane, Kathy Bates
Plot: A seventeen-year-old aristocrat falls in love with a kind but poor artist aboard the luxurious, ill-fated R.M.S. Titanic.
Language: English, Swedish
Country: USA
Awards: Won 11 Oscars. Another 111 wins &amp; 77 nominations.
Ratings: [{'Source': 'Internet Movie Database', 'Value': '7.8/10'}, {'Source': 'Rotten Tomatoes', 'Value': '89%'}, {'Source': 'Metacritic', 'Value': '75/100'}]
Metascore: 75
imdbRating: 7.8
imdbVotes: 913,780
imdbID: tt0120338
--------------------------------------------------</pre></li>
				<li>Test the <code>search_movie</code> function by entering <code>Random_error</code> and retrieve the data for <code>Random_error</code> (obviously, this will not be found, and you should be able to check whether your error-catching code is working properly). The expected output is as follows:<pre>http://www.omdbapi.com/?t=Random_error&amp;apikey=&lt;your api key&gt;
Error encountered:  Movie not found!</pre><p class="callout-heading">Note</p><p class="callout">The solution for this activity can be found via <a href="B15780_Solution_Final_RK.xhtml#_idTextAnchor324">this link</a>.</p></li>
			</ol>
			<p>Look for a folder called <code>Posters</code> in the same directory you are working in. It should contain a file called <code>Titanic.jpg</code>. Check the file. </p>
			<p>In this activity, we have seen a few general tricks for working with an API that are fairly common for other popular API services such as Google and Twitter. Now, you should be confident about writing more complex programs to scrape data from such services.</p>
			<h1 id="_idParaDest-253"><a id="_idTextAnchor261"/>Summary</h1>
			<p>In this chapter, we went through several important concepts and learning modules related to advanced data gathering and web scraping. We started by reading data from web pages using two of the most popular Python libraries – <code>requests</code> and <code>BeautifulSoup</code>. In this task, we utilized the knowledge we gained in the previous chapter about the general structure of HTML pages and their interaction with Python code. We extracted meaningful data from the Wikipedia home page during this process.</p>
			<p>Then, we learned how to read data from XML and JSON files – two of the most widely used data streaming/exchange formats on the web. For XML, we showed you how to traverse the tree-structure data string efficiently to extract key information. For JSON, we mixed it with reading data from the web using an API. The API we consumed was RESTful, which is one of the major standards in web APIs. </p>
			<p>At the end of this chapter, we went through a detailed exercise using regex techniques in tricky string-matching problems to scrape useful information from a large and messy text corpus, parsed from HTML. This chapter should come in extremely handy for string and text processing tasks in your data wrangling career.</p>
			<p>In the next chapter, we will learn about databases with Python.</p>
		</div>
		<div><div></div>
		</div>
	</div></body></html>