- en: Chapter 3. External Data Sources
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章. 外部数据源
- en: One of the strengths of Spark is that it provides a single runtime that can
    connect with various underlying data sources.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的一项优势是它提供了一个单一的运行时，可以连接到各种底层数据源。
- en: 'In this chapter, we will connect to different data sources. This chapter is
    divided into the following recipes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将连接到不同的数据源。本章分为以下食谱：
- en: Loading data from the local filesystem
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从本地文件系统加载数据
- en: Loading data from HDFS
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 HDFS 加载数据
- en: Loading data from HDFS using a custom InputFormat
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自定义 InputFormat 从 HDFS 加载数据
- en: Loading data from Amazon S3
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Amazon S3 加载数据
- en: Loading data from Apache Cassandra
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Apache Cassandra 加载数据
- en: Loading data from relational databases
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从关系型数据库加载数据
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Spark provides a unified runtime for big data. HDFS, which is Hadoop's filesystem,
    is the most used storage platform for Spark as it provides cost-effective storage
    for unstructured and semi-structured data on commodity hardware. Spark is not
    limited to HDFS and can work with any Hadoop-supported storage.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了一个统一的大数据运行时。HDFS，即 Hadoop 的文件系统，是 Spark 最常用的存储平台，因为它为通用硬件上的非结构化和半结构化数据提供了成本效益的存储。Spark
    不限于 HDFS，并且可以与任何 Hadoop 支持的存储一起工作。
- en: Hadoop supported storage means a storage format that can work with Hadoop's
    `InputFormat` and `OutputFormat` interfaces. `InputFormat` is responsible for
    creating `InputSplits` from input data and dividing it further into records. `OutputFormat`
    is responsible for writing to storage.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 支持的存储意味着一种可以与 Hadoop 的 `InputFormat` 和 `OutputFormat` 接口一起工作的存储格式。"InputFormat"
    负责从输入数据创建 `InputSplits` 并将其进一步分割成记录。"OutputFormat" 负责写入存储。
- en: 'We will start with writing to the local filesystem and then move over to loading
    data from HDFS. In the *Loading data from HDFS* recipe, we will cover the most
    common file format: regular text files. In the next recipe, we will cover how
    to use any `InputFormat` interface to load data in Spark. We will also explore
    loading data stored in Amazon S3, a leading cloud storage platform.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从写入本地文件系统开始，然后过渡到从 HDFS 加载数据。在 *从 HDFS 加载数据* 食谱中，我们将介绍最常用的文件格式：常规文本文件。在下一个食谱中，我们将介绍如何在
    Spark 中使用任何 `InputFormat` 接口加载数据。我们还将探索从 Amazon S3 加载数据，这是一个领先的云存储平台。
- en: We will explore loading data from Apache Cassandra, which is a NoSQL database.
    Finally, we will explore loading data from a relational database.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探索从 Apache Cassandra 加载数据，这是一个 NoSQL 数据库。最后，我们将探索从关系型数据库加载数据。
- en: Loading data from the local filesystem
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从本地文件系统加载数据
- en: Though the local filesystem is not a good fit to store big data due to disk
    size limitations and lack of distributed nature, technically you can load data
    in distributed systems using the local filesystem. But then the file/directory
    you are accessing has to be available on each node.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然由于磁盘大小限制和缺乏分布式特性，本地文件系统不适合存储大数据，但从技术上讲，您可以使用本地文件系统在分布式系统中加载数据。但此时您访问的文件/目录必须在每个节点上可用。
- en: Please note that if you are planning to use this feature to load side data,
    it is not a good idea. To load side data, Spark has a broadcast variable feature,
    which will be discussed in upcoming chapters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您计划使用此功能来加载旁路数据，这不是一个好主意。要加载旁路数据，Spark 有广播变量功能，将在后续章节中讨论。
- en: In this recipe, we will look at how to load data in Spark from the local filesystem.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将探讨如何在 Spark 中从本地文件系统加载数据。
- en: How to do it...
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Let''s start with the example of Shakespeare''s "to be or not to be":'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从莎士比亚的 "to be or not to be" 的例子开始：
- en: 'Create the `words` directory by using the following command:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令创建 `words` 目录：
- en: '[PRE0]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Get into the `words` directory:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入 `words` 目录：
- en: '[PRE1]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create the `sh.txt` text file and enter `"to be or not to be"` in it:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `sh.txt` 文本文件，并在其中输入 `"to be or not to be"`：
- en: '[PRE2]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Start the Spark shell:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 Spark shell：
- en: '[PRE3]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Load the `words` directory as RDD:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `words` 目录作为 RDD 加载：
- en: '[PRE4]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Count the number of lines:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算行数：
- en: '[PRE5]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Divide the line (or lines) into multiple words:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将行（或行）分割成多个单词：
- en: '[PRE6]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Convert `word` to (word,1)—that is, output `1` as the value for each occurrence
    of `word` as a key:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `word` 转换为 (word,1)—即，将 `1` 作为 `word` 作为键的每个出现的值：
- en: '[PRE7]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Use the `reduceByKey` method to add the number of occurrences for each word
    as a key (this function works on two consecutive values at a time, represented
    by `a` and `b`):'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `reduceByKey` 方法将每个键的单词出现次数相加（此函数一次处理两个连续的值，分别用 `a` 和 `b` 表示）：
- en: '[PRE8]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Print the RDD:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印 RDD：
- en: '[PRE9]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Doing all of the preceding operations in one step is as follows:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有前面的操作合并为一步如下：
- en: '[PRE10]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This gives the following output:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![How to do it...](img/B03056_03_01.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![如何做这件事...](img/B03056_03_01.jpg)'
- en: Loading data from HDFS
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 HDFS 加载数据
- en: HDFS is the most widely used big data storage system. One of the reasons for
    the wide adoption of HDFS is schema-on-read. What this means is that HDFS does
    not put any restriction on data when data is being written. Any and all kinds
    of data are welcome and can be stored in a raw format. This feature makes it ideal
    storage for raw unstructured data and semi-structured data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS 是最广泛使用的海量数据存储系统。HDFS 得到广泛采用的一个原因是它支持在读取时进行模式定义。这意味着 HDFS 在数据写入时不对数据进行任何限制。任何类型的数据都受欢迎，并且可以以原始格式存储。这一特性使其成为原始非结构化数据和半结构化数据的理想存储。
- en: When it comes to reading data, even unstructured data needs to be given some
    structure to make sense. Hadoop uses `InputFormat` to determine how to read the
    data. Spark provides complete support for Hadoop's `InputFormat` so anything that
    can be read by Hadoop can be read by Spark as well.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到读取数据时，即使是非结构化数据也需要一些结构才能有意义。Hadoop 使用 `InputFormat` 来确定如何读取数据。Spark 为 Hadoop
    的 `InputFormat` 提供了完全支持，因此任何可以被 Hadoop 读取的数据也可以被 Spark 读取。
- en: The default `InputFormat` is `TextInputFormat`. `TextInputFormat` takes the
    byte offset of a line as a key and the content of a line as a value. Spark uses
    the `sc.textFile` method to read using `TextInputFormat`. It ignores the byte
    offset and creates an RDD of strings.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的 `InputFormat` 是 `TextInputFormat`。`TextInputFormat` 以行的字节偏移量作为键，以行的内容作为值。Spark
    使用 `sc.textFile` 方法通过 `TextInputFormat` 读取。它忽略字节偏移量，并创建一个字符串 RDD。
- en: Sometimes the filename itself contains useful information, for example, time-series
    data. In that case, you may want to read each file separately. The `sc.wholeTextFiles`
    method allows you to do that. It creates an RDD with the filename and path (for
    example, `hdfs://localhost:9000/user/hduser/words`) as a key and the content of
    the whole file as the value.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 有时文件名本身就包含有用的信息，例如时间序列数据。在这种情况下，您可能希望单独读取每个文件。`sc.wholeTextFiles` 方法允许您这样做。它使用文件名和路径（例如，`hdfs://localhost:9000/user/hduser/words`）作为键，并将整个文件的内容作为值。
- en: Spark also supports reading various serialization and compression-friendly formats
    such as Avro, Parquet, and JSON using DataFrames. These formats will be covered
    in coming chapters.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 还支持使用 DataFrames 读取各种序列化和压缩友好的格式，如 Avro、Parquet 和 JSON。这些格式将在后续章节中介绍。
- en: In this recipe, we will look at how to load data in the Spark shell from HDFS.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将探讨如何在 Spark shell 中从 HDFS 加载数据。
- en: How to do it...
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做这件事...
- en: 'Let''s do the word count, which counts the number of occurrences of each word.
    In this recipe, we will load data from HDFS:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行词频统计，即统计每个单词出现的次数。在这个菜谱中，我们将从 HDFS 加载数据：
- en: 'Create the `words` directory by using the following command:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令创建 `words` 目录：
- en: '[PRE11]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Change the directory to `words`:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目录更改为 `words`：
- en: '[PRE12]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create the `sh.txt text` file and enter `"to be or not to be"` in it:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `sh.txt text` 文件，并在其中输入 `"to be or not to be"`：
- en: '[PRE13]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Start the Spark shell:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 Spark shell：
- en: '[PRE14]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Load the `words` directory as the RDD:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `words` 目录作为 RDD 加载：
- en: '[PRE15]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `sc.textFile` method also supports passing an additional argument for the
    number of partitions. By default, Spark creates one partition for each `InputSplit`
    class, which roughly corresponds to one block.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`sc.textFile` 方法也支持传递一个额外的参数来指定分区数。默认情况下，Spark 为每个 `InputSplit` 类创建一个分区，这大致对应于一个块。'
- en: You can ask for a higher number of partitions. It works really well for compute-intensive
    jobs such as in machine learning. As one partition cannot contain more than one
    block, having fewer partitions than blocks is not allowed.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以请求更高的分区数。这对于计算密集型作业，如机器学习，效果非常好。由于一个分区不能包含多个块，因此不允许分区数少于块数。
- en: 'Count the number of lines (the result will be `1`):'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算行数（结果将是 `1`）：
- en: '[PRE16]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Divide the line (or lines) into multiple words:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将行（或行）分割成多个单词：
- en: '[PRE17]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Convert word to (word,1)—that is, output `1` as a value for each occurrence
    of `word` as a key:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将单词转换为（单词，1）——即对于每个作为键的 `word` 的出现，输出 `1` 作为值：
- en: '[PRE18]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Use the `reduceByKey` method to add the number of occurrences of each word
    as a key (this function works on two consecutive values at a time, represented
    by `a` and `b`):'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `reduceByKey` 方法将每个单词出现的次数作为键（此函数一次处理两个连续的值，分别表示为 `a` 和 `b`）：
- en: '[PRE19]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Print the RDD:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印 RDD：
- en: '[PRE20]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Doing all of the preceding operations in one step is as follows:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有前面的操作合并为一步如下：
- en: '[PRE21]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This gives the following output:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![How to do it...](img/B03056_03_01.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![如何做这件事...](img/B03056_03_01.jpg)'
- en: There's more…
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: Sometimes we need to access the whole file at once. Sometimes the filename contains
    useful data like in the case of time-series. Sometimes you need to process more
    than one line as a record. `sparkContext.wholeTextFiles` comes to the rescue here.
    We will look at weather dataset from [ftp://ftp.ncdc.noaa.gov/pub/data/noaa/](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们需要一次性访问整个文件。有时文件名包含有用的数据，例如在时间序列的情况下。有时需要将多行作为一个记录来处理。`sparkContext.wholeTextFiles`在这里提供了帮助。我们将查看来自[ftp://ftp.ncdc.noaa.gov/pub/data/noaa/](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/)的天气数据集。
- en: 'Here''s what a top-level directory looks like:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个顶级目录的外观：
- en: '![There''s more…](img/B03056_03_02.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![还有更多…](img/B03056_03_02.jpg)'
- en: 'Looking into a particular year directory—for example, 1901 resembles the following
    screenshot:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 查看特定年份的目录，例如，1901看起来如下截图所示：
- en: '![There''s more…](img/B03056_03_03.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![还有更多…](img/B03056_03_03.jpg)'
- en: Data here is divided in such a way that each filename contains useful information,
    that is, USAF-WBAN-year, where USAF is the US air force station number and WBAN
    is the weather bureau army navy location number.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 数据以这种方式划分，每个文件名都包含有用的信息，即USAF-WBAN-year，其中USAF是美国空军站编号，WBAN是气象局陆军海军位置编号。
- en: You will also notice that all files are compressed as gzip with a `.gz` extension.
    Compression is handled automatically so all you need to do is to upload data in
    HDFS. We will come back to this dataset in the coming chapters.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你还会注意到所有文件都压缩为gzip格式，带有`.gz`扩展名。压缩是自动处理的，所以你只需要将数据上传到HDFS。我们将在接下来的章节中回到这个数据集。
- en: 'Since the whole dataset is not large, it can be uploaded in HDFS in the pseudo-distributed
    mode also:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于整个数据集不大，也可以在伪分布式模式下上传到HDFS：
- en: 'Download data:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载数据：
- en: '[PRE22]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Load the weather data in HDFS:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在HDFS中加载天气数据：
- en: '[PRE23]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Start the Spark shell:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell：
- en: '[PRE24]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Load weather data for 1901 in the RDD:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在RDD中加载1901年的天气数据：
- en: '[PRE25]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Cache weather in the RDD so that it is not recomputed every time it''s accessed:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将天气缓存到RDD中，以便每次访问时不需要重新计算：
- en: '[PRE26]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In Spark, there are various StorageLevels at which the RDD can be persisted.
    `rdd.cache` is a shorthand for the `rdd.persist(MEMORY_ONLY)` StorageLevel.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在Spark中，RDD可以持久化到各种StorageLevels。`rdd.cache`是`rdd.persist(MEMORY_ONLY)` StorageLevel的简写。
- en: 'Count the number of elements:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算元素数量：
- en: '[PRE27]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Since the whole contents of a file are loaded as an element, we need to manually
    interpret the data, so let''s load the first element:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于整个文件内容被作为一个元素加载，我们需要手动解释数据，所以让我们加载第一个元素：
- en: '[PRE28]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Read the value of the first RDD:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取第一个RDD的值：
- en: '[PRE29]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `firstElement` contains tuples in the form (string, string). Tuples can
    be accessed in two ways:'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`firstElement`包含形式为（字符串，字符串）的元组。元组可以通过两种方式访问：'
- en: Using a positional function starting with `_1`.
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以`_1`开始的定位函数。
- en: Using the `productElement` method, for example, `tuple.productElement(0)`. Indexes
    here start with `0` like most other methods.
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`productElement`方法，例如，`tuple.productElement(0)`。这里的索引从`0`开始，就像大多数其他方法一样。
- en: 'Split `firstValue` by lines:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按行拆分`firstValue`：
- en: '[PRE30]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Count the number of elements in `firstVals`:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算`firstVals`中的元素数量：
- en: '[PRE31]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The schema of weather data is very rich with the position of the text working
    as a delimiter. You can get more information about schemas at the national weather
    service website. Let''s get wind speed, which is from section 66-69 (in meter/sec):'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 天气数据的模式非常丰富，文本的位置作为分隔符。你可以在国家气象服务网站上获取更多关于模式的信息。让我们获取风速，它位于第66-69节（以米/秒为单位）：
- en: '[PRE32]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Loading data from HDFS using a custom InputFormat
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自定义InputFormat从HDFS加载数据
- en: 'Sometimes you need to load data in a specific format and `TextInputFormat`
    is not a good fit for that. Spark provides two methods for this purpose:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你需要以特定格式加载数据，而`TextInputFormat`并不适合这种情况。Spark为此提供了两种方法：
- en: '`sparkContext.hadoopFile`: This supports the old MapReduce API'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sparkContext.hadoopFile`：这支持旧的MapReduce API'
- en: '`sparkContext.newAPIHadoopFile`: This supports the new MapReduce API'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sparkContext.newAPIHadoopFile`：这支持新的MapReduce API'
- en: These two methods provide support for all of Hadoop's built-in InputFormats
    interfaces as well as any custom `InputFormat`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法为Hadoop的所有内置InputFormats接口以及任何自定义`InputFormat`提供了支持。
- en: How to do it...
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'We are going to load text data in key-value format and load it in Spark using
    `KeyValueTextInputFormat`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`KeyValueTextInputFormat`加载键值格式的文本数据并将其加载到Spark中：
- en: 'Create the `currency` directory by using the following command:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令创建`currency`目录：
- en: '[PRE33]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Change the current directory to `currency`:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将当前目录更改为`currency`：
- en: '[PRE34]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Create the `na.txt` text file and enter currency values in key-value format
    delimited by tab (key: country, value: currency):'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`na.txt`文本文件，并以制表符分隔键值格式输入货币值（键：国家，值：货币）：
- en: '[PRE35]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: You can create more files for each continent.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以为每个大洲创建更多文件。
- en: 'Upload the `currency` folder to HDFS:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`currency`文件夹上传到HDFS：
- en: '[PRE36]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Start the Spark shell:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell：
- en: '[PRE37]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Import statements:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入语句：
- en: '[PRE38]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Load the `currency` directory as the RDD:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`currency`目录作为RDD加载：
- en: '[PRE39]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Convert it from tuple of (Text,Text) to tuple of (String,String):'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其从（Text,Text）元组转换为（String,String）元组：
- en: '[PRE40]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Count the number of elements in the RDD:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算RDD中元素的数量：
- en: '[PRE41]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Print the values:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印值：
- en: '[PRE42]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![How to do it...](img/B03056_03_04.jpg)'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/B03056_03_04.jpg)'
- en: Note
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: You can use this approach to load data in any Hadoop-supported `InputFormat`
    interface.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用这种方法加载任何Hadoop支持的`InputFormat`接口的数据。
- en: Loading data from Amazon S3
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Amazon S3加载数据
- en: Amazon **Simple Storage Service** (**S3**) provides developers and IT teams
    with a secure, durable, and scalable storage platform. The biggest advantage of
    Amazon S3 is that there is no up-front IT investment and companies can build capacity
    (just by clicking a button a button) as they need.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊**简单存储服务**（**S3**）为开发人员和IT团队提供了一个安全、持久和可扩展的存储平台。亚马逊S3的最大优势是无需前期IT投资，公司可以根据需要构建容量（只需点击一下按钮即可）。
- en: Though Amazon S3 can be used with any compute platform, it integrates really
    well with Amazon's cloud services such as Amazon **Elastic Compute Cloud** (**EC2**)
    and Amazon **Elastic Block Storage** (**EBS**). For this reason, companies who
    use **Amazon Web Services** (**AWS**) are likely to have significant data is already
    stored on Amazon S3.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Amazon S3可以与任何计算平台一起使用，但它与亚马逊的云服务（如亚马逊**弹性计算云**（**EC2**）和亚马逊**弹性块存储**（**EBS**））集成得非常好。因此，使用**亚马逊网络服务**（**AWS**）的公司很可能已经在Amazon
    S3上存储了大量的数据。
- en: This makes a good case for loading data in Spark from Amazon S3 and that is
    exactly what this recipe is about.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是从Amazon S3加载数据到Spark的好案例，这正是本菜谱的内容。
- en: How to do it...
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s start with the AWS portal:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从AWS门户开始：
- en: Go to [http://aws.amazon.com](http://aws.amazon.com) and log in with your username
    and password.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往[http://aws.amazon.com](http://aws.amazon.com)，使用您的用户名和密码登录。
- en: Once logged in, navigate to **Storage & Content Delivery** | **S3** | **Create
    Bucket**:![How to do it...](img/B03056_03_05.jpg)
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录后，导航到**存储与内容分发** | **S3** | **创建存储桶**：![如何操作...](img/B03056_03_05.jpg)
- en: Enter the bucket name—for example, `com.infoobjects.wordcount`. Please make
    sure you enter a unique bucket name (no two S3 buckets can have the same name
    globally).
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入存储桶名称——例如，`com.infoobjects.wordcount`。请确保您输入了一个唯一的存储桶名称（全球范围内没有两个S3存储桶可以具有相同的名称）。
- en: Select **Region**, click on **Create**, and then on the bucket name you created
    and you will see the following screen:![How to do it...](img/B03056_03_06.jpg)
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**区域**，点击**创建**，然后点击您创建的存储桶名称，您将看到以下屏幕：![如何操作...](img/B03056_03_06.jpg)
- en: Click on **Create Folder** and enter `words` as the folder name.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建文件夹**，并将文件夹名称输入为`words`。
- en: 'Create the `sh.txt` text file on the local filesystem:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地文件系统上创建`sh.txt`文本文件：
- en: '[PRE43]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Navigate to **Words** | **Upload** | **Add Files** and choose `sh.txt` from
    the dialog box, as shown in the following screenshot:![How to do it...](img/B03056_03_07.jpg)
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到**单词** | **上传** | **添加文件**，并在对话框中选择`sh.txt`，如图所示：![如何操作...](img/B03056_03_07.jpg)
- en: Click on **Start Upload**.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**开始上传**。
- en: Select **sh.txt** and click on **Properties** and it will show you details of
    the file:![How to do it...](img/B03056_03_08.jpg)
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**sh.txt**并点击**属性**，它将显示文件的详细信息：![如何操作...](img/B03056_03_08.jpg)
- en: Set `AWS_ACCESS_KEY` and `AWS_SECRET_ACCESS_KEY` as environment variables.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`AWS_ACCESS_KEY`和`AWS_SECRET_ACCESS_KEY`设置为环境变量。
- en: 'Open the Spark shell and load the `words` directory from `s3` in the `words`
    RDD:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Spark shell，并将`words`目录从`s3`加载到`words` RDD中：
- en: '[PRE44]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Now the RDD is loaded and you can continue doing regular transformations and
    actions on the RDD.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在RDD已加载，您可以在RDD上继续进行常规转换和操作。
- en: Note
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Sometimes there is confusion between `s3://` and `s3n://`. `s3n://` means a
    regular file sitting in the S3 bucket but readable and writable by the outside
    world. This filesystem puts a 5 GB limit on the file size.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 有时会在`s3://`和`s3n://`之间产生混淆。`s3n://`表示位于S3存储桶中的常规文件，但对外部世界来说是可读和可写的。这个文件系统对文件大小设置了5GB的限制。
- en: '`s3://` means an HDFS file sitting in the S3 bucket. It is a block-based filesystem.
    The filesystem requires you to dedicate a bucket for this filesystem. There is
    no limit on file size in this system.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`s3://`表示位于S3桶中的HDFS文件。它是一个基于块的文件系统。该文件系统要求你为这个文件系统指定一个桶。在这个系统中，文件大小没有限制。'
- en: Loading data from Apache Cassandra
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Apache Cassandra加载数据
- en: Apache Cassandra is a NoSQL database with a masterless ring cluster structure.
    While HDFS is a good fit for streaming data access, it does not work well with
    random access. For example, HDFS will work well when your average file size is
    100 MB and you want to read the whole file. If you frequently access the *n*th
    line in a file or some other part as a record, HDFS would be too slow.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Cassandra是一个无主环集群结构的NoSQL数据库。虽然HDFS适合流式数据访问，但它不适合随机访问。例如，当你的平均文件大小为100
    MB且你想读取整个文件时，HDFS会工作得很好。如果你经常访问文件中的*n*行或其他部分作为记录，HDFS会太慢。
- en: Relational databases have traditionally provided a solution to that, providing
    low latency, random access, but they do not work well with big data. NoSQL databases
    such as Cassandra fill the gap by providing relational database type access but
    in a distributed architecture on commodity servers.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 关系型数据库传统上提供了解决方案，提供了低延迟和随机访问，但它们不适合大数据。例如，Cassandra这样的NoSQL数据库通过提供关系型数据库类型的访问来填补这一空白，但是在分布式架构和商用服务器上。
- en: In this recipe, we will load data from Cassandra as a Spark RDD. To make that
    happen Datastax, the company behind Cassandra, has contributed `spark-cassandra-connector`.
    This connector lets you load Cassandra tables as Spark RDDs, write Spark RDDs
    back to Cassandra, and execute CQL queries.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将从Cassandra加载数据作为Spark RDD。为此，Cassandra背后的公司Datastax贡献了`spark-cassandra-connector`。这个连接器允许你将Cassandra表加载为Spark
    RDD，将Spark RDD写回Cassandra，并执行CQL查询。
- en: How to do it...
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Perform the following steps to load data from Cassandra:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '执行以下步骤从Cassandra加载数据:'
- en: 'Create a keyspace named `people` in Cassandra using the CQL shell:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用CQL shell在Cassandra中创建一个名为`people`的键空间：
- en: '[PRE45]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Create a column family (from CQL 3.0 onwards, it can also be called a **table**)
    `person` in newer versions of Cassandra:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '在Cassandra的新版本中创建一个名为`person`的列族（从CQL 3.0开始，也可以称为**表**）:'
- en: '[PRE46]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Insert a few records in the column family:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在列族中插入一些记录：
- en: '[PRE47]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Add Cassandra connector dependency to SBT:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '将Cassandra连接器依赖项添加到SBT:'
- en: '[PRE48]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'You can also add the Cassandra dependency to Maven:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '你也可以将Cassandra依赖项添加到Maven中:'
- en: '[PRE49]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Alternatively, you can also download the `spark-cassandra-connector` JAR to
    use directly with the Spark shell:'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，你也可以下载`spark-cassandra-connector` JAR，直接与Spark shell一起使用：
- en: '[PRE50]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Note
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you would like to build the `uber` JAR with all dependencies, refer to the
    *There's more…* section.
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你想要构建包含所有依赖项的`uber` JAR，请参阅*还有更多…*部分。
- en: Now start the Spark shell.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在启动Spark shell。
- en: 'Set the `spark.cassandra.connection.host` property in the Spark shell:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Spark shell中设置`spark.cassandra.connection.host`属性：
- en: '[PRE51]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Import Cassandra-specific libraries:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '导入Cassandra特定的库:'
- en: '[PRE52]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Load the `person` column family as an RDD:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`person`列族加载为RDD：
- en: '[PRE53]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Count the number of records in the RDD:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '计算RDD中的记录数:'
- en: '[PRE54]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Print data in the RDD:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '在RDD中打印数据:'
- en: '[PRE55]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Retrieve the first row:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '检索第一行:'
- en: '[PRE56]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Get the column names:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '获取列名:'
- en: '[PRE57]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Cassandra can also be accessed through Spark SQL. It has a wrapper around `SQLContext`
    called `CassandraSQLContext`; let''s load it:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cassandra也可以通过Spark SQL访问。它有一个围绕`SQLContext`的包装器，称为`CassandraSQLContext`；让我们加载它：
- en: '[PRE58]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Load the `person` data as `SchemaRDD`:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '将`person`数据加载为`SchemaRDD`:'
- en: '[PRE59]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Retrieve the `person` data:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '检索`person`数据:'
- en: '[PRE60]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: There's more...
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Spark Cassandra's connector library has a lot of dependencies. The connector
    itself and several of its dependencies are third-party to Spark and are not available
    as part of the Spark installation.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Cassandra的连接器库有很多依赖项。连接器本身及其一些依赖项是Spark的第三方组件，并且不是Spark安装的一部分。
- en: These dependencies need to be made available to the driver as well as executors
    at runtime. One way to do this is to bundle all transitive dependencies, but that
    is a laborious and error-prone process. The recommended approach is to bundle
    all the dependencies along with the connector library. This will result in a fat
    JAR, popularly known as the `uber` JAR.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这些依赖项需要在运行时对驱动程序和执行器都可用。一种方法是将所有传递依赖项捆绑在一起，但这是一个费时且容易出错的过程。推荐的方法是将所有依赖项以及连接器库捆绑在一起。这将导致一个胖JAR，通常称为`uber`
    JAR。
- en: 'SBT provides the `sbt-assembly` plugin, which makes creating `uber` JARs very
    easy. The following are the steps to create an `uber` JAR for `spark-cassandra-connector`.
    These steps are general enough so that you can use them to create any `uber` JAR:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: SBT 提供了 `sbt-assembly` 插件，这使得创建 `uber` JAR 非常容易。以下是为 `spark-cassandra-connector`
    创建 `uber` JAR 的步骤。这些步骤足够通用，你可以使用它们来创建任何 `uber` JAR：
- en: 'Create a folder named `uber`:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `uber` 的文件夹：
- en: '[PRE61]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Change the directory to `uber`:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目录更改为 `uber`：
- en: '[PRE62]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Open the SBT prompt:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 SBT 提示符：
- en: '[PRE63]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Give this project a name `sc-uber`:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给这个项目命名为 `sc-uber`：
- en: '[PRE64]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Save the session:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存会话：
- en: '[PRE65]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Exit the session:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 退出会话：
- en: '[PRE66]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'This will create `build.sbt`, `project`, and `target` folders in the `uber`
    folder as shown in the following screenshot:'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将在 `uber` 文件夹中创建 `build.sbt`、`project` 和 `target` 文件夹，如下截图所示：
- en: '![There''s more...](img/B03056_03_09.jpg)'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![还有更多...](img/B03056_03_09.jpg)'
- en: 'Add the `spark-cassandra-driver` dependency to `build.sbt` at the end after
    leaving a blank line as shown in the following screenshot:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `build.sbt` 文件的末尾添加 `spark-cassandra-driver` 依赖项，并在下面留一个空行，如下截图所示：
- en: '[PRE67]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '![There''s more...](img/B03056_03_10.jpg)'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![还有更多...](img/B03056_03_10.jpg)'
- en: We will use `MergeStrategy.first` as the default. Besides that, there are some
    files, such as `manifest.mf`, that every JAR bundles for metadata, and we can
    simply discard them. We are going to use `MergeStrategy.discard` for that. The
    following is the screenshot of `build.sbt` with `assemblyMergeStrategy` added:![There's
    more...](img/B03056_03_11.jpg)
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用 `MergeStrategy.first` 作为默认策略。除此之外，还有一些文件，例如 `manifest.mf`，每个 JAR 都会打包用于元数据，我们可以简单地丢弃它们。我们将使用
    `MergeStrategy.discard` 来处理这些文件。以下是在 `build.sbt` 中添加了 `assemblyMergeStrategy`
    的截图：![还有更多...](img/B03056_03_11.jpg)
- en: 'Now create `plugins.sbt` in the `project` folder and type the following for
    the `sbt-assembly` plugin:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在在 `project` 文件夹中创建 `plugins.sbt` 并为 `sbt-assembly` 插件输入以下内容：
- en: '[PRE68]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We are ready to build (`assembly`) a JAR now:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在准备好构建（`assembly`）一个 JAR：
- en: '[PRE69]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The `uber` JAR is now created in `target/scala-2.10/sc-uber-assembly-0.1-SNAPSHOT.jar`.
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`uber` JAR 已在 `target/scala-2.10/sc-uber-assembly-0.1-SNAPSHOT.jar` 中创建。'
- en: 'Copy it to a suitable location where you keep all third-party JARs—for example,
    `/home/hduser/thirdparty`—and rename it to an easier name (unless you like longer
    names):'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其复制到保存所有第三方 JAR 的合适位置，例如 `/home/hduser/thirdparty`，并将其重命名为一个更容易的名字（除非你喜欢更长的名字）：
- en: '[PRE70]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Load the Spark shell with the `uber` JAR using `--jars`:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `--jars` 选项加载带有 `uber` JAR 的 Spark shell：
- en: '[PRE71]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'To submit the Scala code to a cluster, you can call `spark-submit` with the
    same JARS option:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要将 Scala 代码提交到集群，你可以使用相同的 JARS 选项调用 `spark-submit`：
- en: '[PRE72]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Merge strategies in sbt-assembly
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: sbt-assembly 中的合并策略
- en: If multiple JARs have files with the same name and the same relative path, the
    default merge strategy for the `sbt-assembly` plugin is to verify that content
    is same for all the files and error out otherwise. This strategy is called `MergeStrategy.deduplicate`.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果多个 JAR 有相同名称和相同相对路径的文件，`sbt-assembly` 插件的默认合并策略是验证所有文件的内容是否相同，如果不相同则报错。这种策略称为
    `MergeStrategy.deduplicate`。
- en: 'The following are the available merge strategies in the `sbt-assembly plugin`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在 `sbt-assembly 插件` 中可用的合并策略：
- en: '| Strategy name | Description |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 策略名称 | 描述 |'
- en: '| --- | --- |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `MergeStrategy.deduplicate` | The default strategy |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| `MergeStrategy.deduplicate` | 默认策略 |'
- en: '| `MergeStrategy.first` | Picks first file according to classpath |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| `MergeStrategy.first` | 根据类路径选择第一个文件 |'
- en: '| `MergeStrategy.last` | Picks last file according to classpath |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| `MergeStrategy.last` | 根据类路径选择最后一个文件 |'
- en: '| `MergeStrategy.singleOrError` | Errors out (merge conflict not expected)
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| `MergeStrategy.singleOrError` | 报错（预期不会出现合并冲突） |'
- en: '| `MergeStrategy.concat` | Concatenates all matching files together |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| `MergeStrategy.concat` | 将所有匹配的文件连接在一起 |'
- en: '| `MergeStrategy.filterDistinctLines` | Concatenates leaving out duplicates
    |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| `MergeStrategy.filterDistinctLines` | 连接，忽略重复项 |'
- en: '| `MergeStrategy.rename` | Renames files |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| `MergeStrategy.rename` | 重命名文件 |'
- en: Loading data from relational databases
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从关系型数据库加载数据
- en: A lot of important data lies in relational databases that Spark needs to query.
    JdbcRDD is a Spark feature that allows relational tables to be loaded as RDDs.
    This recipe will explain how to use JdbcRDD.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 许多重要数据都存储在 Spark 需要查询的关系型数据库中。JdbcRDD 是 Spark 的一个功能，允许关系型表被加载为 RDD。本食谱将解释如何使用
    JdbcRDD。
- en: Spark SQL to be introduced in the next chapter includes a data source for JDBC.
    This should be preferred over the current recipe as results are returned as DataFrames
    (to be introduced in the next chapter), which can be easily processed by Spark
    SQL and also joined with other data sources.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中将要介绍的 Spark SQL 包含了一个 JDBC 数据源。这应该比当前的配方更受欢迎，因为结果以 DataFrames（将在下一章介绍）的形式返回，这些数据帧可以很容易地由
    Spark SQL 处理，并且还可以与其他数据源联合。
- en: Getting ready
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Please make sure that the JDBC driver JAR is visible on the client node and
    all slaves nodes on which executor will run.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保 JDBC 驱动程序 JAR 在客户端节点上可见，以及在所有将运行执行器的从节点上。
- en: How to do it...
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Perform the following steps to load data from relational databases:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤从关系型数据库加载数据：
- en: 'Create a table named `person` in MySQL using the following DDL:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下 DDL 在 MySQL 中创建一个名为 `person` 的表：
- en: '[PRE73]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Insert some data:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插入一些数据：
- en: '[PRE74]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Download `mysql-connector-java-x.x.xx-bin.jar` from [http://dev.mysql.com/downloads/connector/j/](http://dev.mysql.com/downloads/connector/j/).
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 [http://dev.mysql.com/downloads/connector/j/](http://dev.mysql.com/downloads/connector/j/)
    下载 `mysql-connector-java-x.x.xx-bin.jar`。
- en: 'Make the MySQL driver available to the Spark shell and launch it:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使 MySQL 驱动程序对 Spark shell 可用并启动它：
- en: '[PRE75]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Note
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Please note that `path-to-mysql-jar` is not the actual path name. You should
    use the actual path name.
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，`path-to-mysql-jar` 不是实际的路径名称。您应该使用实际的路径名称。
- en: 'Create variables for the username, password, and JDBC URL:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为用户名、密码和 JDBC URL 创建变量：
- en: '[PRE76]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Import JdbcRDD:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 JdbcRDD：
- en: '[PRE77]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Import JDBC-related classes:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 JDBC 相关类：
- en: '[PRE78]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Create an instance of the JDBC driver:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 JDBC 驱动程序的实例：
- en: '[PRE79]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Load JdbcRDD:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 JdbcRDD：
- en: '[PRE80]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now query the results:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在查询结果：
- en: '[PRE81]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Save the RDD to HDFS:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 RDD 保存到 HDFS：
- en: '[PRE82]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: How it works…
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'JdbcRDD is an RDD that executes a SQL query on a JDBC connection and retrieves
    the results. The following is a JdbcRDD constructor:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: JdbcRDD 是一个在 JDBC 连接上执行 SQL 查询并检索结果的 RDD。以下是一个 JdbcRDD 构造函数：
- en: '[PRE83]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: The two ?'s are bind variables for a prepared statement inside JdbcRDD. The
    first ? is for the offset (lower bound), that is, which row should we start computing
    with, the second ? is for the limit (upper bound), that is, how many rows should
    we read.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 两个 ? 是 JdbcRDD 内部预处理语句的绑定变量。第一个 ? 是偏移量（下限），即我们应该从哪一行开始计算，第二个 ? 是限制（上限），即我们应该读取多少行。
- en: JdbcRDD is a great way to load data in Spark directly from relational databases
    on an ad-hoc basis. If you would like to load data in bulk from RDBMS, there are
    other approaches that would work better, for example, Apache Sqoop is a powerful
    tool that imports and exports data from relational databases to HDFS.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: JdbcRDD 是一种在 Spark 中直接从关系型数据库加载数据的便捷方式。如果您想从 RDBMS 批量加载数据，还有其他更有效的方法，例如，Apache
    Sqoop 是一个强大的工具，可以从关系型数据库导入和导出到 HDFS。
