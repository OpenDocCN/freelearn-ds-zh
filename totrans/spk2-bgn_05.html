<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Spark Data Analysis with Python"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Spark Data Analysis with Python</h1></div></div></div><p>The ultimate goal of processing data is to use the results for answering business questions. It is very important to understand the data that is being used to answer the business questions. To understand the data better, various tabulation methods, charting, and plotting techniques are used. Visual representation of the data reinforces the understanding of the underlying data. Because of this, data visualization is used extensively in data analysis.</p><p>There are different terms that are used in various publications to mean the analysis of data for answering business questions. Data analysis, data analytics, and business intelligence, are some of the ubiquitous terms floating around. This chapter is not going to delve into the discussion on the meaning, similarities, or differences of these terms. On the other hand, the focus is going to be on how to bridge the gap between two major activities typically done by data scientists or data analysts. The first one being data processing. The second one is the use of the processed data to do analysis with the help of charting and plotting. Data analysis is the forte of data analysts and data scientists. This chapter is going to focus on the usage of Spark and Python to process the data, and produce charts and plots.</p><p>In many data analysis use cases, a super-set of data is processed and the reduced resultant dataset is used for the data analysis. This is specifically valid in the case of big data analysis where a small set of processed data is used for analysis. Depending on the use case, for various data analysis needs, appropriate data processing is done as a prerequisite. Most of the use cases that are going to be covered in this chapter fall into this model, where the first step deals with the necessary data processing, and the second step deals with the charting and plotting required for the data analysis.</p><p>In typical data analysis use cases, the chain of activities involves an extensive and multi-staged <span class="strong"><strong>Extract</strong></span>,<span class="strong"><strong> Transform</strong></span>, and <span class="strong"><strong>Load</strong></span> (<span class="strong"><strong>ETL</strong></span>) pipeline ending with a data analysis platform or application. The end result of this chain of activities includes, but is not limited to, tables of summary data and various visual representations of the data in the form of charts and plots. Since Spark can process data from heterogeneous distributed data sources very effectively, the huge ETL pipeline that existed in legacy data analysis applications can be consolidated into self-contained applications that do the data processing and data analysis.</p><p>We will cover the following topics in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Charting and plotting libraries</li><li class="listitem" style="list-style-type: disc">Setting up a dataset</li><li class="listitem" style="list-style-type: disc">Capturing the high-level details of the data analysis use cases</li><li class="listitem" style="list-style-type: disc">Various charts and plots</li></ul></div><div class="section" title="Charting and plotting libraries"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec40"/>Charting and plotting libraries</h1></div></div></div><p>Python is a programming language heavily used by data analysts and data scientists these days. There are numerous scientific and statistical data processing libraries available, as well as charting and plotting libraries, that can be used in Python programs. Python is also widely used as a programming language to develop data processing applications in Spark. This brings in a great flexibility to have a unified data processing and data analysis framework with Spark, Python and Python libraries, enabling us to do scientific and statistical processing, and charting and plotting. There are numerous such libraries that work with Python. Out of all those, the<span class="strong"><strong>NumPy</strong></span> and <span class="strong"><strong>SciPy </strong></span>libraries are being used here to do numerical, statistical, and scientific data processing. The <span class="strong"><strong>matplotlib </strong></span>library is being used here to do the charting and plotting that produces 2D images.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip33"/>Tip</h3><p>It is very important to make sure that the  <span class="strong"><strong>NumPy</strong></span>, <span class="strong"><strong>SciPy</strong></span> and <span class="strong"><strong>matplotlib</strong></span> Python libraries are working fine with the Python installation before attempting the code samples given in this chapter. This has to be tested and verified in isolation before using it in Spark applications.</p></div></div><p>The block diagram shown in <span class="emphasis"><em>Figure 1</em></span> gives the overall structure of the application stack:</p><p>
</p><div class="mediaobject"><img alt="Charting and plotting libraries" src="graphics/image_05_002.jpg"/><div class="caption"><p>Figure 1</p></div></div><p>
</p></div></div>
<div class="section" title="Setting up a dataset"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec41"/>Setting up a dataset</h1></div></div></div><p>There are many public Datasets available for the consumption of the general public that can be used for education, research, and development purposes. The MovieLens website lets users rate and personalize movie recommendations. GroupLens Research published the rating Datasets from MovieLens. These datasets are available for download from their website, <a class="ulink" href="http://grouplens.org/datasets/movielens/">http://grouplens.org/datasets/movielens/</a>. In this chapter, the MovieLens 100K Dataset is being used to demonstrate the usage of distributed data processing with Spark in conjunction with Python, NumPy, SciPy, and matplotlib.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip34"/>Tip</h3><p>On the GroupLens Research website for the dataset download, apart from the preceding dataset, there are more voluminous datasets such as MovieLens 1M dataset, MovieLens 10M dataset, MovieLens 20M dataset, and MovieLens latest datasets available for download. Once the reader is quite familiar with the programs and has achieved a sufficient level of comfort playing around with data, these additional datasets can be used by the reader to do their own analysis work to strengthen the knowledge acquired from this chapter.</p></div></div><p>The MovieLens 100K dataset has data in multiple files. The following are the ones that are going to be used in the data analysis use cases of this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">u.user</code>: The demographic information about the users who have rated movies. The structure of the dataset is given as follows, reproduced as it is from the README file accompanying the dataset:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">User ID</li><li class="listitem" style="list-style-type: disc">Age</li><li class="listitem" style="list-style-type: disc">Gender</li><li class="listitem" style="list-style-type: disc">Occupation</li><li class="listitem" style="list-style-type: disc">Zip code</li></ul></div><p>
</p></li><li class="listitem" style="list-style-type: disc"><code class="literal">u.item</code>: The information about the movies that are rated by the users. The structure of the dataset is given as follows, reproduced as it is from the README file accompanying the dataset:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Movie ID</li><li class="listitem" style="list-style-type: disc">Movie title</li><li class="listitem" style="list-style-type: disc">Release date</li><li class="listitem" style="list-style-type: disc">Video release date</li><li class="listitem" style="list-style-type: disc">IMDb URL</li><li class="listitem" style="list-style-type: disc">Unknown</li><li class="listitem" style="list-style-type: disc">Action</li><li class="listitem" style="list-style-type: disc">Adventure</li><li class="listitem" style="list-style-type: disc">Animation</li><li class="listitem" style="list-style-type: disc">Children's</li><li class="listitem" style="list-style-type: disc">Comedy</li><li class="listitem" style="list-style-type: disc">Crime</li><li class="listitem" style="list-style-type: disc">Documentary</li><li class="listitem" style="list-style-type: disc">Drama</li><li class="listitem" style="list-style-type: disc">Fantasy</li><li class="listitem" style="list-style-type: disc">Film-Noir</li><li class="listitem" style="list-style-type: disc">Horror</li><li class="listitem" style="list-style-type: disc">Musical</li><li class="listitem" style="list-style-type: disc">Mystery</li><li class="listitem" style="list-style-type: disc">Romance</li><li class="listitem" style="list-style-type: disc">Sci-Fi</li><li class="listitem" style="list-style-type: disc">Thriller</li><li class="listitem" style="list-style-type: disc">War</li><li class="listitem" style="list-style-type: disc">Western</li></ul></div><p>
</p></li></ul></div></div>
<div class="section" title="Data analysis use cases"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec42"/>Data analysis use cases</h1></div></div></div><p>The following list captures the high-level details of the data analysis use cases. Most of the use cases are revolving around the creation of various charts and plots:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Plot the age distribution of the users who have rated the movies using a histogram.</li><li class="listitem" style="list-style-type: disc">Plot the age probability density chart of the users using the same data used to plot the histogram.</li><li class="listitem" style="list-style-type: disc">Plot the summary of the age distribution data to find the minimum, 25<sup>th</sup> percentile, median, 75<sup>th</sup> percentile, and maximum ages of the users.</li><li class="listitem" style="list-style-type: disc">Plot multiple charts or plots on the same figure to have a side-by-side comparison of the data.</li><li class="listitem" style="list-style-type: disc">Create a bar chart capturing the top 10 occupations in terms of the number of users who have rated the movies.</li><li class="listitem" style="list-style-type: disc">Create a stacked bar chart capturing the number of male and female users by their occupation who have rated the movies.</li><li class="listitem" style="list-style-type: disc">Create a pie chart capturing the bottom 10 occupations in terms of the number of who have rated the movies.</li><li class="listitem" style="list-style-type: disc">Create a donut chart capturing the top 10 zip codes in terms of the number of who have rated the movies.</li><li class="listitem" style="list-style-type: disc">Using three occupation categories, create box plots capturing the summary statistics of the users who have rated the movies. All three box plots have to be drawn on a single figure to enable comparison.</li><li class="listitem" style="list-style-type: disc">Create a bar chart capturing the number of movies by their genre.</li><li class="listitem" style="list-style-type: disc">Create a scatter plot capturing the top 10 years in terms of the number of movies released in each year.</li><li class="listitem" style="list-style-type: disc">Create a scatter plot capturing the top 10 years in terms of the number of movies released in each year. In this plot, instead of points in the plot, create circles with the area proportional to the number of movies released in that year.</li><li class="listitem" style="list-style-type: disc">Create a line graph with two datasets with one dataset being the number of action movies released over the last 10 years and the other dataset being the number of drama movies released over the last 10 years to facilitate a comparison.</li></ul></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip35"/>Tip</h3><p>In all the preceding use cases, when it comes to implementation, Spark is used to process the data and prepare the required dataset. Once the required processed data is available in Spark DataFrame, it is collected into the driver program. In other words, the data is transferred from the distributed collection of Spark into a local collection, as tuples in the Python program, for charting and plotting. For charting and plotting, Python needs the data locally. It cannot use Spark DataFrames directly to do charting and plotting.</p></div></div></div>
<div class="section" title="Charts and plots"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec43"/>Charts and plots</h1></div></div></div><p>This section is going to focus on creating various charts and plots to visually represent various aspects of the MovieLens 100K Dataset that are related to the use cases described in the preceding section. The charts and plots drawing process described throughout this chapter follows a pattern. Here are the important steps in that pattern of activities:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Read data from the data file using Spark.</li><li class="listitem">Make the data available in a Spark DataFrame.</li><li class="listitem">Apply the necessary data processing using DataFrame API.</li><li class="listitem">The processing is mainly to make available only the minimal and required data for charting and plotting purposes.</li><li class="listitem">Transfer the processed data from Spark DataFrame to the local Python collection object in the Spark Driver program.</li><li class="listitem">Use the charting and plotting libraries to generate the figures using the data available in the Python collection objects.</li></ol></div><div class="section" title="Histogram"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec21"/>Histogram</h2></div></div></div><p>A histogram is generally used to show how a given numerical dataset is distributed over consecutive non-overlapping intervals of equal size. The interval or bin size is chosen based on the dataset. The bin or interval represents the ranges of data. In this use case, the dataset consists of the ages of the users. In this case it does not make sense to have a bin size of 100 as there will be only one bin and the entire dataset will fall into it. The height of the bars representing the bins indicates the frequency of data items in that bin or interval.</p><p>The following set of commands are used to bring up the Python REPL of Spark, followed by the programs to do the data processing, charting, and plotting:</p><pre class="programlisting">
<span class="strong"><strong>$ cd $SPARK_HOME
$ ./bin/pyspark
&gt;&gt;&gt; # Import all the required libraries 
&gt;&gt;&gt; from pyspark.sql import Row
&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; import pylab as P
&gt;&gt;&gt; plt.rcdefaults()
&gt;&gt;&gt; # TODO - The following location has to be changed to the appropriate data file location
&gt;&gt;&gt; dataDir = "/Users/RajT/Documents/Writing/SparkForBeginners/SparkDataAnalysisWithPython/Data/ml-100k/"&gt;&gt;&gt; # Create the DataFrame of the user dataset
&gt;&gt;&gt; lines = sc.textFile(dataDir + "u.user")
&gt;&gt;&gt; splitLines = lines.map(lambda l: l.split("|"))
&gt;&gt;&gt; usersRDD = splitLines.map(lambda p: Row(id=p[0], age=int(p[1]), gender=p[2], occupation=p[3], zipcode=p[4]))
&gt;&gt;&gt; usersDF = spark.createDataFrame(usersRDD)
&gt;&gt;&gt; usersDF.createOrReplaceTempView("users")
&gt;&gt;&gt; usersDF.show()
      +---+------+---+-------------+-------+
    
      |age|gender| id|   occupation|zipcode|
    
      +---+------+---+-------------+-------+
    
      | 24|     M|  1|   technician|  85711|
    
      | 53|     F|  2|        other|  94043|
    
      | 23|     M|  3|       writer|  32067|
    
      | 24|     M|  4|   technician|  43537|
    
      | 33|     F|  5|        other|  15213|
    
      | 42|     M|  6|    executive|  98101|
    
      | 57|     M|  7|administrator|  91344|
    
      | 36|     M|  8|administrator|  05201|
    
      | 29|     M|  9|      student|  01002|
    
      | 53|     M| 10|       lawyer|  90703|
    
      | 39|     F| 11|        other|  30329|
    
      | 28|     F| 12|        other|  06405|
    
      | 47|     M| 13|     educator|  29206|
    
      | 45|     M| 14|    scientist|  55106|
    
      | 49|     F| 15|     educator|  97301|
    
      | 21|     M| 16|entertainment|  10309|
    
      | 30|     M| 17|   programmer|  06355|
    
      | 35|     F| 18|        other|  37212|
    
      | 40|     M| 19|    librarian|  02138|
    
      | 42|     F| 20|    homemaker|  95660|
    
      +---+------+---+-------------+-------+
    
      only showing top 20 rows
    &gt;&gt;&gt; # Create the DataFrame of the user dataset with only one column age
	&gt;&gt;&gt; ageDF = spark.sql("SELECT age FROM users")
	&gt;&gt;&gt; ageList = ageDF.rdd.map(lambda p: p.age).collect()
	&gt;&gt;&gt; ageDF.describe().show()
      +-------+------------------+
    
      |summary|               age|
    
      +-------+------------------+
    
      |  count|               943|
    
      |   mean| 34.05196182396607|
    
      | stddev|12.186273150937206|
    
      |    min|                 7|
    
      |    max|                73|
    
      +-------+------------------+
 &gt;&gt;&gt; # Age distribution of the users
 &gt;&gt;&gt; plt.hist(ageList)
 &gt;&gt;&gt; plt.title("Age distribution of the users\n")
 &gt;&gt;&gt; plt.xlabel("Age")
 &gt;&gt;&gt; plt.ylabel("Number of users")
 &gt;&gt;&gt; plt.show(block=False)</strong></span>
</pre><p>In the preceding section, the user dataset was read line by line to form the RDD. From the RDD, a Spark DataFrame was created. Using Spark SQL, another Spark DataFrame was created containing only the age column. The summary of that Spark DataFrame was displayed to show the summary statistics of the contents; the contents were collected into a local Python collection object. Using the collected data, a histogram of the age column was plotted, as given in <span class="emphasis"><em>Figure 2</em></span>:</p><p>
</p><div class="mediaobject"><img alt="Histogram" src="graphics/image_05_003.jpg"/><div class="caption"><p>Figure 2</p></div></div><p>
</p></div><div class="section" title="Density plot"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec22"/>Density plot</h2></div></div></div><p>There is another plot that is very close to a histogram. It is the density plot. Whenever there is a finite data sample with a need to estimate the probability density function of a random variable, density plots are used heavily. A histogram doesn't show when data smooths out or when there is continuity in the data points. For that purpose, density plots are used.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note36"/>Note</h3><p>Since a histogram and density plot are used for similar purposes, but show different behavior for the same data, generally, a histogram and density plot are used side by side in many applications.</p></div></div><p>
<span class="emphasis"><em>Figure 3</em></span> is a density plot drawn for the same dataset that is used to plot the histogram.</p><p>As a continuation of the same Python REPL of Spark, run the following commands:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; # Draw a density plot
&gt;&gt;&gt; from scipy.stats import gaussian_kde
&gt;&gt;&gt; density = gaussian_kde(ageList)
&gt;&gt;&gt; xAxisValues = np.linspace(0,100,1000)
&gt;&gt;&gt; density.covariance_factor = lambda : .5
&gt;&gt;&gt; density._compute_covariance()
&gt;&gt;&gt; plt.title("Age density plot of the users\n")
&gt;&gt;&gt; plt.xlabel("Age")
&gt;&gt;&gt; plt.ylabel("Density")
&gt;&gt;&gt; plt.plot(xAxisValues, density(xAxisValues))
&gt;&gt;&gt; plt.show(block=False)</strong></span>
</pre><p>
</p><div class="mediaobject"><img alt="Density plot" src="graphics/image_05_004.jpg"/><div class="caption"><p>Figure 3</p></div></div><p>
</p><p>In the preceding section, the same Spark DataFrame that was created containing only the age column was used and the contents were collected into a local Python collection object. Using the collected data, a density plot of the age column was plotted as given in <span class="emphasis"><em>Figure 3</em></span>, with the line space from 0 to 100 representing the age.</p><p>If multiple charts or plots are to be looked at side by side, the <span class="strong"><strong>matplotlib</strong></span> library provides ways to do that. Figure 4 shows a histogram and a box plot side by side.</p><p>As a continuation of the same Python REPL of Spark, run the following commands:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; # The following example demonstrates the creation of multiple diagrams
        in one figure
		&gt;&gt;&gt; # There are two plots on one row
		&gt;&gt;&gt; # The first one is the histogram of the distribution 
		&gt;&gt;&gt; # The second one is the boxplot containing the summary of the 
        distribution
		&gt;&gt;&gt; plt.subplot(121)
		&gt;&gt;&gt; plt.hist(ageList)
		&gt;&gt;&gt; plt.title("Age distribution of the users\n")
		&gt;&gt;&gt; plt.xlabel("Age")
		&gt;&gt;&gt; plt.ylabel("Number of users")
		&gt;&gt;&gt; plt.subplot(122)
		&gt;&gt;&gt; plt.title("Summary of distribution\n")
		&gt;&gt;&gt; plt.xlabel("Age")
		&gt;&gt;&gt; plt.boxplot(ageList, vert=False)
		&gt;&gt;&gt; plt.show(block=False)</strong></span>
</pre><p>
</p><div class="mediaobject"><img alt="Density plot" src="graphics/image_05_005.jpg"/><div class="caption"><p>Figure 4</p></div></div><p>
</p><p>In the preceding section, the same Spark DataFrame that was created containing only the age column was used, and the contents were collected into a local Python collection object. Using the collected data, a histogram of the age column was plotted along with a box plot containing indicators for the minimum, 25<sup>th</sup> percentile, median, 75<sup>th</sup> percentile, and maximum values, as given in <span class="emphasis"><em>Figure 4</em></span>. When drawing multiple charts or plots in one figure, for a way to control the layout, look at the method call <code class="literal">plt.subplot(121)</code>. This is talking about the selection of the plot laid out in one row and two columns, and selects the first one. In the same way, <code class="literal">plt.subplot(122)</code> talks about the selection of the plot laid out in one row and two columns, and selects the second one.</p></div><div class="section" title="Bar chart"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec23"/>Bar chart</h2></div></div></div><p>Bar charts can be drawn in different ways. The most common one is where the bars are standing vertically on the <span class="emphasis"><em>X</em></span> axis. Another variation is where the bars are drawn on the <span class="emphasis"><em>Y</em></span> axis and have the bars laid out horizontally. <span class="emphasis"><em>Figure 5</em></span> shows a horizontal bar chart.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note37"/>Note</h3><p>It is very common to get confused between a histogram and bar chart. The important difference is that a histogram is used to plot continuous but finite numerical values, but a bar chart is used to represent categorical data.</p></div></div><p>As a continuation of the same Python REPL of Spark, run the following commands:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; occupationsTop10 = spark.sql("SELECT occupation, count(occupation) as usercount FROM users GROUP BY occupation ORDER BY usercount DESC LIMIT 10")
&gt;&gt;&gt; occupationsTop10.show()
      +-------------+---------+
    
      |   occupation|usercount|
    
      +-------------+---------+
    
      |      student|      196|
    
      |        other|      105|
    
      |     educator|       95|
    
      |administrator|       79|
    
      |     engineer|       67|
    
      |   programmer|       66|
    
      |    librarian|       51|
    
      |       writer|       45|
    
      |    executive|       32|
    
      |    scientist|       31|
    
      +-------------+---------+
	  &gt;&gt;&gt; occupationsTop10Tuple = occupationsTop10.rdd.map(lambda p:
	  (p.occupation,p.usercount)).collect()
	  &gt;&gt;&gt; occupationsTop10List, countTop10List = zip(*occupationsTop10Tuple)
	  &gt;&gt;&gt; occupationsTop10Tuple
	  &gt;&gt;&gt; # Top 10 occupations in terms of the number of users having that
	  occupation who have rated movies
	  &gt;&gt;&gt; y_pos = np.arange(len(occupationsTop10List))
	  &gt;&gt;&gt; plt.barh(y_pos, countTop10List, align='center', alpha=0.4)
	  &gt;&gt;&gt; plt.yticks(y_pos, occupationsTop10List)
	  &gt;&gt;&gt; plt.xlabel('Number of users')
	  &gt;&gt;&gt; plt.title('Top 10 user types\n')
	  &gt;&gt;&gt; plt.gcf().subplots_adjust(left=0.15)
	  &gt;&gt;&gt; plt.show(block=False)</strong></span>
</pre><p> </p><div class="mediaobject"><img alt="Bar chart" src="graphics/image_05_006.jpg"/><div class="caption"><p>Figure 5</p></div></div><p>
</p><p>In the preceding section, a Spark DataFrame was created containing the top 10 occupations of the users in terms of the number of users who have rated movies. The data was collected into a Python collection object to plot the bar chart.</p><div class="section" title="Stacked bar chart"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec3"/>Stacked bar chart</h3></div></div></div><p>The bar chart that was drawn in the preceding section gives the top 10 user occupations in terms of the number of users. But that does not give details about how that number is made up in terms of the gender of the users. In this kind of situation, it is good to use a stacked bar chart with each bar showing the counts by gender. <span class="emphasis"><em>Figure 6</em></span> shows a stacked bar chart.</p><p>As a continuation of the same Python REPL of Spark, run the following commands:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; occupationsGender = spark.sql("SELECT occupation, gender FROM users")&gt;&gt;&gt; occupationsGender.show()
      +-------------+------+
    
      |   occupation|gender|
    
      +-------------+------+
    
      |   technician|     M|
    
      |        other|     F|
    
      |       writer|     M|
    
      |   technician|     M|
    
      |        other|     F|
    
      |    executive|     M|
    
      |administrator|     M|
    
      |administrator|     M|
    
      |      student|     M|
    
      |       lawyer|     M|
    
      |        other|     F|
    
      |        other|     F|
    
      |     educator|     M|
    
      |    scientist|     M|
    
      |     educator|     F|
    
      |entertainment|     M|
    
      |   programmer|     M|
    
      |        other|     F|
    
      |    librarian|     M|
    
      |    homemaker|     F|
    
      +-------------+------+
    
      only showing top 20 rows
    &gt;&gt;&gt; occCrossTab = occupationsGender.stat.crosstab("occupation", "gender")&gt;&gt;&gt; occCrossTab.show()
      +-----------------+---+---+
    
      |occupation_gender|  M|  F|
    
      +-----------------+---+---+
    
      |        scientist| 28|  3|
    
      |          student|136| 60|
    
      |           writer| 26| 19|
    
      |         salesman|  9|  3|
    
      |          retired| 13|  1|
    
      |    administrator| 43| 36|
    
      |       programmer| 60|  6|
    
      |           doctor|  7|  0|
    
      |        homemaker|  1|  6|
    
      |        executive| 29|  3|
    
      |         engineer| 65|  2|
    
      |    entertainment| 16|  2|
    
      |        marketing| 16| 10|
    
      |       technician| 26|  1|
    
      |           artist| 15| 13|
    
      |        librarian| 22| 29|
    
      |           lawyer| 10|  2|
    
      |         educator| 69| 26|
    
      |       healthcare|  5| 11|
    
      |             none|  5|  4|
    
      +-----------------+---+---+
    
      only showing top 20 rows
      &gt;&gt;&gt; occupationsCrossTuple = occCrossTab.rdd.map(lambda p:
	 (p.occupation_gender,p.M, p.F)).collect()
	 &gt;&gt;&gt; occList, mList, fList = zip(*occupationsCrossTuple)
	 &gt;&gt;&gt; N = len(occList)
	 &gt;&gt;&gt; ind = np.arange(N) # the x locations for the groups
	 &gt;&gt;&gt; width = 0.75 # the width of the bars
	 &gt;&gt;&gt; p1 = plt.bar(ind, mList, width, color='r')
	 &gt;&gt;&gt; p2 = plt.bar(ind, fList, width, color='y', bottom=mList)
	 &gt;&gt;&gt; plt.ylabel('Count')
	 &gt;&gt;&gt; plt.title('Gender distribution by occupation\n')
	 &gt;&gt;&gt; plt.xticks(ind + width/2., occList, rotation=90)
	 &gt;&gt;&gt; plt.legend((p1[0], p2[0]), ('Male', 'Female'))
	 &gt;&gt;&gt; plt.gcf().subplots_adjust(bottom=0.25)
	 &gt;&gt;&gt; plt.show(block=False)</strong></span>
</pre><p>
</p><div class="mediaobject"><img alt="Stacked bar chart" src="graphics/image_05_007.jpg"/><div class="caption"><p>Figure 6</p></div></div><p>
</p><p>In the preceding section, a Spark DataFrame was created containing only the occupation and gender columns. A cross-tab operation was done on that to produce another Spark DataFrame, which produced columns for occupation, male user count, and female user count. In the first Spark DataFrame, containing the occupation and gender columns, both are non-numerical columns and, because of that, it doesn't make sense to draw as chart or plot based on that data. But if a cross-tab operation is done on these two column values, for every distinct occupation field, then the counts of values of the gender column will be available. In this way, the occupation field becomes a categorical variable and it makes sense to draw a bar chart with the data. Since there are only two gender values in this data, it makes sense to have a stacked bar chart to see the total as well as the proportions of male and female user counts in each occupation category.</p><p>There are many statistical and mathematical functions available within DataFrames in Spark. The cross-tab operation on a Spark DataFrame comes in very handy in this kind of situation. With huge datasets, the cross-tab operation can become very processor-intensive and time consuming, but the distributed processing capabilities of Spark are a great help in this kind of situation.</p><p>Spark SQL comes with lots of mathematical and statistical data processing capabilities. The preceding sections used the <code class="literal">describe().show()</code> method on the <code class="literal">SparkDataFrame</code> objects. In those Spark DataFrames, the preceding method acted on the available numeric columns. There will be situations where there are multiple numeric columns and, in those situations, the preceding method has the ability to pick and choose the desired columns for getting the summary statistics. Similarly, there are methods to find covariance, correlation, and so on, on the data from the Spark DataFrame. The following code snippet demonstrates these methods:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; occCrossTab.describe('M', 'F').show()
      +-------+------------------+------------------+
    
      |summary|                 M|                 F|
    
      +-------+------------------+------------------+
    
      |  count|                21|                21|
    
      |   mean|31.904761904761905|              13.0|
    
      | stddev|31.595516200735347|15.491933384829668|
    
      |    min|                 1|                 0|
    
      |    max|               136|                60|
    
      +-------+------------------+------------------+
    &gt;&gt;&gt; occCrossTab.stat.cov('M', 'F')
      381.15
    &gt;&gt;&gt; occCrossTab.stat.corr('M', 'F')
      0.7416099517313641   </strong></span>
</pre></div></div><div class="section" title="Pie chart"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec24"/>Pie chart</h2></div></div></div><p>If there is a need to visually represent a dataset to explain the whole-part relationship, a pie chart is very commonly used. <span class="emphasis"><em>Figure 7</em></span> shows a pie chart.</p><p>As a continuation of the same Python REPL of Spark, run the following commands:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; occupationsBottom10 = spark.sql("SELECT occupation, count(occupation) as usercount FROM users GROUP BY occupation ORDER BY usercount LIMIT 10")
&gt;&gt;&gt; occupationsBottom10.show()
      +-------------+---------+
    
      |   occupation|usercount|
    
      +-------------+---------+
    
      |    homemaker|        7|
    
      |       doctor|        7|
    
      |         none|        9|
    
      |     salesman|       12|
    
      |       lawyer|       12|
    
      |      retired|       14|
    
      |   healthcare|       16|
    
      |entertainment|       18|
    
      |    marketing|       26|
    
      |   technician|       27|
    
      +-------------+---------+
    &gt;&gt;&gt; occupationsBottom10Tuple = occupationsBottom10.rdd.map(lambda p: (p.occupation,p.usercount)).collect()
	&gt;&gt;&gt; occupationsBottom10List, countBottom10List = zip(*occupationsBottom10Tuple)
	&gt;&gt;&gt; # Bottom 10 occupations in terms of the number of users having that occupation who have rated movies
	&gt;&gt;&gt; explode = (0, 0, 0, 0,0.1,0,0,0,0,0.1)
	&gt;&gt;&gt; plt.pie(countBottom10List, explode=explode, labels=occupationsBottom10List, autopct='%1.1f%%', shadow=True, startangle=90)
	&gt;&gt;&gt; plt.title('Bottom 10 user types\n')
	&gt;&gt;&gt; plt.show(block=False)</strong></span>
</pre><p>
</p><div class="mediaobject"><img alt="Pie chart" src="graphics/image_05_008.jpg"/><div class="caption"><p>Figure 7</p></div></div><p>
</p><p>In the preceding section, a Spark DataFrame was created containing the bottom 10 occupations of the users in terms of the number of users who have rated movies. The data was collected into a Python collection object to plot the pie chart.</p><div class="section" title="Donut chart"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec4"/>Donut chart</h3></div></div></div><p>Pie charts can be drawn in different forms. One such form, the donut chart, is often used these days. Figure 8 shows this donut chart variation of the pie chart.</p><p>As a continuation of the same Python REPL of Spark, run the following commands:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; zipTop10 = spark.sql("SELECT zipcode, count(zipcode) as usercount FROM users GROUP BY zipcode ORDER BY usercount DESC LIMIT 10")
&gt;&gt;&gt; zipTop10.show()
      +-------+---------+
    
      |zipcode|usercount|
    
      +-------+---------+
    
      |  55414|        9|
    
      |  55105|        6|
    
      |  20009|        5|
    
      |  55337|        5|
    
      |  10003|        5|
    
      |  55454|        4|
    
      |  55408|        4|
    
      |  27514|        4|
    
      |  11217|        3|
    
      |  14216|        3|
    
      +-------+---------+
    &gt;&gt;&gt; zipTop10Tuple = zipTop10.rdd.map(lambda p: (p.zipcode,p.usercount)).collect()
	&gt;&gt;&gt; zipTop10List, countTop10List = zip(*zipTop10Tuple)
	&gt;&gt;&gt; # Top 10 zipcodes in terms of the number of users living in that zipcode who have rated movies&gt;&gt;&gt; explode = (0.1, 0, 0, 0,0,0,0,0,0,0)  # explode a slice if required
	&gt;&gt;&gt; plt.pie(countTop10List, explode=explode, labels=zipTop10List, autopct='%1.1f%%', shadow=True)
	&gt;&gt;&gt; #Draw a circle at the center of pie to make it look like a donut
	&gt;&gt;&gt; centre_circle = plt.Circle((0,0),0.75,color='black', fc='white',linewidth=1.25)
	&gt;&gt;&gt; fig = plt.gcf()
	&gt;&gt;&gt; fig.gca().add_artist(centre_circle)
	&gt;&gt;&gt; # The aspect ratio is to be made equal. This is to make sure that pie chart is coming perfectly as a circle.
	&gt;&gt;&gt; plt.axis('equal')
	&gt;&gt;&gt; plt.text(- 0.25,0,'Top 10 zip codes')
	&gt;&gt;&gt; plt.show(block=False)</strong></span>
</pre><p>
</p><div class="mediaobject"><img alt="Donut chart" src="graphics/image_05_009.jpg"/><div class="caption"><p>Figure 8</p></div></div><p>
</p><p>In the preceding section, a Spark DataFrame was created containing the top 10 zip codes of the users in terms of the number of users who live in that area and who have rated movies. The data was collected into a Python collection object to plot the donut chart.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip38"/>Tip</h3><p>Compared to the other figures in this book, the title of <span class="emphasis"><em>Figure 8</em></span> is given in the middle. It is done using the <code class="literal">text()</code> method rather than using the <code class="literal">title()</code> method. This method can be used to print watermark text on the charts and plots.</p></div></div></div></div><div class="section" title="Box plot"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec25"/>Box plot</h2></div></div></div><p>Frequently, it is a common requirement to compare the summary statistics of different datasets in one figure. The box plot is a very common plot used to capture the summary statistics of a dataset in an intuitive way. The following section does exactly the same, and to do this, <span class="emphasis"><em>Figure 9</em></span> shows multiple box plots on a single figure.</p><p>As a continuation of the same Python REPL of Spark, run the following commands:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; ages = spark.sql("SELECT occupation, age FROM users WHERE occupation ='administrator' ORDER BY age")
&gt;&gt;&gt; adminAges = ages.rdd.map(lambda p: p.age).collect()
&gt;&gt;&gt; ages.describe().show()
      +-------+------------------+
    
      |summary|               age|
    
      +-------+------------------+
    
      |  count|                79|
    
      |   mean| 38.74683544303797|
    
      | stddev|11.052771408491363|
    
      |    min|                21|
    
      |    max|                70|
    
      +-------+------------------+
    &gt;&gt;&gt; ages = spark.sql("SELECT occupation, age FROM users WHERE occupation ='engineer' ORDER BY age")&gt;&gt;&gt; engAges = ages.rdd.map(lambda p: p.age).collect()
	&gt;&gt;&gt; ages.describe().show()
      +-------+------------------+
    
      |summary|               age|
    
      +-------+------------------+
    
      |  count|                67|
    
      |   mean| 36.38805970149254|
    
      | stddev|11.115345348003853|
    
      |    min|                22|
    
      |    max|                70|
    
      +-------+------------------+
    &gt;&gt;&gt; ages = spark.sql("SELECT occupation, age FROM users WHERE occupation ='programmer' ORDER BY age")&gt;&gt;&gt; progAges = ages.rdd.map(lambda p: p.age).collect()
	&gt;&gt;&gt; ages.describe().show()
      +-------+------------------+
    
      |summary|               age|
    
      +-------+------------------+
    
      |  count|                66|
    
      |   mean|33.121212121212125|
    
      | stddev| 9.551320948648684|
    
      |    min|                20|
    
      |    max|                63|
    
      +-------+------------------+
 &gt;&gt;&gt; # Box plots of the ages by profession
 &gt;&gt;&gt; boxPlotAges = [adminAges, engAges, progAges]
 &gt;&gt;&gt; boxPlotLabels = ['administrator','engineer', 'programmer' ]
 &gt;&gt;&gt; x = np.arange(len(boxPlotLabels))
 &gt;&gt;&gt; plt.figure()
 &gt;&gt;&gt; plt.boxplot(boxPlotAges)
 &gt;&gt;&gt; plt.title('Age summary statistics\n')
 &gt;&gt;&gt; plt.ylabel("Age")
 &gt;&gt;&gt; plt.xticks(x + 1, boxPlotLabels, rotation=0)
 &gt;&gt;&gt; plt.show(block=False)</strong></span>
</pre><p>
</p><div class="mediaobject"><img alt="Box plot" src="graphics/image_05_010.jpg"/><div class="caption"><p>Figure 9</p></div></div><p>
</p><p>In the preceding section, a Spark DataFrame was created with occupation and age columns for each of three occupations: administrator, engineer, and programmer. Box plots were created for each of these datasets on one figure, which contains indicators for the minimum, 25<sup>th</sup> percentile, median, 75<sup>th</sup> percentile, maximum, and outlier values for each of the datasets to facilitate comparison. The box plot for the programmer occupation shows two value points represented by the <code class="literal">+</code> symbol. They are outlier values.</p></div><div class="section" title="Vertical bar chart"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec26"/>Vertical bar chart</h2></div></div></div><p>In the preceding sections, the main dataset used for eliciting various charting and plotting use cases was the user data. The dataset that is taken up next is the movie dataset. In many datasets, to produce various charts and plots it will be a requirement to make the data suitable for the appropriate figure. Spark is rich with features to do the data processing.</p><p>The following use case demonstrates the preparation of data by applying some aggregation and using Spark SQL; the desired dataset is prepared for a classic bar chart containing the counts of movies by genre. <span class="emphasis"><em>Figure 10</em></span> shows the bar chart after applying the aggregation operation on the movie data.</p><p>As a continuation of the same Python REPL of Spark, run the following commands:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; movieLines = sc.textFile(dataDir + "u.item")
&gt;&gt;&gt; splitMovieLines = movieLines.map(lambda l: l.split("|"))
&gt;&gt;&gt; moviesRDD = splitMovieLines.map(lambda p: Row(id=p[0], title=p[1], releaseDate=p[2], videoReleaseDate=p[3], url=p[4], unknown=int(p[5]),action=int(p[6]),adventure=int(p[7]),animation=int(p[8]),childrens=int(p[9]),comedy=int(p[10]),crime=int(p[11]),documentary=int(p[12]),drama=int(p[13]),fantasy=int(p[14]),filmNoir=int(p[15]),horror=int(p[16]),musical=int(p[17]),mystery=int(p[18]),romance=int(p[19]),sciFi=int(p[20]),thriller=int(p[21]),war=int(p[22]),western=int(p[23])))
&gt;&gt;&gt; moviesDF = spark.createDataFrame(moviesRDD)
&gt;&gt;&gt; moviesDF.createOrReplaceTempView("movies")
&gt;&gt;&gt; genreDF = spark.sql("SELECT sum(unknown) as unknown, sum(action) as action,sum(adventure) as adventure,sum(animation) as animation, sum(childrens) as childrens,sum(comedy) as comedy,sum(crime) as crime,sum(documentary) as documentary,sum(drama) as drama,sum(fantasy) as fantasy,sum(filmNoir) as filmNoir,sum(horror) as horror,sum(musical) as musical,sum(mystery) as mystery,sum(romance) as romance,sum(sciFi) as sciFi,sum(thriller) as thriller,sum(war) as war,sum(western) as western FROM movies")
&gt;&gt;&gt; genreList = genreDF.collect()
&gt;&gt;&gt; genreDict = genreList[0].asDict()
&gt;&gt;&gt; labelValues = list(genreDict.keys())
&gt;&gt;&gt; countList = list(genreDict.values())
&gt;&gt;&gt; genreDict
      {'animation': 42, 'adventure': 135, 'romance': 247, 'unknown': 2, 'musical': 56, 'western': 27, 'comedy': 505, 'drama': 725, 'war': 71, 'horror': 92, 'mystery': 61, 'fantasy': 22, 'childrens': 122, 'sciFi': 101, 'filmNoir': 24, 'action': 251, 'documentary': 50, 'crime': 109, 'thriller': 251}
    &gt;&gt;&gt; # Movie types and the counts
	&gt;&gt;&gt; x = np.arange(len(labelValues))
	&gt;&gt;&gt; plt.title('Movie types\n')
	&gt;&gt;&gt; plt.ylabel("Count")
	&gt;&gt;&gt; plt.bar(x, countList)
	&gt;&gt;&gt; plt.xticks(x + 0.5, labelValues, rotation=90)
	&gt;&gt;&gt; plt.gcf().subplots_adjust(bottom=0.20)
	&gt;&gt;&gt; plt.show(block=False)</strong></span>
</pre><p>
</p><div class="mediaobject"><img alt="Vertical bar chart" src="graphics/image_05_011.jpg"/><div class="caption"><p>Figure 10</p></div></div><p>
</p><p>In the preceding section, a <code class="literal">SparkDataFrame</code> was created with the movie dataset. The genre of the movie was captured in separate columns. Across the dataset, an aggregation was done using Spark SQL, a new <code class="literal">SparkDataFrame</code> summary was created, and the data values were collected into a Python collection object. Since there are too many columns in the dataset, a Python function was used to convert that kind of data structure into a dictionary object containing the column name as the key and the selected single row value is the value of the key. From that dictionary, two datasets were created and a bar chart is drawn. </p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip39"/>Tip</h3><p>When working with Spark, Python is used to develop data analysis applications, and it is almost certain that there are going to be many charts and plots. Instead of trying out all the code samples given in this chapter on the Python REPL for Spark, it is better to use IPython notebook as the IDE so that the code and the results can be seen together. The download section of this book contains the IPython notebook that contains all this code and the results. Readers can directly start using this.</p></div></div></div><div class="section" title="Scatter plot"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec27"/>Scatter plot</h2></div></div></div><p>Scatter plots are very commonly used for plotting values that have two variables, such as a point in Cartesian space having an <code class="literal">X</code> value and <code class="literal">Y</code> value. In this movie dataset, the number of movies released in a given year shows this kind of behavior. In the scatter plots, typically, the values represented at the intersection points of the <code class="literal">X</code> coordinate and <code class="literal">Y</code> coordinate are points. Because of recent technology developments and the availability of sophisticated graphics packages, many use different shapes and colors to represent the points. In the following scatter plot, shown in <span class="emphasis"><em>Figure 11</em></span>, small circles having a uniform area with random colors have been used to represent the values. When employing such intuitive and clever techniques to represent the points in scatter plots, care must be taken to make sure that it does not defeat the purpose and loses the simplicity that scatter plots offer to convey the behavior of the data. Simple and elegant shapes that do not clutter the Cartesian space are ideal for such non-point representations of the values. </p><p>As a continuation of the same Python REPL of Spark, run the following commands:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; yearDF = spark.sql("SELECT substring(releaseDate,8,4) as releaseYear, count(*) as movieCount FROM movies GROUP BY substring(releaseDate,8,4) ORDER BY movieCount DESC LIMIT 10")
&gt;&gt;&gt; yearDF.show()
      +-----------+----------+
    
      |releaseYear|movieCount|
    
      +-----------+----------+
    
      |       1996|       355|
    
      |       1997|       286|
    
      |       1995|       219|
    
      |       1994|       214|
    
      |       1993|       126|
    
      |       1998|        65|
    
      |       1992|        37|
    
      |       1990|        24|
    
      |       1991|        22|
    
      |       1986|        15|
    
      +-----------+----------+
    &gt;&gt;&gt; yearMovieCountTuple = yearDF.rdd.map(lambda p: (int(p.releaseYear),p.movieCount)).collect()
	&gt;&gt;&gt; yearList,movieCountList = zip(*yearMovieCountTuple)
	&gt;&gt;&gt; countArea = yearDF.rdd.map(lambda p: np.pi * (p.movieCount/15)**2).collect()
	&gt;&gt;&gt; plt.title('Top 10 movie release by year\n')
	&gt;&gt;&gt; plt.xlabel("Year")
	&gt;&gt;&gt; plt.ylabel("Number of movies released")
	&gt;&gt;&gt; plt.ylim([0,max(movieCountList) + 20])
	&gt;&gt;&gt; colors = np.random.rand(10)
	&gt;&gt;&gt; plt.scatter(yearList, movieCountList,c=colors)
	&gt;&gt;&gt; plt.show(block=False)</strong></span>
</pre><p>
</p><div class="mediaobject"><img alt="Scatter plot" src="graphics/image_05_012.jpg"/><div class="caption"><p>Figure 11</p></div></div><p>
</p><p>In the preceding section, a <code class="literal">SparkDataFrame</code> was used to collect the top 10 years in terms of the number of movies released in that year and the values were collected into a Python collection object and a scatter plot was drawn. </p><div class="section" title="Enhanced scatter plot"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec5"/>Enhanced scatter plot</h3></div></div></div><p>
<span class="emphasis"><em>Figure 11</em></span> is a very simple and elegant scatter plot, but it does not really convey the comparative behavior of a given plotted value as compared to the other values in the same space. For that, instead of representing the points as fixed-radius circles, if the points are drawn as circles with the area proportional to the value, that will give a different perspective. Figure 12 is going to show the scatter plot with the same data, but with circles that have a proportional area to represent the points.</p><p>As a continuation in the same Python REPL of Spark, run the following commands:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; # Top 10 years where the most number of movies have been released
&gt;&gt;&gt; plt.title('Top 10 movie release by year\n')
&gt;&gt;&gt; plt.xlabel("Year")
&gt;&gt;&gt; plt.ylabel("Number of movies released")
&gt;&gt;&gt; plt.ylim([0,max(movieCountList) + 100])
&gt;&gt;&gt; colors = np.random.rand(10)
&gt;&gt;&gt; plt.scatter(yearList, movieCountList,c=colors, s=countArea)
&gt;&gt;&gt; plt.show(block=False)</strong></span>
</pre><p> </p><div class="mediaobject"><img alt="Enhanced scatter plot" src="graphics/image_05_013.jpg"/><div class="caption"><p>Figure 12</p></div></div><p>
</p><p>In the preceding section, the same dataset was used for <span class="emphasis"><em>Figure 11</em></span> to draw the same scatter plot. Instead of plotting the points with uniform area circles, the points were drawn with proportionate area circles.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip40"/>Tip</h3><p>In all these code samples, the charts and plots are displayed using the show method. There are methods in matplotlib to save the generated charts and plots to disk, and they can be used for e-mailing, publishing to dashboards, and more.</p></div></div></div></div><div class="section" title="Line graph"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec28"/>Line graph</h2></div></div></div><p>There are similarities between a scatter plot and a line graph. A scatter plot is ideal for representing individual data points, but taking all the points together gives a trend. A line graph also represents individual data points, but the points are connected. This is ideal for seeing the transition from one point to another point. In one figure, multiple line graphs can be drawn, enabling the comparison of two datasets. The preceding use case used a scatter plot to represent the number of movies released over a few years. Those numbers are just discrete data points plotted on one figure. If the need is to see a trend of how movie releases are changing over the years, a line graph is ideal. Similarly, if there is a need to compare movie releases over the years for two different genres, then one line can be used for each genre and both can be plotted on a single line graph. <span class="emphasis"><em>Figure 13</em></span> is a line graph with multiple datasets. </p><p>As a continuation of the same Python REPL of Spark, run the following commands:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; yearActionDF = spark.sql("SELECT substring(releaseDate,8,4) as actionReleaseYear, count(*) as actionMovieCount FROM movies WHERE action = 1 GROUP BY substring(releaseDate,8,4) ORDER BY actionReleaseYear DESC LIMIT 10")
&gt;&gt;&gt; yearActionDF.show()
      +-----------------+----------------+
    
      |actionReleaseYear|actionMovieCount|
    
      +-----------------+----------------+
    
      |             1998|              12|
    
      |             1997|              46|
    
      |             1996|              44|
    
      |             1995|              40|
    
      |             1994|              30|
    
      |             1993|              20|
    
      |             1992|               8|
    
      |             1991|               2|
    
      |             1990|               7|
    
      |             1989|               6|
    
      +-----------------+----------------+
    &gt;&gt;&gt; yearActionDF.createOrReplaceTempView("action")
	&gt;&gt;&gt; yearDramaDF = spark.sql("SELECT substring(releaseDate,8,4) as dramaReleaseYear, count(*) as dramaMovieCount FROM movies WHERE drama = 1 GROUP BY substring(releaseDate,8,4) ORDER BY dramaReleaseYear DESC LIMIT 10")
	&gt;&gt;&gt; yearDramaDF.show()
      +----------------+---------------+
    
      |dramaReleaseYear|dramaMovieCount|
    
      +----------------+---------------+
    
      |            1998|             33|
    
      |            1997|            113|
    
      |            1996|            170|
    
      |            1995|             89|
    
      |            1994|             97|
    
      |            1993|             64|
    
      |            1992|             14|
    
      |            1991|             11|
    
      |            1990|             12|
    
      |            1989|              8|
    
      +----------------+---------------+
    &gt;&gt;&gt; yearDramaDF.createOrReplaceTempView("drama")
	&gt;&gt;&gt; yearCombinedDF = spark.sql("SELECT a.actionReleaseYear as releaseYear, a.actionMovieCount, d.dramaMovieCount FROM action a, drama d WHERE a.actionReleaseYear = d.dramaReleaseYear ORDER BY a.actionReleaseYear DESC LIMIT 10")
	&gt;&gt;&gt; yearCombinedDF.show()
      +-----------+----------------+---------------+
    
      |releaseYear|actionMovieCount|dramaMovieCount|
    
      +-----------+----------------+---------------+
    
      |       1998|              12|             33|
    
      |       1997|              46|            113|
    
      |       1996|              44|            170|
    
      |       1995|              40|             89|
    
      |       1994|              30|             97|
    
      |       1993|              20|             64|
    
      |       1992|               8|             14|
    
      |       1991|               2|             11|
    
      |       1990|               7|             12|
    
      |       1989|               6|              8|
    
      +-----------+----------------+---------------+
   &gt;&gt;&gt; yearMovieCountTuple = yearCombinedDF.rdd.map(lambda p: (p.releaseYear,p.actionMovieCount, p.dramaMovieCount)).collect()
   &gt;&gt;&gt; yearList,actionMovieCountList,dramaMovieCountList = zip(*yearMovieCountTuple)
   &gt;&gt;&gt; plt.title("Movie release by year\n")
   &gt;&gt;&gt; plt.xlabel("Year")
   &gt;&gt;&gt; plt.ylabel("Movie count")
   &gt;&gt;&gt; line_action, = plt.plot(yearList, actionMovieCountList)
   &gt;&gt;&gt; line_drama, = plt.plot(yearList, dramaMovieCountList)
   &gt;&gt;&gt; plt.legend([line_action, line_drama], ['Action Movies', 'Drama Movies'],loc='upper left')
   &gt;&gt;&gt; plt.gca().get_xaxis().get_major_formatter().set_useOffset(False)
   &gt;&gt;&gt; plt.show(block=False)</strong></span>
</pre><p>
</p><div class="mediaobject"><img alt="Line graph" src="graphics/image_05_014.jpg"/><div class="caption"><p>Figure 13</p></div></div><p>
</p><p>In the preceding section, Spark DataFrames were created to get the datasets for the number of action movies and drama movies released over the period of the last 10 years. The data was collected into Python collection objects and line graphs were drawn in the same figure. </p><p>Python, in conjunction with the matplotlib library, is very rich in terms of methods to produce publication-quality charts and plots. Spark can be used as the workhorse for processing the data coming from heterogeneous sources of data, and the results can also be saved to a wide variety of data formats.</p><p>Those who are exposed to the Python data analysis library <span class="strong"><strong>pandas</strong></span> will find it easy to understand the material covered in this chapter because Spark DataFrames designed from the ground up by taking inspiration from the R DataFrame as well as <span class="strong"><strong>pandas</strong></span>. </p><p>This chapter has covered only a few sample charts and plots that can be created using the <span class="strong"><strong>matplotlib</strong></span> library. The main idea of this chapter was to help the reader understand the capability of using this library in conjunction with Spark, where Spark is doing the data processing, and <span class="strong"><strong>matplotlib</strong></span> is doing the charting and plotting.</p><p>The data file used in this chapter is read from a local filesystem. Instead of this, it can be read from HDFS or any other Spark-supported data source.</p><p>When using Spark as the primary framework for data processing, the most important point to keep in mind is that any possible data processing is to be done by Spark, mainly because Spark can do data processing in the best way. Only the processed data is to be returned to the Spark driver program for doing the charting and plotting. </p></div></div>
<div class="section" title="References"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec44"/>References</h1></div></div></div><p>For more information please refer to following links:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://www.numpy.org/">http://www.numpy.org/</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://www.scipy.org/">http://www.scipy.org/</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://matplotlib.org/">http://matplotlib.org/</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://movielens.org/">https://movielens.org/</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://grouplens.org/datasets/movielens/">http://grouplens.org/datasets/movielens/</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://pandas.pydata.org/">http://pandas.pydata.org/</a></li></ul></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec45"/>Summary</h1></div></div></div><p>Processed data is used for data analysis. Data analysis requires a deep understanding of the processed data. Charts and plots enhance the understanding of the characteristics of the underlying data. In essence, for a data analysis application, data processing, charting, and plotting are essential. This chapter has covered the usage of Spark with Python, in conjunction with Python charting and plotting libraries, for developing data analysis applications.</p><p>In most organizations, business requirements are driving the need to build data processing applications involving the real-time ingestion of data, in various shapes and forms, with tremendous velocity. This mandates the need to process a stream of incoming data to the organizational data sink. The next chapter is going to discuss Spark Streaming, which is a library that works on top of Spark and enables the processing of various types of data streams.</p></div></body></html>