<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-78"><a id="_idTextAnchor078" class="calibre6 pcalibre1 pcalibre"/>5</h1>
<h1 id="_idParaDest-79" class="calibre5"><a id="_idTextAnchor079" class="calibre6 pcalibre1 pcalibre"/>Developing Data Science Projects with Snowpark</h1>
<p class="calibre3">The emergence of cloud technologies has ushered in a new era of possibilities. With the advent of Data Cloud, a robust platform that unifies data storage, processing, and analysis, data scientists have many opportunities to explore, analyze, and extract meaningful insights from vast datasets. In this intricate digital realm, the role of Snowpark, a cutting-edge data processing framework, becomes paramount. This chapter serves as an illuminating guide, delving deep into developing data science projects with Snowpark, unraveling its intricacies, and harnessing its potential to the fullest extent.</p>
<p class="calibre3">In this chapter, we’re going to cover the following main topics:</p>
<ul class="calibre15">
<li class="calibre14">Data science in Data Cloud</li>
<li class="calibre14">Exploring and preparing data</li>
<li class="calibre14">Training <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models in Snowpark</li>
</ul>
<h1 id="_idParaDest-80" class="calibre5"><a id="_idTextAnchor080" class="calibre6 pcalibre1 pcalibre"/>Technical requirements</h1>
<p class="calibre3">For this chapter, you’ll require an active Snowflake account and Python installed with Anaconda configured locally. You can sign up for a Snowflake Trial account at <a href="https://signup.snowflake.com/" class="calibre6 pcalibre1 pcalibre">https://signup.snowflake.com/</a>.</p>
<p class="calibre3">To configure Anaconda, follow <a href="https://conda.io/projects/conda/en/latest/user-guide/getting-started.html" class="calibre6 pcalibre1 pcalibre">https://conda.io/projects/conda/en/latest/user-guide/getting-started.html</a>. </p>
<p class="calibre3">In addition, to install and set up Python for VS Code, follow <a href="https://code.visualstudio.com/docs/datascience/jupyter-notebooks" class="calibre6 pcalibre1 pcalibre">https://code.visualstudio.com/docs/python/python-tutorial</a>. </p>
<p class="calibre3">To learn how to operate Jupyter Notebook in VS Code, go to <a href="https://code.visualstudio.com/docs/datascience/jupyter-notebooks" class="calibre6 pcalibre1 pcalibre">https://code.visualstudio.com/docs/datascience/jupyter-notebooks</a>.</p>
<p class="calibre3">The supporting materials for this chapter are available in this book’s GitHub repository at <a href="https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark" class="calibre6 pcalibre1 pcalibre">https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark</a>.</p>
<h1 id="_idParaDest-81" class="calibre5"><a id="_idTextAnchor081" class="calibre6 pcalibre1 pcalibre"/>Data science in Data Cloud</h1>
<p class="calibre3">Data science <a id="_idIndexMarker283" class="calibre6 pcalibre1 pcalibre"/>transcends traditional boundaries in the Data Cloud ecosystem, offering a dynamic environment where data scientists can harness the power of distributed computing and advanced analytics. With the ability to seamlessly integrate various data sources, including structured and unstructured data, Data Cloud provides a data exploration and experimentation environment. We will start this section with a brief data science and ML refresher.</p>
<h2 id="_idParaDest-82" class="calibre7"><a id="_idTextAnchor082" class="calibre6 pcalibre1 pcalibre"/>Data science and ML concepts</h2>
<p class="calibre3">Data science <a id="_idIndexMarker284" class="calibre6 pcalibre1 pcalibre"/>and ML have surged to the forefront of technological and business innovation, becoming integral components of decision-making, strategic planning, and product development across virtually all industries. The journey to their current popularity and influence is a testament to several factors, including advancements in technology, the explosion of data, and the increasing computational power available. This section will briefly discuss data science and ML concepts.</p>
<h3 class="calibre9">Data science</h3>
<p class="calibre3">Data science is a multidisciplinary field that relies on various software tools, algorithms, and ML principles to extract valuable insights from extensive datasets. Data scientists are pivotal in collecting, transforming, and converting data into predictive and prescriptive insights. By employing sophisticated techniques, data science uncovers hidden patterns and meaningful correlations within data, enabling businesses to act on informed decisions based on empirical evidence.</p>
<h3 class="calibre9">Artificial intelligence</h3>
<p class="calibre3"><strong class="bold">Artificial intelligence</strong> (<strong class="bold">AI</strong>) encompasses <a id="_idIndexMarker285" class="calibre6 pcalibre1 pcalibre"/>the science and engineering of creating intelligent machines and brilliant computer programs capable of autonomously processing information and generating outcomes. AI systems are designed to solve intricate problems using logic and reasoning, similar to human cognitive processes. These systems operate autonomously, aiming to emulate human-like intelligence in decision-making and problem-solving tasks.</p>
<h3 class="calibre9">ML</h3>
<p class="calibre3">ML, a subset of AI, involves<a id="_idIndexMarker286" class="calibre6 pcalibre1 pcalibre"/> specialized algorithms integrated into the data science workflow. These algorithms are meticulously crafted software programs that are designed to detect patterns, identify correlations, and pinpoint anomalies within data. ML algorithms excel at predicting outcomes based on existing data and continue to learn and improve their accuracy as they encounter new data and situations. Unlike humans, ML algorithms can process thousands of attributes and features, enabling the discovery of unique combinations and correlations in vast datasets. This capability makes ML indispensable for extracting valuable insights and predictions from extensive data collections.</p>
<p class="calibre3">Now that we have the terminologies straightened out, we will discuss how the Data Cloud paradigm has helped the growth of data science and ML for organizations.</p>
<h2 id="_idParaDest-83" class="calibre7"><a id="_idTextAnchor083" class="calibre6 pcalibre1 pcalibre"/>The Data Cloud paradigm</h2>
<p class="calibre3">Data science <a id="_idIndexMarker287" class="calibre6 pcalibre1 pcalibre"/>in the cloud represents a paradigm shift in how data-driven insights are derived and applied. In this innovative approach, data science processes, tools, and techniques are seamlessly integrated into the cloud, allowing organizations to leverage the power of scalable infrastructure and advanced analytics.</p>
<p class="calibre3">At the heart of this paradigm lies Data Cloud, a dynamic ecosystem that transcends traditional data storage and processing constraints. The Data Cloud paradigm represents a seismic shift from conventional data silos, offering a unified platform where structured and unstructured data coalesce seamlessly. Through distributed computing, parallel processing, and robust data management, Data Cloud sets the stage for a data science revolution. The capabilities and tools that empower data scientists are seamlessly integrated and are designed to handle diverse data types and analytical workloads within Data Cloud. As such, Data Cloud offers various advantages for running data science and ML workloads.</p>
<h3 class="calibre9">Advantages of Data Cloud for data science</h3>
<p class="calibre3">One of the<a id="_idIndexMarker288" class="calibre6 pcalibre1 pcalibre"/> key advantages of Snowflake’s Data Cloud is the ability to store and process vast amounts of data without the constraints of hardware limitations. It offers a scalable solution to handling vast volumes of data, enabling data scientists to work with extensive datasets without having to worry about computing or storage constraints. The cloud-based interface provides a collaborative and flexible environment for data scientists and analysts and comes with built-in collaboration features, version control, and support for popular data science libraries and frameworks through Snowpark.</p>
<p class="calibre3">Furthermore, Data Cloud offers a diverse ecosystem of services and resources tailored for data science tasks through managed services that simplify these processes, from data ingestion and preparation to ML model training and deployment. For instance, data pipelines can be automated using serverless computing, and ML models can be trained on powerful GPU instances, leading to faster experimentation and iteration. Data security and compliance are paramount in data science, especially when dealing with sensitive information, and Data Cloud provides different security measures, including encryption, access control, and row-level policies, ensuring that data scientists can work with sensitive data in a secure and compliant manner, adhering to industry regulations and organizational policies. The Snowpark framework is at the center of Snowflake’s Data Cloud to support these capabilities. The following section will discuss why Snowpark is used for data science and ML.</p>
<h2 id="_idParaDest-84" class="calibre7"><a id="_idTextAnchor084" class="calibre6 pcalibre1 pcalibre"/>Why Snowpark for data science and ML?</h2>
<p class="calibre3">Snowpark <a id="_idIndexMarker289" class="calibre6 pcalibre1 pcalibre"/>offers unparalleled integration capabilities for data engineers, enabling seamless interaction with data stored in large volumes and diverse formats. Its versatile API facilitates effortless data exploration, transformation, and manipulation, laying a robust foundation for data science models and ML development and empowering data scientists to harness the full potential of their analytical workflows. Data science teams can now focus on their core tasks without the hassle of infrastructure or environment maintenance.</p>
<p class="calibre3">Snowpark excels in scalability and performance, which is crucial for enterprise data science and ML workloads; leveraging Snowflake’s distributed architecture to handle massive datasets and complex computations with remarkable efficiency and the ability to parallelize processing tasks and distribute workloads across multiple nodes ensures lightning-fast execution, even when dealing with petabytes of data. These features, combined with Snowflake’s automatic optimization features, allow data scientists to focus on their analyses without being burdened by infrastructure limitations.</p>
<p class="calibre3">Snowpark <a id="_idIndexMarker290" class="calibre6 pcalibre1 pcalibre"/>offers a rich array of advanced analytics capabilities that are indispensable for data science and ML tasks. From statistical analysis to predictive modeling, geospatial analytics, or even data mining, it provides a comprehensive toolkit for data scientists to explore complex patterns and extract valuable insights. Its support for ML libraries and algorithms further amplifies its utility, enabling the development of sophisticated models for classification, regression, and clustering. With the rich features and functionalities mentioned previously, Snowpark provides many benefits for data science and ML workloads. In the next section, we will explore the world of the Snowpark ML library and its different functionalities.</p>
<h2 id="_idParaDest-85" class="calibre7"><a id="_idTextAnchor085" class="calibre6 pcalibre1 pcalibre"/>Introduction to Snowpark ML</h2>
<p class="calibre3">Snowpark <a id="_idIndexMarker291" class="calibre6 pcalibre1 pcalibre"/>constitutes a compendium of libraries and runtimes within Snowflake, facilitating the secure deployment and processing of non-SQL code by encompassing languages such as Python with the code execution that occurs server-side within the Snowflake infrastructure, all while leveraging a virtual warehouse. The newest addition to the Snowpark libraries is Snowpark ML. Snowpark ML represents a groundbreaking fusion of Snowflake’s powerful data processing capabilities and the transformative potential of ML. As the frontier of data science expands, Snowpark ML emerges as a cutting-edge framework that’s designed to empower data professionals to harness the full potential of their data within Snowflake’s cloud-based environment.</p>
<p class="calibre3">At its core, Snowpark ML is engineered to facilitate seamless integration between Snowflake’s data processing capabilities and advanced ML techniques. With Snowpark ML, data scientists, analysts, and engineers can leverage familiar programming languages and libraries to develop sophisticated ML models directly within Snowflake. This integration eliminates the barriers between data storage, processing, and modeling, streamlining the end-to-end data science workflow. Snowpark ML catalyzes innovation, enabling data professionals to efficiently explore, transform, and model data. By bridging the gap between data processing and ML, Snowpark ML empowers organizations to make data-driven decisions, uncover valuable insights, and drive business growth in the digital age. The following figure shows the Snowpark ML framework:</p>
<div><div><img alt="Figure 5.1 – Snowpark ML" src="img/B19923_05_1.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Snowpark ML</p>
<p class="calibre3">The <a id="_idIndexMarker292" class="calibre6 pcalibre1 pcalibre"/>preceding architecture consists of various components that work cohesively together. We will look at each of these components in more detail in the next section.</p>
<h3 class="calibre9">Snowpark ML API</h3>
<p class="calibre3">Similar to <a id="_idIndexMarker293" class="calibre6 pcalibre1 pcalibre"/>Snowpark DataFrame, which helps operate with the data, Snowpark ML provides APIs as a Python library called <code>snowflake-ml</code> to support every stage of the ML development and deployment process, allowing support for pre-processing data and training, managing, and deploying ML models all within Snowflake:</p>
<div><div><img alt="Figure 5.2 – Snowpark ML API" src="img/B19923_05_2.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Snowpark ML API</p>
<p class="calibre3">The Snowpark ML API consists of Snowpark ML modeling for developing and training the models and<a id="_idIndexMarker294" class="calibre6 pcalibre1 pcalibre"/> Snowpark ML Ops for monitoring and operating the model. The <code>snowflake.ml.modeling</code> module provides APIs for pre-processing, feature engineering, and model training based on familiar libraries, such as scikit-learn and XGBoost. The complete end-to-end ML experience can be done using Snowpark. We’ll cover this in the next section.</p>
<h2 id="_idParaDest-86" class="calibre7"><a id="_idTextAnchor086" class="calibre6 pcalibre1 pcalibre"/>End-to-end ML with Snowpark</h2>
<p class="calibre3">The<a id="_idIndexMarker295" class="calibre6 pcalibre1 pcalibre"/> quest for seamless, end-to-end ML solutions has become paramount, and Snowpark offers a comprehensive ecosystem for end-to-end ML. This <a id="_idIndexMarker296" class="calibre6 pcalibre1 pcalibre"/>section delves into the intricate world of leveraging Snowpark to craft end-to-end ML pipelines, from data ingestion and preprocessing to model development, training, and deployment, unveiling the seamless process of developing ML within Snowflake’s robust framework.</p>
<p class="calibre3">ML processes involve a systematic approach to solving complex problems through data processing. This typically<a id="_idIndexMarker297" class="calibre6 pcalibre1 pcalibre"/> includes stages such as defining the problem, collecting and preparing data, <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>), feature engineering, model selection, training, evaluation, and deployment, with each operation being crucial and iterative. It allows data scientists to refine their approaches based on insights gained along the way. The process is often cyclical, with continuous iterations to improve models and predictions:</p>
<div><div><img alt="Figure 5.3 – End-to-end ML flow" src="img/B19923_05_3.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.3 – End-to-end ML flow</p>
<p class="calibre3">We can broadly classify the ML stages as preparing and transforming, training and building the model, and interfering with the model to obtain prediction results. We will discuss each of these steps next.</p>
<h3 class="calibre9">Preparing and transforming the data</h3>
<p class="calibre3">Raw data is<a id="_idIndexMarker298" class="calibre6 pcalibre1 pcalibre"/> often messy, containing missing values, outliers, and inconsistencies. Getting the correct data from multiple systems usually consumes most of the data scientist’s time. Snowflake solves this problem by <a id="_idIndexMarker299" class="calibre6 pcalibre1 pcalibre"/>providing a governed Data Cloud paradigm that supports all types of data and provides a unified place to instantly store and consume relevant data to unlock ML models’ power. The data preparation and transformation process involves EDA, cleaning, and processing, and ends with feature engineering. This step also consists of data engineering pipelines, which help apply data transformations to prepare the data for the next step. For data pre-processing, <code>snowflake.ml.modeling</code>, preprocessing, and Snowpark functions can be used to transform the data.</p>
<h4 class="calibre16">EDA</h4>
<p class="calibre3">EDA is a <a id="_idIndexMarker300" class="calibre6 pcalibre1 pcalibre"/>critical step that involves preliminary investigations to understand the data’s structure, patterns, and trends as it helps uncover<a id="_idIndexMarker301" class="calibre6 pcalibre1 pcalibre"/> hidden patterns and guide feature selection. Data scientists and analysts collaborate closely with business stakeholders to define the specific questions that need to be answered or the problems that need to be solved, which guides them in selecting the relevant data. Through charts, graphs, and statistical summaries, data scientists can identify patterns, trends, correlations, and outliers within the dataset, all of which provide valuable insights into the data’s distribution and help them understand the data better to build feature selection.</p>
<h4 class="calibre16">Data cleaning and preprocessing</h4>
<p class="calibre3">Data cleaning <a id="_idIndexMarker302" class="calibre6 pcalibre1 pcalibre"/>involves handling missing data, correcting errors, and ensuring consistency. The data is suitable for training the model through<a id="_idIndexMarker303" class="calibre6 pcalibre1 pcalibre"/> preprocessing techniques such as normalization, scaling, and transformations, along with various sampling techniques that are applied to evaluate a subset of the data, providing insights into its richness and variability.</p>
<h4 class="calibre16">Feature engineering</h4>
<p class="calibre3">Feature engineering<a id="_idIndexMarker304" class="calibre6 pcalibre1 pcalibre"/> involves creating new features or modifying existing ones to enhance the performance of ML models. It requires domain expertise to identify relevant features that can improve predictive accuracy. Performing feature engineering on the centralized data in Snowflake accelerates model development, reduces costs, and enables the reuse of new features. Some techniques, such as creating interaction terms and transforming variables, extract meaningful information from raw data, making it more informative for modeling.</p>
<h3 class="calibre9">Training and building the model</h3>
<p class="calibre3">Once the data<a id="_idIndexMarker305" class="calibre6 pcalibre1 pcalibre"/> is ready and the features have been built, the next step is to train and develop the model. In this stage, the data scientist trains various models, such as regression, classification, clustering, or deep learning, depending on the nature of the problem, by passing a subset of the data, or training set, through the modeling function to derive a predictive function. The model is developed using statistical methods for hypothesis testing and inferential statistics. Advanced techniques, such as ensemble methods, neural networks, and natural language processing, are also used, depending on the data.</p>
<p class="calibre3">Once the model <a id="_idIndexMarker306" class="calibre6 pcalibre1 pcalibre"/>has been developed, it’s tested on data that wasn’t part of the training set to determine its effectiveness, which is usually measured in terms of its predictive strength and robustness, and the model is optimized with hyperparameter tuning. Cross-validation techniques optimize the model’s performance, ensuring accurate predictions and valuable insights.</p>
<p class="calibre3">This combination of steps enables data scientists to conduct in-depth feature engineering, tune hyperparameters, and iteratively create and assess ML models. Intuitions become accurate predictions as data scientists experiment with various algorithms, evaluating the performance of each model and adjusting parameters on their chosen model to optimize the code for their specific datasets. <code>snowflake.ml.modeling</code> can be used for training by utilizing the <code>fit()</code> method for an algorithm such as XGBoost.</p>
<h3 class="calibre9">Inference</h3>
<p class="calibre3">Once the<a id="_idIndexMarker307" class="calibre6 pcalibre1 pcalibre"/> models have been trained, Snowpark ML supports their seamless deployment and inference. Models can be deployed for inference, enabling organizations to make data-driven decisions based on predictive insights. Snowpark ML has a model registry to manage and organize Snowpark models throughout their life cycle. The model registry supports versioning of the models and stores metadata information about the models, hyperparameters, and evaluation metrics, facilitating experimentation and model comparison. It also supports model monitoring and auditing and aids in collaboration between data scientists working on the model. The model registry is part of Snowpark MLOps and can be accessed through <code>snowpark.ml.registry</code>. The pipelines can be orchestrated using Snowflake Tasks.</p>
<p class="calibre3">Now that we have established the foundations of Snowpark ML, its place in ML, and how Snowpark supports data science workloads, we will dive deep into the complete data science scenario with Snowpark. The following section will focus on exploring and preparing the data.</p>
<p class="callout-heading">A note on data engineering</p>
<p class="callout">In the next section, we’ll conduct exploration, transformation, and feature engineering using Snowpark Python and pandas. As we proceed to build models with SnowparkML, we will incorporate some of the steps discussed earlier in this section.</p>
<h1 id="_idParaDest-87" class="calibre5"><a id="_idTextAnchor087" class="calibre6 pcalibre1 pcalibre"/>Exploring and preparing data</h1>
<p class="calibre3">In the first step of<a id="_idIndexMarker308" class="calibre6 pcalibre1 pcalibre"/> the ML process, we must explore and prepare the data in Snowflake using Snowpark to make it available for training the ML models. We will work with the Bike Sharing dataset from Kaggle, which offers an hourly record of rental data for 2 years. The primary objective is to forecast the number of bikes rented each hour for a specific timeframe based solely on the information available before the rental period. In essence, the model will harness the power of historical data to predict future bike rental patterns using Snowpark. More information about the particular dataset has been provided in the respective GitHub repository (<a href="https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark" class="calibre6 pcalibre1 pcalibre">https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark</a>).</p>
<p class="calibre3">Data exploration allows us to dissect the data to uncover intricate details that might otherwise stay hidden, acting as the foundation for our entire analysis. We will start the process by loading the dataset into a Snowpark DataFrame:</p>
<pre class="source-code">
df_table=session.table("BSD_TRAINING")</pre> <p class="calibre3">Once the data has been successfully loaded, the subsequent imperative is to gain a comprehensive understanding of the dataset’s scale:</p>
<pre class="source-code">
number_of_rows = df_table.count()
number_of_columns = len(df_table.columns)</pre> <p class="calibre3">Fortunately, Snowpark provides functions specifically designed to facilitate this critical task:</p>
<div><div><img alt="Figure 5.4 – Total number of columns" src="img/B19923_05_4.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Total number of columns</p>
<p class="calibre3">Now that we<a id="_idIndexMarker309" class="calibre6 pcalibre1 pcalibre"/> know the scale of the data, let’s get a sense of it by looking at a few rows of the dataset:</p>
<pre class="source-code">
df_table.sample(n=2).show()</pre> <p class="calibre3">This returns the two rows from the data for analysis:</p>
<div><div><img alt="Figure 5.5 – Two rows of data" src="img/B19923_05_5.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Two rows of data</p>
<p class="calibre3">As depicted in the preceding figure, the <code>COUNT</code> column is a straightforward aggregation of <code>CASUAL</code> and <code>REGISTERED</code>. In data science, these types of variables are commonly referred to as “leakage variables.” When we construct our models, we’ll delve deeper into strategies for managing these variables. Date columns consistently present an intriguing and complex category to grapple with. Within this dataset, there is potential to create valuable new features derived from the <code>DATETIME</code> column, which could significantly influence our response variables. Before we start with data cleansing and the feature engineering process, let’s see the column type to understand and make more informed decisions:</p>
<pre class="source-code">
import pprint
data_types = df_table.schema
data_types = df_table.schema.fields
pprint.pprint(data_types)</pre> <p class="calibre3">This will give <a id="_idIndexMarker310" class="calibre6 pcalibre1 pcalibre"/>us the schema and the data types for each field so that we can understand the data better:</p>
<div><div><img alt="Figure 5.6 – Schema information" src="img/B19923_05_6.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.6 – Schema information</p>
<p class="calibre3">Now that we are equipped with basic information about the data, let’s start finding the missing values in the data.</p>
<h2 id="_idParaDest-88" class="calibre7"><a id="_idTextAnchor088" class="calibre6 pcalibre1 pcalibre"/>Missing value analysis</h2>
<p class="calibre3">Addressing <a id="_idIndexMarker311" class="calibre6 pcalibre1 pcalibre"/>missing values is a fundamental preprocessing step in ML. Incomplete data can disrupt model training and hinder predictive accuracy, potentially leading to erroneous conclusions or suboptimal performance. By systematically imputing or filling these gaps, we can bolster the integrity of our dataset, providing ML algorithms with a more comprehensive and coherent dataset for more robust and reliable analyses and predictions. This practice is akin to affording our models the necessary information to make sound, data-driven decisions. Let’s check for any missing values in our dataset:</p>
<pre class="source-code">
from snowflake.snowpark.functions import count, col
data_types = df_table.schema
print(data_types)
for column in df_table.columns:
    print(f"Null values in {column} is {number_of_rows - df_table.agg(count(col(column))).collect()[0][0]}")</pre> <p class="calibre3">The preceding code helps us find out whether any values are empty or missing:</p>
<div><div><img alt="Figure 5.7  – Missing value analysis" src="img/B19923_05_7.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.7  – Missing value analysis</p>
<p class="calibre3">Our<a id="_idIndexMarker312" class="calibre6 pcalibre1 pcalibre"/> initial examination of missing values in the column shows no missing values in our dataset. However, a closer examination reveals the presence of numerous 0s within the <code>WINDSPEED</code> column, which is indicative of potentially missing values. Logically, windspeed cannot equate to zero, implying that each <code>0</code> within the column signifies a missing value:</p>
<pre class="source-code">
print(f"Zero Values in windspeed column is {df_table.filter(df_table['WINDSPEED']==0).count()}")</pre> <p class="calibre3">This will print out the following output:</p>
<div><div><img alt="Figure 5.8 – Output value" src="img/B19923_05_8.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.8 – Output value</p>
<p class="calibre3">We can see that there are <code>1313</code> values in the <code>WINDSPEED</code> column. With this column harboring missing data, the subsequent challenge is determining an effective strategy for imputing these missing values. As is widely acknowledged, various methods exist for addressing missing data within a column. In our case, we’ll employ a straightforward imputation, substituting the 0s with the mean value of the column:</p>
<pre class="source-code">
from snowflake.snowpark.functions import iff, avg
wind_speed_mean = df_train.select(mean("windspeed")).collect()[0][0]
df_train = df_train.replace({0:wind_speed_mean}, subset=["windspeed"])
df_train.show()
df_train.write.mode("overwrite").save_as_table("model_data")</pre> <p class="calibre3">The preceding code replaces the 0s with the mean value of the column:</p>
<div><div><img alt="Figure 5.9 – Pre-processed data" src="img/B19923_05_9.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.9 – Pre-processed data</p>
<p class="calibre3">This <a id="_idIndexMarker313" class="calibre6 pcalibre1 pcalibre"/>concludes our preprocessing journey. Next, we’ll perform outlier analysis.</p>
<h2 id="_idParaDest-89" class="calibre7"><a id="_idTextAnchor089" class="calibre6 pcalibre1 pcalibre"/>Outlier analysis</h2>
<p class="calibre3">The<a id="_idIndexMarker314" class="calibre6 pcalibre1 pcalibre"/> process of detecting and removing outliers is pivotal in enhancing model accuracy and robustness. Outliers are data points that significantly deviate from most datasets, often stemming from errors, anomalies, or rare events. These aberrations can unduly influence model training, leading to skewed predictions or reduced generalization capabilities. By identifying and eliminating outliers, we can improve the quality and reliability of our models and ensure that they are better equipped to discern meaningful patterns within the data. This practice fosters more accurate predictions and a higher level of resilience, ultimately contributing to the overall success of ML endeavors.</p>
<p class="calibre3">We will be transforming the DataFrame into a pandas DataFrame so that we can conduct insightful analyses, including constructing visualizations to extract meaningful patterns. Our initial focus is on the <code>COUNT</code> column as the response variable. Before model development, it is imperative to ascertain whether the <code>COUNT</code> column contains any outlier values:</p>
<pre class="source-code">
import seaborn as sns
import matplotlib.pyplot as plt
f, axes = plt.subplots(1, 2)
sns.boxplot(x=df_table.to_pandas()['COUNT'], ax=axes[0])
sns.boxplot(x=df_without_outlier.to_pandas()['COUNT'], ax=axes[1])
plt.show()</pre> <p class="calibre3">The<a id="_idIndexMarker315" class="calibre6 pcalibre1 pcalibre"/> preceding code generates a plot using the <code>seaborn</code> and the <code>matplotlib</code> library to help us find the outliers:</p>
<div><div><img alt="Figure 5.10 – Outlier plot" src="img/B19923_05_10.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.10 – Outlier plot</p>
<p class="calibre3">As we can see, the <code>COUNT</code> column exhibits outlier data points that can potentially negatively impact model performance if they’re not adequately addressed. Mitigating outliers is a critical preprocessing step. One widely adopted approach involves removing data points that lie beyond a predefined threshold or permissible range, as outlined here:</p>
<pre class="source-code">
from snowflake.snowpark.functions \
    import mean, stddev, abs, date_part
mean_value = df_table.select(mean("count")).collect()[0][0]
print(mean_value)
std_value = df_table.select(stddev("count")).collect()[0][0]
print(std_value)
df_without_outlier = df_table.filter(
    (abs(df_table["count"] - mean_value)) &gt;= (3 * std_value))
df_without_outlier.show()</pre> <p class="calibre3">The preceding <a id="_idIndexMarker316" class="calibre6 pcalibre1 pcalibre"/>code uses the Snowpark library to analyze a dataset stored in <code>df_table</code>. It calculates the mean (average) and standard deviation (a measure of data spread) of the <code>'count'</code> column in the dataset. Then, it identifies and removes outliers from the dataset. Outliers are data points that significantly differ from the average. In this case, it defines outliers as data points more than three times the standard deviation away from the mean. After identifying these outliers, it displays the dataset without the outlier values, using <code>df_without_outlier.show()</code> to help with further analysis:</p>
<div><div><img alt="Figure 5.11 – Outliers removed" src="img/B19923_05_11.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.11 – Outliers removed</p>
<p class="calibre3">Now that we have taken care of the outliers, we can perform correlation analysis.</p>
<h2 id="_idParaDest-90" class="calibre7"><a id="_idTextAnchor090" class="calibre6 pcalibre1 pcalibre"/>Correlation analysis</h2>
<p class="calibre3">Identifying <a id="_idIndexMarker317" class="calibre6 pcalibre1 pcalibre"/>correlations among variables is of paramount importance for several vital reasons. Correlations provide valuable insights into how different features in the dataset relate to each other. By understanding these relationships, ML models can make more informed predictions as they leverage the strength and direction of correlations to uncover patterns and dependencies. Moreover, identifying and quantifying correlations aids feature selection, where irrelevant or highly correlated features can be excluded to enhance model efficiency and interpretability. It also helps identify potential multicollinearity issues, where two or more features are highly correlated, leading to unstable model coefficients. Recognizing and harnessing correlations empowers ML models to make better predictions <a id="_idIndexMarker318" class="calibre6 pcalibre1 pcalibre"/>and yield more robust results, making it a fundamental aspect of modeling.</p>
<pre>Chapter_5.ipynb</strong>:</pre>
<pre class="source-code">
corr_matrix = df_without_outlier.to_pandas().corr()
plt.figure(figsize=(12, 6))
sns.heatmap(corr_matrix, cmap='coolwarm', annot=True)</pre> <p class="calibre3">The preceding code generates the correlation matrix as a heatmap visualization:</p>
<div><div><img alt="Figure 5.12 – Correlation matrix heatmap" src="img/B19923_05_12_V.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.12 – Correlation matrix heatmap</p>
<p class="calibre3">This heatmap visualization reveals a substantial correlation between the <code>TEMP</code> and <code>ATEMP</code> variables, signifying a condition known as multicollinearity. Multicollinearity occurs when <a id="_idIndexMarker319" class="calibre6 pcalibre1 pcalibre"/>two or more predictors in a model are highly correlated, distorting the model’s interpretability and stability. To mitigate this issue and ensure the reliability of our analysis, we have opted to retain the <code>TEMP</code> variable while removing <code>ATEMP</code> from consideration in our subsequent modeling endeavors. This strategic decision is made to maintain model robustness and effectively capture the essence of the data without the confounding effects of multicollinearity.</p>
<h2 id="_idParaDest-91" class="calibre7"><a id="_idTextAnchor091" class="calibre6 pcalibre1 pcalibre"/>Leakage variables</h2>
<p class="calibre3"><strong class="bold">Leakage variables</strong> in <a id="_idIndexMarker320" class="calibre6 pcalibre1 pcalibre"/>data science inadvertently<a id="_idIndexMarker321" class="calibre6 pcalibre1 pcalibre"/> include information that would not be available during prediction or decision-making in a real-world scenario. Eliminating them is crucial because using leakage variables can lead to overly optimistic model performance and unreliable results. It’s essential to detect and exclude these variables during data preprocessing to ensure that our models make predictions based on the same information that would be accessible. By doing so, we prevent the risk of building models that work well on historical data but fail to perform in real-world situations, which is a crucial goal in data science.</p>
<p class="calibre3">As mentioned previously, the <code>CASUAL</code>, <code>REGISTERED</code>, and <code>COUNT</code> columns exhibit high collinearity, with <code>COUNT</code> being an explicit summation of <code>CASUAL</code> and <code>REGISTERED</code>. This redundancy renders the inclusion of all three variables undesirable, resulting in a leakage variable situation. To preserve the integrity of our model-building process, we shall eliminate <code>CASUAL</code> and <code>REGISTERED</code> from our feature set, thereby mitigating any potential confounding effects and ensuring the model’s ability to make predictions based on the most relevant and non-redundant information. The next step is to perform feature engineering with the prepared data.</p>
<h2 id="_idParaDest-92" class="calibre7"><a id="_idTextAnchor092" class="calibre6 pcalibre1 pcalibre"/>Feature engineering</h2>
<p class="calibre3"><strong class="bold">Feature engineering</strong> in <a id="_idIndexMarker322" class="calibre6 pcalibre1 pcalibre"/>ML is like crafting the<a id="_idIndexMarker323" class="calibre6 pcalibre1 pcalibre"/> perfect tool for a specific job. It involves selecting, transforming, or creating new features (variables) from the available data to make it more suitable for ML algorithms. This process is crucial because it helps the models better understand the patterns and relationships in the data, leading to improved predictions and insights. By carefully engineering features, we can uncover hidden information, reduce noise, and enhance the model’s performance, making it a vital step in building effective and accurate ML systems.</p>
<p class="calibre3">Analyzing<a id="_idIndexMarker324" class="calibre6 pcalibre1 pcalibre"/> the data shows that the <code>DATETIME</code> column is a promising candidate for feature engineering within this dataset. Given the dependency of the predictive outcome on temporal factors such as the time of day and <a id="_idIndexMarker325" class="calibre6 pcalibre1 pcalibre"/>day of the week, deriving time-related features assumes paramount significance. Extracting these temporal features is pivotal as it enhances the model’s performance and elevates the overall predictive accuracy by capturing essential nuances about the dataset’s material characteristics:</p>
<pre class="source-code">
from snowflake.snowpark.functions import hour, month,to_date,dayofweek
df_table = df_table.with_column("hour", hour("DATETIME"))
df_table = df_table.with_column("month", month("DATETIME"))
df_table = df_table.with_column("date", to_date("DATETIME"))
df_table = df_table.with_column("weekday", dayofweek("DATETIME"))
df_table.show()</pre> <p class="calibre3">The preceding code enriches a DataFrame by creating new columns that capture specific time and date details from the <code>DATETIME</code> column:</p>
<div><div><img alt="Figure 5.13 – DATETIME data" src="img/B19923_05_13.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.13 – DATETIME data</p>
<p class="calibre3">The <code>hour</code> column tells us the hour of the day, the <code>month</code> column identifies the month, the <code>date</code> column extracts the date itself, and the <code>weekday</code> column signifies the day of the week. These additional columns provide a more comprehensive view of the time-related information within the dataset, enhancing its potential for in-depth analysis and ML applications.</p>
<p class="calibre3">This step concludes our data preparation and exploration journey. The following section will use this<a id="_idIndexMarker326" class="calibre6 pcalibre1 pcalibre"/> prepared data to build and train our <a id="_idIndexMarker327" class="calibre6 pcalibre1 pcalibre"/>model using Snowpark.</p>
<p class="callout-heading">A note on the model-building process</p>
<p class="callout">In our model-building process, we won’t be incorporating all the steps we’ve discussed thus far. Instead, we’ll focus on two significant transformations to showcase Snowpark ML pipelines. Additionally, the accompanying notebook (<strong class="source-inline1">chapter_5.ipynb</strong>) illustrates model building using Python’s scikit-learn library and how to call them as stored procedures. This will allow you to compare and contrast how the model-building process is simplified through Snowpark ML. To follow through the chapter, you can skip the model building process using the scikit-learn section and directly go to the Snowpark ML section in the notebook.</p>
<h1 id="_idParaDest-93" class="calibre5"><a id="_idTextAnchor093" class="calibre6 pcalibre1 pcalibre"/>Training ML models in Snowpark</h1>
<p class="calibre3">Now <a id="_idIndexMarker328" class="calibre6 pcalibre1 pcalibre"/>that we have prepared our dataset, the pinnacle of our journey involves the model-building process, for which we will be leveraging the power of Snowpark ML. Snowpark ML emerges as a recent addition to the Snowpark arsenal, strategically deployed to streamline the intricacies of the model-building process. Its elegance becomes apparent when we engage in a comparative exploration of the model-building procedure through the novel ML library. We will start by developing the pipeline that we’ll use to train the model using the data we prepared previously:</p>
<pre class="source-code">
import snowflake.ml.modeling.preprocessing as snowml
from snowflake.ml.modeling.pipeline import Pipeline
import joblib
df = session.table("BSD_TRAINING")
df = df.drop("DATETIME","DATE")
CATEGORICAL_COLUMNS = ["SEASON","WEATHER"]
CATEGORICAL_COLUMNS_OHE = ["SEASON_OE","WEATHER_OE"]
MIN_MAX_COLUMNS = ["TEMP"]
import numpy as np
categories = {
    "SEASON": np.array([1,2,3,4]),
    "WEATHER": np.array([1,2,3,4]),
}
preprocessing_pipeline = Pipeline(
    steps=[
        (
            "OE",
            snowml.OrdinalEncoder(
                input_cols=CATEGORICAL_COLUMNS,
                output_cols=CATEGORICAL_COLUMNS_OHE,
                categories=categories
            )
        ),
        (
            "MMS",
            snowml.MinMaxScaler(
                clip=True,
                input_cols=MIN_MAX_COLUMNS,
                output_cols=MIN_MAX_COLUMNS,
            )
        )
    ]
)
PIPELINE_FILE = 'preprocessing_pipeline.joblib'
joblib.dump(preprocessing_pipeline, PIPELINE_FILE)
transformed_df = preprocessing_pipeline.fit(df).transform(df)
transformed_df.show()
session.file.put(PIPELINE_FILE,"@snowpark_test_stage",overwrite=True)</pre> <p class="calibre3">The <a id="_idIndexMarker329" class="calibre6 pcalibre1 pcalibre"/>preceding code creates a preprocessing pipeline for the dataset by using various Snowpark ML functions. The <code>preprocessing</code> and <code>pipeline</code> modules are imported as these are essential for developing and training the model:</p>
<div><div><img alt="Figure 5.14 – Transformed data" src="img/B19923_05_14.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.14 – Transformed data</p>
<p class="calibre3">The pipeline includes ordinal encoding for categorical columns (<code>SEASON</code> and <code>WEATHER</code>) and min-max scaling for numerical columns (<code>TEMP</code>). The pipeline is saved into the stage using the <code>joblib</code> library, which can be utilized for consistent preprocessing in future analyses. Now that we have the pipeline code ready, we will build the features that are required for the model:</p>
<pre class="source-code">
CATEGORICAL_COLUMNS = ["SEASON","WEATHER"]
CATEGORICAL_COLUMNS_OHE = ["SEASON_OE","WEATHER_OE"]
MIN_MAX_COLUMNS = ["TEMP","ATEMP"]
FEATURE_LIST = \
    ["HOLIDAY","WORKINGDAY","HUMIDITY","TEMP","ATEMP","WINDSPEED"]
LABEL_COLUMNS = ['COUNT']
OUTPUT_COLUMNS = ['PREDICTED_COUNT']
PIPELINE_FILE = 'preprocessing_pipeline.joblib'
preprocessing_pipeline = joblib.load(PIPELINE_FILE)</pre> <p class="calibre3">The<a id="_idIndexMarker330" class="calibre6 pcalibre1 pcalibre"/> preceding code defines lists representing categorical columns, one-hot encoded categorical columns, and columns for min-max scaling. It also specifies a feature list, label columns, and output columns for an ML model. The <code>preprocessing_pipeline.joblib</code> file is loaded and assumed to contain a previously saved preprocessing pipeline. These elements collectively prepare the necessary data and configurations for subsequent ML tasks, ensuring consistent handling of categorical variables, feature scaling, and model predictions based on the pre-established pipeline. We will now split the data into training and testing sets:</p>
<pre class="source-code">
bsd_train_df, bsd_test_df = df.random_split(
    weights=[0.7,0.3], seed=0)
train_df = preprocessing_pipeline.fit(
    bsd_train_df).transform(bsd_train_df)
test_df = preprocessing_pipeline.transform(bsd_test_df)
train_df.show()
test_df.show()</pre> <p class="calibre3">The preceding code divides the dataset into training (70%) and testing (30%) sets using a random split. It applies the previously defined preprocessing pipeline to transform both sets, displaying the transformed training and testing datasets and ensuring consistent preprocessing for model training and evaluation. The output shows the different training and testing data:</p>
<div><div><img alt="Figure 5.15 – Training and testing dataset" src="img/B19923_05_15.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.15 – Training and testing dataset</p>
<p class="calibre3">Next, we’ll train <a id="_idIndexMarker331" class="calibre6 pcalibre1 pcalibre"/>the model with the training data:</p>
<pre class="source-code">
from snowflake.ml.modeling.linear_model import LinearRegression
regressor = LinearRegression(
    input_cols=CATEGORICAL_COLUMNS_OHE+FEATURE_LIST,
    label_cols=LABEL_COLUMNS,
    output_cols=OUTPUT_COLUMNS
)
# Train
regressor.fit(train_df)
# Predict
result = regressor.predict(test_df)
result.show()</pre> <p class="calibre3">The <code>LinearRegression</code> class defines the model, specifying the input columns (categorical columns after one-hot encoding and additional features), label columns (the target variable – that is, <code>COUNT</code>), and output columns for predictions. The model is trained on the transformed training dataset using <code>fit</code>, and then predictions are generated for the transformed testing dataset using <code>predict</code>. The resulting predictions are displayed, assessing the model’s performance on the test data:</p>
<div><div><img alt="Figure 5.16 – Predicted output" src="img/B19923_05_16.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.16 – Predicted output</p>
<p class="calibre3">The next step is<a id="_idIndexMarker332" class="calibre6 pcalibre1 pcalibre"/> to calculate various performance metrics to evaluate the accuracy of the linear regression model’s predictions:</p>
<pre class="source-code">
from snowflake.ml.modeling.metrics import mean_squared_error, explained_variance_score, mean_absolute_error, mean_absolute_percentage_error, d2_absolute_error_score, d2_pinball_score
mse = mean_squared_error(df=result,
    y_true_col_names="COUNT",
    y_pred_col_names="PREDICTED_COUNT")
evs = explained_variance_score(df=result,
    y_true_col_names="COUNT",
    y_pred_col_names="PREDICTED_COUNT")
mae = mean_absolute_error(df=result,
    y_true_col_names="COUNT",
    y_pred_col_names="PREDICTED_COUNT")
mape = mean_absolute_percentage_error(df=result,
    y_true_col_names="COUNT",
    y_pred_col_names="PREDICTED_COUNT")
d2aes = d2_absolute_error_score(df=result,
    y_true_col_names="COUNT",
    y_pred_col_names="PREDICTED_COUNT")
d2ps = d2_pinball_score(df=result,
    y_true_col_names="COUNT",
    y_pred_col_names="PREDICTED_COUNT")
print(f"Mean squared error: {mse}")
print(f"explained_variance_score: {evs}")
print(f"mean_absolute_error: {mae}")
print(f"mean_absolute_percentage_error: {mape}")
print(f"d2_absolute_error_score: {d2aes}")
print(f"d2_pinball_score: {d2ps}")</pre> <p class="calibre3">The preceding<a id="_idIndexMarker333" class="calibre6 pcalibre1 pcalibre"/> code calculates various<a id="_idTextAnchor094" class="calibre6 pcalibre1 pcalibre"/> performance metrics to assess the accuracy of the linear regression model’s predictions. Metrics such as mean squared error, explained variance score, mean absolute error, mean fundamental percentage error, d2 definitive error score, and d2 pinball score are computed based on the actual (<code>COUNT</code>) and predicted (<code>PREDICTED_COUNT</code>) values stored in the <code>result</code> DataFrame:</p>
<div><div><img alt="Figure 5.17 – Performance metrics" src="img/B19923_05_17.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 5.17 – Performance metrics</p>
<p class="calibre3">These performance metrics provide a comprehensive evaluation of the model’s performance across different aspects of prediction accuracy.</p>
<p class="callout-heading">Model results and efficiency</p>
<p class="callout">The presented model metrics might need to showcase more exceptional results. It’s crucial to emphasize that the primary objective of this case study is to elucidate the model-building process and highlight the facilitative role of Snowpark ML. The focus of this chapter has been on illustrating the construction of a linear regression model.</p>
<h2 id="_idParaDest-94" class="calibre7"><a id="_idTextAnchor095" class="calibre6 pcalibre1 pcalibre"/>The efficiency of Snowpark ML</h2>
<p class="calibre3">In delving<a id="_idIndexMarker334" class="calibre6 pcalibre1 pcalibre"/> into the intricacies of the model-building process facilitated by Snowpark ML, the initial standout feature is its well-thought-out design. A notable departure from the conventional approach is evident as Snowpark ML closely mirrors the streamlined methodology found in scikit-learn. A significant advantage is eliminating the need to<a id="_idIndexMarker335" class="calibre6 pcalibre1 pcalibre"/> create separate <strong class="bold">user-defined functions</strong> (<strong class="bold">UDFs</strong>) and stored procedures, streamlining the entire model-building workflow.</p>
<p class="calibre3">It’s crucial to recognize that Snowpark ML seamlessly integrates with scikit-learn while adhering to similar conventions in the model construction process. A noteworthy distinction is a prerequisite in scikit-learn for data to be passed as a pandas DataFrame. Consequently, the Snowflake table must be converted into a pandas DataFrame before you can initiate the model-building phase. However, it’s imperative to be mindful of potential memory constraints, especially when dealing with substantial datasets. Converting a large table into a pandas DataFrame demands a significant amount of memory since the entire dataset is loaded into memory.</p>
<p class="calibre3">In contrast, Snowpark ML provides a more native and memory-efficient approach to the model-building process. This native integration with Snowflake’s environment not only enhances the efficiency of the workflow but also mitigates memory-related challenges associated with large datasets. The utilization of Snowpark ML emerges as a strategic and seamless choice for executing complex model-building tasks within the Snowflake ecosystem.</p>
<h1 id="_idParaDest-95" class="calibre5"><a id="_idTextAnchor096" class="calibre6 pcalibre1 pcalibre"/>Summary</h1>
<p class="calibre3">Snowpark ML emerges as a versatile and powerful tool for data scientists, enabling them to tackle complex data science tasks within Snowflake’s unified data platform. Its integration with popular programming languages, scalability, and real-time processing capabilities make it invaluable for various applications, from predictive modeling to real-time analytics and advanced AI tasks. With Snowpark ML, organizations can harness the full potential of their data, drive innovation, and gain a competitive edge in today’s data-driven landscape.</p>
<p class="calibre3">In the next chapter, we will continue by deploying the model in Snowflake and operationalizing it.</p>
</div>
</body></html>