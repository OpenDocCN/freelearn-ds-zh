<html><head></head><body>
<div id="_idContainer132" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-78"><a id="_idTextAnchor078" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1.1">5</span></h1>
<h1 id="_idParaDest-79" class="calibre5"><a id="_idTextAnchor079" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2.1">Developing Data Science Projects with Snowpark</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.3.1">The emergence of cloud technologies has ushered in a new era of possibilities. </span><span class="kobospan" id="kobo.3.2">With the advent of Data Cloud, a robust platform that unifies data storage, processing, and analysis, data scientists have many opportunities to explore, analyze, and extract meaningful insights from vast datasets. </span><span class="kobospan" id="kobo.3.3">In this intricate digital realm, the role of Snowpark, a cutting-edge data processing framework, becomes paramount. </span><span class="kobospan" id="kobo.3.4">This chapter serves as an illuminating guide, delving deep into developing data science projects with Snowpark, unraveling its intricacies, and harnessing its potential to the </span><span><span class="kobospan" id="kobo.4.1">fullest extent.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.5.1">In this chapter, we’re going to cover the following </span><span><span class="kobospan" id="kobo.6.1">main topics:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.7.1">Data science in </span><span><span class="kobospan" id="kobo.8.1">Data Cloud</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.9.1">Exploring and </span><span><span class="kobospan" id="kobo.10.1">preparing data</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.11.1">Training </span><strong class="bold"><span class="kobospan" id="kobo.12.1">machine learning</span></strong><span class="kobospan" id="kobo.13.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.14.1">ML</span></strong><span class="kobospan" id="kobo.15.1">) models </span><span><span class="kobospan" id="kobo.16.1">in Snowpark</span></span></li>
</ul>
<h1 id="_idParaDest-80" class="calibre5"><a id="_idTextAnchor080" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.17.1">Technical requirements</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.18.1">For this chapter, you’ll require an active Snowflake account and Python installed with Anaconda configured locally. </span><span class="kobospan" id="kobo.18.2">You can sign up for a Snowflake Trial account </span><span><span class="kobospan" id="kobo.19.1">at </span></span><a href="https://signup.snowflake.com/" class="calibre6 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.20.1">https://signup.snowflake.com/</span></span></a><span><span class="kobospan" id="kobo.21.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.22.1">To configure Anaconda, follow </span><a href="https://conda.io/projects/conda/en/latest/user-guide/getting-started.html" class="calibre6 pcalibre1 pcalibre"><span class="kobospan" id="kobo.23.1">https://conda.io/projects/conda/en/latest/user-guide/getting-started.html</span></a><span class="kobospan" id="kobo.24.1">. </span></p>
<p class="calibre3"><span class="kobospan" id="kobo.25.1">In addition, to install and set up Python for VS Code, follow </span><a href="https://code.visualstudio.com/docs/datascience/jupyter-notebooks" class="calibre6 pcalibre1 pcalibre"><span class="kobospan" id="kobo.26.1">https://code.visualstudio.com/docs/python/python-tutorial</span></a><span class="kobospan" id="kobo.27.1">. </span></p>
<p class="calibre3"><span class="kobospan" id="kobo.28.1">To learn how to operate Jupyter Notebook in VS Code, go </span><span><span class="kobospan" id="kobo.29.1">to </span></span><a href="https://code.visualstudio.com/docs/datascience/jupyter-notebooks" class="calibre6 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.30.1">https://code.visualstudio.com/docs/datascience/jupyter-notebooks</span></span></a><span><span class="kobospan" id="kobo.31.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.32.1">The supporting materials for this chapter are available in this book’s GitHub repository </span><span><span class="kobospan" id="kobo.33.1">at </span></span><a href="https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark" class="calibre6 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.34.1">https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark</span></span></a><span><span class="kobospan" id="kobo.35.1">.</span></span></p>
<h1 id="_idParaDest-81" class="calibre5"><a id="_idTextAnchor081" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.36.1">Data science in Data Cloud</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.37.1">Data science </span><a id="_idIndexMarker283" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.38.1">transcends traditional boundaries in the Data Cloud ecosystem, offering a dynamic environment where data scientists can harness the power of distributed computing and advanced analytics. </span><span class="kobospan" id="kobo.38.2">With the ability to seamlessly integrate various data sources, including structured and unstructured data, Data Cloud provides a data exploration and experimentation environment. </span><span class="kobospan" id="kobo.38.3">We will start this section with a brief data science and </span><span><span class="kobospan" id="kobo.39.1">ML refresher.</span></span></p>
<h2 id="_idParaDest-82" class="calibre7"><a id="_idTextAnchor082" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.40.1">Data science and ML concepts</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.41.1">Data science </span><a id="_idIndexMarker284" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.42.1">and ML have surged to the forefront of technological and business innovation, becoming integral components of decision-making, strategic planning, and product development across virtually all industries. </span><span class="kobospan" id="kobo.42.2">The journey to their current popularity and influence is a testament to several factors, including advancements in technology, the explosion of data, and the increasing computational power available. </span><span class="kobospan" id="kobo.42.3">This section will briefly discuss data science and </span><span><span class="kobospan" id="kobo.43.1">ML concepts.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.44.1">Data science</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.45.1">Data science is a multidisciplinary field that relies on various software tools, algorithms, and ML principles to extract valuable insights from extensive datasets. </span><span class="kobospan" id="kobo.45.2">Data scientists are pivotal in collecting, transforming, and converting data into predictive and prescriptive insights. </span><span class="kobospan" id="kobo.45.3">By employing sophisticated techniques, data science uncovers hidden patterns and meaningful correlations within data, enabling businesses to act on informed decisions based on </span><span><span class="kobospan" id="kobo.46.1">empirical evidence.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.47.1">Artificial intelligence</span></h3>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.48.1">Artificial intelligence</span></strong><span class="kobospan" id="kobo.49.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.50.1">AI</span></strong><span class="kobospan" id="kobo.51.1">) encompasses </span><a id="_idIndexMarker285" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.52.1">the science and engineering of creating intelligent machines and brilliant computer programs capable of autonomously processing information and generating outcomes. </span><span class="kobospan" id="kobo.52.2">AI systems are designed to solve intricate problems using logic and reasoning, similar to human cognitive processes. </span><span class="kobospan" id="kobo.52.3">These systems operate autonomously, aiming to emulate human-like intelligence in decision-making and </span><span><span class="kobospan" id="kobo.53.1">problem-solving tasks.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.54.1">ML</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.55.1">ML, a subset of AI, involves</span><a id="_idIndexMarker286" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.56.1"> specialized algorithms integrated into the data science workflow. </span><span class="kobospan" id="kobo.56.2">These algorithms are meticulously crafted software programs that are designed to detect patterns, identify correlations, and pinpoint anomalies within data. </span><span class="kobospan" id="kobo.56.3">ML algorithms excel at predicting outcomes based on existing data and continue to learn and improve their accuracy as they encounter new data and situations. </span><span class="kobospan" id="kobo.56.4">Unlike humans, ML algorithms can process thousands of attributes and features, enabling the discovery of unique combinations and correlations in vast datasets. </span><span class="kobospan" id="kobo.56.5">This capability makes ML indispensable for extracting valuable insights and predictions from extensive </span><span><span class="kobospan" id="kobo.57.1">data collections.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.58.1">Now that we have the terminologies straightened out, we will discuss how the Data Cloud paradigm has helped the growth of data science and ML </span><span><span class="kobospan" id="kobo.59.1">for organizations.</span></span></p>
<h2 id="_idParaDest-83" class="calibre7"><a id="_idTextAnchor083" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.60.1">The Data Cloud paradigm</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.61.1">Data science </span><a id="_idIndexMarker287" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.62.1">in the cloud represents a paradigm shift in how data-driven insights are derived and applied. </span><span class="kobospan" id="kobo.62.2">In this innovative approach, data science processes, tools, and techniques are seamlessly integrated into the cloud, allowing organizations to leverage the power of scalable infrastructure and </span><span><span class="kobospan" id="kobo.63.1">advanced analytics.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.64.1">At the heart of this paradigm lies Data Cloud, a dynamic ecosystem that transcends traditional data storage and processing constraints. </span><span class="kobospan" id="kobo.64.2">The Data Cloud paradigm represents a seismic shift from conventional data silos, offering a unified platform where structured and unstructured data coalesce seamlessly. </span><span class="kobospan" id="kobo.64.3">Through distributed computing, parallel processing, and robust data management, Data Cloud sets the stage for a data science revolution. </span><span class="kobospan" id="kobo.64.4">The capabilities and tools that empower data scientists are seamlessly integrated and are designed to handle diverse data types and analytical workloads within Data Cloud. </span><span class="kobospan" id="kobo.64.5">As such, Data Cloud offers various advantages for running data science and </span><span><span class="kobospan" id="kobo.65.1">ML workloads.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.66.1">Advantages of Data Cloud for data science</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.67.1">One of the</span><a id="_idIndexMarker288" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.68.1"> key advantages of Snowflake’s Data Cloud is the ability to store and process vast amounts of data without the constraints of hardware limitations. </span><span class="kobospan" id="kobo.68.2">It offers a scalable solution to handling vast volumes of data, enabling data scientists to work with extensive datasets without having to worry about computing or storage constraints. </span><span class="kobospan" id="kobo.68.3">The cloud-based interface provides a collaborative and flexible environment for data scientists and analysts and comes with built-in collaboration features, version control, and support for popular data science libraries and frameworks </span><span><span class="kobospan" id="kobo.69.1">through Snowpark.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.70.1">Furthermore, Data Cloud offers a diverse ecosystem of services and resources tailored for data science tasks through managed services that simplify these processes, from data ingestion and preparation to ML model training and deployment. </span><span class="kobospan" id="kobo.70.2">For instance, data pipelines can be automated using serverless computing, and ML models can be trained on powerful GPU instances, leading to faster experimentation and iteration. </span><span class="kobospan" id="kobo.70.3">Data security and compliance are paramount in data science, especially when dealing with sensitive information, and Data Cloud provides different security measures, including encryption, access control, and row-level policies, ensuring that data scientists can work with sensitive data in a secure and compliant manner, adhering to industry regulations and organizational policies. </span><span class="kobospan" id="kobo.70.4">The Snowpark framework is at the center of Snowflake’s Data Cloud to support these capabilities. </span><span class="kobospan" id="kobo.70.5">The following section will discuss why Snowpark is used for data science </span><span><span class="kobospan" id="kobo.71.1">and ML.</span></span></p>
<h2 id="_idParaDest-84" class="calibre7"><a id="_idTextAnchor084" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.72.1">Why Snowpark for data science and ML?</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.73.1">Snowpark </span><a id="_idIndexMarker289" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.74.1">offers unparalleled integration capabilities for data engineers, enabling seamless interaction with data stored in large volumes and diverse formats. </span><span class="kobospan" id="kobo.74.2">Its versatile API facilitates effortless data exploration, transformation, and manipulation, laying a robust foundation for data science models and ML development and empowering data scientists to harness the full potential of their analytical workflows. </span><span class="kobospan" id="kobo.74.3">Data science teams can now focus on their core tasks without the hassle of infrastructure or </span><span><span class="kobospan" id="kobo.75.1">environment maintenance.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.76.1">Snowpark excels in scalability and performance, which is crucial for enterprise data science and ML workloads; leveraging Snowflake’s distributed architecture to handle massive datasets and complex computations with remarkable efficiency and the ability to parallelize processing tasks and distribute workloads across multiple nodes ensures lightning-fast execution, even when dealing with petabytes of data. </span><span class="kobospan" id="kobo.76.2">These features, combined with Snowflake’s automatic optimization features, allow data scientists to focus on their analyses without being burdened by </span><span><span class="kobospan" id="kobo.77.1">infrastructure limitations.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.78.1">Snowpark </span><a id="_idIndexMarker290" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.79.1">offers a rich array of advanced analytics capabilities that are indispensable for data science and ML tasks. </span><span class="kobospan" id="kobo.79.2">From statistical analysis to predictive modeling, geospatial analytics, or even data mining, it provides a comprehensive toolkit for data scientists to explore complex patterns and extract valuable insights. </span><span class="kobospan" id="kobo.79.3">Its support for ML libraries and algorithms further amplifies its utility, enabling the development of sophisticated models for classification, regression, and clustering. </span><span class="kobospan" id="kobo.79.4">With the rich features and functionalities mentioned previously, Snowpark provides many benefits for data science and ML workloads. </span><span class="kobospan" id="kobo.79.5">In the next section, we will explore the world of the Snowpark ML library and its </span><span><span class="kobospan" id="kobo.80.1">different functionalities.</span></span></p>
<h2 id="_idParaDest-85" class="calibre7"><a id="_idTextAnchor085" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.81.1">Introduction to Snowpark ML</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.82.1">Snowpark </span><a id="_idIndexMarker291" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.83.1">constitutes a compendium of libraries and runtimes within Snowflake, facilitating the secure deployment and processing of non-SQL code by encompassing languages such as Python with the code execution that occurs server-side within the Snowflake infrastructure, all while leveraging a virtual warehouse. </span><span class="kobospan" id="kobo.83.2">The newest addition to the Snowpark libraries is Snowpark ML. </span><span class="kobospan" id="kobo.83.3">Snowpark ML represents a groundbreaking fusion of Snowflake’s powerful data processing capabilities and the transformative potential of ML. </span><span class="kobospan" id="kobo.83.4">As the frontier of data science expands, Snowpark ML emerges as a cutting-edge framework that’s designed to empower data professionals to harness the full potential of their data within Snowflake’s </span><span><span class="kobospan" id="kobo.84.1">cloud-based environment.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.85.1">At its core, Snowpark ML is engineered to facilitate seamless integration between Snowflake’s data processing capabilities and advanced ML techniques. </span><span class="kobospan" id="kobo.85.2">With Snowpark ML, data scientists, analysts, and engineers can leverage familiar programming languages and libraries to develop sophisticated ML models directly within Snowflake. </span><span class="kobospan" id="kobo.85.3">This integration eliminates the barriers between data storage, processing, and modeling, streamlining the end-to-end data science workflow. </span><span class="kobospan" id="kobo.85.4">Snowpark ML catalyzes innovation, enabling data professionals to efficiently explore, transform, and model data. </span><span class="kobospan" id="kobo.85.5">By bridging the gap between data processing and ML, Snowpark ML empowers organizations to make data-driven decisions, uncover valuable insights, and drive business growth in the digital age. </span><span class="kobospan" id="kobo.85.6">The following figure shows the Snowpark </span><span><span class="kobospan" id="kobo.86.1">ML framework:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer115">
<span class="kobospan" id="kobo.87.1"><img alt="Figure 5.1 – Snowpark ML" src="image/B19923_05_1.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.88.1">Figure 5.1 – Snowpark ML</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.89.1">The </span><a id="_idIndexMarker292" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.90.1">preceding architecture consists of various components that work cohesively together. </span><span class="kobospan" id="kobo.90.2">We will look at each of these components in more detail in the </span><span><span class="kobospan" id="kobo.91.1">next section.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.92.1">Snowpark ML API</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.93.1">Similar to </span><a id="_idIndexMarker293" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.94.1">Snowpark DataFrame, which helps operate with the data, Snowpark ML provides APIs as a Python library called </span><strong class="source-inline"><span class="kobospan" id="kobo.95.1">snowflake-ml</span></strong><span class="kobospan" id="kobo.96.1"> to support every stage of the ML development and deployment process, allowing support for pre-processing data and training, managing, and deploying ML models all </span><span><span class="kobospan" id="kobo.97.1">within Snowflake:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer116">
<span class="kobospan" id="kobo.98.1"><img alt="Figure 5.2 – Snowpark ML API" src="image/B19923_05_2.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.99.1">Figure 5.2 – Snowpark ML API</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.100.1">The Snowpark ML API consists of Snowpark ML modeling for developing and training the models and</span><a id="_idIndexMarker294" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.101.1"> Snowpark ML Ops for monitoring and operating the model. </span><span class="kobospan" id="kobo.101.2">The </span><strong class="source-inline"><span class="kobospan" id="kobo.102.1">snowflake.ml.modeling</span></strong><span class="kobospan" id="kobo.103.1"> module provides APIs for pre-processing, feature engineering, and model training based on familiar libraries, such as scikit-learn and XGBoost. </span><span class="kobospan" id="kobo.103.2">The complete end-to-end ML experience can be done using Snowpark. </span><span class="kobospan" id="kobo.103.3">We’ll cover this in the </span><span><span class="kobospan" id="kobo.104.1">next section.</span></span></p>
<h2 id="_idParaDest-86" class="calibre7"><a id="_idTextAnchor086" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.105.1">End-to-end ML with Snowpark</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.106.1">The</span><a id="_idIndexMarker295" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.107.1"> quest for seamless, end-to-end ML solutions has become paramount, and Snowpark offers a comprehensive ecosystem for end-to-end ML. </span><span class="kobospan" id="kobo.107.2">This </span><a id="_idIndexMarker296" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.108.1">section delves into the intricate world of leveraging Snowpark to craft end-to-end ML pipelines, from data ingestion and preprocessing to model development, training, and deployment, unveiling the seamless process of developing ML within Snowflake’s </span><span><span class="kobospan" id="kobo.109.1">robust framework.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.110.1">ML processes involve a systematic approach to solving complex problems through data processing. </span><span class="kobospan" id="kobo.110.2">This typically</span><a id="_idIndexMarker297" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.111.1"> includes stages such as defining the problem, collecting and preparing data, </span><strong class="bold"><span class="kobospan" id="kobo.112.1">exploratory data analysis</span></strong><span class="kobospan" id="kobo.113.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.114.1">EDA</span></strong><span class="kobospan" id="kobo.115.1">), feature engineering, model selection, training, evaluation, and deployment, with each operation being crucial and iterative. </span><span class="kobospan" id="kobo.115.2">It allows data scientists to refine their approaches based on insights gained along the way. </span><span class="kobospan" id="kobo.115.3">The process is often cyclical, with continuous iterations to improve models </span><span><span class="kobospan" id="kobo.116.1">and predictions:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer117">
<span class="kobospan" id="kobo.117.1"><img alt="Figure 5.3 – End-to-end ML flow" src="image/B19923_05_3.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.118.1">Figure 5.3 – End-to-end ML flow</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.119.1">We can broadly classify the ML stages as preparing and transforming, training and building the model, and interfering with the model to obtain prediction results. </span><span class="kobospan" id="kobo.119.2">We will discuss each of these </span><span><span class="kobospan" id="kobo.120.1">steps next.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.121.1">Preparing and transforming the data</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.122.1">Raw data is</span><a id="_idIndexMarker298" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.123.1"> often messy, containing missing values, outliers, and inconsistencies. </span><span class="kobospan" id="kobo.123.2">Getting the correct data from multiple systems usually consumes most of the data scientist’s time. </span><span class="kobospan" id="kobo.123.3">Snowflake solves this problem by </span><a id="_idIndexMarker299" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.124.1">providing a governed Data Cloud paradigm that supports all types of data and provides a unified place to instantly store and consume relevant data to unlock ML models’ power. </span><span class="kobospan" id="kobo.124.2">The data preparation and transformation process involves EDA, cleaning, and processing, and ends with feature engineering. </span><span class="kobospan" id="kobo.124.3">This step also consists of data engineering pipelines, which help apply data transformations to prepare the data for the next step. </span><span class="kobospan" id="kobo.124.4">For data pre-processing, </span><strong class="source-inline"><span class="kobospan" id="kobo.125.1">snowflake.ml.modeling</span></strong><span class="kobospan" id="kobo.126.1">, preprocessing, and Snowpark functions can be used to transform </span><span><span class="kobospan" id="kobo.127.1">the data.</span></span></p>
<h4 class="calibre16"><span class="kobospan" id="kobo.128.1">EDA</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.129.1">EDA is a </span><a id="_idIndexMarker300" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.130.1">critical step that involves preliminary investigations to understand the data’s structure, patterns, and trends as it helps uncover</span><a id="_idIndexMarker301" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.131.1"> hidden patterns and guide feature selection. </span><span class="kobospan" id="kobo.131.2">Data scientists and analysts collaborate closely with business stakeholders to define the specific questions that need to be answered or the problems that need to be solved, which guides them in selecting the relevant data. </span><span class="kobospan" id="kobo.131.3">Through charts, graphs, and statistical summaries, data scientists can identify patterns, trends, correlations, and outliers within the dataset, all of which provide valuable insights into the data’s distribution and help them understand the data better to build </span><span><span class="kobospan" id="kobo.132.1">feature selection.</span></span></p>
<h4 class="calibre16"><span class="kobospan" id="kobo.133.1">Data cleaning and preprocessing</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.134.1">Data cleaning </span><a id="_idIndexMarker302" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.135.1">involves handling missing data, correcting errors, and ensuring consistency. </span><span class="kobospan" id="kobo.135.2">The data is suitable for training the model through</span><a id="_idIndexMarker303" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.136.1"> preprocessing techniques such as normalization, scaling, and transformations, along with various sampling techniques that are applied to evaluate a subset of the data, providing insights into its richness </span><span><span class="kobospan" id="kobo.137.1">and variability.</span></span></p>
<h4 class="calibre16"><span class="kobospan" id="kobo.138.1">Feature engineering</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.139.1">Feature engineering</span><a id="_idIndexMarker304" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.140.1"> involves creating new features or modifying existing ones to enhance the performance of ML models. </span><span class="kobospan" id="kobo.140.2">It requires domain expertise to identify relevant features that can improve predictive accuracy. </span><span class="kobospan" id="kobo.140.3">Performing feature engineering on the centralized data in Snowflake accelerates model development, reduces costs, and enables the reuse of new features. </span><span class="kobospan" id="kobo.140.4">Some techniques, such as creating interaction terms and transforming variables, extract meaningful information from raw data, making it more informative </span><span><span class="kobospan" id="kobo.141.1">for modeling.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.142.1">Training and building the model</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.143.1">Once the data</span><a id="_idIndexMarker305" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.144.1"> is ready and the features have been built, the next step is to train and develop the model. </span><span class="kobospan" id="kobo.144.2">In this stage, the data scientist trains various models, such as regression, classification, clustering, or deep learning, depending on the nature of the problem, by passing a subset of the data, or training set, through the modeling function to derive a predictive function. </span><span class="kobospan" id="kobo.144.3">The model is developed using statistical methods for hypothesis testing and inferential statistics. </span><span class="kobospan" id="kobo.144.4">Advanced techniques, such as ensemble methods, neural networks, and natural language processing, are also used, depending on </span><span><span class="kobospan" id="kobo.145.1">the data.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.146.1">Once the model </span><a id="_idIndexMarker306" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.147.1">has been developed, it’s tested on data that wasn’t part of the training set to determine its effectiveness, which is usually measured in terms of its predictive strength and robustness, and the model is optimized with hyperparameter tuning. </span><span class="kobospan" id="kobo.147.2">Cross-validation techniques optimize the model’s performance, ensuring accurate predictions and </span><span><span class="kobospan" id="kobo.148.1">valuable insights.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.149.1">This combination of steps enables data scientists to conduct in-depth feature engineering, tune hyperparameters, and iteratively create and assess ML models. </span><span class="kobospan" id="kobo.149.2">Intuitions become accurate predictions as data scientists experiment with various algorithms, evaluating the performance of each model and adjusting parameters on their chosen model to optimize the code for their specific datasets. </span><strong class="source-inline"><span class="kobospan" id="kobo.150.1">snowflake.ml.modeling</span></strong><span class="kobospan" id="kobo.151.1"> can be used for training by utilizing the </span><strong class="source-inline"><span class="kobospan" id="kobo.152.1">fit()</span></strong><span class="kobospan" id="kobo.153.1"> method for an algorithm such </span><span><span class="kobospan" id="kobo.154.1">as XGBoost.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.155.1">Inference</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.156.1">Once the</span><a id="_idIndexMarker307" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.157.1"> models have been trained, Snowpark ML supports their seamless deployment and inference. </span><span class="kobospan" id="kobo.157.2">Models can be deployed for inference, enabling organizations to make data-driven decisions based on predictive insights. </span><span class="kobospan" id="kobo.157.3">Snowpark ML has a model registry to manage and organize Snowpark models throughout their life cycle. </span><span class="kobospan" id="kobo.157.4">The model registry supports versioning of the models and stores metadata information about the models, hyperparameters, and evaluation metrics, facilitating experimentation and model comparison. </span><span class="kobospan" id="kobo.157.5">It also supports model monitoring and auditing and aids in collaboration between data scientists working on the model. </span><span class="kobospan" id="kobo.157.6">The model registry is part of Snowpark MLOps and can be accessed through </span><strong class="source-inline"><span class="kobospan" id="kobo.158.1">snowpark.ml.registry</span></strong><span class="kobospan" id="kobo.159.1">. </span><span class="kobospan" id="kobo.159.2">The pipelines can be orchestrated using </span><span><span class="kobospan" id="kobo.160.1">Snowflake Tasks.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.161.1">Now that we have established the foundations of Snowpark ML, its place in ML, and how Snowpark supports data science workloads, we will dive deep into the complete data science scenario with Snowpark. </span><span class="kobospan" id="kobo.161.2">The following section will focus on exploring and preparing </span><span><span class="kobospan" id="kobo.162.1">the data.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.163.1">A note on data engineering</span></p>
<p class="callout"><span class="kobospan" id="kobo.164.1">In the next section, we’ll conduct exploration, transformation, and feature engineering using Snowpark Python and pandas. </span><span class="kobospan" id="kobo.164.2">As we proceed to build models with SnowparkML, we will incorporate some of the steps discussed earlier in </span><span><span class="kobospan" id="kobo.165.1">this section.</span></span></p>
<h1 id="_idParaDest-87" class="calibre5"><a id="_idTextAnchor087" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.166.1">Exploring and preparing data</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.167.1">In the first step of</span><a id="_idIndexMarker308" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.168.1"> the ML process, we must explore and prepare the data in Snowflake using Snowpark to make it available for training the ML models. </span><span class="kobospan" id="kobo.168.2">We will work with the Bike Sharing dataset from Kaggle, which offers an hourly record of rental data for 2 years. </span><span class="kobospan" id="kobo.168.3">The primary objective is to forecast the number of bikes rented each hour for a specific timeframe based solely on the information available before the rental period. </span><span class="kobospan" id="kobo.168.4">In essence, the model will harness the power of historical data to predict future bike rental patterns using Snowpark. </span><span class="kobospan" id="kobo.168.5">More information about the particular dataset has been provided in the respective GitHub </span><span><span class="kobospan" id="kobo.169.1">repository (</span></span><a href="https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark" class="calibre6 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.170.1">https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark</span></span></a><span><span class="kobospan" id="kobo.171.1">).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.172.1">Data exploration allows us to dissect the data to uncover intricate details that might otherwise stay hidden, acting as the foundation for our entire analysis. </span><span class="kobospan" id="kobo.172.2">We will start the process by loading the dataset into a </span><span><span class="kobospan" id="kobo.173.1">Snowpark DataFrame:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.174.1">
df_table=session.table("BSD_TRAINING")</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.175.1">Once the data has been successfully loaded, the subsequent imperative is to gain a comprehensive understanding of the </span><span><span class="kobospan" id="kobo.176.1">dataset’s scale:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.177.1">
number_of_rows = df_table.count()
number_of_columns = len(df_table.columns)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.178.1">Fortunately, Snowpark provides functions specifically designed to facilitate this </span><span><span class="kobospan" id="kobo.179.1">critical task:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer118">
<span class="kobospan" id="kobo.180.1"><img alt="Figure 5.4 – Total number of columns" src="image/B19923_05_4.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.181.1">Figure 5.4 – Total number of columns</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.182.1">Now that we</span><a id="_idIndexMarker309" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.183.1"> know the scale of the data, let’s get a sense of it by looking at a few rows of </span><span><span class="kobospan" id="kobo.184.1">the dataset:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.185.1">
df_table.sample(n=2).show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.186.1">This returns the two rows from the data </span><span><span class="kobospan" id="kobo.187.1">for analysis:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer119">
<span class="kobospan" id="kobo.188.1"><img alt="Figure 5.5 – Two rows of data" src="image/B19923_05_5.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.189.1">Figure 5.5 – Two rows of data</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.190.1">As depicted in the preceding figure, the </span><strong class="source-inline"><span class="kobospan" id="kobo.191.1">COUNT</span></strong><span class="kobospan" id="kobo.192.1"> column is a straightforward aggregation of </span><strong class="source-inline"><span class="kobospan" id="kobo.193.1">CASUAL</span></strong><span class="kobospan" id="kobo.194.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.195.1">REGISTERED</span></strong><span class="kobospan" id="kobo.196.1">. </span><span class="kobospan" id="kobo.196.2">In data science, these types of variables are commonly referred to as “leakage variables.” </span><span class="kobospan" id="kobo.196.3">When we construct our models, we’ll delve deeper into strategies for managing these variables. </span><span class="kobospan" id="kobo.196.4">Date columns consistently present an intriguing and complex category to grapple with. </span><span class="kobospan" id="kobo.196.5">Within this dataset, there is potential to create valuable new features derived from the </span><strong class="source-inline"><span class="kobospan" id="kobo.197.1">DATETIME</span></strong><span class="kobospan" id="kobo.198.1"> column, which could significantly influence our response variables. </span><span class="kobospan" id="kobo.198.2">Before we start with data cleansing and the feature engineering process, let’s see the column type to understand and make more </span><span><span class="kobospan" id="kobo.199.1">informed decisions:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.200.1">
import pprint
data_types = df_table.schema
data_types = df_table.schema.fields
pprint.pprint(data_types)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.201.1">This will give </span><a id="_idIndexMarker310" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.202.1">us the schema and the data types for each field so that we can understand the </span><span><span class="kobospan" id="kobo.203.1">data better:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer120">
<span class="kobospan" id="kobo.204.1"><img alt="Figure 5.6 – Schema information" src="image/B19923_05_6.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.205.1">Figure 5.6 – Schema information</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.206.1">Now that we are equipped with basic information about the data, let’s start finding the missing values in </span><span><span class="kobospan" id="kobo.207.1">the data.</span></span></p>
<h2 id="_idParaDest-88" class="calibre7"><a id="_idTextAnchor088" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.208.1">Missing value analysis</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.209.1">Addressing </span><a id="_idIndexMarker311" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.210.1">missing values is a fundamental preprocessing step in ML. </span><span class="kobospan" id="kobo.210.2">Incomplete data can disrupt model training and hinder predictive accuracy, potentially leading to erroneous conclusions or suboptimal performance. </span><span class="kobospan" id="kobo.210.3">By systematically imputing or filling these gaps, we can bolster the integrity of our dataset, providing ML algorithms with a more comprehensive and coherent dataset for more robust and reliable analyses and predictions. </span><span class="kobospan" id="kobo.210.4">This practice is akin to affording our models the necessary information to make sound, data-driven decisions. </span><span class="kobospan" id="kobo.210.5">Let’s check for any missing values in </span><span><span class="kobospan" id="kobo.211.1">our dataset:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.212.1">
from snowflake.snowpark.functions import count, col
data_types = df_table.schema
print(data_types)
for column in df_table.columns:
    print(f"Null values in {column} is {number_of_rows - df_table.agg(count(col(column))).collect()[0][0]}")</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.213.1">The preceding code helps us find out whether any values are empty </span><span><span class="kobospan" id="kobo.214.1">or missing:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer121">
<span class="kobospan" id="kobo.215.1"><img alt="Figure 5.7  – Missing value analysis" src="image/B19923_05_7.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.216.1">Figure 5.7  – Missing value analysis</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.217.1">Our</span><a id="_idIndexMarker312" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.218.1"> initial examination of missing values in the column shows no missing values in our dataset. </span><span class="kobospan" id="kobo.218.2">However, a closer examination reveals the presence of numerous 0s within the </span><strong class="source-inline"><span class="kobospan" id="kobo.219.1">WINDSPEED</span></strong><span class="kobospan" id="kobo.220.1"> column, which is indicative of potentially missing values. </span><span class="kobospan" id="kobo.220.2">Logically, windspeed cannot equate to zero, implying that each </span><strong class="source-inline"><span class="kobospan" id="kobo.221.1">0</span></strong><span class="kobospan" id="kobo.222.1"> within the column signifies a </span><span><span class="kobospan" id="kobo.223.1">missing value:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.224.1">
print(f"Zero Values in windspeed column is {df_table.filter(df_table['WINDSPEED']==0).count()}")</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.225.1">This will print out the </span><span><span class="kobospan" id="kobo.226.1">following output:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer122">
<span class="kobospan" id="kobo.227.1"><img alt="Figure 5.8 – Output value" src="image/B19923_05_8.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.228.1">Figure 5.8 – Output value</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.229.1">We can see that there are </span><strong class="source-inline"><span class="kobospan" id="kobo.230.1">1313</span></strong><span class="kobospan" id="kobo.231.1"> values in the </span><strong class="source-inline"><span class="kobospan" id="kobo.232.1">WINDSPEED</span></strong><span class="kobospan" id="kobo.233.1"> column. </span><span class="kobospan" id="kobo.233.2">With this column harboring missing data, the subsequent challenge is determining an effective strategy for imputing these missing values. </span><span class="kobospan" id="kobo.233.3">As is widely acknowledged, various methods exist for addressing missing data within a column. </span><span class="kobospan" id="kobo.233.4">In our case, we’ll employ a straightforward imputation, substituting the 0s with the mean value of </span><span><span class="kobospan" id="kobo.234.1">the column:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.235.1">
from snowflake.snowpark.functions import iff, avg
wind_speed_mean = df_train.select(mean("windspeed")).collect()[0][0]
df_train = df_train.replace({0:wind_speed_mean}, subset=["windspeed"])
df_train.show()
df_train.write.mode("overwrite").save_as_table("model_data")</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.236.1">The preceding code replaces the 0s with the mean value of </span><span><span class="kobospan" id="kobo.237.1">the column:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer123">
<span class="kobospan" id="kobo.238.1"><img alt="Figure 5.9 – Pre-processed data" src="image/B19923_05_9.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.239.1">Figure 5.9 – Pre-processed data</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.240.1">This </span><a id="_idIndexMarker313" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.241.1">concludes our preprocessing journey. </span><span class="kobospan" id="kobo.241.2">Next, we’ll perform </span><span><span class="kobospan" id="kobo.242.1">outlier analysis.</span></span></p>
<h2 id="_idParaDest-89" class="calibre7"><a id="_idTextAnchor089" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.243.1">Outlier analysis</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.244.1">The</span><a id="_idIndexMarker314" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.245.1"> process of detecting and removing outliers is pivotal in enhancing model accuracy and robustness. </span><span class="kobospan" id="kobo.245.2">Outliers are data points that significantly deviate from most datasets, often stemming from errors, anomalies, or rare events. </span><span class="kobospan" id="kobo.245.3">These aberrations can unduly influence model training, leading to skewed predictions or reduced generalization capabilities. </span><span class="kobospan" id="kobo.245.4">By identifying and eliminating outliers, we can improve the quality and reliability of our models and ensure that they are better equipped to discern meaningful patterns within the data. </span><span class="kobospan" id="kobo.245.5">This practice fosters more accurate predictions and a higher level of resilience, ultimately contributing to the overall success of </span><span><span class="kobospan" id="kobo.246.1">ML endeavors.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.247.1">We will be transforming the DataFrame into a pandas DataFrame so that we can conduct insightful analyses, including constructing visualizations to extract meaningful patterns. </span><span class="kobospan" id="kobo.247.2">Our initial focus is on the </span><strong class="source-inline"><span class="kobospan" id="kobo.248.1">COUNT</span></strong><span class="kobospan" id="kobo.249.1"> column as the response variable. </span><span class="kobospan" id="kobo.249.2">Before model development, it is imperative to ascertain whether the </span><strong class="source-inline"><span class="kobospan" id="kobo.250.1">COUNT</span></strong><span class="kobospan" id="kobo.251.1"> column contains any </span><span><span class="kobospan" id="kobo.252.1">outlier values:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.253.1">
import seaborn as sns
import matplotlib.pyplot as plt
f, axes = plt.subplots(1, 2)
sns.boxplot(x=df_table.to_pandas()['COUNT'], ax=axes[0])
sns.boxplot(x=df_without_outlier.to_pandas()['COUNT'], ax=axes[1])
plt.show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.254.1">The</span><a id="_idIndexMarker315" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.255.1"> preceding code generates a plot using the </span><strong class="source-inline"><span class="kobospan" id="kobo.256.1">seaborn</span></strong><span class="kobospan" id="kobo.257.1"> and the </span><strong class="source-inline"><span class="kobospan" id="kobo.258.1">matplotlib</span></strong><span class="kobospan" id="kobo.259.1"> library to help us find </span><span><span class="kobospan" id="kobo.260.1">the outliers:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer124">
<span class="kobospan" id="kobo.261.1"><img alt="Figure 5.10 – Outlier plot" src="image/B19923_05_10.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.262.1">Figure 5.10 – Outlier plot</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.263.1">As we can see, the </span><strong class="source-inline"><span class="kobospan" id="kobo.264.1">COUNT</span></strong><span class="kobospan" id="kobo.265.1"> column exhibits outlier data points that can potentially negatively impact model performance if they’re not adequately addressed. </span><span class="kobospan" id="kobo.265.2">Mitigating outliers is a critical preprocessing step. </span><span class="kobospan" id="kobo.265.3">One widely adopted approach involves removing data points that lie beyond a predefined threshold or permissible range, as </span><span><span class="kobospan" id="kobo.266.1">outlined here:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.267.1">
from snowflake.snowpark.functions \
    import mean, stddev, abs, date_part
mean_value = df_table.select(mean("count")).collect()[0][0]
print(mean_value)
std_value = df_table.select(stddev("count")).collect()[0][0]
print(std_value)
df_without_outlier = df_table.filter(
    (abs(df_table["count"] - mean_value)) &gt;= (3 * std_value))
df_without_outlier.show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.268.1">The preceding </span><a id="_idIndexMarker316" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.269.1">code uses the Snowpark library to analyze a dataset stored in </span><strong class="source-inline"><span class="kobospan" id="kobo.270.1">df_table</span></strong><span class="kobospan" id="kobo.271.1">. </span><span class="kobospan" id="kobo.271.2">It calculates the mean (average) and standard deviation (a measure of data spread) of the </span><strong class="source-inline"><span class="kobospan" id="kobo.272.1">'count'</span></strong><span class="kobospan" id="kobo.273.1"> column in the dataset. </span><span class="kobospan" id="kobo.273.2">Then, it identifies and removes outliers from the dataset. </span><span class="kobospan" id="kobo.273.3">Outliers are data points that significantly differ from the average. </span><span class="kobospan" id="kobo.273.4">In this case, it defines outliers as data points more than three times the standard deviation away from the mean. </span><span class="kobospan" id="kobo.273.5">After identifying these outliers, it displays the dataset without the outlier values, using </span><strong class="source-inline"><span class="kobospan" id="kobo.274.1">df_without_outlier.show()</span></strong><span class="kobospan" id="kobo.275.1"> to help with </span><span><span class="kobospan" id="kobo.276.1">further analysis:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer125">
<span class="kobospan" id="kobo.277.1"><img alt="Figure 5.11 – Outliers removed" src="image/B19923_05_11.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.278.1">Figure 5.11 – Outliers removed</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.279.1">Now that we have taken care of the outliers, we can perform </span><span><span class="kobospan" id="kobo.280.1">correlation analysis.</span></span></p>
<h2 id="_idParaDest-90" class="calibre7"><a id="_idTextAnchor090" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.281.1">Correlation analysis</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.282.1">Identifying </span><a id="_idIndexMarker317" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.283.1">correlations among variables is of paramount importance for several vital reasons. </span><span class="kobospan" id="kobo.283.2">Correlations provide valuable insights into how different features in the dataset relate to each other. </span><span class="kobospan" id="kobo.283.3">By understanding these relationships, ML models can make more informed predictions as they leverage the strength and direction of correlations to uncover patterns and dependencies. </span><span class="kobospan" id="kobo.283.4">Moreover, identifying and quantifying correlations aids feature selection, where irrelevant or highly correlated features can be excluded to enhance model efficiency and interpretability. </span><span class="kobospan" id="kobo.283.5">It also helps identify potential multicollinearity issues, where two or more features are highly correlated, leading to unstable model coefficients. </span><span class="kobospan" id="kobo.283.6">Recognizing and harnessing correlations empowers ML models to make better predictions </span><a id="_idIndexMarker318" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.284.1">and yield more robust results, making it a fundamental aspect </span><span><span class="kobospan" id="kobo.285.1">of modeling.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.286.1">Since we’ve already transformed our Snowpark DataFrame into a pandas DataFrame, we can readily create a correlation matrix, a fundamental tool for exploring relationships between variables. </span><span class="kobospan" id="kobo.286.2">Only the snippet to generate a correlation matrix is demonstrated here but the complete code is available </span><span><span class="kobospan" id="kobo.287.1">in </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.288.1">Chapter_5.ipynb</span></strong></span><span><span class="kobospan" id="kobo.289.1">:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.290.1">
corr_matrix = df_without_outlier.to_pandas().corr()
plt.figure(figsize=(12, 6))
sns.heatmap(corr_matrix, cmap='coolwarm', annot=True)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.291.1">The preceding code generates the correlation matrix as a </span><span><span class="kobospan" id="kobo.292.1">heatmap visualization:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer126">
<span class="kobospan" id="kobo.293.1"><img alt="Figure 5.12 – Correlation matrix heatmap" src="image/B19923_05_12_V.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.294.1">Figure 5.12 – Correlation matrix heatmap</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.295.1">This heatmap visualization reveals a substantial correlation between the </span><strong class="source-inline"><span class="kobospan" id="kobo.296.1">TEMP</span></strong><span class="kobospan" id="kobo.297.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.298.1">ATEMP</span></strong><span class="kobospan" id="kobo.299.1"> variables, signifying a condition known as multicollinearity. </span><span class="kobospan" id="kobo.299.2">Multicollinearity occurs when </span><a id="_idIndexMarker319" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.300.1">two or more predictors in a model are highly correlated, distorting the model’s interpretability and stability. </span><span class="kobospan" id="kobo.300.2">To mitigate this issue and ensure the reliability of our analysis, we have opted to retain the </span><strong class="source-inline"><span class="kobospan" id="kobo.301.1">TEMP</span></strong><span class="kobospan" id="kobo.302.1"> variable while removing </span><strong class="source-inline"><span class="kobospan" id="kobo.303.1">ATEMP</span></strong><span class="kobospan" id="kobo.304.1"> from consideration in our subsequent modeling endeavors. </span><span class="kobospan" id="kobo.304.2">This strategic decision is made to maintain model robustness and effectively capture the essence of the data without the confounding effects </span><span><span class="kobospan" id="kobo.305.1">of multicollinearity.</span></span></p>
<h2 id="_idParaDest-91" class="calibre7"><a id="_idTextAnchor091" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.306.1">Leakage variables</span></h2>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.307.1">Leakage variables</span></strong><span class="kobospan" id="kobo.308.1"> in </span><a id="_idIndexMarker320" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.309.1">data science inadvertently</span><a id="_idIndexMarker321" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.310.1"> include information that would not be available during prediction or decision-making in a real-world scenario. </span><span class="kobospan" id="kobo.310.2">Eliminating them is crucial because using leakage variables can lead to overly optimistic model performance and unreliable results. </span><span class="kobospan" id="kobo.310.3">It’s essential to detect and exclude these variables during data preprocessing to ensure that our models make predictions based on the same information that would be accessible. </span><span class="kobospan" id="kobo.310.4">By doing so, we prevent the risk of building models that work well on historical data but fail to perform in real-world situations, which is a crucial goal in </span><span><span class="kobospan" id="kobo.311.1">data science.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.312.1">As mentioned previously, the </span><strong class="source-inline"><span class="kobospan" id="kobo.313.1">CASUAL</span></strong><span class="kobospan" id="kobo.314.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.315.1">REGISTERED</span></strong><span class="kobospan" id="kobo.316.1">, and </span><strong class="source-inline"><span class="kobospan" id="kobo.317.1">COUNT</span></strong><span class="kobospan" id="kobo.318.1"> columns exhibit high collinearity, with </span><strong class="source-inline"><span class="kobospan" id="kobo.319.1">COUNT</span></strong><span class="kobospan" id="kobo.320.1"> being an explicit summation of </span><strong class="source-inline"><span class="kobospan" id="kobo.321.1">CASUAL</span></strong><span class="kobospan" id="kobo.322.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.323.1">REGISTERED</span></strong><span class="kobospan" id="kobo.324.1">. </span><span class="kobospan" id="kobo.324.2">This redundancy renders the inclusion of all three variables undesirable, resulting in a leakage variable situation. </span><span class="kobospan" id="kobo.324.3">To preserve the integrity of our model-building process, we shall eliminate </span><strong class="source-inline"><span class="kobospan" id="kobo.325.1">CASUAL</span></strong><span class="kobospan" id="kobo.326.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.327.1">REGISTERED</span></strong><span class="kobospan" id="kobo.328.1"> from our feature set, thereby mitigating any potential confounding effects and ensuring the model’s ability to make predictions based on the most relevant and non-redundant information. </span><span class="kobospan" id="kobo.328.2">The next step is to perform feature engineering with the </span><span><span class="kobospan" id="kobo.329.1">prepared data.</span></span></p>
<h2 id="_idParaDest-92" class="calibre7"><a id="_idTextAnchor092" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.330.1">Feature engineering</span></h2>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.331.1">Feature engineering</span></strong><span class="kobospan" id="kobo.332.1"> in </span><a id="_idIndexMarker322" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.333.1">ML is like crafting the</span><a id="_idIndexMarker323" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.334.1"> perfect tool for a specific job. </span><span class="kobospan" id="kobo.334.2">It involves selecting, transforming, or creating new features (variables) from the available data to make it more suitable for ML algorithms. </span><span class="kobospan" id="kobo.334.3">This process is crucial because it helps the models better understand the patterns and relationships in the data, leading to improved predictions and insights. </span><span class="kobospan" id="kobo.334.4">By carefully engineering features, we can uncover hidden information, reduce noise, and enhance the model’s performance, making it a vital step in building effective and accurate </span><span><span class="kobospan" id="kobo.335.1">ML systems.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.336.1">Analyzing</span><a id="_idIndexMarker324" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.337.1"> the data shows that the </span><strong class="source-inline"><span class="kobospan" id="kobo.338.1">DATETIME</span></strong><span class="kobospan" id="kobo.339.1"> column is a promising candidate for feature engineering within this dataset. </span><span class="kobospan" id="kobo.339.2">Given the dependency of the predictive outcome on temporal factors such as the time of day and </span><a id="_idIndexMarker325" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.340.1">day of the week, deriving time-related features assumes paramount significance. </span><span class="kobospan" id="kobo.340.2">Extracting these temporal features is pivotal as it enhances the model’s performance and elevates the overall predictive accuracy by capturing essential nuances about the dataset’s </span><span><span class="kobospan" id="kobo.341.1">material characteristics:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.342.1">
from snowflake.snowpark.functions import hour, month,to_date,dayofweek
df_table = df_table.with_column("hour", hour("DATETIME"))
df_table = df_table.with_column("month", month("DATETIME"))
df_table = df_table.with_column("date", to_date("DATETIME"))
df_table = df_table.with_column("weekday", dayofweek("DATETIME"))
df_table.show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.343.1">The preceding code enriches a DataFrame by creating new columns that capture specific time and date details from the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.344.1">DATETIME</span></strong></span><span><span class="kobospan" id="kobo.345.1"> column:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer127">
<span class="kobospan" id="kobo.346.1"><img alt="Figure 5.13 – DATETIME data" src="image/B19923_05_13.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.347.1">Figure 5.13 – DATETIME data</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.348.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.349.1">hour</span></strong><span class="kobospan" id="kobo.350.1"> column tells us the hour of the day, the </span><strong class="source-inline"><span class="kobospan" id="kobo.351.1">month</span></strong><span class="kobospan" id="kobo.352.1"> column identifies the month, the </span><strong class="source-inline"><span class="kobospan" id="kobo.353.1">date</span></strong><span class="kobospan" id="kobo.354.1"> column extracts the date itself, and the </span><strong class="source-inline"><span class="kobospan" id="kobo.355.1">weekday</span></strong><span class="kobospan" id="kobo.356.1"> column signifies the day of the week. </span><span class="kobospan" id="kobo.356.2">These additional columns provide a more comprehensive view of the time-related information within the dataset, enhancing its potential for in-depth analysis and </span><span><span class="kobospan" id="kobo.357.1">ML applications.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.358.1">This step concludes our data preparation and exploration journey. </span><span class="kobospan" id="kobo.358.2">The following section will use this</span><a id="_idIndexMarker326" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.359.1"> prepared data to build and train our </span><a id="_idIndexMarker327" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.360.1">model </span><span><span class="kobospan" id="kobo.361.1">using Snowpark.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.362.1">A note on the model-building process</span></p>
<p class="callout"><span class="kobospan" id="kobo.363.1">In our model-building process, we won’t be incorporating all the steps we’ve discussed thus far. </span><span class="kobospan" id="kobo.363.2">Instead, we’ll focus on two significant transformations to showcase Snowpark ML pipelines. </span><span class="kobospan" id="kobo.363.3">Additionally, the accompanying notebook (</span><strong class="source-inline1"><span class="kobospan" id="kobo.364.1">chapter_5.ipynb</span></strong><span class="kobospan" id="kobo.365.1">) illustrates model building using Python’s scikit-learn library and how to call them as stored procedures. </span><span class="kobospan" id="kobo.365.2">This will allow you to compare and contrast how the model-building process is simplified through Snowpark ML. </span><span class="kobospan" id="kobo.365.3">To follow through the chapter, you can skip the model building process using the scikit-learn section and directly go to the Snowpark ML section in </span><span><span class="kobospan" id="kobo.366.1">the notebook.</span></span></p>
<h1 id="_idParaDest-93" class="calibre5"><a id="_idTextAnchor093" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.367.1">Training ML models in Snowpark</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.368.1">Now </span><a id="_idIndexMarker328" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.369.1">that we have prepared our dataset, the pinnacle of our journey involves the model-building process, for which we will be leveraging the power of Snowpark ML. </span><span class="kobospan" id="kobo.369.2">Snowpark ML emerges as a recent addition to the Snowpark arsenal, strategically deployed to streamline the intricacies of the model-building process. </span><span class="kobospan" id="kobo.369.3">Its elegance becomes apparent when we engage in a comparative exploration of the model-building procedure through the novel ML library. </span><span class="kobospan" id="kobo.369.4">We will start by developing the pipeline that we’ll use to train the model using the data we </span><span><span class="kobospan" id="kobo.370.1">prepared previously:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.371.1">
import snowflake.ml.modeling.preprocessing as snowml
from snowflake.ml.modeling.pipeline import Pipeline
import joblib
df = session.table("BSD_TRAINING")
df = df.drop("DATETIME","DATE")
CATEGORICAL_COLUMNS = ["SEASON","WEATHER"]
CATEGORICAL_COLUMNS_OHE = ["SEASON_OE","WEATHER_OE"]
MIN_MAX_COLUMNS = ["TEMP"]
import numpy as np
categories = {
    "SEASON": np.array([1,2,3,4]),
    "WEATHER": np.array([1,2,3,4]),
}
preprocessing_pipeline = Pipeline(
    steps=[
        (
            "OE",
            snowml.OrdinalEncoder(
                input_cols=CATEGORICAL_COLUMNS,
                output_cols=CATEGORICAL_COLUMNS_OHE,
                categories=categories
            )
        ),
        (
            "MMS",
            snowml.MinMaxScaler(
                clip=True,
                input_cols=MIN_MAX_COLUMNS,
                output_cols=MIN_MAX_COLUMNS,
            )
        )
    ]
)
PIPELINE_FILE = 'preprocessing_pipeline.joblib'
joblib.dump(preprocessing_pipeline, PIPELINE_FILE)
transformed_df = preprocessing_pipeline.fit(df).transform(df)
transformed_df.show()
session.file.put(PIPELINE_FILE,"@snowpark_test_stage",overwrite=True)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.372.1">The </span><a id="_idIndexMarker329" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.373.1">preceding code creates a preprocessing pipeline for the dataset by using various Snowpark ML functions. </span><span class="kobospan" id="kobo.373.2">The </span><strong class="source-inline"><span class="kobospan" id="kobo.374.1">preprocessing</span></strong><span class="kobospan" id="kobo.375.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.376.1">pipeline</span></strong><span class="kobospan" id="kobo.377.1"> modules are imported as these are essential for developing and training </span><span><span class="kobospan" id="kobo.378.1">the model:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer128">
<span class="kobospan" id="kobo.379.1"><img alt="Figure 5.14 – Transformed data" src="image/B19923_05_14.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.380.1">Figure 5.14 – Transformed data</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.381.1">The pipeline includes ordinal encoding for categorical columns (</span><strong class="source-inline"><span class="kobospan" id="kobo.382.1">SEASON</span></strong><span class="kobospan" id="kobo.383.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.384.1">WEATHER</span></strong><span class="kobospan" id="kobo.385.1">) and min-max scaling for numerical columns (</span><strong class="source-inline"><span class="kobospan" id="kobo.386.1">TEMP</span></strong><span class="kobospan" id="kobo.387.1">). </span><span class="kobospan" id="kobo.387.2">The pipeline is saved into the stage using the </span><strong class="source-inline"><span class="kobospan" id="kobo.388.1">joblib</span></strong><span class="kobospan" id="kobo.389.1"> library, which can be utilized for consistent preprocessing in future analyses. </span><span class="kobospan" id="kobo.389.2">Now that we have the pipeline code ready, we will build the features that are required for </span><span><span class="kobospan" id="kobo.390.1">the model:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.391.1">
CATEGORICAL_COLUMNS = ["SEASON","WEATHER"]
CATEGORICAL_COLUMNS_OHE = ["SEASON_OE","WEATHER_OE"]
MIN_MAX_COLUMNS = ["TEMP","ATEMP"]
FEATURE_LIST = \
    ["HOLIDAY","WORKINGDAY","HUMIDITY","TEMP","ATEMP","WINDSPEED"]
LABEL_COLUMNS = ['COUNT']
OUTPUT_COLUMNS = ['PREDICTED_COUNT']
PIPELINE_FILE = 'preprocessing_pipeline.joblib'
preprocessing_pipeline = joblib.load(PIPELINE_FILE)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.392.1">The</span><a id="_idIndexMarker330" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.393.1"> preceding code defines lists representing categorical columns, one-hot encoded categorical columns, and columns for min-max scaling. </span><span class="kobospan" id="kobo.393.2">It also specifies a feature list, label columns, and output columns for an ML model. </span><span class="kobospan" id="kobo.393.3">The </span><strong class="source-inline"><span class="kobospan" id="kobo.394.1">preprocessing_pipeline.joblib</span></strong><span class="kobospan" id="kobo.395.1"> file is loaded and assumed to contain a previously saved preprocessing pipeline. </span><span class="kobospan" id="kobo.395.2">These elements collectively prepare the necessary data and configurations for subsequent ML tasks, ensuring consistent handling of categorical variables, feature scaling, and model predictions based on the pre-established pipeline. </span><span class="kobospan" id="kobo.395.3">We will now split the data into training and </span><span><span class="kobospan" id="kobo.396.1">testing sets:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.397.1">
bsd_train_df, bsd_test_df = df.random_split(
    weights=[0.7,0.3], seed=0)
train_df = preprocessing_pipeline.fit(
    bsd_train_df).transform(bsd_train_df)
test_df = preprocessing_pipeline.transform(bsd_test_df)
train_df.show()
test_df.show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.398.1">The preceding code divides the dataset into training (70%) and testing (30%) sets using a random split. </span><span class="kobospan" id="kobo.398.2">It applies the previously defined preprocessing pipeline to transform both sets, displaying the transformed training and testing datasets and ensuring consistent preprocessing for model training and evaluation. </span><span class="kobospan" id="kobo.398.3">The output shows the different training and </span><span><span class="kobospan" id="kobo.399.1">testing data:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer129">
<span class="kobospan" id="kobo.400.1"><img alt="Figure 5.15 – Training and testing dataset" src="image/B19923_05_15.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.401.1">Figure 5.15 – Training and testing dataset</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.402.1">Next, we’ll train </span><a id="_idIndexMarker331" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.403.1">the model with the </span><span><span class="kobospan" id="kobo.404.1">training data:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.405.1">
from snowflake.ml.modeling.linear_model import LinearRegression
regressor = LinearRegression(
    input_cols=CATEGORICAL_COLUMNS_OHE+FEATURE_LIST,
    label_cols=LABEL_COLUMNS,
    output_cols=OUTPUT_COLUMNS
)
# Train
regressor.fit(train_df)
# Predict
result = regressor.predict(test_df)
result.show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.406.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.407.1">LinearRegression</span></strong><span class="kobospan" id="kobo.408.1"> class defines the model, specifying the input columns (categorical columns after one-hot encoding and additional features), label columns (the target variable – that is, </span><strong class="source-inline"><span class="kobospan" id="kobo.409.1">COUNT</span></strong><span class="kobospan" id="kobo.410.1">), and output columns for predictions. </span><span class="kobospan" id="kobo.410.2">The model is trained on the transformed training dataset using </span><strong class="source-inline"><span class="kobospan" id="kobo.411.1">fit</span></strong><span class="kobospan" id="kobo.412.1">, and then predictions are generated for the transformed testing dataset using </span><strong class="source-inline"><span class="kobospan" id="kobo.413.1">predict</span></strong><span class="kobospan" id="kobo.414.1">. </span><span class="kobospan" id="kobo.414.2">The resulting predictions are displayed, assessing the model’s performance on the </span><span><span class="kobospan" id="kobo.415.1">test data:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer130">
<span class="kobospan" id="kobo.416.1"><img alt="Figure 5.16 – Predicted output" src="image/B19923_05_16.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.417.1">Figure 5.16 – Predicted output</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.418.1">The next step is</span><a id="_idIndexMarker332" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.419.1"> to calculate various performance metrics to evaluate the accuracy of the linear regression </span><span><span class="kobospan" id="kobo.420.1">model’s predictions:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.421.1">
from snowflake.ml.modeling.metrics import mean_squared_error, explained_variance_score, mean_absolute_error, mean_absolute_percentage_error, d2_absolute_error_score, d2_pinball_score
mse = mean_squared_error(df=result,
    y_true_col_names="COUNT",
    y_pred_col_names="PREDICTED_COUNT")
evs = explained_variance_score(df=result,
    y_true_col_names="COUNT",
    y_pred_col_names="PREDICTED_COUNT")
mae = mean_absolute_error(df=result,
    y_true_col_names="COUNT",
    y_pred_col_names="PREDICTED_COUNT")
mape = mean_absolute_percentage_error(df=result,
    y_true_col_names="COUNT",
    y_pred_col_names="PREDICTED_COUNT")
d2aes = d2_absolute_error_score(df=result,
    y_true_col_names="COUNT",
    y_pred_col_names="PREDICTED_COUNT")
d2ps = d2_pinball_score(df=result,
    y_true_col_names="COUNT",
    y_pred_col_names="PREDICTED_COUNT")
print(f"Mean squared error: {mse}")
print(f"explained_variance_score: {evs}")
print(f"mean_absolute_error: {mae}")
print(f"mean_absolute_percentage_error: {mape}")
print(f"d2_absolute_error_score: {d2aes}")
print(f"d2_pinball_score: {d2ps}")</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.422.1">The preceding</span><a id="_idIndexMarker333" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.423.1"> code calculates various</span><a id="_idTextAnchor094" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.424.1"> performance metrics to assess the accuracy of the linear regression model’s predictions. </span><span class="kobospan" id="kobo.424.2">Metrics such as mean squared error, explained variance score, mean absolute error, mean fundamental percentage error, d2 definitive error score, and d2 pinball score are computed based on the actual (</span><strong class="source-inline"><span class="kobospan" id="kobo.425.1">COUNT</span></strong><span class="kobospan" id="kobo.426.1">) and predicted (</span><strong class="source-inline"><span class="kobospan" id="kobo.427.1">PREDICTED_COUNT</span></strong><span class="kobospan" id="kobo.428.1">) values stored in the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.429.1">result</span></strong></span><span><span class="kobospan" id="kobo.430.1"> DataFrame:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer131">
<span class="kobospan" id="kobo.431.1"><img alt="Figure 5.17 – Performance metrics" src="image/B19923_05_17.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.432.1">Figure 5.17 – Performance metrics</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.433.1">These performance metrics provide a comprehensive evaluation of the model’s performance across different aspects of </span><span><span class="kobospan" id="kobo.434.1">prediction accuracy.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.435.1">Model results and efficiency</span></p>
<p class="callout"><span class="kobospan" id="kobo.436.1">The presented model metrics might need to showcase more exceptional results. </span><span class="kobospan" id="kobo.436.2">It’s crucial to emphasize that the primary objective of this case study is to elucidate the model-building process and highlight the facilitative role of Snowpark ML. </span><span class="kobospan" id="kobo.436.3">The focus of this chapter has been on illustrating the construction of a linear </span><span><span class="kobospan" id="kobo.437.1">regression model.</span></span></p>
<h2 id="_idParaDest-94" class="calibre7"><a id="_idTextAnchor095" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.438.1">The efficiency of Snowpark ML</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.439.1">In delving</span><a id="_idIndexMarker334" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.440.1"> into the intricacies of the model-building process facilitated by Snowpark ML, the initial standout feature is its well-thought-out design. </span><span class="kobospan" id="kobo.440.2">A notable departure from the conventional approach is evident as Snowpark ML closely mirrors the streamlined methodology found in scikit-learn. </span><span class="kobospan" id="kobo.440.3">A significant advantage is eliminating the need to</span><a id="_idIndexMarker335" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.441.1"> create separate </span><strong class="bold"><span class="kobospan" id="kobo.442.1">user-defined functions</span></strong><span class="kobospan" id="kobo.443.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.444.1">UDFs</span></strong><span class="kobospan" id="kobo.445.1">) and stored procedures, streamlining the entire </span><span><span class="kobospan" id="kobo.446.1">model-building workflow.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.447.1">It’s crucial to recognize that Snowpark ML seamlessly integrates with scikit-learn while adhering to similar conventions in the model construction process. </span><span class="kobospan" id="kobo.447.2">A noteworthy distinction is a prerequisite in scikit-learn for data to be passed as a pandas DataFrame. </span><span class="kobospan" id="kobo.447.3">Consequently, the Snowflake table must be converted into a pandas DataFrame before you can initiate the model-building phase. </span><span class="kobospan" id="kobo.447.4">However, it’s imperative to be mindful of potential memory constraints, especially when dealing with substantial datasets. </span><span class="kobospan" id="kobo.447.5">Converting a large table into a pandas DataFrame demands a significant amount of memory since the entire dataset is loaded </span><span><span class="kobospan" id="kobo.448.1">into memory.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.449.1">In contrast, Snowpark ML provides a more native and memory-efficient approach to the model-building process. </span><span class="kobospan" id="kobo.449.2">This native integration with Snowflake’s environment not only enhances the efficiency of the workflow but also mitigates memory-related challenges associated with large datasets. </span><span class="kobospan" id="kobo.449.3">The utilization of Snowpark ML emerges as a strategic and seamless choice for executing complex model-building tasks within the </span><span><span class="kobospan" id="kobo.450.1">Snowflake ecosystem.</span></span></p>
<h1 id="_idParaDest-95" class="calibre5"><a id="_idTextAnchor096" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.451.1">Summary</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.452.1">Snowpark ML emerges as a versatile and powerful tool for data scientists, enabling them to tackle complex data science tasks within Snowflake’s unified data platform. </span><span class="kobospan" id="kobo.452.2">Its integration with popular programming languages, scalability, and real-time processing capabilities make it invaluable for various applications, from predictive modeling to real-time analytics and advanced AI tasks. </span><span class="kobospan" id="kobo.452.3">With Snowpark ML, organizations can harness the full potential of their data, drive innovation, and gain a competitive edge in today’s </span><span><span class="kobospan" id="kobo.453.1">data-driven landscape.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.454.1">In the next chapter, we will continue by deploying the model in Snowflake and </span><span><span class="kobospan" id="kobo.455.1">operationalizing it.</span></span></p>
</div>
</body></html>