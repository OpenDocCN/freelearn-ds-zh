- en: Chapter 9. Building an E-mail Classification System Using Apache Mahout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will create a classifier system using Mahout. In order
    to build this system, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparation of the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will target the creation of two different classifiers. The
    first one will be an easy one because you can both create and test it on a pseudo-distributed
    Hadoop installation. For the second classifier, I will provide you with all the
    details, so you can run it using your fully distributed Hadoop installation. I
    will count the second one as a hands-on exercise for the readers of this book.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, let's understand the problem statement for the first use case.
    Nowadays, in most of the e-mail systems, we see that e-mails are classified as
    spam or not spam. E-mails that are not spam are delivered directly into our inbox
    but spam e-mails are stored in a folder called `Spam`. Usually, based on a certain
    pattern such as message subject, sender's e-mail address, or certain keywords
    in the message body, we categorize an incoming e-mail as spam. We will create
    a classifier using Mahout, which will classify an e-mail into spam or not spam.
    We will use SpamAssassin, an Apache open source project dataset for this task.
  prefs: []
  type: TYPE_NORMAL
- en: For the second use case, we will create a classifier, which can predict a group
    of incoming e-mails. As an open source project, there are lots of projects under
    the Apache software foundation, such as Apache Mahout, Apache Hadoop, Apache Solr,
    and so on. We will take the **Apache** **Software Foundation** (**ASF**) e-mail
    dataset and using this, we will create and train our model so that our model can
    predict a new incoming e-mail. So, based on certain features, we will be able
    to predict which group a new incoming e-mail belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: In Mahout's classification problem, we will have to identify a pattern in the
    dataset to help us predict the group of a new e-mail. We already have a dataset,
    which is separated by project names. We will use the ASF public e-mail archives
    dataset for this use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s consider our first use case: spam e-mail detection classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: Spam e-mail dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As I mentioned, we will be using the Apache SpamAssassin projects dataset.
    Apache SpamAssassin is an open source spam filter. Download `20021010_easy_ham.tar`
    and `20021010_spam.tar` from [http://spamassassin.apache.org/publiccorpus/](http://spamassassin.apache.org/publiccorpus/),
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Spam e-mail dataset](img/4959OS_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Creating the model using the Assassin dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can create the model with the help of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a folder under `tmp` with the name `dataset`, and then click on the
    folder and unzip the datasets using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will create two folders under the `dataset` folder, `easy _ham` and `spam`,
    as shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Creating the model using the Assassin dataset](img/4959OS_09_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Create a folder in `Hdfs` and move this dataset into Hadoop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now our data preparation is done. We have downloaded the data and moved this
    data into `hdfs`. Let's move on to the next step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Convert this data into sequence files so that we can process it using Hadoop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Creating the model using the Assassin dataset](img/4959OS_09_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Convert the `sequence` file into sparse vector (Mahout algorithms accept input
    in vector format, which is why we are converting the `sequence` file into sparse
    vector) by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Creating the model using the Assassin dataset](img/4959OS_09_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'The command in the preceding screenshot is explained as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`lnorm`: This command is used for output vector to be log normalized.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nv`: This command is used for named vector.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wt`: This command is used to identify the kind of weight to use. Here we use
    `tf-idf`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Split the set of vectors for training and testing the model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding command can be explained as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `randomSelectionPct` parameter divides the percentage of data into test
    and training datasets. In this case, it's 80 percent for test and 20 percent for
    training.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The `xm` parameter specifies what portion of the `tf (tf-idf)` vectors is to
    be used expressed in times the standard deviation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The sigma symbol specifies the document frequencies of these vectors. It can
    be used to remove really high frequency terms. It is expressed as a double value.
    A good value to be specified is 3.0\. If the value is less than `0`, no vectors
    will be filtered out.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Creating the model using the Assassin dataset](img/4959OS_09_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Now, train the model using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Creating the model using the Assassin dataset](img/4959OS_09_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Now, test the model using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Creating the model using the Assassin dataset](img/4959OS_09_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: You can see from the results that the output is displayed on the console. As
    per the matrix, the system has correctly classified 99.53 percent of the instances
    given.
  prefs: []
  type: TYPE_NORMAL
- en: We can use this created model to classify new documents. To do this, we can
    either use a Java program or create a servlet that can be deployed on our server.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an example of a Java program in continuation of this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Program to use a classifier model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will create a Java program that will use our model to classify new e-mails.
    This program will take model, labelindex, dictionary-file, document frequency,
    and text file as input and will generate a score for the categories. The category
    will be decided based on the higher scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at this program step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `.jar` files required to make a compilation of this program are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Hadoop-core-x.y.x.jar`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Mahout-core-xyz.jar`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Mahout-integration-xyz.jar`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Mahout-math-xyz.jar`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The `import` statements are listed as follows. We are discussing this because
    there are lots of changes in the Mahout releases and people usually find it difficult
    to get the correct classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import java.io.BufferedReader;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import java.io.FileReader;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import java.io.StringReader;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import java.util.HashMap;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import java.util.Map;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import org.apache.hadoop.conf.Configuration;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import org.apache.hadoop.fs.Path;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import org.apache.lucene.analysis.Analyzer;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import org.apache.lucene.analysis.TokenStream;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import org.apache.lucene.analysis.standard.StandardAnalyzer;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import org.apache.lucene.util.Version;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import org.apache.mahout.classifier.naivebayes.BayesUtils;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import org.apache.mahout.classifier.naivebayes.NaiveBayesModel;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import org.apache.mahout.classifier.naivebayes.StandardNaiveBayesClassifier;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import org.apache.mahout.common.Pair;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import org.apache.mahout.math.RandomAccessSparseVector;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import org.apache.mahout.math.Vector;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import org.apache.mahout.math.Vector.Element;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import org.apache.mahout.vectorizer.TFIDF;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import org.apache.hadoop.io.*;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import com.google.common.collect.ConcurrentHashMultiset;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import com.google.common.collect.Multiset;`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The supporting methods to read the dictionary are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The supporting methods to read the document frequency are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first part of the `main` method is used to perform the following actions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting the input
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the model
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Initializing `StandardNaiveBayesClassifier` using our created model
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading `labelindex`, document frequency, and dictionary created while creating
    the vector from the dataset
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code can be used for the preceding actions:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'The second part of the `main` method is used to extract words from the e-mail:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The third part of the `main` method is used to create vector of the `id` word
    and the `tf-idf` weights:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the fourth part of the `main` method, with `classifier`, we get the score
    for each label and assign the e-mail to the higher scored label:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, put all these codes under one class and create the `.jar` file of this
    class. We will use this `.jar` file to test our new e-mails.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the program
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To test the program, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a folder named `assassinmodeltest` in the local directory, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To use this model, get the following files from `hdfs` to `/tmp/assassinmodeltest`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the earlier created model, use the following command:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'For `labelindex`, use the following command:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'For `df-counts` from the `assassinvec` folder (change the name of the `part-00000`
    file to `df-count`), use the following commands:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Under `/tmp/assassinmodeltest`, create a file with the message shown in the
    following screenshot:![Testing the program](img/4959OS_09_08.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, run the program using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Testing the program](img/4959OS_09_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Now, update the `test` e-mail file with the message shown in the following screenshot:![Testing
    the program](img/4959OS_09_10.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the program again using the same command as given in step 4 and view the
    result as follows:![Testing the program](img/4959OS_09_11.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we have a program ready that can use our classifier model and predict the
    unknown items. Let's move on to our second use case.
  prefs: []
  type: TYPE_NORMAL
- en: Second use case as an exercise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed at the start of this chapter, we will now work on a second use
    case, where we will predict the category of a new e-mail.
  prefs: []
  type: TYPE_NORMAL
- en: The ASF e-mail dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Apache Software Foundation e-mail dataset is partitioned by project. This
    e-mail dataset can be found at [http://aws.amazon.com/datasets/7791434387204566](http://aws.amazon.com/datasets/7791434387204566).
  prefs: []
  type: TYPE_NORMAL
- en: '![The ASF e-mail dataset](img/4959OS_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A smaller dataset can be found at [http://files.grantingersoll.com/ibm.tar.gz](http://files.grantingersoll.com/ibm.tar.gz).
    (Refer to [http://lucidworks.com/blog/scaling-mahout/](http://lucidworks.com/blog/scaling-mahout/)).
    Use this data to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Move this data to the folder of your choice (`/tmp/asfmail`) and unzip the
    folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Move the dataset to `hdfs`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the `mbox` files into Hadoop''s `SequenceFile` format using Mahout''s
    `SequenceFilesFromMailArchives` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![The ASF e-mail dataset](img/4959OS_09_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Convert the `sequence` file into sparse vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![The ASF e-mail dataset](img/4959OS_09_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Modify the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, the next three steps are similar to the ones we performed earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Split the dataset into `training` and `test` datasets using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model using the `training` dataset as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Test the model using the `test` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you may have noticed, all the steps are exactly identical to the ones we
    performed earlier. Hereby, I leave this topic as an exercise for you to create
    your own classifier system using this model. You can use hints as provided for
    the spam filter classifier. We now move our discussion to tuning our classifier.
    Let's take a brief overview of the best practices in this area.
  prefs: []
  type: TYPE_NORMAL
- en: Classifiers tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We already discussed classifiers' evaluation techniques in [Chapter 1](ch01.html
    "Chapter 1. Classification in Data Analysis"), *Classification in Data Analysis*.
    Just as a reminder, we evaluate our model using techniques such as confusion matrix,
    entropy matrix, area under curve, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the explanatory variables, we create the feature vector. To check how
    a particular model is working, these feature vectors need to be investigated.
    In Mahout, there is a class available for this, `ModelDissector`. It takes the
    following three inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Features**: This class takes a feature vector to use (destructively)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TraceDictionary**: This class takes a trace dictionary containing variables
    and the locations in the feature vector that are affected by them'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learner**: This class takes the model that we are probing to find weights
    on features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ModelDissector` tweaks the feature vector and observes how the model output
    changes. By taking an average of the number of examples, we can determine the
    effect of different explanatory variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '`ModelDissector` has a summary method, which returns the most important features
    with their weights, most important category, and the top few categories that they
    affect.'
  prefs: []
  type: TYPE_NORMAL
- en: The output of `ModelDissector` is helpful in troubleshooting problems in a wrongly
    created model.
  prefs: []
  type: TYPE_NORMAL
- en: More details for the code can be found at [https://github.com/apache/mahout/blob/master/mrlegacy/src/main/java/org/apache/mahout/classifier/sgd/ModelDissector.java](https://github.com/apache/mahout/blob/master/mrlegacy/src/main/java/org/apache/mahout/classifier/sgd/ModelDissector.java).
  prefs: []
  type: TYPE_NORMAL
- en: 'While improving the output of the classifier, one should take care with two
    commonly occurring problems: target leak, and broken feature extraction.'
  prefs: []
  type: TYPE_NORMAL
- en: If the model is showing results that are too good to be true or an output beyond
    expectations, we could have a problem with target leak. This error comes once
    information from the target variable is included in the explanatory variables,
    which are used to train the classifier. In this instance, the classifier will
    work too well for the `test` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, broken feature extraction occurs when feature extraction
    is broken. This type of classifier shows the opposite result from the target leak
    classifiers. Here, the model provides results poorer than expected.
  prefs: []
  type: TYPE_NORMAL
- en: To tune the classifier, we can use new explanatory variables, transformations
    of explanatory variables, and can also eliminate some of the variables. We should
    also try different learning algorithms to create the model and choose an algorithm,
    which is good in performance, training time, and speed.
  prefs: []
  type: TYPE_NORMAL
- en: More details on tuning can be found in Chapter 16, *Deploying a classifier*
    in the book *Mahout in Action*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed creating our own production ready classifier model.
    We took up two use cases here, one for an e-mail spam filter and the other for
    classifying the e-mail as per the projects. We used datasets for Apache SpamAssassin
    for the e-mail filter and ASF for the e-mail classifier.
  prefs: []
  type: TYPE_NORMAL
- en: We also saw how to increase the performance of your model.
  prefs: []
  type: TYPE_NORMAL
- en: So you are now ready to implement classifiers using Apache Mahout for your own
    real world use cases. Happy learning!
  prefs: []
  type: TYPE_NORMAL
