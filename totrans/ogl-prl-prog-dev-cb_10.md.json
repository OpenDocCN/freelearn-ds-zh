["```py\n#define N // integers to be sorted with values from 0 – 256\nvoid MSD(char[] s) {\n  msd_sort(s, 0, len(s), 0);\n}\nvoid msd_sort(char[][] s, int lhs, int rhs, int d) {\n  if (rhs <= lhs + 1) return;\n  int* count = (int*)malloc(257 *sizeof(int));\n  for(int i = 0; i < N; ++i)\n    count[s[i][d]+1]++;\n  for(int k = 1; k < 256; ++k) \n    count[k] += count[k-1];\n  for(int j = 0; j < N; ++j) \n    temp[count[s[i][d]]++] = a[i];\n  for(int i = 0; i < N; ++i) \n    s[i] = temp[i];\n  for(int i = 0; i<255;++i)\n    msd_sort(s, 1 + count[i], 1 + count[i+1], d+1);\n}\n```", "```py\nvoid lsd_sort(char[][] a) {\n  int N = len(a);\n  int W = len(a[0]);\n  for(int d = W – 1; d >= 0; d--) {\n    int[] count = (int*) malloc(sizeof(int) * 256);\n    for(int i = 0; i< N; ++i) \n      count[a[i][d]+1]++;\n    for(int k = 1; k < 256; k++)\n      count[k] += count[k-1];\n    for(int i = 0; i< N; ++i) \n      temp[count[a[i][d]]++] = a[i];\n    for(int i= 0; i< N; ++i)\n      a[i] = temp[i];\n  }\n}\n```", "```py\ntemplate<typename T>\nT reduce(T (*f)(T, T),\n         size_t n,\n         T a[],\n         T identity) {\n  T accumulator = identity;\n  for(size_t i = 0; i < n ; ++i)\n        accumulator = f(accumulator, a[i]);\n  return accumulator;\n}\n```", "```py\n__kernel void reduce0(__global uint* input, \n                      __global uint* output, \n                      __local uint* sdata) {\n    unsigned int tid = get_local_id(0);\n    unsigned int bid = get_group_id(0);\n    unsigned int gid = get_global_id(0);\n    unsigned int blockSize = get_local_size(0);\n\n    sdata[tid] = input[gid];\n\n    barrier(CLK_LOCAL_MEM_FENCE);\n    for(unsigned int s = 1; s < BLOCK_SIZE; s <<= 1) {\n        // This has a slight problem, the %-operator is rather slow\n        // and causes divergence within the wavefront as not all threads\n        // within the wavefront is executing.\n        if(tid % (2*s) == 0)\n        {\n            sdata[tid] += sdata[tid + s];\n        }\n        barrier(CLK_LOCAL_MEM_FENCE);\n    }\n\n    // write result for this block to global mem\n    if(tid == 0) output[bid] = sdata[0];\n}\n```", "```py\nif(tid % (2*s) == 0)\n{\n    sdata[tid] += sdata[tid + s];\n}\n```", "```py\n__kernel void reduce1(__global uint* input, \n                      __global uint* output, \n                      __local uint* sdata) {\n    unsigned int tid = get_local_id(0);\n    unsigned int bid = get_group_id(0);\n    unsigned int gid = get_global_id(0);\n    unsigned int blockSize = get_local_size(0);\n\n    sdata[tid] = input[gid];\n\n    barrier(CLK_LOCAL_MEM_FENCE);\n    for(unsigned int s = 1; s < BLOCK_SIZE; s <<= 1) {\n        int index = 2 * s * tid;\n        if(index < BLOCK_SIZE)\n        {\n            sdata[index] += sdata[index + s];\n        }\n        barrier(CLK_LOCAL_MEM_FENCE);\n    }\n\n    // write result for this block to global mem\n    if(tid == 0) output[bid] = sdata[0];\n}\n```", "```py\n__kernel void reduce2(__global uint* input, \n                      __global uint* output, \n                      __local uint* sdata) {\n    unsigned int tid = get_local_id(0);\n    unsigned int bid = get_group_id(0);\n    unsigned int gid = get_global_id(0);\n    unsigned int blockSize = get_local_size(0);\n\n    sdata[tid] = input[gid];\n\n    barrier(CLK_LOCAL_MEM_FENCE);\n    for(unsigned int s = BLOCK_SIZE/2; s > 0 ; s >>= 1) {\n        // Notice that half of threads are already idle on first iteration\n        // and with each iteration, its halved again. Work efficiency isn't very good\n        // now\n        if(tid < s)\n        {\n            sdata[tid] += sdata[tid + s];\n        }\n        barrier(CLK_LOCAL_MEM_FENCE);\n    }\n\n    // write result for this block to global mem\n    if(tid == 0) output[bid] = sdata[0];\n}\n```", "```py\n__kernel void reduce3(__global uint* input, \n                      __global uint* output, \n                      __local uint* sdata) {\n    unsigned int tid = get_local_id(0);\n    unsigned int bid = get_group_id(0);\n    unsigned int gid = get_global_id(0);\n\n    // To mitigate the problem of idling threads in 'reduce2' kernel,\n    // we can halve the number of blocks while each work-item loads\n    // two elements instead of one into shared memory\n    unsigned int index = bid*(BLOCK_SIZE*2) + tid;\n    sdata[tid] = input[index] + input[index+BLOCK_SIZE];\n\n    barrier(CLK_LOCAL_MEM_FENCE);\n    for(unsigned int s = BLOCK_SIZE/2; s > 0 ; s >>= 1) {\n        // Notice that half of threads are already idle on first iteration\n        // and with each iteration, its halved again. Work efficiency isn't very good\n        // now\n        if(tid < s)\n        {\n            sdata[tid] += sdata[tid + s];\n        }\n        barrier(CLK_LOCAL_MEM_FENCE);\n    }\n\n    // write result for this block to global mem\n    if(tid == 0) output[bid] = sdata[0];\n}\n```", "```py\n__kernel void reduce4(__global uint* input, \n                      __global uint* output, \n                      __local uint* sdata) {\n    unsigned int tid = get_local_id(0);\n    unsigned int bid = get_group_id(0);\n    unsigned int gid = get_global_id(0);\n    unsigned int blockSize = get_local_size(0);\n\n    unsigned int index = bid*(BLOCK_SIZE*2) + tid;\n    sdata[tid] = input[index] + input[index+BLOCK_SIZE];\n\n    barrier(CLK_LOCAL_MEM_FENCE);\n    for(unsigned int s = BLOCK_SIZE/2; s > 64 ; s >>= 1) {\n        // Unrolling the last wavefront and we cut 7 iterations of this\n        // for-loop while we practice wavefront-programming\n        if(tid < s)\n        {\n            sdata[tid] += sdata[tid + s];\n        }\n        barrier(CLK_LOCAL_MEM_FENCE);\n    }\n\n    if (tid < 64) {\n        if (blockSize >= 128) sdata[tid] += sdata[tid + 64];\n        if (blockSize >=  64) sdata[tid] += sdata[tid + 32];\n        if (blockSize >=  32) sdata[tid] += sdata[tid + 16];\n        if (blockSize >=  16) sdata[tid] += sdata[tid +  8];\n        if (blockSize >=   8) sdata[tid] += sdata[tid +  4];\n        if (blockSize >=   4) sdata[tid] += sdata[tid +  2];\n        if (blockSize >=   2) sdata[tid] += sdata[tid +  1];\n    }\n    // write result for this block to global mem\n    if(tid == 0) output[bid] = sdata[0];\n}\n```", "```py\nCOUNTING-SORT\n  HISTOGRAM-KEYS\n    do i = 0 to 2r -1\n      Bucket[i] = 0\n    do i = 0 to N – 1\n      Bucket[D[j]] = Bucket[D[j]] + 1\n  SCAN-BUCKETS\n    Sum = 0\n    do i = 0 to 2r – 1\n      Val = Bucket[i]\n      Bucket[i] = Sum\n      Sum = Sum + Val\n  RANK-AND-PERMUTE\n    do j = 0 to N – 1\n      A = Bucket[D[j]]\n      R[A] = K[j]\n      Bucket[D[j]] = A + 1\n```", "```py\nfor(int k = 1; k < 256; k++)\n      count[k] += count[k-1];\n```", "```py\nsum = 0\nout_arr[0] = 0\ndo i = 0 to lengthOf(in_arr)\n  t = in_arr[i+1]\n  sum = sum + t\n  out_arr[i] = sum\n```", "```py\nfor j = 1 to log2n do\n  for all k in parallel do\n    if (k >= 2j) then\n      x[k] = x[k – 2j-1] + x[k]\n    fi\n  endfor\nendfor\n```", "```py\nfor d from 0 to (log2 n) – 1\n  in parallel for i from 0 to n – 1 by 2d+1\n    array[i + 2d+1 – 1] = array[i + 2d – 1] + array[i + 2d+1 – 1]\n```", "```py\nd = 0 => i = [0..7,2] array[i + 1] = array[i] + array[i + 1]\nd = 1 => i = [0..7,4] array[i + 3] = array[i + 1] + array[i + 3]\nd = 2 => i = [0..7,8] array[i + 7] = array[i + 3] + array[i + 7]\n```", "```py\nx[n-1]=0\nfor d = log2 n – 1 to 0 do\n  for all k = 0 to n – 1 by 2d+1 in parallel do\n    temp = x[k + 2d – 1]\n    x[k + 2d – 1] = x[k + 2d+1 – 1]\n    x[k + 2d+1 – 1] = temp + x[k + 2d+1 – 1]\n  endfor\nendfor\n```", "```py\nd = 2 => k = [0..7,8] \n    temp = x[k + 3]\n    x[k + 3] = x[k + 7]\n    x[k + 7] = temp + x[k + 7]\nd = 1 => k = [0..7,4]\n    temp = x[k + 1]\n    x[k + 1] = x[k + 3]\n    x[k + 3] = temp + k[x + 3]\nd = 0 => k = [0..7,2]\n    temp = x[k]\n    x[k] = x[k + 1]\n    x[k + 1] = temp + x[k + 1]\n```", "```py\nint radixSortCPU(cl_uint* unsortedData, cl_uint* hSortedData) {\n\n    cl_uint *histogram = (cl_uint*) malloc(R * sizeof(cl_uint));\n    cl_uint *scratch = (cl_uint*) malloc(DATA_SIZE * sizeof(cl_uint));\n\n    if(histogram != NULL && scratch != NULL) {\n\n        memcpy(scratch, unsortedData, DATA_SIZE * sizeof(cl_uint));\n        for(int bits = 0; bits < sizeof(cl_uint) * bitsbyte ; bits += bitsbyte) {\n\n            // Initialize histogram bucket to zeros\n            memset(histogram, 0, R * sizeof(cl_uint));\n\n            // Calculate 256 histogram for all element\n            for(int i = 0; i < DATA_SIZE; ++i)\n            {\n                cl_uint element = scratch[i];\n                cl_uint value = (element >> bits) & R_MASK;\n                histogram[value]++;\n            }\n\n            // Apply the prefix-sum algorithm to the histogram\n            cl_uint sum = 0;\n            for(int i = 0; i < R; ++i)\n            {\n                cl_uint val = histogram[i];\n                histogram[i] = sum;\n                sum += val;\n            }\n\n            // Rearrange the elements based on prescanned histogram\n            // Thus far, the preceding code is basically adopted from\n            // the \"counting sort\" algorithm.\n            for(int i = 0; i < DATA_SIZE; ++i)\n            {\n                cl_uint element = scratch[i];\n                cl_uint value = (element >> bits) & R_MASK;\n                cl_uint index = histogram[value];\n                hSortedData[index] = scratch[i];\n                histogram[value] = index + 1;\n            }\n\n            // Copy to 'scratch' for further use since we are not done yet\n            if(bits != bitsbyte * 3)\n                memcpy(scratch, hSortedData, DATA_SIZE * sizeof(cl_uint));\n        }\n    }\n\n    free(scratch);\n    free(histogram);\n    return 1;\n}\n```", "```py\n#define bitsbyte 8\n#define R (1 << bitsbyte)\n\n__kernel void computeHistogram(__global const uint* data,\n                               __global uint* buckets,\n                               uint shiftBy,\n                               __local uint* sharedArray) {\n\n    size_t localId = get_local_id(0);\n    size_t globalId = get_global_id(0);\n    size_t groupId = get_group_id(0);\n    size_t groupSize = get_local_size(0);\n\n    /* Initialize shared array to zero i.e. sharedArray[0..63] = {0}*/\n    sharedArray[localId] = 0;\n    barrier(CLK_LOCAL_MEM_FENCE);\n\n    /* Calculate thread-histograms local/shared memory range from 32KB to 64KB */\n\n    uint result= (data[globalId] >> shiftBy) & 0xFFU;\n    atomic_inc(sharedArray+result);\n\n    barrier(CLK_LOCAL_MEM_FENCE);\n\n    /* Copy calculated histogram bin to global memory */\n\n    uint bucketPos = groupId * groupSize + localId ;\n    buckets[bucketPos] = sharedArray[localId];\n} \n__kernel void rankNPermute(__global const uint* unsortedData,\n                           __global const uint* scannedHistogram,\n                           uint shiftCount,\n                           __local ushort* sharedBuckets,\n                           __global uint* sortedData) {\n\n    size_t groupId = get_group_id(0);\n    size_t idx = get_local_id(0);\n    size_t gidx = get_global_id(0);\n    size_t groupSize = get_local_size(0);\n\n    /* There are now GROUP_SIZE * RADIX buckets and we fill\n       the shared memory with those prefix-sums computed previously\n     */\n    for(int i = 0; i < R; ++i)\n    {\n        uint bucketPos = groupId * R * groupSize + idx * R + i;\n        sharedBuckets[idx * R + i] = scannedHistogram[bucketPos];\n    }\n\n    barrier(CLK_LOCAL_MEM_FENCE);\n\n    /* Using the idea behind COUNTING-SORT to place the data values in its sorted\n       order based on the current examined key\n     */\n    for(int i = 0; i < R; ++i)\n    {\n        uint value = unsortedData[gidx * R + i];\n        value = (value >> shiftCount) & 0xFFU;\n        uint index = sharedBuckets[idx * R + value];\n        sortedData[index] = unsortedData[gidx * R + i];\n        sharedBuckets[idx * R + value] = index + 1;\n        barrier(CLK_LOCAL_MEM_FENCE);\n    }\n}\n__kernel void blockScan(__global uint *output,\n                        __global uint *histogram,\n                        __local uint* sharedMem,\n                        const uint block_size,\n                        __global uint* sumBuffer) {\n      int idx = get_local_id(0);\n      int gidx = get_global_id(0);\n      int gidy = get_global_id(1);\n      int bidx = get_group_id(0);\n      int bidy = get_group_id(1);\n\n      int gpos = (gidx << bitsbyte) + gidy;\n      int groupIndex = bidy * (get_global_size(0)/block_size) + bidx;\n\n      /* Cache the histogram buckets into shared memory\n         and memory reads into shared memory is coalesced\n      */\n      sharedMem[idx] = histogram[gpos];\n      barrier(CLK_LOCAL_MEM_FENCE);\n\n    /*\n       Build the partial sums sweeping up the tree using\n       the idea of Hillis and Steele in 1986\n     */\n    uint cache = sharedMem[0];\n    for(int stride = 1; stride < block_size; stride <<= 1)\n    {\n        if(idx>=stride)\n        {\n            cache = sharedMem[idx-stride]+block[idx];\n        }\n        barrier(CLK_LOCAL_MEM_FENCE); // all threads are blocked here\n\n        sharedMem[idx] = cache;\n        barrier(CLK_LOCAL_MEM_FENCE);\n    }\n\n    /* write the array of computed prefix-sums back to global memory */\n    if(idx == 0)\n    {\n        /* store the value in sum buffer before making it to 0 */\n        sumBuffer[groupIndex] = sharedMem[block_size-1];\n        output[gpos] = 0;\n    }\n    else\n    {\n        output[gpos] = sharedMem[idx-1];\n    }\n}\n__kernel void unifiedBlockScan(__global uint *output,\n                               __global uint *input,\n                               __local uint* sharedMem,\n                               const uint block_size) {\n\n    int id = get_local_id(0);\n    int gid = get_global_id(0);\n    int bid = get_group_id(0);\n\n    /* Cache the computational window in shared memory */\n    sharedMem[id] = input[gid];\n\n    uint cache = sharedMem[0];\n\n    /* build the sum in place up the tree */\n    for(int stride = 1; stride < block_size; stride <<= 1)\n    {\n        if(id>=stride)\n        {\n            cache = sharedMem[id-stride]+sharedMem[id];\n        }\n        barrier(CLK_LOCAL_MEM_FENCE);\n\n        sharedMem[id] = cache;\n        barrier(CLK_LOCAL_MEM_FENCE);\n\n    }\n    /*write the results back to global memory */\n    if(tid == 0) {\n        output[gid] = 0;\n    } else {\n        output[gid] = sharedMem[id-1];\n    }\n}\n__kernel void blockPrefixSum(__global uint* output,\n                             __global uint* input,\n                             __global uint* summary,\n                             int stride) {\n\n     int gidx = get_global_id(0);\n     int gidy = get_global_id(1);\n     int Index = gidy * stride +gidx;\n     output[Index] = 0;\n\n      // Notice that you don't need memory fences in this kernel\n      // because there is no race conditions and the assumption\n      // here is that the hardware schedules the blocks with lower\n      // indices first before blocks with higher indices\n     if(gidx > 0)\n     {\n         for(int i =0;i<gidx;i++)\n             output[Index] += input[gidy * stride +i];\n     }\n     // Write out all the prefix sums computed by this block\n     if(gidx == (stride - 1))\n         summary[gidy] = output[Index] + input[gidy * stride + (stride -1)];\n}\n\n__kernel void blockAdd(__global uint* input,\n                       __global uint* output,\n                       uint stride) {\n\n      int gidx = get_global_id(0);\n      int gidy = get_global_id(1);\n      int bidx = get_group_id(0);\n      int bidy = get_group_id(1);\n\n      int gpos = gidy + (gidx << bitsbyte);\n\n      int groupIndex = bidy * stride + bidx;\n\n      uint temp;\n      temp = input[groupIndex];\n\n      output[gpos] += temp;\n}\n__kernel void mergePrefixSums(__global uint* input,\n                        __global uint* output) {\n\n   int gidx = get_global_id(0);\n   int gidy = get_global_id(1);\n   int gpos = gidy + (gidx << bitsbyte );\n   output[gpos] += input[gidy];\n}\n```", "```py\nvoid runKernels(cl_uint* dSortedData, size_t numOfGroups, size_t groupSize) {\n   for(int currByte = 0; currByte < sizeof(cl_uint) * bitsbyte; currByte += bitsbyte) {\n    computeHistogram(currByte);\n    computeBlockScans();\n    computeRankingNPermutations(currByte,groupSize);\n  }\n}\n```", "```py\nvoid computeHistogram(int currByte) {\n    cl_event execEvt;\n    cl_int status;\n    size_t globalThreads = DATA_SIZE;\n    size_t localThreads  = BIN_SIZE;\n    status = clSetKernelArg(histogramKernel, 0, sizeof(cl_mem),\n             (void*)&unsortedData_d);\n    status = clSetKernelArg(histogramKernel, 1, sizeof(cl_mem),\n             (void*)&histogram_d);\n    status = clSetKernelArg(histogramKernel, 2, sizeof(cl_int),\n             (void*)&currByte);\n    status = clSetKernelArg(histogramKernel, 3, sizeof(cl_int) *\n             BIN_SIZE, NULL);\n    status = clEnqueueNDRangeKernel(\n        commandQueue,\n        histogramKernel,\n        1,\n        NULL,\n        &globalThreads,\n        &localThreads,\n        0,\n        NULL,\n        &execEvt);\n    clFlush(commandQueue);\n    waitAndReleaseDevice(&execEvt);\n}\n```", "```py\n    /* Initialize shared array to zero i.e. sharedArray[0..63] = {0}*/\n    sharedArray[localId] = 0;       \n    barrier(CLK_LOCAL_MEM_FENCE);   \n```", "```py\n    uint result= (data[globalId] >> shiftBy) & 0xFFU; //5\n    atomic_inc(sharedArray+result);                         //6\n\n    barrier(CLK_LOCAL_MEM_FENCE);                           //7\n\n    /* Copy calculated histogram bin to global memory */\n\n    uint bucketPos = groupId  * groupSize + localId ; //8\n    buckets[bucketPos] = sharedArray[localId];        //9\n```", "```py\n    size_t numOfGroups = DATA_SIZE / BIN_SIZE;\n    size_t globalThreads[2] = {numOfGroups, R};\n    size_t localThreads[2] = {GROUP_SIZE, 1};\n    cl_uint groupSize = GROUP_SIZE;\n\nstatus = clSetKernelArg(blockScanKernel, 0, sizeof(cl_mem), (void*)&scannedHistogram_d);\n    status = clSetKernelArg(blockScanKernel, 1, sizeof(cl_mem), (void*)&histogram_d);\n    status = clSetKernelArg(blockScanKernel, 2, GROUP_SIZE * sizeof(cl_uint), NULL);\n    status = clSetKernelArg(blockScanKernel, 3, sizeof(cl_uint), &groupSize);\n    status = clSetKernelArg(blockScanKernel, 4, sizeof(cl_mem), &sum_in_d);\n    cl_event execEvt;\n    status = clEnqueueNDRangeKernel(\n                commandQueue,\n                blockScanKernel,\n                2,\n                NULL,\n                globalThreads,\n                localThreads,\n                0,\n                NULL,\n                &execEvt);\n    clFlush(commandQueue);\n    waitAndReleaseDevice(&execEvt);\n```", "```py\n__kernel void blockScan(__global uint *output,\n                        __global uint *histogram,\n                        __local uint* sharedMem,\n                        const uint block_size,\n                        __global uint* sumBuffer) {\n      int idx = get_local_id(0);\n      int gidx = get_global_id(0);\n      int gidy = get_global_id(1);\n      int bidx = get_group_id(0);\n      int bidy = get_group_id(1);\n\n      int gpos = (gidx << bitsbyte) + gidy;\n      int groupIndex = bidy * (get_global_size(0)/block_size) + bidx;\n\n      /* Cache the histogram buckets into shared memory\n         and memory reads into shared memory is coalesced\n      */\n      sharedMem[idx] = histogram[gpos];\n      barrier(CLK_LOCAL_MEM_FENCE);\n```", "```py\n    /*\n       Build the partial sums sweeping up the tree using\n       the idea of Hillis and Steele in 1986\n     */\n    uint cache = sharedMem[0];\n    for(int dis = 1; dis < block_size; dis <<= 1)\n    {\n        if(idx>=dis)\n        {\n            cache = sharedMem[idx-dis]+block[idx];\n        }\n        barrier(CLK_LOCAL_MEM_FENCE); // all threads are blocked here\n\n        sharedMem[idx] = cache;\n        barrier(CLK_LOCAL_MEM_FENCE);\n    }\n```", "```py\n    /* write the array of computed prefix-sums back to global memory */\n    if(idx == 0)\n    {\n        /* store the value in sum buffer before making it to 0 */\n        sumBuffer[groupIndex] = sharedMem[block_size-1];\n        output[gpos] = 0;\n    } else {        \n        output[gpos] = sharedMem[idx-1];\n    }\n}\n```", "```py\n__kernel void blockPrefixSum(__global uint* output,\n                             __global uint* input,\n                             __global uint* summary,\n                             int stride) {\n\n     int gidx = get_global_id(0);\n     int gidy = get_global_id(1);\n     int Index = gidy * stride +gidx;\n     output[Index] = 0;\n\n     if(gidx > 0) {\n         for(int i =0;i<gidx;i++)\n             output[Index] += input[gidy * stride +i];\n     }\n```", "```py\n     // Write out all the prefix sums computed by this block\n     if(gidx == (stride - 1))\n         summary[gidy] = output[Index] + input[gidy * stride + (stride -1)];\n```", "```py\n        cl_event execEvt2;\n        size_t globalThreadsAdd[2] = {numOfGroups, R};\n        size_t localThreadsAdd[2] = {GROUP_SIZE, 1};\n        status = clSetKernelArg(blockAddKernel, 0, sizeof(cl_mem), (void*)&sum_out_d);\n        status = clSetKernelArg(blockAddKernel, 1, sizeof(cl_mem), (void*)&scannedHistogram_d);\n        status = clSetKernelArg(blockAddKernel, 2, sizeof(cl_uint), (void*)&stride);\n        status = clEnqueueNDRangeKernel(\n                    commandQueue,\n                    blockAddKernel,\n                    2,\n                    NULL,\n                    globalThreadsAdd,\n                    localThreadsAdd,\n                    0,\n                    NULL,\n                    &execEvt2);\n        clFlush(commandQueue);\n        waitAndReleaseDevice(&execEvt2);\n```", "```py\n__kernel void blockAdd(__global uint* input,\n                       __global uint* output,\n                       uint stride) {\n\n      int gidx = get_global_id(0);\n      int gidy = get_global_id(1);\n      int bidx = get_group_id(0);\n      int bidy = get_group_id(1);\n\n      int gpos = gidy + (gidx << bitsbyte);\n\n      int groupIndex = bidy * stride + bidx;\n\n      uint temp;\n      temp = input[groupIndex];\n\n      output[gpos] += temp;\n}\n```", "```py\n        cl_event execEvt4;\n        size_t globalThreadsOffset[2] = {numOfGroups, R};\n        status = clSetKernelArg(mergePrefixSumsKernel, 0, sizeof(cl_mem), (void*)&summary_out_d);\n        status = clSetKernelArg(mergePrefixSumsKernel, 1, sizeof(cl_mem), (void*)&scannedHistogram_d);\n        status = clEnqueueNDRangeKernel(commandQueue, mergePrefixSumsKernel, 2, NULL, globalThreadsOffset, NULL, 0, NULL, &execEvt4);\n        clFlush(commandQueue);\n        waitAndReleaseDevice(&execEvt4);\n```", "```py\n__kernel void mergePrefixSums(__global uint* input,\n                              __global uint* output) {\n\n   int gidx = get_global_id(0);\n   int gidy = get_global_id(1);\n   int gpos = gidy + (gidx << bitsbyte );\n   output[gpos] += input[gidy];\n}\n```", "```py\nvoid computeRankingNPermutations(int currByte, size_t groupSize) {\n    cl_int status;\n    cl_event execEvt;\n\n    size_t globalThreads = DATA_SIZE/R;\n    size_t localThreads = groupSize;\n\n    status = clSetKernelArg(permuteKernel, 0, sizeof(cl_mem), (void*)&unsortedData_d);\n    status = clSetKernelArg(permuteKernel, 1, sizeof(cl_mem), (void*)&scannedHistogram_d);\n    status = clSetKernelArg(permuteKernel, 2, sizeof(cl_int), (void*)&currByte);\n    status = clSetKernelArg(permuteKernel, 3, groupSize * R * sizeof(cl_ushort), NULL); // shared memory\n    status = clSetKernelArg(permuteKernel, 4, sizeof(cl_mem), (void*)&sortedData_d);\n\n    status = clEnqueueNDRangeKernel(commandQueue, permuteKernel, 1, NULL, &globalThreads, &localThreads, 0, NULL, &execEvt);\n    clFlush(commandQueue);\n    waitAndReleaseDevice(&execEvt);\n```", "```py\n__kernel void rankNPermute(__global const uint* unsortedData,\n                           __global const uint* scannedHistogram,\n                           uint shiftCount,\n                           __local ushort* sharedBuckets,\n                           __global uint* sortedData) {\n    size_t groupId = get_group_id(0);\n    size_t idx = get_local_id(0);\n    size_t gidx = get_global_id(0);\n    size_t groupSize = get_local_size(0);\n    for(int i = 0; i < R; ++i) {\n        uint bucketPos = groupId * R * groupSize + idx * R + i;\n        sharedBuckets[idx * R + i] = scannedHistogram[bucketPos];\n    }\n    barrier(CLK_LOCAL_MEM_FENCE);\n```", "```py\n    for(int i = 0; i < R; ++i) {\n        uint value = unsortedData[gidx * R + i];\n        value = (value >> shiftCount) & 0xFFU;\n        uint index = sharedBuckets[idx * R + value];\n        sortedData[index] = unsortedData[gidx * R + i];\n        sharedBuckets[idx * R + value] = index + 1;\n        barrier(CLK_LOCAL_MEM_FENCE);\n    }\n```"]