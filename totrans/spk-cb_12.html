<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch12"/>Chapter 12. Optimizations and Performance Tuning</h1></div></div></div><p>This chapter covers various optimizations and performance-tuning best practices when working with Spark.</p><p>The chapter is divided into the following recipes:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Optimizing memory</li><li class="listitem" style="list-style-type: disc">Using compression to improve performance</li><li class="listitem" style="list-style-type: disc">Using serialization to improve performance</li><li class="listitem" style="list-style-type: disc">Optimizing garbage collection</li><li class="listitem" style="list-style-type: disc">Optimizing the level of parallelism</li><li class="listitem" style="list-style-type: disc">Understanding the future of optimization – project Tungsten</li></ul></div><div><div><div><div><h1 class="title"><a id="ch12lvl1sec75"/>Introduction</h1></div></div></div><p>Before looking into various ways to optimize Spark, it is a good idea to look at the Spark internals. So far, we have looked at Spark at higher level, where focus was the functionality provided by the various libraries.</p><p>Let's start with <a id="id606" class="indexterm"/>redefining an RDD. Externally, an RDD is a distributed immutable collection of objects. Internally, it consists of the following five parts:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Set of partitions (<code class="literal">rdd.getPartitions</code>)</li><li class="listitem" style="list-style-type: disc">List of dependencies on parent RDDs (<code class="literal">rdd.dependencies</code>)</li><li class="listitem" style="list-style-type: disc">Function to compute a partition, given its parents</li><li class="listitem" style="list-style-type: disc">Partitioner (optional) (<code class="literal">rdd.partitioner</code>)</li><li class="listitem" style="list-style-type: disc">Preferred location of each partition (optional) (<code class="literal">rdd.preferredLocations</code>)</li></ul></div><p>The first three are needed for an RDD to be recomputed, in case the data is lost. When combined, it is <a id="id607" class="indexterm"/>called <strong>lineage</strong>. The last two parts are optimizations.</p><p>A set of partitions is<a id="id608" class="indexterm"/> how data is divided into nodes. In case of HDFS, it means <code class="literal">InputSplits</code>, which are mostly the same as block (except when a record crosses block boundaries; in that case, it will be slightly bigger than a block).</p><p>Let's revisit<a id="id609" class="indexterm"/> our <code class="literal">wordCount</code> example to understand these five parts. This is how the RDD graph looks for <code class="literal">wordCount</code> at dataset level view:</p><div><img src="img/3056_12_01.jpg" alt="Introduction"/></div><p>Basically, this is how the flow goes:</p><div><ol class="orderedlist arabic"><li class="listitem">Load the <code class="literal">words</code> folder as an RDD:<div><pre class="programlisting">
<strong>scala&gt; val words = sc.textFile("hdfs://localhost:9000/user/hduser/words")</strong>
</pre></div><p>The following are the five parts of <code class="literal">words</code> RDD:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p><strong>Partitions</strong></p>
</td><td style="text-align: left" valign="top">
<p>One partition per hdfs inputsplit/block (<code class="literal">org.apache.spark.rdd.HadoopPartition</code>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><strong>Dependencies</strong></p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><strong>Compute function</strong></p>
</td><td style="text-align: left" valign="top">
<p>Read the block</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><strong>Preferred location</strong></p>
</td><td style="text-align: left" valign="top">
<p>The hdfs block location</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><strong>Partitioner</strong></p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td></tr></tbody></table></div></li><li class="listitem">Tokenize the <a id="id610" class="indexterm"/>words from <code class="literal">words</code> RDD with each word on a separate line:<div><pre class="programlisting">
<strong>scala&gt; val wordsFlatMap = words.flatMap(_.split("\\W+"))</strong>
</pre></div><p>The following are the five parts of <code class="literal">wordsFlatMap</code> RDD:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p><strong>Partitions</strong></p>
</td><td style="text-align: left" valign="top">
<p>Same as parent RDD, that is, <code class="literal">words</code> (<code class="literal">org.apache.spark.rdd.HadoopPartition</code>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><strong>Dependencies</strong></p>
</td><td style="text-align: left" valign="top">
<p>Same as parent RDD, that is, <code class="literal">words</code> (<code class="literal">org.apache.spark.OneToOneDependency</code>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><strong>Compute function</strong></p>
</td><td style="text-align: left" valign="top">
<p>Compute parent and split each element and flattens the results</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><strong>Preferred location</strong></p>
</td><td style="text-align: left" valign="top">
<p>Ask parent</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><strong>Partitioner</strong></p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td></tr></tbody></table></div></li><li class="listitem">Transform each word in <code class="literal">wordsFlatMap</code> RDD to (word,1) tuple:<div><pre class="programlisting">
<strong>scala&gt; val wordsMap = wordsFlatMap.map( w =&gt; (w,1))</strong>
</pre></div><p>The following are the five parts of <code class="literal">wordsMap</code> RDD:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p><strong>Partitions</strong></p>
</td><td style="text-align: left" valign="top">
<p>Same as parent RDD, that is, wordsFlatMap (org.apache.spark.rdd.HadoopPartition)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><strong>Dependencies</strong></p>
</td><td style="text-align: left" valign="top">
<p>Same as parent RDD, that is, wordsFlatMap (org.apache.spark.OneToOneDependency)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><strong>Compute function</strong></p>
</td><td style="text-align: left" valign="top">
<p>Compute parent and map it to PairRDD</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><strong>Preferred Location</strong></p>
</td><td style="text-align: left" valign="top">
<p>Ask parent</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><strong>Partitioner</strong></p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td></tr></tbody></table></div></li><li class="listitem">Reduce all the values for a given key and sum them up:<div><pre class="programlisting">
<strong>scala&gt; val wordCount = wordsMap.reduceByKey(_+_)</strong>
</pre></div><p>The following are the five parts of <code class="literal">wordCount</code> RDD:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p><strong>Partitions</strong></p>
</td><td style="text-align: left" valign="top">
<p>One per reduce task (<code class="literal">org.apache.spark.rdd.ShuffledRDDPartition</code>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><strong>Dependencies</strong></p>
</td><td style="text-align: left" valign="top">
<p>Shuffle dependency on each parent (<code class="literal">org.apache.spark.ShuffleDependency</code>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><strong>Compute function</strong></p>
</td><td style="text-align: left" valign="top">
<p>Do addition on shuffled data</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><strong>Preferred location</strong></p>
</td><td style="text-align: left" valign="top">
<p>None</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><strong>Partitioner</strong></p>
</td><td style="text-align: left" valign="top">
<p>HashPartitioner (<code class="literal">org.apache.spark.HashPartitioner</code>)</p>
</td></tr></tbody></table></div></li></ol></div><p>This is how <a id="id611" class="indexterm"/>an RDD graph for <code class="literal">wordCount</code> looks at the partition level view:</p><div><img src="img/3056_12_02.jpg" alt="Introduction"/></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch12lvl1sec76"/>Optimizing memory</h1></div></div></div><p>Spark is a complex distributed computing framework, and has many moving parts. Various<a id="id612" class="indexterm"/> cluster resources, such as memory, CPU, and network bandwidth, can become bottlenecks at various points. As Spark is an in-memory compute framework, the impact of the memory is the biggest.</p><p>Another issue is that it is common for Spark applications to use a huge amount of memory, sometimes more than 100 GB. This amount of memory usage is not common in traditional Java applications.</p><p>In Spark, there are two places where memory optimization is needed, and that is at the driver and at the executor level.</p><p>You can use the following commands to set the driver memory:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Spark shell:<div><pre class="programlisting">
<strong>$ spark-shell --drive-memory 4g</strong>
</pre></div></li><li class="listitem" style="list-style-type: disc">Spark submit:<div><pre class="programlisting">
<strong>$ spark-submit --drive-memory 4g</strong>
</pre></div></li></ul></div><p>You can use the following commands to set the executor memory:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Spark shell:<div><pre class="programlisting">
<strong>$ spark-shell --executor-memory 4g</strong>
</pre></div></li><li class="listitem" style="list-style-type: disc">Spark submit:<div><pre class="programlisting">
<strong>$ spark-submit --executor-memory 4g</strong>
</pre></div></li></ul></div><p>To understand memory optimization, it is a good idea to understand how memory management <a id="id613" class="indexterm"/>works in Java. Objects reside in Heap in Java. Heap is created when JVM starts, and it can resize itself when needed (based on minimum and maximum size, that is, <code class="literal">-Xms</code> and <code class="literal">-Xmx</code>, respectively assigned in configuration).</p><p>Heap is divided into two spaces or generations: young space and old space. The young space is reserved for the allocation of new objects. Young space consists of an area called <strong>Eden</strong> and<a id="id614" class="indexterm"/> two smaller survivor spaces. When the nursery becomes full, garbage is collected by running<a id="id615" class="indexterm"/> a special process called <strong>young collection</strong>, where all the objects, which have lived long enough, are promoted to old space. When the old space becomes full, the garbage is collected there by running a <a id="id616" class="indexterm"/>process called <strong>old collection</strong>.</p><div><img src="img/3056_12_03.jpg" alt="Optimizing memory"/></div><p>The logic behind nursery is that most objects have a very short life span. A young collection is designed to be fast at finding newly allocated objects and moving them to the old space.</p><p>The JVM uses mark and sweep algorithm for garbage collection. Mark and sweep collection consists of two phases.</p><p>During the mark phase, all the objects, which have live references, are marked alive, the rest are presumed candidates for garbage collection. During the sweep phase, the space occupied by garbage collectable candidates is added to the free list, that is, they are available to be allocated to new objects.</p><p>There are<a id="id617" class="indexterm"/> two improvements to mark and sweep. One is <strong>concurrent mark and sweep</strong> (<strong>CMS</strong>) and the other is parallel mark and sweep. CMS<a id="id618" class="indexterm"/> focuses on lower latency, while the latter focuses on higher throughput. Both strategies have performance trade-offs. CMS does not do <a id="id619" class="indexterm"/>compaction, while parallel <strong>garbage collector</strong> (<strong>GC</strong>) performs whole-heap only compaction, which results in pause times. As a thumb rule, for real-time streaming, CMS should be used, and parallel GC otherwise.</p><p>If you would like to have both low latency and high throughput, Java 1.7 update 4 onwards has another option called <strong>garbage-first GC</strong> (<strong>G1</strong>). G1 is a server-style garbage collector, primarily <a id="id620" class="indexterm"/>meant for multicore machines with large memories. It is planned as a long-term replacement for CMS. So, to modify our thumb rule, if you are using Java 7 onwards, simply use G1.</p><p>G1 partitions the heap into a set of equal-sized regions, where each set is a contiguous range of virtual memory. Each region is assigned a role like Eden, Survivor, and Old. G1 performs a concurrent global marking phase to determine the live references of objects throughout the heap. After the mark phase is over, G1 knows which regions are mostly empty. It collects in these regions first and this frees the larger amount of memory.</p><div><img src="img/3056_12_04.jpg" alt="Optimizing memory"/></div><p>The regions selected by G1 as candidates for garbage collection are garbage collected using evacuation. G1 copies objects from one or more regions of the heap to a single region on the heap, and it both compacts and frees up memory. This evacuation is performed in parallel on multiple cores to reduce pause times and increase throughput. So, each garbage collection round reduces fragmentation while working within user-defined pause times.</p><p>There are three <a id="id621" class="indexterm"/>aspects in memory optimization in Java:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Memory footprint</li><li class="listitem" style="list-style-type: disc">Cost of accessing objects in memory</li><li class="listitem" style="list-style-type: disc">Cost of garbage collection</li></ul></div><p>Java objects, in general, are fast to access but consume much more space than the actual data inside them.</p></div>
<div><div><div><div><h1 class="title"><a id="ch12lvl1sec77"/>Using compression to improve performance</h1></div></div></div><p>Data compression involves encoding information using fewer bits than the original representation. Compression<a id="id622" class="indexterm"/> has an important role to play in big data technologies. It makes both storage and transport of data more efficient.</p><p>When data is <a id="id623" class="indexterm"/>compressed, it becomes <a id="id624" class="indexterm"/>smaller, so both disk I/O and network I/O become faster. It also saves storage space. Every optimization has a cost, and the cost of compression comes in the form of added CPU cycles to compress and decompress data.</p><p>Hadoop needs to split data to put them into blocks, irrespective of whether the data is compressed or not. Only few compression formats are splittable.</p><p>Two most popular compression<a id="id625" class="indexterm"/> formats for big data loads are LZO and<a id="id626" class="indexterm"/> Snappy. Snappy is not splittable, while LZO is. Snappy, on the other hand, is a much faster format.</p><p>If compression format is splittable like LZO, input file is first split into blocks and then compressed. Since compression happened at block level, decompression can happen at block level as well as node level.</p><p>If compression format is not splittable, compression happens at file level and then it is split into blocks. In this case, blocks have to be merged back to file before they can be decompressed, so decompression cannot happen at node level.</p><p>For supported compression formats, Spark will deploy codecs automatically to decompress, and no action is required from the user's side.</p></div>
<div><div><div><div><h1 class="title"><a id="ch12lvl1sec78"/>Using serialization to improve performance</h1></div></div></div><p>Serialization <a id="id627" class="indexterm"/>plays an important part in <a id="id628" class="indexterm"/>distributed computing. There are two persistence (storage) levels, which support serializing RDDs:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">MEMORY_ONLY_SER</code>: This stores RDDs as serialized objects. It will create one byte array per partition</li><li class="listitem" style="list-style-type: disc"><code class="literal">MEMORY_AND_DISK_SER</code>: This is similar to the <code class="literal">MEMORY_ONLY_SER</code>, but it spills partitions that do not fit in the memory to disk</li></ul></div><p>The following are the steps to add appropriate persistence levels:</p><div><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell:<div><pre class="programlisting">
<strong>$ spark-shell</strong>
</pre></div></li><li class="listitem">Import the <code class="literal">StorageLevel</code> and implicits associated with it:<div><pre class="programlisting">
<strong>scala&gt; import org.apache.spark.storage.StorageLevel._</strong>
</pre></div></li><li class="listitem">Create an RDD:<div><pre class="programlisting">
<strong>scala&gt; val words = sc.textFile("words")</strong>
</pre></div></li><li class="listitem">Persist the RDD:<div><pre class="programlisting">
<strong>scala&gt; words.persist(MEMORY_ONLY_SER)</strong>
</pre></div></li></ol></div><p>Though <a id="id629" class="indexterm"/>serialization reduces the <a id="id630" class="indexterm"/>memory footprint substantially, it adds extra CPU cycles due to deserialization.</p><p>By default, Spark uses Java's serialization. Since the Java serialization is slow, the better approach is to use <code class="literal">Kryo</code> library. <code class="literal">Kryo</code> is much faster and sometimes even 10 times more compact than the<a id="id631" class="indexterm"/> default.</p><div><div><div><div><h2 class="title"><a id="ch12lvl2sec113"/>How to do it…</h2></div></div></div><p>You can use <code class="literal">Kryo</code> by doing the following settings in your <code class="literal">SparkConf</code>:</p><div><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell by setting <code class="literal">Kryo</code> as serializer:<div><pre class="programlisting">
<strong>$ spark-shell --conf spark.serializer=org.apache.spark.serializer.KryoSerializer</strong>
</pre></div></li><li class="listitem"><code class="literal">Kryo</code> automatically registers most of the core Scala classes, but if you would like to register your own classes, you can use the following command:<div><pre class="programlisting">
<strong>scala&gt; sc.getConf.registerKryoClasses(Array(classOf[com.infoobjects.CustomClass1],classOf[com.infoobjects.CustomClass2])</strong>
</pre></div></li></ol></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch12lvl1sec79"/>Optimizing garbage collection</h1></div></div></div><p>JVM garbage collection can be a challenge if you have a lot of short lived RDDs. JVM needs to <a id="id632" class="indexterm"/>go over all the objects to find the ones it needs to garbage collect. The cost of the garbage collection is proportional to the number of objects the GC needs to go through. Therefore, using fewer objects and the data structures that use fewer objects (simpler data structures, such as arrays) helps.</p><p>Serialization also shines here as a byte array needs only one object to be garbage collected.</p><p>By default, Spark uses 60 percent of the executor memory to cache RDDs and the rest 40 percent for regular objects. Sometimes, you may not need 60 percent for RDDs and can reduce this limit so that more space is available for object creation (less need for GC).</p><div><div><div><div><h2 class="title"><a id="ch12lvl2sec114"/>How to do it…</h2></div></div></div><p>You can <a id="id633" class="indexterm"/>set the memory allocated for RDD cache to 40 percent by starting the Spark shell and setting the memory fraction:</p><div><pre class="programlisting">
<strong>$ spark-shell --conf spark.storage.memoryFraction=0.4</strong>
</pre></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch12lvl1sec80"/>Optimizing the level of parallelism</h1></div></div></div><p>Optimizing the level <a id="id634" class="indexterm"/>of parallelism is very important to fully utilize the cluster capacity. In the case of HDFS, it means that the number of partitions is the same as the number of <code class="literal">InputSplits</code>, which is mostly the same as the number of blocks.</p><p>In this recipe, we will cover different ways to optimize the number of partitions.</p><div><div><div><div><h2 class="title"><a id="ch12lvl2sec115"/>How to do it…</h2></div></div></div><p>Specify the number of partitions when loading a file into RDD with the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell:<div><pre class="programlisting">
<strong>$ spark-shell</strong>
</pre></div></li><li class="listitem">Load the RDD with a custom number of partitions as a second parameter:<div><pre class="programlisting">
<strong>scala&gt; sc.textFile("hdfs://localhost:9000/user/hduser/words",10)</strong>
</pre></div></li></ol></div><p>Another approach is to change the default parallelism by performing the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell with the new value of default parallelism:<div><pre class="programlisting">
<strong>$ spark-shell --conf spark.default.parallelism=10</strong>
</pre></div></li><li class="listitem">Check the default value of parallelism:<div><pre class="programlisting">
<strong>scala&gt; sc.defaultParallelism</strong>
</pre></div></li></ol></div><div><div><h3 class="title"><a id="note23"/>Note</h3><p>You can also reduce the number of partitions using an RDD method called <code class="literal">coalesce(numPartitions)</code> where <code class="literal">numPartitions</code> is the final number of partitions you would like. If you would like the data to be reshuffled over the network, you can call the RDD method called <code class="literal">repartition(numPartitions)</code> where <code class="literal">numPartitions</code> is the final number of partitions you would like.</p></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch12lvl1sec81"/>Understanding the future of optimization – project Tungsten</h1></div></div></div><p>Project Tungsten, starting with Spark Version 1.4, is the initiative to bring Spark closer to bare metal. The goal of this project is to substantially improve the memory and CPU efficiency of the<a id="id635" class="indexterm"/> Spark applications and push the limits of underlying hardware.</p><p>In distributed systems, conventional wisdom has been to always optimize network I/O as that has been the most scarce and bottlenecked resource. This trend has changed in the last few years. Network bandwidth, in the last 5 years, has changed from 1 gigabit per second to 10 gigabit per second.</p><p>On similar lines, the disk bandwidth has increased from 50 MB/s to 500 MB/s and SSDs are being deployed more and more. CPU clock speed, on the other hand, was ~3 GHz 5 years back and is still the same. This has unseated the network and made CPU the new bottleneck in distributed processing.</p><div><div><h3 class="title"><a id="note24"/>Note</h3><p>Another trend that has put more load on CPU performance is the new compressed data formats such as Parquet. Both compression and serialization, as we have seen in the previous recipes in this chapter, lead to more CPU cycles. This trend has also pushed the need for CPU optimization to reduce the CPU cycle cost.</p></div></div><p>On the similar lines, let's look at the memory footprint. In Java, GC does memory management. GC has done an amazing job at taking away the memory management from the programmer and making it transparent. To do this, Java has to put a lot of overhead, and that substantially increases the memory footprint. As an example, a simple String "abcd", which should ideally take 4 bytes, takes 48 bytes in Java.</p><p>What if we do away with GC and manage memory manually like in lower-level programming languages such as C? Java does provide a way to do that since 1.7 version and it is called <code class="literal">sun.misc.Unsafe</code>. Unsafe essentially means that you can build long regions of memory without any safety checks. This is the first feature of project Tungsten.</p><div><div><div><div><h2 class="title"><a id="ch12lvl2sec116"/>Manual memory management by leverage application semantics</h2></div></div></div><p>Manual memory management by leverage application semantics, which can be very risky if you do<a id="id636" class="indexterm"/> not know what you are doing, is<a id="id637" class="indexterm"/> a blessing <a id="id638" class="indexterm"/>with Spark. We used knowledge of data schema (DataFrames) to directly layout the memory ourselves. It not only gets rid of GC overheads, but lets you minimize the memory footprint.</p><p>The second point is storing data in CPU cache versus memory. Everyone knows CPU cache is <a id="id639" class="indexterm"/>great as it takes three cycles to <a id="id640" class="indexterm"/>get data from <a id="id641" class="indexterm"/>the main memory versus one cycle in cache. This is the second feature of project Tungsten.</p></div><div><div><div><div><h2 class="title"><a id="ch12lvl2sec117"/>Using algorithms and data structures</h2></div></div></div><p>Algorithms <a id="id642" class="indexterm"/>and data structures are used to exploit <a id="id643" class="indexterm"/>memory hierarchy and enable more cache-aware computation.</p><p>CPU caches are small pools of memory that store the data the CPU is going to need next. CPUs have two types of caches: instruction cache and data cache. Data caches are arranged in hierarchy of L1, L2, and L3:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">L1 cache is the fastest and most expensive cache in a computer. It stores the most critical data and is the first place the CPU looks for information.</li><li class="listitem" style="list-style-type: disc">L2 cache is slightly slower than L1, but still located on the same processor chip. It is the second place the CPU looks for information.</li><li class="listitem" style="list-style-type: disc">L3 cache is still slower, but is shared by all cores, such as DRAM (memory).</li></ul></div><p>These can be seen in the following diagram:</p><div><img src="img/3056_12_05.jpg" alt="Using algorithms and data structures"/></div><p>The third point is that Java is not very good at bytecode generation for things like expression evaluation. If this code generation is done manually, it is much more efficient. Code <a id="id644" class="indexterm"/>generation is the third feature of project <a id="id645" class="indexterm"/>Tungsten.</p><div><div><div><div><h3 class="title"><a id="ch12lvl3sec06"/>Code generation</h3></div></div></div><p>This involves<a id="id646" class="indexterm"/> exploiting modern compliers and CPUs to allow efficient operations directly on binary data. Project Tungsten is in its infancy at present and will have much wider support in version 1.5.</p></div></div></div></body></html>