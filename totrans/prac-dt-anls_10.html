<html><head></head><body>
        <section>

                            <header class="header-title chapter-title">
                    Understanding Joins, Relationships, and Aggregates
                </header>
            
            <article>
                
<p>I'm really excited about this chapter because we are going to learn about the foundation of blending multiple datasets. This concept has been around for decades using SQL and other technologies including R, pandas, Excel, Cognos, and Qlikview.</p>
<p>The ability to merge data is a powerful skill that applies across different technologies and helps you to answer complex questions such as how product sales can be impacted by weather forecasts. The data sources are mutually exclusive, but today, access to weather data can be added to your data model with a few joins based on geographic location and time of day. We will be covering how this can be done along with the different types of joins. Once exposed to this concept, you will learn what questions can be answered depending on the granularity of data available. For our weather and sales data example, the details become important to understand the level of analysis that can be done. If you wanted to know whether rain impacts sales, the more common fields available, such as date, day, and time, and geographic location tags, such as latitude and longitude, must be available in both sources for you to be accurate in your conclusions after joining the data together.</p>
<p>In this chapter, we will learn how to construct high-quality datasets for further analysis. We will continue to advance your<span> </span><span>hands-on</span><span> data literacy skills by learning how to work with join relationships and how to create aggregate data for analysis.</span></p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Foundations of join relationships</li>
<li>Join types in action</li>
<li>Explaining data aggregation</li>
<li>Summary statistics and outliers</li>
</ul>
<h1 id="uuid-57ecd2d8-373c-4180-8185-438169f35e5b">Technical requirements</h1>
<p>Here's the<span> </span>GitHub repository<span> </span>of this book: <a href="https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter08">https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter08</a>.</p>
<p>You can download and install the required software from the following<span> </span>link: <a href="https://www.anaconda.com/products/individual" target="_blank">https://www.anaconda.com/products/individual</a>.</p>
<h1 id="uuid-3b450790-c7c9-4dca-81ce-0b31b5c7af19">Foundations of join relationships</h1>
<p>For anyone familiar with SQL, the concept of joining data together is well understood. The ability to join one or more tables together for the purpose of analytics has remained relevant throughout my 20+ year career of working with data and I hope it continues to be relevant.</p>
<p>In prior chapters, we introduced the concept of data models and the need for primary and foreign key fields to define relationships. We will now elaborate on these concepts by explaining joins and the different types of joins that exist in SQL and DataFrames.</p>
<p>Joining, in SQL, simply means merging two or more tables into a single dataset. The resulting size and shape of that single dataset will vary depending on the type of join that is used. Some key concepts you want to remember any time you are creating a join between datasets will be that the <strong>common unique key</strong> should always be used. Ideally, the key field functions as both the primary and foreign key but it can be derived using multiple fields to define a unique record for all rows of data.</p>
<p>We will go through the following types of join relationships in this section:</p>
<ul>
<li>One-to-one</li>
<li>Many-to-many</li>
<li>Left join</li>
<li>Right join</li>
<li>Inner join</li>
<li>Outer join</li>
</ul>
<div class="packt_tip"><span>With pandas DataFrames, the index in our examples has been the default or a single defined field because it has distinct values, but that is not always the case.</span></div>
<h2 id="uuid-886e07a8-19d9-441a-a725-f2f1d664845a">One-to-one relationships</h2>
<p>A <strong>one-to-one</strong> relationship means the sources have common unique values by row and duplicates do not exist. A similar feature in Excel is the <kbd>vlookup</kbd> function with the exact match parameter enabled. When you use this Excel function, any matches to the source identifier return a distinct value from the target lookup. In SQL, one-to-one relationships ensure that the integrity between two tables is consistent. There are multiple reasons why these types of relationships are needed, but a good example is when a reference table that has the sales region is joined to a unique <kbd>Customer</kbd> table. In this example, a customer identifier (<kbd>ID</kbd>) field would exist in both tables and you would never have a sales region without a customer record and vice versa. </p>
<h2 id="uuid-a77055ca-6524-42d7-ba73-7c458c3ecbc4">Many-to-one relationships</h2>
<p>A <strong>many-to-one relationship </strong>means one of the sources can have duplicate rows but not both sources. You still need a unique key, index, or identifier between the sources. An example would be a lookup dimensional table joined to a fact table, which we covered in <a href="7282a629-c59a-4922-8422-e27ed44563db.xhtml">Chapter 7</a>, <em>Exploring Cleaning, Refining, and Blending Datasets</em>.</p>
<p>A transaction fact table will have duplicate records because a user hit for a website or a product sale is recorded by date/time for <span>each occurrence</span>. The result will generate millions of rows with duplicate records in the <kbd>userid</kbd> field. When the join uses a common field such as <kbd>userid</kbd> between the <span>fact table and the second table, the second table </span>must be unique in each row. This second table will have <span>additional attributes about <kbd>userid</kbd></span>, <span>such as <kbd>city</kbd>, <kbd>state</kbd>, and <kbd>zip code</kbd>, which will offer richer analysis options. </span><span>Once you understand that this type of join relationship exists between two source tables, you can confidently join them together.</span></p>
<h2 id="uuid-dcf32094-c3a0-4164-9312-d67de965cf19">Many-to-many relationship</h2>
<p>A <strong>many-to-many relationship </strong>is when both sources have duplicate rows. Again, you should find a common unique key (one or more fields) or index between the sources. These types of joins are commonly referred to as <em>expensive</em> joins because the number of computing resources (memory and CPU) required will increase dramatically depending on the number of records and columns from both sources. A common example is a logical relationship between students and classes where many students can take many different classes. Conversely, many classes can have many different students. As a best practice, I would try to avoid direct many-to-many joins and use a bridge table to resolve them for analysis. For the students-to-classes example, you would need a roster table that marries together the unique list of one student per class, as in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6acde73e-e704-45c2-aac9-7e460f6a48ef.png"/></p>
<div class="packt_tip">For any join, you should avoid a <strong>Cartesian product</strong>, which is where all possible combinations of rows and columns exist as a result of the join. There are some cases where this might be useful in your analysis but be cautious, especially with big data volumes.</div>
<p>A Cartesian product is when all possible combinations of rows and columns are combined. For our example, if we joined <kbd>tbl_students</kbd> and <kbd>tbl_classes</kbd> without including <kbd>tbl_roster</kbd>, you would end up with students assigned to classes they never signed up for. There are occasions where I have deliberately constructed a Cartesian product because it was needed for certain types of analysis or a chart. For example, if you have a student ranking scale from 1 to 10 but none of the students achieved all of the possible values, you could create a Cartesian join to fill in the gaps of missing values.</p>
<p>Similar to concerns about exceeding memory and CPU utilization when working with many-to-many join types, a Cartesian product can easily consume all available memory, which could lead to a crash of your workstation or workspace.</p>
<h2 id="uuid-c5b4bfc6-b19f-4f0c-a1af-fce1fbe987a0">Left join</h2>
<p>Now that we have covered the key concepts, I'm going to begin with a visual representation of one of the most common types of joins used in analysis, which is called a <strong>left join</strong>. Let's start by looking at our source data for this example, which is represented in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/12434f84-6fb4-41d4-a70d-70ed4704987e.png" style="width:25.75em;height:10.50em;"/></p>
<p>As you can see, we have tables named <kbd>tbl_user_hits</kbd> and <kbd>tbl_user_geo</kbd>. The width and length of <kbd>tbl_user_hits</kbd> is two columns and three rows. In <kbd>tbl_user_geo</kbd>, which represents the user's geographical location, we have three columns and five rows. We have a common field named <kbd>userid</kbd> in both tables, which I highlighted and will use to join the data. These tables have a primary and foreign key many-to-one<span> </span>relationship because not all of the records from one table exist in the other.</p>
<p>For this example, we want to keep all of the records in <kbd>tbl_user_hits</kbd> and enrich the data by blending together matching attributes such as city and state where <kbd>userid</kbd> exists only for the records in the user hits table. The result is in the following screenshot, where the original source, <kbd>tbl_user_hits</kbd>, has the same number of rows and but now includes the columns from the <kbd>tbl_user_geo</kbd><span> </span><span>table</span><span>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/66e00ca9-6dc5-4a48-b67a-5b8c3e02551f.png" style="width:18.67em;height:7.67em;"/></p>
<p>Why did the number of rows remain the same but the number of columns increase? A successful left join preserves the number of rows from the source and extends the number of columns.</p>
<div class="packt_tip">The specific columns included in the join result can be defined but the default will include all of the columns.</div>
<p>Why are left joins common in analytics? That's because we are interested in adding more dimensional fields to answer more questions about the data that does not exist in a single table alone. Also, in your analysis, you typically don't want to include anything that doesn't match our source user hits. With this new join result, we can now answer questions such as which city has the highest number of users. Using a few date calculations, we can also provide monthly trends by state.</p>
<h2 id="uuid-64567d1c-3be3-4ede-8105-494fa2a3b1b6">Right join</h2>
<p>Next, we have a join type called a <strong>right join</strong>, which I find is less common. That is because there are not many use cases where the desire is to create gaps in the records of your merged data. A right join is where you want to preserve all of the columns and rows of the second table and fill in matching values from the first. </p>
<p>To understand the concept, start by referring back to our prior source tables, <kbd>tbl_user_hits</kbd>, and <kbd>tbl_user_geo</kbd>. The result of a successful right join is shown in the following screenshot, where the join result is shown with five rows and four columns. The date field from <kbd>tbl_user_hits</kbd> has been combined with the <kbd>tbl_user_geo</kbd> source but missing values will appear as <kbd>null()</kbd> or <kbd>NaN</kbd>. Note that if <kbd>tbl_user_hits</kbd> had thousands or millions of rows, the join result would increase the original size of <kbd>tbl_user_geo</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-807 image-border" src="assets/b6498169-9f5a-4efb-b2eb-8132350e11d4.png" style="width:20.08em;height:11.00em;"/></p>
<p class="mce-root"/>
<p>One advantage of using the right join is that now you can identify which cities and states do not have any user hits. This could be useful information and could be used for marketing campaigns.</p>
<h2 id="uuid-8c9460cf-e432-4eb6-b4c4-f915a3474e84">Inner join</h2>
<p>Next, we have an <strong>inner join</strong>. This is when only the exact values from both tables are returned along with all of the columns. To demonstrate the effect, I have made some adjustments to our source tables, which are shown in the following screenshot. The table names are the same but now some of the prior records are removed from <kbd>tbl_user_geo</kbd>. This could be due to a regulation request or the <kbd>userid</kbd> rows could have been determined to be invalid, so now we can use an inner join to remove them from <kbd>tbl_user_hits</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0b92bbd9-fb31-4be7-b59f-1315cea80175.png" style="width:26.08em;height:7.83em;"/></p>
<p>The join results are shown in the following screenshot, where only the matching values found in the <kbd>userid</kbd> key field are displayed along with all of the combined columns between both source tables:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d5915b31-4d17-4cd3-b1a5-4c702ce43841.png" style="width:19.25em;height:6.58em;"/></div>
<div class="packt_tip">The pandas function for performing joins is called <kbd>merge</kbd><strong> </strong>and an inner join is the default option.</div>
<p class="mce-root"/>
<h2 id="uuid-d2faab72-6e64-491b-8670-6424406fb233">Outer join</h2>
<p>Finally, we have an <strong>outer join</strong>, which provides a comprehensive list of all rows and columns from both sources. Unlike a <strong>Cartesian</strong>, where any possible combination of values is created, an outer join reflects the truth from both source tables. For our example, we will use the same source as the following screenshot, where records from <kbd>tbl_user_geo</kbd> were removed. Unlike an inner join, the outer join results provide you with the ability to see any missing records as null in SQL or <kbd>NaN</kbd> in Python/pandas:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-810 image-border" src="assets/07560fb3-1b9b-463e-82df-5da700bb27ad.png" style="width:16.25em;height:7.92em;"/></p>
<p>While these concepts and common join types are not exhaustive, you now have a good foundational understanding of joining data sources together so we can move forward with walking through practical examples.</p>
<h1 id="uuid-6aace89e-fa14-4c68-9319-49628ae1ebb2">Join types in action</h1>
<p class="mce-root">Unfortunately, the SQLite database that we use does not support all of the join options (right and outer), so I will provide only two examples of a join (left and inner) using SQL in this walk-through. The good news is that the <kbd>pandas</kbd> library supports all join types using <span>the <kbd>merge()</kbd> function, </span>so we can recreate all of the examples already discussed. Feel free to walk through the following code; I have placed a copy of the Jupyter Notebook code on GitHub for reference.</p>
<div class="packt_tip">Be sure to copy any dependencies files into your working folder before walking through all of the steps.</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We will begin by launching a new Jupyter notebook and naming it <kbd>ch_08_exercises</kbd>:</p>
<ol>
<li>Load a SQLite database connection:</li>
</ol>
<pre style="padding-left: 60px">In[]: import sqlite3</pre>
<p style="padding-left: 60px" class="mce-root">This library should already be available using Anaconda. Refer to <a href="e0fe6eb2-8f38-41f7-9dea-2b177578fd3c.xhtml">Chapter 2</a>, <em>Overview of Python and Installing Jupyter Notebook</em>, for help with setting up your environment.</p>
<ol start="2">
<li>Next, we need to assign a connection to a variable named <kbd>conn</kbd> and point to the location of the database file, which is named <kbd>user_hits.db</kbd>. Since we already imported the <kbd>sqlite3</kbd> library in the prior <kbd>In[]</kbd><strong> </strong>line, we can use this built-in function to communicate with the database:</li>
</ol>
<pre style="padding-left: 60px">In[]: conn = sqlite3.connect('user_hits.db')</pre>
<div class="packt_tip">Be sure that you copied the <kbd>user_hits.db</kbd> file to the correct Jupyter folder directory to avoid errors with the connection.</div>
<ol start="3">
<li>Import the <kbd>pandas</kbd> library:</li>
</ol>
<pre style="padding-left: 60px">In[]: import pandas as pd</pre>
<ol start="4">
<li>To run a SQL statement and assign the results to a DataFrame, we have to run this one line of code. The <kbd>pandas</kbd> library includes a <kbd>read_sql_query</kbd> function to make it easier to communicate with databases using SQL. It requires a connection parameter that we named <kbd>conn</kbd> in the previous steps. We assign the results to a new DataFrame as <kbd>df_left_join</kbd> to make it easier to identify:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_left_join = pd.read_sql_query("select u.userid, u.date, g.city, g.state from tbl_user_hits u left join tbl_user_geo g on u.userid = g.userid;", conn)</pre>
<div class="packt_tip">SQL supports the concept of an alias for table names so you can shorten the syntax. In this example, <kbd>tbl_user_hits</kbd> has an alias of <kbd>u</kbd> and <kbd>tbl_user_geo</kbd> is <kbd>g</kbd>. This helps when explicitly calling field names that require a prefix of the table name.</div>
<ol start="5">
<li>Now that we have the results in a DataFrame, we can use all of the available <kbd>pandas</kbd> library commands against this data without going back to the database. To view the results, we can just run the <kbd>head()</kbd> command against this DataFrame using this code:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_left_join.head()</pre>
<p style="padding-left: 60px"><span>The output will look like the following screenshot, </span><span>where the SQL results have been loaded into a DataFrame with a labeled header row with the index column to the left starting with a value of <kbd>0</kbd>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/df65ebe0-1ab5-41f8-b7d9-d7077c60267f.png" style="width:19.83em;height:9.83em;"/></p>
<ol start="6">
<li>The next join will be an inner and we will assign the results to a new DataFrame as <kbd>df_inner_join</kbd> to make it easier to identify:</li>
</ol>
<pre style="padding-left: 60px">df_inner_join = pd.read_sql_query("select u.userid, u.date, g.city, g.state from tbl_user_hits u, tbl_user_geo g where u.userid = g.userid;", conn)</pre>
<ol start="7">
<li>Now that we have the results in a DataFrame, we can use all of the available <kbd>pandas</kbd> library commands against this data without going back to the database. To view the results, we can just run the <kbd>head()</kbd> command against this DataFrame using this code:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_inner_join.head()</pre>
<p style="padding-left: 60px"><span>The output will look like the following screenshot,</span><span> where the SQL results have been loaded into a DataFrame with a labeled header row with the index column to the left starting with a value of <kbd>0:</kbd></span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ca7e2492-9be7-4989-aee6-d8dcf457d106.png" style="width:20.67em;height:8.25em;"/></div>
<p class="mce-root"/>
<p style="padding-left: 60px">We will continue the walk-through of the remaining exercise using the <kbd>merge()</kbd> function in pandas. I have a reference with more details about the function in the <em>Further reading</em> section, but using it is very straightforward. Once you have the two input tables stored as DataFrames, you can input them into the <kbd>merge()</kbd> function as parameters along with the join type you want to use, which is controlled using the <kbd>how</kbd> parameter. The default when you don't specify a parameter is an inner join, and it supports all of the SQL joins we have discussed, including left, right, and outer. The result of using the <kbd>merge()</kbd> function is to return a DataFrame with the source objects merged. Let's continue this walk-through exercise by loading our source SQL tables as DataFrames.</p>
<ol start="8">
<li>
<p>Create a new DataFrame called <kbd>df_user_hits</kbd>, which is a duplicate of the <kbd>tbl_user_hits</kbd> table, so we can use it later in the examples:</p>
</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_hits = pd.read_sql_query("select * from tbl_user_hits;", conn)</pre>
<ol start="9">
<li>To validate the results, you can run the <kbd>head()</kbd> function against this DataFrame using this code:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_hits.head()</pre>
<p style="padding-left: 60px"><span>The output will look like the following screenshot,</span><span> where the SQL results have been loaded into a DataFrame with a labeled header row with the index column to the left starting with a value of <kbd>0</kbd>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ac631f3a-a94f-456f-ba70-eadd2e708bb5.png" style="width:15.42em;height:10.08em;"/></div>
<ol start="10">
<li>
<p>Create a new DataFrame called <kbd>df_user_geo</kbd>, which is a duplicate of the <kbd>tbl_user_geo</kbd> table, so we can use it later in the examples:</p>
</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_geo = pd.read_sql_query("select * from tbl_user_geo;", conn)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p style="padding-left: 60px"><span>The output would look like the following screenshot, </span><span>where the SQL results have been loaded into a DataFrame with a labeled header row with the index column to the left starting with a value of <kbd>0</kbd>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/454ba42c-acad-44f2-ba97-6c157bbff459.png" style="width:17.58em;height:11.17em;"/></p>
<ol start="11">
<li>It's a best practice to close the database connection since we no longer need to run any SQL queries and have retrieved all of the data. You would run this command to close it:</li>
</ol>
<pre style="padding-left: 60px">In[]: conn.close()</pre>
<p style="padding-left: 60px">Now that we have all of the data loaded into pandas DataFrames, we can walk through the different join types by slightly modifying the parameters in the <kbd>merge()</kbd> function. The <kbd>left</kbd> and <kbd>right</kbd> parameters for all of the examples will be <kbd>df_user_hits</kbd> and <kbd>df_user_geo</kbd> respectively. The join fields are consistent, which is <kbd>userid</kbd> for both DataFrames. In this example, the source tables use the same common field name for their unique identifier, which is helpful. </p>
<p style="padding-left: 60px">The last parameter we will pass into the <kbd>merge()</kbd> function is named <kbd>how</kbd> which determines which type of join will be performed. We will start with my favorite, which is the left join.</p>
<ol start="12">
<li>Create a new DataFrame with the results of the pandas <kbd>merge()</kbd> function that creates a left join between the two DataFrames. In the next line, you can include the new DataFrame name, which will output the results similar to using the <kbd>head()</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_left_join=pd.merge(left=df_user_hits, right=df_user_geo, how='left', left_on='userid', right_on='userid')<br/>df_left_join</pre>
<p style="padding-left: 60px"><span>The output will look like the following screenshot, where the <kbd>merge()</kbd> results have been loaded into a new DataFrame named <kbd>df_left_join</kbd> with a labeled header row with the index column to the left starting with a value of <kbd>0</kbd>:</span></p>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7889a052-b564-40a4-9249-6b68798c6107.png" style="width:20.58em;height:8.08em;"/></div>
<p style="padding-left: 60px">The expected result, only <kbd>userid</kbd> from <kbd>df_user_hits</kbd>, will be displayed.</p>
<div class="packt_tip">Notice the difference between SQL and pandas, where the blank <kbd>null()</kbd> values are replaced with <strong>NaN</strong>, which stands for <strong>Not a Number</strong>.</div>
<p style="line-height: 24pt;padding-left: 60px"><span>Next, we will create a right join, which has a slight variation to our prior syntax.<br/></span></p>
<ol start="13">
<li>Create a new DataFrame with the results of the pandas <kbd>merge()</kbd> function that creates a right join between the two DataFrames. In the next line, you can include the new DataFrame name, which will output the results similar to using the <kbd>head()</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_right_join=pd.merge(left=df_user_hits, right=df_user_geo, how='right', left_on='userid', right_on='userid')<br/>df_right_join</pre>
<p style="padding-left: 60px"><span>The output will look like the following screenshot,</span><span> where the merge results have been loaded into a new DataFrame named <kbd>df_right_join</kbd> with a labeled header row with the index column to the left starting with a value of <kbd>0</kbd>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b8785b52-6adf-48be-a8aa-c33c844a14f2.png" style="width:20.67em;height:8.08em;"/></p>
<p style="padding-left: 60px">The expected result, only <kbd>userid</kbd> from <kbd>df_user_geo</kbd>, will be displayed.</p>
<p style="padding-left: 60px">Similar to our SQL example, we can perform an inner join using the merge function by using the default, which excludes the <kbd>how</kbd> parameter when passing it to the <kbd>merge()</kbd> function.</p>
<p class="mce-root"/>
<div class="packt_tip">You can always explicitly include the <kbd>how</kbd> parameter if you want to be sure to define the type of join used.</div>
<ol start="14">
<li>Create a new DataFrame with the results of the pandas <kbd>merge()</kbd> function that creates an inner join between the two DataFrames. In the next line, you can include the new DataFrame name, which will output the results similar to using the <kbd>head()</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_inner_join=pd.merge(left=df_user_hits, right=df_user_geo, left_on='userid', right_on='userid')<br/>df_inner_join</pre>
<p style="padding-left: 60px"><span>The output will look like the following screenshot, </span><span>where the merge results have been loaded into a new DataFrame named </span><kbd>df_inner_join</kbd><span> with a labeled header row with the index column to the left starting with a value of <kbd>0</kbd>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ebb1e35e-2d23-4da9-8583-8c06f83f8c8c.png" style="width:20.08em;height:6.00em;"/></p>
<p style="padding-left: 60px">The expected result, only <kbd>userid</kbd> that exists in both DataFrames, will be displayed.</p>
<p style="padding-left: 60px">Finally, let's create an outer join using the <kbd>merge()</kbd> function by adding the <kbd>how</kbd> parameter and including the value of <kbd>outer</kbd>.</p>
<ol start="15">
<li>Create a new DataFrame with the results of the pandas <kbd>merge()</kbd> function that creates an outer join between the two DataFrames. In the next line, you can include the new DataFrame name, which will output the results similar to using the <kbd>head()</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_outer_join=pd.merge(left=df_user_hits, right=df_user_geo, how='outer', left_on='userid', right_on='userid')<br/>df_outer_join</pre>
<p style="padding-left: 60px"><span>The output will look like the following screenshot, </span><span>where the merge results have been loaded into a new DataFrame named <kbd>df_outer_join</kbd> with a labeled header row with the index column to the left starting with a value of <kbd>0</kbd>:</span></p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0d1dbcf8-85f4-4356-9fa9-a6d0b92b353a.png" style="width:18.50em;height:8.92em;"/></p>
<p style="padding-left: 60px">The expected result, all <kbd>userid</kbd> instances that exist in either DataFrame, will be displayed.</p>
<p>Excellent, we have successfully recreated all of the join types discussed throughout this chapter so far. Whether you feel more comfortable using SQL or <kbd>pandas</kbd>, the ability to join datasets together is a powerful skill and significantly increases your data literacy acumen.</p>
<h1 id="uuid-16b245c1-0157-4487-bc02-5a4f66370684">Explaining data aggregation</h1>
<p class="mce-root">Data aggregation is part of your daily life and you may or may not even realize it. When you pull up a review of a restaurant that uses one to five stars or if you purchase an item on Amazon because it has thousands of customer ratings, both examples are data aggregates. A data aggregate can be defined as a summary typically based on a significantly larger detail. In SQL, an aggregation is when a <kbd>groupby</kbd> command is applied against one or more tables, which includes a statistical calculation such as sum, average, min, or max against one or more fields.</p>
<h2 id="uuid-9db7da57-3e07-447f-a96c-faeac7b7be81">Understanding the granularity of data</h2>
<p class="mce-root">The aggregation of calculations would be known as the measure. When you are grouping by one or more fields to get their distinct values, they are classified as dimensions.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">So, this should all sound very familiar because we introduced the concept of dimensions and measures in both <a href="bea8a62c-4469-47e3-a668-10fbb91815ea.xhtml">Chapter 5</a>, <em>Gathering and Loading Data in Python</em>, and <a href="6e8b782b-2dad-4b16-9bba-73cd644e9529.xhtml">Chapter 6</a>, <em>Visualizing and Working with Time Series Data</em>, because it's the foundation for data modeling and visualizations. To reinforce the concept, let's see how a table or DataFrame gets summarized visually. As you can see in the following screenshot, an input table with any number of rows and columns can be summarized and reduced by many different types of aggregations, such as by a user or by date:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-819 image-border" src="assets/5a3cf2de-9d89-4e80-9b1f-ce54846e022c.png" style="width:48.67em;height:19.00em;"/></p>
<p class="mce-root">So, why are aggregations needed and important to analytics? First, as you can see in the preceding screenshot, the shape and size are significantly reduced, which helps large datasets to be manageable for ease of consumption by humans or machines.</p>
<p class="mce-root">For humans, when data is aggregated, it becomes easy to view and understand because the person does not have to visually sift through thousands of rows and columns. For machines, the reduction of very large data sources in both shape and size helps to reduce the file size, memory footprint, and input/output to process the data. When you see the sizes of structured data in <strong>gigabytes</strong> (<strong>GB</strong>), the main factor impacting that size (either file, DataFrame, or database table) is the density of the data. The density of data is defined by each data point value within the rows and columns of the table or source file. When you aggregate data, the volume of data will be significantly reduced because one or more of the fields containing the distinct values are removed.<span> </span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"><span>For example, if you have a high volume transaction table with millions of distinct <kbd>userid</kbd> values by timestamp, the daily number of records could be in the tens of millions if a user performs an action or event every few seconds. </span>If your analysis requirement is to measure the average count of the number of users per day, you could create a daily snapshot table that has one row for each day. So, a simple aggregation reduced tens of millions of rows down to one per day! So, what's the catch? Well, first we lost all of the granularity of what actions each user was performing on each day. Another factor is the time to process and manage the aggregate table. Any data engineer will tell you that downtime and bugs occur, so they must keep the source table and any aggregate tables in sync and reinstate if the source changes.</p>
<p class="mce-root">To solve for any loss of granularity, you can create other aggregates based on different fields/dimensions, but that may create other problems if you add more attributes later after the aggregate table is created. For example, if you snapshot an aggregate table with average daily user counts by city and state for a year and then want the analysis by zip code, you would have to reprocess to backfill all of the history or have two different average <span>granularities </span>that change before and after a specific date.</p>
<h2 id="uuid-5afa393c-67ac-4a13-8f77-4922f66191da"><span>Data aggregation </span>in action</h2>
<p class="mce-root">So, now that we have a better understanding of what aggregates exist and why, let's walk through how to create them in SQL and pandas. For this example, we will be working with a similar version of the user hits data named <kbd>tbl_user_geo_hits</kbd>. This table has the combined records from the sources we have been working with before, except we can now focus on the aggregation and <kbd>groupby</kbd> syntax. The SQL language can be complex and is robust enough to handle both joins and aggregation at the same time, but I find breaking down the process will make it easier to learn. Also, it is common to have persisted tables or views (database objects that behave like a table but are derived from one or more joins) available because of high data volumes and/or reporting.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We will begin by launching a new Jupyter notebook and naming it <kbd>ch_08_sql_and_pandas_group_by</kbd>: </p>
<ol>
<li>Load a SQLite database connection:</li>
</ol>
<pre style="padding-left: 60px">In[]: import sqlite3</pre>
<p style="padding-left: 60px"><span>This library should already be available using Anaconda. Refer to </span><a href="e0fe6eb2-8f38-41f7-9dea-2b177578fd3c.xhtml">Chapter 2</a>,<em> Overview of Python and Installing Jupyter Notebook</em>,<span> for help with setting up your environment.</span></p>
<ol start="2">
<li>Next, we need to assign a connection to a variable named <kbd>conn</kbd> and point to the location of the database file, which is named <kbd>user_hits.db</kbd>. Since we already imported the <kbd>sqlite3</kbd> library in the prior <kbd>In[]</kbd><strong> </strong>line, we can use this built-in function to communicate with the database:</li>
</ol>
<div>
<pre style="padding-left: 60px">In[]: conn = sqlite3.connect('user_hits.db')</pre>
<p style="padding-left: 60px">Be sure that you copied the <kbd>user_hits.db</kbd> file to the correct Jupyter folder directory to avoid errors with the connection.</p>
<ol start="3">
<li>Import the <kbd>pandas</kbd> library:</li>
</ol>
<pre style="padding-left: 60px">In[]: import pandas as pd</pre></div>
<ol start="4">
<li>To run a SQL statement and assign the results to a DataFrame, we have to run this one line of code. The <kbd>pandas</kbd> library includes a <kbd>read_sql_query</kbd> function to make it easier to communicate with databases using SQL. It requires a connection parameter that we named <kbd>conn</kbd> in the previous steps. We assign the results to a new DataFrame as <kbd>df_user_geo_hits</kbd> to make it easier to identify:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_geo_hits = pd.read_sql_query("select * from tbl_user_geo_hits;", conn)</pre>
<ol start="5">
<li>To validate the results, you can run the <kbd>head()</kbd> function against this DataFrame using this code:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_user_geo_hits.head(10)</pre>
<div>
<p style="padding-left: 60px">The output will look like the following screenshot, where the merge results have been loaded into a new DataFrame named <kbd>df_user_geo_hits</kbd> with a labeled header row with the index column to the left starting with a value of <kbd>0</kbd>:</p>
</div>
<p class="CDPAlignCenter CDPAlign"><img src="assets/633c07ac-0826-4d8e-b36c-2a21e77f14b9.png" style="width:21.33em;height:14.75em;"/></p>
<div>
<p style="padding-left: 60px">So, we have user hits available and have previewed the data by loading it into a DataFrame. The advantage of a group by feature is that it allows us to ask specific questions about the data and return answers with some slight adjustments to the dimensions and aggregation used in the SQL syntax. How many user hits occurred by city and state across all time? To answer this question, let's identify what dimensions and measures are required. The dimensions are <kbd>city</kbd> and <kbd>state</kbd> and the measure is an aggregation created by counting the frequency of the occurrence of the number of records, which is represented by the function of <kbd>count (*)</kbd>. Since we have all of this information that we need in a single table, no join is required.</p>
</div>
<div class="packt_tip">If we only included the one dimension of <kbd>city</kbd>, then we would combine any duplicate <kbd>city</kbd> names and misrepresent the data. For example, the city of <kbd>Dover</kbd> exists in multiple states, such as Delaware and New Jersey. This is where the granularity of the data could be lost by not including the right fields in the group by aggregation.</div>
<ol start="6">
<li>To run a SQL statement and assign the results to a DataFrame, we have to run this one line of code. The <kbd>pandas</kbd> library includes a <kbd>read_sql_query()</kbd> function to make it easier to communicate with databases using SQL. It requires a connection parameter that we named <kbd>conn</kbd> in the previous steps. We assign the results to a new DataFrame as <kbd>df_groupby_SQL</kbd> to make it easier to identify:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_groupby_SQL=pd.read_sql_query("select city, state, count(*) as hits from tbl_user_geo_hits group by 1, 2;", conn)</pre>
<ol start="7">
<li>To validate the results, you can run the <kbd>head()</kbd> function against this DataFrame using this code:</li>
</ol>
<pre style="padding-left: 60px">[]: df_groupby_SQL.head()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p style="padding-left: 60px"><span>The output will look like the following screenshot,</span><span> where the merge results have been loaded into a new DataFrame named <kbd>df_groupby_SQL</kbd> with a labeled header row with the index column to the left starting with a value of <kbd>0</kbd>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3121f316-28d2-47eb-8c6c-bf0f451b4a43.png" style="width:17.42em;height:9.83em;"/></div>
<p style="padding-left: 60px">The SQL language supports shortcuts, so we use <kbd>group by 1, 2</kbd> to represent the two-dimensional fields of <kbd>city</kbd> and <kbd>state</kbd>. It also allows alias of field names, so <kbd>count(*) as hits</kbd> is used to make it easier to represent.</p>
<p style="padding-left: 60px">To recreate the SQL results using <kbd>pandas</kbd>, we can use the DataFrame we loaded and use the <kbd>groupby()</kbd> function with a few parameters. In our example, we pass the name of the columns we want to group by, which is both <kbd>city</kbd> and <kbd>state</kbd>. The measure will be similar to before by including <kbd>.count()</kbd>, and we include the field you want to perform the aggregation, which can be any field because we are counting the frequency. We use <kbd>userid</kbd> since the analysis is focused on the user.</p>
<ol start="8">
<li>To run a SQL statement and assign the results to a DataFrame, we have to run this one line of code. The <kbd>pandas</kbd> library includes a <kbd>read_sql_query</kbd> function to make it easier to communicate with databases using SQL. It requires a connection parameter that we named <kbd>conn</kbd> in the previous steps. We assign the results to a new DataFrame as <kbd>df_groupby_city_state</kbd> to make it easier to identify:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_groupby_city_state=df_user_geo_hits.groupby(["city", "state"]) ["userid"].count()</pre>
<ol start="9">
<li>To validate the results, you can run the <kbd>head()</kbd> function against this DataFrame using this code:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_groupby_city_state.head()</pre>
<p style="padding-left: 60px"><span>The output will look like the following screenshot,</span><span> where the merge results have been loaded into a new DataFrame named <kbd>df_groupby_city_state</kbd>:</span></p>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f6752132-adb0-4155-9c1e-aa53d368e346.png" style="width:20.67em;height:8.92em;"/></div>
<ol start="10">
<li>It's a best practice to close the database connection since we no longer need to run any SQL queries and have retrieved all of the data. You would run this command to close it:</li>
</ol>
<pre style="padding-left: 60px">In[]: conn.close()</pre>
<p>So, we have demonstrated the power of summarizing data by using an aggregation group in both <kbd>pandas</kbd> and SQL. Aggregation can be performed against a single table or a join between multiple tables. The common elements between using SQL or <kbd>pandas</kbd> are defining the dimension and measures, which are abstracted from the fields available in either a DataFrame or SQL object (table, join, or view). So far, we have only scratched the surface using one measure type, which has been count. There are more statistical functions available for data analysis. So, next, we will explore the differences between mean, median, and mode.</p>
<h1 id="uuid-d25399e7-8fb2-4c87-bfd1-f8d23870d188">Summary statistics and outliers</h1>
<p class="mce-root">We touched on the necessity of fundamental statistics when working with data in <a href="bea8a62c-4469-47e3-a668-10fbb91815ea.xhtml">Chapter 5</a>, <em>Gathering and Loading Data in Python</em>.<q> </q>Let's walk through the differences between mean, median, and mode in statistics as it applies to data analysis. The mean or average is when you sum the values of numeric values in a series divided by the count of those same numbers. The mean or average is a measure in analytics and is typically used to gauge performance over a period of time and define a comparison for each period of time. </p>
<p class="mce-root">For example, you see average daily temperatures all the time in the news—how is that calculated? Depending on your geographic location, <span>the weather will have the temperature</span> recorded in specific increments, such as hours. The <strong>National Oceanic and Atmospheric Administration</strong> (<strong><span>NOAA</span></strong>), for example, uses stations and a scientific approach to calculate the minimum and maximum temperature values for each day and location. Those individual records are then used to <span>create an average monthly temperature and 30-year averages by month.</span></p>
<p class="mce-root"/>
<div class="packt_tip">Be sure to understand the lineage of your data, especially when working with averages, because an average of an average will limit how you can work with the data and is most likely a misuse of the metric. Ideally, have the lowest level of detail so you can re-calculate an average based on any period of time desired.</div>
<p>For the example that we have been working with throughout this chapter, calculating the average or mean would provide a standard that we can use for comparison. So, if we want to know the average number of hits per day, all we have to do is count all of the records and divide that by the distinct count of the <span>values of </span>date. From the <kbd>tbl_user_geo_hits</kbd> data, the average would be 2.3 hits per day because you had seven records with three distinct days. We can now use this as a litmus test to measure when each day has significant increases or decreases when compared to that mean value.</p>
<p>The median, a central value across a series of values, can be determined by finding the middle value, where fifty percent of the values are greater than or less than that specific data value. It is common to have those values ordered first to make it easier to identify that middle value. Identifying the mean is good for measuring the central tendency, which helps you to identify how the data values are distributed and is not affected by outlier data points. We will explore the shape of distributions in <a href="e3570c4a-c8ad-483f-9f3f-3e113156e9c2.xhtml">Chapter 9</a>, <em>Plotting, Visualization, and Storytelling</em><q>.</q></p>
<p class="CDPAlignLeft CDPAlign">Finally, we have the mode, which is the value in a series that occurs the most frequently. The mode is less commonly used in analytics and analysis but is useful when identifying outliers in your data. What is an outlier? Mathematically, it could be defined as a value that is multiple standard deviations from the mean. Practically, it's when a value or aggregated value is out of the normal pattern as compared to the rest of the data. A good way to visually identify outliers is to plot the values against a normal distribution or what is commonly known as a <em>bell curve</em>, which is represented in the following diagram as a dotted black line:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/27778f95-265a-42de-8fcb-27ad2d17be2f.png" style="width:26.58em;height:14.75em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>As you can see in the preceding diagram, the distribution line follows a shape based on the mean and standard deviation. A true normal distribution would have values where the mean, median, and mode are all equal. Left and right of that peak at the top of the line are based on one, two, or three standard deviations. A value plus or minus three standard deviations would represent only 99.7% of the population of values. </p>
<p>In practical terms, we are saying less than one percent of all of the data would be toward the min and max values. Having your data in a normal distribution may or may not be relevant but having this understanding helps to identify the shape of your data values. In the preceding example, there are red dots that fall significantly below and above the line, which are considered outliers. Having this visual representation of the outliers as compared to the normal distribution will help you as a data analyst to have a conversation with others about the data. This is the power of data literacy, where a visual representation can spark dialog and help to answer questions about what is expected or normal and what is an exception or an outlier.</p>
<h1 id="uuid-7a2a2a22-c584-4c31-8ddd-73e3d7cec744" class="mce-root">Summary</h1>
<p class="mce-root">Congratulations, we have covered the fundamentals of joining and merging data in both SQL and Python using pandas DataFrames. Throughout the process, we discussed practical examples of which joins to use along with why you should use them against user hits data. Enriching our data by blending multiple data tables allows deeper analysis and the ability to answer many more questions about the original single data source. After learning about joins and the <kbd>merge()</kbd> function, we uncovered the advantages and disadvantages of data aggregation. We walked through practical examples of using the <kbd>groupby</kbd> feature in both SQL and DataFrames. We walked through the differences between statistical functions and mean, median, and mode, along with tips for finding outliers in your data by comparing results to a normal distribution bell curve.</p>
<p class="mce-root">In our next chapter, we will be heading back to using plot libraries and visualizing data.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<h1 id="uuid-a0e8dd55-3b6d-49ac-a330-816d647e8e6d" class="mce-root">Further reading<a href="https://matplotlib.org/index.html"/></h1>
<p>For more information on the topics of this chapter, you can refer to the following links:</p>
<ul>
<li>Guide to using the merge function: <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html</a></li>
<li>NOAA weather data: <a href="https://www.ncdc.noaa.gov/cdo-web/datatools/records">https://www.ncdc.noaa.gov/cdo-web/datatools/records</a></li>
<li>SQL join types: <a href="https://www.w3schools.com/sql/sql_join.asp">https://www.w3schools.com/sql/sql_join.asp</a></li>
<li>Data Literacy Project – understanding aggregations: <a href="https://thedataliteracyproject.org/learn">http://qcc.qlik.com/mod/url/view.php?id=5268</a></li>
</ul>


            </article>

            
        </section>
    </body></html>