<html><head></head><body>
		<div id="_idContainer027">
			<h1 id="_idParaDest-163" class="chapter-number"><a id="_idTextAnchor164"/>6</h1>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor165"/>SQL Queries in Spark</h1>
			<p>In this chapter, we will explore the vast capabilities of Spark SQL for structured data processing. We will dive into loading and manipulating data, executing SQL queries, performing advanced analytics, and integrating Spark SQL with external systems. By the end of this chapter, you will have a solid understanding of Spark SQL’s features and be equipped with the knowledge to leverage its power in your data <span class="No-Break">processing tasks.</span></p>
			<p>We will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>What is <span class="No-Break">Spark SQL?</span></li>
				<li>Getting Started with <span class="No-Break">Spark SQL</span></li>
				<li>Advanced Spark <span class="No-Break">SQL operations</span></li>
			</ul>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor166"/>What is Spark SQL?</h1>
			<p>Spark SQL is a powerful module within the Apache Spark ecosystem that allows for the efficient processing <a id="_idIndexMarker424"/>and analysis of structured data. It provides a higher-level interface for working with structured data compared to the traditional RDD-based API of Apache Spark. Spark SQL combines the benefits of both relational and procedural processing, enabling users to seamlessly integrate SQL queries with complex analytics. By leveraging Spark’s distributed computing capabilities, Spark SQL enables scalable and high-performance <span class="No-Break">data processing.</span></p>
			<p>It provides a programming interface to work with structured data using SQL queries, DataFrame API, and <span class="No-Break">Datasets API.</span></p>
			<p>It allows users to query data using SQL-like syntax and provides a powerful engine for executing SQL queries on large datasets. Spark SQL also supports reading and writing data from various structured sources such as Hive tables, Parquet files, and <span class="No-Break">JDBC databases.</span></p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor167"/>Advantages of Spark SQL</h2>
			<p>Spark SQL offers <a id="_idIndexMarker425"/>several key advantages that make it a popular choice for structured <span class="No-Break">data processing:</span></p>
			<h3>Unified data processing with Spark SQL</h3>
			<p>With Spark SQL, users can process both structured and unstructured data using a single engine. This <a id="_idIndexMarker426"/>means that users can use the <a id="_idIndexMarker427"/>same programming interface to query data stored in different formats such as JSON, CSV, <span class="No-Break">and Parquet.</span></p>
			<p>Users can seamlessly switch between SQL queries, DataFrame transformations, and Spark’s machine learning APIs. This unified data processing approach allows for the easier integration of different data processing tasks within a single application, reducing <span class="No-Break">development complexity.</span></p>
			<h3>Performance and scalability</h3>
			<p>Spark SQL leverages the distributed computing capabilities of Apache Spark, enabling the processing <a id="_idIndexMarker428"/>of large-scale datasets across a cluster of machines. It utilizes advanced query optimization techniques, such a the Catalyst optimizer (discussed in detail in <a href="B19176_05.xhtml#_idTextAnchor115"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>), to optimize and accelerate query execution. Additionally, Spark SQL supports data partitioning and caching mechanisms, further enhancing performance <span class="No-Break">and scalability.</span></p>
			<p>Spark SQL uses an optimized execution engine that can process queries much faster than traditional SQL engines. It achieves this by using in-memory caching and optimized query <span class="No-Break">execution plans.</span></p>
			<p>Spark SQL is designed to scale horizontally across a cluster of machines. It can handle large datasets by partitioning them across multiple machines and processing them <span class="No-Break">in parallel.</span></p>
			<h3>Seamless integration with existing infrastructure</h3>
			<p>Spark SQL integrates seamlessly with existing Apache Spark infrastructure and tools. It provides <a id="_idIndexMarker429"/>interoperability with other Spark components, such as Spark Streaming for real-time data processing and Spark MLlib for machine learning tasks. Furthermore, Spark SQL integrates with popular storage systems and data formats, including Parquet, Avro, ORC, and Hive, making it compatible with a wide range of <span class="No-Break">data sources.</span></p>
			<h3>Advanced analytics capabilities</h3>
			<p>Spark SQL extends traditional SQL capabilities by using advanced analytics features. It supports <a id="_idIndexMarker430"/>window functions, which enable users to perform complex analytical operations, such as ranking, aggregation over sliding windows, and cumulative aggregations. The integration with machine learning libraries in Spark allows for the seamless integration of predictive analytics and data <span class="No-Break">science workflows.</span></p>
			<h3>Ease of use</h3>
			<p>Spark SQL <a id="_idIndexMarker431"/>provides a simple programming interface that allows users to query data using SQL-like syntax. This makes it easy for users who are familiar with SQL to get started with <span class="No-Break">Spark SQL.</span></p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor168"/>Integration with Apache Spark</h2>
			<p>Spark SQL is an integral part of the Apache Spark framework and works seamlessly with other Spark <a id="_idIndexMarker432"/>components. It leverages Spark’s core functionalities, such as fault tolerance, data parallelism, and distributed computing, to provide scalable and efficient data processing. Spark SQL can read data from a variety of sources, including distributed file systems (such as HDFS), object stores (like Amazon S3), and relational databases (via JDBC). It also integrates with external systems such as Hive, allowing users to leverage existing Hive metadata <span class="No-Break">and queries.</span></p>
			<p>Now let’s take a look at some basic constructs of <span class="No-Break">Spark SQL.</span></p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor169"/>Key concepts – DataFrames and datasets</h2>
			<p>Spark SQL <a id="_idIndexMarker433"/>introduces two fundamental abstractions for working with structured data: DataFrames <span class="No-Break">and Datasets.</span></p>
			<h3>DataFrames</h3>
			<p>DataFrames represent distributed collections of data organized into named columns. They provide <a id="_idIndexMarker434"/>a higher-level interface for working with <a id="_idIndexMarker435"/>structured data and offer rich APIs for data manipulation, filtering, aggregation, and querying. DataFrames are immutable and lazily evaluated, enabling optimized execution plans through Spark’s Catalyst optimizer. They can be created from various data sources, including structured files (CSV, JSON, and Parquet), Hive tables, and <span class="No-Break">existing RDDs.</span></p>
			<h3>Datasets</h3>
			<p>Datasets are an extension of DataFrames and provide a type-safe, object-oriented programming <a id="_idIndexMarker436"/>interface. Datasets combine the benefits of Spark’s RDDs (strong typing and user-defined functions) with the performance <a id="_idIndexMarker437"/>optimizations of DataFrames. Datasets enable compile-time type checking and can be seamlessly converted to DataFrames, allowing for flexible and efficient <span class="No-Break">data processing.</span></p>
			<p>Now that we know what DataFrames and Datasets are, we’ll see how to apply different Spark SQL operations to these constructs in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor170"/>Getting started with Spark SQL</h1>
			<p>To get started <a id="_idIndexMarker438"/>with Spark SQL operations, we would first need to load data into a DataFrame. We’ll see how to do that next. Then, we will see how we can switch between PySpark and Spark SQL data and apply different transformations <span class="No-Break">to it.</span></p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor171"/>Loading and saving data</h2>
			<p>In this <a id="_idIndexMarker439"/>section, we will explore various techniques <a id="_idIndexMarker440"/>for loading data into Spark SQL from different sources and saving this as a table. We will delve into Python code examples that demonstrate how to effectively load data into Spark SQL, perform the necessary transformations, and save the processed data as a table for <span class="No-Break">further analysis.</span></p>
			<p>Executing SQL queries in Spark SQL allows us to leverage the familiar SQL syntax and take advantage of its expressive power. Let’s take a look at the syntax and an example of executing an SQL query using <span class="No-Break">Spark SQL:</span></p>
			<p>To execute <a id="_idIndexMarker441"/>an SQL query in Spark SQL, we use the <strong class="source-inline">spark.sql()</strong> method <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
results = spark.sql("SELECT * FROM tableName")</pre>			<ul>
				<li>The <strong class="source-inline">spark.sql()</strong> method is used to execute SQL queries in <span class="No-Break">Spark SQL</span></li>
				<li>Inside the method, we provide the SQL query as a string argument. In this example, we select all columns from the <span class="No-Break"><strong class="source-inline">tableName</strong></span><span class="No-Break"> table</span></li>
				<li>The results of the query are stored in the <strong class="source-inline">results</strong> variable, which can be further processed or displayed <span class="No-Break">as desired</span></li>
			</ul>
			<p>To start with <a id="_idIndexMarker442"/>code examples in this chapter, we will use the DataFrame we created in <a href="B19176_04.xhtml#_idTextAnchor071"><span class="No-Break"><em class="italic">Chapter 4</em></span></a><span class="No-Break">:</span></p>
			<pre class="source-code">
salary_data_with_id = [(1, "John", "Field-eng", 3500, 40), \
    (2, "Robert", "Sales", 4000, 38), \
    (3, "Maria", "Finance", 3500, 28), \
    (4, "Michael", "Sales", 3000, 20), \
    (5, "Kelly", "Finance", 3500, 35), \
    (6, "Kate", "Finance", 3000, 45), \
    (7, "Martin", "Finance", 3500, 26), \
    (8, "Kiran", "Sales", 2200, 35), \
  ]
columns= ["ID", "Employee", "Department", "Salary", "Age"]
salary_data_with_id = spark.createDataFrame(data = salary_data_with_id, schema = columns)
salary_data_with_id.show()</pre>			<p>The output will be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
+---+--------+----------+------+---+
| ID|Employee|Department|Salary|Age|
+---+--------+----------+------+---+
|  1|    John| Field-eng|  3500| 40|
|  2|  Robert|     Sales|  4000| 38|
|  3|   Maria|   Finance|  3500| 28|
|  4| Michael|     Sales|  3000| 20|
|  5|   Kelly|   Finance|  3500| 35|
|  6|    Kate|   Finance|  3000| 45|
|  7|  Martin|   Finance|  3500| 26|
|  8|   Kiran|     Sales|  2200| 35|
+---+--------+----------+------+---+</pre>			<p>I have added an <strong class="source-inline">Age</strong> column in this DataFrame for <span class="No-Break">further processing.</span></p>
			<p>Remember, in <a href="B19176_04.xhtml#_idTextAnchor071"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, we saved this DataFrame as a CSV file. The code snippet we used to read CSV files can be seen in the <span class="No-Break">following code.</span></p>
			<p>As you <a id="_idIndexMarker443"/>might recall, we write CSV files with Spark using this line <span class="No-Break">of code:</span></p>
			<pre class="source-code">
salary_data_with_id.write.format("csv").mode("overwrite").option("header", "true").save("salary_data.csv")</pre>			<p>The output <a id="_idIndexMarker444"/>will be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
+---+--------+----------+------+---+
| ID|Employee|Department|Salary|Age|
+---+--------+----------+------+---+
|  1|    John| Field-eng|  3500| 40|
|  2|  Robert|     Sales|  4000| 38|
|  3|   Maria|   Finance|  3500| 28|
|  4| Michael|     Sales|  3000| 20|
|  5|   Kelly|   Finance|  3500| 35|
|  6|    Kate|   Finance|  3000| 45|
|  7|  Martin|   Finance|  3500| 26|
|  8|   Kiran|     Sales|  2200| 35|
+---+--------+----------+------+---+</pre>			<p>Now that <a id="_idIndexMarker445"/>we have the DataFrame, we can use SQL <a id="_idIndexMarker446"/>operations <span class="No-Break">on it:</span></p>
			<pre class="source-code">
# Perform transformations on the loaded data
processed_data = csv_data.filter(csv_data["Salary"] &gt; 3000)
# Save the processed data as a table
processed_data.createOrReplaceTempView("high_salary_employees")
# Perform SQL queries on the saved table
results = spark.sql("SELECT * FROM high_salary_employees ")
results.show()</pre>			<p>The output will be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
+---+--------+----------+------+---+
| ID|Employee|Department|Salary|Age|
+---+--------+----------+------+---+
|  1|    John| Field-eng|  3500| 40|
|  2|  Robert|     Sales|  4000| 38|
|  3|   Maria|   Finance|  3500| 28|
|  5|   Kelly|   Finance|  3500| 35|
|  7|  Martin|   Finance|  3500| 26|
+---+--------+----------+------+---+</pre>			<p>The preceding code snippet shows how to perform a transformation on the loaded data. In this case, we filter <a id="_idIndexMarker447"/>the data to only include rows where <a id="_idIndexMarker448"/>the <strong class="source-inline">Salary</strong> column is greater <span class="No-Break">than 3,000.</span></p>
			<p>By using the <strong class="source-inline">filter()</strong> function, we can apply specific conditions to select the desired subset <span class="No-Break">of data.</span></p>
			<p>The transformed data will be stored in the results variable and ready for <span class="No-Break">further analysis.</span></p>
			<h3>Saving transformed data as a view</h3>
			<p>Once we have performed the necessary transformations, it is often useful to save the processed <a id="_idIndexMarker449"/>data as a view for easier access and future analysis. Let’s see how we can accomplish this in <span class="No-Break">Spark SQL:</span></p>
			<p>The <strong class="source-inline">createOrReplaceTempView()</strong> method allows us to save the processed data as a <a id="_idIndexMarker450"/>view in Spark SQL. We provide a name for the view, in this <span class="No-Break">case, </span><span class="No-Break"><strong class="source-inline">high_salary_employees</strong></span><span class="No-Break">.</span></p>
			<p>By giving the table a meaningful name, we can easily refer to it in subsequent operations and queries. The saved table acts as a structured representation of the processed data, facilitating further analysis <span class="No-Break">and exploration.</span></p>
			<p>With the transformed data saved as a table, we can leverage the power of SQL queries to gain insights and extract <span class="No-Break">valuable information.</span></p>
			<p>By using <a id="_idIndexMarker451"/>the <strong class="source-inline">spark.sql()</strong> method, we can execute SQL queries on the saved <span class="No-Break">view </span><span class="No-Break"><strong class="source-inline">high_salary_employees</strong></span><span class="No-Break">.</span></p>
			<p>In the preceding example, we perform a simple query to select all columns from the view based on a <span class="No-Break">filter condition.</span></p>
			<p>The <strong class="source-inline">show()</strong> function displays <a id="_idIndexMarker452"/>the results of the SQL query, allowing us to examine the desired information extracted from <span class="No-Break">the dataset.</span></p>
			<p>Another method to create a view in Spark SQL is <strong class="source-inline">createTempView()</strong>. The difference between this method and <strong class="source-inline">createOrReplaceTempView()</strong> method is that <strong class="source-inline">createTempView()</strong> would only try to create a view. If that view name already exists in a catalog, then it would throw a <span class="No-Break"><strong class="source-inline">TempTableAlreadyExistsException</strong></span><span class="No-Break"> exception.</span></p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor172"/>Utilizing Spark SQL to filter and select data based on specific criteria</h2>
			<p>In this section, we will explore the syntax and practical examples of executing SQL queries <a id="_idIndexMarker453"/>and applying transformations using <span class="No-Break">Spark SQL.</span></p>
			<p>Let’s <a id="_idIndexMarker454"/>consider a practical example where we execute an SQL query to filter and select specific data from <span class="No-Break">a table:</span></p>
			<pre class="source-code">
# Save the processed data as a view
salary_data_with_id.createOrReplaceTempView("employees")
#Apply filtering on data
filtered_data = spark.sql("SELECT Employee, Department, Salary, Age FROM employees WHERE age &gt; 30")
# Display the results
filtered_data.show()</pre>			<p>The output will be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
+--------+----------+------+---+
|Employee|Department|Salary|Age|
+--------+----------+------+---+
|    John| Field-eng|  3500| 40|
|  Robert|     Sales|  4000| 38|
|   Kelly|   Finance|  3500| 35|
|    Kate|   Finance|  3000| 45|
|   Kiran|     Sales|  2200| 35|
+--------+----------+------+---+</pre>			<p>In this example, we create a temp view with the names of <strong class="source-inline">employees</strong> and execute an SQL query using Spark SQL to filter and select specific columns from the <span class="No-Break"><strong class="source-inline">employees</strong></span><span class="No-Break"> table.</span></p>
			<p>The query <a id="_idIndexMarker455"/>selects the <strong class="source-inline">employee</strong>, <strong class="source-inline">department</strong>, <strong class="source-inline">salary</strong>, and <strong class="source-inline">age</strong> columns from the table where <strong class="source-inline">age</strong> is greater than 30. The <a id="_idIndexMarker456"/>results of the query are stored in the <span class="No-Break"><strong class="source-inline">filtered_data</strong></span><span class="No-Break"> variable.</span></p>
			<p>Finally, we call the <strong class="source-inline">show()</strong> method to display the <span class="No-Break">filtered data.</span></p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor173"/>Exploring sorting and aggregation operations using Spark SQL</h2>
			<p>Spark SQL <a id="_idIndexMarker457"/>provides a rich set of transformation <a id="_idIndexMarker458"/>functions that can be applied to manipulate and transform data. Let’s explore some of the practical examples of transformations in <span class="No-Break">Spark SQL:</span></p>
			<h3>Aggregation</h3>
			<p>In this <a id="_idIndexMarker459"/>example, we perform an aggregation operation using Spark SQL to calculate the average salary from the <span class="No-Break"><strong class="source-inline">employees</strong></span><span class="No-Break"> table.</span></p>
			<pre class="source-code">
# Perform an aggregation to calculate the average salary
average_salary = spark.sql("SELECT AVG(Salary) AS average_salary FROM employees")
# Display the average salary
average_salary.show()</pre>			<p>The output will be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
+--------------+
|average_salary|
+--------------+
|        3275.0|
+--------------+</pre>			<p>The <strong class="source-inline">AVG()</strong> function calculates the average of the <strong class="source-inline">salary</strong> column. We alias the result as <strong class="source-inline">average_salary</strong> using the <span class="No-Break">AS keyword.</span></p>
			<p>The results <a id="_idIndexMarker460"/>are stored in the <strong class="source-inline">average_salary</strong> variable and displayed using the <span class="No-Break"><strong class="source-inline">show()</strong></span><span class="No-Break"> method:</span></p>
			<h3>Sorting</h3>
			<p>In this <a id="_idIndexMarker461"/>example, we apply a sorting transformation to the <strong class="source-inline">employees</strong> table using <span class="No-Break">Spark SQL.</span></p>
			<pre class="source-code">
# Sort the data based on the salary column in descending order
sorted_data = spark.sql("SELECT * FROM employees ORDER BY Salary DESC")
# Display the sorted data
sorted_data.show()</pre>			<p>The output will be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
+---+--------+----------+------+---+
| ID|Employee|Department|Salary|Age|
+---+--------+----------+------+---+
|  2|  Robert|     Sales|  4000| 38|
|  1|    John| Field-eng|  3500| 40|
|  5|   Kelly|   Finance|  3500| 35|
|  3|   Maria|   Finance|  3500| 28|
|  7|  Martin|   Finance|  3500| 26|
|  6|    Kate|   Finance|  3000| 45|
|  4| Michael|     Sales|  3000| 20|
|  8|   Kiran|     Sales|  2200| 35|
+---+--------+----------+------+---+</pre>			<p>The <strong class="source-inline">ORDER BY</strong> clause is used to specify the sorting criteria, in this case, the <strong class="source-inline">salary</strong> column in <span class="No-Break">descending order.</span></p>
			<p>The sorted <a id="_idIndexMarker462"/>data are stored in the <strong class="source-inline">sorted_data</strong> variable and displayed using the <span class="No-Break"><strong class="source-inline">show()</strong></span><span class="No-Break"> method.</span></p>
			<h3>Combining aggregations</h3>
			<p>We can <a id="_idIndexMarker463"/>also combine different aggregations in one SQL command, such as in the following <span class="No-Break">code example:</span></p>
			<pre class="source-code">
# Sort the data based on the salary column in descending order
filtered_data = spark.sql("SELECT Employee, Department, Salary, Age FROM employees WHERE age &gt; 30 AND Salary &gt; 3000 ORDER BY Salary DESC")
# Display the results
filtered_data.show()</pre>			<p>The output will be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
+--------+----------+------+---+
|Employee|Department|Salary|Age|
+--------+----------+------+---+
|  Robert|     Sales|  4000| 38|
|   Kelly|   Finance|  3500| 35|
|    John| Field-eng|  3500| 40|
+--------+----------+------+---+</pre>			<p>In this example, we combine different transformations to the <strong class="source-inline">employees</strong> table using Spark SQL. First, we select those employees whose age is greater than 30 and who have a salary greater than 3,000. The <strong class="source-inline">ORDER BY</strong> clause is used to specify the sorting criteria; in this case, the <strong class="source-inline">salary</strong> column in <span class="No-Break">descending order.</span></p>
			<p>The resulting data are stored in the “<strong class="source-inline">filtered_data</strong>” variable and displayed using the <span class="No-Break"><strong class="source-inline">show()</strong></span><span class="No-Break"> method.</span></p>
			<p>In this section, we explored the process of executing SQL queries and applying transformations using Spark SQL. We learned about the syntax for executing SQL queries and demonstrated practical examples of executing queries, filtering data, performing <a id="_idIndexMarker464"/>aggregations, and sorting data. By leveraging the expressive power of SQL and the flexibility of Spark SQL, you can efficiently analyze and manipulate structured data for a wide range of data <span class="No-Break">analysis tasks.</span></p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor174"/>Grouping and aggregating data – grouping data based on specific columns and performing aggregate functions</h2>
			<p>In Spark SQL, grouping and aggregating data are common operations that are performed to gain insights and summarize information from large datasets. This section will explore how to group data based on specific columns and perform various aggregate functions using Spark SQL. We will walk through code examples that demonstrate the capabilities of Spark SQL in <span class="No-Break">this regard.</span></p>
			<h3>Grouping data</h3>
			<p>When we want <a id="_idIndexMarker465"/>to group data based on specific columns, we can utilize the <strong class="source-inline">GROUP BY</strong> clause in SQL queries. Let’s consider an example where we have a DataFrame of employees with the columns <strong class="source-inline">department</strong> and <strong class="source-inline">salary</strong>. We want to calculate the average salary for <span class="No-Break">each department:</span></p>
			<pre class="source-code">
# Group the data based on the Department column and take average salary for each department
grouped_data = spark.sql("SELECT Department, avg(Salary) FROM employees GROUP BY Department")
# Display the results
grouped_data.show()</pre>			<p>The output will be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
+----------+------------------+
|Department|       avg(Salary)|
+----------+------------------+
| Field-eng|            3500.0|
|     Sales|3066.6666666666665|
|   Finance|            3375.0|
+----------+------------------+</pre>			<p>In this example, we group <a id="_idIndexMarker466"/>the data based on different transformations to the <strong class="source-inline">employees</strong> table using Spark SQL. First, we group employees based on the <strong class="source-inline">Department</strong> column. We take the average salary of each department from the <span class="No-Break"><strong class="source-inline">employees</strong></span><span class="No-Break"> table.</span></p>
			<p>The resulting data are stored in the <strong class="source-inline">grouped_data</strong> variable and displayed using the <span class="No-Break"><strong class="source-inline">show()</strong></span><span class="No-Break"> method.</span></p>
			<h3>Aggregating data</h3>
			<p>Spark SQL <a id="_idIndexMarker467"/>provides a wide range of aggregate functions to calculate summary statistics on grouped data. Let’s consider another example where we want to calculate the total salary and the maximum salary for <span class="No-Break">each department:</span></p>
			<pre class="source-code">
# Perform grouping and multiple aggregations
aggregated_data = spark.sql("SELECT Department, sum(Salary) AS total_salary, max(Salary) AS max_salary FROM employees GROUP BY Department")
# Display the results
aggregated_data.show()</pre>			<p>The output will be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
+----------+-----------+-----------+
|Department|sum(Salary)|max(Salary)|
+----------+-----------+-----------+
| Field-eng|       3500|       3500|
|     Sales|       9200|       4000|
|   Finance|      13500|       3500|
+----------+-----------+-----------+</pre>			<p>In this example, we combine and group the data based on different transformations to the <strong class="source-inline">employees</strong> table using Spark SQL. First, we group the employees based on <strong class="source-inline">Department</strong> column. We take the total salary and maximum salary of each department from the employees table. We also use an alias for these <span class="No-Break">aggregated columns.</span></p>
			<p>The resulting <a id="_idIndexMarker468"/>data are stored in the <strong class="source-inline">aggregated_data</strong> variable and displayed using the <span class="No-Break"><strong class="source-inline">show()</strong></span><span class="No-Break"> method.</span></p>
			<p>In this section, we have explored the capabilities of Spark SQL in grouping and aggregating data. We have seen examples of how to group data based on specific columns and perform various aggregate functions. Spark SQL provides a wide range of aggregate functions and allows for the creation of custom aggregate functions to suit specific requirements. With these capabilities, you can efficiently summarize and gain insights from large datasets using <span class="No-Break">Spark SQL.</span></p>
			<p>In the next section, we will look at advanced Spark SQL functions for complex data <span class="No-Break">manipulation operations.</span></p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor175"/>Advanced Spark SQL operations</h1>
			<p>Let's explore the key capabilities of Apache Spark's <span class="No-Break">advanced operations.</span></p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor176"/>Leveraging window functions to perform advanced analytical operations on DataFrames</h2>
			<p>In this <a id="_idIndexMarker469"/>section, we will explore the powerful capabilities of window functions in Spark SQL for performing advanced analytical operations <a id="_idIndexMarker470"/>on DataFrames. Window functions provide a way to perform calculations across a set of rows within a partition, allowing us to derive insights and perform complex computations efficiently. In this section, we will dive into the topic of window functions and showcase code examples that demonstrate their usage in Spark <span class="No-Break">SQL queries.</span></p>
			<h3>Understanding window functions</h3>
			<p>Window functions in Spark SQL enable advanced analytical operations by dividing a dataset <a id="_idIndexMarker471"/>into groups or partitions based on specified criteria. These functions operate on a sliding window of rows within each partition, performing calculations or aggregations. </p>
			<h3>The general syntax for using window functions in Spark SQL is as follows:</h3>
			<pre class="source-code">
function().over(Window.partitionBy("column1", "column2").orderBy("column3").rowsBetween(start, end))</pre>			<p>The <strong class="source-inline">function()</strong> represents the window function that you want to apply, such as <strong class="source-inline">sum</strong>, <strong class="source-inline">avg</strong>, <strong class="source-inline">row_number</strong>, or custom-defined functions. The <strong class="source-inline">over()</strong> clause defines the window to which the function is applied. <strong class="source-inline">Window.partitionBy()</strong> specifies the columns used to divide the dataset into partitions. It also determines the order of rows within each partition. <strong class="source-inline">rowsBetween(start, end)</strong> specifies the range of rows included in the window. It can be unbounded or defined relative to the <span class="No-Break">current row.</span></p>
			<h3>Calculating cumulative sum using window functions</h3>
			<p>Let’s <a id="_idIndexMarker472"/>explore a practical example that demonstrates the usage of window functions to calculate a cumulative sum of a column in <span class="No-Break">a DataFrame:</span></p>
			<pre class="source-code">
from pyspark.sql.window import Window
from pyspark.sql.functions import col, sum
# Define the window specification
window_spec = Window.partitionBy("Department").orderBy("Age")
# Calculate the cumulative sum using window function
df_with_cumulative_sum = salary_data_with_id.withColumn("cumulative_sum", sum(col("Salary")).over(window_spec))
# Display the result
df_with_cumulative_sum.show()</pre>			<p>The <a id="_idIndexMarker473"/>output will be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
+---+--------+----------+------+---+--------------+
| ID|Employee|Department|Salary|Age|cumulative_sum|
+---+--------+----------+------+---+--------------+
|  1|    John| Field-eng|  3500| 40|          3500|
|  7|  Martin|   Finance|  3500| 26|          3500|
|  3|   Maria|   Finance|  3500| 28|          7000|
|  5|   Kelly|   Finance|  3500| 35|         10500|
|  6|    Kate|   Finance|  3000| 45|         13500|
|  4| Michael|     Sales|  3000| 20|          3000|
|  8|   Kiran|     Sales|  2200| 35|          5200|
|  2|  Robert|     Sales|  4000| 38|          9200|
+---+--------+----------+------+---+--------------+</pre>			<p>In this example, we start by importing the necessary libraries. We use the same DataFrame as our previous <span class="No-Break">examples: </span><span class="No-Break"><strong class="source-inline">salary_data_with_id</strong></span><span class="No-Break">.</span></p>
			<p>Next, we define a window specification using <strong class="source-inline">Window.partitionBy("Department").orderBy("Age")</strong>, which partitions the data according to the <strong class="source-inline">Department</strong> column and orders the rows within each partition according to the <span class="No-Break"><strong class="source-inline">Age</strong></span><span class="No-Break"> column.</span></p>
			<p>We then use the <strong class="source-inline">sum()</strong> function as a window function, applied over the defined window specification, to calculate the cumulative sum of the <strong class="source-inline">Salary</strong> column. The result is stored in a new column <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">cumulative_sum</strong></span><span class="No-Break">.</span></p>
			<p>Finally, we call the <strong class="source-inline">show()</strong> method to display the DataFrame with the added cumulative sum column. By leveraging window functions, we can efficiently calculate cumulative sums, running totals, rolling averages, and other complex analytical calculations over defined windows in <span class="No-Break">Spark SQL.</span></p>
			<p>In this section, we explored the powerful capabilities of window functions in Spark SQL for <a id="_idIndexMarker474"/>advanced analytics. We discussed the syntax and usage of window functions, allowing us to perform complex calculations and aggregations within defined partitions and windows. By incorporating window functions into Spark SQL queries, you can derive valuable insights and gain a deeper understanding of your data for advanced <span class="No-Break">analytical operations.</span></p>
			<p>In the next section, we will explore Spark <span class="No-Break">user-defined functions.</span></p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor177"/>User-defined functions</h2>
			<p>In this <a id="_idIndexMarker475"/>section, we will delve into the topic of <strong class="bold">user-defined functions</strong> (<strong class="bold">UDFs</strong>) in Spark SQL. UDFs allow us to extend the functionality <a id="_idIndexMarker476"/>of Spark SQL by defining our custom functions that can be applied to DataFrames or SQL queries. In this section, we will explore the concept of UDFs and provide code examples to demonstrate their usage and benefits in <span class="No-Break">Spark SQL.</span></p>
			<p>UDFs in Spark SQL enable us to create custom functions to perform transformations or computations on columns in a DataFrame or in SQL queries. UDFs are particularly useful when Spark’s built-in functions do not meet our specific requirements. </p>
			<h3>To define <a id="_idIndexMarker477"/>a UDF in Spark SQL, we use the <strong class="source-inline">udf()</strong> function from the <strong class="source-inline">pyspark.sql.functions</strong> module. The general syntax is as follows:</h3>
			<pre class="source-code">
from pyspark.sql.functions import udf
udf_name = udf(lambda_function, return_type)</pre>			<p>First, we import the <strong class="source-inline">udf()</strong> function from the <strong class="source-inline">pyspark.sql.functions</strong> module. Next, we define the UDF by providing a lambda function or a regular Python function as the <strong class="source-inline">lambda_function</strong> argument. This function encapsulates the custom logic we want <span class="No-Break">to apply.</span></p>
			<p>We also <a id="_idIndexMarker478"/>specify <strong class="source-inline">return_type</strong> for the UDF, which represents the data type that the UDF <span class="No-Break">will return.</span></p>
			<h3>Applying a UDF to a DataFrame</h3>
			<p>Let’s <a id="_idIndexMarker479"/>explore a practical example that demonstrates the usage of UDFs in Spark SQL by applying a custom function to <span class="No-Break">a DataFrame:</span></p>
			<pre class="source-code">
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
# Define a UDF to capitalize a string
capitalize_udf = udf(lambda x: x.upper(), StringType())
# Apply the UDF to a column
df_with_capitalized_names = salary_data_with_id.withColumn("capitalized_name", capitalize_udf("Employee"))
# Display the result
df_with_capitalized_names.show()</pre>			<p>The output will be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
+---+--------+----------+------+---+----------------+
| ID|Employee|Department|Salary|Age|capitalized_name|
+---+--------+----------+------+---+----------------+
|  1|    John| Field-eng|  3500| 40|            JOHN|
|  2|  Robert|     Sales|  4000| 38|          ROBERT|
|  3|   Maria|   Finance|  3500| 28|           MARIA|
|  4| Michael|     Sales|  3000| 20|         MICHAEL|
|  5|   Kelly|   Finance|  3500| 35|           KELLY|
|  6|    Kate|   Finance|  3000| 45|            KATE|
|  7|  Martin|   Finance|  3500| 26|          MARTIN|
|  8|   Kiran|     Sales|  2200| 35|           KIRAN|
+---+--------+----------+------+---+----------------+</pre>			<p>In this <a id="_idIndexMarker480"/>example, we start by defining a UDF called <strong class="source-inline">capitalize_udf</strong> using the <strong class="source-inline">udf()</strong> function. It applies a lambda function that changes the input string to upper case. We use the <strong class="source-inline">withColumn()</strong> method to apply the UDF <strong class="source-inline">capitalize_udf</strong> to the <strong class="source-inline">name</strong> column, creating a new column called <strong class="source-inline">capitalized_name</strong> in the <span class="No-Break">resulting DataFrame.</span></p>
			<p>Finally, we call the <strong class="source-inline">show()</strong> method to display the DataFrame with the <span class="No-Break">transformed column.</span></p>
			<p>UDFs allow us to apply custom logic and transformations to columns in DataFrames, enabling us to handle complex computations, perform string manipulations, or apply domain-specific operations that are not available in Spark’s <span class="No-Break">built-in functions.</span></p>
			<p>In this section, we explored the concept of UDFs in Spark SQL. We discussed the syntax for defining UDFs and demonstrated their usage through a code example. UDFs provide a powerful mechanism to extend Spark SQL’s functionality by allowing us to apply custom transformations and computations to DataFrames or SQL queries. By incorporating UDFs into your Spark SQL workflows, you can handle complex data operations and tailor your data processing pipelines to meet specific requirements or <span class="No-Break">domain-specific needs.</span></p>
			<h3>Applying a function</h3>
			<p>PySpark also supports various UDFs and APIs to allow users to execute native Python functions. For instance, the following example allows users to directly use the APIs in a pandas <a id="_idIndexMarker481"/>series within the Python <span class="No-Break">native function:</span></p>
			<pre class="source-code">
import pandas as pd
from pyspark.sql.functions import pandas_udf
@pandas_udf('long')
def pandas_plus_one(series: pd.Series) -&gt; pd.Series:
    # Simply plus one by using pandas Series.
    return series + 1
salary_data_with_id.select(pandas_plus_one(salary_data_with_id.Salary)).show()</pre>			<p>The output will be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
+-----------------------+
|pandas_plus_one(Salary)|
+-----------------------+
|                   3501|
|                   4001|
|                   3501|
|                   3001|
|                   3501|
|                   3001|
|                   3501|
|                   2201|
+-----------------------+</pre>			<p>In this example, we start by defining a pandas UDF called <strong class="source-inline">pandas_plus_one</strong> using the <strong class="source-inline">@pandas_udf()</strong> function. We define this function so that it adds 1 to a pandas series. We use the already created DataFrame named <strong class="source-inline">salary_data_with_id</strong> and call the pandas UDF to apply this function to the <strong class="source-inline">salary</strong> column of <span class="No-Break">the DataFrame.</span></p>
			<p>Finally, we call the <strong class="source-inline">show()</strong> method in the same statement, to display the DataFrame with the <span class="No-Break">transformed column.</span></p>
			<p>In addition, UDFs <a id="_idIndexMarker482"/>can be registered and invoked in SQL out of the box. The following is an example of how we can <span class="No-Break">achieve this:</span></p>
			<pre class="source-code">
@pandas_udf("integer")
def add_one(s: pd.Series) -&gt; pd.Series:
    return s + 1
spark.udf.register("add_one", add_one)
spark.sql("SELECT add_one(Salary) FROM employees").show()</pre>			<p>The output will be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
+---------------+
|add_one(Salary)|
+---------------+
|           3501|
|           4001|
|           3501|
|           3001|
|           3501|
|           3001|
|           3501|
|           2201|
+---------------+</pre>			<p>In this example, we start by defining a pandas UDF called <strong class="source-inline">add_one</strong> by using the <strong class="source-inline">@pandas_udf()</strong> function. We define this function so that it adds 1 to a pandas series. We then register this UDF for use in SQL functions. We use the already created employees <a id="_idIndexMarker483"/>table and call the pandas UDF to apply this function to the <strong class="source-inline">salary</strong> column of <span class="No-Break">the table.</span></p>
			<p>Finally, we call the <strong class="source-inline">show()</strong> method in the same statement to display <span class="No-Break">the results.</span></p>
			<p>In this section, we explored the powerful capabilities of UDFs and how we can use them in <span class="No-Break">calculating aggregations.</span></p>
			<p>In the next section, we will explore pivot and <span class="No-Break">unpivot functions.</span></p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor178"/>Working with complex data types – pivot and unpivot</h2>
			<p>Pivot and unpivot <a id="_idIndexMarker484"/>operations are used to transform data from a row-based <a id="_idIndexMarker485"/>format to a column-based format and vice versa. In Spark SQL, these operations can be performed using the pivot and <span class="No-Break">unpivot functions.</span></p>
			<p>The <a id="_idIndexMarker486"/>pivot function is used to transform rows into columns. It takes three arguments: the column to use as <a id="_idIndexMarker487"/>the new column headers, the column to use as the new row headers and the columns to use as the values in the new table. The resulting table will have one row for each unique value in the row header column, and one column for each unique value in the column <span class="No-Break">header column.</span></p>
			<p>The unpivot function is used to transform columns into rows. It takes two arguments: the columns to use as the new row headers, and the column to use as the values in the new table. The resulting table will have one row for each unique combination of values in the row header columns and one column for <span class="No-Break">the values.</span></p>
			<p>Some use <a id="_idIndexMarker488"/>cases for pivot and unpivot operations include <a id="_idIndexMarker489"/><span class="No-Break">the following:</span></p>
			<ul>
				<li>Converting data from a wide format to a long format or <span class="No-Break">vice versa</span></li>
				<li>Aggregating data by <span class="No-Break">multiple dimensions</span></li>
				<li>Creating summary tables <span class="No-Break">or reports</span></li>
				<li>Preparing data for visualization <span class="No-Break">or analysis</span></li>
			</ul>
			<p>Overall, pivot and unpivot operations are useful tools for transforming data in <span class="No-Break">Spark SQL.</span></p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor179"/>Summary</h1>
			<p>In this chapter, we explored the process of transforming and analyzing data in Spark SQL. We learned how to filter and manipulate loaded data, save the transformed data as a table, and execute SQL queries to extract meaningful insights. By following the Python code examples provided, you can apply these techniques to your own datasets, unlocking the potential of Spark SQL for data analysis <span class="No-Break">and exploration.</span></p>
			<p>After covering those topics, we explored the powerful capabilities of window functions in Spark SQL for advanced analytics. We discussed the syntax and usage of window functions, allowing us to perform complex calculations and aggregations within defined partitions and windows. By incorporating window functions into Spark SQL queries, you can derive valuable insights and gain a deeper understanding of your data for advanced <span class="No-Break">analytical operations.</span></p>
			<p>We then discussed some ways to use UDFs in Spark and how they can be useful in complex aggregations over multiple rows and columns of <span class="No-Break">a DataFrame.</span></p>
			<p>Finally, we covered some of the ways to use pivot and unpivot in <span class="No-Break">Spark SQL.</span></p>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor180"/>Sample questions</h1>
			<p><span class="No-Break"><strong class="bold">Question 1</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code snippets creates a view in Spark SQL that will replace existing views if <span class="No-Break">already present?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">dataFrame.createOrReplaceTempView()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">dataFrame.createTempView()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">dataFrame.createTableView()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">dataFrame.createOrReplaceTableView()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">dataDF.write.path(filePath)</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 2</strong></span><span class="No-Break">:</span></p>
			<p>Which function do we use to join two <span class="No-Break">DataFrames together?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">DataFrame.filter()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">DataFrame.distinct()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">DataFrame.intersect()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">DataFrame.join()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">DataFrame.count()</strong></span></li>
			</ol>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor181"/>Answers</h2>
			<ol>
				<li>A</li>
				<li>D</li>
			</ol>
		</div>
	

		<div id="_idContainer028" class="Content">
			<h1 id="_idParaDest-181" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor182"/>Part 4: Spark Applications</h1>
			<p>In this part, we will cover Spark’s Structured Streaming, focusing on real-time data processing with concepts such as event time processing, watermarking, triggers, and output modes. Practical examples will illustrate building and deploying streaming applications using Structured Streaming. Additionally, we will delve into Spark ML, Spark’s machine learning library, exploring supervised and unsupervised techniques, model building, evaluation, and hyperparameter tuning across various algorithms. Practical examples will demonstrate Spark ML's application in real-world machine learning tasks, crucial in contemporary data science. While not included in the Spark certification exam, understanding these concepts is essential in modern <span class="No-Break">data engineering.</span></p>
			<p>This part has the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B19176_07.xhtml#_idTextAnchor183"><em class="italic">Chapter 7</em></a>, <em class="italic">Structured Streaming in Spark</em></li>
				<li><a href="B19176_08.xhtml#_idTextAnchor220"><em class="italic">Chapter 8</em></a>, <em class="italic">Machine Learning with Spark ML</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer029">
			</div>
		</div>
		<div>
			<div id="_idContainer030" class="Basic-Graphics-Frame">
			</div>
		</div>
	</body></html>