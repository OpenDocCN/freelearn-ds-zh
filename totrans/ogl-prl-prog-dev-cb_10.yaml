- en: Chapter 10. Developing the Radix Sort with OpenCL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to explore the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Radix sort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the MSD and LSD Radix sorts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing the Radix sort in OpenCL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about developing the Bitonic sort using
    OpenCL. In this chapter, we are going to explore how to develop the Radix sort
    with OpenCL. Radix sorting is also known as **bucket sorting**, and we'll see
    why later on.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first Radix sort algorithms came from a machine called the **Hollerith machine**
    that was used in 1890 to tabulate the United States census, and though it may
    not be quite as famous as the machine created by *Charles Babbage*, it does have
    its place in computing history.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Radix sort
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Radix sort is not a comparison-based sorting algorithm, and it has a few
    qualities that make it more suitable to parallel computation, especially on vector
    processors such as GPU and modern CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I am somewhat reluctant to use the term *modern* since processor technology
    has evolved so quickly over time that the use of this word somehow seems dated.
  prefs: []
  type: TYPE_NORMAL
- en: The way the Radix sort works is rather interesting when you compare it with
    the comparison-based sorting algorithms such as quicksort; the main difference
    between them is how they process the keys of the input data. The Radix sort does
    this by breaking down a key into smaller sequences of sub-keys, if you will, and
    sorts these sub-keys one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Numbers can be translated in binary and can be viewed as a sequence of bits;
    the same analogy can be drawn from strings where they are sequences of characters.
    The Radix sort, when applied to such keys, does not compare the individual keys,
    but rather it works on processing and comparing pieces of those keys.
  prefs: []
  type: TYPE_NORMAL
- en: 'Radix sort algorithms treat the keys like numbers in a base-R number system.
    *R* is known as the radix, hence the given name of this algorithm. Different values
    of *R* can be applied to different types of sorting. Examples could be:'
  prefs: []
  type: TYPE_NORMAL
- en: '*R = 256* would be sorting strings where each character is an 8-bit ASCII value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*R = 65536* would be sorting Unicode strings where each character is a 16-bit
    Unicode value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*R = 2* would be sorting binary numbers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, let's examine an example to see how the Radix sort would sort
    the numbers 44565, 23441, 16482, 98789, and 56732, assuming that each number is
    a five-digit number laid out in memory in contiguous locations
  prefs: []
  type: TYPE_NORMAL
- en: '| 44565 | 23441 | 16482 | 98789 | 56732 |'
  prefs: []
  type: TYPE_TB
- en: 'We are going to extract each digit in a right-to-left fashion examining the
    least significant digit first. Therefore, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| 5 | 1 | 2 | 9 | 2 |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s assume we apply counting sort to this array of numbers and it becomes
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| 1 | 2 | 2 | 5 | 9 |'
  prefs: []
  type: TYPE_TB
- en: 'This translates to the following order. Take note that the sorting is stable:'
  prefs: []
  type: TYPE_NORMAL
- en: '| 23441 | 16482 | 56732 | 44565 | 98789 |'
  prefs: []
  type: TYPE_TB
- en: 'Next, we shift to the left by one digit. Notice that now the array of numbers
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '| 4 | 8 | 3 | 6 | 8 |'
  prefs: []
  type: TYPE_TB
- en: 'Applying the counting sort again and translating it back to the order of the
    numbers, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '| 56732 | 23441 | 16482 | 98789 | 44565 |'
  prefs: []
  type: TYPE_TB
- en: 'For the 1000^(th) digit we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '| 23441 | 16482 | 56732 | 98789 | 44565 |'
  prefs: []
  type: TYPE_TB
- en: 'For the 10,000^(th) digit we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '| 23441 | 44565 | 16482 | 56732 | 98789 |'
  prefs: []
  type: TYPE_TB
- en: 'For the 100,000^(th) digit we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '| 16482 | 23441 | 44565 | 56732 | 98789 |'
  prefs: []
  type: TYPE_TB
- en: Voila! Radix sorting sorted the array of five-digit numbers. We should note
    that the sort is *stable*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Stable sorting** refers to the capability of the algorithm to be able to
    maintain the relative order between any two elements with equal keys. Let us assume
    that an array, `int a[5]`, of the values `1`, `2`, `3`, `4`, `9`, and `2`, through
    some sorting algorithm, X, will sort the elements to `1`, `2`, `2`, `3`, `4`,
    and `9`. The point here is that the two equal values we saw, which are both the
    number `2`, occur at positions `1` and `5` (assuming arrays are zero indexed).
    Then, through X, the sorted list will be such that `a[1]` is always before `a[5]`.'
  prefs: []
  type: TYPE_NORMAL
- en: There are actually two basic approaches to Radix sorting. We have seen one approach
    in which we examine the least-significant digit and sort it. This is commonly
    referred to as **LSD Radix sorting** since we work our way from right to left.
    The other approach would be to work from left to right.
  prefs: []
  type: TYPE_NORMAL
- en: The key consideration in Radix sorting is the concept of the *key*. Depending
    on the context, a key may be a word or a string, and each of them would be of
    fixed length or variable length.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the MSD and LSD Radix sorts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us take some time to understand how the MSD Radix sort and the LSD Radix
    sort work before we start working on developing the equivalent on OpenCL.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Radix sort assumes that we wish to sort Radix-R numbers by considering the
    most significant digit first. For this to happen, we can partition the input into
    *R* rather than just two, and we have actually seen this done before. This is
    data binning, but it extends that with the counting sort. A Radix sort can be
    run on ASCII characters, Unicode characters, integer numbers (32-bit / 64-bit),
    or floating-point numbers (sorting floating-point numbers is tricky). You need
    to figure out what constitutes a key. Keys can be thought of as 8-bit keys, 16-bit
    keys, and so on, and we know by now that Radix sorts require repeated iterations
    to extract the keys and sort and bin them based on base *R*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, we have an MSD Radix sort that sorts the characters
    in a given string in the programming language C, and the radix we use is 256 (the
    maximum value of an unsigned 8-bit number, otherwise a signed 8-bit would be -128
    to 127):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The second approach in Radix sorting scans the input from right to left and
    examines each element by applying a similar operation as in an MSD Radix sort.
    This is known as the **Least Significant Digit** (**LSD) Radix sort**. LSD Radix
    sorting works because when any two elements differ, the sorting will place them
    in the proper relative order, and even when these two elements differ, the fact
    that LSD exhibits stable sorting means that their relative order is still maintained.
    Let''s take a look at how it would work for sorting three character strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/4520OT_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A typical LSD Radix sort for sorting characters in a given string might look
    like the following code (assuming all keys have a fixed width; let''s call it
    `W`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both approaches are similar as they both bin the characters into *R* bins, that
    is, 256 bins, and they also use the idea of the counting sort to work out where
    the final sorting arrangement is going to be using a temporary storage, `temp`,
    and then use that temporary storage and move the data to their sorted places.
    The nice thing about MSD over LSD Radix sorts is that MSD may not examine all
    of the keys and works for variable-length keys; although, in that lies another
    problem—MSD can experience sub-linear sorts; in practice LSD is generally preferred
    when the size of the key is fixed.
  prefs: []
  type: TYPE_NORMAL
- en: The runtime of an LSD Radix sort is ![How it works…](img/4520OT_10_16.jpg) when
    compared to the runtimes of other sorting algorithms that are based on the divide-conquer
    approach, which generally have a runtime of ![How it works…](img/4520OT_10_17.jpg)
    you might be tempted to conclude that Radix sorting would be faster than comparison-based
    sorts like quicksort, and you could be right. But, in practice, a well-tuned quicksort
    can outperform a Radix sort by 24 percent by applying more advanced techniques
    to improve cache friendliness during the execution. However, technology is constantly
    evolving, and researchers and engineers will find opportunities to maximize the
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may wish to read the papers *The influence of cache on sorting* by *LaMarca*
    and *Adapting Radix Sort to the memory hierarchy* by *Rahman and Raman* for more
    algorithmic improvements that they have worked on.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Radix sorting employs two techniques: **reduction** and **scan**. These are
    classified as data collection patterns as they occur frequently in parallel computing.
    This recipe will focus on reduction, which allows data to be condensed to a single
    element using associative binary operators. The scan pattern can be easily mistaken
    for the reduction pattern and the key difference is that this pattern reduces
    every subsequence of a collection up to every position in the input. We''ll defer
    the discussion of scans until we get to the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the reduction pattern, we typically have an associative binary operator,
    ![Understanding reduction](img/4520OT_10_18.jpg)that we use to collate all elements
    in a container in a pair-wise fashion. The fact that we need an associative binary
    operator is an important one, because it implies that the developer can reorganize
    the combination function to check if it performs efficiently; we''ll go into that
    a little later. Let''s take a look at a serial algorithm for conducting reduction
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The algorithm basically takes an associative binary operator, `f` (that is,
    a pointer to a function), and an array `a`, of length `n` and computes the operation
    ![Understanding reduction](img/4520OT_10_19.jpg)over the array with an initial
    value identified by `identity`.
  prefs: []
  type: TYPE_NORMAL
- en: 'An associative binary operator can allow the developer to extract parallelism
    from it because associativity means that the operator would produce the same result
    regardless of the order in which it is applied to the elements. That is to say:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding reduction](img/4520OT_10_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The previous expression is equivalent to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding reduction](img/4520OT_10_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Putting on the many core hat, we can actually imagine a tree of computations
    in which the sub-trees represent the computation of the form![Understanding reduction](img/4520OT_10_22.jpg).
    The first sweep would compute the result of this sub-tree while the second sweep
    would collate the results of the other sub-trees. This will be evident once you
    have had a chance to examine them visually in the next two diagrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding reduction](img/4520OT_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It will be very useful for you to contrast the manner in which these diagrams
    differ. One of the ways is that the former implies a sequence of operations in
    traversal order, and this is very different from the latter (as shown in the following
    diagram):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding reduction](img/4520OT_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s great news to know that associative operators allow the reduction to
    be parallelized, but it''s not the entire story, because associativity only allows
    us to group the operations and does not reveal to us whether these groups of binary
    operations need to occur in a specific order. If you are wondering whether we
    are talking about commutativity, you are spot on! Commutativity gives us the important
    property of changing the order of application. We know that some operations exhibit
    one of these while others exhibit both; for example, we know that addition and
    multiplication of numbers is both associative and commutative. The following is
    what a commutative parallel reduction might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding reduction](img/4520OT_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, seeing this information, you might wonder how this can be translated into
    OpenCL. We are going to demonstrate a few reductions kernels in this recipe where
    each one will provide you with an improvement over the previous one.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we are going to assume that we have a large array of a few
    million elements and that we like to apply the reduction algorithm to compute
    the sum of all elements. The first thing to do is produce a parallel algorithm
    for the serial version we saw earlier. All the kernels we are demonstrating are
    in `Ch10/Reduction/reduction.cl`.
  prefs: []
  type: TYPE_NORMAL
- en: In the serial version of the algorithm, you would have noticed that we simply
    pass the accumulator into the binary function to perform the operation. However,
    we cannot use this method in the GPU since it cannot support tens of thousands
    of executing threads and also the device can contain many more processors than
    an x86 CPU has. The only solution is to partition the data across the processors
    so that each block processes a portion of the input, and when all of the processors
    are executing in parallel, we should expect the work to be completed in a short
    span of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming that a block has computed its summed value, we still need a way to
    collate all those partial sums from all blocks, and considering that OpenCL does
    not have a global synchronization primitive or API, we have two options: have
    OpenCL collate the partial sums or have the host code collate the partial sums;
    for our examples, the second option is chosen.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first kernel, `reduce0`, is a direct translation of the serial algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This kernel block would load the elements to its shared memory, `sdata`, and
    we conduct the reduction in `sdata` in various stages governed by the `for` loop,
    allowing work items with IDs that are multiples of two to perform the pair-wise
    reduction. Therefore, in the first iteration of the loop, work items with IDs
    *{0, 2, 4, 6, 8, 10, 12, 14, ..., 254}* would execute, in the second iteration,
    only work items with IDs *{0, 4, 8, 12, 252}* would execute, and so on. Following
    the reduction algorithm, the partial sum would be deposited into `sdata[0]`, and
    finally this value would be copied out by one thread which happens to have an
    ID value equal to `0`. Admittedly, this kernel is pretty good but it suffers from
    two problems: the modulus operator takes a longer time to execute and wavefronts
    are diverged. The larger issue here is the problem of wavefront divergence since
    it means that some work items in the wavefronts are executing while some are not,
    and in this case, the work items with odd IDs are not executing while those with
    even IDs are, GPUs deal with this problem by implementing predication, and this
    means that all work items in the following code snippet actually get executed.
    However, the predication unit on the GPU will apply a mask so that only those
    work items whose IDs matched the condition, `if(tid % (2*s) == 0)`, will execute
    the statement in the `if` statement, while those work items who fail the condition,
    `false`, would invalidate their results. Obviously, this is a waste of computing
    resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Fortunately, this can be solved with little effort, and the next kernel code
    demonstrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We replaced the conditional evaluation after the modulus operator has been applied
    to something more palatable. The appetizing portion is the fact that we no longer
    have diverging wavefronts, and we have also made strided accesses to the shared
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we have seen how we can apply our understanding of associativity to
    build the reduction kernel and also how to make use of our new understanding of
    commutativity in the reduction process. The commutative reduction tree is actually
    better than the associative reduction tree because it makes better use of the
    shared memory by compacting the reduced values and hence raising efficiency; the
    following kernel, `reduce2`, reflects this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'However, this isn''t very good because now during the first iteration, we have
    already made half of those work items idle and efficiency is definitely affected.
    Fortunately, however, the remedy is simple. We reduce half the number of blocks
    and during the hydration of the shared memory, we load two elements and store
    the sum of these two elements instead of just loading values from global memory
    and storing them into shared memory. The kernel, `reduce3`, reflects this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, things are starting to look much better and we''ve used what we call **reversed
    loop** (which is basically counting backwards) to get rid of the problem of divergent
    wavefronts; in the meantime, we have also not reduced our capacity to reduce elements
    because we''ve performed that while hydrating the shared memory. The question
    is whether there''s more we can do? Actually, there is another idea we can qualify
    and that is to take advantage of atomicity of wavefronts or warps executing on
    GPUs. The next kernel, `reduce4`, demonstrates how we utilized wavefront programming
    to reduce blocks atomically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the code block demarcated by the statement `if (tid < 64)`, we no longer
    need to place the memory barriers because the code block only hosts one wavefront
    which executes atomically in the lock step.
  prefs: []
  type: TYPE_NORMAL
- en: Developing the Radix sort in OpenCL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From this section onwards, we are going to develop this sorting method for
    OpenCL. We are going to do two things: implement the parallel Radix sort described
    in the paper that *Marco Zagha* and *Guy E. Blelloch* wrote in 1991 titled *Radix
    Sort for Vector Multiprocessors*. The former algorithm was crafted for the CRAY
    Y-MP computer (which, in turn, was adapted from the parallel Radix sort algorithm
    that worked on the **Connection Machine (CM-2)**).'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Radix sorting attempts to treat keys as multi-digit numbers, where each digit
    is an integer depending on the size of the Radix, *R*. An example would be sorting
    a large array of 32-bit numbers. We can see that each such number is made up of
    four bytes (each byte is 8-bits on today's CPU and GPU processors), and if we
    decide to assume that each digit would be 8-bits, we naturally would treat a 32-bit
    number as comprised of four digits. This notion is most natural when you apply
    the concept back to a string of words, treating each word as comprising of more
    than one character.
  prefs: []
  type: TYPE_NORMAL
- en: 'The original algorithm worded in the 1999 paper basically uses the counting
    sort algorithm and it has three main components which will in turn sort the input
    by iterating all three components until the job is done. The pseudo code, which
    is a serial algorithm, is presented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The algorithm `HISTOGRAM-KEYS` is something that we have already encountered
    a few chapters ago, and it is really the histogram. This algorithm computes the
    distribution of the keys that it encounters during the sort. This algorithm is
    expressed in a serial fashion, that is, it is supposed to run on a single executing
    thread; we have already learned how to parallelize that and you can apply those
    techniques here. However, what we are going to do now deviates from what you have
    seen in that previous chapter, and we'll reveal that soon enough.
  prefs: []
  type: TYPE_NORMAL
- en: The next algorithm is `SCAN-BUCKETS`, and it is named as such because it actually
    scans the entire histogram to compute the prefix sums (we'll examine prefix sums
    in fair detail later). In this scan operation, `Bucket[i]` contains the number
    of digits with a value, `j`, such that `j` is greater than `i`, and this value
    is also the position, that is, the array index in the output.
  prefs: []
  type: TYPE_NORMAL
- en: The final algorithm is `RANK-AND-PERMUTE`, and each key with a digit of value
    of `i` is placed in its final location by getting the offset from `Bucket[i]`
    and incrementing the bucket so that the next key with the same value `i` gets
    placed in the next location. You should also notice that `COUNTING SORT` is stable.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into parallelization of the algorithms and how they work in a
    cohesive manner, it's important to take the next few paragraphs to understand
    what prefix sums are; the next paragraph highlights why they matter in Radix sorts.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous sections, we introduced MSD and LSD Radix sorts and the prefix
    sums computation is embedded in the code. However, we didn''t flag it out for
    you then. So, now''s the time and the following is the code (taken from the previous
    `lsd_sort` and `msd_sort` sections):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If you recall how MSD/LSD works, we basically create a histogram of the values
    we have encountered and, at each stage of the sorting, we compute the prefix sums
    so that the algorithm can know where to place the output in a sorted order. If
    you are still doubtful, you should stop now and flip back to that section and
    work through the LSD sorting for strings of three characters.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The prefix sums is actually a generalization of the global sum, and its original
    formulation goes something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The prefix sum operation takes a binary associative operator ![Getting ready](img/4520OT_10_23.jpg),
    and an ordered set of n elements, ![Getting ready](img/4520OT_10_24.jpg), and
    returns the ordered set ![Getting ready](img/4520OT_10_25.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: We use a concrete example like taking a summation over an arbitrary array like
    `[39, 23, 44, 15, 86]`. Using the addition operator, the output would be `[39,
    62, 108, 125, 211]`, and it is not obvious why this sort of computation is important
    or is even needed. In fact it is not even clear whether there is a direct way
    to parallelize this algorithm because of dependencies that each subsequent computation
    relies on the previous.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sequential version of the prefix sums which has a runtime of ![Getting ready](img/4520OT_10_16.jpg)
    can be expressed as follows, assuming there are two arrays `in_arr` and `out_arr`,
    and `out_arr` is designed to contain the prefix sums for `in_arr`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: To extract parallelism from this, we need to adjust the way we view the arbitrary
    array of input values, and the adjustment we are talking about is actually imagining
    the array to be consumed by a tree of computations. Let's go on a little further
    to see why.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we think it''s important to step back into history and see who
    came up with the original prefix sum computation. As far as I am aware, two researchers
    in 1986, *Daniel Hillis* and *Guy L. Steele*, presented a version of the prefix
    sum as part of an article titled *Data Parallel Algorithms* in the *ACM (Association
    for Computing Machinery)* magazine, and the algorithm they presented worked as
    follows (cited as such in that article):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The following diagram (courtesy of *Mark Harris* from the NVIDIA Corporation),
    pictorially illustrates what the Hillis and Steele algorithm does. It starts at
    the level where all eight elements are looked upon as leaves of the binary tree
    and proceeds to work its way through computing the partial sums. Each level of
    the computation, `d`, will compute partial sums based on the previous level's
    computation. An assumption found in the algorithm is that it assumes that there
    are as many processors as there are elements and this is demonstrated by the conditional
    statement in the algorithm, `if (k >= 2j)`. Another problem it has got is that
    it's not very efficient; it has a runtime complexity of ![Getting ready](img/4520OT_10_26.jpg),
    and you will recall that our sequential scan runs at ![Getting ready](img/4520OT_10_16.jpg),
    so it is definitely slower.
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/4520OT_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'However, *Guy Blelloch* found ways to improve this, and they are based on the
    idea of building a balanced binary tree and building out that tree by performing
    addition on each node (conceptually speaking). Because such a tree with *n* leaves
    (which is corresponding to the number of elements in the array) would have ![Getting
    ready](img/4520OT_10_27.jpg) levels and each level has *2^d* nodes, the runtime
    complexity is ![Getting ready](img/4520OT_10_16.jpg). The following diagram is
    an illustration of how a balanced binary tree can compute the array of arbitrary
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/4520OT_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The previous diagram created juxtaposition, and it alters the way the same piece
    of data you saw, that is, one dimensional flat array containing arbitrary values.
    Imagine a tree of computations that scans and operates on two values. One way
    of storing those partial sums is to write the value in place to the array and
    another way is to use shared memory on the device.
  prefs: []
  type: TYPE_NORMAL
- en: 'The astute reader in you would notice that we can probably parallelize the
    computation at each level of the tree by allowing one thread to read two elements,
    sum them up, and write them back into the array, and then you just read off the
    last element of that array for the final sum. This algorithm that we just described
    is known as a **reduction** kernel or an **up-sweep** kernel (since we are sweeping
    values up to the root of the tree), and we have seen how it works in the chapter
    where we discussed about sparse matrix computations in OpenCL. The following is
    the more formal definition of the reduction phase by *Guy Blelloch* when it''s
    applied to a balanced binary tree with depth ![Getting ready](img/4520OT_10_28.jpg):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: You might think that this up-sweep kernel still doesn't compute the prefix sums,
    but we do appear to have found a solution to solving summation in parallel; at
    this point, the following diagram will help us learn what actually goes on during
    a run of the up-sweep, and we find it helpful to flatten the loop a little to
    examine its memory access pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming we have eight elements in our array (that is, `n = 8`), our tree would
    have a depth of `3` and `d` would range from `0` to `2`. Imagining that we are
    at `d = 0`, through to `2` we would have the following expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The next diagram best explains the evaluation of the preceding expressions,
    and a picture does reveal more about the story than plain equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/4520OT_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From this diagram, we can observe that partial sums are built up at each level
    of the tree and one of the efficiencies introduced here is not repeating any addition,
    that is, no redundancies. Let's demonstrate how this would work for an array of
    eight elements, and we will also employ the up-sweep algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the writes that occurred at each level of
    the tree we''re scanning; in that diagram, the boxes colored blue represent the
    partial sums that were built up at each level of the tree, and the red box represents
    the final summed value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/4520OT_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To be able to compute the prefix sums from the up-sweep phase we need to proceed
    from the root of this tree and perform a *down-sweep* using this algorithm by
    *Guy Blelloch*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This down-sweep works its way down from the top (or root) of the tree after
    the reduce phase and builds the prefix sums. Let's flatten the loop to examine
    its memory access pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'As before with the up-sweep, let''s assume that we have eight elements (that
    is, `n = 8`); we would have a depth of `3`, and that implies `d` would range from
    `0` to `2`. The following are the flattened expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram best expresses how the prefix sums are computed from
    the reduce/up-sweep phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/4520OT_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let us concretize these ideas by looking at how the down-sweep phase would
    proceed after the reduce/up-sweep phase using the following diagram; the `input`
    array is the original array, and we have kept it there for you to verify that
    the prefix sum computation according to the previous algorithm is correct. The
    lower portion of the diagram illustrates how memory is accessed. Keep in mind
    that updates are done in place, and when you combine the diagrams of the up-sweep
    and down-sweep phases, you''ll notice that we make two passes over the original
    input array to arrive at the solution of prefix sums, which is what we wanted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/4520OT_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How to do it …
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The kernel we present here is found in `Ch10/RadixSort_GPU/RadixSort.cl`, and
    the implementation drew inspiration from the academic paper entitled *Radix Sort
    for Vector Multiprocessors* by *Mark Zagha* and *Guy E. Blelloch* for 32-bit integers.
    The algorithm is based on the LSD Radix sort, and it iterates all the keys while
    shifting the keys based on the chosen Radix and executing OpenCL kernels in sequence;
    this is best described in the previous diagram.
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, we present the sequential version of the Radix sort that was translated
    based on *Zagha* and *Blelloch*, and like what we have done previously, this is
    the golden reference which we will use to determine the correctness of the data
    calculated by the OpenCL equivalent. We won''t spend too much time discussing
    about this implementation here, but rather it serves as a reference point where
    you can draw the similarities and contrasts when we demonstrate how the parallel
    and sequential code differs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This sequential code is akin to the `lsd_sort` code we showed earlier, and it
    essentially builds a histogram of the examined keys that uses the counting sort
    to sort them, and it keeps doing this until all data is acted upon.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following kernels are taken from `Ch10/RadixSort_GPU/RadixSort.cl`, and
    we''ll refer to the appropriate code when we explain the internal workings of
    the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The strategy we present here is to break keys, that is, break 32-bit integers
    into 8-bit digits, and then sort them one at a time starting from the least significant
    digit. Based on this idea, we are going to loop four times and at each loop number
    *i*, we are going to examine the *i* numbered 8-bit digit.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general looping structure based on the previous description is given in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The three invocations in the loop are the work horses of this implementation
    and they invoke the kernels to compute the histogram from the input based on the
    current byte we are looking at. The algorithm will basically compute the histogram
    of the keys that it has examined; the next phase is to compute the prefix sums
    (we'll be using the Hillis and Steele algorithm for this), and finally we will
    update the data structures and write out the values in a sorted order. Let's go
    into detail about how this works.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the host code, you will need to prepare the data structures slightly differently
    than what we have shown you so far, because these structures need to be shared
    across various kernels while we swing between host code and kernel code. The following
    diagram illustrates this general idea for `runKernels()`, and this situation is
    because we created a single command queue which all kernels will latch on to in
    program order; this applies to their execution as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/4520OT_10_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For this implementation, the data structure that holds the unsorted data (that
    is, `unsortedData_d`) needs to be read and shared across the kernels. Therefore,
    you need to create the device buffer with the flag `CL_MEM_USE_HOST_PTR` since
    the OpenCL specification guarantees that the implementations cached it across
    multiple kernel invocations. Next, we will look at how the histogram is computed
    on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'The computation of the histogram is based on the threaded histogram we introduced
    in a previous chapter, but this time around, we decided to show you another implementation
    which is based on using atomic functions in OpenCL, and in particular using `atomic_inc()`.
    The `atomic_inc` function will update the value pointed by the location by one.
    The histogram works on the OpenCL-supported GPU because we have chosen to use
    the shared memory and CPU doesn''t support that yet. The strategy is to divide
    our input array into blocks of *N x R* elements where *R* is the radix (in our
    case *R = 8* since each digit is 8-bits wide and *2⁸=256*) and *N* is the number
    of threads executing the block. This strategy is based on the assumption that
    our problem sizes are always going to be much larger than the amount of threads
    available, and we configure it programmatically on the host code prior to launching
    the kernel as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: By setting up the OpenCL thread block to be equal to `BIN_SIZE`, that is, 256,
    the kernel waits for the computation to complete by polling the OpenCL device
    for its execution status; this poll-release mechanism is encapsulated by `waitAndReleaseDevice()`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you have multiple kernel invocations and one kernel waits on the other,
    you need synchronization, and OpenCL provides this via `clGetEventInfo` and `clReleaseEvent`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the histogram kernel, we built up the histogram by reading the inputs into
    shared memory (after initializing it to zero), and to prevent any threads from
    executing kernel code that reads from shared memory before all data is loaded
    into it, we placed a memory barrier as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's debatable whether we should initialize the shared memory, but it's best
    practice to initialize data structures, just like you would do in other programming
    languages. The trade off, in this case, is program correctness versus wasting
    processor cycles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we shift the data value (residing in shared memory) by a number, `shiftBy`,
    which is the key we are sorting, extract the byte, and then update the local histogram
    atomically. We placed a memory barrier thereafter. Finally, we write out the binned
    values to their appropriate location in the global histogram, and you will notice
    that this implementation performs what we call *scattered writes*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Once the histogram is established, the next task that `runKernels()` performs
    is to execute the computations of prefix sums in the kernels `blockScan`, `blockPrefixSum`,
    `blockAdd`, `unifiedBlockScan`, and `mergePrefixSums` in turn. We'll explain what
    each kernel does in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general strategy for this phase (encapsulated in `computeBlockScans()`)
    is to pre-scan the histogram bins so that we generate the prefix sums for each
    bin. We then write out that value to an auxiliary data structure, `sum_in_d`,
    and write out all intermediary sums into another auxiliary data structure, `scannedHistogram_d`.
    The following is the configuration we sent to the `blockScan` kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The general strategy behind scanning is illustrated in the following diagram,
    where the input is divided into separate blocks and each block will be submitted
    for a block scan. The generated results are prefix sums, but we need to collate
    these results across all blocks to obtain a cohesive view. After which, the histogram
    bins are updated with these prefix sum values, and then finally we can use the
    updated histogram bins to sort the input array.
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/4520OT_10_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at how the block scan is done by examining `blockScan`. First,
    we load the values from the previously computed histogram bin into its shared
    memory as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we perform the Hillis and Steele prefix sum algorithm locally, and build
    the summed values for the current block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we write out a prefix sum for this block to `sum_in_d`, represented
    in the following code by `sumBuffer`, and the intermediary prefix sums to the
    `scannedHistogram_d` object, represented here by `output`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram illustrates this concept for two parallel block scans
    (assuming we have a shared memory that holds eight elements) and shows how it''s
    stored into the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/4520OT_10_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'At this phase of the computation, we have managed to compute the prefix sums
    for all the individual blocks. We need to collate them through the next phase,
    which is in the kernel `blockPrefixSum` where the individual block''s summed value
    is accumulated by each work item. The work done by each thread will compute the
    sum across different blocks. Depending on the thread with ID, `i`, will gather
    all sums from block number `0` to `(i – 1)`. The following code in `blockPrefixSum`
    illustrates this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The astute reader will notice that we have left out the prefix sum for one
    block, and the following remedies are obtained by computing the final accumulated
    prefix sums for this block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The following diagram best represents what computation goes on in the previous
    kernel code. It assumes that we have a block scan for 16 elements that has been
    completed in `blockScanKernel`, and each element contains the prefixed sum. To
    collate these sums, we configure our kernel to run eight threads with a striding
    factor of `8` (assuming a block size of eight), and the diagram expresses what
    each of the eight threads are working on. The threads collate the sums by working
    out the summation of the entire input, progressively computing ![How it works…](img/4520OT_10_28_1.jpg)
    and writing them out to `sum_out_d` and `summary_in_d`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a diagram that illustrates the process where given an input,
    all elements of that input are the summed values of the block scan for all blocks;
    the algorithm basically sums everything and writes to the output array:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/4520OT_10_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, we have to collate the intermediary prefix sums computed, that
    is, ![How it works…](img/4520OT_10_29.jpg) inside `sum_out_d`, and with that from
    `scannedHistogram_d`. We basically add the two intermediary sums together using
    `blockAddKernel`. The following is how we prepare the kernel prior to launch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We then basically collate them back to `scannedHistogram_d` with `blockAddKernel`
    whose code is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we perform another prefix sum to collate the values in `summary_in_d`,
    as all elements inside that array contains each individual block's prefix sum.
    Because our chosen Radix value is `256`, we need to work out the prefix sums computation
    for blocks `0` to `y` using ![How it works…](img/4520OT_10_30.jpg)through to ![How
    it works…](img/4520OT_10_31.jpg). This is illustrated in the following diagram,
    and it is encapsulated in the `unifiedBlockScan` kernel. We won't show the kernel
    code as it's similar to the `blockPrefixSum` kernel.
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/4520OT_10_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point in time, we are left with writing the collated prefix sums we
    have just performed previously into `scannedHistogram_d`. This collation exercise
    is different from the previous one where we gather the intermediary prefix sums
    across the blocks, but nonetheless, it''s still a collation exercise, and we need
    to push in the values from `summary_in_d`. We accomplished this with `mergePrefixSumsKernel`
    with the inputs reflected in the following host code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `mergePrefixSumsKernel` exercise is a relatively simple exercise to shift
    the values to their proper positions with the following kernel code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'With this, the prefix sums are properly computed. The next phase of the algorithm
    will be to rank and permute the keys using each work item / thread to permute
    its 256 elements via the prescanned histogram bins, encapsulated in `computeRankNPermutations()`.
    The following is the host code for the kernel launch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Once the kernel has completed successfully, the data values will be in a sorted
    order and will be held in the device memory by `sortedData_d`. We need to copy
    those data into `unsortedData_d` again, and we will continue to do this until
    we have not completed the iteration of the keys.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `rankNPermute` kernel, we will again make use of shared memory. The
    data into shared memory, and the data is organized as `GROUP_SIZE * RADIX` where
    the `GROUP_SIZE = 64` and `RADIX = 256` expressions hold true, and because each
    work group is configured to execute with 64 threads, we basically have one thread
    hydrating 256 elements of its shared memory (which the following code snippet
    demonstrates):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, it ranks the elements based on the same idea as in the sequential algorithm,
    and you should refer back to that now. The difference is that we are pulling data
    values from `unsortedData` in global device memory, processing them in device
    memory, figuring out where the values should be, and writing them out to `sortedData`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: After the ranking and permutation is done, the data values in the `sortedData_d`
    object are sorted based on the current examined key. The algorithm will copy the
    data in `sortedData_d` into `unsortedData_d` so that the entire process can be
    repeated for a total of four times.
  prefs: []
  type: TYPE_NORMAL
