<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Classifying UFO Sightings</h1></div></div></div><p>In this chapter, we're going to look at a dataset of UFO sightings. Sometimes, data analysis begins with a specific question or problem. Sometimes, however, it's more nebulous and vague. We'll engage with this UFO sighting dataset, and along the way, we'll learn more about data exploration, data visualization, and topic modeling before we dive into Naïve Bayesian classification.</p><p>This dataset was collected by the <strong>National UFO Reporting Center</strong> (<strong>NUFORC)</strong>, and is available at <a class="ulink" href="http://www.nuforc.org/">http://www.nuforc.org/</a>. They <a id="id281" class="indexterm"/>have included dates, rough locations, shapes, and descriptions of the sightings. We'll download and pull in this dataset. We'll see how to extract more structured data from messy, free-form text. And from there, we'll see how to visualize, analyze, and gain insights into our data.</p><p>In the process, we'll discover when is the best time to look for UFOs. We'll also learn what their important characteristics are. And we'll learn how to tell a description of a possible hoax sighting from one that may be real. In the end, hopefully, we'll be better prepared for seeing one of these ourselves. After all, we'll know when to look and for what to look.</p><div><div><div><div><h1 class="title"><a id="ch04lvl1sec26"/>Getting the data</h1></div></div></div><p>For this chapter, actually<a id="id282" class="indexterm"/> acquiring the data will be relatively easy. In other chapters, this step involves screen scraping, SPARQL, or other data extraction, munging, and cleaning techniques. For this dataset, we'll just download it from<a id="id283" class="indexterm"/> Infochimps (<a class="ulink" href="http://www.infochimps.com/">http://www.infochimps.com/</a>). Infochimps<a id="id284" class="indexterm"/> is a company (and their website) devoted to Big Data<a id="id285" class="indexterm"/> and doing more with data analysis. They provide a collection of datasets that are online and freely available. To download this specific <a id="id286" class="indexterm"/>dataset, browse to <a class="ulink" href="http://www.infochimps.com/datasets/60000-documented-ufo-sightings-with-text-descriptions-and-metada">http://www.infochimps.com/datasets/60000-documented-ufo-sightings-with-text-descriptions-and-metada</a> and download the data from the link there, as shown in the following screenshot:</p><div><img src="img/4139OS_04_01.jpg" alt="Getting the data"/></div><p>The data is in a <a id="id287" class="indexterm"/>ZIP-compressed file. This expands the files into the <code class="literal">chimps_16154-2010-10-20_14-33-35</code> directory. This contains a file that lists metadata for the dataset as well as the data itself in several different formats. For the purposes of this chapter, we'll use the <strong>tab separated values</strong> (<strong>TSV</strong>) file. It's <a id="id288" class="indexterm"/>similar to a <a id="id289" class="indexterm"/>
<strong>comma separated values</strong> (<strong>CSV</strong>) file, but it uses the tab character as a delimiter instead of a comma. This works nicely, because the tab character is used less often in text files in general, so it's often possible to use this data format without escaping many, if any, fields.</p><p>If we open the <code class="literal">16154.yaml</code> file, we'll see metadata and other information about the dataset. And we learn that the fields in the dataset are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">sighted_at</code>: The date (as YYYYMMDD) the sighting happened</li><li class="listitem" style="list-style-type: disc"><code class="literal">reported_at</code>: The date the sighting was reported to NUFORC</li><li class="listitem" style="list-style-type: disc"><code class="literal">location</code>: The city and state the event happened in</li><li class="listitem" style="list-style-type: disc"><code class="literal">shape</code>: The shape of the object</li><li class="listitem" style="list-style-type: disc"><code class="literal">duration</code>: The duration the event lasted</li><li class="listitem" style="list-style-type: disc"><code class="literal">description</code>: A longer description of the sighting as a raw text string</li></ul></div><p>We can get a better feel for this data by examining a row from the downloaded file. The following table represents what the fields contain for that record:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Field</p>
</th><th style="text-align: left" valign="bottom">
<p>Value</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">sighted_at</code>
</p>
</td><td style="text-align: left" valign="top">
<p>19950202</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">reported_at</code>
</p>
</td><td style="text-align: left" valign="top">
<p>19950203</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">location</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Denmark, WI</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">shape</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Cone</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">duration</code>
</p>
</td><td style="text-align: left" valign="top">
<p>75 min</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">description</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Caller, and apparently several other people, witnessed multiple strange craft streaking through the night sky in the vicinity of Denmark and Mirabel, WI. Craft were seen to streak overhead, as well as to descend vertically, as fast as a meteorite, then stop suddenly just above the ground. During the last 30 minutes of the sighting, aircraft, which appeared to be US military craft, were seen either pursuing, or chaperoning, the strange craft. The objects were cone shaped, with a red nose and a green tail (sic).</p>
</td></tr></tbody></table></div><p>Browsing through other rows, <a id="id290" class="indexterm"/>you will observe that some important fields—shape and duration—may be missing data. The description has XML entities and abbreviations such as <em>w/</em> and <em>repts</em>.</p><p>Let's see what we can do with that.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec27"/>Extracting the data</h1></div></div></div><p>Before we go further, let's <a id="id291" class="indexterm"/>look at the following Leiningen 2 (<a class="ulink" href="http://leiningen.org/">http://leiningen.org/</a>) <code class="literal">project.clj</code> file that we'll use for this chapter:</p><div><pre class="programlisting">(defproject ufo-data "0.1.0-SNAPSHOT"
  :plugins [[lein-cljsbuild "0.3.2"]]
  :profiles {:dev {:plugins [[com.cemerick/austin "0.1.0"]]}}
  :dependencies [[org.clojure/clojure "1.5.1"]
                 [org.clojure/data.json "0.2.2"]
                 [org.clojure/data.csv "0.1.2"]
                 [clj-time "0.5.1"]
                 [incanter "1.5.2"]
                 [cc.mallet/mallet "2.0.7"]
                 [me.raynes/fs "1.4.4"]]
  :cljsbuild
    {:builds [{:source-paths ["src-cljs"],
               :compiler {:pretty-printer true,
                          :output-to "www/js/main.js",
                          :optimizations :whitespace}}]})</pre></div><p>The preceding code shows that over the course of this chapter, we'll parse time with the <code class="literal">clj-time</code> library (<a class="ulink" href="https://github.com/clj-time/clj-time">https://github.com/clj-time/clj-time</a>). This provides a rich, robust date and time library. We'll also use ClojureScript (<a class="ulink" href="https://github.com/clojure/clojurescript">https://github.com/clojure/clojurescript</a>) for the visualizations.</p><p>Our first step in working with this data is to load it from the data file. To facilitate this, we'll read it into a record type that we'll define just to store the UFO sightings. We'll work with the <code class="literal">model.clj</code> file placed at <code class="literal">src/ufo_data/</code>. The following is a namespace declaration with the imports and requirements that we'll use in this module:</p><div><pre class="programlisting">(ns ufo-data.model
  (:require [clojure.java.io :as io]
            [clojure.core.reducers :as r]
            [clojure.string :as str]
            [clojure.data.json :as json]
            [clj-time.format :as tf]
            [ufo-data.text :as t]
            [ufo-data.util :refer :all]
            [me.raynes.fs :as fs])
  (:import [java.lang StringBuffer]))</pre></div><p>Now we'll define the record. It simply lists the same fields that we walked through earlier. We also include a few new fields. We'll use these to parse the year, month, and season from the <code class="literal">reported_at</code> field as follows:</p><div><pre class="programlisting">(defrecord UfoSighting
  [sighted-at reported-at location shape duration description
   year month season])</pre></div><p>Now, when we take a row from <a id="id292" class="indexterm"/>the TSV file, we'll need to parse it into one of these structures. Because each line of input only has six fields, we'll make sure that it's padded out to nine fields. We'll also verify that there are exactly six input fields. If there are more or less, we'll take steps to either further pad the fields or to join some of the fields, as shown in the following code:</p><div><pre class="programlisting">(defn -&gt;ufo [row]
  (let [row (cond
              (&gt; (count row) 6)
                   (concat (take 5 row)
                      [(str/join \t (drop 5 row))])
              (&lt; (count row) 6)
                   (concat row (repeat (- 6 (count row)) nil))
              :else row)]
    (apply -&gt;UfoSighting (concat row [nil nil nil]))))</pre></div><p>Some of the fields (the most important ones, actually) are dates, and we'll want to parse them into valid date objects. To do this, we'll use the excellent <code class="literal">clj-time</code> library (<a class="ulink" href="https://github.com/clj-time/clj-time">https://github.com/clj-time/clj-time</a>). This provides a more "Clojuresque" interface for the Joda time library (<a class="ulink" href="http://joda-time.sourceforge.net/">http://joda-time.sourceforge.net/</a>). The code that does this takes a custom date format and attempts to parse the dates. If any fail, we just fall back on using <code class="literal">nil</code>. Look at the following code:</p><div><pre class="programlisting">(def date-formatter (tf/formatter "yyyyMMdd"))
(defn read-date [date-str]
  (try
    (tf/parse date-formatter date-str)
    (catch Exception ex
      nil)))</pre></div><p>We use the following function to coerce the raw string date fields into the more useful date objects that Joda time provides:</p><div><pre class="programlisting">(defn coerce-fields [ufo]
  (assoc ufo
         :sighted-at (read-date (:sighted-at ufo))
         :reported-at (read-date (:reported-at ufo))))</pre></div><p>That's all that we need to load the data. Now we can write the function that will actually take care of reading the data from the file on disk into a sequence of records, as follows:</p><div><pre class="programlisting">(defn read-data
  [filename]
  (with-open [f (io/reader filename)]
    (-&gt;&gt; (csv/read-csv f :separator \tab)
      vec
      (r/map -&gt;ufo)
      (r/map coerce-fields)
      (into []))))</pre></div><p>Now that we can read in the data, we can start picking it apart and learn about the data that we have.</p></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec28"/>Dealing with messy data</h1></div></div></div><p>The first thing that we<a id="id293" class="indexterm"/> need to<a id="id294" class="indexterm"/> deal with is qualitative data from the <code class="literal">shape</code> and <code class="literal">description</code> fields.</p><p>The <code class="literal">shape</code> field seems like a likely place to start. Let's see how many items have good data for it:</p><div><pre class="programlisting">
<strong>user=&gt; (def data (m/read-data "data/ufo_awesome.tsv"))</strong>
<strong>user=&gt; (count (remove (comp str/blank? :shape) data))</strong>
<strong>58870</strong>
<strong>user=&gt; (count (filter (comp str/blank? :shape) data))</strong>
<strong>2523</strong>
<strong>user=&gt; (count data)</strong>
<strong>61393</strong>
<strong>user=&gt; (float 2506/61137)</strong>
<strong>0.04098991</strong>
</pre></div><p>So 4 percent of the data does not have the <code class="literal">shape</code> field set to meaningful data. Let's see what the most popular values for that field are:</p><div><pre class="programlisting">
<strong>user=&gt; (def shape-freqs</strong>
<strong>           (frequencies</strong>
<strong>             (map str/trim</strong>
<strong>                  (map :shape</strong>
<strong>                       (remove (comp str/blank? :shape) data)))))</strong>
<strong>#'user/shape-freqs</strong>
<strong>user=&gt; (pprint (take 10 (reverse (sort-by second shape-freqs))))</strong>
<strong>(["light" 12202]</strong>
<strong> ["triangle" 6082]</strong>
<strong> ["circle" 5271]</strong>
<strong> ["disk" 4825]</strong>
<strong> ["other" 4593]</strong>
<strong> ["unknown" 4490]</strong>
<strong> ["sphere" 3637]</strong>
<strong> ["fireball" 3452]</strong>
<strong> ["oval" 2869]</strong>
<strong> ["formation" 1788])</strong>
</pre></div><p>Interesting! The most frequent shape isn't a shape at all. The values <code class="literal">other</code> and <code class="literal">unknown</code> also rank pretty high. We<a id="id295" class="indexterm"/> can use the <code class="literal">shape</code> field, but we need to keep these things<a id="id296" class="indexterm"/> in mind.</p></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec29"/>Visualizing UFO data</h1></div></div></div><p>We'll spend a good bit of time <a id="id297" class="indexterm"/>visualizing the data, and we'll use the same system that we have in the previous chapters: a bit of HTML, a splash of CSS, and a lot of JavaScript, which we'll generate from ClojureScript.</p><p>We've already taken care of the configuration for using ClojureScript in the <code class="literal">project.clj</code> file that I mentioned earlier. The rest of it involves a couple of more parts:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The code to generate the JSON data for the graph. This will be in the <code class="literal">src/ufo_data/analysis.clj</code> file. We'll write this code first.</li><li class="listitem" style="list-style-type: disc">An HTML page that loads the JavaScript libraries that we'll use—jQuery<a id="id298" class="indexterm"/> (<a class="ulink" href="https://jquery.org/">https://jquery.org/</a>) and D3<a id="id299" class="indexterm"/> (<a class="ulink" href="http://d3js.org/">http://d3js.org/</a>)—and creates a <code class="literal">div</code> container in which to put the graph itself.</li><li class="listitem" style="list-style-type: disc">The source code for the graph. This will include a namespace for utilities in <code class="literal">src-cljs/ufo-data/utils.cljs</code> and the main namespace at <code class="literal">src-cljs/ufo-data/viz.cljs</code>.</li></ul></div><p>With these prerequisites in place, we can start creating the graph of the frequencies of the different shapes.</p><p>First, we need to make sure we have what we need for this namespace. This will be in the <code class="literal">src/ufo_data/analysis.clj</code> file. The following code gives the <code class="literal">ns</code> declaration. Most of these dependencies won't be needed immediately, but we will use them at some point in this chapter:</p><div><pre class="programlisting">(ns ufo-data.analysis
  (:require [ufo-data.text :as t]
            [clj-time.core :as time]
            [clj-time.coerce :as coerce]
            [clojure.string :as str]
            [incanter.core :as i]
            [incanter.stats :as s]))</pre></div><p>Now, we'll define a rather long function that takes the input data. It will pull out the <code class="literal">shape</code> field, remove blanks, break it into words, and count their frequencies. A few of the functions that this function uses aren't listed here, but they're available in the code download for this chapter. Then, the following function will remove any shapes that don't occur at least once, reverse-sort them by their frequencies, and finally turn them into map structures in a vector:</p><div><pre class="programlisting">(defn get-shape-freqs
  "This computes the :shape field's frequencies. This also
  removes any items with a frequency less than min-freq."
  [coll min-freq]
  (-&gt;&gt; coll
    (map :shape)
    (remove str/blank?)
    (map normalize)
    (mapcat tokenize)
    frequencies
    (remove #(&lt; (second %) min-freq))
    (sort-by second)
    reverse
    (map #(zipmap [:shape :count] %))
    (into [])))</pre></div><p>We can then use the <code class="literal">clojure.data.json</code> package (<a class="ulink" href="https://github.com/clojure/data.json">https://github.com/clojure/data.json</a>) to save it to disk. I saved it to <code class="literal">www/term-freqs.json</code>. The following is a small sample of the first two records:</p><div><pre class="programlisting">[{"count":12202,"shape":"light"},
 {"count":6082,"shape":"triangle"},
 …]</pre></div><p>Now we need a web page in which to draw the graph. I downloaded a template from the HTML 5 Boilerplate project (<a class="ulink" href="http://html5boilerplate.com/">http://html5boilerplate.com/</a>) and saved it as <code class="literal">www/term-freqs.html</code>. I removed almost everything inside the <code class="literal">body</code> tag. I left only the following <code class="literal">div</code> tag and a string of <code class="literal">script</code> tags:</p><div><pre class="programlisting">&lt;div class="container"&gt;&lt;/div&gt;</pre></div><p>This takes care of the <a id="id300" class="indexterm"/>HTML page, so we can move on to the ClojureScript that will create the graph.</p><p>All of the ClojureScript files for this chapter will be in the <code class="literal">src-cljs</code> directory. Under this directory is a tree of Clojure namespaces, similar to how the code in <code class="literal">src</code> is organized for Clojure. Most of the ClojureScript for this chapter will be in the <code class="literal">src-cljs/ufo-data/viz.cljs</code> file. There are a number of utility functions in another namespace, but those are primarily boilerplate, and you can find them in the code download for this chapter. The following function loads the data and creates the graph. We'll walk through it step-by-step.</p><div><pre class="programlisting">(defn ^:export term-freqs []
  (let [{:keys [x y]} (u/get-bar-scales)
        {:keys [x-axis y-axis]} (u/axes x y)
        svg (u/get-svg)]
    (u/caption "Frequencies of Shapes" 300)
    (.json js/d3 "term-freqs.json"
      (fn [err json-data]
        (u/set-domains json-data [x get-shape] [y get-count])
        (u/setup-x-axis svg x-axis)
           (u/setup-y-axis svg y-axis "")
<strong>        (.. svg</strong>
<strong>          (selectAll ".bar") (data json-data)</strong>
<strong>          (enter)</strong>
<strong>          (append "rect")</strong>
<strong>          (attr "id" #(str "id" (get-shape %)))</strong>
<strong>          (attr "class" "bar")</strong>
<strong>          (attr "x" (comp x get-shape))</strong>
<strong>          (attr "width" (.rangeBand x))</strong>
<strong>          (attr "y" (comp y get-count))</strong>
<strong>          (attr "height"</strong>
<strong>                #(- u/height (y (get-count %))))))))))</strong>
</pre></div><p>The part of the function before the highlighting sets up the axes, the scales, and the parent SVG element. Then, we load the data from the server. Once it's loaded, we set the domains on the axes and draw the axes themselves.</p><p>The main part of the<a id="id301" class="indexterm"/> function is highlighted. This creates the bars in the SVG element. All these tasks take place in the following manner:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">(selectAll ".bar") (data data)</code>: This command selects all elements with the <code class="literal">bar</code> class. Currently, there aren't any elements to select because we haven't created any, but that's all right. Then it joins those elements with the data.</li><li class="listitem" style="list-style-type: disc"><code class="literal">(enter)</code>: This command starts processing any data rows that don't have previously created <code class="literal">.bar</code> elements.</li><li class="listitem" style="list-style-type: disc"><code class="literal">(append "rect")</code>: For each row of data with no <code class="literal">.bar</code> elements, this command appends a <code class="literal">rect</code> tag to the element.</li><li class="listitem" style="list-style-type: disc"><code class="literal">(attr "id" #(str "id" (get-shape %))) (attr "class" "bar")</code>: This line of code adds the <code class="literal">ID</code> and <code class="literal">class</code> attributes to the rectangle.</li><li class="listitem" style="list-style-type: disc"><code class="literal">(attr "x" (comp x get-shape)) (attr "y" (comp y get-count))</code>: This line of code populates the <em>x</em> and <em>y</em> attributes with values from each data row, projected onto the graph's pixel grid.</li><li class="listitem" style="list-style-type: disc"><code class="literal">(attr "width" (.rangeBand x)) (attr "height" #(- u/height (y (get-count %)))))</code>: This line of code finally sets the height and width for each rectangle.</li></ul></div><p>These commands together create the graph. There's a little bit of CSS involved, also. Refer to the code download for all the details. But in the end, the graph looks as follows:</p><div><img src="img/4139OS_04_02.jpg" alt="Visualizing UFO data"/></div><p>This set of files <a id="id302" class="indexterm"/>acts as a framework for all of the visualizations and charts that we'll see in this chapter. Although bar charts are simple, once in place, this framework can be used for much more complex and sophisticated types of graphs.</p><p>This graph shows us more clearly what the quick frequency dump at the REPL also showed us: most of the people listed the shape as <em>light</em>. More than twice as many people listed the shape of <em>light</em> as listed the runner-up, <em>triangle</em>. In fact, almost one in five observations listed that as the shape.</p><p>Now let's try to get a feel for some other facts about this data.</p><p>First, when have UFOs been observed? To find this out, we have to group the observations by the year from the <code class="literal">sighted-at</code> field. We group the items under each year, and then we save that to graph it. The following are the functions in <code class="literal">ufo-data.analysis</code> that will take care of getting the right data for us:</p><div><pre class="programlisting">(defn group-by-year [coll]
  (group-by #(timestamp-&gt;year (:sighted-at %)) coll))
(defn get-year-counts [by-year]
  (map #(zipmap [:year :count] %)
       (map (on-second count)
            by-year)))</pre></div><p>Once we've created the graph from this data, the following is the output:</p><div><img src="img/4139OS_04_03.jpg" alt="Visualizing UFO data"/></div><p>This graph suggests<a id="id303" class="indexterm"/> that the number of observations in the dataset increased dramatically in the mid-1990s, and that they have continued to increase. NUFORC, the organization that collects the data, was established in 1974. I was unable to discover when they began collecting data online, but the increased widespread use of the Internet could also be a factor in the increase in reported sightings. Also, wider cultural trends, such as the popularity of X-Files, may have contributed to a greater awareness of UFOs during this time period.</p><p>As we continue to get to know our data, another interesting distribution is looking at the number of sightings each month. The process for getting this data is very similar to the process for getting the number of sightings by year, so we won't go into that now.</p><div><img src="img/4139OS_04_04.jpg" alt="Visualizing UFO data"/></div><p>The preceding graph shows that<a id="id304" class="indexterm"/> the summer, starting in June, is a good time to see a UFO. One explanation for this is that during these months, people are outside more in the evenings.</p></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec30"/>Description</h1></div></div></div><p>While the <em>shape</em> field is important, the <em>description</em> <a id="id305" class="indexterm"/>has more information. Let's see what we can do with it.</p><p>First, let's examine a few and see what some of them look like. The following example is one that I selected randomly:</p><div><blockquote class="blockquote"><p><em>Large boomerang shaped invisible object blocked starlight while flying across sky. I have a sketch and noted the year was 1999, but did not write down the day. The sighting took place in the late evening when it was completely dark and the sky was clear and full of stars. Out of the corner of my eye, I noticed movement in the sky from the north moving to the south. When I looked closer, however, it wasn&amp;rsquo;t an object that I was seeing move, rather it was the disappearance and reappearance of stars behind an object. The object itself was black or invisible with no lights. Given the area of stars that were blocked out, I would say the object was five times larger than a jet. It was completely silent. It was shaped like a boomerang only a little more rounded in front rather than triangle and a slightly sharper points on the &amp;ldquo;wing&amp;rdquo; tips. Since the object was invisible, I can only suggest the shape based on the black area absent of stars like a silhouette as it moved across the sky. If the object was indeed five times the size of a jet and flying at about the attitude of a jet, then it was moving much faster than a jet. I blinked a couple times, looked away and looked back, and then followed the object across the remainder of the horizon until it was out of sight. In all it took about 8-10 seconds to span the sky and flew at the same altitude the whole time. Given the triangular shape, I suppose it could have been a low-flying Stealth Bomber that just appeared much larger if flying low. But is a Stealth completely silent? Also, Stealth Bombers have three triangles pointing backwards from the mid section. The object I saw did not seem to have any mid section as such.((NUFORC Note: Witness indicates that date of incident is approximate. PD))</em></p></blockquote></div><p>So we can see that <a id="id306" class="indexterm"/>some examples are fairly long, and they may have characters encoded as HTML/XML entities (<code class="literal">&amp;ldquo;</code> and <code class="literal">&amp;rdquo;</code> in this example). And this quote is relatively clean: some have two or more words jammed together with just punctuation—often several periods—stuck between the words.</p><p>In order to deal with this data, we'll need to clean it up some and break the words out, or tokenize it. You can see the details of this in the code download, most of which is just pasting together a lot of string manipulation methods, but it's helpful to remind ourselves with what we're working and how we need to deal with it. I also filtered on a standard English stop-words list, which I augmented by adding a few words that are specific to the <em>description</em> fields, such as <em>PD</em> and <em>NUFORC</em>.</p><p>Let's see what the most frequent words are in the description fields:</p><div><pre class="programlisting">
<strong>user=&gt; (def descr-counts (a/get-descr-counts data 50))</strong>
<strong>#'user/descr-counts</strong>
<strong>user=&gt; (take 10 descr-counts)</strong>
<strong>({:count 85428, :descr "object"}</strong>
<strong> {:count 82526, :descr "light"}</strong>
<strong> {:count 73182, :descr "lights"}</strong>
<strong> {:count 72011, :descr "sky"}</strong>
<strong> {:count 58016, :descr "like"}</strong>
<strong> {:count 47193, :descr "one"}</strong>
<strong> {:count 40690, :descr "bright"}</strong>
<strong> {:count 38225, :descr "time"}</strong>
<strong> {:count 37065, :descr "could"}</strong>
<strong> {:count 35953, :descr "looked"})</strong>
</pre></div><p>This seems more like what we'd expect. The most frequent word is <em>object</em>, which seems appropriate for a corpus made up of people talking about things that they can't identify. The next two words are <em>light</em> and <em>lights</em>, which would be expected, especially since <em>light</em> is the most common item in the <em>shape</em> field.</p><p>Let's graph these terms too. We won't be able to see the details of the words' frequencies but it will give us a better feel for their distribution. There are enough tokens; however, we'll only look at the 75 most frequent ones in the following graph:</p><div><img src="img/4139OS_04_05.jpg" alt="Description"/></div><p>The distribution of<a id="id307" class="indexterm"/> these words seems very similar. In fact, it very roughly conforms to Zipf's law, which predicts the power-law distribution of many types of physical and social data, including language frequencies.</p></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec31"/>Topic modeling descriptions</h1></div></div></div><p>Another way to gain a better <a id="id308" class="indexterm"/>understanding of the descriptions is to use topic modeling. We learned about this text mining and machine learning algorithm in <a class="link" href="ch03.html" title="Chapter 3. Topic Modeling – Changing Concerns in the State of the Union Addresses">Chapter 3</a>, <em>Topic Modeling – Changing Concerns in the State of the Union Addresses</em>. In this case, we'll see if we can use it to create topics over these descriptions and to pull out the differences, trends, and patterns from this set of texts.</p><p>First, we'll create a new namespace to handle our topic modeling. We'll use the <code class="literal">src/ufo_data/tm.clj</code> file. The following is the namespace declaration for it:</p><div><pre class="programlisting">(ns ufo-data.tm
  (:require [clojure.java.io :as io]
            [clojure.string :as str]
            [clojure.pprint :as pp])
  (:import [cc.mallet.util.*]
           [cc.mallet.types InstanceList]
           [cc.mallet.pipe
            Input2CharSequence TokenSequenceLowercase
            CharSequence2TokenSequence SerialPipes
            TokenSequenceRemoveStopwords
            TokenSequence2FeatureSequence]
           [cc.mallet.pipe.iterator ArrayIterator]
           [cc.mallet.topics ParallelTopicModel]
           [java.io FileFilter]
           [java.util Formatter Locale]))</pre></div><p>The process for<a id="id309" class="indexterm"/> generating the topic model is very similar to the process that we used in <a class="link" href="ch03.html" title="Chapter 3. Topic Modeling – Changing Concerns in the State of the Union Addresses">Chapter 3</a>, <em>Topic Modeling – Changing Concerns in the State of the Union Addresses</em>. The first change that we need to make is that we'll load the instances from the in-memory data that we read earlier in this chapter. We'll create a function that pushes an input collection into an array and uses <code class="literal">ArrayIterator</code> to then feed that array into the processing pipeline. The function to train the data is the same as it was in the previous chapter.</p><p>In this chapter, we'll look at more functions that help us introspect on the trained model, the instances, and the probabilities and keywords that are important to each topic. The first function returns the words that apply to a topic and their weights. We get the feature vectors from the model, and the words themselves from the instance list as follows:</p><div><pre class="programlisting">(defn get-topic-words [model instances topic-n]
  (let [topic-words (.getSortedWords model)
        data-alpha (.getDataAlphabet instances)]
    (map #(vector (.lookupObject data-alpha (.getID %))
                  (.getWeight %))
         (iterator-seq (.. topic-words (get topic-n)
                         iterator)))))</pre></div><p>The other reporting function that we'll use ranks the instances by their probabilities for each topic. We can use this to look at the documents that are most likely to apply to any particular topic:</p><div><pre class="programlisting">(defn rank-instances [model data topic-id]
  (let [get-p (fn [n]
                [(aget (.getTopicProbabilities model n) topic-id)
                 (nth data n)])]
    (-&gt;&gt; data count range (map get-p) (sort-by first) reverse)))</pre></div><p>We can use these functions—as well as a few others based on these—from the REPL to explore our data.</p><p>Generally, when deciding how many topics to use, we'll want to use some kind of objective metric to find a good definition of the sets. However, for exploring in an off-the-cuff way, we'll use something more subjective. First, after playing around with the number of topics, I chose to use a topic count of twelve. Since all of these are really about just one thing, UFO sightings, I didn't expect there to be too many meaningful topics, even at a fairly detailed, narrow level. At twelve topics, there still seemed to be some vague, less helpful topics, but the more interesting topics that I'd seen before were still there. When I attempted fewer topics, some of those interesting topics disappeared.</p><p>So to get started, let's <a id="id310" class="indexterm"/>see the topics and the top 10 words for each. Remember that the topic descriptions here aren't generated by the computer. I came up with them after looking at the top words and the top few descriptions for those topics. Some of these are not obvious, given the small sample of terms included here. However, diving further into the topic terms, the documents themselves gave these categorizations. In some cases, I've included notes in parentheses as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Remembering childhood experiences</strong>: back time house craft car looked years remember road home</li><li class="listitem" style="list-style-type: disc"><strong>Lots of NUFORC notes, thanks to other organizations or local chapters</strong>: report witness nuforc note ufo sighting pd date reported object</li><li class="listitem" style="list-style-type: disc"><strong>Bright, silent objects in the sky</strong>: light sky bright lights white star object red moving looked</li><li class="listitem" style="list-style-type: disc"><strong>Visual descriptions</strong>: lights sky light time night red minutes objects back bright (this one doesn't have a clear topic as it's commonly defined)</li><li class="listitem" style="list-style-type: disc"><strong>White, red, and reddish-orange lights</strong>: light sky lights looked bright moving object back red white</li><li class="listitem" style="list-style-type: disc"><strong>Very fast, bright objects in the sky, compared to airplanes and meteors</strong>: lights sky object aircraft light west north appeared flying south</li><li class="listitem" style="list-style-type: disc"><strong>NUFORC notes. "Witness elects to remain totally anonymous"</strong>: nuforc note pd witness date sky light anonymous remain approximate</li><li class="listitem" style="list-style-type: disc"><strong>Vague</strong>: ufo camera air object picture time pictures photo photos day (again, the subject of this topic isn't clear)</li><li class="listitem" style="list-style-type: disc"><strong>Objects in the sky, no lights, or not mentioned</strong>: object driving road car lights shaped craft looked side feet</li><li class="listitem" style="list-style-type: disc"><strong>Abductions, visitations, fear. Close encounters of the fourth kind</strong>: time night back looked light house thing window lights sound</li><li class="listitem" style="list-style-type: disc"><strong>Sightings. Moving in different directions</strong>: lights object craft light flying white north south east moving</li><li class="listitem" style="list-style-type: disc"><strong>Technical descriptions</strong>: object sky light moving objects appeared bright time high north</li></ul></div><p>Several of these topics, for instance, the third, fifth, sixth, and ninth bullet, seem to be pretty generic descriptions of sightings. They describe lots of moving lights in the sky.</p><p>Other topics are more<a id="id311" class="indexterm"/> interesting. Topic one contained a number of descriptions written by people looking back at their childhood or college years. For instance, in the following paragraph, someone describes having a close encounter when they were about six years old. There are a number of spelling mistakes, and part of the reason I've kept it in is to illustrate just how messy this data can be:</p><div><blockquote class="blockquote"><p><em>Blus light, isolated road, possible missing timeI was six years old at the time, and even now, if I concentrate, I can recall what happened.  My mother, her best friend, and myself were driving on a section of road called "Grange Road."  Today, there are a lot of houses, but at the time, it was all farmland with maybe one or two houses. It was just after midnight, and I remember waking up.  I was alseep in the back seat, and I woke up feeling very frightened.  I sat up, and my mother and her friend were obviously worried.  The car we were in was cutting in-and-out, and finally died. As soon as the car stopped, we all saw a blue light directly ahead, maybe about 20 feet off of the ground, and about a football field legnth away.  It glided towards us, made no noise, and as it got to within 15 feet, it stopped in midair, hoovering. My mom grabbed me from the backseat and held on, and her friend was crying.  I was crying, too, because whatever it was, it was making us all upset.  After about five minutes, I don't recall what happened, because for whatever reason, I fell alseep.  Weird, I know, but I swear it happened. I woke up sometime later, and we three were sitting there, shocked, and the light was gone.  My mom and her friend - to this day - swear they had missing time, about 10 minutes worth. I hope this helps...((NUFORC Note:  Witness indicates that date of sighting is approximate.  PD))</em></p></blockquote></div><p>And some topics are puzzling, number eight, for instance. The top 10 documents for it had nothing obvious that appeared to make them a coherent subject. There may be something about some of the subtler vocabulary selection that was getting identified, but it wasn't readily apparent.</p></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec32"/>Hoaxes</h1></div></div></div><p>One of the most interesting<a id="id312" class="indexterm"/> finds in this was topic seven. This topic was focused on annotations added to the descriptions for which the witnesses wished to remain anonymous. But its most likely document was the following:</p><div><blockquote class="blockquote"><p><em>Round, lighted object over Shelby, NC, hovered then zoomed away. It was my birthday party and me and my friends were walking around the block about     21:30. I just happened to look up and I saw a circular object with  white and bright blue lights all over the bottom of it. It hovered in place for about 8 seconds then shot off faster than anything I have ever seen.((NUFORC Note:  Witness elects to remain totally anonymous; provides no contact information.  Possible hoax??  PD))((NUFORC Note:  Source of report indicates that the date of the sighting is approximate.  PD))</em></p></blockquote></div><p>What caught my attention was the note "Possible hoax??" Several other descriptions in this topic had similar notes, often including the word <em>hoax</em>.</p><p>Finding this raised an interesting possibility: could we train a classifier to recognize possible hoaxes? My initial reaction was to be skeptical. But I still thought it would be an interesting experiment.</p><p>Eventually, we'll want to load this data and process it with MALLET (<a class="ulink" href="http://mallet.cs.umass.edu/">http://mallet.cs.umass.edu/</a>). MALLET<a id="id313" class="indexterm"/> works a little easier with data that's kept in a particular directory format. The template for this is <code class="literal">base-directory/tag/data-file.txt</code>. In fact, we'll include a directory above these, and for <code class="literal">base-directory</code>, we'll define a directory for training data and one for test data.</p><p>The training group is used<a id="id314" class="indexterm"/> to train the classifier, and the test group is used to evaluate the classifier after it's been trained in order to determine how successful it is. Having two different groups for these tasks helps to find whether the classifier is over-fitting, that is, whether it has learned the training group so well that it performs poorly on new data.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec29"/>Preparing the data</h2></div></div></div><p>So before we get started, we'll <a id="id315" class="indexterm"/>preprocess the data to put it into a directory <a id="id316" class="indexterm"/>structure such as <code class="literal">src/ufo_data/</code>. All the code for this will go into the <code class="literal">model.clj</code> file. The namespace declaration for this is as follows:</p><div><pre class="programlisting">(ns ufo-data.model
  (:require [clojure.java.io :as io]
            [clojure.core.reducers :as r]
            [clojure.string :as str]
            [clojure.data.json :as json]
            [clj-time.format :as tf]
            [ufo-data.text :as t]
            [ufo-data.util :refer :all]
            [me.raynes.fs :as fs])
  (:import [java.lang StringBuffer]))</pre></div><p>Now, to process this dataset into a form that MALLET can deal with easily, we're going to put it through the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Read the data into a sequence of data records.</li><li class="listitem">Split out the NUFORC comments.</li><li class="listitem">Categorize the documents based on the comments.</li><li class="listitem">Partition them into directories based on the categories.</li><li class="listitem">Divide them into training and test sets.</li></ol></div><p>Let's see how <a id="id317" class="indexterm"/>we'll put these together.</p><div><div><div><div><h3 class="title"><a id="ch04lvl3sec07"/>Reading the data into a sequence of data records</h3></div></div></div><p>The data in the <a id="id318" class="indexterm"/>downloaded file has a number of problems with values that can't be escaped properly. I've cleaned this up and made a new data file, available at <a class="ulink" href="http://www.ericrochester.com/clj-data-master/data/ufo.json">http://www.ericrochester.com/clj-data-master/data/ufo.json</a>. I've saved this into my <code class="literal">data</code> directory and bound that path to the name <code class="literal">*data-file*</code>. You can find this and a few other definitions in the code download for this chapter.</p><p>But primarily, I'd like to focus on the data record for a minute. This just contains the fields from the JSON objects being read in. The following definition will serve as documentation of our data and make working with the rows a little easier:</p><div><pre class="programlisting">(defrecord UfoSighting
  [sighted-at reported-at location shape duration description
   year month season])</pre></div><p>The data as we read it in from the JSON file won't be quite right, however. We'll still need to convert date strings into data objects. We'll do that with <code class="literal">read-date</code>, which parses a single date string, and with <code class="literal">coerce-fields</code>, which coordinates the calling of <code class="literal">read-date</code> on the appropriate fields in <code class="literal">UfoSighting</code>, as shown in the following code:</p><div><pre class="programlisting">(def date-formatter (tf/formatter "yyyyMMdd"))
(defn read-date [date-str]
  (try
    (tf/parse date-formatter date-str)
    (catch Exception ex
      nil)))
(defn coerce-fields [ufo]
  (assoc ufo
         :sighted-at (read-date (:sighted-at ufo))
         :reported-at (read-date (:reported-at ufo))))</pre></div><p>Now we can use these functions to read and parse each line of the input data file. As shown in the following code, each line is a separate JSON object:</p><div><pre class="programlisting">(defn read-data
  ([] (read-data *data-file*))
  ([filename]
   (with-open [f (io/reader filename)]
     (-&gt;&gt; f
       line-seq
       vec
       (r/map #(json/read-str % :key-fn keyword))
       (r/map map-&gt;UfoSighting)
       (r/map coerce-fields)
       (into [])))))</pre></div><p>Now we can use these on the REPL to load the data file. As shown in the following code, in this session, <code class="literal">model</code> is bound to <code class="literal">ufo-data.model</code>:</p><div><pre class="programlisting">
<strong>user=&gt; (def data (model/read-data))</strong>
<strong>user=&gt; (count data)</strong>
<strong>61067</strong>
<strong>user=&gt; (first data)</strong>
<strong>{:sighted-at nil,</strong>
<strong> :reported-at nil,</strong>
<strong> :location " Iowa City, IA",</strong>
<strong> :shape "",</strong>
<strong> :duration "",</strong>
<strong> :description</strong>
<strong> "Man repts. witnessing &amp;quot;flash, followed by a classic UFO, w/ a tailfin at back.&amp;quot; Red color on top half of tailfin. Became triangular.",</strong>
<strong> :year nil,</strong>
<strong> :month nil,</strong>
<strong> :season nil,</strong>
<strong> :reported_at "19951009",</strong>
<strong> :sighted_at "19951009"}</strong>
</pre></div><p>Looks good. We're<a id="id319" class="indexterm"/> ready to start processing the descriptions further.</p></div><div><div><div><div><h3 class="title"><a id="ch04lvl3sec08"/>Splitting the NUFORC comments</h3></div></div></div><p>Many of the <a id="id320" class="indexterm"/>descriptions contain comments by <a id="id321" class="indexterm"/>NUFORC (<a class="ulink" href="http://www.nuforc.org/">http://www.nuforc.org/</a>). These contain editorial remarks – some of them <a id="id322" class="indexterm"/>about the authenticity of the report. The following is a sample description with NUFORC commentary:</p><div><blockquote class="blockquote"><p><em>Telephoned Report:Husband and wife were awakened by a very bright light outside their house in Rio Vista area of McCall.  It was so bright, it was &amp;quot;like being inside a football stadium.&amp;quot;  No sound.  Ground was covered with snow at the time.  It lasted for 10 seconds.((NUFORC Note:  We spoke with the husband and wife, and found them to be quite credible and convincing in their description of what they allegedly had seen.   Both have responsible jobs.  PD))</em></p></blockquote></div><p>This is a standard format for these comments: They're enclosed in double parentheses and begin with "NUFORC." We can leverage this information, and a regular expression, to pull all the notes out of the document.</p><p>To do this, we'll go a <a id="id323" class="indexterm"/>little deeper into the Java regular expression API than <a id="id324" class="indexterm"/>Clojure has utility functions defined to do. Let's see what we need to do, and then we can take it apart after the following code listing:</p><div><pre class="programlisting">(defn split-nuforc [text]
  (let [m (.matcher #"\(\(.*?\)\)" text), sb (StringBuffer.)]
    (loop [accum []]
      (if (.find m)
        (let [nuforc (.substring text (.start m) (.end m))]
          (.appendReplacement m sb "")
          (recur (conj accum nuforc)))
        (do
          (.appendTail m sb)
          [(str sb) (str/join " " accum)])))))</pre></div><p>So first we create a regular expression that picks out text enclosed in double parentheses. We also create <code class="literal">java.lang.StringBuffer</code>. We'll use this to accumulate the description of the UFO sighting, with the NUFORC comments stripped out.</p><p>The body of the function is a loop that has a single parameter, a vector named <code class="literal">accum</code>. This will accumulate the NUFORC comments.</p><p>Inside the loop, every time the regular expression finds a match, we extract the NUFORC comment out of the original string and replace the match with an empty string in <code class="literal">StringBuffer</code>. Finally, when there are no more matches on the regular expression, we append the rest of the string onto <code class="literal">StringBuffer</code>, and we can retrieve its contents and the comments, joined together.</p><p>Let's see what happens when we strip the NUFORC comments from the description quoted earlier:</p><div><pre class="programlisting">
<strong>user=&gt; (def split-descr (model/split-nuforc description))</strong>
<strong>user=&gt; (first split-descr)</strong>
<strong>"Telephoned Report:Husband and wife were awakened by a very bright light outside their house in Rio Vista area of McCall.  It was so bright, it was &amp;quot;like being inside a football stadium.&amp;quot;  No sound.  Ground was covered with snow at the time.  It lasted for 10 seconds."</strong>
<strong>user=&gt; (second split-descr)</strong>
<strong>"((NUFORC Note:  We spoke with the husband and wife, and found them to be quite credible and convincing in their description of what they allegedly had seen.   Both have responsible jobs.  PD))"</strong>
</pre></div><p>So we can see that<a id="id325" class="indexterm"/> the first item in the pair returned by <code class="literal">split-nuforc</code> contains the <a id="id326" class="indexterm"/>description by itself, and the second item is the comments.</p><p>Now we can use the comments to categorize the descriptions in the first part. And we'll use that to figure out where to save the cleaned-up descriptions.</p></div><div><div><div><div><h3 class="title"><a id="ch04lvl3sec09"/>Categorizing the documents based on the comments</h3></div></div></div><p>Categorizing the documents <a id="id327" class="indexterm"/>is relatively easy. We'll use a <code class="literal">tokenize</code> function, which can be found in the code download for this chapter, in the namespace <code class="literal">ufo-data.text</code> (which is aliased to <code class="literal">t</code> in the code). We can convert the words in the comment to a set of tokens and then look for the word <code class="literal">"</code><em>hoax</em><code class="literal">"</code>. If found, we'll categorize it as follows:</p><div><pre class="programlisting">(defn get-category [tokens]
  (if (contains? (set tokens) "hoax")
    :hoax
    :non-hoax))</pre></div><p>When called with the tokens of a comment, it returns the category of the description as follows:</p><div><pre class="programlisting">
<strong>user=&gt; (model/get-category</strong>
<strong>         (map t/normalize (t/tokenize (second split-descr))))</strong>
<strong>:non-hoax</strong>
</pre></div><p>Of course, this is very rough, but it should be all right for this experiment.</p></div><div><div><div><div><h3 class="title"><a id="ch04lvl3sec10"/>Partitioning the documents into directories based on the categories</h3></div></div></div><p>Now that they're in categories, we<a id="id328" class="indexterm"/> can use those categories to save the descriptions into files. Each description will be in its own file.</p><p>Initially, we'll put all of the files into one pair of directories. In the next step, we'll divide them further into test and training sets.</p><p>The first function for this section will take a base directory, a number, and the document pair, as returned by <code class="literal">ufo-data.model/split-nuforc</code>. From there, it will save the text to a file and return the file's category and filename, as shown in the following code:</p><div><pre class="programlisting">(defn save-document [basedir n doc]
  (let [[text category] doc
        filename (str basedir \/ (name category) \/ n ".txt")]
    (spit filename text)
    {:category category, :filename filename}))</pre></div><p>The next function, <code class="literal">make-dirtree-sighting</code>, will do a lot of the work. It will take an instance of <code class="literal">UfoSighting</code> and will split out the NUFORC commentary, tokenize both parts, get the category, and use it to save the filename, as shown in the following code:</p><div><pre class="programlisting">(defn make-dirtree-sighting
  ([basedir]
   (fn [sighting n]
     (make-dirtree-sighting basedir sighting n)))
  ([basedir sighting n]
   (-&gt;&gt; sighting
     :description
     split-nuforc
     (on-both #(map t/normalize (t/tokenize %)))
     (on-second get-category)
     (on-first #(str/join " " %))
     (save-document basedir n))))</pre></div><p>This will handle <a id="id329" class="indexterm"/>saving each file individually into one pair of directories: one for hoaxes and one for non-hoaxes. We'll want to process all of the UFO sightings, however, and we'll want to divide the two sets of documents into a test set and a training set. We'll do all of this in the next section.</p></div><div><div><div><div><h3 class="title"><a id="ch04lvl3sec11"/>Dividing them into training and test sets</h3></div></div></div><p>Now, we can divide the <a id="id330" class="indexterm"/>data that we have into a <a id="id331" class="indexterm"/>training set and a test set. We'll need the following two utility functions to do this:</p><div><ol class="orderedlist arabic"><li class="listitem">We'll need to create subdirectories for the categories several times. Let's put that into the following function:<div><pre class="programlisting">(defn mk-cat-dirs [base]
  (doseq [cat ["hoax" "non-hoax"]]
    (fs/mkdirs (fs/file base cat))))</pre></div></li><li class="listitem">We'll also need to divide a collection into two groups by ratio, as shown in the following code. That is, one subgroup will be 80 percent of the original and the other subgroup will be 20 percent of the original.<div><pre class="programlisting">(defn into-sets [ratio coll]
  (split-at (int (* (count coll) ratio)) coll))</pre></div></li></ol></div><p>Now, the function to move a collection of files into a stage's subdirectory (testing or training) will be <code class="literal">mv-stage</code>. The collection of files is generated by <code class="literal">save-document</code>, so it's a collection of maps, each containing the category and filename of the file, as shown in the following code:</p><div><pre class="programlisting">(defn mv-stage [basedir stage coll]
  (let [stage-dir (fs/file basedir stage)]
    (doseq [{:keys [category filename]} coll]
      (fs/copy filename
               (fs/file stage-dir (name category)
                        (fs/base-name filename))))))</pre></div><p>To control this <a id="id332" class="indexterm"/>whole process, we'll<a id="id333" class="indexterm"/> use <code class="literal">make-dirtree</code>. This will take a collection of instances of <code class="literal">UfoSighting</code> and process them into separate text files. All of the files will be in the <code class="literal">basedir</code> directory, and then they'll be divided into a training set and a test set. These will be put into sibling directories under <code class="literal">basedir</code> as shown in the following code:</p><div><pre class="programlisting">(defn make-dirtree [basedir training-ratio sightings]
  (doseq [dir [basedir (fs/file basedir "train")
               (fs/file basedir "test")]]
    (mk-cat-dirs dir))
  (let [outputs (map (make-dirtree-sighting basedir)
                     sightings (range))
        {:keys [hoax non-hoax]} (group-by :category
                                          (shuffle outputs))
        [hoax-train hoax-test] (into-sets training-ratio hoax)
        [nhoax-train nhoax-test] (into-sets
                                   training-ratio non-hoax)]
    (mv-stage basedir "train" (concat hoax-train nhoax-train))
    (mv-stage basedir "test" (concat hoax-test nhoax-test))))</pre></div><p>Now, let's use this to divide out sightings data into groups and save them into the <code class="literal">bayes-data</code> directory as follows:</p><div><pre class="programlisting">
<strong>user=&gt; (model/make-dirtree "bayes-data" 0.8 data)</strong>
</pre></div><p>We have the data now, and it's in a shape that MALLET can use. Let's look at how we're going to leverage that library for Naïve Bayesian classification.</p></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec30"/>Classifying the data</h2></div></div></div><p>Bayesian inference can <a id="id334" class="indexterm"/>seem off-putting at first, but at its most basic level, it's how<a id="id335" class="indexterm"/> we tend to deal with the world. We start out with an idea of how likely something is, and then we update that expectation as we receive more information. In this case, depending on our background, training, history, and tendencies, we may think that all UFO reports are hoaxes or that most of them are. We may think that few UFO reports are hoaxes, or we may be completely undecided and assume that about half of them are hoaxes and half are true. But as we hear reports that we know the truth of, we change our opinions and expectations of the other reports. We may notice patterns, too. Hoaxes may talk about green men, while true reports may talk about grays. So you may also further refine your intuition based on that. Now, <a id="id336" class="indexterm"/>when you see a report that talks about little green men, you're more <a id="id337" class="indexterm"/>likely to think it's a hoax than when you see a report that talks about little gray men.</p><p>You may also notice that triangular UFOs are considered hoaxes, while circular UFOs are not. Now, when you read another document, this observation then further influences your beliefs about whether that document is a hoax or not.</p><p>In Bayesian terms, our original expectation that a document is a hoax or not is called the <strong>prior or assumed probability</strong><a id="id338" class="indexterm"/>, and its notation is <em>P(H)</em>, where <em>H</em> is the probability that the document is considered a hoax. The updated expectation after seeing the color of the aliens in the description, <em>C</em>, is called the <a id="id339" class="indexterm"/>
<strong>conditional probability</strong>, and its notation is <em>P(C|H)</em>, which is read as <em>the probability of C given H</em>. In this case, it's the probability distribution over the alien's color, given that the document is a hoax.</p><p>Bayes' theorem is a way of swapping the conditions for a set of conditional probabilities. That is, we can now find <em>P(H|C)</em>, or the probability distribution over the document's being a hoax, given that the alien is green or gray.</p><p>The formula to do this is pretty simple. To compute the probability that the document is a hoax, given the aliens' color, consider the following conditions:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The probability of the aliens' color, given that the document is a hoax or not</li><li class="listitem" style="list-style-type: disc">The probability that the document is a hoax</li><li class="listitem" style="list-style-type: disc">The probability of the aliens' color.</li></ul></div><p>For Naïve Bayesian classification, we make an important assumption: we assume that the features in a document are independent. This means that the probability that whether aliens are green or gray in a document is independent of whether the UFO is a disk or a triangle.</p><p>In spite of this assumption, Naïve Bayesian classifiers often work well in the real world. We can train them easily and quickly, and they classify new data quickly and often perform well enough to be useful.</p><p>So with that understanding, let's look at how MALLET handles Naïve Bayesian classification.</p><div><div><div><div><h3 class="title"><a id="ch04lvl3sec12"/>Coding the classifier interface</h3></div></div></div><p>Before we <a id="id340" class="indexterm"/>begin the next<a id="id341" class="indexterm"/> part of this chapter, it's probably a good time to start a new namespace for the following code to live in. Let's put it into the <code class="literal">src/ufo_data/bayes.clj</code> file. The <code class="literal">ns</code> declaration is as follows:</p><div><pre class="programlisting">(ns ufo-data.bayes
  (:require [clojure.java.io :as io])
  (:import [cc.mallet.util.*]
           [cc.mallet.types InstanceList]
           [cc.mallet.pipe Input2CharSequence
            TokenSequenceLowercase
            TokenSequenceRemoveStoplist
            CharSequence2TokenSequence SerialPipes
            SaveDataInSource Target2Label
            TokenSequence2FeatureSequence
            FeatureSequence2AugmentableFeatureVector]
           [cc.mallet.pipe.iterator FileIterator]
           [cc.mallet.classify NaiveBayesTrainer]
           [java.io ObjectInputStream ObjectOutputStream]))</pre></div><p>With the preceding code in place, let's see what we need to do.</p><div><div><div><div><h4 class="title"><a id="ch04lvl4sec01"/>Setting up the Pipe and InstanceList</h4></div></div></div><p>MALLET processes all input through <code class="literal">Pipe</code>. Pipes represent a series of transformations over the text. When you're working with a classifier, the data that's used for training, testing, and later for classifying new documents, all need to be put through the same pipe of processes. Also, all of them must use the same set of features and labels. MALLET calls these <em>alphabets</em>.</p><p>Each data document, at whatever stage of processing, is stored in an <code class="literal">Instance</code> object, and corpora of these are kept in <code class="literal">InstanceList</code>. <code class="literal">Pipe</code> objects are associated with <code class="literal">InstanceList</code> objects. This makes sure that all <code class="literal">Instance</code> objects in a collection are processed consistently.</p><p>In order to keep things straight, we'll define <code class="literal">make-pipe-list</code>. This will create the <code class="literal">Pipe</code> object as shown in the following code:</p><div><pre class="programlisting">(defn make-pipe-list []
  (SerialPipes.
    [(Target2Label.)
     (SaveDataInSource.)
     (Input2CharSequence. "UTF-8")
     (CharSequence2TokenSequence. #"\p{L}[\p{L}\p{P}]+\p{L}")
     (TokenSequenceLowercase.)
     (TokenSequenceRemoveStoplist.)
     (TokenSequence2FeatureSequence.)
     (FeatureSequence2AugmentableFeatureVector. false)]))</pre></div><p>This processing pipeline performs the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem"><code class="literal">Target2Label</code> takes the category from the directory path and assigns it to the <code class="literal">Instance</code> object's label. Labels are the categories or classes used for classification.</li><li class="listitem"><code class="literal">SaveDataInSource</code> takes the path name, which is currently in the data property, and puts it into the <code class="literal">Instance</code> object's source property.</li><li class="listitem"><code class="literal">Input2CharSequence</code> reads the data from the filename and replaces it with the file's contents.</li><li class="listitem"><code class="literal">CharSequence2TokenSequence</code> tokenizes the file's contents.</li><li class="listitem"><code class="literal">TokenSequenceLowercase</code> converts all uppercase characters in the tokens to lowercase.</li><li class="listitem"><code class="literal">TokenSequenceRemoveStoplist</code> removes common English words so that the classifier can focus on content words.</li><li class="listitem"><code class="literal">TokenSequence2FeatureSequence</code> categorizes the tokens as sequences. Each unique word is assigned a unique integer identifier.</li><li class="listitem"><code class="literal">FeatureSequence2AugmentableFeatureVector</code> converts the sequence of tokens into a vector. The token's feature identifier is that token's index in the feature vector.</li></ol></div><p>MALLET's classifier expects feature vectors as input, so this is the appropriate pipeline to use.</p><p>Now we need to take an input directory, generate <code class="literal">Instance</code> objects from it, and associate their processing with a pipeline. In the following code, we'll use the <code class="literal">add-input-directory</code> function to do all of that:</p><div><pre class="programlisting">(defn add-input-directory [dir-name pipe]
  (doto (InstanceList. pipe)
    (.addThruPipe
      (FileIterator. (io/file dir-name)
                     #".*/([^/]*?)/\d+.txt$"))))</pre></div><p>The regular expression in the last line takes the name of the file's directory and uses that as the <code class="literal">Instance</code> object's classification. We can use these two functions to handle the loading and processing of the inputs.</p></div><div><div><div><div><h4 class="title"><a id="ch04lvl4sec02"/>Training</h4></div></div></div><p>Training is<a id="id342" class="indexterm"/> pretty simple. We create an instance of <code class="literal">NaiveBayesTrainer</code>. Its <code class="literal">train</code> method returns an instance of <code class="literal">NaiveBayes</code>, which is the classifier. We'll wrap this in the following function to make it slightly easier to use:</p><div><pre class="programlisting">(defn train [instance-list]
  (.train (NaiveBayesTrainer.) instance-list))</pre></div><p>Wrapping it in this way provides a Clojure-native way of dealing with this library. It also keeps users of our module from needing to import <code class="literal">NaiveBayesTrainer</code> and the other classes from MALLET directly.</p></div><div><div><div><div><h4 class="title"><a id="ch04lvl4sec03"/>Classifying</h4></div></div></div><p>Just like training, classifying<a id="id343" class="indexterm"/> is also easy. The classifier returned by the <code class="literal">train</code> function just defers to the <code class="literal">classify</code> method as follows:</p><div><pre class="programlisting">(defn classify [bayes instance-list]
  (.classify bayes instance-list))</pre></div><p>The preceding code will return an instance of type <code class="literal">cc.mallet.classify.Classification</code>. This returns not only the best label and the probabilities associated with it, but also the probabilities of the other labels and the classifier and document instance involved.</p></div><div><div><div><div><h4 class="title"><a id="ch04lvl4sec04"/>Validating</h4></div></div></div><p>We can now train a classifier and<a id="id344" class="indexterm"/> run it on new documents. We'd like to be able to test it as well, by comparing our expectations from preclassified documents with how the classifier actually performs.</p><p>At the lowest level, we'll want to compare the expected classification with the actual classification and keep a count of each pairing of these values. We can do that with <code class="literal">validate1</code>. This gets the expected and actual labels, and it creates a vector pair of them. The <code class="literal">confusion-matrix</code> function then gets the frequency of those pairs as follows:</p><div><pre class="programlisting">(defn validate1 [bayes instance]
  (let [c (.classify bayes instance)
        expected (.. c getInstance getTarget toString)
        actual (.. c getLabeling getBestLabel toString)]
    [expected actual]))
(defn confusion-matrix [classifier instances labels]
  (frequencies (map #(validate1 classifier %) instances)))</pre></div><p>A confusion matrix is a table with the counts of the correctly classified instances (expected and actual match), the false positives (expected is to not classify, but the actual is to classify it), and the false negatives (expected is to classify the instance, but the actual is to not classify it). This provides an easy-to-comprehend overview of the performance of a classifier.</p></div><div><div><div><div><h4 class="title"><a id="ch04lvl4sec05"/>Tying it all together</h4></div></div></div><p>In the following code, we'll create a <code class="literal">bayes</code> function that creates, trains, and tests a classifier on a directory of data. It will take the hash map of information returned by <code class="literal">validate</code> and add the classifier and the <code class="literal">Pipe</code> object to it. Having the pipe object available later will be necessary to run the classifier on more data in the future.</p><div><pre class="programlisting">(defn bayes [training-dir testing-dir]
  (let [pipe (make-pipe-list)
        training (add-input-directory training-dir pipe)
        testing (add-input-directory testing-dir pipe)
        classifier (train training)
        labels (iterator-seq
                 (.iterator (.getLabelAlphabet classifier)))
        c-matrix (confusion-matrix classifier testing labels)]
    {:bayes classifier
     :pipe pipe
     :confusion c-matrix}))</pre></div><p>Now that we have all the pieces in place, let's see how to run the classifier.</p></div></div><div><div><div><div><h3 class="title"><a id="ch04lvl3sec13"/>Running the classifier and examining the results</h3></div></div></div><p>For this section, I've loaded <a id="id345" class="indexterm"/>the <code class="literal">ufo-data.bayes</code> namespace into <a id="id346" class="indexterm"/>the REPL and aliased it with the name <code class="literal">bayes</code>.</p><p>We can pass to the <code class="literal">bayes</code> function the test and training directories that we created from the sightings as shown in the following code:</p><div><pre class="programlisting">
<strong>user=&gt; (def bayes-out</strong>
<strong>         (bayes/bayes "bayes-data/train" "bayes-data/test"))</strong>
<strong>user=&gt; (:confusion bayes-out)</strong>
<strong>{["hoax" "non-hoax"] 83, ["non-hoax" "non-hoax"] 12102,</strong>
<strong>["non-hoax" "hoax"] 29}</strong>
</pre></div><p>Let's put this into a more traditional form for this information. The expected values have their labels across the top of the table. The actual values have theirs down the side. Look at the following table:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom">
<p>Hoax</p>
</th><th style="text-align: left" valign="bottom">
<p>Non-hoax</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Hoax</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>31</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Non-hoax</p>
</td><td style="text-align: left" valign="top">
<p>83</p>
</td><td style="text-align: left" valign="top">
<p>12100</p>
</td></tr></tbody></table></div><p>Well, that seems pretty useless. Evidently, my previous skepticism was warranted. The classifier managed to identify no hoaxes correctly, and it incorrectly identified 31 non-hoaxes as hoaxes (false positives).</p><p>But that's not all that we can learn about this. Instances of <code class="literal">NaiveBayes</code> also include a way to print out the top-weighted words for each category. Let's see what the top 10 words for each classification are:</p><div><pre class="programlisting">
<strong>user=&gt; (.printWords (:bayes bayes-out) 10)</strong>

<strong>Feature probabilities hoax</strong>
<strong>apos 0.002311333180377461</strong>
<strong>lights 0.0022688454380911096</strong>
<strong>light 0.00217537240506114</strong>
<strong>object 0.0020988944689457082</strong>
<strong>sky 0.002081899372031169</strong>
<strong>quot 0.0015295587223086145</strong>
<strong>looked 0.0014360856892786434</strong>
<strong>craft 0.0011556665901887302</strong>
<strong>red 0.0011301739448169206</strong>
<strong>back 0.0010961837509878402</strong>

<strong>Feature probabilities non-hoax</strong>
<strong>object 0.016553223428401043</strong>
<strong>light 0.016198059821948316</strong>
<strong>apos 0.015460989114397925</strong>
<strong>lights 0.014296272431730976</strong>
<strong>sky 0.014028337606877127</strong>
<strong>quot 0.010350232305991571</strong>
<strong>bright 0.007963812802535785</strong>
<strong>time 0.007237239541481537</strong>
<strong>moving 0.007063281856688359</strong>
<strong>looked 0.007037538118852588</strong>
</pre></div><p>So the terms are in slightly different order, but the vocabulary describing hoaxes and non-hoaxes is almost identical. Both mention <em>object</em>, <em>light</em>, <em>lights</em>, <em>sky</em>, and <em>looked</em>. So, based on the features we've selected here (single-word tokens), it's not surprising that we didn't get good results.</p><p>However, the primary<a id="id347" class="indexterm"/> thing that we can learn is that hoaxes<a id="id348" class="indexterm"/> are considered to be extremely rare, and the decision that a sighting is a hoax or not is often based on external data. Consider the sighting quoted earlier. To support the judgment that the sighting is not a hoax, the commenter mentions that they have a stable job, even though that's not mentioned in the description itself.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec33"/>Summary</h1></div></div></div><p>This has been a wandering and hopefully fun trip through the UFO sightings dataset. We've learned something about the language used in describing close encounters, and we've learned about how to use visualizations, exploratory data analysis, and Naïve Bayesian classification to learn more about the data.</p><p>But the primary impression I have of this is the feedback analysis, visualization, and exploration. The visualization led us to topic modeling, and something we discovered there led us to Bayesian classification. This is typical of data analysis, where one thing we learn informs and motivates the next stage in the analysis. Each answer can raise further questions and drive us back into the data.</p></div></body></html>