<html><head></head><body>
		<div><h1 id="_idParaDest-106"><em class="italic"><a id="_idTextAnchor108"/>Chapter 10</em>: Deploying Data Pipelines</h1>
			<p>In software engineering, you will usually have <strong class="bold">development</strong>, <strong class="bold">testing</strong>, and <strong class="bold">production</strong> environments. The testing environment may be called <strong class="bold">quality control</strong> or <strong class="bold">staging</strong> or some other name, but the idea is the same. You develop in an environment, then push it to another environment that will be a clone of the production environment and if everything goes well, then it is pushed into the production environment. The same methodology is used in data engineering. So far, you have built data pipelines and run them on a single machine. In this chapter, you will learn methods for building data pipelines that can be deployed to a production environment.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Finalizing your data pipelines for production</li>
				<li>Using the NiFi variable registry</li>
				<li>Deploying your data pipelines</li>
			</ul>
			<h1 id="_idParaDest-107"><a id="_idTextAnchor109"/>Finalizing your data pipelines for production</h1>
			<p>In <a id="_idIndexMarker551"/>the last few chapters, you have learned <a id="_idIndexMarker552"/>about the features and methods for creating production data pipelines. There are still a few more features needed before you can deploy your data pipelines—backpressure, processor groups with input and output ports, and funnels. This section will walk you through each one of these features.</p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor110"/>Backpressure</h2>
			<p>In your data pipelines, each <a id="_idIndexMarker553"/>processor or task will take different amounts of time to finish. For example, a database query may return hundreds of thousands of results that are split into single flowfiles in a few seconds, but the processor that evaluates and modifies the attributes within the flowfiles may take much longer. It doesn't make sense to dump all of the data into the queue faster than the downstream processor can actually process it. Apache NiFi allows you to control the number of flowfiles or the size of the data that is sent to the queue. This is called <strong class="bold">backpressure</strong>.</p>
			<p>To understand how <a id="_idIndexMarker554"/>backpressure works, let's make a data pipeline that generates data and writes it to a file. The data pipeline is shown in the following screenshot:</p>
			<div><div><img src="img/Figure_10.1_B15739.jpg" alt="Figure 10.1 – A data pipeline to generate data and write the flowfiles to a file"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – A data pipeline to generate data and write the flowfiles to a file</p>
			<p>The preceding <a id="_idIndexMarker555"/>data pipeline a creates connection between the <code>GenerateFlowFile</code> processor and the <code>PutFile</code> processor for the success relationship. I have configured the <code>PutFile</code> processor to write files to <code>/home/paulcrickard/output</code>. The <code>GenerateFlowFile</code> processor is using the default configuration.</p>
			<p>If you run the data pipeline by starting the <code>GenerateFlowFile</code> processor only, you will see that the queue has 10,000 flowfiles and is red, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_10.2_B15739.jpg" alt="Figure 10.2 – A full queue with 10,000 flowfiles&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – A full queue with 10,000 flowfiles</p>
			<p>If you refresh NiFi, the <a id="_idIndexMarker556"/>number of flowfiles in the queue will not increase. It has 10,000 flowfiles and cannot hold anymore. But is 10,000 the maximum number?</p>
			<p>Queues can be configured just like the processors that feed them. Right-click on the queue and select <strong class="bold">Configure</strong>. Select the <strong class="bold">SETTINGS</strong> tab, and you will see the following options:</p>
			<div><div><img src="img/Figure_10.3_B15739.jpg" alt="Figure 10.3 – Queue configuration settings&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.3 – Queue configuration settings</p>
			<p>You will notice <a id="_idIndexMarker557"/>that <code>10000</code> flowfiles and that <code>1 GB</code>. The <code>GenerateFlowFile</code> processor set the size of each flowfile to 0 bytes, so the object threshold was hit before the size threshold. You can test hitting the size threshold by changing the <code>GenerateFlowFile</code> processor. I have changed it to 50 MB. When I start the processor, the queue now stops at 21 flowfiles because it has exceeded 1 GB of data. The following screenshot shows the full queue:</p>
			<div><div><img src="img/Figure_10.4_B15739.jpg" alt="Figure 10.4 – Queue that has the size threshold&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.4 – Queue that has the size threshold</p>
			<p>By adjusting <strong class="bold">Object Threshold</strong> or <strong class="bold">Size Threshold</strong>, you can control the amount of data that gets sent to a<a id="_idIndexMarker558"/> queue and create backpressure slowing down an upstream processor. While loading the queues does not break your data pipeline, it will run much more smoothly if the data flows in a more even manner.</p>
			<p>The next section will zoom out on your data pipelines and show other techniques for improving the use of processor groups.</p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor111"/>Improving processor groups</h2>
			<p>Up to this point, you have used<a id="_idIndexMarker559"/> processor groups to hold a single data pipeline. If you were to push all of these data pipelines to production, what you would soon realize is that you have a lot of processors in each processor group doing the same exact task. For example, you may have several processors that SplitJson used followed by an <code>EvaluateJsonPath</code> processor that extracts the ID from a flowfile. Or, you might have several processors that insert flowfiles in to Elasticsearch.</p>
			<p>You would not have several functions in code that do the exact same thing on different variables; you would have one that accepted parameters. The same holds true for data pipelines, and you accomplish this<a id="_idIndexMarker560"/> using processor groups with the input and output ports.</p>
			<p>To illustrate how to break data pipelines into logical pieces, let's walk through an example:</p>
			<ol>
				<li>In NiFi, create a processor group and name it <code>Generate Data</code>.</li>
				<li>Inside the processor group, drag the <code>GenerateFlowFile</code> processor to the canvas. I have set the <code>{"ID":123}</code>.</li>
				<li>Next, drag an output port to the canvas. You will be prompted for <code>FromGeneratedData</code> and <strong class="bold">Send To</strong> is set to <strong class="bold">Local connections</strong>. </li>
				<li>Lastly, connect the <code>GenerateFlowfile</code> processor to <strong class="bold">Output Port</strong>. You will have a warning on the output port that it is invalid because it has no outgoing connections. We will fix that in the next steps.</li>
				<li>Exit the processor group.</li>
				<li>Create a new processor group and name it <code>Write Data</code>.</li>
				<li>Enter the processor group and drag the <code>EvaluateJsonPath</code> processor to the canvas. Configure it by creating a property ID with the value of <code>$.{ID}</code>, and set the <strong class="bold">Destination</strong> property to <strong class="bold">flowfile-attribute</strong>.</li>
				<li>Next, drag the <code>UpdateAttribute</code> processor to the canvas and create a new property filename and set the value to <code>${ID}</code>.</li>
				<li>Now, drag the <code>PutFile</code> processor to the canvas. Set the <code>/home/paulcrickard/output</code>.</li>
				<li>Lastly, drag an <strong class="bold">Input Port</strong> to the canvas and make it the first processor in the data pipeline. The completed pipeline should look like the following screenshot:<div><img src="img/Figure_10.5_B15739.jpg" alt="Figure 10.5 – A data pipeline that starts with an input port&#13;&#10;"/></div><p class="figure-caption">Figure 10.5 – A data pipeline that starts with an input port</p></li>
				<li>Exit the <a id="_idIndexMarker561"/>processor group. You should now have two processor groups on the canvas—<code>Generate Data</code> and <code>Write Data</code>. You can connect these processor groups just like you do with single processors. When you connect them by dragging the arrow from <code>Generate Data</code> to <code>Write Data</code>, you will be prompted to select which ports to connect, as shown in the following screenshot:<div><img src="img/Figure_10.6_B15739.jpg" alt="Figure 10.6 – Connecting two processor groups&#13;&#10;"/></div><p class="figure-caption">Figure 10.6 – Connecting two processor groups</p></li>
				<li>The default <a id="_idIndexMarker562"/>values will work because you only have one output port and one input port. If you had more, you could use the drop-down menus to select the proper ports. This is where naming them something besides input and output becomes important. Make the names descriptive.</li>
				<li>With the processor groups connected, start the <code>Generate Data</code> group only. You will see the queue fill up with flowfiles. To see how the ports work, enter the <code>Write Data</code> processor group.</li>
				<li>Start only the incoming data input port. Once it starts running, the downstream queue will fill with flowfiles.</li>
				<li>Right-click the queue and select <code>Generate Data</code> processor group. You can now start the rest of the processor.</li>
				<li>As the data pipeline runs, you will have a file, <code>123</code>, created in your output directory. </li>
			</ol>
			<p>You have successfully<a id="_idIndexMarker563"/> connected two processor groups using input and output ports. In production, you can now have a single process group to write data to a file and it can receive data from any processor group that needs to write data, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_10.7_B15739.jpg" alt="Figure 10.7 – Two processor groups utilizing the Write Data processor group&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.7 – Two processor groups utilizing the Write Data processor group</p>
			<p>In the preceding<a id="_idIndexMarker564"/> data pipeline, I made a copy of <code>Generate Data</code> and configured the <code>{"ID":456}</code> and set the run schedule to an hour so that I would only get one flowfile from each processor—<code>Generate Data</code> and <code>Generate Data2</code>. Running all of the processor groups, you list the queue and confirm that one flowfile comes from each processor group, and your output directory now has two files—<code>123</code> and <code>456</code>.</p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor112"/>Using the NiFi variable registry</h1>
			<p>When you are building your<a id="_idIndexMarker565"/> data pipelines, you are hardcoding variables—with the exception of some expression language where you extract data from the flowfile. When you move the data pipeline to production, you will need to change the variables in your data pipeline, and this can be time consuming and error prone. For example, you will have a different test database than production. When you deploy your data pipeline to production, you need to point to production and change the processor. Or you can use the variable registry.</p>
			<p>Using the <code>postgresToelasticsearch</code> processor group from <a href="B15739_04_ePub_AM.xhtml#_idTextAnchor049"><em class="italic">Chapter 4</em></a><em class="italic">, Working with Databases</em>, I will modify the data pipeline to use the NiFi variable registry. As a reminder, the data pipeline is shown in the following screenshot:</p>
			<div><div><img src="img/Figure_10.8_B15739.jpg" alt="Figure 10.8 – A data pipeline to query PostgreSQL and save the results to Elasticsearch&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.8 – A data pipeline to query PostgreSQL and save the results to Elasticsearch</p>
			<p>From outside the <a id="_idIndexMarker566"/>processor group, right-click on it and select <strong class="bold">Variables</strong>. To add a new variable, you can click the plus sign and provide a name and a value. These variables are now associated with the processor group.</p>
			<p>Just like functions in programming, variables have a scope. Variables in a processor group are local variables. You can right-click on the NiFi canvas and create a variable, which you can consider global in scope. I have created two local variables, <code>elastic</code> and <code>index</code>, and one global, <code>elastic</code>. When I open the variables in the group, it looks like the following screenshot:</p>
			<div><div><img src="img/Figure_10.9_B15739.jpg" alt="Figure 10.9 – NiFi variable registry&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.9 – NiFi variable registry</p>
			<p>In the preceding <a id="_idIndexMarker567"/>screenshot, you can see the scopes. The scope of <code>elastic</code>, the local variable takes precedence. </p>
			<p>You can now reference these variables using the expression language. In the <code>PutElasticsearchHttp</code> process, I have set the <code>${elastic}</code> and the <code>${index}</code>. These will populate with the local variables—<code>http://localhost:9200</code> and <code>nifivariable</code>.</p>
			<p>Running the data pipeline, you can see the results in Elasticsearch. There is now a new index with the name <code>nifivariable</code> and 1,001 records. The following screenshot shows the result:</p>
			<div><div><img src="img/Figure_10.10_B15739.jpg" alt="Figure 10.10 – The new index, nifivariable, is the second row&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.10 – The new index, nifivariable, is the second row</p>
			<p>You have now put the <a id="_idIndexMarker568"/>finishing touches on production pipelines and have completed all the steps needed to deploy them. The next section will teach you different ways to deploy your data pipelines.</p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor113"/>Deploying your data pipelines</h1>
			<p>There are many ways to <a id="_idIndexMarker569"/>handle the different environments—<strong class="bold">development</strong>, <strong class="bold">testing</strong>, <strong class="bold">production</strong>—and how you choose to do that is up to what works best with your business practices. Having said that, any strategy you take should involve using the NiFi registry.</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor114"/>Using the simplest strategy</h2>
			<p>The simplest <a id="_idIndexMarker570"/>strategy would be to run NiFi over the network and split the canvas into multiple environments. When you have promoted a process group, you would move it in to the next environment. When you needed to rebuild a data pipeline, you would add it back to development and modify it, then update the production data pipeline to the newest version. Your NiFi instance would look like the following screenshot:</p>
			<div><div><img src="img/Figure_10.11_B15739.jpg" alt="Figure 10.11 – A single NiFi instance working as DEV, TEST, and PROD&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.11 – A single NiFi instance working as DEV, TEST, and PROD</p>
			<p>Notice in the preceding<a id="_idIndexMarker571"/> screenshot that only <code>PROD</code> has a green checkmark. The <code>DEV</code> environment created the processor group, then changes were committed, and they were brought into <code>TEST</code>. If any changes were made, they were committed, and the newest version was brought in to <code>PROD</code>. To improve the data pipeline later, you would bring the newest version into <code>DEV</code> and start the process over until <code>PROD</code> has the newest version as well.</p>
			<p>While this will work, if you have the resources to build out a separate NiFi instance, you should.</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor115"/>Using the middle strategy</h2>
			<p>The middle strategy <a id="_idIndexMarker572"/>utilizes the NiFi registry but also adds a production NiFi instance. I have installed NiFi on another machine, separate from the one I have used through this book, that is also running the NiFi registry—this could also live on a separate machine.</p>
			<p>After launching my new NiFi instance, I added the NiFi registry as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_10.12_B15739.jpg" alt="Figure 10.12 – Adding the NiFi registry to another NiFi instance&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.12 – Adding the NiFi registry to another NiFi instance</p>
			<p>On the development machine, the registry was created using localhost. However, other machines can connect by specifying the IP address of the host machine. After reading it, the NiFi instance has access to all the versioned data pipelines.</p>
			<p>Drag a processor group to the canvas and select <strong class="bold">Import</strong>. You can now select the processor group that has been<a id="_idIndexMarker573"/> promoted to production, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_10.13_B15739.jpg" alt="Figure 10.13 – Importing the processor group&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.13 – Importing the processor group</p>
			<p>Once you import the processor, it will come over with the variables that were defined in the development environment. You can overwrite the values of the variables. Once you change the variables, you will not need to do it again. You can make the changes in the development environment and update the production environment and the new variables will stay. The updated variables are shown in the following screenshot:</p>
			<div><div><img src="img/Figure_10.14_B15739.jpg" alt="Figure 10.14 – Updating local variables for production&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.14 – Updating local variables for production</p>
			<p>In the development environment, you <a id="_idIndexMarker574"/>can change the processor and commit the local changes. The production environment will now show that there is a new version available, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_10.15_B15739.jpg" alt="Figure 10.15 – Production is now no longer using the current version&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.15 – Production is now no longer using the current version</p>
			<p>You can right-click the processor<a id="_idIndexMarker575"/> group and select the new version. The following screenshot shows version 2:</p>
			<div><div><img src="img/Figure_10.16_B15739.jpg" alt="Figure 10.16 – A new version&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.16 – A new version</p>
			<p>After selecting the new version, the<a id="_idIndexMarker576"/> production environment is now up to date. The following screenshot shows the production environment. You can right-click on the processor group to see that the variable still points to the production values:</p>
			<div><div><img src="img/Figure_10.17_B15739.jpg" alt="Figure 10.17 – Production is up to date&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.17 – Production is up to date</p>
			<p>This strategy should<a id="_idIndexMarker577"/> work for most users' needs. In this example, I used development and production environments, but you can add <code>TEST</code> and use the same strategy here, just change the local variables to point to your test databases. </p>
			<p>The preceding strategies used a single NiFi registry, but you can use a registry per environment.</p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor116"/>Using multiple registries</h2>
			<p>A more advanced<a id="_idIndexMarker578"/> strategy for managing development, test, and production would be to use multiple NiFi registries. In this strategy, you would set up two NiFi registries—one for development and one for test and production. You would connect the development environment to the development registry and the test and production environments to the second registry.</p>
			<p>When you have promoted a data pipeline to test, an administrator would use the NiFi CLI tools to export the data pipeline and import it in to the second NiFi registry. From there, you could test and promote it to development. You would import the version from the second registry to the production environment, just like you did in the middle strategy. This strategy makes mistakes much more difficult to handle as you cannot commit data pipelines to<a id="_idIndexMarker579"/> test and production without manually doing so. This is an excellent strategy but requires many more resources.</p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor117"/>Summary</h1>
			<p>In this chapter, you learned how to finalize your data pipelines for deployment into production. By using processor groups for specific tasks, much like functions in code, you could reduce the duplication of processors. Using input and output ports, you connected multiple processor groups together. To deploy data pipelines, you learned how NiFi variables could be used to declare global and locally scoped variables.</p>
			<p>In the next chapter, you will use all the skills you have learned in this section to create and deploy a production data pipeline.</p>
		</div>
	</body></html>