["```py\n$ docker ps\n```", "```py\n- ./config/airflow.cfg:/usr/local/airflow/airflow.cfg\n- ./files_to_test:/opt/airflow/files_to_test\n```", "```py\n    from airflow import DAG\n    from airflow.settings import AIRFLOW_HOME\n    from airflow.operators.bash import BashOperator\n    from airflow.operators.python_operator import PythonOperator\n    import json\n    from datetime import datetime, timedelta\n    ```", "```py\n    # Define default arguments\n    default_args = {\n        'owner': 'airflow',\n        'depends_on_past': False,\n        'start_date': datetime(2023, 3, 22),\n        'retries': 1,\n        'retry_delay': timedelta(minutes=5)\n    }\n    ```", "```py\n    def get_ids_from_json(filename_json):\n        with open (filename_json, 'r') as f:\n            git = json.loads(f.read())\n        print([item['id'] for item in git])\n    ```", "```py\n    # Instantiate a DAG object\n    with DAG(\n        dag_id='simple_ids_ingest',\n        default_args=default_args,\n        schedule_interval=timedelta(days=1),\n    ) as dag:\n        first_task = BashOperator(\n                task_id=\"first_task\",\n                bash_command=\"echo $AIRFLOW_HOME\",\n            )\n        filename_json = f\"{AIRFLOW_HOME}/files_to_test/github_events.json\"\n        get_id_from_json = PythonOperator(\n            task_id=\"get_id_from_json\",\n            python_callable=get_ids_from_json,\n            op_args=[filename_json]\n        )\n    ```", "```py\nwith DAG(\n    dag_id='simple_ids_ingest',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n) as dag:\n```", "```py\nfirst_task >> get_id_from_json\n```", "```py\n    from airflow.settings import AIRFLOW_HOME\n    from airflow.models.baseoperator import BaseOperator\n    import requests\n    import json\n    file_output_path = f\"{AIRFLOW_HOME}/files_to_test/output_files/\"\n    ```", "```py\n    class HolidayAPIIngestOperator(BaseOperator):\n        def __init__(self, filename, secret_key, country, year, **kwargs):\n            super().__init__(**kwargs)\n            self.filename = filename\n            self.secret_key = secret_key\n            self.country = country\n            self.year = year\n        def execute(self, context):\n            params = { 'key': self.secret_key,\n                    'country': self.country,\n                    'year': self.year\n            }\n            url = \"https://holidayapi.com/v1/holidays?\"\n            output_file = file_output_path + self.filename\n            try:\n                req = requests.get(url, params=params)\n                print(req.json())\n                with open(output_file, \"w\") as f:\n                    json.dump(req.json(), f)\n                return \"Holidays downloaded successfully\"\n            except Exception as e:\n                raise e\n    ```", "```py\n    from airflow import DAG\n    # Other imports\n    from operators.holiday_api_plugin import HolidayAPIIngestOperator\n    # Define default arguments\n    # Instantiate a DAG object\n    with DAG(\n        dag_id='holiday_ingest',\n        default_args=default_args,\n        schedule_interval=timedelta(days=1),\n    ) as dag:\n        filename_json = f\"holiday_brazil.json\"\n        task = HolidayAPIIngestOperator(\n            task_id=\"holiday_api_ingestion\",\n            filename=filename_json,\n            secret_key=Variable.get(\"SECRET_HOLIDAY_API\"),\n            country=\"BR\",\n            year=2022\n        )\n    task\n    ```", "```py\nclass HolidayAPIIngestOperator(BaseOperator):\n    def __init__(self, filename, secret_key, country, year, **kwargs):\n        super().__init__(**kwargs)\n        self.filename = filename\n        self.secret_key = secret_key\n        self.country = country\n        self.year = year\n```", "```py\ndef execute(self, context):\n```", "```py\nfrom airflow.models import Variable\nfrom operators.holiday_api_plugin import HolidayAPIIngestOperator\n```", "```py\ntask = HolidayAPIIngestOperator(\n        task_id=\"holiday_api_ingestion\",\n        filename=filename_json,\n        secret_key=Variable.get(\"SECRET_HOLIDAY_API\"),\n        country=\"BR\",\n        year=2022)\n```", "```py\n    from airflow import DAG\n    from airflow.settings import AIRFLOW_HOME\n    from airflow.operators.bash import BashOperator\n    from airflow.sensors.weekday import DayOfWeekSensor\n    from airflow.utils.weekday import WeekDay\n    from datetime import datetime, timedelta\n    default_args = {\n        'owner': 'airflow',\n        'start_date': datetime(2023, 3, 22),\n        'retry_delay': timedelta(minutes=5)\n    }\n    # Instantiate a DAG object\n    with DAG(\n        dag_id='sensors_move_file',\n        default_args=default_args,\n        schedule_interval=\"@once\",\n    ) as dag:\n    ```", "```py\n        move_file_on_saturdays = DayOfWeekSensor(\n            task_id=\"move_file_on_saturdays\",\n            timeout=120,\n            soft_fail=True,\n            week_day=WeekDay.SATURDAY\n        )\n    ```", "```py\n        move_file_task = BashOperator(\n                task_id=\"move_file_task\",\n                bash_command=\"mv $AIRFLOW_HOME/files_to_test/sensors_files/*.json $AIRFLOW_HOME/files_to_test/output_files/\",\n            )\n    ```", "```py\n    move_file_on_saturdays.set_downstream(move_file_task)\n    ```", "```py\n[2023-03-25, 00:19:03 UTC] {weekday.py:83} INFO - Poking until weekday is in WeekDay.SATURDAY, Today is SATURDAY\n[2023-03-25, 00:19:03 UTC] {base.py:301} INFO - Success criteria met. Exiting.\n[2023-03-25, 00:19:03 UTC] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=sensors_move_file, task_id=move_file_on_saturdays, execution_date=20230324T234623, start_date=20230325T001903, end_date=20230325T001903\n[2023-03-25, 00:19:03 UTC] {local_task_job.py:156} INFO - Task exited with return code 0\n```", "```py\n    move_file_on_saturdays = DayOfWeekSensor(\n        task_id=\"move_file_on_saturdays\",\n        timeout=120,\n        soft_fail=True,\n        week_day=WeekDay.SATURDAY\n    )\n```", "```py\n    from airflow import DAG\n    from airflow.settings import AIRFLOW_HOME\n    from airflow.providers.mongo.hooks.mongo import MongoHook\n    from airflow.operators.python import PythonOperator\n    import os\n    import json\n    from datetime import datetime, timedelta\n    default_args = {\n        'owner': 'airflow',\n        'depends_on_past': False,\n        'start_date': datetime(2023, 3, 22),\n        'retries': 1,\n        'retry_delay': timedelta(minutes=5)\n    }\n    ```", "```py\n    def get_mongo_collection():\n        hook = MongoHook(conn_id ='mongodb')\n        client = hook.get_conn()\n        print(client)\n        print( hook.get_collection(mongo_collection=\"reviews\", mongo_db=\"db_airbnb\"))\n    ```", "```py\n    # Instantiate a DAG object\n    with DAG(\n        dag_id='mongodb_check_conn',\n        default_args=default_args,\n        schedule_interval=timedelta(days=1),\n    ) as dag:\n    ```", "```py\n        mongo_task = PythonOperator(\n            task_id='mongo_task',\n            python_callable=get_mongo_collection\n        )\n    ```", "```py\nmongo_task\n```", "```py\nfrom airflow.providers.mongo.hooks.mongo import MongoHook\n```", "```py\nhook = MongoHook(conn_id ='mongodb')\n```", "```py\n    # (DAG configuration above)\n        t_0 = BashOperator(\n                task_id=\"t_0\",\n                bash_command=\"echo 'This tasks will be executed first'\",\n            )\n        t_1 = BashOperator(\n                task_id=\"t_1\",\n                bash_command=\"echo 'This tasks no1 will be executed in parallel with t_2 and t_3'\",\n            )\n        t_2 = BashOperator(\n                task_id=\"t_2\",\n                bash_command=\"echo 'This tasks no2 will be executed in parallel with t_1 and t_3'\",\n            )\n        t_3 = BashOperator(\n                task_id=\"t_3\",\n                bash_command=\"echo 'This tasks no3 will be executed in parallel with t_1 and t_2'\",\n            )\n        t_final = BashOperator(\n            task_id=\"t_final\",\n            bash_command=\"echo 'Finished all tasks in parallel'\",\n        )\n    ```", "```py\n    t_0 >> [t_1, t_2, t_3] >> t_final\n    ```", "```py\n    from airflow.sensors.external_task import ExternalTaskSensor\n    ```", "```py\n    def get_holiday_dates(filename_json):\n        with open (filename_json, 'r') as f:\n            json_hol = json.load(f)\n            holidays = json_hol[\"holidays\"]\n        print([item['date'] for item in holidays])\n    ```", "```py\n        wait_holiday_api_ingest = ExternalTaskSensor(\n            task_id='wait_holiday_api_ingest',\n            external_dag_id='holiday_ingest',\n            external_task_id='holiday_api_ingestion',\n            allowed_states=[\"success\"],\n            execution_delta = timedelta(minutes=1),\n            timeout=600,\n        )\n        filename_json = f\"{AIRFLOW_HOME}/files_to_test/output_files/holiday_brazil.json\"\n        date_tasks = PythonOperator(\n            task_id='date_tasks',\n            python_callable=get_holiday_dates,\n            op_args=[filename_json]\n        )\n    wait_holiday_api_ingest >> date_tasks\n    ```", "```py\n[2023-03-26, 20:50:23 UTC] {external_task.py:166} INFO - Poking for tasks ['holiday_api_ingestion'] in dag holiday_ingest on 2023-03-24T23:50:00+00:00 ...\n[2023-03-26, 20:51:23 UTC] {external_task.py:166} INFO - Poking for tasks ['holiday_api_ingestion'] in dag holiday_ingest on 2023-03-24T23:50:00+00:00 ...\n```", "```py\n[2023-03-25, 16:39:31 UTC] {logging_mixin.py:115} INFO - ['2022-01-01', '2022-02-28', '2022-03-01', '2022-03-02', '2022-03-20', '2022-04-15', '2022-04-17', '2022-04-21', '2022-05-01', '2022-05-08', '2022-06-16', '2022-06-21', '2022-08-14', '2022-09-07', '2022-09-23', '2022-10-12', '2022-11-02', '2022-11-15', '2022-12-21', '2022-12-24', '2022-12-25', '2022-12-31']\n```", "```py\nwait_holiday_api_ingest = ExternalTaskSensor(\n        task_id='wait_holiday_api_ingest',\n        external_dag_id='holiday_ingest',\n        external_task_id='holiday_api_ingestion',\n        allowed_states=[\"success\"],\n        execution_delta = timedelta(minutes=1),\n        timeout=300,\n    )\n```"]