<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Offloading Data Processing to Database Systems</h1></div></div></div><p>We have learned many different ways to optimize the performance of an R code for speed and memory efficiency. But sometimes R alone is not enough. Perhaps, a very large dataset is stored in a data warehouse. It would be infeasible to extract all the data into R for processing. We might even wish to tap into the power of specially-designed analytical databases that can perform computations on data much more efficiently than R can. In this chapter, we will learn how to tap into the power of external database systems from within R and combine that power with the flexibility and ease of use of the R language.</p><p>This chapter covers the following:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Extracting data into R versus processing data in a database</li><li class="listitem" style="list-style-type: disc">Preprocessing data in a relational database using SQL</li><li class="listitem" style="list-style-type: disc">Converting R expressions into SQL</li><li class="listitem" style="list-style-type: disc">Running statistical and machine learning algorithms in a database</li><li class="listitem" style="list-style-type: disc">Using columnar databases for improved performance</li><li class="listitem" style="list-style-type: disc">Using array databases for maximum scientific computing performance</li></ul></div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec49"/>Extracting data into R versus processing data in a database</h1></div></div></div><p>Most R programmers are familiar with and very comfortable manipulating data in R using R data <a id="id306" class="indexterm"/>structures and packages. This requires moving all the data into R whether in memory or on a disk, on a single computer or on a cluster. In some situations, this might not be efficient especially if the data constantly changes and needs to be updated often—extracting data out of a database or data warehouse every time it needs to be analyzed takes a lot of time and computational resources. In some cases, it might not be feasible at all to move terabytes or more of data from their sources into R.</p><p>Instead of moving the data into R, another approach is to move the computational tasks to the data. In other words, we can process the data in the database and retrieve only the results into R, which are usually much smaller than the raw data. This reduces the amount of network bandwidth required to transmit the data and the local storage and memory required to process the data in R. It also allows R programmers to tap into powerful databases that are purpose-built for analytical workloads on large datasets.</p><p>In order to perform in-database computations and analyses, a new set of tools is needed. At the foundation of all in-database tools is the SQL language, which most relational databases support. While this book is not about SQL, knowing how to run even simple SQL statements in a database can help speed up many tasks in R. Other tools such as <code class="literal">dplyr</code> build on SQL to provide easy and familiar interfaces such as data frame-like objects in order to manipulate the data in the database. Yet other tools like MonetDB.R and SciDB allow us to tap into databases that are designed for high-performance analytical workloads such as columnar and array databases. We shall look at these tools in the following sections.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec50"/>Preprocessing data in a relational database using SQL</h1></div></div></div><p>We will start by learning how to run SQL statements in the database from R. The first <a id="id307" class="indexterm"/>few examples <a id="id308" class="indexterm"/>show how processing data in a database instead of moving all the data into <a id="id309" class="indexterm"/>R can result in faster performance even for simple operations.</p><p>To run the examples in this chapter, you will need a database server supported by R. The CRAN package, <code class="literal">RJDBC</code> provides an interface to JDBC drivers that most databases come with. Alternatively, search on CRAN for packages such as <code class="literal">RPostgreSQL</code>, <code class="literal">RMySQL</code>, and <code class="literal">ROracle</code> that offer functionalities and optimizations specific to each database.</p><p>The following examples are based on a PostgreSQL database and the <code class="literal">RPostgreSQL</code> package as we will need them later in this chapter when we learn about the <code class="literal">PivotalR</code> package and MADlib software. Feel free, however, to adapt the code to the database that you use.</p><p>Configuring <a id="id310" class="indexterm"/>PostgreSQL to <a id="id311" class="indexterm"/>work with R involves setting up both the server and the client. First, we <a id="id312" class="indexterm"/>need to set <a id="id313" class="indexterm"/>up the PostgreSQL database server. This can be on a different computer than the one running R to simulate tapping into an existing database from R; or it can be on the same computer for simplicity. In our case, we will set up a Linux virtual machine to host the PostgreSQL database server and use Mac OS X as the client. Here are the steps to set up the database server:</p><div><ol class="orderedlist arabic"><li class="listitem">Download PostgreSQL from <a class="ulink" href="http://www.postgresql.org/download/">http://www.postgresql.org/download/</a> and follow <a id="id314" class="indexterm"/>the installation instructions for your operating system.</li><li class="listitem">Enable username/password authentication on the database server by adding the following command line to <code class="literal">pg_hba.conf</code> (in the PostgreSQL <code class="literal">data</code> folder):<div><pre class="programlisting">host    all    all    0.0.0.0/0    md5</pre></div></li><li class="listitem">Create a user account and password that can be used to connect to the database from R by running the following command line (you might need to be the <code class="literal">root</code> or the <code class="literal">postgres</code> user to run this):<div><pre class="programlisting">
<strong>$ createuser  --pwprompt ruser</strong>
</pre></div></li><li class="listitem">Create a database for the examples in this chapter by running the following command line (you might need to be the <code class="literal">root</code> or the <code class="literal">postgres</code> user to run this):<div><pre class="programlisting">
<strong>$ createdb --owner=ruser rdb</strong>
</pre></div></li><li class="listitem">Ensure that the database is accessible via a network connection from the computer that runs R by adding the following lines to <code class="literal">postgresql.conf</code> (in the PostgreSQL <code class="literal">data</code> folder):<div><pre class="programlisting">listen_address = '*'
port = 5432</pre></div></li><li class="listitem">Restart the PostgreSQL server for the changes to take effect (you might need to be the <code class="literal">root</code> user to do this).</li></ol></div><p>Next, we will set up the client by installing the <code class="literal">RPostgreSQL</code> package on the computer that <a id="id315" class="indexterm"/>runs R:</p><div><ol class="orderedlist arabic"><li class="listitem">Non-Windows <a id="id316" class="indexterm"/>only: install <code class="literal">libpq</code>, the PostgreSQL C libraries, that are needed to install <code class="literal">RPostgreSQL</code>. If you have installed the PostgreSQL server on the same <a id="id317" class="indexterm"/>computer as R, the libraries are already in the system, so you can skip this step. Otherwise, make sure that the version of the libraries matches the version of the PostgreSQL server:<div><pre class="programlisting">
<strong># On Mac OS X (using Homebrew)</strong>
<strong>$ brew install postgresql </strong>
<strong># On Debian / Ubuntu</strong>
<strong>$ sudo apt-get install libpq-dev</strong>
<strong># On Redhat / CentOS</strong>
<strong>$ sudo yum install postgresql-devel</strong>
<strong># On Windows: this step is not needed</strong>
</pre></div></li><li class="listitem">Run R and install the <code class="literal">RPostgreSQL</code> CRAN package from its source code:<div><pre class="programlisting"># On platforms other than Windows
install.packages("RPostgreSQL", type="source")
# On Windows
install.packages("RPostgreSQL")</pre></div></li><li class="listitem">Test the database connection from R by substituting the details with the correct information for your database:<div><pre class="programlisting">library(RPostgreSQL)
db.drv &lt;- PostgreSQL()
db.conn &lt;- dbConnect(db.drv, host = "hostname",
                     port = 5432, dbname = "rdb",
                     user = "ruser",
                     password = "rpassword")
dbListTables(db.conn)
## character(0)</pre></div></li></ol></div><p>Once the database is set up, we will generate some sales data for the examples to follow. The example database has two tables, <code class="literal">sales</code> and <code class="literal">trans_items</code>. The <code class="literal">sales</code> table contains information about sales transactions in a retail chain, including the transaction ID, customer ID, and store ID. The <code class="literal">trans_items</code> table records the individual items in each transaction and the total price for each item. Once the data is generated in R, we will use <code class="literal">dbWriteTable()</code> to write the data into new tables in the database, as follows:</p><div><pre class="programlisting">ntrans &lt;- 1e5
ncust &lt;- 1e4
nstore &lt;- 100
sales &lt;- data.frame(
    trans_id = seq_len(ntrans),
    cust_id = sample.int(ncust, ntrans, TRUE),
    store_id = sample.int(nstore, ntrans, TRUE))
<strong>dbWriteTable(db.conn, "sales", sales)</strong>
trans.lengths &lt;- rpois(ntrans, 3) + 1L
trans.items &lt;- data.frame(
    trans_id = rep.int(seq_len(ntrans), trans.lengths),
    item_id = unlist(lapply(trans.lengths, sample.int, n = 1000)),
    price = exp(rnorm(sum(trans.lengths))))
<strong>dbWriteTable(db.conn, "trans_items", trans.items)</strong>
</pre></div><p>The first task is<a id="id318" class="indexterm"/> to calculate the<a id="id319" class="indexterm"/> total sales for each store. Let's compare two different ways of doing this. The <a id="id320" class="indexterm"/>first way is to extract all the store IDs along with the prices of the items associated with each store by joining the <code class="literal">sales</code> and <code class="literal">trans_items</code> tables. Once this data is in R, the sales for each store is computed by summing the item prices for each store ID using <code class="literal">tapply()</code>. The second way to compute the same data is to perform the aggregation in the database using the SQL <code class="literal">GROUP BY</code> clause and <code class="literal">SUM()</code> function. We will use <code class="literal">microbenchmark()</code> to compare the execution times for both methods:</p><div><pre class="programlisting">library(microbenchmark)
microbenchmark({
    res &lt;- dbGetQuery(
        db.conn,
        'SELECT store_id, price
        FROM sales INNER JOIN trans_items USING (trans_id);')
    res &lt;- tapply(res$price, res$store_id, sum)
}, times = 10)
## Unit: milliseconds
##       min       lq   median       uq      max neval
##  740.7533 745.2563 <strong>771.3706</strong> 775.3665 780.3819    10
microbenchmark({
    res &lt;- dbGetQuery(
        db.conn,
        'SELECT store_id, SUM(price) as total_sales
        FROM sales INNER JOIN trans_items USING (trans_id)
        GROUP BY store_id;')
}, times = 10)
## Unit: milliseconds
##      min       lq   median       uq      max neval
##  244.779 248.6401 <strong>251.1465</strong> 255.3652 279.6666    10</pre></div><p>In this simple test, performing the computations in the database takes only 33 percent of the<a id="id321" class="indexterm"/> time to do the same by extracting the data into R. Let's take a look at another example. The <a id="id322" class="indexterm"/>second task is to get a list of the top ten customers who have spent the most <a id="id323" class="indexterm"/>money, in decreasing order. Again, we will compare the speed of performing the computations in R versus in the database:</p><div><pre class="programlisting">microbenchmark({
    res &lt;- dbGetQuery(
        db.conn,
        'SELECT cust_id, price
        FROM sales INNER JOIN trans_items USING (trans_id);')
    res &lt;- tapply(res$price, res$cust_id, sum)
    res &lt;- sort(res, decreasing = TRUE)
    res &lt;- head(res, 10L)
}, times = 10)
## Unit: milliseconds
##       min       lq   median       uq      max neval
##  814.2492 828.7774 <strong>843.1869</strong> 846.4235 952.1318    10
microbenchmark({
    res &lt;- dbGetQuery(
        db.conn,
        'SELECT cust_id, SUM(price) as spending
        FROM sales INNER JOIN trans_items USING (trans_id)
        GROUP BY cust_id
        ORDER BY spending DESC
        LIMIT 10;')
}, times = 10)
## Unit: milliseconds
##       min       lq   median       uq      max neval
##  259.1621 260.5494 <strong>260.9566</strong> 265.1368 294.1732    10</pre></div><p>Again, running the computations in the database instead of in R has resulted in a 70 percent reduction in the execution time.</p><p>Once we are done, we need to disconnect from the database:</p><div><pre class="programlisting">dbDisconnect(db.conn)</pre></div><p>These tests were conducted on the same computer with the database server running in a virtual machine. Even on such a small dataset and over a very small network (the virtual network between the host computer and the virtual machine), the differences in the performance were dramatic. These tests clearly demonstrate that minimizing the amount of data being copied out of the database can provide a big performance boost. On larger datasets and powerful analytical databases, the performance difference can be even more pronounced.</p></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec51"/>Converting R expressions to SQL</h1></div></div></div><p>While SQL is <a id="id324" class="indexterm"/>a powerful <a id="id325" class="indexterm"/>and flexible language used to manipulate data in a database, not everyone is proficient in it. Fortunately, the R community has developed a few packages that translate familiar R syntax into SQL statements that are then executed on the database. We will look at two of them—<code class="literal">dplyr</code> and <code class="literal">PivotalR</code>.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec27"/>Using dplyr</h2></div></div></div><p>The <code class="literal">dplyr</code> package is a handy package designed to allow the manipulation of table-like <a id="id326" class="indexterm"/>data with a <a id="id327" class="indexterm"/>standard <a id="id328" class="indexterm"/>set of operations and transformations, no matter where the data is stored—in a data frame, data table, or database. It supports SQLite, PostgreSQL, MySQL, Amazon RedShift, Google BigQuery, and MonetDB databases.</p><p>The <code class="literal">dplyr</code> package provides a way to specify a set of operations to be performed on the data without actually performing the computations on the database server until we instruct R to do so, by calling the <code class="literal">collect()</code>function. By pooling a few operations together (as opposed to executing them one by one), the database server can optimize the execution. This in turn helps to minimize computational load of the server. Let's see how this works with an example.</p><p>First, we need to establish a connection with the database, as before. Here, we will use the <code class="literal">src_postgres()</code> function provided by <code class="literal">dplyr</code>. The syntax is slightly different from <code class="literal">dbConnect()</code> of <code class="literal">RPostgreSQL</code>, but the arguments are similar. After establishing the connection, we will create references to the <code class="literal">sales</code> and <code class="literal">trans_items</code> tables in the database using the <code class="literal">tbl()</code> function:</p><div><pre class="programlisting">library(dplyr)
db.conn &lt;- src_postgres(dbname = "rdb", host = "hostname",
                        port = 5432, user = "ruser",
                        password = "rpassword")
sales.tb &lt;- tbl(db.conn, "sales")
trans_items.tb &lt;- tbl(db.conn, "trans_items")</pre></div><p>Let's recreate the previous example using <code class="literal">dplyr</code>:</p><div><pre class="programlisting">joined.tb &lt;- inner_join(sales.tb, trans_items.tb, by = "trans_id")
cust.items &lt;- group_by(joined.tb, cust_id)
cust.spending &lt;- summarize(cust.items, spending = sum(price))
cust.spending &lt;- arrange(cust.spending, desc(spending))
cust.spending &lt;- select(cust.spending, cust_id, spending)</pre></div><p>The first step is to join the <code class="literal">sales</code> and <code class="literal">trans_items</code> tables using <code class="literal">inner_join()</code>. Then, <code class="literal">group_by()</code> groups the items according to customer ID, and <code class="literal">summarize()</code> sums the total spending for each customer. Finally, we will use <code class="literal">arrange()</code> to sort the customer in decreasing order of spending, and <code class="literal">select()</code> to select only the columns we want.</p><p>The output of each of these steps is a <code class="literal">tbl</code> object:</p><div><pre class="programlisting">class(cust.spending)
## [1] "tbl_postgres" "tbl_sql"      "tbl"</pre></div><p>These are <a id="id329" class="indexterm"/>virtual tables <a id="id330" class="indexterm"/>that are an accumulation of all the operations applied so far. Up to this point, no SQL has been sent to the database server and no computation has been performed on it. We can examine the SQL query that will be executed when the results are retrieved by retrieving the <code class="literal">query</code> member of the <code class="literal">tbl</code> object:</p><div><pre class="programlisting">cust.spending$query
## &lt;Query&gt; SELECT "cust_id" AS "cust_id", "spending" AS "spending"
## FROM (SELECT "cust_id", SUM("price") AS "spending"
## FROM (SELECT "row.names" AS "row.names.x", "trans_id" AS
## "trans_id", "cust_id" AS "cust_id", "store_id" AS "store_id"
## FROM "sales") AS "nsthygziij"
##
## INNER JOIN 
##
## (SELECT "row.names" AS "row.names.y", "trans_id" AS "trans_id",
## "item_id" AS "item_id", "price" AS "price"
## FROM "trans_items") AS "cuwpqadrgf"
##
## USING ("trans_id")
## GROUP BY "cust_id") AS "_W8"
## ORDER BY "spending" DESC
## &lt;PostgreSQLConnection:(11726,2)&gt; </pre></div><p>Normally, the <code class="literal">collect()</code> function is used to run the SQL statement and retrieve the results:</p><div><pre class="programlisting">custs.by.spending &lt;- collect(cust.spending)</pre></div><p>Since we want only the top 10 customers and not all the customers, we can use <code class="literal">head()</code> to minimize the data being transferred from the database into R:</p><div><pre class="programlisting">top.custs &lt;- head(cust.spending, 10L)</pre></div><p>As more complex data manipulation operations are constructed in <code class="literal">dplyr</code>, the individual R statements and temporary variables created can get unwieldy. The <code class="literal">dplyr</code> package provides the <code class="literal">%&gt;%</code> operator to chain operations together. The preceding construct can be rewritten more succinctly as:</p><div><pre class="programlisting">top.custs &lt;-
    sales.tb %&gt;% inner_join(trans_items.tb, by = "trans_id") %&gt;%
    group_by(cust_id) %&gt;%
    summarize(spending = sum(price)) %&gt;%
    arrange(desc(spending)) %&gt;%
    select(cust_id, spending) %&gt;%
    head(10L)</pre></div><p>The <code class="literal">dplyr</code> package <a id="id331" class="indexterm"/>provides <a id="id332" class="indexterm"/>other useful operations like <code class="literal">filter()</code> for filtering rows, and <code class="literal">mutate()</code> for defining new columns as functions of the existing columns. These operations can be combined in many creative and useful ways to process data in a database before retrieving the results into R.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec28"/>Using PivotalR</h2></div></div></div><p>The <code class="literal">PivotalR</code> package provides<a id="id333" class="indexterm"/> similar capabilities as <code class="literal">dplyr</code>, but with a different <a id="id334" class="indexterm"/>syntax. Because it <a id="id335" class="indexterm"/>was developed by Pivotal Software Inc., it supports only PostgreSQL or Pivotal (Greenplum) databases.</p><p>As usual, the first step in using the package is to establish a connection to the database:</p><div><pre class="programlisting">library(PivotalR)
db.conn &lt;- db.connect(host = "hostname", port = 5432,
                      dbname = "rdb", user = "ruser",
                      password = "rpassword")</pre></div><div><div><h3 class="title"><a id="note16"/>Note</h3><p>If you have not installed MADlib on the PostgreSQL database (see the next section of this chapter), you might get a warning that says "MADlib does not exist in database." This is not a problem for the examples in this section as they do not cover the MADlib functions.</p></div></div><p>The next step is to create references to the database tables using <code class="literal">db.data.frame()</code>:</p><div><pre class="programlisting">sales.tb &lt;- db.data.frame("sales", db.conn)
trans_items.tb &lt;- db.data.frame("trans_items", db.conn)</pre></div><p>The <code class="literal">db.data.frame</code> objects behave similar to standard R data frames in many ways, except that they are wrappers for SQL queries that need to be executed on the database. Many <a id="id336" class="indexterm"/>of the standard R information and statistical functions are supported. In <a id="id337" class="indexterm"/>order to execute the SQL and retrieve the results, use the <code class="literal">lookat()</code> function (or the shorthand <code class="literal">lk()</code>). For example:</p><div><pre class="programlisting">dim(sales.tb)
## [1] 1e+05 4e+00
names(sales.tb)
## [1] "row.names" "trans_id"  "cust_id"   "store_id" 
lookat(count(sales.tb$cust_id))
## [1] 1e+05
lookat(min(trans_items.tb$price))
## [1] 0.009554177
lookat(max(trans_items.tb$price))
## [1] 121.3909</pre></div><p>To see the SQL query that will be executed on the database server, use the <code class="literal">content()</code> method:</p><div><pre class="programlisting">content(max(trans_items.tb$price))
## [1] "select max(\"price\") as \"price_max\"
## from \"trans_items\""</pre></div><div><div><h3 class="title"><a id="note17"/>Note</h3><p>If you get the error message "Invalid SciDB object", it could mean that some of the <code class="literal">PivotalR</code> functions are being masked by functions of the same name in the <code class="literal">SciDB</code> package, which we will cover later in this chapter. In particular, both packages provide the <code class="literal">count()</code> function. To run the examples in this section successfully, unload the <code class="literal">scidb</code> package with <code class="literal">detach("package:scidb", unload=TRUE)</code>.</p></div></div><p>New columns can be computed from existing columns by using the familiar R syntax without affecting the data on the database; instead, the transformations are translated into SQL functions that compute the new columns on the fly. In the following example, we will compute a new column <code class="literal">foreign_price</code> that is returned to R in memory and not stored in the database:</p><div><pre class="programlisting">trans_items.tb$foreign_price &lt;- trans_items.tb$price * 1.25
content(trans_items.tb)
## [1] "select \"row.names\" as \"row.names\", \"trans_id\" as
## \"trans_id\", \"item_id\" as \"item_id\", \"price\" as
## \"price\", (\"price\") * (1.25) as \"foreign_price\" from 
## \"trans_items\""</pre></div><p>Let's take a <a id="id338" class="indexterm"/>look at a full example of how to construct a query in <code class="literal">PivotalR</code>. Say we want to compute some statistics to understand the purchasing patterns of consumers at the transaction level. We have to group the data by transactions and then group it again by customers to <a id="id339" class="indexterm"/>compute the statistics for each customer:</p><div><pre class="programlisting">trans &lt;- by(trans_items.tb["price"], trans_items.tb$trans_id, sum)
sales.value &lt;- merge(sales.tb[c("trans_id", "cust_id",
                                "store_id")],
                     trans, by = "trans_id")
cust.sales &lt;- by(sales.value, sales.value$cust_id,
                 function(x) {
                     trans_count &lt;- count(x$trans_id)
                     total_spend &lt;- sum(x$price_sum)
                     stores_visited &lt;- count(x$store_id)
                     cbind(trans_count, total_spend,
                           stores_visited)
                 })
names(cust.sales) &lt;- c("cust_id", "trans_count", "total_spend",
                       "stores_visited")
lookat(cust.sales, 5)
##   cust_id trans_count total_spend stores_visited
## 1       1           9    44.73121              9
## 2       2           7    41.90196              7
## 3       3          13    87.37564             13
## 4       4          11    58.34653             11
## 5       5          15    95.09015             15</pre></div><p>The first call to <code class="literal">by()</code> aggregates the item-level sales data into transactions; summing up the total value of each transaction. Next, <code class="literal">merge()</code> joins the <code class="literal">sales</code> table with the aggregated transaction data to match the customers with how much they have spent. Then, we will use <code class="literal">by()</code> again to aggregate all the transactions by customer. For each customer, we will calculate the number of transactions they made, the total value of those transactions, and the number of stores they visited.</p><p>Instead of returning the results, they can also be stored into a new database table by using <code class="literal">as.db.data.frame()</code>. This is useful for lengthy computations with many intermediate steps. Storing intermediate results in the database helps to reduce the amount of data being transferred between R and the database.</p><div><pre class="programlisting">cust_sales.tb &lt;- as.db.data.frame(cust.sales, "cust_sales")</pre></div><p>Further <a id="id340" class="indexterm"/>statistics can be<a id="id341" class="indexterm"/> computed from the intermediate data, such as the minimum, maximum, mean and standard deviation of customer spending:</p><div><pre class="programlisting">lookat(min(cust_sales.tb$total_spend))
## [1] 0.4961619
lookat(max(cust_sales.tb$total_spend))
## [1] 227.8077
lookat(mean(cust_sales.tb$total_spend))
## [1] 66.16597
lookat(sd(cust_sales.tb$total_spend))
## [1] 26.71887</pre></div><p>When the intermediate data is no longer required, it can be deleted from the database:</p><div><pre class="programlisting">delete(cust_sales.tb)</pre></div><p>Both <code class="literal">dplyr</code> and <code class="literal">PivotalR</code> provide flexible easy ways to manipulate data in a database using R functions and syntax. They allow us to tap into the processing power and speed of high-performance databases to query large datasets and integrate the results of the queries into other analyses in R. Because they are quite similar in capabilities, choosing between the two is largely a matter of compatibility with existing database systems and personal preference for one syntax over the other.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec52"/>Running statistical and machine learning algorithms in a database</h1></div></div></div><p>So far, the examples in this chapter have performed simple computations on data in a database. Sometimes we need to perform more complex computations than that. Several database <a id="id342" class="indexterm"/>vendors <a id="id343" class="indexterm"/>have begun to build advanced statistics or even machine learning <a id="id344" class="indexterm"/>capabilities into their database products, allowing these advanced algorithms to run in the database <a id="id345" class="indexterm"/>using highly optimized code for maximum performance. In this chapter, we will look at one open <a id="id346" class="indexterm"/>source project, MADlib (<a class="ulink" href="http://madlib.net/">http://madlib.net/</a>), whose development is supported by Pivotal Inc., that brings advanced statistics and machine learning capabilities to PostgreSQL databases.</p><p>MADlib adds a host of statistical capabilities to PostgreSQL, including descriptive statistics, hypothesis tests, array arithmetic, probability functions, dimensionality reduction, linear models, clustering models, association rules, and text analysis. New models and statistical methods are constantly being added to the library to expand its capabilities.</p><div><div><h3 class="title"><a id="note18"/>Note</h3><p>At the moment, MADlib binaries are only available for Mac OS X and Red Hat/CentOS Linux. For other operating systems, <a class="ulink" href="https://github.com/madlib/madlib/wiki/Building-MADlib-from-Source">https://github.com/madlib/madlib/wiki/Building-MADlib-from-Source</a> provides<a id="id347" class="indexterm"/> instructions to build MADlib from source. MADlib does not support Windows at the time of writing.</p></div></div><p>Before <a id="id348" class="indexterm"/>installing MADlib, ensure<a id="id349" class="indexterm"/> that the <code class="literal">plpython</code> module from PostgreSQL is installed. On Redhat/CentOS, run this command by substituting the package name with one <a id="id350" class="indexterm"/>that matches the version of PostgreSQL:</p><div><pre class="programlisting">$ yum install postgresql93-plpython</pre></div><p>On Mac OS X, check the documentation for your PostgreSQL installation method. For example, using Homebrew, the following command installs PostgreSQL with <code class="literal">plpython</code> support:</p><div><pre class="programlisting">$ brew install postgresql --with-python</pre></div><p>Once PostgreSQL has <a id="id351" class="indexterm"/>been set <a id="id352" class="indexterm"/>up with <code class="literal">plpython</code>, follow the instructions at <a class="ulink" href="https://github.com/madlib/madlib/wiki/Installation-Guide">https://github.com/madlib/madlib/wiki/Installation-Guide</a> to install MADlib. The <a id="id353" class="indexterm"/>user account<a id="id354" class="indexterm"/> being used to install MADlib needs superuser privileges, which can be granted by running <code class="literal">ALTER ROLE ruser WITH SUPERUSER;</code> in PostgreSQL.</p><p>Now, return to R and connect to PostgreSQL using the <code class="literal">RPostgreSQL</code> package:</p><div><pre class="programlisting">db.drv &lt;- PostgreSQL()
db.conn &lt;- dbConnect(db.drv, host = "hostname",
                     port = 5432, dbname = "rdb",
                     user = "ruser",
                     password = "rpassword")</pre></div><p>Say we want to mine our sales database for association rules. As you can remember from <a class="link" href="ch06.html" title="Chapter 6. Simple Tweaks to Use Less RAM">Chapter 6</a>, <em>Simple Tweaks to Use Less RAM</em>, the <code class="literal">arules</code> package provides functions to mine for frequent itemsets and association rules. In order to use the <code class="literal">arules</code> package, the entire <code class="literal">trans_items</code> table would need to be extracted into R and converted into a <code class="literal">transactions</code> object. If the dataset is large, this might take a long time, or might not be possible at all.</p><p>Alternatively, we can mine the association rules in the database using MADlib functions. The data does not need to be copied out of the database at all, and all the computations can take place in the database as long as the database server or cluster has sufficient capacity.</p><p>Running the <a id="id355" class="indexterm"/>association <a id="id356" class="indexterm"/>rules mining <a id="id357" class="indexterm"/>algorithm is as simple as calling the <code class="literal">madlib.assoc_rules()</code> function in <a id="id358" class="indexterm"/>an SQL <code class="literal">SELECT</code> statement:</p><div><pre class="programlisting">dbGetQuery(
    db.conn,
    "SELECT *
    FROM <strong>madlib.assoc_rules</strong>(
        0.001,         -- support
        0.01,          -- confidence
        'trans_id',    -- tid_col
        'item_id',     -- item_col
        'trans_items', -- input_table
        'public',      -- output_schema
        TRUE           -- verbose
    );")
## INFO:  finished checking parameters
## CONTEXT:  PL/Python function "assoc_rules"
## INFO:  finished removing duplicates
## CONTEXT:  PL/Python function "assoc_rules"
## # Output truncated
## INFO:  6 Total association rules found. Time: 0.00557494163513
## CONTEXT:  PL/Python function "assoc_rules"
##   output_schema output_table total_rules      total_time
## 1        public  assoc_rules           6 00:01:21.860964</pre></div><p>The preceding code includes comments that describe the arguments to <code class="literal">madlib.assoc_rules()</code>. Here, the algorithm is asked to search for association rules with a support of at least 0.001 and confidence of at least 0.01. The name of the input table and columns are specified, as well as the name of the schema in which you can store the results. In this case, the results will be stored in a table called <code class="literal">assoc_rules</code> in the <code class="literal">public</code> schema.</p><p>Every time the function is run, the <code class="literal">assoc_rules</code> table will be overwritten; so if you would like to keep a copy of the results, you will have to make a copy of the table.</p><p>Let's retrieve the results, that is, the association rules that meet the minimum support and confidence:</p><div><pre class="programlisting">dbGetQuery(
    db.conn,
    'SELECT * FROM assoc_rules;')
##   ruleid   pre  post support confidence     lift conviction
## 1      1 {353} {656}   1e-04 0.02272727 5.516328   1.019040
## 2      2 {656} {353}   1e-04 0.02427184 5.516328   1.020366
## 3      3 {770} {420}   1e-04 0.02444988 6.022137   1.020901
## 4      4 {420} {770}   1e-04 0.02463054 6.022137   1.021059
## 5      5 {755} {473}   1e-04 0.02469136 6.203859   1.021236
## 6      6 {473} {755}   1e-04 0.02512563 6.203859   1.021619</pre></div><p>The results indicate<a id="id359" class="indexterm"/> the items <a id="id360" class="indexterm"/>on the left- and right-hand sides of each association rule, along<a id="id361" class="indexterm"/> with the statistics for <a id="id362" class="indexterm"/>each rule such as the support, confidence, lift, and conviction.</p><p>Most of the other MADlib functions work in a similar way—data is supplied to a function in a database table, the function is called with the appropriate arguments, and the results are written to a new database table in the specified schema.</p><div><div><h3 class="title"><a id="note19"/>Note</h3><p>Because Pivotal, Inc. developed both the <code class="literal">PivotalR</code> package and MADlib, it is natural that <code class="literal">PivotalR</code> provides interfaces to some MADlib functions such as linear models, ARIMA time series models and decision trees. It also provides useful functions to extract information such as regression coefficients from the MADlib output. Unfortunately, <code class="literal">PivotalR</code> does not provide wrappers to all the MADlib functions such as the <code class="literal">madlib.assoc_rules()</code> function used in the preceding code. For maximum flexibility in using the MADlib library, use SQL statements to call the MADlib functions.</p></div></div><p>In-database analytics libraries such as MADlib allow us to harness the power of advanced analytics in large databases and bring the results of the algorithms into R for further analysis and processing.</p></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec53"/>Using columnar databases for improved performance</h1></div></div></div><p>Most relational databases use a row-based data storage architecture—the data is stored in the database <a id="id363" class="indexterm"/>row by row. Whenever the database performs a query, it retrieves the relevant rows for the query before processing the query. This architecture is well suited for business transactional uses, where complete records (that is, including all columns) are written, read, updated, or deleted, a few rows at a time. For most statistical or analytical use cases, however, many rows of data, often with only a few columns, need to be read. As a result, row-based databases are sometimes inefficient at analytical tasks because they read entire records at a time regardless of how many columns are actually needed for analysis. The following figure depicts how a row-based database might compute the sum of one column.</p><div><img src="img/9263OS_09_01.jpg" alt="Using columnar databases for improved performance"/><div><p>Computing the sum of one column in a row-based database</p></div></div><p>The increase in demand for data analysis platforms in recent years has led to the development of databases that use alternative storage architectures that are optimized for <a id="id364" class="indexterm"/>data analysis instead of business transactions. One such architecture is <strong>columnar storage</strong>. Columnar databases store data in columns instead of rows. This is very similar to R data frames where each column of a data frame is stored in a contiguous block of memory in the form of an R vector. When computing the sum of one column, a columnar database needs to read only one column of data, as shown in the following figure:</p><div><img src="img/9263OS_09_02.jpg" alt="Using columnar databases for improved performance"/><div><p>Computing the sum of one column in a columnar database</p></div></div><p>One example of a <a id="id365" class="indexterm"/>columnar database<a id="id366" class="indexterm"/> is MonetDB, which can be downloaded from <a class="ulink" href="https://www.monetdb.org/Downloads">https://www.monetdb.org/Downloads</a>. Follow<a id="id367" class="indexterm"/> the instructions there to install it. After installation, take the following steps to initialize and start the database.</p><p>On Linux or Mac OS X, run the following commands in a terminal window:</p><div><pre class="programlisting"># Create a new database farm
# (Replace the path with a location of your choice)
$ monetdbd create /path/to/mydbfarm
# Start the database server
$ monetdbd start /path/to/mydbfarm
# Create a new database within the farm
$ monetdb create rdb
# Release the new database from administration locks
$ monetdb release rdb</pre></div><p>On Windows, initialize and start the server by going to <strong>Start</strong> | <strong>Programs</strong> | <strong>MonetDB</strong> | <strong>Start server</strong>.</p><p>Because MonetDB is based on SQL, connecting to and working with MonetDB from R is similar to working with PostgreSQL. We can either execute SQL statements using the <code class="literal">MonetDB.R</code> CRAN package or use <code class="literal">dplyr</code>. For example, we can load the same sales and transaction data into MonetDB using <code class="literal">MonetDB.R</code>:</p><div><pre class="programlisting">library(MonetDB.R)
db.drv &lt;- MonetDB.R()
db.conn &lt;- dbConnect(db.drv, host = "hostname",
                     port = 50000, dbname = "rdb",
                     user = "monetdb",
                     password = "monetdb")
dbWriteTable(db.conn, "sales", sales)
dbWriteTable(db.conn, "trans_items", trans.items)</pre></div><p>Now, let's benchmark the <a id="id368" class="indexterm"/>query performance for the same SQL queries used in the <code class="literal">RPostgreSQL</code> examples:</p><div><pre class="programlisting">library(microbenchmark)
microbenchmark({
    res &lt;- dbGetQuery(
        db.conn,
        'SELECT store_id, SUM(price) as total_sales
        FROM sales INNER JOIN trans_items USING (trans_id)
        GROUP BY store_id;')
}, times = 10)
## Unit: milliseconds
##       min       lq   median       uq      max neval
##  112.1666 113.0484 <strong>113.9021</strong> 114.4349 114.7049    10
microbenchmark({
    res &lt;- dbGetQuery(
        db.conn,
        'SELECT cust_id, SUM(price) as spending
        FROM sales INNER JOIN trans_items USING (trans_id)
        GROUP BY cust_id
        ORDER BY spending DESC
        LIMIT 10;')
}, times = 10)
## Unit: milliseconds
##       min       lq  median       uq      max neval
##  114.2376 115.4617 <strong>116.515</strong> 117.1967 118.4736    10</pre></div><p>Compared to PostgreSQL, MonetDB took 55 percent less time for both queries (as you can remember from before that the median times needed by PostgreSQL for the first and second queries were 251.1 and 260.1 milliseconds, respectively). Of course, this is not a comprehensive or rigorous comparison between row-based and columnar databases, but it gives an indication of the performance gains that can be achieved by selecting the right database architecture for the task at hand.</p></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec54"/>Using array databases for maximum scientific-computing performance</h1></div></div></div><p>Columnar databases provide good query performance for datasets that resemble R data frames, for <a id="id369" class="indexterm"/>example, most data from business IT systems. These datasets are usually two dimensional and can contain heterogeneous data types. On the other hand, scientific data sometimes contain homogeneous data types but are multidimensional. An example of this is weather readings in different points in time and space. For such applications, a new type of database called the <strong>array database</strong> provides even better query and scientific computing performance. One example of this is SciDB, available<a id="id370" class="indexterm"/> for <a id="id371" class="indexterm"/>download at <a class="ulink" href="http://www.scidb.org/">http://www.scidb.org/</a>. <code class="literal">SciDB</code> provides a <strong>massively parallel processing</strong> (<strong>MPP</strong>) architecture that can perform queries in parallel on petabytes of <a id="id372" class="indexterm"/>array data. It supports in-database linear algebra, graph operations, linear models, correlations, and statistical tests. It also offers an R interface through the <code class="literal">SciDB</code> package that is available on CRAN.</p><p>To <a id="id373" class="indexterm"/>download <a id="id374" class="indexterm"/>and install SciDB, follow the instructions at <a class="ulink" href="https://github.com/Paradigm4/deployment">https://github.com/Paradigm4/deployment</a>. Then, install <a id="id375" class="indexterm"/>
<code class="literal">shim</code> (<a class="ulink" href="https://github.com/paradigm4/shim">https://github.com/paradigm4/shim</a>) on the SciDB server, which is needed for R in order to communicate with SciDB. Finally, install the <code class="literal">scidb</code> package from CRAN.</p><p>Connect to the SciDB database using the <code class="literal">scidbconnect()</code> function:</p><div><pre class="programlisting">library(scidb)
scidbconnect(host = "hostname", port = 8080)</pre></div><p>We can then load some data into the database using <code class="literal">as.scidb()</code>:</p><div><pre class="programlisting">A &lt;- as.scidb(matrix(rnorm(1200), 40, 30), name = "A")
B &lt;- as.scidb(matrix(rnorm(1200), 30, 40), name = "B")</pre></div><p><code class="literal">scidb</code> provides familiar R syntax to manipulate SciDB matrices and arrays:</p><div><pre class="programlisting"># Matrix multiplication
A %*% B
# Transpose and addition / subtraction
A + t(B)
# Scalar operations
A * 1.5</pre></div><p>We can even mix SciDB matrices/arrays with R matrices/arrays:</p><div><pre class="programlisting">C &lt;- matrix(rnorm(1200), 30, 40)
A %*% C</pre></div><p>As with the other <a id="id376" class="indexterm"/>database packages, operations are not actually performed until the results are retrieved. In the case of <code class="literal">SciDB</code>, the <code class="literal">[]</code> operator causes the database to perform the computations and return the results:</p><div><pre class="programlisting"># Filter only the positive elements of A, and materialize the
# results into R
(A &gt; 0)[]</pre></div><p><code class="literal">SciDB</code> supports many other common array/matrix operations such as subsetting, comparison, filtering, apply, joining, aggregation, and sorting. It is a powerful tool for working with large, multidimensional numerical data.</p></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec55"/>Summary</h1></div></div></div><p>In this chapter, we took a tour of various database systems and the R packages that allow us to interface with them, and saw how in-database querying and analysis can provide better performance than copying the data into R to do the same analysis. This is especially true for large datasets that cannot be easily processed in R; using a database that is tuned for querying and analysis can help to avoid performance issues in R. As technology improves, more and more advanced analysis and algorithms can be run in databases providing more options for R programmers who face the challenge of analyzing large datasets efficiently. These powerful data processing tools can complement R very nicely—they provide the computing muscle to analyze large datasets, while R provides easy interfaces for data manipulation and analysis. R can also help to bring together different threads of analyses, regardless of the tool used, to present a coherent and compelling picture of the data using tools such as data visualization.</p><p>In the next and final chapter, we will go to the frontiers of Big Data and take a look at how R can be used alongside Big Data tools to process extremely large datasets.</p></div></body></html>