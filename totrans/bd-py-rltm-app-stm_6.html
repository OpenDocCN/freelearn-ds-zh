<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 6. Petrel in Practice"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Petrel in Practice</h1></div></div></div><p>In previous chapters, we saw working examples of Storm topologies, both simple and complex. In doing so, however, we skipped some of the tools and techniques that you'll need while developing your own topologies:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Storm is a great environment for running your code, but deploying to Storm (even on your local machine) adds complexity and takes extra time. We'll see how to test your spouts and bolts outside of Storm.</li><li class="listitem" style="list-style-type: disc">When components run inside Storm, they can't read from the console, which prevents the use of pdb, the standard Python debugger. This chapter demonstrates Winpdb, an interactive debugging tool suitable for debugging components inside Storm.</li><li class="listitem" style="list-style-type: disc">Storm lets you easily harness the power of many servers, but performance of your code still matters. In this chapter, we'll see some ways of measuring the performance of our topology's components.</li></ul></div><div class="section" title="Testing a bolt"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec31"/>Testing a bolt</h1></div></div></div><p>Storm makes it easy to <a id="id192" class="indexterm"/>deploy and run Python topologies, but developing and testing them in Storm is challenging, whether running in standalone Storm or a full Storm deployment:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Storm launches programs on your behalf—not only your Python code but auxiliary Java processes as well</li><li class="listitem" style="list-style-type: disc">It controls the Python components' standard input and output channels</li><li class="listitem" style="list-style-type: disc">The Python programs must respond regularly to heartbeat messages or be shut down by Storm</li></ul></div><p>This makes it difficult to debug Storm topologies using the typical tools and techniques used for other pieces of Python code, such as the common technique of running from the command line and debugging with pdb.</p><p>Petrel's mock module helps us with this. It provides a simple, standalone Python container for testing simple topologies and verifying that the expected results are returned.</p><p>In Petrel terms, a <span class="strong"><strong>simple</strong></span> topology is one<a id="id193" class="indexterm"/> that only outputs to the default stream and has no branches or loops. The <code class="literal">run_simple_topology()</code> assumes that the first component in the list is a spout, passing the<a id="id194" class="indexterm"/> output of each component to the next component in the sequence.</p><div class="section" title="Example – testing SplitSentenceBolt"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec48"/>Example – testing SplitSentenceBolt</h2></div></div></div><p>Let's look at an example. Here is<a id="id195" class="indexterm"/> the <code class="literal">splitsentence.py</code> file from the <a id="id196" class="indexterm"/>first example in <a class="link" href="ch03.html" title="Chapter 3. Introducing Petrel">Chapter 3</a>, <span class="emphasis"><em>Introducing Petrel</em></span> with a unit test added:</p><div class="informalexample"><pre class="programlisting">from nose.tools import assert_equal

from petrel import mock, storm
from petrel.emitter import BasicBolt

from randomsentence import RandomSentenceSpout

class SplitSentenceBolt(BasicBolt):
    def __init__(self):
        super(SplitSentenceBolt, self).__init__(script=__file__)

    def declareOutputFields(self):
        return ['word']

    def process(self, tup):
        words = tup.values[0].split(" ")
        for word in words:
          storm.emit([word])


def test():
    bolt = SplitSentenceBolt()
    mock_spout = mock.MockSpout(
        RandomSentenceSpout.declareOutputFields(),
        [["Madam, I'm Adam."]])
    
    result = mock.run_simple_topology(
        None, [mock_spout, bolt], result_type=mock.LIST)
        assert_equal([['Madam,'], ["I'm"], ['Adam.']], result[bolt])


def run():
    SplitSentenceBolt().run()</pre></div><p>To run the test, enter the following commands:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>pip install nosetests</strong></span></pre></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, install the<a id="id197" class="indexterm"/> Python <code class="literal">nosetests</code> library by running<a id="id198" class="indexterm"/> the following:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>pip install nosetests</strong></span></pre></div></li><li class="listitem">Next, run this line:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>nosetests -v splitsentence.py</strong></span></pre></div></li></ol></div><p>If all goes well, you'll see the following output:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>splitsentence.test ... ok</strong></span>

<span class="strong"><strong>----------------------------------------------------------------------</strong></span>
<span class="strong"><strong>Ran 1 test in 0.001s</strong></span>

<span class="strong"><strong>OK</strong></span></pre></div><p>Nose is a very powerful tool<a id="id199" class="indexterm"/> with many features. We won't cover it in detail here, but you can find the documentation at <a class="ulink" href="https://nose.readthedocs.org/en/latest/">https://nose.readthedocs.org/en/latest/</a>.</p></div><div class="section" title="Example – testing SplitSentenceBolt with WordCountBolt"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec49"/>Example – testing SplitSentenceBolt with WordCountBolt</h2></div></div></div><p>The next example shows <a id="id200" class="indexterm"/>how to test a sequence of related components. In the following code, we see a new version of <code class="literal">wordcount.py</code> that tests<a id="id201" class="indexterm"/> the interaction between <code class="literal">SplitSentenceBolt</code> and <code class="literal">WordCountBolt</code>:</p><div class="informalexample"><pre class="programlisting">from collections import defaultdict

from nose.tools import assert_equal

from petrel import mock, storm
from petrel.emitter import BasicBolt

from randomsentence import RandomSentenceSpout
from splitsentence import SplitSentenceBolt

class WordCountBolt(BasicBolt):
    def __init__(self):
        super(WordCountBolt, self).__init__(script=__file__)
        self._count = defaultdict(int)

    @classmethod
    def declareOutputFields(cls):
        return ['word', 'count']

    def process(self, tup):
        word = tup.values[0]
        self._count[word] += 1
        storm.emit([word, self._count[word]])


def test():
    ss_bolt = SplitSentenceBolt()
    wc_bolt = WordCountBolt()

    mock_spout = mock.MockSpout(
        RandomSentenceSpout.declareOutputFields(),
        [["the bart the"]])

     result = mock.run_simple_topology(
       None,
       [mock_spout, ss_bolt, wc_bolt],
       result_type=mock.LIST)
       assert_equal([['the', 1], ['bart', 1], ['the', 2]], result[wc_bolt])


def run():
    WordCountBolt().run()</pre></div><p>The test is pretty straightforward; we <a id="id202" class="indexterm"/>simply instantiate both components and include them in the right sequence when calling <code class="literal">mock.run_simple_topology()</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note09"/>Note</h3><p>Both example tests<a id="id203" class="indexterm"/> specify <code class="literal">result_type=mock.LIST</code> while calling <code class="literal">run_simple_topology()</code>. This parameter option tells Petrel which format to use when returning output tuples. The options include:
</p><p><code class="literal">STORM_TUPLE</code></p><p><code class="literal">LIST</code></p><p><code class="literal">TUPLE</code></p><p><code class="literal">NAMEDTUPLE</code></p><p>Generally, <code class="literal">LIST</code> is a good choice for <a id="id204" class="indexterm"/>components with a small number of output fields, while <code class="literal">NAMEDTUPLE</code> is more readable for a larger number of fields (that is, by allowing the test to access result fields by field name rather than numeric indices). <code class="literal">STORM_TUPLE</code> is useful if the test needs to check other attributes of the result, for example, the <a id="id205" class="indexterm"/>lesser-used stream property.</p></div></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Debugging"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec32"/>Debugging</h1></div></div></div><p>Until now, we've debugged topologies using log messages and automated tests. These techniques are very powerful, but <a id="id206" class="indexterm"/>sometimes it may be necessary to debug directly inside the Storm environment. For example, the problem may:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Depend on running as a particular user</li><li class="listitem" style="list-style-type: disc">Occur only with real data</li><li class="listitem" style="list-style-type: disc">Occur only when there are many instances of the component running in parallel</li></ul></div><p>This section introduces a tool for debugging inside Storm.</p><p>Winpdb is a portable, GUI-based debugger for Python, with support for embedded debugging. If you're not familiar with the term "embedded debugging", note this: it simply means that Winpdb can attach to a program that was launched in some other way and not necessarily from WinDbg or your command shell. For this reason, it is a good fit for debugging Petrel components that run in Storm.</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="Installing Winpdb"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec33"/>Installing Winpdb</h1></div></div></div><p>Activate your Petrel <a id="id207" class="indexterm"/>virtual environment and then use <code class="literal">pip</code> to install it:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>source &lt;virtualenv directory&gt;/bin/activate</strong></span>
<span class="strong"><strong>pip install winpdb</strong></span></pre></div><div class="section" title="Add Winpdb breakpoint"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec50"/>Add Winpdb breakpoint</h2></div></div></div><p>In the <code class="literal">splitsentence.py</code> file, add the<a id="id208" class="indexterm"/> following at the beginning of the <code class="literal">run()</code> function:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>import rpdb2</strong></span>
<span class="strong"><strong>rpdb2.start_embedded_debugger('password')</strong></span></pre></div><p>The <code class="literal">'password'</code> value can be anything; this is simply the password that you will use in the next step to attach to <code class="literal">splitsentence.py</code>.</p><p>When this line of code executes, the script will freeze for a default period of 5 minutes, waiting for a debugger to attach.</p></div><div class="section" title="Launching and attaching the debugger"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec51"/>Launching and attaching the debugger</h2></div></div></div><p>Now run the topology:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>petrel submit --config topology.yaml</strong></span></pre></div><p>Once you see log <a id="id209" class="indexterm"/>messages from the spout, you will know that the topology is up <a id="id210" class="indexterm"/>and running, so you can connect with the debugger.</p><p>Launch <code class="literal">Winpdb</code> simply by running <code class="literal">winpdb</code>.</p><p>For more details on how to use Winpdb for embedded debugging, see the documentation at <a class="ulink" href="http://winpdb.org/docs/embedded-debugging/">http://winpdb.org/docs/embedded-debugging/</a>.</p><p>When the window appears, select <span class="strong"><strong>File</strong></span> | <span class="strong"><strong>Attach</strong></span> from the menu. A password dialog will appear. Here, enter the same password that you passed to <code class="literal">start_embedded_debugger()</code> and click on the <span class="strong"><strong>OK</strong></span> button, as shown in this screenshot:</p><div class="mediaobject"><img src="images/B03471_06_01.jpg" alt="Launching and attaching the debugger"/></div><p>Next, choose the process to attach to and click on <span class="strong"><strong>OK</strong></span>, as shown in the following screenshot:</p><div class="mediaobject"><img src="images/B03471_06_02.jpg" alt="Launching and attaching the debugger"/></div><p>Now you'll see<a id="id211" class="indexterm"/> the main Winpdb window, with the line below the <a id="id212" class="indexterm"/>breakpoint highlighted. If you've used other debuggers, Winpdb should be straightforward to use. If you need help using Winpdb, the following tutorial is very good for you:</p><p>
<a class="ulink" href="https://code.google.com/p/winpdb/wiki/DebuggingTutorial">https://code.google.com/p/winpdb/wiki/DebuggingTutorial</a>.</p><div class="mediaobject"><img src="images/B03471_06_03.jpg" alt="Launching and attaching the debugger"/></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Profiling your topology's performance"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec34"/>Profiling your topology's performance</h1></div></div></div><p>Performance<a id="id213" class="indexterm"/> can be a concern for any application. This is true for Storm topologies as well, perhaps more so.</p><p>When you're trying to push <a id="id214" class="indexterm"/>a lot of data through a topology, raw performance is certainly a concern—faster components means that more data can be processed. But it's also important to understand the tuple processing performance of individual components. This information can be used in two ways.</p><p>The first is knowing which components are slower, because this tells you where to focus your attention if you are trying to make the code faster. Once you know which component (or components) is slow, you can use tools such as the Python cProfile module (<a class="ulink" href="http://pymotw.com/2/profile/">http://pymotw.com/2/profile/</a>) and the line profiler (<a class="ulink" href="https://github.com/rkern/line_profiler">https://github.com/rkern/line_profiler</a>) to understand where the code is spending most of its time.</p><p>Even after profiling, some components will still be faster than others. In this case, understanding the relative performance between components can help you configure the topology for best performance.</p><p>This second point is somewhat subtle, so let's look at an example. In the following code, we see log excerpts for two <a id="id215" class="indexterm"/>Storm components from the word count topology. These log messages are generated automatically by Petrel. The first is the split sentence bolt, and the second is the word count bolt:</p><div class="informalexample"><pre class="programlisting">[2015-05-07 22:51:44,772][storm][DEBUG]BasicBolt profile: total_num_tuples=79, num_tuples=79, avg_read_time=0.002431 (19.1%), avg_process_time=0.010279 (80.7%), avg_ack_time=0.000019 (0.2%)
[2015-05-07 22:51:45,776][storm][DEBUG]BasicBolt profile: total_num_tuples=175, num_tuples=96, avg_read_time=0.000048 (0.5%), avg_process_time=0.010374 (99.3%), avg_ack_time=0.000025 (0.2%)
[2015-05-07 22:51:46,784][storm][DEBUG]BasicBolt profile: total_num_tuples=271, num_tuples=96, avg_read_time=0.000043 (0.4%), avg_process_time=0.010417 (99.3%), avg_ack_time=0.000026 (0.2%)
[2015-05-07 22:51:47,791][storm][DEBUG]BasicBolt profile: total_num_tuples=368, num_tuples=97, avg_read_time=0.000041 (0.4%), avg_process_time=0.010317 (99.4%), avg_ack_time=0.000021 (0.2%)</pre></div><div class="section" title="Split sentence bolt log"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec52"/>Split sentence bolt log</h2></div></div></div><p>The following is the <a id="id216" class="indexterm"/>split sentence bolt log:</p><div class="informalexample"><pre class="programlisting">[2015-05-07 22:51:44,918][storm][DEBUG]BasicBolt profile: total_num_tuples=591, num_tuples=591, avg_read_time=0.001623 (95.8%), avg_process_time=0.000052 (3.1%), avg_ack_time=0.000019 (1.1%)
[2015-05-07 22:51:45,924][storm][DEBUG]BasicBolt profile: total_num_tuples=1215, num_tuples=624, avg_read_time=0.001523 (94.7%), avg_process_time=0.000060 (3.7%), avg_ack_time=0.000025 (1.5%)
[2015-05-07 22:51:46,930][storm][DEBUG]BasicBolt profile: total_num_tuples=1829, num_tuples=614, avg_read_time=0.001559 (95.4%), avg_process_time=0.000055 (3.3%), avg_ack_time=0.000021 (1.3%)
[2015-05-07 22:51:47,938][storm][DEBUG]BasicBolt profile: total_num_tuples=2451, num_tuples=622, avg_read_time=0.001547 (95.7%), avg_process_time=0.000049 (3.0%), avg_ack_time=0.000020 (1.3%)</pre></div></div><div class="section" title="Word count bolt log"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec53"/>Word count bolt log</h2></div></div></div><p>These logs demonstrate that the split sentence bolt spends 0.010338 seconds processing and acknowledging each tuple (0.010317 + 0.000021), while the word count bolt spends 0.000069 seconds (0.000049 + 0.000020) per tuple. The split sentence bolt is slower, which suggests that you may <a id="id217" class="indexterm"/>want more instances of the split sentence bolt than the word count bolt.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note12"/>Note</h3><p>Why wasn't the read time considered in the preceding calculation? Read time includes the CPU time taken to read tuples from Storm, but it also includes time spent waiting (that is, sleeping) for the tuples to arrive. If the upstream component is providing data slowly, we don't want to count that time against our component. So for simplicity, we omitted the read time from the calculation.</p></div></div><p>Of course, the per-tuple performance is only part of the picture. You must also consider the sheer number of tuples to be processed. During the 4 seconds covered by the preceding logs, the split sentence bolt received 97 tuples (sentences), while the word count bolt received 622 tuples (words). Now we'll apply these numbers to the per-tuple processing times:</p><div class="informalexample"><pre class="programlisting">0.010338 seconds/tuple * 97 tuples = 1.002786 seconds (Split sentence)
0.000069 seconds/tuple * 622 tuples = 0.042918 seconds (Word count)</pre></div><p>The total time used by the split sentence bolt is much larger (roughly 23 times greater), and we should take this into account while configuring the parallelism of the topology. For example, we might configure <code class="literal">topology.yaml</code> as follows:</p><div class="informalexample"><pre class="programlisting">petrel.parallelism.splitsentence: 24
petrel.parallelism.wordcount: 1</pre></div><p>By configuring the topology in this way, we help ensure that at high traffic rates, there are enough split sentence bolts to avoid becoming a bottleneck, keeping the word count bolts busy all the time.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note13"/>Note</h3><p>The logs from the preceding section used a version of the split sentence bolt that was deliberately modified to run slower and make the example clearer.</p></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec35"/>Summary</h1></div></div></div><p>In this chapter, you learned some skills that will help make you more productive building your own topologies. As you develop spouts or bolts, you can test them individually before assembling them into a complete topology and deploying on Storm. If you encounter a tricky problem that occurs only while running in Storm, you can use Winpdb in addition to (or instead of) log messages. When your code is working, you can get insights into which components take most of the time, so you can focus on improving performance in those areas. With these skills, you are now ready to go out and build your own topologies. Good luck!</p></div></div>


  <div id="sbo-rt-content"><div class="appendix" title="Appendix A. Managing Storm Using Supervisord"><div class="titlepage"><div><div><h1 class="title"><a id="appA"/>Appendix A. Managing Storm Using Supervisord</h1></div></div></div><p>This appendix gives you an overview of the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Storm administration over a cluster</li><li class="listitem" style="list-style-type: disc">Introducing supervisord</li><li class="listitem" style="list-style-type: disc">Components of supervisord</li><li class="listitem" style="list-style-type: disc">Supervisord installation and configuration</li></ul></div><div class="section" title="Storm administration over a cluster"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec36"/>Storm administration over a cluster</h1></div></div></div><p>There are many tools<a id="id218" class="indexterm"/> available that can create multiple virtual machines, install predefined software and even manage the state of that software.</p><div class="section" title="Introducing supervisord"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec54"/>Introducing supervisord</h2></div></div></div><p>Supervisord is a process<a id="id219" class="indexterm"/> control system. It is a client-server system that allows its <a id="id220" class="indexterm"/>users to monitor and control a number of processes on Unix-like operating<a id="id221" class="indexterm"/> systems. For details, visit <a class="ulink" href="http://supervisord.org/">http://supervisord.org/</a>.</p></div><div class="section" title="Supervisord components"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec55"/>Supervisord components</h2></div></div></div><p>The server piece of the <a id="id222" class="indexterm"/>supervisor is known as supervisord. It is responsible for starting child programs upon its own invocation, responding to<a id="id223" class="indexterm"/> commands from clients, restarting crashed or exited subprocesses, logging its subprocess <code class="literal">stdout</code> and <code class="literal">stderr</code> output, and generating and handling "events" corresponding to points in subprocess lifetimes. The server process uses a configuration file. This is typically located in <code class="literal">/etc/supervisord.conf</code>. This configuration file is a Windows-INI style <code class="literal">config</code> file. It is important to keep this file secure via proper filesystem permissions because it might contain decrypted usernames<a id="id224" class="indexterm"/> and passwords:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>supervisorctl</strong></span>: The command-line client piece of the supervisor is known as supervisorctl. It<a id="id225" class="indexterm"/> provides a shell-like interface for the features provided by supervisord. From supervisorctl, a user can connect to different supervisord processes. They can get the status on the subprocesses controlled by, stop and start subprocesses of, and get lists of running processes of a supervisord. The command-line client talks to the server across a Unix domain socket or an Internet (TCP) socket. The server can assert that the user of a client should present authentication credentials before it allows them to use commands. The client process typically uses the same configuration file as the server, but any configuration file with a <code class="literal">[supervisorctl]</code> section in it will work.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Web server</strong></span>: A (sparse) web user interface with functionality comparable to supervisorctl may be accessed <a id="id226" class="indexterm"/>via a browser if you start supervisord against an Internet socket. Visit the server URL (for example, <code class="literal">http://localhost:9001/</code>) to view and control the process status through the web interface after activating the configuration file's <code class="literal">[inet_http_server]</code> section.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>XML-RPC interface</strong></span>: The same HTTP server that serves the web UI serves up an XML-RPC interface that <a id="id227" class="indexterm"/>can be used to interrogate and control the supervisor and the programs it runs. See <span class="emphasis"><em>XML-RPC API Documentation</em></span>.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Machines</strong></span>: Let's assume<a id="id228" class="indexterm"/> that we have two EC2 machines of IP addresses <code class="literal">172-31-19-62</code> and <code class="literal">172.31.36.23</code>. We will install supervisord on both machines and later configure to decide what services of Storm would be running on each machine.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Storm and Zookeeper setup</strong></span>: Let's run Zookeeper, Nimbus, supervisor, and the UI on machine <code class="literal">172.31.36.23</code> and<a id="id229" class="indexterm"/> only the supervisor on <code class="literal">172-31-19-62</code>.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Zookeeper </strong></span><a id="id230" class="indexterm"/><span class="strong"><strong>version</strong></span>: <code class="literal">zookeeper-3.4.6.tar.gz</code>.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Storm </strong></span><a id="id231" class="indexterm"/><span class="strong"><strong>version</strong></span>: <code class="literal">apache-storm-0.9.5.tar.gz</code>.</li></ul></div><p>Here is the process <a id="id232" class="indexterm"/>of the Zookeeper server setup and configuration:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Download Zookeeper's latest version and extract it:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>tar –xvf zookeeper-3.4.6.tar.gz</strong></span></pre></div></li><li class="listitem">Configure <code class="literal">zoo.cfg</code> in the <code class="literal">conf</code> directory to start Zookeeper in cluster mode.</li><li class="listitem">Zookeeper conf:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>server.1=172.31.36.23:2888:3888</strong></span>
<span class="strong"><strong>tickTime=2000</strong></span>
<span class="strong"><strong>initLimit=10</strong></span>
<span class="strong"><strong>syncLimit=5</strong></span>
<span class="strong"><strong># the directory where the snapshot is stored.</strong></span>
<span class="strong"><strong>dataDir=/home/ec2-user/zookeeper-3.4.6/tmp/zookeeper</strong></span>
<span class="strong"><strong>clientPort=2181</strong></span></pre></div></li><li class="listitem">Make sure that the directory specified in <code class="literal">dataDir</code> is created and the user has read and write permissions on it.</li><li class="listitem">Then, go to the Zookeeper <code class="literal">bin</code> directory and start the <code class="literal">zookeeper</code> server using the following command:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[ec2-user@ip-172-31-36-23 bin~]$ zkServer.sh start</strong></span></pre></div></li></ol></div><p>Storm server setup <a id="id233" class="indexterm"/>and configuration:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Download Storm's latest version from the Apache Storm website and extract it:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>tar –xvf apache-storm-0.9.5.tar.gz</strong></span></pre></div></li><li class="listitem">Here is the configuration of the Storm Nimbus machine as well as the slave (added/changed configuration only):<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>storm.zookeeper.servers: - "172.31.36.23"</strong></span>

<span class="strong"><strong>nimbus.host: "172.31.36.23"</strong></span>

<span class="strong"><strong>nimbus.childopts: "-Xmx1024m -Djava.net.preferIPv4Stack=true"</strong></span>

<span class="strong"><strong>ui.childopts: "-Xmx768m -Djava.net.preferIPv4Stack=true"</strong></span>

<span class="strong"><strong>supervisor.childopts: "-Djava.net.preferIPv4Stack=true"</strong></span>

<span class="strong"><strong>worker.childopts: "-Xmx768m -Djava.net.preferIPv4Stack=true"</strong></span>

<span class="strong"><strong>storm.local.dir: "/home/ec2-user/apache-storm-0.9.5/local"</strong></span>

<span class="strong"><strong>supervisor.slots.ports:</strong></span>
<span class="strong"><strong>    - 6700</strong></span>
<span class="strong"><strong>    - 6701</strong></span>
<span class="strong"><strong>    - 6702</strong></span>
<span class="strong"><strong>    - 6703</strong></span></pre></div></li></ol></div><div class="section" title="Supervisord installation"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec10"/>Supervisord installation</h3></div></div></div><p>It is possible to install <a id="id234" class="indexterm"/>supervisord by the following two ways:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Installing on a system with Internet access:<p>Download the Setup tool and <a id="id235" class="indexterm"/>use the <code class="literal">easy_install</code> method.</p></li><li class="listitem">Installing on a system without Internet access:<p>Download all dependencies, copy to each machine, and then install it.</p></li></ol></div><p>We will follow the second method of installation, the one in which Internet access is not required. We will download all dependencies and supervisord, and copy it to the servers.</p><p>Supervisord <code class="literal">[supervisor-3.1.3.tar.gz]</code> requires the following dependencies to be installed:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Python 2.7 or <a id="id236" class="indexterm"/>later</li><li class="listitem" style="list-style-type: disc"><code class="literal">setuptools</code> (latest) from <a class="ulink" href="http://pypi.python.org/pypi/setuptools">http://pypi.python.org/pypi/setuptools</a></li><li class="listitem" style="list-style-type: disc"><code class="literal">elementtree</code> (latest) from <a class="ulink" href="http://effbot.org/downloads#elementtree">http://effbot.org/downloads#elementtree</a>. <code class="literal">elementtree-1.2-20040618.tar.gz</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">meld3-0.6.5.tar.gz</code></li></ul></div><p>Let's install supervisord<a id="id237" class="indexterm"/> and the necessary dependencies on both machines, <code class="literal">172.31.36.23</code> and <code class="literal">172-31-19-62</code>.</p><p>The following are the <a id="id238" class="indexterm"/>steps for installing the dependencies:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><code class="literal">setuptools</code>:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Unzip the <code class="literal">.zip</code> file using this command:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[ec2-user@ip-172-31-19-62 ~]$ tar -xvf setuptools-17.1.1.zip</strong></span></pre></div></li><li class="listitem" style="list-style-type: disc">Go to the <code class="literal">setuptools-17.1.1</code> directory and run the installation command with <code class="literal">sudo</code>:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[ec2-user@ip-172-31-19-62 setuptools-17.1.1]$ sudo python setup.py install</strong></span></pre></div><div class="mediaobject"><img src="images/B03471_AppendixA_01.jpg" alt="Supervisord installation"/></div><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>storm.zookeeper.servers: - "172.31.36.23"</strong></span>

<span class="strong"><strong>nimbus.host: "172.31.36.23"</strong></span>

<span class="strong"><strong>nimbus.childopts: "-Xmx1024m -Djava.net.preferIPv4Stack=true"</strong></span>

<span class="strong"><strong>ui.childopts: "-Xmx768m -Djava.net.preferIPv4Stack=true"</strong></span>

<span class="strong"><strong>supervisor.childopts: "-Djava.net.preferIPv4Stack=true"</strong></span>

<span class="strong"><strong>worker.childopts: "-Xmx768m -Djava.net.preferIPv4Stack=true"</strong></span>

<span class="strong"><strong>storm.local.dir: "/home/ec2-user/apache-storm-0.9.5/local"</strong></span>

<span class="strong"><strong>supervisor.slots.ports:</strong></span>
<span class="strong"><strong>    - 6700</strong></span>
<span class="strong"><strong>    - 6701</strong></span>
<span class="strong"><strong>    - 6702</strong></span>
<span class="strong"><strong>    - 6703</strong></span></pre></div></li></ul></div></li><li class="listitem"><code class="literal">meld3</code>:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Extract the <code class="literal">.ts.gz</code> file using the following command:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[ec2-user@ip-172-31-19-62 ~]$ tar -xvf meld3-0.6.5.tar.gz</strong></span></pre></div></li><li class="listitem" style="list-style-type: disc">Go to the <code class="literal">meld3.-0.6.5</code> directory and run this command:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[ec2-user@ip-172-31-19-62 meld3-0.6.5]$ sudo pyth setup.py install</strong></span></pre></div></li></ul></div><div class="mediaobject"><img src="images/B03471_AppendixA_02.jpg" alt="Supervisord installation"/></div></li><li class="listitem"><code class="literal">elementtree</code>:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Extract<a id="id239" class="indexterm"/> the <code class="literal">.ts.gz</code> file:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[ec2-user@ip-172-31-19-62 ~]$ tar -xvf elementtree-1.2-20040618.tar.gz</strong></span></pre></div></li><li class="listitem" style="list-style-type: disc">Go to <code class="literal">elementtree-1.2-20040618</code> and run the following command:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[ec2-user@ip-172-31-19-62 elementtree-1.2-20040618]$ sudo python setup.py install</strong></span></pre></div></li></ul></div><div class="mediaobject"><img src="images/B03471_AppendixA_03.jpg" alt="Supervisord installation"/></div></li></ol></div><p>The following are the<a id="id240" class="indexterm"/> supervisord installations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Extract <code class="literal">supervisor-3.1.3</code> using this command:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[ec2-user@ip-172-31-19-62 ~]$ tar -xvf supervisor-3.1.3.tar.gz</strong></span></pre></div></li><li class="listitem" style="list-style-type: disc">Go to the <code class="literal">supervisor-3.1.3</code> directory and run the following command:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[ec2-user@ip-172-31-19-62 supervisor-3.1.3]$ sudo python setup.py install</strong></span></pre></div><div class="mediaobject"><img src="images/B03471_AppendixA_04.jpg" alt="Supervisord installation"/></div></li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note14"/>Note</h3><p>A similar setup of supervisord is required on another machine, that is, <code class="literal">172.31.36.23</code>.</p></div></div><div class="section" title="Configuration of supervisord.conf"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec01"/>Configuration of supervisord.conf</h4></div></div></div><p>Lets configure <a id="id241" class="indexterm"/>services on the <code class="literal">172.31.36.23</code> machine and assume that the supervisord installation is done as explained previously. Once supervisor is installed, you can build the <code class="literal">supervisord.conf</code> file to start the <code class="literal">supervisord</code> <a id="id242" class="indexterm"/>and <code class="literal">supervisorctl</code> commands:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Make the <code class="literal">supervisor.conf</code> file. Put it into the <code class="literal">/etc</code> directory.</li><li class="listitem" style="list-style-type: disc">We can refer get sample <code class="literal">supervisord.conf</code> using the following command:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[ec2-user@ip-172-31-36-23 ~]$ echo_supervisord_conf</strong></span></pre></div></li></ul></div><p>Take a look at the <code class="literal">supervisord.conf</code> file:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[unix_http_server]</strong></span>
file = /home/ec2-user/supervisor.sock
chmod = 0777

<span class="strong"><strong>[inet_http_server]</strong></span>         ; inet (TCP) server disabled by default
port=172.31.36.23:9001        ; (ip_address:port specifier, *:port for all iface)
username=user              ; (default is no username (open server))
password=123               ; (default is no password (open server))

<span class="strong"><strong>[rpcinterface:supervisor]</strong></span>
supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface

<span class="strong"><strong>[supervisord]</strong></span>
logfile_backups=10           ; (num of main logfile rotation backups;default 10)
logfile=/home/ec2-user/supervisord.log ; (main log file;default $CWD/supervisord.log)
logfile_maxbytes=50MB        ; (max main logfile bytes b4 rotation;default 50MB)
pidfile=/home/ec2-user/supervisord.pid ; (supervisord pidfile;default supervisord.pid)
nodaemon=false               ; (start in foreground if true;default false)
minfds=1024                  ; (min. avail startup file descriptors;default 1024)

<span class="strong"><strong>[supervisorctl]</strong></span>
;serverurl = unix:///home/ec2-user/supervisor.sock
serverurl=http://172.31.36.23:9001 ; use an http:// url to specify an inet socket
;username=chris              ; should be same as http_username if set
;password=123                ; should be same as http_password if set

<span class="strong"><strong>[program:storm-nimbus]</strong></span>
command=/home/ec2-user/apache-storm-0.9.5/bin/storm nimbus
user=ec2-user
autostart=false
autorestart=false
startsecs=10
startretries=999
log_stdout=true
log_stderr=true
stdout_logfile=/home/ec2-user/storm/logs/nimbus.out
logfile_maxbytes=20MB
logfile_backups=10

<span class="strong"><strong>[program:storm-ui]</strong></span>
command=/home/ec2-user/apache-storm-0.9.5/bin/storm ui
user=ec2-user
autostart=false
autorestart=false
startsecs=10
startretries=999
log_stdout=true
log_stderr=true
stdout_logfile=/home/ec2-user/storm/logs/ui.out
logfile_maxbytes=20MB
logfile_backups=10

<span class="strong"><strong>[program:storm-supervisor]</strong></span>
command=/home/ec2-user/apache-storm-0.9.5/bin/storm supervisor
user=ec2-user
autostart=false
autorestart=false
startsecs=10
startretries=999
log_stdout=true
log_stderr=true
stdout_logfile=/home/ec2-user/storm/logs/supervisor.out
logfile_maxbytes=20MB
logfile_backups=10</pre></div><p>Start the supervisor server first:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[ec2-user@ip-172-31-36-23 ~] sudo /usr/bin/supervisord -c /etc/supervisord.conf</strong></span></pre></div><p>Then, start all processes using <code class="literal">supervisorctl</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[ec2-user@ip-172-31-36-23 ~] sudo /usr/bin/supervisorctl -c /etc/supervisord.conf status</strong></span>
<span class="strong"><strong>storm-nimbus                     STOPPED   Not started</strong></span>
<span class="strong"><strong>storm-supervisor                 STOPPED   Not started</strong></span>
<span class="strong"><strong>storm-ui                         STOPPED   Not started</strong></span>
<span class="strong"><strong>[ec2-user@ip-172-31-36-23 ~]$ sudo /usr/bin/supervisorctl -c /etc/supervisord.conf start all</strong></span>
<span class="strong"><strong>storm-supervisor: started</strong></span>
<span class="strong"><strong>storm-ui: started</strong></span>
<span class="strong"><strong>storm-nimbus: started</strong></span>
<span class="strong"><strong>[ec2-user@ip-172-31-36-23 ~]$ jps</strong></span>
<span class="strong"><strong>14452 Jps</strong></span>
<span class="strong"><strong>13315 QuorumPeerMain</strong></span>
<span class="strong"><strong>14255 nimbus</strong></span>
<span class="strong"><strong>14233 supervisor</strong></span>
<span class="strong"><strong>14234 core</strong></span>
<span class="strong"><strong>[ec2-user@ip-172-31-36-23 ~]$</strong></span></pre></div><div class="mediaobject"><img src="images/B03471_AppendixA_05.jpg" alt="Configuration of supervisord.conf"/></div><p>We can view the<a id="id243" class="indexterm"/> supervisord web UI and control processes on the browser. <code class="literal">52.11.193.108</code> is the public IP address of the <code class="literal">172-31-36-23</code> machine (<code class="literal">http://52.11.193.108:9001</code>):</p><div class="mediaobject"><img src="images/B03471_AppendixA_06.jpg" alt="Configuration of supervisord.conf"/></div></div><div class="section" title="Configuration of supervisord.conf on 172-31-19-62"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec02"/>Configuration<a id="id244" class="indexterm"/> of supervisord.conf on 172-31-19-62 
</h4></div></div></div><p>Keep only the following services in the configuration file:</p><div class="informalexample"><pre class="programlisting">[unix_http_server]
[rpcinterface:supervisor]
[supervisord]
[supervisorctl]
[program:storm-supervisor]</pre></div><p>After that, you can start the supervisor server and all processes using <code class="literal">supervisorctl</code> on <code class="literal">172-31-19-62</code> machine.</p></div></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec37"/>Summary</h1></div></div></div><p>In this chapter, we saw how distributed Storm processes running over multiple machines can be managed using the supervisord process. There are many options available in supervisord, such as <code class="literal">autostart=true</code>. If we set this option for any Storm process, it also increases the reliability of the overall system and manages failure of Nimbus.</p></div></div>
</body></html>