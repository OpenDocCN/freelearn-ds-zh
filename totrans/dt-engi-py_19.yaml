- en: '*Appendix*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*附录*'
- en: Building a NiFi cluster
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建NiFi集群
- en: In this book, you have built a Kafka cluster, a ZooKeeper cluster, and a Spark
    cluster. Instead of increasing the power of a single server, through clustering,
    you are able to add more machines to increase the processing power of a data pipeline.
    In this chapter, you will learn how to cluster NiFi so that your data pipelines
    can run across multiple machines.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，你已经构建了Kafka集群、ZooKeeper集群和Spark集群。通过集群，你能够增加更多机器来提高数据管道的处理能力，而不是增加单个服务器的功率。在本章中，你将学习如何集群化NiFi，以便你的数据管道可以在多台机器上运行。
- en: 'In this appendix, we''re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本附录中，我们将涵盖以下主要主题：
- en: The basics of NiFi clustering
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NiFi集群的基本知识
- en: Building a NiFi cluster
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建NiFi集群
- en: Building a distributed data pipeline
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建分布式数据处理管道
- en: Managing the distributed data pipeline
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理分布式数据处理管道
- en: The basics of NiFi clustering
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NiFi集群的基本知识
- en: Clustering in Apache NiFi follows a **Zero-Master Clustering** architecture.
    In this type of clustering, there is no pre-defined master. Every node can perform
    the same tasks, and the data is split between them. NiFi uses Zookeeper when deployed
    as a cluster.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Apache NiFi中的集群遵循**零主集群**架构。在这种类型的集群中，没有预定义的主节点。每个节点都可以执行相同的任务，数据在他们之间分配。当作为集群部署时，NiFi使用Zookeeper。
- en: Zookeeper will elect a **Cluster Coordinator**. The Cluster Coordinator is responsible
    for deciding whether new nodes can join – the nodes will connect to the coordinator
    – and to provide the updated flows to the new nodes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Zookeeper将选举一个**集群协调器**。集群协调器负责决定新节点是否可以加入——节点将连接到协调器——并提供更新后的流程给新节点。
- en: While it sounds like the Cluster Coordinator is the master, it is not. You can
    make changes to the data pipelines on any node and they will be replicated to
    all the other nodes, meaning a non-Cluster Coordinator or a non-Primary Node can
    submit changes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然听起来集群协调器是主节点，但实际上并非如此。你可以在任何节点上更改数据管道，并且这些更改将被复制到所有其他节点，这意味着非集群协调器或非主节点可以提交更改。
- en: The `ExecuteSQL` processor can run on the Primary Node, and then distribute
    the data to the other nodes downstream for processing. You will see how this is
    done later in this chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExecuteSQL`处理器可以在主节点上运行，然后将数据分发到下游的其他节点进行处理。你将在本章后面看到这是如何操作的。'
- en: Clustering allows you to build data pipelines that can process larger amounts
    of data than on a single machine. Furthermore, it allows a single point to build
    and monitor data pipelines. If you had several single-node NiFi instances running,
    you would need to manage all of them. Changes to a data pipeline on one would
    need to be replicated on the others or at least checked to make sure it is not
    a duplicate. Which machine is running the data warehouse pipeline again? I forgot.
    Managing a cluster, from any node, makes it much easier and more efficient.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 集群允许你构建能够处理比单台机器更多数据量的数据管道。此外，它允许单个点构建和监控数据管道。如果你运行了多个单节点NiFi实例，你需要管理所有这些实例。一个数据管道的更改需要复制到其他实例，或者至少检查以确保它不是重复的。数据仓库管道又是在哪台机器上运行的？我忘了。从任何节点管理集群，会使它更加容易和高效。
- en: Building a NiFi cluster
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建NiFi集群
- en: 'In this section, you will build a two-node cluster on different machines. Just
    like with MiNiFi, however, there are some compatibility issues with the newest
    versions of NiFi and Zookeeper. To work around these issues and demonstrate the
    concepts, this chapter will use an older version of NiFi and the pre-bundled Zookeeper.
    To build the NiFi cluster, perform the following steps:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将在不同的机器上构建一个双节点集群。然而，与MiNiFi一样，NiFi和Zookeeper的最新版本存在一些兼容性问题。为了解决这些问题并展示概念，本章将使用较旧的NiFi版本和预捆绑的Zookeeper。要构建NiFi集群，请执行以下步骤：
- en: 'As root, or using sudo, open your `/etc/hosts` file. You will need to assign
    names to the machines that you will use in your cluster. It is best practice to
    use a hostname instead of IP addresses. Your hosts file should look like the following
    example:'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以root用户或使用sudo，打开你的`/etc/hosts`文件。你需要为你的集群中使用的机器分配名称。最佳实践是使用主机名而不是IP地址。你的hosts文件应该看起来像以下示例：
- en: '[PRE0]'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the preceding hosts file, I have added the last two lines. The nodes are
    `nifi-node-1` and `nifi-node-2` and you can see that they have different IP addresses.
    Make these changes in the hosts file for each machine. When you have finished,
    you can test that it works by using `ping`. From each machine, try to use `ping`
    to hit the other machine by hostname. The following is the command to hit `nifi-node-2`
    from the `nifi-node-1` machine:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的 hosts 文件中，我已经添加了最后两行。节点是 `nifi-node-1` 和 `nifi-node-2`，您可以看到它们有不同的 IP 地址。请在每台机器的
    hosts 文件中做出这些更改。完成之后，您可以通过使用 `ping` 来测试它是否工作。从每台机器，尝试通过主机名使用 `ping` 来击中另一台机器。以下是从
    `nifi-node-1` 机器击中 `nifi-node-2` 的命令：
- en: '[PRE1]'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you do the opposite from your other node, `nifi-node-2`, you should get the
    same results – `nifi-node-1` will return data.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您对另一个节点，`nifi-node-2`，做相反的操作，您应该得到相同的结果——`nifi-node-1` 将返回数据。
- en: 'Next, download an older version of Apache NiFi, 1.0.0, at [https://archive.apache.org/dist/nifi/1.0.0/](https://archive.apache.org/dist/nifi/1.0.0/).
    Select the `-bin.tar.gz` file as it contains the binaries. Once the file has downloaded,
    extract the files using your file manager or with the following command:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，下载 Apache NiFi 的旧版本，1.0.0，请访问 [https://archive.apache.org/dist/nifi/1.0.0/](https://archive.apache.org/dist/nifi/1.0.0/)。选择
    `-bin.tar.gz` 文件，因为它包含二进制文件。一旦文件下载完成，使用您的文件管理器或以下命令提取文件：
- en: '[PRE2]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Once you have extracted the files, you will edit the configuration files.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦您提取了文件，您将编辑配置文件。
- en: 'To edit the Zookeeper configuration file, open `zookeeper.properties` in the
    `$NIFI_HOME/conf` directory. At the bottom of the file, add your servers as shown:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要编辑 Zookeeper 配置文件，请在 `$NIFI_HOME/conf` 目录中打开 `zookeeper.properties` 文件。在文件底部，按照以下示例添加您的服务器：
- en: '[PRE3]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'At the top of the file, you will see `clientPort` and `dataDir`. It should
    look like the following example:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在文件顶部，您将看到 `clientPort` 和 `dataDir`。它应该看起来像以下示例：
- en: '[PRE4]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In `dataDir`, you will need to add a file named `myfile` with the number of
    the server as the content. On `server.1` (`nifi-node-1`), you will create a `myid`
    ID with `1` as the content. To do that, from the `$NIFI_HOME` directory, use the
    following commands:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `dataDir` 中，您需要添加一个名为 `myfile` 的文件，其内容为服务器的编号。在 `server.1` (`nifi-node-1`)
    上，您将创建一个 `myid` ID，内容为 `1`。为此，从 `$NIFI_HOME` 目录，使用以下命令：
- en: '[PRE5]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'On `nifi-node-2`, repeat the preceding steps, except change `echo` to the following
    line:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `nifi-node-2` 上，重复前面的步骤，除了将 `echo` 改为以下行：
- en: '[PRE6]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To edit `nifi.properties`, you will need to change several properties. The
    first property is `nifi.state.management.embedded.zookeeper.start`, which needs
    to be set to `true`. The section of the file is shown as follows:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要编辑 `nifi.properties`，您需要更改几个属性。第一个属性是 `nifi.state.management.embedded.zookeeper.start`，需要将其设置为
    `true`。文件的部分内容如下所示：
- en: '[PRE7]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The preceding commands tells NiFi to use the embedded version of Zookeeper.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述命令告诉 NiFi 使用内嵌版本的 Zookeeper。
- en: 'You now need to tell NiFi how to connect to Zookeeper in `nifi.zookeeper.connect.string`.
    The string is a comma-separated list of the Zookeeper servers in the format of
    `<hostname>:<port>`, and the port is `clientPort` from the `zookeeper.config`
    file, which was `2181`. The section of the file is shown in the following code
    block:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您现在需要告诉 NiFi 如何在 `nifi.zookeeper.connect.string` 中连接到 Zookeeper。该字符串是逗号分隔的 Zookeeper
    服务器列表，格式为 `<hostname>:<port>`，端口是 `zookeeper.config` 文件中的 `clientPort`，它是 `2181`。文件的部分内容如下所示：
- en: '[PRE8]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, you will configure the `cluster` properties of NiFi. Specifically, you
    will set `nifi.cluster.node` to `true`. You will add the hostname of the node
    to `nifi.cluster.node.address`, as well as adding the port at `nifi.cluster.node.protocol.port`.
    You can set this to anything available and high enough such that you do not need
    root to access it (over `1024`). Lastly, you can change `nifi.cluster.flow.election.max.wait.time`
    to something shorter than 5 minutes and you can add a value for `nifi.cluster.flow.election.max.candidates`.
    I have changed the wait time to `1` minute and left the candidates blank. The
    section of the file is shown in the following code block:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您将配置 NiFi 的 `cluster` 属性。具体来说，您将设置 `nifi.cluster.node` 为 `true`。您将添加节点的主机名到
    `nifi.cluster.node.address`，以及添加端口到 `nifi.cluster.node.protocol.port`。您可以将其设置为任何可用且足够高的端口，这样您就不需要
    root 权限来访问它（超过 `1024`）。最后，您可以将 `nifi.cluster.flow.election.max.wait.time` 更改为小于
    5 分钟，并为 `nifi.cluster.flow.election.max.candidates` 添加一个值。我已经将等待时间更改为 `1` 分钟，并留空候选人。文件的部分内容如下所示：
- en: '[PRE9]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The web properties require the hostname of the machine as well as the port.
    By default, `nifi.web.http.port` is `8080`, but if you have something running
    on that port already, you can change it. I have changed it to `8888`. The hostname
    is `nifi-node-1` or `nifi-mode-2`. The web properties are shown in the following
    code block:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络属性需要机器的主机名以及端口号。默认情况下，`nifi.web.http.port` 是 `8080`，但如果该端口上已经运行了其他东西，您可以更改它。我已经将其更改为
    `8888`。主机名是 `nifi-node-1` 或 `nifi-mode-2`。网络属性在以下代码块中显示：
- en: '[PRE10]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Lastly, NiFi uses Site-to-Site to communicate. You will need to configure the
    `nifi.remote.input.host` property to the machine hostname, and `nifi.remote.input.socket.port`
    to an available port. The properties file is shown in the following code block:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，NiFi使用站点到站点进行通信。您需要将 `nifi.remote.input.host` 属性配置为机器的主机名，并将 `nifi.remote.input.socket.port`
    配置为可用端口。属性文件在以下代码块中显示：
- en: '[PRE11]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Your cluster is now configured, and you are ready to launch the two nodes.
    From each machine, launch NiFi as normal using the following command:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您的集群现在已配置完成，您准备好启动两个节点。从每台机器上，使用以下命令以正常方式启动NiFi：
- en: '[PRE12]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You should now be able to browse to any node at `http://nifi-node-1:8888/nifi`.
    You will see NiFi as usual, shown in the following screenshot:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在应该能够浏览到 `http://nifi-node-1:8888/nifi` 上的任何节点。您将看到与往常一样的NiFi，如下面的屏幕截图所示：
- en: '![Figure 16.1 – NiFi running as a cluster'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![图16.1 – 以集群方式运行的NiFi'
- en: '](img/Figure_16.1_B15739.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_16.1_B15739.jpg](img/Figure_16.1_B15739.jpg)'
- en: Figure 16.1 – NiFi running as a cluster
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1 – 以集群方式运行的NiFi
- en: 'Everything looks exactly the same, except for the top-left corner of the status
    bar. You should now have a cloud with **2/2** next to it. This is telling you
    that NiFi is running as a cluster with 2 out of 2 nodes available and connected.
    You can see the events by hovering over the messages on the right of the status
    bar. The following screenshot shows the election and connection of nodes:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一切看起来都完全一样，只是在状态栏的左上角。现在您应该有一个带有 **2/2** 的云图标。这表示NiFi作为一个集群运行，有2个节点中的2个可用并连接。您可以通过在状态栏右侧的消息上悬停来查看事件。以下屏幕截图显示了节点的选举和连接：
- en: '![Figure 16.2 – Messages showing events in the cluster](img/Figure_16.2_B15739.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图16.2 – 显示集群中事件的消息](img/Figure_16.2_B15739.jpg)'
- en: Figure 16.2 – Messages showing events in the cluster
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2 – 显示集群中事件的消息
- en: 'Lastly, you can open the cluster window by selecting **Cluster** from the waffle
    menu in the right corner of the NiFi window. The cluster is shown in the following
    screenshot:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以通过在NiFi窗口右上角的waffle菜单中选择**集群**来打开集群窗口。集群在以下屏幕截图中显示：
- en: '![Figure 16.3 – Cluster details'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![图16.3 – 集群详细信息'
- en: '](img/Figure_16.3_B15739.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_16.3_B15739.jpg](img/Figure_16.3_B15739.jpg)'
- en: Figure 16.3 – Cluster details
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3 – 集群详细信息
- en: The preceding screenshot shows which node is the Primary Node, along with the
    Controller Node and a Regular Node. From here you can also see details about the
    queues and disconnect or reconnect the nodes. The cluster is working, and you
    can now build a distributed data pipeline.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的屏幕截图显示了哪个节点是主节点，以及控制器节点和一个常规节点。从这里你还可以查看有关队列的详细信息，或者断开或重新连接节点。集群正在运行，你现在可以构建一个分布式数据管道。
- en: Building a distributed data pipeline
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建分布式数据管道
- en: 'Building a distributed data pipeline is almost exactly the same as building
    a data pipeline to run on a single machine. NiFi will handle the logistics of
    passing and recombining the data. A basic data pipeline is shown in the following
    screenshot:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 构建分布式数据管道几乎与构建在单台机器上运行的数据管道完全相同。NiFi将处理传递和重新组合数据的物流。以下屏幕截图显示了基本数据管道：
- en: '![Figure 16.4 – A basic data pipeline to generate data, extract attributes
    to json, and write to disk'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图16.4 – 一个基本的数据管道，用于生成数据、提取属性到JSON并写入磁盘'
- en: '](img/Figure_16.4_B15739.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_16.4_B15739.jpg](img/Figure_16.4_B15739.jpg)'
- en: Figure 16.4 – A basic data pipeline to generate data, extract attributes to
    json, and write to disk
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4 – 一个基本的数据管道，用于生成数据、提取属性到JSON并写入磁盘
- en: The preceding data pipeline uses the `GenerateFlowFile` processor to create
    unique flowfiles. This is passed downstream to the `AttributesToJSON` processor,
    which extracts the attributes and writes to the flowfile content. Lastly, the
    file is written to disk at `/home/paulcrickard/output`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的数据管道使用 `GenerateFlowFile` 处理器来创建唯一的流文件。这些文件随后传递到 `AttributesToJSON` 处理器，该处理器提取属性并将它们写入流文件内容。最后，文件被写入到
    `/home/paulcrickard/output` 目录下的磁盘。
- en: Before running the data pipeline, you will need to make sure that you have the
    output directory for the `PutFile` processor on each node. Earlier, I said that
    data pipelines are no different when distributed, but there are some things you
    must keep in mind, one being that `PutFile` will write to disk on every node by
    default. You will need to configure your processors to be able to run on any node.
    We will fix this later in this section.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行数据管道之前，你需要确保每个节点上都有`PutFile`处理器的输出目录。之前我说过，在分布式情况下数据管道没有不同，但是有一些事情你必须记住，其中之一就是`PutFile`默认情况下会在每个节点上写入磁盘。你需要配置你的处理器以便在任何节点上运行。我们将在本节稍后解决这个问题。
- en: One more thing before you run the data pipeline. Open the browser to your other
    node. You will see the exact same data pipeline in that node. Even the layout
    of the processors is the same. Changes to any node will be distributed to all
    the other nodes. You can work from any node.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行数据管道之前还有一件事。打开浏览器到你的另一个节点。你将看到那个节点上完全相同的数据管道。甚至处理器的布局也是一样的。对任何节点的更改都将分布到所有其他节点。你可以在任何节点上工作。
- en: 'When you run the data pipeline, you will see files written to the output directory
    of both nodes. The data pipeline is running and distributing the load across the
    nodes. The following screenshot shows the output of the data pipeline:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行数据管道时，你将看到两个节点的输出目录中都有文件被写入。数据管道正在运行并将负载分配到各个节点。以下截图显示了数据管道的输出：
- en: '![Figure 16.5 – Data pipeline writing flowfiles to a node'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 16.5 – Data pipeline writing flowfiles to a node'
- en: '](img/Figure_16.5_B15739.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_16.5_B15739.jpg]'
- en: Figure 16.5 – Data pipeline writing flowfiles to a node
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5 – 数据管道将流文件写入节点
- en: If you are getting the same results as the preceding screenshot, congratulations,
    you have just built a distributed data pipeline. Next, you will learn some more
    features of the NiFi cluster.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你得到的结果与前面的截图相同，恭喜你，你刚刚构建了一个分布式数据管道。接下来，你将学习NiFi集群的一些更多功能。
- en: Managing the distributed data pipeline
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理分布式数据管道
- en: The preceding data pipeline runs on each node. To compensate for that, you had
    to create the same path on both nodes for the `PutFile` processor to work. Earlier,
    you learned that there are several processors that can result in race conditions
    – trying to read the same file at the same time – which will cause problems. To
    resolve these issues, you can specify that a processor should only run on the
    Primary Node – as an isolated process.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的数据管道在每个节点上运行。为了补偿这一点，你必须为`PutFile`处理器在两个节点上创建相同的路径才能工作。之前你了解到有几个处理器会导致竞争条件——尝试同时读取相同的文件，这将会引起问题。为了解决这些问题，你可以指定一个处理器应该只在主节点上运行——作为一个独立的过程。
- en: 'In the configuration for the `PutFile` processor, select the **Scheduling**
    tab. In the dropdown for **Scheduling Strategy**, choose **On primary node**,
    as shown in the following screenshot:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在`PutFile`处理器的配置中，选择**调度**选项卡。在**调度策略**下拉菜单中，选择**主节点**，如下截图所示：
- en: '![Figure 16.6 – Running a processor on the Primary Node only'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 16.6 – Running a processor on the Primary Node only'
- en: '](img/Figure_16.6_B15739.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_16.6_B15739.jpg]'
- en: Figure 16.6 – Running a processor on the Primary Node only
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6 – 仅在主节点上运行处理器
- en: Now, when you run the data pipeline, the files will only be placed on the Primary
    Node. You can schedule processors such as `GetFile` or `ExecuteSQL` to do the
    same thing.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当你运行数据管道时，文件将只放置在主节点上。你可以调度像`GetFile`或`ExecuteSQL`这样的处理器来完成相同的事情。
- en: 'To see the load of the data pipeline on each node, you can look at the cluster
    details from the waffle menu. As data moves through the data pipeline, you can
    see how many flowfiles are sitting in the queues of each node. The following screenshot
    shows the pipeline running on my cluster:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看每个节点上数据管道的负载，你可以从waffle菜单查看集群详情。当数据通过数据管道移动时，你可以看到有多少流文件坐在每个节点的队列中。以下截图显示了在我集群上运行的管道：
- en: '![Figure 16.7 – Viewing the queues of each node. Each node has four flowfiles'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 16.7 – Viewing the queues of each node. Each node has four flowfiles'
- en: '](img/Figure_16.7_B15739.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_16.7_B15739.jpg]'
- en: Figure 16.7 – Viewing the queues of each node. Each node has four flowfiles
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7 – 查看每个节点的队列。每个节点有四个流文件
- en: The data pipeline is distributing the flowfiles evenly across the nodes. In
    **Zero-Master Clustering**, the data is not copied or replicated. It exists only
    on the node that is processing it. If a node goes down, the data needs to be redistributed.
    This can only happen if the node is still connected to the network, otherwise,
    it will not happen until the node rejoins.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道正在将flowfiles均匀地分配到各个节点。在**零主集群**中，数据不会被复制或复制。它只存在于正在处理它的节点上。如果一个节点宕机，数据需要重新分配。这只能在节点仍然连接到网络的情况下发生，否则，它将不会发生，直到节点重新加入。
- en: 'You can manually disconnect a node by clicking the power icon on the right
    of the node''s row. The following screenshot shows a node being disconnected:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过点击节点行右侧的电源图标手动断开节点连接。以下截图显示了节点正在断开连接的情况：
- en: '![Figure 16.8 – nifi-node-1 has been disconnected from the cluster'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '![图16.8 – nifi-node-1已从集群断开连接'
- en: '](img/Figure_16.9_B15739.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_16.9_B15739.jpg)'
- en: Figure 16.8 – nifi-node-1 has been disconnected from the cluster
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.8 – nifi-node-1已从集群断开连接
- en: 'In the preceding screenshot, you can see that `nifi-node-1` has a status of
    **DISCONNECTED**. But you should also notice that it has eight flowfiles that
    need to be redistributed. Since you disconnected the node, but did not drop it
    from the network, NiFi will redistribute the flowfiles. You can see the results
    when the screen is refreshed, as shown in the following screenshot:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，你可以看到`nifi-node-1`的状态为**断开连接**。但你也应该注意到它有八个需要重新分配的flowfiles。由于你断开了节点，但没有将其从网络中删除，NiFi将重新分配flowfiles。你可以通过刷新屏幕看到结果，如下面的截图所示：
- en: '![Figure 16.9 – Redistributed flowfiles from a disconnected node'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图16.9 – 从断开连接的节点重新分配的flowfiles'
- en: '](img/Figure_16.8_B15739.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_16.8_B15739.jpg)'
- en: Figure 16.9 – Redistributed flowfiles from a disconnected node
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.9 – 从断开连接的节点重新分配的flowfiles
- en: 'You can also reconnect any disconnected nodes. You do this by clicking the
    plug icon. When you do, the node will rejoin the cluster and the flowfiles will
    be redistributed. The following screenshot shows the node rejoined to the cluster:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以重新连接任何断开的节点。你通过点击插头图标来完成此操作。当你这样做时，节点将重新加入集群，flowfiles将被重新分配。以下截图显示了节点重新加入集群的情况：
- en: '![Figure 16.10 – Reconnecting a node and flowfile redistribution](img/Figure_16.10_B15739.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图16.10 – 重新连接节点和flowfile重新分配](img/Figure_16.10_B15739.jpg)'
- en: Figure 16.10 – Reconnecting a node and flowfile redistribution
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.10 – 重新连接节点和flowfile重新分配
- en: In the preceding screenshot, the flowfiles have accumulated since the node was
    disconnected evenly across the nodes.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，由于节点断开连接，flowfiles在节点之间均匀累积。
- en: Summary
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this Appendix, you learned the basics of NiFi clustering, as well as how
    to build a cluster with the embedded Zookeeper and how to build distributed data
    pipelines. NiFi handles most of the distribution of data; you only need to keep
    in mind the gotchas – such as race conditions and the fact that processors need
    to be configured to run on any node. Using a NiFi cluster allows you to manage
    NiFi on several machines from a single instance. It also allows you to process
    large amounts of data and have some redundancy in case an instance crashes.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本附录中，你学习了NiFi集群的基本知识，以及如何使用嵌入式Zookeeper构建集群以及如何构建分布式数据管道。NiFi处理大部分数据分布；你只需要记住一些注意事项——例如竞争条件和处理器需要配置在任意节点上运行。使用NiFi集群允许你从单个实例管理多个机器上的NiFi。它还允许你处理大量数据，并在实例崩溃的情况下提供一些冗余。
