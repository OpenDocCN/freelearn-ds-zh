- en: '*Appendix*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building a NiFi cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, you have built a Kafka cluster, a ZooKeeper cluster, and a Spark
    cluster. Instead of increasing the power of a single server, through clustering,
    you are able to add more machines to increase the processing power of a data pipeline.
    In this chapter, you will learn how to cluster NiFi so that your data pipelines
    can run across multiple machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this appendix, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The basics of NiFi clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a NiFi cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a distributed data pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing the distributed data pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The basics of NiFi clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering in Apache NiFi follows a **Zero-Master Clustering** architecture.
    In this type of clustering, there is no pre-defined master. Every node can perform
    the same tasks, and the data is split between them. NiFi uses Zookeeper when deployed
    as a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Zookeeper will elect a **Cluster Coordinator**. The Cluster Coordinator is responsible
    for deciding whether new nodes can join – the nodes will connect to the coordinator
    – and to provide the updated flows to the new nodes.
  prefs: []
  type: TYPE_NORMAL
- en: While it sounds like the Cluster Coordinator is the master, it is not. You can
    make changes to the data pipelines on any node and they will be replicated to
    all the other nodes, meaning a non-Cluster Coordinator or a non-Primary Node can
    submit changes.
  prefs: []
  type: TYPE_NORMAL
- en: The `ExecuteSQL` processor can run on the Primary Node, and then distribute
    the data to the other nodes downstream for processing. You will see how this is
    done later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering allows you to build data pipelines that can process larger amounts
    of data than on a single machine. Furthermore, it allows a single point to build
    and monitor data pipelines. If you had several single-node NiFi instances running,
    you would need to manage all of them. Changes to a data pipeline on one would
    need to be replicated on the others or at least checked to make sure it is not
    a duplicate. Which machine is running the data warehouse pipeline again? I forgot.
    Managing a cluster, from any node, makes it much easier and more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Building a NiFi cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will build a two-node cluster on different machines. Just
    like with MiNiFi, however, there are some compatibility issues with the newest
    versions of NiFi and Zookeeper. To work around these issues and demonstrate the
    concepts, this chapter will use an older version of NiFi and the pre-bundled Zookeeper.
    To build the NiFi cluster, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As root, or using sudo, open your `/etc/hosts` file. You will need to assign
    names to the machines that you will use in your cluster. It is best practice to
    use a hostname instead of IP addresses. Your hosts file should look like the following
    example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the preceding hosts file, I have added the last two lines. The nodes are
    `nifi-node-1` and `nifi-node-2` and you can see that they have different IP addresses.
    Make these changes in the hosts file for each machine. When you have finished,
    you can test that it works by using `ping`. From each machine, try to use `ping`
    to hit the other machine by hostname. The following is the command to hit `nifi-node-2`
    from the `nifi-node-1` machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you do the opposite from your other node, `nifi-node-2`, you should get the
    same results – `nifi-node-1` will return data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, download an older version of Apache NiFi, 1.0.0, at [https://archive.apache.org/dist/nifi/1.0.0/](https://archive.apache.org/dist/nifi/1.0.0/).
    Select the `-bin.tar.gz` file as it contains the binaries. Once the file has downloaded,
    extract the files using your file manager or with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once you have extracted the files, you will edit the configuration files.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To edit the Zookeeper configuration file, open `zookeeper.properties` in the
    `$NIFI_HOME/conf` directory. At the bottom of the file, add your servers as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At the top of the file, you will see `clientPort` and `dataDir`. It should
    look like the following example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In `dataDir`, you will need to add a file named `myfile` with the number of
    the server as the content. On `server.1` (`nifi-node-1`), you will create a `myid`
    ID with `1` as the content. To do that, from the `$NIFI_HOME` directory, use the
    following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'On `nifi-node-2`, repeat the preceding steps, except change `echo` to the following
    line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To edit `nifi.properties`, you will need to change several properties. The
    first property is `nifi.state.management.embedded.zookeeper.start`, which needs
    to be set to `true`. The section of the file is shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding commands tells NiFi to use the embedded version of Zookeeper.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You now need to tell NiFi how to connect to Zookeeper in `nifi.zookeeper.connect.string`.
    The string is a comma-separated list of the Zookeeper servers in the format of
    `<hostname>:<port>`, and the port is `clientPort` from the `zookeeper.config`
    file, which was `2181`. The section of the file is shown in the following code
    block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you will configure the `cluster` properties of NiFi. Specifically, you
    will set `nifi.cluster.node` to `true`. You will add the hostname of the node
    to `nifi.cluster.node.address`, as well as adding the port at `nifi.cluster.node.protocol.port`.
    You can set this to anything available and high enough such that you do not need
    root to access it (over `1024`). Lastly, you can change `nifi.cluster.flow.election.max.wait.time`
    to something shorter than 5 minutes and you can add a value for `nifi.cluster.flow.election.max.candidates`.
    I have changed the wait time to `1` minute and left the candidates blank. The
    section of the file is shown in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The web properties require the hostname of the machine as well as the port.
    By default, `nifi.web.http.port` is `8080`, but if you have something running
    on that port already, you can change it. I have changed it to `8888`. The hostname
    is `nifi-node-1` or `nifi-mode-2`. The web properties are shown in the following
    code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, NiFi uses Site-to-Site to communicate. You will need to configure the
    `nifi.remote.input.host` property to the machine hostname, and `nifi.remote.input.socket.port`
    to an available port. The properties file is shown in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Your cluster is now configured, and you are ready to launch the two nodes.
    From each machine, launch NiFi as normal using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You should now be able to browse to any node at `http://nifi-node-1:8888/nifi`.
    You will see NiFi as usual, shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.1 – NiFi running as a cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_16.1_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 16.1 – NiFi running as a cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'Everything looks exactly the same, except for the top-left corner of the status
    bar. You should now have a cloud with **2/2** next to it. This is telling you
    that NiFi is running as a cluster with 2 out of 2 nodes available and connected.
    You can see the events by hovering over the messages on the right of the status
    bar. The following screenshot shows the election and connection of nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.2 – Messages showing events in the cluster](img/Figure_16.2_B15739.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.2 – Messages showing events in the cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, you can open the cluster window by selecting **Cluster** from the waffle
    menu in the right corner of the NiFi window. The cluster is shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.3 – Cluster details'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_16.3_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 16.3 – Cluster details
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot shows which node is the Primary Node, along with the
    Controller Node and a Regular Node. From here you can also see details about the
    queues and disconnect or reconnect the nodes. The cluster is working, and you
    can now build a distributed data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Building a distributed data pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Building a distributed data pipeline is almost exactly the same as building
    a data pipeline to run on a single machine. NiFi will handle the logistics of
    passing and recombining the data. A basic data pipeline is shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.4 – A basic data pipeline to generate data, extract attributes
    to json, and write to disk'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_16.4_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 16.4 – A basic data pipeline to generate data, extract attributes to
    json, and write to disk
  prefs: []
  type: TYPE_NORMAL
- en: The preceding data pipeline uses the `GenerateFlowFile` processor to create
    unique flowfiles. This is passed downstream to the `AttributesToJSON` processor,
    which extracts the attributes and writes to the flowfile content. Lastly, the
    file is written to disk at `/home/paulcrickard/output`.
  prefs: []
  type: TYPE_NORMAL
- en: Before running the data pipeline, you will need to make sure that you have the
    output directory for the `PutFile` processor on each node. Earlier, I said that
    data pipelines are no different when distributed, but there are some things you
    must keep in mind, one being that `PutFile` will write to disk on every node by
    default. You will need to configure your processors to be able to run on any node.
    We will fix this later in this section.
  prefs: []
  type: TYPE_NORMAL
- en: One more thing before you run the data pipeline. Open the browser to your other
    node. You will see the exact same data pipeline in that node. Even the layout
    of the processors is the same. Changes to any node will be distributed to all
    the other nodes. You can work from any node.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run the data pipeline, you will see files written to the output directory
    of both nodes. The data pipeline is running and distributing the load across the
    nodes. The following screenshot shows the output of the data pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.5 – Data pipeline writing flowfiles to a node'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_16.5_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 16.5 – Data pipeline writing flowfiles to a node
  prefs: []
  type: TYPE_NORMAL
- en: If you are getting the same results as the preceding screenshot, congratulations,
    you have just built a distributed data pipeline. Next, you will learn some more
    features of the NiFi cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Managing the distributed data pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The preceding data pipeline runs on each node. To compensate for that, you had
    to create the same path on both nodes for the `PutFile` processor to work. Earlier,
    you learned that there are several processors that can result in race conditions
    – trying to read the same file at the same time – which will cause problems. To
    resolve these issues, you can specify that a processor should only run on the
    Primary Node – as an isolated process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the configuration for the `PutFile` processor, select the **Scheduling**
    tab. In the dropdown for **Scheduling Strategy**, choose **On primary node**,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.6 – Running a processor on the Primary Node only'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_16.6_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 16.6 – Running a processor on the Primary Node only
  prefs: []
  type: TYPE_NORMAL
- en: Now, when you run the data pipeline, the files will only be placed on the Primary
    Node. You can schedule processors such as `GetFile` or `ExecuteSQL` to do the
    same thing.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the load of the data pipeline on each node, you can look at the cluster
    details from the waffle menu. As data moves through the data pipeline, you can
    see how many flowfiles are sitting in the queues of each node. The following screenshot
    shows the pipeline running on my cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.7 – Viewing the queues of each node. Each node has four flowfiles'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_16.7_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 16.7 – Viewing the queues of each node. Each node has four flowfiles
  prefs: []
  type: TYPE_NORMAL
- en: The data pipeline is distributing the flowfiles evenly across the nodes. In
    **Zero-Master Clustering**, the data is not copied or replicated. It exists only
    on the node that is processing it. If a node goes down, the data needs to be redistributed.
    This can only happen if the node is still connected to the network, otherwise,
    it will not happen until the node rejoins.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can manually disconnect a node by clicking the power icon on the right
    of the node''s row. The following screenshot shows a node being disconnected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.8 – nifi-node-1 has been disconnected from the cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_16.9_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 16.8 – nifi-node-1 has been disconnected from the cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding screenshot, you can see that `nifi-node-1` has a status of
    **DISCONNECTED**. But you should also notice that it has eight flowfiles that
    need to be redistributed. Since you disconnected the node, but did not drop it
    from the network, NiFi will redistribute the flowfiles. You can see the results
    when the screen is refreshed, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.9 – Redistributed flowfiles from a disconnected node'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_16.8_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 16.9 – Redistributed flowfiles from a disconnected node
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also reconnect any disconnected nodes. You do this by clicking the
    plug icon. When you do, the node will rejoin the cluster and the flowfiles will
    be redistributed. The following screenshot shows the node rejoined to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.10 – Reconnecting a node and flowfile redistribution](img/Figure_16.10_B15739.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.10 – Reconnecting a node and flowfile redistribution
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, the flowfiles have accumulated since the node was
    disconnected evenly across the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this Appendix, you learned the basics of NiFi clustering, as well as how
    to build a cluster with the embedded Zookeeper and how to build distributed data
    pipelines. NiFi handles most of the distribution of data; you only need to keep
    in mind the gotchas – such as race conditions and the fact that processors need
    to be configured to run on any node. Using a NiFi cluster allows you to manage
    NiFi on several machines from a single instance. It also allows you to process
    large amounts of data and have some redundancy in case an instance crashes.
  prefs: []
  type: TYPE_NORMAL
