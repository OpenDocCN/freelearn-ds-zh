- en: '*Chapter 10*: Deploying Data Pipelines'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In software engineering, you will usually have **development**, **testing**,
    and **production** environments. The testing environment may be called **quality
    control** or **staging** or some other name, but the idea is the same. You develop
    in an environment, then push it to another environment that will be a clone of
    the production environment and if everything goes well, then it is pushed into
    the production environment. The same methodology is used in data engineering.
    So far, you have built data pipelines and run them on a single machine. In this
    chapter, you will learn methods for building data pipelines that can be deployed
    to a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Finalizing your data pipelines for production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the NiFi variable registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying your data pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finalizing your data pipelines for production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last few chapters, you have learned about the features and methods for
    creating production data pipelines. There are still a few more features needed
    before you can deploy your data pipelines—backpressure, processor groups with
    input and output ports, and funnels. This section will walk you through each one
    of these features.
  prefs: []
  type: TYPE_NORMAL
- en: Backpressure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In your data pipelines, each processor or task will take different amounts of
    time to finish. For example, a database query may return hundreds of thousands
    of results that are split into single flowfiles in a few seconds, but the processor
    that evaluates and modifies the attributes within the flowfiles may take much
    longer. It doesn't make sense to dump all of the data into the queue faster than
    the downstream processor can actually process it. Apache NiFi allows you to control
    the number of flowfiles or the size of the data that is sent to the queue. This
    is called **backpressure**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how backpressure works, let''s make a data pipeline that generates
    data and writes it to a file. The data pipeline is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – A data pipeline to generate data and write the flowfiles to
    a file](img/Figure_10.1_B15739.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – A data pipeline to generate data and write the flowfiles to a
    file
  prefs: []
  type: TYPE_NORMAL
- en: The preceding data pipeline a creates connection between the `GenerateFlowFile`
    processor and the `PutFile` processor for the success relationship. I have configured
    the `PutFile` processor to write files to `/home/paulcrickard/output`. The `GenerateFlowFile`
    processor is using the default configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run the data pipeline by starting the `GenerateFlowFile` processor only,
    you will see that the queue has 10,000 flowfiles and is red, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – A full queue with 10,000 flowfiles'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.2_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.2 – A full queue with 10,000 flowfiles
  prefs: []
  type: TYPE_NORMAL
- en: If you refresh NiFi, the number of flowfiles in the queue will not increase.
    It has 10,000 flowfiles and cannot hold anymore. But is 10,000 the maximum number?
  prefs: []
  type: TYPE_NORMAL
- en: 'Queues can be configured just like the processors that feed them. Right-click
    on the queue and select **Configure**. Select the **SETTINGS** tab, and you will
    see the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Queue configuration settings'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.3_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 – Queue configuration settings
  prefs: []
  type: TYPE_NORMAL
- en: 'You will notice that `10000` flowfiles and that `1 GB`. The `GenerateFlowFile`
    processor set the size of each flowfile to 0 bytes, so the object threshold was
    hit before the size threshold. You can test hitting the size threshold by changing
    the `GenerateFlowFile` processor. I have changed it to 50 MB. When I start the
    processor, the queue now stops at 21 flowfiles because it has exceeded 1 GB of
    data. The following screenshot shows the full queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Queue that has the size threshold'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.4_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.4 – Queue that has the size threshold
  prefs: []
  type: TYPE_NORMAL
- en: By adjusting **Object Threshold** or **Size Threshold**, you can control the
    amount of data that gets sent to a queue and create backpressure slowing down
    an upstream processor. While loading the queues does not break your data pipeline,
    it will run much more smoothly if the data flows in a more even manner.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will zoom out on your data pipelines and show other techniques
    for improving the use of processor groups.
  prefs: []
  type: TYPE_NORMAL
- en: Improving processor groups
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Up to this point, you have used processor groups to hold a single data pipeline.
    If you were to push all of these data pipelines to production, what you would
    soon realize is that you have a lot of processors in each processor group doing
    the same exact task. For example, you may have several processors that SplitJson
    used followed by an `EvaluateJsonPath` processor that extracts the ID from a flowfile.
    Or, you might have several processors that insert flowfiles in to Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: You would not have several functions in code that do the exact same thing on
    different variables; you would have one that accepted parameters. The same holds
    true for data pipelines, and you accomplish this using processor groups with the
    input and output ports.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate how to break data pipelines into logical pieces, let''s walk
    through an example:'
  prefs: []
  type: TYPE_NORMAL
- en: In NiFi, create a processor group and name it `Generate Data`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inside the processor group, drag the `GenerateFlowFile` processor to the canvas.
    I have set the `{"ID":123}`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, drag an output port to the canvas. You will be prompted for `FromGeneratedData`
    and **Send To** is set to **Local connections**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, connect the `GenerateFlowfile` processor to **Output Port**. You will
    have a warning on the output port that it is invalid because it has no outgoing
    connections. We will fix that in the next steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exit the processor group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new processor group and name it `Write Data`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the processor group and drag the `EvaluateJsonPath` processor to the canvas.
    Configure it by creating a property ID with the value of `$.{ID}`, and set the
    **Destination** property to **flowfile-attribute**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, drag the `UpdateAttribute` processor to the canvas and create a new property
    filename and set the value to `${ID}`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, drag the `PutFile` processor to the canvas. Set the `/home/paulcrickard/output`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, drag an **Input Port** to the canvas and make it the first processor
    in the data pipeline. The completed pipeline should look like the following screenshot:![Figure
    10.5 – A data pipeline that starts with an input port
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_10.5_B15739.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.5 – A data pipeline that starts with an input port
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Exit the processor group. You should now have two processor groups on the canvas—`Generate
    Data` and `Write Data`. You can connect these processor groups just like you do
    with single processors. When you connect them by dragging the arrow from `Generate
    Data` to `Write Data`, you will be prompted to select which ports to connect,
    as shown in the following screenshot:![Figure 10.6 – Connecting two processor
    groups
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_10.6_B15739.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.6 – Connecting two processor groups
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The default values will work because you only have one output port and one input
    port. If you had more, you could use the drop-down menus to select the proper
    ports. This is where naming them something besides input and output becomes important.
    Make the names descriptive.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the processor groups connected, start the `Generate Data` group only. You
    will see the queue fill up with flowfiles. To see how the ports work, enter the
    `Write Data` processor group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start only the incoming data input port. Once it starts running, the downstream
    queue will fill with flowfiles.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Right-click the queue and select `Generate Data` processor group. You can now
    start the rest of the processor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As the data pipeline runs, you will have a file, `123`, created in your output
    directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You have successfully connected two processor groups using input and output
    ports. In production, you can now have a single process group to write data to
    a file and it can receive data from any processor group that needs to write data,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Two processor groups utilizing the Write Data processor group'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.7_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.7 – Two processor groups utilizing the Write Data processor group
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding data pipeline, I made a copy of `Generate Data` and configured
    the `{"ID":456}` and set the run schedule to an hour so that I would only get
    one flowfile from each processor—`Generate Data` and `Generate Data2`. Running
    all of the processor groups, you list the queue and confirm that one flowfile
    comes from each processor group, and your output directory now has two files—`123`
    and `456`.
  prefs: []
  type: TYPE_NORMAL
- en: Using the NiFi variable registry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you are building your data pipelines, you are hardcoding variables—with
    the exception of some expression language where you extract data from the flowfile.
    When you move the data pipeline to production, you will need to change the variables
    in your data pipeline, and this can be time consuming and error prone. For example,
    you will have a different test database than production. When you deploy your
    data pipeline to production, you need to point to production and change the processor.
    Or you can use the variable registry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `postgresToelasticsearch` processor group from [*Chapter 4*](B15739_04_ePub_AM.xhtml#_idTextAnchor049)*,
    Working with Databases*, I will modify the data pipeline to use the NiFi variable
    registry. As a reminder, the data pipeline is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – A data pipeline to query PostgreSQL and save the results to
    Elasticsearch'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.8_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.8 – A data pipeline to query PostgreSQL and save the results to Elasticsearch
  prefs: []
  type: TYPE_NORMAL
- en: From outside the processor group, right-click on it and select **Variables**.
    To add a new variable, you can click the plus sign and provide a name and a value.
    These variables are now associated with the processor group.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like functions in programming, variables have a scope. Variables in a
    processor group are local variables. You can right-click on the NiFi canvas and
    create a variable, which you can consider global in scope. I have created two
    local variables, `elastic` and `index`, and one global, `elastic`. When I open
    the variables in the group, it looks like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – NiFi variable registry'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.9_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.9 – NiFi variable registry
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, you can see the scopes. The scope of `elastic`,
    the local variable takes precedence.
  prefs: []
  type: TYPE_NORMAL
- en: You can now reference these variables using the expression language. In the
    `PutElasticsearchHttp` process, I have set the `${elastic}` and the `${index}`.
    These will populate with the local variables—`http://localhost:9200` and `nifivariable`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the data pipeline, you can see the results in Elasticsearch. There
    is now a new index with the name `nifivariable` and 1,001 records. The following
    screenshot shows the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – The new index, nifivariable, is the second row'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.10_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.10 – The new index, nifivariable, is the second row
  prefs: []
  type: TYPE_NORMAL
- en: You have now put the finishing touches on production pipelines and have completed
    all the steps needed to deploy them. The next section will teach you different
    ways to deploy your data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying your data pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many ways to handle the different environments—**development**, **testing**,
    **production**—and how you choose to do that is up to what works best with your
    business practices. Having said that, any strategy you take should involve using
    the NiFi registry.
  prefs: []
  type: TYPE_NORMAL
- en: Using the simplest strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The simplest strategy would be to run NiFi over the network and split the canvas
    into multiple environments. When you have promoted a process group, you would
    move it in to the next environment. When you needed to rebuild a data pipeline,
    you would add it back to development and modify it, then update the production
    data pipeline to the newest version. Your NiFi instance would look like the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11 – A single NiFi instance working as DEV, TEST, and PROD'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.11_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.11 – A single NiFi instance working as DEV, TEST, and PROD
  prefs: []
  type: TYPE_NORMAL
- en: Notice in the preceding screenshot that only `PROD` has a green checkmark. The
    `DEV` environment created the processor group, then changes were committed, and
    they were brought into `TEST`. If any changes were made, they were committed,
    and the newest version was brought in to `PROD`. To improve the data pipeline
    later, you would bring the newest version into `DEV` and start the process over
    until `PROD` has the newest version as well.
  prefs: []
  type: TYPE_NORMAL
- en: While this will work, if you have the resources to build out a separate NiFi
    instance, you should.
  prefs: []
  type: TYPE_NORMAL
- en: Using the middle strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The middle strategy utilizes the NiFi registry but also adds a production NiFi
    instance. I have installed NiFi on another machine, separate from the one I have
    used through this book, that is also running the NiFi registry—this could also
    live on a separate machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'After launching my new NiFi instance, I added the NiFi registry as shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.12 – Adding the NiFi registry to another NiFi instance'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.12_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.12 – Adding the NiFi registry to another NiFi instance
  prefs: []
  type: TYPE_NORMAL
- en: On the development machine, the registry was created using localhost. However,
    other machines can connect by specifying the IP address of the host machine. After
    reading it, the NiFi instance has access to all the versioned data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Drag a processor group to the canvas and select **Import**. You can now select
    the processor group that has been promoted to production, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.13 – Importing the processor group'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.13_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.13 – Importing the processor group
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you import the processor, it will come over with the variables that were
    defined in the development environment. You can overwrite the values of the variables.
    Once you change the variables, you will not need to do it again. You can make
    the changes in the development environment and update the production environment
    and the new variables will stay. The updated variables are shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.14 – Updating local variables for production'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.14_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.14 – Updating local variables for production
  prefs: []
  type: TYPE_NORMAL
- en: 'In the development environment, you can change the processor and commit the
    local changes. The production environment will now show that there is a new version
    available, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.15 – Production is now no longer using the current version'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.15_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.15 – Production is now no longer using the current version
  prefs: []
  type: TYPE_NORMAL
- en: 'You can right-click the processor group and select the new version. The following
    screenshot shows version 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.16 – A new version'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.16_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.16 – A new version
  prefs: []
  type: TYPE_NORMAL
- en: 'After selecting the new version, the production environment is now up to date.
    The following screenshot shows the production environment. You can right-click
    on the processor group to see that the variable still points to the production
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.17 – Production is up to date'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.17_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.17 – Production is up to date
  prefs: []
  type: TYPE_NORMAL
- en: This strategy should work for most users' needs. In this example, I used development
    and production environments, but you can add `TEST` and use the same strategy
    here, just change the local variables to point to your test databases.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding strategies used a single NiFi registry, but you can use a registry
    per environment.
  prefs: []
  type: TYPE_NORMAL
- en: Using multiple registries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A more advanced strategy for managing development, test, and production would
    be to use multiple NiFi registries. In this strategy, you would set up two NiFi
    registries—one for development and one for test and production. You would connect
    the development environment to the development registry and the test and production
    environments to the second registry.
  prefs: []
  type: TYPE_NORMAL
- en: When you have promoted a data pipeline to test, an administrator would use the
    NiFi CLI tools to export the data pipeline and import it in to the second NiFi
    registry. From there, you could test and promote it to development. You would
    import the version from the second registry to the production environment, just
    like you did in the middle strategy. This strategy makes mistakes much more difficult
    to handle as you cannot commit data pipelines to test and production without manually
    doing so. This is an excellent strategy but requires many more resources.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to finalize your data pipelines for deployment
    into production. By using processor groups for specific tasks, much like functions
    in code, you could reduce the duplication of processors. Using input and output
    ports, you connected multiple processor groups together. To deploy data pipelines,
    you learned how NiFi variables could be used to declare global and locally scoped
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will use all the skills you have learned in this section
    to create and deploy a production data pipeline.
  prefs: []
  type: TYPE_NORMAL
