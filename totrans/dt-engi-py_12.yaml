- en: '*Chapter 10*: Deploying Data Pipelines'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第10章*：部署数据管道'
- en: In software engineering, you will usually have **development**, **testing**,
    and **production** environments. The testing environment may be called **quality
    control** or **staging** or some other name, but the idea is the same. You develop
    in an environment, then push it to another environment that will be a clone of
    the production environment and if everything goes well, then it is pushed into
    the production environment. The same methodology is used in data engineering.
    So far, you have built data pipelines and run them on a single machine. In this
    chapter, you will learn methods for building data pipelines that can be deployed
    to a production environment.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件工程中，你通常会有**开发**、**测试**和**生产**环境。测试环境可能被称为**质量控制**、**预发布**或某些其他名称，但理念是相同的。你在某个环境中开发，然后将其推送到另一个环境，该环境将是生产环境的克隆，如果一切顺利，它将被推送到生产环境。在数据工程中，也使用相同的方法。到目前为止，你已经构建了数据管道并在单个机器上运行它们。在本章中，你将学习构建可以部署到生产环境的数据管道的方法。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Finalizing your data pipelines for production
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为生产环境最终确定你的数据管道
- en: Using the NiFi variable registry
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用NiFi变量注册表
- en: Deploying your data pipelines
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署你的数据管道
- en: Finalizing your data pipelines for production
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为生产环境最终确定你的数据管道
- en: In the last few chapters, you have learned about the features and methods for
    creating production data pipelines. There are still a few more features needed
    before you can deploy your data pipelines—backpressure, processor groups with
    input and output ports, and funnels. This section will walk you through each one
    of these features.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后几章中，你已经学习了创建生产数据管道的功能和方法。在你能够部署你的数据管道之前，还需要一些其他功能——背压、具有输入和输出端口的处理器组，以及漏斗。本节将带你了解这些功能的每一个。
- en: Backpressure
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 背压
- en: In your data pipelines, each processor or task will take different amounts of
    time to finish. For example, a database query may return hundreds of thousands
    of results that are split into single flowfiles in a few seconds, but the processor
    that evaluates and modifies the attributes within the flowfiles may take much
    longer. It doesn't make sense to dump all of the data into the queue faster than
    the downstream processor can actually process it. Apache NiFi allows you to control
    the number of flowfiles or the size of the data that is sent to the queue. This
    is called **backpressure**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的数据管道中，每个处理器或任务完成所需的时间不同。例如，一个数据库查询可能返回数十万个结果，这些结果在几秒钟内被分割成单个流文件，但评估和修改流文件内属性的处理器可能需要更长的时间。将所有数据以比下游处理器实际处理速度更快的速度放入队列中是没有意义的。Apache
    NiFi允许你控制发送到队列的流文件数量或数据大小。这被称为**背压**。
- en: 'To understand how backpressure works, let''s make a data pipeline that generates
    data and writes it to a file. The data pipeline is shown in the following screenshot:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解背压是如何工作的，让我们创建一个生成数据并将其写入文件的管道。数据管道如下截图所示：
- en: '![Figure 10.1 – A data pipeline to generate data and write the flowfiles to
    a file](img/Figure_10.1_B15739.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1 – 生成数据并将流文件写入文件的管道](img/Figure_10.1_B15739.jpg)'
- en: Figure 10.1 – A data pipeline to generate data and write the flowfiles to a
    file
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – 生成数据并将流文件写入文件的管道
- en: The preceding data pipeline a creates connection between the `GenerateFlowFile`
    processor and the `PutFile` processor for the success relationship. I have configured
    the `PutFile` processor to write files to `/home/paulcrickard/output`. The `GenerateFlowFile`
    processor is using the default configuration.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的数据管道a创建了`GenerateFlowFile`处理器和`PutFile`处理器之间的连接，用于成功关系。我已经配置了`PutFile`处理器将文件写入`/home/paulcrickard/output`。`GenerateFlowFile`处理器使用默认配置。
- en: 'If you run the data pipeline by starting the `GenerateFlowFile` processor only,
    you will see that the queue has 10,000 flowfiles and is red, as shown in the following
    screenshot:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只通过启动`GenerateFlowFile`处理器来运行数据管道，你会看到队列中有10,000个流文件，并且是红色的，如下面的截图所示：
- en: '![Figure 10.2 – A full queue with 10,000 flowfiles'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.2 – 填满10,000个流文件的完整队列'
- en: '](img/Figure_10.2_B15739.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.2_B15739.jpg)'
- en: Figure 10.2 – A full queue with 10,000 flowfiles
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 – 填满10,000个流文件的完整队列
- en: If you refresh NiFi, the number of flowfiles in the queue will not increase.
    It has 10,000 flowfiles and cannot hold anymore. But is 10,000 the maximum number?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你刷新NiFi，队列中流文件的数量不会增加。它有10,000个流文件，不能再容纳更多。但10,000是最大数量吗？
- en: 'Queues can be configured just like the processors that feed them. Right-click
    on the queue and select **Configure**. Select the **SETTINGS** tab, and you will
    see the following options:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 队列的配置方式与为其提供数据的处理器类似。右键点击队列并选择**配置**。选择**设置**选项卡，你将看到以下选项：
- en: '![Figure 10.3 – Queue configuration settings'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.3 – 队列配置设置'
- en: '](img/Figure_10.3_B15739.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.3_B15739.jpg)'
- en: Figure 10.3 – Queue configuration settings
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 – 队列配置设置
- en: 'You will notice that `10000` flowfiles and that `1 GB`. The `GenerateFlowFile`
    processor set the size of each flowfile to 0 bytes, so the object threshold was
    hit before the size threshold. You can test hitting the size threshold by changing
    the `GenerateFlowFile` processor. I have changed it to 50 MB. When I start the
    processor, the queue now stops at 21 flowfiles because it has exceeded 1 GB of
    data. The following screenshot shows the full queue:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到有`10000`个流文件和`1 GB`。`GenerateFlowFile`处理器将每个流文件的大小设置为0字节，因此对象阈值在大小阈值之前被触发。你可以通过更改`GenerateFlowFile`处理器来测试触发大小阈值。我已经将其更改为50
    MB。当我启动处理器时，队列现在停止在21个流文件，因为它已经超过了1 GB的数据。以下截图显示了完整的队列：
- en: '![Figure 10.4 – Queue that has the size threshold'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.4 – 具有大小阈值的队列'
- en: '](img/Figure_10.4_B15739.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.4_B15739.jpg)'
- en: Figure 10.4 – Queue that has the size threshold
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 – 具有大小阈值的队列
- en: By adjusting **Object Threshold** or **Size Threshold**, you can control the
    amount of data that gets sent to a queue and create backpressure slowing down
    an upstream processor. While loading the queues does not break your data pipeline,
    it will run much more smoothly if the data flows in a more even manner.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整**对象阈值**或**大小阈值**，你可以控制发送到队列的数据量，并创建反向压力以减缓上游处理器。虽然加载队列不会破坏你的数据管道，但如果数据流动更加均匀，它将运行得更加顺畅。
- en: The next section will zoom out on your data pipelines and show other techniques
    for improving the use of processor groups.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将放大你的数据管道，并展示其他改进处理器组使用的技术。
- en: Improving processor groups
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改进处理器组
- en: Up to this point, you have used processor groups to hold a single data pipeline.
    If you were to push all of these data pipelines to production, what you would
    soon realize is that you have a lot of processors in each processor group doing
    the same exact task. For example, you may have several processors that SplitJson
    used followed by an `EvaluateJsonPath` processor that extracts the ID from a flowfile.
    Or, you might have several processors that insert flowfiles in to Elasticsearch.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经使用了处理器组来保存单个数据管道。如果你要将所有这些数据管道推送到生产环境，你很快就会意识到每个处理器组中有很多处理器正在执行完全相同的任务。例如，你可能有多几个使用`SplitJson`的处理器后面跟着一个`EvaluateJsonPath`处理器，用于从流文件中提取ID。或者，你可能有多几个将流文件插入到Elasticsearch的处理器。
- en: You would not have several functions in code that do the exact same thing on
    different variables; you would have one that accepted parameters. The same holds
    true for data pipelines, and you accomplish this using processor groups with the
    input and output ports.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你不会在代码中有几个执行相同操作但针对不同变量的函数；你会有一个接受参数的函数。同样的规则适用于数据处理管道，你通过使用具有输入和输出端口的处理器组来实现这一点。
- en: 'To illustrate how to break data pipelines into logical pieces, let''s walk
    through an example:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明如何将数据处理管道分解成逻辑部分，让我们通过一个例子来演示：
- en: In NiFi, create a processor group and name it `Generate Data`.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在NiFi中创建一个处理器组，并将其命名为`Generate Data`。
- en: Inside the processor group, drag the `GenerateFlowFile` processor to the canvas.
    I have set the `{"ID":123}`.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在处理器组内部，将`GenerateFlowFile`处理器拖到画布上。我已经设置了`{"ID":123}`。
- en: Next, drag an output port to the canvas. You will be prompted for `FromGeneratedData`
    and **Send To** is set to **Local connections**.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将一个输出端口拖到画布上。你将被提示选择`FromGeneratedData`，并将**发送到**设置为**本地连接**。
- en: Lastly, connect the `GenerateFlowfile` processor to **Output Port**. You will
    have a warning on the output port that it is invalid because it has no outgoing
    connections. We will fix that in the next steps.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将`GenerateFlowfile`处理器连接到**输出端口**。你将在输出端口上收到一个警告，表明它无效，因为它没有出向连接。我们将在下一步中修复这个问题。
- en: Exit the processor group.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 退出处理器组。
- en: Create a new processor group and name it `Write Data`.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的处理器组，并将其命名为`Write Data`。
- en: Enter the processor group and drag the `EvaluateJsonPath` processor to the canvas.
    Configure it by creating a property ID with the value of `$.{ID}`, and set the
    **Destination** property to **flowfile-attribute**.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入处理器组，将`EvaluateJsonPath`处理器拖到画布上。通过创建一个值为`$.{ID}`的属性ID来配置它，并将**目标**属性设置为**flowfile-attribute**。
- en: Next, drag the `UpdateAttribute` processor to the canvas and create a new property
    filename and set the value to `${ID}`.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将“更新属性”处理器拖动到画布上，并创建一个新的属性文件名，将其值设置为`${ID}`。
- en: Now, drag the `PutFile` processor to the canvas. Set the `/home/paulcrickard/output`.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将“PutFile”处理器拖动到画布上。设置`/home/paulcrickard/output`。
- en: Lastly, drag an **Input Port** to the canvas and make it the first processor
    in the data pipeline. The completed pipeline should look like the following screenshot:![Figure
    10.5 – A data pipeline that starts with an input port
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将一个**输入端口**拖动到画布上，并使其成为数据管道中的第一个处理器。完成的管道应如下截图所示：![图10.5 – 以输入端口开始的数据管道
- en: '](img/Figure_10.5_B15739.jpg)'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_10.5_B15739.jpg)'
- en: Figure 10.5 – A data pipeline that starts with an input port
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图10.5 – 以输入端口开始的数据管道
- en: Exit the processor group. You should now have two processor groups on the canvas—`Generate
    Data` and `Write Data`. You can connect these processor groups just like you do
    with single processors. When you connect them by dragging the arrow from `Generate
    Data` to `Write Data`, you will be prompted to select which ports to connect,
    as shown in the following screenshot:![Figure 10.6 – Connecting two processor
    groups
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 退出处理器组。现在，您应该在画布上看到两个处理器组——“生成数据”和“写入数据”。您可以像连接单个处理器一样连接这些处理器组。当您通过从“生成数据”拖动箭头到“写入数据”来连接它们时，您将提示选择要连接的端口，如下面的截图所示：![图10.6
    – 连接两个处理器组
- en: '](img/Figure_10.6_B15739.jpg)'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_10.6_B15739.jpg)'
- en: Figure 10.6 – Connecting two processor groups
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图10.6 – 连接两个处理器组
- en: The default values will work because you only have one output port and one input
    port. If you had more, you could use the drop-down menus to select the proper
    ports. This is where naming them something besides input and output becomes important.
    Make the names descriptive.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认值将适用，因为您只有一个输出端口和一个输入端口。如果您有更多，可以使用下拉菜单选择正确的端口。这就是将它们命名为除输入和输出之外的其他名称变得重要的地方。使名称具有描述性。
- en: With the processor groups connected, start the `Generate Data` group only. You
    will see the queue fill up with flowfiles. To see how the ports work, enter the
    `Write Data` processor group.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接处理器组后，仅启动“生成数据”组。您将看到队列中充满了流文件。要了解端口的工作方式，进入“写入数据”处理器组。
- en: Start only the incoming data input port. Once it starts running, the downstream
    queue will fill with flowfiles.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅启动传入数据输入端口。一旦开始运行，下游队列将充满流文件。
- en: Right-click the queue and select `Generate Data` processor group. You can now
    start the rest of the processor.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 右键单击队列并选择“生成数据”处理器组。现在您可以启动其他处理器。
- en: As the data pipeline runs, you will have a file, `123`, created in your output
    directory.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随着数据管道的运行，您将在输出目录中创建一个名为`123`的文件。
- en: 'You have successfully connected two processor groups using input and output
    ports. In production, you can now have a single process group to write data to
    a file and it can receive data from any processor group that needs to write data,
    as shown in the following screenshot:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您已成功使用输入和输出端口连接了两个处理器组。在生产环境中，现在您可以有一个单独的处理器组将数据写入文件，并且它可以接收任何需要写入数据的处理器组的数据，如下面的截图所示：
- en: '![Figure 10.7 – Two processor groups utilizing the Write Data processor group'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.7 – 两个处理器组使用“写入数据”处理器组'
- en: '](img/Figure_10.7_B15739.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.7_B15739.jpg)'
- en: Figure 10.7 – Two processor groups utilizing the Write Data processor group
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7 – 两个处理器组使用“写入数据”处理器组
- en: In the preceding data pipeline, I made a copy of `Generate Data` and configured
    the `{"ID":456}` and set the run schedule to an hour so that I would only get
    one flowfile from each processor—`Generate Data` and `Generate Data2`. Running
    all of the processor groups, you list the queue and confirm that one flowfile
    comes from each processor group, and your output directory now has two files—`123`
    and `456`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的数据管道中，我复制了“生成数据”，并配置了`{"ID":456}`，并将运行计划设置为每小时一次，这样我就可以从每个处理器——`生成数据`和`生成数据2`——只获取一个流文件。运行所有处理器组，您会列出队列并确认每个处理器组都来自一个流文件，并且您的输出目录现在有两个文件——`123`和`456`。
- en: Using the NiFi variable registry
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NiFi变量注册表
- en: When you are building your data pipelines, you are hardcoding variables—with
    the exception of some expression language where you extract data from the flowfile.
    When you move the data pipeline to production, you will need to change the variables
    in your data pipeline, and this can be time consuming and error prone. For example,
    you will have a different test database than production. When you deploy your
    data pipeline to production, you need to point to production and change the processor.
    Or you can use the variable registry.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当您构建数据管道时，您正在硬编码变量——除了某些表达式语言，您可以从flowfile中提取数据。当您将数据管道移动到生产环境时，您需要更改数据管道中的变量，这可能既耗时又容易出错。例如，您将有一个与生产不同的测试数据库。当您将数据管道部署到生产环境时，您需要指向生产并更改处理器。或者，您可以使用变量注册表。
- en: 'Using the `postgresToelasticsearch` processor group from [*Chapter 4*](B15739_04_ePub_AM.xhtml#_idTextAnchor049)*,
    Working with Databases*, I will modify the data pipeline to use the NiFi variable
    registry. As a reminder, the data pipeline is shown in the following screenshot:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用来自[*第4章*](B15739_04_ePub_AM.xhtml#_idTextAnchor049)*，*与数据库一起工作*的`postgresToelasticsearch`处理器组，我将修改数据管道以使用NiFi变量注册表。作为提醒，数据管道如下所示：
- en: '![Figure 10.8 – A data pipeline to query PostgreSQL and save the results to
    Elasticsearch'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 10.8 – A data pipeline to query PostgreSQL and save the results to
    Elasticsearch'
- en: '](img/Figure_10.8_B15739.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_10.8_B15739.jpg]'
- en: Figure 10.8 – A data pipeline to query PostgreSQL and save the results to Elasticsearch
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8 – 一个查询PostgreSQL并将结果保存到Elasticsearch的数据管道
- en: From outside the processor group, right-click on it and select **Variables**.
    To add a new variable, you can click the plus sign and provide a name and a value.
    These variables are now associated with the processor group.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从处理器组外部，右键单击它并选择**变量**。要添加新变量，您可以点击加号并提供名称和值。这些变量现在与处理器组相关联。
- en: 'Just like functions in programming, variables have a scope. Variables in a
    processor group are local variables. You can right-click on the NiFi canvas and
    create a variable, which you can consider global in scope. I have created two
    local variables, `elastic` and `index`, and one global, `elastic`. When I open
    the variables in the group, it looks like the following screenshot:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 就像编程中的函数一样，变量有作用域。处理器组中的变量是局部变量。您可以在NiFi画布上右键单击并创建一个变量，您可以考虑它在作用域上是全局的。我创建了两个局部变量`elastic`和`index`，以及一个全局变量`elastic`。当我打开组中的变量时，它看起来如下所示：
- en: '![Figure 10.9 – NiFi variable registry'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 10.9 – NiFi variable registry'
- en: '](img/Figure_10.9_B15739.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_10.9_B15739.jpg]'
- en: Figure 10.9 – NiFi variable registry
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9 – NiFi变量注册表
- en: In the preceding screenshot, you can see the scopes. The scope of `elastic`,
    the local variable takes precedence.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图中，您可以看到作用域。`elastic`的作用域，局部变量具有优先级。
- en: You can now reference these variables using the expression language. In the
    `PutElasticsearchHttp` process, I have set the `${elastic}` and the `${index}`.
    These will populate with the local variables—`http://localhost:9200` and `nifivariable`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以使用表达式语言引用这些变量。在`PutElasticsearchHttp`过程中，我设置了`${elastic}`和`${index}`。这些将填充为局部变量——`http://localhost:9200`和`nifivariable`。
- en: 'Running the data pipeline, you can see the results in Elasticsearch. There
    is now a new index with the name `nifivariable` and 1,001 records. The following
    screenshot shows the result:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 运行数据管道，您可以在Elasticsearch中看到结果。现在有一个名为`nifivariable`的新索引和1,001条记录。以下屏幕截图显示了结果：
- en: '![Figure 10.10 – The new index, nifivariable, is the second row'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 10.10 – The new index, nifivariable, is the second row'
- en: '](img/Figure_10.10_B15739.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_10.10_B15739.jpg]'
- en: Figure 10.10 – The new index, nifivariable, is the second row
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10 – 新索引，nifivariable，是第二行
- en: You have now put the finishing touches on production pipelines and have completed
    all the steps needed to deploy them. The next section will teach you different
    ways to deploy your data pipelines.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经完成了生产管道的收尾工作，并完成了部署所需的全部步骤。下一节将向您介绍不同的部署数据管道的方法。
- en: Deploying your data pipelines
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署您的数据管道
- en: There are many ways to handle the different environments—**development**, **testing**,
    **production**—and how you choose to do that is up to what works best with your
    business practices. Having said that, any strategy you take should involve using
    the NiFi registry.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 处理不同环境——**开发**、**测试**、**生产**——有许多方法，您如何选择取决于最适合您的业务实践。话虽如此，您采取的任何策略都应该涉及使用NiFi注册表。
- en: Using the simplest strategy
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用最简单的策略
- en: 'The simplest strategy would be to run NiFi over the network and split the canvas
    into multiple environments. When you have promoted a process group, you would
    move it in to the next environment. When you needed to rebuild a data pipeline,
    you would add it back to development and modify it, then update the production
    data pipeline to the newest version. Your NiFi instance would look like the following
    screenshot:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的策略是在网络上运行NiFi并将画布分割成多个环境。当你提升了一个流程组时，你会将其移动到下一个环境。当你需要重建数据管道时，你会将其添加回开发环境并修改它，然后更新生产数据管道到最新版本。你的NiFi实例将如下截图所示：
- en: '![Figure 10.11 – A single NiFi instance working as DEV, TEST, and PROD'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.11 – 单个NiFi实例作为DEV、TEST和PROD运行'
- en: '](img/Figure_10.11_B15739.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_10.11_B15739.jpg)'
- en: Figure 10.11 – A single NiFi instance working as DEV, TEST, and PROD
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11 – 单个NiFi实例作为DEV、TEST和PROD运行
- en: Notice in the preceding screenshot that only `PROD` has a green checkmark. The
    `DEV` environment created the processor group, then changes were committed, and
    they were brought into `TEST`. If any changes were made, they were committed,
    and the newest version was brought in to `PROD`. To improve the data pipeline
    later, you would bring the newest version into `DEV` and start the process over
    until `PROD` has the newest version as well.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在前面的截图中，只有`PROD`有一个绿色的勾选标记。`DEV`环境创建了处理器组，然后提交了更改，并将它们带入`TEST`。如果进行了任何更改，它们将被提交，并将最新版本带入`PROD`。为了以后改进数据管道，你会将最新版本带入`DEV`并重新开始，直到`PROD`也有最新版本。
- en: While this will work, if you have the resources to build out a separate NiFi
    instance, you should.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这样做也可以，但如果你有资源构建一个独立的NiFi实例，你应该这样做。
- en: Using the middle strategy
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用中间策略
- en: The middle strategy utilizes the NiFi registry but also adds a production NiFi
    instance. I have installed NiFi on another machine, separate from the one I have
    used through this book, that is also running the NiFi registry—this could also
    live on a separate machine.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 中间策略利用NiFi注册表，但还添加了一个生产NiFi实例。我在另一台机器上安装了NiFi，这台机器与本书中使用的机器不同，也在运行NiFi注册表——这也可以位于另一台机器上。
- en: 'After launching my new NiFi instance, I added the NiFi registry as shown in
    the following screenshot:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动我的新NiFi实例后，我添加了NiFi注册表，如下截图所示：
- en: '![Figure 10.12 – Adding the NiFi registry to another NiFi instance'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.12 – 将NiFi注册表添加到另一个NiFi实例'
- en: '](img/Figure_10.12_B15739.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_10.12_B15739.jpg)'
- en: Figure 10.12 – Adding the NiFi registry to another NiFi instance
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12 – 将NiFi注册表添加到另一个NiFi实例
- en: On the development machine, the registry was created using localhost. However,
    other machines can connect by specifying the IP address of the host machine. After
    reading it, the NiFi instance has access to all the versioned data pipelines.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发机器上，注册表使用localhost创建。然而，其他机器可以通过指定主机机的IP地址进行连接。读取后，NiFi实例可以访问所有版本化的数据管道。
- en: 'Drag a processor group to the canvas and select **Import**. You can now select
    the processor group that has been promoted to production, as shown in the following
    screenshot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 将处理器组拖到画布上并选择**导入**。你现在可以选中已提升到生产的处理器组，如下截图所示：
- en: '![Figure 10.13 – Importing the processor group'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.13 – 导入处理器组'
- en: '](img/Figure_10.13_B15739.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_10.13_B15739.jpg)'
- en: Figure 10.13 – Importing the processor group
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.13 – 导入处理器组
- en: 'Once you import the processor, it will come over with the variables that were
    defined in the development environment. You can overwrite the values of the variables.
    Once you change the variables, you will not need to do it again. You can make
    the changes in the development environment and update the production environment
    and the new variables will stay. The updated variables are shown in the following
    screenshot:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你导入了处理器，它将带有在开发环境中定义的变量。你可以覆盖变量的值。一旦你更改了变量，你就不需要再次这样做。你可以在开发环境中进行更改并更新生产环境，新变量将保留。更新的变量如下截图所示：
- en: '![Figure 10.14 – Updating local variables for production'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.14 – 更新用于生产的本地变量'
- en: '](img/Figure_10.14_B15739.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_10.14_B15739.jpg)'
- en: Figure 10.14 – Updating local variables for production
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.14 – 更新用于生产的本地变量
- en: 'In the development environment, you can change the processor and commit the
    local changes. The production environment will now show that there is a new version
    available, as shown in the following screenshot:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发环境中，你可以更改处理器并提交本地更改。现在，生产环境将显示有新版本可用，如下截图所示：
- en: '![Figure 10.15 – Production is now no longer using the current version'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.15 – 生产不再使用当前版本'
- en: '](img/Figure_10.15_B15739.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.15_B15739.jpg)'
- en: Figure 10.15 – Production is now no longer using the current version
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.15 – 生产不再使用当前版本
- en: 'You can right-click the processor group and select the new version. The following
    screenshot shows version 2:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以右键单击处理器组并选择新版本。以下截图显示了版本 2：
- en: '![Figure 10.16 – A new version'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.16 – 新版本'
- en: '](img/Figure_10.16_B15739.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.16_B15739.jpg)'
- en: Figure 10.16 – A new version
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.16 – 新版本
- en: 'After selecting the new version, the production environment is now up to date.
    The following screenshot shows the production environment. You can right-click
    on the processor group to see that the variable still points to the production
    values:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 选择新版本后，生产环境现在是最新的。以下截图显示了生产环境。你可以右键单击处理器组以查看变量仍然指向生产值：
- en: '![Figure 10.17 – Production is up to date'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.17 – 生产状态是最新的'
- en: '](img/Figure_10.17_B15739.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_10.17_B15739.jpg)'
- en: Figure 10.17 – Production is up to date
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.17 – 生产状态是最新的
- en: This strategy should work for most users' needs. In this example, I used development
    and production environments, but you can add `TEST` and use the same strategy
    here, just change the local variables to point to your test databases.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略应该适用于大多数用户的需求。在这个例子中，我使用了开发和生产环境，但你也可以添加 `TEST` 并在这里使用相同的策略，只需更改本地变量以指向你的测试数据库。
- en: The preceding strategies used a single NiFi registry, but you can use a registry
    per environment.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 之前使用的策略使用了单个 NiFi 注册表，但你可以为每个环境使用一个注册表。
- en: Using multiple registries
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用多个注册表
- en: A more advanced strategy for managing development, test, and production would
    be to use multiple NiFi registries. In this strategy, you would set up two NiFi
    registries—one for development and one for test and production. You would connect
    the development environment to the development registry and the test and production
    environments to the second registry.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 管理开发、测试和生产的一个更高级策略是使用多个 NiFi 注册表。在这个策略中，你会设置两个 NiFi 注册表——一个用于开发，一个用于测试和生产。你会将开发环境连接到开发注册表，将测试和生产环境连接到第二个注册表。
- en: When you have promoted a data pipeline to test, an administrator would use the
    NiFi CLI tools to export the data pipeline and import it in to the second NiFi
    registry. From there, you could test and promote it to development. You would
    import the version from the second registry to the production environment, just
    like you did in the middle strategy. This strategy makes mistakes much more difficult
    to handle as you cannot commit data pipelines to test and production without manually
    doing so. This is an excellent strategy but requires many more resources.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将数据管道提升到测试阶段时，管理员会使用 NiFi CLI 工具导出数据管道并将其导入到第二个 NiFi 注册表中。从那里，你可以对其进行测试并将其提升到开发阶段。你会将第二个注册表中的版本导入到生产环境中，就像在中端策略中所做的那样。这种策略使得错误处理变得更加困难，因为你不能在没有手动操作的情况下将数据管道提交到测试和生产环境。这是一个非常好的策略，但需要更多的资源。
- en: Summary
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to finalize your data pipelines for deployment
    into production. By using processor groups for specific tasks, much like functions
    in code, you could reduce the duplication of processors. Using input and output
    ports, you connected multiple processor groups together. To deploy data pipelines,
    you learned how NiFi variables could be used to declare global and locally scoped
    variables.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何将数据管道最终化以部署到生产环境。通过使用特定任务的处理器组，就像代码中的函数一样，你可以减少处理器的重复。使用输入和输出端口，你将多个处理器组连接在一起。为了部署数据管道，你学习了如何使用
    NiFi 变量来声明全局和局部作用域的变量。
- en: In the next chapter, you will use all the skills you have learned in this section
    to create and deploy a production data pipeline.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将使用在本节中学到的所有技能来创建和部署生产数据管道。
