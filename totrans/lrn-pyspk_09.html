<html><head></head><body>
  <div><div><div><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Polyglot Persistence with Blaze</h1></div></div></div><p>Our world is complex and no single approach exists that solves all problems. Likewise, in the data world one cannot solve all problems with one piece of technology.</p><p>Nowadays, any big technology company uses (in one form or another) a MapReduce paradigm to sift through terabytes (or even petabytes) of data collected daily. On the other hand, it is much easier to store, retrieve, extend, and update information about products in a document-type database (such as MongoDB) than it is in a relational database. Yet, persisting transaction records in a relational database aids later data summarizing and reporting.</p><p>Even these simple examples show that solving a vast array of business problems requires adapting to different technologies. This means that you, as a database manager, data scientist, or data engineer, would have to learn all of these separately if you were to solve your problems with the tools that are designed to solve them easily. This, however, does not make your company agile and is prone to errors and lots of tweaking and hacking needing to be done to your system.</p><p>Blaze abstracts most of the technologies and exposes a simple and elegant data structure and API.</p><p>In this chapter, you will learn:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How to install Blaze</li><li class="listitem" style="list-style-type: disc">What polyglot persistence is about</li><li class="listitem" style="list-style-type: disc">How to abstract data stored in files, pandas DataFrames, or NumPy arrays</li><li class="listitem" style="list-style-type: disc">How to work with archives (GZip)</li><li class="listitem" style="list-style-type: disc">How to connect to SQL (PostgreSQL and SQLite) and No-SQL (MongoDB) databases with Blaze</li><li class="listitem" style="list-style-type: disc">How to query, join, sort, and transform the data, and perform simple summary statistics</li></ul></div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec61"/>Installing Blaze</h1></div></div></div><p>If you run<a id="id597" class="indexterm"/> Anaconda it is easy to install Blaze. Just issue the following command in your CLI (see the Bonus <a class="link" href="ch01.html" title="Chapter 1. Understanding Spark">Chapter 1</a>, <em>Installing Spark</em> if you do not know what a CLI is):</p><div><pre class="programlisting">
<strong>conda install blaze</strong>
</pre></div><p>Once the command is issued, you will see a screen similar to the following screenshot:</p><div><img src="img/B05793_09_01.jpg" alt="Installing Blaze"/></div><p>We will later use Blaze to connect to the PostgreSQL and MongoDB databases, so we need to install some additional packages that Blaze will use in the background.</p><p>We will install SQL Alchemy and PyMongo, both of which are part of Anaconda:</p><div><pre class="programlisting">
<strong>conda install sqlalchemy</strong>
<strong>conda install pymongo</strong>
</pre></div><p>All that is now left to do is to import Blaze itself in our notebook:</p><div><pre class="programlisting">import blaze as bl</pre></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec62"/>Polyglot persistence</h1></div></div></div><p>Neal Ford introduced the, somewhat similar, term polyglot programming in 2006. He used it to illustrate<a id="id598" class="indexterm"/> the fact that there is no such thing as a one-size-fits-all solution and advocated using multiple programming languages that were more suitable for certain problems.</p><p>In the parallel world of data, any business that wants to remain competitive needs to adapt a range of technologies that allows it to solve the problems in a minimal time, thus minimizing the costs.</p><p>Storing transactional data in Hadoop files is possible, but makes little sense. On the other hand, processing petabytes of Internet logs using a <strong>Relational Database Management System</strong> (<strong>RDBMS</strong>) would also be ill-advised. These tools were designed to tackle specific types of tasks; even though they can be co-opted to solve other problems, the cost of adapting the<a id="id599" class="indexterm"/> tools to do so would be enormous. It is a virtual equivalent of trying to fit a square peg in a round hole.</p><p>For example, consider a company that sells musical instruments and accessories online (and in a network of shops). At a high-level, there are a number of problems that a company needs<a id="id600" class="indexterm"/> to solve to be successful:</p><div><ol class="orderedlist arabic"><li class="listitem">Attract customers to its stores (both virtual and physical).</li><li class="listitem">Present them with relevant products (you would not try to sell a drum kit to a pianist, would you?!).</li><li class="listitem">Once they decide to buy, process the payment and organize shipping.</li></ol></div><p>To solve these problems a company might choose from a number of available technologies that were designed to solve these problems:</p><div><ol class="orderedlist arabic"><li class="listitem">Store all the products in a document-based database such as MongoDB, Cassandra, DynamoDB, or DocumentDB. There are multiple advantages of document databases: flexible schema, sharding (breaking bigger databases into a set of smaller, more manageable ones), high availability, and replication, among others.</li><li class="listitem">Model the recommendations using a graph-based database (such as Neo4j, Tinkerpop/Gremlin, or GraphFrames for Spark): such databases reflect the factual and abstract relationships between customers and their preferences. Mining such a graph is invaluable and can produce a more tailored offering for a customer.</li><li class="listitem">For searching, a company might use a search-tailored solution such as Apache Solr or ElasticSearch. Such a solution provides fast, indexed text searching capabilities.</li><li class="listitem">Once a product is sold, the transaction normally has a well-structured schema (such as product name, price, and so on.) To store such data (and later process and report on it) relational databases are best suited.</li></ol></div><p>With polyglot persistence, a company always chooses the right tool for the right job instead of trying to coerce a single technology into solving all of its problems.</p><p>Blaze, as we will see, abstracts these technologies and introduces a simple API to work with, so you do not have to learn the APIs of each and every technology you want to use. It is, in essence, a great<a id="id601" class="indexterm"/> working example of polyglot persistence.</p><div><div><h3 class="title"><a id="note56"/>Note</h3><p>To see<a id="id602" class="indexterm"/> how others do it, check out <a class="ulink" href="http://www.slideshare.net/Couchbase/couchbase-at-ebay-2014">http://www.slideshare.net/Couchbase/couchbase-at-ebay-2014</a>
</p><p>or</p><p>
<a class="ulink" href="https://www.slideshare.net/bijoor1/case-study-polyglot-persistence-in-pharmaceutical-industry">http://www.slideshare.net/bijoor1/case-study-polyglotpersistence-in-pharmaceutical-industry</a>.
</p></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec63"/>Abstracting data</h1></div></div></div><p>Blaze can<a id="id603" class="indexterm"/> abstract many different data structures and expose a single, easy-to-use API. This helps to get a consistent behavior and reduce the need to learn multiple interfaces to handle data. If you know pandas, there is not really that much to learn, as the differences in the syntax are subtle. We will go through some examples to illustrate this.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec95"/>Working with NumPy arrays</h2></div></div></div><p>Getting<a id="id604" class="indexterm"/> data from a NumPy array into the<a id="id605" class="indexterm"/> DataShape object of Blaze is extremely easy. First, let's create a simple NumPy array: we first load NumPy and then create a matrix with two rows and three columns:</p><div><pre class="programlisting">import numpy as np
simpleArray = np.array([
        [1,2,3],
        [4,5,6]
    ])</pre></div><p>Now that we have an array, we can abstract it with Blaze's DataShape structure:</p><div><pre class="programlisting">simpleData_np = bl.Data(simpleArray)</pre></div><p>That's it! Simple enough.</p><p>In order to peek inside the structure you can use the .<code class="literal">peek()</code> method:</p><div><pre class="programlisting">simpleData_np.peek()</pre></div><p>You should see an output similar to what is shown in the following screenshot:</p><div><img src="img/B05793_09_02.jpg" alt="Working with NumPy arrays"/></div><p>You can also use (familiar to those of you versed in pandas' syntax) the .<code class="literal">head(...)</code> method.</p><div><div><h3 class="title"><a id="note57"/>Note</h3><p>The difference between <code class="literal">.peek()</code> and <code class="literal">.head(...)</code> is that <code class="literal">.head(...)</code> allows the specification of the number of rows as its only parameter, whereas <code class="literal">.peek()</code> does not allow that and will always print the top 10 records.</p></div></div><p>If you want<a id="id606" class="indexterm"/> to retrieve the first column of<a id="id607" class="indexterm"/> your DataShape, you can use indexing:</p><div><pre class="programlisting">simpleData_np[0]</pre></div><p>You should see a table, as shown here:</p><div><img src="img/B05793_09_03.jpg" alt="Working with NumPy arrays"/></div><p>On the other hand, if you were interested in retrieving a row, all you would have to do (like in NumPy) is transpose your DataShape:</p><div><pre class="programlisting">simpleData_np.T[0]</pre></div><p>What you will then get is presented in the following figure:</p><div><img src="img/B05793_09_04.jpg" alt="Working with NumPy arrays"/></div><p>Notice that<a id="id608" class="indexterm"/> the name of the column is <code class="literal">None</code>. DataShapes, just like pandas' DataFrames, support named columns. Thus, let's specify the<a id="id609" class="indexterm"/> names of our fields:</p><div><pre class="programlisting">simpleData_np = bl.Data(simpleArray, fields=['a', 'b', 'c'])</pre></div><p>Now you can retrieve the data simply by calling the column by its name:</p><div><pre class="programlisting">simpleData_np['b']</pre></div><p>In return, you will get the following output:</p><div><img src="img/B05793_09_05.jpg" alt="Working with NumPy arrays"/></div><p>As you can see, defining the fields transposes the NumPy array and, now, each element of the array forms a <em>row</em>, unlike when we first created the <code class="literal">simpleData_np</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec96"/>Working with pandas' DataFrame</h2></div></div></div><p>Since pandas' DataFrame<code class="literal"> </code>internally uses NumPy data structures, translating a DataFrame to DataShape is effortless.</p><p>First, let's create<a id="id610" class="indexterm"/> a simple DataFrame. We start by<a id="id611" class="indexterm"/> importing pandas:</p><div><pre class="programlisting">import pandas as pd</pre></div><p>Next, we create a DataFrame:</p><div><pre class="programlisting">simpleDf = pd.DataFrame([
        [1,2,3],
        [4,5,6]
    ], columns=['a','b','c'])</pre></div><p>We then transform it into a DataShape:</p><div><pre class="programlisting">simpleData_df = bl.Data(simpleDf)</pre></div><p>You can retrieve<a id="id612" class="indexterm"/> data in the same manner as with the DataShape created from the NumPy array. Use the following command:</p><div><pre class="programlisting">simpleData_df['a']</pre></div><p>Then, it will<a id="id613" class="indexterm"/> produce the following output:</p><div><img src="img/B05793_09_06.jpg" alt="Working with pandas' DataFrame"/></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec97"/>Working with files</h2></div></div></div><p>A DataShape object can be created directly from a <code class="literal">.csv</code> file. In this example, we will use a dataset<a id="id614" class="indexterm"/> that consists of 404,536 traffic violations that happened in the Montgomery county of Maryland.</p><div><div><h3 class="title"><a id="note58"/>Note</h3><p>We downloaded<a id="id615" class="indexterm"/> the data from <a class="ulink" href="https://catalog.data.gov/dataset/traffic-violations-56dda">https://catalog.data.gov/dataset/traffic-violations-56dda</a> on 8/23/16; the dataset is updated daily, so the number of traffic violations might differ if you retrieve the dataset at a later date.</p></div></div><p>We store the dataset in the <code class="literal">../Data</code> folder locally. However, we modified the dataset slightly so we could store it in the MongoDB: in its original form, with date columns, reading data back from MongoDB caused errors. We filed a bug with Blaze to fix this issue <a class="ulink" href="https://github.com/blaze/blaze/issues/1580">https://github.com/blaze/blaze/issues/1580</a>:</p><div><pre class="programlisting">import odo
traffic = bl.Data('../Data/TrafficViolations.csv')</pre></div><p>If you do not know the names of the columns in any dataset, you can get these from the DataShape. To get a list of all the fields, you can use the following command:</p><div><pre class="programlisting">print(traffic.fields)</pre></div><div><img src="img/B05793_09_07.jpg" alt="Working with files"/></div><div><div><h3 class="title"><a id="tip35"/>Tip</h3><p>Those of you familiar with pandas would easily recognize the similarity between the <code class="literal">.fields</code> and <code class="literal">.columns</code> attributes, as these work in essentially the same way - they both return the list of columns (in the case of pandas DataFrame), or the list of fields, as columns are called in the case of Blaze DataShape.</p></div></div><p>Blaze can also<a id="id616" class="indexterm"/> read directly from a <code class="literal">GZipped</code> archive, saving space:</p><div><pre class="programlisting">traffic_gz = bl.Data('../Data/TrafficViolations.csv.gz')</pre></div><p>To validate that we get exactly the same data, let's retrieve the first two records from each structure. You can either call the following:</p><div><pre class="programlisting">traffic.head(2)</pre></div><p>Or you can choose to call:</p><div><pre class="programlisting">traffic_gz.head(2)</pre></div><p>It produces the same results (columns abbreviated here):</p><div><img src="img/B05793_09_08.jpg" alt="Working with files"/></div><p>It is easy to notice, however, that it takes significantly more time to retrieve the data from the archived file<a id="id617" class="indexterm"/> because Blaze needs to decompress the data.</p><p>You can also read from multiple files at one time and create one big dataset. To illustrate this, we have split the original dataset into four <code class="literal">GZipped</code> datasets by year of violation (these are stored in the <code class="literal">../Data/Years</code> folder).</p><p>Blaze uses <code class="literal">odo</code> to handle saving DataShapes<code class="literal"> </code>to a variety of formats. To save <code class="literal">traffic</code> data for traffic violations by year you can call <code class="literal">odo</code> like this:</p><div><pre class="programlisting">import odo
for year in traffic.Stop_year.distinct().sort():
    odo.odo(traffic[traffic.Stop_year == year], 
        '../Data/Years/TrafficViolations_{0}.csv.gz'\
        .format(year))</pre></div><p>The preceding instruction saves the data into a <code class="literal">GZip</code> archive, but you can save it to any of the formats mentioned earlier. The first argument to the <code class="literal">.odo(...)</code> method specifies the input object (in our case, the DataShape<code class="literal"> </code>with traffic violations that occurred in 2013), the second argument is the output object - the path to the file we want to save the data to. As we are about to learn - storing data is not limited to files only.</p><p>To read from multiple files you can use the asterisk character <code class="literal">*</code>:</p><div><pre class="programlisting">traffic_multiple = bl.Data(
    '../Data/Years/TrafficViolations_*.csv.gz')
traffic_multiple.head(2)</pre></div><p>The preceding snippet, once again, will produce a familiar table:</p><div><img src="img/B05793_09_09.jpg" alt="Working with files"/></div><p>Blaze reading<a id="id618" class="indexterm"/> capabilities are not limited to <code class="literal">.csv</code> or <code class="literal">GZip</code> files only: you can read data from JSON or Excel files (both, <code class="literal">.xls</code> and <code class="literal">.xlsx</code>), HDFS, or bcolz formatted files.</p><div><div><h3 class="title"><a id="tip36"/>Tip</h3><p>To learn more<a id="id619" class="indexterm"/> about the bcolz format, check its documentation at <a class="ulink" href="https://github.com/Blosc/bcolz">https://github.com/Blosc/bcolz</a>.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec98"/>Working with databases</h2></div></div></div><p>Blaze can also easily<a id="id620" class="indexterm"/> read from SQL databases such as PostgreSQL<a id="id621" class="indexterm"/> or SQLite. While SQLite would normally be a local database, the PostgreSQL can be run either locally or on a server.</p><p>Blaze, as mentioned earlier, uses <code class="literal">odo</code> in the background to handle the communication to and from the databases.</p><div><div><h3 class="title"><a id="note59"/>Note</h3><p>
<code class="literal">odo</code> is one of the<a id="id622" class="indexterm"/> requirements for Blaze and it gets installed along with the package. Check it out here <a class="ulink" href="https://github.com/blaze/odo">https://github.com/blaze/odo</a>.</p></div></div><p>In order to execute the code in this section, you will need two things: a running local instance of a PostgreSQL database, and a locally running MongoDB database.</p><div><div><h3 class="title"><a id="tip37"/>Tip</h3><p>In order to<a id="id623" class="indexterm"/> install PostgreSQL, download the package from <a class="ulink" href="http://www.postgresql.org/download/">http://www.postgresql.org/download/</a> and follow the installation instructions for your operating system found there.</p><p>To install MongoDB, go to <a class="ulink" href="https://www.mongodb.org/downloads">https://www.mongodb.org/downloads</a> and download the package; the installation<a id="id624" class="indexterm"/> instructions can be found here <a class="ulink" href="http://docs.mongodb.org/manual/installation/">http://docs.mongodb.org/manual/installation/</a>.</p></div></div><p>Before you proceed, we assume that you have a PostgreSQL database up and running at <code class="literal">http://localhost:5432/</code>, and MongoDB database running at <code class="literal">http://localhost:27017</code>.</p><p>We have already loaded the<a id="id625" class="indexterm"/> traffic data to both of the databases and stored them in the <code class="literal">traffic</code> table (PostgreSQL) or the <code class="literal">traffic</code> collection (MongoDB).</p><div><div><h3 class="title"><a id="tip38"/>Tip</h3><p>If you do not<a id="id626" class="indexterm"/> know how to upload your data, I have explained this in my other book <a class="ulink" href="https://www.packtpub.com/big-data-and-business-intelligence/practical-data-analysis-cookbook">https://www.packtpub.com/big-data-and-business-intelligence/practical-data-analysis-cookbook</a>.</p></div></div><div><div><div><div><h3 class="title"><a id="ch09lvl3sec18"/>Interacting with relational databases</h3></div></div></div><p>Let's read the data<a id="id627" class="indexterm"/> from the PostgreSQL database now. The <strong>Uniform Resource Identifier</strong> (<strong>URI</strong>) for accessing a PostgreSQL database has the following syntax <code class="literal">postgresql://&lt;user_name&gt;:&lt;password&gt;@&lt;server&gt;:&lt;port&gt;/&lt;database&gt;::&lt;table&gt;</code>.</p><p>To read the data<a id="id628" class="indexterm"/> from PostgreSQL, you just wrap the URI around <code class="literal">.Data(...)</code> - Blaze will take care of the rest:</p><div><pre class="programlisting">traffic_psql = bl.Data(
    'postgresql://{0}:{1}@localhost:5432/drabast::traffic'\
    .format('&lt;your_username&gt;', '&lt;your_password&gt;')
)</pre></div><p>We use Python's <code class="literal">.format(...)</code> method to fill in the string with the appropriate data.</p><div><div><h3 class="title"><a id="tip39"/>Tip</h3><p>Substitute your credentials to access your PostgreSQL database in the previous example. If you want to read more about the <code class="literal">.format(...) </code>method, you can check out<a id="id629" class="indexterm"/> the Python 3.5 documentation <a class="ulink" href="https://docs.python.org/3/library/string.html#format-string-syntax">https://docs.python.org/3/library/string.html#format-string-syntax</a>.</p></div></div><p>It is quite easy to output the data to either the PostgreSQL or SQLite databases. In the following example, we will output traffic violations that involved cars manufactured in 2016 to both PostgreSQL and SQLite databases. As previously noted, we will use <code class="literal">odo</code> to manage the transfers:</p><div><pre class="programlisting">traffic_2016 = traffic_psql[traffic_psql['Year'] == 2016]
# Drop commands
# odo.drop('sqlite:///traffic_local.sqlite::traffic2016')
# odo.drop('postgresql://{0}:{1}@localhost:5432/drabast::traffic'\
.format('&lt;your_username&gt;', '&lt;your_password&gt;'))
# Save to SQLite
odo.odo(traffic_2016,
'sqlite:///traffic_local.sqlite::traffic2016')
# Save to PostgreSQL
odo.odo(traffic_2016,  
    'postgresql://{0}:{1}@localhost:5432/drabast::traffic'\
    .format('&lt;your_username&gt;', '&lt;your_password&gt;'))</pre></div><p>In a similar fashion to pandas, to filter the data, we effectively select the <code class="literal">Year</code> column (the <code class="literal">traffic_psql['Year']</code> part of the first line) and create a Boolean flag by checking whether each and every record in that column equals <code class="literal">2016</code>. By indexing the <code class="literal">traffic_psql</code> object with<a id="id630" class="indexterm"/> such a truth vector, we extract only the records where the corresponding value equals <code class="literal">True</code>.</p><p>The two commented<a id="id631" class="indexterm"/> out lines should be uncommented if you already have the <code class="literal">traffic2016</code> tables in your databases; otherwise <code class="literal">odo</code> will append the data to the end of the table.</p><p>The URI for SQLite is slightly different than the one for PostgreSQL; it has the following syntax <code class="literal">sqlite://&lt;/relative/path/to/db.sqlite&gt;::&lt;table_name&gt;</code>.</p><p>Reading data from the SQLite database should be trivial for you by now:</p><div><pre class="programlisting">traffic_sqlt = bl.Data(
    'sqlite:///traffic_local.sqlite::traffic2016'
)</pre></div></div><div><div><div><div><h3 class="title"><a id="ch09lvl3sec19"/>Interacting with the MongoDB database</h3></div></div></div><p>MongoDB has<a id="id632" class="indexterm"/> gained lots of popularity over<a id="id633" class="indexterm"/> the years. It is a simple, fast, and flexible document-based database. The database is a go-to storage solution for all full-stack developers, using the <code class="literal">MEAN.js</code> stack: M here stands for Mongo (see <a class="ulink" href="http://meanjs.org">http://meanjs.org</a>).</p><p>Since Blaze is meant to work in a very familiar way no matter what your data source, reading from MongoDB is very similar to reading from PostgreSQL or SQLite databases:</p><div><pre class="programlisting">traffic_mongo = bl.Data(
    'mongodb://localhost:27017/packt::traffic'
)</pre></div></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec64"/>Data operations</h1></div></div></div><p>We have already<a id="id634" class="indexterm"/> presented some of the most common methods you will use with DataShapes (for example, <code class="literal">.peek()</code>), and ways to filter the data based on the column value. Blaze has implemented many methods that make working with any data extremely easy.</p><p>In this section, we will review a host of other commonly used ways of working with data and methods associated with them. For those of you coming from <code class="literal">pandas</code> and/or SQL, we will provide a respective syntax where equivalents exist.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec99"/>Accessing columns</h2></div></div></div><p>There are two ways of<a id="id635" class="indexterm"/> accessing columns: you can get a single column at a time by accessing them as if they were a DataShape attribute:</p><div><pre class="programlisting">traffic.Year.head(2)</pre></div><p>The preceding script produces the following output:</p><div><img src="img/B05793_09_10.jpg" alt="Accessing columns"/></div><p>You can also use indexing that allows the selection of more than one column at a time:</p><div><pre class="programlisting">(traffic[['Location', 'Year', 'Accident', 'Fatal', 'Alcohol']]
    .head(2))</pre></div><p>This generates the following output:</p><div><img src="img/B05793_09_11.jpg" alt="Accessing columns"/></div><p>The preceding<a id="id636" class="indexterm"/> syntax would be the same for pandas DataFrames. For those of you unfamiliar with Python and pandas API, please note three things here:</p><div><ol class="orderedlist arabic"><li class="listitem">To specify multiple columns, you need to enclose them in another list: note the double brackets <code class="literal">[[</code> and <code class="literal">]]</code>.</li><li class="listitem">If the chain of all methods does not fit on one line (or you want to break the chain for better readability) you have two choices: either enclose the whole chain of methods in brackets <code class="literal">(...)</code> where the <code class="literal">...</code> is the chain of all methods, or, before breaking into the new line, put the backslash character <code class="literal">\</code> at the end of every line in the chain. We prefer the latter and will use that in our examples from now on.</li><li class="listitem">Note that the equivalent SQL code would be:<div><pre class="programlisting">SELECT *
FROM traffic
LIMIT 2</pre></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec100"/>Symbolic transformations</h2></div></div></div><p>The beauty of Blaze comes from the fact that it can operate <em>symbolically</em>. What this means is that<a id="id637" class="indexterm"/> you can specify transformations, filters, or other operations on your data and store them as object(s). You can then <em>feed</em> such object with almost any form of data conforming to the original schema, and Blaze will return the transformed data.</p><p>For example, let's select all the traffic violations that occurred in 2013, and return only the <code class="literal">'Arrest_Type'</code>, <code class="literal">'Color'</code>, and <code class="literal">'Charge`</code> columns. First, if we could not reflect the schema from an already existing object, we would have to specify the schema manually. To do this, we will use the <code class="literal">.symbol(...)</code> method to achieve that; the first argument to the method specifies a symbolic name of the transformation (we prefer keeping it the same as the name of the object, but it can be anything), and the second argument is a long string that specifies the schema in a <code class="literal">&lt;column_name&gt;: &lt;column_type&gt;</code> fashion, separated by commas:</p><div><pre class="programlisting">schema_example = bl.symbol('schema_exampl', 
                           '{id: int, name: string}')</pre></div><p>Now, you could use the <code class="literal">schema_example</code> object and specify some transformations. However, since we already have an existing <code class="literal">traffic</code> dataset, we can <em>reuse</em> the schema by using <code class="literal">traffic.dshape</code> and specifying our transformations:</p><div><pre class="programlisting">traffic_s = bl.symbol('traffic', traffic.dshape)
traffic_2013 = traffic_s[traffic_s['Stop_year'] == 2013][
    ['Stop_year', 'Arrest_Type','Color', 'Charge']
]</pre></div><p>To present how this works, let's read the original dataset into pandas' <code class="literal">DataFrame</code>:</p><div><pre class="programlisting">traffic_pd = pd.read_csv('../Data/TrafficViolations.csv')</pre></div><p>Once read, we pass the<a id="id638" class="indexterm"/> dataset directly to the <code class="literal">traffic_2013</code> object and perform the computation using the <code class="literal">.compute(...)</code> method of Blaze; the first argument to the method specifies the transformation object (ours is <code class="literal">traffic_2013</code>) and the second parameter is the data that the transformations are to be performed on:</p><div><pre class="programlisting">bl.compute(traffic_2013, traffic_pd).head(2)</pre></div><p>Here is the output of the preceding snippet:</p><div><img src="img/B05793_09_12.jpg" alt="Symbolic transformations"/></div><p>You can also pass a list of lists or a list of NumPy arrays. Here, we use the <code class="literal">.values</code> attribute of the DataFrame to access the underlying list of NumPy arrays that form the DataFrame:</p><div><pre class="programlisting">bl.compute(traffic_2013, traffic_pd.values)[0:2]</pre></div><p>This code will produce precisely what we would expect:</p><div><img src="img/B05793_09_13.jpg" alt="Symbolic transformations"/></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec101"/>Operations on columns</h2></div></div></div><p>Blaze allows for easy mathematical operations to be done on numeric columns. All the traffic violations<a id="id639" class="indexterm"/> cited in the dataset occurred between 2013 and 2016. You can check that by getting all the distinct values for the <code class="literal">Stop_year</code> column using the <code class="literal">.distinct()</code> method. The <code class="literal">.sort()</code> method sorts the results in an ascending order:</p><div><pre class="programlisting">traffic['Stop_year'].distinct().sort()</pre></div><p>The preceding code produces the following output table:</p><div><img src="img/B05793_09_14.jpg" alt="Operations on columns"/></div><p>An equivalent syntax for pandas would be as follows:</p><div><pre class="programlisting">traffic['Stop_year'].unique().sort()</pre></div><p>For SQL, use the following code:</p><div><pre class="programlisting">SELECT DISTINCT Stop_year
FROM traffic</pre></div><p>You can also make some mathematical transformations/arithmetic to the columns. Since all the traffic violations occurred after year <code class="literal">2000</code>, we can subtract <code class="literal">2000</code> from the <code class="literal">Stop_year</code> column without losing any accuracy:</p><div><pre class="programlisting">traffic['Stop_year'].head(2) - 2000</pre></div><p>Here is what you should get in return:</p><div><img src="img/B05793_09_15.jpg" alt="Operations on columns"/></div><p>The same could be attained from pandas <code class="literal">DataFrame</code> with an identical syntax (assuming <code class="literal">traffic</code> was of pandas <code class="literal">DataFrame </code>type). For SQL, the equivalent would be:</p><div><pre class="programlisting">SELECT Stop_year - 2000 AS Stop_year
FROM traffic</pre></div><p>However, if you want to do some more complex mathematical operations (for example, <code class="literal">log</code> or <code class="literal">pow</code>) then you<a id="id640" class="indexterm"/> first need to use the one provided by Blaze (that, in the background, will translate your command to a suitable method from NumPy, math, or pandas).</p><p>For example, if you wanted to log-transform the <code class="literal">Stop_year</code> you need to use this code:</p><div><pre class="programlisting">bl.log(traffic['Stop_year']).head(2)</pre></div><p>This will produce the following output:</p><div><img src="img/B05793_09_16.jpg" alt="Operations on columns"/></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec102"/>Reducing data</h2></div></div></div><p>Some reduction<a id="id641" class="indexterm"/> methods are also available, such as <code class="literal">.mean()</code> (that calculates the average), <code class="literal">.std</code> (that calculates standard deviation), or <code class="literal">.max()</code> (that returns the maximum from the list). Executing the following code:</p><div><pre class="programlisting">traffic['Stop_year'].max()</pre></div><p>It will return the following output:</p><div><img src="img/B05793_09_17.jpg" alt="Reducing data"/></div><p>If you had a pandas DataFrame you can use the same syntax, whereas for SQL the same could be done with the following code:</p><div><pre class="programlisting">SELECT MAX(Stop_year) AS Stop_year_max
FROM traffic</pre></div><p>It is also quite easy to<a id="id642" class="indexterm"/> add more columns to your dataset. Say, you wanted to calculate the age of the car (in years) at the time when the violation occurred. First, you would take the <code class="literal">Stop_year</code> and subtract the <code class="literal">Year</code> of manufacture.</p><p>In the following code snippet, the first argument to the <code class="literal">.transform(...)</code> method is the DataShape, the transformation is to be performed on, and the other(s) would be a list of transformations.</p><div><pre class="programlisting">traffic = bl.transform(traffic,
             Age_of_car = traffic.Stop_year - traffic.Year)
traffic.head(2)</pre></div><div><div><h3 class="title"><a id="note60"/>Note</h3><p>In the source code of the <code class="literal">.transform(...)</code> method such lists would be expressed as <code class="literal">*args</code> as you could specify more than one column to be created in one go. The <code class="literal">*args</code> argument to any method would take any number of subsequent arguments and treat it as if it was a list.</p></div></div><p>The above code produces the following table:</p><div><img src="img/B05793_09_18.jpg" alt="Reducing data"/></div><p>An equivalent operation in pandas could be attained through the following code:</p><div><pre class="programlisting">traffic['Age_of_car'] = traffic.apply(
    lambda row: row.Stop_year - row.Year,
    axis = 1
)</pre></div><p>For SQL you can use the following code:</p><div><pre class="programlisting">SELECT *
    , Stop_year - Year AS Age_of_car
FROM traffic</pre></div><p>If you wanted to calculate the average age of the car involved in a fatal traffic violation and count the number<a id="id643" class="indexterm"/> of occurrences, you can perform a <code class="literal">group by</code> operation using the <code class="literal">.by(...)</code> operation:</p><div><pre class="programlisting">bl.by(traffic['Fatal'], 
      Fatal_AvgAge=traffic.Age_of_car.mean(),
      Fatal_Count =traffic.Age_of_car.count()
)</pre></div><p>The first argument to <code class="literal">.by(...)</code> specifies the column of the DataShape to perform the aggregation by, followed by a series of aggregations we want to get. In this example, we select the <code class="literal">Age_of_car</code> column and calculate an average and count the number of rows per each value of the <code class="literal">'Fatal'</code> column.</p><p>The preceding script produces the following aggregation:</p><div><img src="img/B05793_09_19.jpg" alt="Reducing data"/></div><p>For pandas, an equivalent would be as follows:</p><div><pre class="programlisting">traffic\
    .groupby('Fatal')['Age_of_car']\
    .agg({
        'Fatal_AvgAge': np.mean,
        'Fatal_Count':  np.count_nonzero
    })</pre></div><p> For SQL, it would be as follows:</p><div><pre class="programlisting">SELECT Fatal
    , AVG(Age_of_car)   AS Fatal_AvgAge
    , COUNT(Age_of_car) AS Fatal_Count
FROM traffic
GROUP BY Fatal</pre></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec103"/>Joins</h2></div></div></div><p>Joining two <code class="literal">DataShapes</code> is straightforward as well. To present how this is done, although the same<a id="id644" class="indexterm"/> result could be attained differently, we first select all the traffic<a id="id645" class="indexterm"/> violations by violation type (the <code class="literal">violation</code> object) and the traffic violations involving belts (the <code class="literal">belts</code> object):</p><div><pre class="programlisting">violation = traffic[
    ['Stop_month','Stop_day','Stop_year',
     'Stop_hr','Stop_min','Stop_sec','Violation_Type']]
belts = traffic[
    ['Stop_month','Stop_day','Stop_year',
     'Stop_hr','Stop_min','Stop_sec','Belts']]</pre></div><p>Now, we join the two objects on the six date and time columns.</p><div><div><h3 class="title"><a id="note61"/>Note</h3><p>The same effect could have been attained if we just simply selected the two columns: <code class="literal">Violation_type</code> and <code class="literal">Belts</code> in one go. However, this example is to show the mechanics of the <code class="literal">.join(...)</code> method, so bear with us.</p></div></div><p>The first argument to the <code class="literal">.join(...)</code> method is the first DataShape we want to join with, the second argument is the second DataShape, while the third argument can be either a single column or a list of columns to perform the join on:</p><div><pre class="programlisting">violation_belts = bl.join(violation, belts, 
      ['Stop_month','Stop_day','Stop_year',
       'Stop_hr','Stop_min','Stop_sec'])</pre></div><p>Once we have the full dataset in place, let's check how many traffic violations involved belts and what sort of punishment was issued to the driver:</p><div><pre class="programlisting">bl.by(violation_belts[['Violation_Type', 'Belts']],
      Violation_count=violation_belts.Belts.count()
).sort('Violation_count', ascending=False)</pre></div><p>Here's the output of the preceding script:</p><div><img src="img/B05793_09_20.jpg" alt="Joins"/></div><p>The same could<a id="id646" class="indexterm"/> be achieved in pandas with the following code:</p><div><pre class="programlisting">violation.merge(belts, 
    on=['Stop_month','Stop_day','Stop_year',
        'Stop_hr','Stop_min','Stop_sec']) \
    .groupby(['Violation_type','Belts']) \
    .agg({
        'Violation_count':  np.count_nonzero
    }) \
    .sort('Violation_count', ascending=False)</pre></div><p>With SQL, you would<a id="id647" class="indexterm"/> use the following snippet:</p><div><pre class="programlisting">SELECT innerQuery.*
FROM (
    SELECT a.Violation_type
        , b.Belts
        , COUNT() AS Violation_count
    FROM violation AS a
    INNER JOIN belts AS b
        ON      a.Stop_month = b.Stop_month
            AND a.Stop_day = b.Stop_day
            AND a.Stop_year = b.Stop_year
            AND a.Stop_hr = b.Stop_hr
            AND a.Stop_min = b.Stop_min
            AND a.Stop_sec = b.Stop_sec
    GROUP BY Violation_type
        , Belts
) AS innerQuery
ORDER BY Violation_count DESC</pre></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec65"/>Summary</h1></div></div></div><p>The concepts presented in this chapter are just the beginning of the road to using Blaze. There are many other ways it can be used and data sources it can connect with. Treat this as a starting point to build your understanding of polyglot persistence.</p><p>Note, however, that these days most of the concepts explained in this chapter can be attained natively within Spark, as you can use SQLAlchemy directly within Spark making it easy to work with a variety of data sources. The advantage of doing so, despite the initial investment of learning the API of SQLAlchemy, is that the data returned will be stored in a Spark DataFrame and you will have access to everything that PySpark has to offer. This, by no means, implies that you never should never use Blaze: the choice, as always, is yours.</p><p>In the next chapter, you will learn about streaming and how to do it with Spark. Streaming has become an increasingly important topic these days, as, daily (true as of 2016), the world produces roughly 2.5 exabytes of data (source: <a class="ulink" href="http://www.northeastern.edu/levelblog/2016/05/13/how-much-data-produced-every-day/">http://www.northeastern.edu/levelblog/2016/05/13/how-much-data-produced-every-day/</a>) that need to be ingested, processed and made sense of.</p></div></div>
</body></html>