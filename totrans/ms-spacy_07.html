<html><head></head><body><div><div><p><a id="_idTextAnchor086"/></p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor087"/>Chapter 5: Working with Word Vectors and Semantic Similarity</h1>
			<p><strong class="bold">Word vectors</strong> are handy tools and have been the hot topic of NLP for almost a decade. A word vector is basically a dense representation of a word. What's surprising about these vectors is that semantically similar words have similar word vectors. Word vectors are great for semantic similarity applications, such as calculating the similarity between words, phrases, sentences, and documents. At a word level, word vectors provide information about synonymity, semantic analogies, and more. We can build semantic similarity applications by using word vectors. </p>
			<p>Word vectors are produced by algorithms that make use of the fact that similar words appear in similar contexts. To capture the meaning of a word, a word vector algorithm collects information about the surrounding words that the target word appears with. This paradigm of capturing semantics for words by their surrounding words is called <strong class="bold">distributional semantics</strong>.</p>
			<p>In this chapter, we will introduce the <strong class="bold">distributional semantics paradigm</strong> and its associated <strong class="bold">semantic similarity methods</strong>. We will start by taking a conceptual look at <strong class="bold">text vectorization</strong> so that you know what NLP problems word vectors solve.</p>
			<p>Next, we will become familiar with word vector computations such as <strong class="bold">distance calculation</strong>, <strong class="bold">analogy calculations</strong>, and <strong class="bold">visualization</strong>. Then, we will learn how to benefit from spaCy's pretrained word vectors, as well as import and use third-party vectors. Finally, we will go through advanced semantic similarity methods using spaCy.</p>
			<p>In this chapter, we're going to cover the following main topics: </p>
			<ul>
				<li>Understanding word vectors</li>
				<li>Using spaCy's pretrained vectors</li>
				<li>Using third-party word vectors</li>
				<li>Advanced semantic similarity methods</li>
			</ul>
			<h1 id="_idParaDest-86"><a id="_idTextAnchor088"/>Technical requirements</h1>
			<p>In this chapter, we have used some external Python libraries besides spaCy for code visualization purposes. If you want to generate word vector visualizations in this chapter, you will need the following:</p>
			<ul>
				<li>NumPy</li>
				<li>scikit-learn</li>
				<li>Matplotlib</li>
			</ul>
			<p>You can find this chapter's code in this book's GitHub repository: <a href="https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter05">https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter05</a>.</p>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor089"/>Understanding word vectors</h1>
			<p>The invention of word vectors (or <strong class="bold">word2vec</strong>) has been one of the most thrilling advancements in the NLP world. Those of you who are practicing NLP have definitely heard of word <a id="_idIndexMarker280"/>vectors at some point. This chapter will help you understand the underlying idea that caused the invention of word2vec, what word vectors look like, and how to use them in NLP applications.</p>
			<p>The statistical world works with numbers, and all statistical methods, including statistical NLP algorithms, work with vectors. As a result, while working with statistical methods, we need to represent every real-world quantity as a vector, including text. In this section, we will learn about the different ways we can represent text as vectors and discover how word vectors provide semantic representation for words.</p>
			<p>We will start by discovering text vectorization by covering the simplest implementation possible: one-hot encoding.</p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor090"/>One-hot encoding</h2>
			<p><strong class="bold">One-hot encoding</strong> is a <a id="_idIndexMarker281"/>simple and straightforward <a id="_idIndexMarker282"/>way to assign vectors to words: assign an index value to each word in the vocabulary and then encode this value into a <strong class="bold">sparse vector</strong>. Let's look <a id="_idIndexMarker283"/>at an example. Here, we will consider the vocabulary of a pizza ordering application; we can assign an index to each word in the order they appear in the vocabulary:</p>
			<pre>1   a 
2   e-mail  
3   I
4   cheese  
5   order    
6   phone
7   pizza   
8   salami  
9   topping 
10 want</pre>
			<p>Now, the vector <a id="_idIndexMarker284"/>of a vocabulary word will be 0, except for the position <a id="_idIndexMarker285"/>of the word's corresponding index value:</p>
			<pre>a           1 0 0 0 0 0 0 0 0 0
e-mail   0 1 0 0 0 0 0 0 0 0
I            0 0 1 0 0 0 0 0 0 0
cheese  0 0 0 1 0 0 0 0 0 0
order     0 0 0 0 1 0 0 0 0 0
phone    0 0 0 0 0 1 0 0 0 0
pizza      0 0 0 0 0 0 1 0 0 0
salami    0 0 0 0 0 0 0 1 0 0
topping  0 0 0 0 0 0 0 0 1 0
want      0 0 0 0 0 0 0 0 0 1   </pre>
			<p>Now, we can represent a sentence as a matrix, where each row corresponds to one word. For example, the sentence <em class="italic">I want a pizza</em> can be represented by the following matrix:</p>
			<pre>I          0 0 1 0 0 0 0 0 0 0
want   0 0 0 0 0 0 0 0 0 1
a         1 0 0 0 0 0 0 0 0 0
pizza   0 0 0 0 0 0 1 0 0 0</pre>
			<p>As we can see from the preceding vocabulary and indices, the length of the vectors is equal to the number of the words in the vocabulary. Each dimension is devoted to one word explicitly. When we apply one-hot encoding vectorization to our text, each word is replaced by <a id="_idIndexMarker286"/>its vector and the sentence is <a id="_idIndexMarker287"/>transformed into a <code>(N, V)</code> matrix, where <code>N</code> is the number of words in the sentence and <code>V</code> is the vocabulary's size.</p>
			<p>This way of representing text is straightforward to compute, as well as easy to debug and understand. This looks good so far, but there are some potential problems here, such as the following:</p>
			<ul>
				<li>The vectors are sparse. Each vector contains many 0s but only one <code>1</code>. Obviously, this is a waste of space if we know that words with similar meanings can be grouped together and share some dimensions. Also, numerical algorithms don't really like high-dimensional and sparse vectors in general.</li>
				<li>Secondly, what if the vocabulary size is over 1 million words? Obviously, we would need to use 1 million dimensional vectors, which is not really feasible in terms of memory and computation. </li>
				<li>Another problem is that the vectors are not <em class="italic">meaningful</em> at all. Similar words are not assigned similar vectors somehow. In the preceding vocabulary, the words <code>cheese</code>, <code>topping</code>, <code>salami</code>, and <code>pizza</code> actually carry related meanings, but their vectors are not related in any way. These vectors are indeed assigned randomly, depending on the corresponding word's index in the vocabulary. The one-hot encoded vectors don't capture any semantic relationships at all. </li>
			</ul>
			<p>Word vectors were invented to answer the preceding list of concerns.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor091"/>Word vectors</h2>
			<p>Word vectors <a id="_idIndexMarker288"/>are the solution to the preceding problems. A word vector is a <strong class="bold">fixed-size</strong>, <strong class="bold">dense</strong>, <strong class="bold">real-valued</strong> vector. From a broader perspective, a word <a id="_idIndexMarker289"/>vector is a <a id="_idIndexMarker290"/>learned representation of the text <a id="_idIndexMarker291"/>where semantically similar words have similar <a id="_idIndexMarker292"/>vectors. The following is what a word vector looks like. This has been extracted from <strong class="bold">Glove English vectors</strong> (we'll look at Glove in detail in the <em class="italic">How word vectors are produced</em> section):</p>
			<pre>the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581</pre>
			<p>This is a 50-dimensional vector for the word <code>the</code>. As you can see, the dimensions are floating points. But what do the dimensions represent? These individual dimensions typically don't have inherent meanings. Instead, they represent locations in the vector space, and the distance between these vectors indicates the similarity of the corresponding words' meanings. Hence, a word's meaning is distributed across the dimensions. This way <a id="_idIndexMarker293"/>of representing a word's meaning is called <strong class="bold">distributional semantics</strong>. </p>
			<p>We've already mentioned that semantically similar words have similar representations. Let's look at the vectors of the different words and how they offer semantic representations. We can <a id="_idIndexMarker294"/>use the word vector visualizer for TensorFlow at <a href="https://projector.tensorflow.org/">https://projector.tensorflow.org/</a> for this. On this website, Google offers word vectors for 10,000 words. Each vector is 200-dimensional and projected onto three dimensions for visualization. Let's look at the representation of the word <code>cheese</code> from our humble pizza ordering vocabulary:</p>
			<div><div><img src="img/B16570_5_1.jpg" alt="Figure 5.1 – The vector representation of the word “cheese” and semantically similar words&#13;&#10;" width="814" height="621"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – The vector representation of the word "cheese" and semantically similar words</p>
			<p>As we can see, the word <code>cheese</code> is semantically grouped with the other words about food. These are <a id="_idIndexMarker295"/>the words that are used together with the word <code>cheese</code> quite often: sauce, cola, food, and so on. In the following screenshot, we can see the closest words sorted by their cosine distance (think of cosine distance as a way of calculating the distance between vectors):</p>
			<div><div><img src="img/B16570_5_2.jpg" alt="Figure 5.2 – Closest points to “cheese” in the three-dimensional space&#13;&#10;" width="223" height="648"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – Closest points to "cheese" in the three-dimensional space</p>
			<p>How about some proper nouns? Word vectors are trained on a huge corpus, such as Wikipedia, which is <a id="_idIndexMarker296"/>why the representations of some proper nouns are also learned. For example, the proper noun <strong class="bold">elizabeth</strong> is represented by the following vector:</p>
			<div><div><img src="img/B16570_5_3.jpg" alt="Figure 5.3 – Vector representation of elizabeth&#13;&#10;" width="917" height="698"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – Vector representation of elizabeth</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Notice that all the words in the preceding screenshot are in lowercase. Most of the word vector algorithms make all the vocabulary input words lowercase to avoid there being two representations of the same word.</p>
			<p>Here, we can see that <strong class="bold">elizabeth</strong> indeed points to Queen Elizabeth of England. The surrounding words are <strong class="bold">monarch</strong>, <strong class="bold">empress</strong>, <strong class="bold">princess</strong>, <strong class="bold">royal</strong>, <strong class="bold">lord</strong>, <strong class="bold">lady</strong>, <strong class="bold">crown</strong>, <strong class="bold">England</strong>, <strong class="bold">Tudor</strong>, <strong class="bold">Buckingham</strong>, her mother's name, <strong class="bold">anne</strong>, her father's name, <strong class="bold">henry</strong>, and even her mother's rival queen's name, <strong class="bold">catherine</strong>! Both ordinary words such as <strong class="bold">crown</strong> and proper nouns such as <strong class="bold">henry</strong> are grouped together with <strong class="bold">elizabeth</strong>. We can also see that the syntactic category of all the neighbor words is noun; verbs don't go together with nouns. </p>
			<p>Word vectors <a id="_idIndexMarker297"/>can capture synonyms, antonyms, and semantic categories such as animals, places, plants, names, and abstract concepts. Next, we'll dive deep into semantics and explore a surprising feature provided by word vectors – <strong class="bold">word analogies</strong>.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor092"/>Analogies and vector operations</h2>
			<p>We've already seen that learned representations can capture semantics. What's more, word vectors <a id="_idIndexMarker298"/>support vector operations, such as vector addition <a id="_idIndexMarker299"/>and subtraction, in a meaningful way. Indeed, adding and <a id="_idIndexMarker300"/>subtracting word vectors is one way to support analogies. </p>
			<p>A word analogy is a semantic relationship between a pair of words. There are many types of relationship, such as synonymity, anonymity, and wholepart relation. Some example pairs are (King – man, Queen – woman), (airplane – air, ship - sea), (fish – sea, bird - air), (branch – tree, arm – human), (forward – backward, absent – present), and so on.</p>
			<p>For example, we can represent gender mapping between the Queen and King as <code>Queen – Woman + Man = King</code>. Here, if we subtract <em class="italic">woman</em> from <em class="italic">Queen</em> and add <em class="italic">man</em> instead, we get <em class="italic">King</em>. Then, this analogy reads as, <em class="italic">queen is to king as woman is to man</em>. Embeddings can generate remarkable analogies such as gender, tense, and capital city. The following diagram shows these analogies:</p>
			<div><div><img src="img/B16570_5_4.jpg" alt="Figure 5.4 – Analogies created by the word vectors (Source: https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space) " width="1318" height="501"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – Analogies created by the word vectors (Source: https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space) </p>
			<p>Obviously, word vectors <a id="_idIndexMarker301"/>provide great semantic capabilities <a id="_idIndexMarker302"/>for NLP developers, but how are they produced? We'll learn more about word vector generation algorithms in the next section.</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor093"/>How word vectors are produced</h2>
			<p>There is more <a id="_idIndexMarker303"/>than one way to produce word vectors. Let's look at the most popular pretrained vectors and how they are trained: </p>
			<ul>
				<li><strong class="bold">word2vec</strong> is the name of the statistical algorithm that was created by Google to produce word vectors. Word vectors are trained with a neural network architecture, which processes windows of words and predicts the vector for each word, depending <a id="_idIndexMarker304"/>on the surrounding words. These pretrained word vectors can be downloaded from <a href="https://developer.syn.co.in/tutorial/bot/oscova/pretrained-vectors.html#word2vec-and-glove-models">https://developer.syn.co.in/tutorial/bot/oscova/pretrained-vectors.html#word2vec-and-glove-models</a>. We won't go into the details here, but you can read the excellent blog at <a href="https://jalammar.github.io/illustrated-word2vec/">https://jalammar.github.io/illustrated-word2vec/</a> for more details about the algorithm and data preparation steps. </li>
				<li><strong class="bold">Glove</strong> vectors are <a id="_idIndexMarker305"/>trained in another way and were invented by the Stanford NLP group. This method depends on singular value decomposition, which is used on the word co-occurrences matrix. A comprehensive guide to the Glove algorithm is available at <a href="https://www.youtube.com/watch?v=Fn_U2OG1uqI">https://www.youtube.com/watch?v=Fn_U2OG1uqI</a>. The pretrained vectors are available at <a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a>.</li>
				<li><strong class="bold">fastText</strong> was created <a id="_idIndexMarker306"/>by Facebook Research and is similar to word2vec, but offers more. word2vec predicts words based on their surrounding context, while fastText predicts subwords; that is, character n-grams. For example, the word <em class="italic">chair</em> generates the following subwords:<pre>ch, ha, ai, ir, cha, hai, air</pre></li>
			</ul>
			<p>fastText produces a vector for each subword, including misspelled words, numbers, partial words, and <a id="_idIndexMarker307"/>single characters. fastText is robust when it comes to misspelled words and rare words. It can compute a vector for the tokens that are not proper lexicon words.</p>
			<p>Facebook Research published pretrained fastText vectors for 157 languages. You can find these models at <a href="https://fasttext.cc/docs/en/crawl-vectors.html">https://fasttext.cc/docs/en/crawl-vectors.html</a></p>
			<p>All the preceding algorithms follow the same idea: similar words occur in a similar context. The context – the surrounding words around a word – is key to generating the word vector for a specific word in any case. All the pretrained word vectors that are generated with the preceding three algorithms are trained on a huge corpus such as Wikipedia, the news, or Twitter. </p>
			<p class="callout-heading">Pro tip</p>
			<p class="callout">When we say similar words, the first concept that comes to mind is <strong class="bold">synonymity</strong>. Synonym words occur in a similar context; for example, <em class="italic">freedom</em> and <em class="italic">liberty</em> both mean the same thing:</p>
			<p class="callout">We want free healthcare, education, and liberty.</p>
			<p class="callout">We want free healthcare, education, and freedom.</p>
			<p class="callout">How about antonyms? Antonyms can be used in the same context. Take <em class="italic">love</em> and <em class="italic">hate</em>, for example:</p>
			<p class="callout">I hate cats.</p>
			<p class="callout">I love cats.</p>
			<p class="callout">As you can see, antonyms also appear in similar contexts; hence, usually, their vectors are also similar. If your downstream NLP task is sensitive in this aspect, be careful while using word vectors. In this case, always either train your own vectors or refine your word vectors by training them in the downstream task as well. You can train your own word vectors <a id="_idIndexMarker308"/>with the Gensim package (<a href="https://radimrehurek.com/gensim/">https://radimrehurek.com/gensim/</a>). The Keras library allows word vectors to be trained on downstream tasks. We'll revisit this issue in <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a><em class="italic">, Text Classification with spaCy</em>.</p>
			<p>Now that <a id="_idIndexMarker309"/>we know more about word vectors, let's look at how to use spaCy's pretrained word vectors. </p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor094"/>Using spaCy's pretrained vectors</h1>
			<p>We installed a medium-sized English spaCy language model in <a href="B16570_01_Final_JM_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a><em class="italic">, Getting Started with spaCy</em>, so that we can directly use word vectors. Word vectors are part of many spaCy <a id="_idIndexMarker310"/>language models. For instance, the <code>en_core_web_md</code> model ships with 300-dimensional vectors for 20,000 words, while the <code>en_core_web_lg</code> model ships with 300-dimensional vectors with a 685,000 word vocabulary. </p>
			<p>Typically, small models (those whose names end with <code>sm</code>) do not include any word vectors but include context-sensitive tensors. You can still make the following semantic similarity calculations, but the results won't be as accurate as word vector computations.</p>
			<p>You can reach a word's vector via the <code>token.vector</code> method. Let's look at this method in an example. The following code queries the word vector for banana:</p>
			<pre>import spacy
nlp = spacy.load("en_core_web_md")
doc = nlp("I ate a banana.")
doc[3].vector</pre>
			<p>The following <a id="_idIndexMarker311"/>screenshot was taken within the Python shell:</p>
			<div><div><img src="img/B16570_5_5.jpg" alt="Figure 5.5 – Word vector for the word “banana”&#13;&#10;" width="521" height="457"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5 – Word vector for the word "banana"</p>
			<p><code>token.vector</code> returns a NumPy <code>ndarray</code>. You can call <code>numpy</code> methods on the result:</p>
			<pre>type(doc[3].vector)
&lt;class 'numpy.ndarray'&gt;
doc[3].vector.shape
(300,)</pre>
			<p>In this code segment, first, we queried the Python type of the word vector. Then, we invoked the <code>shape()</code> method of the NumPy array on the vector. </p>
			<p>The <code>Doc</code> and <code>Span</code> objects also have vectors. The vector of a sentence or a span is the average of its words' vectors. Run the following code and view the results: </p>
			<pre>doc = nlp("I like a banana,")
doc.vector
doc[1:3].vector</pre>
			<p>Only the <a id="_idIndexMarker312"/>words in the model's vocabulary have vectors; words <a id="_idIndexMarker313"/>that are not in the vocabulary are called <code>token.is_oov</code> and <code>token.has_vector</code> are two methods we can use to query whether a token is in the model's vocabulary and has a word vector:</p>
			<pre>doc = nlp("You went there afskfsd.")
for token in doc:
          token.is_oov, token.has_vector
(False, True)
(False, True)
(False, True)
(True, False)
(False, True)</pre>
			<p>This is basically <a id="_idIndexMarker314"/>how we use spaCy's pretrained word vectors. Next, we'll discover how to invoke spaCy's semantic similarity method on <code>Doc</code>, <code>Span</code>, and <code>Token</code> objects. </p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor095"/>The similarity method</h2>
			<p>In spaCy, every container type object has a similarity method that allows us to calculate the semantic <a id="_idIndexMarker315"/>similarity of other container objects by comparing their word vectors.</p>
			<p>We can <a id="_idIndexMarker316"/>calculate the semantic similarity between two container objects, even though they are different types of containers. For instance, we can compare a <code>Token</code> object to a <code>Doc</code> object and a <code>Doc</code> object to a <code>Span</code> object. The following example computes how similar two <code>Span</code> objects are:</p>
			<pre>doc1 = nlp("I visited England.")
doc2 = nlp("I went to London.")
doc1[1:3].similarity(doc2[1:4])
0.6539691</pre>
			<p>We can compare the two <code>Token</code> objects, <code>London</code> and <code>England</code>, as well:</p>
			<pre>doc1[2].similarity(doc2[3])
0.73891276</pre>
			<p>The sentence's similarity is computed by calling <code>similarity()</code> on the <code>Doc</code> objects:</p>
			<pre>doc1.similarity(doc2)
0.7995623615797786</pre>
			<p>The preceding code segment calculates the semantic similarity between the two sentences <code>I visited England.</code> and <code>I went to London.</code>. The similarity score is high enough that it considers both sentences are similar (the degree of similarity ranges from <code>0</code> to <code>1</code>, with <code>0</code> being unrelated and <code>1</code> being identical).</p>
			<p>Not surprisingly, the <code>similarity()</code> method returns <code>1</code> when you compare an object to itself:</p>
			<pre>doc1.similarity(doc1)
1.0</pre>
			<p>Judging the distance with numbers is difficult sometimes, but looking at the vectors on paper can also help us understand how our vocabulary words are grouped. The following code snippet visualizes a simple vocabulary of two semantic classes. The first class of words <a id="_idIndexMarker317"/>is for animals, while the second class is for food. We expect <a id="_idIndexMarker318"/>these two classes of words to become two groups on the graphics:</p>
			<pre>import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import numpy as np
import spacy
nlp = spacy.load("en_core_web_md")
vocab = nlp("cat dog tiger elephant bird monkey lion cheetah burger pizza food cheese wine salad noodles macaroni fruit vegetable")
words = [word.text for word in vocab]
&gt;&gt;&gt; vecs = np.vstack([word.vector for word in vocab if word.has_vector])
pca = PCA(n_components=2)
vecs_transformed = pca.fit_transform(vecs)
plt.figure(figsize=(20,15))
plt.scatter(vecs_transformed[:,0], vecs_transformed[:,1])
for word, coord in zip(words, vecs_transformed):
           x,y = coord
          plt.text(x,y,word, size=15)
plt.show()</pre>
			<p>This code snippet achieves a lot. Let's take a look: </p>
			<ol>
				<li>First, we imported the matplotlib library for creating our graphic. </li>
				<li>The next two imports are for calculating the vectors.</li>
				<li>We imported <code>spacy</code> and created an <code>nlp</code> object as usual.</li>
				<li>Then, we created a <code>Doc</code> object from our vocabulary.</li>
				<li>Next, we stacked our word vectors vertically by calling <code>np.vstack</code>.</li>
				<li>Since the vectors are 300-dimensional, we needed to project them into a two-dimensional space for visualization purposes. We made this projection by extracting <a id="_idIndexMarker319"/>the two principal components via <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>).</li>
				<li>The rest <a id="_idIndexMarker320"/>of the code deals with <a id="_idIndexMarker321"/>matplotlib function calls to create a scatter plot.</li>
			</ol>
			<p>The resulting visual looks as follows:</p>
			<div><div><img src="img/B16570_5_6.jpg" alt="Figure 5.6 – Two semantic classes grouped&#13;&#10;" width="1127" height="729"/>
				</div>
			</div>
			<p class="figure-caption">  </p>
			<p class="figure-caption">Figure 5.6 – Two semantic classes grouped</p>
			<p>Voil•! Our spaCy word vectors really worked! Here, we can see the two semantic classes that were grouped on the visualization. Notice that the distance between the animals is less and more uniformly distributed, while the food class formed groups inside the group.</p>
			<p>Previously, we <a id="_idIndexMarker322"/>mentioned that we can create <a id="_idIndexMarker323"/>our own word vectors or refine them on our own corpus. Once we've done that, can we use them within spaCy? The answer is yes! In the next section, we'll learn how to load custom word vectors into spaCy.</p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor096"/>Using third-party word vectors</h1>
			<p>We can also use third-party word vectors within spaCy. In this section, we'll learn how to import a <a id="_idIndexMarker324"/>third-party word vector package into spaCy. We'll use fastText's subword-based pretrained vectors from the Facebook AI. You can view the list of all the available English pretrained vectors at <a href="https://fasttext.cc/docs/en/english-vectors.html">https://fasttext.cc/docs/en/english-vectors.html</a>. </p>
			<p>The name of the package identifies the vector's dimension, the vocabulary size, and the corpus genre that the vectors will be trained on. For instance, <code>wiki-news-300d-1M-subword.vec.zip</code> indicates that it contains 1 million 300-dimensional word vectors that have been trained on a Wikipedia corpus.</p>
			<p>Let's start downloading the vectors: </p>
			<ol>
				<li value="1">In your terminal, type the following command. Alternatively, you can copy and paste the URL into your browser and the download should start:<pre><strong class="bold">$ wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.vec.zip</strong></pre><p>The preceding line will download the 300-dimensional word vectors onto your machine.</p></li>
				<li>Next, we will unzip the following <code>.zip</code> file. You can either unzip it by right-clicking or by using the following code:<pre><code>wiki-news-300d-1M-subword.vec</code> file.</p></li>
				<li>Now, we're <a id="_idIndexMarker325"/>ready to use spaCy's <code>init-model</code> command:<pre><code>wiki-news-300d-1M-subword.vec</code> vectors into spaCy's vector format.</p><p>b) Creates a language model directory named <code>en_subwords_wiki_lg</code> that contains the newly created vectors.</p></li>
				<li>If everything goes well, you should see the following message:<pre><strong class="bold">Reading vectors from wiki-news-300d-1M-subword.vec</strong>
<strong class="bold">Open loc</strong>
<strong class="bold">999994it [02:05, 7968.84it/s]</strong>
<strong class="bold">Creating model...</strong>
<strong class="bold">0it [00:00, ?it/s]      Successfully compiled vocab</strong>
<strong class="bold">      999731 entries, 999994 vectors</strong></pre></li>
				<li>With that, we've created the language model. Now, we can load it:<pre>import spacy
nlp = spacy.load("en_subwords_wiki_lg")</pre></li>
				<li>Now, we can create a <code>doc</code> object with this <code>nlp</code> object, just like we did with spaCy's default language models:<pre>doc = nlp("I went there.")</pre></li>
			</ol>
			<p>The model we just created is an empty model that was initiated with the word vectors, so it does <a id="_idIndexMarker326"/>not contain any other pipeline components. For instance, making a call to <code>doc.ents</code> will fail with an error. So, be careful while working with third-party vectors and favor built-in spaCy vectors whenever possible.</p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor097"/>Advanced semantic similarity methods</h1>
			<p>In this section, we'll discover advanced semantic similarity methods for word, phrase, and sentence <a id="_idIndexMarker327"/>similarity. We've already learned how to calculate semantic similarity with spaCy's <strong class="bold">similarity</strong> method and obtained some scores. But what do these scores mean? How are they calculated? Before we look at more advanced methods, first, we'll learn how semantic similarity is calculated.</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor098"/>Understanding semantic similarity</h2>
			<p>When we collect text data (any sort of data), we want to see how some examples are similar, different, or related. We want to measure how similar two pieces of text are by calculating <a id="_idIndexMarker328"/>their similarity scores. Here, the term <em class="italic">semantic similarity</em> comes into the picture; <strong class="bold">semantic similarity</strong> is a <strong class="bold">metric</strong> that's defined over texts, where the distance between two texts is based on their semantics. </p>
			<p>A metric in mathematics is basically a distance function. Every metric induces a topology on the vector space. Word vectors are vectors, so we want to calculate the distance between them and use this as a similarity score. </p>
			<p>Now, we'll learn about two commonly used distance functions: <strong class="bold">Euclidian distance</strong> and <strong class="bold">cosine distance</strong>. Let's start with Euclidian distance.</p>
			<h3>Euclidian distance</h3>
			<p>The Euclidian distance <a id="_idIndexMarker329"/>between two points in a k-dimensional space is the length of the path between them. The distance between two points is calculated by the Pythagorean theorem. We calculate this distance by summing the difference of each coordinate's square and then taking the square root of this sum. The following diagram <a id="_idIndexMarker330"/>shows the Euclidian distance between two vectors, dog and cat:</p>
			<div><div><img src="img/B16570_5_7.jpg" alt="Figure 5.7 – Euclidian distance between two vectors, dog and cat&#13;&#10;" width="707" height="631"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.7 – Euclidian distance between two vectors, dog and cat</p>
			<p>What does <a id="_idIndexMarker331"/>Euclidian distance mean for word vectors? First, Euclidian distance has no idea of <strong class="bold">vector orientation</strong>; what matters is the <strong class="bold">vector magnitude</strong>. If we take <a id="_idIndexMarker332"/>a pen and draw a vector from the origin to the <strong class="bold">dog</strong> point (let's call it <strong class="bold">dog vector</strong>) and <a id="_idIndexMarker333"/>do the same for the <strong class="bold">cat</strong> point (let's call it <strong class="bold">cat vector</strong>) and subtract one vector from and other, then the distance is <a id="_idIndexMarker334"/>basically the magnitude of this difference vector. </p>
			<p>What happens if we add two more semantically similar words (<em class="italic">canine</em>, <em class="italic">terrier</em>) to <strong class="bold">dog</strong> and make it a <a id="_idIndexMarker335"/>text of three words? Obviously, the dog vector will now grow in magnitude, possibly in the same direction. This time, the distance will be much bigger due to geometry (as shown in the following diagram), although the semantics of the first piece of text (now <strong class="bold">dog canine terrier</strong>) remain the same. </p>
			<p>This is the main drawback of using Euclidian distance for semantic similarity – the orientation of the two vectors in the space is not taken into account. The following diagram illustrates the distance between <strong class="bold">dog</strong> and <strong class="bold">cat</strong> and the distance between <strong class="bold">dog</strong> <strong class="bold">canine terrier</strong> and <strong class="bold">cat</strong>:</p>
			<div><div><img src="img/B16570_5_8.jpg" alt="Figure 5.8 – Distance between “dog” and “cat,” as well as the distance between “dog canine terrier” and “cat”" width="608" height="581"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.8 – Distance between "dog" and "cat," as well as the distance between "dog canine terrier" and "cat"</p>
			<p>How can <a id="_idIndexMarker336"/>we fix this problem? There's another way of calculating similarity that addresses this problem, called <strong class="bold">cosine similarity</strong>. Let's take a look.</p>
			<h3>Cosine distance and cosine similarity</h3>
			<p>Contrary to <a id="_idIndexMarker337"/>Euclidian distance, cosine distance is more concerned with the orientation of the two vectors in the space. The cosine similarity of two vectors <a id="_idIndexMarker338"/>is basically the cosine of the angle that's created by these two vectors. The following diagram shows the angle between the <strong class="bold">dog</strong> and <strong class="bold">cat</strong> vectors:</p>
			<div><div><img src="img/B16570_5_9.jpg" alt="Figure 5.9 – The angle between the dog and cat vectors. Here, the semantic similarity is calculated by cos(θ)" width="665" height="619"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.9 – The angle between the dog and cat vectors. Here, the semantic similarity is calculated by cos(θ)</p>
			<p>The maximum similarity score that's allowed by cosine similarity is <code>1</code>. This is obtained when the angle between two vectors is 0 degrees (hence, the vectors coincide). The similarity between two vectors is 0 when the angle between them is 90 degrees. </p>
			<p>Cosine similarity provides us with scalability when the vectors grow in magnitude. We will refer to <em class="italic">Figure 5.8</em> again here. If we grow one of the input vectors, the angle between them remains the same, so the cosine similarity score is the same.  </p>
			<p>Note that here, we are calculating the semantic similarity score, not the distance. The highest possible value is 1 when the vectors coincide, while the lowest score is 0 when two vectors are perpendicular. The cosine distance is 1 – cos(θ), which is a distance function. </p>
			<p>spaCy uses cosine <a id="_idIndexMarker339"/>similarity to calculate semantic similarity. Hence, calling the <code>similarity</code> method helps us make cosine similarity calculations. </p>
			<p>So far, we've <a id="_idIndexMarker340"/>learned how to calculate similarity scores, but we still haven't discovered words we should look for meaning in. Obviously, not all the words in a sentence have the same impact on the semantics of the sentence. The similarity method will calculate the semantic similarity score for us, but for the results of that calculation to be useful, we need to choose the right keywords to compare. To understand why, consider the following text snippet:</p>
			<pre>Blue whales are the biggest mammals in the world. They're observed in California coast during spring.</pre>
			<p>If we're interested in finding the biggest mammals on the planet, the phrases <code>biggest mammals</code> and <code>in the world</code> will be the key words. Comparing these phrases with the search phrases <em class="italic">largest mammals</em> and <em class="italic">on the planet</em> should give us a high similarity score. But if we're interested in finding out about some places in the world, <code>California</code> will be the keyword. <code>California</code> is semantically similar to the word <em class="italic">geography</em> and, even better, the entity type is a geographical noun.</p>
			<p>We have already learned <em class="italic">how</em> to calculate the similarity score. In the next section, we'll learn about <em class="italic">where</em> to look for the <em class="italic">meaning</em>. We'll extract the key phrases and named entities from the sentences and then use them in similarity score calculations. We'll start by covering a case study on text categorization before improving the task results via key phrase extraction.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor099"/>Categorizing text with semantic similarity</h2>
			<p>Determining two sentence's semantic similarity can help you categorize texts into predefined <a id="_idIndexMarker341"/>categories or spot only the relevant texts. In this case study, we'll filter all user comments in an e-commerce website <a id="_idIndexMarker342"/>related to the word <em class="italic">perfume</em>. Suppose you need to evaluate the following user comments:</p>
			<pre>I purchased a science fiction book last week.
I loved everything related to this fragrance: light, floral and feminine …
I purchased a bottle of wine. </pre>
			<p>Here, we can see that only the second sentence is related. This is because it contains the word <code>fragrance</code>, as well as the adjectives describing scents. To understand which sentences are related, we can try several comparison strategies. </p>
			<p>First, we can compare <code>perfume</code> to each sentence. Recall that spaCy generates a word vector for a sentence by averaging the word vector of its tokens. The following code snippet compares the preceding sentences to the <code>perfume</code> search key:</p>
			<pre>sentences = nlp("I purchased a science fiction book last week. I loved everything related to this fragrance: light, floral and feminine... I purchased a bottle of wine.  ")
key = nlp("perfume")
for sent in sentences.sents:
           print(sent.similarity(key))
... 
0.2481654331382154
0.5075297559861377
0.42154297167069865</pre>
			<p>Here, we performed the following steps: </p>
			<ol>
				<li value="1">First, we created a <code>Doc</code> object with the three preceding sentences. </li>
				<li>Then, for each sentence, we calculated the similarity score with <code>perfume</code>.</li>
				<li>Then, we printed the score by invoking the <code>similarity()</code> method on the sentence.</li>
			</ol>
			<p>The degree of similarity between <code>perfume</code> and the first sentence is small, indicating that this sentence is not very relevant to our search key. The second sentence looks relevant, which means that we correctly spotted the semantic similarity.</p>
			<p>How about the third sentence? The script identified that the third sentence is relevant somehow, most probably because it includes the word <code>bottle</code> and perfumes are sold in bottles. The word <code>bottle</code> appears in similar contexts with the word <code>perfume</code>. For this reason, the similarity score of this sentence and the search key is not low enough; also, the scores <a id="_idIndexMarker343"/>of the second sentence and the <a id="_idIndexMarker344"/>third sentence are not far away enough to make the second sentence significant.</p>
			<p>There's another potential problem with comparing the key to the whole sentence. In practice, we occasionally deal with quite long texts, such as web documents. Averaging over a very long text lowers the importance of key words. </p>
			<p>To improve performance, we can extract the <em class="italic">important</em> words. Let's look at how we can spot the key phrases in a sentence. </p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor100"/>Extracting key phrases </h2>
			<p>A better way to do semantic categorization is to extract the important words/phrases and compare <a id="_idIndexMarker345"/>only them to the search key. Instead of comparing the key to the different parts of speech, we can compare the key to just the noun phrases. Noun phrases are the subjects, direct objects, and indirect objects of the sentences and carry a big percentage of the sentence's semantics on their shoulders. </p>
			<p>For example, in the sentence <em class="italic">Blue whales live in California.</em>, you'd probably like to focus on <em class="italic">blue whales</em>, <em class="italic">whales</em>, <em class="italic">California</em>, or <em class="italic">whales in California</em>. </p>
			<p>Similarly, in the preceding sentence about perfume, we focused on picking out the noun, <em class="italic">fragrance</em>. In different semantic tasks, you might need other context words such as verbs to decide what the sentence is about, but for semantic similarity, noun phrases carry most weight.</p>
			<p>What is a <a id="_idIndexMarker346"/>noun phrase, then? A <strong class="bold">noun phrase</strong> (<strong class="bold">NP</strong>) is a group of words that consist of a noun and its modifiers. Modifiers are usually pronouns, adjectives, and determiners. The following phrases are noun phrases:</p>
			<pre>A dog
My dog
My beautiful dog
A beautiful dog
A beautiful and happy dog
My happy and cute dog</pre>
			<p>spaCy extracts noun phases by parsing the output of the dependency parser. We can see the noun phrases of a sentence by using the <code>doc.noun_chunks</code> method:</p>
			<pre>doc = nlp("My beautiful and cute dog jumped over the fence")
doc.noun_chunks
&lt;generator object at 0x7fa3c529be58&gt;
list(doc.noun_chunks)
[My beautiful and cute dog, the fence]</pre>
			<p>Let's modify the <a id="_idIndexMarker347"/>preceding code snippet a bit. Instead of comparing the search key <em class="italic">perfume</em> to the entire sentence, this time, we will only compare it with the sentence's noun chunks:</p>
			<pre>for sent in sentences.sents:
          nchunks = [nchunk.text for nchunk in sent.noun_chunks]
             nchunk_doc = nlp(" ".join(nchunks)) 
             print(nchunk_doc.similarity(key))
0.21390893517254456
0.6047741393523175
0.44506391511570403</pre>
			<p>In the preceding code, we did the following: </p>
			<ol>
				<li value="1">First, we iterated over the sentences. </li>
				<li>Then, for each sentence, we extracted the noun chunks and stored them in a Python list. </li>
				<li>Next, we joined the noun chunks in the list into a Python string and converted it into a <code>Doc</code> object. </li>
				<li>Finally, we compared this <code>Doc</code> object of noun chunks to the search key <em class="italic">perfume</em> to determine their semantic similarity score. </li>
			</ol>
			<p>If we compare <a id="_idIndexMarker348"/>these scores to the previous scores, we will see that the first sentence is still irrelevant, so its score went down slightly. The second sentence's score increased significantly. Now, the second sentence's and the third sentence's scores look so far away from each other for us to confidently say that the second sentence is the most related sentence here.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor101"/>Extracting and comparing named entities</h2>
			<p>In some cases, instead of extracting every noun, we will only focus on the proper nouns; hence, we want <a id="_idIndexMarker349"/>to extract the named entities. Let's say we <a id="_idIndexMarker350"/>want to compare the following paragraphs:</p>
			<pre>"Google Search, often referred as Google, is the most popular search engine nowadays. It answers a huge volume of queries every day."
"Microsoft Bing is another popular search engine. Microsoft is known by its star product Microsoft Windows, a popular operating system sold over the world."
"The Dead Sea is the lowest lake in the world, located in the Jordan Valley of Israel. It is also the saltiest lake in the world."</pre>
			<p>Our code should be able to recognize that the first two paragraphs are about large technology companies and their products, while the third paragraph is about a geographic location. </p>
			<p>Comparing all the noun phrases in these sentences may not be very helpful because many of them, such as <code>volume</code>, aren't relevant to the categorization. The topics of these paragraphs are determined by the phrases within them; that is, <code>Google Search</code>, <code>Google</code>, <code>Microsoft Bing</code>, <code>Microsoft</code>, <code>Windows</code>, <code>Dead Sea</code>, <code>Jordan Valley</code>, and <code>Israel</code>. spaCy can spot these entities:</p>
			<pre>doc1 =  nlp("Google Search, often referred as Google, is the most popular search engine nowadays. It answers a huge volume of queries every day.")
doc2 = nlp("Microsoft Bing is another popular search engine. Microsoft is known by its star product Microsoft Windows, a popular operating system sold over the world.")
doc3 = nlp("The Dead Sea is the lowest lake in the world, located in the Jordan Valley of Israel. It is also the saltiest lake in the world.")
doc1.ents
(Google,)
doc2.ents
(Microsoft Bing, Microsoft, Microsoft, Windows)
doc3.ents
(The Dead Sea, the Jordan Valley, Israel)</pre>
			<p>Now that <a id="_idIndexMarker351"/>we have extracted the words we want to compare, let's <a id="_idIndexMarker352"/>calculate the similarity scores:</p>
			<pre>ents1 = [ent.text for ent in doc1.ents]
ents2 = [ent.text for ent in doc2.ents]
ents3 = [ent.text for ent in doc3.ents]
ents1 = nlp(" ".join(ents1))
ents2 = nlp(" ".join(ents2))
ents3 = nlp(" ".join(ents3))
ents1.similarity(ents2)
0.6078712596225045
ents1.similarity(ents3)
0.374100398233877
ents2.similarity(ents3)
0.36244710903224026</pre>
			<p>Looking at these figures, we can see that the highest level of similarity exists between the first and the second paragraph, which are both about large tech companies. The third paragraph is not really similar to the other paragraphs. How did we get this calculation by <a id="_idIndexMarker353"/>just using word vectors? Probably because <a id="_idIndexMarker354"/>the words <em class="italic">Google</em> and <em class="italic">Microsoft</em> often appear together in news and other social media text corpuses, hence creating similar word vectors.</p>
			<p>Congratulations! You've reached the end of the <em class="italic">Advanced semantic similarity methods</em> section! You explored different ways of combining word vectors with linguistic features such as key phrases and named entities. By finishing this section, we are now ready to conclude this chapter.</p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor102"/>Summary</h1>
			<p>In this chapter, you worked with word vectors, which are floating-point vectors that represent word semantics. First, you learned about the different ways to perform text vectorization, as well as how to use word vectors and distributed semantics. Then, you explored the vector operations that word vectors allow and what semantics these operations bring. </p>
			<p>You also learned how to use spaCy's built-in word vectors and how to import third-party vectors into spaCy. Finally, you learned about vector-based semantic similarity and how to blend linguistic concepts with word vectors to get the best out of these semantics. </p>
			<p>The next chapter is full of surprises – we'll look at a real-word case-based study that allows you to blend what you've learned about in the past five chapters. Let's see what spaCy can do when it comes to real-world problems!</p>
		</div>
	</div></body></html>