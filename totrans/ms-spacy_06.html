<html><head></head><body><div><div><h1 id="_idParaDest-69"><a id="_idTextAnchor069"/>Chapter <a id="_idTextAnchor070"/>4: Rule-Based Matching</h1>
			<p><strong class="bold">Rule-based information extraction</strong> is indispensable for any NLP pipeline. Certain types of entities, such as times, dates, and telephone numbers have distinct formats that can be recognized by a set of rules, without having to train statistical models. </p>
			<p>In this chapter, you will learn how to quickly extract information from the text by matching patterns and phrases. You will use <code>Matcher</code> objects. You will continue with fine-graining statistical models with rule-based matching to lift statistical models to better accuracies.</p>
			<p>By the end of this chapter, you will know a vital part of information extraction. You will be able to extract entities of specific formats, as well as entities specific to your domain.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Token-based matching</li>
				<li>PhraseMatcher</li>
				<li>EntityRuler</li>
				<li>Combining spaCy models and matchers</li>
			</ul>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor071"/>Token-based matching</h1>
			<p>So far, we've explored the sophisticated linguistic concepts that require statistical models and <a id="_idIndexMarker230"/>their usages with spaCy. Some NLU tasks can be solved in tricky ways without the help <a id="_idIndexMarker231"/>of any statistical model. One of those ways is <strong class="bold">regex</strong>, which we use to match a predefined set of patterns to our text.</p>
			<p>A regex (a regular expression) is a sequence of characters that specifies a search pattern. A regex describes a set of strings that follows the specified pattern. A regex can include letters, digits, and characters with special meanings, such as <em class="italic">?</em>, <em class="italic">.</em>, and <em class="italic">*</em>. Python's built-in library provides great support to define and match regular expressions. There's another Python 3 library called regex that aims wants to replace <strong class="bold">re</strong> in the future.</p>
			<p>Readers who are actively developing NLP applications with Python have definitely come across regex code and, even better, have written regex themselves.</p>
			<p>What does a regex look like, then? The following regex matches the following strings:</p>
			<ul>
				<li>Barack Obama</li>
				<li>Barack Obama</li>
				<li>Barack Hussein Obama</li>
			</ul>
			<pre>reg = r"Barack\s(Hussein\s)?Obama"  </pre>
			<p>This pattern can be read as: the string <code>Barack</code> can be followed optionally by the string <code>Hussein</code> (the <code>? </code>character in regex means optional, that is, <code>0</code> or <code>1</code> occurrence) and should be followed by the string <code>Obama</code>. The inter-word spaces can be a single space character, a tab, or any other whitespace character (<code>\s</code> matches all sorts of whitespace characters, including the newline character).</p>
			<p>It's not very <a id="_idIndexMarker232"/>readable, even for such a short and uncomplicated pattern, is it? That is the downside of regex, it is the following:</p>
			<ul>
				<li>Difficult to read</li>
				<li>Difficult to debug</li>
				<li>Error prone with space, punctuation, and number characters</li>
			</ul>
			<p>For these reasons, many software engineers don't like to work with regex in their production code. spaCy provides a very clean, readable, production-level, and maintainable alternative: the <code>Matcher</code> class. The <code>Matcher</code> class can match our predefined rules to the sequence of tokens in <code>Doc</code> and <code>Span</code> objects; moreover, the rules can refer to the token or its linguistic attributes (more on this subject later in this section).</p>
			<p>Let's start with a basic example of how to call the <code>Matcher</code> class:</p>
			<pre> import spacy
 from spacy.matcher import Matcher
 nlp = spacy.load("en")
 doc = nlp("Good morning, I want to reserve a ticket.")
 matcher = Matcher(nlp.vocab)
 pattern = [{"LOWER": "good"}, {"LOWER": "morning"}, 
            {"IS_PUNCT": True}]
 matcher.add("morningGreeting", None, pattern)
 matches = matcher(doc)
 for match_id, start, end in matches:
     m_span = doc[start:end]  
     print(start, end, m_span.text)
...
0 3 Good morning,</pre>
			<p>It looks <a id="_idIndexMarker233"/>complicated, but don't be intimidated, we'll go over the lines one by one:</p>
			<ul>
				<li>We imported <code>spacy</code> in the first line; this should be familiar.</li>
				<li>On the second line, we imported the <code>Matcher</code> class in order to use it in the rest of the code.</li>
				<li>On the next lines, we created the <code>nlp</code> object as usual and created the <code>doc</code> object with our example sentence.</li>
				<li>Now, pay attention: a <code>matcher</code> object needs to be initialized with a <code>Vocabulary</code> object, so on line 5 we initialize our <code>matcher</code> object with the language model's vocabulary (this is the usual way to do it). </li>
				<li>What comes next is to define the pattern we want to match. Here, we define <em class="italic">pattern</em> as a list where every list item enclosed in a bracelet represents one token object. </li>
			</ul>
			<p>You can read the pattern list in the preceding code snippet as follows:</p>
			<ol>
				<li>A token whose lowered text is <code>good</code></li>
				<li>A token whose lowered text is <code>morning</code></li>
				<li>A token that is punctuation (that is, the <code>IS_PUNCT</code> feature is <code>True</code>)</li>
			</ol>
			<p>Then, we need to introduce this pattern to the <code>matcher</code>; this is what the <code>matcher.add()</code> line does. On line 7, we introduced our pattern to the <code>matcher</code> object and named this rule <code>morningGreeting</code>. Finally, we can do the matching operation on line 8 by calling <code>matcher</code> on the <code>doc</code>. After that, we examine the result we get. A match result is a list of triplets in the form <code>(match id, start position, end position)</code>. On the final line, we iterate over the result list and print the result match's start position, end position, and text.</p>
			<p>As you might <a id="_idIndexMarker234"/>have noticed, the whitespace between <code>Good</code> and <code>morning</code> didn't matter at all. Indeed, we could have put two whitespaces in between, written down <code>Good morning</code>, and the result would be identical. Why? Because <code>Matcher</code> matches the tokens and the token attributes. </p>
			<p>A pattern always refers to a continuous sequence of token objects, and every item in bracelets corresponds to one token object. Let's go back to the pattern in the preceding code snippet:</p>
			<pre>pattern = [{"LOWER": "good"}, {"LOWER": "morning"}, 
           {"IS_PUNCT": True}]</pre>
			<p>We see that the result is always a three-token match. </p>
			<p>Can we add more than one pattern? The answer is yes. Let's see it with an example and also see an example of <code>match_id</code> as follows:</p>
			<pre> import spacy
 from spacy.matcher import Matcher
 nlp = spacy.load("en")
 doc = nlp("Good morning, I want to reserve a ticket. I will then say good evening!")
 matcher = Matcher(nlp.vocab)
 pattern1 = [{"LOWER": "good"}, {"LOWER": "morning"}, 
             {"IS_PUNCT": True}]
 matcher.add("morningGreeting",  [pattern1])
 pattern2 = [{"LOWER": "good"}, {"LOWER": "evening"}, 
             {"IS_PUNCT": True}]
 matcher.add("eveningGreeting",  [pattern2])
 matches = matcher(doc)
 for match_id, start, end in matches:
     pattern_name = nlp.vocab_strings[match_id]
     m_span = doc[start:end]  
     print(pattern_name, start, end, m_span.text)
...
morningGreeting 0 3 Good morning,
eveningGreeting 15 18 good evening!</pre>
			<p>This time <a id="_idIndexMarker235"/>we did things a bit differently:</p>
			<ul>
				<li>On line 8, we defined a second pattern, again matching three tokens, but this time <code>evening</code> instead of <code>morning</code>.</li>
				<li>On the next line, we added it to the <code>matcher</code>. At this point, <code>matcher</code> contains 2 patterns: <code>morningGreeting</code> and <code>eveningGreeting</code>. </li>
				<li>Again, we called the <code>matcher</code> on our sentence and examined the result. This time the results list has two items, <code>Good morning</code>, and <code>good evening!</code>, corresponding to two different patterns, <code>morningGreeting</code> and <code>eveningGreeting</code>.</li>
			</ul>
			<p>In the preceding code example, <code>pattern1</code> and <code>pattern2</code> differ only by one token: <code>evening/morning</code>. Instead of writing two patterns, can we say <code>evening</code> or <code>morning</code>? We can do that as well. Here are the attributes that Matcher recognizes: </p>
			<div><div><img src="img/B16570_4_01.jpg" alt="Figure 4.1 – Token attributes for Matcher&#13;&#10;" width="1161" height="697"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1 – Token attributes for Matcher</p>
			<p>Let's go over the attributes one by one with some examples. We used <code>LOWER</code> in the preceding examples; it means the <em class="italic">lowercase form of the token text</em>. <code>ORTH</code> and <code>TEXT</code> are similar <a id="_idIndexMarker236"/>to <code>LOWER</code>: they mean an exact match of the token text, including the case. Here's an example:</p>
			<pre> pattern = [{"TEXT": "Bill"}]</pre>
			<p>The preceding code will match <code>BIll</code>, but not <code>bill</code>. <code>LENGTH</code> is used for specifying the token length. The following code finds all tokens of length <code>1</code>:</p>
			<pre> doc = nlp("I bought a pineapple.")
 matcher = Matcher(nlp.vocabulary)
 pattern = [{"LENGTH": 1}]
 matcher.add("onlyShort",  [pattern])
 matches = matcher(doc)
 for mid, start, end in matches:
     print(start, end, doc[start:end])
...
0 1 I
2 3 a</pre>
			<p>The next block of token attributes is <code>IS_ALPHA</code>, <code>IS_ASCII</code>, and <code>IS_DIGIT</code>. These features <a id="_idIndexMarker237"/>are handy for finding number tokens and <em class="italic">ordinary</em> words (which do not include any interesting characters). The following pattern matches a sequence of two tokens, a number followed by an ordinary word:</p>
			<pre> doc1 = nlp("I met him at 2 o'clock.")
 doc2 = nlp("He brought me 2 apples.")
 pattern = [{"IS_DIGIT": True},{"IS_ALPHA": True}] 
 matcher.add("numberAndPlainWord",  [pattern])
 matcher(doc1)
[]
 matches = matcher(doc2)
 len(matches)
1
 mid, start, end = matches[0]
 print(start, end, doc2[start:end])
3, 5, 2 apples</pre>
			<p>In the preceding code segment, <code>2 o'clock</code> didn't match the pattern because <code>o'clock</code> contains an apostrophe, which is not an alphabetic character (alphabetic characters are digits, letters, and the underscore character). <code>2 apples</code> matched because the token <code>apples</code> consists of letters.</p>
			<p><code>IS_LOWER</code>, <code>IS_UPPER</code>, and <code>IS_TITLE</code> are useful attributes for recognizing the token's casing. <code>IS_UPPER</code> is <code>True</code> if the token is all uppercase letters and <code>IS_TITLE</code> is <code>True</code> if the token starts with a capital letter. <code>IS_LOWER</code> is <code>True</code> if the token is all lowercase letters. Imagine we want to find emphasized words in a text; one way is to look for the tokens with all uppercase letters. The uppercase tokens usually have significant weights in sentiment analysis models.</p>
			<pre> doc = nlp("Take me out of your SPAM list. We never asked you to contact me. If you write again we'll SUE!!!!")
 pattern = [{"IS_UPPER": True}]
 matcher.add("capitals",  [pattern])
 matches = matcher(doc)
 for mid, start, end in matches:
     print(start, end, doc[start:end])
...
5, 6, SPAM
22, 23, SUE</pre>
			<p><code>IS_PUNCT</code>, <code>IS_SPACE</code>, and <code>IS_STOP</code> are usually used in patterns that include some helper tokens <a id="_idIndexMarker238"/>and correspond to punctuation, space, and <code>IS_SENT_START</code> is another useful attribute; it matches sentence start tokens. Here's a pattern for sentences that start with <em class="italic">can</em> and the second word has a capitalized first letter:</p>
			<pre> doc1 = nlp("Can you swim?")
 doc2 = nlp("Can Sally swim?")
 pattern = [{"IS_SENT_START": True, "LOWER": "can"},
            {"IS_TITLE": True}]
 matcher.add("canThenCapitalized",  [pattern])
 matcher(doc)
[]
 matches = matcher(doc2)
 len(matches)
1
 mid, start, end = matches[0]
 print(start, end, doc2[start:end])
0, 2, Can Sally</pre>
			<p>Here, we did a different thing: we put two attributes into one brace. In this example, the first item in <code>pattern</code> means that a token that is the first token of the sentence and whose lowered text is <em class="italic">can</em>. We can add as many attributes as we like. For instance, <code>{"IS_SENT_START": False, "IS_TITLE": True, "LOWER": "bill"}</code> is a completely valid attribute dictionary, and it describes a token that is capitalized, not the first token of sentence, and has the text <code>bill</code>. So, it is the set of <code>Bill</code> instances that does not appear as the first word of a sentence.</p>
			<p><code>LIKE_NUM</code>, <code>LIKE_URL</code>, and <code>LIKE_EMAIL</code> are attributes that are related to token shape again; remember, we saw them in <a href="B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055"><em class="italic">Chapter 3</em></a><em class="italic">, Linguistic Features</em>. These attributes match tokens that look like numbers, URLs, and emails. </p>
			<p>Though the <a id="_idIndexMarker239"/>preceding code looks short and simple, the shape attributes can be lifesavers in NLU applications. Most of the time you need nothing other than clever combinations of shape and linguistic attributes.</p>
			<p>After seeing the shape attributes, let's see the <code>POS</code>, <code>TAG</code>, <code>DEP</code>, <code>LEMMA</code>, and <code>SHAPE</code> linguistic attributes. You saw these token attributes in the previous chapter; now we'll use them in token matching. The following code snippet spots sentences that start with an auxiliary verb:</p>
			<pre> doc = nlp("Will you go there?')
 pattern = [{"IS_SENT_START": True, "TAG": "MD"}]
 matcher.add([pattern])
 matches = matcher(doc)
 len(matches)
1
 mid, start, end = matches[0]
 print(start, end, doc[start:end])
0, 1, Will
 doc2 = nlp("I might go there.")
 matcher(doc2)
[]</pre>
			<p>You may recall from <a href="B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055"><em class="italic">Chapter 3</em></a><em class="italic">, Linguistic Features</em>, that <code>MD</code> is the tag for modal and auxiliary verbs. The <a id="_idIndexMarker240"/>preceding code snippet is a standard way of finding yes/no question sentences. In such cases, we usually look for sentences that start with a modal or an auxiliary verb.</p>
			<p class="callout-heading">Pro tip</p>
			<p class="callout">Don't be afraid to work with <code>TEXT</code>/<code>LEMMA</code> with <code>POS</code>/<code>TAG</code>. For instance, the word <em class="italic">match</em> is <em class="italic">to go together</em> when it's a verb or it can be a <em class="italic">fire starter tool</em> when it's a noun. In this case, we make the distinction as follows:</p>
			<p class="callout"><code>{"LEMMA": "match", "POS": "VERB"}</code> and</p>
			<p class="callout"><code>{"LEMMA": "match", "POS": "NOUN".</code></p>
			<p class="callout">Similarly, you can combine other linguistic features with token shape attributes to make sure that you extract only the pattern you mean to.</p>
			<p>We'll see more examples of combining linguistic features with the <code>Matcher</code> class in the upcoming sections. Now, we'll explore more Matcher features.</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor072"/>Extended syntax support</h2>
			<p>Matcher allows <a id="_idIndexMarker241"/>patterns to be more expressive by allowing some operators inside the curly brackets. These operators are for extended comparison and look similar to Python's <code>in</code>, <code>not in</code>, and comparison operators. Here's the list of the operators:</p>
			<div><div><img src="img/B16570_4_02.jpg" alt="Fig 4.2 – List of rich comparison operators&#13;&#10;" width="1184" height="255"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – List of rich comparison operators</p>
			<p>In our very first example, we matched <code>good evening</code> and <code>good morning</code> with two different <a id="_idIndexMarker242"/>patterns. Now, we can match <code>good morning</code>/<code>evening</code> with one pattern with the help of <code>IN</code> as follows:</p>
			<pre> doc = nlp("Good morning, I'm here. I'll say good evening!!")
 pattern = [{"LOWER": "good"},
            {"LOWER": {"IN": ["morning", "evening"]}},
            {"IS_PUNCT": True}]
 matcher.add("greetings",  [pattern])
 matches = matcher(doc)
 for mid, start, end in matches:
     print(start, end, doc[start:end])
...
0, 3, Good morning,
10, 13, good evening!</pre>
			<p>Comparison operators usually go together with the <code>LENGTH</code> attribute. Here's an example of finding long tokens:</p>
			<pre> doc = nlp("I suffered from Trichotillomania when I was in college. The doctor prescribed me Psychosomatic medicine.")
 pattern = [{"LENGTH": {"&gt;=" : 10}}]
 matcher.add("longWords",  [pattern])
 matches = matcher(doc)
 for mid, start, end in matches:
     print(start, end, doc[start:end])
...
3, 4, Trichotillomania
14, 15, Psychosomatic</pre>
			<p>They were <a id="_idIndexMarker243"/>fun words to process! Now, we'll move onto another very practical feature of Matcher patterns, regex-like operators. </p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor073"/>Regex-like operators</h2>
			<p>At the beginning of the chapter, we pointed out that spaCy's <code>Matcher</code> class offers a cleaner <a id="_idIndexMarker244"/>and more readable equivalent to regex operations, indeed much cleaner and much more readable. The most common regex operations are optional match (<code>?</code>), match at least once (<code>+</code>), and match 0 or more times (<code>*</code>). spaCy's Matcher also offers these operators by using the following syntax:</p>
			<div><div><img src="img/B16570_4_03.jpg" alt="Fig 4.3 – OP key description&#13;&#10;" width="679" height="303"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3 – OP key description</p>
			<p>The very first example regex of this chapter was matching Barack Obama's first name, with the middle name being optional. The regex was as follows:</p>
			<pre>R"Barack\s(Hussein\s)?Obama</pre>
			<p>The <code>?</code> operator after <code>Hussein</code> means the pattern in the brackets is optional, hence this regex matches both <code>Barack Obama</code> and <code>Barack Hussein Obama</code>. We use the <code>?</code> operator in a Matcher pattern as follows:</p>
			<pre> doc1 = nlp("Barack Obama visited France.")
 doc2 = nlp("Barack Hussein Obama visited France.")
 pattern = [{"LOWER": "barack"},
            {"LOWER": "hussein", "OP": "?"},
            {"LOWER": "obama"}]
 matcher.add("obamaNames",  [pattern])
 matcher(doc1)
[(1881848298847208418, 0, 2)]
 matcher(doc2)
[(1881848298847208418, 0, 3)]</pre>
			<p>Here, by using the <code>"OP": "?"</code> in the second list item, we made this token optional. The <code>matcher</code> picked <code>Barack Obama</code> in the first doc object and <code>Barack Hussein Obama</code> in the second one as a result.</p>
			<p>We previously <a id="_idIndexMarker245"/>pointed that the <code>+</code> and <code>*</code> operators have the same meaning as their regex counterparts. <code>+</code> means the token should occur at least once and <code>*</code> means the token can occur 0 or more times. Let's see some examples:</p>
			<pre> doc1 = nlp("Hello hello hello, how are you?")
 doc2 = nlp("Hello, how are you?")
 doc3 = nlp("How are you?")
 pattern = [{"LOWER": {"IN": ["hello", "hi", "hallo"]},
             “OP”:”*”, {"IS_PUNCT": True}]
 matcher.add("greetings",  [pattern])
 for mid, start, end in matcher(doc1):
     print(start, end, doc1[start:end])
... 
2, 4, hello,
1, 4, hello hello,
0, 4, Hello hello hello,
 for mid, start, end in matcher(doc2):
     print(start, end, doc2[start:end])
... 
0 2 Hello, 
matcher(doc3)
...
[]</pre>
			<p>Here's <a id="_idIndexMarker246"/>what happened: </p>
			<ul>
				<li>In the pattern, the first token reads as <em class="italic">any one of hello, hi, hallo should occur 1 or more times</em> and the second token is punctuation. </li>
				<li>The third <code>doc</code> object does not match at all; there's no greeting word.</li>
				<li> The second <code>doc</code> object matches <code>hello,</code>.</li>
			</ul>
			<p>When we come to the results of the first <code>doc</code> objects' matches, we see that there are not one, but three distinct matches. This is completely normal because there are indeed three sequences matching the pattern. If you have a closer look at the match results, all of them match the pattern we created, because <code>hello</code>, <code>hello hello</code>, and <code>hello hello hello</code> all match the <code>(hello)+</code> pattern.</p>
			<p>Let's do the same pattern with <code>*</code> and see what happens this time:</p>
			<pre>doc1 = nlp("Hello hello hello, how are you?")
doc2 = nlp("Hello, how are you?")doc3 = nlp("How are you?")
pattern = [{"LOWER": {"IN": ["hello", "hi", "hallo"]},
            "OP": "+"}, {"IS_PUNCT": True}]
matcher.add("greetings",  [pattern])
for mid, start, end in matcher(doc1):
     print(start, end, doc1[start:end])
...
(0, 4, Hello hello hello,)
(1, 4, hello hello,)
(2, 4, hello,)
(3, 4, ,)
(7, 8, ?)
for mid, start, end in matcher(doc2):
     start, end, doc2[start:end]
... 
(0, 2, hello,)
(1, 2, ,)
(5, 6, ?)
for mid, start, end in matcher(doc3):
     start, end, doc3[start:end]
... 
(3, 4, ?)</pre>
			<p>In the <a id="_idIndexMarker247"/>first <code>doc</code> object's matches, there are two extra items: <code>""</code> and <code>?</code>. The <code>"*"</code> operator matches <code>0</code> or more, so our <code>(hello)*punct_character</code> pattern grabs <code>""</code> and <code>?</code>. The same applies to the second and third documents: punctuation marks alone without any greeting word are picked. This is not what you want in your NLP applications, probably.</p>
			<p>The preceding example is a good example that we should be careful of while creating our patterns; sometimes, we get unwanted matches. For this reason, we usually consider using <code>IS_SENT_START</code> and take care with <code>"*"</code> operator.</p>
			<p>The spaCy Matcher class also accepts a very special pattern, a <strong class="bold">wildcard</strong> token pattern. A wildcard token will match any token. We usually use it for the words we want to pick independent from their text or attributes or for the words we ignore. Let's see an example:</p>
			<pre>doc = nlp("My name is Alice and his name was Elliot.")
pattern = [{"LOWER": "name"},{"LEMMA": "be"},{}]
matcher.add("pickName", [pattern])
for mid, start, end in matcher(doc):
     print(start, end, doc[start:end])
... 
1 4 name is Alice
6 9 name was Elliot</pre>
			<p>Here, we wanted <a id="_idIndexMarker248"/>to capture the first names in the sentence. We achieved it by parsing out token sequences in the form <em class="italic">name is/was/be firstname</em>. The first token pattern, <code>LOWER:</code> <code>"name"</code>, matches the tokens whose lowered text is <code>name</code>. The second token pattern, <code>LEMMA: "be"</code>, matches the <code>is</code>, <code>was</code>, and <code>be</code> tokens. The third token is the wildcard token, <code>{}</code>, which means <em class="italic">any</em> token. We pick up any token that comes after <em class="italic">name is/was/be</em> with this pattern.</p>
			<p>We also use a wildcard token when we want to ignore a token. Let's make an example together:</p>
			<pre>doc1 = nlp("I forwarded his email to you.")
doc2 = nlp("I forwarded an email to you.")
doc3 = nlp("I forwarded the email to you.")
pattern = [{"LEMMA": "forward"}, {}, {"LOWER": "email"}]
matcher.add("forwardMail",  [pattern])
for mid, start, end in matcher(doc1):
     print(start, end, doc1[start:end])
... 
1 4 forwarded his email
for mid, start, end in matcher(doc2):
     print(start, end, doc2[start:end])
... 
1 4 forwarded an email
for mid, start, end in matcher(doc3):
.    print(start, end, doc3[start:end])
... 
1 4 forwarded the email</pre>
			<p>It's just the <a id="_idIndexMarker249"/>opposite of the previous example. Here, we wanted to pick up <em class="italic">forward email</em> sequences, and we allowed that one token to come between <code>forward</code> and <code>email</code>. Here, the semantically important part is the forwarding an email action; whose email is it doesn't matter much.</p>
			<p>We have mentioned regex quite a lot in this chapter so far, so now it's time to see how spaCy's Matcher class makes use of regex syntax.</p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor074"/>Regex support</h2>
			<p>When we <a id="_idIndexMarker250"/>match individual tokens, usually we want to allow some variations, such as common typos, UK/US English character differences, and so on. Regex is very handy for this task and spaCy Matcher offers full support for token-level regex matching. Let's explore how we can use regex for our applications:</p>
			<pre>doc1 = nlp("I travelled by bus.")
doc2 = nlp("She traveled by bike.")
pattern = [{"POS": "PRON"}, 
           {"TEXT": {"REGEX": "[Tt]ravell?ed"}}]
for mid, start, end in matcher(doc1):
     print(start, end, doc1[start:end])
... 
0 2 I traveled
for mid, start, end in matcher(doc2):
     print(start, end, doc2[start:end])
... 
0 2 I travelled</pre>
			<p>Here, our second token pattern is <code>[Tt]ravell?ed</code>, which means the token can be capitalized or not. Also, there's an optional <code>l</code> after the first <code>l</code>. Allowing twin vowels and <em class="italic">ise/ize</em> alteration is a standard way of dealing with British and American English variations.</p>
			<p>Another way <a id="_idIndexMarker251"/>of using regex is using it not only with text, but also with <code>POS</code> tags. What does the following code segment do?</p>
			<pre>doc = nlp("I went to Italy; he has been there too. His mother also has told me she wants to visit Rome.")
pattern = [{"TAG": {"REGEX": "^V"}}] 
matcher.add("verbs",  [pattern])
for mid, start, end in matcher(doc):
    print(start, end, doc1[start:end])
... 
1 2 went
6 7 has
7 8 been
14 15 has
15 16 told
18 19 wants
20 21 visit</pre>
			<p>We have extracted all the finite verbs (you can think of a finite verb as a non-modal verb). How did we do it? Our token pattern includes the regex <code>^V</code>, which means all fine-grained POS tags that start with <code>V</code>: <code>VB</code>, <code>VGD</code>, <code>VBG</code>, <code>VBN</code>, <code>VBP</code>, and <code>VBZ</code>. Then we extracted <a id="_idIndexMarker252"/>tokens with verbal POS tags.</p>
			<p>Looks tricky! Occasionally we use some tricks in NLU applications; while going through the examples of this book, you'll pick them up too. We encourage you to go over our examples and then try some example sentences of yours.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor075"/>Matcher online demo</h2>
			<p>In the whole matching business, we occasionally see the match results visually. Regex offers <em class="italic">regex101</em> (<a href="https://regex101.com/">https://regex101.com/</a>), an online tool for checking if your regex pattern is <a id="_idIndexMarker253"/>working correctly (surprises with regex always happen). The following figure shows an example pattern and checking it against a text:</p>
			<div><div><img src="img/B16570_4_04.jpg" alt="Fig 4.4 – An example regex match and pattern explanations&#13;&#10;" width="1145" height="488"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4 – An example regex match and pattern explanations</p>
			<p>The explanations on the right side are quite detailed and illuminating. This is a tool used not only by NLP learners/beginners, but also professionals (regex can be quite difficult to read sometimes).</p>
			<p>spaCy Matcher offers <a id="_idIndexMarker254"/>a similar tool on its online demo page (<a href="https://explosion.ai/demos/matcher">https://explosion.ai/demos/matcher</a>). We can create patterns and test them against the text we want, interactively.</p>
			<p>In the following screenshot, we can see a match example. On the right side we can select the attributes, values, and operators (such as +, *, !, and ?). After making this selection, the demo <a id="_idIndexMarker255"/>outputs the corresponding pattern string on the right side, below the checkboxes. On the left side, we first choose the spaCy language model we want (in this example, English core small), then see the results:</p>
			<div><div><img src="img/B16570_4_05.jpg" alt="Fig 4.5 – spaCy Matcher online demo&#13;&#10;" width="1009" height="760"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5 – spaCy Matcher online demo</p>
			<p>Just like regex101, spaCy's Matcher demo helps you to see why your pattern matched or didn't match.</p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor076"/>PhraseMatcher</h1>
			<p>While processing financial, medical, or legal text, often we have long lists and dictionaries <a id="_idIndexMarker256"/>and we want to scan the text against our lists. As we saw in the previous section, Matcher patterns are quite handcrafted; we coded each token individually. If you have a long list of phrases, Matcher is not very handy. It's not possible to code all the terms one by one.</p>
			<p>spaCy offers a solution for comparing text against long dictionaries – the <code>PhraseMatcher</code> class. The <code>PhraseMatcher</code> class helps us match long dictionaries. Let's get started with an example:</p>
			<pre>import spacy
from spacy.matcher import PhraseMatcher
nlp = spacy.load("en_core_web_md")
matcher = PhraseMatcher(nlp.vocab)
terms = ["Angela Merkel", "Donald Trump", "Alexis Tsipras"]
patterns = [nlp.make_doc(term) for term in terms]
matcher.add("politiciansList", None, *patterns)
doc = nlp("3 EU leaders met in Berlin. German chancellor Angela Merkel first welcomed the US president Donald Trump. The following day Alexis Tsipras joined them in Brandenburg.")
matches = matcher(doc)
for mid, start, end in matches:
    print(start, end, doc[start:end])
…
9 11 Angela Merkel
16 18 Donald Trump
22 24 Alexis Tsipras</pre>
			<p>Here's what we did:</p>
			<ul>
				<li>First, we imported <code>spacy</code>, then we imported the <code>PhraseMatcher</code> class.</li>
				<li>After the imports, we created a <code>Language</code> object, <code>nlp</code>, and initialized a <code>PhraseMatcher</code> object, <code>matcher</code>, with its vocabulary. </li>
				<li>The next two lines are where we created the pattern list.</li>
				<li>On line 6, we called <code>nlp.make_doc()</code> on the terms one by one to create the patterns.</li>
				<li><code>make_doc()</code> creates a Doc from every term, and it's quite efficient in terms of processing because instead of the whole pipeline, it only calls the <code>Tokenizer</code>. </li>
				<li>The rest of the code is similar to what we did with Matcher: we iterated over the resulting spans.</li>
			</ul>
			<p>This way, we match <a id="_idIndexMarker257"/>the pattern by their exact text values. What if we want to match them with other attributes? Here's an example of matching by the <code>LOWER</code> attribute:</p>
			<pre>matcher = PhraseMatcher(nlp.vocab, attr="LOWER")
terms = ["Asset", "Investment", "Derivatives", 
         "Demand",  "Market"]
patterns = [nlp.make_doc(term) for term in terms]
matcher.add("financeTerms", None, *patterns)
doc = nlp("During the last decade, derivatives market became an asset class of their own and influenced the financial landscape strongly.")
matches = matcher(doc)
for mid, start, end in matches:
    print(start, end, doc[start:end])
…
5 6 derivatives
6 7 market</pre>
			<p>On line 1, while creating a <code>PhraseMatcher</code> instance, we passed an additional argument, <code>attr=LOWER</code>. This way, the <code>PhraseMatcher</code> used the <code>token.lower</code> attribute during the match. Notice that the terms are uppercase and the matches are lowercase.</p>
			<p>Another possible usage of PhraseMatcher is matching the <code>SHAPE</code> attribute. This matching strategy can be used on system logs, where IP numbers, dates, and other numerical values occur a lot. The good thing here is that you do not need to worry how the numbers <a id="_idIndexMarker258"/>are tokenized, you just leave it to <code>PhraseMatcher</code>. Let's see an example:</p>
			<pre>matcher = PhraseMatcher(nlp.vocab, attr="SHAPE")
ip_nums = ["127.0.0.0", "127.256.0.0"]
patterns = [nlp.make_doc(ip) for ip in ip_nums]
matcher.add("IPNums", None, *pattern)
doc = nlp("This log contains the following IP addresses: 192.1.1.1 and 192.12.1.1 and 192.160.1.1 .")
for mid, start, end in matcher(doc):
    print(start, end, doc[start:end])
8 9 192.1.1.1
12 13 192.160.1.1</pre>
			<p>That's it! We matched the tokens and phrases successfully; what's left is named entities. Named entity <a id="_idIndexMarker259"/>extraction is an essential component of any NLP system and most of the pipelines you'll design will include an <strong class="bold">named entity recognition</strong> (<strong class="bold">NER</strong>) component. The next section is devoted to rule-based named entity extraction.</p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor077"/>EntityRuler</h1>
			<p>While covering <a id="_idIndexMarker260"/>Matcher, we saw that we can extract named entities with Matcher by using the <code>ENT_TYPE</code> attribute. We recall from the previous chapter that <code>ENT_TYPE</code> is a linguistic attribute that refers to the entity type of the token, such as person, place, or organization. Let's see an example:</p>
			<pre>pattern = [{"ENT_TYPE": "PERSON"}]
matcher.add("personEnt",  [pattern])
doc = nlp("Bill Gates visited Berlin.")
matches = matcher(doc)
for mid, start, end in matches:
    print(start, end, doc[start:end])
... 
0 1 Bill
1 2 Gates</pre>
			<p>Again, we created a <code>Matcher</code> object called <code>matcher</code> and called it on the <code>Doc</code> object, <code>doc</code>. The result is two tokens, <code>Bill</code> and <code>Gates</code>; Matcher always matches at the token level. We got <code>Bill</code> and <code>Gates</code>, instead of the full entity, <code>Bill Gates</code>. If you want to get the full entity <a id="_idIndexMarker261"/>rather than the individual tokens, you can do this:</p>
			<pre>pattern = [{"ENT_TYPE": "PERSON", "OP": "+"}]
matcher.add("personEnt",  [pattern])
doc = nlp("Bill Gates visited Berlin.")
matches = matcher(doc)
for mid, start, end in matches:
    print(start, end, doc[start:end])
... 
0 1 Bill
1 2 Gates
0 2 Bill Gates</pre>
			<p>Usually, we match two or more entities together, or with other linguistic attributes to extract information. Here's an example of how we can understand the action in the sentence and which person in the sentence committed this action:</p>
			<pre>pattern = [{"ENT_TYPE": "PERSON", "OP": "+"}, {
            "POS" : "VERB"}]
matcher.add("personEntAction",  [pattern])
doc = nlp("Today German chancellor Angela Merkel met with the US president.")
matches = matcher(doc)
for mid, start, end in matches:
    print(start, end, doc[start:end])
... 
4 6 Merkel met
3 6 Angela Merkel met</pre>
			<p>We noticed that <a id="_idIndexMarker262"/>the Matcher returns two matches here; usually, we loop through the results and pick the longest match.</p>
			<p>In the preceding examples, we matched the entities that the spaCy statistical model already extracted. What if we have domain-specific entities that we want to match? For instance, our dataset consists of wiki pages about ancient Greek philosophers. The philosopher names are naturally in Greek and don't follow English statistical patterns; it's expected that a tagger trained on English text would fail to extract the entity name occasionally. In these situations, we'd like spaCy to tell our entities and combine them with the statistical rules.</p>
			<p>spaCy's <code>EntityRuler</code> is the component that allows us to add rules on top of the statistical model and <a id="_idIndexMarker263"/>creates an even more powerful <strong class="bold">NER</strong> model.</p>
			<p><code>EntityRuler</code> is not a matcher, it's a pipeline component that we can add to our pipeline via <code>nlp.add_pipe</code>. When it finds a match, the match is appended to <code>doc.ents</code> and <code>ent_type</code> will be the label we pass in the pattern. Let's see it in action:</p>
			<pre>doc = nlp("I have an acccount with chime since 2017")
doc.ents
(2017,)
patterns = [{"label": "ORG", 
             "pattern": [{"LOWER": "chime"}]}]
ruler = nlp.add_pipe("entity_ruler")
ruler.add_patterns(patterns)
doc.ents
(chime, 2017)
doc[5].ent_type_
'ORG'</pre>
			<p>That's it, really easy, yet powerful! We added our own entity with just a couple of lines. </p>
			<p>The <code>Matcher</code> class and <code>EntityRuler</code> are exciting and powerful features of the spaCy library, as we <a id="_idIndexMarker264"/>saw from the examples. Now, we move onto an exclusive section of quick and very handy recipes.</p>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor078"/>Combining spaCy models and matchers</h1>
			<p>In this section, we'll go through some recipes that will guide you through the entity extraction <a id="_idIndexMarker265"/>types you'll encounter in your NLP career. All the examples are ready-to-use and real-world recipes. Let's start with number-formatted entities.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor079"/>Extracting IBAN and account numbers</h2>
			<p>IBAN <a id="_idIndexMarker266"/>and account numbers are two important entity types that occur in finance and banking frequently. We'll learn how to parse them out. </p>
			<p>An IBAN is an international number format for bank account numbers. It has the format of a two-digit country code followed by numbers. Here are some IBANs from different countries:</p>
			<div><div><img src="img/B16570_4_06.jpg" alt="" width="514" height="472"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6 – IBAN formats from different countries (source: Wikipedia)</p>
			<p>How can <a id="_idIndexMarker267"/>we create a pattern for an IBAN? Obviously, in all cases, we start with two capital letters, followed by two digits. Then any number of digits can follow. We can express the country code and the next two digits as follows:</p>
			<pre>{"SHAPE": "XXdd"}</pre>
			<p>Here, <code>XX</code> corresponds to two capital letters and <code>dd</code> is two digits. Then <code>XXdd</code> pattern matches the first block of the IBAN perfectly. How about the rest of the digit blocks? For the rest of the blocks, we need to match a block of 1-4 digits. The regex <code>\d{1,4}</code> means a token consisting of 1 to 4 digits. This pattern will match a digit block:</p>
			<pre>{"TEXT": {"REGEX": "\d{1,4}"}}</pre>
			<p>We have a number of these blocks, so the pattern to match the digit blocks of an IBAN is as follows:</p>
			<pre>{"TEXT": {"REGEX": "\d{1,4}"}, "OP": "+"}</pre>
			<p>Then, we <a id="_idIndexMarker268"/>combine the first block with the rest of the blocks. Let's see the code and the matches:</p>
			<pre>doc = nlp("My IBAN number is BE71 0961 2345 6769, please send the money there.")
doc1 = nlp("My IBAN number is FR76 3000 6000 0112 3456 7890 189, please send the money there.")
pattern = [{"SHAPE": "XXdd"}, 
           {"TEXT": {"REGEX": "\d{1,4}"}, "OP":"+"}]
matcher = Matcher(nlp.vocab)
matcher.add("ibanNum",  [pattern])
for mid, start, end in matcher(doc):
    print(start, end, doc[start:end])
... 
4 6 BE71 0961
4 7 BE71 0961 2345
4 8 BE71 0961 2345 6769
for mid, start, end in matcher(doc1):
    print(start, end, doc1[start:end])
... 
4 6 FR76 3000
4 7 FR76 3000 6000
4 8 FR76 3000 6000 0112
4 9 FR76 3000 6000 0112 3456
4 10 FR76 3000 6000 0112 3456 7890
4 11 FR76 3000 6000 0112 3456 7890 189</pre>
			<p>You can always follow a similar strategy when parsing numeric entities: first, divide the entity into some meaningful parts/blocks, then try to determine the shape or the length of the individual blocks.</p>
			<p>We successfully parsed IBANs, now we can parse the account numbers. Parsing the account numbers is a bit trickier; account numbers are just plain numbers and don't have a <a id="_idIndexMarker269"/>special shape to help us differentiate them from usual numbers. What do we do, then? We can make a context lookup in this case; we can look around the number token and see if we can find <em class="italic">account number</em> or <em class="italic">account num</em> around the number token. This pattern should do the trick:</p>
			<pre>{"LOWER": "account"}, {"LOWER": {"IN": ["num", "number"]}},{}, {"IS_DIGIT": True}</pre>
			<p>We used a wildcard here: <code>{}</code> means any token. We allowed one token to go in between <em class="italic">number</em> and <em class="italic">account number</em>; this can be <em class="italic">is</em>, <em class="italic">was</em>, and so on. Let's see the code:</p>
			<pre> doc = nlp("My account number is 8921273.")
pattern = [{"LOWER": "account"}, 
           {"LOWER": {"IN": ["num", "number"]}},{}, 
           {"IS_DIGIT": True}]
matcher = Matcher(nlp.vocab)
matcher.add("accountNum",  [pattern])
for mid, start, end in matcher(doc):
    print(start, end, doc[start:end])
... 
1 5 account number is 8921273</pre>
			<p>If you <a id="_idIndexMarker270"/>want, you can include a possessive pronoun such as <em class="italic">my</em>, <em class="italic">your</em>, or <em class="italic">his</em> in the match, depending on the application's needs.</p>
			<p>That's it for banking numbers. Now we'll extract another type of common numeric entity, phone numbers.</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor080"/>Extracting phone numbers</h2>
			<p>Phone numbers can have very different formats depending on the country, and matching <a id="_idIndexMarker271"/>phone numbers is often a tricky business. The best strategy here is to be specific about the country phone number format you want to parse. If there are several countries, you can add corresponding individual patterns to the matcher. If you have too many countries, then you can relax some conditions and go for a more general pattern (we'll see how to do that).</p>
			<p>Let's start with the US phone number format. A US number is written as <em class="italic">(541) 754-3010</em> domestically or <em class="italic">+1 (541) 754-3010</em> internationally. We can form our pattern with an optional <em class="italic">+1</em>, then a three-digit area code, then two blocks of numbers separated with an optional <em class="italic">-</em>.  Here is the pattern:</p>
			<pre>{"TEXT": "+1", "OP": "?"}, {"TEXT": "("}, {"SHAPE": "ddd"}, {"TEXT": ")"}, {"SHAPE": "ddd"}, {"TEXT": "-", "OP": "?"}, {"SHAPE": "dddd"}</pre>
			<p>Let's see an example:</p>
			<pre>doc1 = nlp("You can call my office on +1 (221) 102-2423 or email me directly.")
doc2 = nlp("You can call me on (221) 102 2423 or text me.")
pattern = [{"TEXT": "+1", "OP": "?"}, {"TEXT": "("}, 
           {"SHAPE": "ddd"}, {"TEXT": ")"}, 
           {"SHAPE": "ddd"}, {"TEXT": "-", "OP": "?"}, 
           {"SHAPE": "dddd"}]
matcher = Matcher(nlp.vocab)
matcher.add("usPhonNum",  [pattern])
for mid, start, end in matcher(doc1):
    print(start, end, doc1[start:end])
... 
 6 13 +1 (221) 102-2423
for mid, start, end in matcher(doc2):
    print(start, end, doc2[start:end])
... 
 5 11 (221) 102-2423</pre>
			<p>How about <a id="_idIndexMarker272"/>we make the pattern more general to apply to other countries as well? In this case, we can start with a 1 to 3-digit country code, followed by some digit blocks. It will match a broader set of numbers, so it's better to be careful not to match other numeric entities in your text.</p>
			<p>We'll move onto textual entities from numeric entities. Now we'll process social media text and extract different types of entities that can occur in social media text.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor081"/>Extracting mentions</h2>
			<p>Imagine analyzing a dataset of social media posts about companies and products and your task <a id="_idIndexMarker273"/>is to find out which companies are mentioned in which ways. The dataset will contain this sort of sentence:</p>
			<pre>CafeA is very generous with the portions.
CafeB is horrible, we waited for mins for a table.
RestaurantA is terribly expensive, stay away!
RestaurantB is pretty amazing, we recommend.</pre>
			<p>What we're looking for is most probably patterns of the <em class="italic">BusinessName is/was/be adverb* adjective</em> form. The following pattern would work:</p>
			<pre>[{"ENT_TYPE": "ORG"}, {"LEMMA": "be"}, {"POS": "ADV", "OP": "*"}, {"POS": "ADJ"}]</pre>
			<p>Here, we look for an organization type entity, followed by a <em class="italic">is/was/be</em>, then optional adverbs, and finally an adjective.</p>
			<p>What if <a id="_idIndexMarker274"/>you want to extract a specific business, let's say the company <em class="italic">ACME</em>? All you have to do is replace the first token with the specific company name:</p>
			<pre>[{"LOWER": "acme"}, {"LEMMA": "be"}, {"POS": "ADV", "OP": "*"}, {"POS": "ADJ"}]</pre>
			<p>That's it, easy peasy! After extracting the social media mentions, the next thing to do is to extract the hashtags and the emojis.</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor082"/>Hashtag and emoji extraction</h2>
			<p>Processing social media text is a hot topic and has some challenges. Social media text includes <a id="_idIndexMarker275"/>two sorts of unusual token types: hashtags and emojis. Both token types have a huge impact on the text meaning. The hashtag refers to the subject/object of the sentence, usually, and emojis can assign the sentiment of the sentence by themselves.</p>
			<p>A hashtag consists of a <code>#</code> character at the beginning, then followed by a word of <code>ASCII</code> characters, with no inter-word spaces. Some examples are <em class="italic">#MySpace</em>, <em class="italic">#MondayMotivation</em> and so on. The spaCy tokenizer tokenizes these words into two tokens:</p>
			<pre>doc = nlp("#MySpace")
[token.text for token in doc]
['#', 'MySpace']</pre>
			<p>As a result, our pattern needs to match two tokens, the <code>#</code> character and the rest. The following pattern will match a hashtag easily:</p>
			<pre>{"TEXT": "#"}, {"IS_ASCII": True}</pre>
			<p>The following code extracts a hashtag:</p>
			<pre>doc = nlp("Start working out now #WeekendShred")
pattern = [{"TEXT": "#"}, {"IS_ASCII": True}]
matcher = Matcher(nlp.vocab)
matcher.add("hashTag",  [pattern])
matches = matcher(doc)
for mid, start, end in matches:
    print(start, end doc[start:end])
...
4 6 #WeekendShred</pre>
			<p>How about an emoji? An emoji is usually coded with lists according to their sentiment value, such <a id="_idIndexMarker276"/>as positive, negative, happy, sad, and so on. Here, we'll separate emojis into two classes, positive and negative. The following code spots the selected emoji in the text:</p>
			<pre>pos_emoji = ["<img src="img/B16570_4_emoj1.png" alt="" width="33" height="31"/>", "<img src="img/B16570_4_emoj2.png" alt="" width="33" height="31"/>", "<img src="img/B16570_4_emoj3.png" alt="" width="35" height="32"/>", "<img src="img/B16570_4_emoj5.png" alt="" width="35" height="32"/>", "<img src="img/B16570_4_emoj51.png" alt="" width="34" height="32"/>", "<img src="img/B16570_4_emoj6.png" alt="" width="32" height="33"/>"]  
neg_emoji = ["<img src="img/B16570_4_emoj7.png" alt="" width="36" height="31"/>", "<img src="img/B16570_4_emoj8.png" alt="" width="32" height="32"/>", "<img src="img/B16570_4_emoj9.png" alt="" width="32" height="33"/>", "<img src="img/B16570_4_emoj10.png" alt="" width="35" height="31"/>", "<img src="img/B16570_4_emoj11.png" alt="" width="33" height="32"/>", "<img src="img/B16570_4_emoj12.png" alt="" width="33" height="32"/>"]
pos_patterns = [[{"ORTH": emoji}] for emoji in pos_emoji]
neg_patterns = [[{"ORTH": emoji}] for emoji in neg_emoji]
matcher = matcher(nlp.vocab)
matcher.add("posEmoji", pos_patterns)
matcher.add("negEmoji", neg_patterns)
doc = nlp(" I love Zara <img src="img/B16570_4_emoj13.png" alt="" width="40" height="32"/> ")
for mid, start, end in matcher(doc):
    print(start, end, doc[start:end])
...
3 4 <img src="img/B16570_4_emoj14.png" alt="" width="44" height="33"/></pre>
			<p>Et voilà, the emoji <img src="img/B16570_4_emoj15.png" alt="" width="36" height="34"/> is extracted happily! We'll make use of emojis in sentiment analysis chapter as well.</p>
			<p>Now, let's extract some entities. We'll start with the common procedure of expanding named entities.</p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor083"/>Expanding named entities</h2>
			<p>Often, we would like to expand a named entity's span to the left or to the right. Imagine you want <a id="_idIndexMarker277"/>to extract <code>PERSON</code> type named entities with titles so that you can deduce the gender or profession easily. spaCy's <code>NER</code> class already extracts person names, so how about the titles?</p>
			<pre>doc = nlp("Ms. Smith left her house 2 hours ago.")
doc.ents
(Smith, 2 hours ago)</pre>
			<p>As you see, the word <code>Ms.</code> is not included in the named entity because it's not a part of the person's name. A quick solution is to make a new entity type called <code>TITLE</code>:</p>
			<pre>patterns = [{"label": "TITLE", "pattern": [{"LOWER": {"IN": ["ms.", "mr.", "mrs.", "prof.", "dr."]}}]}]
ruler = nlp.add_pipe("entity_ruler")
ruler.add_patterns(patterns)
nlp.add_pipe(ruler)
doc = nlp("Ms. Smith left her house")
print([(ent.text, ent.label_) for ent in doc.ents])
[('Ms.', 'TITLE'), ('SMITH', 'PERSON')]</pre>
			<p>This is a quick and very handy recipe. You'll come across parsing titles a lot if you process wiki text or financial text.</p>
			<p>In our next and final example, we'll combine POS attributes, dependency labels, and named entities.</p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor084"/>Combining linguistic features and named entities</h2>
			<p>While charging meaning to a sentence, we evaluate word semantics by considering the contexts <a id="_idIndexMarker278"/>they occur in. Matching the words individually usually does not help us understand the full meaning. In most NLU tasks we have to combine linguistic features.</p>
			<p>Imagine you're parsing professional biographies and make a work history of the subjects. You want to extract person names, the cities they have lived in, and the city they're currently working in.</p>
			<p>Obviously we'll look for the word <em class="italic">live</em>; however, the POS tags hold the key here: whether it's the present tense or the past tense. In order to determine which city/place, we'll use syntactic information that is given by the dependency labels.</p>
			<p>Let's examine the following example:</p>
			<pre>doc = nlp("Einstein lived in Zurich.")
[(ent.text, ent.label_) for ent in doc.ents]
[('Einstein', 'PERSON'), ('Zurich', 'GPE')]</pre>
			<p>Here is a visual representation of the preceding example:</p>
			<div><div><img src="img/B16570_4_07.jpg" alt="Fig 4.7 – Example parse, the entity ”Einstein” being subject of the sentence&#13;&#10;" width="614" height="156"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7 – Example parse, the entity "Einstein" being subject of the sentence</p>
			<p>Here, <code>lived</code> is the main verb of the sentence, hence the root of the sentence. <code>Einstein</code> is the subject of the sentence, at the same time the person entity who <code>lived</code>. As we can see, the <code>Einstein</code> token's head is <code>lived</code>. There's also a place entity in the sentence, <code>Zurich</code>. If we follow the arcs from <code>lived</code>, we reach <code>Zurich</code> via a prepositional attachment. Finally, to determine the verb's tense, we can examine the POS tag. Let's see it in the following code:</p>
			<pre>person_ents = [ent for ent in doc.ents if ent.label_ == "PERSON"]
for person_ent in person_entities:
    #We use head of the entity's last token
    head = person_ent[-1].head  
    If head.lemma_ == "live":
    #Check if the children of live contains prepositional 
    attachment 
    preps = [token for token in head.children if token.dep_ == "prep"]
    for prep in preps:         
        places = [token for token in prep.children if token.ent_type_ == "GPE"]   
        # Verb is in past or present tense
        print({'person': person_ent, 'city': places, 
               'past': head.tag_ == "VBD"})</pre>
			<p>Here, we <a id="_idIndexMarker279"/>combined POS tag information, dependency labels (hence syntactic information of the sentence), and named entities. It may not be easy for you to grasp it at first sight, but you'll get there by practicing. </p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor085"/>Summary</h1>
			<p>This chapter introduced you to a very handy and powerful feature of spaCy, spaCy's matcher classes. You learned how to do rule-based matching with linguistic and token-level features. You learned about the <code>Matcher</code> class, spaCy's rule-based matcher. We explored the <code>Matcher</code> class by using it with different token features, such as shape, lemma, text, and entity type.</p>
			<p>Then, you learned about <code>EntityRuler</code>, another lifesaving class that you can achieve a lot with. You learned how to extract named entities with the <code>EntityRuler</code> class.</p>
			<p>Finally, we put together what you've learned in this chapter and your previous knowledge and combined linguistic features with rule-based matching with several examples. You learned how to extract patterns, entities of specific formats, and entities specific to your domain.</p>
			<p>With this chapter, you completed the linguistic features. In the next chapter, we'll dive into the world of statistical semantics via a very important concept – <strong class="bold">word vectors</strong>. You'll discover the power of statistics in representing words, phrases, and sentences. Let's discover the world of semantics together!</p>
		</div>
	</div></body></html>