<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Predicting Votes with Linear Models</h1>
                
            
            <article>
                
<p class="calibre2">This chapter shows how to work with statistical models using R. It shows how to check data assumptions, specify linear models, make predictions, and measure predictive accuracy. It also shows how to find good models programatically to avoid doing analysis by hand, which can potentially save a lot of time. By the end of this chapter, we will have worked with various quantitative tools that are used in many business and research areas nowadays. The packages used in this chapter are the same ones from the previous chapter.</p>
<p class="calibre2">Just like in the previous chapter, the focus here will be on automating the analysis programatically rather than on deeply understanding the statistical techniques used in the chapter. Furthermore, since we have seen in <a href="part0059.html#1O8H60-f494c932c729429fb734ce52cafce730" class="calibre4">Chapter 2</a>, <em class="calibre19">Understanding Votes With Descriptive Statistics</em>, how to work efficiently with functions, we will use that approach directly in this chapter, meaning that when possible we'll work directly with functions that will be used to automate our analysis. We will cover the following:</p>
<ul class="calibre11">
<li class="calibre12">Splitting data into training and testing sets</li>
<li class="calibre12">Creating linear regression models used for prediction</li>
<li class="calibre12">Checking model assumptions with various techniques</li>
<li class="calibre12">Measuring predictive accuracy for numerical and categorical data</li>
<li class="calibre12">Programatically finding the best possible model</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Required packages</h1>
                
            
            <article>
                
<p class="calibre2">During this chapter we will make use of the following R packages, which were already used in the previous chapter, so you should be good to go.</p>
<table class="calibre5">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">Package</strong></td>
<td class="calibre8"><strong class="calibre1">Reason</strong></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre41">ggplot2</kbd></td>
<td class="calibre8">High-quality graphs</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre41">corrplot</kbd></td>
<td class="calibre8">Correlation plots</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre41">progress</kbd></td>
<td class="calibre8">Show progress for Iteration</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Setting up the data</h1>
                
            
            <article>
                
<p class="calibre2">As it's usual with data analysis, the first step is to understand the data we will be working with. In this case, the data is the same as in <a href="part0059.html#1O8H60-f494c932c729429fb734ce52cafce730" class="calibre4">Chapter 2</a>, <em class="calibre19">Understanding Votes with Descriptive Statistics</em>, and we have already understood some of its main characteristics. Mainly, we've understood that age, education, and race have considerable effects over the propensity to vote in favor of the UK leaving or remaining in the EU.</p>
<p class="calibre2">The focus of this chapter will be on using linear models to predict the <kbd class="calibre9">Proportion</kbd> and <kbd class="calibre9">Vote</kbd> variables, which contain the percentage of votes in favor of leaving the EU and whether the ward had more votes for <kbd class="calibre9">"Leave"</kbd> or <kbd class="calibre9">"Remain"</kbd>, respectively. Both variables have similar information, the difference being that one is a numerical continuous variable with values between 0 and 1 (<kbd class="calibre9">Proportion</kbd>) and the other is a categorical variable with two categories (<kbd class="calibre9">Vote</kbd> with <kbd class="calibre9">Leave</kbd> and <kbd class="calibre9">Remain</kbd> categories).</p>
<p class="calibre2">We'll keep observations that contain <em class="calibre19">complete cases</em> in the <kbd class="calibre9">data</kbd> object, and observations that have missing values for the <kbd class="calibre9">Proportion</kbd> and <kbd class="calibre9">Vote</kbd> variables in the <kbd class="calibre9">data_incomplete</kbd> object (we'll make predictions over these in the latter part of this chapter). The functions <kbd class="calibre9">prepare_data()</kbd>, <kbd class="calibre9">adjust_data()</kbd>, and <kbd class="calibre9">get_numerical_variables()</kbd> come from <a href="part0059.html#1O8H60-f494c932c729429fb734ce52cafce730" class="calibre4">Chapter 2</a>, <em class="calibre19">Understanding Votes with Descriptive Statistics</em>, so you may want to take a look if you're not clear about what they do. Basically, they load the data with the adjusted version that we created by compressing the data spread among various variables regarding age, education, and race:</p>
<pre class="mce-root">data &lt;- adjust_data(prepare_data("./data_brexit_referendum.csv"))

data_incomplete     &lt;- data[!complete.cases(data), ]
data                &lt;- data[ complete.cases(data), ]
numerical_variables &lt;- get_numerical_variable_names(data)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Training and testing datasets</h1>
                
            
            <article>
                
<p class="calibre2">For us to be able to measure the predictive accuracy of our models, we need to use some observations to validate our results. This means that our data will be split into three different groups:</p>
<ul class="calibre11">
<li class="calibre12">Training data</li>
<li class="calibre12">Testing data</li>
<li class="calibre12">Predicting data</li>
</ul>
<p class="calibre2">The predicting data is the data that we don't have complete cases for, specifically these are wards for which the <kbd class="calibre9">Vote</kbd> and <kbd class="calibre9">Proportion</kbd> variables have <kbd class="calibre9">NA</kbd> values. Our final objective is to provide predictions for these ward's <kbd class="calibre9">Proportion</kbd> and <kbd class="calibre9">Vote</kbd> variables using what we can learn from other wards for which we do have data for these variables, and it's something we'll do toward the end of the chapter.</p>
<p class="calibre2">The data that has complete cases will be split into two parts, training, and testing data. Training data is used to extract knowledge and learn the relationship among variables. Testing is treated as if it had <kbd class="calibre9">NA</kbd> values for <kbd class="calibre9">Proportion</kbd> and <kbd class="calibre9">Vote</kbd>, and we produce predictions for them. These predictions are then compared to the real values in the corresponding observations, and this helps us understand how good our predictions are in a way that is objective since those observations are never seen by the trained models.</p>
<p class="calibre2">We created the predicting data in the previous section, and we called it <kbd class="calibre9">data_incomplete</kbd>. To create the training and testing data, we use the <kbd class="calibre9">sample()</kbd> function. It will take as input a list of numbers from which it will pick a certain number of values (<kbd class="calibre9">size</kbd>). The list of numbers will go from 1 to the total number of observations available in the data with complete cases. We specify the number of observations that will be picked for the training data as around 70% of the total number of observations available, and use the <kbd class="calibre9">replace = FALSE</kbd> argument to specify that the picked observations may not be duplicated (by avoiding a sample with replacement).</p>
<p class="calibre2">The testing data is composed of the remaining 30% of the observations. Since <kbd class="calibre9">sample</kbd> is a Boolean vector that contains a <kbd class="calibre9">TRUE</kbd> or <kbd class="calibre9">FALSE</kbd> value for each observation to specify whether or not it should be included, respectively, we can negate the vector to pick the other part of the data by prepending a minus sign (<kbd class="calibre9">-</kbd>) to the binary vector, effectively making every <kbd class="calibre9">TRUE</kbd> value a <kbd class="calibre9">FALSE</kbd> value, and vice versa. To understand this, let's look at the following code:</p>
<pre class="mce-root">set.seed(12345)

n          &lt;- nrow(data)
sample     &lt;- sample(1:n, size = round(0.7 * n), replace = FALSE)
data_train &lt;- data[ sample, ]
data_test  &lt;- data[-sample, ]</pre>
<p class="calibre2">If we did this process various times, we would find that every time we get different samples for the training and testing sets, and this may confuse us about our results. This is because the <kbd class="calibre9">sample()</kbd> function is stochastic, meaning that it will use <em class="calibre19">pseudo random number generator</em> to make the selection for us (computers can not generate real randomness, they simulate numbers that appear to be random even though they are not, that's why it's called <strong class="calibre1">pseudo random</strong>). If we want our process to be reproducible, meaning that, every time we run it the exact same samples are selected, then we must specify an initial seed before applying this process to precondition the pseudo random number generator. To do so, we need to pass an integer to the <kbd class="calibre9">set.seed()</kbd> function, as we do at the beginning of the code snippet. The seed argument must stay fixed to reproduce the same samples, and with it in place, every time we generate a random sample, we will get the same sample so that our results are reproducible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Predicting votes with linear models</h1>
                
            
            <article>
                
<p class="calibre2">Before we can make any predictions, we need to specify a model and train it with our training data (<kbd class="calibre9">data_train</kbd>) so that it learns how to provide us with the predictions we're looking for. This means that we will solve an optimization problem that outputs certain numbers that will be used as parameters for our model's predictions. R makes it very easy for us to accomplish such a task.</p>
<p class="calibre2">The standard way of specifying a linear regression model in R is using the <kbd class="calibre9">lm()</kbd> function with the model we want to build expressed as a formula and the data that should be used, and save it into an object (in this case <kbd class="calibre9">fit</kbd>) that we can use to explore the results in detail. For example, the simplest model we can build is one with a single regressor (independent variable) as follows:</p>
<pre class="mce-root">fit &lt;- lm(Proportion ~ Students, data_train)</pre>
<p class="calibre2">In this simple model, we would let R know that we want to run a regression where we try to explain the <kbd class="calibre9">Proportion</kbd> variable using only the <kbd class="calibre9">Students</kbd> variable in the data. This model is too simple, what happens if we want to include a second variable? Well, we can add it using the plus (<kbd class="calibre9">+</kbd>) sign after our other regressors. For example (keep in mind that this would override the previous <kbd class="calibre9">fit</kbd> object with the new results, so if you want to keep both of them, make sure that you give the resulting objects different names):</p>
<pre class="mce-root">fit &lt;- lm(Proportion ~ Students + Age_18to44, data_train)</pre>
<p class="calibre2">This may be a better way of explaining the <kbd class="calibre9">Proportion</kbd> variable since we are working with more information. However, keep in mind the collinearity problem; it's likely that the higher the students percentage is in a ward (<kbd class="calibre9">Students</kbd>), the higher the percentage of relatively young people (<kbd class="calibre9">Age_18to44</kbd>), meaning that we may not be adding independent information into the regression. Of course, in most situations, this is not a binary issue, it's an issue of degree and the analyst must be able to handle such situations. We'll touch more on this when checking the model's assumptions in the next section. For now let's get back to programming, shall we? What if we want to include all the variables in the data? Well, we have two options, include all variables manually or use R's shortcut for doing so:</p>
<pre class="mce-root"># Manually
fit &lt;- lm(Proportion ~ ID + RegionName + NVotes + Leave + Residents + Households + White + <br class="title-page-name"/>          Owned + OwnedOutright + SocialRent + PrivateRent + Students + Unemp + UnempRate_EA + <br class="title-page-name"/>          HigherOccup + Density + Deprived + MultiDepriv + Age_18to44 + Age_45plus + NonWhite + <br class="title-page-name"/>          HighEducationLevel + LowEducationLevel, data_train)

# R's shortcut
fit &lt;- lm(Proportion ~ ., data_train)</pre>
<p class="calibre2">These two models are exactly the same. However, there are a couple of subtle points we need to mention. First, when specifying the model manually, we had to leave the <kbd class="calibre9">Proportion</kbd> variable explicitly out of the regressors (variables after the <kbd class="calibre9">~</kbd> symbol) so that we don’t get an error when running the regressions (it would not make sense for R to allow us to try to explain the <kbd class="calibre9">Proportion</kbd> variable by using the same <kbd class="calibre9">Proportion</kbd> variable and other things). Second, if we make any typos while writing the variable names, we will get errors since those names will not be present in the variable names (if by coincidence your typo actually refers to another existing variable in the data it may be a hard mistake to diagnose). Third, in both cases the list of regressors includes variables that should not be there, like <kbd class="calibre9">ID</kbd>, <kbd class="calibre9">RegionName</kbd>, <kbd class="calibre9">NVotes</kbd>, <kbd class="calibre9">Leave</kbd>, and <kbd class="calibre9">Vote</kbd>. In the case <kbd class="calibre9">of</kbd><br class="title-page-name"/>
<kbd class="calibre9">ID</kbd> it doesn’t make sense for that variable to be included in the analysis as it doesn’t have any information regarding the <kbd class="calibre9">Proportion</kbd>, it's just an identifier. In the case of <kbd class="calibre9">RegionName</kbd> it's a categorical variable so the regression would stop being a <em class="calibre19">Standard Multiple Linear Regression</em> and R would automatically make it work for us, but if we do not understand what we’re doing, it may produce confusing results. In this case we want to work only with numerical variables so we can remove it easily from the manual case, but we can’t do that in the shortcut case. Finally, in the case of <kbd class="calibre9">NVotes</kbd>, <kbd class="calibre9">Leave</kbd>, and <kbd class="calibre9">Vote</kbd>, those variables are expressing the same information in slightly the same way so they shouldn’t be included since we would have a multicollinearity problem.</p>
<p class="calibre2">Let's say the final model we want to work with includes all the valid numerical variables:</p>
<pre class="mce-root">fit &lt;- lm(Proportion ~ Residents + Households + White + Owned + OwnedOutright + SocialRent + PrivateRent + Students + Unemp + UnempRate_EA + HigherOccup + Density + Deprived + MultiDepriv + Age_18to44 + Age_45plus + NonWhite + HighEducationLevel + LowEducationLevel, data_train)</pre>
<div class="packt_tip">If we want to use the shortcut method, we can make sure that the data does not contain the problematic variables (using the selection techniques we looked at in <a href="part0022.html#KVCC0-f494c932c729429fb734ce52cafce730" class="calibre26">Chapter 1</a>, <em class="calibre27">Introduction to R</em>) and then using the shortcut.</div>
<p class="calibre2">To take a look at the results in detail, we use the <kbd class="calibre9">summary()</kbd> function on the <kbd class="calibre9">fit</kbd> object:</p>
<pre class="mce-root">summary(fit)<br class="title-page-name"/><strong class="calibre1">#&gt;</strong><br class="title-page-name"/><strong class="calibre1">#&gt; Call:</strong><br class="title-page-name"/><strong class="calibre1">#&gt; lm(formula = Proportion ~ Residents + Households + White + Owned +</strong><br class="title-page-name"/><strong class="calibre1">#&gt;    OwnedOutright + SocialRent + PrivateRent + Students + Unemp +</strong><br class="title-page-name"/><strong class="calibre1">#&gt;    UnempRate_EA + HigherOccup + Density + Deprived + MultiDepriv +</strong><br class="title-page-name"/><strong class="calibre1">#&gt;    Age_18to44 + Age_45plus + NonWhite + HighEducationLevel +</strong><br class="title-page-name"/><strong class="calibre1">#&gt;    LowEducationLevel, data = data_train)</strong><br class="title-page-name"/><strong class="calibre1">#&gt;</strong><br class="title-page-name"/><strong class="calibre1">#&gt; Residuals:</strong><br class="title-page-name"/><strong class="calibre1">#&gt;      Min       1Q  Median      3Q     Max</strong><br class="title-page-name"/><strong class="calibre1">#&gt; -0.21606 -0.03189 0.00155 0.03393 0.26753</strong><br class="title-page-name"/><strong class="calibre1">#&gt;</strong><br class="title-page-name"/><strong class="calibre1">#&gt; Coefficients:</strong><br class="title-page-name"/><strong class="calibre1">#&gt;                     Estimate Std. Error  t value  Pr(&gt;|t|)</strong><br class="title-page-name"/><strong class="calibre1">#&gt; (Intercept)         3.30e-02   3.38e-01  0.10      0.92222</strong><br class="title-page-name"/><strong class="calibre1">#&gt; Residents           7.17e-07   2.81e-06  0.26      0.79842</strong><br class="title-page-name"/><strong class="calibre1">#&gt; Households         -4.93e-06   6.75e-06 -0.73      0.46570</strong><br class="title-page-name"/><strong class="calibre1">#&gt; White               4.27e-03   7.23e-04  5.91      6.1e-09 ***</strong><br class="title-page-name"/><strong class="calibre1">#&gt; Owned              -2.24e-03   3.40e-03 -0.66      0.51071</strong><br class="title-page-name"/><strong class="calibre1">#&gt; OwnedOutright      -3.24e-03   1.08e-03 -2.99      0.00293 **</strong><br class="title-page-name"/><strong class="calibre1">#&gt; SocialRent         -4.08e-03   3.60e-03 -1.13      0.25847</strong><br class="title-page-name"/><strong class="calibre1">#&gt; PrivateRent        -3.17e-03   3.59e-03 -0.89      0.37629</strong><br class="title-page-name"/><strong class="calibre1">#&gt; Students           -8.34e-04   8.67e-04 -0.96      0.33673</strong><br class="title-page-name"/><strong class="calibre1">#&gt; Unemp               5.29e-02   1.06e-02  5.01      7.3e-07 ***</strong><br class="title-page-name"/><strong class="calibre1">#&gt; UnempRate_EA       -3.13e-02   6.74e-03 -4.65      4.1e-06 ***</strong><br class="title-page-name"/><strong class="calibre1">#&gt; HigherOccup         5.21e-03   1.24e-03  4.21      2.9e-05 ***</strong><br class="title-page-name"/><strong class="calibre1">#&gt; Density            -4.84e-04   1.18e-04 -4.11      4.6e-05 ***</strong><br class="title-page-name"/><strong class="calibre1">#&gt; Deprived            5.10e-03   1.52e-03  3.35      0.00087 ***</strong><br class="title-page-name"/><strong class="calibre1">#&gt; MultiDepriv        -6.26e-03   1.67e-03 -3.75      0.00019 ***</strong><br class="title-page-name"/><strong class="calibre1">#&gt; Age_18to44          3.46e-03   1.36e-03  2.55      0.01117 *</strong><br class="title-page-name"/><strong class="calibre1">#&gt; Age_45plus          4.78e-03   1.27e-03  3.75      0.00019 ***</strong><br class="title-page-name"/><strong class="calibre1">#&gt; NonWhite            2.59e-03   4.47e-04  5.80      1.1e-08 ***</strong><br class="title-page-name"/><strong class="calibre1">#&gt; HighEducationLevel -1.14e-02   1.14e-03 -9.93      &lt; 2e-16 ***</strong><br class="title-page-name"/><strong class="calibre1">#&gt; LowEducationLevel   4.92e-03   1.28e-03  3.85      0.00013 ***</strong><br class="title-page-name"/><strong class="calibre1">#&gt; ---</strong><br class="title-page-name"/><strong class="calibre1">#&gt; Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong><br class="title-page-name"/><strong class="calibre1">#&gt; Residual standard error: 0.0523 on 542 degrees of freedom</strong><br class="title-page-name"/><strong class="calibre1">#&gt; Multiple R-squared: 0.868, Adjusted R-squared: 0.863</strong><br class="title-page-name"/><strong class="calibre1">#&gt; F-statistic: 187 on 19 and 542 DF, p-value: &lt;2e-16</strong><br class="title-page-name"/></pre>
<p class="calibre2">These results tell us which command was used to create our model, which is useful when you're creating various models and want to quickly know the model associated to the results you're looking at. It also shows some information about the distribution of the residuals. Next, it shows the regression's results for each variable used in the mode. We get the name of the variable (<kbd class="calibre9">(Intercept)</kbd> is the Standard Linear Regression intercept used in the model's specification<span>)</span>, the coefficient estimate for the variable, the standard error, the <em class="calibre19">t statistic</em>, the <em class="calibre19">p-value</em>, and a visual representation of the <em class="calibre19">p-value</em> using asterisks for significance codes. At the end of the results, we see other results associated with the model, including the <em class="calibre19">R-squared</em> and the <em class="calibre19">F-statistic</em>. As mentioned earlier, we won't go into details about what each of these mean, and we will continue to focus on the programming techniques. If you're interested, you may look at Casella and Berger's, <em class="calibre19">Statistical Inference, 2002</em>, or Rice's, <em class="calibre19">Mathematical Statistics and Data Analysis, 1995</em>.</p>
<p class="calibre2">Now that we have a fitted model ready in the <kbd class="calibre9">fit</kbd> object, we can use it to make predictions. To do so, we use the <kbd class="calibre9">predict()</kbd> function with the <kbd class="calibre9">fit</kbd> object and the data we want to produce predictions for, <kbd class="calibre9">data_test</kbd> in our case. This returns a vector of predictions that we store in the <kbd class="calibre9">predictions</kbd> object. We will get one prediction for each observation in the <kbd class="calibre9">data_test</kbd> object:</p>
<pre class="mce-root">predictions &lt;- predict(fit, data_test)</pre>
<p class="calibre2">These predictions can be measured for accuracy as we will do in a later section in this chapter. For now, we know how to generate predictions easily with R.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Checking model assumptions</h1>
                
            
            <article>
                
<p class="calibre2">Linear models, as with any kind of models, require that we check their assumptions to justify their application. The accuracy and interpretability of the results comes from adhering to a model's assumptions. Sometimes these will be rigorous assumptions in the sense that if they are not strictly met, then the model is not considered to be valid at all. Other times, we will be working with more flexible assumptions in which a degree of criteria from the analyst will come into play.</p>
<p class="calibre2">For those of you interested, a great article about models' assumptions is David Robinson's, <em class="calibre19">K-means clust</em><em class="calibre19">ering is not free lunch, 2015</em> (<a href="http://varianceexplained.org/r/kmeans-free-lunch/" class="calibre4">http://varianceexplained.org/r/kmeans-free-lunch/</a>).</p>
<p class="calibre2">For linear models, the following are some of the core assumptions:</p>
<ul class="calibre11">
<li class="calibre12"><strong class="calibre1">Linearity</strong>: There is a linear relation among the variables</li>
<li class="calibre12"><strong class="calibre1">Normality</strong>: Residuals are normally distributed</li>
<li class="calibre12"><strong class="calibre1">Homoscedasticity</strong>: Residuals have constant variance</li>
<li class="calibre12"><strong class="calibre1">No collinearity</strong>: Variables are not linear combinations of each other</li>
<li class="calibre12"><strong class="calibre1">Independence</strong>: Residuals are independent or at least not correlated</li>
</ul>
<p class="calibre2">We will show how to briefly check four of the them: linearity, normality, homoscedasticity, and no collinearity. We should mention that the independence assumption is probably the most difficult assumption to test, and you can generally handle it with common sense and understanding how the data <span>was</span> collected. We will not get into that here as it's more in the statistics side of things and we want to keep the book focused on programming techniques. For the statistically-interested reader, we recommend looking at Jeffrey M. Wooldridge's, <em class="calibre19">Introductory Econometrics, 2013</em> and Joshua D. Angrist and Jorn-Steffen Pischke's, <em class="calibre19">Mostly Harmless Econometrics, 2008</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Checking linearity with scatter plots</h1>
                
            
            <article>
                
<p class="calibre2">A basic way of checking the linearity assumption is to make a scatter plot with the dependent variable in the <em class="calibre19">y</em> axis and an independent variable in the <em class="calibre19">x</em> axis. If the relation appears to be linear, the assumption is validated. In any interesting problem it's extremely hard to find a scatter plot that shows a very clear linear relation, and if it does happen we should be a little suspicious and careful with the data. To avoid reinventing the wheel, we will use the <kbd class="calibre9">plot_scatterlot()</kbd> function we created in <a href="part0059.html#1O8H60-f494c932c729429fb734ce52cafce730" class="calibre4">Chapter 2</a>, <em class="calibre19">Understanding Votes with Descriptive Statistics</em>:</p>
<pre class="mce-root">plot_scatterplot(
    data = data,
    var_x = "Age_18to44",
    var_y = "Proportion",
    var_color = FALSE,
    regression = TRUE
)
plot_scatterplot(
    data = data,
    var_x = "Students",
    var_y = "Proportion",
    var_color = FALSE,
    regression = TRUE
)</pre>
<p class="calibre2">As we can see, the scatter plot on the left shows a clear linear relation, as the percentage of people between 18 and 44 years of age (<kbd class="calibre9">Age_18to44</kbd>) increases, the proportion of people in favor of leaving the EU (<kbd class="calibre9">Proportion</kbd>) decreases. On the right hand, we see that the relation among the percentage of students in a ward (<kbd class="calibre9">Students</kbd>) and <kbd class="calibre9">Proportion</kbd> is clearly linear in the initial area (where <kbd class="calibre9">Students</kbd> is between 0 and 20), after that the relation too seems to be linear, but it is polluted by observations with very high percentage of students. However, we can still assume a linear relation between <kbd class="calibre9">Students</kbd> and <kbd class="calibre9">Proportion</kbd>.</p>
<div class="cdpaligncenter"><img src="../images/00022.jpeg" class="calibre42"/></div>
<p class="calibre2">When we're doing a <em class="calibre19">Multiple Linear Regression</em> as we're doing here, the assumption should be checked for the rest of the variables, which we omit here to preserve space, but we encourage you to do so. Keep in mind that it's very hard to find a linear relation in all of them, and this assumption is mostly an indicator of the predictive power of the variable in the regression. As long as the relation appears to be slightly linear, we should be all set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Checking normality with histograms and quantile-quantile plots</h1>
                
            
            <article>
                
<p class="calibre2">We will check normality with two different techniques so that we can exemplify the usage of a technique known as the <strong class="calibre1">strategy pattern</strong>, which is part of a set of patterns from object-oriented programming. We will go deeper into these patterns in <a href="part0178.html#59O440-f494c932c729429fb734ce52cafce730" class="calibre4">Chapter 8</a>, <em class="calibre19">Object-Oriented System </em><em class="calibre19">to Track Cryptocurrencies</em>.</p>
<p class="calibre2">For now, you can think of the strategy pattern as a technique that will re-use code that would otherwise be duplicated and simply changes a way of doing things called the <strong class="calibre1">strategy</strong>. In the following code you can see that we create a function called <kbd class="calibre9">save_png()</kbd> which contains the code that would be duplicated (saving PNG files) and doesn't need to be. We will have two strategies, in the form of functions, to check data normality—histograms and quantile-quantile plots. These will be sent through the argument conveniently named <kbd class="calibre9">functions_to_create_images</kbd>. As you can see, this code receives some data, a variable that will be used for the graph, the file name for the image, and a function that will be used to create the graphs. This last parameter, the function, should not be unfamiliar to the reader as we have seen in <a href="part0022.html#KVCC0-f494c932c729429fb734ce52cafce730" class="calibre4">Chapter 1</a>, <em class="calibre19">Introduction to R</em>, that we can send functions as arguments, and use them as we do in this code, by calling them through their <em class="calibre19">new name</em> inside the function, <kbd class="calibre9">function_to_create_image()</kbd> in this case:</p>
<pre class="mce-root">save_png &lt;- function(data, variable, save_to, function_to_create_image) {
    if (not_empty(save_to)) png(save_to)
    function_to_create_image(data, variable)
    if (not_empty(save_to)) dev.off()
}</pre>
<p class="calibre2">Now we show the code that will make use of this <kbd class="calibre9">save_png()</kbd> function and encapsulate the knowledge of the function that is used for each case. In the case of the histograms, the <kbd class="calibre9">histogram()</kbd> function shown in the following code simply wraps the <kbd class="calibre9">hist()</kbd> function used to create the graph with a common interface that will also be used by the other strategies (the <kbd class="calibre9">quantile_quantile()</kbd> function shown in the following code in this case). This common interface allows us to use these strategies as plugins that can be substituted easily as we do in the corresponding <kbd class="calibre9">variable_histogram()</kbd> and <kbd class="calibre9">variable_qqplot()</kbd> functions (they both do the same call, but use a different strategy in each case). As you can see, other details that are not part of the common interface (for example, <kbd class="calibre9">main</kbd> and <kbd class="calibre9">xlab</kbd>) are handled within each strategy's code. We could add them as optional arguments if we wanted to, but it's not necessary for this example:</p>
<pre class="mce-root">variable_histogram &lt;- function(data, variable, save_to = "") {
    save_png(data, variable, save_to, histogram)
}

histogram &lt;- function(data, variable) {
    hist(data[, variable], main = "Histogram", xlab = "Proportion")
}

variable_qqplot &lt;- function(data, variable, save_to = "") {
    save_png(data, variable, save_to, quantile_quantile)
}

quantile_quantile &lt;- function(data, variable) {
    qqnorm(data[, variable], main = "Normal QQ-Plot for Proportion")
    qqline(data[, variable])
}</pre>
<p class="calibre2">The following shows the graph for checking proportion normality:</p>
<div class="cdpaligncenter"><img src="../images/00023.jpeg" class="calibre22"/></div>
<pre class="mce-root">quantile_quantile &lt;- function(data, variable) {<br class="title-page-name"/>    qqnorm(data[, variable], main = "Normal QQ-Plot for Proportion")<br class="title-page-name"/>    qqline(data[, variable])<br class="title-page-name"/>}</pre>
<p class="calibre2">If we wanted to share the code used to create the PNG images with a third (or more) strategies, then we can simply add a strategy wrapper for each new case without worrying about duplicating the code that creates the PNG images. It may seem that this is not a big deal, but imagine that the code used to create the PNG files was complex and suddenly you found a bug. What would you need to fix that bug? Well, you'd have to go to every place where you duplicated the code and fix it there. Doesn't seem very efficient. Now, what happens if you no longer want to save PNG files and want to instead save JPG files? Well, again, you would have to go everywhere you have duplicated your code and change it. Again, not very efficient. As you can see, this way of programming requires a little investment upfront (creating the common interfaces and providing wrappers), but the benefit of doing so will pay for itself through the saved time, you do need to change the code, if only once, as well as more understandable and simpler code. This is a form of <strong class="calibre1">dependency management</strong> and is something you should learn how to do to become a more efficient programmer.</p>
<p class="calibre2">You may have noticed that in the previous code, we could have avoided one function call by having the user call directly the <kbd class="calibre9">save_png()</kbd> function. However, doing so would require the user to have knowledge of two things, the <kbd class="calibre9">save_png()</kbd> function to save the image and the <kbd class="calibre9">quantile_quantile()</kbd> or <kbd class="calibre9">histogram()</kbd> functions to produce the plots, depending on what she was trying to plot. This extra burden in the user, although seemingly not problematic, could make things very confusing for her since not many users are used to sending functions as arguments, and they would have to know two function signatures, instead of one.</p>
<p class="calibre2">Providing a wrapper whose signature is easily usable as we do with <kbd class="calibre9">variable_histogram()</kbd> and <kbd class="calibre9">variable_qqplot()</kbd> makes it easier on the user, and allows us to expand the way we want to show graphs in case we want to change that later without making the user learn a new function signature.</p>
<p class="calibre2">To actually produce the plots we're looking for, we use the following code:</p>
<pre class="mce-root">variable_histogram(data = data, variable = "Proportion")
variable_qqplot(data = data, variable = "Proportion")</pre>
<p class="calibre2">As you can see, the histogram shows an approximate normal distribution slightly skewed towards the right, but we can easily accept it as being normal. The corresponding quantile-quantile plot shows the same information in a slightly different way. The line it shows corresponds to the quantiles of the normal distribution, and the dots show the actual distribution in the data. The closer these dots are to the line, the closer the variable's distribution is to being normally distributed. As we can see, for the most part, <kbd class="calibre9">Proportion</kbd> is normally distributed, and it's at the extremes that we can see a slight deviation, which probably comes from the fact that our <kbd class="calibre9">Proportion</kbd> variable actually has hard limits at 0 and 1. However, we can also accept it as being normally distributed, and we can proceed to the next assumption safely.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Checking homoscedasticity with residual plots</h1>
                
            
            <article>
                
<p class="calibre2"><strong class="calibre1">Homoscedasticity</strong> simply means that we need the data to have constant variance in our residuals. To check for it, we can use the <kbd class="calibre9">plot(fit)</kbd> function call. However, this will show one plot at a time asking you to hit <em class="calibre19">Enter</em> on your keyboard to show the next one. This kind of mechanism is not friendly to the automation processes we are creating. So we need a little adjustment. We will use the <kbd class="calibre9">par(mfrow = c(2, 2))</kbd> call to tell the <kbd class="calibre9">plot()</kbd> function to graph all four plots at the same time and show it in a single image. We wrap the command around our already familiar mechanism to save PNGs around the <kbd class="calibre9">fit_plot()</kbd> function, and we're all set:</p>
<pre class="mce-root">fit_plot &lt;- function(fit, save_to = "") {
    if (not_empty(save_to)) png(save_to)
    par(mfrow = c(2, 2))
    plot(fit)
    if (not_empty(save_to)) dev.off()
}</pre>
<p class="calibre2">With the <kbd class="calibre9">fit_plot()</kbd> function in place, we can show the regressions graphical results with the following:</p>
<pre class="mce-root">fit_plot(fit)</pre>
<div class="cdpaligncenter"><img src="../images/00024.jpeg" class="calibre43"/></div>
<p class="calibre2">The information we're looking for is in the plots on the left-hand side, where we see fitted values in the x axis and residuals in the y axis. In these plots, we are looking for residuals to be randomly distributed in a tubular pattern, indicated by the dotted lines. We do not want residuals with a pattern that looks similar to a fan or funnel or in any way curvilinear. As we can see, the pattern we see does resemble a tubular pattern, so we can say the assumption of homoscedasticity holds for the data. As an extra, you can also see, in the top-right quantile-quantile plot, that the residuals follow a normal distribution which is also good. The plot on the lower-right shows a statistics concept, which we won't go into, called Cook's distance, which is used to find <em class="calibre19">influential</em> observations in a regression. To read more about it, you may look at John Fox's, <em class="calibre19">Regression Diagnostics, 1991</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Checking no collinearity with correlations</h1>
                
            
            <article>
                
<p class="calibre2">To check no collinearity, we could use a number of different techniques. For example, for those familiar with linear algebra, the condition number is a measure of how singular a matrix is, where singularity would imply perfect collinearity among the covariates. This number could provide a measure of this collinearity. Another technique is to use the <em class="calibre19">Variance Inflation Factor</em>, which is a more formal technique that provides a measure of how much a regression's variance is increased because of collinearity. Another, and a more common, way of checking this is with simple correlations. Are any variables strongly correlated among themselves in the sense that there could be a direct relation among them? If so, then we may have a multicollinearity problem. To get a sense of how correlated our variables are, we will use the correlations matrix techniques shown in <a href="part0059.html#1O8H60-f494c932c729429fb734ce52cafce730" class="calibre4">Chapter 2</a>, <em class="calibre19">Understanding Votes with Descriptive Statistics</em>.</p>
<p class="calibre2">The following code shows how correlations work in R:</p>
<pre class="mce-root">library(corrplot)
corrplot(corr = cor(data[, numerical_variables]), tl.col = "black", tl.cex = 0.6)</pre>
<p class="calibre2">As you can see, the strong correlations (either positive or negative) are occurring intra-groups not inter-groups, meaning that variables that measure the same thing in different ways appear to be highly correlated, while variables that measure different things don't appear to be highly correlated.</p>
<div class="cdpaligncenter"><img class="aligncenter1" src="../images/00025.jpeg"/></div>
<p class="calibre2">For example, <kbd class="calibre9">Age_18to44</kbd> and <kbd class="calibre9">Age_45plus</kbd> are variables that measure age, and we expect them to have a negative relation since the higher the percentage of young people in a ward is, by necessity, the percentage of older people is lower. The same relation can be seen in the housing group (<kbd class="calibre9">Owned</kbd>, <kbd class="calibre9">OwnedOutright</kbd>, <kbd class="calibre9">SocialRent</kbd>, and <kbd class="calibre9">PrivateRent</kbd>), the employment group (<kbd class="calibre9">Unemp</kbd>, <kbd class="calibre9">UnempRate_EA</kbd>, and <kbd class="calibre9">HigherOccup</kbd>), the deprived group (<kbd class="calibre9">Deprived</kbd> and <kbd class="calibre9">MultiDepriv</kbd>), ethnic group (<kbd class="calibre9">White</kbd> and <kbd class="calibre9">NonWhite</kbd>), the residency group (<kbd class="calibre9">Residents</kbd> and <kbd class="calibre9">Households</kbd>), and the education group (<kbd class="calibre9">LowEducationLevel</kbd> and <kbd class="calibre9">HighEducationLevel</kbd>). If you pick variables belonging to different groups, the number of strong correlations is significantly lower, but it's there. For example, <kbd class="calibre9">HigherOccup</kbd> is strongly correlated to <kbd class="calibre9">HighEducationLevel</kbd> and <kbd class="calibre9">LowEducationLevel</kbd>, positively and negatively, respectively. Also, variables in the housing group seem to be correlated with variables in the age group. These kinds of relations are expected and natural since highly educated people will most probably have better jobs, and young people probably can't afford a house yet, so they rent. As analysts, we can assume that these variables are in fact measuring different aspects of society and continue on with our analysis. However, these are still things you may want to keep in mind when interpreting the results, and we may also want to only include one of the variables in each group to avoid inter-group collinearity, but we'll avoid these complexities and continue with our analysis for now.</p>
<p class="calibre2">Linear regression is one of those types of models that require criteria from the analyst to be accepted or rejected. In our specific case, it seems that our model's assumptions are valid enough and we may safely use it to provide credible predictions as we will do in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Measuring accuracy with score functions</h1>
                
            
            <article>
                
<p class="calibre2">Now that we have checked our model's assumptions, we turn toward measuring it's predictive power. To measure our predictive accuracy, we will use two methods, one for numerical data (<kbd class="calibre9">Proportion</kbd>) and the other for categorical data (<kbd class="calibre9">Vote</kbd>). We know that the <kbd class="calibre9">Vote</kbd> variable is a transformation from the <kbd class="calibre9">Proportion</kbd> variable, meaning that we are measuring the same information in two different ways. However, both numerical and categorical data are frequently encountered in data analysis, and thus we wanted to show both approaches here. Both functions, <kbd class="calibre9">score_proportions()</kbd> (numerical) and <kbd class="calibre9">score_votes()</kbd> (categorical) receive the data we use for testing and the predictions for each of the observations in the testing data, which come from the model we built in previous sections.</p>
<p class="calibre2">In the numerical case, <kbd class="calibre9">score_proportions()</kbd> computes a score using the following expression:</p>
<div class="cdpaligncenter"><img src="../images/00026.jpeg" class="calibre22"/></div>
<p class="calibre2">Here, <kbd class="calibre9">Y_i</kbd> is the <em class="calibre19">real response</em> variable value for the <em class="calibre19">i</em>th observation in the testing data, <kbd class="calibre9">Y'_i</kbd> is our prediction for that same observation, <kbd class="calibre9">SE</kbd> is our prediction's standard error, and <kbd class="calibre9">n</kbd> is the number of observations in the testing data. This equation establishes that the score, which we want to minimize, is the average of <em class="calibre19">studentized residuals</em>. Studentized residuals, as you may know, are residuals divided by a measure of the standard errors. This formula gives us an average measure of how close we are to predicting an observation's value correctly relative to the variance observed for that data range. If we have a high degree of variance (resulting in high standard errors), we don't want to be too strict with the prediction, but if we are in a low-variance area, we want to make sure that our predictions are very accurate:</p>
<pre class="mce-root">score_proportions &lt;- function(data_test, predictions) {
    # se := standard errors
    se &lt;- predictions$se.fit
    real &lt;- data_test$Proportion
    predicted &lt;- predictions$fit
    return(sum((real - predicted)^2 / se^2) / nrow(data))
}</pre>
<p class="calibre2">In the categorical case, <kbd class="calibre9">score_votes()</kbd> computes a score by simply counting the number of times our predictions pointed toward the correct category, which we want to maximize. We do that by first using the same classification mechanism (if the predicted <kbd class="calibre9">Proportion</kbd> is larger than 0.5, then we classify it as a <kbd class="calibre9">"Leave"</kbd> vote and vice versa), and compare the categorical values. We know that the sum of Boolean vector will be equal to the number of <kbd class="calibre9">TRUE</kbd> values, and that's what we're using in the <kbd class="calibre9">sum(real == predicted)</kbd> expression:</p>
<pre class="mce-root">score_votes &lt;- function(data_test, predictions) {
    real &lt;- data_test$Vote
    predicted &lt;- ifelse(predictions$fit &gt; 0.5, "Leave", "Remain")
    return(sum(real == predicted))
}</pre>
<p class="calibre2">To test our model's scores, we do the following:</p>
<pre class="mce-root">predictions &lt;- predict(fit, data_test, se.fit = TRUE)

score_proportions(data_test, predictions)<br class="title-page-name"/><strong class="calibre1">#&gt; [1] 10.66</strong>
score_votes(data_test, predictions)<br class="title-page-name"/><strong class="calibre1">#&gt; [1] 216</strong>
nrow(data_test)<br class="title-page-name"/><strong class="calibre1">#&gt; [1] 241</strong></pre>
<p class="calibre2">In the case of the <kbd class="calibre9">score_votes()</kbd> function, the measure by itself tells us how well we are doing with our predictions since we can take the number of correct predictions (the output of the function call, which is 216), and divide it by the number of observations (rows) in the <kbd class="calibre9">data_test</kbd> object (which is 241). This gives us a precision of 89%. This means that if we are given the data from the regressors but we don't know how the ward actually voted, 89% of the time, we would provide a prediction for whether they wanted to leave or remain in the EU, which would be correct. This is pretty good if you ask me.</p>
<p class="calibre2">In the case of the <kbd class="calibre9">score_proportions()</kbd> function, since we're using a more abstract measure to be able to know how good we're doing, we would like to compare it against other model's scores and get a relative sense of the model's predictive power, and that's exactly what we'll do in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Programatically finding the best model</h1>
                
            
            <article>
                
<p class="calibre2">Now that we have seen how to produce scores that represent how good or bad a model's predictive power is, you may go ahead and start specifying lots of models manually by changing the combinations of variables sent to the <kbd class="calibre9">lm()</kbd> function, compute each model's scores, and then choose the ones with the highest predictive power. This can potentially take a large amount of time, and you may want to delegate it to someone else since it's tedious work. However, fear not. There's a better way! Computers are good at repetitive and tedious tasks, and now we'll see how to tell the computer to find the best model for us with a little bit of programming.</p>
<p class="calibre2">The following sections will increase the programming level, but don't worry we'll explain the code in detail to make sure that everything is understood. If at any point you feel confused, you can always copy-paste small snippets of code into your R terminal and see what each of them are doing individually to gradually get a sense of the whole thing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Generating model combinations</h1>
                
            
            <article>
                
<p class="calibre2">The first thing we need to do is develop a way of getting the combinations of regressors we want to test. Since this is a combinatorial problem, the number of combinations is exponential with the number of available options. In our case, with the 19 available variables, the number of possible models is the sum of the number of models we can create with one regressor plus the number of models we can create with two regressors, and so on, until we sum the number of models we can create with all 19 regressors. This is what the sum is:</p>
<div class="cdpaligncenter"><img class="fm-editor-equation" src="../images/00027.jpeg"/></div>
<p class="calibre2">Of course, computing so many models, although easy for a computer, may take a while, so we want to limit the minimum and maximum number of regressors allowed in the combinations. To do so, we specify the minimum and maximum percentage of regressors that will be included in the <kbd class="calibre9">min_percentage</kbd> and <kbd class="calibre9">max_percentage</kbd> parameters, respectively. In our case, if we specify <kbd class="calibre9">min_percentage = 0.9</kbd> and <kbd class="calibre9">max_percentage = 1.0</kbd>, we're asking for all combinations that contain between 17 and 19 of the regressors, which adds up to 191 models. Imagine the time it would take you to generate 191 model specifications manually! Hopefully thinking about that will make you realize the power of this technique.</p>
<p class="calibre2">To start, we create the <kbd class="calibre9">generate_combinations_unvectorized()</kbd> function that will output the a list with all the possible combinations given the <kbd class="calibre9">variables</kbd> and the <kbd class="calibre9">min_percentage</kbd> and <kbd class="calibre9">max_percentage</kbd> parameters mentioned earlier. The first thing we do is remove the <kbd class="calibre9">Proportion</kbd> variable by specifying it as <kbd class="calibre9">FALSE</kbd> in the <kbd class="calibre9">variables</kbd> vector (the <kbd class="calibre9">variables</kbd> object here corresponds to the <kbd class="calibre9">numerical_variables</kbd> object, but we have adjusted its name within this function to make it more readable). The other unwanted variables (<kbd class="calibre9">NVotes</kbd>, <kbd class="calibre9">Leave</kbd>, <kbd class="calibre9">Vote</kbd>, and <kbd class="calibre9">RegionName</kbd>) were removed in the <kbd class="calibre9">get_numerical_variable_names()</kbd> function at the beginning of the chapter. Next, we get the actual names of the variables with <kbd class="calibre9">TRUE</kbd> values so that we can work with string and not Boolean. After that, we compute the total number of variables as <kbd class="calibre9">n</kbd>, and the actual number of variables we will include in the combinations by taking the percentage parameters, multiplying them by the number of variables, and getting either the <em class="calibre19">floor</em> or <em class="calibre19">ceiling</em> for that number to make sure that we include the extremes. After that, we initialize the <kbd class="calibre9">all_combinations</kbd> object that will contain the list of combinations we want. The next part is the progress bar object that we won't explain as we have used it before.</p>
<p class="calibre2">The actual work is done inside the <kbd class="calibre9">for</kbd> loop. Notice that it goes from the minimum to the maximum number of variables we want inside our combinations. In each iteration, we compute the number of combinations which is returned to us as a matrix where each column represents a different combination and each row contains the index of the variables for that particular combination. This means that we need to add each of those columns to our total list of combinations (<kbd class="calibre9">all_combinations</kbd>), which is what we do inside the nested <kbd class="calibre9">for</kbd> loop. Finally, since we have nested lists, we want to use the <kbd class="calibre9">unlist()</kbd> function to bring them to the <em class="calibre19">same level</em>, but we don't want to do it recursively because we would just end with a single long list and we wouldn't be able to differentiate one combination from another.</p>
<p class="calibre2">I encourage you to change the return statement to avoid using the <kbd class="calibre9">recursive = FALSE</kbd> parameter, as well as avoiding the use of the <kbd class="calibre9">unlist()</kbd> function at all. Doing so will quickly show you what effect they have on the function's output, and why we need them.</p>
<pre class="mce-root">library(progress)

generate_combinations_unvectorized &lt;- function(variables, min_percentage, max_percentage) {
    variables[["Proportion"]] &lt;- FALSE
    variables                 &lt;- names(variables[variables == TRUE])
    n                         &lt;- length(variables)
    n_min                     &lt;- floor(n * min_percentage)
    n_max                     &lt;- ceiling(n * max_percentage)
    all_combinations          &lt;- NULL

    progress_bar &lt;- progress_bar$new(
        format = "Progress [:bar] :percent ETA: :eta",
        total = length(n_min:n_max)
    )

    for (k in n_min:n_max) {
        progress_bar$tick()
        combinations &lt;- combn(variables, k)
        for (column in 1:ncol(combinations)) {
            new_list &lt;- list(combinations[, column])
            all_combinations &lt;- c(all_combinations, list(new_list))
        }
    }
    return(unlist(all_combinations, recursive = FALSE))
}</pre>
<p class="calibre2">A sample output of the object that the <kbd class="calibre9">generate_combinations_unvectorized()</kbd> function does is shown next. As you can see, it's a list where each element is a vector or type <kbd class="calibre9">character</kbd>. The first combination created contains only 17 variables, which is the minimum number of variables used when the total number of variables is 19 and the minimum percentage requested is 90%. The last combination (combination number 191), contains all 19 variables and corresponds to the model we built manually earlier in this chapter:</p>
<pre class="mce-root">combinations &lt;- generate_combinations_unvectorized(
    numerical_variables, 0.9, 1.0
)

combinations
[[1]]
 [1] "Residents"     "Households"    "White"         "Owned"
 [5] "OwnedOutright" "SocialRent"    "PrivateRent"   "Students"
 [9] "Unemp"         "UnempRate_EA"  "HigherOccup"   "Density"
[13] "Deprived"      "MultiDepriv"   "Age_18to44"    "Age_45plus"
[17] "NonWhite"

...

[[191]]
 [1] "Residents"          "Households"         "White"
 [4] "Owned"              "OwnedOutright"      "SocialRent"
 [7] "PrivateRent"        "Students"           "Unemp"
[10] "UnempRate_EA"       "HigherOccup"        "Density"
[13] "Deprived"           "MultiDepriv"        "Age_18to44"
[16] "Age_45plus"         "NonWhite"           "HighEducationLevel"
[19] "LowEducationLevel"</pre>
<p class="calibre2">Getting only those combinations that contain between 90% and 100% of the variables may seem a bit restrictive. What if we want to generate all possible combinations? In that case, we would change the first parameter to be 0, but it may not finish in a practical amount of time. The reason is that our <kbd class="calibre9">generate_combinations_unvectorized()</kbd> function, as the name implies, is not vectorized, and even worse, has nested <kbd class="calibre9">for</kbd> loops. This is a huge bottleneck in this particular case, and it's something you want to look out for in your own code. One possible solution is to make a <em class="calibre19">vectorized</em> version of the function. For those of you interested, we have included a file named <kbd class="calibre9">vectorized_vs_unvectorized.R</kbd> in this book's code repository (<a href="https://github.com/PacktPublishing/R-Programming-By-Example" class="calibre4">https://github.com/PacktPublishing/R-Programming-By-Example</a>), that shows the said implementation. We also include some tests that will show you just how much faster the vectorized implementation is. Just to give you a spoiler, it can be hundreds of times faster! For those cases where vectorizing and other approaches that only depend on R itself are not good enough, you can try delegating the task to a faster (compiled) language. We will see how to do that in <a href="part0229.html#6QCGQ0-f494c932c729429fb734ce52cafce730" class="calibre4">Chapter 9</a>, <em class="calibre19">Implementing an Efficient Simple Moving Average.</em></p>
<p class="calibre2">Going back to our example, the next thing to do is to create the <kbd class="calibre9">find_best_fit()</kbd> function, which will go through each of the combinations generated, use the <kbd class="calibre9">data_train</kbd> data to train a model with the corresponding combination, test it's accuracy with the <kbd class="calibre9">measure</kbd> selection (either <kbd class="calibre9">Proportion</kbd> (numerical) or <kbd class="calibre9">Vote</kbd> (categorical)) and will save the corresponding score in a <kbd class="calibre9">scores</kbd> vector. Then, it will find the index of the optimal score by either finding the minimum or maximum score, depending on the <kbd class="calibre9">measure</kbd> selection we're using (<kbd class="calibre9">Proportion</kbd> requires us to minimize while <kbd class="calibre9">Vote</kbd> requires us to maximize), and finally it will recreate the optimal model, print it's information, and return the model to the user. The <kbd class="calibre9">compute_model_and_fit()</kbd>, <kbd class="calibre9">compute_score()</kbd>, and <kbd class="calibre9">print_best_model_info()</kbd> functions will be developed next as we're following a top-down approach:</p>
<pre class="mce-root">find_best_fit &lt;- function(measure, data_train, data_test, combinations) {
    n_cases &lt;- length(combinations)
    progress_bar &lt;- progress_bar$new(
        format = "Progress [:bar] :percent ETA: :eta",
        total = n_cases
    )
    scores &lt;- lapply(1:n_cases, function(i) {
        progress_bar$tick()
        results &lt;- compute_model_and_fit(combinations[[i]], data_train)
        score &lt;- compute_score(measure, results[["fit"]], data_test)
        return(score)
    })
    i &lt;- ifelse(measure == "Proportion", which.min(scores), which.max(scores))
    best_results &lt;- compute_model_and_fit(combinations[[i]], data_train)
    best_score &lt;- compute_score(measure, best_results[["fit"]], data_test)
    print_best_model_info(i, best_results[["model"]], best_score, measure)
    return(best_results[["fit"]])
}</pre>
<p class="calibre2">Next, we create the <kbd class="calibre9">compute_model_and_fit()</kbd> function, which simply generates the formula for the selected combination and uses it within the <kbd class="calibre9">lm()</kbd> function. As you can see in the <kbd class="calibre9">combinations</kbd> object, we returned previously from the <kbd class="calibre9">generate_combinations_unvectorized()</kbd> function, it's a list with character vectors, it's not a formula we can pass to the <kbd class="calibre9">lm()</kbd> function; this is why we need the <kbd class="calibre9">generate_model()</kbd> function, which will take on of these vectors, and concatenate its elements into a single string with the plus (<kbd class="calibre9">+</kbd>) sign between them by using the <kbd class="calibre9">paste()</kbd> function with the <kbd class="calibre9">collapse = " + "</kbd> argument, and it will prepend the <kbd class="calibre9">Proportion ~</kbd> string to it. This gives us back a formula object specified by a string like <kbd class="calibre9">Proportion ~ Residents + ... + NonWhite</kbd>, which contains, instead of the dots, all the variables in the first combination shown in the preceding code. This string is then used inside the <kbd class="calibre9">lm()</kbd> function to execute our regression, and both <kbd class="calibre9">model</kbd> and <kbd class="calibre9">fit</kbd> are returned within a list to be used in the following steps:</p>
<pre class="mce-root">compute_model_and_fit &lt;- function(combination, data_train) {
    model &lt;- generate_model(combination)
    return(list(model = model, fit = lm(model, data_train)))
}

generate_model &lt;- function(combination) {
    sum &lt;- paste(combination, collapse = " + ")
    return(formula(paste("Proportion", "~", sum)))
}</pre>
<p class="calibre2">As can be seen by the <kbd class="calibre9">score &lt;- compute_score(measure, results[["fit"]], data_test)</kbd> line, the <kbd class="calibre9">compute_score()</kbd> function receives a <kbd class="calibre9">measure</kbd> object, a <kbd class="calibre9">fit</kbd> object (which comes from the <kbd class="calibre9">results</kbd> list), and the data used for testing. It computes the score using the <em class="calibre19">strategy</em> pattern mentioned earlier for the plots used to check the normality assumption. Basically, depending on the value of the <kbd class="calibre9">measure</kbd> string (the chosen strategy), it will choose one of the two functions that share the same signature, and that function will be used to compute the final predictions. We send the <kbd class="calibre9">se.fit = TRUE</kbd> parameter to the <kbd class="calibre9">predict()</kbd> function we had seen before because we want the standard errors to also be sent in case we use the numerical score which requires them. The <kbd class="calibre9">score_proportions()</kbd> and <kbd class="calibre9">score_votes()</kbd> functions were defined previously in this chapter:</p>
<pre class="mce-root">compute_score &lt;- function(measure, fit, data_test) {
    if (measure == "Proportion") {
        score &lt;- score_proportions
    } else {
        score &lt;- score_votes
    }
    predictions &lt;- predict(fit, data_test, se.fit = TRUE)
    return(score(data_test, predictions))
}</pre>
<p class="calibre2">Finally, we create a little convenience function called <kbd class="calibre9">print_best_model_info()</kbd> that will print results about the best model found. It simply takes the index of the best model, the model formula, its score, and the measure type, and prints all of that for the user. As you can see, since the <kbd class="calibre9">model</kbd> object is not a simple string but a <em class="calibre19">formula</em> object, we need to work a little with it to get the results we want by converting it into a string and splitting it using the plus sign (<kbd class="calibre9">+</kbd>) we know is included; otherwise, it would be a very long string:</p>
<pre class="mce-root">print_best_model_info &lt;- function(i, model, best_score, measure){
    print("*************************************")
    print(paste("Best model number:", i))
    print(paste("Best score:       ", best_score))
    print(paste("Score measure:    ", measure))
    print("Best model:")
    print(strsplit(toString(model), "\\+"))
    print("*************************************")
}</pre>
<p class="calibre2">We can find the best model, according to the <kbd class="calibre9">Proportion</kbd> measure by calling the following:</p>
<p class="calibre2"> </p>
<pre class="mce-root">best_lm_fit_by_proportions &lt;- find_best_fit(
    measure = "Proportion",
    data_train = data_train,
    data_test = data_test,
    combinations = combinations
)<br class="title-page-name"/><strong class="calibre1">#&gt; [1] "*************************************"</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [1] "Best model number: 3"</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [1] "Best score:        10.2362983528259"</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [1] "Score measure:     Proportion"</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [1] "Best model:"</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [[1]]</strong><br class="title-page-name"/><strong class="calibre1">#&gt;  [1] "~, Proportion, Residents " " Households "</strong><br class="title-page-name"/><strong class="calibre1">#&gt;  [3] " White "                   " Owned "</strong><br class="title-page-name"/><strong class="calibre1">#&gt;  [5] " OwnedOutright "           " SocialRent "</strong><br class="title-page-name"/><strong class="calibre1">#&gt;  [7] " PrivateRent "             " Students "</strong><br class="title-page-name"/><strong class="calibre1">#&gt;  [9] " Unemp "                   " UnempRate_EA "</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [11] " HigherOccup "             " Density "</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [13] " Deprived "                " MultiDepriv "</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [15] " Age_18to44 "              " Age_45plus "</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [17] " LowEducationLevel"</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [1] "*************************************"</strong><br class="title-page-name"/></pre>
<p class="calibre2">As we can see, the best model was the third one out of the 191 models, and it had a score of 10.23. We can also see the regressors used in the model. As you can see, <kbd class="calibre9">NonWhite</kbd> and <kbd class="calibre9">HighEducationLevel</kbd> were left out by the optimization method, probably due to their counterparts containing all the information necessary for their respective groups. It's no coincidence that those are among the most representative variables in the data.</p>
<p class="calibre2">To find the best model according to the <kbd class="calibre9">Vote</kbd> measure, we use the following code. Note that given the good techniques we used to create this function, all we have to do is change the value of the <kbd class="calibre9">measure</kbd> parameter to optimize our search using a different approach:</p>
<pre class="mce-root">best_lm_fit_by_votes &lt;- find_best_fit(
    measure = "Vote",
    data_train = data_train,
    data_test = data_test,
    combinations = combinations
)<br class="title-page-name"/><strong class="calibre1">#&gt; [1] "*************************************"</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [1] "Best model number: 7"</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [1] "Best score:        220"</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [1] "Score measure:     Vote"</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [1] "Best model:"</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [[1]]</strong><br class="title-page-name"/><strong class="calibre1">#&gt;  [1] "~, Proportion, Residents " " Households "</strong><br class="title-page-name"/><strong class="calibre1">#&gt;  [3] " White "                   " Owned "</strong><br class="title-page-name"/><strong class="calibre1">#&gt;  [5] " OwnedOutright "           " SocialRent "</strong><br class="title-page-name"/><strong class="calibre1">#&gt;  [7] " PrivateRent "             " Students "</strong><br class="title-page-name"/><strong class="calibre1">#&gt;  [9] " Unemp "                   " UnempRate_EA "</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [11] " HigherOccup "             " Density "</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [13] " Deprived "                " MultiDepriv "</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [15] " Age_45plus "              " NonWhite "104</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [17] " HighEducationLevel"</strong><br class="title-page-name"/><strong class="calibre1">#&gt; [1] "*************************************"</strong><br class="title-page-name"/></pre>
<p class="calibre2">In this case, the best model was the seventh one out of the 191 models, with 220 out of 241 correct predictions, which gives us an accuracy of 91%, an improvement given the accuracy we had computed earlier in the chapter. In this case, <kbd class="calibre9">LowEducationLevel</kbd> and <kbd class="calibre9">Age_18to44</kbd> were left out. Again, no coincidence that these are part of the most important variables in the data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Predicting votes from wards with unknown data</h1>
                
            
            <article>
                
<p class="calibre2">Now that we know how to train our models and find the best one possible, we will provide predictions for those wards for which we don't have voting data using the best models we found using the <kbd class="calibre9">Vote</kbd> measure. To do so, we simply execute the following line:</p>
<pre class="mce-root">predictions &lt;- predict(best_lm_fit_by_votes, data_incomplete)

predictions<br class="title-page-name"/><strong class="calibre1">#&gt;    804    805    806    807    808    809    810    811    812    813</strong><br class="title-page-name"/><strong class="calibre1">#&gt; 0.6845 0.6238 0.5286 0.4092 0.5236 0.6727 0.6322 0.6723 0.6891 0.6004</strong><br class="title-page-name"/><strong class="calibre1">#&gt;    814    815    816    817    818    819    820    821    822    823</strong><br class="title-page-name"/><strong class="calibre1">#&gt; 0.6426 0.5854 0.6966 0.6073 0.4869 0.5974 0.5611 0.4784 0.5534 0.6151</strong><br class="title-page-name"/><strong class="calibre1">(Truncated output)</strong><br class="title-page-name"/></pre>
<p class="calibre2">This will take the best model we found earlier using the <kbd class="calibre9">Votes</kbd> measure and use it to generate predictions for the <kbd class="calibre9">Proportion</kbd> variable in the <kbd class="calibre9">data_incomplete</kbd> data, which contains those observations for which we don't have any voting data. These are the best predictions we can provide with what we have done so far and we can expect them to have a 91% accuracy when used to categorize the <kbd class="calibre9">Proportion</kbd> variable into the <kbd class="calibre9">Vote</kbd> variable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">This chapter showed how to use multiple linear regression models, one of the most commonly used family of models, to predict numerical and categorical data. Our focus was on showing programming techniques that allow analysts to be more efficient in the projects while keeping their code quality high. We did so by showing how to create different model combinations programatically, measuring the predictive accuracy, and selecting the best one. The techniques used can easily be used with other, more advanced, types of models, and we encourage you to try to improve on the predictive accuracy by using other families of models. In the code that accompanies this book (<a href="https://github.com/PacktPublishing/R-Programming-By-Example" class="calibre4">https://github.com/PacktPublishing/R-Programming-By-Example</a>), you can find an implementation that also uses generalized linear models to produce predictions.</p>
<p class="calibre2">In the following chapter, we will start working with a different and slightly less technical example that uses product data from a hypothetical company to show how to work with manipulative data in a variety of ways and use it with many kinds of visualizations, including 3D, interactive, and geospatial graphs.</p>


            </article>

            
        </section>
    </body></html>