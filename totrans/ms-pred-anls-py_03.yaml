- en: Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the natural questions to ask about a dataset is if it contains groups.
    For example, if we examine financial market data consisting of stock price fluctuations
    over time, are there groups of stocks that fall and rise with a similar pattern?
    Similarly, for a set of customer transactions from an e-commerce business we might
    ask if are there groups of user accounts distinguished by patterns of similar
    purchasing activity? By identifying groups of related items using the methods
    described in this chapter, we can understand data as a set of general patterns
    rather than just individual points. These patterns can help in making high-level
    summaries at the outset of a predictive modeling project, or as an ongoing way
    to report on the shape of the data we are modeling. The groupings produced can
    serve as insights themselves, or they can provide starting points for the models
    we will cover in later chapters. For example, the group to which a datapoint is
    assigned can become a feature of this observation, adding additional information
    beyond its individual values. Additionally, we can potentially calculate statistics
    (such as mean and standard deviation) item features within these groups, which
    may be more robust as model features than individual entries.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the methods we will discuss in later chapters, grouping or *clustering*
    algorithms are known as **unsupervised learning**, meaning we have no response
    value, such as a sale price or click-through rate, which is used to determine
    the optimal parameters of the algorithm. Rather, we identify similar datapoints
    using only the features, and as a secondary analysis might ask whether the clusters
    we identify share a common pattern in their responses (and thus suggest the cluster
    is useful in finding groups associated with the outcome we are interested in).
  prefs: []
  type: TYPE_NORMAL
- en: 'The task of finding these groups, or *clusters*, has a few common ingredients
    that vary between algorithms. One is a notion of distance or similarity between
    items in the dataset, which will allow us to quantitatively compare them. A second
    is the number of groups we wish to identify; this can be specified initially using
    domain knowledge, or determined by running an algorithm with different numbers
    of clusters. We can then identify the best number of clusters that describes a
    dataset through statistics, such as examining numerical variance within the groups
    determined by the algorithm, or by visual inspection. In this chapter we will
    dive into:'
  prefs: []
  type: TYPE_NORMAL
- en: How to normalize data for use in a clustering algorithm and to compute similarity
    measurements for both categorical and numerical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use k-means clustering to identify an optimal number of clusters by examining
    the squared error function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use agglomerative clustering to identify clusters at different scales
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using affinity propagation to automatically identify the number of clusters
    in a dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use spectral methods to cluster data with nonlinear relationships between
    points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarity and distance metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in clustering any new dataset is to decide how to compare the
    similarity (or dissimilarity) between items. Sometimes the choice is dictated
    by what kinds of similarity we are most interested in trying to measure, in others
    it is restricted by the properties of the dataset. In the following sections we
    illustrate several kinds of distance for numerical, categorical, time series,
    and set-based data—while this list is not exhaustive, it should cover many of
    the common use cases you will encounter in business analysis. We will also cover
    normalizations that may be needed for different data types prior to running clustering
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Numerical distance metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us begin by exploring the data in the `wine.data` file. It contains a set
    of chemical measurements describing the properties of different kinds of wines,
    and the quality level (I-III) to which the wines are assigned (Forina, M., et
    al. *PARVUS An Extendible Package for Data Exploration*. Classification and Correla
    (1988)). Open the file in an iPython notebook and look at the first few rows with
    the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![Numerical distance metrics](img/B04881_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that in this dataset we have no column descriptions, which makes the
    data hard to understand since we do not know what the features are. We need to
    parse the column names from the dataset description file `wine.names`, which in
    addition to the column names contains additional information about the dataset.With
    the following code, we generate a regular expression that will match a column
    name (using a pattern where a number followed by a parenthesis has a column name
    after it, as you can see in the list of column names in the file):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We then create an array where the first element is the class label of the wine
    (whether it belongs to quality category I-III).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterating through the lines in the file, we extract those that match our regular
    expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We then assign this list to the dataframe columns property, which contains
    the names of the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have appended the column names, we can look at a summary of the
    dataset using the `df.describe()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Numerical distance metrics](img/B04881_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Having performed some cleanup on the data, how can we calculate a similarity
    measurement between wines based on the information in each row? One option would
    be to consider each of the wines as a point in a thirteen-dimensional space specified
    by its dimensions (each of the columns other than `Class`). Since the resulting
    space has thirteen dimensions, we cannot directly visualize the datapoints using
    a scatterplot to see if they are nearby, but we can calculate distances just the
    same as with a more familiar 2- or 3-dimensional space using the Euclidean distance
    formula, which is simply the length of the straight line between two points. This
    formula for this length can be used whether the points are in a 2-dimensional
    plot or a more complex space such as this example, and is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Numerical distance metrics](img/B04881_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here *x* and *y* are rows of the dataset and *n* is the number of columns. One
    important aspect of the Euclidean distance formula is that columns whose scale
    is much different from others can dominate the overall result of the calculation.
    In our example, the values describing the magnesium content of each wine are `~100x`
    greater than the magnitude of features describing the alcohol content or ash percentage.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we were to calculate the distance between these datapoints, it would largely
    be determined by the magnesium concentration (as even small differences on this
    scale overwhelmingly determine the value of the distance calculation), rather
    than any of its other properties. While this might sometimes be desirable (for
    example, if the column with the largest numerical value is the one we most care
    about for judging similarity), in most applications we do not favor one feature
    over another and want to give equal weight to all columns. To get a fair distance
    comparison between these points, we need to normalize the columns so that they
    fall into the same numerical range (have similar maximal and minimal values).
    We can do so using the `scale()` function in scikit-learn and the following commands,
    which uses the array `header_names` we constructed previously to access all columns
    but the class label (the first element of the array):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Numerical distance metrics](img/B04881_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This function will subtract the mean value of a column from each element and
    then divide each point by the standard deviation of the column. This normalization
    centers the data in each column at mean 0 with variance 1, and in the case of
    normally distributed data this results in a standard normal distribution. Also,
    note that the `scale()` function returns a `numpy array`, which is why we must
    call `dataframe` on the output to use the pandas function `describe()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have scaled the data, we can calculate Euclidean distances between
    the rows using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You can verify that this command produces a square matrix of dimension 178
    x 178 (the number of rows in the original dataset by the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We have now converted our dataset of 178 rows and 13 columns into a square matrix,
    giving the distance between each of these rows. In other words, row i, column
    j in this matrix represents the Euclidean distance between rows i and j in our
    dataset. This 'distance matrix' is the input we will use for clustering inputs
    in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 'If want to get a sense of how the points are distributed relative to one another
    using a given distance metric, we can use **multidimensional scaling** (**MDS**)—(Borg,
    Ingwer, and Patrick JF Groenen. Modern multidimensional scaling: Theory and applications.
    Springer Science & Business Media, 2005; Kruskal, Joseph B. *Nonmetric multidimensional
    scaling: a numerical method.* Psychometrika 29.2 (1964): 115-129; Kruskal, Joseph
    B. *Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis.*
    Psychometrika 29.1 (1964): 1-27) to create a visualization. MDS attempts to find
    the set of lower dimensional coordinates (here, we will use two dimensions) that
    best represents the distances between points in the original, higher dimensions
    of a dataset (here, the pairwise Euclidean distances we calculated from the 13
    dimensions). MDS finds the optimal 2-d coordinates according to the strain function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Numerical distance metrics](img/B04881_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where `D` are the distances we calculated between points. The 2-d coordinates
    that minimize this function are found using **Singular Value Decomposition** (**SVD**),
    which we will discuss in more detail in [Chapter 6](ch06.html "Chapter 6. Words
    and Pixels – Working with Unstructured Data"), *Words and Pixels – Working with
    Unstructured Data*. After obtaining the coordinates from MDS, we can then plot
    the results using the `wine` class to color points in the diagram. Note that the
    coordinates themselves have no interpretation (in fact, they could change each
    time we run the algorithm due to numerical randomness in the algorithm). Rather,
    it is the relative position of points that we are interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Numerical distance metrics](img/B04881_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Given that there are many ways we could have calculated the distance between
    datapoints, is the Euclidean distance a good choice here? Visually, based on the
    multidimensional scaling plot, we can see there is separation between the classes
    based on the features we have used to calculate distance, so conceptually it appears
    that this is a reasonable choice in this case. However, the decision also depends
    on what we are trying to compare. If we are interested in detecting wines with
    similar attributes in absolute values, then it is a good metric. However, what
    if we're not interested so much in the absolute composition of the wine, but whether
    its variables follow similar trends among wines with different alcohol contents?
    In this case, we would not be interested in the absolute difference in values,
    but rather the *correlation* between the columns. This sort of comparison is common
    for time series, which we consider next.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation similarity metrics and time series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For time series data, we are often concerned with whether the patterns between
    series exhibit the same variation over time, rather than their absolute differences
    in value. For example, if we were to compare stocks, we might want to identify
    groups of stocks whose prices move up and down in similar patterns over time.
    The absolute price is of less interest than this pattern of increase and decrease.
    Let us look at an example using the variation in prices of stocks in the **Dow
    Jones Industrial Average** (**DJIA**) over time (Brown, Michael Scott, Michael
    J. Pelosi, and Henry Dirska. *Dynamic-radius species-conserving genetic algorithm
    for the financial forecasting of Dow Jones index stocks.* Machine Learning and
    Data Mining in Pattern Recognition. Springer Berlin Heidelberg, 2013\. 27-41.).
    Start by importing the data and examining the first rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Correlation similarity metrics and time series](img/B04881_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This data contains the daily stock price (for 6 months) for a set of 30 stocks.
    Because all of the numerical values (the prices) are on the same scale, we won't
    normalize this data as we did with the wine dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: We notice two things about this data. First, the closing price per week (the
    variable we will use to calculate correlation) is presented as a string. Second,
    the date is not in the correct format for plotting. We will process both columns
    to fix this, converting these columns to a `float` and `datetime` object, respectively,
    using the following commands.
  prefs: []
  type: TYPE_NORMAL
- en: To convert the closing price to a number, we apply an anonymous function that
    takes all characters but the dollar sign and casts it as a float.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To convert the date, we also use an anonymous function on each row of the date
    column, splitting the string in to year, month, and day elements and casting them
    as integers to form a tuple input for a `datetime` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'With this transformation, we can now make a pivot table (as we covered in [Chapter
    2](ch02.html "Chapter 2. Exploratory Data Analysis and Visualization in Python"),
    *Exploratory Data Analysis and Visualization in Python*) to place the closing
    prices for each week as columns and individual stocks as rows using the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![Correlation similarity metrics and time series](img/B04881_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, we only need columns after 2 to calculate correlations between
    rows, as the first two columns are the index and stock ticker symbol. Let us now
    calculate the correlation between these time series of stock prices by selecting
    the second column to end columns of the data frame for each row, calculating the
    pairwise correlations distance metric, and visualizing it using MDS, as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![Correlation similarity metrics and time series](img/B04881_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It is important to note that the Pearson coefficient, which we have calculated
    here, is a measure of linear correlation between these time series. In other words,
    it captures the linear increase (or decrease) of the trend in one price relative
    to another, but won''t necessarily capture nonlinear trends (such as a parabola
    or sigmoidal pattern). We can see this by looking at the formula for the Pearson
    correlation, which is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Correlation similarity metrics and time series](img/B04881_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here μ and σ represent the mean and standard deviation of series *a* and *b*.
    This value varies from 1 (highly correlated) to -1 (inversely correlated), with
    0 representing no correlation (such as a spherical cloud of points). You might
    recognize the numerator of this equation as the **covariance**, which is a measure
    of how much two datasets, *a* and *b*, vary in synch with one another. You can
    understand this by considering that the numerator is maximized when corresponding
    points in both datasets are above or below their mean value. However, whether
    this accurately captures the similarity in the data depends upon the scale. In
    data that is distributed in regular intervals between a maximum and minimum, with
    roughly the same difference between consecutive values it captures this pattern
    well. However, consider a case in which the data is exponentially distributed,
    with orders of magnitude differences between the minimum and maximum, and the
    difference between consecutive datapoints also varying widely. Here, the Pearson
    correlation would be numerically dominated by only the largest values in the series,
    which might or might not represent the overall similarity in the data. This numerical
    sensitivity also occurs in the denominator, which represents the product of the
    standard deviations of both datasets. The value of the correlation is maximized
    when the variation in the two datasets is roughly explained by the product of
    their individual variations; there is no left-over variation between the datasets
    that is not explained by their respective standard deviations. By extracting data
    for the first two stocks in this collection and plotting their pairwise values,
    we see that this assumption of linearity appears to be a valid one for comparing
    datapoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![Correlation similarity metrics and time series](img/B04881_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In addition to verifying that these stocks have a roughly linear correlation,
    this command introduces some new functions in pandas you may find useful. The
    first is `iloc`, which allows you to select indexed rows from a dataframe. The
    second is `transpose`, which inverts the rows and columns. Here, we select the
    first two rows, transpose, and then select all rows (prices) after the second
    (since the first is the index and the second is the *Ticker* symbol).
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the trend we see in this example, we could imagine there might a nonlinear
    trend between prices. In these cases, it might be better to measure not the linear
    correlation of the prices themselves, but whether the high prices for one stock
    coincide with another. In other words, the rank of market days by price should
    be the same, even if the prices are nonlinearly related. We can also calculate
    this rank correlation, also known as the Spearman''s Rho, using SciPy, with the
    following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Correlation similarity metrics and time series](img/B04881_03_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that this formula assumes that the ranks are distinct (no ties); in the
    case of ties, we can instead calculate the Pearson correlation using the ranks
    of the datasets instead of their raw values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Where *n* is the number of datapoints in each of two sets *a* and *b*, and
    *d* is the difference in ranks between each pair of datapoints *ai* and *bi*.
    Because we only compare the ranks of the data, not their actual values, this measure
    can capture variations between two datasets, even if their numerical value vary
    over wide ranges. Let us see if plotting the results using the Spearman correlation
    metric generates any differences in the pairwise distance of the stocks computed
    from MDS, using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![Correlation similarity metrics and time series](img/B04881_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Spearman correlation distances, based on the *x* and *y* axes, appear closer
    to each other than the Pearson distances, suggesting from the perspective of rank
    correlation that the time series are more similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'Though they differ in their assumptions about how two datasets are distributed
    numerically, Pearson and Spearman correlations share the requirement that the
    two sets are of the same length. This is usually a reasonable assumption, and
    will be true of most of the examples we consider in this book. However, for cases
    where we wish to compare time series of unequal lengths, we can use **Dynamic
    Time Warping** (**DTW**). Conceptually, the idea of DTW is to *warp* one time
    series to align with a second, by allowing us to open gaps in either dataset so
    that it becomes the same size as the second. What the algorithm needs to resolve
    is where the most similar points of the two series are, so that gaps can be places
    in the appropriate locations. In the simplest implementation, DTW consists of
    the following steps (see the following diagram):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Correlation similarity metrics and time series](img/B04881_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For dataset *a* of length *n* and a dataset *b* of length *m*, construct a matrix
    of size *n* by *m*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the top row and the leftmost column of this matrix to both be infinity (left,
    in figure above).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each point *i* in set *a*, and point *j* in set *b*, compare their similarity
    using a cost function. To this cost function, add the minimum of the element (*i-1,
    j-1*), (*i-1, j*), and (*j-1, i*)—that is, from moving up and left, left, or up
    in the matrix). These conceptually represent the costs of opening a gap in one
    of the series, versus aligning the same element in both (middle, above).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the end of step 2, we will have traced the minimum cost path to align the
    two series, and the DTW distance will be represented by the bottommost corner
    of the matrix, (*n.m*) (right, above).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A negative aspect of this algorithm is that step 3 involves computing a value
    for every element of series *a* and *b*. For large time series or large datasets,
    this can be computationally prohibitive. While a full discussion of algorithmic
    improvements is beyond the scope of our present examples, we refer interested
    readers to FastDTW (which we will use in our example) and SparseDTW as examples
    of improvements that can be evaluated using many fewer calculations (Al-Naymat,
    Ghazi, Sanjay Chawla, and Javid Taheri. *Sparsedtw: A novel approach to speed
    up dynamic time warping*. Proceedings of the Eighth Australasian Data Mining Conference-Volume
    101\. Australian Computer Society, Inc., 2009\. Salvador, Stan, and Philip Chan.
    *Toward accurate dynamic time warping in linear time and space*. Intelligent Data
    Analysis 11.5 (2007): 561-580.).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the FastDTW algorithm to compare the stocks data and plot the results
    again using MDS. First we will compare pairwise each pair of stocks and record
    their DTW distance in a matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This function is found in the fastdtw library, which you can install using pip
    or `easy_install`.
  prefs: []
  type: TYPE_NORMAL
- en: For computational efficiency (because the distance between `i` and `j` equals
    the distance between stocks `j` and `i`), we calculate only the upper triangle
    of this matrix. We then add the transpose (for example, the lower triangle) to
    this result to get the full distance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can use MDS again to plot the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![Correlation similarity metrics and time series](img/B04881_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Compared to the distribution of coordinates along the *x* and *y* axis for Pearson
    correlation and rank correlation, the DTW distances appear to span a wider range,
    picking up more nuanced differences between the time series of stock prices.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have looked at numerical and time series data, as a last example
    let us examine calculating similarity measurements for categorical datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Similarity metrics for categorical data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Text represents one class of categorical data: for instance, we might have
    use a vector to represent the presence or absence of a given keyword for a set
    of papers submitted to an academic conference, as in our example dataset (Moran,
    Kelly H., Byron C. Wallace, and Carla E. Brodley. *Discovering Better AAAI Keywords
    via Clustering with Community-sourced Constraints*. Twenty-Eighth AAAI Conference
    on Artificial Intelligence. 2014.). If we open the data, we see that the keywords
    are represented as a string in one column, which we will need to convert into
    a binary vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![Similarity metrics for categorical data](img/B04881_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'While in [Chapter 6](ch06.html "Chapter 6. Words and Pixels – Working with
    Unstructured Data"), *Words and Pixels – Working with Unstructured Data*, we will
    examine special functions to do this conversion from text to vector, for illustrative
    purposes we will now code the solution ourselves. We need to gather all the unique
    keywords, and assign a unique index to each of them to generate a new column name
    `''keword_n`'' for each keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We then generate a new set of columns using this keyword to column name mapping,
    to set a 1 in each row where the keyword appears in that article''s keywords:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'These columns will be appended to the right of the existing columns and we
    can select out these binary indicators using the `iloc` command, as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![Similarity metrics for categorical data](img/B04881_03_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this case, a Euclidean distance between articles could be computed, but because
    each coordinate is either 0 or 1, it does not provide the continuous distribution
    of distances we would like (we will get many ties, since there are only so many
    ways to add and subtract 1 and 0). Similarly, measurements of correlation between
    these binary vectors are less than ideal because the values can only be identical
    or non-identical, again leading to many duplicate correlation values.
  prefs: []
  type: TYPE_NORMAL
- en: 'What kinds of similarity metric could we use instead? One is the Jaccard coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Similarity metrics for categorical data](img/B04881_03_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the number of intersecting items (positions where both *a* and *b*
    are set to *1* in our example) divided by the union (the total number of positions
    where either *a* or *b* are set to 1).This measure could be biased, however, if
    the articles have very different numbers of keywords, as a larger set of words
    will have a greater probability of being similar to another article. If we are
    concerned about such bias, we could use the cosine similarity, which measure the
    angle between vectors and is sensitive to the number of elements in each:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Similarity metrics for categorical data](img/B04881_03_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Similarity metrics for categorical data](img/B04881_03_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We could also use the Hamming distance (Hamming, Richard W. *Error detecting
    and error correcting codes*. Bell System technical journal 29.2 (1950): 147-160.),
    which simply sums whether the elements of two sets are identical or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Similarity metrics for categorical data](img/B04881_03_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Clearly, this measure will be best if we are primarily looking for matches
    and mismatches. It is also, like the Jaccard coefficient, sensitive to the number
    of items in each set, as simply increasing the number of elements increases the
    possible upper bound of the distance. Similar to Hamming is the Manhattan distance,
    which does not require the data to be binary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Similarity metrics for categorical data](img/B04881_03_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we use the Manhattan distance as an example, we can use MDS again to plot
    the arrangement of the documents in keyword space using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![Similarity metrics for categorical data](img/B04881_03_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We see a number of groups of papers, suggesting that a simple comparison of
    common keywords provides a way to distinguish between articles.
  prefs: []
  type: TYPE_NORMAL
- en: The diagram below provides a summary of the different distance methods we have
    discussed and the decision process for choosing one over another for a particular
    problem. While it is not exhaustive, we hope it provides a starting point for
    your own clustering applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![Similarity metrics for categorical data](img/B04881_03_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Aside: Normalizing categorical data**'
  prefs: []
  type: TYPE_NORMAL
- en: As you may have noted, we don't normalize categorical data in the same way that
    we used the `scale()` function for numerical data. The reason for this is twofold.
    First, with categorical data we are usually dealing with data in the range `[0,1]`,
    so the problem of one column of the dataset containing wildly larger values that
    overwhelm the distance metric is minimized. Secondly, the notion of the scale()
    function is that the data in the column is biased, and we are removing that bias
    by subtracting the mean. For categorical data, the 'mean' has a less clear interpretation,
    as the data can only take the value of 0 or 1, while the mean might be somewhere
    between the two (for example, 0.25). Subtracting this value doesn't make sense
    as it would make the data elements no longer binary indicators.
  prefs: []
  type: TYPE_NORMAL
- en: '**Aside: Blending distance metrics**'
  prefs: []
  type: TYPE_NORMAL
- en: In the examples considered so far in this chapter, we have dealt with data that
    may be described as either numerical, time series, or categorical. However, we
    might easily find examples where this is not true. For instance, we could have
    a dataset of stock values over time that also contains categorical information
    about which industry the stock belongs to and numerical information about the
    size and revenue of the company. In this case, it would be difficult to choose
    a single distance metric that adequately handles all of these features. Instead,
    we could calculate a different distance metric for each of the three sets of features
    (time-series, categorical, and numerical) and blend them (by taking their average,
    sum, or product, for instance). Since these distances might cover very different
    numerical scales, we might need to normalize them, for instance using the `scale()`
    function discussed above to convert each distance metric into a distribution with
    mean 0, standard deviation 1 before combining them.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have some ways to compare the similarity of items in a dataset,
    let us start implementing some clustering pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'K-means clustering is the classical divisive clustering algorithm. The idea
    is relatively simple: the **k** in the title comes from the number of clusters
    we wish to identify, which we need to decide before running the algorithm, while
    **means** denotes the fact that the algorithm attempts to assign datapoints to
    clusters where they are closest to the average value of the cluster. Thus a given
    datapoint chooses among k different means in order to be assigned to the most
    appropriate cluster. The basic steps of the simplest version of this algorithm
    are (MacKay, David. *An example inference task: clustering*. Information theory,
    inference and learning algorithms (2003): 284-292):'
  prefs: []
  type: TYPE_NORMAL
- en: Choose a desired number of groups *k*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign k cluster centers; these could simply be k random points from the dataset,
    which is known as the Forgy method ( Hamerly, Greg, and Charles Elkan. *Alternatives
    to the k-means algorithm that find better clusterings*. Proceedings of the eleventh
    international conference on Information and knowledge management. ACM, 2002.).
    Alternatively, we could assign a random cluster to each datapoint, and compute
    the average of the datapoints assigned to the same cluster as the k centers, a
    method called Random Partitioning (Hamerly, Greg, and Charles Elkan. *Alternatives
    to the k-means algorithm that find better clusterings*. Proceedings of the eleventh
    international conference on Information and knowledge management. ACM, 2002).
    More sophisticated methods are also possible, as we will see shortly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Assign any remaining datapoints to the nearest cluster, based on some similarity
    measure (such as the squared Euclidean distance).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recalculate the center of each of the *k* groups by averaging the points assigned
    to them. Note that at this point the center may no longer represent the location
    of a single datapoint, but is the weighted center of mass of all points in the
    group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat 3 and 4 until no points change cluster assignment or the maximum number
    of iterations is reached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: '**K-means ++**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the initialization of the algorithm in step 2 above, there are two potential
    problems. If we simple choose random points as cluster centers, they may not be
    optimally distributed within the data (particularly if the size of the clusters
    is unequal). The k points may not actually end up in the k clusters in the data
    (for example, multiple random points may reside within the largest cluster in
    the dataset, as in the figure below, top middle panel), which means the algorithm
    may not converge to the ''correct'' solution or may take a long time to do so.
    Similarly, the Random Partitioning method will tend to place all the centers near
    the greatest mass of datapoints (see figure below, top right panel), as any random
    set of points will be dominated by the largest cluster. To improve the initial
    choice of parameters, we can use instead the k++ initialization proposed in 2007
    (Arthur, David, and Sergei Vassilvitskii. "k-means++: The advantages of careful
    seeding." Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete
    algorithms. Society for Industrial and Applied Mathematics, 2007.). In this algorithm,
    we choose an initial datapoint at random to be the center of the first cluster.
    We then calculate the squared distance from every other datapoint to the chosen
    datapoint, and choose the next cluster center with probability proportional to
    this distance. Subsequently, we choose the remaining clusters by calculating this
    squared distance to the previously selected centers for a given datapoint. Thus,
    this initialization will choose points with higher probability that are far from
    any previously chosen point, and spread the initial centers more evenly in the
    space. This algorithm is the default used in scikit-learn.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![K-means clustering](img/B04881_03_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Kmeans++ clustering. (Top, left): Example data with three clusters of unequal
    size. (Top, middle). Random choice of cluster centers is biased towards points
    in the largest underlying cluster. (Top, right): Random partitioning results in
    center of mass for all three random clusters near the bottom of the plot. (Bottom
    panels). Kmeans++ results in sequential selection of three cluster centers that
    are evenly spaced across the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s think for a moment about why this works; even if we start with random
    group centers, once we assign points to these groups the centers are pulled towards
    the average position of observations in our dataset. The updated center is nearer
    to this average value. After many iterations, the center of each group will be
    dominated by the average position of datapoints near the randomly chosen starting
    point. If the center was poorly chosen to begin with, it will be dragged towards
    this average, while datapoints that were perhaps incorrectly assigned to this
    group will gradually be reassigned. During this process, the overall value that
    is minimized is typically the sum of squared errors (when we are using the Euclidean
    distance metric), given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![K-means clustering](img/B04881_03_44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where D is the Euclidean distance and c is the cluster center for the cluster
    to which a point is assigned. This value is also sometimes referred to as the
    inertia. If we think about this for a moment, we can see that this has the effect
    that the algorithm works best for data that is composed of circles (or spheres
    in higher dimensions); the overall SSE for a cluster is minimized when points
    are uniformly distant from it in a spherical cloud. In contrast, a non-uniform
    shape (such as an ellipse) will tend to have higher SSE values, and the algorithm
    will be optimized by splitting the data into two clusters, even if visually we
    can see that they appear to be represented well by one. This fact reinforces why
    normalization is often beneficial (as the 0 mean, 1 standard deviation normalization
    attempts to approximate the shape of a normal distribution for all dimensions,
    thus forming circles of spheres of data), and the important role of data visualization
    in addition to numerical statistics in judging the quality of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to consider the implications of this minimization criteria
    for step 3\. The SSE is equivalent to the summed squared Euclidean distance between
    cluster points and their centroid. Thus, using the squared Euclidean distance
    as the metric for comparison, we guarantee that the cluster assignment is also
    optimizing the minimization criteria. We could use other distance metrics, but
    then this will not be guaranteed. If we are using Manhattan or Hamming distance,
    we could instead make our minimization criteria the sum of distances to the cluster
    center, which we term the k-median, since the value that optimizes this statistic
    is the cluster median (Jain, Anil K., and Richard C. Dubes. Algorithms for clustering
    data. Prentice-Hall, Inc., 1988.). Alternatively, we could use an arbitrary distance
    metric with an algorithm such as k-medoids (see below).
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, this method will be sensitive to our initial choice of group centers,
    so we will usually run the algorithm many times and use the best result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example: type the following commands in the notebook to read
    in a sample dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![K-means clustering](img/B04881_03_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: By visual inspection, this dataset clearly has a number of clusters in it. Let's
    try clustering with *k=5*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![K-means clustering](img/B04881_03_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'You will notice that we use the slice operators ''`[]`'' to index a `numpy`
    array that we create from the input dataframe, and select all rows and the columns
    after the first (the first contains the label, so we don''t need it as it isn''t
    part of the data being used for clustering). We call the KMeans model using a
    pattern that will become familiar for many algorithms in scikit-learn and PySpark:
    we create model object (KMeans) with parameters (here, 5, which is the number
    of clusters), and call ''fit_predict'' to both calibrate the model parameters
    and apply the model to the input data. Here, applying the model generates cluster
    centers, while in regression or classification models that we will discuss in
    [Chapters 4](ch04.html "Chapter 4. Connecting the Dots with Models – Regression
    Methods"), *Connecting the Dots with Models – Regression Methods* and [Chapter
    5](ch05.html "Chapter 5. Putting Data in its Place – Classification Methods and
    Analysis"), *Putting Data in its Place – Classification Methods and Analysis*,
    ''predict'' will yield an estimated continuous response or class label, respectively,
    for each data point. We could also simply call the `fit` method for KMeans, which
    would simply return an object describing the cluster centers and the statistics
    resulting from fitting the model, such as the inertia measure we describe above.'
  prefs: []
  type: TYPE_NORMAL
- en: Is this a good number of clusters to fit to the data or not? We can explore
    this question if we cluster using several values of `k` and plot the inertia at
    each. In Python we can use the following commands.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall that the inertia is defined as the sum of squared distance points in
    a cluster to the center of the cluster to which they are assigned, which is the
    objective we are trying to optimize in k-means. By visualizing this inertia value
    at each cluster number *k*, we can get a feeling for the number of clusters that
    best fits the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![K-means clustering](img/B04881_03_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We notice there is an *elbow* around the five-cluster mark, which fortunately
    was the value we initially selected. This elbow indicates that after five clusters
    we do not significantly decrease the inertia as we add more clusters, suggesting
    that at k=5 we have captured the important group structure in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This exercise also illustrates some problems: as you can see from the plot,
    some of our clusters might be formed from what appear to be overlapping segments,
    forming a cross shape. Is this a single cluster or two mixed clusters? Unfortunately,
    without a specification in our cluster model of what shape the clusters should
    conform to, the results are driven entirely by distance metrics, not pattern which
    you might notice yourself visually. This underscores the importance of visualizing
    the results and examining them with domain experts to judge whether the obtained
    clusters makes sense. In the absence of a domain expert, we could also see whether
    the obtained clusters contain all points labeled with a known assignment—if a
    high percentage of the clusters are enriched for a single label, this indicates
    the clusters are of good conceptual quality as well as minimizing our distance
    metric.'
  prefs: []
  type: TYPE_NORMAL
- en: We could also try using a method that will automatically calculate the best
    number of clusters for a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Affinity propagation – automatically choosing cluster numbers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the weaknesses of the k-means algorithm is that we need to define upfront
    the number of clusters we expect to find in the data. When we are not sure what
    an appropriate choice is, we may need to run many iterations to find a reasonable
    value. In contrast, the Affinity Propagation algorithm (Frey, Brendan J., and
    Delbert Dueck. *Clustering by passing messages between data points*. science 315.5814
    (2007): 972-976.) finds the number of clusters automatically from a dataset. The
    algorithm takes a similarity matrix as input (S) (which might be the inverse Euclidean
    distance, for example – thus, closer points have larger values in S), and performs
    the following steps after initializing a matrix of *responsibilit*y and *availability*
    values with all zeroes. It calculates the *responsibility* for one datapoint *k*
    to be the cluster center for another datapoint *i*. This is represented numerically
    by the similarity between the two datapoints. Since all availabilities begin at
    zero, in the first round we simply subtract the highest similarity to any other
    point (k'') for *i*. Thus, a high responsibility score occurs when point k is
    much more similar to *i* than any other point.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Affinity propagation – automatically choosing cluster numbers](img/B04881_03_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *i* is the point for which we are trying to find the cluster center,
    *k* is a potential cluster center to which point *i* might be assigned, s is their
    similarity, and a is the ''availability'' described below. In the next step, the
    algorithm calculates the availability of the datapoint *k* as a cluster center
    for point *i*, which represents how appropriate k is as a cluster center for *i*
    by judging if it is the center for other points as well. Points which are chosen
    with high responsibility by many other points have high availability, as per the
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Affinity propagation – automatically choosing cluster numbers](img/B04881_03_53.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *r* is the responsibility given above. If *i=k*, then this formula is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Affinity propagation – automatically choosing cluster numbers](img/B04881_03_54.jpg)'
  prefs: []
  type: TYPE_IMG
- en: These steps are sometimes described as **message passing**, as they represent
    information being *exchanged* by the two datapoints about the relative probability
    of one being a cluster center for another. Looking at steps 1 and 2, you can see
    that as the algorithm proceeds the responsibility will drop for many of the datapoints
    to a negative number (as we subtract not only the highest similarity of other
    datapoints, but also the availability score of these points, leaving only a few
    positives that will determine the cluster centers. At the end of the algorithm
    (once the responsibilities and availabilities are no longer changing by an appreciable
    numerical amount), each datapoint points at another as a cluster center, meaning
    the number of clusters is automatically determined from the data. This method
    has the advantage that we don't need to know the number of clusters in advance,
    but does not scale as well as other methods, since in the simplest implantation
    we need an n-by-n similarity matrix as the input. If we fit this algorithm on
    our dataset from before, we see that it detects far more clusters than our elbow
    plot suggests, since running the following commands gives a cluster number of
    309.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if we look at a histogram of the number of datapoints in each cluster,
    using the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that only a few clusters are large, while many points are identified
    as belonging to their own group:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Affinity propagation – automatically choosing cluster numbers](img/B04881_03_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where K-Means Fails: Clustering Concentric Circles'
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, our data has been well clustered using k-means or variants such as
    affinity propagation. What examples might this algorithm perform poorly on? Let''s
    take one example by loading our second example dataset and plotting it using the
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![Affinity propagation – automatically choosing cluster numbers](img/B04881_03_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'By eye alone, you can clearly see that there are two groups: two circles nested
    one within the other. However, if we try to run k-means clustering on this data,
    we get an unsatisfactory result, as you can see from running the following commands
    and plotting the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![Affinity propagation – automatically choosing cluster numbers](img/B04881_03_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this case, the algorithm was unable to identify the two natural clusters
    in the data—because the center ring of data is the same distance to the outer
    ring at many points, the randomly assigned cluster center (which is more likely
    to land somewhere in the outer ring) is a mathematically sound choice for the
    nearest cluster. This example suggests that, in some circumstances, we may need
    to change our strategy and use a conceptually different algorithm. Maybe our objective
    of squared error (inertia) is incorrect, for example. In this case, we might try
    k-medoids.
  prefs: []
  type: TYPE_NORMAL
- en: k-medoids
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have described earlier, the k-means (medians) algorithm is best suited
    to particular distance metrics, the squared Euclidean and Manhattan distance (respectively),
    since these distance metrics are equivalent to the optimal value for the statistic
    (such as total squared distance or total distance) that these algorithms are attempting
    to minimize. In cases where we might have other distance metrics (such as correlations),
    we might also use the k-medoid method (Theodoridis, Sergios, and Konstantinos
    Koutroumbas. *Pattern recognition.* (2003).), which consists of the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Select *k* initial points as the initial cluster centers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the nearest cluster center for each datapoint by any distance metric
    and assign it to that cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each point and each cluster center, swap the cluster center with the point
    and calculate the reduction in overall distances to the cluster center across
    all cluster members using this swap. If it doesn't improve, undo it. Iterate step
    3 for all points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is obviously not an exhaustive search (since we don''t repeat step 1),
    but has the advantage that the optimality criterion is not a specific optimization
    function but rather improving the compactness of the clusters by a flexible distance
    metric. Can k-medoids improve our clustering of concentric circles? Let''s try
    running using the following commands and plotting the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that k-medoids is not included in sci-kit learn, so you will need to install
    the pyclust library using `easy_install` or `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: '![k-medoids](img/B04881_03_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There isn't much improvement over k-means, so perhaps we need to change our
    clustering algorithm entirely. Perhaps instead of generating a similarity between
    datapoints in a single stage, we could examine hierarchical measures of similarity
    and clustering, which is the goal of the agglomerative clustering algorithms we
    will examine next.
  prefs: []
  type: TYPE_NORMAL
- en: Agglomerative clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In contrast to algorithms, such as k-means, where the dataset is partitioned
    into individual groups, **agglomerative** or **hierarchical** clustering techniques
    start by considering each datapoint as its own cluster and merging them together
    into larger groups from the bottom up (Maimon, Oded, and Lior Rokach, eds. *Data
    mining and knowledge discovery handbook*. Vol. 2\. New York: Springer, 2005).
    The classical application of this idea is in phylogenetic trees in evolution,
    where common ancestors connect individual organisms. Indeed, these methods organize
    the data into tree diagrams, known as **dendrograms**, which visualize how the
    data is sequentially merged into larger groups.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic steps of an agglomerative algorithm are (diagrammed visually in the
    figure below):'
  prefs: []
  type: TYPE_NORMAL
- en: Start with each point in its own cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare each pair of datapoints using a distance metric. This could be any of
    the methods discussed above.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use a linkage criterion to merge datapoints (at the first stage) or clusters
    (in subsequent phases), where linkage is represented by a function such as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The maximum (also known as complete linkage) distance between two sets of points.The
    minimum (also known as single linkage) distance between two sets of points.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The average distance between two sets of points, also known as **Unweighted
    Pair Group Method with Arithmetic Mean** (**UPGMA**). The points in each group
    could also be weighted to give a weighted average, or WUPGMA.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference between centroids (centers of mass), or UPGMC.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The squared Euclidean distance between two sets of points, or Ward''s Method
    (Ward Jr, Joe H. *Hierarchical grouping to optimize an objective function*. *Journal
    of the American statistical association* 58.301 (1963): 236-244).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat steps 2-3 until there is only a single cluster containing all data points.
    Note that following the first round, the first stage of clusters become a new
    point to compare with all other clusters, and at each stage the clusters formed
    become larger as the algorithm proceeds. Along the way, we will construct a tree
    diagram as we sequentially merge clusters from the prior steps together.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Agglomerative clustering](img/B04881_03_40.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Agglomerative clustering: From top to bottom, example of tree construction
    (right) from a dataset (left) by sequentially merging closest points'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note that we could also run this process in reverse, taking an initial dataset
    and splitting it into individual points, which would also construct a tree diagram.
    In either case, we could then find clusters at several levels of resolution by
    choosing a cutoff depth of the tree and assigning points to the largest cluster
    they have been assigned to below that cutoff. This depth is often calculated using
    the linkage score given in step 3, allowing us conceptually to choose an appropriate
    distance between groups to consider a cluster (either relatively close or far
    away as we move up the tree).
  prefs: []
  type: TYPE_NORMAL
- en: Where agglomerative clustering fails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Agglomerative algorithms have many of the same ingredients as k-means; we choose
    a number of clusters (which will determine where we cut the tree generated by
    clustering—in the most extreme case, all points become members of a single cluster)
    and a similarity metric. We also need to choose a **linkage metric** for step
    3, which determines the rules for merging individual branches of our tree. Can
    agglomerative clustering succeed where k-means failed? Trying this approach on
    the circular data suggests otherwise, as shown by plotting results of the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![Where agglomerative clustering fails](img/B04881_03_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In order to correctly group the inner and outer circle, we can attempt to modify
    our notion of similarity using connectivity, a concept taken from graph analysis
    in which a set of nodes are connected by edges, and connectivity refers to whether
    two nodes share an edge between them. Here, we essentially construct a graph between
    pairs of points by thresholding the number of points that can be considered similar
    to one another, rather than measuring a continuous distance metric between each
    pair of points. This potentially reduces our difficulties with the concentric
    circles data, since if we set a very small value (say 10 nearby points), the uniform
    distance from the middle to the outer ring is no longer problematic because the
    central points will always be closer to each other than to the periphery. To construct
    this connectivity-based similarity, we could take a distance matrix such as those
    we've already calculated, and threshold it for some value of similarity by which
    we think points are connected, giving us a binary matrix of 0 and 1\. This kind
    of matrix, representing the presence or absence of an edge between nodes in a
    graph, is also known as an **adjacency matrix**. We could choose this value through
    inspecting the distribution of pairwise similarity scores, or based on prior knowledge.
    We can then supply this matrix as an argument to our agglomerative clustering
    routine, providing a neighborhood of points to be considered when comparing datapoints,
    which gives an initial structure to the clusters. We can see that this makes a
    huge difference to the algorithms results after running the following commands.
    Note that when we generate the adjacency matrix L, we may end up with an asymmetric
    matrix since we threshold the ten most similar points for each member of the data.
    This could lead to situations where two points are not mutually closest to each
    other, leading to an edge represented in only the top or bottom triangle of the
    adjacency matrix. To generate a symmetric input for our clustering algorithm,
    we take the average of the matrix L added to its transpose, which effectively
    adds edges in both directions between two points.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, as you can see, this algorithm can correctly identify and separate two
    clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, constructing this neighborhood graph and partitioning it into
    sub graphs (splitting the whole graph into a set of nodes and edges that are primarily
    connected to each other, rather than to other elements of the network) is equivalent
    to performing k-means clustering on a transformed distance matrix, an approach
    known as Spectral Clustering (Von Luxburg, Ulrike. *A tutorial on spectral clustering.*
    Statistics and computing 17.4 (2007): 395-416). The transformation here is from
    taking the Euclidean distance D we calculated earlier and calculating the kernel
    score—the Gaussian kernel given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Where agglomerative clustering fails](img/B04881_03_70.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'between each pair of points i and j, with a bandwidth γ instead of making a
    hard threshold as when we constructed the neighborhood before. Using the pairwise
    kernel matrix K calculated from all points i and j, we can then construct the
    Laplacian matrix of a graph, which is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Where agglomerative clustering fails](img/B04881_03_71.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *I* is the identity matrix (with a one along the diagonal and zero elsewhere),
    and *D* is the diagonal matrix whose elements are :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Where agglomerative clustering fails](img/B04881_03_72.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Giving the number of neighbors for each point *i*. In essence by calculating
    L we now represent the dataset as a series of nodes (points) connected by edges
    (the elements of this matrix), which have been normalized so that the total value
    of all edges for each node sums to 1\. Since the Gaussian kernel score is continuous,
    in this normalization divides pairwise distances between a given point and all
    others into a probability distribution where the distances (edges) sum to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may recall from linear algebra that Eigenvectors of a matrix A are vectors
    v for which, if we multiply the matrix by the eigenvector v, we get the same result
    as if we had multiplied the vector by a constant amount λ: ![Where agglomerative
    clustering fails](img/B04881_03_73.jpg). Thus, the matrix here represents a kind
    of operation on the vector. For example, the identity matrix gives an eigenvalue
    of 1, since multiplying v by the identity gives v itself. We could also have a
    matrix such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Where agglomerative clustering fails](img/B04881_03_74.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'which doubles the value of the vector with which it is multiplied, suggesting
    the matrix acts as a ''stretching'' operation on the vector. From this perspective,
    larger eigenvalues correspond to greater stretching of the vector, while the eigenvector
    gives the direction in which the stretching occurs. This is useful because it
    gives us the primary axes along which the matrix operation acts. In our example,
    if we take the two eigenvectors with the largest eigenvalues (in essence, the
    directions in which the matrix represents the greatest transformation of a vector),
    we are extracting the two greatest axes of variation in the matrix. We will return
    to this concept in more detail when we discuss *Principal components* in [Chapter
    6](ch06.html "Chapter 6. Words and Pixels – Working with Unstructured Data"),
    *Words and Pixels – Working with Unstructured Data*, but suffice to say that if
    we run `run-kmeans` on these eigenvectors (an approach known as **spectral clustering**,
    since the eigenvalues of a matrix that we cluster are known as the spectrum of
    a matrix), we get a result very similar to the previous agglomerative clustering
    approach using neighborhoods, as we can see from executing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![Where agglomerative clustering fails](img/B04881_03_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can successfully capture this nonlinear separation boundary because we''ve
    represented the points in the space of the greatest variation in pairwise distance,
    which is the difference between the inner and outermost circle in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding examples should have given you a number of approaches you can
    use to tackle clustering problems, and as a rule of thumb guide, the following
    diagram illustrates the decision process for choosing between them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Where agglomerative clustering fails](img/B04881_03_46.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For the last part of our exploration of clustering, let's look at an example
    application utilizing Spark Streaming and k-means, which will allow us to incrementally
    update our clusters as they are received.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming clustering in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up to this point, we have mainly demonstrated examples for ad hoc exploratory
    analysis. In building up analytical applications, we need to begin putting these
    into a more robust framework. As an example, we will demonstrate the use of a
    streaming clustering pipeline using PySpark. This application will potentially
    scale to very large datasets, and we will compose the pieces of the analysis in
    such a way that it is robust to failure in the case of malformed data.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we will be using similar examples with PySpark in the following chapters,
    let''s review the key ingredients we need in such application, some of which we
    already saw in [Chapter 2](ch02.html "Chapter 2. Exploratory Data Analysis and
    Visualization in Python"), *Exploratory Data Analysis and Visualization in Python*.
    Most PySpark jobs we will create in this book consist of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Construct a Spark context. The context contains information about the name of
    the application, and parameters such as memory and number of tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Spark context may be used to construct secondary context objects, such as
    the streaming context we will use in this example. This context object contains
    parameters specifically about a particular kind of task, such as a streaming dataset,
    and inherits all the information we have previously initialized in the base Spark
    context.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a dataset, which is represented in Spark as a **Resilient Distributed
    Dataset** (**RDD**). While from a programmatic standpoint we can operate on this
    RDD just as we do with, for example, a pandas dataframe, under the hood it is
    potentially parallelized across many machines during analysis. We may parallelize
    the data after reading it from a source file, or reading from parallel file systems
    such as Hadoop. Ideally we don't want to fail our whole job if one line of data
    is erroneous, so we would like to place an error handling mechanism here that
    will alert us to a failure to parse a row without blocking the whole job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We frequently need to transform our input dataset into a subclass of RDD known
    as a **Labeled RDD**. Labeled RDDs contain a label (such as the cluster label
    for the clustering algorithms we have been studying in this chapter) and a set
    of features. For our clustering problems, we will only perform this transformation
    when we predict (as we usually don't know the cluster ahead of time), but for
    the regression and classification models we will look at in [Chapter 4](ch04.html
    "Chapter 4. Connecting the Dots with Models – Regression Methods"), *Connecting
    the Dots with Models – Regression Methods*, and [Chapter 5](ch05.html "Chapter 5. Putting
    Data in its Place – Classification Methods and Analysis"), *Putting Data in its
    Place – Classification Methods and Analysis*, the label is used as a part of fitting
    the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll frequently want a way to save the output of our modeling to be used by
    downstream applications, either on disk or in a database, where we can later query
    models indexed by history.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s look at some of these components using the Python notebook. Assuming
    we have Spark installed on our system, we''ll start by importing the required
    dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then test starting the `SparkContext`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall that the first argument gives the URL for our Spark master, the machine
    that coordinates execution of Spark jobs and distributes tasks to the worker machines
    in a cluster. In this case, we will run it locally, so give this argument as `localhost`,
    but otherwise this could be the URL of a remote machine in our cluster. The second
    argument is just the name we give to our application. With a context running,
    we can also generate the streaming context, which contains information about our
    streaming application, using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The first argument is simply the `SparkContext` used as a parent of the `StreamingContext`:
    the second is the frequency in seconds at which we will check our streaming data
    source for new data. If we expect regularly arriving data we could make this lower,
    or make it higher if we expect new data to be available less frequently.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a `StreamingContext`, we can add data sources. Let''s assume
    for now we''ll have two sources for training data (which could be historical).
    We want the job not to die if we give one line of bad data, and so we use a `Parser`
    class that gives this flexibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We log error lines to a file with the name of our job ID, which will allow
    us to locate them later if we need to. We can then use this parser to train and
    evaluate the model. To train the model, we move files with three columns (a label
    and the data to be clustered) into the training directory. We can also add to
    the test data directory files with two columns only the coordinate features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The decay factor in the parameters gives the recipe for combining current cluster
    centers and old ones. For parameter 1.0, we use an equal weight between old and
    new, while for the other extreme, at 0, we only use the new data. If we stop the
    model at any point we, can inspect it using the `lastestModel()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We could also predict using the `predict()` function on an appropriately sized
    vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we learned how to identify groups of similar items in a dataset,
    an exploratory analysis that we might frequently use as a first step in deciphering
    new datasets. We explored different ways of calculating the similarity between
    datapoints and described what kinds of data these metrics might best apply to.
    We examined both divisive clustering algorithms, which split the data into smaller
    components starting from a single group, and agglomerative methods, where every
    datapoint starts as its own cluster. Using a number of datasets, we showed examples
    where these algorithms will perform better or worse, and some ways to optimize
    them. We also saw our first (small) data pipeline, a clustering application in
    PySpark using streaming data.
  prefs: []
  type: TYPE_NORMAL
