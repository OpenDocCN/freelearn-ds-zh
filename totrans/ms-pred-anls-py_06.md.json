["```py\n>>> spam = pd.read_csv('smsspamcollection/SMSSpamCollection',sep='\\t',header=None)\n>>> spam.columns = ['label','text']\n>>> spam.head()\n\n```", "```py\n>>> def clean_text(input):\n…      return \"\".join([i.lower() for i in input])\n\n```", "```py\n>>> spam.text = spam.text.map(lambda x: clean_text(x))\n\n```", "```py\n>>> import nltk\n>>> nltk.download()\n>>> from nltk.corpus import stopwords\n>>> stop_words = stopwords.words('english')\n\n```", "```py\n>>> def stem_text(input):\n…    return \" \".join([nltk.stem.porter.PorterStemmer().stem(t) if t not in \\\n…       stop_words else for t in nltk.word_tokenize(input)])\n\n```", "```py\n>>> spam.text = spam.text.map(lambda x: stem_text(x))\n\n```", "```py\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> count_vect_sparse = CountVectorizer().fit_transform(spam.text)\n\n```", "```py\n>>> count_vect_sparse[0].todense().size\n\n```", "```py\n>>> len(CountVectorizer().fit(spam.text).vocabulary_)Recall from the earlier that individual words might not informative features if their meaning is dependent upon the context given by other words in a sentence. Thus, if we want to expand our feature set to potentially more powerful features, we could also consider n-grams, sets of n co-occurring words (for example, the phrase \\the red house contains the n-grams the red, and red house (2-grams), and the red house (3-gram)). These features are calculated similarly as above, by supplying the argument ngram_range to the CountVectorizer constructor:\n>>> count_vect_sparse = CountVectorizer(ngram_range=(1, 3)).fit_transform(spam.text)\n\n```", "```py\n>>> count_vect_sparse[0].todense().sizeInsert\n\n```", "```py\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> tf_idf = TfidfVectorizer().fit_transform(spam.text)\n\n```", "```py\n>>> tf_idf.todense().max(1)\n\n```", "```py\n>>> from sklearn.feature_extraction.text import HashingVectorizer\n>>> h = HashingVectorizer(n_features=1024).fit_transform(spam.text)\n\n```", "```py\n>>> syn_1 = np.random.normal(0,1,100)\n>>> syn_2 = -1*syn_1\n>>> syn_data = [ syn_1, syn_1, syn_1, syn_2, syn_2, syn_2]\n\n```", "```py\n>>> syn_cov = np.cov(syn_data)\n\n```", "```py\n>>> [eigenvalues, eigenvectors] = np.linalg.eig(syn_cov)\n\n```", "```py\narray([  0.00000000e+00,   4.93682786e+00,   1.23259516e-32,          1.50189461e-16,   0.00000000e+00,  -9.57474477e-34])\n\n```", "```py\n>>> plt.hist(np.dot(np.array(syn_data).transpose(),np.array(eigenvectors[:,1])))\n\n```", "```py\n>>> syn_pca = PCA().fit(np.array(syn_data))\n\n```", "```py\narray([  1.00000000e+000,   6.38413622e-032,   2.02691244e-063,          2.10702767e-094,   3.98369984e-126,   5.71429334e-157])\n\n```", "```py\n>>> scree, ax = plt.subplots()\n>>> plt.bar(np.arange(0,6),syn_pca.explained_variance_ratio_)\n>>> ax.set_xlabel('Component Number')\n>>> ax.set_ylabel('Variance Explained')\n>>> plt.show()\n\n```", "```py\n>>> plt.hist(syn_pca.components_[0])\n\n```", "```py\n>>>  pca_text = PCA(num_components=10).fit(np.transpose(count_vect_sparse.toarray()))\n\n```", "```py\n>>> scree, ax = plt.subplots()\n>>> plt.bar(np.arange(0,10),pca_text.explained_variance_ratio_)\n>>>ax.set_xlabel('Component Number')\n>>>ax.set_ylabel('Variance Explained')\n>>> plt.show()\n\n```", "```py\n>>> scree, ax = plt.subplots()\n>>> plt.plot(pca_text.explained_variance_ratio_.cumsum())\n>>> ax.set_xlabel('Number of Components')\n>>> ax.set_ylabel('Cumulative Variance Explained')\n>>> plt.show() \n\n```", "```py\n>>> cur = pymf.CUR(count_vect_sparse.toarray().transpose(),crank=100,rrank=100)\n>>> cur.factorize() >>> cur.factorize()\n\n```", "```py\n>>> vocab = CountVectorizer().fit(spam.text).vocabulary_\n>>> vocab_array = ['']*len(vocab.values())\n>>> for k,v in vocab.items():\n…      vocab_array[v]=k\n>>>vocab_array = np.array(vocab_array)\n\n```", "```py\n>>> for i in cur._cid:\n… print(vocab_array[i])\n\n```", "```py\n>>> from sklearn.decomposition import NMF\n>>> nmf_text = NMF(n_components=10).fit(np.transpose(count_vect_sparse.toarray())\n\n```", "```py\n>>> nmf_text_transform = nmf_text.transform(count_vect_sparse.toarray())\n\n```", "```py\n>>> plt.bar(range(10),nmf_text_transform[spam.label=='spam'].mean(0))\n\n```", "```py\n>>> plt.bar(range(10),nmf_text_transform[spam.label=='ham'].mean(0))\n\n```", "```py\n>>> lda = LatentDirichletAllocation(n_topics=10).fit(count_vect_sparse)\n\n```", "```py\n>>> for i in range(10):\n…    print(vocab_array[np.argsort(lda.components_[i])[1:10]])\n\n```", "```py\n>>> topic_dist = lda.transform(count_vect_sparse)\n\n```", "```py\n>>> plt.bar(range(10),topic_dist[spam.label=='ham'].mean(0))\n\n```", "```py\n>>> plt.bar(range(10),topic_dist[spam.label=='spam'].mean(0))\n\n```", "```py\n>>> from skimage import data, io, segmentation\n>>> image = data.coffee()\n>>> io.imshow(image)\n>>> plt.axis('off');\n\n```", "```py\n>>> grey_image = skimage.color.rgb2gray(image)\n>>> io.imshow(grey_image)\n>>> plt.axis('off');\n\n```", "```py\n>>> from skimage import exposure\n>>> image_equalized = exposure.equalize_hist(grey_image)\n>>> io.imshow(image_equalized)\n>>> plt.axis('off'); \n\n```", "```py\n>>> plt.hist(grey_image.ravel())\n\n```", "```py\n>>> plt.hist(image_equalized.ravel(),color='b')\n\n```", "```py\n>>> coins_equalized = exposure.equalize_hist(skimage.color.rgb2gray(data.coins()))\n>>> io.imshow(coins_equalized)\n\n```", "```py\n>>> from skimage.morphology import opening, disk\n>>> d=disk(50)\n>>> background = opening(coins_equalized,d)\n>>> io.imshow(coins_equalized-background) \n\n```", "```py\n>>> from skimage import filter\n>>> threshold_global_otsu = filter.threshold_otsu(coins_equalized-background)\n>>> global_otsu = (coins_equalized-background) >= threshold_global_otsu\n>>> io.imshow(global_otsu)\n\n```", "```py\n>>> faces = skimage.io.imread_collection('senthil_database_version1/S1/*.tif')\n>>> io.imshow(faces[1])\n\n```", "```py\n>>> faces_flatten = [f.ravel() for f in faces]\n>>> import pylab\n>>> faces_flatten_demean = pylab.demean(faces_flatten,axis=1)\n\n```", "```py\n>>> from sklearn.decomposition import PCA\n>>> faces_components = PCA(n_components=3).fit(faces_flatten_demean)\n>>> io.imshow(np.reshape(faces_components.components_[1],(188,140)))\n\n```", "```py\n>>> plt.plot(faces_components.explained_variance_ratio_.cumsum())\n\n```", "```py\n>>> from sklearn.decomposition import NMF\n>>> faces_nmf = NMF(n_components=3).fit(np.transpose(faces_flatten)) \n>>> io.imshow(np.reshape(faces_nmf.components_[0],(188,140))) \n\n```", "```py\n>>> def parse_data(line):\n…     try: \n…         line_array = line.split(',')\n…      return (line_array[6],line_array[1]) # user-term pairs\n…      except:\n…         return None\n\n```", "```py\n>>> f = open('Online Retail.csv',encoding=\"Windows-1252\")\n>>> purchases = []\n>>> users = {}\n>>> items = {}\n>>>user_index = 0\n>>>item_index = 0\n>>>for index, line in enumerate(f):\n…    if index > 0: # skip header\n…         purchase = parse_data(line)\n…         if purchase is not None:\n …            if users.get(purchase[0],None) is not None:\n …                purchase_user = users.get(purchase[0])\n …            else:\n …                users[purchase[0]] = user_index\n …                user_index += 1\n …                purchase_user = users.get(purchase[0])\n …            if items.get(purchase[1],None) is not None:\n …               purchase_item = items.get(purchase[1])\n…             else:\n …                items[purchase[1]] = item_index\n …                item_index += 1\n …                purchase_item = items.get(purchase[1])\n …           purchases.append((purchase_user,purchase_item))>>>f.close()\n\n```", "```py\n>>> purchasesRdd = sc.parallelize(purchases,5).map(lambda x: Rating(x[0],x[1],1.0))\n\n```", "```py\n>>> from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n\n>>> k = 10\n>>> iterations = 10\n>>> mfModel = ALS.train(purchasesRdd, k, iterations)\n\n```"]