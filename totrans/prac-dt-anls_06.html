<html><head></head><body>
        

                            
                    Gathering and Loading Data in Python
                
            
            
                
<p>This chapter will explain what SQL is and why it is important for data analysis by teaching you how to use and access databases using SQLite for our examples. An overview of relational database technology will be provided along with insightful information on database systems to help to improve your data literacy when communicating with experts. You will also learn how to run SQL <kbd>SELECT</kbd> queries from the Jupyter Notebook and how to load them into DataFrames. Basic statistics, data lineage, and metadata (data about data) will be explained using the <kbd>pandas</kbd> library.</p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>Introduction to SQL and relational databases</li>
<li>From SQL to pandas DataFrames</li>
<li>Data about your data explained</li>
<li>The importance of data lineage</li>
</ul>
<h1 id="uuid-2834a00d-1fb9-4497-971c-acc99aadb120">Technical requirements</h1>
<p>Here's the GitHub repository of this book: <a href="https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter05">https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter05</a>.</p>
<p>You can download and install the required software from the following link: <a href="https://www.anaconda.com/products/individual" target="_blank">https://www.anaconda.com/products/individual</a>.</p>
<p class="mce-root"/>
<h1 id="uuid-622950cd-7ff2-46c1-a043-f892748ceb4c">Introduction to SQL and relational databases</h1>
<p>We are now at a point in this book where my professional career started, working with databases and SQL. <strong>Structured Query Language</strong> (<strong>SQL</strong>) was created decades ago as a means to communicate with structured data stored in tables. Over the years, SQL has evolved from multiple variations that were specific to the underlining database technology. For example, IBM, Oracle, and Sybase all had variations in their SQL commands, which built loyalty in their customers but required changes when switching vendors. The adoption of the <strong>International Organization for Standardization </strong>(<strong>ISO</strong>) and <strong>American National Standards Institute</strong> (<strong>ANSI</strong>) standards helped to define what is commonly used today.</p>
<p>So far in this book, all of the examples of structured data focused on one table or file. Relational databases solve the problem of storing data together in multiple tables while keeping consistency across them using the concept of a primary and foreign key:</p>
<ul>
<li>A primary key is the unique value (typically an integer) used to represent a single distinct record or tuple in each table.</li>
<li>A foreign key would be a field in one table that references the primary key from another.</li>
</ul>
<p>This relationship defines integrity between one or more tables for consistency for all of the data. Since the concept of storing and joining the data is abstract, this allows it to be applied to many different data subjects. For example, you can create a database to store sales from a manufacturing company, user hits from a website, or stock purchases in a financial services company. Because of this versatility, SQL remains a top programming language and a must-have skill for data analysis.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>SQL was created to communicate with data stored in database tables that have a defined schema. A database schema is like a blueprint that defines a structure for storing data before the data is loaded. This definition includes rules, conditions, and specific data types for each field in a table. The foundation for the database technology was created by Dr. EF Codd back in 1970 and was a milestone of the <em>Evolution of Data Analysis</em>, which I defined in <a href="0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml">Chapter 1</a>, <em>Fundamentals of Data Analysis</em>. The concept of persisting data in defined columns and rows as tables in a structured relationship showcases the legacy of Dr. Codd's contribution to this technology and data. His contributions to the technology along with others such as Ralph Kimball and Bill Inmon has created new industries and careers over the years. If you come across an <strong>Enterprise Data Warehouse</strong> (<strong>EDW</strong>), you can bet money it uses the Kimball or Inmon methods as a standard. Their influence, which defined new skills to work with data, cannot be understated. I have immense gratitude for the people who have evolved technologies and concepts supporting all things data.</p>
<p>What is defined as a relational database is a vast subject so I'm going to focus on what is relevant for building your data literacy and the analysis of data from consuming data using SQL. The key concepts to focus on behind working with any <strong>Relational Database Management System</strong> (<strong>RDBMS</strong>) begin with how to communicate with the system or servers that host the database. Most of them support using an ODBC driver, which handles authentication and communication over a network. <strong>Open Database Connectivity</strong> (<strong>ODBC</strong>) is a common standard used to send and receive data between your analysis tool, for example, the Jupyter Notebook, and where the data is stored. Most large-scale, enterprise relational database systems support ODBC connections to communicate with the database.</p>
<p>Traditionally, this would be known as a client-server architecture, where your local computer is known as the client and the location of the database would be managed by one or more servers. When I was a consultant, the most common enterprise RDBMSes I worked with were Microsoft SQL Server, Oracle, IBM DB2, MySQL, and PostgreSQL.</p>
<p>An ODBC driver may need to be installed and configured on your workstation to communicate with a client-server architecture.</p>
<p>Today, other flavors of both open source and vendor database products exist but many do and should support SQL or a variation of it. For example, Apache's HiveQL is very similar to ASCI SQL but runs on top of the <strong>Hadoop Distributed File System</strong> (<strong>HDFS</strong>) instead of a database. For our examples, we will be using SQLite, which is a file-based database you can install locally or connect with via ODBC. SQLite is open source and cross-platform, which means we can install it on any operating system, and it is touted as the <em>most widely deployed and used database engine in the world</em><strong> </strong>according to their download page, which you can find in the <em>Further reading</em> section.</p>
<p class="mce-root"/>
<p>Once a connection has been established, a user ID and password are commonly required, which control what actions you can perform and which tables you can access. If you installed the database yourself, you are the owner of the database and probably have system administrator rights, which gives you full access to create, delete, and read any table. If you are a client, the <strong>Database Administrator</strong> (<strong>DBA</strong>) would be responsible for setting up access and permission for your user ID.</p>
<p>I find what makes the SQL a popular language even today is the learning curve required to use it. In my experience, many business users and data analysts find the syntax intuitively obvious even without a background in computer science. SQL code is easy to read and it's quickly understood what the expected results are. It also supports instant gratification where a few commands can produce results in less than one second even with large volumes of data once it's been optimized for performance.</p>
<p>For example, let's say I want to know the highest closing stock price of Apple stock in all of 2018. Even without really understanding all of the details behind how or where that data is stored, the syntax for this one line of code is easy to interpret:</p>
<pre>SELECT max(closing_price) FROM tbl_stock_price WHERE year = 2018</pre>
<p>Let's walk through this code and break out the key components:</p>
<ul>
<li>First, I capitalized the reserved words, which are universal across any RDBMS that supports ISO standard/ASCI SQL.</li>
<li>The <kbd>SELECT</kbd> command instructs the code to retrieve data in the form of rows and columns from a table defined after the <kbd>FROM</kbd> statement.</li>
<li>Between the <kbd>SELECT</kbd> and the <kbd>FROM</kbd> reserved words is the <kbd>max(closing_price)</kbd> command. This is using the <kbd>max()</kbd> function that is available in SQL to retrieve the maximum or largest value from the <kbd>closing_price</kbd> field. The max function will only return one row and one value regardless of whether duplicate values exist in the data.</li>
<li>The <kbd>FROM</kbd> section of the code lets the SQL interpreter know a table or object is being referenced immediately afterward. For this example, we are looking for records from the <kbd>tbl_stock_price</kbd> table.</li>
<li class="mce-root">The <kbd>WHERE</kbd> clause from the <kbd>SELECT</kbd> SQL statement restricts the data by reducing the number of rows to a specific condition, which is defined by a specific field of <kbd>year</kbd> and <kbd>value</kbd> to the right of the equals sign of <kbd>2018</kbd>. </li>
</ul>
<div><kbd>SELECT</kbd> is the most common SQL command and has many different use cases and levels of complexity. We are just scratching the surface but you can find more resources in the <em>Further reading</em> section.</div>
<p class="mce-root">SQL is not case sensitive but the tables and fields referenced might be, depending on which RDBMS is being used. Spaces are important between reserve words but you typically won't find spaces in the table or field names. Rather, underscores or dashes are common.</p>
<h1 id="uuid-fdcb1877-4f12-434b-95cc-d4d88d47e3e8">From SQL to pandas DataFrames</h1>
<p class="mce-root">Now that we have some background on SQL and relational databases, let's download a local copy of an SQLite database file, set up a connection, and load some data into a <kbd>pandas</kbd> DataFrame. For this example, I have provided the database file named <kbd>customer_sales.db</kbd> so be sure to download it from the GitHub repository beforehand. </p>
<p class="mce-root">To give you some context about this database file and support the <strong>Know Your Data</strong> (<strong>KYD</strong>) concept that we learned in <a href="0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml">Chapter 1</a>, <em>Fundamentals</em> <em>of Data Analysis</em>, we have three tables named <kbd>tbl_customers</kbd>, <kbd>tbl_products</kbd>, and <kbd>tbl_sales</kbd>. This would be a simple example of any company that has customers who purchase products that generate sales over any period of time. A visual representation of how the data is stored and joined together, which is commonly known as an <strong>ERD</strong> (short for <strong>Entity Relationship Diagram</strong>), is shown in the following diagram:<em> </em></p>
<p class="CDPAlignCenter CDPAlign"><img src="img/349bdfe8-02e1-43dd-9926-6fc1bbe4084b.png"/></p>
<p class="mce-root"/>
<p class="mce-root">In the preceding diagram, we have a visual of three tables with the column name defined on the left side of each box and the data type of each column immediately to the right. The primary key for each table is identified with a suffix in the name of <kbd>_ID</kbd>, along with bolding the text in the first row of each table. The primary key commonly has a data type of integer, which is also the case here.</p>
<p class="mce-root">The <kbd>tbl_sales</kbd> table includes two of those fields, <kbd>Customer_ID</kbd> and <kbd>Product_ID</kbd>, which means they are classified as foreign keys. The lines between the tables reinforce the relationship between them, which also indicates how to join them together. The small lines that look like <em>crow's feet</em> tell the consumer these tables are defined with a one-to-many relationship. In this example, <kbd>tbl_sales</kbd> will have many customers and many products but a record in <kbd>tbl_customers</kbd> will only have one value assigned per <kbd>Customer_ID</kbd> and <kbd>tbl_products</kbd> will only have one value assigned per <kbd>Product_ID</kbd>.</p>
<p class="mce-root">Now that we have more information about the data, let's launch a new Jupyter notebook and name it <kbd>retrieve_sql_and_create_dataframe</kbd>. To create a connection and use SQLite, we have to import a new library using code:</p>
<ol>
<li>To load an SQLite database connection, you just need to add the following command in your Jupyter notebook and run the cell. Feel free to follow along by creating your own notebook (I have placed a copy in GitHub for reference):</li>
</ol>
<pre style="padding-left: 60px">In[]: import sqlite3</pre>
<p>The <kbd>sqlite3</kbd> module comes with the Anaconda distribution installed. Refer to <a href="e0fe6eb2-8f38-41f7-9dea-2b177578fd3c.xhtml">Chapter 2</a>, <em>Overview of Python and Installing Jupyter Notebook</em>, for help with setting up your environment.</p>
<ol start="2">
<li>Next, we need to assign a connection to a variable named <kbd>conn</kbd> and point to the location of the database file, which is named <kbd>customer_sales.db</kbd>. Since we already imported the <kbd>sqlite3</kbd> library in the prior <kbd>In[]</kbd> line, we can use this built-in function to communicate with the database:</li>
</ol>
<pre style="padding-left: 60px">In[]: conn = sqlite3.connect('customer_sales.db')</pre>
<p>Be sure to copy the <kbd>customer_sales.db</kbd> file to the correct Jupyter folder directory to avoid errors with the connection.</p>
<ol start="3">
<li>The next library to import should be very familiar, which allows us to use <kbd>pandas</kbd> so the code will be as follows:</li>
</ol>
<pre style="padding-left: 60px">In[]: import pandas as pd</pre>
<ol start="4">
<li>
<p>To run a SQL statement and assign the results to a DataFrame, we have to run this one line of code. The <kbd>pandas</kbd> library includes a <kbd>read_sql_query()</kbd> function to make it easier to communicate with databases using SQL. It requires a connection parameter, which we named <kbd>conn</kbd> in the previous steps. We assign the results to a new DataFrame as <kbd>df_sales</kbd> to make it easier to identify:</p>
</li>
</ol>
<pre style="padding-left: 60px">In[]: df_sales = pd.read_sql_query("SELECT * FROM tbl_sales;", conn)</pre>
<ol start="5">
<li>
<p>Now that we have the results in a DataFrame, we can use all of the available <kbd>pandas</kbd> library commands against this data without going back to the database. To view the results, we can just run the <kbd>head()</kbd> command against this DataFrame using this code:</p>
</li>
</ol>
<pre style="padding-left: 60px">In[]: df_sales.head()</pre>
<p style="padding-left: 60px" class="mce-root">The output will look like the following screenshot where the <kbd>tbl_sales</kbd> table has been loaded into a DataFrame with a labeled header row with the index column to the left starting with a value of <kbd>0</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-739 image-border" src="img/c32c6615-5457-4e20-8630-ffcfbdc6d11e.png" style="width:52.75em;height:13.42em;"/></p>
<ol start="6">
<li>
<p>To sort the values in the DataFrame, we can use the <kbd>sort_values()</kbd> function and include a parameter of the field name, which will default to ascending order. Let's begin by sorting the results by date to see when the first sale was recorded in the database by using this command:</p>
</li>
</ol>
<pre style="padding-left: 60px">In[]: df_sales.sort_values(by='Sale_Date')</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">The output would look like the following screenshot where the DataFrame output is now sorted by the <kbd>Sale_Date</kbd> field from <kbd>1/15/2015</kbd> to <kbd>6/9/2019</kbd>. Notice the difference in <kbd>Sale_ID</kbd>, which is out of sequence:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/9169e662-b2d9-4523-8f1a-c531c5104c27.png"/></p>
<ol start="7">
<li>To limit the data displayed, we can use the <kbd>DataFrame.loc</kbd> command to isolate specific rows or columns based on how it is labeled by the header row. To retrieve the first row available, we simply run this command against our DataFrame and reference the index value, which begins with <kbd>0</kbd>:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_sales.loc[0]</pre>
<p style="padding-left: 60px">The output would look like the following screenshot where a single record is displayed as a series with the rows transposed from multiple columns to multiple rows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/4ded9bc9-f8e9-4e4c-8cb4-17bd6eb75751.png" style="width:22.75em;height:10.50em;"/></p>
<p>Using this method, you must know which specific record you are looking for by index, which reflects how the data was loaded from the SQL statement. To ensure consistency between the database tables, you may want to include an <kbd>ORDER BY</kbd> command when loading the data into the DataFrame.</p>
<ol start="8">
<li>To restrict the data displayed, we can use a nested command to isolate specific rows based on a condition. A business task you could address using this data would be to <em>identify customers with high sales so we can thank them personally</em>. To do this, we can filter the sales by a specific value and display only the rows that meet or exceed that condition. For this example, we assigned <kbd>high</kbd> to an arbitrary number so any <kbd>Sales_Amount</kbd> over 100 will be displayed using this command:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_sales[(df_sales['Sales_Amount'] &gt; 100)]</pre>
<p style="padding-left: 60px">The output would look like the following screenshot where a single record is displayed based on the condition because there is only one record where <kbd>Sales_Amount</kbd> is greater than <kbd>100</kbd>, which is <kbd>Sale_ID</kbd> equal to <kbd>4</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-742 image-border" src="img/0d0d28d3-e4f3-4bb9-934d-ba765dd9a288.png" style="width:61.83em;height:7.92em;"/></p>
<ol start="9">
<li>Another example of how to restrict results would be looking for a specific value assigned to a specific field in the DataFrame. If we wanted to better understand this data, we could do so by looking at the <kbd>Sales_Quantity</kbd> field and seeing which records only had one product purchased:</li>
</ol>
<pre style="padding-left: 60px">In[]: df_sales[(df_sales['Sales_Quantity'] == 1)]</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">The output would look like the following screenshot, where multiple records are displayed based on the condition where <kbd>Sales_Quantity</kbd> is equal to <kbd>1</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-743 image-border" src="img/07b0a5ba-380a-40ac-a8a7-25996b2f9da3.png" style="width:59.42em;height:16.08em;"/></p>
<p class="mce-root">The steps define a best practice for an analysis workflow. Retrieving SQL results, storing them in one or more DataFrames, and then performing analysis in your notebook is common and encouraged. Migrating data between sources (from database to Jupyter Notebook) can take high compute resources depending on the volume of data, so be conscious of how much memory you have available and how large the databases you are working with are.</p>
<h1 id="uuid-60ed7f53-5fe1-449c-8895-e6e32612b436">Data about your data explained</h1>
<p class="mce-root">Now that we have a better understanding of how to work with SQL sourced data using Python and pandas, let's explore some fundamental statistics along with practical usage for data analysis. So far, we have focused on descriptive statistics versus predictive statistics. However, I recommend not proceeding with any data science predictive analytics without a firm understanding of descriptive analytics first.</p>
<h2 id="uuid-c3d1fb9c-b974-44b9-bd07-d3074972fa7b">Fundamental statistics</h2>
<p class="mce-root">Descriptive analytics is based on what has already happened in the past by analyzing the digital footprint of data to gain insights, analyze trends, and identify patterns. Using SQL to read data from one or more tables supports this effort, which should include basic statistics and arithmetic. Having the data structured and conformed, which includes defined data types per column, makes this type of analysis easier once you understand some key concepts and commands.There are many statistical functions available in both SQL and Python.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">I have summarized a few that are fundamental to your data analysis in this table:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>Statistical Measure</strong></p>
</td>
<td>
<p><strong>Description</strong></p>
</td>
<td>
<p><strong>Best For/Use Case</strong></p>
</td>
<td>
<p><strong>SQL Syntax</strong></p>
</td>
<td>
<p><strong>pandas Function</strong></p>
</td>
</tr>
<tr>
<td>
<p>Count</p>
</td>
<td>
<p>The number of occurrences of a value regardless of data type</p>
</td>
<td>
<p>Finding out the size of a table/number of records</p>
</td>
<td>
<p><kbd>SELECT Count(*) FROM table_name</kbd></p>
</td>
<td>
<p><kbd>df.count()</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Count Distinct</p>
</td>
<td>
<p>The number of distinct occurrences of a value regardless of data type</p>
</td>
<td>
<p>Removing duplicate values/verify distinct values used for categories of data</p>
</td>
<td>
<p><kbd>SELECT Count(distinct field_name) FROM table_name</kbd></p>
</td>
<td>
<p><kbd>df.nunique()</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Sum</p>
</td>
<td>
<p>The aggregation of values as a whole or total against numeric data types</p>
</td>
<td>
<p>Finding the total population or measuring the amount of money</p>
</td>
<td>
<p><kbd>SELECT Sum(field_name) FROM table_name</kbd></p>
</td>
<td>
<p><kbd>df.sum()</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Mean</p>
</td>
<td>
<p>The arithmetic average from a set of two or more numeric data types</p>
</td>
<td>
<p>Sum of values divided by the count of values</p>
</td>
<td>
<p><kbd>SELECT AVG(field_name) FROM table_name</kbd></p>
</td>
<td>
<p><kbd>df.mean()</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Min</p>
</td>
<td>
<p>The lowest numeric value of a value in a field</p>
</td>
<td>
<p>Finding the lowest value </p>
</td>
<td>
<p><kbd>SELECT MIN(field_name) FROM table_name</kbd></p>
</td>
<td>
<p><kbd>df.min()</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Max</p>
</td>
<td>
<p>The highest numeric value of a value in a field</p>
</td>
<td>
<p>Finding the highest value </p>
</td>
<td>
<p><kbd>SELECT MAX(field_name) FROM table_name</kbd></p>
</td>
<td>
<p><kbd>df.max()</kbd></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="mce-root">The most common statistical measure I use in SQL is <em>Count</em> where you are counting the number of records per table. Using this function helps to validate that the volume of data you are working with is in line with the source system, producers of data, and business sponsors. For example, you are told by the business sponsor that they use a database to store customers, products, and sales and they have over 30,000 customers. Let's say you run the following SQL query:</p>
<pre class="mce-root">SELECT count(*) from customers</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root">There are 90,000 results. Why is there such a dramatic difference? The first question would be: are you using the correct table? Any database is flexible so it can be organized by the DBA to manage relationships based on application and business needs, so active customers (customers who purchased a product and created sales data) could be stored in a different table, such as <kbd>active_customers</kbd>. Another question would be: is there a field used to identify whether the record is active or not? If so, that field should be included in the <kbd>WHERE</kbd> section of your <kbd>SELECT</kbd> statement, for example, <kbd>SELECT count(*) from customers where active_flag = true</kbd>.</p>
<p class="mce-root">A second advantage of using the <kbd>count()</kbd> function for analysis is to set expectations for yourself as to how much time it takes for each query to return results. If you run a <kbd>count(*)</kbd> on products, customers, and sales tables, the amount of time taken to retrieve the results will vary depending on the volume of data and how the DBA has optimized the performance. Tables have shapes, which means the number of rows and columns will vary between them. They also can grow or shrink depending on their intended use. A table such as <kbd>sales</kbd> is transactional so the number of rows will dramatically increase over time. We can classify transaction tables as deep because the number of columns is minimal, but the number of rows will grow. Tables such as <kbd>customers</kbd> and <kbd>products</kbd> are known as reference tables, which are wide in shape because they could have dozens of columns with significantly fewer rows compared to transaction tables.</p>
<p class="mce-root">Tables with high numbers of rows and columns and densely-populated distinct values take up more disk space and require more memory and CPU to process. If the <kbd>sales</kbd> table has billons of rows, counting the number of rows could take hours waiting for the response from <kbd>SELECT count(*) from sales</kbd> and would be discouraged by the administrators/IT support team. I worked with a data engineering team that was able to retrieve SQL results in less than 10 seconds against a 100 billion record table. That kind of response time requires developer expertise and administrative access to configure the table to support a super-fast response time.</p>
<p class="mce-root">Another valid point when dealing with the <kbd>count()</kbd> function is knowing the difference between frequency versus distinct values. Depending on which table you are performing a counting function against, you may be just counting the number of occurrences, or frequency of records. For the 30,000 customers example, if there is a difference in the results between <kbd>count(customer_id)</kbd> and <kbd>count(distinct customer_id)</kbd>, we know counting the records includes duplicate customers. This may not be an issue depending on the analysis you are performing. If you wanted to know how often a customer buys any product, then <kbd>counting(customer_id)</kbd> will answer that question. If you wanted to know how many customers are buying each product, using <kbd>distinct</kbd> would provide more accurate information.</p>
<p class="mce-root"/>
<p class="mce-root">The <kbd>sum()</kbd> function, which is short for summation, is another common measure used for statistical analysis in descriptive analytics. One key difference between counting versus summing would be that sum requires a number value to calculate accurate results whereas counting can be done against any data type. For example, you cannot and should not sum the <kbd>customer_name</kbd> field in the <kbd>customers</kbd> table because the data type is defined as a string. You can technically sum the <kbd>customer_id</kbd> field if it's defined as an integer, however, that would give you misleading information because that is not the intended use of the field. Like <kbd>count</kbd>, <kbd>sum</kbd> is an aggregate measure used to add together all of the values found in a specific field such as <kbd>sales_amount</kbd> or quantity from a <kbd>sales</kbd> table.</p>
<p class="mce-root">To use the <kbd>sum()</kbd> function in SQL is easy. If you want to know the sum for all time with no constraints or conditions, use the following:</p>
<pre class="mce-root"> SELECT sum(field_name) from table_name</pre>
<p class="mce-root">You can then add a condition such as only active customers by including the <kbd>WHERE</kbd> clause with the <kbd>flag</kbd> field, which has the following syntax: <kbd>SELECT sum(field_name) from table_name WHERE active_flg = TRUE</kbd>.</p>
<p>We will uncover more advanced features such as aggregation using SQL in <a href="9bdac090-8534-480e-8154-a854115c0b7a.xhtml">Chapter 8</a>, <em>Understanding Joins, Relationships, and Aggregates</em>.</p>
<p class="mce-root">The mean or average function is another common statistical function very useful for data analysis, and it's easy to write the command using SQL. average is the sum of all values divided by the count with the syntax of <kbd>SELECT avg(field_name) from table_name</kbd>.</p>
<p class="mce-root">The denominator of counting values is using the frequency/number of occurrences versus distinct values so you should understand how the table is populated before running the SQL command. For example, a sales table is transaction-based with many customers and products so the average would be different from the average against the product or customer table because each record would be distinct.</p>
<p class="mce-root">The <kbd>min</kbd> and <kbd>max</kbd> functions are also useful and easy to interpret using SQL. The built-in functions are <kbd>min()</kbd> and <kbd>max()</kbd>, which return the minimum numeric value from a population of data along with the maximum or highest value. A good business question to understand from your table would be what is the lowest and highest sales amount for 2018? The syntax in SQL would be as follows:</p>
<pre class="mce-root">SELECT min(sales_amount), max(sales_amount) from sales_table where year = 2018</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root">This information would be useful to know to understand the range of sales per customer and product across all periods of time. </p>
<p class="mce-root">An important factor to recognize when running these statistical functions against your data is to understand when values are blank or what is commonly known as null. In SQL, <kbd>NULL</kbd> represents nothing and the nonexistence of a value. In RDBMS, null values are a rule when a DBA defines the schema for each table. During that process of creating columns by defining the data type for each field, there is an option to allow null values. The reasons vary by use case whether to allow nulls during the design of a database table, but what's important to understand for analysis is whether they exist in your data and how they should be treated. </p>
<p class="mce-root">Let's start with an example from our <kbd>customers</kbd> table where one of the fields such as the second address line allows <kbd>NULL</kbd>, which is common. Why is this common? Because a second address field is optional and is not even used in many cases, but what if you are a company that needs to physically mail marketing materials or invoices to customers? If the data entry always required a value, it would unnecessarily populate a value in that second address field in the database, which is inefficient because it takes more time to enter a value for each customer and takes more storage space. In most cases, forcing a value in large-scale enterprise systems creates poor data quality, which then requires time to fix the data or creates confusion working with the data, especially working with millions of customers.</p>
<h2 id="uuid-05ece364-5da6-42a7-9d08-eccfe221de62">Metadata explained</h2>
<p class="mce-root">Metadata is commonly known as descriptive information about the data source. A key concept exposed in metadata analysis is related to understanding that nulls exist in databases. From a data analysis perspective, we need to make sure we understand how it impacts our analysis. In Python and other coding languages such as Java, you may see the word <kbd>NaN</kbd> returned. This is an acronym for <em>Not a Number</em> and helps you to understand that you may not be able to perform statistical calculations or functions against those values. In other cases such as Python, <kbd>NaN</kbd> values will have special functions to handle them, such as the following:</p>
<ul>
<li>In NumPy, use the <kbd>nansum()</kbd> function</li>
<li>Use pandas with the <kbd>isnull()</kbd> function</li>
<li>In SQL, use <kbd>is null</kbd> or <kbd>isnull</kbd> depending on the RDBMS you are working with</li>
</ul>
<p>Since you are testing for a condition to exist, you can also include the keyword of <kbd>NOT</kbd> to test for the opposite, for example, <kbd>Select * from customer_table where customer_name is NOT null</kbd>.</p>
<p class="mce-root">Understanding nulls and <kbd>NaN</kbd> boils down to KYD and metadata about the source datasets you are working with. If you don't have access to the database system to see the metadata and underlining schema, we can use pandas and DataFrames to gain some insights about SQL data. Let's walk through an example, by loading a single table from the database into a DataFrame in a notebook and run some metadata functions to gain more information.</p>
<p class="mce-root">To begin, create a new Jupyter notebook and name it <kbd>test_for_nulls_using_sql_and_pandas</kbd>:</p>
<ol>
<li>Similar to the prior example, to load an SQLite database connection, you just need to add the following command in your Jupyter notebook and run the cell:</li>
</ol>
<pre style="padding-left: 60px">In[]: import sqlite3</pre>
<ol start="2">
<li>Next, we need to assign a connection to a variable named <kbd>conn</kbd> and point to the location of the database file, which is named <kbd>customer_sales.db</kbd>. Since we already imported the <kbd>sqlite3</kbd> library in the prior <kbd>In[]</kbd> line, we can use this built-in function to communicate with the database:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">In[]: conn = sqlite3.connect('customer_sales.db')</pre>
<ol start="3">
<li>Import the <kbd>pandas</kbd> library as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">In[]: import pandas as pd</pre>
<ol start="4">
<li>
<p>Using the <kbd>read_sql_query()</kbd> function, we assign the results to a new DataFrame as <kbd>df_customers</kbd> to make it easier to identify:</p>
</li>
</ol>
<pre style="padding-left: 60px">In[]: df_customers = pd.read_sql_query("SELECT * from tbl_customers;", conn)</pre>
<ol start="5">
<li>
<p> To view the results, we can just run the <kbd>head()</kbd> command against this DataFrame using this code:</p>
</li>
</ol>
<pre style="padding-left: 60px">In[]: df_customers.head()</pre>
<p style="padding-left: 60px" class="mce-root">The output would look like the following screenshot where the <kbd>tbl_customers</kbd> table has been loaded into a DataFrame with a labeled header row with the index column to the left starting with a value of <kbd>0</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-744 image-border" src="img/d95da4e6-ed39-443c-94bc-0895a5548000.png" style="width:79.83em;height:19.50em;"/></p>
<ol start="6">
<li>We can profile the DataFrame and easily identify any <kbd>NULL</kbd> values using the following command. The <kbd>isnull()</kbd> pandas function tests for null values across the entire DataFrame:</li>
</ol>
<pre style="padding-left: 60px">In[]: pd.isnull(df_customers)</pre>
<p style="padding-left: 60px">The output would look like the following screenshot where the DataFrame will return a <kbd>True</kbd> or <kbd>False</kbd> value rather than the actual value by the cell for each row and column:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-745 image-border" src="img/b9fa1c24-fe8c-4668-aaa6-a5f90b5b4c7e.png" style="width:64.17em;height:15.92em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>With a few commands, we learned how to communicate with databases and identify some important metadata about the data stored in tables. To continue improving our data literacy, let's understand how the data was populated into the database by understanding data lineage. </p>
<h1 id="uuid-03502c19-4cea-4c81-a527-f8014b6db831" class="mce-root">The importance of data lineage</h1>
<p class="mce-root"><strong>Data lineage</strong> is the ability to trace back the source of a dataset to how it was created. It is a fun topic for me because it typically requires investigating the history of how systems generate data, identifying how it was processed, and working with the people who produce and consume the data. This process helps to improve your data literacy, which is the ability to read, write, analyze, and argue with data because you can learn how the data impacts the organization. Is the data critical to business functions such as generating sales or was it created for compliance purposes? These types of questions should be answered by learning more about the lineage of the data.</p>
<p class="mce-root">From experience, this process of tracing data lineage involves working sessions directly with the people who are responsible for the data and uncovering any documentation like an ERD demonstrated in the <em>From SQL to pandas DataFrames</em> section or help guides. In many cases, the documentation available for enterprise systems that have matured over time will not reflect the nuances that you will see when analyzing the data. For example, if a new field was created on an existing table that is populated from a web form that did not exist before, historical data will have <kbd>NULL</kbd> or <kbd>NaN</kbd> values until the point in time when the data entry started.</p>
<p class="mce-root">Data lineage can quickly become complex, which takes time to unwind and multiple resources to expose the details when not properly documented. When multiple systems are involved, working with <strong>Subject Matter Experts</strong> (<strong>SMEs</strong>) will fast track the process so you don't have to reverse engineer all of the steps in the data flow.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<h2 id="uuid-a15a9d98-742c-42fd-a49e-2bad5dad225b">Data flow</h2>
<p><strong>Data flow</strong> is a subset of data lineage that is typically part of a larger data governance strategy within large organizations so there may be existing tools or systems already in place that visually represent how the data is processed, which is commonly known as data flow. A hypothetical example of a data flow diagram would be the following diagram where we look at some of the data we have been working with in our exercises so far. In this diagram, we have a logical representation of how the <kbd>tbl_customers</kbd> table is populated from our SQLite database. I have documented the inputs and outputs as stages one to four:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-746 image-border" src="img/41e59c35-0b8b-4de8-8e0a-47bbee0751b7.png" style="width:72.08em;height:25.83em;"/></p>
<h3 id="uuid-90d8dd52-706e-4d22-bc80-1af718c8fbb1">The input stage</h3>
<p>First, we have the <strong>Input</strong> stage, which is identified as the <strong>Mobile App</strong>, <strong>Web App,</strong> and <strong>Client PC</strong> systems. These systems have created feeds out into multiple file formats. In our example, this data is batch processed, where the data files are saved and sent out for the next stage.</p>
<h3 id="uuid-8425b600-5a1d-46bc-a11b-cc05211e5a51">The data ingestion stage</h3>
<p>The <strong>Data Ingestion</strong> stage is where multiple files such as <kbd>customers.json</kbd> and <kbd>customers.xml</kbd> are processed. Because this is a logical diagram rather than a highly technical one, the details behind what technologies are used to process the data ingestion are omitted. Data ingestion is also known as <strong>ETL</strong>, which is an acronym for <strong>Extract, Transform, and Load</strong>, which is automated and maintained by data engineering teams or developers. </p>
<p class="mce-root"/>
<p>We can see an intermediary step called <kbd>tbl_stage_customers</kbd> during this ETL, which is a landing table for processing the data between the source files and the target table in the database. Also included in this stage is an <kbd>ODBC</kbd> connection where the <strong>Client PC</strong> system has direct access to insert, update, and delete records from the <kbd>tbl_customers</kbd> table. </p>
<p>During the process of learning more about the data flow, be sure to ask whether the tables are defined with logical delete versus the physical deleting of rows. In most cases, the direct removal of rows in a table is not supported, so Boolean data type columns are used to indicate whether the record is active or flagged for deletion by the system or user.</p>
<h3 id="uuid-7cf575d6-f5d9-446a-a6f6-8a7c7e47f418">The data source stage</h3>
<p class="mce-root">The third stage is named <strong>Data Source</strong>, which is defined as the <kbd>tbl_customers</kbd> table. Some questions to ask the developer or DBA are as follows:</p>
<ul>
<li class="mce-root">What is your retention policy for this data/how long is the data preserved?</li>
<li class="mce-root">What is the average daily volume of records being populated in this table?</li>
<li class="mce-root">Can they provide some metadata such as how many rows, columns, and data types for each field?</li>
<li class="mce-root">What are the dependencies/joins to this table including primary and foreign keys?</li>
<li class="mce-root">How often is this table backed up and is there system downtime we should be aware of that would impact analysis?</li>
<li class="mce-root">Does a data dictionary exist for this table/database?</li>
</ul>
<h3 id="uuid-a3654bc9-48ea-4cd4-bfa2-2cfc93ac1186">The data target stage</h3>
<p class="mce-root">The fourth stage, named <strong>Data Target</strong>, helps a data analyst to understand downstream dependencies from the source table. In this example, we have a <strong>Sales Report</strong>, the <kbd>compliance_feed.json</kbd> file, and <strong>Jupyter Notebook</strong>. Some useful information to uncover would be the frequency of how often that compliance feed is sent and who the consumers of that data are.</p>
<p class="mce-root">This may become important if the timing of your analysis is not in line with data feeds from the <strong>Data Source</strong> stage. Trust in your analysis and the ability to argue that your analysis is complete and accurate comes from understanding timing issues and your ability to reconcile and match counts between multiple data-target outputs.</p>
<p class="mce-root"/>
<h2 id="uuid-ae284658-566a-4f6e-94c9-5ab13801afc6">Business rules</h2>
<p class="mce-root">Another important point about data lineage is to uncover business rules, lookup values, or mapping reference sources. A business rule is an abstract concept that helps you to understand software code that is applied during data processing. An example would be when the user of the <strong>Mobile App</strong> clicks a <strong>Submit</strong> button, a new <kbd>customers.json</kbd> file is created. Business rules can also be more complex, such as <kbd>tbl_stage_customers</kbd> table does not populate records in the <kbd>tbl_customers</kbd> until all source files are received and a batch process runs at 12 A.M. EST daily. Business rules may be explicitly defined in the database during the creation of a database table such as the rule to define a primary key on a column, coded on a web form or mobile application. </p>
<p class="mce-root">Documenting these business rules should be included in your methodology to support your analysis. This helps you to argue insights from your data analysis by either verifying the existence of the business rule or identifying outliers that contradict assumptions made about the source data. For example, if you were told a database table was created to not allow <kbd>NULL</kbd> in specific fields but you end up finding it, you can review your findings with the DBA to uncover how this occurred. It could have easily been a business exception that was created or that the enforcement of the business rule was implemented after the table was already populated.</p>
<p class="mce-root">Understanding business rules helps to identify data gaps and verifies accuracy during analysis. If the average daily volume of records for this table drops to zero records for multiple consecutive days, there might be an issue in stage 2 during the <strong>Data Ingestion</strong> or it just might be a holiday where no customer records were received and processed. </p>
<p class="mce-root">In either case, learning how to ask these questions of the subject matter experts and verifying the data lineage will build confidence in your analysis and trust with both producers and consumers of the data.</p>
<p>Now that you understand all of the concepts, let's walk through the data lineage of the data we are working with in this chapter—<kbd>customer_sales.db</kbd>:</p>
<ol>
<li>In the <strong>Input</strong> stage for this database, three source CSV files were manually created for example purposes. Each source table has a one-for-one match with a CSV file named <kbd>tbl_customers</kbd>, <kbd>tbl_products</kbd>, and <kbd>tbl_sales</kbd>. </li>
<li>In the <strong>Data Ingestion</strong> stage, each file was imported using a few SQL commands, which created the schema for each table (the field names, defined data types, and join relationships). This process is commonly known as an ETL where the source data is ingested and persisted as tables in the database. If any changes between the source files and the target database table are required, a business rule should be documented to help to provide transparency between the producers and consumers of the data. For this example, the source and target match.</li>
<li>The <strong>Data Source</strong> stage in this example would be <kbd>customer_sales.db</kbd>. This now becomes the golden copy for data flowing out of the database for analysis and any reporting. </li>
<li>The <strong>Target</strong> stage in our example would be the Jupyter notebook and the creation of DataFrames for analysis.</li>
</ol>
<p>While this is a small example with only a few steps, the concepts apply to large-scale enterprise solutions with many more data sources and technologies used to automate the data flow. I commonly sketch out the stages for data lineage before doing any data analysis to ensure I understand the complete process. This helps to communicate with stakeholders and SMEs to ensure accuracy in the insights you gain from data sources.</p>
<h1 id="uuid-a550fd16-e459-431a-97af-59d019343f78" class="mce-root">Summary</h1>
<p class="mce-root">We have covered a few key topics in this chapter to help you to improve your data literacy by learning about working with databases and using SQL. We learned about the history of SQL and the people who created the foundation for storing structured data in databases. We walked through some examples and how to insert records from a SQL <kbd>SELECT</kbd> statement into a <kbd>pandas</kbd> DataFrame for analysis. </p>
<p class="mce-root">By using the <kbd>pandas</kbd> library, we learned about how to sort, limit, and restrict data along with fundamental statistical functions such as counting, summing, and average. We covered how to identify and work with <kbd>NaN</kbd> (that is, nulls) in datasets along with the importance of data lineage during analysis.<br/>
<br/>
In our next chapter, we will explore time series data and learn how to visualize your data using additional Python libraries to help to improve your data literacy skills.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<h1 id="uuid-6e416007-6348-4647-a18e-73cb94501c40" class="mce-root">Further reading</h1>
<p>Here are some links that you can refer to for more information on the relative topics of this chapter:</p>
<ul>
<li>Historical details about how SQL was created: <a href="http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt">http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt </a></li>
<li>Handling <kbd>NULL</kbd> values: <a href="https://codeburst.io/understanding-null-undefined-and-nan-b603cb74b44c">https://codeburst.io/understanding-null-undefined-and-nan-b603cb74b44c</a></li>
<li>Handling duplicate values with pandas: <a href="https://www.python-course.eu/dealing_with_NaN_in_python.php">https://www.python-course.eu/dealing_with_NaN_in_python.php</a></li>
<li>About SQLite databases: <a href="https://www.sqlite.org/about.html">https://www.sqlite.org/about.html</a></li>
<li>Data modeling techniques: <a href="https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/">https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/</a></li>
<li>pandas DataFrame functions: <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html</a></li>
</ul>


            

            
        
    </body></html>