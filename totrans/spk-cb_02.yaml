- en: Chapter 2. Developing Applications with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Spark shell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a Spark application in Eclipse with Maven
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing Spark applications in Eclipse with SBT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a Spark application in Intellij IDEA with Maven
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a Spark application in Intellij IDEA with SBT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To create production quality Spark jobs/application, it is useful to use various
    **integrated development environments** (**IDEs**) and build tools. This chapter
    will cover various IDEs and build tools.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Spark shell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark comes bundled with a REPL shell, which is a wrapper around the Scala shell.
    Though the Spark shell looks like a command line for simple things, in reality
    a lot of complex queries can also be executed using it. This chapter explores
    different development environments in which Spark applications can be developed.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hadoop MapReduce''s word count becomes very simple with the Spark shell. In
    this recipe, we are going to create a simple 1-line text file, upload it to the
    **Hadoop distributed file system** (**HDFS**), and use Spark to count occurrences
    of words. Let''s see how:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `words` directory by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get into the `words` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `sh.txt` text file and enter `"to be or not to be"` in it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the `words` directory as RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Count the number of lines ( result: 1):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Divide the line (or lines) into multiple words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert `word` to (word,1)—that is, output `1` as the value for each occurrence
    of `word` as a key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `reduceByKey` method to add the number of occurrences for each word
    as a key (the function works on two consecutive values at a time represented by
    `a` and `b`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Sort the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Doing all of the preceding operations in one step is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now you understand the basics, load HDFS with a large amount of text—for example,
    stories—and see the magic.
  prefs: []
  type: TYPE_NORMAL
- en: If you have the files in a compressed format, you can load them as is in HDFS.
    Both Hadoop and Spark have codecs for unzipping, which they use based on file
    extensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'When `wordsFlatMap` was converted to `wordsMap` RDD, there was an implicit
    conversion. This converts RDD into `PairRDD`. This is an implicit conversion,
    which does not require anything to be done. If you are doing it in Scala code,
    please add the following `import` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Developing Spark applications in Eclipse with Maven
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Maven as a build tool has become the de-facto standard over the years. It''s
    not surprising if we look little deeper into the promise Maven brings. Maven has
    two primary features and they are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Convention over configuration**: Build tools prior to Maven gave developers
    freedom about where to put source files, where to put test files, where to put
    compiled files, and so on. Maven takes away that freedom. With this freedom, all
    the confusion about locations also goes. In Maven, there is a specific directory
    structure for everything. The following table shows a few of the most common locations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| `/src/main/scala` | Source code in Scala |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `/src/main/java` | Source code in Java |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `/src/main/resources` | Resources to be used by source code such as configuration
    files |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `/src/test/scala` | Test code in Scala |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `/src/test/java` | Test code in Java |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `/src/test/resources` | Resources to be used by test code such as configuration
    files |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '**Declarative dependency management**: In Maven, every library is defined by
    following three coordinates:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| `groupId` | A logical way of grouping libraries similar to a package in Java/Scala,
    which has to be at least the domain name you own—for example, `org.apache.spark`
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `artifactId` | The name of the project and JAR |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `version` | Standard version numbers |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'In `pom.xml` (the configuration file that tells Maven all the information about
    a project), dependencies are declared in the form of these three coordinates.
    There is no need to search over the Internet and download, unpack, and copy libraries.
    All you need to do is to provide three coordinates of the dependency JAR you need
    and Maven will do the rest for you. The following is an example of using a JUnit
    dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This makes dependency management including transitive dependencies very easy.
    Build tools that came after Maven such as SBT and Gradle also follow these two
    rules as-is and provide enhancements in other aspects.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From this recipe onwards, this chapter assumes you have installed Eclipse. Please
    visit [http://www.eclipse.org](http://www.eclipse.org) for details.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s see how to install the Maven plugin for Eclipse:'
  prefs: []
  type: TYPE_NORMAL
- en: Open Eclipse and navigate to **Help** | **Install New Software**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Work with** drop-down menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the <eclipse version> update site.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Collaboration tools**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check Maven's integration with Eclipse, as in the following screenshot:![How
    to do it...](img/B03056_02_01.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Next** and then click on **Finish**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There will be a prompt to restart Eclipse and Maven will be installed after
    the restart.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now let''s see how we can install the Scala plugin for Eclipse:'
  prefs: []
  type: TYPE_NORMAL
- en: Open Eclipse and navigate to **Help** | **Install New Software**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Work with** drop-down menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the <eclipse version> update site.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type `http://download.scala-ide.org/sdk/helium/e38/scala210/stable/site`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press *Enter*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Scala IDE for Eclipse**:![How to do it...](img/B03056_02_02.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Next** and then click on **Finish**. You will be prompted to restart
    Eclipse and Scala will be installed after the restart.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to **Window** | **Open Perspective** | **Scala**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Eclipse is now ready for Scala development!
  prefs: []
  type: TYPE_NORMAL
- en: Developing Spark applications in Eclipse with SBT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Simple Build Tool** (**SBT**) is a build tool made especially for Scala-based
    development. SBT follows Maven-based naming conventions and declarative dependency
    management.'
  prefs: []
  type: TYPE_NORMAL
- en: 'SBT provides the following enhancements over Maven:'
  prefs: []
  type: TYPE_NORMAL
- en: Dependencies are in the form of key-value pairs in the `build.sbt` file as opposed
    to `pom.xml` in Maven
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides a shell that makes it very handy to perform build operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For simple projects without dependencies, you do not even need the `build.sbt`
    file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In `build.sbt`, the first line is the project definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Each project has an immutable map of key-value pairs. This map is changed by
    settings in SBT like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Every change in the settings leads to a new map, as it's an immutable map.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s how we go about adding the `sbteclipse` plugin:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add this to the global plugin file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Alternatively, you can add the following to your project:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the `sbt` shell without any arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Type `eclipse` and it will make an Eclipse-ready project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now you can navigate to **File** | **Import** | **Import existing project into
    workspace** to load the project into Eclipse.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now you can develop the Spark application in Scala using Eclipse and SBT.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a Spark application in IntelliJ IDEA with Maven
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: IntelliJ IDEA comes bundled with support for Maven. We will see how to create
    a new Maven project in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to develop a Spark application on IntelliJ IDEA
    with Maven:'
  prefs: []
  type: TYPE_NORMAL
- en: Select **Maven** in new project window and click on **Next**:![How to do it...](img/B03056_02_03.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter three dimensions of the project:![How to do it...](img/B03056_02_04.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the project's name and location:![How to do it...](img/B03056_02_05.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Finish** and the Maven project is ready.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Developing a Spark application in IntelliJ IDEA with SBT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before Eclipse became famous, IntelliJ IDEA was considered best of the breed
    in IDEs. IDEA has not shed its former glory yet and a lot of developers love IDEA.
    IDEA also has a community edition, which is free. IDEA provides native support
    for SBT, which makes it ideal for SBT and Scala development.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to develop a Spark application on IntelliJ IDEA
    with SBT:'
  prefs: []
  type: TYPE_NORMAL
- en: Add the `sbt-idea` plugin.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add to the global plugin file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Alternatively, you can add to your project as well:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: IDEA is ready to use with SBT.
  prefs: []
  type: TYPE_NORMAL
- en: Now you can develop Spark code using Scala and build using SBT.
  prefs: []
  type: TYPE_NORMAL
