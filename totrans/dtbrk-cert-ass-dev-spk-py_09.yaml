- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mock Test 1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Try your hand at these practice questions to test your knowledge of Apache
    Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question 1:**'
  prefs: []
  type: TYPE_NORMAL
- en: Which statement does not accurately describe a feature of the Spark driver?
  prefs: []
  type: TYPE_NORMAL
- en: The Spark driver serves as the node where the main method of a Spark application
    runs to co-ordinate the application
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Spark driver can be horizontally scaled to enhance overall processing throughput
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Spark driver houses the SparkContext object
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Spark driver is tasked with scheduling the execution of data by using different
    worker nodes in cluster mode
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimal performance dictates that the Spark driver should be positioned as close
    as possible to worker nodes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 2**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of these statements accurately describes stages?
  prefs: []
  type: TYPE_NORMAL
- en: Tasks within a stage can be simultaneously executed by multiple machines
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Various stages within a job can run concurrently
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stages comprise one or more jobs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stages temporarily store transactions before committing them through actions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 3:**'
  prefs: []
  type: TYPE_NORMAL
- en: Which of these statements accurately describes Spark’s cluster execution mode?
  prefs: []
  type: TYPE_NORMAL
- en: Cluster mode runs executor processes on gateway nodes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cluster mode involves the driver being hosted on a gateway machine
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In cluster mode, the Spark driver and the cluster manager are not co-located
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The driver in cluster mode is located on a worker node
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 4:**'
  prefs: []
  type: TYPE_NORMAL
- en: Which of these statements accurately describes Spark’s client execution mode?
  prefs: []
  type: TYPE_NORMAL
- en: Client mode runs executor processes on gateway nodes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In client mode, the driver is co-located with the executor
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In client mode, the Spark driver and the cluster manager are co-located
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In client mode, the driver is found on an edge node
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 5:**'
  prefs: []
  type: TYPE_NORMAL
- en: Which statement accurately describes Spark’s standalone deployment mode?
  prefs: []
  type: TYPE_NORMAL
- en: Standalone mode utilizes only one executor per worker for each application
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In standalone mode, the driver is located on a worker node
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In standalone mode, the cluster does not need the driver
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In standalone mode, the driver is found on an edge node
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 6**:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a task in Spark?
  prefs: []
  type: TYPE_NORMAL
- en: The unit of work performed for each data partition within a task is slots
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tasks are the second-smallest entity that can be executed within Spark
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tasks featuring wide dependencies can be combined into a single task
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A task is a single unit of work done by a partition within Spark
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 7**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following is the highest level in Spark’s execution hierarchy?
  prefs: []
  type: TYPE_NORMAL
- en: Job
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Task
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Executor
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stage
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 8**:'
  prefs: []
  type: TYPE_NORMAL
- en: How can the concept of slots be accurately described in Spark’s context?
  prefs: []
  type: TYPE_NORMAL
- en: The creation and termination of slots align with the workload of an executor
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spark strategically stores data on disk across various slots to enhance I/O
    performance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each slot is consistently confined to a solitary core
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Slots enable the tasks to run in parallel
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 9**:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the role of an executor in Spark?
  prefs: []
  type: TYPE_NORMAL
- en: The executor’s role is to request the transformation of operations into DAG
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There can only be one executor within a Spark environment
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The executor processes partitions in an optimized and distributed manner
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The executor schedules queries for execution
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 10**:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the role of shuffle in Spark?
  prefs: []
  type: TYPE_NORMAL
- en: Shuffle broadcasts variables to different partitions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With shuffle, data is written to the disk
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The shuffle command transforms data in Spark
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shuffles are a narrow transformation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 11**:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the role of actions in Spark?
  prefs: []
  type: TYPE_NORMAL
- en: Actions only read data from a disk
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Actions are used to modify existing RDDs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Actions trigger the execution of tasks
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Actions are used to establish stage boundaries
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 12**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following is one of the tasks of the cluster manager in Spark?
  prefs: []
  type: TYPE_NORMAL
- en: In the event of an executor failure, the cluster manager will collaborate with
    the driver to initiate a new executor
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cluster manager can coalesce partitions to increase the speed of complex
    data processing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cluster manager collects runtime statistics of queries
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cluster manager creates query plans
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 13**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following is one of the tasks of adaptive query execution in Spark?
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive query execution can coalesce partitions to increase the speed of complex
    data processing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the event of an executor failure, the adaptive query execution feature will
    collaborate with the driver to initiate a new executor
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adaptive query execution creates query plans
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adaptive query execution is responsible for spawning multiple executors to carry
    our tasks in Spark
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 14**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following operations is considered a transformation?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.select()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.show()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.head()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.count()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 15**:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a feature of lazy evaluation in Spark?
  prefs: []
  type: TYPE_NORMAL
- en: Spark will fail a job only during execution but not during definition
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spark will fail a job only during definition
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spark will execute upon receiving a transformation operation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spark will fail upon receiving an action
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 16**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following statements about Spark’s execution hierarchy is correct?
  prefs: []
  type: TYPE_NORMAL
- en: In Spark’s execution hierarchy, tasks are above the level of jobs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In Spark’s execution hierarchy, multiple jobs are contained in a stage
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In Spark’s execution hierarchy, a job can potentially span multiple stage boundaries
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In Spark’s execution hierarchy, slots are the smallest unit
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 17**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following is the characteristic of the Spark driver?
  prefs: []
  type: TYPE_NORMAL
- en: The worker nodes are responsible for transforming Spark operations into DAGs
    when the driver sends a command
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Spark driver is responsible for executing tasks and returning results to
    executors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spark driver can be scaled by adding more machines so that the performance of
    Spark tasks can be improved
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Spark driver processes partitions in an optimized and distributed fashion
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 18**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following statements about broadcast variables is accurate?
  prefs: []
  type: TYPE_NORMAL
- en: Broadcast variables are only present on driver nodes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Broadcast variables can only be used for tables that fit into memory
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Broadcast variables are not immutable, meaning they can be shared across clusters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Broadcast variables are not shared across the worker nodes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 19**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns unique values in columns `employee_state`
    and `employee_salary` in DataFrame `df` for all columns?
  prefs: []
  type: TYPE_NORMAL
- en: '`Df.select(''employee_state'').join(df.select(''employee_salary''),` `col(''employee_state'')==col(''employee_salary''),
    ''left'').show()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.select(col(''employee_state''),` `col(''employee_salary'')).agg({''*'':
    ''count''}).show()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.select(''employee_state'', ''employee_salary'').distinct().show()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.select(''employee_state'').union(df.select(''employee_salary'')).distinct().show()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 20**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks reads a Parquet file from the `my_fle_path`
    location, where the file name is `my_file.parquet`, into a DataFrame `df`?
  prefs: []
  type: TYPE_NORMAL
- en: '`df =` `spark.mode("parquet").read("my_fle_path/my_file.parquet")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df =` `spark.read.path("my_fle_path/my_file.parquet")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df =` `spark.read().parquet("my_fle_path/my_file.parquet")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df =` `spark.read.parquet("/my_fle_path/my_file.parquet")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 21**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks performs an inner join of the `salarydf`
    and `employeedf` DataFrames for columns `employeeSalaryID` and `employeeID`, respectively?
  prefs: []
  type: TYPE_NORMAL
- en: '`salarydf.join(employeedf, salarydf.employeeID ==` `employeedf.employeeSalaryID)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Salarydf.createOrReplaceTempView(salarydf)`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`employeedf.createOrReplaceTempView(''employeedf'')`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`spark.sql("SELECT * FROM salarydf CROSS JOIN employeedf ON` `employeeSalaryID
    ==employeeID")`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salarydf`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``join(employeedf, col(employeeID)==col(employeeSalaryID))`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Salarydf.createOrReplaceTempView(salarydf)`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`employeedf.createOrReplaceTempView(''employeedf'')`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`SELECT *` `FROM salarydf`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`INNER` `JOIN employeedf`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`ON salarydf.employeeSalaryID ==` `employeedf. employeeID`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 22**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns the `df` DataFrame sorted in descending
    order by column salary, showing missing values in the end?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.sort(nulls_last("salary"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.orderBy("salary").nulls_last()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.sort("salary", ascending=False)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.nulls_last("salary")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 23**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The code block should return a copy
    of the `df` DataFrame, where the name of the column state is changed to `stateID`.
    Find the error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The arguments to the method `"stateID"` and `"state"` should be swapped
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `withColumn` method should be replaced by the `withColumnRenamed` method
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `withColumn` method should be replaced by `withColumnRenamed` method, and
    the arguments to the method need to be reordered
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is no such method whereby the column name can be changed
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 24**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks performs an inner join between the `salarydf`
    and `employeedf` DataFrames, using the `employeeID` and `salaryEmployeeID` columns
    as join keys, respectively?
  prefs: []
  type: TYPE_NORMAL
- en: '`salarydf.join(employeedf, "inner", salarydf.employeedf ==` `employeeID.salaryEmployeeID)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salarydf.join(employeedf, employeeID ==` `salaryEmployeeID)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salarydf.join(employeedf, salarydf.salaryEmployeeID ==` `employeedf.employeeID,
    "inner")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salarydf.join(employeedf, salarydf.employeeID ==` `employeedf.salaryEmployeeID,
    "inner")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 25**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block should return a `df` DataFrame, where the `employeeID`
    column is converted into an integer. Choose the answer that correctly fills the
    blanks in the code block to accomplish this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`select`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`col("employeeID")`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`as`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`IntegerType`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`select`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`col("employeeID")`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`as`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Integer`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`cast`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`"``employeeID"`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`as`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`IntegerType()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`select`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`col("employeeID")`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`cast`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`IntegerType()`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 26**:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the number of records that are not empty in the column department of the
    resulting DataFrame when we join the `employeedf` and `salarydf` DataFrames for
    the `employeeID` and `employeeSalaryID` columns, respectively. Which code blocks
    (in order) should be executed to achieve this?
  prefs: []
  type: TYPE_NORMAL
- en: 1\. `.filter(col("department").isNotNull())`
  prefs: []
  type: TYPE_NORMAL
- en: 2\. `.count()`
  prefs: []
  type: TYPE_NORMAL
- en: 3\. `employeedf.join(salarydf, employeedf.employeeID ==` `salarydf.employeeSalaryID)`
  prefs: []
  type: TYPE_NORMAL
- en: 4\. `employeedf.join(salarydf, employeedf.employeeID ==salarydf.` `employeeSalaryID,
    how='inner')`
  prefs: []
  type: TYPE_NORMAL
- en: 5\. `.filter(col(department).isnotnull())`
  prefs: []
  type: TYPE_NORMAL
- en: 6\. `.sum(col(department))`
  prefs: []
  type: TYPE_NORMAL
- en: 3, 1, 6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3, 1, 2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 4, 1, 2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3, 5, 2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 27**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns only those rows from the `df` DataFrame
    in which the values in the column state are unique?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.dropDuplicates(subset=["state"]).show()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.distinct(subset=["state"]).show()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.drop_duplicates(subset=["state"]).show()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.unique("state").show()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 28**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The code block should return a copy
    of the `df` DataFrame with an additional column named `squared_number`, which
    has the square of the column number. Find the error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The arguments to the `withColumnRenamed` method need to be reordered
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `withColumnRenamed` method should be replaced by the `withColumn` method
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `withColumnRenamed` method should be replaced by the `select` method, and
    `0.2` should be replaced with `2`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The argument `0.2` should be replaced by `2`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 29**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns a new DataFrame in which column salary
    is renamed to `new_salary` and employee is renamed to `new_employee` in the `df`
    DataFrame?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.withColumnRenamed(salary,` `new_salary).withColumnRenamed(employee, new_employee)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumnRenamed("salary", "new_salary")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumnRenamed("employee", "new_employee")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumn("salary", "``new_salary").withColumn("employee", "new_employee")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumnRenamed("salary", "``new_salary").withColumnRenamed("employee",
    "new_employee")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 30**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns a copy of the `df` DataFrame, where
    the column salary has been renamed to `employeeSalary`?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.withColumn(["salary", "employeeSalary"])`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumnRenamed("salary").alias("employeeSalary ")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumnRenamed("salary", "``employeeSalary ")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumn("salary", "``employeeSalary ")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 31**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The code block should save the `df`
    DataFrame to the `my_file_path` path as a Parquet file, appending to any existing
    parquet file. Find the error.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The code is not saved to the correct path
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `save()` and `format` functions should be swapped
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code block is missing a reference to the `DataFrameWriter`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `option` mode should be overwritten to correctly write the file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 32**:'
  prefs: []
  type: TYPE_NORMAL
- en: How can we reduce the `df` DataFrame from 12 to 6 partitions?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.repartition(12)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.coalesce(6).shuffle()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.coalesce(6, shuffle=True)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.repartition(6)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 33**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns a DataFrame where the timestamp column
    is converted into unix epoch timestamps in a new column named `record_timestamp`
    with a format of day, month, and year?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.withColumn("record_timestamp",` `from_unixtime(unix_timestamp(col("timestamp")),
    "dd-MM-yyyy"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumnRenamed("record_timestamp",` `from_unixtime(unix_timestamp(col("timestamp")),
    "dd-MM-yyyy"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.select ("record_timestamp",` `from_unixtime(unix_timestamp(col("timestamp")),
    "dd-MM-yyyy"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumn("record_timestamp",` `from_unixtime(unix_timestamp(col("timestamp")),
    "MM-dd-yyyy"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 34**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks creates a new DataFrame by appending the
    rows of the DataFrame `salaryDf` to the rows of the DataFrame `employeeDf`, regardless
    of the fact that both DataFrames have different column names?
  prefs: []
  type: TYPE_NORMAL
- en: '`salaryDf.join(employeeDf)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salaryDf.union(employeeDf)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salaryDf.concat(employeeDf)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salaryDf.unionAll(employeeDf)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 35**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The code block should calculate
    the total of all salaries in the `employee_salary` column across each department.
    Find the error.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Instead of `avg("value")`, `avg(col("value"))` should be used
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All column names should be wrapped in `col()` operators
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`"storeId"` and “`value"` should be swapped'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Agg` should be replaced by `groupBy`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 36**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The code block is intended to perform
    a cross-join of the `salarydf` and `employeedf` DataFrames for the `employeeSalaryID`
    and `employeeID` columns, respectively. Find the error.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The join type `"cross"` in the argument needs to be replaced with `crossJoin`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[`salarydf.employeeSalaryID, employeedf.employeeID`] should be replaced by
    `salarydf.employeeSalaryID ==` `employeedf.employeeID`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `"cross"` argument should be eliminated since `"cross"` is the default join
    type
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `"cross"` argument should be eliminated from the call and `join` should
    be replaced by `crossJoin`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 37**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The code block should display the
    schema of the `df` DataFrame. Find the error.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In Spark, we cannot print the schema of a DataFrame
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`printSchema` is not callable through `df.rdd` and should be called directly
    from `df`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is no method in Spark named `printSchema()`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `print_schema()` method should be used instead of `printSchema()`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 38**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block should write the `df` DataFrame as a Parquet file
    to the `filePath` path, replacing any existing file. Choose the answer that correctly
    fills the blanks in the code block to accomplish this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`save`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`mode`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`"``ignore"`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`path`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`store`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`with`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`"``replace"`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`path`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`write`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`mode`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`"``overwrite"`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`save`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`save`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`mode`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`"``overwrite"`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`path`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 39**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The code block is supposed to sort
    the `df` DataFrame according to salary in descending order. Then, it should sort
    based on the bonus column, putting nulls to last. Find the error.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `salary` column should be sorted in a descending way. Moreover, it should
    be wrapped in a `col()` operator
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `salary` column should be wrapped by the `col()` operator
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `bonus` column should be sorted in a descending way, putting `nulls` last
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `bonus` column should be sorted by `desc_nulls_first()` instead
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 40**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The code block should use the `square_root_method`
    Python method to find the square root of the `salary` column in the `df` DataFrame
    and return it in a new column called `sqrt_salary`. Find the error.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: There is no return type specified for `square_root_method`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the second line of the code, Spark needs to call `squre_root_method_udf`
    instead of `square_root_method`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`udf` is not registered with Spark'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A new column needs to be added
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 41**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The code block should return the
    `df` DataFrame with `employeeID` renamed to `employeeIdColumn`. Find the error.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Instead of `withColumn`, the `withColumnRenamed` method should be used
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of `withColumn`, the `withColumnRenamed` method should be used and argument
    `"employeeIdColumn"` should be swapped with argument `"employeeID"`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Arguments `"employeeIdColumn"` and `"employeeID"` should be swapped
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `withColumn` operator should be replaced with the `withColumnRenamed` operator
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 42**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks will return a new DataFrame with the same
    columns as DataFrame `df`, except for the `salary` column?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.drop("salary")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.drop(col(salary))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.drop(salary)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.delete("salary")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 43**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns a DataFrame showing the mean of the
    salary column from the `df` DataFrame, grouped by column department?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.groupBy("department").agg(avg("salary"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.groupBy(col(department).avg())`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.groupBy("department").avg(col("salary"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.groupBy("department").agg(average("salary"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 44**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks creates a DataFrame that shows the mean of
    the salary column of the `salaryDf` DataFrame, based on the department and state
    columns, where age is greater than 35?
  prefs: []
  type: TYPE_NORMAL
- en: '`salaryDf.filter(col("age") >` `35)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``filter(col("employeeID")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``filter(col("employeeID").isNotNull())`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``groupBy("department")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``groupBy("department", "state")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``agg(avg("salary").alias("mean_salary"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``agg(average("salary").alias("mean_salary"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1,2,5,6
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 1,3,5,6
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 1,3,6,7
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 1,2,4,6
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 45**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The code block needs to cache the
    `df` DataFrame so that this DataFrame is fault-tolerant. Find the error.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`persist()` is not a function of the API DataFrame'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.write()` should be used in conjunction with `df.persist` to correctly write
    the DataFrame'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The storage level is incorrect and should be `MEMORY_AND_DISK_2`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.cache()` should be used instead of `df.persist()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 46**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks concatenates the rows of the `salaryDf` and
    `employeeDf` DataFrames without any duplicates (assuming the columns of both DataFrames
    are similar)?
  prefs: []
  type: TYPE_NORMAL
- en: '`salaryDf.concat(employeeDf).unique()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`spark.union(salaryDf, employeeDf).distinct()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salaryDf.union(employeeDf).unique()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salaryDf.union(employeeDf).distinct()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 47**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks reads a complete folder of CSV files from
    `filePath` with column headers?
  prefs: []
  type: TYPE_NORMAL
- en: '`spark.option("header",True).csv(filePath)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`spark.read.load(filePath)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`spark.read().option("header",True).load(filePath)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`spark.read.format("csv").option("header",True).load(filePath)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 48**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The `df` DataFrame contains columns
    [`employeeID`, `salary`, and `department`]. The code block should return a DataFrame
    that contains only the `employeeID` and `salary` columns from DataFrame `df`.
    Find the error.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: All column names from the `df` DataFrame should be specified in the `select`
    arguments
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `select` operator should be replaced by a `drop` operator, and all the column
    names from the `df` DataFrame should be listed as a list
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `select` operator should be replaced by a `drop` operator
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The column name `department` should be listed like `col("department")`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 49**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The code block should write DataFrame
    `df` as a Parquet file to the `filePath` location, after partitioning it for the
    `department` column. Find the error.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`partitionBy()` method should be used instead of `partition()`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`partitionBy()` method should be used instead of `partition()` and `filePath`
    should be added to the `parquet` method'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `partition()` method should be called before the write method and `filePath`
    should be added to `parquet` method
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `"department"` column should be wrapped in a `col()` operator
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 50**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks removes the cached `df` DataFrame from memory
    and disk?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.unpersist()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`drop df`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.clearCache()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.persist()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 51**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block should return a copy of the `df` DataFrame with an
    additional column: `test_column`, which has a value of `19`. Choose the answer
    that correctly fills the blanks in the code block to accomplish this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`withColumn`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`''``test_column''`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`19`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`withColumnRenamed`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`test_column`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`lit(19)`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`withColumn`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`''``test_column''`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`lit(19)`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`withColumnRenamed`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`test_column`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`19`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 52**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block should return a DataFrame with the columns `employeeId`,
    `salary`, `bonus`, and `department` from `transactionsDf` DataFrame. Choose the
    answer that correctly fills the blanks to accomplish this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '`drop`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`"employeeId", "salary", "``bonus", "department"`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`filter`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`"employeeId, salary,` `bonus, department"`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`select`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`["employeeId", "salary", "``bonus", "department"]`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`select`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`col(["employeeId", "``salary", "bonus","department"])`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 53**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns a DataFrame with the `salary` column
    converted into a string in the `df` DataFrame?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.withColumn("salary",` `castString("salary", "string"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumn("salary", col("salary").cast("string"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.select(cast("salary", "string"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumn("salary", col("salary").castString("string"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 54**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The code block should combine data
    from DataFrames `salaryDf` and `employeeDf`, showing all rows of DataFrame `salaryDf`
    that have a matching value in column `employeeSalaryID` with a value in column
    `employeeID` of DataFrame `employeeDf`. Find the error.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `join` statement is missing the right-hand DataFrame, where the column name
    is `employeeSalaryID`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `union` method should be used instead of `join`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of `join`, `innerJoin` should have been used
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`salaryDf` should come in place of `employeeDf`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 55**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks reads a JSON file stored at `my_file_path`
    as a DataFrame?
  prefs: []
  type: TYPE_NORMAL
- en: '`spark.read.json(my_file_path)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`spark.read(my_file_path, source="json")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`spark.read.path(my_file_path)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`spark.read().json(my_file_path)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 56**:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code block contains an error. The code block should return a new
    DataFrame filtered by the rows where `salary` column is greater than 2000 in DataFrame
    `df`. Find the error.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Instead of `where()`, `filter()` should be used
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The argument to the `where` method should be `"col(salary) >` `2000"`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of `>=`, the operator `>` should be used
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The argument to the `where` method should be `"salary >` `2000"`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 57**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns a DataFrame in which the `salary`
    and `state` columns are dropped from the `df` DataFrame?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.withColumn ("``salary", "state")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.drop(["salary", "state"])`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.drop("salary", "state")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumnRenamed ("``salary", "state")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 58**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns a two-column DataFrame that contains
    counts of each department in the `df` DataFrame?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.count("department").distinct()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.count("department")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.groupBy("department").count()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.groupBy("department").agg(count("department"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 59**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks prints the schema of a DataFrame and contains
    both column names and types?
  prefs: []
  type: TYPE_NORMAL
- en: '`print(df.columns)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.printSchema()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.rdd.printSchema()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.print_schema()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 60**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Which of the following code blocks creates a new DataFrame with three columns:
    `department`, `age`, and `max_salary` and has the maximum salary for each employee
    from each department and each age group from the `df` DataFrame?'
  prefs: []
  type: TYPE_NORMAL
- en: '`df.max(salary)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.groupBy(["department", "age"]).agg(max("salary").alias("max_salary"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.agg(max(salary).alias(max_salary'')`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.groupby(department).agg(max(salary).alias(max_salary)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: E
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
