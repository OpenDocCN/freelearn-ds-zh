["```py\n>>> df = pd.read_csv(\"wine.data\",header=None)\n>>> df.head()\n\n```", "```py\n>>> import re\n>>> expr = re.compile('.*[0-9]+\\)\\s?(\\w+).*')\n\n```", "```py\n>>> header_names = ['Class']\n\n```", "```py\n>>> df_header = open(\"wine.names\")\n>>> for l in df_header.readlines():\n if len(expr.findall(l.strip()))!=0:\n header_names.append(expr.findall(l.strip())[0])\n>>> df_header.close()\n\n```", "```py\n>>> df.columns = header_names\n\n```", "```py\n>>> from sklearn import preprocessing\n>>> df_normalized = pd.DataFrame(preprocessing.scale(df[header_names[1:]]))\n>>> df_normalized.columns = header_names[1:]\n>>> df_normalized.describe()\n\n```", "```py\n>>> import sklearn.metrics.pairwise as pairwise\n>>> distances = pairwise.euclidean_distances(df_normalized)\n\n```", "```py\n>>> distances.shape\n\n```", "```py\n>>> from sklearn.manifold import MDS\n>>> mds_coords = MDS().fit_transform(distances)\n>>> pd.DataFrame(mds_coords).plot(kind='scatter',x=1,y=0,color=df.Class[:],colormap='Reds')\n\n```", "```py\n>>> df = pd.read_csv(\"dow_jones_index/dow_jones_index.data\")\n>>> df.head()\n\n```", "```py\n>>> df.close = df.close.apply( lambda x: float(x[1:]))\n\n```", "```py\n>>> import datetime\n>>> df.date = df.date.apply( lambda x: datetime.\\\n datetime(int(x.split('/')[2]),int(x.split('/')[0]),int(x.split('/')[1])))\n\n```", "```py\n>>> df_pivot = df.pivot('stock','date','close').reset_index()\n>>> df_pivot.head()\n\n```", "```py\n>>> import numpy as np\n>>> correlations = np.corrcoef(np.float64(np.array(df_pivot)[:,2:]))\n>>> mds_coords = MDS().fit_transform(correlations)\n>>> pd.DataFrame(mds_coords).plot(kind='scatter',x=1,y=0)\n\n```", "```py\n>>> df_pivot.iloc[0:2].transpose().iloc[2:].plot(kind='scatter',x=0,y=1)\n\n```", "```py\n>>> import scipy.stats\n>>> correlations2 = scipy.stats.spearmanr(np.float64(np.array(df_pivot)[:,1:]))\n>>> mds_coords = MDS().fit_transform(correlations2.correlation)\n>>> pd.DataFrame(mds_coords).plot(kind='scatter',x=1,y=0)\n\n```", "```py\n>>> from fastdtw import fastdtw\n>>> dtw_matrix = np.zeros(shape=(df_pivot.shape[0],df_pivot.shape[0]))\n…  for i in np.arange(0,df_pivot.shape[0]):\n…     for j in np.arange(i+1,df_pivot.shape[0]):\n …      dtw_matrix[i,j] = fastdtw(df_pivot.iloc[i,2:],df_pivot.iloc[j,2:])[0]\n\n```", "```py\n>>> dtw_matrix+=dtw_matrix.transpose()\n\n```", "```py\n>>> mds_coords = MDS().fit_transform(dtw_matrix)\n>>> pd.DataFrame(mds_coords).plot(kind='scatter',x=1,y=0) \n\n```", "```py\n>>> df2 = pd.read_csv(\"Papers.csv\",sep=\",\")\n>>> df2.head()\n\n```", "```py\n>>> keywords_mapping = {}\n>>> keyword_index = 0\n\n>>> for k in df2.keywords:\n…    k = k.split('\\n')\n…    for kw in k:\n…        if keywords_mapping.get(kw,None) is None:\n…           keywords_mapping[kw]='keyword_'+str(keyword_index)\n…           keyword_index+=1\n\n```", "```py\n>>>for (k,v) in keywords_mapping.items():\n…        df2[v] = df2.keywords.map( lambda x: 1 if k in x.split('\\n') else 0 ) Image_B04881_03_18.png\n\n```", "```py\n>>> df2.head().iloc[:,6:]\n\n```", "```py\n>>> distances = pairwise.pairwise_distances(np.float64(np.array(df2)[:,6:]),metric='manhattan')\n>>> mds_coords = MDS().fit_transform(distances)\npd.DataFrame(mds_coords).plot(kind='scatter',x=1,y=0)\n\n```", "```py\n>>> df = pd.read_csv('kmeans.txt',sep='\\t')\n>>> df.plot(kind='scatter',x='x_coord',y='y_coord')\n\n```", "```py\n>>> from sklearn.cluster import KMeans\n>>> kmeans_clusters = KMeans(5).fit_predict(X=np.array(df)[:,1:])\n>>> df.plot(kind='scatter', x='x_coord', y='y_coord', c=kmeans_clusters)\n\n```", "```py\n>>> inertias = []\n>>> ks = np.arange(1,20)\n>>> for k in ks:\n …      inertias.append(KMeans(k).fit(X=np.array(df)[:,1:]).inertia_)\n>>> results = pd.DataFrame({\"num_clusters\": ks, \"sum_distance\": inertias})\n\n```", "```py\n>>> results.plot(kind='scatter', x='num_clusters', y='sum_distance')\n\n```", "```py\n>>> affinity_p_clusters = sklearn.cluster.AffinityPropagation().fit_predict(X=np.array(df)[:,1:]) \n>>> len(np.unique(affinity_p_clusters))\n\n```", "```py\n>>> pd.DataFrame(affinity_p_clusters).plot(kind='hist',bins=np.unique(affinity_p_clusters))\n\n```", "```py\n>>> df = pd.read_csv(\"kmeans2.txt\",sep=\"\\t\")\n>>> df.plot(x='x_coord',y='y_coord',kind='scatter')\n\n```", "```py\n>>> kmeans_clusters = KMeans(2).fit_predict(X=np.array(df)[:,1:])\n>>> df.plot(kind='scatter', x='x_coord', y='y_coord', c=kmeans_clusters)\n\n```", "```py\n>>> from pyclust import KMedoids\n>>> kmedoids_clusters = KMedoids(2).fit_predict(np.array(df)[:,1:])\n>>> df.plot(kind='scatter', x='x_coord', y='y_coord', c=kmedoids_clusters)\n\n```", "```py\n>>> from sklearn.cluster import AgglomerativeClustering\n>>> agglomerative_clusters = AgglomerativeClustering(2,linkage='ward').fit_predict(X=np.array(df)[:,1:])\n>>> df.plot(kind='scatter', x='x_coord', y='y_coord', c=agglomerative_clusters)\n\n```", "```py\n>>> from sklearn.cluster import AgglomerativeClustering\n>>> from sklearn.neighbors import kneighbors_graph\n>>> L = kneighbors_graph(np.array(df)[:,1:], n_neighbors=10, include_self=False)\n>>> L = 0.5 * (L + L.T)\n>>> agglomerative_clusters = AgglomerativeClustering(n_clusters=2,connectivity=L,linkage='average').fit_predict(X=np.array(df)[:,1:])\n>>> df.plot(kind='scatter', x='x_coord', y='y_coord', c=agglomerative_clusters)\n\n```", "```py\n>>> spectral_clusters = sklearn.cluster.SpectralClustering(2).fit_predict(np.array(df)[:,1:])\n>>> df.plot(kind='scatter', x='x_coord', y='y_coord', c=spectral_clusters)\n\n```", "```py\n>>> from pyspark import SparkContext\n>>> from pyspark.streaming import StreamingContext\n\n```", "```py\n>>> sc = SparkContext( 'local', 'streaming-kmeans')\n\n```", "```py\n>>> ssc = StreamingContext(sc, 10)\n\n```", "```py\n>>> class Parser():\n def __init__(self,type='train',delimiter=',',num_elements=5, job_uuid=''):\n self.type=type\n self.delimiter=delimiter\n self.num_elements=num_elements\n self.job_uuid=job_uuid\n\n def parse(self,l):\n try:\n line = l.split(self.delimiter) \n if self.type=='train':\n category = float(line[0])\n feature_vector = Vectors.dense(line[1:])\n return LabeledPoint(category, feature_vector)\n elif self.type=='test':\n category = -1\n feature_vector = Vectors.dense(line)\n return LabeledPoint(category, feature_vector)\n else:\n # log exceptions\n f = open('/errors_events/{0}.txt'.format(self.job_uuid),'a')\n f.write('Unknown type: {0}'.format(self.type))\n f.close()\n except:\n # log errors\n f = open('/error_events/{0}.txt'.format(self.job_uuid),'a')\n f.write('Error parsing line: {0}'.format)\n f.close() \n\n```", "```py\n>>> num_features = 2\nnum_clusters = 3\n\ntraining_parser = Parser('train',',',num_features+1,job_uuid)\ntest_parser = Parser('test',',',num_features,job_uuid)\n\ntrainingData = ssc.textFileStream(\"/training_data\").\\\n map(lambda x: training_parser.parse(x)).map(lambda x: x.features)\ntestData = ssc.textFileStream(\"/test_data\").\\\n map(lambda x: test_parser.parse(x)).map(lambda x: x.features)\nstreaming_clustering = StreamingKMeans(k=num_clusters, decayFactor=1.0).\\\n setRandomCenters(num_features,0,0)\nstreaming_clustering.trainOn(trainingData)\nstreaming_clustering.predictOn(testData).\\\n pprint()\nssc.start() \n\n```", "```py\n>>>  streaming_clustering.latestModel().clusterCenters\n\n```", "```py\n>> streaming_clustering.latestModel().predict([ … ])\n\n```"]