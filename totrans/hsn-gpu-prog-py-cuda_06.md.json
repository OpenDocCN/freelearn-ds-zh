["```py\nker = SourceModule('''\n__global__ void hello_world_ker()\n{\n    printf(\"Hello world from thread %d, in block %d!\\\\n\", threadIdx.x, blockIdx.x);\n```", "```py\n if(threadIdx.x == 0 && blockIdx.x == 0)\n {\n```", "```py\n printf(\"-------------------------------------\\\\n\");\n printf(\"This kernel was launched over a grid consisting of %d blocks,\\\\n\", gridDim.x);\n printf(\"where each block has %d threads.\\\\n\", blockDim.x);\n }\n}\n''')\n```", "```py\nhello_ker = ker.get_function(\"hello_world_ker\")\nhello_ker( block=(5,1,1), grid=(2,1,1) )\n```", "```py\ntest_a = np.float32( [xrange(1,5)] * 4 )\ntest_b = np.float32([xrange(14,10, -1)]*4 )\noutput_mat = np.matmul(test_a, test_b)\n\ntest_a_gpu = gpuarray.to_gpu(test_a)\ntest_b_gpu = gpuarray.to_gpu(test_b)\noutput_mat_gpu = gpuarray.empty_like(test_a_gpu)\n\nmatrix_ker(test_a_gpu, test_b_gpu, output_mat_gpu, np.int32(4), block=(2,2,1), grid=(2,2,1))\n\nassert( np.allclose(output_mat_gpu.get(), output_mat) )\n```", "```py\nker = SourceModule('''\n// row-column dot-product for matrix multiplication\n__device__ float rowcol_dot(float *matrix_a, float *matrix_b, int row, int col, int N)\n{\n float val = 0;\n\n for (int k=0; k < N; k++)\n {\n     val += matrix_a[ row + k*N ] * matrix_b[ col*N + k];\n }\n return(val);\n}\n\n// matrix multiplication kernel that is parallelized over row/column tuples.\n\n__global__ void matrix_mult_ker(float * matrix_a, float * matrix_b, float * output_matrix, int N)\n{\n int row = blockIdx.x + threadIdx.x;\n int col = blockIdx.y + threadIdx.y;\n\n output_matrix[col + row*N] = rowcol_dot(matrix_a, matrix_b, col, row, N);\n}\n''')\n```", "```py\nprintf(\"threadIdx.x,y: %d,%d blockIdx.x,y: %d,%d -- row is %d, col is %d.\\\\n\", threadIdx.x, threadIdx.y, blockIdx.x, blockIdx.y, row, col);\n```", "```py\nint row = blockIdx.x*blockDim.x + threadIdx.x;\nint col = blockIdx.y*blockDim.y + threadIdx.y;\n```", "```py\nprintf(\"threadIdx.x,y: %d,%d blockIdx.x,y: %d,%d -- row is %d, col is %d, N is %d.\\\\n\", threadIdx.x, threadIdx.y, blockIdx.x, blockIdx.y, row, col, N);\n```", "```py\nif(threadIdx.x == 0 && threadIdx.y == 0 && blockIdx.x == 0 && blockIdx.y == 0)\n            printf(\"Dot-product loop: k value is %d, matrix_a value is %f, matrix_b is %f.\\\\n\", k, matrix_a[ row + k*N ], matrix_b[ col*N + k]);\n\n```", "```py\nval += matrix_a[ row*N + k ] * matrix_b[ col + k*N];\n```", "```py\n#include <cuda_runtime.h>\n#include <stdio.h>\n#include <stdlib.h>\n```", "```py\n#define _EPSILON 0.001\n#define _ABS(x) ( x > 0.0f ? x : -x )\n```", "```py\n__host__ int allclose(float *A, float *B, int len)\n{\n\n  int returnval = 0;\n\n  for (int i = 0; i < len; i++)\n  {\n    if ( _ABS(A[i] - B[i]) > _EPSILON )\n    {\n      returnval = -1;\n      break;\n    }\n  }\n\n  return(returnval);\n}\n```", "```py\n\n__device__ float rowcol_dot(float *matrix_a, float *matrix_b, int row, int col, int N)\n{\n  float val = 0;\n\n  for (int k=0; k < N; k++)\n  {\n        val += matrix_a[ row*N + k ] * matrix_b[ col + k*N];\n  }\n\n  return(val);\n}\n\n__global__ void matrix_mult_ker(float * matrix_a, float * matrix_b, float * output_matrix, int N)\n{\n\n    int row = blockIdx.x*blockDim.x + threadIdx.x;\n    int col = blockIdx.y*blockDim.y + threadIdx.y;\n\n  output_matrix[col + row*N] = rowcol_dot(matrix_a, matrix_b, row, col, N);\n}\n```", "```py\n__host__ int main()\n{\n```", "```py\ncudaSetDevice(0);\n```", "```py\nint N = 4;\nint num_bytes = sizeof(float)*N*N;\n```", "```py\n\n float h_A[] = { 1.0, 2.0, 3.0, 4.0, \\\n                 1.0, 2.0, 3.0, 4.0, \\\n                 1.0, 2.0, 3.0, 4.0, \\\n                 1.0, 2.0, 3.0, 4.0 };\n\n float h_B[] = { 14.0, 13.0, 12.0, 11.0, \\\n                 14.0, 13.0, 12.0, 11.0, \\\n                 14.0, 13.0, 12.0, 11.0, \\\n                 14.0, 13.0, 12.0, 11.0 };\n```", "```py\nfloat h_AxB[] = { 140.0, 130.0, 120.0, 110.0, \\\n                 140.0, 130.0, 120.0, 110.0, \\\n                 140.0, 130.0, 120.0, 110.0, \\\n                 140.0, 130.0, 120.0, 110.0 };\n```", "```py\nfloat * d_A;\nfloat * d_B;\nfloat * d_output;\n```", "```py\ncudaMalloc((float **) &d_A, num_bytes);\ncudaMalloc((float **) &d_B, num_bytes);\n```", "```py\ncudaMemcpy(d_A, h_A, num_bytes, cudaMemcpyHostToDevice);\ncudaMemcpy(d_B, h_B, num_bytes, cudaMemcpyHostToDevice);\n```", "```py\ncudaMalloc((float **) &d_output, num_bytes);\n```", "```py\nfloat * h_output;\nh_output = (float *) malloc(num_bytes);\n```", "```py\ndim3 block(2,2,1);\ndim3 grid(2,2,1);\n```", "```py\nmatrix_mult_ker <<< grid, block >>> (d_A, d_B, d_output, N);\n```", "```py\ncudaDeviceSynchronize();\n```", "```py\ncudaMemcpy(h_output, d_output, num_bytes, cudaMemcpyDeviceToHost);\n```", "```py\ncudaDeviceSynchronize();\n```", "```py\ncudaFree(d_A);\ncudaFree(d_B);\ncudaFree(d_output);\n```", "```py\ncudaDeviceReset();\n```", "```py\nif (allclose(h_AxB, h_output, N*N) < 0)\n {\n     printf(\"Error! Output of kernel does not match expected output.\\n\");\n     free(h_output);\n     return(-1);\n }\n else\n {\n     printf(\"Success! Output of kernel matches expected output.\\n\");\n     free(h_output);\n     return(0);\n }\n}\n```", "```py\n#include <cuda_runtime.h>\n#include <stdio.h>\n\n__global__ void divergence_test_ker()\n{\n    if( threadIdx.x % 2 == 0)\n        printf(\"threadIdx.x %d : This is an even thread.\\n\", threadIdx.x);\n    else\n        printf(\"threadIdx.x %d : This is an odd thread.\\n\", threadIdx.x);\n}\n\n__host__ int main()\n{\n    cudaSetDevice(0);\n    divergence_test_ker<<<1, 32>>>();\n    cudaDeviceSynchronize();\n    cudaDeviceReset();\n}\n```"]