<html><head></head><body><div class="chapter" title="Chapter&#xA0;10.&#xA0;Developing the Radix Sort with OpenCL"><div class="titlepage"><div><div><h1 class="title"><a id="ch10"/>Chapter 10. Developing the Radix Sort with OpenCL</h1></div></div></div><p>In this chapter, we are going to explore the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding the Radix sort</li><li class="listitem" style="list-style-type: disc">Understanding the MSD and LSD Radix sorts</li><li class="listitem" style="list-style-type: disc">Understanding reduction</li><li class="listitem" style="list-style-type: disc">Developing the Radix sort in OpenCL</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec67"/>Introduction</h1></div></div></div><p>In the previous chapter, we learned about developing the <a id="id675" class="indexterm"/>Bitonic sort using OpenCL. In this chapter, we are going to explore how to develop the Radix sort with OpenCL. Radix sorting is also known as <span class="strong"><strong>bucket sorting</strong></span>, and we'll see why later on.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note43"/>Note</h3><p>The first Radix sort algorithms came from a machine called the<a id="id676" class="indexterm"/> <span class="strong"><strong>Hollerith machine</strong></span> that was used in 1890 to tabulate the United States census, and though it may not be quite as famous as the machine created by <span class="emphasis"><em>Charles Babbage</em></span>, it does have its place in computing history.</p></div></div></div></div>
<div class="section" title="Understanding the Radix sort"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec68"/>Understanding the Radix sort</h1></div></div></div><p>The Radix sort<a id="id677" class="indexterm"/> is not a comparison-based sorting algorithm, and it has a few qualities that make it more suitable to parallel computation, especially on vector processors such as GPU and modern CPUs.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip33"/>Tip</h3><p>I am somewhat reluctant to use the term <span class="emphasis"><em>modern</em></span> since processor technology has evolved so quickly over time that the use of this word somehow seems dated.</p></div></div><p>The way the Radix sort<a id="id678" class="indexterm"/> works is rather interesting when you compare it with the comparison-based sorting algorithms such as quicksort; the main difference between them is how they process the keys of the input data. The Radix sort does this by breaking down a key into smaller sequences of sub-keys, if you will, and sorts these sub-keys one by one.</p><p>Numbers can be translated in binary and can be viewed as a sequence of bits; the same analogy can be drawn from strings where they are sequences of characters. The Radix sort, when applied to such keys, does not compare the individual keys, but rather it works on processing and comparing pieces of those keys.</p><p>Radix sort algorithms treat the keys like numbers in a<a id="id679" class="indexterm"/> base-R number system. <span class="emphasis"><em>R</em></span> is known as the radix, hence the given name of this algorithm. Different values of <span class="emphasis"><em>R</em></span> can be applied to different types of sorting. Examples could be:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>R = 256</em></span> would be sorting strings where each character is an 8-bit ASCII value</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>R = 65536</em></span> would be sorting Unicode strings where each character is a 16-bit Unicode value</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>R = 2</em></span> would be sorting binary numbers</li></ul></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec151"/>How to do it…</h2></div></div></div><p>At this point, let's examine an example to see how the Radix sort would sort the numbers 44565, 23441, 16482, 98789, and 56732, assuming that each number is a five-digit number laid out in memory in contiguous locations</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>44565</p>
</td><td style="text-align: left" valign="top">
<p>23441</p>
</td><td style="text-align: left" valign="top">
<p>16482</p>
</td><td style="text-align: left" valign="top">
<p>98789</p>
</td><td style="text-align: left" valign="top">
<p>56732</p>
</td></tr></tbody></table></div><p>We are going to extract each digit in a right-to-left fashion examining the least significant digit first. Therefore, we have the following:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>9</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr></tbody></table></div><p>Let's assume we apply counting sort to this array of numbers and it becomes the following:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>9</p>
</td></tr></tbody></table></div><p>This translates to the following order. Take note that the sorting is stable:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>23441</p>
</td><td style="text-align: left" valign="top">
<p>16482</p>
</td><td style="text-align: left" valign="top">
<p>56732</p>
</td><td style="text-align: left" valign="top">
<p>44565</p>
</td><td style="text-align: left" valign="top">
<p>98789</p>
</td></tr></tbody></table></div><p>Next, we shift to the left by one digit. Notice that now the array of numbers is:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td></tr></tbody></table></div><p>Applying the counting sort again and translating it back to the order of the numbers, we have:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>56732</p>
</td><td style="text-align: left" valign="top">
<p>23441</p>
</td><td style="text-align: left" valign="top">
<p>16482</p>
</td><td style="text-align: left" valign="top">
<p>98789</p>
</td><td style="text-align: left" valign="top">
<p>44565</p>
</td></tr></tbody></table></div><p>For the 1000<sup>th</sup> digit we have:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>23441</p>
</td><td style="text-align: left" valign="top">
<p>16482</p>
</td><td style="text-align: left" valign="top">
<p>56732</p>
</td><td style="text-align: left" valign="top">
<p>98789</p>
</td><td style="text-align: left" valign="top">
<p>44565</p>
</td></tr></tbody></table></div><p>For the 10,000<sup>th</sup> digit we have:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>23441</p>
</td><td style="text-align: left" valign="top">
<p>44565</p>
</td><td style="text-align: left" valign="top">
<p>16482</p>
</td><td style="text-align: left" valign="top">
<p>56732</p>
</td><td style="text-align: left" valign="top">
<p>98789</p>
</td></tr></tbody></table></div><p>For the 100,000<sup>th</sup> digit we have:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>16482</p>
</td><td style="text-align: left" valign="top">
<p>23441</p>
</td><td style="text-align: left" valign="top">
<p>44565</p>
</td><td style="text-align: left" valign="top">
<p>56732</p>
</td><td style="text-align: left" valign="top">
<p>98789</p>
</td></tr></tbody></table></div><p>Voila! Radix <a id="id680" class="indexterm"/>sorting sorted the array of five-digit numbers. We should note that the sort is <span class="emphasis"><em>stable</em></span>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note44"/>Note</h3><p><span class="strong"><strong>Stable sorting</strong></span><a id="id681" class="indexterm"/> refers to the capability of the algorithm to be able to maintain the relative order between any two elements with equal keys. Let us assume that an array, <code class="literal">int a[5]</code>, of the values <code class="literal">1</code>, <code class="literal">2</code>, <code class="literal">3</code>, <code class="literal">4</code>, <code class="literal">9</code>, and <code class="literal">2</code>, through some sorting algorithm, X, will sort the elements to <code class="literal">1</code>, <code class="literal">2</code>, <code class="literal">2</code>, <code class="literal">3</code>, <code class="literal">4</code>, and <code class="literal">9</code>. The point here is that the two equal values we saw, which are both the number <code class="literal">2</code>, occur at positions <code class="literal">1</code> and <code class="literal">5</code> (assuming arrays are zero indexed). Then, through X, the sorted list will be such that <code class="literal">a[1]</code> is always before <code class="literal">a[5]</code>.</p></div></div><p>There are actually two basic approaches to Radix sorting. We have seen one approach in which we examine the least-significant digit and sort it. This is commonly referred to as <span class="strong"><strong>LSD Radix sorting</strong></span><a id="id682" class="indexterm"/> since we work our way from right to left. The other approach would be to work from left to right.</p><p>The<a id="id683" class="indexterm"/> key consideration in Radix sorting is the concept of the <span class="emphasis"><em>key</em></span>. Depending on the context, a key may be a word or a string, and each of them would be of fixed length or variable length.</p></div></div>
<div class="section" title="Understanding the MSD and LSD Radix sorts"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec69"/>Understanding the MSD and LSD Radix sorts</h1></div></div></div><p>Let us take some time to understand how the MSD Radix sort<a id="id684" class="indexterm"/> and the LSD Radix sort<a id="id685" class="indexterm"/> work before we start working on developing the equivalent on OpenCL.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec152"/>How to do it…</h2></div></div></div><p>The Radix sort assumes that we wish to sort Radix-R numbers by considering the most significant digit first. For this to happen, we can partition the input into <span class="emphasis"><em>R</em></span> rather than just two, and we have actually seen this done before. This is data binning, but it extends that with the counting sort. A Radix sort can be run on ASCII characters, Unicode characters, integer numbers (32-bit / 64-bit), or floating-point numbers (sorting floating-point numbers is tricky). You need to figure out what constitutes a key. Keys can be thought of as 8-bit keys, 16-bit keys, and so on, and we know by now that Radix sorts require repeated iterations to extract the keys and sort and bin them based on base <span class="emphasis"><em>R</em></span>.</p><p>In the following code snippet, we have an <a id="id686" class="indexterm"/>MSD Radix sort that sorts the characters in a given string in the programming language C, and the radix we use is 256 (the maximum value of an unsigned 8-bit number, otherwise a signed 8-bit would be -128 to 127):</p><div class="informalexample"><pre class="programlisting">#define N // integers to be sorted with values from 0 – 256
void MSD(char[] s) {
  msd_sort(s, 0, len(s), 0);
}
void msd_sort(char[][] s, int lhs, int rhs, int d) {
  if (rhs &lt;= lhs + 1) return;
  int* count = (int*)malloc(257 *sizeof(int));
  for(int i = 0; i &lt; N; ++i)
    count[s[i][d]+1]++;
  for(int k = 1; k &lt; 256; ++k) 
    count[k] += count[k-1];
  for(int j = 0; j &lt; N; ++j) 
    temp[count[s[i][d]]++] = a[i];
  for(int i = 0; i &lt; N; ++i) 
    s[i] = temp[i];
  for(int i = 0; i&lt;255;++i)
    msd_sort(s, 1 + count[i], 1 + count[i+1], d+1);
}</pre></div><p>The second approach in Radix sorting scans the input from right to left and examines each element by applying a similar<a id="id687" class="indexterm"/> operation as in an MSD Radix sort. This is known as the <span class="strong"><strong>Least Significant Digit</strong></span> (<span class="strong"><strong>LSD) Radix sort</strong></span>. LSD Radix sorting works because when any two elements differ, the sorting will place them in the proper relative order, and even when these two elements differ, the fact that LSD exhibits stable sorting means that their relative order is still maintained. Let's take a look at how it would work for sorting three character strings:</p><div class="mediaobject"><img src="graphics/4520OT_10_01.jpg" alt="How to do it…"/></div><p>A typical<a id="id688" class="indexterm"/> LSD Radix sort for sorting characters in a given string might look like the following code (assuming all keys have a fixed width; let's call it <code class="literal">W</code>):</p><div class="informalexample"><pre class="programlisting">void lsd_sort(char[][] a) {
  int N = len(a);
  int W = len(a[0]);
  for(int d = W – 1; d &gt;= 0; d--) {
    int[] count = (int*) malloc(sizeof(int) * 256);
    for(int i = 0; i&lt; N; ++i) 
      count[a[i][d]+1]++;
    for(int k = 1; k &lt; 256; k++)
      count[k] += count[k-1];
    for(int i = 0; i&lt; N; ++i) 
      temp[count[a[i][d]]++] = a[i];
    for(int i= 0; i&lt; N; ++i)
      a[i] = temp[i];
  }
}</pre></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec153"/>How it works…</h2></div></div></div><p>Both approaches are similar as they both bin the characters into <span class="emphasis"><em>R</em></span> bins, that is, 256 bins, and they also use the idea of the counting sort to work out where the final sorting arrangement is going to be using a temporary storage, <code class="literal">temp</code>, and then use that temporary storage and move the data to their sorted places. The nice thing about MSD over LSD Radix sorts is that <a id="id689" class="indexterm"/>MSD may not examine all of the keys and works for variable-length keys; although, in that lies another problem—MSD can experience sub-linear sorts; in practice LSD is generally preferred when the size of the key is fixed.</p><p>The runtime of an <a id="id690" class="indexterm"/>LSD Radix sort is <span class="inlinemediaobject"><img src="graphics/4520OT_10_16.jpg" alt="How it works…"/></span> when compared to the runtimes of other sorting algorithms that are based on the divide-conquer approach, which generally have a runtime of <span class="inlinemediaobject"><img src="graphics/4520OT_10_17.jpg" alt="How it works…"/></span> you might be tempted to conclude that Radix sorting would be faster than comparison-based sorts like quicksort, and you could be right. But, in practice, a well-tuned quicksort can outperform a Radix sort by 24 percent by applying more advanced techniques to improve cache friendliness during the execution. However, technology is constantly evolving, and researchers and engineers will find opportunities to maximize the performance.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip34"/>Tip</h3><p>You may wish to read the papers <span class="emphasis"><em>The influence of cache on sorting</em></span> by <span class="emphasis"><em>LaMarca</em></span> and <span class="emphasis"><em>Adapting Radix Sort to the memory hierarchy</em></span> by <span class="emphasis"><em>Rahman and Raman</em></span> for more algorithmic improvements that they have worked on.</p></div></div></div></div>
<div class="section" title="Understanding reduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec70"/>Understanding reduction</h1></div></div></div><p>Radix sorting employs two techniques: <span class="strong"><strong>reduction</strong></span><a id="id691" class="indexterm"/> and <span class="strong"><strong>scan</strong></span>. These are classified as <a id="id692" class="indexterm"/>data collection patterns as they occur frequently in parallel computing. This recipe will focus on reduction, which allows data to be condensed to a single element using associative binary operators. The<a id="id693" class="indexterm"/> scan <a id="id694" class="indexterm"/>pattern can be easily mistaken for the reduction pattern and the key difference is that this pattern reduces every subsequence of a collection up to every position in the input. We'll defer the discussion of scans until we get to the next section.</p><p>In the reduction pattern, we typically have an associative binary operator, <span class="inlinemediaobject"><img src="graphics/4520OT_10_18.jpg" alt="Understanding reduction"/></span>that we use to collate all elements in a container in a pair-wise fashion. The fact that we need an associative binary operator is an important one, because it implies that the developer can reorganize the combination function to check if it performs efficiently; we'll go into that a little later. Let's take a look at a serial algorithm for conducting reduction in the following code snippet:</p><div class="informalexample"><pre class="programlisting">template&lt;typename T&gt;
T reduce(T (*f)(T, T),
         size_t n,
         T a[],
         T identity) {
  T accumulator = identity;
  for(size_t i = 0; i &lt; n ; ++i)
        accumulator = f(accumulator, a[i]);
  return accumulator;
}</pre></div><p>The algorithm basically takes an associative binary operator, <code class="literal">f</code> (that is, a pointer to a function), and an array <code class="literal">a</code>, of length <code class="literal">n</code> and computes the operation <span class="inlinemediaobject"><img src="graphics/4520OT_10_19.jpg" alt="Understanding reduction"/></span>over the array with an initial value identified by <code class="literal">identity</code>.</p><p>An associative binary operator can allow the developer to extract parallelism from it because associativity means that the operator would produce the same result regardless of the order in which it is applied to the elements. That is to say:</p><div class="mediaobject"><img src="graphics/4520OT_10_20.jpg" alt="Understanding reduction"/></div><p>The previous expression is equivalent to:</p><div class="mediaobject"><img src="graphics/4520OT_10_21.jpg" alt="Understanding reduction"/></div><p>Putting on <a id="id695" class="indexterm"/>the many core hat, we can actually imagine a tree of computations in which the<a id="id696" class="indexterm"/> sub-trees represent the computation of the form<span class="inlinemediaobject"><img src="graphics/4520OT_10_22.jpg" alt="Understanding reduction"/></span>. The first sweep would compute the result of this sub-tree while the second sweep would collate the results of the other sub-trees. This will be evident once you have had a chance to examine them visually in the next two diagrams:</p><div class="mediaobject"><img src="graphics/4520OT_10_02.jpg" alt="Understanding reduction"/></div><p>It will be very<a id="id697" class="indexterm"/> useful for you to contrast the manner in which <a id="id698" class="indexterm"/>these diagrams differ. One of the ways is that the former implies a sequence of operations in traversal order, and this is very different from the latter (as shown in the following diagram):</p><div class="mediaobject"><img src="graphics/4520OT_10_03.jpg" alt="Understanding reduction"/></div><p>It's great news to know that associative operators allow the reduction to be parallelized, but it's not the entire story, because associativity only allows us to group the operations and does not reveal to us whether these groups of binary operations need to occur in a specific order. If you are wondering whether we are talking about commutativity, you are spot on! Commutativity gives<a id="id699" class="indexterm"/> us the important property of changing the order of <a id="id700" class="indexterm"/>application. We know that some operations exhibit one of these while others exhibit both; for example, we know that addition and multiplication of numbers is both associative and commutative. The following is what a commutative parallel reduction might look like:</p><div class="mediaobject"><img src="graphics/4520OT_10_04.jpg" alt="Understanding reduction"/></div><p>Now, seeing this information, you might wonder how this can be translated into OpenCL. We are going to demonstrate a few reductions kernels in this recipe where each one will provide you with an improvement over the previous one.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec154"/>How to do it…</h2></div></div></div><p>For this recipe, we are going to assume that we have a large array of a few million elements and that we like to apply the reduction algorithm to compute the sum of all elements. The first thing to do is produce a parallel algorithm for the serial version we saw earlier. All the kernels we are demonstrating are in <code class="literal">Ch10/Reduction/reduction.cl</code>.</p><p>In the serial version of the algorithm, you would have noticed that we simply pass the accumulator into the binary function to perform the operation. However, we cannot use this method in the GPU since it cannot support tens of thousands of executing threads and also the device can contain many more processors than an x86 CPU has. The only solution is to partition the data across the processors so that each block processes a portion of the input, and when all of the processors are executing in parallel, we should expect the work to be completed in a short span of time.</p><p>Assuming that a block has computed its summed value, we still need a way to collate all those partial sums from all blocks, and considering that OpenCL does not have a global synchronization<a id="id701" class="indexterm"/> primitive or API, we have two options: have OpenCL collate<a id="id702" class="indexterm"/> the partial sums or have the host code collate the partial sums; for our examples, the second option is chosen.</p><p>The first kernel, <code class="literal">reduce0</code>, is a direct translation of the serial algorithm:</p><div class="informalexample"><pre class="programlisting">__kernel void reduce0(__global uint* input, 
                      __global uint* output, 
                      __local uint* sdata) {
    unsigned int tid = get_local_id(0);
    unsigned int bid = get_group_id(0);
    unsigned int gid = get_global_id(0);
    unsigned int blockSize = get_local_size(0);

    sdata[tid] = input[gid];

    barrier(CLK_LOCAL_MEM_FENCE);
    for(unsigned int s = 1; s &lt; BLOCK_SIZE; s &lt;&lt;= 1) {
        // This has a slight problem, the %-operator is rather slow
        // and causes divergence within the wavefront as not all threads
        // within the wavefront is executing.
        if(tid % (2*s) == 0)
        {
            sdata[tid] += sdata[tid + s];
        }
        barrier(CLK_LOCAL_MEM_FENCE);
    }

    // write result for this block to global mem
    if(tid == 0) output[bid] = sdata[0];
}</pre></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec155"/>How it works…</h2></div></div></div><p>This kernel block would load the elements to its shared memory, <code class="literal">sdata</code>, and we conduct the reduction in <code class="literal">sdata</code> in various stages governed by the <code class="literal">for</code> loop, allowing work items with IDs that are multiples of two to perform the pair-wise reduction. Therefore, in the first iteration of the loop, work items with IDs <span class="emphasis"><em>{0, 2, 4, 6, 8, 10, 12, 14, ..., 254}</em></span> would execute, in the second iteration, only work items with IDs <span class="emphasis"><em>{0, 4, 8, 12, 252}</em></span> would execute, and so on. Following the reduction algorithm, the partial sum would be deposited into <code class="literal">sdata[0]</code>, and finally this value would be copied out by one thread which happens to have an ID value equal to <code class="literal">0</code>. Admittedly, this kernel is pretty good but it suffers from two problems: the modulus operator takes a longer time to execute and wavefronts are diverged. The larger issue here is the problem of wavefront divergence since it means that some work items in the <a id="id703" class="indexterm"/>wavefronts are executing while some are not, and in this<a id="id704" class="indexterm"/> case, the work items with odd IDs are not executing while those with even IDs are, GPUs deal with this problem by implementing predication, and this means that all work items in the following code snippet actually get executed. However, the predication unit on the GPU will apply a mask so that only those work items whose IDs matched the condition, <code class="literal">if(tid % (2*s) == 0)</code>, will execute the statement in the <code class="literal">if</code> statement, while those work items who fail the condition, <code class="literal">false</code>, would invalidate their results. Obviously, this is a waste of computing resources:</p><div class="informalexample"><pre class="programlisting">if(tid % (2*s) == 0)
{
    sdata[tid] += sdata[tid + s];
}</pre></div><p>Fortunately, this can be solved with little effort, and the next kernel code demonstrates this:</p><div class="informalexample"><pre class="programlisting">__kernel void reduce1(__global uint* input, 
                      __global uint* output, 
                      __local uint* sdata) {
    unsigned int tid = get_local_id(0);
    unsigned int bid = get_group_id(0);
    unsigned int gid = get_global_id(0);
    unsigned int blockSize = get_local_size(0);

    sdata[tid] = input[gid];

    barrier(CLK_LOCAL_MEM_FENCE);
    for(unsigned int s = 1; s &lt; BLOCK_SIZE; s &lt;&lt;= 1) {
        int index = 2 * s * tid;
        if(index &lt; BLOCK_SIZE)
        {
            sdata[index] += sdata[index + s];
        }
        barrier(CLK_LOCAL_MEM_FENCE);
    }

    // write result for this block to global mem
    if(tid == 0) output[bid] = sdata[0];
}</pre></div><p>We replaced the conditional evaluation after the modulus operator has been applied to something more palatable. The appetizing portion is the fact that we no longer have diverging <a id="id705" class="indexterm"/>wavefronts, and we have also made strided accesses to the shared <a id="id706" class="indexterm"/>memory.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec156"/>There's more…</h2></div></div></div><p>So far, we have seen how we can apply our understanding of associativity to build the reduction kernel and also how to make use of our new understanding of commutativity in the reduction process. The commutative reduction tree<a id="id707" class="indexterm"/> is actually better than the associative reduction tree<a id="id708" class="indexterm"/> because it makes better use of the shared memory by compacting the reduced values and hence raising efficiency; the following kernel, <code class="literal">reduce2</code>, reflects this:</p><div class="informalexample"><pre class="programlisting">__kernel void reduce2(__global uint* input, 
                      __global uint* output, 
                      __local uint* sdata) {
    unsigned int tid = get_local_id(0);
    unsigned int bid = get_group_id(0);
    unsigned int gid = get_global_id(0);
    unsigned int blockSize = get_local_size(0);

    sdata[tid] = input[gid];

    barrier(CLK_LOCAL_MEM_FENCE);
    for(unsigned int s = BLOCK_SIZE/2; s &gt; 0 ; s &gt;&gt;= 1) {
        // Notice that half of threads are already idle on first iteration
        // and with each iteration, its halved again. Work efficiency isn't very good
        // now
        if(tid &lt; s)
        {
            sdata[tid] += sdata[tid + s];
        }
        barrier(CLK_LOCAL_MEM_FENCE);
    }

    // write result for this block to global mem
    if(tid == 0) output[bid] = sdata[0];
}</pre></div><p>However, this isn't very good because now during the first iteration, we have already made half of those work items idle and efficiency is definitely affected. Fortunately, however, the remedy is simple. We reduce half the number of blocks and during the hydration of the shared memory, we load two elements and store the sum of these two elements instead of just loading values from global memory and storing them into shared memory. The kernel, <code class="literal">reduce3</code>, reflects this:</p><div class="informalexample"><pre class="programlisting">__kernel void reduce3(__global uint* input, 
                      __global uint* output, 
                      __local uint* sdata) {
    unsigned int tid = get_local_id(0);
    unsigned int bid = get_group_id(0);
    unsigned int gid = get_global_id(0);

    // To mitigate the problem of idling threads in 'reduce2' kernel,
    // we can halve the number of blocks while each work-item loads
    // two elements instead of one into shared memory
    unsigned int index = bid*(BLOCK_SIZE*2) + tid;
    sdata[tid] = input[index] + input[index+BLOCK_SIZE];

    barrier(CLK_LOCAL_MEM_FENCE);
    for(unsigned int s = BLOCK_SIZE/2; s &gt; 0 ; s &gt;&gt;= 1) {
        // Notice that half of threads are already idle on first iteration
        // and with each iteration, its halved again. Work efficiency isn't very good
        // now
        if(tid &lt; s)
        {
            sdata[tid] += sdata[tid + s];
        }
        barrier(CLK_LOCAL_MEM_FENCE);
    }

    // write result for this block to global mem
    if(tid == 0) output[bid] = sdata[0];
}</pre></div><p>Now, things are starting to<a id="id709" class="indexterm"/> look much better and we've used what we call <span class="strong"><strong>reversed loop</strong></span><a id="id710" class="indexterm"/> (which is basically counting backwards) to get rid of the problem of divergent wavefronts; in the meantime, we have also not reduced our capacity to reduce elements because we've performed that while hydrating the shared memory. The question is whether there's more we can do? Actually, there is another idea we can qualify and that is to take advantage of atomicity of wavefronts or warps executing on GPUs. The next kernel, <code class="literal">reduce4</code>, demonstrates how we utilized wavefront programming to reduce blocks atomically:</p><div class="informalexample"><pre class="programlisting">__kernel void reduce4(__global uint* input, 
                      __global uint* output, 
                      __local uint* sdata) {
    unsigned int tid = get_local_id(0);
    unsigned int bid = get_group_id(0);
    unsigned int gid = get_global_id(0);
    unsigned int blockSize = get_local_size(0);

    unsigned int index = bid*(BLOCK_SIZE*2) + tid;
    sdata[tid] = input[index] + input[index+BLOCK_SIZE];

    barrier(CLK_LOCAL_MEM_FENCE);
    for(unsigned int s = BLOCK_SIZE/2; s &gt; 64 ; s &gt;&gt;= 1) {
        // Unrolling the last wavefront and we cut 7 iterations of this
        // for-loop while we practice wavefront-programming
        if(tid &lt; s)
        {
            sdata[tid] += sdata[tid + s];
        }
        barrier(CLK_LOCAL_MEM_FENCE);
    }

    if (tid &lt; 64) {
        if (blockSize &gt;= 128) sdata[tid] += sdata[tid + 64];
        if (blockSize &gt;=  64) sdata[tid] += sdata[tid + 32];
        if (blockSize &gt;=  32) sdata[tid] += sdata[tid + 16];
        if (blockSize &gt;=  16) sdata[tid] += sdata[tid +  8];
        if (blockSize &gt;=   8) sdata[tid] += sdata[tid +  4];
        if (blockSize &gt;=   4) sdata[tid] += sdata[tid +  2];
        if (blockSize &gt;=   2) sdata[tid] += sdata[tid +  1];
    }
    // write result for this block to global mem
    if(tid == 0) output[bid] = sdata[0];
}</pre></div><p>In the code block demarcated by the statement <code class="literal">if (tid &lt; 64)</code>, we no longer need to place the memory barriers because the code block only hosts one wavefront which executes atomically in the lock step.</p></div></div>
<div class="section" title="Developing the Radix sort in OpenCL"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec71"/>Developing the Radix sort in OpenCL</h1></div></div></div><p>From this section <a id="id711" class="indexterm"/>onwards, we are going to develop this sorting method for <a id="id712" class="indexterm"/>OpenCL. We are going to do two things: implement the parallel Radix sort described in the paper that <span class="emphasis"><em>Marco Zagha</em></span> and <span class="emphasis"><em>Guy E. Blelloch</em></span> wrote in 1991 titled <span class="emphasis"><em>Radix Sort for Vector Multiprocessors</em></span>. The former algorithm was crafted for the <a id="id713" class="indexterm"/>CRAY Y-MP computer (which, in turn, was adapted from the parallel Radix sort algorithm that worked on the<a id="id714" class="indexterm"/> <span class="strong"><strong>Connection Machine (CM-2)</strong></span>).</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec157"/>Getting ready</h2></div></div></div><p>Radix sorting attempts to treat keys as multi-digit numbers, where each digit is an integer depending on the size of the Radix, <span class="emphasis"><em>R</em></span>. An example would be sorting a large array of 32-bit numbers. We can see that each such number is made up of four bytes (each byte is 8-bits on today's CPU and GPU processors), and if we decide to assume that each digit would be 8-bits, we naturally would treat a 32-bit number as comprised of four digits. This notion is most natural when you apply the concept back to a string of words, treating each word as comprising of more than one character.</p><p>The original algorithm worded in the 1999 paper basically uses the counting sort algorithm and it has three main components which will in turn sort the input by iterating all three components until the job is done. The pseudo code, which is a serial algorithm, is presented as follows:</p><div class="informalexample"><pre class="programlisting">COUNTING-SORT
  HISTOGRAM-KEYS
    do i = 0 to 2r -1
      Bucket[i] = 0
    do i = 0 to N – 1
      Bucket[D[j]] = Bucket[D[j]] + 1
  SCAN-BUCKETS
    Sum = 0
    do i = 0 to 2r – 1
      Val = Bucket[i]
      Bucket[i] = Sum
      Sum = Sum + Val
  RANK-AND-PERMUTE
    do j = 0 to N – 1
      A = Bucket[D[j]]
      R[A] = K[j]
      Bucket[D[j]] = A + 1</pre></div><p>The algorithm <code class="literal">HISTOGRAM-KEYS</code> <a id="id715" class="indexterm"/>is something that we have already encountered a few chapters ago, and it is really the histogram. This algorithm computes the distribution of the keys that it encounters during the sort. This algorithm is expressed in a serial fashion, that is, it is supposed to run on a single executing thread; we have already learned how to parallelize that and you can apply those techniques here. However, what we are going to do now deviates from what you have seen in that previous chapter, and we'll reveal that soon enough.</p><p>The next algorithm is<a id="id716" class="indexterm"/> <code class="literal">SCAN-BUCKETS</code>, and it is named as such because it actually scans the entire histogram to compute the prefix sums (we'll examine prefix sums in fair detail later). In this scan operation, <code class="literal">Bucket[i]</code> contains the number of digits with a value, <code class="literal">j</code>, such that <code class="literal">j</code> is greater than <code class="literal">i</code>, and this value is also the position, that is, the array index in the output.</p><p>The final algorithm is<a id="id717" class="indexterm"/> <code class="literal">RANK-AND-PERMUTE</code>, and each key with a digit of value of <code class="literal">i</code> is placed in its final location by getting the offset from <code class="literal">Bucket[i]</code> and incrementing the bucket so that the next key with the same value <code class="literal">i</code> gets placed in the next location. You should also notice that <code class="literal">COUNTING SORT</code> is stable.</p><p>Before we dive into parallelization of the algorithms and how they work in a cohesive manner, it's important to take the next few paragraphs to understand what prefix sums are; the next paragraph highlights why they matter in Radix sorts.</p><p>In the <a id="id718" class="indexterm"/>previous sections, we introduced MSD and LSD Radix sorts and the prefix sums computation is embedded in the code. However, we didn't flag it out for you then. So, now's the time and the following is the code (taken from the previous <code class="literal">lsd_sort</code> and <code class="literal">msd_sort</code> sections):</p><div class="informalexample"><pre class="programlisting">for(int k = 1; k &lt; 256; k++)
      count[k] += count[k-1];</pre></div><p>If you recall how MSD/LSD works, we basically create a histogram of the values we have encountered and, at each stage of the sorting, we compute the prefix sums so that the algorithm <a id="id719" class="indexterm"/>can know where to place the output in a sorted order. If you are still doubtful, you should stop now and flip back to that section and work through the LSD sorting for strings of three characters.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note45"/>Note</h3><p>The prefix sums <a id="id720" class="indexterm"/>is actually a generalization of the global sum, and its original formulation goes something like the following:</p><p>The prefix sum operation takes a binary associative operator <span class="inlinemediaobject"><img src="graphics/4520OT_10_23.jpg" alt="Getting ready"/></span>, and an ordered set of n elements, <span class="inlinemediaobject"><img src="graphics/4520OT_10_24.jpg" alt="Getting ready"/></span>, and returns the ordered set <span class="inlinemediaobject"><img src="graphics/4520OT_10_25.jpg" alt="Getting ready"/></span>.</p></div></div><p>We use a concrete example like taking a summation over an arbitrary array like <code class="literal">[39, 23, 44, 15, 86]</code>. Using the addition operator, the output would be <code class="literal">[39, 62, 108, 125, 211]</code>, and it is not obvious why this sort of computation is important or is even needed. In fact it is not even clear whether there is a direct way to parallelize this algorithm because of dependencies that each subsequent computation relies on the previous.</p><p>A sequential <a id="id721" class="indexterm"/>version of the prefix sums which has a runtime of <span class="inlinemediaobject"><img src="graphics/4520OT_10_16.jpg" alt="Getting ready"/></span> can be expressed as follows, assuming there are two arrays <code class="literal">in_arr</code> and <code class="literal">out_arr</code>, and <code class="literal">out_arr</code> is designed to contain the prefix sums for <code class="literal">in_arr</code>:</p><div class="informalexample"><pre class="programlisting">sum = 0
out_arr[0] = 0
do i = 0 to lengthOf(in_arr)
  t = in_arr[i+1]
  sum = sum + t
  out_arr[i] = sum</pre></div><p>To extract parallelism <a id="id722" class="indexterm"/>from this, we need to adjust the way we view the arbitrary array of input values, and the adjustment we are talking about is actually imagining the array to be consumed by a tree of computations. Let's go on a little further to see why.</p><p>At this point, we think it's important to step back into history and see who came up with the original prefix sum computation. As far as I am aware, two researchers in 1986, <span class="emphasis"><em>Daniel Hillis</em></span> and <span class="emphasis"><em>Guy L. Steele</em></span>, presented a version of the prefix sum as part of an article titled <span class="emphasis"><em>Data Parallel Algorithms</em></span> in the <span class="emphasis"><em>ACM (Association for Computing Machinery)</em></span> magazine, and the algorithm they presented worked as follows (cited as such in that article):</p><div class="informalexample"><pre class="programlisting">for j = 1 to log2n do
  for all k in parallel do
    if (k &gt;= 2j) then
      x[k] = x[k – 2j-1] + x[k]
    fi
  endfor
endfor</pre></div><p>The following diagram (courtesy of <span class="emphasis"><em>Mark Harris</em></span> from the NVIDIA Corporation), pictorially illustrates what the Hillis and Steele algorithm does. It starts at the level where all eight elements are looked upon as leaves of the binary tree and proceeds to work its way through computing the partial sums. Each level of the computation, <code class="literal">d</code>, will compute partial sums based on the previous level's computation. An assumption found in the algorithm is that it assumes that there are as many processors as there are elements and this is demonstrated by the conditional statement in the algorithm, <code class="literal">if (k &gt;= 2j)</code>. Another problem it <a id="id723" class="indexterm"/>has got is that it's not very efficient; it has a runtime complexity of <span class="inlinemediaobject"><img src="graphics/4520OT_10_26.jpg" alt="Getting ready"/></span>, and you will recall that our sequential scan runs at <span class="inlinemediaobject"><img src="graphics/4520OT_10_16.jpg" alt="Getting ready"/></span>, so it is definitely slower.</p><div class="mediaobject"><img src="graphics/4520OT_10_05.jpg" alt="Getting ready"/></div><p>However, <span class="emphasis"><em>Guy Blelloch</em></span> found <a id="id724" class="indexterm"/>ways to improve this, and they are based on the idea of building a balanced binary tree and building out that tree by performing addition on each node (conceptually speaking). Because such a tree with <span class="emphasis"><em>n</em></span> leaves (which is corresponding to the number of elements in the array) would have <span class="inlinemediaobject"><img src="graphics/4520OT_10_27.jpg" alt="Getting ready"/></span> levels and each level has <span class="emphasis"><em>2<sup>d</sup></em></span> nodes, the runtime complexity is <span class="inlinemediaobject"><img src="graphics/4520OT_10_16.jpg" alt="Getting ready"/></span>. The following diagram is an illustration of how a balanced binary tree can compute the array of arbitrary values:</p><div class="mediaobject"><img src="graphics/4520OT_10_06.jpg" alt="Getting ready"/></div><p>The previous diagram <a id="id725" class="indexterm"/>created juxtaposition, and it alters the way the same piece of data you saw, that is, one dimensional flat array containing arbitrary values. Imagine a tree of computations that scans and operates on two values. One way of storing those partial sums is to write the value in place to the array and another way is to use shared memory on the device.</p><p>The astute reader in you <a id="id726" class="indexterm"/>would notice that we can probably parallelize the computation at each level of the tree by allowing one thread to read two elements, sum them up, and write them back into the array, and then you just read off the last element of that array for the final sum. This algorithm that we just described is known as a <span class="strong"><strong>reduction</strong></span> kernel<a id="id727" class="indexterm"/> or an <span class="strong"><strong>up-sweep</strong></span> kernel<a id="id728" class="indexterm"/> (since we are sweeping values up to the root of the tree), and we have seen how it works in the chapter where we discussed about sparse matrix computations in OpenCL. The following is the more formal definition of the reduction phase by <span class="emphasis"><em>Guy Blelloch</em></span> when it's applied to a balanced binary tree with depth <span class="inlinemediaobject"><img src="graphics/4520OT_10_28.jpg" alt="Getting ready"/></span>:</p><div class="informalexample"><pre class="programlisting">for d from 0 to (log2 n) – 1
  in parallel for i from 0 to n – 1 by 2d+1
    array[i + 2d+1 – 1] = array[i + 2d – 1] + array[i + 2d+1 – 1]</pre></div><p>You might think that this up-sweep kernel still doesn't compute the prefix sums, but we do appear to have found a solution to solving summation in parallel; at this point, the following diagram will help us learn what actually goes on during a run of the up-sweep, and we find it helpful to flatten the loop a little to examine its memory access pattern.</p><p>Assuming we have eight elements in our array (that is, <code class="literal">n = 8</code>), our tree would have a depth of <code class="literal">3</code> and <code class="literal">d</code> would range from <code class="literal">0</code> to <code class="literal">2</code>. Imagining that we are at <code class="literal">d = 0</code>, through to <code class="literal">2</code> we would have the following expressions:</p><div class="informalexample"><pre class="programlisting">d = 0 =&gt; i = [0..7,2] array[i + 1] = array[i] + array[i + 1]
d = 1 =&gt; i = [0..7,4] array[i + 3] = array[i + 1] + array[i + 3]
d = 2 =&gt; i = [0..7,8] array[i + 7] = array[i + 3] + array[i + 7]</pre></div><p>The next diagram best <a id="id729" class="indexterm"/>explains the evaluation of the preceding expressions, and a<a id="id730" class="indexterm"/> picture does reveal more about the story than plain equations:</p><div class="mediaobject"><img src="graphics/4520OT_10_07.jpg" alt="Getting ready"/></div><p>From this diagram, we can observe that partial sums are built up at each level of the tree and one of the efficiencies introduced here is not repeating any addition, that is, no redundancies. Let's demonstrate how this would work for an array of eight elements, and we will also employ the up-sweep algorithm.</p><p>The following diagram illustrates the writes that occurred at each level of the tree we're scanning; in that diagram, the boxes colored blue represent the partial sums that were built up at each level of the tree, and the red box represents the final summed value:</p><div class="mediaobject"><img src="graphics/4520OT_10_08.jpg" alt="Getting ready"/></div><p>To be able to compute<a id="id731" class="indexterm"/> the prefix sums from the up-sweep phase we need to<a id="id732" class="indexterm"/> proceed from the root of this tree and perform a <span class="emphasis"><em>down-sweep</em></span> using this algorithm by <span class="emphasis"><em>Guy Blelloch</em></span>:</p><div class="informalexample"><pre class="programlisting">x[n-1]=0
for d = log2 n – 1 to 0 do
  for all k = 0 to n – 1 by 2d+1 in parallel do
    temp = x[k + 2d – 1]
    x[k + 2d – 1] = x[k + 2d+1 – 1]
    x[k + 2d+1 – 1] = temp + x[k + 2d+1 – 1]
  endfor
endfor</pre></div><p>This down-sweep works its way down from the top (or root) of the tree after the reduce phase and builds the prefix sums. Let's flatten the loop to examine its memory access pattern.</p><p>As before with the up-sweep, let's assume that we have eight elements (that is, <code class="literal">n = 8</code>); we would have a depth of <code class="literal">3</code>, and that implies <code class="literal">d</code> would range from <code class="literal">0</code> to <code class="literal">2</code>. The following are the flattened expressions:</p><div class="informalexample"><pre class="programlisting">d = 2 =&gt; k = [0..7,8] 
    temp = x[k + 3]
    x[k + 3] = x[k + 7]
    x[k + 7] = temp + x[k + 7]
d = 1 =&gt; k = [0..7,4]
    temp = x[k + 1]
    x[k + 1] = x[k + 3]
    x[k + 3] = temp + k[x + 3]
d = 0 =&gt; k = [0..7,2]
    temp = x[k]
    x[k] = x[k + 1]
    x[k + 1] = temp + x[k + 1]</pre></div><p>The following <a id="id733" class="indexterm"/>diagram <a id="id734" class="indexterm"/>best expresses how the prefix sums are computed from the reduce/up-sweep phase:</p><div class="mediaobject"><img src="graphics/4520OT_10_09.jpg" alt="Getting ready"/></div><p>Let us concretize these ideas by looking at how the down-sweep phase would proceed after the reduce/up-sweep phase using the following diagram; the <code class="literal">input</code> array is the original array, and we have kept it there for you to verify that the prefix sum computation according to the previous algorithm is correct. The lower portion of the diagram illustrates how <a id="id735" class="indexterm"/>memory is accessed. Keep in mind that updates are done in <a id="id736" class="indexterm"/>place, and when you combine the diagrams of the up-sweep and down-sweep phases, you'll notice that we make two passes over the original input array to arrive at the solution of prefix sums, which is what we wanted:</p><div class="mediaobject"><img src="graphics/4520OT_10_10.jpg" alt="Getting ready"/></div></div><div class="section" title="How to do it …"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec158"/>How to do it …</h2></div></div></div><p>The kernel we present here is found in <code class="literal">Ch10/RadixSort_GPU/RadixSort.cl</code>, and the implementation drew inspiration from the academic paper entitled <span class="emphasis"><em>Radix Sort for Vector Multiprocessors</em></span> by <span class="emphasis"><em>Mark Zagha</em></span> and <span class="emphasis"><em>Guy E. Blelloch</em></span> for 32-bit integers. The algorithm is based on the LSD Radix sort, and it iterates all the keys while shifting the keys based on the chosen Radix and executing OpenCL kernels in sequence; this is best described in the previous diagram.</p><p>As before, we <a id="id737" class="indexterm"/>present the sequential version of the Radix sort that was<a id="id738" class="indexterm"/> translated based on <span class="emphasis"><em>Zagha</em></span> and <span class="emphasis"><em>Blelloch</em></span>, and like what we have done previously, this is the golden reference which we will use to determine the correctness of the data calculated by the OpenCL equivalent. We won't spend too much time discussing about this implementation here, but rather it serves as a reference point where you can draw the similarities and contrasts when we demonstrate how the parallel and sequential code differs:</p><div class="informalexample"><pre class="programlisting">int radixSortCPU(cl_uint* unsortedData, cl_uint* hSortedData) {

    cl_uint *histogram = (cl_uint*) malloc(R * sizeof(cl_uint));
    cl_uint *scratch = (cl_uint*) malloc(DATA_SIZE * sizeof(cl_uint));

    if(histogram != NULL &amp;&amp; scratch != NULL) {

        memcpy(scratch, unsortedData, DATA_SIZE * sizeof(cl_uint));
        for(int bits = 0; bits &lt; sizeof(cl_uint) * bitsbyte ; bits += bitsbyte) {

            // Initialize histogram bucket to zeros
            memset(histogram, 0, R * sizeof(cl_uint));

            // Calculate 256 histogram for all element
            for(int i = 0; i &lt; DATA_SIZE; ++i)
            {
                cl_uint element = scratch[i];
                cl_uint value = (element &gt;&gt; bits) &amp; R_MASK;
                histogram[value]++;
            }

            // Apply the prefix-sum algorithm to the histogram
            cl_uint sum = 0;
            for(int i = 0; i &lt; R; ++i)
            {
                cl_uint val = histogram[i];
                histogram[i] = sum;
                sum += val;
            }

            // Rearrange the elements based on prescanned histogram
            // Thus far, the preceding code is basically adopted from
            // the "counting sort" algorithm.
            for(int i = 0; i &lt; DATA_SIZE; ++i)
            {
                cl_uint element = scratch[i];
                cl_uint value = (element &gt;&gt; bits) &amp; R_MASK;
                cl_uint index = histogram[value];
                hSortedData[index] = scratch[i];
                histogram[value] = index + 1;
            }

            // Copy to 'scratch' for further use since we are not done yet
            if(bits != bitsbyte * 3)
                memcpy(scratch, hSortedData, DATA_SIZE * sizeof(cl_uint));
        }
    }

    free(scratch);
    free(histogram);
    return 1;
}</pre></div><p>This sequential <a id="id739" class="indexterm"/>code is akin to the <code class="literal">lsd_sort</code> code we showed earlier, <a id="id740" class="indexterm"/>and it essentially builds a histogram of the examined keys that uses the counting sort to sort them, and it keeps doing this until all data is acted upon.</p><p>The following kernels are taken from <code class="literal">Ch10/RadixSort_GPU/RadixSort.cl</code>, and we'll refer to the appropriate code when we explain the internal workings of the algorithm:</p><div class="informalexample"><pre class="programlisting">#define bitsbyte 8
#define R (1 &lt;&lt; bitsbyte)

__kernel void computeHistogram(__global const uint* data,
                               __global uint* buckets,
                               uint shiftBy,
                               __local uint* sharedArray) {

    size_t localId = get_local_id(0);
    size_t globalId = get_global_id(0);
    size_t groupId = get_group_id(0);
    size_t groupSize = get_local_size(0);

    /* Initialize shared array to zero i.e. sharedArray[0..63] = {0}*/
    sharedArray[localId] = 0;
    barrier(CLK_LOCAL_MEM_FENCE);

    /* Calculate thread-histograms local/shared memory range from 32KB to 64KB */

    uint result= (data[globalId] &gt;&gt; shiftBy) &amp; 0xFFU;
    atomic_inc(sharedArray+result);

    barrier(CLK_LOCAL_MEM_FENCE);

    /* Copy calculated histogram bin to global memory */

    uint bucketPos = groupId * groupSize + localId ;
    buckets[bucketPos] = sharedArray[localId];
} 
__kernel void rankNPermute(__global const uint* unsortedData,
                           __global const uint* scannedHistogram,
                           uint shiftCount,
                           __local ushort* sharedBuckets,
                           __global uint* sortedData) {

    size_t groupId = get_group_id(0);
    size_t idx = get_local_id(0);
    size_t gidx = get_global_id(0);
    size_t groupSize = get_local_size(0);

    /* There are now GROUP_SIZE * RADIX buckets and we fill
       the shared memory with those prefix-sums computed previously
     */
    for(int i = 0; i &lt; R; ++i)
    {
        uint bucketPos = groupId * R * groupSize + idx * R + i;
        sharedBuckets[idx * R + i] = scannedHistogram[bucketPos];
    }

    barrier(CLK_LOCAL_MEM_FENCE);

    /* Using the idea behind COUNTING-SORT to place the data values in its sorted
       order based on the current examined key
     */
    for(int i = 0; i &lt; R; ++i)
    {
        uint value = unsortedData[gidx * R + i];
        value = (value &gt;&gt; shiftCount) &amp; 0xFFU;
        uint index = sharedBuckets[idx * R + value];
        sortedData[index] = unsortedData[gidx * R + i];
        sharedBuckets[idx * R + value] = index + 1;
        barrier(CLK_LOCAL_MEM_FENCE);
    }
}
__kernel void blockScan(__global uint *output,
                        __global uint *histogram,
                        __local uint* sharedMem,
                        const uint block_size,
                        __global uint* sumBuffer) {
      int idx = get_local_id(0);
      int gidx = get_global_id(0);
      int gidy = get_global_id(1);
      int bidx = get_group_id(0);
      int bidy = get_group_id(1);

      int gpos = (gidx &lt;&lt; bitsbyte) + gidy;
      int groupIndex = bidy * (get_global_size(0)/block_size) + bidx;

      /* Cache the histogram buckets into shared memory
         and memory reads into shared memory is coalesced
      */
      sharedMem[idx] = histogram[gpos];
      barrier(CLK_LOCAL_MEM_FENCE);

    /*
       Build the partial sums sweeping up the tree using
       the idea of Hillis and Steele in 1986
     */
    uint cache = sharedMem[0];
    for(int stride = 1; stride &lt; block_size; stride &lt;&lt;= 1)
    {
        if(idx&gt;=stride)
        {
            cache = sharedMem[idx-stride]+block[idx];
        }
        barrier(CLK_LOCAL_MEM_FENCE); // all threads are blocked here

        sharedMem[idx] = cache;
        barrier(CLK_LOCAL_MEM_FENCE);
    }

    /* write the array of computed prefix-sums back to global memory */
    if(idx == 0)
    {
        /* store the value in sum buffer before making it to 0 */
        sumBuffer[groupIndex] = sharedMem[block_size-1];
        output[gpos] = 0;
    }
    else
    {
        output[gpos] = sharedMem[idx-1];
    }
}
__kernel void unifiedBlockScan(__global uint *output,
                               __global uint *input,
                               __local uint* sharedMem,
                               const uint block_size) {

    int id = get_local_id(0);
    int gid = get_global_id(0);
    int bid = get_group_id(0);

    /* Cache the computational window in shared memory */
    sharedMem[id] = input[gid];

    uint cache = sharedMem[0];

    /* build the sum in place up the tree */
    for(int stride = 1; stride &lt; block_size; stride &lt;&lt;= 1)
    {
        if(id&gt;=stride)
        {
            cache = sharedMem[id-stride]+sharedMem[id];
        }
        barrier(CLK_LOCAL_MEM_FENCE);

        sharedMem[id] = cache;
        barrier(CLK_LOCAL_MEM_FENCE);

    }
    /*write the results back to global memory */
    if(tid == 0) {
        output[gid] = 0;
    } else {
        output[gid] = sharedMem[id-1];
    }
}
__kernel void blockPrefixSum(__global uint* output,
                             __global uint* input,
                             __global uint* summary,
                             int stride) {

     int gidx = get_global_id(0);
     int gidy = get_global_id(1);
     int Index = gidy * stride +gidx;
     output[Index] = 0;

      // Notice that you don't need memory fences in this kernel
      // because there is no race conditions and the assumption
      // here is that the hardware schedules the blocks with lower
      // indices first before blocks with higher indices
     if(gidx &gt; 0)
     {
         for(int i =0;i&lt;gidx;i++)
             output[Index] += input[gidy * stride +i];
     }
     // Write out all the prefix sums computed by this block
     if(gidx == (stride - 1))
         summary[gidy] = output[Index] + input[gidy * stride + (stride -1)];
}

__kernel void blockAdd(__global uint* input,
                       __global uint* output,
                       uint stride) {

      int gidx = get_global_id(0);
      int gidy = get_global_id(1);
      int bidx = get_group_id(0);
      int bidy = get_group_id(1);


      int gpos = gidy + (gidx &lt;&lt; bitsbyte);

      int groupIndex = bidy * stride + bidx;

      uint temp;
      temp = input[groupIndex];

      output[gpos] += temp;
}
__kernel void mergePrefixSums(__global uint* input,
                        __global uint* output) {

   int gidx = get_global_id(0);
   int gidy = get_global_id(1);
   int gpos = gidy + (gidx &lt;&lt; bitsbyte );
   output[gpos] += input[gidy];
}</pre></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec159"/>How it works…</h2></div></div></div><p>The strategy we present here is to break keys, that is, break 32-bit integers into 8-bit digits, and then sort them one at a time starting from the least significant digit. Based on this idea, we are going to loop four times and at each loop number <span class="emphasis"><em>i</em></span>, we are going to examine the <span class="emphasis"><em>i</em></span> numbered 8-bit digit.</p><p>The general looping structure based on the previous description is given in the following code:</p><div class="informalexample"><pre class="programlisting">void runKernels(cl_uint* dSortedData, size_t numOfGroups, size_t groupSize) {
   for(int currByte = 0; currByte &lt; sizeof(cl_uint) * bitsbyte; currByte += bitsbyte) {
    computeHistogram(currByte);
    computeBlockScans();
    computeRankingNPermutations(currByte,groupSize);
  }
}</pre></div><p>The three <a id="id741" class="indexterm"/>invocations in the loop are the work horses of this <a id="id742" class="indexterm"/>implementation and they invoke the kernels to compute the histogram from the input based on the current byte we are looking at. The algorithm will basically compute the histogram of the keys that it has examined; the next phase is to compute the prefix sums (we'll be using the Hillis and Steele algorithm for this), and finally we will update the data structures and write out the values in a sorted order. Let's go into detail about how this works.</p><p>In the host code, you will need to prepare the data structures slightly differently than what we have shown you so far, because these structures need to be shared across various kernels while we swing between host code and kernel code. The following diagram illustrates this general idea for <code class="literal">runKernels()</code>, and this situation is because we created a single command queue which all kernels will latch on to in program order; this applies to their execution as well:</p><div class="mediaobject"><img src="graphics/4520OT_10_11.jpg" alt="How it works…"/></div><p>For this <a id="id743" class="indexterm"/>implementation, the data structure that holds the<a id="id744" class="indexterm"/> unsorted data (that is, <code class="literal">unsortedData_d</code>) needs to be read and shared across the kernels. Therefore, you need to create the device buffer with the flag <code class="literal">CL_MEM_USE_HOST_PTR</code> since the OpenCL specification guarantees that the implementations cached it across multiple kernel invocations. Next, we will look at how the histogram is computed on the GPU.</p><p>The computation of the histogram is based on the threaded histogram we introduced in a previous chapter, but this time around, we decided to show you another implementation which is based on using atomic functions in OpenCL, and in particular using <code class="literal">atomic_inc()</code>. The <code class="literal">atomic_inc</code> function will update the value pointed by the location by one. The histogram works on the OpenCL-supported GPU because we have chosen to use the shared memory and CPU doesn't support that yet. The strategy is to divide our input array into blocks of <span class="emphasis"><em>N x R</em></span> elements where <span class="emphasis"><em>R</em></span> is the radix (in our case <span class="emphasis"><em>R = 8</em></span> since each digit is 8-bits wide and <span class="emphasis"><em>2<sup>8</sup>=256</em></span>) and <span class="emphasis"><em>N</em></span> is the number of threads executing the block. This strategy is based on the assumption that our problem sizes are always going to be much larger than the amount of threads available, and we configure it programmatically on the host code prior to launching the kernel as shown in the following code:</p><div class="informalexample"><pre class="programlisting">void computeHistogram(int currByte) {
    cl_event execEvt;
    cl_int status;
    size_t globalThreads = DATA_SIZE;
    size_t localThreads  = BIN_SIZE;
    status = clSetKernelArg(histogramKernel, 0, sizeof(cl_mem),
             (void*)&amp;unsortedData_d);
    status = clSetKernelArg(histogramKernel, 1, sizeof(cl_mem),
             (void*)&amp;histogram_d);
    status = clSetKernelArg(histogramKernel, 2, sizeof(cl_int),
             (void*)&amp;currByte);
    status = clSetKernelArg(histogramKernel, 3, sizeof(cl_int) *
             BIN_SIZE, NULL);
    status = clEnqueueNDRangeKernel(
        commandQueue,
        histogramKernel,
        1,
        NULL,
        &amp;globalThreads,
        &amp;localThreads,
        0,
        NULL,
        &amp;execEvt);
    clFlush(commandQueue);
    waitAndReleaseDevice(&amp;execEvt);
}</pre></div><p>By setting up the<a id="id745" class="indexterm"/> OpenCL thread block to be equal to <code class="literal">BIN_SIZE</code>, that is, 256, the kernel waits for the computation to complete by polling the OpenCL <a id="id746" class="indexterm"/>device for its execution status; this poll-release mechanism is encapsulated by <code class="literal">waitAndReleaseDevice()</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note46"/>Note</h3><p>When you have multiple kernel invocations and one kernel waits on the other, you need synchronization, and OpenCL provides this via <code class="literal">clGetEventInfo</code> and <code class="literal">clReleaseEvent</code>.</p></div></div><p>In the histogram kernel, we built up the histogram by reading the inputs into shared memory (after initializing it to zero), and to prevent any threads from executing kernel code that reads from shared memory before all data is loaded into it, we placed a memory barrier as follows:</p><div class="informalexample"><pre class="programlisting">    /* Initialize shared array to zero i.e. sharedArray[0..63] = {0}*/
    sharedArray[localId] = 0;       
    barrier(CLK_LOCAL_MEM_FENCE);   </pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note47"/>Note</h3><p>It's debatable whether we should initialize the shared memory, but it's best practice to initialize data structures, just like you would do in other programming languages. The trade off, in this case, is program correctness versus wasting processor cycles.</p></div></div><p>Next, we shift the data value (residing in shared memory) by a number, <code class="literal">shiftBy</code>, which is the key we are sorting, extract the byte, and then update the local histogram atomically. We placed a memory barrier thereafter. Finally, we write out the binned values to their appropriate location in the global histogram, and you will notice that this implementation performs what we call <span class="emphasis"><em>scattered writes</em></span>:</p><div class="informalexample"><pre class="programlisting">    uint result= (data[globalId] &gt;&gt; shiftBy) &amp; 0xFFU; //5
    atomic_inc(sharedArray+result);                         //6

    barrier(CLK_LOCAL_MEM_FENCE);                           //7

    /* Copy calculated histogram bin to global memory */

    uint bucketPos = groupId  * groupSize + localId ; //8
    buckets[bucketPos] = sharedArray[localId];        //9</pre></div><p>Once the<a id="id747" class="indexterm"/> histogram is established, the next task that <code class="literal">runKernels()</code> performs <a id="id748" class="indexterm"/>is to execute the computations of prefix sums in the kernels <code class="literal">blockScan</code>, <code class="literal">blockPrefixSum</code>, <code class="literal">blockAdd</code>, <code class="literal">unifiedBlockScan</code>, and <code class="literal">mergePrefixSums</code> in turn. We'll explain what each kernel does in the following sections.</p><p>The general strategy for this phase (encapsulated in <code class="literal">computeBlockScans()</code>) is to pre-scan the histogram bins so that we generate the prefix sums for each bin. We then write out that value to an auxiliary data structure, <code class="literal">sum_in_d</code>, and write out all intermediary sums into another auxiliary data structure, <code class="literal">scannedHistogram_d</code>. The following is the configuration we sent to the <code class="literal">blockScan</code> kernel:</p><div class="informalexample"><pre class="programlisting">    size_t numOfGroups = DATA_SIZE / BIN_SIZE;
    size_t globalThreads[2] = {numOfGroups, R};
    size_t localThreads[2] = {GROUP_SIZE, 1};
    cl_uint groupSize = GROUP_SIZE;

status = clSetKernelArg(blockScanKernel, 0, sizeof(cl_mem), (void*)&amp;scannedHistogram_d);
    status = clSetKernelArg(blockScanKernel, 1, sizeof(cl_mem), (void*)&amp;histogram_d);
    status = clSetKernelArg(blockScanKernel, 2, GROUP_SIZE * sizeof(cl_uint), NULL);
    status = clSetKernelArg(blockScanKernel, 3, sizeof(cl_uint), &amp;groupSize);
    status = clSetKernelArg(blockScanKernel, 4, sizeof(cl_mem), &amp;sum_in_d);
    cl_event execEvt;
    status = clEnqueueNDRangeKernel(
                commandQueue,
                blockScanKernel,
                2,
                NULL,
                globalThreads,
                localThreads,
                0,
                NULL,
                &amp;execEvt);
    clFlush(commandQueue);
    waitAndReleaseDevice(&amp;execEvt);</pre></div><p>The general <a id="id749" class="indexterm"/>strategy behind scanning is illustrated in <a id="id750" class="indexterm"/>the following diagram, where the input is divided into separate blocks and each block will be submitted for a block scan. The generated results are prefix sums, but we need to collate these results across all blocks to obtain a cohesive view. After which, the histogram bins are updated with these prefix sum values, and then finally we can use the updated histogram bins to sort the input array.</p><div class="mediaobject"><img src="graphics/4520OT_10_12.jpg" alt="How it works…"/></div><p>Let's look at <a id="id751" class="indexterm"/>how the block scan is done by examining <code class="literal">blockScan</code>. First, we <a id="id752" class="indexterm"/>load the values from the previously computed histogram bin into its shared memory as shown in the following code:</p><div class="informalexample"><pre class="programlisting">__kernel void blockScan(__global uint *output,
                        __global uint *histogram,
                        __local uint* sharedMem,
                        const uint block_size,
                        __global uint* sumBuffer) {
      int idx = get_local_id(0);
      int gidx = get_global_id(0);
      int gidy = get_global_id(1);
      int bidx = get_group_id(0);
      int bidy = get_group_id(1);

      int gpos = (gidx &lt;&lt; bitsbyte) + gidy;
      int groupIndex = bidy * (get_global_size(0)/block_size) + bidx;

      /* Cache the histogram buckets into shared memory
         and memory reads into shared memory is coalesced
      */
      sharedMem[idx] = histogram[gpos];
      barrier(CLK_LOCAL_MEM_FENCE);</pre></div><p>Next, we perform the Hillis and Steele prefix sum algorithm locally, and build the summed values for the current block:</p><div class="informalexample"><pre class="programlisting">    /*
       Build the partial sums sweeping up the tree using
       the idea of Hillis and Steele in 1986
     */
    uint cache = sharedMem[0];
    for(int dis = 1; dis &lt; block_size; dis &lt;&lt;= 1)
    {
        if(idx&gt;=dis)
        {
            cache = sharedMem[idx-dis]+block[idx];
        }
        barrier(CLK_LOCAL_MEM_FENCE); // all threads are blocked here

        sharedMem[idx] = cache;
        barrier(CLK_LOCAL_MEM_FENCE);
    }</pre></div><p>Finally, we write<a id="id753" class="indexterm"/> out a prefix sum for this block to <code class="literal">sum_in_d</code>, represented in the<a id="id754" class="indexterm"/> following code by <code class="literal">sumBuffer</code>, and the intermediary prefix sums to the <code class="literal">scannedHistogram_d</code> object, represented here by <code class="literal">output</code>:</p><div class="informalexample"><pre class="programlisting">    /* write the array of computed prefix-sums back to global memory */
    if(idx == 0)
    {
        /* store the value in sum buffer before making it to 0 */
        sumBuffer[groupIndex] = sharedMem[block_size-1];
        output[gpos] = 0;
    } else {        
        output[gpos] = sharedMem[idx-1];
    }
}</pre></div><p>The following diagram illustrates this concept for two parallel block scans (assuming we have a shared memory that holds eight elements) and shows how it's stored into the output:</p><div class="mediaobject"><img src="graphics/4520OT_10_13.jpg" alt="How it works…"/></div><p>At this phase of the <a id="id755" class="indexterm"/>computation, we have managed to compute the <a id="id756" class="indexterm"/>prefix sums for all the individual blocks. We need to collate them through the next phase, which is in the kernel <code class="literal">blockPrefixSum</code> where the individual block's summed value is accumulated by each work item. The work done by each thread will compute the sum across different blocks. Depending on the thread with ID, <code class="literal">i</code>, will gather all sums from block number <code class="literal">0</code> to <code class="literal">(i – 1)</code>. The following code in <code class="literal">blockPrefixSum</code> illustrates this process:</p><div class="informalexample"><pre class="programlisting">__kernel void blockPrefixSum(__global uint* output,
                             __global uint* input,
                             __global uint* summary,
                             int stride) {

     int gidx = get_global_id(0);
     int gidy = get_global_id(1);
     int Index = gidy * stride +gidx;
     output[Index] = 0;

     if(gidx &gt; 0) {
         for(int i =0;i&lt;gidx;i++)
             output[Index] += input[gidy * stride +i];
     }</pre></div><p>The astute reader will notice that we have left out the prefix sum for one block, and the following remedies are obtained by computing the final accumulated prefix sums for this block:</p><div class="informalexample"><pre class="programlisting">     // Write out all the prefix sums computed by this block
     if(gidx == (stride - 1))
         summary[gidy] = output[Index] + input[gidy * stride + (stride -1)];</pre></div><p>The following diagram best represents what computation goes on in the previous kernel code. It assumes that we have a block scan for 16 elements that has been completed in <code class="literal">blockScanKernel</code>, and each element contains the prefixed sum. To collate these sums, we configure our <a id="id757" class="indexterm"/>kernel to run eight threads with a striding factor <a id="id758" class="indexterm"/>of <code class="literal">8</code> (assuming a block size of eight), and the diagram expresses what each of the eight threads are working on. The threads collate the sums by working out the summation of the entire input, progressively computing <span class="inlinemediaobject"><img src="graphics/4520OT_10_28_1.jpg" alt="How it works…"/></span> and writing them out to <code class="literal">sum_out_d</code> and <code class="literal">summary_in_d</code>.</p><p>The following is a diagram that illustrates the process where given an input, all elements of that input are the summed values of the block scan for all blocks; the algorithm basically sums everything and writes to the output array:</p><div class="mediaobject"><img src="graphics/4520OT_10_14.jpg" alt="How it works…"/></div><p>At this point, we have to collate the intermediary prefix sums computed, that is, <span class="inlinemediaobject"><img src="graphics/4520OT_10_29.jpg" alt="How it works…"/></span> inside <code class="literal">sum_out_d</code>, and with that from <code class="literal">scannedHistogram_d</code>. We basically add the <a id="id759" class="indexterm"/>two intermediary sums together using <code class="literal">blockAddKernel</code>. The following<a id="id760" class="indexterm"/> is how we prepare the kernel prior to launch:</p><div class="informalexample"><pre class="programlisting">        cl_event execEvt2;
        size_t globalThreadsAdd[2] = {numOfGroups, R};
        size_t localThreadsAdd[2] = {GROUP_SIZE, 1};
        status = clSetKernelArg(blockAddKernel, 0, sizeof(cl_mem), (void*)&amp;sum_out_d);
        status = clSetKernelArg(blockAddKernel, 1, sizeof(cl_mem), (void*)&amp;scannedHistogram_d);
        status = clSetKernelArg(blockAddKernel, 2, sizeof(cl_uint), (void*)&amp;stride);
        status = clEnqueueNDRangeKernel(
                    commandQueue,
                    blockAddKernel,
                    2,
                    NULL,
                    globalThreadsAdd,
                    localThreadsAdd,
                    0,
                    NULL,
                    &amp;execEvt2);
        clFlush(commandQueue);
        waitAndReleaseDevice(&amp;execEvt2);</pre></div><p>We then basically <a id="id761" class="indexterm"/>collate them back to <code class="literal">scannedHistogram_d</code> with <code class="literal">blockAddKernel</code> whose code is shown as follows:</p><div class="informalexample"><pre class="programlisting">__kernel void blockAdd(__global uint* input,
                       __global uint* output,
                       uint stride) {

      int gidx = get_global_id(0);
      int gidy = get_global_id(1);
      int bidx = get_group_id(0);
      int bidy = get_group_id(1);


      int gpos = gidy + (gidx &lt;&lt; bitsbyte);

      int groupIndex = bidy * stride + bidx;

      uint temp;
      temp = input[groupIndex];

      output[gpos] += temp;
}</pre></div><p>Finally, we perform another<a id="id762" class="indexterm"/> prefix sum to collate the values in <code class="literal">summary_in_d</code>, as all elements inside that array contains each individual block's prefix sum. Because our chosen Radix value is <code class="literal">256</code>, we need to work out the prefix sums computation for blocks <code class="literal">0</code> to <code class="literal">y</code> using <span class="inlinemediaobject"><img src="graphics/4520OT_10_30.jpg" alt="How it works…"/></span>through to <span class="inlinemediaobject"><img src="graphics/4520OT_10_31.jpg" alt="How it works…"/></span>. This is illustrated in the following diagram, and it is encapsulated in the <code class="literal">unifiedBlockScan</code> kernel. We won't show the kernel code as it's similar to the <code class="literal">blockPrefixSum</code> kernel.</p><div class="mediaobject"><img src="graphics/4520OT_10_15.jpg" alt="How it works…"/></div><p>At this point in time, we are left with writing the collated prefix sums we have just performed previously into <code class="literal">scannedHistogram_d</code>. This collation exercise is different from the previous one where we gather the intermediary prefix sums across the blocks, but nonetheless, it's still a<a id="id763" class="indexterm"/> collation exercise, and we need to push in the values from <code class="literal">summary_in_d</code>. We accomplished this with <code class="literal">mergePrefixSumsKernel</code> with the inputs reflected in<a id="id764" class="indexterm"/> the following host code:</p><div class="informalexample"><pre class="programlisting">        cl_event execEvt4;
        size_t globalThreadsOffset[2] = {numOfGroups, R};
        status = clSetKernelArg(mergePrefixSumsKernel, 0, sizeof(cl_mem), (void*)&amp;summary_out_d);
        status = clSetKernelArg(mergePrefixSumsKernel, 1, sizeof(cl_mem), (void*)&amp;scannedHistogram_d);
        status = clEnqueueNDRangeKernel(commandQueue, mergePrefixSumsKernel, 2, NULL, globalThreadsOffset, NULL, 0, NULL, &amp;execEvt4);
        clFlush(commandQueue);
        waitAndReleaseDevice(&amp;execEvt4);</pre></div><p>The <code class="literal">mergePrefixSumsKernel</code> exercise is a relatively simple exercise to shift the values to their proper positions with the following kernel code:</p><div class="informalexample"><pre class="programlisting">__kernel void mergePrefixSums(__global uint* input,
                              __global uint* output) {

   int gidx = get_global_id(0);
   int gidy = get_global_id(1);
   int gpos = gidy + (gidx &lt;&lt; bitsbyte );
   output[gpos] += input[gidy];
}</pre></div><p>With this, the prefix sums are properly computed. The next phase of the algorithm will be to rank and permute the keys using each work item / thread to permute its 256 elements via the prescanned histogram bins, encapsulated in <code class="literal">computeRankNPermutations()</code>. The following is <a id="id765" class="indexterm"/>the host code for the kernel launch:</p><div class="informalexample"><pre class="programlisting">void computeRankingNPermutations(int currByte, size_t groupSize) {
    cl_int status;
    cl_event execEvt;

    size_t globalThreads = DATA_SIZE/R;
    size_t localThreads = groupSize;

    status = clSetKernelArg(permuteKernel, 0, sizeof(cl_mem), (void*)&amp;unsortedData_d);
    status = clSetKernelArg(permuteKernel, 1, sizeof(cl_mem), (void*)&amp;scannedHistogram_d);
    status = clSetKernelArg(permuteKernel, 2, sizeof(cl_int), (void*)&amp;currByte);
    status = clSetKernelArg(permuteKernel, 3, groupSize * R * sizeof(cl_ushort), NULL); // shared memory
    status = clSetKernelArg(permuteKernel, 4, sizeof(cl_mem), (void*)&amp;sortedData_d);

    status = clEnqueueNDRangeKernel(commandQueue, permuteKernel, 1, NULL, &amp;globalThreads, &amp;localThreads, 0, NULL, &amp;execEvt);
    clFlush(commandQueue);
    waitAndReleaseDevice(&amp;execEvt);</pre></div><p>Once the kernel has completed successfully, the data values will be in a sorted order and will be held in the device memory by <code class="literal">sortedData_d</code>. We need to copy those data into <code class="literal">unsortedData_d</code> again, and we will continue to do this until we have not completed the iteration of the keys.</p><p>In the <code class="literal">rankNPermute</code> kernel, we <a id="id766" class="indexterm"/>will again make use of shared memory. The data into shared memory, and the data is organized as <code class="literal">GROUP_SIZE * RADIX</code> where the <code class="literal">GROUP_SIZE = 64</code> and <code class="literal">RADIX = 256</code> expressions hold true, and because each work group is configured to execute with 64 threads, we basically have one thread hydrating 256 elements of its shared memory (which the following code snippet demonstrates):</p><div class="informalexample"><pre class="programlisting">__kernel void rankNPermute(__global const uint* unsortedData,
                           __global const uint* scannedHistogram,
                           uint shiftCount,
                           __local ushort* sharedBuckets,
                           __global uint* sortedData) {
    size_t groupId = get_group_id(0);
    size_t idx = get_local_id(0);
    size_t gidx = get_global_id(0);
    size_t groupSize = get_local_size(0);
    for(int i = 0; i &lt; R; ++i) {
        uint bucketPos = groupId * R * groupSize + idx * R + i;
        sharedBuckets[idx * R + i] = scannedHistogram[bucketPos];
    }
    barrier(CLK_LOCAL_MEM_FENCE);</pre></div><p>Next, it ranks the elements based on the same idea as in the sequential algorithm, and you should refer back to that now. The difference is that we are pulling data values from <code class="literal">unsortedData</code> in global device memory, processing them in device memory, figuring out where the values should be, and writing them out to <code class="literal">sortedData</code>:</p><div class="informalexample"><pre class="programlisting">    for(int i = 0; i &lt; R; ++i) {
        uint value = unsortedData[gidx * R + i];
        value = (value &gt;&gt; shiftCount) &amp; 0xFFU;
        uint index = sharedBuckets[idx * R + value];
        sortedData[index] = unsortedData[gidx * R + i];
        sharedBuckets[idx * R + value] = index + 1;
        barrier(CLK_LOCAL_MEM_FENCE);
    }</pre></div><p>After the ranking and<a id="id767" class="indexterm"/> permutation is done, the data values in the <code class="literal">sortedData_d</code> object<a id="id768" class="indexterm"/> are sorted based on the current examined key. The algorithm will copy the data in <code class="literal">sortedData_d</code> into <code class="literal">unsortedData_d</code> so that the entire process can be repeated for a total of four times.</p></div></div></body></html>