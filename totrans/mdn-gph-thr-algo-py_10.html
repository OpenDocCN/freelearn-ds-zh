<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-78"><a id="_idTextAnchor078" class="pcalibre calibre6 pcalibre1"/>6</h1>
<h1 id="_idParaDest-79" class="calibre5"><a id="_idTextAnchor079" class="pcalibre calibre6 pcalibre1"/>Stock Market Data</h1>
<p class="calibre3">In this chapter, we’ll <a id="_idIndexMarker260" class="pcalibre calibre6 pcalibre1"/>introduce <strong class="bold">temporal data</strong> and dive into stock market trend analysis. To understand trends over time, we’ll return to <strong class="bold">centrality measurements</strong> on <a id="_idIndexMarker261" class="pcalibre calibre6 pcalibre1"/>networks and introduce some more advanced algorithms. Finally, we’ll analyze stock pricing data over time using our centrality measurements and pinpoint changes in behavior over time within and across different stocks to predict spikes and crashes in price.</p>
<p class="calibre3">By the end of this chapter, you’ll be able to wrangle datasets with time components into a series of networks and analyze structural changes over time with centrality metrics. Many of the centrality metrics scale well to large networks, particularly when they are run in parallel.</p>
<p class="calibre3">Specifically, we will cover the following topics:</p>
<ul class="calibre10">
<li class="calibre11">Introduction to temporal data</li>
<li class="calibre11">Introduction to centrality metrics</li>
<li class="calibre11">Application of centrality metrics across time slices</li>
<li class="calibre11">Extending network metrics for time series analytics</li>
</ul>
<p class="calibre3">Let’s get started by returning to temporal datasets and the limitations of non-network-based models to analyze them.</p>
<h1 id="_idParaDest-80" class="calibre5"><a id="_idTextAnchor080" class="pcalibre calibre6 pcalibre1"/>Technical requirements</h1>
<p class="calibre3">You will require Jupyter Notebook to run the practical examples in this chapter. </p>
<p class="calibre3">The code for this chapter is available here: <a href="https://github.com/PacktPublishing/Modern-Graph-Theory-Algorithms-with-Python" class="pcalibre calibre6 pcalibre1">https://github.com/PacktPublishing/Modern-Graph-Theory-Algorithms-with-Python</a></p>
<h1 id="_idParaDest-81" class="calibre5"><a id="_idTextAnchor081" class="pcalibre calibre6 pcalibre1"/>Introduction to temporal data</h1>
<p class="calibre3">In <a href="B21087_02.xhtml#_idTextAnchor028" class="pcalibre calibre6 pcalibre1"><em class="italic">Chapter 2</em></a>, we briefly introduced temporal <a id="_idIndexMarker262" class="pcalibre calibre6 pcalibre1"/>data or data in the form of a time series or a group of time series. <strong class="bold">Time series data</strong> tracks important metrics in many <a id="_idIndexMarker263" class="pcalibre calibre6 pcalibre1"/>different industries: daily store sales volumes, weekly software product marketing lead volumes, daily incidence of an emerging disease, yearly behavior rates (such as smoking or vegetable consumption) in a population, or hourly stock prices tracking market trends. Many related factors can influence trends over time, and some models consider these factors directly if they are known in advance.</p>
<p class="calibre3">However, consider<a id="_idIndexMarker264" class="pcalibre calibre6 pcalibre1"/> the case of sales trends for a new gem store in a city where gem stores are a new phenomenon, perhaps somewhere rural between Haifa and Tel Aviv (<em class="italic">Figure 6</em><em class="italic">.1</em>). Thus, there is very little known about what might influence sales. Understanding what trends exist in the time series data is critical when mining for factors that might influence sales. However, time series datasets pose significant challenges to many supervised learning methods, such <a id="_idIndexMarker265" class="pcalibre calibre6 pcalibre1"/>as <strong class="bold">random forest models</strong> or <strong class="bold">linear regression</strong>. At<a id="_idIndexMarker266" class="pcalibre calibre6 pcalibre1"/> one point in time, sales are not independent; they rely on factors that influence sales in the previous days, limiting the use of supervised learning and many types<a id="_idIndexMarker267" class="pcalibre calibre6 pcalibre1"/> of <strong class="bold">unsupervised learning</strong>. The lack of predictors also poses a challenge.</p>
<div><div><img alt="Figure 6.1 – ﻿An illustration of a gem store located partway between Tel Aviv and Haifa, Israel" src="img/B21087_06_01.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.1 – An illustration of a gem store located partway between Tel Aviv and Haifa, Israel</p>
<p class="calibre3">Fortunately, for our gem store, many algorithms are designed to <a id="_idIndexMarker268" class="pcalibre calibre6 pcalibre1"/>handle time series data, such <a id="_idIndexMarker269" class="pcalibre calibre6 pcalibre1"/>as <strong class="bold">autoregressive integrated moving average models</strong> (<strong class="bold">ARIMA models</strong>), <strong class="bold">singular spectrum analysis</strong> (<strong class="bold">SSA</strong>), and <strong class="bold">Holt–Winters models</strong>. However, changes in time series <a id="_idIndexMarker270" class="pcalibre calibre6 pcalibre1"/>behavior (spikes, crashes, and changes in variance) pose a<a id="_idIndexMarker271" class="pcalibre calibre6 pcalibre1"/> challenge to these models. Capturing and predicting these changes is critical if you want to create a predictive model or mine for factors influencing the time series values. In our gem store example, seasonality in tourism, conflicts in the region, and holiday travel promotions may influence traffic along the route in which our store is positioned.</p>
<p class="calibre3">One industry with abundant and very complex time series data is finance. Many social and economic factors influence stock prices, and untangling the relationships and randomness in stock price fluctuations underlies much of the finance industry. Let’s have a look at the stock market pricing data and common tasks in analyzing stock market pricing data.</p>
<h2 id="_idParaDest-82" class="calibre7"><a id="_idTextAnchor082" class="pcalibre calibre6 pcalibre1"/>Stock market applications</h2>
<p class="calibre3">In recent years, the <a id="_idIndexMarker272" class="pcalibre calibre6 pcalibre1"/>financial sector has shifted from an expert-driven model of stock market insight to a more machine learning-based approach. Machine learning models sift out emerging trends and catch subtle trends that may escape a human pouring over the data. This tactic also allows analysts to process a much larger data collection than a human could process, including many different sectors, international stock exchanges, and even individual frontier markets. By collecting more insight, it is possible for investors and investment management firms to invest in a wider variety of markets without as much expertise in those markets.</p>
<p class="calibre3">Stock market analytics covers a vast field of applications. Investments can be made within certain market sectors (such as technology or agriculture) or across markets. They can focus on short-term gains (including those made in minutes or hours) and long-term gains (which may span decades). They can also focus on foreign markets, where stock prices may be influenced by factors very different from those influencing a local market’s prices. All these scenarios guide analytics efforts and the time scale of data collected for analysis.</p>
<p class="calibre3">In general, analyzing stock market data involves assessing many types of trends over time. Stocks can have constant prices over time, experience gradual price growth and reduction, crash suddenly, or grow exponentially. Each of these suggests a different purchase/sale strategy for investors in the short term and the long term. <em class="italic">Figure 6</em><em class="italic">.2</em> shows a hypothetical stock that exhibits many of these patterns:</p>
<div><div><img alt="Figure 6.2 – An example of stock data trends, including stagnant periods, growth, shrinkage, and a crash" src="img/B21087_06_02.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.2 – An example of stock data trends, including stagnant periods, growth, shrinkage, and a crash</p>
<p class="calibre3">We can see in <em class="italic">Figure 6</em><em class="italic">.2</em> that<a id="_idIndexMarker273" class="pcalibre calibre6 pcalibre1"/> Stock A begins 2022 with a consistent price before entering a growth phase around July 2022. This growth phase lasts until early 2023, at which point it enters a constant pricing phase again. As an event happens in March of 2024, the price of Stock A crashes and then enters a period of price decline until the end of our tracked time period.</p>
<p class="calibre3">Often, these trends do not occur in isolation. The stocks of companies that share supply chains may exhibit similar trends. Stocks in the same industry may exhibit similar or opposite trends, depending on how companies relate to each other or news in the industry. Stocks in shared trade or defense regions may exhibit similar trends, as well, given the sociopolitical ties across countries and their markets (such as the COVID crash across most economies).</p>
<p class="calibre3"><strong class="bold">Tipping points</strong>, where<a id="_idIndexMarker274" class="pcalibre calibre6 pcalibre1"/> trends change dramatically, attract a lot of attention in financial analytics. These represent opportunities to invest before a period of accelerating growth or warnings to pull out of a market or particular investment before a crash. However, detecting these trends challenges many of the commonly used tools in market analytics.</p>
<p class="calibre3">Thankfully, newer tools, including a few rooted in network science, identify tipping points more readily than traditional methods. Many tools hinge on large-scale coupling across markets, sectors, or stocks. As more and more stocks (or markets) exhibit similar behavior, the system becomes vulnerable to outside influences that can tip it into a crash (such as supply chain issues, new legislation, or a pandemic). Simply calculating the correlations among individual stocks or sectors of a market can provide some insight, but transforming<a id="_idIndexMarker275" class="pcalibre calibre6 pcalibre1"/> correlations (within slices of time) to networks allows us to leverage many network science tools that dive deeper into the nature of their correlations and their changes over time. Specifically, centrality metrics allow us to quantify and classify relationships that exist within a network. Let’s explore a few of these centrality metrics.</p>
<h1 id="_idParaDest-83" class="calibre5"><a id="_idTextAnchor083" class="pcalibre calibre6 pcalibre1"/>Introduction to centrality metrics</h1>
<p class="calibre3">We’ve encountered some centrality metrics in <a href="B21087_03.xhtml#_idTextAnchor042" class="pcalibre calibre6 pcalibre1"><em class="italic">Chapter 3</em></a>, where we learned about bridges and hubs. Many vertex-based centrality metrics calculate properties related to hubs—the connection of a vertex to its nearest neighbors and their nearest neighbors. Many edge-based centrality metrics calculate bridging properties, where the edges near a vertex act as connectors between different hubs.</p>
<p class="calibre3">Degree is the<a id="_idIndexMarker276" class="pcalibre calibre6 pcalibre1"/> simplest <strong class="bold">vertex-based centrality metric</strong>, which we <a id="_idIndexMarker277" class="pcalibre calibre6 pcalibre1"/>encountered in <a href="B21087_05.xhtml#_idTextAnchor066" class="pcalibre calibre6 pcalibre1"><em class="italic">Chapter 5</em></a>. <strong class="bold">Degree centrality</strong> is simply the number of vertices directly connected to the<a id="_idIndexMarker278" class="pcalibre calibre6 pcalibre1"/> vertex of interest. Many Laplacian-based metrics or algorithms depend on the degree matrix within algorithm calculations. On the surface, this metric seems to capture important hub properties; a vertex with a high degree centrality will carry a lot of influence within the network (and, thus, might make a good intervention target). It also scales well to very large networks. However, one limitation of degree centrality is its lack of awareness of a vertex’s position beyond any immediate connections to neighbors; a vertex with a low degree centrality may be connected to many vertices with a high degree centrality, giving it more influence over network behavior and structure than its degree centrality suggests.</p>
<p class="calibre3"><strong class="bold">Eigenvector centrality</strong> and its <a id="_idIndexMarker279" class="pcalibre calibre6 pcalibre1"/>variants (including <strong class="bold">PageRank</strong> and <strong class="bold">Katz centrality</strong>) incorporate <a id="_idIndexMarker280" class="pcalibre calibre6 pcalibre1"/>awareness about connectivity beyond immediate<a id="_idIndexMarker281" class="pcalibre calibre6 pcalibre1"/> neighbors to give a more comprehensive vertex-based centrality metric. Thus, <strong class="bold">eigenvector centrality</strong> would score our hypothetical low-degree centrality vertex connected to high-degree centrality vertices highly, as that vertex is near very connected vertices. Technically, to find the eigenvector centrality of each vertex in a network, we can perform an eigen decomposition on the adjacency matrix. Because the adjacency matrix does not include negative values, we can assume that the first eigenvalue is the largest, and its eigenvector yields the eigenvector centrality scores for our vertices.</p>
<p class="calibre3">PageRank centrality<a id="_idIndexMarker282" class="pcalibre calibre6 pcalibre1"/> extends eigenvector centrality<a id="_idIndexMarker283" class="pcalibre calibre6 pcalibre1"/> and increases its flexibility by replacing the adjacency matrix with an adjusted adjacency matrix that is constructed by performing a random walk across vertices to determine the adjacency properties. In addition, random surfer properties, where a random walk can cross the unconnected areas of a graph with a low probability, create an adjusted adjacency matrix that is connected. This adjusted adjacency matrix is then scaled<a id="_idIndexMarker284" class="pcalibre calibre6 pcalibre1"/> before performing the eigen decomposition on the adjusted adjacency matrix, which is carried out to compute eigenvector centrality scores. PageRank centrality scores, thus, provide flexibility. In addition, this computation is typically easier and faster with the adjusted adjacency matrix, allowing the algorithm to scale well to networks of even hundreds of millions of vertices.</p>
<p class="calibre3"><strong class="bold">Edge-based centrality measures</strong> capture <a id="_idIndexMarker285" class="pcalibre calibre6 pcalibre1"/>network infrastructure that is important for spreading processes and connectivity across different hubs. <strong class="bold">Betweenness centrality</strong> is<a id="_idIndexMarker286" class="pcalibre calibre6 pcalibre1"/> one of the most common edge-based centrality metrics, capturing the relative number of shortest paths that include a given vertex among all shortest paths that exist in the network. Consider a network with 10 shortest paths, 8 of which include a particular vertex. Without this vertex, many of the shortest paths would not exist, inconveniencing the network greatly in terms of spreading processes on a social network or route efficiency on a transportation network. However, betweenness centrality does not scale well, and it should not be used on large networks without some sort of parallelization of the operation.</p>
<p class="calibre3">One recent edge-based centrality network has proven to be an efficient tool for finding stock market tipping points. <strong class="bold">Forman–Ricci curvature</strong> is <a id="_idIndexMarker287" class="pcalibre calibre6 pcalibre1"/>a geometry-based tool that considers adjacent edges in relation to an edge of interest. On an unweighted network, Forman–Ricci curvature is calculated by subtracting the degree centrality of the two vertices attached to an edge of interest from 2. The constant, 2, represents the connection between the two vertices connected by the edge of interest. The degree centrality of both vertices connected by the edge counts the number of adjacent edges to our edge of interest (minus the vertices connected by that edge, whereby we obtain our constant of 2). In <em class="italic">Figures 6–3</em>, we see three vertices: two vertices with a single edge in relation to the middle vertex (both with a degree centrality of 1) and a middle vertex connecting to both of the outer vertices (with a degree centrality of 2). Both edges, thus, have a Forman–Ricci curvature of -1, as the sum of the vertex degree<a id="_idIndexMarker288" class="pcalibre calibre6 pcalibre1"/> centralities for both is 3 (2 - 3 = -1).</p>
<div><div><img alt="Figure 6.3 – An example network demonstrating Forman–Ricci curvature, where the middle vertex is pulled from both the first and third outer vertices" src="img/B21087_06_03.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.3 – An example network demonstrating Forman–Ricci curvature, where the middle vertex is pulled from both the first and third outer vertices</p>
<p class="calibre3">To obtain vertex centrality metrics from this edge metric, we can sum the edge metrics for each vertex to score the vertices. In our example in <em class="italic">Figure 6</em><em class="italic">.3</em>, we have outer vertices with a Forman–Ricci vertex<a id="_idIndexMarker289" class="pcalibre calibre6 pcalibre1"/> centrality of -1, as both only connect to a single edge. However, the middle vertex connects to two edges, with a Forman–Ricci curvature of -1, giving it a Forman–Ricci vertex centrality of -2. Because this centrality metric relies on low-cost computations, it scales well to large networks and can be used as an alternative edge-based centrality score when betweenness centrality is not feasible.</p>
<p class="calibre3">Now, let’s explore some stock market data and see how network science can help us identify key trends over time.</p>
<h1 id="_idParaDest-84" class="calibre5"><a id="_idTextAnchor084" class="pcalibre calibre6 pcalibre1"/>Application of centrality metrics across time slices</h1>
<p class="calibre3">The NASDAQ stock<a id="_idIndexMarker290" class="pcalibre calibre6 pcalibre1"/> market is an American stock exchange in New York City that includes publicly traded companies such as Apple, Alphabet, Nvidia, and Microsoft. Kaggle provides a full history of NASDAQ stock prices under a public license (<a href="https://www.kaggle.com/datasets/jacksoncrow/stock-market-dataset?resource=download" class="pcalibre calibre6 pcalibre1">https://www.kaggle.com/datasets/jacksoncrow/stock-market-dataset?resource=download</a>) that can give us stock data for these four tech companies during the period in which they were all publicly traded up to April 1, 2020. We’ve munged the data for you to include only these four stocks in the period from August 19, 2004, to April 1, 2020. Let’s take a peek at these data to see what trends might exist (<em class="italic">Figure 6</em><em class="italic">.4</em>):</p>
<div><div><img alt="Figure 6.4 – NASDAQ selected stock closing values from August 19, 2004, to April 1, 2020" src="img/B21087_06_04.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.4 – NASDAQ selected stock closing values from August 19, 2004, to April 1, 2020</p>
<p class="calibre3">In <em class="italic">Figure 6</em><em class="italic">.4</em>, we can see that Alphabet has a consistently higher price, but all stocks exhibit the typical trends of constant pricing, dips, spikes, and upward or downward trends over this long period of trading. Of note is the 2020 trend, where stocks experienced the COVID-19 crash. Periods of volatility include 2008–2009 and 2016–2020.</p>
<p class="calibre3">One of the important <a id="_idIndexMarker291" class="pcalibre calibre6 pcalibre1"/>aspects of time series analysis is <strong class="bold">windowing</strong> the <a id="_idIndexMarker292" class="pcalibre calibre6 pcalibre1"/>time series data into overlapping pieces. While windowing can be optimized by using a grid search, windowing tends to be more of an art than a science. Choosing a window impacts the length of time in which trends can be captured. In our stock market data, window length limits the period in which we can search for trends and, thus, limits the time period after the last window in which we can forecast market behavior. A window that is too large can miss important trends that impact short-term market behavior. A window that is too small can limit forecasting to the immediate future.</p>
<p class="calibre3">We’ll choose a window of 5 trading days or roughly a week’s worth of stock data. This allows us to capture trends relevant to day trading, where stocks are traded frequently based on volatility. Our network metrics should work well for volatility-based trading for quick gains, as they capture increasing correlations across the two stocks.</p>
<p class="calibre3">Another aspect that is important to windowing time series data is the choice of overlap. For our example, we’ll choose maximal overlap (4 days’ overlap) to maximize our sensitivity to day-to-day trends. In other applications, less overlap may be desirable to investigate longer time trends. Note that our example uses a path on one of our machines. Your file path will be different from ours.</p>
<p class="calibre3">Let’s import our packages and load our data into Python with <code>Script 6.1</code> to get started:</p>
<pre class="source-code">
#import packages
import igraph as ig
from igraph import Graph
import numpy as np
import pandas as pd
import os
import matplotlib as plt
#import stock data
File="C:/users/njfar/OneDrive/Desktop/AAPL_GOOGL_Stock_2004_2020.csv"
pwd=os.getcwd()
os.chdir(os.path.dirname(File))
mydata=pd.read_csv(os.path.basename(File),encoding='latin1')</pre> <p class="calibre3">Now that we have our data<a id="_idIndexMarker293" class="pcalibre calibre6 pcalibre1"/> imported, we can add a loop that windows our time series to 5-day periods that overlap by 4 days across slices. This window strategy yields the best chance to find daily changes in trends. We’ll then create correlations among the four stocks within that time slice, threshold those correlations to limit our analysis to high correlations, create an unweighted network from those thresholds, and analyze Pagerank centrality, degree centrality, betweenness centrality, and Forman–Ricci curvature centrality across the time slices. We’ll also save each unweighted network for future retrieval, as well as the network metrics and their averages over each time slice. Let’s add these pieces to <code>Script 6.1</code>:</p>
<pre class="source-code">
#script to create time slices, derive networks,
#and compute centrality metrics
stock_networks=[]
bet_t=[]
deg_t=[]
eig_t=[]
vcurv_t=[]
bet_ave=[]
deg_ave=[]
eig_ave=[]
vcurv_ave=[]
for Date in range(5,3932):
    #wrangle data into graph
    data=mydata.iloc[(Date-5):(Date),1:5]
    cor=np.corrcoef(data.transpose())
    cor[cor&gt;=0.5]=1
    cor[cor&lt;0.5]=0
    stock_data=Graph.Adjacency(cor)
    stock_networks.append(stock_data)
    #derive some centrality metrics
    d=Graph.degree(stock_data)
    deg_t.append(d)
    deg_ave.append(np.mean(d))
    b=Graph.betweenness(stock_data)
    bet_t.append(b)
    bet_ave.append(np.mean(b))
    e=Graph.pagerank(stock_data)
    eig_t.append(e)
    eig_ave.append(np.mean(e))
    #create Forman–Ricci curvature calculations
    ecurvw=[]
    for edge in stock_data.es:
        s=edge.source
        t=edge.target
        ecurvw.append(2-d[s]-d[t])
    vcurvw=[]
    for vertex in stock_data.vs:
        inc=Graph.incident(stock_data,vertex)
        inc_curv=[]
        for i in inc:
            inc_curv.append(ecurvw[i])
        vcurvw.append(sum(inc_curv))
    vcurv_t.append(vcurvw)
    vcurv_ave.append(np.mean(vcurvw))</pre> <p class="calibre3">This script should run quickly on your machine. If you include a very large number of stocks in your analyses, you may wish to run the script in parallel to save time or exclude betweenness centrality, as betweenness centrality does not scale well as the number of vertices increases.</p>
<p class="calibre3">Now that we<a id="_idIndexMarker294" class="pcalibre calibre6 pcalibre1"/> have computed our metrics, let’s examine the correlations between the metrics across our set of time series windows to see how the different metrics relate to each other. Given that Forman–Ricci curvature depends on degree centrality metrics, we’d expect to see a strong correlation. We can add to <code>Script 6.1</code> to obtain these correlations:</p>
<pre class="source-code">
#examine correlations among metrics across the time series
print(np.corrcoef(deg_ave,eig_ave))
print(np.corrcoef(deg_ave,bet_ave))
print(np.corrcoef(deg_ave,vcurv_ave))
print(np.corrcoef(eig_ave,bet_ave))
print(np.corrcoef(eig_ave,vcurv_ave))
print(np.corrcoef(bet_ave,vcurv_ave))</pre> <p class="calibre3">You should see a strong correlation between degree centrality and Forman–Ricci curvature centrality (-0.99 in our analysis) but fairly weak correlations among the other centrality metrics (-0.05 for degree and Pagerank centralities, 0.01 for degree and betweenness centralities, 0.04 for Pagerank and betweenness centralities, 0.04 for Pagerank and Forman–Ricci curvature centrality, and 0.05 for betweenness and Forman–Ricci curvature centrality). This suggests that degree centrality and Forman–Ricci curvature may be interchangeable in these analyses, though a more complex Forman–Ricci curvature metric may capture a bit more information that may be relevant to certain trends. It’s unclear if these correlations would hold for other datasets, though degree centrality and Forman–Ricci curvature on unweighted networks will correlate to some extent, given that the<a id="_idIndexMarker295" class="pcalibre calibre6 pcalibre1"/> Forman–Ricci curvature formula depends on degree centrality.</p>
<p class="calibre3">Let’s visualize the network metric trends over time to see if any patterns emerge or extreme values stand out that might indicate behavior changes in our stock pricing. We’ll add visualization code to <code>Script 6.1</code> to do this:</p>
<pre class="source-code">
#plot metric averages across time slices
time=range(0,3927)
plt.plot(time, deg_ave, label = "Degree Average")
plt.plot(time, eig_ave, label = "Pagerank Average")
plt.plot(time, bet_ave, label = "Betweenness Average")
plt.plot(time, vcurv_ave, label = "Forman–Ricci Curvature Average")
plt.legend()
plt.show()</pre> <p class="calibre3">This should give you a plot similar to <em class="italic">Figure 6</em><em class="italic">.5</em>:</p>
<div><div><img alt="Figure 6.5 – A plot of centrality averages across time slices for our stock market data" src="img/B21087_06_05.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.5 – A plot of centrality averages across time slices for our stock market data</p>
<p class="calibre3">Note that Pagerank centrality<a id="_idIndexMarker296" class="pcalibre calibre6 pcalibre1"/> does not show up in our plot. Pagerank and betweenness centrality fill a similar range of values, masking Pagerank centrality in our plot. However, we do see that differences in the average centrality values emerge regularly in our plot, suggesting that our centrality values may be capturing different states of the market over time.</p>
<p class="calibre3">Our Forman–Ricci curvature centrality averages suggest periods of relative stability in the market, where the values are near zero. Two prominent periods of relative stability occur at the start of our time series (roughly 2004–2006) and again in the mid-to-late 2010s. However, as we approach 2008 and 2020, the correlations among our stocks increase considerably before two major market crashes.</p>
<p class="calibre3">It’s probable that our choice of threshold value influences our results. To hone in on periods of tight coupling in terms of stock behavior, let’s raise our threshold value to <code>0.9</code> and rerun <code>Script 6.1</code>:</p>
<pre class="source-code">
#script to create time slices, derive networks,
#and compute centrality metrics
stock_networks=[]
bet_t=[]
deg_t=[]
eig_t=[]
vcurv_t=[]
bet_ave=[]
deg_ave=[]
eig_ave=[]
vcurv_ave=[]
for Date in range(5,3932):
    #wrangle data into graph
    data=mydata.iloc[(Date-5):(Date),1:5]
    cor=np.corrcoef(data.transpose())
    cor[cor&gt;=0.9]=1
    cor[cor&lt;0.9]=0
    stock_data=Graph.Adjacency(cor)
    stock_networks.append(stock_data)
    #derive some centrality metrics
    d=Graph.degree(stock_data)
    deg_t.append(d)
    deg_ave.append(np.mean(d))
    b=Graph.betweenness(stock_data)
    bet_t.append(b)
    bet_ave.append(np.mean(b))
    e=Graph.pagerank(stock_data)
    eig_t.append(e)
    eig_ave.append(np.mean(e))
    #create Forman–Ricci curvature calculations
    ecurvw=[]
    for edge in stock_data.es:
        s=edge.source
        t=edge.target
        ecurvw.append(2-d[s]-d[t])
    vcurvw=[]
    for vertex in stock_data.vs:
        inc=Graph.incident(stock_data,vertex)
        inc_curv=[]
        for i in inc:
            inc_curv.append(ecurvw[i])
        vcurvw.append(sum(inc_curv))
    vcurv_t.append(vcurvw)
    vcurv_ave.append(np.mean(vcurvw))</pre> <p class="calibre3">Now, we can rerun our correlation analysis to understand how threshold value might influence any <a id="_idIndexMarker297" class="pcalibre calibre6 pcalibre1"/>correlations among the metrics. Let’s rerun our correlation analysis in <code>Script 6.1</code>:</p>
<pre class="source-code">
#examine correlations among metrics across the time series
print(np.corrcoef(deg_ave,eig_ave))
print(np.corrcoef(deg_ave,bet_ave))
print(np.corrcoef(deg_ave,vcurv_ave))
print(np.corrcoef(eig_ave,bet_ave))
print(np.corrcoef(eig_ave,vcurv_ave))
print(np.corrcoef(bet_ave,vcurv_ave))</pre> <p class="calibre3">You should see some notable differences compared to our prior results with this new threshold. The degree and Pagerank centralities are still not correlated very much (-0.04), which is mirrored by the correlations between Pagerank centrality and betweenness centrality (0.003) and Pagerank and Forman–Ricci curvature centrality (0.04). However, degree and betweenness centrality are moderately correlated now (0.44), as are betweenness and Forman–Ricci curvature centrality (-0.39). The degree and Forman–Ricci curvature centralities are still highly correlated, though slightly less than in our prior threshold (-0.98).</p>
<p class="calibre3">Let’s plot our new<a id="_idIndexMarker298" class="pcalibre calibre6 pcalibre1"/> results to investigate any trends that may have been masked with a low threshold value in our initial analysis. We can replot this using <code>Script 6.1</code>:</p>
<pre class="source-code">
#plot metric averages across time slices
time=range(0,3927)
plt.plot(time, deg_ave, label = "Degree Average")
plt.plot(time, eig_ave, label = "Pagerank Average")
plt.plot(time, bet_ave, label = "Betweenness Average")
plt.plot(time, vcurv_ave, label = "Forman–Ricci Curvature Average")
plt.legend()
plt.show()</pre> <p class="calibre3">This should yield a plot that looks very different from <em class="italic">Figure 6</em><em class="italic">.5</em>. In <em class="italic">Figure 6</em><em class="italic">.6</em>, we can see the periods of volatility much more clearly than we could in <em class="italic">Figure 6</em><em class="italic">.5</em>:</p>
<div><div><img alt="Figure 6.6 – A plot of network centrality metrics across time slices with a 0.9 threshold" src="img/B21087_06_06.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.6 – A plot of network centrality metrics across time slices with a 0.9 threshold</p>
<p class="calibre3"><em class="italic">Figure 6</em><em class="italic">.6</em> shows many more <a id="_idIndexMarker299" class="pcalibre calibre6 pcalibre1"/>Forman–Ricci curvature values that are near 0, suggesting less volatility in the market during those time periods. We see a few periods of increasing volatility (more extreme Forman–Ricci curvature values) in the form of dips in our plot. Those periods of intense volatility and coupling precede the crashes in 2008 and 2020, as well as periods of quick rebuilding after crashes. These are periods of interest for investors, as large sums of money are either lost or gained during those periods.</p>
<p class="calibre3">In general, network metrics seem to pick up on market volatility very well, particularly when high thresholds are applied to the data. This suggests what has been stated in the recent literature: network metrics are useful tools to identify market volatility before crashes and exponential growth. In fact, these tools picked up growing market volatility long before the COVID crash of 2020, which could have saved gains in the months leading up to 2020 had investors heeded the volatility warnings and pulled out before trouble hit the market. Given the volatility and long period of steep growth, it’s likely that any number of factors would have caused a major crash. A large-scale or badly placed regional conflict, a breakdown in the supply chain, or a change in policy within the technology sector probably would have produced a major crash.</p>
<p class="calibre3">Returning to <em class="italic">Figure 6</em><em class="italic">.7</em>, we<a id="_idIndexMarker300" class="pcalibre calibre6 pcalibre1"/> observe very different trends before the 2008 crash than the 2020 crash. While 2020 was preceded by a long period of growth with recent small crashes, 2008 was preceded by relative stability and slow growth. Given the increasing volatility and accelerated growth of stock prices for these four NASDAQ stocks, these trends make the market more vulnerable to large crashes in the future, and should the trends hold across other NASDAQ sectors, then we’d expect less certainty and more opportunities for large gains and losses in the near future.</p>
<div><div><img alt="Figure 6.7 – Returning to the plot of the stock data" src="img/B21087_06_07.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.7 – Returning to the plot of the stock data</p>
<p class="calibre3">Given these trends in the market, network science tools are poised to play a critical role in stock market analytics. Relatively little work exists in terms of the application of these tools to stock market data or time series data more generally, and few centrality metrics have been studied systematically. Much of the existing research on the application of centrality metrics to understand stock market trends over time involves extensions of networks to include multi-way relationships. Let’s now turn our attention to an extension of <a id="_idIndexMarker301" class="pcalibre calibre6 pcalibre1"/>applying networks to the relationships that exist among more than two entities.</p>
<h1 id="_idParaDest-85" class="calibre5"><a id="_idTextAnchor085" class="pcalibre calibre6 pcalibre1"/>Extending network metrics for time series analytics</h1>
<p class="calibre3">Because networks<a id="_idIndexMarker302" class="pcalibre calibre6 pcalibre1"/> are topological objects and <a id="_idIndexMarker303" class="pcalibre calibre6 pcalibre1"/>because our correlation matrix can use a threshold of any value, it is possible to extend network metrics to the realm of topological data analysis. Networks capture the two-way relationships between entities. However, three-way, four-way, and even larger-way interactions can exist, as well. <strong class="bold">Simplicial complexes</strong> extend <a id="_idIndexMarker304" class="pcalibre calibre6 pcalibre1"/>the idea of networks to capture these higher-numbered interactions. Three-way interactions are represented as faces (or triangles) outlined by two-way lines. Four-way interactions are represented as tetrahedra, comprised of three-way faces that have four-way interactions. This process can continue to any value of mutual interactions, where the lower-dimensional interactions form the boundaries of the higher-dimensional interactions. A simplicial complex collects the highest-level interactions that exist among vertices into a single object. <em class="italic">Figure 6</em><em class="italic">.8</em> shows an example of a three-way interaction bounded by mutual, two-way interactions:</p>
<div><div><img alt="Figure 6.8 – A diagram showing how a three-way interaction is defined by mutual, two-way interactions" src="img/B21087_06_08.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.8 – A diagram showing how a three-way interaction is defined by mutual, two-way interactions</p>
<p class="calibre3">Just as networks can be weighted or unweighted, simplicial complexes can be weighted or unweighted across <em class="italic">n</em>-way interactions within the simplicial complex. In addition, just as networks are constructed from an adjacency matrix, so simplicial complexes are created from adjacency matrices at each level of <em class="italic">n</em>-way interactions (technically the boundary matrices). Just as we typically need to construct an adjacency matrix to create a network from raw data, we can use raw data to define the <em class="italic">n</em>-way interactions existing in that data.</p>
<p class="calibre3">First, a <a id="_idIndexMarker305" class="pcalibre calibre6 pcalibre1"/>relationship metric must be<a id="_idIndexMarker306" class="pcalibre calibre6 pcalibre1"/> defined for the raw data. In our stock dataset, we chose correlation metrics. Distance metrics are also commonly used when constructing both networks and simplicial complexes, and many, many distance metrics can be defined for continuous or discrete measurements on a dataset. Once a metric is defined, a threshold or series of thresholds are applied to the metric matrix to define the relationships within a given radius of each other (defined by the threshold). Two main options exist for constructing the simplicial complex and its relationships:</p>
<ul class="calibre10">
<li class="calibre11">Defining the <em class="italic">n</em>-way relationships through the union of points, the radii of which touch (called <a id="_idIndexMarker307" class="pcalibre calibre6 pcalibre1"/>a <strong class="bold">Vietoris–Rips complex</strong>)</li>
<li class="calibre11">Counting the connection points, which involves the mutual overlap of points within a given radius, where all connected points must lie within each other’s radius (called <a id="_idIndexMarker308" class="pcalibre calibre6 pcalibre1"/>a <strong class="bold">Čech complex</strong>)</li>
</ul>
<p class="calibre3">The <strong class="bold">filtration</strong> of <a id="_idIndexMarker309" class="pcalibre calibre6 pcalibre1"/>simplicial complexes involves varying the radius by different metric thresholds. Remember how applying different correlation thresholds produced different results and insights for our stock market dataset? Different filtration levels of simplicial complexes can produce different simplicial complexes at each filtration level with different properties that may contain important information for an analyst.</p>
<p class="calibre3">Let’s create a function that defines the Vietoris–Rips simplicial complex for two-way interactions (corresponding to a network) using <code>Script 6.2</code>:</p>
<pre class="source-code">
#define Vietoris–Rips complex
from itertools import combinations
from numpy import linalg as LA
def graph_VR(points, eps):
    points=[np.array(x) for x in points]
    vr=[(x,y) for (x,y) in combinations(points, 2)
    if LA.norm(x - y) &lt;= 2*eps]
    return np.array(vr)</pre> <p class="calibre3">Now, we can apply this to the first slice of our stock market time series data. We’ll choose thresholds of 1 and 10 as a starting point to understand which vertex pairs will be included in our simplicial complex (here, only at the two-way interaction level). Let’s add this to <code>Script 6.2</code> to calculate the Vietoris–Rips simplicial complex for the first slice of our stock market time series data:</p>
<pre class="source-code">
#apply Vietoris–Rips with multiple thresholds to a slice of our stock #dataset
data=mydata.iloc[0:5,1:5]
vr1=graph_VR(data.transpose(),1)
vr2=graph_VR(data.transpose(),10)</pre> <p class="calibre3">We can examine<a id="_idIndexMarker310" class="pcalibre calibre6 pcalibre1"/> the vertex pairs included in <a id="_idIndexMarker311" class="pcalibre calibre6 pcalibre1"/>each filtration by adding the following to <code>Script 6.2</code>:</p>
<pre class="source-code">
#print the results
print("Vietoris–Rips Complex, Threshold=1:")
print(vr1)
print("Vietoris–Rips Complex, Threshold=10:")
print(vr2)</pre> <p class="calibre3">This gives us pairs of vertices for each filtration, which should show the following:</p>
<pre class="source-code">
Vietoris–Rips Complex, Threshold=1:
[[0 1]
 [0 2]
 [1 2]
 [1 3]
 [2 3]
 [2 4]
 [3 4]]
Vietoris–Rips Complex, Threshold=10:
[[0 1]
 [0 2]
 [0 3]
 [0 4]
 [1 2]
 [1 3]
 [1 4]
 [2 3]
 [2 4]
 [3 4]]</pre> <p class="calibre3">We can visualize <a id="_idIndexMarker312" class="pcalibre calibre6 pcalibre1"/>the threshold = 1 complex <a id="_idIndexMarker313" class="pcalibre calibre6 pcalibre1"/>with two-way interactions by adding the following to <code>Script 6.2</code>:</p>
<pre class="source-code">
edges1 = [(0,1),(0,2),(1,2),(1,3),(2,3),(2,4), (3,4)]
import networkx as nx
G1 = nx.Graph()
G1.add_edges_from(edges1)
nx.draw(G1,with_labels=True)</pre> <p class="calibre3">This gives a figure similar to the one in <em class="italic">Figure 6</em><em class="italic">.9</em>, showing a set of three triangles connected by two-way interactions:</p>
<div><div><img alt="Figure 6.9 – A visualization of the threshold = 1 results for two-way interactions" src="img/B21087_06_09.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.9 – A visualization of the threshold = 1 results for two-way interactions</p>
<p class="calibre3">Similarly, we can <a id="_idIndexMarker314" class="pcalibre calibre6 pcalibre1"/>visualize our threshold = 10 <a id="_idIndexMarker315" class="pcalibre calibre6 pcalibre1"/>results by adding the following to <code>Script 6.2</code>:</p>
<pre class="source-code">
edges10 = [(0,1),(0,2),(0,3),(0,4),(1,2),(1,3),(1,4),(2,3),(2,4),(3,4)]
import networkx as nx
G10 = nx.Graph()
G10.add_edges_from(edges10)
nx.draw(G10,with_labels=True)</pre> <p class="calibre3">This should give a plot similar to <em class="italic">Figure 6</em><em class="italic">.10</em>, which shows a more complex connectivity than the threshold = 1 results:</p>
<div><div><img alt="Figure 6.10 – A visualization of the threshold = 10 results for two-way interactions" src="img/B21087_06_10.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 6.10 – A visualization of the threshold = 10 results for two-way interactions</p>
<p class="calibre3">These results<a id="_idIndexMarker316" class="pcalibre calibre6 pcalibre1"/> show that different radius <a id="_idIndexMarker317" class="pcalibre calibre6 pcalibre1"/>thresholds yield different networks, just as our correlation thresholds produced different networks. We could build a full filtration from the first appearance of an edge until all possible edges exist in our network to track changes in network structure based on distances between points. We could also include three-way and four-way interactions in our construction of the Vietoris–Rips complex to extend our analysis to the fully simplicial complexes that exist in each slice of our dataset.</p>
<p class="calibre3">While simplicial complex metrics are beyond the scope of this book, many extensions of network metrics to simplicial complexes exist (such as Forman–Ricci curvature centrality) and may merit investigation in the analysis of time series data. Currently, very little work has elucidated the use of simplicial complex metrics or methods on stock market change point detection.</p>
<p class="calibre3">If you are interested, we encourage you to extend <code>Scripts 6.1</code> and <code>6.2</code> to include an analysis of the full time series through the lens of simplicial complexes and calculate the extensions of the network metrics in terms of simplicial complexes. Our chapter reference section includes papers that experiment with these extensions.</p>
<h1 id="_idParaDest-86" class="calibre5"><a id="_idTextAnchor086" class="pcalibre calibre6 pcalibre1"/>Summary</h1>
<p class="calibre3">In this chapter, we’ve introduced some common uses of time series data and time series analytics, including stock market data. We explored several vertex- and edge-based centrality metrics that are common in network analytics. Then, we applied network metrics to a time series problem on NASDAQ stock data from 2004 to 2020 to investigate how network metrics and time series thresholding impact the ability to extract useful information from time series data, such as our stock data. Finally, we investigated extending networks to simplicial complexes and constructed a network by building two simplicial complexes using the Vietoris–Rips method and various threshold values. In <a href="B21087_07.xhtml#_idTextAnchor088" class="pcalibre calibre6 pcalibre1"><em class="italic">Chapter 7</em></a>, we'll look at sales and goods pricing across both time and geography to see how network science can solve problems in spatiotemporal data.</p>
<h1 id="_idParaDest-87" class="calibre5"><a id="_idTextAnchor087" class="pcalibre calibre6 pcalibre1"/>References</h1>
<p class="calibre3">De Floriani, L., &amp; Hui, A. (2005, July). <em class="italic">Data Structures for Simplicial Complexes: An Analysis And A Comparison</em>. In Symposium on Geometry Processing (pp. 119-128).</p>
<p class="calibre3">Durbach, I., Katshunga, D., &amp; Parker, H. (2013). <em class="italic">Community structure and centrality effects in the South African company network</em>. <em class="italic">South African Journal of Business Management</em>, 44(2), 35-43.</p>
<p class="calibre3">Estrada, E., &amp; Ross, G. J. (2018). <em class="italic">Centralities in simplicial complexes. Applications to protein interaction networks.</em> <em class="italic">Journal of theoretical biology</em>, 438, 46-60.</p>
<p class="calibre3">Johansen, A., &amp; Sornette, D. (1998). <em class="italic">Stock market crashes are outliers</em>. <em class="italic">The European Physical Journal B-Condensed Matter and Complex Systems</em>, 1, 141-143.</p>
<p class="calibre3">Rodrigues, F. A. (2019). <em class="italic">Network centrality: an introduction</em>. <em class="italic">A mathematical modeling approach from nonlinear dynamics to complex </em><em class="italic">systems</em>, 177-196.</p>
<p class="calibre3">Salnikov, V., Cassese, D., &amp; Lambiotte, R. (2018). <em class="italic">Simplicial complexes and complex systems</em>. <em class="italic">European Journal of Physics</em>, 40(1), 014001.</p>
<p class="calibre3">Samal, A., Pharasi, H. K., Ramaia, S. J., Kannan, H., Saucan, E., Jost, J., &amp; Chakraborti, A. (2021). <em class="italic">Network geometry and market instability</em>. <em class="italic">Royal Society open science</em>, 8(2), 201734.</p>
<p class="calibre3">Valente, T. W., Coronges, K., Lakon, C., &amp; Costenbader, E. (2008). <em class="italic">How correlated are network centrality measures?</em> <em class="italic">Connections</em> (Toronto, Ont.), 28(1), 16.</p>
<p class="calibre3">Xiong, J., &amp; Xiao, W. (2021). <em class="italic">Identification of key nodes in abnormal fund trading network based on improved pagerank algorithm</em>. In <em class="italic">Journal of Physics</em>: Conference Series (Vol. 1774, No. 1, p. 012001). IOP Publishing.</p>
</div>
</body></html>