- en: '*Chapter 12*: Building a Kafka Cluster'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will move beyond batch processing – running queries on
    a complete set of data – and learn about the tools used in stream processing.
    In stream processing, the data may be infinite and incomplete at the time of a
    query. One of the leading tools in handling streaming data is Apache Kafka. Kafka
    is a tool that allows you to send data in real time to topics. These topics can
    be read by consumers who process the data. This chapter will teach you how to
    build a three-node Apache Kafka cluster. You will also learn how to create and
    send messages (**produce**) and read data from topics (**consume**).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating ZooKeeper and Kafka clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the Kafka cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating ZooKeeper and Kafka clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most tutorials on running applications that can be distributed often only show
    how to run a single node and then you are left wondering how you would run this
    in production. In this section, you will build a three-node ZooKeeper and Kafka
    cluster. It will run on a single machine. However, I will split each instance
    into its own folder and each folder simulates a server. The only modification
    when running on different servers would be to change localhost to the server IP.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next chapter will go into detail on the topic of Apache Kafka, but for
    now it is enough to understand that Kafka is a tool for building real-time data
    streams. Kafka was developed at LinkedIn and is now an Apache project. You can
    find Kafka on the web at [http://kafka.apache.org](http://kafka.apache.org). The
    website is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – Apache Kafka website'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_12.1_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.1 – Apache Kafka website
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka requires another application, ZooKeeper, to manage information about
    the cluster, to handle discovery, and to elect leaders. You can install and build
    a ZooKeeper cluster on your own, but for this example, you will use the ZooKeeper
    scripts provided by Kafka. To learn more about ZooKeeper, you can find it at [http://zookeeper.apache.org](http://zookeeper.apache.org).
    The website is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – The Apache ZooKeeper website'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_12.2_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.2 – The Apache ZooKeeper website
  prefs: []
  type: TYPE_NORMAL
- en: The following section will walk you through building the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading Kafka and setting up the environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can download Apache Kafka from the website under the `wget` to download
    it from the command line. From your home directory, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding commands download the current Kafka version and extract it into
    the current directory. Because you will run three nodes, you will need to create
    three separate folders for Kafka. Use the following commands to create the directories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You will now have three Kafka folders. You will also need to specify a log
    directory for each instance of Kafka. You can create three folders using the `mkdir`
    command, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you will need a `data` folder for ZooKeeper. Create the directory, and
    then enter it using `cd`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You will run three ZooKeeper instances, so you will need to create a folder
    for each instance. You can do that using `mkdir`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Each ZooKeeper instance needs an ID. It will look for a file named `myid` with
    an integer value in it. In each folder, create the corresponding `myid` file with
    the correct value. The following commands will create the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You have completed the prerequisite tasks for configuring ZooKeeper and Kafka.
    Now you can edit the configuration files for both. The next section will walk
    you through the process.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring ZooKeeper and Kafka
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The configuration files for both ZooKeeper and Kafka are in the Kafka directory
    in the `conf` folder. Since you have three Kafka directories, I will walk through
    using `Kafka_1` and the steps will need to be applied to every other directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the `~/kafka_1/conf` directory, you will need to edit the `zookeeper.properties`
    file. You will edit the data directory and the servers, as well as adding properties.
    The configuration file is shown in the following code block, with the modifications
    in bold (for the full file, refer to the GitHub repo):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: After making the changes, you can save the file. You will now need to modify
    this file in the `kafka_2` and `kafka_3` directories. Note that the `dataDir`
    setting will end in `zookeeper_2` and `zookeeper_3`, respectively. Also, the port
    number should increment by one to `2182` and `2183`. Everything else will remain
    the same. Again, the only reason you are changing the directory and ports is so
    that you can run three servers on a single machine. On three distinct servers,
    you would leave the settings as they are, only changing localhost to the IP address
    of the server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that ZooKeeper is configured, you can configure Kafka. In the same `conf`
    directory, open the `server.properties` file. The file is shown with the edits
    in bold (for the full file, refer to the GitHub repo):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For each Kafka directory, you will modify the `server.properties` file to have
    a broker ID of `1`, `2`, and `3`. You can use any integer, but I am keeping them
    the same as the folder names. Also, you will set the listeners to `localhost:9092`,
    `localhost:9093`, and `localhost:9094`. The `log.dirs` property will be set to
    each of the `log_1`, `log_2`, and `log_3` folders. All three configurations will
    have the same value for the `zookeeper.connect` property.
  prefs: []
  type: TYPE_NORMAL
- en: You have created all the necessary directories to simulate three servers and
    have configured both ZooKeeper and Kafka. You can now move on to starting the
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Starting the ZooKeeper and Kafka clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To run the servers, you will need to open six terminals – you will not run them
    in the background.
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  prefs: []
  type: TYPE_NORMAL
- en: You could use Docker Compose to run multiple containers and launch everything
    with a single file. Containers are an excellent tool, but beyond the scope of
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first three terminals, you will launch the ZooKeeper cluster. In each
    terminal, enter the Kafka folder for each instance. Run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: When you start all of the servers, a lot of text will scroll by as the servers
    look for others and hold an election. Once they connect, the text will stop, and
    the cluster will be ready.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the Kafka cluster, enter an instance of the `kafka` directory in each
    of the three remaining terminals. You can then run the following command in each
    terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When you are finished, you will have a line in each terminal that should look
    like the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You now have two clusters of three nodes running for both ZooKeeper and Kafka.
    To test out the clusters and make sure everything is working properly, the next
    section will create a topic, a consumer and, a producer, and send some messages.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Kafka cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka comes with scripts to allow you to perform some basic functions from the
    command line. To test the cluster, you can create a topic, create a producer,
    send some messages, and then create a consumer to read them. If the consumer can
    read them, your cluster is running.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a topic, run the following command from your `kafka_1` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command runs the `kafka-topics` script with the `create` flag.
    It then specifies the ZooKeeper cluster IP addresses and the topic. If the topic
    was created, the terminal will have printed the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You can verify this by listing all the topics in the Kafka cluster using the
    same script, but with the `list` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The result should be a single line: `dataengineering`. Now that you have a
    topic, you can send and receive messages on it. The next section will show you
    how.'
  prefs: []
  type: TYPE_NORMAL
- en: Testing the cluster with messages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the next chapters, you will use Apache NiFi and Python to send and receive
    messages, but for a quick test of the cluster, you can use the scripts provided
    to do this as well. To create a producer, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command uses the `kafka-console-producer` script with the `broker-list`
    flag that passes the `kafka` cluster servers. Lastly, it takes a topic, and since
    we only have one, it is `dataengineering`. When it is ready, you will have a `>`
    prompt to type messages into.
  prefs: []
  type: TYPE_NORMAL
- en: 'To read the messages, you will need to use the `kafka-console-consumer` script.
    The command is as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The consumer passes the `zookeeper` flag with the list of servers. It also specifies
    the topic and the `from-beginning` flag. If you had already read messages, you
    could specify an `offset` flag with the index of the last message so that you
    start from your last position.
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting the producer and consumer terminals next to each other, you should
    have something like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Producer and consumer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_12.3_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.3 – Producer and consumer
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, you will notice that I typed *first message* and
    *second message* twice. When the consumer turned on, it read all the messages
    on the topic. Once it has read them all, it will await new messages. If you type
    a message in the producer, it will show up in the consumer window after a short
    lag.
  prefs: []
  type: TYPE_NORMAL
- en: You now have a fully functional Kafka cluster and are ready to move on to stream
    processing with NiFi and Python in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to create a Kafka cluster, which required the
    creation of a ZooKeeper cluster. While you ran all of the instances on a single
    machine, the steps you took will work on different servers too. Kafka allows the
    creation of real-time data streams and will require a different way of thinking
    than the batch processing you have been doing.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will explain the concepts involved in streams in depth. You
    will also learn how to process streams in both NiFi and Python.
  prefs: []
  type: TYPE_NORMAL
