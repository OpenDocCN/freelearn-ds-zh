- en: Implementation of a Deep Neural Network
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络的实现
- en: We will now use our accumulated knowledge of GPU programming to implement our
    very own deep neural network (DNN) with PyCUDA. DNNs have attracted a lot of interest
    in the last decade, as they provide a robust and elegant model for machine learning
    (ML). DNNs was also one of the first applications (outside of rendering graphics)
    that were able to show the true power of GPUs by leveraging their massive parallel
    throughput, which ultimately helped NVIDIA rise to become a major player in the
    field of artificial intelligence.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用我们积累的GPU编程知识，使用PyCUDA实现我们自己的深度神经网络（DNN）。在过去的十年中，DNN吸引了大量的关注，因为它们为机器学习（ML）提供了一个强大而优雅的模型。DNN也是第一个（除了渲染图形之外）能够通过利用其巨大的并行吞吐量来展示GPU真正力量的应用之一，这最终帮助NVIDIA崛起成为人工智能领域的主要参与者。
- en: In the course of this book, we have mostly been covering individual topics in
    a *bubble* on a chapter-by-chapter basis—here, we will build on many of the subjects
    we have learned about thus far for our very own implementation of a DNN. While
    there are several open source frameworks for GPU-based DNNs currently available
    to the general public—for example, Google's TensorFlow and Keras, Microsoft's
    CNTK, Facebook's Caffe2, and PyTorch—it is very instructive to go through an implementation
    of one from scratch, which will give us a greater insight and appreciation of
    the underlying technologies required for DNNs. We have a lot of material to cover
    here, so we'll cut right to the chase after a brief introduction to some of the
    basic concepts.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的编写过程中，我们主要是一章一章地以“气泡”的形式覆盖个别主题——在这里，我们将基于我们迄今为止所学的许多主题，为我们自己的DNN实现构建一个实现。虽然目前有多个开源框架可供公众使用，用于基于GPU的DNN，例如Google的TensorFlow和Keras、Microsoft的CNTK、Facebook的Caffe2和PyTorch，但从头开始实现一个是非常有教育意义的，这将使我们更深入地了解DNN所需的底层技术。这里有大量的材料要介绍，所以我们在简要介绍一些基本概念后，将直接进入正题。
- en: 'In this chapter, we will be looking at the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下内容：
- en: Understanding what an **artificial neuron** (**AN**) is
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解**人工神经元**（**AN**）是什么
- en: Understanding how many ANs can be combined together in a **deep neural network**
    (**DNN**)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解在一个**深度神经网络**（**DNN**）中可以组合多少个AN
- en: Implementing a DNN from scratch in CUDA and Python
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在CUDA和Python中从头实现DNN
- en: Understanding how cross-entropy loss can be used to evaluate the output of a
    neural network
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解如何使用交叉熵损失来评估神经网络的输出
- en: Implementing gradient descent to train an NN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现梯度下降以训练NN
- en: Learning how to train and test an NN on a small dataset
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何在小型数据集上训练和测试NN
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: A Linux or Windows 10 PC with a modern NVIDIA GPU (2016—onward) is required
    for this chapter, with all of the necessary GPU drivers and the CUDA Toolkit (9.0–onward)
    installed. A suitable Python 2.7 installation (such as Anaconda Python 2.7) with
    the PyCUDA module is also required.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 需要一台装有现代NVIDIA GPU（2016年及以后）的Linux或Windows 10 PC，并安装所有必要的GPU驱动程序和CUDA Toolkit（9.0及以后）。还需要一个合适的Python
    2.7安装（例如Anaconda Python 2.7），并安装PyCUDA模块。
- en: This chapter's code is also available on GitHub at [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码也可在GitHub上找到：[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)。
- en: For more information about the prerequisites for this chapter, check out the
    preface of this book. For the software and hardware requirements, check out the
    README file in [https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 关于本章先决条件的更多信息，请参阅本书的序言。有关软件和硬件要求，请参阅[https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA)中的README文件。
- en: Artificial neurons and neural networks
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经元和神经网络
- en: Let's briefly go over some of the basics of **machine learning (ML)** and **neural
    networks (NNs)**. In Machine Learning, our goal is to take a collection of data
    with a particular set of labeled classes or characteristics and use these examples
    to train our system to predict the values of future data. We call a program or
    function that predicts classes or labels of future data based on prior training
    data a **classifier**.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要回顾一下 **机器学习（ML）** 和 **神经网络（NNs）** 的基础知识。在机器学习中，我们的目标是收集一组具有特定标签类别或特征的数据，并使用这些示例来训练我们的系统以预测未来数据的值。我们称根据先前训练数据预测未来数据类别或标签的程序或函数为
    **分类器**。
- en: 'There are many types of classifiers, but here we will be focusing on NNs. The
    idea behind NNs is that they (allegedly) work in a way that is similar to the
    human brain, in that they learn and classify data using a collection of **artificial
    neurons (ANs)**, all connected together to form a particular structure. Let''s
    step back for a moment, though, and look at what an individual AN is. In mathematics,
    this is just an *affine* function from the linear space **R^n** to **R**, like
    so:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多类型的分类器，但在这里我们将专注于神经网络。神经网络背后的想法是它们（据说）以类似于人类大脑的方式工作，即它们通过使用一组 **人工神经元（AN**）
    的集合来学习和分类数据，所有这些神经元都连接在一起形成一个特定的结构。然而，让我们暂时退后一步，看看单个 AN 是什么。在数学上，这只是一个从线性空间 **R^n**
    到 **R** 的 **仿射** 函数，如下所示：
- en: '![](img/25478608-c882-49f1-85a9-9e3f6b0d472d.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/25478608-c882-49f1-85a9-9e3f6b0d472d.png)'
- en: We can see that this can be characterized as a dot product between a constant
    weight vector ***w*** and an input vector ***x***, with an additional bias constant
    *b* added to the end. (Again, the only *input* into this function here is *x*;
    the other values are constants!)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这可以被描述为一个常数权重向量 ***w*** 与输入向量 ***x*** 之间的点积，并在末尾添加一个额外的偏置常数 *b*。 (再次强调，这个函数的唯一
    *输入* 是 *x*；其他值都是常数！)
- en: 'Now, individuall*y* a single AN is fairly useless (and stupid), as their *intelligence*
    only emerges when acting in cooperation with a large number of other ANs. Our
    first step is to stack a collection of *m* similar ANs on top of each other so
    as to form what we will call a **dense layer (DL)**. This is dense because each
    neuron will process every single input value from *x –* each AN will take in an
    array or vector value from **R^n** and output a single value in **R.** Since there
    are *m* neurons, this means that we can say their output collectively is in the
    space **R^m**. We will notice that if we stack the weights for each neuron in
    our layer, so as to form an *m x n* matrix of weights, we can then just calculate
    the output of each neuron with a matrix multiplication followed by the addition
    of the appropriate biases:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，单个 **人工神经网络（AN**） 个体来说相当无用（而且愚蠢），因为它们的 *智能* 只有在与大量其他 AN 协作时才会出现。我们的第一步是将一组
    *m* 个类似的 AN 逐个堆叠起来，以形成一个我们称之为 **密集层（DL**） 的结构。这是密集的，因为每个神经元将处理来自 *x* 的每个单个输入值
    - 每个 AN 将从 **R^n** 中接收一个数组或向量值，并输出一个 **R** 中的单个值。由于有 *m* 个神经元，这意味着我们可以说它们的输出总体上是在
    **R^m** 空间中。我们会注意到，如果我们把每层的权重堆叠起来，形成一个 *m x n* 的权重矩阵，我们就可以通过矩阵乘法加上适当的偏置来计算每个神经元的输出：
- en: '![](img/03bf9c48-e6dc-4f86-9b18-9a0eac05c7cb.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/03bf9c48-e6dc-4f86-9b18-9a0eac05c7cb.png)'
- en: Now, let's suppose that we want to build an NN classifier that can classify
    *k* different classes; we can create a new additional dense layer that takes in
    the *m* values from the prior dense layer, and outputs *k* values. Supposing that
    we have the appropriate weight and bias values for each layer (which are certainly
    not trivial to find), and that we also have the appropriate **activation function**
    set up after each layer (which we will define later), this will act as a classifier
    between our *k* distinct classes, giving us the probability of *x* falling into
    each respective class based on the outputs of the final layer. Of course, we're
    getting way ahead of ourselves here, but that is, in a nutshell, how an NN works.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们想要构建一个能够对 *k* 个不同的类别进行分类的神经网络分类器；我们可以创建一个新的额外的密集层，它接收来自先前密集层的 *m* 个值，并输出
    *k* 个值。假设我们为每一层都找到了适当的权重和偏置值（这当然不是一件简单的事情），并且我们还在每一层之后设置了适当的 **激活函数**（我们将在后面定义），这将作为我们
    *k* 个不同类别之间的分类器，根据最终层的输出给出 *x* 落入每个相应类别的概率。当然，我们在这里已经走得有点远了，但简而言之，这就是神经网络的工作原理。
- en: Now, it seems like we can just keep connecting dense layers to each other into
    long chains to achieve classifications. This is what is known as a DNN. When we
    have a layer that is not directly connected to the inputs or outputs, that is
    known as a hidden layer. The strength of a DNN is that the additional layers allow
    the NN to capture abstractions and subtleties of the data that a shallow NN could
    not pick up on.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，看起来我们只需将密集层连接成长链，就可以实现分类。这就是所谓的深度神经网络（DNN）。当我们有一个不直接连接到输入或输出的层时，它被称为隐藏层。DNN
    的优势是额外的层允许神经网络捕捉到浅层神经网络无法捕捉到的数据的抽象和细微之处。
- en: Implementing a dense layer of artificial neurons
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现人工神经元的密集层
- en: 'Now, let''s implement the most important building block of an NN, the **dense
    layer**. Let''s start by declaring a CUDA kernel, like so:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现神经网络最重要的构建块，即 **密集层**。让我们首先声明一个 CUDA 内核，如下所示：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let's go over the inputs, one by one. `num_outputs`, of course, indicates the
    total number of outputs this layer has; this is exactly the number of neurons
    in the layer. `num_inputs` tells us the size of the input data. Setting a positive
    value for `relu` and `sigmoid` will indicate that we should use the corresponding
    activation function on the output of this layer, which we will define later. `w`
    and `b` are arrays containing the weights and biases of this layer, while `x`
    and `y` will act as our inputs and outputs. Oftentimes, we wish to classify more
    than one piece of data at a time. We can indicate this by setting `batch_size`
    to be the number of points we wish to predict. Finally, `w_t`, `b_t`, and `delta`
    will be used in the training process to determine the appropriate weights and
    biases for this layer by means of **gradient descent**. (We will see more on gradient
    descent in a later section.)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个检查输入。`num_outputs` 当然表示这一层总共有多少个输出；这正是这一层中的神经元数量。`num_inputs` 告诉我们输入数据的大小。为
    `relu` 和 `sigmoid` 设置正值将指示我们应该在层的输出上使用相应的激活函数，这我们将在稍后定义。`w` 和 `b` 是包含这一层权重和偏差的数组，而
    `x` 和 `y` 将作为我们的输入和输出。通常，我们希望一次对多个数据进行分类。我们可以通过将 `batch_size` 设置为我们希望预测的点数来表示这一点。最后，`w_t`、`b_t`
    和 `delta` 将在训练过程中使用，通过 **梯度下降** 确定这一层的适当权重和偏差。（我们将在稍后的部分看到更多关于梯度下降的内容。）
- en: 'Now, let''s start writing our kernel. We will parallelize the computations
    over each output, so we will set an integer `i` to be the global thread ID to
    this end, and have any unnecessary extra threads which happen to be running this
    kernel to just not do anything with the appropriate `if` statement:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始编写我们的内核。我们将并行化每个输出的计算，为此，我们将设置一个整数 `i` 作为全局线程 ID，并且对于任何运行此内核的额外不必要的线程，我们将使用适当的
    `if` 语句使其不执行任何操作：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let''s iterate over each data point in the batch with the appropriate
    `for` loop:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用适当的 `for` 循环遍历批处理中的每个数据点：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will multiply and accumulate the 32-bit floats from the weights and inputs
    into a 64-bit double `temp` and then add the appropriate bias point. We will then
    typecast this back to a 32-bit float and put the value in the output array, and
    then close off the loop over `k`:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将权重和输入的 32 位浮点数相乘并累加到一个 64 位的双精度 `temp` 变量中，然后添加适当的偏差点。然后我们将这个变量转换回 32 位浮点，并将值放入输出数组中，然后关闭对
    `k` 的循环：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Multiply and accumulate* types of operations are generally subject to a great
    loss of numerical precision. This can be mitigated by using a temporary variable
    of higher precision to store values in the course of the operation, and then typecasting
    this variable back to the original precision after the operation is completed.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*乘法和累加* 类型的操作通常会导致很大的数值精度损失。这可以通过在操作过程中使用更高精度的临时变量来存储值，并在操作完成后将该变量类型转换回原始精度来缓解。'
- en: 'To train an NN, we will ultimately have to calculate the derivative (from calculus)
    of our NN with respect to each weight and bias within each individual layer, which
    is with respect to a particular batch of inputs. Remember that the derivative
    of a mathematical function *f* at the value *x* can be estimated as *f**(x + δ)
    - f(x) / δ*, where delta (δ) is some sufficiently small positive value. We will
    use the input values `w_t` and `b_t` to indicate to the kernel whether we want
    to calculate the derivative with respect to a particular weight or bias; otherwise,
    we will set these input values to a negative value to evaluate only for this layer.
    We will also set delta to be an appropriately small value for the calculation
    of the derivative, and use this to increment the value of the appropriate bias
    or weight:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个神经网络（NN），我们最终必须计算每个单独层中每个权重和偏置相对于我们的神经网络（NN）的导数（来自微积分）。记住，数学函数 *f* 在值 *x*
    处的导数可以估计为 *f**(x + δ) - f(x) / δ*，其中 delta (δ) 是某个足够小的正数。我们将使用输入值 `w_t` 和 `b_t`
    来指示内核我们是否想要计算相对于特定权重或偏置的导数；否则，我们将这些输入值设置为负值，仅为此层进行评估。我们还将设置 delta 为计算导数时适当小的值，并使用它来增加适当的偏置或权重的值：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, we will add some code for what is known as the **rectified linear unit**
    (or **ReLU**) and **sigmoid activation functions**. These are used for processing
    the immediate output of a dense neural layer. ReLU just sets all negative values
    to 0, while acting as an identity for positive inputs, while sigmoid just computes
    the value of the `sigmoid` function on each value ( *1 / (1 + e^(-x))* ). ReLU
    (or any other activation function) is used between hidden layers in an NN as a
    means to make the entire NN act as a nonlinear function; otherwise, the entire
    NN would constitute a trivial (and inefficiently computed) matrix operation. (While
    there are many other nonlinear activation functions that can be used between layers,
    ReLU has been found to be a particularly effective function for training.) Sigmoid
    is used as a final layer in an NN intended for **labeling**, that is, one that
    may assign multiple labels for a given input, as opposed to assigning an input
    to a single class.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将添加一些代码，用于所谓的**修正线性单元**（或**ReLU**）和**sigmoid 激活函数**。这些用于处理密集神经网络层的即时输出。ReLU
    只将所有负值设置为 0，而对于正输入则充当恒等函数，而 sigmoid 则计算每个值的 `sigmoid` 函数值（ *1 / (1 + e^(-x))*）。ReLU（或任何其他激活函数）在神经网络中的隐藏层之间用作使整个神经网络作为非线性函数的手段；否则，整个神经网络将构成一个平凡的（且计算效率低下的）矩阵运算。（虽然还有许多其他非线性激活函数可以在层之间使用，但ReLU被发现是训练中特别有效的函数。）Sigmoid
    被用作神经网络中用于**标记**的最终层，即可能为给定输入分配多个标签的层，而不是将输入分配给单个类别。
- en: 'Let''s go up a little bit in the file, before we even begin to define this
    CUDA kernel, and define these operations as C macros. We will also remember to
    put in the CUDA-C code we''ve just written while we are at it:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们甚至开始定义这个 CUDA 内核之前，让我们先在文件中稍微向上移动一点，并将这些操作定义为 C 宏。同时，我们也将记得在此时添加我们刚刚编写的 CUDA-C
    代码：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we will use the kernel inputs `relu` and `sigmoid` to indicate whether
    we should use these additional layers; we will take a positive input from these
    to indicate that they should be used, respectively. We can add this, close off
    our kernel, and compile it into a usable Python function:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用内核输入 `relu` 和 `sigmoid` 来指示我们是否应该使用这些额外的层；我们将从这些输入中获取正值来分别指示它们应该被使用。我们可以添加这个，关闭我们的内核，并将其编译成一个可用的
    Python 函数：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, let''s go to the beginning of the file and set up the appropriate import
    statements. Notice that we will include the `csv` module, which will be used for
    processing data inputs for testing and training:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到文件的开始部分，设置适当的导入语句。注意，我们将包含 `csv` 模块，该模块将用于处理测试和训练的数据输入：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, let''s continue setting up our dense layer; we will want to wrap this
    within a Python class for ease of use, which will make our lives much easier when
    we start connecting these dense layers together into a full-blown NN. We''ll call
    `class DenseLayer` and start by writing a constructor. Most of the inputs and
    setup here should be self-explanatory: we should definitely add an option to load
    weights and biases from a pre-trained network, and we''ll also include the option
    to specify a default *delta* value as well as a default stream. (If no weights
    or biases are given, weights are initialized to random values, while all biases
    are set to 0.) We will also specify whether to use ReLU or sigmoid layers here,
    as well. Toward the end, notice how we set up the block and grid sizes:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续设置我们的密集层；我们希望将其包装在一个Python类中，以便于使用，这将使我们在开始将这些密集层连接成一个完整的神经网络时生活得更加轻松。我们将称之为`DenseLayer`，并从编写构造函数开始。这里的大部分输入和设置应该是自解释的：我们绝对应该添加一个选项来从预训练网络中加载权重和偏差，我们还将包括指定默认*delta*值以及默认流的选项。（如果没有提供权重或偏差，权重将初始化为随机值，而所有偏差都设置为0。）我们还将指定是否在这里使用ReLU或sigmoid层。在最后，注意我们是如何设置块和网格大小的：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we will set up a function in this class to evaluate inputs from this layer;
    we will meticulously check the input (x) to determine if it is already on the
    GPU (transferring it over to a `gpuarray` if not), and we will let the user specify
    a preallocated `gpuarray` for output (y), manually allocating an output array
    if one is not specified. We will also check the delta and `w_t`/`b_t` values for
    the case of training, as well as `batch_size`. We will then run the kernel on
    the `x` input with outputs going into `y`, and finally return `y` as the output
    value:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将在本类中设置一个函数来评估来自这一层的输入；我们将仔细检查输入（x），以确定它是否已经在GPU上（如果没有，将其传输到`gpuarray`），并且我们将允许用户指定一个预分配的`gpuarray`作为输出（y），如果没有指定，将手动分配一个输出数组。我们还将检查训练情况下的delta和`w_t`/`b_t`值，以及`batch_size`。然后，我们将对`x`输入运行内核，输出进入`y`，最后返回`y`作为输出值：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: There we go. We have fully implemented a dense layer!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。我们已经完全实现了一个密集层！
- en: Implementation of the softmax layer
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: softmax层的实现
- en: We will now look at how we can implement a **softmax layer**. As we have already
    discussed, a sigmoid layer is used for assigning labels to a class—that is, if
    you want to have multiple nonexclusive characteristics that you want to infer
    from an input, you should use a sigmoid layer. A **softmax layer** is used when
    you only want to assign a single class to a sample by inference—this is done by
    computing a probability for each possible class (with probabilities over all classes,
    of course, summing to 100%). We can then select the class with the highest probability
    to give the final classification.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将探讨如何实现一个**softmax层**。正如我们之前讨论的，sigmoid层用于为类别分配标签——也就是说，如果你想要从输入中推断出多个非互斥的特征，你应该使用sigmoid层。**softmax层**用于当你只想通过推断为样本分配单个类别时——这是通过计算每个可能类别的概率（当然，所有类别的概率之和为100%）来完成的。然后，我们可以选择概率最高的类别来给出最终的分类。
- en: 'Now, let''s see exactly what the softmax layer does—given a set of a collection
    of *N* real numbers (*c[0], ..., c[N-1]*) , we first compute the sum of the exponential
    function on each number (![](img/90e15296-aedc-4f2b-a9bf-8099d5a28a07.png)), and
    then calculate the exponential of each number divided by this sum to yield the
    softmax:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看softmax层到底做了什么——给定一个包含*N*个实数的集合（*c[0]，...，c[N-1]*），我们首先计算每个数的指数函数之和（![](img/90e15296-aedc-4f2b-a9bf-8099d5a28a07.png)），然后计算每个数除以这个和的指数，以得到softmax：
- en: '![](img/ce20fc54-36ee-46a9-86a9-40387fb73545.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ce20fc54-36ee-46a9-40387fb73545.png)'
- en: 'Let''s start with our implementation. We will start by writing two very short
    CUDA kernels: one that takes the exponential of each input, and another that takes
    the mean over all of the points:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们的实现开始。我们将先编写两个非常短的CUDA内核：一个用于计算每个输入的指数，另一个用于计算所有点的平均值：
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, let''s write a Python wrapper class, like we did previously. First, we
    will start with the constructor, and we will indicate the number of both inputs
    and outputs with `num`. We can also specify a default stream, if we wish:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编写一个Python包装类，就像我们之前做的那样。首先，我们将从构造函数开始，并使用`num`来指示输入和输出的数量。我们还可以指定一个默认流，如果需要的话：
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, let''s write `eval_ function` in a way that is similar to the dense layer:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们以与密集层类似的方式编写`eval_函数`：
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Implementation of Cross-Entropy loss
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉熵损失的实现
- en: 'Now, let''s implement what is known as the **cross-entropy loss** function.
    This is used to measure how accurate an NN is on a small subset of data points
    during the training process; the bigger the value that is output by our loss function,
    the more inaccurate our NN is at properly classifying the given data. We do this
    by calculating a standard mean log-entropy difference between the expected output
    and the actual output of the NN. For numerical stability, we will limit the value
    of the output to `1`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现所谓的 **交叉熵损失** 函数。这个函数用于在训练过程中测量神经网络在数据点的子集上的准确性；我们的损失函数输出的值越大，我们的神经网络在正确分类给定数据时的准确性就越低。我们通过计算预期输出和神经网络实际输出之间的标准平均对数熵差异来实现这一点。为了数值稳定性，我们将输出值限制为
    `1`：
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Implementation of a sequential network
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 顺序网络的实现
- en: Now, let's implement one final class that will combine multiple dense layer
    and softmax layer objects into a single coherent feed-forward sequential neural
    network. This will be implemented as another class, which will subsume the other
    classes. Let's first start by writing the constructor—we will be able to set the
    max batch size here, which will affect how much memory is allocated for the use
    of this network – we'll store some allocated memory used for weights and input/output
    for each layer in the list variable, `network_mem`. We will also store the `DenseLayer`
    and `SoftmaxLayer` objects in the list network, and information about each layer
    in the NN in `network_summary`. Notice how we can also set up some training parameters
    here, including the delta, how many streams to use for gradient descent (we'll
    see this later), as well as the number of training epochs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个最终类，该类将多个密集层和 softmax 层对象组合成一个单一的、连贯的前馈顺序神经网络。这将被实现为另一个类，它将包含其他类。让我们首先从编写构造函数开始——我们可以在这里设置最大批量大小，这将影响为使用此网络分配的内存量——我们将在列表变量
    `network_mem` 中存储为每个层的权重和输入/输出分配的内存。我们还将存储 `DenseLayer` 和 `SoftmaxLayer` 对象在列表
    `network` 中，以及关于 NN 中每个层的信息在 `network_summary` 中。注意我们还可以在这里设置一些训练参数，包括 delta、用于梯度下降的流数量（我们稍后将看到），以及训练的轮数。
- en: 'We can also see one other input at the beginning called layers. Here, we can
    indicate the construction of the NN by describing each layer, which the constructor
    will create by iterating through each element of layers and calling the `add_layer`
    method, which we will implement next:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在开始时看到一个名为 layers 的其他输入。在这里，我们可以通过描述每个层来指示神经网络的结构，构造函数将通过遍历 layers 的每个元素并调用我们即将实现的
    `add_layer` 方法来创建这些层：
- en: '[PRE14]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, let''s implement the `add_layer` method. We will use a dictionary data
    type to pass all of the relevant information about the layer to the sequential
    network—including the type of layer (dense, softmax, and so on), the number of
    inputs/outputs, weights, and biases. This will append the appropriate object and
    information to the object''s network and `network_summary` list variables, as
    well as appropriately allocate `gpuarray` objects to the `network_mem` list:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现 `add_layer` 方法。我们将使用字典数据类型来传递有关层的所有相关信息到序列网络中——包括层的类型（密集型、softmax等）、输入/输出的数量、权重和偏差。这将向对象的网络和
    `network_summary` 列表变量追加适当的对象和信息，以及适当地分配 `gpuarray` 对象到 `network_mem` 列表：
- en: '[PRE15]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Implementation of inference methods
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理方法的实现
- en: We will now add two methods for inference to our `SequentialNetwork` class—that
    is, for predicting an output given for a particular input. The first method we
    will just call `predict`, which will be used by the end user. In the course of
    the training process, we will have to make predictions based on a partial result
    from only some of the layers, and we will make another method to this end called
    `partial_predict`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将为 `SequentialNetwork` 类添加两个推理方法——即，对于特定的输入预测输出。我们将首先调用 `predict` 方法，这将由最终用户使用。在训练过程中，我们必须基于仅从一些层中得到的部分结果进行预测，我们将为此目的创建另一个方法，称为
    `partial_predict`。
- en: 'Let''s start by implementing *predict*. This will take two inputs—a collection
    of samples in the form of a one- or two-dimensional NumPy array, and possibly
    a user-defined CUDA stream. We will start by doing some type-checks and formatting
    on the samples (here, called `x`), remembering that the samples will be stored
    row-wise:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从实现 *predict* 开始。这个方法将接受两个输入——一个形式为一维或二维 NumPy 数组的样本集合，以及可能是一个用户定义的 CUDA
    流。我们首先将对样本（这里称为 `x`）进行一些类型检查和格式化，记住样本将按行存储：
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, let''s perform the actual inference step. We just have to iterate through
    our entire neural network, performing an `eval_` on each layer:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们执行实际的推理步骤。我们只需迭代整个神经网络，对每一层执行`eval_`：
- en: '[PRE17]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We will now pull the final output of the NN, the GPU, and return it to the
    user. If the number of samples in `x` is actually smaller than the maximum batch
    size, we will slice the output array appropriately before it is returned:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将从神经网络、GPU的最终输出中提取，并将其返回给用户。如果`x`中的样本数量实际上小于最大批量大小，我们将在返回之前适当地切片输出数组：
- en: '[PRE18]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, with that done, let''s implement `partial_predict`. Let''s briefly discuss
    the idea behind this. When we are in the training process, we will evaluate a
    collection of samples, and then look at how a subtle change of adding *delta*
    to each weight and bias individually will affect the outputs. To save time, we
    can calculate the outputs of each layer and store them for a given collection
    of samples, and then only recompute the output for the layer where we change the
    weight, as well as for all subsequent layers. We''ll see the idea behind this
    in a little more depth soon, but for now, we can implement this like so:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，完成了这些，让我们实现`partial_predict`。让我们简要地讨论一下这个想法背后的原理。当我们处于训练过程中，我们将评估一系列样本，然后观察对每个权重和偏置分别添加*delta*的细微变化将如何影响输出。为了节省时间，我们可以计算每层的输出并将它们存储在给定的样本集合中，然后只需重新计算权重改变的那一层以及所有后续层的输出。我们很快就会更深入地了解这个想法，但现在，我们可以这样实现：
- en: '[PRE19]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Gradient descent
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: We will now make a full implementation of the training method for our NN in
    the form of **batch-stochastic gradient descent (BSGD)**. Let's think about what
    this means, word by word. **Batch** means that this training algorithm will operate
    on a collection of training samples at once, rather than all of the samples simultaneously,
    while **stochastic** indicates that each batch is chosen randomly. **Gradient**
    means that we will be using a gradient from calculus—which, here, is the collection
    of derivatives for each weight and bias on the loss function. Finally, **descent**
    means that we are trying to reduce the loss function—we do this by iteratively
    making subtle changes on the weights and biases by *subtracting* the Gradient.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将以**批量随机梯度下降（BSGD）**的形式实现我们神经网络的完整训练方法。让我们逐字思考这意味着什么。**批量**意味着这个训练算法将一次处理一组训练样本，而不是同时处理所有样本，而**随机**表示每个批量是随机选择的。**梯度**意味着我们将使用微积分中的梯度——在这里，是损失函数上每个权重和偏置的导数集合。最后，**下降**意味着我们正在尝试减少损失函数——我们通过迭代地对权重和偏置进行细微的*减法*更改来实现这一点。
- en: Remember from calculus that the gradient of a point always points in the direction
    of the greatest *increase*, with its opposite direction being that of the greatest
    *decrease*. Since we want a *decrease*, we subtract the gradient.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从微积分中记住，一个点的梯度始终指向最大*增加*的方向，其相反方向是最大*减少*的方向。由于我们想要一个*减少*，所以我们减去梯度。
- en: 'We will now implement BSGD as the `bsgd` method in our `SequentialNetwork`
    class. Let''s go over the input parameters of `bsgd`, one by one:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将实现BSGD作为我们`SequentialNetwork`类中的`bsgd`方法。让我们逐一过一下`bsgd`的输入参数：
- en: '`training` will be a two-dimensional NumPy array of training samples'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training`将是一个二维NumPy数组，包含训练样本'
- en: '`labels` will be the desired output of the final layer of the NN corresponding
    to each training sample'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`将是每个训练样本对应的神经网络最终层的期望输出'
- en: '`delta` will indicate how much we should increase a weight for the calculation
    of derivatives by'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delta`将指示我们在计算导数时应该增加权重多少'
- en: '`max_streams` will indicate the maximum number of concurrent CUDA streams that
    BSGD will perform calculations over'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_streams`将指示BSGD将执行计算的并发CUDA流的最大数量'
- en: '`batch_size` will indicate how large we want the batches that we will calculate
    the loss function on for each update of the weights'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`将指示我们希望每个权重更新的损失函数计算批量大小的程度'
- en: '`epochs` will indicate how many times we shuffle the order of the current set
    of samples, break into a collection of batches, and then perform BSGD on'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs`将指示我们多少次洗牌当前样本集的顺序，将其拆分为一批，然后执行BSGD'
- en: '`training_rate` will indicate the rate at which we will update our weights
    and biases with our gradient calculations'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training_rate`将指示我们使用梯度计算更新权重和偏置的速率'
- en: 'We''ll start out this method as usual and perform some checks and typecasting,
    set up the collection of CUDA stream objects into a Python list, and allocate
    some additional needed GPU memory in another list:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将像往常一样开始这个方法，执行一些检查和类型转换，将CUDA流对象集合设置为一个Python列表，并在另一个列表中分配一些额外的GPU内存：
- en: '[PRE20]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we can begin training. We will start by doing an iteration of the entire
    BSGD for each `epoch`, performing a random shuffle of the entire dataset for each
    epoch. We''ll print some information to the terminal as well so that the user
    will have some status updates in the training process:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始训练了。我们将从对每个`epoch`执行整个BSGD迭代开始，每个epoch对整个数据集进行随机打乱。我们还会在终端打印一些信息，以便用户在训练过程中有一些状态更新：
- en: '[PRE21]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we will make a loop that iterates over each batch in the shuffled dataset.
    We start by calculating the entropy from the current batch, and we will print
    this as well. If the user sees decreases in entropy, then they will know that
    gradient descent is working here:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个循环，遍历打乱的数据集中的每个批次。我们首先从当前批次计算熵，并且也会打印出来。如果用户看到熵的下降，那么他们将知道梯度下降在这里正在起作用：
- en: '[PRE22]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We will now iterate through each dense layer of our NN, calculating the gradient
    for the entire set of weights and biases. We will store these derivatives for
    the weights and biases in *flattened* (one-dimensional) arrays, which will correspond
    to the `w_t` and `b_t` indices in our CUDA kernels, which are also flattened.
    Since we will have multiple streams process different outputs for different weights,
    we will use a Python Queue container to store the set of weights and biases that
    are yet to be processed for this batch: we can then just pop values off the top
    of this container to the next available stream (we''ll store these as tuples,
    with the first element indicating whether this is a weight or bias, in particular):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将遍历我们神经网络中的每个密集层，计算整个权重和偏置集的梯度。我们将这些权重和偏置的导数存储在*展平*（一维）数组中，这些数组将对应于我们的CUDA内核中的`w_t`和`b_t`索引，这些索引也是展平的。由于我们将有多个流处理不同权重的不同输出，我们将使用Python队列容器来存储尚未处理此批次的权重和偏置集：然后我们只需从这个容器顶部弹出值到下一个可用的流（我们将这些存储为元组，第一个元素表示这是一个权重还是偏置，特别是）：
- en: '[PRE23]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we need to iterate over each and every weight and bias, which we can do
    with a `while` loop that checks if the `queue` object we just set up is empty.
    We will set up another queue, `stream_weights`, that will help us organize which
    weights and biases each stream has processed. After setting up the weight and
    bias inputs appropriately, we can now use `partial_predict` by using the current
    stream and corresponding GPU memory arrays:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要遍历每个权重和偏置，我们可以使用一个`while`循环来检查我们刚刚设置的`queue`对象是否为空。我们将设置另一个队列`stream_weights`，这将帮助我们组织每个流处理了哪些权重和偏置。在适当地设置权重和偏置输入后，我们现在可以使用`partial_predict`，通过使用当前流和相应的GPU内存数组：
- en: Notice that we already performed a `predict` for this batch of samples to calculate
    the entropy, so we are now able to perform `partial_predict` on this batch, provided
    we are careful about which memory and layers we use.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们已经在计算熵的批次样本上执行了`predict`，所以我们现在能够在这个批次上执行`partial_predict`，前提是我们小心地使用哪些内存和层。
- en: '[PRE24]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We have only computed the prediction of the output for alterations of a small
    set of weights and biases. We will have to compute the entropy for each, and then
    store the value of the derivative in the flattened arrays:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只计算了小部分权重和偏置的输出预测。我们必须为每个计算熵，然后将导数值存储在展平的数组中：
- en: '[PRE25]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We have now finished the `while` loop. Once we reach the outside of this, we
    will know that we''ve calculated the derivatives for all weights and biases for
    this particular layer. Before we iterate to the next layer, we will append the
    calculated values for the gradient of the current set of weights and biases into
    the `all_grad` list. We will also reshape the flattened list of weights back into
    the original shape while we''re at it:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成了`while`循环。一旦我们到达这个循环的外面，我们就知道我们已经计算了该特定层的所有权重和偏置的导数。在我们迭代到下一层之前，我们将当前权重和偏置集的梯度计算值追加到`all_grad`列表中。同时，我们也将展平的权重列表重塑回原始形状：
- en: '[PRE26]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After we are done iterating over every layer, we can perform the optimization
    of the weights and biases of our NN on this batch. Notice how if the `training_rate`
    variable is far less than `1`, this will reduce how fast the weights are updated:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们迭代完每一层之后，我们可以在这一批数据上对神经网络（NN）的权重和偏置进行优化。注意，如果`training_rate`变量远小于`1`，这将减慢权重更新的速度：
- en: '[PRE27]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We have fully implemented a (very simple) GPU-based DNN!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完全实现了一个（非常简单）基于GPU的DNN！
- en: Conditioning and normalizing data
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据的预处理和标准化
- en: 'Before we move on to training and testing our brand-new NN, we need to step
    back for a moment and talk about **conditioning** and **normalizing** data. NNs
    are highly susceptible to numerical error, especially when inputs have a large
    variance in scale. This can be mitigated by properly **conditioning** our training
    data; this means that for each point in an input sample, we will calculate the
    mean and variance of each point over all samples, and then subtract the mean and
    divide by the standard deviation for each point in each sample before it is input
    into the NN for either training or inference (prediction). This method is known
    as n**ormalization**. Let''s put together a small Python function that can do
    this for us:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续进行训练和测试我们全新的神经网络之前，我们需要暂时退后一步，谈谈**数据预处理**和**数据标准化**。神经网络对数值误差非常敏感，尤其是当输入具有较大尺度差异时。这可以通过正确**预处理**我们的训练数据来缓解；这意味着对于输入样本中的每个点，我们将计算所有样本中每个点的平均值和方差，然后在输入神经网络进行训练或推理（预测）之前，对每个样本中的每个点减去平均值并除以标准差。这种方法被称为**标准化**。让我们编写一个小的Python函数来完成这项工作：
- en: '[PRE28]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The Iris dataset
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 爱丽丝数据集
- en: 'We will now construct our very own DNN for a real-life problem: classification
    of flower types based on the measurements of petals. We will be working with the
    well-known *Iris dataset* for this. This dataset is stored as a comma-separated
    value (CSV) text file, with each line containing four different numerical values
    (petal measurements), followed by the flower type (here, there are three classes—*Irissetosa*,
    *Irisversicolor*, and *Irisvirginica*). We will now design a small DNN that will
    classify the type of iris, based on this set.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将构建我们自己的DNN来解决一个实际问题：根据花瓣的测量值对花卉类型进行分类。我们将使用著名的*爱丽丝数据集*来完成这项工作。这个数据集存储为逗号分隔值（CSV）文本文件，每行包含四个不同的数值（花瓣测量值），后面跟着花卉类型（在这里，有三个类别—*Irissetosa*，*Irisversicolor*和*Irisvirginica*）。现在，我们将设计一个小型DNN来根据这个集合对鸢尾花类型进行分类。
- en: 'Before we continue, please download the Iris dataset and put it into your working
    directory. This is available from the UC Irvine Machine Learning repository, which
    can be found here: [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，请下载爱丽丝数据集并将其放入您的当前工作目录。这个数据集可以从加州大学欧文分校机器学习存储库获得，网址如下：[https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data)。
- en: 'We will start by processing this file into appropriate data arrays that we
    can use for training and validating our DNN. Let''s start by opening up our main
    function; we will need to translate the names of the flowers into actual classes
    that a DNN can output, so let''s make a small dictionary that will give us a corresponding
    label for each class. We will also set up some empty lists to store our training
    data and labels:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将处理这个文件，将其转换为适当的数据数组，以便我们用于训练和验证我们的深度神经网络（DNN）。让我们从打开我们的主函数开始；我们需要将花卉的名称转换为DNN可以输出的实际类别，因此让我们创建一个小的字典，它将为每个类别提供相应的标签。我们还将设置一些空列表来存储我们的训练数据和标签：
- en: '[PRE29]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, let''s read from the CSV file. We will use the `reader` function from
    Python''s `csv` module, which we imported earlier:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从CSV文件中读取数据。我们将使用Python的`csv`模块中的`reader`函数，这是我们之前导入的：
- en: '[PRE30]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We will now randomly shuffle the data and use two-third of these samples as
    training data. The remaining one-third will be used for test (validation) data:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将随机打乱数据，并使用其中的三分之二作为训练数据。剩余的三分之一将用于测试（验证）数据：
- en: '[PRE31]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, finally, we can begin building our DNN! First, let''s create a `SequentialNetwork`
    object. We''ll set the `max_batch_size` to `32`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，最后，我们可以开始构建我们的DNN了！首先，让我们创建一个`SequentialNetwork`对象。我们将`max_batch_size`设置为`32`：
- en: '[PRE32]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, let''s create our NN. This will consist of four dense layers (two hidden)
    and a softmax layer. We will increment the number of neurons in each layer until
    the final layer, which will only have three outputs (one for each class). This
    increasing amount of neurons per layer allows us to capture some of the subtleties
    of the data:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建我们的神经网络。这将由四个密集层（两个隐藏层）和一个 softmax 层组成。我们将逐步增加每层的神经元数量，直到最后一层，该层将只有三个输出（每个类别一个）。这种每层神经元数量的增加使我们能够捕捉到数据的一些细微之处：
- en: '[PRE33]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We will now condition our training data and begin the training with our BSGD
    method that we just implemented. We will train with `batch_size` set to `16`,
    `max_streams` set to `10`, the number of `epochs` set to 100, the `delta` set
    to 0.0001, and the `training_rate` set to 1—these will be admissible parameters
    for virtually any modern GPU. We will also time the training procedure while we''re
    at it, which can be rather time-consuming:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将对训练数据进行条件化处理，并使用我们刚刚实现的 BSGD 方法开始训练。我们将设置 `batch_size` 为 `16`，`max_streams`
    为 `10`，`epochs` 的数量设置为 100，`delta` 设置为 0.0001，`training_rate` 设置为 1——这些参数对于几乎任何现代
    GPU 都是可接受的。同时，我们也会计时训练过程，这可能相当耗时：
- en: '[PRE34]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, our DNN is fully trained. We are ready to begin the validation process!
    Let''s set up a Python variable called `hits` to count the total number of correct
    classifications. We will also need to condition the validation/testing data too.
    One more thing—we determine the class by the index corresponding to the largest
    value of the softmax layer of our DNN. We can check whether this gives us the
    correct classification by using NumPy''s `argmax` function, like so:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的 DNN 已经完全训练完毕。我们准备好开始验证过程了！让我们设置一个名为 `hits` 的 Python 变量来统计总的正确分类数量。我们还需要对验证/测试数据进行条件化处理。还有一件事——我们通过
    DNN 的 softmax 层的最大值对应的索引来确定类别。我们可以通过使用 NumPy 的 `argmax` 函数来检查这给我们的是否是正确的分类，如下所示：
- en: '[PRE35]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, we are ready to check how well our DNN actually works. Let''s output the
    accuracy as well as the total training time:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好检查我们的 DNN 实际上工作得如何了。让我们输出准确率以及总的训练时间：
- en: '[PRE36]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now, we are done. We can now fully implement a DNN with Python and CUDA! Generally
    speaking, you can expect an accuracy ranging from 80%-97% for this particular
    problem, with a training time of 10-20 minutes on any Pascal-level GPU.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经完成了。现在我们可以完全使用 Python 和 CUDA 实现一个深度神经网络（DNN）了！一般来说，对于这个问题，你可以期望得到 80%-97%
    的准确率，在任意 Pascal 级别的 GPU 上训练时间大约为 10-20 分钟。
- en: The code for this chapter is available in the `deep_neural_network.py` file,
    under the appropriate directory in this book's GitHub repository.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码存储在 `deep_neural_network.py` 文件中，位于本书 GitHub 仓库的相应目录下。
- en: Summary
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we started by giving the definition of an artificial neural
    network, and showed you how individual ANs can be combined into dense layers,
    which combine together into a full-on deep neural network. We then implemented
    a dense layer in CUDA-C and made an appropriate corresponding Python wrapper class.
    We also included functionality to add ReLU and sigmoid layers on the outputs of
    a dense layer. We saw the definition and motivation of using a softmax layer,
    which is used for classification problems, and then implemented this in CUDA-C
    and Python. Finally, we implemented a Python class so that we could build a sequential
    feed-forward DNN from the prior classes; we implemented a cross-entropy loss function,
    and then used this in our loss function in our implementation of gradient descent
    to train the weights and biases in our DNN. Finally, we used our implementation
    to construct, train, and test a DNN on a real-life dataset.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先给出了人工神经网络的定义，并展示了如何将单个人工神经网络（AN）组合成密集层，这些密集层最终组合成一个完整的深度神经网络。然后，我们在
    CUDA-C 中实现了一个密集层，并创建了一个相应的 Python 包装类。我们还包含了在密集层的输出上添加 ReLU 和 sigmoid 层的功能。我们看到了使用
    softmax 层的定义和动机，该层用于分类问题，并在 CUDA-C 和 Python 中实现了它。最后，我们实现了一个 Python 类，以便我们可以从之前的类构建一个顺序前馈
    DNN；我们实现了一个交叉熵损失函数，然后在我们的 DNN 实现中使用这个损失函数来训练权重和偏差。最后，我们使用我们的实现在一个真实数据集上构建、训练和测试了一个
    DNN。
- en: We now have a great deal of self-confidence in our CUDA programming abilities,
    since we can write our own GPU-based DNN! We will now move on to some very advanced
    material in the next two chapters, where we will look at how we can write our
    own interfaces to compiled CUDA code, as well as some of the very technical ins
    and outs of NVIDIA GPUs.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们能够编写自己的基于GPU的深度神经网络（DNN），我们现在对自己的CUDA编程能力非常有信心！接下来，我们将进入下一章的非常高级的内容，我们将探讨如何编写自己的接口来调用编译好的CUDA代码，以及一些关于NVIDIA
    GPU的非常技术性的细节。
- en: Questions
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Suppose you construct a DNN and after training it, it yields only garbage. After
    inspection, you find that all of the weights and biases are either huge numbers
    or NaNs. What might the problem be?
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你构建了一个DNN，在训练后，它只产生了垃圾输出。经过检查，你发现所有的权重和偏置要么是巨大的数字，要么是NaN。可能的问题是什么？
- en: Name one possible problem with a small `training_rate` value.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出一个小`training_rate`值可能存在的问题。
- en: Name one possible problem with a large `training_rate` value.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出一个大`training_rate`值可能存在的问题。
- en: Suppose we want to train a DNN that will assign multiple labels to an image
    of an animal ("slimey", "furry", "red", "brown", and so on). Should we use a sigmoid
    or softmax layer at the end of the DNN?
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们想要训练一个DNN，该DNN将为动物图像分配多个标签（如“粘滑的”、“毛茸茸的”、“红色的”、“棕色的”等等）。我们应该在DNN的末尾使用sigmoid层还是softmax层？
- en: Suppose we want to classify an image of a single animal as either a cat or dog.
    Do we use sigmoid or softmax?
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们想要将一张单独的动物图像分类为猫或狗。我们应该使用sigmoid函数还是softmax函数？
- en: If we decrease the batch size, will there be more or less updates to the weights
    and biases during gradient descent training?
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们减小批处理大小，在梯度下降训练过程中，权重和偏置的更新会有更多还是更少？
