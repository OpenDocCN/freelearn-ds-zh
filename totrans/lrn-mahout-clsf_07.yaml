- en: Chapter 7. Learning Multilayer Perceptron Using Mahout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand a **Multilayer Perceptron** (**MLP**), we will first explore
    one more popular machine learning technique: **neural network**. In this chapter,
    we will explore the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Neural network and neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Mahout for MLP implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural network and neurons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neural network is an old algorithm, and it was developed with a goal in mind:
    to provide the computer with a brain. Neural network is inspired by the biological
    structure of the human brain where multiple neurons are connected and form columns
    and layers. A **neuron** is an electrically excitable cell that processes and
    transmits information through electrical and chemical signals. Perceptual input
    enters into the neural network through our sensory organs and is then further
    processed into higher levels. Let''s understand how neurons work in our brain.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Neurons are computational units in the brain that collect the input from input
    nerves, which are called **dendrites**. They perform computation on these input
    messages and send the output using output nerves, which are called **axons**.
    See the following figure ([http://vv.carleton.ca/~neil/neural/neuron-a.html](http://vv.carleton.ca/~neil/neural/neuron-a.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural network and neurons](img/4959OS_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'On the same lines, we develop a neural network in computers. We can represent
    a neuron in our algorithm as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural network and neurons](img/4959OS_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, **x1**, **x2**, and **x3** are the feature vectors, and they are assigned
    to a function **f**, which will do the computation and provide the output. This
    activation function is usually chosen from the family of sigmoidal functions (as
    defined in [Chapter 3](ch03.html "Chapter 3. Learning Logistic Regression / SGD
    Using Mahout"), *Learning Logistic Regression / SGD Using Mahout*). In the case
    of classification problems, softmax activation functions are used. In classification
    problems, we want the output as the probabilities of target classes. So, it is
    desirable for the output to lie between 0 and 1 and the sum close to 1\. Softmax
    function enforces these constraints. It is a generalization of the logistic function.
    More details on softmax function can be found at [http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-12.html](http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-12.html).
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer Perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A neural network or artificial neural network generally refers to an MLP network.
    We defined neuron as an implementation in computers in the previous section. An
    MLP network consists of multiple layers of these neuron units. Let''s understand
    a perceptron network of three layers, as shown in the next figure. The first layer
    of the MLP represents the input and has no other purpose than routing the input
    to every connected unit in a feed-forward fashion. The second layer is called
    hidden layers, and the last layer serves the special purpose of determining the
    output. The activation of neurons in the hidden layers can be defined as the sum
    of the weight of all the input. Neuron 1 in layer 2 is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Y12 = g(w110x0 +w111x1+w112x2+w113x3)
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part where *x0 = 0* is called the bias and can be used as an offset,
    independent of the input. Neuron 2 in layer 2 is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Y22 = g(w120x0 +w121x1+w122x2+w123x3)
  prefs: []
  type: TYPE_NORMAL
- en: '![Multilayer Perceptron](img/4959OS_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Neuron 3 in layer 2 is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Y32 = g (w130x0 +w131x1+w132x2+w133x3)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, g is a sigmoid function, as defined in [Chapter 3](ch03.html "Chapter 3. Learning
    Logistic Regression / SGD Using Mahout"), *Learning Logistic Regression / SGD
    Using Mahout*. The function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: g(z) = 1/1+e (-z)
  prefs: []
  type: TYPE_NORMAL
- en: In this MLP network output, from each input and hidden layers, neuron units
    are distributed to other nodes, and this is why this type of network is called
    a fully connected network. In this network, no values are fed back to the previous
    layer. (Feed forward is another strategy and is also known as back propagation.
    Details on this can be found at [http://home.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html](http://home.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html).)
  prefs: []
  type: TYPE_NORMAL
- en: An MLP network can have more than one hidden layer. To get the value of the
    weights so that we can get the predicted value as close as possible to the actual
    one is a training process of the MLP. To build an effective network, we consider
    a lot of items such as the number of hidden layers and neuron units in each layer,
    the cost function to minimize the error in predicted and actual values, and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s discuss two more important and problematic questions that arise
    when creating an MLP network:'
  prefs: []
  type: TYPE_NORMAL
- en: How many hidden layers should one use for the network?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many numbers of hidden units (neuron units) should one use in a hidden layer?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zero hidden layers are required to resolve linearly separable data. Assuming
    your data does require separation by a non-linear technique, always start with
    one hidden layer. Almost certainly, that''s all you will need. If your data is
    separable using an MLP, then this MLP probably only needs a single hidden layer.
    In order to select the number of units in different layers, these are the guidelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input layer**: This refers to the number of explanatory variables in the
    model plus one for the bias node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output layer**: In the case of classification, this refers to the number
    of target variables, and in the case of regression, this is obviously one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden layer**: Start your network with one hidden layer and use the number
    of neuron units equivalent to the units in the input layer. The best way is to
    train several neural networks with different numbers of hidden layers and hidden
    neurons and measure the performance of these networks using cross-validation.
    You can stick with the number that yields the best-performing network. Problems
    that require two hidden layers are rarely encountered. However, neural networks
    that have more than one hidden layer can represent functions with any kind of
    shape. There is currently no theory to justify the use of neural networks with
    more than two hidden layers. In fact, for many practical problems, there is no
    reason to use any more than one hidden layer. A network with no hidden layer is
    only capable of representing linearly separable functions. Networks with one layer
    can approximate any function that contains a continuous mapping from one finite
    space to another, and networks with two hidden layers can represent an arbitrary
    decision boundary to arbitrary accuracy with rational activation functions and
    can approximate any smooth mapping to any accuracy (Chapter 5 of the book *Introduction
    to Neural Networks for Java*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of neurons or hidden units**: Use the number of neuron units equivalent
    to the units in the input layer. The number of hidden units should be less than
    twice the number of units in the input layer. Another rule to calculate this is
    *(number of input units + number of output units)* 2/3*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do the testing for generalization errors, training errors, bias, and variance.
    When a generalization error dips, then just before it begins to increase again,
    the numbers of nodes are usually found to be perfect at this point.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's move on to the next section and explore how we can use Mahout for
    an MLP.
  prefs: []
  type: TYPE_NORMAL
- en: MLP implementation in Mahout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The MLP implementation is based on a more general neural network class. It is
    implemented to run on a single machine using Stochastic Gradient Descent, where
    the weights are updated using one data point at a time.
  prefs: []
  type: TYPE_NORMAL
- en: The number of layers and units per layer can be specified manually and determines
    the whole topology with each unit being fully connected to the previous layer.
    A bias unit is automatically added to the input of every layer. A bias unit is
    helpful for shifting the activation function to the left or right. It is like
    adding a coefficient to the linear function.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, the logistic sigmoid is used as a squashing function in every hidden
    and output layer.
  prefs: []
  type: TYPE_NORMAL
- en: The command-line version does not perform iterations that lead to bad results
    on small datasets. Another restriction is that the CLI version of the MLP only
    supports classification, since the labels have to be given explicitly when executing
    the implementation in the command line.
  prefs: []
  type: TYPE_NORMAL
- en: A learned model can be stored and updated with new training instances using
    the `` `--update` `` flag. The output of the classification result is saved as
    a `.txt` file and only consists of the assigned labels. Apart from the command-line
    interface, it is possible to construct and compile more specialized neural networks
    using the API and interfaces in the `mrlegacy` package. (The core package is renamed
    as `mrlegacy`.)
  prefs: []
  type: TYPE_NORMAL
- en: 'In the command line, we use `TrainMultilayerPerceptron` and `RunMultilayerPerceptron`
    classes that are available in the `mrlegacy` package with three other classes:
    Neural `network.java`, `NeuralNetworkFunctions.java`, and `MultilayerPerceptron.java`.
    For this particular implementation, users can freely control the topology of the
    MLP, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The size of the input layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of hidden layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of each hidden layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of the output layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cost function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The squashing function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is trained in an online learning approach, where the weights of neurons
    in the MLP is updated and incremented using the backPropagation algorithm proposed
    by *Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986), Learning representations
    by back-propagating errors. Nature, 323, 533-536*.
  prefs: []
  type: TYPE_NORMAL
- en: Using Mahout for MLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mahout has implementation for an MLP network. The MLP implementation is currently
    located in the `Map-Reduce-Legacy` package. As with other classification algorithms,
    two separated classes are implemented to train and use this classifier. For training
    the classifier, the `org.apache.mahout.classifier.mlp.TrainMultilayerPerceptron`
    class, and for running the classifier, the `org.apache.mahout.classifier.mlp.RunMultilayerPerceptron`
    class is used. There are a number of parameters defined that are used with these
    classes, but we will discuss these parameters once we run our example on a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will train an MLP to classify the iris dataset. The iris
    flower dataset contains data of three flower species, where each data point consists
    of four features. This dataset was introduced by Sir Ronald Fisher. It consists
    of 50 samples from each of three species of iris. These species are Iris setosa,
    Iris virginica, and Iris versicolor. Four features were measured from each sample:'
  prefs: []
  type: TYPE_NORMAL
- en: Sepal length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal width
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal width
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All measurements are in centimeters. You can download this dataset from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/)
    and save it as a `.csv` file, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using Mahout for MLP](img/4959OS_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This dataset will look like the the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using Mahout for MLP](img/4959OS_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Steps to use the MLP algorithm in Mahout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps to use the MLP algorithm in Mahout are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the MLP model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To create the MLP model, we will use the `TrainMultilayerPerceptron` class.
    Use the following command to generate the model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also run using the core jar: Mahout core jar (`xyz` stands for the
    version). If you have directly installed Mahout, it can be found under the `/usr/lib/mahout`
    folder. Execute the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `TrainMultilayerPerceptron` class is used here and it takes different parameters.
    Also, `i` is the path for the input dataset. Here, we have put the dataset under
    the `/tmp` folder (local filesystem). Additionally, labels are defined in the
    dataset. Here we have the following labels:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`mo` is the output location for the created model.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ls` is the number of units per layer, including input, hidden, and output
    layers. This parameter specifies the topology of the network. Here, we have `4`
    as the input feature, `8` for the hidden layer, and `3` for the output class number.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l` is the learning rate that is used for weight updates. The default is 0.5\.
    To approximate gradient descent, neural networks are trained with algorithms.
    Learning is possible either by batch or online methods. In batch training, weight
    changes are accumulated over an entire presentation of the training data (an epoch)
    before being applied, while online training updates weighs after the presentation
    of each training example (instance). More details can be found at [http://axon.cs.byu.edu/papers/Wilson.nn03.batch.pdf](http://axon.cs.byu.edu/papers/Wilson.nn03.batch.pdf).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`m` is the momentum weight that is used for gradient descent. This must be
    in the range between 0–1.0.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r` is the regularization value for the weight vector. This must be in the
    range between 0–0.1\. It is used to prevent overfitting.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Steps to use the MLP algorithm in Mahout](img/4959OS_07_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'To test/run the MLP classification of the trained model, we can use the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also run using the Mahout core jar (`xyz` stands for version). If you
    have directly installed Mahout, it can be found under the `/usr/lib/mahout` folder.
    Execute the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `RunMultilayerPerceptron` class is employed here to use the model. This
    class also takes different parameters, which are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`i` indicates the input dataset location'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cr` is the range of columns to use from the input file, starting with 0 (that
    is, `` `-cr 0 5` `` for including the first six columns only)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mo` is the location of the model built earlier'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`o` is the path to store labeled results from running the model'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Steps to use the MLP algorithm in Mahout](img/4959OS_07_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed one of the newly implemented algorithms in Mahout:
    MLP. We started our discussion by understanding neural networks and neuron units
    and continued our discussion further to understand the MLP network algorithm.
    We discussed how to choose different layer units. We then moved to Mahout and
    used the iris dataset to test and run an MLP algorithm implemented in Mahout.
    With this, we have finished our discussion on classification algorithms available
    in Apache Mahout.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we move on to the next chapter of this book where we will discuss the new
    changes coming up in the new Mahout release.
  prefs: []
  type: TYPE_NORMAL
