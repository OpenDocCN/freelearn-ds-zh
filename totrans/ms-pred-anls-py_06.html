<html><head></head><body>
  <div><div><div><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Words and Pixels – Working with Unstructured Data</h1></div></div></div><p>Most of the data we have looked at thus far is composed of rows and columns with numerical or categorical values. This sort of information fits in both traditional spreadsheet software and the interactive Python notebooks used in the previous exercises. However, data is increasingly available in both this form, usually called structured data, and more complex formats such as images and free text. These other data types, also known as unstructured data, are more challenging than tabular information to parse and transform into features that can be used in machine learning algorithms.</p><p>What makes unstructured data challenging to use? It is challenging largely because images and text are extremely high dimensional, consisting of a much larger number of columns or features than we have seen previously. For example, this means that a document may have thousands of words, or an image thousands of individual pixels. Each of these components may individually or in complex combinations comprise a feature for our algorithms. However, to use these data types in prediction, we need to somehow distill this extremely complex data into common features or trends that might be used effectively in a model. This often involves both removing noise from these data types and finding simpler representations. At the same time, the greater inherent complexity of these data types potentially captures more information than available in tabular datasets, or may reveal information that is not available in any other source.</p><p>In this chapter, we will explore unstructured data by:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Cleaning raw text through stemming, stop word removal, and other normalizations</li><li class="listitem" style="list-style-type: disc">Using tokenization and n-grams to find common patterns in textual data</li><li class="listitem" style="list-style-type: disc">Normalizing image data and removing noise</li><li class="listitem" style="list-style-type: disc">Decomposing images into lower dimensional features through several common matrix factorization algorithms</li></ul></div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec34"/>Working with textual data</h1></div></div></div><p>In the <a id="id391" class="indexterm"/>following example, we will consider the problem of separating text messages sent between cell phone users. Some of these messages are spam advertisements, and the objective is to separate these from normal communications (Almeida, Tiago A., José María G. Hidalgo, and Akebo Yamakami. <em>Contributions to the study of SMS spam filtering: new collection and results.</em> Proceedings of the 11th ACM symposium on Document engineering. ACM, 2011). By looking for patterns of words that are typically found in spam advertisements, we could potentially derive a smart filter that would automatically remove these messages from a user's inbox. However, while in previous chapters we were concerned with fitting a predictive model for this kind of problem, here we will be shifting focus to cleaning up the data, removing noise, and extracting features. Once these tasks are done, either simple or lower-dimensional features can be input into many of the algorithms we have already studied.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec57"/>Cleaning textual data</h2></div></div></div><p>Let us start by <a id="id392" class="indexterm"/>loading and inspecting the data using the following commands. Note that we need to supply column names for this data ourselves:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; spam = pd.read_csv('smsspamcollection/SMSSpamCollection',sep='\t',header=None)</strong>
<strong>&gt;&gt;&gt; spam.columns = ['label','text']</strong>
<strong>&gt;&gt;&gt; spam.head()</strong>
</pre></div><p>This gives the following output:</p><div><img src="img/B04881_06_13.jpg" alt="Cleaning textual data"/></div><p>The dataset <a id="id393" class="indexterm"/>consists of two columns: the first contains the label (<code class="literal">spam</code> or <code class="literal">ham</code>) indicating whether the message is an advertisement or a normal message, respectively. The second column contains the text of the message. Right at the start, we can see a number of problems with using this raw text as input to an algorithm to predict the spam/nonspam label:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The text of each message contains a mixture of upper and lower case letters, but this capitalization does not affect the meaning of a word.</li><li class="listitem" style="list-style-type: disc">Many words (<em>to</em>, <em>he</em>, <em>the</em>, and so on) are common, but tell us relatively little about the message.</li></ul></div><p>Other issues are subtler:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">When we compare words such as <em>larger</em> and <em>largest</em>, the most information about the meaning of the words is carried by the root, <em>large</em>—differentiating between the two forms may actually prevent us from capturing common information about the presence of the word <em>large</em> in a text, since the count of this stem in the message will be divided between the variants. Looking only at individual words does not tell us about the context in which they are used. Indeed, it may be more informative to consider sets of words.</li><li class="listitem" style="list-style-type: disc">Even for words that do not fall into the common category, such as <em>and</em>, <em>the</em>, and <em>to</em>, it is sometimes unclear whether a word is present in a document because it is common across all documents or whether it contains special information about a particular document. For example, in a set of online movie reviews, words such as <em>character</em> and <em>film</em> will appear frequently, but do not help to distinguish one review from another since they are common across all reviews. Because the English language has a large vocabulary, the size of the resulting feature set could be enormous.</li></ul></div><p>Let us start by cleaning up the text before delving into the other feature issues. We can base by lowercasing each word in the text using the following function:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; def clean_text(input):</strong>
<strong>…      return "".join([i.lower() for i in input])</strong>
</pre></div><p>We then apply this function to each message using the map function we have seen in previous examples:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; spam.text = spam.text.map(lambda x: clean_text(x))</strong>
</pre></div><p>Inspecting the <a id="id394" class="indexterm"/>resulting we can verify that all the letters are now indeed lowercase:</p><div><img src="img/B04881_06_14.jpg" alt="Cleaning textual data"/></div><p>Next, we want to remove common words and trim the remaining vocabulary to just the stem portion of the word that is most useful for predictive modeling. We do this using the <a id="id395" class="indexterm"/>
<strong>natural language toolkit</strong> (<strong>NLTK</strong>) library (Bird, Steven. <em>NLTK: the natural language toolkit</em>. Proceedings of the COLING/ACL on Interactive presentation sessions. Association for Computational Linguistics, 2006.). The list of stop words is part of the dataset associated for download with this library; if this is your first time opening NLTK, you can use the <code class="literal">nltk.download()</code> command to open a <a id="id396" class="indexterm"/>
<strong>graphical user interface</strong> (<strong>GUI</strong>) where you can select the content you wish to copy to your local machine using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; import nltk</strong>
<strong>&gt;&gt;&gt; nltk.download()</strong>
<strong>&gt;&gt;&gt; from nltk.corpus import stopwords</strong>
<strong>&gt;&gt;&gt; stop_words = stopwords.words('english')</strong>
</pre></div><p>We then define a function to perform stemming:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; def stem_text(input):</strong>
<strong>…    return " ".join([nltk.stem.porter.PorterStemmer().stem(t) if t not in \</strong>
<strong>…       stop_words else for t in nltk.word_tokenize(input)])</strong>
</pre></div><p>Finally, we again use a lambda function to perform this operation on each message, and visually inspect the results:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; spam.text = spam.text.map(lambda x: stem_text(x))</strong>
</pre></div><div><img src="img/B04881_06_15.jpg" alt="Cleaning textual data"/></div><p>For example, you can see the stem <em>joke</em> has been extracted from <em>joking</em>, and <em>avail</em> from <em>available</em>.</p><p>Now that <a id="id397" class="indexterm"/>we have performed lower casing and stemming, the messages are in relatively cleaned up form, and we can proceed to generate features for predictive modeling from this data.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec58"/>Extracting features from textual data</h2></div></div></div><p>In perhaps the <a id="id398" class="indexterm"/>simplest possible feature for text data, we use a binary vector of <em>0s</em> and <em>1s</em> to simply record the presence or absence of each word in our vocabulary in each message. To do this we can utilize the <code class="literal">CountVectorizer</code> function in the <code class="literal">scikit-learn</code> library, using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer</strong>
<strong>&gt;&gt;&gt; count_vect_sparse = CountVectorizer().fit_transform(spam.text)</strong>
</pre></div><p>By default, the result is stored as a <em>sparse vector</em>, which means that only the non-zero elements are held in memory. To calculate the total size of this vector we need to transform it back into a <em>dense</em> vector (where all elements, even 0, are stored in memory):</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; count_vect_sparse[0].todense().size</strong>
</pre></div><p>By checking the length of the feature vector created for the first message, we can see that it creates a vector of length 7,468 for each message with 1 and 0 indicating the presence or absence, respectively, of a particular word out of all words in this document list.</p><p>We can <a id="id399" class="indexterm"/>check that this length is in fact the same as the vocabulary (union of all unique words in the messages) using the following command to extract the <code class="literal">vocabulary_ element</code> of the vectorizer, which also gives a value of 7,468:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; len(CountVectorizer().fit(spam.text).vocabulary_)Recall from the earlier that individual words might not informative features if their meaning is dependent upon the context given by other words in a sentence. Thus, if we want to expand our feature set to potentially more powerful features, we could also consider n-grams, sets of n co-occurring words (for example, the phrase \the red house contains the n-grams the red, and red house (2-grams), and the red house (3-gram)). These features are calculated similarly as above, by supplying the argument ngram_range to the CountVectorizer constructor:</strong>
<strong>&gt;&gt;&gt; count_vect_sparse = CountVectorizer(ngram_range=(1, 3)).fit_transform(spam.text)</strong>
</pre></div><p>We can see that this increases the size of the resulting feature by about 10-fold by again inspecting the length of the first row using:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; count_vect_sparse[0].todense().sizeInsert</strong>
</pre></div><p>However, even after calculating n-grams, we still have not accounted for the fact that some words or n-grams might be common across all messages and thus provide little information in distinguishing spam from nonspam. To account for this, instead of simply recording the presence or absence of a word (or n-gram), we might compare the frequency of words within a document to the frequency across all documents. This ratio, the <a id="id400" class="indexterm"/>
<strong>term-frequency-inverse document frequency</strong> (<strong>tf-idf</strong>) is calculated in the simplest form as:</p><div><img src="img/B04881_06_01.jpg" alt="Extracting features from textual data"/></div><p>Where <em>ti</em> is a particular term (word or n-gram), <em>dj</em> is a particular document, <em>D</em> is the number of documents, <em>Vj</em> is the set of words in document <em>j</em>, and <em>vk</em> is a particular word in document <em>j</em>. The subscripted 1 in this formula is known as an <a id="id401" class="indexterm"/>
<strong>Indicator Function</strong>, which returns <code class="literal">1</code> if the subscripted condition is <code class="literal">true</code>, and <code class="literal">0</code> otherwise. In essence, this formula compares the frequency (count) of a word within a document to the number of documents that contain this word. As the number of documents containing the word decreases, the denominator decreases, and thus the overall formula becomes larger from dividing by a value much less than <code class="literal">1</code>. This is balanced by the frequency of the word within a document in the numerator. Thus, the <code class="literal">tf-idf</code> score will more heavily weight words that are present at greater frequency within a document compared to those common among all documents and thus might be indicative of special features of a particular message.</p><p>Note <a id="id402" class="indexterm"/>that the formula above represents only the simplest version of this expression. There are also variants in which we might logarithmically transform the counts (to offset the bias from large documents), or scale the numerator by the maximum frequency found for any term within a document (again, to offset bias that longer documents could have higher term frequencies than shorter documents by virtue of simply having more words) (Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. <em>Scoring, term weighting and the vector space model.</em> Introduction to Information Retrieval 100 (2008): 2-4.).  We can apply <code class="literal">tf-idf</code> to the spam data using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer</strong>
<strong>&gt;&gt;&gt; tf_idf = TfidfVectorizer().fit_transform(spam.text)</strong>
</pre></div><p>We can see the effect of this transformation by taking the maximum value across rows using:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; tf_idf.todense().max(1)</strong>
</pre></div><p>Where the '1' argument to max indicates that the function is applied along rows (instead of columns, which would be specified with '0' ). When our features consisted only of binary values, the maximum across each rows would be 1, but we can see that it is now a float value.</p><p>The final text feature we will discuss is concerned with condensing our feature set. Simply put, as we consider larger and larger vocabularies, we will encounter many words that are so infrequent as to almost never appear. However, from a computational standpoint,  even a single instance of a word in one document is enough to expand the number of columns in our text features for all documents. Given this, instead of directly recording whether a word is present, we might think of compressing this space requirement so that we use fewer columns to represent the same dataset. While in some cases, two words might map to the same column, in practice <a id="id403" class="indexterm"/>this happens infrequently enough due to the long-tailed distribution of word frequencies that it can serve as a handy way to reduce the dimensionality of our text data. To perform this mapping, we make use of a hash function that takes as input a word and outputs a random number (column location) that is keyed to the value of that string.  The number of columns we ultimately map to in our transformed dataset is controlled by the <code class="literal">n_features</code> argument to the <code class="literal">HashingVectorizer</code>, which we can apply to our dataset using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from sklearn.feature_extraction.text import HashingVectorizer</strong>
<strong>&gt;&gt;&gt; h = HashingVectorizer(n_features=1024).fit_transform(spam.text)</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec59"/>Using dimensionality reduction to simplify datasets</h2></div></div></div><p>Even though <a id="id404" class="indexterm"/>using the  <code class="literal">HashingVectorizer</code> allows us to reduce the data to a set of 1,024 columns from a feature set that was much larger, we are still left with many variables in our dataset. Intuition tells us that some of these features, either before or after the application of the <code class="literal">HashingVectorizer</code>, are probably correlated. For example, a set of words may co-occur in a document that is spam. If we use n-grams and the words are adjacent to one another, we could pick up on this feature, but not if the words are simply present in the message but separated by other text. The latter might occur, for example, if some common terms are in the first sentence of the message, while others are near the end.More broadly, given a large set of variables such as we have already seen for textual data, we might ask whether we could represent these data using a more compact set of features. In other words, is there an underlying pattern to the variation in thousands of variables that may be extracted by calculating a much smaller number of features representing patterns of correlation between individual variables? In a sense, we already saw several examples of this idea in <a class="link" href="ch03.html" title="Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning">Chapter 3</a>, <em>Finding Patterns in the Noise – Clustering and Unsupervised Learning</em>, in which we reduced the complexity of a dataset by aggregating individual datapoints into clusters. In the following examples, we have a similar goal, but rather than aggregating individual datapoints, we want to capture groups of correlated variables.</p><p>While we might achieve this goal in part through the variable selection techniques such as regularization, which <a id="id405" class="indexterm"/>we discussed in the <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em>, we do not necessarily want to remove variables, but rather capture their common patterns of variation.</p><p>Let us examine some of the common methods of dimensionality reduction and how we might choose between them for a given problem.</p></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec35"/>Principal component analysis</h1></div></div></div><p>One of the most <a id="id406" class="indexterm"/>commonly used methods of dimensionality reduction is <strong>Principal Component Analysis</strong> (<strong>PCA</strong>). Conceptually, PCA computes the axes along which the variation in the data is greatest. You may recall that in <a class="link" href="ch03.html" title="Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning">Chapter 3</a>, <em>Finding Patterns in the Noise – Clustering and Unsupervised Learning</em>, we calculated the eigenvalues of the adjacency matrix of a dataset to perform spectral clustering. In PCA, we also want to find the eigenvalue of the dataset, but here, instead of any adjacency matrix, we will use the covariance matrix of the data, which is the relative variation within and between columns. The covariance for columns <code class="literal">xi</code> and <code class="literal">xj</code> in the data matrix <code class="literal">X</code> is given by:</p><div><img src="img/B04881_06_02.jpg" alt="Principal component analysis"/></div><p>This is the average product of the offsets from the mean column values. We saw this value before when we computed the correlation coefficient in <a class="link" href="ch03.html" title="Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning">Chapter 3</a>, <em>Finding Patterns in the Noise – Clustering and Unsupervised Learning</em>, as it is the denominator of the Pearson coefficient. Let us use a simple example to illustrate how PCA works. We will make a dataset in which the six columns are derived from the same underlying normal distribution, one of which is given reversed in sign, using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; syn_1 = np.random.normal(0,1,100)</strong>
<strong>&gt;&gt;&gt; syn_2 = -1*syn_1</strong>
<strong>&gt;&gt;&gt; syn_data = [ syn_1, syn_1, syn_1, syn_2, syn_2, syn_2]</strong>
</pre></div><p>Note that each of our columns has mean <code class="literal">0</code> and standard deviation <code class="literal">1</code>. If this were not the case, we could use the scikit-learn utility StandardScaler as we discussed in <a class="link" href="ch03.html" title="Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning">Chapter 3</a>, <em>Finding Patterns in the Noise – Clustering and Unsupervised Learning</em>, when we normalized data for use in k means clustering. We might simply center the variables at <code class="literal">0</code> and use the resulting covariance matrix if we believe that the differences in scale of the variables are important to our problem. Otherwise, differences in scale will tend to be reflected by the differing variance values within the columns of the data, so our resulting PCA <a id="id407" class="indexterm"/>will reflect not only correlations within variables but also their differences in magnitude. If we do not want to emphasize these differences and are only interested in the relative correlation among variables, we can also divide each column of the data by its standard deviation to give each column a variance of 1. We could also potentially run PCA not on the covariance matrix, but the Pearson correlation matrix between variables, which is already naturally scaled to 0 and a constant range of a values (from -1 to 1) (Kromrey, Jeffrey D., and Lynn Foster-Johnson. <em>Mean centering in moderated multiple regression: Much ado about nothing.</em> Educational and Psychological Measurement 58.1 (1998): 42-67.). For now, we can compute the  covariance matrix of our data with the following command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; syn_cov = np.cov(syn_data)</strong>
</pre></div><p>Recalling our discussion of spectral clustering in <a class="link" href="ch03.html" title="Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning">Chapter 3</a>, <em>Finding Patterns in the Noise – Clustering and Unsupervised Learning</em>, if we consider the covariance matrix as a stretching operation on a vector, then, if we find the vectors that lie along these directions of distortion, we have in a sense found the axes that define the variation in the data. If we then compare the eigenvalues of these vectors, we could determine if one or more of these directions reflect a greater proportion of the overall variation of the data. Let us compute the eigenvalues and vectors of the covariance matrix using:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; [eigenvalues, eigenvectors] = np.linalg.eig(syn_cov)</strong>
</pre></div><p>This gives the following eigenvalue variable as:</p><div><pre class="programlisting">
<strong>array([  0.00000000e+00,   4.93682786e+00,   1.23259516e-32,          1.50189461e-16,   0.00000000e+00,  -9.57474477e-34])</strong>
</pre></div><p>You can see that most of the eigenvalues are effectively zero, except the second. This reflects the fact that the data we constructed, despite having six columns, is effectively derived from only one dataset (a normal distribution). Another important property of these eigenvectors is that they are orthogonal, which means that they are at right angles to each other in n-dimensional space: if we were to take a dot product between them, it would be 0, and they thus represent independent vectors that, when linearly combined, can be used to represent the dataset.</p><p>If we were to multiply the data by the eigenvector corresponding to this second eigenvalue, we would project the data from a six-dimensional to a one-dimensional space:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; plt.hist(np.dot(np.array(syn_data).transpose(),np.array(eigenvectors[:,1])))</strong>
</pre></div><p>Note that we <a id="id408" class="indexterm"/>needed to transpose the data to have the 100 rows and 6 columns, as we initially constructed it as a list of 6 columns, which NumPy interprets as instead having 6 rows and 100 columns. The resulting histogram is as shown in the following:</p><div><img src="img/B04881_06_17.jpg" alt="Principal component analysis"/></div><p>In other words, by projecting the data onto the axis of greatest variance, we have recovered that fact that this six-column data was actually generated from a single distribution. Now if we instead use the PCA command, we get a similar result:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; syn_pca = PCA().fit(np.array(syn_data))</strong>
</pre></div><p>When we extract the <code class="literal">explained_variance_ratio_</code>, the algorithm has effectively taken the preceding eigenvalues, ordered them by magnitude, and divided by the largest one, giving:</p><div><pre class="programlisting">
<strong>array([  1.00000000e+000,   6.38413622e-032,   2.02691244e-063,          2.10702767e-094,   3.98369984e-126,   5.71429334e-157])</strong>
</pre></div><p>If we were to plot these as a barplot, a visualization known as a <code class="literal">scree plot</code> could help us determine how many underlying components are represented in our data:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; scree, ax = plt.subplots()</strong>
<strong>&gt;&gt;&gt; plt.bar(np.arange(0,6),syn_pca.explained_variance_ratio_)</strong>
<strong>&gt;&gt;&gt; ax.set_xlabel('Component Number')</strong>
<strong>&gt;&gt;&gt; ax.set_ylabel('Variance Explained')</strong>
<strong>&gt;&gt;&gt; plt.show()</strong>
</pre></div><p>This generates the following plot:</p><div><img src="img/B04881_06_18.jpg" alt="Principal component analysis"/></div><p>Evidently, only <a id="id409" class="indexterm"/>the first component carries any variance, represented by the height of the bar, with all other components being near 0 and so not appearing in the plot. This sort of visual analysis is comparable to how we looked for an elbow in the inertia function for k-means in <a class="link" href="ch03.html" title="Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning">Chapter 3</a>, <em>Finding Patterns in the Noise – Clustering and Unsupervised Learning</em>, as a function of k to determine how many clusters were present in the data.We can also extract the data projected onto the first principal components and see a similar plot as shown previously when we projected the data onto an eigenvector of the covariance matrix:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; plt.hist(syn_pca.components_[0])</strong>
</pre></div><div><img src="img/B04881_06_38.jpg" alt="Principal component analysis"/></div><p>Why are they not exactly identical? While conceptually PCA computes the eigenvalues of the covariance matrix, in practice most packages do not actually implement the calculation we illustrated previously for purposes of numerical efficiency. Instead, they employ a matrix operation known as <a id="id410" class="indexterm"/>the <strong>Singular Value Decomposition</strong> (<strong>SVD</strong>), which seeks to represent a covariance matrix of <em>X </em>as a set of lower dimensional row and column matrices:</p><div><img src="img/B04881_06_03.jpg" alt="Principal component analysis"/></div><p>Where if <em>X</em> is an <em>n</em> by <em>m</em>, W may be <em>n</em> by <em>k</em>, where <em>k &lt;&lt; m</em>. Here, σ represents a matrix with 0 everywhere but the diagonal, which contains non-zero entries. Thus, the covariance matrix is represented as the product of two smaller matrices and a scaling factor given by the diagonal elements in σ. Instead of calculating all eigenvectors of the covariance matrix, as we did previously, we can ask only for the k columns or WT we think are likely to be significant judged by the sort of scree plot analysis we demonstrated above. However, when we project the data onto the principal components we obtain through this method, the calculation of the SVD can potentially give different signs to the projection of the data on the principal components, even if the relative magnitude and signs of these components remains the same. Thus, when <a id="id411" class="indexterm"/>we look at the scores assigned to a given row of data after projecting it onto the first k principal components, we should analyze them relative to other values in the dataset, just as when we examined the coordinates produced by Multidimensional Scaling in <a class="link" href="ch03.html" title="Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning">Chapter 3</a>, <em>Finding Patterns in the Noise – Clustering and Unsupervised Learning</em>. Details of the SVD calculation used by the default scikit-learn implementation of PCA are given in (Tipping, Michael E., and Christopher M. Bishop. <em>Probabilistic principal component analysis</em>. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 61.3 (1999): 611-622.).</p><p>Now that we have examined conceptually what PCA calculates, let us see if it can help us reduce the dimensionality of our text dataset. Let us run PCA on the n-gram feature set from above, asking for 100 components. Note that because the original dataset is a <a id="id412" class="indexterm"/>sparse matrix and PCA requires a dense matrix as an input, we need to convert it using <code class="literal">toarray()</code>. Also, to retain the right dimensionality for use with the PCA fit function, we need to transpose the result:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt;  pca_text = PCA(num_components=10).fit(np.transpose(count_vect_sparse.toarray()))</strong>
</pre></div><p>If we make a scree plot of total variance explained by the first <code class="literal">10</code> principal components of this dataset, we see that we will probably require a relatively large number of variables to capture the variation in our data since the upward trend in variance explained is relatively smooth:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; scree, ax = plt.subplots()</strong>
<strong>&gt;&gt;&gt; plt.bar(np.arange(0,10),pca_text.explained_variance_ratio_)</strong>
<strong>&gt;&gt;&gt;ax.set_xlabel('Component Number')</strong>
<strong>&gt;&gt;&gt;ax.set_ylabel('Variance Explained')</strong>
<strong>&gt;&gt;&gt; plt.show()</strong>
</pre></div><div><img src="img/B04881_06_19.jpg" alt="Principal component analysis"/></div><p>We could also visualize this by looking at the cumulative variance explained using <em>k</em> components using the following curve:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; scree, ax = plt.subplots()</strong>
<strong>&gt;&gt;&gt; plt.plot(pca_text.explained_variance_ratio_.cumsum())</strong>
<strong>&gt;&gt;&gt; ax.set_xlabel('Number of Components')</strong>
<strong>&gt;&gt;&gt; ax.set_ylabel('Cumulative Variance Explained')</strong>
<strong>&gt;&gt;&gt; plt.show()  </strong>
</pre></div><div><img src="img/B04881_06_21.jpg" alt="Principal component analysis"/></div><p>A word on <a id="id413" class="indexterm"/>normalization: in practice, for document data, we might not want to scale the data by subtracting the mean and dividing by the variance as the data is mostly binary. Instead, we would just apply the SVD to a binary matrix or perhaps the tF-idf scores we computed previously, an approach also <a id="id414" class="indexterm"/>known as <strong>Latent Semantic Indexing</strong> (<strong>LSI</strong>) (Berry, Michael W., Susan T. Dumais, and Gavin W. O'Brien. <em>Using linear algebra for intelligent information retrieval</em>. SIAM review 37.4 (1995): 573-595; Laham, T. K. L. D., and Peter Foltz. <em>Learning human-like knowledge by singular value decomposition: A progress report</em>. Advances in Neural Information Processing Systems 10: Proceedings of the 1997 Conference. Vol. 10. MIT Press, 1998.). CUR decomposition and nonnegative matrix factorization</p><p>What drawbacks might there be to using PCA to reduce the dimensionality of a dataset? For one, the components (covariance matrix eigenvectors) generated by PCA are still essentially mathematical entities: the patterns in variables represented by these axes might not actually correspond to any element of the data, but rather a linear combination of them. This representation is not always easily interpretable, and can particularly difficult when trying to convey the results of such analyses to domain experts <a id="id415" class="indexterm"/>to generate subject-matter specific insights. Second, the fact that PCA produces negative values in its eigenvectors, even for positive-only data such as text (where a term cannot be negatively present in a document, just 0, 1, a count, or a frequency), is due to the fact that the data is linearly combined using these factors. In other words, positive and negative values may be summed together when we project the data onto its components through matrix multiplication, yielding an overall positive value for the projection. Again, it may be preferable to have factors that give some insight into the structure of the data itself, for example, by giving a factor that consists of binary indicators for a group of words that tend to co-occur in a particular group of documents. These goals are addressed by two other matrix factorization techniques: CUR Decomposition and Non-negative Matrix Factorization.</p><p>Like the SVD used in PCA, CUR attempts to represent a matrix of data X as a product of lower dimensional matrices. Here, instead of eigenvectors, the CUR decomposition attempts to find the set of columns and rows of the matrix that best represent the dataset as:</p><div><img src="img/B04881_06_04.jpg" alt="Principal component analysis"/></div><p>Where <em>C</em> is a matrix of <em>c</em> columns of the original dataset, <em>R</em> is a set of <em>r</em> rows from the original dataset, and <em>U</em> is a matrix of scaling factors. The <em>c</em> columns and <em>r</em> rows used in this reconstruction are sampled from the columns and rows of the original matrix, with probability proportional to the <code class="literal">leverage score</code>, given by:</p><div><img src="img/B04881_06_05.jpg" alt="Principal component analysis"/></div><p>Where <em>lvj</em> is the statistical leverage for column (row) <em>j</em>, <em>k</em> is the number of components in the SVD of <em>X</em>, and <em>vj</em> are the <em>jth</em> elements of these <em>k</em> component vectors. Thus, columns (rows) are sampled with high probability if they contribute significantly to the overall norm of the matrix's singular values, meaning they are also have a major influence on the reconstruction error from the SVD (for example, how well the SVD approximates the original matrix) (Chatterjee, Samprit, and Ali S. Hadi. Sensitivity analysis in linear regression. Vol. 327. John Wiley &amp; Sons, 2009; Bodor, András, et al. <em>rCUR: an R package for CUR matrix decomposition</em>. BMC bioinformatics 13.1 (2012): 1).</p><p>While this decomposition is not expected to approximate the original dataset with the same accuracy as the SVD approach used in PCA, the resulting factors may be easier to interpret since they are actual elements of the original dataset.</p><div><div><h3 class="title"><a id="note09"/>Note</h3><p>Please note that while we use SVD to determine sampling probabilities for the columns and rows, the final factorization of CUR does not.</p></div></div><p>There are many <a id="id416" class="indexterm"/>algorithms for generating a CUR decomposition (Mahoney, Michael W., and Petros Drineas. <em>CUR matrix decompositions for improved data analysis. Proceedings of the National Academy of Sciences 106.3 (2009): 697-702.  Boutsidis, Christos, and David P. Woodruff. Optimal cur matrix decompositions</em>. Proceedings of the 46th Annual ACM Symposium on Theory of Computing. ACM, 2014). CUR decomposition is implemented in the <code class="literal">pymf</code> library, and we can call it using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; cur = pymf.CUR(count_vect_sparse.toarray().transpose(),crank=100,rrank=100)</strong>
<strong>&gt;&gt;&gt; cur.factorize() &gt;&gt;&gt; cur.factorize()</strong>
</pre></div><p>The <code class="literal">crank</code> and <code class="literal">rrank</code> parameters indicate how many rows and columns, respectively, should be chosen from the original matrix in the process of performing the decomposition. We can then examine which columns (words from the vocabulary) were chosen in this reconstruction using the following commands to print these significant words whose indices are contained in the cur object's .<code class="literal">_cid</code> (column index) element. First we need to collect a list of all words in the vocabulary of our spam dataset:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; vocab = CountVectorizer().fit(spam.text).vocabulary_</strong>
<strong>&gt;&gt;&gt; vocab_array = ['']*len(vocab.values())</strong>
<strong>&gt;&gt;&gt; for k,v in vocab.items():</strong>
<strong>…      vocab_array[v]=k</strong>
<strong>&gt;&gt;&gt;vocab_array = np.array(vocab_array)</strong>
</pre></div><p>Since the <code class="literal">vocabulary_</code> variable returned by the <code class="literal">CountVectorizer</code> is a dictionary giving the positions of terms in the array to which they are mapped, we need to construct our array by placing the word at the position given by this dictionary. Now we can print the corresponding words using:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; for i in cur._cid:</strong>
<strong>… print(vocab_array[i])</strong>
</pre></div><p>Like CUR, nonnegative matrix factorization attempts to find a set of positive components that represents the structure of a dataset (Lee, Daniel D., and H. Sebastian Seung. <em>Learning the parts of objects by non-negative matrix factorization.</em> Nature 401.6755 (1999): 788-791; Lee, Daniel D., and H. Sebastian Seung. <em>Algorithms for non-negative matrix factorization.</em> Advances in neural information processing systems. 2001.; P. Paatero, U. Tapper (1994).  Paatero, Pentti, and Unto Tapper. <em>Positive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data values</em>. Environmetrics 5.2 (1994): 111-126. Anttila, Pia, et al. <em>Source identification of bulk wet deposition in Finland by positive matrix factorization</em>. Atmospheric Environment 29.14 (1995): 1705-1718.). Similarly, it tries to reconstruct the data using:</p><div><img src="img/B04881_06_06.jpg" alt="Principal component analysis"/></div><p>Where <em>W</em> and <em>H</em> are lower <a id="id417" class="indexterm"/>dimensional matrices that when multiplied, reconstruct <em>X</em>; all three of <em>W</em>, <em>H</em>, and <em>X</em> are constrained to have no negative values. Thus, the columns of <em>X</em> are linear combinations of <em>W</em>, using <em>H</em> as the coefficients. For example, if the rows of <em>X</em> are words and the columns are documents, then each document in <em>X</em> is represented as a linear combination of underlying document types in <em>W</em> with weighted given by <em>H</em>. Like the elements returned by CUR decomposition, the components <em>W</em> from nonnegative matrix factorization are potentially more interpretable than the eigenvectors we get from PCA.</p><p>There are several algorithms to compute <em>W</em> and <em>H</em>, with one of the simplest being through multiplicative updates (Lee, Daniel D., and H. Sebastian Seung. <em>Algorithms for non-negative matrix factorization</em>. Advances in neural information processing systems. 2001). For example, if we want to minimize the Euclidean distance between <em>X</em> and <em>WH</em>:</p><div><img src="img/B04881_06_07.jpg" alt="Principal component analysis"/></div><p>We can calculate the derivative of this value with respective to <em>W</em>:</p><div><img src="img/B04881_06_08.jpg" alt="Principal component analysis"/></div><p>Then to update <em>W</em> we multiply at each step by this gradient:</p><div><img src="img/B04881_06_09.jpg" alt="Principal component analysis"/></div><p>And the same for <em>H</em>:</p><div><img src="img/B04881_06_10.jpg" alt="Principal component analysis"/></div><p>These <a id="id418" class="indexterm"/>steps are repeated until the values of <em>W</em> and <em>H</em> converge. Let us examine what components we retrieve from our text data when we use NMF to extract components:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from sklearn.decomposition import NMF</strong>
<strong>&gt;&gt;&gt; nmf_text = NMF(n_components=10).fit(np.transpose(count_vect_sparse.toarray())</strong>
</pre></div><p>We can then look at the words represented by the components in NMF, where the words have a large value in the components matrix resulting from the decomposition.</p><p>We can see that they appear to capture distinct groups of words, but are any correlated with distinguishing spam versus nonspam? We can transform our original data using the NMF decomposition, which will give the weights for linearly combining these features (for example, the weights to linearly combine the 10 basis documents we get from the decomposition to reconstruct the message) using the command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; nmf_text_transform = nmf_text.transform(count_vect_sparse.toarray())</strong>
</pre></div><p>Now let us plot the average weight assigned to each of these <code class="literal">nmf</code> factors for the normal and spam messages. We can do this by plotting a bar chart where the <em>x</em> axis are the 10 <code class="literal">nmf</code> <a id="id419" class="indexterm"/>factors, and the <em>y</em> axis are the average weight assigned to this factor for a subset of documents:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; plt.bar(range(10),nmf_text_transform[spam.label=='spam'].mean(0))</strong>
</pre></div><div><img src="img/B04881_06_22.jpg" alt="Principal component analysis"/></div><div><pre class="programlisting">
<strong>&gt;&gt;&gt; plt.bar(range(10),nmf_text_transform[spam.label=='ham'].mean(0))</strong>
</pre></div><div><img src="img/B04881_06_23.jpg" alt="Principal component analysis"/></div><p>Promisingly, the factors 8 and 9 seem to have very different average weights between these two classes of messages. In fact, we may need fewer than 10 factors to represent the data, since <a id="id420" class="indexterm"/>these two classes may well correspond to the underlying spam versus nonspam messages.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec60"/>Latent Dirichlet Allocation</h2></div></div></div><p>A related <a id="id421" class="indexterm"/>method of decomposing <a id="id422" class="indexterm"/>data into an interpretable set of features is <strong>Latent Dirichlet Allocation</strong> (<strong>LDA</strong>), a method initially developed for textual and genetics data that has since been extended to other areas (Blei, David M., Andrew Y. Ng, and Michael I. Jordan. <em>Latent dirichlet allocation</em>. the Journal of machine Learning research 3 (2003): 993-1022. Pritchard, Jonathan K., Matthew Stephens, and Peter Donnelly. <em>Inference of population structure using multilocus genotype data</em>. Genetics 155.2 (2000): 945-959.). Unlike the methods we looked at previously, where the data is represented as a set of lower dimensional matrices that, when multiplied, approximate the original data, LDA uses a probability model. This model is often explained using a plate diagram that illustrates the dependencies among the variables, as shown in the following diagram:</p><div><img src="img/B04881_06_24.jpg" alt="Latent Dirichlet Allocation"/></div><p>What exactly does this diagram describe? It is what is known as a generative model: a set of instructions by which to generate a probability distribution over documents. The idea is comparable to a distribution such as the Gaussian 'bell-curve' you are probably familiar with, except here instead of drawing real numbers from the distribution we sample documents. Generative models may be contrasted with the predictive methods which we have seen in previous chapters that attempts to fit the data to a response (as in the regression or classification models we have studied in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapters 4</a>, <em>Connecting the Dots with Models – Regression Methods</em>, and <a class="link" href="ch05.html" title="Chapter 5. Putting Data in its Place – Classification Methods and Analysis">Chapter 5</a>, <em>Putting Data in its Place – Classification Methods and Analysis</em>), instead of simply generate samples of the data according to a distribution. The plate diagram represents the components of this generative model, and we can think of this model as the following series of steps to generate a document:Initialize a Dirichlet distribution to choose from a set of topics. These topics are analogous to the components we found in NMF, and can be thought of as <em>basis documents</em> representing groups of commonly co-occurring words. The Dirichlet distribution is given by the following formula:</p><div><img src="img/B04881_06_11.jpg" alt="Latent Dirichlet Allocation"/></div><p>The <a id="id423" class="indexterm"/>preceding <a id="id424" class="indexterm"/>formula gives the probability of observing a given distribution of items (here topics) among <code class="literal">K</code> classes and can be used to sample a vector of <code class="literal">K</code> class memberships (for example, sample a random vector giving what fraction of documents in the collection belong to a given topic). The alpha parameter in the Dirichlet distribution is used as an exponent of the K category probabilities and increases the significance ascribed to a particular component (for example, a more frequent topic). The term <code class="literal">B</code> is the beta function, which is simply a normalization term. We use the Dirichlet distribution in step 1 to generate a per-topic probability distribution for a document <em>i</em>. This distribution would be, for example, a series of weights that sum to 1 giving the relative probability that a document belongs to a given topic. This is the parameter θ in the plate diagram. M represents the number of documents in our dataset.</p><div><ol class="orderedlist arabic"><li class="listitem">For each of the <em>N</em> word positions in the document, choose a topic Z from the distribution θ. Each of the M topics has a Dirichlet distribution with parameter β instead of giving per word probabilities, given by ϕ. Use this distribution to choose word in each N position in a document.</li><li class="listitem">Repeat steps 2–4 for each word position for each document in a dataset to generate a group of documents.</li></ol></div><p>In the previous diagram, the numbers (<strong>M</strong>, <strong>N</strong>, <strong>K</strong>) inside the rectangles indicate the number of time that the variables represented by circles are generated in the generative model. Thus, the words w, being innermost, are generated <em>N × M</em> times. You can also notice that the rectangles enclose variables that are generated the same number of times, while arrows indicate dependence among variables during this data generation process. You can also now appreciate where the name of this model comes from, as a document is latently allocated among many topics, just as we used the factors in NMF to find linear combinations of 'basis documents' that could reconstruct our observed data.</p><p>This <a id="id425" class="indexterm"/>recipe can also <a id="id426" class="indexterm"/>be used to find a set of topics (for example word probability distributions) that fit a dataset, assuming the model described previously was used to generate the documents. Without going into the full details of the derivation, we randomly initialize a fixed number of <em>K</em> topics and run the model, as described previously, by always sampling a document's topic, given all other documents, and a word, given the probability of all other words in the document. We then update the parameters of the model based on the observed data and use the updated probabilities to generate the data again. Over many iterations, this process, known as Gibbs sampling, will converge from randomly initialized values to a set of model parameters that best fit the observed document data. Let us now fit an LDA model to the spam dataset using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; lda = LatentDirichletAllocation(n_topics=10).fit(count_vect_sparse)</strong>
</pre></div><p>As with NMF, we can examine the highest probability words for each topic using:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; for i in range(10):</strong>
<strong>…    print(vocab_array[np.argsort(lda.components_[i])[1:10]])</strong>
</pre></div><p>Likewise, we can see if these topics represent a meaningful separation between the spam and nonspam messages. First we find the topic distribution among the 10 latent topics for each document using the following <code class="literal">transform</code> command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; topic_dist = lda.transform(count_vect_sparse)</strong>
</pre></div><p>This is analogous to the weights we calculated in NMF. We can now plot the average topic weight for each message class as follows:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; plt.bar(range(10),topic_dist[spam.label=='ham'].mean(0))</strong>
</pre></div><div><img src="img/B04881_06_25.jpg" alt="Latent Dirichlet Allocation"/></div><div><pre class="programlisting">
<strong>&gt;&gt;&gt; plt.bar(range(10),topic_dist[spam.label=='spam'].mean(0))</strong>
</pre></div><div><img src="img/B04881_06_26.jpg" alt="Latent Dirichlet Allocation"/></div><p>Again, promisingly, we <a id="id427" class="indexterm"/>find a <a id="id428" class="indexterm"/>different average weight for topic 5 for spam than nonspam, indicating that the LDA model has successfully separated out the axis of variation we are interested in for classification purposes.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec61"/>Using dimensionality reduction in predictive modeling</h2></div></div></div><p>The analysis we <a id="id429" class="indexterm"/>have outlined previously has been largely devoted to trying to extract a lower-dimensional representation of a text collection by finding a smaller set of components that capture the variation among individual documents. In some cases, this sort of analysis can be useful as an exploratory data analysis tool, which, like the clustering techniques we described in <a class="link" href="ch03.html" title="Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning">Chapter 3</a>, <em>Finding Patterns in the Noise – Clustering and Unsupervised Learning</em>, allows us to understand the structure in a dataset. We might even combine clustering and dimensionality reduction, which is in essence the idea of spectral clustering as we examined in <a class="link" href="ch03.html" title="Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning">Chapter 3</a>, <em>Finding Patterns in the Noise – Clustering and Unsupervised Learning</em> using SVD to reduce the adjacency matrix to a more compact representation and then clustering this reduced space to yield a cleaner separation between datapoints.</p><p>Like the groups assigned through clustering, we can also potentially use the components derived from these dimensionality reduction methods as features in a predictive model. For example, the NMF components we extracted previously could be used as inputs to a classification model to separate spam from nonspam messages. We have even seen this use earlier, as the online news popularity dataset we used in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em>, had columns derived from LDA topics. Like the regularization methods we saw in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em>, dimensionality reduction can help reduce overfitting by extracting the underlying correlations among variables since these lower-dimensional variables are often less noisy than using the whole feature space. Now that we have seen how dimensionality reduction could help us find structure in textual data, let us examine another class of potentially high-dimensional data found in images.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec36"/>Images</h1></div></div></div><p>Like textual data, images <a id="id430" class="indexterm"/>are potentially noisy and complex. Furthermore, unlike language, which has a structure of words, paragraphs, and sentences, images have no predefined rules that we might use to simplify raw data. Thus, much of image analysis will involve extracting patterns from the input's features, which are ideally interpretable to a human analyst based only on the input pixels.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec62"/>Cleaning image data</h2></div></div></div><p>One of the <a id="id431" class="indexterm"/>common operations we will perform on images is to enhance contrast or change their color scale. For example, let us start with an example image of a coffee cup from the <code class="literal">skimage</code> package, which you can import and visualize using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from skimage import data, io, segmentation</strong>
<strong>&gt;&gt;&gt; image = data.coffee()</strong>
<strong>&gt;&gt;&gt; io.imshow(image)</strong>
<strong>&gt;&gt;&gt; plt.axis('off');</strong>
</pre></div><p>This produces the following image:</p><div><img src="img/B04881_06_27.jpg" alt="Cleaning image data"/></div><p>In Python, this image is represented as a three-dimensional matrix with the dimensions corresponding to height, width, and color channels. In many applications, the color is not of interest, and instead we are trying to determine common shapes or features in a set of images that may be differentiated based on grey scale alone. We can easily convert this image into a grey scale version using the commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; grey_image = skimage.color.rgb2gray(image)</strong>
<strong>&gt;&gt;&gt; io.imshow(grey_image)</strong>
<strong>&gt;&gt;&gt; plt.axis('off');</strong>
</pre></div><div><img src="img/B04881_06_28.jpg" alt="Cleaning image data"/></div><p>A frequent <a id="id432" class="indexterm"/>task in image analysis is to identify different regions or objects within an image. This can be made more difficult if the pixels are clumped into one region (for example, if there is very strong shadow or a strong light in the image), rather than evenly distributed along the intensity spectrum. To identify different objects, it is often desirable to have these intensities evenly distributed, which we can do by performing histogram equalization using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from skimage import exposure</strong>
<strong>&gt;&gt;&gt; image_equalized = exposure.equalize_hist(grey_image)</strong>
<strong>&gt;&gt;&gt; io.imshow(image_equalized)</strong>
<strong>&gt;&gt;&gt; plt.axis('off'); </strong>
</pre></div><div><img src="img/B04881_06_29.jpg" alt="Cleaning image data"/></div><p>To see the effect of this normalization, we can plot the histogram of pixels by intensity before and after the transformation with the command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; plt.hist(grey_image.ravel())</strong>
</pre></div><p>This gives the following pixel distribution for the uncorrected image:</p><div><img src="img/B04881_06_30.jpg" alt="Cleaning image data"/></div><p>The <code class="literal">ravel()</code> command <a id="id433" class="indexterm"/>used here is used to flatten the 2-d array we started with into a single vector that may be input to the histogram function. Similarly, we can plot the distribution of pixel intensities following normalization using:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; plt.hist(image_equalized.ravel(),color='b')</strong>
</pre></div><div><img src="img/B04881_06_31.jpg" alt="Cleaning image data"/></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec63"/>Thresholding images to highlight objects</h2></div></div></div><p>Another common <a id="id434" class="indexterm"/>task for image analysis is to identify individual objects within a single image. To do so, we need to choose a threshold to binarize an image into white and black regions and separate overlapping objects. For the former, we can use thresholding algorithms such as Otsu thresholding (Otsu, Nobuyuki. <em>A threshold selection method from gray-level histograms.</em> Automatica 11.285-296 (1975): 23-27), which uses a <em>structuring element</em> (such as disk with n pixels) and attempts to find a pixel intensity, which will best separate pixels inside that structuring element into two classes (for example, black and white). We can imagine rolling a disk over an entire image and doing this calculation, resulting in either a local value within the disk or a global value that separates the image into foreground and background. We can then turn the image into a binary mask by thresholding pixels above or below this value.</p><p>To illustrate, let us consider a picture of coins, where we want to separate the coins from their background. We can visualize the histogram-equalized coin image using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; coins_equalized = exposure.equalize_hist(skimage.color.rgb2gray(data.coins()))</strong>
<strong>&gt;&gt;&gt; io.imshow(coins_equalized)</strong>
</pre></div><div><img src="img/B04881_06_32.jpg" alt="Thresholding images to highlight objects"/></div><p>One problem we can see is that the background has a gradient of illumination increasing toward the upper left corner of the image. This difference doesn't change the distinction between background and objects (coins), but because part of the background is in the same intensity range as the coins, it will make it difficult to separate out the coins themselves. To subtract the background, we can use the closing <a id="id435" class="indexterm"/>function, which sequentially erodes (removes white regions with size less than the structuring element) and then dilates (if there is a white pixel within the structuring element, all elements within the structuring element are flipped to white). In practice, this means we remove small white specks and enhance regions of remaining light color. If we then subtract this from the image, we subtract the background, as illustrated here:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from skimage.morphology import opening, disk</strong>
<strong>&gt;&gt;&gt; d=disk(50)</strong>
<strong>&gt;&gt;&gt; background = opening(coins_equalized,d)</strong>
<strong>&gt;&gt;&gt; io.imshow(coins_equalized-background) </strong>
</pre></div><div><img src="img/B04881_06_33.jpg" alt="Thresholding images to highlight objects"/></div><p>Now that we have removes the background, we can apply the Otsu thresholding algorithm mentioned previously to find the ideal pixel to separate the image into background and object using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from skimage import filter</strong>
<strong>&gt;&gt;&gt; threshold_global_otsu = filter.threshold_otsu(coins_equalized-background)</strong>
<strong>&gt;&gt;&gt; global_otsu = (coins_equalized-background) &gt;= threshold_global_otsu</strong>
<strong>&gt;&gt;&gt; io.imshow(global_otsu)</strong>
</pre></div><div><img src="img/B04881_06_34.jpg" alt="Thresholding images to highlight objects"/></div><p>The image <a id="id436" class="indexterm"/>has now been segmented into coins and non-coin regions. We could use this segmented image to count the number coins, to highlight the coins in the original image using the regions obtained above as a <em>mask</em>, for example if we want to record pixel data only from the coin regions as part of a predictive modeling feature using image data.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec64"/>Dimensionality reduction for image analysis</h2></div></div></div><p>Once we have our <a id="id437" class="indexterm"/>images appropriately cleaned, how can we turn them into more general features for modeling? One approach is to try to capture common patterns of variation between a group of images using the same dimensionality reduction techniques as we used previously for document data. Instead of words in documents, we have patterns of pixels within an image, but otherwise the same algorithms and analysis largely apply. As an example, let us consider a set of images of faces (<a class="ulink" href="http://www.geocities.ws/senthilirtt/Senthil%20Face%20Database%20Version1">http://www.geocities.ws/senthilirtt/Senthil%20Face%20Database%20Version1</a>) which we can load and examine using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; faces = skimage.io.imread_collection('senthil_database_version1/S1/*.tif')</strong>
<strong>&gt;&gt;&gt; io.imshow(faces[1])</strong>
</pre></div><div><img src="img/B04881_06_35.jpg" alt="Dimensionality reduction for image analysis"/></div><p>For each of these two-dimenional images, we want to convert it into a vector just as we did when we plotted the pixel frequency histograms during our discussion of normalization. We will also construct a set where the average pixel intensity across faces has been subtracted from each pixel, yielding each face as an offset from the <em>average face</em> in the data through the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; faces_flatten = [f.ravel() for f in faces]</strong>
<strong>&gt;&gt;&gt; import pylab</strong>
<strong>&gt;&gt;&gt; faces_flatten_demean = pylab.demean(faces_flatten,axis=1)</strong>
</pre></div><p>We consider two possible ways to factor faces into a more general features. The first is to use PCA to extract the major vectors of variation in this data—these vectors happen to also<a id="id438" class="indexterm"/> look like faces. Since they are formed from the eigenvalues of the covariance matrix, these sorts of features are sometimes known as eigenfaces. The following commands illustrate the result of performing PCA on the face dataset:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from sklearn.decomposition import PCA</strong>
<strong>&gt;&gt;&gt; faces_components = PCA(n_components=3).fit(faces_flatten_demean)</strong>
<strong>&gt;&gt;&gt; io.imshow(np.reshape(faces_components.components_[1],(188,140)))</strong>
</pre></div><div><img src="img/B04881_06_36.jpg" alt="Dimensionality reduction for image analysis"/></div><p>How much variation in the face data is captured by the principal components? In contrast to the document data, we can see that using PCA even with only three components <a id="id439" class="indexterm"/>allows to explain around two-thirds of the variation in the dataset:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; plt.plot(faces_components.explained_variance_ratio_.cumsum())</strong>
</pre></div><div><img src="img/B04881_06_37.jpg" alt="Dimensionality reduction for image analysis"/></div><p>We could also apply NMF, as we described previously, to find a set of basis faces. You can notice from the preceding heatmap that the eigenfaces we extracted can have negative values, which highlights one of the interpretational difficulties we mentioned previously: we cannot really have negative pixels (since , so a latent feature with negative elements is hard to interpret. In contrast, the components we extract using NMF will look much more like elements of the original dataset, as shown below using the commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from sklearn.decomposition import NMF</strong>
<strong>&gt;&gt;&gt; faces_nmf = NMF(n_components=3).fit(np.transpose(faces_flatten)) </strong>
<strong>&gt;&gt;&gt; io.imshow(np.reshape(faces_nmf.components_[0],(188,140))) </strong>
</pre></div><div><img src="img/B04881_06_39.jpg" alt="Dimensionality reduction for image analysis"/></div><p>Unlike the <a id="id440" class="indexterm"/>eigenfaces, which resemble averaged versions of many images, the NMF components extracted from this data look like individual faces. While we will not go through the exercise here, we could even apply LDA to image data to find topics represented by distributions of pixels and indeed it has been used for this purpose (Yu, Hua, and Jie Yang. <em>A direct LDA algorithm for high-dimensional data—with application to face recognition</em>. Pattern recognition 34.10 (2001): 2067-2070; Thomaz, Carlos E., et al. <em>Using a maximum uncertainty LDA-based approach to classify and analyse MR brain images</em>. Medical Image Computing and Computer-Assisted Intervention–MICCAI 2004. Springer Berlin Heidelberg, 2004. 291-300.).</p><p>While the dimensionality reduction techniques we have discussed previously are useful in the context of understanding datasets, clustering, or modeling, they are also potentially useful in storing compressed versions of data. Particularly in model services such as the one we will develop in <a class="link" href="ch08.html" title="Chapter 8. Sharing Models with Prediction Services">Chapter 8</a>, <em>Sharing Models with Prediction Services</em>, being able to store a smaller version of the data can reduce system load and provide an easier way to process incoming data into a form that can be understood by a predictive model. We can quickly extract the few components we need, for example, from a new piece of text data, without having to persist the entire record.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec37"/>Case Study: Training a Recommender System in PySpark</h1></div></div></div><p>To close this <a id="id441" class="indexterm"/>chapter, let us look at an example of how we might generate a large-scale recommendation system using dimensionality reduction. The dataset we will work with comes from a set of user transactions from an online store (Chen, Daqing, Sai Laing Sain, and Kun Guo. <em>Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining</em>. Journal of Database Marketing &amp; Customer Strategy Management 19.3 (2012): 197-208). In this model, we will input a matrix in which the rows are users and the columns represent items in the catalog of an e-commerce site. Items purchased by a user are indicated by a 1. Our goal is to factorize this matrix into 1 x k <em>user factors</em> (row components) and k x 1 <em>item factors</em> (column components) using k components. Then, presented with a new user and their purchase history, we can predict what items they are like to buy in the future, and thus what we might recommend to them on a homepage. The steps to do so are as follows:</p><div><ol class="orderedlist arabic"><li class="listitem">Consider a user's prior purchase history as <em>a</em> vector <em>p</em>. We imagine this vector is the product of an unknown <em>user factor</em> component <em>u</em> with the item factors <em>i</em> we obtained through matrix factorization: each element of the vector <em>p</em> is then the dot product of this unknown user factor with the item factor for a given item. Solve for the unknown user factor <em>u</em> in the equation:<div><img src="img/B04881_06_12.jpg" alt="Case Study: Training a Recommender System in PySpark"/></div><p>Given the item factors <em>i</em> and the purchase history <em>p</em>, using matrix.Use the resulting user factor <em>u</em>, take the dot product with each item factor to obtain and sort by the result to determine a list of the top ranked items.</p></li></ol></div><p>Now that we have described what is happening <em>under the hood</em> in this example, we can begin to parse this data using the following commands. First, we create a parsing function to read the 2nd and 7th columns of the data containing the item ID and user ID, respectively:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; def parse_data(line):</strong>
<strong>…     try: </strong>
<strong>…         line_array = line.split(',')</strong>
<strong>…      return (line_array[6],line_array[1]) # user-term pairs</strong>
<strong>…      except:</strong>
<strong>…         return None</strong>
</pre></div><p>Next, we read in the file and convert the user and item IDs, which are both string, into a numerical index by incrementing a counter as we add unique items to a dictionary:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; f = open('Online Retail.csv',encoding="Windows-1252")</strong>
<strong>&gt;&gt;&gt; purchases = []</strong>
<strong>&gt;&gt;&gt; users = {}</strong>
<strong>&gt;&gt;&gt; items = {}</strong>
<strong>&gt;&gt;&gt;user_index = 0</strong>
<strong>&gt;&gt;&gt;item_index = 0</strong>
<strong>&gt;&gt;&gt;for index, line in enumerate(f):</strong>
<strong>…    if index &gt; 0: # skip header</strong>
<strong>…         purchase = parse_data(line)</strong>
<strong>…         if purchase is not None:</strong>
<strong> …            if users.get(purchase[0],None) is not None:</strong>
<strong> …                purchase_user = users.get(purchase[0])</strong>
<strong> …            else:</strong>
<strong> …                users[purchase[0]] = user_index</strong>
<strong> …                user_index += 1</strong>
<strong> …                purchase_user = users.get(purchase[0])</strong>
<strong> …            if items.get(purchase[1],None) is not None:</strong>
<strong> …               purchase_item = items.get(purchase[1])</strong>
<strong>…             else:</strong>
<strong> …                items[purchase[1]] = item_index</strong>
<strong> …                item_index += 1</strong>
<strong> …                purchase_item = items.get(purchase[1])</strong>
<strong>  …           purchases.append((purchase_user,purchase_item))&gt;&gt;&gt;f.close()</strong>
</pre></div><p>Next, we <a id="id442" class="indexterm"/>convert the resulting array of purchase into an <code class="literal">rdd</code> and convert the resulting entries into Rating objects  -- a (user, item, rating) tuple. Here, we will just indicate that the purchase occurred by giving a rating of 1.0 to all observed purchases, but we could just as well have a system where the ratings indicate user preference (such as movie ratings) and follow a numerical scale.</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; purchasesRdd = sc.parallelize(purchases,5).map(lambda x: Rating(x[0],x[1],1.0))</strong>
</pre></div><p>Now we can fit the matrix factorization model using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating</strong>

<strong>&gt;&gt;&gt; k = 10</strong>
<strong>&gt;&gt;&gt; iterations = 10</strong>
<strong>&gt;&gt;&gt; mfModel = ALS.train(purchasesRdd, k, iterations)</strong>
</pre></div><p>The algorithm <a id="id443" class="indexterm"/>for matrix factorization used in PySpark is <a id="id444" class="indexterm"/>
<strong>Alternating Least Squares</strong> (<strong>ALS</strong>), which has parameters for the number of row (column) components chosen (<code class="literal">k</code>) and a regularization parameter λ which we did not specify here, but functions similarly to its role in the regression algorithms we studied in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em>, by constraining the values in the row (column) vectors from becoming too large and potentially causing overfitting.</p><p>We could try several values of k and λ, and measure the mean squared error between the observed and predicted matrix (from multiplying the row factors by the column factors) to determine the optimal values.</p><p>Once we have obtained a good fit, we can use the <code class="literal">predict</code> and <code class="literal">predictAll</code> methods of the model object to obtain predictions for new users, and the persist it on disk using the <code class="literal">save</code> method.</p></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec38"/>Summary</h1></div></div></div><p>In this chapter, we have examined complex, unstructured data. We cleaned and tokenized text and examined several ways of extracting features from documents in a way that could be incorporated into predictive models such as n-grams and tf-idf scores. We also examined dimensionality reduction techniques, such as the HashingVectorizer, matrix decompositions, such as PCA, CUR, NMF, and probabilistic models, such as LDA. We also examined image data, including normalization and thresholding operations, and how we can use dimensionality reduction techniques to find common patterns among images. Finally, we used a matrix factorization algorithm to prototype a recommender system in PySpark.</p><p>In the next section, you will also look at image data, but in a different context: trying to capture complex features from these data using sophisticated deep learning models.</p></div></div>
</body></html>