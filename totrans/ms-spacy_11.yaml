- en: 'Chapter 8: Text Classification with spaCy'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章：使用 spaCy 进行文本分类
- en: 'This chapter is devoted to a very basic and popular task of NLP: text classification.
    You will first learn how to train spaCy''s text classifier component, `TextCategorizer`.
    For this, you will learn how to prepare data and feed the data to the classifier;
    then we''ll proceed to train the classifier. You''ll also practice your new `TextCategorizer`
    skills on a popular dataset for sentiment analysis.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章致力于一个非常基础且流行的自然语言处理任务：文本分类。您将首先学习如何训练 spaCy 的文本分类组件 `TextCategorizer`。为此，您将学习如何准备数据并将数据输入到分类器中；然后我们将继续训练分类器。您还将在一个流行的情感分析数据集上练习您的
    `TextCategorizer` 技能。
- en: Next, you will also do text classification with the popular framework TensorFlow's
    Keras API together with spaCy. You will learn the basics of neural networks, sequential
    data modeling with LSTMs, and how to prepare text for machine learning tasks with
    Keras's text preprocessing module. You will also learn how to design a neural
    network with `tf.keras`.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您还将使用流行的框架 TensorFlow 的 Keras API 与 spaCy 一起进行文本分类。您将学习神经网络的基础知识、使用 LSTM
    对序列数据进行建模，以及如何使用 Keras 的文本预处理模块准备文本以进行机器学习任务。您还将学习如何使用 `tf.keras` 设计神经网络。
- en: Following that, we will then make an end-to-end text classification experiment,
    from data preparation to preprocessing text with Keras `Tokenizer`, neural network
    designing, model training, and interpreting the classification results. That's
    a whole package of machine learning!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，我们将进行一个端到端的文本分类实验，从数据准备到使用 Keras `Tokenizer` 预处理文本，再到设计神经网络、模型训练以及解释分类结果。这是一整套机器学习的内容！
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Understanding the basics of text classification
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解文本分类的基础知识
- en: Training the spaCy text classifier
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练 spaCy 文本分类器
- en: Sentiment analysis with spaCy
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 spaCy 进行情感分析
- en: Text classification with spaCy and Keras
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 spaCy 和 Keras 进行文本分类
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code in the sections *Training the spaCy text classifier* and *Sentiment
    analysis with spaCy* is spaCy v3.0 compatible.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *训练 spaCy 文本分类器* 和 *使用 spaCy 进行情感分析* 这两部分的代码与 spaCy v3.0 兼容。
- en: 'The section *Text classification with spaCy and Keras* requires the following
    Python libraries:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *使用 spaCy 和 Keras 进行文本分类* 这一部分中，需要以下 Python 库：
- en: TensorFlow >=2.2.0
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow >=2.2.0
- en: NumPy
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: pandas
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas
- en: Matplotlib
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib
- en: 'You can install the latest version of these libraries with `pip` as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令使用 `pip` 安装这些库的最新版本：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We also use Jupyter notebooks in the last two sections. You can follow the instructions
    on the Jupyter website ([https://jupyter.org/install](https://jupyter.org/install))
    to install the Jupyter notebook onto your system. If you don't want to use notebooks,
    you can copy-paste code as Python code as well.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在最后两节中使用了 Jupyter 笔记本。您可以根据 Jupyter 网站的说明（[https://jupyter.org/install](https://jupyter.org/install)）将
    Jupyter 笔记本安装到您的系统上。如果您不想使用笔记本，也可以将代码复制粘贴为 Python 代码。
- en: You can find the chapter code and data files in the book's GitHub repository
    at [https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter08](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter08).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书的 GitHub 仓库中找到本章的代码和数据文件，网址为 [https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter08](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter08)。
- en: Let's get started with spaCy's text classifier component first, then we'll transition
    to designing our own neural network.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从 spaCy 的文本分类组件开始，然后我们将过渡到设计我们自己的神经网络。
- en: Understanding the basics of text classification
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解文本分类的基础知识
- en: Text classification is the task of assigning a set of predefined labels to text.
    Given a set of predefined classes and some text, you want to understand which
    predefined class this text falls into. We have to determine the classes ourselves
    by the nature of our data before starting the classification task. For example,
    a customer review can be positive, negative, or neutral.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类是将一组预定义的标签分配给文本的任务。给定一组预定义的类别和一些文本，您想要了解该文本属于哪个预定义类别。在开始分类任务之前，我们必须根据数据的性质自行确定类别。例如，客户评论可以是积极的、消极的或中性的。
- en: Text classifiers are used for detecting spam emails in your mailbox, determining
    the sentiment of customer's reviews, understanding customer's intent, sorting
    customer's complaint tickets, and so on.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类器用于检测邮箱中的垃圾邮件、确定客户评论的情感、理解客户的意图、对客户投诉工单进行分类等等。
- en: Text classification is a fundamental task of NLP. It is gaining importance in
    the business world, as it enables businesses to automate their processes. One
    immediate example is spam filters. Every day, users receive many spam emails but
    most of the time never see these emails and don't get any notifications because
    spam filters save the users from bothering about irrelevant emails and from spending
    time deleting these emails.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类是NLP的基本任务。它在商业世界中越来越重要，因为它使企业能够自动化其流程。一个直接的例子是垃圾邮件过滤器。每天，用户都会收到许多垃圾邮件，但大多数时候用户从未看到这些邮件，也没有收到任何通知，因为垃圾邮件过滤器帮助用户免受无关邮件的打扰，并节省了删除这些邮件的时间。
- en: 'Text classifiers can come in different flavors. Some classifiers focus on the
    overall emotion of the text, some classifiers focus on detecting the language
    of the text, and some classifiers focus on only some words of the text, such as
    verbs. The following are some of the most common types of text classification
    and their use cases:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类器可以有不同的类型。一些分类器关注文本的整体情感，一些分类器关注检测文本的语言，还有一些分类器只关注文本中的某些单词，例如动词。以下是一些最常见的文本分类类型及其用例：
- en: '**Topic detection**: Topic detection is the task of understanding the topic
    of a given text. For example, the text in a customer email could be asking about
    a refund, asking for a past bill, or simply complaining about the customer service.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题检测**：主题检测是理解给定文本主题的任务。例如，客户电子邮件中的文本可能是询问退款、请求过去的账单，或者仅仅是抱怨客户服务。'
- en: '**Sentiment analysis**: Sentiment analysis is the task of understanding whether
    the text contains positive or negative emotions about a given subject. Sentiment
    analysis is used often to analyze customer reviews about products and services.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**：情感分析是理解文本是否包含关于给定主题的积极或消极情绪的任务。情感分析常用于分析产品和服务客户评价。'
- en: '**Language detection**: Language detection is the first step of many NLP systems,
    such as machine translation.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言检测**：语言检测是许多NLP系统（如机器翻译）的第一步。'
- en: 'The following figure shows a text classifier for a customer service automation
    system:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个客户服务自动化系统的文本分类器：
- en: '![Figure 8.1 – Topic detection is used to label a customer complaint with a
    predefined label'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.1 – 主题检测用于使用预定义标签标记客户投诉'
- en: '](img/B16570_8_1.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_8_1.jpg)'
- en: Figure 8.1 – Topic detection is used to label a customer complaint with a predefined
    label
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 主题检测用于使用预定义标签标记客户投诉
- en: 'Coming to the technical details, text classification is a *supervised* machine
    learning task. It means that the classifier can predict the class label of a text
    based on *example*input text-class label pairs. Hence, to train a text classifier,
    we need a *labeled dataset*. A labeled dataset is basically a list of text-label
    pairs. Here is an example dataset of five training sentences with their labels:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在技术细节方面，文本分类是一个*监督学习*任务。这意味着分类器可以根据*示例*输入文本-类别标签对来预测文本的类别标签。因此，为了训练文本分类器，我们需要一个*标记数据集*。标记数据集基本上是一系列文本-标签对。以下是一个包含五个训练句子及其标签的示例数据集：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then we train the classifier by showing the text and the corresponding class
    labels to the classifier. When the classifier sees new text that was not in the
    training text, it then predicts the class label of this unseen text based on the
    examples it saw during the training phase. The output of a text classifier is
    *always* a class label.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过向分类器展示文本和相应的类别标签来训练分类器。当分类器看到训练文本中没有的新文本时，它就会根据训练阶段看到的例子预测这个未见文本的类别标签。文本分类器的输出*总是*一个类别标签。
- en: 'Text classification can also be divided into three categories depending on
    the number of classes used:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据使用的类别数量，文本分类也可以分为三个类别：
- en: '**Binary text classification** means that we want to categorize our text into
    two classes.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二元文本分类**意味着我们希望将文本分类为两个类别。'
- en: '**Multiclass text classification** means that there are more than two classes.
    Each class is mutually exclusive – one text can belong to one class only. Equivalently,
    a training instance can be labeled with only one class label. An example is rating
    customer reviews. A review can have 1, 2, 3, 4, or 5 stars (each star category
    is a class).'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多类文本分类**意味着存在超过两个类别。每个类别是互斥的——一个文本只能属于一个类别。等价地，一个训练实例只能被标记为一个类别标签。例如，对客户评价进行评级。评价可以有1，2，3，4或5颗星（每个星级类别是一个类别）。'
- en: '**Multilabel text classification** is a generalization of multiclass classification,
    where multiple labels can be assigned to each example text. For example, classifying
    toxic social media messages is done with multiple labels. This way, our model
    can distinguish different levels of toxicity. Class labels are typically toxic,
    severe toxic, insult, threat, obscenity. A message can include both insults and
    threats, or be classed as insult, toxicity, and obscenity, and so on. Hence for
    this problem, using multiple classes is more suitable.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多标签文本分类**是多类分类的推广，其中可以为每个示例文本分配多个标签。例如，使用多个标签对有毒社交媒体消息进行分类。这样，我们的模型可以区分不同级别的毒性。类标签通常是毒性、严重毒性、侮辱、威胁、淫秽。一条消息可以包含侮辱和威胁，或者被归类为侮辱、毒性和淫秽等。因此，对于这个问题，使用多个类别更合适。'
- en: 'Labels are the name of the classes we want to see as the output. A class label
    can be categorical (string) or numerical (a number). Here are some commonly used
    class labels:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 标签是我们希望作为输出看到的类别的名称。一个类标签可以是分类的（字符串）或数字的（一个数字）。以下是一些常用的类标签：
- en: For sentiment analysis, we usually use the class labels positive and negative.
    Their abbreviations, pos and neg, are also commonly used. Binary class labels
    are popular as well – 0 means negative sentiment and 1 means positive sentiment.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于情感分析，我们通常使用正负类标签。它们的缩写，pos 和 neg，也常被使用。二元类标签也很受欢迎 - 0 表示负面情感，1 表示正面情感。
- en: The same applies to binary classification problems. We usually use 0-1 for class
    labels.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这同样适用于二元分类问题。我们通常用 0-1 作为类标签。
- en: For multiclass and multilabel problems, we usually name the classes with a meaningful
    name. For a movie genre classifier, we can use the labels family, international,
    Sunday evening, Disney, action, and so on. Numbers are used as labels as well.
    For a five-class classification problem, we can use the labels 1, 2, 3, 4, and
    5.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于多类和多标签问题，我们通常用有意义的名称命名类别。对于一个电影类型分类器，我们可以使用家庭、国际、周日晚上、迪士尼、动作等标签。数字也用作标签。对于一个五类分类问题，我们可以使用标签
    1、2、3、4 和 5。
- en: Now we've covered the basic concepts of text classification, let's do some coding!
    In the next section, we'll explore how to train spaCy's text classifier component.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了文本分类的基本概念，让我们来做一些编码！在下一节中，我们将探讨如何训练 spaCy 的文本分类器组件。
- en: Training the spaCy text classifier
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 spaCy 文本分类器
- en: In this section, we will learn about the details of spaCy's text classifier
    component `TextCategorizer`. In [*Chapter 2*](B16570_02_Final_JM_ePub.xhtml#_idTextAnchor037),
    *Core Operations with spaCy*, we saw that the spaCy NLP pipeline consists of components.
    In [*Chapter 3*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055), *Linguistic
    Features*, we learned about the essential components of the spaCy NLP pipeline,
    which are the sentence tokenizer, POS tagger, dependency parser, and **named entity
    recogition** (**NER**).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解 spaCy 文本分类器组件 `TextCategorizer` 的详细信息。在 [*第 2 章*](B16570_02_Final_JM_ePub.xhtml#_idTextAnchor037)，*spaCy
    的核心操作* 中，我们了解到 spaCy NLP 管道由组件组成。在 [*第 3 章*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)，*语言特征*
    中，我们学习了 spaCy NLP 管道的核心组件，包括句子分词器、词性标注器、依存句法分析器和 **命名实体识别**（**NER**）。
- en: '`TextCategorizer` is an optional and trainable pipeline component. In order
    to train it, we need to provide examples and their class labels. We first add
    `TextCategorizer` to the NLP pipeline and then do the training procedure. *Figure
    8.2* shows where exactly the `TextCategorizer` component lies in the NLP pipeline;
    this component comes after the essential components. In the following diagram,
    `TextCategorizer` component.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`TextCategorizer` 是一个可选的可训练管道组件。为了训练它，我们需要提供示例及其类别标签。我们首先将 `TextCategorizer`
    添加到 NLP 管道中，然后进行训练过程。*图 8.2* 展示了 `TextCategorizer` 组件在 NLP 管道中的确切位置；该组件位于基本组件之后。在以下图中，`TextCategorizer`
    组件。'
- en: '![Figure 8.2 – TextCategorizer in the nlp pipeline'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 8.2 – TextCategorizer in the nlp pipeline'
- en: '](img/B16570_8_2.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_8_2.jpg]'
- en: Figure 8.2 – TextCategorizer in the nlp pipeline
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 8.2 – TextCategorizer in the nlp pipeline
- en: A neural network architecture lies behind spaCy's `TextCategorizer`. `TextCategorizer`
    provides us with user-friendly and end-to-end approaches to train the classifier,
    so we don't have to deal directly with the neural network architecture. We'll
    design our own neural network architecture in the upcoming *Text classification
    with spaCy and Keras* section. After looking at the architecture, we’re ready
    to dive into TextCategorizer code. Let’s get to know `TextCategorizer` class first.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy的`TextCategorizer`背后是一个神经网络架构。`TextCategorizer`为我们提供了用户友好的端到端方法来训练分类器，因此我们不必直接处理神经网络架构。在接下来的*使用spaCy和Keras进行文本分类*部分，我们将设计自己的神经网络架构。在查看架构之后，我们将深入到`TextCategorizer`代码中。首先让我们了解`TextCategorizer`类。
- en: Getting to know TextCategorizer class
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解TextCategorizer类
- en: 'Now let''s get to know the `TextCategorizer` class in detail. First of all,
    we import `TextCategorizer` from the pipeline components:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们详细了解`TextCategorizer`类。首先，我们从管道组件中导入`TextCategorizer`：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`TextCategorizer` is available in two flavors, single-label classifier and
    multilabel classifier. As we remarked in the previous section, a multilabel classifier
    can predict more than one class. A single-label classifier predicts only one class
    for each example and classes are mutually exclusive. The preceding `import` line
    imports the single-label classifier and the following code imports the multilabel
    classifier:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`TextCategorizer`有两种形式，单标签分类器和多标签分类器。正如我们在上一节中提到的，多标签分类器可以预测多个类别。单标签分类器对每个示例只预测一个类别，且类别是互斥的。前面的`import`行导入单标签分类器，接下来的代码导入多标签分类器：'
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, we need to provide a configuration to the `TextCategorizer` component.
    We provide two parameters here, a threshold value and a model name (either `Single`
    or `Multi` depending on the classification task). `TextCategorizer` internally
    generates a probability for each class and a class is assigned to the text if
    the probability of this class is higher than the threshold value.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要为`TextCategorizer`组件提供一个配置。我们在这里提供两个参数，一个阈值值和一个模型名称（根据分类任务，可以是`Single`或`Multi`）。`TextCategorizer`内部为每个类别生成一个概率，如果一个类别的概率高于阈值值，则将该类别分配给文本。
- en: A traditional threshold value for text classification is `0.5`, however, if
    you want to make a prediction with higher confidence, you can make the threshold
    higher, such as 0.6, 0.7, or 0.8.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类的传统阈值是`0.5`，然而，如果您想做出更有信心的预测，可以将阈值提高，例如0.6、0.7或0.8。
- en: 'Putting it altogether, we can add a single-label `TextCategorizer` component
    to the `nlp` pipeline as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容整合起来，我们可以将单标签`TextCategorizer`组件添加到`nlp`管道中，如下所示：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Adding a multilabel component to the `nlp` pipeline is similar:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 将多标签组件添加到`nlp`管道中类似：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the last line of each of the preceding code blocks, we added a `TextCategorizer`
    pipeline component to the nlp pipeline object. The newly created `TextCategorizer`
    component is captured by the `textcat` variable. We're ready to train the `TextCategorizer`
    component now. The training code looks quite similar to the NER component training
    code from [*Chapter 7*](B16570_07_Final_JM_ePub.xhtml#_idTextAnchor120)*, Customizing
    spaCy Models*, except for some minor details.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面每个代码块的最后一行，我们向`nlp`管道对象添加了一个`TextCategorizer`管道组件。新创建的`TextCategorizer`组件被`textcat`变量捕获。我们现在可以开始训练`TextCategorizer`组件了。训练代码看起来与[*第7章*](B16570_07_Final_JM_ePub.xhtml#_idTextAnchor120)*，自定义spaCy模型*中的NER组件训练代码非常相似，除了一些细节上的不同。
- en: Formatting training data for the TextCategorizer
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 格式化TextCategorizer的训练数据
- en: 'Let''s start our code by preparing a small training set. We''ll prepare a customer
    sentiment dataset for binary text classification. The label will be called `sentiment`
    and can obtain two possible values, 0 and 1 corresponding to negative and positive
    sentiment. The following training set contains 6 examples, 3 being positive and
    3 being negative:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从准备一个小型训练集开始我们的代码。我们将准备一个客户情感数据集用于二元文本分类。标签将称为`sentiment`，可以获取两个可能的值，0和1分别对应负面和正面情感。以下训练集包含6个示例，其中3个是正面的，3个是负面的：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Each training example is a tuple of a text and a nested dictionary. The dictionary
    contains the class label in a format that spaCy recognizes. The `cts` field means
    the categories. Then we include the class label sentiment and its value. The value
    should always be a floating-point number.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 每个训练示例都是一个文本和嵌套字典的元组。字典包含spaCy能识别的类标签格式。`cts`字段表示类别。然后我们包括类标签的情感及其值。该值始终应该是浮点数。
- en: 'In the code, we will introduce the class label we choose to the `TextCategorizer`
    component. Let''s see the complete code. First, we do the necessary imports:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们将我们选择的类别标签引入到`TextCategorizer`组件中。让我们看看完整的代码。首先，我们进行必要的导入：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We imported the built-in library `random` for shuffling our dataset. We imported
    `spacy` as usual, and we imported `Example` to prepare the training examples in
    spaCy format. In the last line of the code block, we imported a text categorizer
    model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入了内置库`random`来打乱我们的数据集。我们像往常一样导入了`spacy`，并导入了`Example`来准备spaCy格式的训练示例。在代码块的最后一行，我们导入了文本分类器模型。
- en: 'Next, we''ll do the pipeline and `TextCategorizer` component initialization:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进行管道和`TextCategorizer`组件的初始化：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, we'll do some work on the newly created `TextCategorizer` component, `textcat`.
    We'll introduce our label `sentiment` to the `TextCategorizer` componenet by calling
    `add_label`. Then, we need to initialize this component with our examples. This
    step is different than what we did in the NER training code in [*Chapter 7*](B16570_07_Final_JM_ePub.xhtml#_idTextAnchor120)*,
    Customizing spaCy Models*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将对新建的`TextCategorizer`组件`textcat`做一些工作。我们将通过调用`add_label`将我们的标签`sentiment`引入到`TextCategorizer`组件中。然后，我们需要用我们的示例初始化这个组件。这一步与我们在[*第7章*](B16570_07_Final_JM_ePub.xhtml#_idTextAnchor120)*自定义spaCy模型*中NER训练代码所做的不一样。
- en: 'The reason is NER is an essential component, hence it''s initialized by the
    pipeline always. TextCategorizer is an optional component, and it comes as a blank
    statistical model. The following code adds our label to the `TextCategorizer`
    component and then initializes the `TextCategorizer` model''s weights with the
    training examples:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是命名实体识别（NER）是一个基本组件，因此管道总是初始化它。`TextCategorizer`是一个可选组件，它作为一个空白统计模型提供。以下代码将我们的标签添加到`TextCategorizer`组件中，然后使用训练示例初始化`TextCategorizer`模型的权重：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that we feed the examples to `textcat.initialize` as `Example` objects.
    Recall from [*Chapter 7*](B16570_07_Final_JM_ePub.xhtml#_idTextAnchor120), *Customizing
    spaCy Models*, that spaCy training methods always work with `Example` objects.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将示例作为`Example`对象传递给`textcat.initialize`。回想一下[*第7章*](B16570_07_Final_JM_ePub.xhtml#_idTextAnchor120)，*自定义spaCy模型*，spaCy的训练方法始终与`Example`对象一起工作。
- en: Defining the training loop
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义训练循环
- en: 'We''re ready to define the training loop. First of all, we''ll disable other
    pipe components so that only `textcat` will be trained. Second, we will create
    an optimizer object by calling `resume_training`, keeping the weights of the existing
    statistical models. For each epoch, we go over training examples one by one and
    update the weights of `textcat`. We go over the data for 20 epochs. The following
    code defines the training loop:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备定义训练循环。首先，我们将禁用其他管道组件，以便只训练`textcat`。其次，我们将通过调用`resume_training`创建一个优化器对象，保留现有统计模型的权重。对于每个epoch，我们将逐个遍历训练示例并更新`textcat`的权重。我们遍历数据20个epochs。以下代码定义了训练循环：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'That''s it! With this relatively short code segment, we trained a text classifier!
    Here''s the output on my machine (your loss values might be different):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是全部了！用这段相对简短的代码片段，我们训练了一个文本分类器！以下是我机器上的输出（你的损失值可能不同）：
- en: '![Figure 8.3 – Loss values at each epoch'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.3 – 每个epoch的损失值'
- en: '](img/B16570_8_3.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_8_3.jpg)'
- en: Figure 8.3 – Loss values at each epoch
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 每个epoch的损失值
- en: Testing the new component
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试新组件
- en: 'Let''s test the new text categorizer component. The `doc.cats` property holds
    the class labels:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试新的文本分类器组件。`doc.cats`属性持有类别标签：
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Great! Our small dataset successfully trained the spaCy text classifier for
    a binary text classification problem, indeed a sentiment analysis task. Now, we'll
    see how to do multilabel classification with spaCy's `TextCategorizer`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们的小数据集成功训练了spaCy文本分类器，用于二元文本分类问题，确实是一个情感分析任务。现在，我们将看看如何使用spaCy的`TextCategorizer`进行多标签分类。
- en: Training TextCategorizer for multilabel classification
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为多标签分类训练TextCategorizer
- en: Recall from the first section that multilabel classification means the classifier
    can predict more than one label for an example text. Naturally, the classes are
    not mutually exclusive at all. In order to train a multilabel classifier, we need
    to provide a dataset that contains examples that have more than one label.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从第一部分回忆，多标签分类意味着分类器可以为示例文本预测多个标签。自然地，这些类别根本不是互斥的。为了训练一个多标签分类器，我们需要提供一个包含具有多个标签的示例的数据库。
- en: 'To train spaCy''s `TextCategorizer` for multilabel classification, we''ll again
    start by building a small training set. This time, we''ll form a set of movie
    reviews, where the labels are `FAMILY,` `THRILLER`, and `SUNDAY_EVENING`. Here
    is our small dataset:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要为多标签分类训练 spaCy 的 `TextCategorizer`，我们再次从构建一个小型训练集开始。这次，我们将形成一组电影评论，标签为 `FAMILY`、`THRILLER`
    和 `SUNDAY_EVENING`。以下是我们的小型数据集：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We provided some examples with one label, such as the first example (the first
    sentence of `train_data`, the second line of the preceding code block), and we
    also provided examples with more than one label, such as the fourth example of
    the `train_data`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了一些只有一个标签的示例，例如第一个示例（`train_data` 的第一句话，上一代码块的第二行），我们还提供了具有多个标签的示例，例如 `train_data`
    的第四个示例。
- en: 'We''ll make the imports after we''ve formed the training set:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在形成训练集之后进行导入：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, the last line is different than the code of the previous section. We imported
    the multilabel model instead of the single-label model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，最后一行与上一节中的代码不同。我们导入了多标签模型而不是单标签模型。
- en: 'Next, we add the multilabel classifier component to the nlp pipeline. Again,
    pay attention to the pipeline component name – this time, it is `textcat_multilabel`,
    compared to the previous section''s `textcat`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将多标签分类器组件添加到 nlp 管道中。再次注意管道组件的名称——这次是 `textcat_multilabel`，与上一节的 `textcat`
    相比：
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Adding the labels to the `TextCategorizer` component and initializing the model
    is similar to the *Training the spaCy text classifier* section. This time, we''ll
    add three labels instead of one:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将标签添加到 `TextCategorizer` 组件并初始化模型与 *训练 spaCy 文本分类器* 部分类似。这次，我们将添加三个标签而不是一个：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We''re ready to define the training loop. The code functions are similar to
    the previous section''s code. The only difference is the component name in the
    first line. Now it''s `textcat_multilabel`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好定义训练循环。代码函数与上一节的代码类似。唯一的区别是第一行中的组件名称。现在它是 `textcat_multilabel`：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output should look similar to the output of the previous section, a loss
    value per epoch. Now, let''s test our brand new multilabel classifier:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该类似于上一节的输出，每个 epoch 的损失值。现在，让我们测试我们全新的多标签分类器：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Notice that each label admitted a positive probability at the output. Also,
    the probabilities do not sum up to 1, because they're not mutually exclusive.
    For this example, the `SUNDAY_EVENING` and `THRILLER` label probabilities are
    predicted correctly, but the `FAMILY` label probability does not look ideal. This
    is mainly due to the fact that we didn't provide enough examples. Usually, for
    multilabel classification problems, the classifier needs more examples than binary
    classification since the classifier needs to learn more labels.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到每个标签在输出中都存在一个正概率。而且，这些概率的总和并不等于 1，因为它们不是互斥的。在这个例子中，`SUNDAY_EVENING` 和 `THRILLER`
    标签的概率预测是正确的，但 `FAMILY` 标签的概率看起来并不理想。这主要是因为我们没有提供足够的例子。通常，对于多标签分类问题，分类器需要比二分类更多的例子，因为分类器需要学习更多的标签。
- en: We've learned how to train spaCy's `TextCategorizer` component for binary text
    classification and multilabel text classification. Now, we'll train `TextCategorizer`
    on a real-world dataset for a sentiment analysis problem.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了如何训练 spaCy 的 `TextCategorizer` 组件进行二进制文本分类和多标签文本分类。现在，我们将在一个真实世界的数据集上训练
    `TextCategorizer` 以进行情感分析问题。
- en: Sentiment analysis with spaCy
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 spaCy 进行情感分析
- en: In this section, we'll work on a real-world dataset and train spaCy's `TextCategorizer`
    on this dataset. We'll be working on the Amazon Fine Food Reviews dataset ([https://www.kaggle.com/snap/amazon-fine-food-reviews](https://www.kaggle.com/snap/amazon-fine-food-reviews))
    from Kaggle in this chapter. The original dataset is huge, with 100,000 rows.
    We sampled 4,000 rows. This dataset contains customer reviews about fine food
    sold on Amazon. Reviews include user and product information, user rating, and
    text.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将处理一个真实世界的数据集，并在该数据集上训练 spaCy 的 `TextCategorizer`。在本章中，我们将使用 Kaggle 上的
    Amazon Fine Food Reviews 数据集（[https://www.kaggle.com/snap/amazon-fine-food-reviews](https://www.kaggle.com/snap/amazon-fine-food-reviews)）。原始数据集非常大，有
    10 万行。我们采样了 4,000 行。这个数据集包含了关于在亚马逊上销售的精致食品的客户评论。评论包括用户和产品信息、用户评分和文本。
- en: 'You can download the dataset from the book''s GitHub repository. Type the following
    command into your terminal:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从本书的 GitHub 仓库下载数据集。在您的终端中输入以下命令：
- en: '[PRE18]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Alternatively, you can click on the URL in the preceding command and the download
    will start. You can unzip the zip file with the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以点击前面的命令中的URL，下载将开始。您可以使用以下方法解压zip文件：
- en: '[PRE19]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Alternatively, you can right-click on the ZIP file and choose **Extract here**
    to inflate the ZIP file.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以在ZIP文件上右键单击并选择**Extract here**来解压ZIP文件。
- en: Exploring the dataset
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索数据集
- en: Now, we're ready to explore the dataset. In this section, we'll be using a Jupyter
    notebook. If you already have Jupyter installed, you can execute the notebook
    cells directly. If you don't have the Jupyter Notebook on your system, you can
    follow the instructions on the Jupyter website ([https://jupyter.org/install](https://jupyter.org/install)).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好探索数据集了。在本节中，我们将使用Jupyter笔记本。如果您已经安装了Jupyter，您可以直接执行笔记本单元格。如果您系统上没有Jupyter
    Notebook，您可以按照Jupyter网站上的说明进行操作（[https://jupyter.org/install](https://jupyter.org/install)）。
- en: 'Let''s do our dataset exploration step by step:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地进行数据集探索：
- en: 'First, we''ll do the imports for reading and visualizing the dataset:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将进行读取和可视化数据集的导入：
- en: '[PRE20]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We''ll read the CSV file into a pandas DataFrame and output the shape of the
    DataFrame:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将读取CSV文件到pandas DataFrame中，并输出DataFrame的形状：
- en: '[PRE21]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we examine the rows and the columns of the dataset by printing the first
    10 rows:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通过打印前10行来检查数据集的行和列：
- en: '[PRE22]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The resulting view tells us there are 10 rows including the review text and
    the review score:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成的视图告诉我们有10行，包括评论文本和评论评分：
- en: '![ Figure 8.4 – First 10 rows of the reviews dataframe'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.4 – 评论数据框的前10行'
- en: '](img/B16570_8_4.jpg)'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16570_8_4.jpg)'
- en: Figure 8.4 – First 10 rows of the reviews dataframe
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.4 – 评论数据框的前10行
- en: 'We''ll be using the `Text` and `Score` columns; hence, we''ll drop the other
    columns that we won''t use. We’ll also call the dropna() method to drop the rows
    with missing values::'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '我们将使用`Text`和`Score`列；因此，我们将删除其他不会使用的列。我们还将调用`dropna()`方法来删除包含缺失值的行::'
- en: '[PRE23]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can have a quick look at how the review scores are distributed:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以快速查看评论评分的分布：
- en: '[PRE24]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This piece of code calls the `plot` method of `dataframe reviews_df` and exhibits
    a bar plot:![Figure 8.5 – Distribution of review scores
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码调用了`dataframe reviews_df`的`plot`方法，并展示了一个条形图：![图8.5 – 评论评分的分布
- en: '](img/B16570_8_5.jpg)'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16570_8_5.jpg)'
- en: Figure 8.5 – Distribution of review scores
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.5 – 评论评分的分布
- en: The number of 5-star ratings is quite high; it looks like customers are happy
    with the food they purchased. However, it might create an imbalance in the training
    data if a class has significantly more weight than the others.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 5星评价的数量相当高；看起来顾客对购买的食品很满意。然而，如果一个类别比其他类别有显著更多的权重，这可能会在训练数据中造成不平衡。
- en: '**Class imbalance** creates trouble for classification algorithms in general.
    For example, it is considered as imbalance when a class has significantly more
    training examples than the other classes (usually a ratio of 1:5 between examples).
    There are different ways to handle imbalance, one way is **up**-**sampling**/**down**-**sampling**.
    In down-sampling, we randomly remove training examples from the majority class.
    In up-sampling, we randomly replicate training example from the minority class.
    Both methods aim to balance the number of training examples of majority and minority
    classes.'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**类别不平衡**通常会给分类算法带来麻烦。例如，当一个类别比其他类别有显著更多的训练示例时，这被认为是不平衡（通常示例之间的比例为1:5）。处理不平衡的方法有很多，其中一种方法是**上采样**/**下采样**。在下采样中，我们从多数类别随机删除训练示例。在上采样中，我们随机复制少数类别的训练示例。这两种方法的目标都是平衡多数和少数类别的训练示例数量。'
- en: Here we’ll apply another method. We’ll combine 1,2,3 star reviews and 4,5 star
    reviews to get a more balanced dataset.
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们将应用另一种方法。我们将合并1星、2星、3星评价和4星、5星评价，以获得一个更平衡的数据集。
- en: 'In order to prevent this, we''ll treat 1-, 2-, and 3-star ratings as negative
    and ratings that have more than 4 stars as positive. The following code segment
    assigns a negative label to all the reviews that have a rating of fewer than 4
    stars and a positive label to all the reviews that have a higher rating than 4
    stars:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了防止这种情况，我们将1星、2星和3星评价视为负面评价，而将超过4星的评价视为正面评价。以下代码段将所有评分少于4星的评论分配为负面标签，将所有评分高于4星的评论分配为正面标签：
- en: '[PRE25]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let''s plot the distribution of the ratings again:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再次绘制评分的分布：
- en: '[PRE26]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The resulting rating distribution looks much better than *Figure 8.5*. Still,
    the number of positive reviews is greater, but the number of negative reviews
    is significant as well, as can be seen from the following graph:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果的评分分布看起来比*图8.5*好得多。尽管如此，正面评论的数量仍然更多，但负面评论的数量也很显著，如下面的图表所示：
- en: '![Figure 8.6 – Distribution of positive and negative ratings'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.6 – 正面和负面评分的分布'
- en: '](img/B16570_8_6.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_8_6.jpg)'
- en: Figure 8.6 – Distribution of positive and negative ratings
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 正面和负面评分的分布
- en: 'After processing the dataset, we reduced it to a two-column dataset with negative
    and positive ratings. We call `reviews_df.head()` once again and the following
    is the result we get:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理完数据集后，我们将其缩减为包含负面和正面评分的两列数据集。我们再次调用`reviews_df.head()`，以下是我们得到的结果：
- en: '![Figure 8.7 – The DataFrame’s first four rows'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.7 – DataFrame的前四行'
- en: '](img/B16570_8_7.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_8_7.jpg)'
- en: Figure 8.7 – The DataFrame's first four rows
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – DataFrame的前四行
- en: We'll finish our dataset exploration here. We saw the distribution of the review
    scores and the class labels. The dataset is now ready to be processed. We dropped
    the unused columns and converted review scores to binary class labels. Let's go
    ahead and start the training procedure!
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里结束数据集的探索。我们看到了评论分数和类别标签的分布。现在数据集已经准备好进行处理了。我们删除了未使用的列，并将评论分数转换为二进制类别标签。让我们继续开始训练过程！
- en: Training the TextClassifier component
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练TextClassifier组件
- en: 'Now, we''re ready to start the training procedure. We''ll train a binary text
    classifier with the multilabel classifier this time. Again, let''s go step by
    step:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好开始训练过程了。这次我们将使用多标签分类器训练一个二进制文本分类器。再次，让我们一步一步来：
- en: 'We start by importing the spaCy classes as follows:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先按照以下方式导入spaCy类：
- en: '[PRE27]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we''ll create a pipeline object, `nlp`, define the classifier configuration,
    and add the `TextCategorizer` component to `nlp` with the following configuration:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个pipeline对象`nlp`，定义分类器配置，并将`TextCategorizer`组件添加到`nlp`中，配置如下：
- en: '[PRE28]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'After creating the text classifier component, we''ll convert training sentences
    and ratings into a spaCy usable format. We''ll iterate every row of the DataFrame
    with `iterrows()`, and for each row we''ll extract the `Text` and `Score` fields.
    Then, we''ll create a spaCy `Doc` object from the review text and make a dictionary
    of the class labels as well. Finally, we will create an `Example` object and append
    it to the list of training examples:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建文本分类器组件后，我们将训练句子和评分转换为spaCy可用的格式。我们将使用`iterrows()`遍历DataFrame的每一行，对于每一行，我们将提取`Text`和`Score`字段。然后，我们将从评论文本创建一个spaCy
    `Doc`对象，并创建一个包含类别标签的字典。最后，我们将创建一个`Example`对象并将其追加到训练示例列表中：
- en: '[PRE29]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We''ll use `POS` and `NEG` labels for positive and negative sentiment, respectively.
    We''ll introduce these labels to the new component and also initialize the component
    with examples:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`POS`和`NEG`标签分别表示正面和负面情感。我们将这些标签引入新组件，并用示例初始化该组件：
- en: '[PRE30]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We''re ready to define the training loop! We went over the training set for
    two epochs, but you can go over more if you like. The following code snippet will
    train the new text categorizer component:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经准备好定义训练循环了！我们遍历了训练集两个epoch，但如果你愿意，可以遍历更多。以下代码片段将训练新的文本分类器组件：
- en: '[PRE31]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Finally, we''ll test how the text classifier component works for two example
    sentences:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将测试文本分类器组件对两个示例句子的处理效果：
- en: '[PRE32]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Both the `NEG` and `POS` labels appear in the prediction result because we used
    the multilabel classifier. The results look good. The first sentence outputs a
    very high positive probability, and the second sentence is predicted as negative
    with a high probability.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用了多标签分类器，`NEG`和`POS`标签都出现在预测结果中。结果看起来不错。第一句话输出了一个非常高的正面概率，第二句话则被预测为负面，概率也很高。
- en: We've completed training spaCy's text classifier component. In the next section,
    we'll dive into the world of a very popular deep learning library, Keras. We'll
    explore how to write Keras code to do text classification by using another popular
    machine learning library – TensorFlow's Keras API. Let's go ahead and explore
    Keras and TensorFlow!
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了spaCy文本分类器组件的训练。在下一节中，我们将深入探讨一个非常流行的深度学习库Keras的世界。我们将探索如何使用另一个流行的机器学习库——TensorFlow的Keras
    API来编写Keras代码进行文本分类。让我们继续探索Keras和TensorFlow！
- en: Text classification with spaCy and Keras
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用spaCy和Keras进行文本分类
- en: In this section, we will learn about methods for blending spaCy with neural
    networks using another very popular Python deep learning library, **TensorFlow**,
    and its high-level API, **Keras**.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用另一个非常流行的Python深度学习库**TensorFlow**及其高级API**Keras**来将spaCy与神经网络结合使用。
- en: '**Deep learning** is a broad family of machine learning algorithms that are
    based on neural networks. **Neural networks** are human brain-inspired algorithms
    that contain connected layers, which are made from neurons. Each neuron is a mathematical
    operation that takes its input, multiplies it by its weights, and then passes
    the sum through the activation function to the other neurons. The following diagram
    shows a neural network architecture with three layers -- the **input layer**,
    **hidden layer**, and **output layer**:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习**是一系列基于神经网络的机器学习算法的统称。**神经网络**是受人类大脑启发的算法，包含相互连接的层，这些层由神经元组成。每个神经元都是一个数学运算，它接收输入，将其与其权重相乘，然后将总和通过激活函数传递给其他神经元。以下图表展示了一个具有三层结构的神经网络架构——**输入层**、**隐藏层**和**输出层**：'
- en: '![Figure 8.8 – A neural network architecture with three layers'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.8 – 具有三层的神经网络架构]'
- en: '](img/B16570_8_8.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_8_8.jpg]'
- en: Figure 8.8 – A neural network architecture with three layers
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – 具有三层的神经网络架构
- en: '**TensorFlow** is an end-to-end open source platform for machine learning.
    TensorFlow might be the most popular deep learning library among research engineers
    and scientists. It has huge community support and great documentation, available
    at [https://www.tensorflow.org/](https://www.tensorflow.org/).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow**是一个端到端的开源机器学习平台。TensorFlow可能是研究工程师和科学家中最受欢迎的深度学习库。它拥有庞大的社区支持和优秀的文档，可在[https://www.tensorflow.org/](https://www.tensorflow.org/)找到。'
- en: '**Keras** is a high-level deep learning API that can run on top of popular
    machine learning libraries such as TensorFlow, Theano, and CNTK. Keras is very
    popular in the research and development world because it supports rapid prototyping
    and provides a user-friendly API to neural network architectures.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**Keras**是一个高级深度学习API，可以在TensorFlow、Theano和CNTK等流行的机器学习库之上运行。Keras在研发领域非常受欢迎，因为它支持快速原型设计和提供了一个用户友好的API来构建神经网络架构。'
- en: '**TensorFlow 2** introduced great changes in machine learning methods by tightly
    integrating with Keras and providing a high-level API, **tf.keras**. TensorFlow
    1 was a bit ugly with symbolic graph computations and other low-level computations.
    With TensorFlow 2, developers can take advantage of Keras'' user-friendliness
    as well as TensorFlow''s low-level methods.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow 2**通过紧密集成Keras并提供高级API**tf.keras**，在机器学习方法上引入了重大变化。TensorFlow
    1在符号图计算和其他低级计算方面有些丑陋。随着TensorFlow 2的推出，开发者可以利用Keras的用户友好性以及TensorFlow的低级方法。'
- en: Neural networks are commonly used for computer vision and NLP tasks, including
    object detection, image classification, and scene understanding as well as text
    classification, POS tagging, text summarization, and natural language generation.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通常用于计算机视觉和NLP任务，包括目标检测、图像分类、场景理解以及文本分类、词性标注、文本摘要和自然语言生成。
- en: In the following sections, we'll go through the details of a neural network
    architecture for text classification implemented with `tf.keras`. Throughout this
    section, we'll use TensorFlow 2 as we stated in the *Technical requirements* section.
    Let's warm up to neural networks with some neural network basics, and then start
    building our Keras code.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将详细介绍使用`tf.keras`实现的文本分类神经网络架构的细节。在整个本节中，我们将使用我们在*技术要求*部分中提到的TensorFlow
    2。让我们从一些神经网络基础知识开始，然后开始构建我们的Keras代码。
- en: What is a layer?
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是层？
- en: A neural network is formed by connecting layers. **Layers** are basically the
    building blocks of the neural network. A layer consists of several **neurons**,
    as in *Figure 8.8*.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是通过连接层形成的。**层**基本上是神经网络的构建块。一个层由几个**神经元**组成，如*图8.8*所示。
- en: In *Figure 8.8*, the first layer of this neural network has two layers, and
    the second layer has six neurons. Each neuron in each layer is connected to all
    neurons in the next layer. Each layer might have different functionalities; some
    layers can lower the dimensions of their input, some layers can flatten their
    input (flattening means collapsing a multidimensional vector into one dimension),
    and so on. At each layer, we transform the input vectors and feed them to the
    next layer to get a final vector.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图8.8*中，这个神经网络的第一层有两个子层，第二层有六个神经元。每个子层中的每个神经元都与下一层的所有神经元相连。每个子层可能具有不同的功能；有些子层可以降低其输入的维度，有些子层可以将输入展平（展平意味着将多维向量折叠成一维），等等。在每一层，我们转换输入向量并将它们传递给下一层以获得最终向量。
- en: 'Keras provides different sorts of layers, such as input layers, dense layers,
    dropout layers, embedding layers, activation layers, recurrent layers, and so
    on. Let''s get to know some useful layers one by one:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供了不同类型的层，例如输入层、密集层、dropout层、嵌入层、激活层、循环层等等。让我们逐一了解一些有用的层：
- en: '**Input layer**: The input layer is responsible for sending our input data
    to the rest of the network. While initializing an input layer, we provide the
    input data shape.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：输入层负责将我们的输入数据发送到网络的其余部分。在初始化输入层时，我们提供输入数据形状。'
- en: '**Dense layers**: Dense layers transform the input of a given shape to the
    output shape we want. Layer 2 in *Figure 8.8* represents a dense layer, which
    collapses a 5-dimensional input into a 1-dimensional output.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**密集层**：密集层将给定形状的输入转换为所需的输出形状。*图8.8*中的层2代表一个密集层，它将5维输入折叠成1维输出。'
- en: '**Recurrent layers**: Keras provides strong support for RNN, GRU, and LSTM
    cells. If you''re not familiar with RNN variations at all, please refer to the
    resources in the *Technical requirements* section. We''ll use an LSTM layer in
    our code. The *LSTM layer* subsection contains the input and output shape information.
    In the next subsection, *Sequential modeling with LSTMs*, we''ll get into details
    of modeling with LSTMs.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环层**：Keras为RNN、GRU和LSTM单元提供了强大的支持。如果您对RNN变体完全不熟悉，请参阅*技术要求*部分中的资源。在我们的代码中，我们将使用LSTM层。*LSTM层*小节包含输入和输出形状信息。在下一小节*使用LSTMs进行序列建模*中，我们将深入了解使用LSTMs进行建模的细节。'
- en: '**Dropout layers**: Dropout is a technique to prevent overfitting. Overfitting
    happens when neural networks memorize data instead of learning it. Dropout layers
    randomly select a given number of neurons and set their weights to zero for the
    forward and backward passes, that is, for one iteration. We usually place dropout
    layers after dense layers.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout层**：Dropout是一种防止过拟合的技术。当神经网络记住数据而不是学习数据时，就会发生过拟合。Dropout层随机选择一定数量的神经元，并在正向和反向传递中（即在一次迭代中）将它们的权重设置为零。我们通常在密集层之后放置dropout层。'
- en: These are the basic layers that are used in NLP models. The next subsection
    is devoted to modeling sequential data with LSTMs, which is the core of statistical
    modeling for NLP.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在NLP模型中使用的基本层。下一小节将致力于使用LSTMs建模序列数据，这是NLP统计建模的核心。
- en: Sequential modeling with LSTMs
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LSTMs进行序列建模
- en: '**LSTM** is an RNN variation. **RNNs** are special neural networks that can
    process sequential data in steps. In usual neural networks, we assume that all
    the inputs and outputs are independent of each other. Of course, it''s not true
    for text data. Every word''s presence depends on the neighbor words.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**LSTM**是一种RNN变体。**RNNs**是特殊的神经网络，可以按步骤处理序列数据。在通常的神经网络中，我们假设所有输入和输出都是相互独立的。当然，对于文本数据来说，这并不正确。每个单词的存在都依赖于相邻的单词。'
- en: 'For example, during a machine translation task, we predict a word by considering
    all the words we predicted before. RNNs capture information about the past sequence
    elements by holding a `i`, we feed the input word `xi`, and RNN outputs a value,
    `hi`, for this time step:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在机器翻译任务中，我们通过考虑之前预测的所有单词来预测一个单词。RNN通过保持一个`i`来捕获关于过去序列元素的信息，我们输入单词`xi`，RNN为这个时间步输出一个值，`hi`：
- en: '![Figure 8.9 – RNN illustration, taken from Colah’s notable blog on LSTMs'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 8.9 – RNN示意图，摘自Colah的著名博客](img/B16570_8_9.jpg)'
- en: '](img/B16570_8_9.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_8_9.jpg](img/B16570_8_9.jpg)'
- en: Figure 8.9 – RNN illustration, taken from Colah's notable blog on LSTMs
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 – RNN示意图，摘自Colah关于LSTMs的著名博客
- en: LSTMs were invented to fix some computational problems of RNNs. RNNs have the
    problem of forgetting some data back in the sequence, as well as some numerical
    stability issues due to chain multiplications called **vanishing and exploding
    gradients**. If you are interested, you can refer to Colah's blog, the link to
    which you will find in the *References* section.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 是为了解决 RNN 的一些计算问题而发明的。RNN 存在着在序列中忘记一些数据的问题，以及由于链式乘法导致的数值稳定性问题，称为 **梯度消失和梯度爆炸**。如果您感兴趣，可以参考
    Colah 的博客，您将在 *参考文献* 部分找到链接。
- en: 'An LSTM cell is slightly more complicated than an RNN cell, but the logic of
    computation is the same: we feed one input word at each time step and LSTM outputs
    an output value at each time step. The following diagram shows what''s inside
    an LSTM cell. Note that the input steps and output steps are identical to the
    RNN counterparts:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 单元比 RNN 单元稍微复杂一些，但计算逻辑是相同的：我们在每个时间步输入一个输入单词，LSTM 在每个时间步输出一个输出值。以下图显示了 LSTM
    单元内部的情况。请注意，输入步骤和输出步骤与 RNN 的对应步骤相同：
- en: '![Figure 8.10 – LSTM illustration from Colah’s LSTM blog article'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.10 – 来自 Colah 的 LSTM 博文文章的 LSTM 插图'
- en: '](img/B16570_8_10.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_8_10.jpg)'
- en: Figure 8.10 – LSTM illustration from Colah's LSTM blog article
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 – 来自 Colah 的 LSTM 博文文章的 LSTM 插图
- en: 'Keras has extensive support for the RNN variations GRU and LSTM, as well as
    a simple API for training RNNs. RNN variations are crucial for NLP tasks, as language
    data''s nature is sequential: text is a sequence of words, speech is a sequence
    of sounds, and so on.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 对 RNN 变体 GRU 和 LSTM 提供了广泛的支持，同时还有一个简单的 API 用于训练 RNN。RNN 变体对于 NLP 任务至关重要，因为语言数据的本质是序列性的：文本是一系列单词，语音是一系列声音，等等。
- en: 'Now that we have learned what type of statistical model to use in our design,
    we can switch to a more practical subject: how to represent a sequence of words.
    In the next section, we''ll learn how to transform a sequence of words into a
    sequence of word IDs and build vocabularies at the same time with Keras''s preprocessing
    module.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了在我们的设计中使用哪种类型的统计模型，我们可以转向一个更实际的主题：如何表示单词序列。在下一节中，我们将学习如何使用 Keras 的预处理模块将单词序列转换为单词
    ID 序列，并同时构建词汇表。
- en: Keras Tokenizer
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras Tokenizer
- en: 'As we remarked in the previous section, text is sequential data (a sequence
    of words or characters). We''ll feed a sentence as a sequence of words. Neural
    networks can work only with vectors, so we need a way to vectorize the words.
    In [*Chapter 5*](B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087), *Working with
    Word Vectors and Semantic Similarity*, we saw how to vectorize words with word
    vectors. A word vector is a continuous representation of a word. In order to vectorize
    a word, we follow these steps:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中提到的，文本是序列数据（单词或字符的序列）。我们将句子作为单词序列输入。神经网络只能处理向量，因此我们需要一种将单词向量化的方法。在[*第
    5 章*](B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087)，“使用单词向量和语义相似性”，我们看到了如何使用单词向量来向量化单词。单词向量是单词的连续表示。为了向量化一个单词，我们遵循以下步骤：
- en: We tokenize each sentence and turn sentences into a sequence of words.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将每个句子进行分词，并将句子转换成单词序列。
- en: We create a vocabulary from the set of words present in *step 1*. These are
    words that are supposed to be recognized by our neural network design.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从 *步骤 1* 中出现的单词集合创建一个词汇表。这些是我们神经网络设计中应该被识别的单词。
- en: Creating a vocabulary should assign an ID to each word.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建词汇表应该为每个单词分配一个 ID。
- en: Then word IDs are mapped to word vectors.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将单词 ID 映射到单词向量。
- en: 'Let''s look at a short example. We can work on a small corpus of three sentences:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个简短的例子。我们可以处理一个小型语料库中的三个句子：
- en: '[PRE33]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let''s first tokenize the words into sentences:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先对单词进行分词成句子：
- en: '[PRE34]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In the preceding code, we iterated over all tokens of the Doc object generated
    by calling `nlp(sentence)`. Notice that we didn't filter out the punctuation marks.
    Filtering punctuation depends on the task. For instance, in sentiment analysis,
    punctuation marks such as "!" correlate to the result. In this example, we'll
    keep the punctuation marks as well.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们遍历了由调用 `nlp(sentence)` 生成的 Doc 对象的所有标记。请注意，我们没有过滤掉标点符号。过滤标点符号取决于任务。例如，在情感分析中，标点符号如
    "!" 与结果相关。在这个例子中，我们将保留标点符号。
- en: 'Keras''s text preprocessing module is used to create vocabularies and `trn`
    word sequences into word-ID sequences with the `Tokenizer` class. The following
    code segment exhibits how to use a `Tokenizer` object:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 的文本预处理模块使用 `Tokenizer` 类创建词汇表，并将 `trn` 单词序列转换为单词-ID 序列。以下代码段展示了如何使用 `Tokenizer`
    对象：
- en: '[PRE35]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We did the following:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做了以下事情：
- en: We imported `Tokenizer` from the Keras text preprocessing module.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从 Keras 文本预处理模块导入了 `Tokenizer`。
- en: We created a `tokenizer` object with the parameter `lower=True`, which means
    `tokenizer` should lower all words while building the vocabulary.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用参数 `lower=True` 创建了一个 `tokenizer` 对象，这意味着 `tokenizer` 在构建词汇表时应该将所有单词转换为小写。
- en: We called `tokenizer.fit_on_texts` on `data` to build the vocabulary. `fit_on_text`
    works on a sequence of words; input should always be a list of words.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在 `data` 上调用 `tokenizer.fit_on_texts` 来构建词汇表。`fit_on_text` 在单词序列上工作；输入始终应为单词的列表。
- en: We examined the vocabulary by printing `tokenizer.word_index`. `Word_index`
    is basically a dictionary where keys are vocabulary words and values are word-IDs.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过打印 `tokenizer.word_index` 来检查词汇表。`Word_index` 基本上是一个字典，其中键是词汇表中的单词，值是单词-ID。
- en: 'In order to get a word''s word-ID, we call `tokenizer.texts_to_sequences`.
    Notice that the input to this method should always be a list, even if we want
    to feed only one word. In the following code segment, we feed one-word input as
    a list (notice the list brackets):'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取一个单词的单词-ID，我们调用 `tokenizer.texts_to_sequences`。请注意，此方法的输入始终应为列表，即使我们只想输入一个单词。在下面的代码段中，我们将一个单词输入作为一个列表（注意列表括号）：
- en: '[PRE36]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The reverse of `texts_to_sequences` is the `sequences_to_texts`. `sequences_to_texts`
    method will input a list of lists and return the corresponding word sequences:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`texts_to_sequences` 的逆操作是 `sequences_to_texts`。`sequences_to_texts` 方法将输入一个列表的列表，并返回相应的单词序列：'
- en: '[PRE37]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We also notice that the word-IDs start from `1`, not `0`. `0` is a reserved
    value and has a special meaning, which means a padding value. Keras cannot process
    sentences of different lengths, hence we need to pad shorter sentences to reach
    the longest sentence''s length. We pad each sentence of the dataset to a maximum
    length by adding padding words either to the start or end of the sentence. Keras
    inserts `0` for the padding, which means it''s not a real word, but a padding
    value. Let''s understand padding with a simple example:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还注意到，单词-ID 从 `1` 开始，而不是 `0`。`0` 是一个保留值，具有特殊含义，表示填充值。Keras 无法处理不同长度的句子，因此我们需要将较短的句子填充到最长句子的长度。我们将数据集中的每个句子填充到最大长度，通过在句子的开始或末尾添加填充单词来实现。Keras
    插入 `0` 作为填充，这意味着它不是一个真正的单词，而是一个填充值。让我们用一个简单的例子来理解填充：
- en: '[PRE38]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Our sequences are of length 1, 2, and 4\. We called `pad_sequences` on this
    list of sequences and every sequence is padded with zeros such that its length
    reaches `MAX_LEN=4`, the length of the longest sequence. We can pad the sequences
    from the right or left with the `post` and `pre` options. In the preceding code,
    we padded our sentences with the `post` option, hence the sentences are padded
    from the right.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的序列长度为 1、2 和 4。我们对这个序列列表调用了 `pad_sequences`，每个序列都被填充为零，使其长度达到 `MAX_LEN=4`，即最长序列的长度。我们可以使用
    `post` 和 `pre` 选项从右或左填充序列。在先前的代码中，我们使用 `post` 选项填充我们的句子，因此句子是从右向左填充的。
- en: 'If we put it all together, the complete text preprocessing steps are as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将所有这些放在一起，完整的文本预处理步骤如下：
- en: '[PRE39]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Now, we've transformed sentences into a sequence of word-IDs. We've come one
    step closer to vectorizing the words. In the next subsection, we'll finally transform
    words into vectors. Then our sentences will be ready to be fed into the neural
    network.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经将句子转换成了单词-ID 的序列。我们离将单词向量化又近了一步。在下一个小节中，我们将最终将单词转换成向量。然后我们的句子就可以输入到神经网络中了。
- en: Embedding words
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入单词
- en: 'We''re ready to transform words into word vectors. Embedding words into vectors
    happens via an embedding table. An embedding table is basically a lookup table.
    Each row holds the word vector of a word. We index the rows by word-IDs, hence
    the flow of obtaining a word''s word vector is as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好将单词转换成单词向量了。将单词嵌入到向量中是通过嵌入表来实现的。嵌入表基本上是一个查找表。每一行都包含一个单词的单词向量。我们通过单词-ID
    来索引行，因此获取一个单词的单词向量的流程如下：
- en: '`Tokenizer`. `Tokenizer` holds all the vocabulary and maps each vocabulary
    word to an ID, which is an integer.'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Tokenizer`。`Tokenizer` 包含所有词汇，并将每个词汇映射到一个 ID，这是一个整数。'
- en: '**word-ID->word vector**: A word-ID is an integer and therefore can be used
    as an index to the embedding table''s rows. Each word-ID corresponds to one row
    and when we want to get a word''s word vector, we first obtain its word-ID and
    then do a lookup in the embedding table rows with this word-ID.'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**单词-ID->单词向量**：单词-ID 是一个整数，因此可以用作嵌入表行的索引。每个单词-ID 对应一行，当我们想要获取一个单词的单词向量时，我们首先获取其单词-ID，然后在嵌入表行中使用此单词-ID
    进行查找。'
- en: 'The following diagram shows how embedding words into word vectors works:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了将单词嵌入到词向量中的工作方式：
- en: '![Figure 8.11 – Steps of transforming a word to its word vector with Keras'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.11 – 使用Keras将单词转换为单词向量的步骤](#)'
- en: '](img/B16570_8_11.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B16570_8_11.jpg)'
- en: Figure 8.11 – Steps of transforming a word to its word vector with Keras
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 – 使用Keras将单词转换为单词向量的步骤
- en: 'Remember that in the previous section, we started with a list of sentences.
    Then we did the following:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在前一个部分中，我们从一个句子列表开始。然后我们做了以下几步：
- en: We broke each sentence into words and built a vocabulary with Keras' `Tokenizer`.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将每个句子分解成单词，并使用Keras的`Tokenizer`构建一个词汇表。
- en: The `Tokenizer` object held a word index, which was a **word->word-ID** mapping.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Tokenizer`对象包含一个单词索引，这是一个**单词->单词-ID**映射。'
- en: After obtaining the word-ID, we could do a lookup to the embedding table rows
    with this word-ID and got a word vector.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在获得单词-ID之后，我们可以通过这个单词-ID查找嵌入表中的行，并获取一个单词向量。
- en: Finally, we fed this word vector to the neural network.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将这个单词向量馈送到神经网络中。
- en: Training a neural network is not easy. We have to take several steps to transform
    sentences into vectors. After these preliminary steps, we're ready to design the
    neural network architecture and do the model training.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络并不容易。我们必须采取几个步骤将句子转换为向量。在这些初步步骤之后，我们就准备好设计神经网络架构并进行模型训练了。
- en: Neural network architecture for text classification
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本分类的神经网络架构
- en: 'In this section, we will design the neural network architecture for our text
    classifier. We''ll follow these steps to train the classifier:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将为我们的文本分类器设计神经网络架构。我们将遵循以下步骤来训练分类器：
- en: First, we'll preprocess, tokenize, and pad the review sentences. After this
    step, we'll obtain a list of sequences.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将对评论句子进行预处理、标记化和填充。在此步骤之后，我们将获得一个序列列表。
- en: We'll feed this list of sequences to the neural network through the input layer.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将通过输入层将这个序列列表馈送到神经网络中。
- en: Next, we'll vectorize each word by looking up its word ID in the embedding layer.
    At this point, a sentence is now a sequence of word vectors, each word vector
    corresponding to a word.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将通过查找嵌入层中的单词ID来向量化每个单词。在这个时候，一个句子现在是一个单词向量的序列，每个单词向量对应一个单词。
- en: After that, we'll feed the sequence of word vectors to LSTM.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们将把单词向量序列馈送到LSTM中。
- en: Finally, we'll squash the LSTM output with a sigmoid layer to obtain class probabilities.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将使用sigmoid层压缩LSTM输出以获得类概率。
- en: Let's get started by remembering the dataset again.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从再次记住数据集开始。
- en: Dataset
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集
- en: 'We''ll use the same Amazon fine food reviews dataset from the *Sentiment analysis
    with spaCy* section. We already processed the dataset with pandas in that section
    and reduced it to two columns and binary labels. Here is how the `reviews_df`
    dataset looks:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与*使用spaCy进行情感分析*部分相同的亚马逊美食评论数据集。在那个部分中，我们已经使用pandas处理了数据集，并将其减少到两列和二进制标签。以下是`reviews_df`数据集的显示方式：
- en: '![Figure 8.12 - Result of the reviews_df.head()'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.12 - `reviews_df.head()`的结果](#)'
- en: '](img/B16570_8_12.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B16570_8_12.jpg)'
- en: Figure 8.12 – Result of the reviews_df.head()
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 – `reviews_df.head()`的结果
- en: 'We''ll transform our dataset a bit. We''ll extract the review text and review
    label from each dataset row and append them into Python lists:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对数据集进行一些转换。我们将从每个数据集行中提取评论文本和评论标签，并将它们附加到Python列表中：
- en: '[PRE40]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Notice that we appended a list of words to `train_examples`, hence each element
    of this list is a list of words. Next, we'll invoke Keras' `Tokenizer` on this
    list of words to build our vocabulary.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将一个单词列表附加到`train_examples`中，因此这个列表的每个元素都是一个单词列表。接下来，我们将调用Keras的`Tokenizer`对这个单词列表进行操作以构建我们的词汇表。
- en: Data and vocabulary preparation
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据和词汇准备
- en: 'We already processed our dataset, hence we are ready to tokenize the dataset
    sentences and create a vocabulary. Let''s go step by step:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经处理了我们的数据集，因此我们准备好对数据集句子进行标记化并创建词汇表。让我们一步一步来：
- en: 'First, we''ll do the necessary imports:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将进行必要的导入：
- en: '[PRE41]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We''re ready to fit the `Tokenizer` object on our list of words. First, we''ll
    fit the `Tokenizer`, then we''ll convert words to their IDs by calling `texts_to_sequences`:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们准备好将`Tokenizer`对象拟合到我们的单词列表。首先，我们将拟合`Tokenizer`，然后我们将通过调用`texts_to_sequences`将单词转换为它们的ID：
- en: '[PRE42]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then, we''ll pad the short sequences to a maximum length of `50` (we picked
    this number). Also, this will truncate long reviews to a length of `50` words:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将短序列填充到最大长度为`50`（我们选择了这个数字）。同时，这将截断长评论到`50`个单词的长度：
- en: '[PRE43]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now `X` is a list of sequences of `50` words. Finally, we''ll convert this
    list of reviews and the labels to `numpy` arrays:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在`X`是一个包含`50`个单词的序列列表。最后，我们将这个评论列表和标签转换为`numpy`数组：
- en: '[PRE44]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'At this point, we''re ready to feed our data to our neural network. We''ll
    feed our data to the input layer. For all the necessary imports, please follow
    the notebook of this section from our GitHub repository: [https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter08/Keras_train.ipynb](https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter08/Keras_train.ipynb).'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经准备好将数据输入到我们的神经网络中。我们将数据输入到输入层。对于所有必要的导入，请参考我们GitHub仓库中本节的工作簿：[https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter08/Keras_train.ipynb](https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter08/Keras_train.ipynb)。
- en: Here, notice that we didn’t do any lemmatization/stemming or stopwords removal.
    This is completely fine and indeed the standard way to go with neural network
    algorithms, because words that are variations of the same root word (liked, liking,
    like) will obtain similar word vectors (recall from [*Chapter 5*](B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087)*,
    Working with Word Vectors and Semantic Similarity* that similar words obtain similar
    word vectors). Also, stopwords occur frequently in different contexts, hence neural
    network can deduce that these words are just common words of the language and
    don't carry much importance.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们没有进行任何词形还原/词干提取或停用词去除。这完全没问题，实际上这是与神经网络算法一起使用的标准方式，因为具有相同词根的单词（如liked、liking、like）将获得相似的词向量（回想一下[*第5章*](B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087)*，处理词向量和语义相似性*，相似单词获得相似的词向量）。此外，停用词在不同的上下文中频繁出现，因此神经网络可以推断出这些单词只是语言中的常见单词，并不携带太多重要性。
- en: The input layer
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入层
- en: 'The following piece of code defines our input layer:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块定义了我们的输入层：
- en: '[PRE45]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Don't be confused by `None` as the input shape. Here, `None` means that this
    dimension can be any scalar number, hence, we use this expression when we want
    Keras to infer the input shape.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 不要被`None`作为输入形状所困惑。在这里，`None`表示这个维度可以是任何标量数，因此，当我们想要Keras推断输入形状时，我们使用这个表达式。
- en: The embedding layer
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入层
- en: 'We define the embedding layer as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如下定义嵌入层：
- en: '[PRE46]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: While defining the embedding layer, the input dimension should always be the
    number of words in the vocabulary (here, there's a plus `1` because the indices
    start from `1`, not `0`. Index `0` is reserved for the padding value).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义嵌入层时，输入维度应该是词汇表中的单词数量（在这里，由于索引从`1`开始而不是`0`，所以有一个加号`+1`。索引`0`是为填充值保留的）。
- en: Here, we chose the output shape to be 100, hence the word vectors for the vocabulary
    words will be 100-dimensional. Popular numbers for word vector dimensions are
    50, 100, and 200 depending on the complexity of the task.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们选择了输出形状为`100`，因此词汇表单词的词向量将是`100`维度的。根据任务的复杂度，常见的词向量维度是`50`、`100`和`200`。
- en: The LSTM layer
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LSTM层
- en: 'We''ll feed the word vectors to our LSTM:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将词向量输入到我们的LSTM中：
- en: '[PRE47]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Here, the `units` parameter means the dimension of the hidden state. The LSTM
    output shape and hidden state shape are the same due to the LSTM architecture.
    Here, our LSTM layer will output a `256`-dimensional vector.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`units`参数表示隐藏状态的维度。由于LSTM架构，LSTM输出形状和隐藏状态形状是相同的。在这里，我们的LSTM层将输出一个`256`维度的向量。
- en: The output layer
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出层
- en: 'We obtained a `256`-dimensional vector from the LSTM layer and we want to squash
    it to a `1`-dimensional vector (possible values of this vector are `0` and `1`,
    which are the class labels):'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从LSTM层获得了一个`256`维度的向量，并希望将其压缩成一个`1`维度的向量（这个向量的可能值是`0`和`1`，它们是类别标签）：
- en: '[PRE48]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We used the sigmoid function to squash the values. The sigmoid function is an
    S-shaped function and maps its input to a [0-1] range. You can find out more about
    this function at [https://deepai.org/machine-learning-glossary-and-terms/sigmoid-function](https://deepai.org/machine-learning-glossary-and-terms/sigmoid-function).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了sigmoid函数来压缩值。sigmoid函数是一个S形函数，将输入映射到[0-1]的范围内。你可以在[https://deepai.org/machine-learning-glossary-and-terms/sigmoid-function](https://deepai.org/machine-learning-glossary-and-terms/sigmoid-function)了解更多关于这个函数的信息。
- en: Compiling the model
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编译模型
- en: 'After defining the model, we need to compile it with an optimizer, a loss function,
    and an evaluation metric:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义模型后，我们需要使用优化器、损失函数和评估指标来编译它：
- en: '[PRE49]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '**Adaptive Moment Estimation** (**ADAM**) is a popular optimizer in deep learning.
    It basically adapts how fast the neural network should learn. You can learn about
    different optimizers in this blog post: [https://ruder.io/optimizing-gradient-descent/](https://ruder.io/optimizing-gradient-descent/).
    Binary cross-entropy is a loss that is used in binary classification tasks. Keras
    supports different loss functions depending on the tasks. You can find the list
    on the Keras website at [https://keras.io/api/losses/](https://keras.io/api/losses/).'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '**自适应矩估计**（**ADAM**）是深度学习中的一种流行优化器。它基本上调整神经网络应该学习多快的速度。你可以在这篇博客文章中了解不同的优化器：[https://ruder.io/optimizing-gradient-descent/](https://ruder.io/optimizing-gradient-descent/)。二元交叉熵是用于二元分类任务的损失。Keras根据任务支持不同的损失函数。你可以在Keras网站上找到列表，网址为[https://keras.io/api/losses/](https://keras.io/api/losses/)。'
- en: A **metric** is a function that we use to evaluate our model's performance.
    The accuracy metric basically compares how many times the predicted label and
    the real label matches. A list of supported metrics can be found in Keras's documentation
    ([https://keras.io/api/metrics/](https://keras.io/api/metrics/)).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '**度量**是我们用来评估模型性能的函数。准确度度量基本上是比较预测标签和真实标签匹配的次数。支持的度量列表可以在Keras的文档中找到（[https://keras.io/api/metrics/](https://keras.io/api/metrics/))。'
- en: Fitting the model and experiment evaluation
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型拟合和实验评估
- en: 'Finally, we''ll fit the model on our data:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将模型拟合到我们的数据上：
- en: '[PRE50]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Here, `x` is the list of training examples and `y` is the list of labels. We
    want to make 5 passes over the data, hence we set the `epochs` parameter to `5`.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`x`是训练示例的列表，`y`是标签的列表。我们想要对数据进行5次遍历，因此将`epochs`参数设置为`5`。
- en: We went over the data `5` times in batch sizes of `64`. Usually, we don't fit
    all of the dataset into the memory at once (due to memory limitations), but we
    feed the dataset to the classifier in smaller chunks, each chunk being called
    a `batch_size=64` means we want to feed a batch of 64 training sentences at once.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以`64`的批量大小对数据进行`5`次遍历。通常，我们不会一次性将整个数据集放入内存中（由于内存限制），而是将数据集分批提供给分类器，每个批次被称为`batch_size=64`，这意味着我们希望一次性提供64个训练句子。
- en: Finally, the parameter `validation_split` is used to evaluate the experiment.
    This parameter simply will separate 20 percent of the data as the validation set
    and validate the model on this validation set. Our experiment results in 0.795
    accuracy, which is quite good for such a basic neural network design.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，参数`validation_split`用于评估实验。该参数简单地将20%的数据作为验证集，并在该验证集上验证模型。我们的实验结果准确度为0.795，对于这样一个基本的神经网络设计来说相当不错。
- en: We encourage you to experiment more. You can experiment with the code more by
    placing dropout layers at different locations (such as after the embedding layer
    or after the LSTM layer). Another way of experimenting is to try different values
    for the embedding dimensions, such as 50, 150, and 200, and observe the change
    in the accuracy. The same applies to the LSTM layer's hidden dimension – you can
    experiment with different values instead of 256.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励你进行更多实验。你可以通过在不同位置（如嵌入层之后或LSTM层之后）放置dropout层来更多地实验代码。另一种实验方法是尝试不同的嵌入维度值，例如50、150和200，并观察准确度的变化。同样适用于LSTM层的隐藏维度——你可以用不同的值而不是256来实验。
- en: We finished training with `tf.keras` in this section and also concluded the
    chapter. Keras is a great, efficient, and user-friendly deep learning API; the
    spaCy and Keras combination is especially powerful. Text classification is an
    essential task of NLP and we discovered how to do this task with spaCy.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用`tf.keras`完成了训练，并结束了本章。Keras是一个伟大、高效且用户友好的深度学习API；spaCy和Keras的组合特别强大。文本分类是NLP的一个基本任务，我们发现了如何使用spaCy来完成这个任务。
- en: Summary
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We have finished this chapter about a very hot NLP topic – text classification.
    In this chapter, you first learned about text classification concepts such as
    binary classification, multilabel classification, and multiclass classification.
    Next, you learned how to train `TextCategorizer`, spaCy's text classifier component.
    You learned how to transform your data into spaCy training format and then train
    the `TextCategorizer` component with this data.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们完成了关于一个非常热门的NLP主题——文本分类——的章节。在本章中，你首先了解了文本分类的概念，如二元分类、多标签分类和多类分类。接下来，你学习了如何训练`TextCategorizer`，spaCy的文本分类组件。你学习了如何将你的数据转换为spaCy训练格式，然后使用这些数据训练`TextCategorizer`组件。
- en: After learning text classification with spaCy's `TextCategorizer`, in the final
    section, you learned how to combine spaCy code and Keras code. First, you learned
    the basics of neural networks, including some handy layers such as the dense layer,
    dropout layer, embedding layer, and recurrent layers. Then, you learned how to
    tokenize and preprocess the data with Keras' `Tokenizer`.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用spaCy的`TextCategorizer`学习文本分类后，在最后一节中，你学习了如何结合spaCy代码和Keras代码。首先，你学习了神经网络的基础知识，包括一些实用的层，如密集层、dropout层、嵌入层和循环层。然后，你学习了如何使用Keras的`Tokenizer`进行数据分词和预处理。
- en: You had a quick review of sequential modeling with LSTMs, as well as recalling
    word vectors from [*Chapter 5*](B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087),
    *Working with Word Vectors and Semantic Similarity*, to understand the embedding
    layer better. Finally, you went through neural network design with `tf.keras`
    code. You learned how to design and evaluate a statistical experiment with LSTM.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 你快速回顾了使用LSTMs进行序列建模，以及从[*第五章*](B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087)，“处理词向量与语义相似性”，回顾词向量，以更好地理解嵌入层。最后，你通过`tf.keras`代码进行了神经网络设计。你学习了如何使用LSTM设计和评估一个统计实验。
- en: Looks like a lot! Indeed, it is a lot of material; no worries if it takes time
    to digest. Practicing text classification can be intense, but in the end, you
    earn crucial NLP skills.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来有很多！确实，这是一堆材料；如果需要时间消化，请不要担心。练习文本分类可能会很紧张，但最终，你将获得关键的NLP技能。
- en: 'The next chapter is again devoted to a brand-new technology: **transformers**.
    In the next chapter, we''ll explore how to design high-accuracy NLP pipelines
    in only a few lines. Let''s move onto the next chapter and see what transformers
    offer for your NLP skills!'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将再次介绍一项全新的技术：**transformers**。在下一章中，我们将探讨如何仅用几行代码设计高精度的NLP管道。让我们进入下一章，看看transformers能为你的NLP技能带来什么！
- en: References
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'It''d be good but not mandatory if you''re familiar with neural networks, particularly
    RNN variations. Here is some great material for neural networks:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉神经网络，特别是RNN变体，这将是有益的但不是强制性的。以下是一些关于神经网络的优秀材料：
- en: 'Free online book: *Neural Networks and Deep Learning* ([http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/))'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 免费在线书籍：*神经网络与深度学习* ([http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/))
- en: Video tutorial at [https://www.youtube.com/watch?v=ob1yS9g-Zcs](https://www.youtube.com/watch?v=ob1yS9g-Zcs)
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频教程：[https://www.youtube.com/watch?v=ob1yS9g-Zcs](https://www.youtube.com/watch?v=ob1yS9g-Zcs)
- en: 'RNN variations, especially LSTMs, have great tutorials too:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: RNN变体，特别是LSTMs，也有很好的教程：
- en: 'RNN tutorial on the WildML blog: [http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WildML博客上的RNN教程：[http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)
- en: RNN tutorial by the University of Toronto:  [https://www.cs.toronto.edu/~tingwuwang/rnn_tutorial.pdf](https://www.cs.toronto.edu/~tingwuwang/rnn_tutorial.pdf)
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多伦多大学RNN教程：[https://www.cs.toronto.edu/~tingwuwang/rnn_tutorial.pdf](https://www.cs.toronto.edu/~tingwuwang/rnn_tutorial.pdf)
- en: 'Colah''s blog: [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Colah的博客：[https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- en: 'Blog post by Michael Phi: [https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Michael Phi的博客文章：[https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)
- en: Video tutorial at [https://www.youtube.com/watch?v=lWkFhVq9-nc](https://www.youtube.com/watch?v=lWkFhVq9-nc)
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频教程：[https://www.youtube.com/watch?v=lWkFhVq9-nc](https://www.youtube.com/watch?v=lWkFhVq9-nc)
- en: 'Although we have introduced neural networks in this chapter, you can read these
    references in order to learn more about how neural networks work. More explanations
    on neural network and LSTM concepts will follow in [*Chapter 10*](B16570_10_Final_JM_ePub.xhtml#_idTextAnchor173),
    *Putting Everything Together: Designing Your Chatbot with spaCy*.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在本章介绍了神经网络，但你仍然可以阅读这些参考文献，以了解更多关于神经网络如何工作的信息。在[*第十章*](B16570_10_Final_JM_ePub.xhtml#_idTextAnchor173)，“将一切整合：使用spaCy设计你的聊天机器人”，将会有更多关于神经网络和LSTM概念的说明。
