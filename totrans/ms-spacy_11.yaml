- en: 'Chapter 8: Text Classification with spaCy'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter is devoted to a very basic and popular task of NLP: text classification.
    You will first learn how to train spaCy''s text classifier component, `TextCategorizer`.
    For this, you will learn how to prepare data and feed the data to the classifier;
    then we''ll proceed to train the classifier. You''ll also practice your new `TextCategorizer`
    skills on a popular dataset for sentiment analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will also do text classification with the popular framework TensorFlow's
    Keras API together with spaCy. You will learn the basics of neural networks, sequential
    data modeling with LSTMs, and how to prepare text for machine learning tasks with
    Keras's text preprocessing module. You will also learn how to design a neural
    network with `tf.keras`.
  prefs: []
  type: TYPE_NORMAL
- en: Following that, we will then make an end-to-end text classification experiment,
    from data preparation to preprocessing text with Keras `Tokenizer`, neural network
    designing, model training, and interpreting the classification results. That's
    a whole package of machine learning!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the basics of text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the spaCy text classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis with spaCy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text classification with spaCy and Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code in the sections *Training the spaCy text classifier* and *Sentiment
    analysis with spaCy* is spaCy v3.0 compatible.
  prefs: []
  type: TYPE_NORMAL
- en: 'The section *Text classification with spaCy and Keras* requires the following
    Python libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow >=2.2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can install the latest version of these libraries with `pip` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We also use Jupyter notebooks in the last two sections. You can follow the instructions
    on the Jupyter website ([https://jupyter.org/install](https://jupyter.org/install))
    to install the Jupyter notebook onto your system. If you don't want to use notebooks,
    you can copy-paste code as Python code as well.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the chapter code and data files in the book's GitHub repository
    at [https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter08](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter08).
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started with spaCy's text classifier component first, then we'll transition
    to designing our own neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the basics of text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text classification is the task of assigning a set of predefined labels to text.
    Given a set of predefined classes and some text, you want to understand which
    predefined class this text falls into. We have to determine the classes ourselves
    by the nature of our data before starting the classification task. For example,
    a customer review can be positive, negative, or neutral.
  prefs: []
  type: TYPE_NORMAL
- en: Text classifiers are used for detecting spam emails in your mailbox, determining
    the sentiment of customer's reviews, understanding customer's intent, sorting
    customer's complaint tickets, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Text classification is a fundamental task of NLP. It is gaining importance in
    the business world, as it enables businesses to automate their processes. One
    immediate example is spam filters. Every day, users receive many spam emails but
    most of the time never see these emails and don't get any notifications because
    spam filters save the users from bothering about irrelevant emails and from spending
    time deleting these emails.
  prefs: []
  type: TYPE_NORMAL
- en: 'Text classifiers can come in different flavors. Some classifiers focus on the
    overall emotion of the text, some classifiers focus on detecting the language
    of the text, and some classifiers focus on only some words of the text, such as
    verbs. The following are some of the most common types of text classification
    and their use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Topic detection**: Topic detection is the task of understanding the topic
    of a given text. For example, the text in a customer email could be asking about
    a refund, asking for a past bill, or simply complaining about the customer service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: Sentiment analysis is the task of understanding whether
    the text contains positive or negative emotions about a given subject. Sentiment
    analysis is used often to analyze customer reviews about products and services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language detection**: Language detection is the first step of many NLP systems,
    such as machine translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure shows a text classifier for a customer service automation
    system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Topic detection is used to label a customer complaint with a
    predefined label'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_8_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – Topic detection is used to label a customer complaint with a predefined
    label
  prefs: []
  type: TYPE_NORMAL
- en: 'Coming to the technical details, text classification is a *supervised* machine
    learning task. It means that the classifier can predict the class label of a text
    based on *example*input text-class label pairs. Hence, to train a text classifier,
    we need a *labeled dataset*. A labeled dataset is basically a list of text-label
    pairs. Here is an example dataset of five training sentences with their labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Then we train the classifier by showing the text and the corresponding class
    labels to the classifier. When the classifier sees new text that was not in the
    training text, it then predicts the class label of this unseen text based on the
    examples it saw during the training phase. The output of a text classifier is
    *always* a class label.
  prefs: []
  type: TYPE_NORMAL
- en: 'Text classification can also be divided into three categories depending on
    the number of classes used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary text classification** means that we want to categorize our text into
    two classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiclass text classification** means that there are more than two classes.
    Each class is mutually exclusive – one text can belong to one class only. Equivalently,
    a training instance can be labeled with only one class label. An example is rating
    customer reviews. A review can have 1, 2, 3, 4, or 5 stars (each star category
    is a class).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multilabel text classification** is a generalization of multiclass classification,
    where multiple labels can be assigned to each example text. For example, classifying
    toxic social media messages is done with multiple labels. This way, our model
    can distinguish different levels of toxicity. Class labels are typically toxic,
    severe toxic, insult, threat, obscenity. A message can include both insults and
    threats, or be classed as insult, toxicity, and obscenity, and so on. Hence for
    this problem, using multiple classes is more suitable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Labels are the name of the classes we want to see as the output. A class label
    can be categorical (string) or numerical (a number). Here are some commonly used
    class labels:'
  prefs: []
  type: TYPE_NORMAL
- en: For sentiment analysis, we usually use the class labels positive and negative.
    Their abbreviations, pos and neg, are also commonly used. Binary class labels
    are popular as well – 0 means negative sentiment and 1 means positive sentiment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same applies to binary classification problems. We usually use 0-1 for class
    labels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For multiclass and multilabel problems, we usually name the classes with a meaningful
    name. For a movie genre classifier, we can use the labels family, international,
    Sunday evening, Disney, action, and so on. Numbers are used as labels as well.
    For a five-class classification problem, we can use the labels 1, 2, 3, 4, and
    5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we've covered the basic concepts of text classification, let's do some coding!
    In the next section, we'll explore how to train spaCy's text classifier component.
  prefs: []
  type: TYPE_NORMAL
- en: Training the spaCy text classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn about the details of spaCy's text classifier
    component `TextCategorizer`. In [*Chapter 2*](B16570_02_Final_JM_ePub.xhtml#_idTextAnchor037),
    *Core Operations with spaCy*, we saw that the spaCy NLP pipeline consists of components.
    In [*Chapter 3*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055), *Linguistic
    Features*, we learned about the essential components of the spaCy NLP pipeline,
    which are the sentence tokenizer, POS tagger, dependency parser, and **named entity
    recogition** (**NER**).
  prefs: []
  type: TYPE_NORMAL
- en: '`TextCategorizer` is an optional and trainable pipeline component. In order
    to train it, we need to provide examples and their class labels. We first add
    `TextCategorizer` to the NLP pipeline and then do the training procedure. *Figure
    8.2* shows where exactly the `TextCategorizer` component lies in the NLP pipeline;
    this component comes after the essential components. In the following diagram,
    `TextCategorizer` component.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – TextCategorizer in the nlp pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_8_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 – TextCategorizer in the nlp pipeline
  prefs: []
  type: TYPE_NORMAL
- en: A neural network architecture lies behind spaCy's `TextCategorizer`. `TextCategorizer`
    provides us with user-friendly and end-to-end approaches to train the classifier,
    so we don't have to deal directly with the neural network architecture. We'll
    design our own neural network architecture in the upcoming *Text classification
    with spaCy and Keras* section. After looking at the architecture, we’re ready
    to dive into TextCategorizer code. Let’s get to know `TextCategorizer` class first.
  prefs: []
  type: TYPE_NORMAL
- en: Getting to know TextCategorizer class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let''s get to know the `TextCategorizer` class in detail. First of all,
    we import `TextCategorizer` from the pipeline components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`TextCategorizer` is available in two flavors, single-label classifier and
    multilabel classifier. As we remarked in the previous section, a multilabel classifier
    can predict more than one class. A single-label classifier predicts only one class
    for each example and classes are mutually exclusive. The preceding `import` line
    imports the single-label classifier and the following code imports the multilabel
    classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need to provide a configuration to the `TextCategorizer` component.
    We provide two parameters here, a threshold value and a model name (either `Single`
    or `Multi` depending on the classification task). `TextCategorizer` internally
    generates a probability for each class and a class is assigned to the text if
    the probability of this class is higher than the threshold value.
  prefs: []
  type: TYPE_NORMAL
- en: A traditional threshold value for text classification is `0.5`, however, if
    you want to make a prediction with higher confidence, you can make the threshold
    higher, such as 0.6, 0.7, or 0.8.
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting it altogether, we can add a single-label `TextCategorizer` component
    to the `nlp` pipeline as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Adding a multilabel component to the `nlp` pipeline is similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the last line of each of the preceding code blocks, we added a `TextCategorizer`
    pipeline component to the nlp pipeline object. The newly created `TextCategorizer`
    component is captured by the `textcat` variable. We're ready to train the `TextCategorizer`
    component now. The training code looks quite similar to the NER component training
    code from [*Chapter 7*](B16570_07_Final_JM_ePub.xhtml#_idTextAnchor120)*, Customizing
    spaCy Models*, except for some minor details.
  prefs: []
  type: TYPE_NORMAL
- en: Formatting training data for the TextCategorizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start our code by preparing a small training set. We''ll prepare a customer
    sentiment dataset for binary text classification. The label will be called `sentiment`
    and can obtain two possible values, 0 and 1 corresponding to negative and positive
    sentiment. The following training set contains 6 examples, 3 being positive and
    3 being negative:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Each training example is a tuple of a text and a nested dictionary. The dictionary
    contains the class label in a format that spaCy recognizes. The `cts` field means
    the categories. Then we include the class label sentiment and its value. The value
    should always be a floating-point number.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code, we will introduce the class label we choose to the `TextCategorizer`
    component. Let''s see the complete code. First, we do the necessary imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We imported the built-in library `random` for shuffling our dataset. We imported
    `spacy` as usual, and we imported `Example` to prepare the training examples in
    spaCy format. In the last line of the code block, we imported a text categorizer
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll do the pipeline and `TextCategorizer` component initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now, we'll do some work on the newly created `TextCategorizer` component, `textcat`.
    We'll introduce our label `sentiment` to the `TextCategorizer` componenet by calling
    `add_label`. Then, we need to initialize this component with our examples. This
    step is different than what we did in the NER training code in [*Chapter 7*](B16570_07_Final_JM_ePub.xhtml#_idTextAnchor120)*,
    Customizing spaCy Models*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason is NER is an essential component, hence it''s initialized by the
    pipeline always. TextCategorizer is an optional component, and it comes as a blank
    statistical model. The following code adds our label to the `TextCategorizer`
    component and then initializes the `TextCategorizer` model''s weights with the
    training examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note that we feed the examples to `textcat.initialize` as `Example` objects.
    Recall from [*Chapter 7*](B16570_07_Final_JM_ePub.xhtml#_idTextAnchor120), *Customizing
    spaCy Models*, that spaCy training methods always work with `Example` objects.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the training loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''re ready to define the training loop. First of all, we''ll disable other
    pipe components so that only `textcat` will be trained. Second, we will create
    an optimizer object by calling `resume_training`, keeping the weights of the existing
    statistical models. For each epoch, we go over training examples one by one and
    update the weights of `textcat`. We go over the data for 20 epochs. The following
    code defines the training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it! With this relatively short code segment, we trained a text classifier!
    Here''s the output on my machine (your loss values might be different):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Loss values at each epoch'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_8_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – Loss values at each epoch
  prefs: []
  type: TYPE_NORMAL
- en: Testing the new component
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s test the new text categorizer component. The `doc.cats` property holds
    the class labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Great! Our small dataset successfully trained the spaCy text classifier for
    a binary text classification problem, indeed a sentiment analysis task. Now, we'll
    see how to do multilabel classification with spaCy's `TextCategorizer`.
  prefs: []
  type: TYPE_NORMAL
- en: Training TextCategorizer for multilabel classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall from the first section that multilabel classification means the classifier
    can predict more than one label for an example text. Naturally, the classes are
    not mutually exclusive at all. In order to train a multilabel classifier, we need
    to provide a dataset that contains examples that have more than one label.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train spaCy''s `TextCategorizer` for multilabel classification, we''ll again
    start by building a small training set. This time, we''ll form a set of movie
    reviews, where the labels are `FAMILY,` `THRILLER`, and `SUNDAY_EVENING`. Here
    is our small dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We provided some examples with one label, such as the first example (the first
    sentence of `train_data`, the second line of the preceding code block), and we
    also provided examples with more than one label, such as the fourth example of
    the `train_data`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll make the imports after we''ve formed the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here, the last line is different than the code of the previous section. We imported
    the multilabel model instead of the single-label model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we add the multilabel classifier component to the nlp pipeline. Again,
    pay attention to the pipeline component name – this time, it is `textcat_multilabel`,
    compared to the previous section''s `textcat`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Adding the labels to the `TextCategorizer` component and initializing the model
    is similar to the *Training the spaCy text classifier* section. This time, we''ll
    add three labels instead of one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re ready to define the training loop. The code functions are similar to
    the previous section''s code. The only difference is the component name in the
    first line. Now it''s `textcat_multilabel`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should look similar to the output of the previous section, a loss
    value per epoch. Now, let''s test our brand new multilabel classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Notice that each label admitted a positive probability at the output. Also,
    the probabilities do not sum up to 1, because they're not mutually exclusive.
    For this example, the `SUNDAY_EVENING` and `THRILLER` label probabilities are
    predicted correctly, but the `FAMILY` label probability does not look ideal. This
    is mainly due to the fact that we didn't provide enough examples. Usually, for
    multilabel classification problems, the classifier needs more examples than binary
    classification since the classifier needs to learn more labels.
  prefs: []
  type: TYPE_NORMAL
- en: We've learned how to train spaCy's `TextCategorizer` component for binary text
    classification and multilabel text classification. Now, we'll train `TextCategorizer`
    on a real-world dataset for a sentiment analysis problem.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis with spaCy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll work on a real-world dataset and train spaCy's `TextCategorizer`
    on this dataset. We'll be working on the Amazon Fine Food Reviews dataset ([https://www.kaggle.com/snap/amazon-fine-food-reviews](https://www.kaggle.com/snap/amazon-fine-food-reviews))
    from Kaggle in this chapter. The original dataset is huge, with 100,000 rows.
    We sampled 4,000 rows. This dataset contains customer reviews about fine food
    sold on Amazon. Reviews include user and product information, user rating, and
    text.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the dataset from the book''s GitHub repository. Type the following
    command into your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can click on the URL in the preceding command and the download
    will start. You can unzip the zip file with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, you can right-click on the ZIP file and choose **Extract here**
    to inflate the ZIP file.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we're ready to explore the dataset. In this section, we'll be using a Jupyter
    notebook. If you already have Jupyter installed, you can execute the notebook
    cells directly. If you don't have the Jupyter Notebook on your system, you can
    follow the instructions on the Jupyter website ([https://jupyter.org/install](https://jupyter.org/install)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do our dataset exploration step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll do the imports for reading and visualizing the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll read the CSV file into a pandas DataFrame and output the shape of the
    DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we examine the rows and the columns of the dataset by printing the first
    10 rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting view tells us there are 10 rows including the review text and
    the review score:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![ Figure 8.4 – First 10 rows of the reviews dataframe'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16570_8_4.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.4 – First 10 rows of the reviews dataframe
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We''ll be using the `Text` and `Score` columns; hence, we''ll drop the other
    columns that we won''t use. We’ll also call the dropna() method to drop the rows
    with missing values::'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can have a quick look at how the review scores are distributed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This piece of code calls the `plot` method of `dataframe reviews_df` and exhibits
    a bar plot:![Figure 8.5 – Distribution of review scores
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16570_8_5.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.5 – Distribution of review scores
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The number of 5-star ratings is quite high; it looks like customers are happy
    with the food they purchased. However, it might create an imbalance in the training
    data if a class has significantly more weight than the others.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Class imbalance** creates trouble for classification algorithms in general.
    For example, it is considered as imbalance when a class has significantly more
    training examples than the other classes (usually a ratio of 1:5 between examples).
    There are different ways to handle imbalance, one way is **up**-**sampling**/**down**-**sampling**.
    In down-sampling, we randomly remove training examples from the majority class.
    In up-sampling, we randomly replicate training example from the minority class.
    Both methods aim to balance the number of training examples of majority and minority
    classes.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here we’ll apply another method. We’ll combine 1,2,3 star reviews and 4,5 star
    reviews to get a more balanced dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In order to prevent this, we''ll treat 1-, 2-, and 3-star ratings as negative
    and ratings that have more than 4 stars as positive. The following code segment
    assigns a negative label to all the reviews that have a rating of fewer than 4
    stars and a positive label to all the reviews that have a higher rating than 4
    stars:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s plot the distribution of the ratings again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting rating distribution looks much better than *Figure 8.5*. Still,
    the number of positive reviews is greater, but the number of negative reviews
    is significant as well, as can be seen from the following graph:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Distribution of positive and negative ratings'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_8_6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.6 – Distribution of positive and negative ratings
  prefs: []
  type: TYPE_NORMAL
- en: 'After processing the dataset, we reduced it to a two-column dataset with negative
    and positive ratings. We call `reviews_df.head()` once again and the following
    is the result we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – The DataFrame’s first four rows'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_8_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.7 – The DataFrame's first four rows
  prefs: []
  type: TYPE_NORMAL
- en: We'll finish our dataset exploration here. We saw the distribution of the review
    scores and the class labels. The dataset is now ready to be processed. We dropped
    the unused columns and converted review scores to binary class labels. Let's go
    ahead and start the training procedure!
  prefs: []
  type: TYPE_NORMAL
- en: Training the TextClassifier component
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we''re ready to start the training procedure. We''ll train a binary text
    classifier with the multilabel classifier this time. Again, let''s go step by
    step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the spaCy classes as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll create a pipeline object, `nlp`, define the classifier configuration,
    and add the `TextCategorizer` component to `nlp` with the following configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After creating the text classifier component, we''ll convert training sentences
    and ratings into a spaCy usable format. We''ll iterate every row of the DataFrame
    with `iterrows()`, and for each row we''ll extract the `Text` and `Score` fields.
    Then, we''ll create a spaCy `Doc` object from the review text and make a dictionary
    of the class labels as well. Finally, we will create an `Example` object and append
    it to the list of training examples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll use `POS` and `NEG` labels for positive and negative sentiment, respectively.
    We''ll introduce these labels to the new component and also initialize the component
    with examples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''re ready to define the training loop! We went over the training set for
    two epochs, but you can go over more if you like. The following code snippet will
    train the new text categorizer component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we''ll test how the text classifier component works for two example
    sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Both the `NEG` and `POS` labels appear in the prediction result because we used
    the multilabel classifier. The results look good. The first sentence outputs a
    very high positive probability, and the second sentence is predicted as negative
    with a high probability.
  prefs: []
  type: TYPE_NORMAL
- en: We've completed training spaCy's text classifier component. In the next section,
    we'll dive into the world of a very popular deep learning library, Keras. We'll
    explore how to write Keras code to do text classification by using another popular
    machine learning library – TensorFlow's Keras API. Let's go ahead and explore
    Keras and TensorFlow!
  prefs: []
  type: TYPE_NORMAL
- en: Text classification with spaCy and Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn about methods for blending spaCy with neural
    networks using another very popular Python deep learning library, **TensorFlow**,
    and its high-level API, **Keras**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep learning** is a broad family of machine learning algorithms that are
    based on neural networks. **Neural networks** are human brain-inspired algorithms
    that contain connected layers, which are made from neurons. Each neuron is a mathematical
    operation that takes its input, multiplies it by its weights, and then passes
    the sum through the activation function to the other neurons. The following diagram
    shows a neural network architecture with three layers -- the **input layer**,
    **hidden layer**, and **output layer**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – A neural network architecture with three layers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_8_8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.8 – A neural network architecture with three layers
  prefs: []
  type: TYPE_NORMAL
- en: '**TensorFlow** is an end-to-end open source platform for machine learning.
    TensorFlow might be the most popular deep learning library among research engineers
    and scientists. It has huge community support and great documentation, available
    at [https://www.tensorflow.org/](https://www.tensorflow.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Keras** is a high-level deep learning API that can run on top of popular
    machine learning libraries such as TensorFlow, Theano, and CNTK. Keras is very
    popular in the research and development world because it supports rapid prototyping
    and provides a user-friendly API to neural network architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: '**TensorFlow 2** introduced great changes in machine learning methods by tightly
    integrating with Keras and providing a high-level API, **tf.keras**. TensorFlow
    1 was a bit ugly with symbolic graph computations and other low-level computations.
    With TensorFlow 2, developers can take advantage of Keras'' user-friendliness
    as well as TensorFlow''s low-level methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are commonly used for computer vision and NLP tasks, including
    object detection, image classification, and scene understanding as well as text
    classification, POS tagging, text summarization, and natural language generation.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we'll go through the details of a neural network
    architecture for text classification implemented with `tf.keras`. Throughout this
    section, we'll use TensorFlow 2 as we stated in the *Technical requirements* section.
    Let's warm up to neural networks with some neural network basics, and then start
    building our Keras code.
  prefs: []
  type: TYPE_NORMAL
- en: What is a layer?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A neural network is formed by connecting layers. **Layers** are basically the
    building blocks of the neural network. A layer consists of several **neurons**,
    as in *Figure 8.8*.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 8.8*, the first layer of this neural network has two layers, and
    the second layer has six neurons. Each neuron in each layer is connected to all
    neurons in the next layer. Each layer might have different functionalities; some
    layers can lower the dimensions of their input, some layers can flatten their
    input (flattening means collapsing a multidimensional vector into one dimension),
    and so on. At each layer, we transform the input vectors and feed them to the
    next layer to get a final vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras provides different sorts of layers, such as input layers, dense layers,
    dropout layers, embedding layers, activation layers, recurrent layers, and so
    on. Let''s get to know some useful layers one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input layer**: The input layer is responsible for sending our input data
    to the rest of the network. While initializing an input layer, we provide the
    input data shape.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dense layers**: Dense layers transform the input of a given shape to the
    output shape we want. Layer 2 in *Figure 8.8* represents a dense layer, which
    collapses a 5-dimensional input into a 1-dimensional output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recurrent layers**: Keras provides strong support for RNN, GRU, and LSTM
    cells. If you''re not familiar with RNN variations at all, please refer to the
    resources in the *Technical requirements* section. We''ll use an LSTM layer in
    our code. The *LSTM layer* subsection contains the input and output shape information.
    In the next subsection, *Sequential modeling with LSTMs*, we''ll get into details
    of modeling with LSTMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dropout layers**: Dropout is a technique to prevent overfitting. Overfitting
    happens when neural networks memorize data instead of learning it. Dropout layers
    randomly select a given number of neurons and set their weights to zero for the
    forward and backward passes, that is, for one iteration. We usually place dropout
    layers after dense layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are the basic layers that are used in NLP models. The next subsection
    is devoted to modeling sequential data with LSTMs, which is the core of statistical
    modeling for NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential modeling with LSTMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**LSTM** is an RNN variation. **RNNs** are special neural networks that can
    process sequential data in steps. In usual neural networks, we assume that all
    the inputs and outputs are independent of each other. Of course, it''s not true
    for text data. Every word''s presence depends on the neighbor words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, during a machine translation task, we predict a word by considering
    all the words we predicted before. RNNs capture information about the past sequence
    elements by holding a `i`, we feed the input word `xi`, and RNN outputs a value,
    `hi`, for this time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – RNN illustration, taken from Colah’s notable blog on LSTMs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_8_9.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.9 – RNN illustration, taken from Colah's notable blog on LSTMs
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs were invented to fix some computational problems of RNNs. RNNs have the
    problem of forgetting some data back in the sequence, as well as some numerical
    stability issues due to chain multiplications called **vanishing and exploding
    gradients**. If you are interested, you can refer to Colah's blog, the link to
    which you will find in the *References* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'An LSTM cell is slightly more complicated than an RNN cell, but the logic of
    computation is the same: we feed one input word at each time step and LSTM outputs
    an output value at each time step. The following diagram shows what''s inside
    an LSTM cell. Note that the input steps and output steps are identical to the
    RNN counterparts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – LSTM illustration from Colah’s LSTM blog article'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_8_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.10 – LSTM illustration from Colah's LSTM blog article
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras has extensive support for the RNN variations GRU and LSTM, as well as
    a simple API for training RNNs. RNN variations are crucial for NLP tasks, as language
    data''s nature is sequential: text is a sequence of words, speech is a sequence
    of sounds, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have learned what type of statistical model to use in our design,
    we can switch to a more practical subject: how to represent a sequence of words.
    In the next section, we''ll learn how to transform a sequence of words into a
    sequence of word IDs and build vocabularies at the same time with Keras''s preprocessing
    module.'
  prefs: []
  type: TYPE_NORMAL
- en: Keras Tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we remarked in the previous section, text is sequential data (a sequence
    of words or characters). We''ll feed a sentence as a sequence of words. Neural
    networks can work only with vectors, so we need a way to vectorize the words.
    In [*Chapter 5*](B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087), *Working with
    Word Vectors and Semantic Similarity*, we saw how to vectorize words with word
    vectors. A word vector is a continuous representation of a word. In order to vectorize
    a word, we follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We tokenize each sentence and turn sentences into a sequence of words.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a vocabulary from the set of words present in *step 1*. These are
    words that are supposed to be recognized by our neural network design.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating a vocabulary should assign an ID to each word.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then word IDs are mapped to word vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s look at a short example. We can work on a small corpus of three sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s first tokenize the words into sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we iterated over all tokens of the Doc object generated
    by calling `nlp(sentence)`. Notice that we didn't filter out the punctuation marks.
    Filtering punctuation depends on the task. For instance, in sentiment analysis,
    punctuation marks such as "!" correlate to the result. In this example, we'll
    keep the punctuation marks as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras''s text preprocessing module is used to create vocabularies and `trn`
    word sequences into word-ID sequences with the `Tokenizer` class. The following
    code segment exhibits how to use a `Tokenizer` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We imported `Tokenizer` from the Keras text preprocessing module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We created a `tokenizer` object with the parameter `lower=True`, which means
    `tokenizer` should lower all words while building the vocabulary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We called `tokenizer.fit_on_texts` on `data` to build the vocabulary. `fit_on_text`
    works on a sequence of words; input should always be a list of words.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We examined the vocabulary by printing `tokenizer.word_index`. `Word_index`
    is basically a dictionary where keys are vocabulary words and values are word-IDs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In order to get a word''s word-ID, we call `tokenizer.texts_to_sequences`.
    Notice that the input to this method should always be a list, even if we want
    to feed only one word. In the following code segment, we feed one-word input as
    a list (notice the list brackets):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The reverse of `texts_to_sequences` is the `sequences_to_texts`. `sequences_to_texts`
    method will input a list of lists and return the corresponding word sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We also notice that the word-IDs start from `1`, not `0`. `0` is a reserved
    value and has a special meaning, which means a padding value. Keras cannot process
    sentences of different lengths, hence we need to pad shorter sentences to reach
    the longest sentence''s length. We pad each sentence of the dataset to a maximum
    length by adding padding words either to the start or end of the sentence. Keras
    inserts `0` for the padding, which means it''s not a real word, but a padding
    value. Let''s understand padding with a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Our sequences are of length 1, 2, and 4\. We called `pad_sequences` on this
    list of sequences and every sequence is padded with zeros such that its length
    reaches `MAX_LEN=4`, the length of the longest sequence. We can pad the sequences
    from the right or left with the `post` and `pre` options. In the preceding code,
    we padded our sentences with the `post` option, hence the sentences are padded
    from the right.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we put it all together, the complete text preprocessing steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Now, we've transformed sentences into a sequence of word-IDs. We've come one
    step closer to vectorizing the words. In the next subsection, we'll finally transform
    words into vectors. Then our sentences will be ready to be fed into the neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''re ready to transform words into word vectors. Embedding words into vectors
    happens via an embedding table. An embedding table is basically a lookup table.
    Each row holds the word vector of a word. We index the rows by word-IDs, hence
    the flow of obtaining a word''s word vector is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Tokenizer`. `Tokenizer` holds all the vocabulary and maps each vocabulary
    word to an ID, which is an integer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**word-ID->word vector**: A word-ID is an integer and therefore can be used
    as an index to the embedding table''s rows. Each word-ID corresponds to one row
    and when we want to get a word''s word vector, we first obtain its word-ID and
    then do a lookup in the embedding table rows with this word-ID.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following diagram shows how embedding words into word vectors works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Steps of transforming a word to its word vector with Keras'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_8_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.11 – Steps of transforming a word to its word vector with Keras
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that in the previous section, we started with a list of sentences.
    Then we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We broke each sentence into words and built a vocabulary with Keras' `Tokenizer`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `Tokenizer` object held a word index, which was a **word->word-ID** mapping.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After obtaining the word-ID, we could do a lookup to the embedding table rows
    with this word-ID and got a word vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we fed this word vector to the neural network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training a neural network is not easy. We have to take several steps to transform
    sentences into vectors. After these preliminary steps, we're ready to design the
    neural network architecture and do the model training.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network architecture for text classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will design the neural network architecture for our text
    classifier. We''ll follow these steps to train the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we'll preprocess, tokenize, and pad the review sentences. After this
    step, we'll obtain a list of sequences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll feed this list of sequences to the neural network through the input layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we'll vectorize each word by looking up its word ID in the embedding layer.
    At this point, a sentence is now a sequence of word vectors, each word vector
    corresponding to a word.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, we'll feed the sequence of word vectors to LSTM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we'll squash the LSTM output with a sigmoid layer to obtain class probabilities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's get started by remembering the dataset again.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''ll use the same Amazon fine food reviews dataset from the *Sentiment analysis
    with spaCy* section. We already processed the dataset with pandas in that section
    and reduced it to two columns and binary labels. Here is how the `reviews_df`
    dataset looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 - Result of the reviews_df.head()'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_8_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.12 – Result of the reviews_df.head()
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll transform our dataset a bit. We''ll extract the review text and review
    label from each dataset row and append them into Python lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we appended a list of words to `train_examples`, hence each element
    of this list is a list of words. Next, we'll invoke Keras' `Tokenizer` on this
    list of words to build our vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Data and vocabulary preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We already processed our dataset, hence we are ready to tokenize the dataset
    sentences and create a vocabulary. Let''s go step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll do the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''re ready to fit the `Tokenizer` object on our list of words. First, we''ll
    fit the `Tokenizer`, then we''ll convert words to their IDs by calling `texts_to_sequences`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we''ll pad the short sequences to a maximum length of `50` (we picked
    this number). Also, this will truncate long reviews to a length of `50` words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now `X` is a list of sequences of `50` words. Finally, we''ll convert this
    list of reviews and the labels to `numpy` arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At this point, we''re ready to feed our data to our neural network. We''ll
    feed our data to the input layer. For all the necessary imports, please follow
    the notebook of this section from our GitHub repository: [https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter08/Keras_train.ipynb](https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter08/Keras_train.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Here, notice that we didn’t do any lemmatization/stemming or stopwords removal.
    This is completely fine and indeed the standard way to go with neural network
    algorithms, because words that are variations of the same root word (liked, liking,
    like) will obtain similar word vectors (recall from [*Chapter 5*](B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087)*,
    Working with Word Vectors and Semantic Similarity* that similar words obtain similar
    word vectors). Also, stopwords occur frequently in different contexts, hence neural
    network can deduce that these words are just common words of the language and
    don't carry much importance.
  prefs: []
  type: TYPE_NORMAL
- en: The input layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following piece of code defines our input layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Don't be confused by `None` as the input shape. Here, `None` means that this
    dimension can be any scalar number, hence, we use this expression when we want
    Keras to infer the input shape.
  prefs: []
  type: TYPE_NORMAL
- en: The embedding layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We define the embedding layer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: While defining the embedding layer, the input dimension should always be the
    number of words in the vocabulary (here, there's a plus `1` because the indices
    start from `1`, not `0`. Index `0` is reserved for the padding value).
  prefs: []
  type: TYPE_NORMAL
- en: Here, we chose the output shape to be 100, hence the word vectors for the vocabulary
    words will be 100-dimensional. Popular numbers for word vector dimensions are
    50, 100, and 200 depending on the complexity of the task.
  prefs: []
  type: TYPE_NORMAL
- en: The LSTM layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''ll feed the word vectors to our LSTM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `units` parameter means the dimension of the hidden state. The LSTM
    output shape and hidden state shape are the same due to the LSTM architecture.
    Here, our LSTM layer will output a `256`-dimensional vector.
  prefs: []
  type: TYPE_NORMAL
- en: The output layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We obtained a `256`-dimensional vector from the LSTM layer and we want to squash
    it to a `1`-dimensional vector (possible values of this vector are `0` and `1`,
    which are the class labels):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We used the sigmoid function to squash the values. The sigmoid function is an
    S-shaped function and maps its input to a [0-1] range. You can find out more about
    this function at [https://deepai.org/machine-learning-glossary-and-terms/sigmoid-function](https://deepai.org/machine-learning-glossary-and-terms/sigmoid-function).
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After defining the model, we need to compile it with an optimizer, a loss function,
    and an evaluation metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '**Adaptive Moment Estimation** (**ADAM**) is a popular optimizer in deep learning.
    It basically adapts how fast the neural network should learn. You can learn about
    different optimizers in this blog post: [https://ruder.io/optimizing-gradient-descent/](https://ruder.io/optimizing-gradient-descent/).
    Binary cross-entropy is a loss that is used in binary classification tasks. Keras
    supports different loss functions depending on the tasks. You can find the list
    on the Keras website at [https://keras.io/api/losses/](https://keras.io/api/losses/).'
  prefs: []
  type: TYPE_NORMAL
- en: A **metric** is a function that we use to evaluate our model's performance.
    The accuracy metric basically compares how many times the predicted label and
    the real label matches. A list of supported metrics can be found in Keras's documentation
    ([https://keras.io/api/metrics/](https://keras.io/api/metrics/)).
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the model and experiment evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, we''ll fit the model on our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Here, `x` is the list of training examples and `y` is the list of labels. We
    want to make 5 passes over the data, hence we set the `epochs` parameter to `5`.
  prefs: []
  type: TYPE_NORMAL
- en: We went over the data `5` times in batch sizes of `64`. Usually, we don't fit
    all of the dataset into the memory at once (due to memory limitations), but we
    feed the dataset to the classifier in smaller chunks, each chunk being called
    a `batch_size=64` means we want to feed a batch of 64 training sentences at once.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the parameter `validation_split` is used to evaluate the experiment.
    This parameter simply will separate 20 percent of the data as the validation set
    and validate the model on this validation set. Our experiment results in 0.795
    accuracy, which is quite good for such a basic neural network design.
  prefs: []
  type: TYPE_NORMAL
- en: We encourage you to experiment more. You can experiment with the code more by
    placing dropout layers at different locations (such as after the embedding layer
    or after the LSTM layer). Another way of experimenting is to try different values
    for the embedding dimensions, such as 50, 150, and 200, and observe the change
    in the accuracy. The same applies to the LSTM layer's hidden dimension – you can
    experiment with different values instead of 256.
  prefs: []
  type: TYPE_NORMAL
- en: We finished training with `tf.keras` in this section and also concluded the
    chapter. Keras is a great, efficient, and user-friendly deep learning API; the
    spaCy and Keras combination is especially powerful. Text classification is an
    essential task of NLP and we discovered how to do this task with spaCy.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have finished this chapter about a very hot NLP topic – text classification.
    In this chapter, you first learned about text classification concepts such as
    binary classification, multilabel classification, and multiclass classification.
    Next, you learned how to train `TextCategorizer`, spaCy's text classifier component.
    You learned how to transform your data into spaCy training format and then train
    the `TextCategorizer` component with this data.
  prefs: []
  type: TYPE_NORMAL
- en: After learning text classification with spaCy's `TextCategorizer`, in the final
    section, you learned how to combine spaCy code and Keras code. First, you learned
    the basics of neural networks, including some handy layers such as the dense layer,
    dropout layer, embedding layer, and recurrent layers. Then, you learned how to
    tokenize and preprocess the data with Keras' `Tokenizer`.
  prefs: []
  type: TYPE_NORMAL
- en: You had a quick review of sequential modeling with LSTMs, as well as recalling
    word vectors from [*Chapter 5*](B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087),
    *Working with Word Vectors and Semantic Similarity*, to understand the embedding
    layer better. Finally, you went through neural network design with `tf.keras`
    code. You learned how to design and evaluate a statistical experiment with LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: Looks like a lot! Indeed, it is a lot of material; no worries if it takes time
    to digest. Practicing text classification can be intense, but in the end, you
    earn crucial NLP skills.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next chapter is again devoted to a brand-new technology: **transformers**.
    In the next chapter, we''ll explore how to design high-accuracy NLP pipelines
    in only a few lines. Let''s move onto the next chapter and see what transformers
    offer for your NLP skills!'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It''d be good but not mandatory if you''re familiar with neural networks, particularly
    RNN variations. Here is some great material for neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Free online book: *Neural Networks and Deep Learning* ([http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Video tutorial at [https://www.youtube.com/watch?v=ob1yS9g-Zcs](https://www.youtube.com/watch?v=ob1yS9g-Zcs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RNN variations, especially LSTMs, have great tutorials too:'
  prefs: []
  type: TYPE_NORMAL
- en: 'RNN tutorial on the WildML blog: [http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RNN tutorial by the University of Toronto:  [https://www.cs.toronto.edu/~tingwuwang/rnn_tutorial.pdf](https://www.cs.toronto.edu/~tingwuwang/rnn_tutorial.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Colah''s blog: [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blog post by Michael Phi: [https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Video tutorial at [https://www.youtube.com/watch?v=lWkFhVq9-nc](https://www.youtube.com/watch?v=lWkFhVq9-nc)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Although we have introduced neural networks in this chapter, you can read these
    references in order to learn more about how neural networks work. More explanations
    on neural network and LSTM concepts will follow in [*Chapter 10*](B16570_10_Final_JM_ePub.xhtml#_idTextAnchor173),
    *Putting Everything Together: Designing Your Chatbot with spaCy*.'
  prefs: []
  type: TYPE_NORMAL
