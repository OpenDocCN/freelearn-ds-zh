<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Python and Remote Sensing</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will discuss remote sensing. Remote sensing is about gathering a collection of information about the Earth without making physical contact with it. Typically, this means having to use satellite or aerial imagery, <strong>Light Detection and Ranging</strong> (<strong>LIDAR</strong>), which measures laser pulses from an aircraft to the Earth, or synthetic aperture radar. Remote sensing can also refer to processing data that's been collected, which is how we'll use the term in this chapter. <span><span>Remote sensing</span></span> grows in a more exciting way every day as more satellites are launched and the distribution of data becomes easier. The high availability of satellite and aerial images, as well as interesting new types of sensors launching each year, is changing the role that remote sensing plays in understanding our world.</p>
<p>In remote sensing, we step through each pixel in an image and perform some form of query or mathematical process. An image can be thought of as a large numerical array. In remote sensing, these arrays can be quite large, in the order of tens of megabytes to several gigabytes in size. While Python is fast, only C-based libraries can provide the speed that's needed to loop through arrays at a tolerable speed.</p>
<p>We'll use the <strong>Python Imaging Library</strong> (<strong>PIL</strong>) for image processing and NumPy, which provides multidimensional array mathematics. While written in C for speed, these libraries are designed for Python and provide a Pythonic API.</p>
<p>In this chapter, we'll cover the following topics:</p>
<ul>
<li>Swapping image bands</li>
<li>Creating image histograms</li>
<li class="h1">Performing a histogram stretch</li>
<li>Clipping and classifying images</li>
<li>Extracting features from images</li>
<li>Change detection</li>
</ul>
<p><span>First, we'll start with basic image manipulation and then build on each exercise, all the way to automatic change detection. These techniques will compliment the previous chapters by adding the ability to process satellite data and other remote sensing products to our toolbox.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<ul>
<li>Python 3.6 or higher</li>
<li>RAM: Minimum 6 GB (Windows), 8 GB (macOS), recommended 8 GB</li>
<li>Storage: Minimum 7200 RPM SATA with 20 GB of available space; recommended<span> S</span>SD with 40 GB of available space</li>
<li class="mce-root">Processor: Minimum Intel Core i3 2.5 GHz; recommended Intel Core i5</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Swapping image bands</h1>
                </header>
            
            <article>
                
<p>Our eyes can only see colors in the visible spectrum as combinations of <strong>red, green, and blue</strong> (<strong>RGB</strong>). Air and space-borne sensors can collect wavelengths of the energy outside of the visible spectrum. To view this data, we move images representing different wavelengths of light reflectance in and out of the RGB channels to make color images.</p>
<p>These images often end up as bizarre and alien color combinations that can make visual analysis difficult. An example of a typical satellite image is shown in the following Landsat 7 satellite scene near the NASA Stennis Space Center in Mississippi along the Gulf of Mexico, which is a leading center for remote sensing and geospatial analysis in general:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/49b6f836-41c4-4c32-ae3a-ae7c8b75071c.png" style="width:29.00em;height:30.08em;" width="704" height="730"/></p>
<p>Most of the vegetation appears red and water appears almost black. This image is a type of false-color image, meaning the color of the image is not based on the RGB light. However, we can change the order of the bands or swap out certain bands to create another type of false-color image that looks more like the world we are used to seeing. To do so, you first need to download this image as a ZIP file from here: <a href="https://git.io/vqs41">https://git.io/vqs41</a>.</p>
<p>We installed the GDAL library with Python bindings in <a href="ff05aa7d-3aac-40b6-b857-e6bf08498141.xhtml">Chapter 4</a>, <em>Geospatial Python Toolbox</em>, in the <em>Installing GDAL and NumPy</em> section. The GDAL library includes a module called <kbd>gdal_array</kbd> that loads and saves remotely-sensed images to and from NumPy arrays for easy manipulation. GDAL itself is a data access library and does not provide much in the name of processing. So, in this chapter, we will rely heavily on NumPy to actually change images.</p>
<p>In this example, we'll load the image into a NumPy array using <kbd>gdal_array</kbd> and then we'll immediately save it back to a new GeoTiff file. However, upon saving, we'll use NumPy's advanced array-slicing feature to change the order of the bands. Images in NumPy are multi-dimensional arrays in the order of band, height, and width. This means that an image with three bands will be an array of length 3, containing an array for the band, height, and width of the image. It's important to note that NumPy references array locations as <em>y,x (row, column)</em> instead of the usual <em>x, y (column, row)</em> format we work with in spreadsheets and other software. Let's get started:</p>
<ol>
<li>First, we'll import <kbd>gdal_array</kbd>:</li>
</ol>
<pre style="padding-left: 60px">from gdal import gdal_array</pre>
<ol start="2">
<li>Next, we'll load an image named <kbd>FalseColor.tif</kbd> into a <kbd>numpy</kbd> array:</li>
</ol>
<pre style="padding-left: 60px"># name of our source image<br/>src = "FalseColor.tif"<br/># load the source image into an array<br/>arr = gdal_array.LoadFile(src)</pre>
<ol start="3">
<li>Next, we'll reorder the image bands by slicing the array, rearranging the order, and saving it back out:</li>
</ol>
<pre style="padding-left: 60px"># swap bands 1 and 2 for a natural color image.<br/># We will use numpy "advanced slicing" to reorder the bands.<br/># Using the source image<br/>output = gdal_array.SaveArray(arr[[1, 0, 2], :], "swap.tif",<br/> format="GTiff", prototype=src)<br/># Dereference output to avoid corrupted file on some platforms<br/>output = None</pre>
<p>In the <kbd>SaveArray</kbd> method, the last argument is called a <strong>prototype</strong>. This argument lets you specify another image for GDAL from which you copy spatial reference information and some other image parameters. Without this argument, we'd end up with an image without georeferencing information, which could not be used in a GIS. In this case, we specify our input image file name because the images are identical, except for the band order. In this method, you can tell that the Python GDAL API is a wrapper around a C library and is not as Pythonic as a Python-designed library. For example, a pure Python library would have written the <kbd>SaveArray()</kbd> method as <kbd>save_array()</kbd> to follow Python standards.</p>
<p>The result of this example produces the <kbd>swap.tif</kbd> image, which is a much more visually appealing image with green vegetation and blue water:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/d57d2ccf-ba6e-4595-a858-620f2869675f.png" style="width:27.83em;height:28.83em;" width="704" height="730"/></p>
<p style="margin-bottom: .0001pt" class="NormalPACKT">There's only one problem with this image: it's kind of dark and difficult to see. Let's see if we can figure out why in the next section.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Creating histograms</h1>
                </header>
            
            <article>
                
<p>A histogram shows the statistical frequency of data distribution within a dataset. In the case of remote sensing, the dataset is an image. The data distribution is the frequency of pixels in the range of <strong>0</strong> to <strong>255</strong>, which is the range of 8-byte numbers that are used to store image information on computers.</p>
<p>In an RGB image, color is represented as a 3-digit tuple with <em>(0,0,0, 0, 0)</em> being black and <em>(255,255,255)</em> being white. We can graph the histogram of an image with the frequency of each value along the y-axis and the range of 256 possible pixel values along the x-axis.</p>
<p>Remember in <a href="6b5bd08a-170c-4471-a3f3-d79d5b91f017.xhtml">Chapter 1</a>, <em>Learning about Geospatial Analysis with Python</em>, in the <em>Creating the simplest possible Python GIS </em>section, when we used the Turtle graphics engine included with Python to create a simple GIS? Well, we can also use it to easily graph histograms.</p>
<p>Histograms are usually a one-off product that makes a quick script. Also, histograms are typically displayed as a bar graph with the width of the bars representing the size of grouped data bins. But, in an image, each <kbd>bin</kbd> is only one value, so we'll create a line graph. We'll use the histogram function in this example and create a red, green, and blue line for each respective band.</p>
<p>The graphing portion of this example also defaults to scaling the <em>y</em>-axis values to the max RGB frequency found in the image. Technically, the <em>y</em>-axis represents the maximum frequency, which is the number of pixels in the image, which would be the case if the image was all one color. We'll use the <kbd>turtle</kbd> module again here, but this example could be easily converted into any graphical output module. Let's take a look at the <kbd>swap.tif</kbd> image we created in the previous example:</p>
<ol>
<li>First, we import the libraries we need, including the <kbd>turtle</kbd> graphics library:</li>
</ol>
<pre style="padding-left: 60px">from gdal import gdal_array<br/>import turtle as t</pre>
<ol start="2">
<li>Now, we create a <kbd>histogram</kbd> function that can take an array and sort the numbers into bins making up the histogram:</li>
</ol>
<pre style="padding-left: 60px">def histogram(a, bins=list(range(0, 256))):<br/> fa = a.flat<br/> n = gdal_array.numpy.searchsorted(gdal_array.numpy.sort(fa), bins)<br/> n = gdal_array.numpy.concatenate([n, [len(fa)]])<br/> hist = n[1:]-n[:-1]<br/> return hist</pre>
<ol start="3">
<li>Finally, we have our <kbd>turtle</kbd> graphics function that takes a histogram and draws it:</li>
</ol>
<pre style="padding-left: 60px">def draw_histogram(hist, scale=True):</pre>
<ol start="4">
<li>Draw the graph axes using the following code:</li>
</ol>
<pre style="padding-left: 60px">t.color("black")<br/>axes = ((-355, -200), (355, -200), (-355, -200), (-355, 250))<br/>t.up()<br/>for p in axes:<br/>  t.goto(p)<br/>  t.down()<br/>  t.up()</pre>
<ol start="5">
<li>Then, we can label them:</li>
</ol>
<pre style="padding-left: 60px">t.goto(0, -250)<br/>t.write("VALUE", font=("Arial, ", 12, "bold"))<br/>t.up()<br/>t.goto(-400, 280)<br/>t.write("FREQUENCY", font=("Arial, ", 12, "bold"))<br/>x = -355<br/>y = -200<br/>t.up()</pre>
<ol start="6">
<li>Now, we'll add tick marks on the x-axis so that we can see the line values:</li>
</ol>
<pre style="padding-left: 60px">for i in range(1, 11):<br/>  x = x+65<br/>  t.goto(x, y)<br/>  t.down()<br/>  t.goto(x, y-10)<br/>  t.up()<br/>  t.goto(x, y-25)<br/>  t.write("{}".format((i*25)), align="center")</pre>
<ol start="7">
<li>We'll do the same for the y-axis:</li>
</ol>
<pre style="padding-left: 60px">x = -355<br/>y = -200<br/>t.up()<br/>pixels = sum(hist[0])<br/>if scale:<br/>  max = 0<br/>  for h in hist:<br/>    hmax = h.max()<br/>    if hmax &gt; max:<br/>      max = hmax<br/>  pixels = max<br/>label = int(pixels/10)<br/>for i in range(1, 11):<br/>  y = y+45<br/>  t.goto(x, y)<br/>  t.down()<br/>  t.goto(x-10, y)<br/>  t.up()<br/>  t.goto(x-15, y-6)<br/>  t.write("{}".format((i*label)), align="right")</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="8">
<li>We can begin plotting our histogram lines:</li>
</ol>
<pre style="padding-left: 60px">x_ratio = 709.0 / 256<br/>y_ratio = 450.0 / pixels<br/>colors = ["red", "green", "blue"]<br/>for j in range(len(hist)):<br/>  h = hist[j]<br/>  x = -354<br/>  y = -199<br/>  t.up()<br/>  t.goto(x, y)<br/>  t.down()<br/>  t.color(colors[j])<br/>  for i in range(256):<br/>    x = i * x_ratio<br/>    y = h[i] * y_ratio<br/>    x = x - (709/2)<br/>    y = y + -199<br/>    t.goto((x, y))</pre>
<ol start="9">
<li>Finally, we can load our image and plot its histogram using the functions we defined previously:</li>
</ol>
<pre style="padding-left: 60px">im = "swap.tif"<br/>histograms = []<br/>arr = gdal_array.LoadFile(im)<br/>for b in arr:<br/>  histograms.append(histogram(b))<br/>draw_histogram(histograms)<br/>t.pen(shown=False)<br/>t.done()</pre>
<p>Here's what the histogram for <kbd>swap.tif</kbd> looks like after running the preceding code example:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/ccabeb94-8ddb-4c33-be02-0847abd958cf.png" width="783" height="562"/></p>
<p>As you can see, all three bands are grouped closely toward the left-hand side of the graph and all have values less than <strong>125</strong> or so. As these values approach zero, the image becomes darker, which is not surprising.</p>
<p>Just for fun, let's run the script again and when we call the <kbd>draw_histogram()</kbd> function, we'll add the <kbd>scale=False</kbd> option to get a sense of the size of the image and provide an absolute scale. We'll change the following line:</p>
<pre class="CodeEndPACKT">draw_histogram(histograms)</pre>
<p>This will be changed to the following:</p>
<pre class="CodeEndPACKT">draw_histogram(histograms, scale=False)</pre>
<p style="margin-bottom: .0001pt" class="NormalPACKT">This change will produce the following histogram graph:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/54b4b7e2-a5b7-42b4-92ba-981a15b4bb8c.png" width="789" height="562"/></p>
<p>As you can see, it's harder to see the details of the value distribution. However, this absolute-scale approach is useful if you are comparing multiple histograms of different products that were produced from the same source image.</p>
<p>So, now that we understand the basics of looking at an image statistically using histograms, how do we make our image brighter? Let's check this out in the next section.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Performing a histogram stretch</h1>
                </header>
            
            <article>
                
<p>A histogram stretch operation does exactly what its name says. It redistributes the pixel values across the whole scale. By doing so, we have more values at the higher-intensity level and the image becomes brighter. So, in this example, we'll reuse our histogram function, but we'll add another function called <kbd>stretch()</kbd> that takes an image array, creates the histogram, and then spreads out the range of values for each band. We'll run these functions on <kbd>swap.tif</kbd> and save the result in an image called <kbd>stretched.tif</kbd>:</p>
<pre>import gdal_array<br/>import operator<br/>from functools import reduce<br/><br/>def histogram(a, bins=list(range(0, 256))):<br/> fa = a.flat<br/> n = gdal_array.numpy.searchsorted(gdal_array.numpy.sort(fa), bins)<br/> n = gdal_array.numpy.concatenate([n, [len(fa)]])<br/> hist = n[1:]-n[:-1]<br/> return hist<br/><br/>def stretch(a):<br/> """<br/> Performs a histogram stretch on a gdal_array array image.<br/> """<br/> hist = histogram(a)<br/> lut = []<br/> for b in range(0, len(hist), 256):<br/> # step size<br/> step = reduce(operator.add, hist[b:b+256]) / 255<br/> # create equalization look-up table<br/> n = 0<br/> for i in range(256):<br/> lut.append(n / step)<br/> n = n + hist[i+b]<br/> gdal_array.numpy.take(lut, a, out=a)<br/> return asrc = "swap.tif"<br/>arr = gdal_array.LoadFile(src)<br/>stretched = stretch(arr)<br/>output = gdal_array.SaveArray(arr, "stretched.tif", format="GTiff", prototype=src)<br/>output = None</pre>
<p>The <kbd>stretch</kbd> algorithm will produce the following image. Look how much brighter and visually appealing it is:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/53cfa72c-2bc1-49bb-aba8-7402f1557e7c.png" style="width:23.75em;height:24.58em;" width="704" height="730"/></p>
<p>We can run our <kbd>turtle</kbd> graphics histogram script on <kbd>stretched.tif</kbd> by changing the file name in the <kbd>im</kbd> variable to <kbd>stretched.tif</kbd>:</p>
<pre class="CodeEndPACKT">im = "stretched.tif"</pre>
<p style="margin-bottom: .0001pt" class="NormalPACKT">Running the preceding code will give us the following histogram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/ecc2138e-13f1-4111-abec-fe8252abe7e8.png" width="783" height="562"/></p>
<p>As you can see, all three bands are distributed evenly now. Their relative distribution to each other is the same, but, within the image, they are now spread across the spectrum.</p>
<p>Now that we can change images for better presentation, let's look at clipping them to examine a particular area of interest.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Clipping images</h1>
                </header>
            
            <article>
                
<p>Very rarely is an analyst interested in an entire satellite scene, which can easily cover hundreds of square miles. Given the size of satellite data, we are highly motivated to reduce the size of an image to only our area of interest. The best way to accomplish this reduction is to clip an image to a boundary that defines our study area. We can use shapefiles (or other vector data) as our boundary definition and basically get rid of all the data outside that boundary.</p>
<p>The following image contains our <kbd>stretched.tif</kbd> image with a county boundary file layered on top, visualized in <strong>Quantum GIS</strong> (<strong>QGIS</strong>):</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/c175ce35-001f-41a4-a87a-c9e164b6f95e.png" style="width:24.33em;height:25.25em;" width="704" height="730"/></p>
<p>To clip the image, we need to follow these steps:</p>
<ol>
<li>Load the image into an array using <kbd>gdal_array</kbd>.</li>
<li>Create a shapefile reader using PyShp.</li>
<li>Rasterize the shapefile into a georeferenced image (convert it from a vector into a raster).</li>
<li>Turn the shapefile image into a binary mask or filter to only grab the image pixels we want within the shapefile boundary.</li>
<li>Filter the satellite image through the mask.</li>
<li>Discard satellite image data outside the mask.</li>
<li>Save the clipped satellite image as <kbd>clip.tif</kbd>.</li>
</ol>
<p>We installed PyShp in <a href="ff05aa7d-3aac-40b6-b857-e6bf08498141.xhtml">Chapter 4</a>, <em>Geospatial Python Toolbox</em>, so you should already have it installed from PyPi. We will also add a couple of useful new utility functions in this script. The first is <kbd>world2pixel()</kbd>, which uses the GDAL GeoTransform object to do the world-coordinate to image-coordinate conversion for us.</p>
<div class="packt_infobox">It's still the same process we've used throughout this book, but it's integrated better with GDAL.</div>
<p>We also add the <kbd>imageToArray()</kbd> function, which converts a PIL image into a NumPy array. The county boundary shapefile is the <kbd>hancock.shp</kbd> boundary we've used in previous chapters, but you can also download it here if you need to: <a href="http://git.io/vqsRH">http://git.io/vqsRH</a>.</p>
<p>We use PIL because it is the easiest way to rasterize our shapefile as a mask image to filter out the pixels beyond the shapefile boundary. Let's get started:</p>
<ol>
<li>First, we'll load the libraries we need:</li>
</ol>
<pre style="padding-left: 60px">import operator<br/>from osgeo import gdal, gdal_array, osr<br/>import shapefile</pre>
<ol start="2">
<li>Now, we'll load PIL. This may need to be installed slightly differently on different platforms, so we have to check for that difference:</li>
</ol>
<pre style="padding-left: 60px">try:<br/> import Image<br/> import ImageDraw<br/>except:<br/> from PIL import Image, ImageDraw</pre>
<ol start="3">
<li>Now, we will set up the variables for our input image, shapefile, and our output image:</li>
</ol>
<pre style="padding-left: 60px"># Raster image to clip<br/>raster = "stretched.tif"<br/># Polygon shapefile used to clip<br/>shp = "hancock"<br/># Name of clipped raster file(s)<br/>output = "clip"</pre>
<ol start="4">
<li>Next, create a function that simply converts an image into a <kbd>numpy</kbd> array so that we can convert the mask image we will create and use it in our NumPy-based clipping process:</li>
</ol>
<pre style="padding-left: 60px">def imageToArray(i):<br/> """<br/> Converts a Python Imaging Library array to a gdal_array image.<br/> """<br/> a = gdal_array.numpy.fromstring(i.tobytes(), 'b')<br/> a.shape = i.im.size[1], i.im.size[0]<br/> return a</pre>
<ol start="5">
<li>Next, we need a function to convert geospatial coordinates into image pixels, which will allow us to use coordinates from our clipping shapefile to limit which image pixels are saved:</li>
</ol>
<pre style="padding-left: 60px">def world2Pixel(geoMatrix, x, y):<br/> """<br/> Uses a gdal geomatrix (gdal.GetGeoTransform()) to calculate<br/> the pixel location of a geospatial coordinate<br/> """<br/> ulX = geoMatrix[0]<br/> ulY = geoMatrix[3]<br/> xDist = geoMatrix[1]<br/> yDist = geoMatrix[5]<br/> rtnX = geoMatrix[2]<br/> rtnY = geoMatrix[4]<br/> pixel = int((x - ulX) / xDist)<br/> line = int((ulY - y) / abs(yDist))<br/> return (pixel, line)</pre>
<ol start="6">
<li>Now, we can load our source image into a <kbd>numpy</kbd> array:</li>
</ol>
<pre style="padding-left: 60px"># Load the source data as a gdal_array array<br/>srcArray = gdal_array.LoadFile(raster)</pre>
<ol start="7">
<li>We'll also load the source image as a gdal image because <kbd>gdal_array</kbd> does not give us the geotransform information we need to convert coordinates into pixels:</li>
</ol>
<pre style="padding-left: 60px"># Also load as a gdal image to get geotransform (world file) info<br/>srcImage = gdal.Open(raster)<br/>geoTrans = srcImage.GetGeoTransform()</pre>
<ol start="8">
<li>Now, we'll use the Python shapefile library to open our shapefile:</li>
</ol>
<pre style="padding-left: 60px"># Use pyshp to open the shapefile<br/>r = shapefile.Reader("{}.shp".format(shp))</pre>
<ol start="9">
<li>Next, we'll convert the shapefile bounding box coordinates into image coordinates based on our source image:</li>
</ol>
<pre style="padding-left: 60px"># Convert the layer extent to image pixel coordinates<br/>minX, minY, maxX, maxY = r.bbox<br/>ulX, ulY = world2Pixel(geoTrans, minX, maxY)<br/>lrX, lrY = world2Pixel(geoTrans, maxX, minY)</pre>
<ol start="10">
<li>Then, we can calculate the size of our output image based on the extents of the shapefile and take just that part of the source image:</li>
</ol>
<pre style="padding-left: 60px"># Calculate the pixel size of the new image<br/>pxWidth = int(lrX - ulX)<br/>pxHeight = int(lrY - ulY)<br/>clip = srcArray[:, ulY:lrY, ulX:lrX]</pre>
<ol start="11">
<li>Next, we'll create new geomatrix data for the output image:</li>
</ol>
<pre style="padding-left: 60px"># Create a new geomatrix for the image<br/># to contain georeferencing data<br/>geoTrans = list(geoTrans)<br/>geoTrans[0] = minX<br/>geoTrans[3] = maxY</pre>
<ol start="12">
<li>Now, we can create a simple black-and-white mask image from the shapefile that will define the pixels we want to extract from the source image:</li>
</ol>
<pre style="padding-left: 60px"># Map points to pixels for drawing the county boundary<br/># on a blank 8-bit, black and white, mask image.<br/>pixels = []<br/>for p in r.shape(0).points:<br/> pixels.append(world2Pixel(geoTrans, p[0], p[1]))<br/>rasterPoly = Image.new("L", (pxWidth, pxHeight), 1)<br/># Create a blank image in PIL to draw the polygon.<br/>rasterize = ImageDraw.Draw(rasterPoly)<br/>rasterize.polygon(pixels, 0)</pre>
<ol start="13">
<li>Next, we convert the mask image into a <kbd>numpy</kbd> array:</li>
</ol>
<pre style="padding-left: 60px"># Convert the PIL image to a NumPy array<br/>mask = imageToArray(rasterPoly)</pre>
<ol start="14">
<li>Finally, we're ready to use the mask array to clip the source array in <kbd>numpy</kbd> and save it to a new geotiff image:</li>
</ol>
<pre style="padding-left: 60px"># Clip the image using the mask<br/>clip = gdal_array.numpy.choose(mask, (clip, 0)).astype(<br/> gdal_array.numpy.uint8)<br/># Save ndvi as tiff<br/>gdal_array.SaveArray(clip, "{}.tif".format(output),<br/> format="GTiff", prototype=raster)</pre>
<p class="mce-root"/>
<p>This script produces the following clipped image:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/c79fbf2a-441a-4cf1-856b-2c65d9aa1f65.png" style="width:18.58em;height:23.83em;" width="573" height="730"/></p>
<p><span>The areas that remain outside the county boundary that appear in black are actually called <kbd>NoData</kbd> values, meaning there is no information at that location, and are ignored by most geospatial software. Because images are rectangular, the <kbd>NoData</kbd> values are common for data that does not completely fill an image.</span></p>
<p>You have now walked through an entire workflow that is used by geospatial analysts around the world every day to prepare multispectral satellite and aerial images for use in a GIS. We'll look at how we can actually analyze images as information in the next section.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Classifying images</h1>
                </header>
            
            <article>
                
<p><strong>Automated remote sensing</strong> (<strong>ARS</strong>) is rarely ever done in the visible spectrum. ARS processes images without any human input. The most commonly available wavelengths outside of the visible spectrum are infrared and near-infrared.</p>
<p>The following illustration is a thermal image (band 10) from a fairly recent Landsat 8 flyover of the US Gulf Coast from New Orleans, Louisiana to Mobile, Alabama. The major natural features in the image have been labeled so that you can orient yourself:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/e1a8105d-059b-43a4-ad0c-ba5d31674e10.png" style="width:28.75em;height:27.67em;" width="737" height="709"/></p>
<p>Because every pixel in that image has a reflectance value, it is information as opposed to just color. The type of reflectance can tell us definitively what a feature is, as opposed to us guessing by looking at it. Python can see those values and pick out features the same way we intuitively do by grouping related pixel values. We can colorize pixels based on their relation to each other to simplify the image and view-related features. This technique is called <strong>classification</strong>.</p>
<p>Classifying can range from fairly simple groupings, based only on some value distribution algorithm derived from the histogram, to complex methods involving training datasets and even computer learning and artificial intelligence. The simplest forms are called <strong>unsupervised classifications</strong>, in which no additional input is given other than the image itself. Methods involving some sort of training data to guide the computer are called <strong>supervised classifications</strong>. It should be noted that classification techniques are used across many fields, from medical doctors searching for cancerous cells in a patient's body scan, to casinos using facial-recognition software on security videos to automatically spot known <strong>con-artists at blackjack tables</strong>.</p>
<p>To introduce remote sensing classification, we'll just use the histogram to group pixels with similar colors and intensities and see what we get. First, you'll need to download the Landsat 8 scene from here: <a href="http://git.io/vByJu">http://git.io/vByJu</a>.</p>
<p>Instead of our <kbd>histogram()</kbd> function from the previous examples, we'll use the version included with NumPy that allows you to easily specify the number of bins and returns two arrays with the frequency, as well as the ranges of the bin values. We'll use the second array with the ranges as our class definitions for the image. The <kbd>lut</kbd> or look-up table is an arbitrary color palette that's used to assign colors to the 20 unsupervised classes. You can use any colors you want. Let's look at the following steps:</p>
<ol>
<li>First, we import our libraries:</li>
</ol>
<pre style="padding-left: 60px">import gdal<br/>from gdal import gdal_array, osr</pre>
<ol start="2">
<li>Next, we set up some variables for our input and output images:</li>
</ol>
<pre style="padding-left: 60px"># Input file name (thermal image)<br/>src = "thermal.tif"<br/># Output file name<br/>tgt = "classified.jpg"</pre>
<ol start="3">
<li>Load the image into a <kbd>numpy</kbd> array for processing:</li>
</ol>
<pre style="padding-left: 60px"># Load the image into numpy using gdal<br/>srcArr = gdal_array.LoadFile(src)</pre>
<ol start="4">
<li>Now, we're going to create a histogram of our image with 20 groups or <kbd>bins</kbd> that we'll use for classifying:</li>
</ol>
<pre style="padding-left: 60px"># Split the histogram into 20 bins as our classes<br/>classes = gdal_array.numpy.histogram(srcArr, bins=20)[1]</pre>
<ol start="5">
<li>Then, we'll create a look-up table that will define the color ranges for our classes so that we can visualize them:</li>
</ol>
<pre style="padding-left: 60px"># Color look-up table (LUT) - must be len(classes)+1.<br/># Specified as R, G, B tuples<br/>lut = [[255, 0, 0], [191, 48, 48], [166, 0, 0], [255, 64, 64], [255, <br/>    115, 115], [255, 116, 0], [191, 113, 48], [255, 178, 115], [0, <br/>    153, 153], [29, 115, 115], [0, 99, 99], [166, 75, 0], [0, 204, <br/>    0], [51, 204, 204], [255, 150, 64], [92, 204, 204], [38, 153, <br/>    38], [0, 133, 0], [57, 230, 57], [103, 230, 103], [184, 138, 0]]</pre>
<ol start="6">
<li>Now that our setup is complete, we can perform the classification:</li>
</ol>
<pre style="padding-left: 60px"># Starting value for classification<br/>start = 1<br/># Set up the RGB color JPEG output image<br/>rgb = gdal_array.numpy.zeros((3, srcArr.shape[0],<br/> srcArr.shape[1], ), gdal_array.numpy.float32)<br/># Process all classes and assign colors<br/>for i in range(len(classes)):<br/>  mask = gdal_array.numpy.logical_and(start &lt;= srcArr, srcArr &lt;= <br/>  classes[i])<br/> for j in range(len(lut[i])):<br/>   rgb[j] = gdal_array.numpy.choose(mask, (rgb[j], lut[i][j]))<br/> start = classes[i]+1</pre>
<ol start="7">
<li>Finally, we can save our classified image:</li>
</ol>
<pre style="padding-left: 60px"># Save the image<br/>output = gdal_array.SaveArray(rgb.astype(gdal_array.numpy.uint8), tgt, format="JPEG")<br/>output = None</pre>
<p>The following image is our classification output, which we just saved as a JPEG:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/a2271208-f024-4758-893d-4a53fde9e4f5.png" style="width:27.67em;height:27.25em;" width="800" height="789"/></p>
<p><span>We didn't specify the prototype argument when saving this as an image, so it has no georeferencing information, though we could easily have done otherwise to save the output as a GeoTIFF.</span></p>
<p>This result isn't bad for a very simple unsupervised classification. The islands and coastal flats show up as different shades of green. The clouds were isolated as shades of orange and dark blues. We did have some confusion inland where the land features were colored the same as the Gulf of Mexico. We could further refine this process by defining the class ranges manually instead of just using the histogram.</p>
<p> Now that we have the ability to separate features in the image, we can try to extract features as vector data for inclusion in a GIS.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Extracting features from images</h1>
                </header>
            
            <article>
                
<p>The ability to classify an image leads us to another remote sensing capability. Now that you've worked with shapefiles over the last few chapters, have you ever wondered where they come from? Vector GIS data such as shapefiles are typically extracted from remotely-sensed images such as the examples we've seen so far.</p>
<p>Extraction normally involves an analyst clicking around each object in an image and drawing the feature to save it as data. But with good remotely-sensed data and proper pre-processing, it is possible to automatically extract features from an image.</p>
<p>For this example, we'll take a subset of our Landsat 8 thermal image to isolate a group of barrier islands in the Gulf of Mexico. The islands appear white as the sand is hot and the cooler water appears black (y<span>ou can download this image here: </span><a href="http://git.io/vqarj">http://git.io/vqarj</a>):</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/bdd03cfc-b79b-424a-b888-f7f571fbb1f2.png" style="width:44.75em;height:16.50em;" width="800" height="295"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Our goal with this example is to automatically extract the three islands in the image as a shapefile. But before we can do that, we need to mask out any data we aren't interested in. For example, the water has a wide range of pixel values, as do the islands themselves. If we just want to extract the islands themselves, we need to push all the pixel values into just two bins to make the image black and white. This technique is called <strong>thresholding</strong>. The islands in the image have enough contrast with the water in the background that thresholding should isolate them nicely.</p>
<p>In the following script, we will read the image into an array and then histogram the image using only two bins. We will then use the colors black and white to color the two bins. This script is simply a modified version of our classification script with very limited output. Let's look at the following steps:</p>
<ol>
<li>First, we import the one library we need:</li>
</ol>
<pre style="padding-left: 60px">from gdal import gdal_array</pre>
<ol start="2">
<li>Next, we define the variables for our input and output image:</li>
</ol>
<pre style="padding-left: 60px"># Input file name (thermal image)<br/>src = "islands.tif"<br/># Output file name<br/>tgt = "islands_classified.tiff"</pre>
<ol start="3">
<li>Then, we can load the image:</li>
</ol>
<pre style="padding-left: 60px"># Load the image into numpy using gdal<br/>srcArr = gdal_array.LoadFile(src)</pre>
<ol start="4">
<li>Now, we can set up our simple classification scheme:</li>
</ol>
<pre style="padding-left: 60px"># Split the histogram into 20 bins as our classes<br/>classes = gdal_array.numpy.histogram(srcArr, bins=2)[1]<br/>lut = [[255, 0, 0], [0, 0, 0], [255, 255, 255]]</pre>
<ol start="5">
<li>Next, we classify the image:</li>
</ol>
<pre style="padding-left: 60px"># Starting value for classification<br/>start = 1<br/># Set up the output image<br/>rgb = gdal_array.numpy.zeros((3, srcArr.shape[0], srcArr.shape[1], ),<br/> gdal_array.numpy.float32)<br/># Process all classes and assign colors<br/>for i in range(len(classes)):<br/>  mask = gdal_array.numpy.logical_and(start &lt;= srcArr, srcArr &lt;= <br/>  classes[i])<br/> for j in range(len(lut[i])):<br/>   rgb[j] = gdal_array.numpy.choose(mask, (rgb[j], lut[i][j]))<br/>   start = classes[i]+1</pre>
<ol start="6">
<li>Finally, we save the image:</li>
</ol>
<pre style="padding-left: 60px"># Save the image<br/>gdal_array.SaveArray(rgb.astype(gdal_array.numpy.uint8),<br/> tgt, format="GTIFF", prototype=src) </pre>
<p> The output looks great, as shown in the following image:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/ae499dec-3998-48cc-ad4d-5ec670e52f70.png" style="width:39.08em;height:13.33em;" width="800" height="273"/></p>
<p>The islands are clearly isolated, so our extraction script will be able to identify them as polygons and save them to a shapefile. The GDAL library has a method called <kbd>Polygonize()</kbd> that does exactly that. It groups all sets of isolated pixels in an image and saves them out as a feature dataset. One interesting technique we will use in this script is to use our input image as a mask.</p>
<p>The <kbd>Polygonize()</kbd> method allows you to specify a mask that will use the color black as a filter that will prevent the water from being extracted as a polygon, and we'll end up with just the islands. Another area to note in the script is that we copy the georeferencing information from our source image to our shapefile to geolocate it properly. Let's look at the following steps:</p>
<ol>
<li>First, we import our libraries:</li>
</ol>
<pre style="padding-left: 60px">import gdal<br/>from gdal import ogr, osr</pre>
<ol start="2">
<li>Next, we set up our input and output image and shapefile variables:</li>
</ol>
<pre style="padding-left: 60px"># Thresholded input raster name<br/>src = "islands_classified.tiff"<br/># Output shapefile name<br/>tgt = "extract.shp"<br/># OGR layer name<br/>tgtLayer = "extract"</pre>
<ol start="3">
<li>Let's open our input image and get the first and only band:</li>
</ol>
<pre style="padding-left: 60px"># Open the input raster<br/>srcDS = gdal.Open(src)<br/># Grab the first band<br/>band = srcDS.GetRasterBand(1)</pre>
<ol start="4">
<li>Then, we'll tell <kbd>gdal</kbd> to use that band as a mask:</li>
</ol>
<pre style="padding-left: 60px"># Force gdal to use the band as a mask<br/>mask = band</pre>
<ol start="5">
<li>Now, we're ready to set up our shapefile:</li>
</ol>
<pre style="padding-left: 60px"># Set up the output shapefile<br/>driver = ogr.GetDriverByName("ESRI Shapefile")<br/>shp = driver.CreateDataSource(tgt)</pre>
<ol start="6">
<li>Then, we need to copy our spatial reference information from the source image to the shapefile, to locate it on the Earth:</li>
</ol>
<pre style="padding-left: 60px"># Copy the spatial reference<br/>srs = osr.SpatialReference()<br/>srs.ImportFromWkt(srcDS.GetProjectionRef())<br/>layer = shp.CreateLayer(tgtLayer, srs=srs)</pre>
<ol start="7">
<li>Now, we can set up our shapefile attributes:</li>
</ol>
<pre style="padding-left: 60px"># Set up the dbf file<br/>fd = ogr.FieldDefn("DN", ogr.OFTInteger)<br/>layer.CreateField(fd)<br/>dst_field = 0</pre>
<ol start="8">
<li>Finally, we can extract our polygons:</li>
</ol>
<pre style="padding-left: 60px"># Automatically extract features from an image!<br/>extract = gdal.Polygonize(band, mask, layer, dst_field, [], None) </pre>
<p>The output shapefile is simply called <kbd>extract.shp</kbd>. As you may remember from <a href="ff05aa7d-3aac-40b6-b857-e6bf08498141.xhtml">Chapter 4</a>, <em>Geospatial Python Toolbox</em>, we created a quick pure Python script using PyShp and PNG Canvas to visualize shapefiles. We'll bring that script back here so that we can look at our shapefile, but we'll add something extra to it. The largest island has a small lagoon which shows up as a hole in the polygon. To properly render it, we have to deal with parts in a shapefile record.</p>
<p>The previous example using that script did not do that, so we'll add that piece as we loop through the shapefile features in the following steps:</p>
<ol>
<li>First, we need to import the libraries we'll need:</li>
</ol>
<pre style="padding-left: 60px">import shapefile<br/>import pngcanvas</pre>
<ol start="2">
<li>Next, we get the spatial information from the shapefile that will allow us to map coordinates to pixels:</li>
</ol>
<pre style="padding-left: 60px">r = shapefile.Reader("extract.shp")<br/>xdist = r.bbox[2] - r.bbox[0]<br/>ydist = r.bbox[3] - r.bbox[1]<br/>iwidth = 800<br/>iheight = 600<br/>xratio = iwidth/xdist<br/>yratio = iheight/ydist</pre>
<ol start="3">
<li>Now, we'll create a list to hold our polygons:</li>
</ol>
<pre style="padding-left: 60px">polygons = []</pre>
<ol start="4">
<li>Then, we will loop through the shapefile and collect our polygons:</li>
</ol>
<pre style="padding-left: 60px">for shape in r.shapes():<br/> for i in range(len(shape.parts)):<br/> pixels = []<br/> pt = None<br/> if i &lt; len(shape.parts)-1:<br/>   pt = shape.points[shape.parts[i]:shape.parts[i+1]]<br/> else:<br/>   pt = shape.points[shape.parts[i]:]</pre>
<ol start="5">
<li>Next, we map each point to an image pixel:</li>
</ol>
<pre style="padding-left: 60px"> for x, y in pt:<br/>   px = int(iwidth - ((r.bbox[2] - x) * xratio))<br/>   py = int((r.bbox[3] - y) * yratio)<br/>   pixels.append([px, py])<br/> polygons.append(pixels)</pre>
<ol start="6">
<li>Next, we draw the image using our polygon pixel information in <kbd>PNGCanvas</kbd>:</li>
</ol>
<pre style="padding-left: 60px">c = pngcanvas.PNGCanvas(iwidth, iheight)<br/>for p in polygons:<br/> c.polyline(p)</pre>
<p class="mce-root"/>
<ol start="7">
<li>Finally, we save the image:</li>
</ol>
<pre style="padding-left: 60px">with open("extract.png", "wb") as f:<br/>    f.write(c.dump())<br/>    f.close()</pre>
<p>The following image shows our automatically extracted island features: </p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/f05b6042-5339-402d-b6bb-078186d2a19b.png" style="width:36.00em;height:8.75em;" width="831" height="202"/></p>
<p><span>Commercial packages that do this kind of work can easily cost tens of thousands of dollars. While these packages are very robust, it is still fun and empowering to see how far you can get with simple Python scripts and a few open-source packages. In many cases, you can do everything you need to do.</span></p>
<p style="margin-bottom: .0001pt" class="NormalPACKT">The western-most island contains the polygon hole, as shown in the following image, and is zoomed in on that area:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/4411871b-5108-48e5-9b8c-9b455393da0c.png" style="width:30.33em;height:17.33em;" width="1173" height="671"/></p>
<div class="packt_tip">If you want to see what would happen if we didn't deal with the polygon holes, then just run the version of the script from <a href="ff05aa7d-3aac-40b6-b857-e6bf08498141.xhtml">Chapter 4</a>, <em>Geospatial Python Toolbox</em>, on this same shapefile to compare the difference. The lagoon is not easy to see, but you will find it if you use the other script.</div>
<p>Automated feature extraction is a holy grail within geospatial analysis because of the cost and tedious effort required to manually extract features. The key to feature extraction is proper image classification. Automated feature extraction works well with water bodies, islands, roads, farm fields, buildings, and other features that tend to have high-contrast pixel values with their background.</p>
<p><span>You now have a good grasp of working with remote sensing data using GDAL, NumPy, and PIL. It's time to move on to our most complex example: change detection.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding change detection</h1>
                </header>
            
            <article>
                
<p>Change detection is the process of taking two geo-registered images of the exact same area from two different dates and automatically identifying differences. It is really just another form of image classification. Just like our previous classification examples, it can range from trivial techniques like those used here, to highly-sophisticated algorithms that provide amazingly precise and accurate results.</p>
<p>For this example, we'll use two images from a coastal area. These images show a populated area before and after a major hurricane, so there are significant differences, many of which are easy to visually spot, making these samples good for learning change detection. Our technique is to simply subtract the first image from the second to get a simple image difference using NumPy. This is a valid and often used technique.</p>
<p>The advantages are it is comprehensive and very reliable. The disadvantage of this overly simple algorithm is that it doesn't isolate the type of change. Many changes are insignificant for analysis, such as the waves on the ocean. In this example, we'll mask the water fairly effectively to avoid that distraction and only focus on the higher reflectance values toward the right-hand side of the difference image histogram.</p>
<div class="packt_infobox">You can download the baseline image from <a href="http://git.io/vqa6h">http://git.io/vqa6h</a>.<br/>
You can download the changed image from <a href="http://git.io/vqaic">http://git.io/vqaic</a>.<br/>
Note these images are quite large <span>–</span> 24 MB and 64 MB, respectively!</div>
<p>The baseline image is panchromatic, while the changed image is in false color. Panchromatic images are created by sensors that capture all visible light and are typical of higher resolution sensors rather than multispectral sensors that capture bands containing restricted wavelengths.</p>
<p>Normally, you would use two identical band combinations, but these samples will work for our purposes. The visual markers we can use to evaluate change detection include a bridge in the southeast quadrant of the image that spans from the Peninsula to the edge of the image. This bridge is clearly visible in the before image and is reduced to pilings by the hurricane. Another marker is a boat in the northwest quadrant which appears in the after image as a white trail but is not in the before image.</p>
<p>A neutral marker is the water and the state highway, which runs through the town and connects to the bridge. This feature is easily visible concrete, which does not change significantly between the two images. The following is a screenshot of the baseline image<span>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/432f940f-0245-47e7-979a-35a54b04bfeb.png" width="800" height="581"/></p>
<p>To view these images up-close yourself, you should use QGIS or OpenEV (FWTools), as described in the <em>Quantum GIS and OpenEv</em> <span>section </span>in <a href="a5e439d1-e7fd-46b4-8fd3-8f811bfe73e4.xhtml"/><a href="a5e439d1-e7fd-46b4-8fd3-8f811bfe73e4.xhtml">Chapter 3</a>, <em>The Geospatial Technology Landscape</em>, to view them easily. The following image is the after image:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/aaeb9249-4c99-47fc-a5dd-5f1d557e577f.png" width="800" height="581"/></p>
<p>So, let's perform change detection:</p>
<ol>
<li>First, we load our libraries:</li>
</ol>
<pre style="padding-left: 60px">import gdal<br/>from gdal import gdal_array<br/>import numpy as np</pre>
<ol start="2">
<li>Now, we set up the variables for our input and output images:</li>
</ol>
<pre style="padding-left: 60px"># "Before" image<br/>im1 = "before.tif"<br/># "After" image<br/>im2 = "after.tif"</pre>
<ol start="3">
<li>Next, we read<span> </span>both images into NumPy arrays with <kbd>gdal_array</kbd>:</li>
</ol>
<pre style="padding-left: 60px"># Load before and after into arrays<br/>ar1 = gdal_array.LoadFile(im1).astype(np.int8)<br/>ar2 = gdal_array.LoadFile(im2)[1].astype(np.int8)</pre>
<ol start="4">
<li>Now, we subtract the before image from the after image (difference = after – before):</li>
</ol>
<pre style="padding-left: 60px"># Perform a simple array difference on the images<br/>diff = ar2 - ar1</pre>
<ol start="5">
<li>Then, we divide the image into five classes:</li>
</ol>
<pre style="padding-left: 60px"># Set up our classification scheme to try<br/># and isolate significant changes<br/>classes = np.histogram(diff, bins=5)[1]</pre>
<ol start="6">
<li>Next, we set our color table to use black to mask the lower classes. We do this to filter water and roads because they are darker in the image:</li>
</ol>
<pre style="padding-left: 60px"># The color black is repeated to mask insignificant changes<br/>lut = [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 255, 0], [255, 0, 0]]</pre>
<ol start="7">
<li>Then, we assign colors to the classes:</li>
</ol>
<pre style="padding-left: 60px"># Starting value for classification<br/>start = 1<br/># Set up the output image<br/>rgb = np.zeros((3, diff.shape[0], diff.shape[1], ), np.int8)<br/># Process all classes and assign colors<br/>for i in range(len(classes)):<br/> mask = np.logical_and(start &lt;= diff, diff &lt;= classes[i])<br/> for j in range(len(lut[i])):<br/> rgb[j] = np.choose(mask, (rgb[j], lut[i][j]))<br/> start = classes[i]+1</pre>
<p class="mce-root"/>
<ol start="8">
<li>Finally, we save our image:</li>
</ol>
<pre style="padding-left: 60px"># Save the output image<br/>output = gdal_array.SaveArray(rgb, "change.tif", format="GTiff", prototype=im2)<br/>output = None</pre>
<p style="margin-bottom: .0001pt" class="NormalPACKT">Here's what our initial difference image looks like:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/79fec589-c670-41f0-8612-98194620107b.png" width="800" height="581"/></p>
<p>For the most part, the green classes represent areas where something was added. The red would be a darker value where something was probably removed. We can see that the boat trail is green in the northwest quadrant. We can also see a lot of changes in vegetation, as would be expected due to seasonal differences. The bridge is an anomaly because the exposed pilings are brighter than the darker surface of the original bridge, which makes them green instead of red.</p>
<p>Concrete is a major indicator in change detection because it is very bright in sunlight and is usually a sign of new development. Conversely, if a building is torn down and the concrete is removed, the difference is also easy to identify. So, the simple difference algorithm that we used here isn't perfect, but it could be greatly improved using thresholding, masking, better class definitions, and other techniques.</p>
<p>To really appreciate our change detection product, you can overlay it on the before or after image in QGIS and set the color black to transparent, as shown in the following image:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/b9a17df0-71c6-4c87-9370-b6b7a4f7f56e.png" width="800" height="492"/></p>
<p><span><span>Potentially, you can combine this change detection analysis with the feature extraction example to extract changes as vector data that can be analyzed in a GIS efficiently.</span></span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we covered the foundations of remote sensing, including band swapping, histograms, image classification, feature extraction, and change detection. Like in the other chapters, we stayed as close to pure Python as possible, and where we compromised on this goal for processing speed, we limited the software libraries as much as possible to keep things simple. However, if you have the tools from this chapter installed, you really have a complete remote sensing package that is limited only by your desire to learn.</p>
<p>The techniques in this chapter are foundational to all remote sensing processes and will allow you to build more complex operations.</p>
<p>In the next chapter, we'll investigate elevation data. Elevation data doesn't fit squarely in GIS or remote sensing as it has elements of both types of processing.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p><span>The authors of GDAL have a set of Python examples that cover a number of advanced topics that may be of interest to you. You can find them at <a href="https://github.com/OSGeo/gdal/tree/master/gdal/swig/python/samples">https://github.com/OSGeo/gdal/tree/master/gdal/swig/python/samples</a>.</span></p>


            </article>

            
        </section>
    </div>



  </body></html>