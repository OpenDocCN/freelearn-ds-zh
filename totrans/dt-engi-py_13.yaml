- en: '*Chapter 11*: Building a Production Data Pipeline'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第11章*: 构建生产数据管道'
- en: In this chapter, you will build a production data pipeline using the features
    and techniques that you have learned in this section of the book. The data pipeline
    will be broken into processor groups that perform a single task. Those groups
    will be version controlled and they will use the NiFi variable registry so that
    they can be deployed in a production environment.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将使用本书本节中学到的功能和技巧来构建生产数据管道。数据管道将被分成执行单个任务的处理器组。这些组将进行版本控制，并且它们将使用NiFi变量注册表，以便它们可以在生产环境中部署。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下主要内容：
- en: Creating a test and production environment
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建测试和生产环境
- en: Building a production data pipeline
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建生产数据管道
- en: Deploying a data pipeline in production
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产环境中部署数据管道
- en: Creating a test and production environment
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建测试和生产环境
- en: In this chapter, we will return to using PostgreSQL for both the extraction
    and loading of data. The data pipeline will require a test and production environment,
    each of which will have a staging and a warehouse table. To create the databases
    and tables, you will use **PgAdmin4**.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将回到使用PostgreSQL进行数据的提取和加载。数据管道需要一个测试和生产环境，每个环境都将有一个临时表和仓库表。要创建数据库和表，您将使用**PgAdmin4**。
- en: Creating the databases
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建数据库
- en: 'To use PgAdmin4, perform the following steps:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用PgAdmin4，请执行以下步骤：
- en: Browse to `http://localhostw/pgadmin4/l`, enter your username and password,
    and then click the **Login** button. Once logged in, expand the server icon in
    the left panel.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 浏览到`http://localhostw/pgadmin4/l`，输入您的用户名和密码，然后点击**登录**按钮。登录后，展开左侧面板中的服务器图标。
- en: To create the databases, right-click on the databases icon and select `test`.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建数据库，右键点击数据库图标并选择`test`。
- en: Next, you will need to add the tables. To create the staging table, right-click
    on `staging`. Then, select the **Columns** tab. Using the plus sign, create the
    fields shown in the following screenshot:![Figure 11.1 – The columns used in the
    staging table
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您需要添加表格。要创建临时表，右键点击`staging`。然后，选择**Columns**选项卡。使用加号创建以下截图所示的字段：![图11.1
    – 临时表中使用的列
- en: '](img/Figure_11.1_B15739.jpg)'
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图11.1 – Figure_11.1_B15739.jpg](img/Figure_11.1_B15739.jpg)'
- en: Figure 11.1 – The columns used in the staging table
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图11.1 – 临时表中使用的列
- en: Save the table when you are done. You will need to create this table once more
    for the test database and twice more for the production database. To save some
    time, you can use **CREATE Script** to do this for you. Right-click on the staging
    table, and then select **Scripts** | **CREATE Script**, as shown in the following
    screenshot:![Figure 11.2 – Generating the CREATE script
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后保存表格。您需要为测试数据库创建此表一次，为生产数据库创建两次。为了节省时间，您可以使用**CREATE Script**来自动完成此操作。右键点击临时表，然后选择**Scripts**
    | **CREATE Script**，如图下所示：![图11.2 – 生成CREATE脚本
- en: '](img/Figure_11.2_B15739.jpg)'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图11.2 – Figure_11.2_B15739.jpg](img/Figure_11.2_B15739.jpg)'
- en: Figure 11.2 – Generating the CREATE script
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图11.2 – 生成CREATE脚本
- en: A window will open in the main screen with the SQL required to generate the
    table. By changing the name from `staging` to `warehouse`, you can make the warehouse
    table in test, which will be identical to staging. Once you have made the change,
    click the play button in the toolbar.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主屏幕将打开一个窗口，显示生成表的SQL语句。通过将名称从`staging`更改为`warehouse`，您可以在测试环境中创建仓库表，这将与临时表相同。一旦您进行了更改，点击工具栏中的播放按钮。
- en: Lastly, right-click on `production`. Use the script to create both the tables.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，右键点击`production`。使用脚本创建这两个表。
- en: Now that you have the tables created for the test and production environments,
    you will need a data lake.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经为测试和生产环境创建了表，您将需要一个数据湖。
- en: Populating a data lake
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 填充数据湖
- en: A **data lake** is usually a place on disk where files are stored. Usually,
    you will find data lakes using Hadoop for the **Hadoop Distributed File System**
    (**HDFS**) and the other tools built on top of the Hadoop ecosystem. In this chapter,
    we will just drop files in a folder to simulate how reading from the data lake
    would work.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据湖**通常是一个磁盘上的存储文件的地方。通常，您会使用Hadoop的**Hadoop分布式文件系统**（**HDFS**）和其他构建在Hadoop生态系统之上的工具来找到数据湖。在本章中，我们将在一个文件夹中放置文件来模拟从数据湖中读取的工作方式。'
- en: To create the data lake, you can use Python and the Faker library. Before you
    write the code, create a folder to act as the data lake. I have created a folder
    named `datalake` in my home directory.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'To populate the data lake, you will need to write JSON files with information
    about an individual. This is similar to the JSON and CSV code you wrote in the
    first section of this book. The steps are as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries, set the data lake directory, and set `userid` to `1`.
    The `userid` variable is going to be a primary key, so we need it to be distinct
    – incrementing will do that for us:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, create a loop that generates a data object containing the user ID, name,
    age, street, city, state, and zip of a fake individual. The `fname` variable holds
    the first and last name of a person without a space in the middle. If you had
    a space, Linux would wrap the file in quotes:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Lastly, dump the JSON object and then write it to a file named after the person.
    Close the file and let the loop continue:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Run the preceding code and you will have 1,000 JSON files in your data lake.
    Now you can start building the data pipeline.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Building a production data pipeline
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data pipeline you build will do the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Read files from the data lake.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Insert the files into staging.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate the staging data.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Move staging to the warehouse.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The final data pipeline will look like the following screenshot:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – The final version of the data pipeline'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_11.3_B15739.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.3 – The final version of the data pipeline
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: We will build the data pipeline processor group by processor group. The first
    processor group will read the data lake.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Reading the data lake
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the first section of this book, you read files from NiFi and will do the
    same here. This processor group will consist of three processors – `GetFile`,
    `EvaluateJsonPath`, and `UpdateCounter` – and an output port. Drag the processors
    and port to the canvas. In the following sections, you will configure them.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: GetFile
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `GetFile` processor reads files from a folder, in this case, our data lake.
    If you were reading a data lake in Hadoop, you would switch out this processor
    for the `GetHDFS` processor. To configure the processor, specify the input directory;
    in my case, it is `/home/paulcrickard/datalake`. Make sure `^.*\.([jJ][sS][oO][nN]??)$`.
    If you leave the default, it will work, but if there are other files in the folder,
    NiFi will try to grab them and will fail.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: EvaluateJsonPath
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `EvaluateJsonPath` processor will extract the fields from the JSON and
    put them into flowfile attributes. To do so, set the **Destination** property
    to **flowfile-attribute**. Leave the rest of the properties as the default. Using
    the plus sign, create a property for each field in the JSON. The configuration
    is shown in the following screenshot:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – The configuration for the EvaluateJsonPath processor'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_11.4_B15739.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.4 – The configuration for the EvaluateJsonPath processor
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: This would be enough to complete the task of reading from the data lake, but
    we will add one more processor for monitoring.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这就足够完成从数据湖读取的任务了，但我们将添加一个处理器用于监控。
- en: UpdateCounter
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更新计数器
- en: This processor allows you to create an increment counter. As flowfiles pass
    through, we can hold a count of how many are being processed. This processor does
    not manipulate or change any of our data, but will allow us to monitor the progress
    of the processor group. We will be able to see the number of FlowFiles that have
    moved through the processor. This is a more accurate way than using the GUI display,
    but it only shows the number of records in the last 5 minutes. To configure the
    processor, leave the `1` and set the `datalakerecordsprocessed`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此处理器允许您创建一个增量计数器。随着流文件通过，我们可以统计正在处理的文件数量。此处理器不会操纵或更改我们的数据，但将允许我们监控处理器组的进度。我们将能够看到通过处理器的FlowFiles数量。这比使用GUI显示更准确，但它只显示过去5分钟内的记录数量。要配置处理器，保留`1`并设置`datalakerecordsprocessed`。
- en: To finish this section of the data pipeline, drag an output port to the canvas
    and name it `OutputDataLake`. Exit the processor group and right-click, select
    `ReadDataLake`, wrote a short description and version comments, and then performed
    a save.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这一节的数据管道，将一个输出端口拖到画布上并命名为`OutputDataLake`。退出处理器组，右键单击，选择`ReadDataLake`，编写简短描述和版本注释，然后保存。
- en: NiFi-Registry
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: NiFi-Registry
- en: I have created a new bucket named `DataLake`. To create buckets, you can browse
    to the registry at `http://localhost:18080/nifi-registry/`. Click the wrench in
    the right corner and then click the **NEW BUCKET** button. Name and save the bucket.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我已创建了一个名为`DataLake`的新存储桶。要创建存储桶，您可以浏览到`http://localhost:18080/nifi-registry/`上的注册表。点击右下角的扳手，然后点击**新建存储桶**按钮。命名并保存存储桶。
- en: The first processor group is complete. You can use this processor group any
    time you need to read from the data lake. The processor group will hand you every
    file with the fields extracted. If the data lake changed, you would only need
    to fix this one processor group to update all of your data pipelines.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个处理器组已完成。您可以在需要从数据湖读取数据时使用此处理器组。处理器组将向您提供每个字段提取的每个文件。如果数据湖发生变化，您只需修复这个处理器组即可更新所有数据管道。
- en: Before continuing down the data pipeline, the next section will take a small
    diversion to show how you can attach other processor groups.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续向下进行数据管道之前，下一节将稍微偏离一下，展示如何附加其他处理器组。
- en: Scanning the data lake
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扫描数据湖
- en: The goal of the data pipeline is to read the data lake and put the data in the
    data warehouse. But let's assume there is another department at our company that
    needs to monitor the data lake for certain people – maybe VIP customers. Instead
    of building a new data pipeline, you can just add their task to the `ReadDataLake`
    processor group.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道的目标是从数据湖读取数据并将数据放入数据仓库。但让我们假设我们公司里还有另一个部门需要监控某些人的数据湖——可能是VIP客户。你不必构建一个新的数据管道，只需将他们的任务添加到`ReadDataLake`处理器组中即可。
- en: 'The `ScanLake` processor group has an input port that is connected to the output
    of the `ReadDataLake` processor. It uses the `ScanContent` processor attached
    to the `EvaluateJsonPath` processor, which is terminated at the `PutSlack` processor,
    as well as sending the data through to an output port. The flow is shown in the
    following screenshot:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`ScanLake`处理器组有一个输入端口，该端口连接到`ReadDataLake`处理器的输出。它使用连接到`EvaluateJsonPath`处理器的`ScanContent`处理器，该处理器终止于`PutSlack`处理器，同时将数据发送到输出端口。流程如下面的截图所示：'
- en: '![Figure 11.5 – The ScanLake processor group'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.5 – ScanLake 处理器组'
- en: '](img/Figure_11.5_B15739.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_11.5_B15739.jpg)'
- en: Figure 11.5 – The ScanLake processor group
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 – ScanLake 处理器组
- en: The previous chapter used the `PutSlack` processor and you are already familiar
    with the `EvaluateJsonPath` processor. `ScanContent`, however, is a new processor.
    The `ScanContent` processor allows you to look at fields in the flowfile content
    and compare them to a dictionary file – a file with content on each line that
    you are looking for. I have put a single name in a file at `/home/paulcrickard/data.txt`.
    I configured the processor by setting the path as the value of the **Dictionary
    File** property. Now, when a file comes through that contains that name, I will
    get a message on Slack.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章使用了`PutSlack`处理器，您已经熟悉了`EvaluateJsonPath`处理器。然而，`ScanContent`是一个新的处理器。`ScanContent`处理器允许您查看flowfile内容中的字段，并将它们与一个字典文件进行比较——一个包含您要查找的每行内容的文件。我已经在`/home/paulcrickard/data.txt`文件中放入了一个单独的名称。我通过将路径设置为**字典文件**属性的值来配置处理器。现在，当包含该名称的文件通过时，我将在Slack上收到一条消息。
- en: Inserting the data into staging
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据插入临时表
- en: The data we read was from the data lake and will not be removed, so we do not
    need to take any intermediary steps, such as writing data to a file, as we would
    have done had the data been from a transactional database. But what we will do
    is place the data in a staging table to make sure that everything works as we
    expect before putting it in the data warehouse. To insert the data into staging
    only requires one processor, `PutSQL`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们读取的数据来自数据湖，并且不会被删除，因此我们不需要采取任何中间步骤，例如将数据写入文件，就像数据来自事务数据库时那样。但我们将要做的是将数据放入一个临时表中，以确保在将其放入数据仓库之前一切按预期工作。仅要将数据插入临时表就需要一个处理器，`PutSQL`。
- en: PutSQL
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PutSQL
- en: 'The `PutSQL` processor will allow you to execute an `INSERT` or `UPDATE` operation
    on a database table. The processor allows you to specify the query in the contents
    of a flowfile, or you can hardcode the query to use as a property in the processor.
    For this example, I have hardcoded the query in the **SQL Statement** property,
    which is shown as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`PutSQL`处理器将允许您在数据库表中执行`INSERT`或`UPDATE`操作。处理器允许您在flowfile的内容中指定查询，或者您可以将查询硬编码为处理器中的一个属性。在这个例子中，我已经在**SQL语句**属性中硬编码了查询，如下所示：'
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The preceding query takes the attributes from the flowfile and passes them into
    the query, so while it is hardcoded, it will change based on the flowfiles it
    receives. You may have noticed that you have not used `${table}` in any of the
    `EvaluateJsonPath` processors. I have declared a variable using the NiFi registry
    and added it to the processor group scope. The value of the table will be `staging`
    for this test environment, but will change later when we deploy the data pipeline
    to production.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 上述查询从flowfile中获取属性并将它们传递到查询中，因此虽然它是硬编码的，但它将根据它接收到的flowfiles而变化。您可能已经注意到，您在所有的`EvaluateJsonPath`处理器中都没有使用`${table}`。我使用NiFi注册声明了一个变量，并将其添加到处理器组作用域中。对于这个测试环境，表的值将是`staging`，但当我们将数据管道部署到生产环境时，它将稍后更改。
- en: You will also need to add a **Java Database Connection** (**JDBC**) pool, which
    you have done in earlier chapters of this book. You can specify the batch size,
    the number of records to retrieve, and whether you want the processor to roll
    back on failure. Setting **Rollback on Failure** to **True** is how you can create
    atomicity in your transactions. If a single flowfile in a batch fails, the processor
    will stop and nothing else can continue.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要添加一个**Java数据库连接**（**JDBC**）池，这在本书的前几章中已经完成。您可以指定批量大小、要检索的记录数以及是否希望在失败时回滚。将**失败时回滚**设置为**True**是您在事务中创建原子性的方法。如果批处理中的单个flowfile失败，处理器将停止，其他任何操作都无法继续。
- en: I have connected the processor to another `UpdateCounter` processor. This processor
    creates and updates `InsertedStaging`. The counter should match `datalakerecordsprocessor`
    when everything has finished. The `UpdateCounter` processor connects to an output
    port named `OutputStaging`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经将处理器连接到另一个`UpdateCounter`处理器。此处理器创建并更新`InsertedStaging`。当一切完成后，计数器应与`datalakerecordsprocessor`匹配。`UpdateCounter`处理器连接到名为`OutputStaging`的输出端口。
- en: Querying the staging database
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询临时数据库
- en: The next processor group is for querying the staging database. Now that the
    data has been loaded, we can query the database to make sure all the records have
    actually made it in. You could perform other validation steps or queries to see
    whether the results match what you would expect – if you have data analysts, they
    would be a good source of information for defining these queries. In the following
    sections, you will query the staging database and route the results based on whether
    it meets your criteria.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个处理器组用于查询阶段数据库。现在数据已经加载，我们可以查询数据库以确保所有记录实际上都已进入。您可以执行其他验证步骤或查询，以查看结果是否符合您的预期
    – 如果您有数据分析师，他们将是定义这些查询的良好信息来源。在以下章节中，您将查询阶段数据库并根据是否符合您的标准来路由结果。
- en: ExecuteSQLRecord
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ExecuteSQLRecord
- en: 'In the previous processor group, you used the `PutSQL` processor to insert
    data into the database, but in this processor group, you want to perform a `select`
    query. The `select` query is shown as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个处理器组中，您使用了`PutSQL`处理器将数据插入到数据库中，但在这个处理器组中，您想要执行一个`select`查询。`select`查询如下所示：
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The preceding query is set as the value of the optional SQL `select` query property.
    The `${table}` is a NiFi variable registry variable assigned to the processor
    group and has a value of `staging`. You will need to define a JDBC connection
    and a record writer in the processor properties. The record writer is a JSON record
    set writer. The return value of the processor will be a JSON object with one field
    – `count`. This processor is sent to an `EvaluateJsonPath` processor to extract
    the count as `recordcount`. That processor is then sent to the next processor.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的查询被设置为可选SQL `select`查询属性的值。`${table}`是分配给处理器组的NiFi变量注册表变量，其值为`staging`。您需要在处理器属性中定义一个JDBC连接和一个记录写入器。记录写入器是一个JSON记录集写入器。处理器的返回值将是一个包含一个字段
    – `count`的JSON对象。这个处理器被发送到`EvaluateJsonPath`处理器以提取`recordcount`。然后，该处理器被发送到下一个处理器。
- en: RouteOnAttribute
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RouteOnAttribute
- en: 'The `RouteOnAttribute` processor allows you to use expressions or values to
    define where a flowfile goes. To configure the processor, I have set the `allrecords`
    and set the value to a NiFi expression, shown as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`RouteOnAttribute`处理器允许您使用表达式或值来定义一个流文件的去向。为了配置处理器，我已经设置了`allrecords`并将值设置为NiFi表达式，如下所示：'
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The preceding expression evaluates the `recordcount` attribute to see whether
    it is greater than or equal to 1,000\. If it is, it will route on this relationship.
    I have attached the output to an output port named `OutputQueryStaging`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的表达式评估`recordcount`属性以查看它是否大于或等于1,000。如果是，它将基于这个关系进行路由。我已经将输出附加到名为`OutputQueryStaging`的输出端口。
- en: Validating the staging data
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证阶段数据
- en: The previous processor group did some validation and you could stop there. However,
    Great Expectations is an excellent library for handling validation for you. You
    learned about Great Expectations in [*Chapter 7*](B15739_07_ePub_AM.xhtml#_idTextAnchor086)*,
    Features of a Production Pipeline*, but I will cover it quickly again here.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个处理器组进行了一些验证，您可以在那里停止。然而，Great Expectations是一个处理验证的出色库。您在[*第7章*](B15739_07_ePub_AM.xhtml#_idTextAnchor086)*，生产管道功能*中学习了关于Great
    Expectations的内容，但我会在这里快速再次介绍它。
- en: 'To use Great Expectations, you need to create a project folder. I have done
    that in the following code snippet and initialized Great Expectations:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Great Expectations，您需要创建一个项目文件夹。我在下面的代码片段中已经创建了它，并初始化了Great Expectations：
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You will be prompted to create your validation suite. Choose **Relational database
    (SQL)**, then **Postgres**, and provide the required information. The prompts
    will look like the following screenshot:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您将被提示创建您的验证套件。选择**关系数据库（SQL**），然后**Postgres**，并提供所需信息。提示将类似于以下截图：
- en: '![Figure 11.6 – Configuring Great Expectations to work with PostgreSQL'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.6 – 配置Great Expectations以与PostgreSQL协同工作'
- en: '](img/Figure_11.6_B15739.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_11.6_B15739.jpg)'
- en: Figure 11.6 – Configuring Great Expectations to work with PostgreSQL
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6 – 配置Great Expectations以与PostgreSQL协同工作
- en: 'When it is finished, Great Expectations will attempt to connect to the database.
    If successful, it will provide the URL for your documents. Since the table is
    empty, it will not create a very detailed validation suite. You can edit the suite
    using the following command:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当它完成后，Great Expectations将尝试连接到数据库。如果成功，它将提供您文档的URL。由于表是空的，它不会创建一个非常详细的验证套件。您可以使用以下命令编辑套件：
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This will launch a Jupyter notebook with the code for the suite. I have deleted
    one line that sets the number of rows between 0 and 0, as shown in the following
    screenshot:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动一个包含套件代码的 Jupyter 笔记本。我已经删除了一行，该行设置了行数在 0 到 0 之间，如下面的截图所示：
- en: '![Figure 11.7 – Editing the Great Expectations suite'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.7 – 编辑 Great Expectations 套件'
- en: '](img/Figure_11.7_B15739.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_11.7_B15739.jpg)'
- en: Figure 11.7 – Editing the Great Expectations suite
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 – 编辑 Great Expectations 套件
- en: 'After deleting the highlighted line, run all the cells in the notebook. Now
    you can refresh your documents and you will see that the expectation on the row
    number is no longer part of the suite, as shown in the following screenshot:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 删除高亮显示的行后，运行笔记本中的所有单元格。现在您可以刷新您的文档，您将看到行数期望不再作为套件的一部分，如下面的截图所示：
- en: '![Figure 11.8 – Great Expectations documents for the suite'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.8 – 套件的 Great Expectations 文档'
- en: '](img/Figure_11.8_B15739.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_11.8_B15739.jpg)'
- en: Figure 11.8 – Great Expectations documents for the suite
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 – 套件的 Great Expectations 文档
- en: 'Now that the suite is complete, you need to generate a file that you can run
    to launch the validation. Use the following command to create a tap using the
    `staging.validation` suite and output the `sv.py` file:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在套件已完成，你需要生成一个可以运行以启动验证的文件。使用以下命令使用 `staging.validation` 套件创建一个 tap 并输出 `sv.py`
    文件：
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now you can run this file to validate the test database staging table.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以运行此文件以验证测试数据库的阶段表。
- en: The first processor receives flowfiles from an input port that is connected
    to the output port of the `QueryStaging` processor group. It connects to an `ExecuteStreamCommand`
    processor.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个处理器从连接到 `QueryStaging` 处理器组输出端口的输入端口接收流文件。它连接到一个 `ExecuteStreamCommand` 处理器。
- en: ExecuteStreamCommand
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ExecuteStreamCommand
- en: The `ExecuteStreamCommand` will execute a command and listen for output, streaming
    the results. Since the `sv.py` file only prints a single line and exits, there
    is no stream, but if your command had multiple outputs, the processor would grab
    them all as they were output.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExecuteStreamCommand` 将执行一个命令并监听输出，流式传输结果。由于 `sv.py` 文件只打印一行并退出，因此没有流，但如果你的命令有多个输出，处理器会捕获它们的所有输出。'
- en: To configure the processor, set the `sv.py`, the `sv.py` file.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置处理器，设置 `sv.py` 文件。
- en: 'The processor connects to an `EvaluateJsonPath` processor that extracts `$.result`
    and sends it to a `RouteOnAttribute` processor. I have configured a single property
    and accorded it the value `pass`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器连接到一个 `EvaluateJsonPath` 处理器，该处理器提取 `$.result` 并将其发送到 `RouteOnAttribute`
    处理器。我已经配置了一个属性并赋予它值 `pass`：
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The preceding expression checks the result attribute to see whether it matches
    `pass`. If so, the processor sends the flowfile to an output port.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 上述表达式检查结果属性以查看它是否与 `pass` 匹配。如果是这样，处理器将流文件发送到输出端口。
- en: Insert Warehouse
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Insert Warehouse
- en: You have made it to the last processor group – `ExecuteSQLRecord` and a `PutSQL`
    processor.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经到达了最后一个处理器组 - `ExecuteSQLRecord` 和一个 `PutSQL` 处理器。
- en: ExecuteSQLRecord
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ExecuteSQLRecord
- en: '`ExecuteSQLProcessor` performs a select operation on the staging table. It
    has a variable table defined in the NiFi variable registry pointing to staging.
    The query is a `select *` query, as shown:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExecuteSQLProcessor` 在阶段表中执行选择操作。它有一个在 NiFi 变量注册表中定义的变量表，指向阶段。查询是一个 `select
    *` 查询，如下所示：'
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This query is the value of the SQL `select` query property. You will need to
    set up a `Database Pooling Connection` service and a `Record Writer` service.
    `Record Writer` will be a `JsonRecordSetWriter` and you will need to make sure
    that you set `SplitText` processor, which connects to the `EvalueJsonPath` processor,
    which is a direct copy of the one from the `ReadDataLake` processor group that
    connects to the final `PutSQL` processor.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此查询是 SQL `select` 查询属性的值。您需要设置一个 `Database Pooling Connection` 服务和一个 `Record
    Writer` 服务。`Record Writer` 将是一个 `JsonRecordSetWriter`，您需要确保设置了 `SplitText` 处理器，该处理器连接到
    `EvalueJsonPath` 处理器，它是来自 `ReadDataLake` 处理器组的直接副本，连接到最终的 `PutSQL` 处理器。
- en: PutSQL
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PutSQL
- en: 'The `PutSQL` processor puts all of the data from the `staging` table into the
    final data `warehouse` table. You can configure the batch size and the rollback
    on failure properties. I have set the SQL Statement property to the same as when
    it was inserted into `staging`, except the variable for the table has been changed
    to `warehouse` and we set it to `warehouse` in the NiFi variable registry. The
    query is as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`PutSQL`处理器将`staging`表中的所有数据放入最终数据`warehouse`表。您可以配置批量大小和失败回滚属性。我已经将SQL语句属性设置为与它被插入到`staging`时相同，除了表变量已被更改为`warehouse`，我们在NiFi变量注册表中将其设置为`warehouse`。查询如下：'
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: I have terminated the processor for all relationships as this is the end of
    the data pipeline. If you start all the processor groups, you will have data in
    your `staging` and `warehouse` tables. You can check your counters to see whether
    the records processed are the same as the number of records inserted. If everything
    worked correctly, you can now deploy your data pipeline to production.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我已终止了所有关系的处理器，因为这标志着数据管道的结束。如果您启动所有处理器组，您将在`staging`和`warehouse`表中获得数据。您可以检查计数器以查看处理的记录数是否与插入的记录数相同。如果一切正常，您现在可以将数据管道部署到生产环境。
- en: Deploying a data pipeline in production
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在生产中部署数据管道
- en: 'In the previous chapter, you learned how to deploy data to production, so I
    will not go into any great depth here, but merely provide a review. To put the
    new data pipeline into production, perform the following steps:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您学习了如何将数据部署到生产环境，所以在这里我不会深入探讨，只是提供一个回顾。要将新的数据管道投入生产，请执行以下步骤：
- en: Browse to your production NiFi instance. I have another instance of NiFi running
    on port `8080` on localhost.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 浏览到您的生产NiFi实例。我在本地主机上运行了另一个NiFi实例，端口号为`8080`。
- en: Drag and drop processor groups to the canvas and select **Import**. Choose the
    latest version of the processor groups you just built.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将处理器组拖放到画布上并选择**导入**。选择您刚刚构建的处理器组的最新版本。
- en: Modify the variables on the processor groups to point to the database production.
    The table names can stay the same.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改处理器组上的变量，使其指向数据库生产。表名可以保持不变。
- en: You can then run the data pipeline and you will see that the data is populated
    in the production database `staging` and `warehouse` tables.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以运行数据管道，您将看到数据已填充到生产数据库的`staging`和`warehouse`表中。
- en: The data pipeline you just built read files from a data lake, put them into
    a database table, ran a query to validate the table, and then inserted them into
    the warehouse. You could have built this data pipeline with a handful of processors
    and been done, but when you build for production, you will need to provide error
    checking and monitoring. Spending the time up front to build your data pipelines
    properly will save you a lot of time when something changes or breaks in production.
    You will be well positioned to debug and modify your data pipeline.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 您刚刚构建的数据管道从数据湖读取文件，将它们放入数据库表，运行查询以验证表，然后将它们插入到仓库。您可以用几个处理器构建这个数据管道并完成，但当你为生产构建时，您需要提供错误检查和监控。在前期花时间正确构建您的数据管道将节省您在生产中遇到变化或故障时的大量时间。您将处于调试和修改数据管道的有利位置。
- en: Summary
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to build and deploy a production data pipeline.
    You learned how to create `TEST` and `PRODUCTION` environments and built the data
    pipeline in `TEST`. You used the filesystem as a sample data lake and learned
    how you would read files from the lake and monitor them as they were processed.
    Instead of loading data into the data warehouse, this chapter taught you how to
    use a staging database to hold the data so that it could be validated before being
    loaded into the data warehouse. Using Great Expectations, you were able to build
    a validation processor group that would scan the staging database to determine
    whether the data was ready to be loaded into the data warehouse. Lastly, you learned
    how to deploy the data pipeline into `PRODUCTION`. With these skills, you can
    now fully build, test, and deploy production batch data pipelines.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何构建和部署生产数据管道。你学习了如何创建 `TEST` 和 `PRODUCTION` 环境，并在 `TEST` 中构建了数据管道。你使用文件系统作为示例数据湖，并学习了如何从湖中读取文件以及如何监控它们在处理过程中的状态。本章不是教你将数据加载到数据仓库中，而是教你如何使用临时数据库来存储数据，以便在将其加载到数据仓库之前进行验证。使用
    Great Expectations，你能够构建一个验证处理器组，该组将扫描临时数据库以确定数据是否已准备好加载到数据仓库中。最后，你学习了如何将数据管道部署到
    `PRODUCTION`。掌握了这些技能，你现在可以完全构建、测试和部署生产批量数据管道。
- en: In the next chapter, you will learn how to build Apache Kafka clusters. Using
    Kafka, you will begin to learn how to process data streams. This data is usually
    near real time, as opposed to the batch processing you have been currently working
    with. You will install and configure the cluster to run on a single machine, or
    multiple devices if you have them.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习如何构建 Apache Kafka 集群。使用 Kafka，你将开始学习如何处理数据流。这些数据通常是接近实时，与您目前一直在处理的批量处理相比。你将安装和配置集群，使其在单个机器上运行，或者如果你有多个设备，也可以在多个设备上运行。
