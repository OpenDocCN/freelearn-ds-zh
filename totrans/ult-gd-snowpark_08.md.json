["```py\nFEATURE_LIST = [ \"HOLIDAY\", \"WORKINGDAY\", \"HUMIDITY\", \"TEMP\", \"ATEMP\", \n    \"WINDSPEED\", \"SEASON\", \"WEATHER\"]\nLABEL_COLUMNS = ['COUNT']\nOUTPUT_COLUMNS = ['PREDICTED_COUNT']\ndf = session.table(\"BSD_TRAINING\")\ndf = df.drop(\"DATETIME\",\"DATE\")\ndf.show(2)\n```", "```py\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.modeling.xgboost import XGBRegressor\nparam_grid = {\n    \"max_depth\":[3, 4, 5, 6, 7, 8],\n    \"min_child_weight\":[1, 2, 3, 4],\n}\ngrid_search = GridSearchCV(\n    estimator=XGBRegressor(),\n    param_grid=param_grid,\n    n_jobs = -1,\n    scoring=\"neg_root_mean_squared_error\",\n    input_cols=FEATURE_LIST,\n    label_cols=LABEL_COLUMNS,\n    output_cols=OUTPUT_COLUMNS\n)\n```", "```py\ntrain_df, test_df = df.random_split(weights=[0.7, 0.3], seed=0)\ngrid_search.fit(train_df)\n```", "```py\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ngs_results = grid_search.to_sklearn().cv_results_\nmax_depth_val = []\nmin_child_weight_val = []\nfor param_dict in gs_results[\"params\"]:\n    max_depth_val.append(param_dict[\"max_depth\"])\n    min_child_weight_val.append(param_dict[\"min_child_weight\"])\nmape_val = gs_results[\"mean_test_score\"]*-1\n```", "```py\ngs_results_df = pd.DataFrame(data={\n    \"max_depth\":max_depth_val,\n    \"min_child_weight\":min_child_weight_val,\n    \"mape\":mape_val})\nsns.relplot(data=gs_results_df, x=\"min_child_weight\",\n    y=\"mape\", hue=\"max_depth\", kind=\"line\")\n```", "```py\ngrid_search.to_sklearn().best_estimator_\n```", "```py\nfrom snowflake.ml.registry import model_registry\nregistry = model_registry.ModelRegistry(session=session,\n    database_name=\"SNOWPARK_DEFINITIVE_GUIDE\",\n    schema_name=\"MY_SCHEMA\", create_if_not_exists=True)\nModelRegistry instance with session information, specified database, and schema names. If not existing, it creates a registry in the SNOWPARK_DEFINITIVE_GUIDE database and MY_SCHEMA schema.\n```", "```py\noptimal_model = grid_search.to_sklearn().best_estimator_\noptimal_max_depth = \\\n    grid_search.to_sklearn().best_estimator_.max_depth\noptimal_min_child_weight = \\\n    grid_search.to_sklearn().best_estimator_.min_child_weight\noptimal_mape = gs_results_df.loc[\n    (gs_results_df['max_depth']==optimal_max_depth) &\n    (gs_results_df['min_child_weight']== \\\n        optimal_min_child_weight), 'mape'].values[0]\n```", "```py\nmodel_name = \"bike_model_xg_boost\"\nmodel_version = 1\nX = train_df.select(FEATURE_LIST).limit(100)\nregistry.log_model( model_name=model_name,\n                    model_version=model_version,\n                    model=optimal_model,\n                    sample_input_data=X,\n                    options={\"embed_local_ml_library\": True, \\\n                             \"relax\": True})\nregistry.set_metric(model_name=model_name,\n                    model_version=model_version,\n                    metric_name=\"mean_abs_pct_err\",\n                    metric_value=optimal_mape)\n```", "```py\nregistry.list_models().to_pandas()\n```", "```py\nmodel_deployment_name = model_name + f\"{model_version}\" + \"_UDF\"\nregistry.deploy(model_name=model_name,\n                model_version=model_version,\n                deployment_name=model_deployment_name,\n                target_method=\"predict\",\n                permanent=True,\n                options={\"relax_version\": True})\npredict target method, ensuring permanence in the deployment. Additionally, it includes an option to relax version constraints during deployment. Just as we’ve showcased the catalog of registered models, an equivalent insight into deployed models can be obtained using the following line of code:\n```", "```py\nregistry.list_deployments(model_name, model_version).to_pandas()\n```", "```py\nmodel_ref = model_registry.ModelReference(\n    registry=registry,\n    model_name=model_name,\n    model_version=model_version)\nresult_sdf = model_ref.predict(\n    deployment_name=model_deployment_name,\n    data=test_df)\nresult_sdf.show()\n```", "```py\nregistry.set_metric(model_name=model_name,\n                    model_version=model_version,\n                    metric_name=\"mean_abs_pct_err\",\n                    metric_value=optimal_mape)\n```", "```py\nregistry.get_metric_value(model_name=model_name,\n                          model_version=model_version,\n                          metric_name=\"mean_abs_pct_err\")\n```", "```py\nregistry.get_metrics(model_name=model_name, \n    model_version=model_version)\n```", "```py\nregistry.set_tag(model_name=model_name,\n                 model_version=model_version,\n                 tag_name=\"usage\",\n                 tag_value=\"experiment\")\nregistry.list_models().to_pandas()[[\"NAME\", \"TAGS\"]]\n```", "```py\nregistry.remove_tag(model_name=model_name,\n                    model_version=model_version,\n                    tag_name=\"usage\")\nregistry.list_models().to_pandas()[[\"NAME\", \"TAGS\"]]\n```", "```py\nregistry.set_model_description(model_name=model_name,\n    model_version=model_version,\n    description=\"this is a test model\")\nprint(registry.get_model_description(model_name=model_name,\n    model_version=model_version))\n```", "```py\nregistry.get_history().to_pandas()\n```", "```py\nregistry.get_model_history(model_name=model_name,\n    model_version=model_version).to_pandas()\n```", "```py\nregistry.delete_deployment(model_name=model_name,\n    model_version=model_version,\n    deployment_name=model_deployment_name)\n    registry.list_deployments(model_name, model_version).to_pandas()\n```", "```py\nregistry.delete_model(model_name=model_name,\n    model_version=model_version)\nregistry.list_models().to_pandas()\n```", "```py\nfrom snowflake.ml.feature_store import (\n    FeatureStore, FeatureView, Entity, CreationMode)\nfs = FeatureStore(\n    session=session,\n    database=\"SNOWPARK_DEFINITIVE_GUIDE\",\n    name=\"BIKE_SHARING_FEATURES\",\n    default_warehouse=\"COMPUTE_WH\",\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n)\n```", "```py\nentity = Entity(name=\"ENTITY_WEATHER\", join_keys=[\"ID\"])\nfs.register_entity(entity)\nfs.list_entities().show()\n```", "```py\nimport snowflake.ml.modeling.preprocessing as snowml\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.snowpark.types import IntegerType\n# CREATING ID COLUMN\nfrom snowflake.snowpark.functions \\\n    import monotonically_increasing_id\ndf = df.withColumn(\"ID\", monotonically_increasing_id())\ndf = df.drop(\"DATETIME\",\"DATE\")\nCATEGORICAL_COLUMNS = [\"SEASON\",\"WEATHER\"]\nCATEGORICAL_COLUMNS_OHE = [\"SEASON_OE\",\"WEATHER_OE\"]\nMIN_MAX_COLUMNS = [\"TEMP\"]\nimport numpy as np\ncategories = {\n    \"SEASON\": np.array([1,2,3,4]),\n    \"WEATHER\": np.array([1,2,3,4]),\n}\n```", "```py\npreprocessing_pipeline = Pipeline(\n    steps=[\n        (\n            \"OE\",\n            snowml.OrdinalEncoder(\n                input_cols=CATEGORICAL_COLUMNS,\n                output_cols=CATEGORICAL_COLUMNS_OHE,\n                categories=categories\n            )\n        ),\n        (\n            \"MMS\",\n            snowml.MinMaxScaler(\n                clip=True,\n                input_cols=MIN_MAX_COLUMNS,\n                output_cols=MIN_MAX_COLUMNS,\n            )\n        )\n    ]\n)\ntransformed_df = preprocessing_pipeline.fit(df).transform(df)\ntransformed_df.show()\n```", "```py\nfeature_df = transformed_df.select([\"SEASON_OE\",\n    \"WEATHER_OE\", \"TEMP\", \"ATEMP\", \"HUMIDITY\",\n    \"WINDSPEED\", \"ID\"])\nfv = FeatureView(\n    name=\"WEATHER_FEATURES\",\n    entities=[entity],\n    feature_df=feature_df,\n    desc=\"weather features\"\n)\nfv = fs.register_feature_view(\n    feature_view=fv,\n    version=\"V1\",\n    block=True\n)\nfs.read_feature_view(fv).show()\n```", "```py\n#GENERATING TRAINING DATA\nspine_df = session.table(\"BSD_TRAINING\")\nspine_df = spine_df.withColumn(\"ID\",\n    monotonically_increasing_id())\nspine_df = spine_df.select(\"ID\", \"COUNT\")\nspine_df.show()\ntrain_data = fs.generate_dataset(\n    spine_df=spine_df,\n    features=[\n        fv.slice([\n            \"HUMIDITY\",\"SEASON_OE\",\"TEMP\",\n            \"WEATHER_OE\",\"WINDSPEED\"\n        ])\n    ],\n    materialized_table=None,\n    spine_timestamp_col=None,\n    spine_label_cols=[\"COUNT\"],\n    save_mode=\"merge\",\n    exclude_columns=['ID']\n)\ntrain_data.df.show()\n```", "```py\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.modeling.ensemble \\\n    import GradientBoostingRegressor\nFEATURE_LIST = [\"TEMP\", \"WINDSPEED\", \"SEASON_OE\", \"WEATHER_OE\"]\nLABEL_COLUMNS = ['COUNT']\nOUTPUT_COLUMNS = ['PREDICTED_COUNT']\nparam_grid = {\n    \"n_estimators\":[100, 200, 300, 400, 500],\n    \"learning_rate\":[0.1, 0.2, 0.3, 0.4, 0.5],\n}\ngrid_search = GridSearchCV(\n    estimator=GradientBoostingRegressor(),\n    param_grid=param_grid,\n    n_jobs = -1,\n    scoring=\"neg_root_mean_squared_error\",\n    input_cols=FEATURE_LIST,\n    label_cols=LABEL_COLUMNS,\n    output_cols=OUTPUT_COLUMNS\n)\ntrain_df = train_data.df.drop([\"ID\"])\ngrid_search.fit(train_df)\n```", "```py\ntest_df = spine_df.limit(3).select(\"ID\")\nenriched_df = fs.retrieve_feature_values(\n    test_df, train_data.load_features())\nenriched_df = enriched_df.drop('ID')\nenriched_df.show()\n```", "```py\npred = grid_search.predict(enriched_df.to_pandas())\npred.head()\n```"]