- en: Chapter 2. Replacing and Correcting Words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Stemming words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lemmatizing words with WordNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating text with Babelfish
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replacing words matching regular expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing repeating characters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spelling correction with Enchant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replacing synonyms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replacing negations with antonyms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will go over various word replacement and correction techniques.
    The recipes cover the gamut of linguistic compression, spelling correction, and
    text normalization. All of these methods can be very useful for pre-processing
    text before search indexing, document classification, and text analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Stemming** is a technique for removing *affixes* from a word, ending up with
    the *stem*. For example, the stem of "cooking" is "cook", and a good stemming
    algorithm knows that the "ing" *suffix* can be removed. Stemming is most commonly
    used by search engines for indexing words. Instead of storing all forms of a word,
    a search engine can store only the stems, greatly reducing the size of index while
    increasing retrieval accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common stemming algorithms is the **Porter Stemming Algorithm**,
    by Martin Porter. It is designed to remove and replace well known suffixes of
    English words, and its usage in NLTK will be covered next.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The resulting stem is not always a valid word. For example, the stem of "cookery"
    is "cookeri". This is a feature, not a bug.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NLTK comes with an implementation of the Porter Stemming Algorithm, which is
    very easy to use. Simply instantiate the `PorterStemmer` class and call the `stem()`
    method with the word you want to stem.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `PorterStemmer` knows a number of regular word forms and suffixes, and uses
    that knowledge to transform your input word to a final stem through a series of
    steps. The resulting stem is often a shorter word, or at least a common form of
    the word, that has the same root meaning.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are other stemming algorithms out there besides the Porter Stemming Algorithm,
    such as the **Lancaster Stemming Algorithm**, developed at Lancaster University.
    NLTK includes it as the `LancasterStemmer` class. At the time of writing, there
    is no definitive research demonstrating the superiority of one algorithm over
    the other. However, Porter Stemming is generally the default choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the stemmers covered next inherit from the `StemmerI` interface, which
    defines the `stem()` method. The following is an inheritance diagram showing this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more...](img/3609OS_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: LancasterStemmer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `LancasterStemmer` functions just like the `PorterStemmer`, but can produce
    slightly different results. It is known to be slightly more aggressive than the
    `P` `orterStemmer`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: RegexpStemmer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can also construct your own stemmer using the `RegexpStemmer`. It takes
    a single regular expression (either compiled or as a string) and will remove any
    prefix or suffix that matches.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A `RegexpStemmer` should only be used in very specific cases that are not covered
    by the `PorterStemmer` or `LancasterStemmer`.
  prefs: []
  type: TYPE_NORMAL
- en: SnowballStemmer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'New in NLTK 2.0b9 is the `SnowballStemmer`, which supports 13 non-English languages.
    To use it, you create an instance with the name of the language you are using,
    and then call the `s` `tem()` method. Here is a list of all the supported languages,
    and an example using the Spanish `SnowballStemmer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next recipe, we will cover lemmatization, which is quite similar to stemming,
    but subtly different.
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatizing words with WordNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Lemmatization** is very similar to stemming, but is more akin to synonym
    replacement. A *lemma* is a root word, as opposed to the root *stem*. So unlike
    stemming, you are always left with a valid word which means the same thing. But
    the word you end up with can be completely different. A few examples will explain
    lemmatization...'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Be sure you have unzipped the `wordnet` corpus in `nltk_data/corpora/wordnet`.
    This will allow the `WordNetLemmatizer` to access WordNet. You should also be
    somewhat familiar with the part-of-speech tags covered in the *Looking up synsets
    for a word in WordNet* recipe of [Chapter 1](ch01.html "Chapter 1. Tokenizing
    Text and WordNet Basics"), *Tokenizing Text and WordNet Basics*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the `WordNetLemmatizer` to find lemmas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `WordNetLemmatizer` is a thin wrapper around the WordNet corpus, and uses
    the `morphy()` function of the `W` `ordNetCorpusReader` to find a lemma. If no
    lemma is found, the word is returned as it is. Unlike with stemming, knowing the
    part of speech of the word is important. As demonstrated previously, "cooking"
    does not have a lemma unless you specify that the part of speech (`pos`) is a
    *verb*. This is because the default part of speech is a *noun*, and since "cooking"
    is not a noun, no lemma is found. "Cookbooks", on the other hand, is a noun, and
    its lemma is the singular form, "cookbook".
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s an example that illustrates one of the major differences between stemming
    and lemmatization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Instead of just chopping off the "es" like the `PorterStemmer`, the `WordNetLemmatizer`
    finds a valid root word. Where a stemmer only looks at the form of the word, the
    lemmatizer looks at the meaning of the word. And by returning a lemma, you will
    always get a valid word.
  prefs: []
  type: TYPE_NORMAL
- en: Combining stemming with lemmatization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'S temming and lemmatization can be combined to compress words more than either
    process can by itself. These cases are somewhat rare, but they do exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: I n this example, stemming saves one character, lemmatizing saves two characters,
    and stemming the lemma saves a total of three characters out of five characters.
    That is nearly a 60% compression rate! This level of word compression over many
    thousands of words, while unlikely to always produce such high gains, can still
    make a huge difference.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous recipe, we covered stemming basics and WordNet was introduced
    in the *Looking up synsets for a word in WordNet* and *Looking up lemmas* and
    *synonyms in WordNet* recipes of [Chapter 1](ch01.html "Chapter 1. Tokenizing
    Text and WordNet Basics"), *Tokenizing Text and WordNet Basics*. Looking forward,
    we will cover the *Using WordNet for Tagging* recipe in [Chapter 4](ch04.html
    "Chapter 4. Part-of-Speech Tagging"), *Part-of-Speech Tagging*.
  prefs: []
  type: TYPE_NORMAL
- en: Translating text with Babelfish
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Babelfish** is an online language translation API provided by Yahoo. With
    it, you can translate text in a *source language* to a *target language*. NLTK
    comes with a simple interface for using it.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Be sure you are connected to the internet first. The `babelfish.translate()`
    function requires access to Yahoo's online API in order to work.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To translate your text, you first need to know two things:'
  prefs: []
  type: TYPE_NORMAL
- en: The language of your text or source language.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The language you want to translate to or target language.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Language detection is outside the scope of this recipe, so we will assume you
    already know the source and target languages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You cannot translate using the same language for both source and target. Attempting
    to do so will raise a `BabelfishChangedError`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `translate()` function is a small function that sends a `urllib` request
    to [http://babelfish.yahoo.com/translate_txt](http://babelfish.yahoo.com/translate_txt),
    and then searches the response for the translated text.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If Yahoo, for whatever reason, had changed their HTML response to the point
    that `translate()` cannot identify the translated text, a `BabelfishChangedError`
    will be raised. This is unlikely to happen, but if it does, you may need to upgrade
    to a newer version of NLTK and/or report the error.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is also a fun function called `babelize()` that translates back and forth
    between the source and target language until there are no more changes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Available languages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can see all the languages available for translation by examining the `available_languages`
    attribute.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The lowercased version of each of these languages can be used as a source or
    target language for translation.
  prefs: []
  type: TYPE_NORMAL
- en: Replacing words matching regular expressions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we are going to get into the process of replacing words. Where stemming
    and lemmatization are a kind of *linguistic compression*, and word replacement
    can be thought of as *error correction*, or *text normalization*.
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe, we will be replacing words based on regular expressions, with
    a focus on *expanding contractions*. Remember when we were tokenizing words in
    [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing
    Text and WordNet Basics* and it was clear that most tokenizers had trouble with
    contractions? This recipe aims to fix that by replacing contractions with their
    expanded forms, such as by replacing "can't" with "cannot", or "would've" with
    "would have".
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understanding how this recipe works will require a basic knowledge of regular
    expressions and the `re` module. The key things to know are *matching patterns*
    and the `re.subn()` function.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we need to define a number of replacement patterns. This will be a list
    of tuple pairs, where the first element is the pattern to match on, and the second
    element is the replacement.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will create a `RegexpReplacer` class that will compile the patterns,
    and provide a `replace()` method to substitute all found patterns with their replacements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code can be found in the `replacers.py` module and is meant to
    be imported, not typed into the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is a simple usage example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`RegexpReplacer.replace()` works by replacing every instance of a replacement
    pattern with its corresponding substitution pattern. In `replacement_patterns`,
    we have defined tuples such as (`r''(\w+)\''ve'', ''\g<1> have''`). The first
    element matches a group of ASCII characters followed by `''ve`. By grouping the
    characters before the `''ve` in parenthesis, a match group is found and can be
    used in the substitution pattern with the `\g<1>` reference. So we keep everything
    before `''ve`, then replace `''ve` with the word `have`. This is how "should''ve"
    can become "should have".'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This replacement technique can work with any kind of regular expression, not
    just contractions. So you could replace any occurrence of "&" with "and", or eliminate
    all occurrences of "-" by replacing it with the empty string. The `RegexpReplacer`
    can take any list of replacement patterns for whatever purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Replacement before tokenization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let us try using the `RegexpReplacer` as a preliminary step before tokenization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Much better! By eliminating the contractions in the first place, the tokenizer
    will produce cleaner results. Cleaning up text before processing is a common pattern
    in natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more information on tokenization, see the first three recipes in [Chapter
    1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing Text
    and WordNet Basics*. For more replacement techniques, continue reading the rest
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Removing repeating characters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In everyday language, people are often not strictly grammatical. They will write
    things like "I looooooove it" in order to emphasize the word "love". But computers
    don't know that "looooooove" is a variation of "love" unless they are told. This
    recipe presents a method for removing those annoying repeating characters in order
    to end up with a "proper" English word.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As in the previous recipe, we will be making use of the `re` module, and more
    specifically, backreferences. A **backreference** is a way to refer to a previously
    matched group in a regular expression. This is what will allow us to match and
    remove repeating characters.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will create a class that has the same form as the `RegexpReplacer` from
    the previous recipe. It will have a `replace()` method that takes a single word
    and returns a more correct version of that word, with dubious repeating characters
    removed. The following code can be found in `replacers.py` and is meant to be
    imported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And now some example use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`RepeatReplacer` starts by compiling a regular expression for matching and
    defining a replacement string with backreferences. The `repeat_regexp` matches
    three groups:'
  prefs: []
  type: TYPE_NORMAL
- en: Zero or more starting characters `(\w*)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A single character `(\w)`, followed by another instance of that character `\2`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zero or more ending characters `(\w*)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The *replacement string* is then used to keep all the matched groups, while
    discarding the backreference to the second group. So the word "looooove" gets
    split into `(l)(o)o(ooove)` and then recombined as "loooove", discarding the second
    "o". This continues until only one "o" remains, when `repeat_regexp` no longer
    matches the string, and no more characters are removed.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the preceding examples, you can see that the `RepeatReplacer` is a bit too
    greedy and ends up changing "goose" into "gose". To correct this issue, we can
    augment the `replace()` function with a WordNet lookup. If WordNet recognizes
    the word, then we can stop replacing characters. Here is the WordNet augmented
    version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now, "goose" will be found in WordNet, and no character replacement will take
    place. And "oooooh" will become "ooh" instead of "oh", because "ooh" is actually
    a word in WordNet, defined as an expression of admiration or pleasure.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Read the next recipe to learn how to correct misspellings. And for more on WordNet,
    refer to the WordNet recipes in [Chapter 1](ch01.html "Chapter 1. Tokenizing Text
    and WordNet Basics"), *Tokenizing Text and WordNet Basics*. We will also be using
    WordNet for antonym replacement later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Spelling correction with Enchant
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: R eplacing repeating characters is actually an extreme form of spelling correction.
    In this recipe, we will take on the less extreme case of correcting minor spelling
    issues using **Enchant**—a spelling correction API.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will need to install Enchant, and a dictionary for it to use. Enchant is
    an offshoot of the "Abiword" open source word processor, and more information
    can be found at [http://www.abisource.com/projects/enchant/](http://www.abisource.com/projects/enchant/).
  prefs: []
  type: TYPE_NORMAL
- en: For dictionaries, **aspell** is a good open source spellchecker and dictionary
    that can be found at [http://aspell.net/](http://aspell.net/).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you will need the **pyenchant** library, which can be found at [http://www.rfk.id.au/software/pyenchant/](http://www.rfk.id.au/software/pyenchant/).
    You should be able to install it with the `easy_install` command that comes with
    *python-setuptools*, such as by doing `sudo easy_install pyenchant` on Linux or
    Unix.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will create a new class called `SpellingReplacer` in `replacers.py`, and
    this time the `replace()` method will check Enchant to see whether the word is
    valid or not. If not, we will look up suggested alternatives and return the best
    match using `nltk.metrics.edit_distance()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding class can be used to correct English spellings as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`SpellingReplacer` starts by creating a reference to an `enchant` dictionary.
    Then, in the `replace()` method, it first checks whether the given `word` is present
    in the dictionary or not. If it is, no spelling correction is necessary, and the
    word is returned. But if the word is not found, it looks up a list of suggestions
    and returns the first suggestion, as long as its edit distance is less than or
    equal to `max_dist`. The **edit distance** is the number of character changes
    necessary to transform the given word into the suggested word. `max_dist` then
    acts as a constraint on the Enchant `suggest()` function to ensure that no unlikely
    replacement words are returned. Here is an example showing all the suggestions
    for "languege", a misspelling of "language":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Except for the correct suggestion, "language", all the other words have an edit
    distance of three or greater.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can use language dictionaries other than `''` `en''`, such as `''en_GB''`,
    assuming the dictionary has already been installed. To check which other languages
    are available, use `enchant.list_languages()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you try to use a dictionary that doesn't exist, you will get `enchant.DictNotFoundError`.
    You can first check whether the dictionary exists using `enchant.dict_exists()`,
    which will return `True` if the named dictionary exists, or `False` otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: en_GB dictionary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Always be sure to use the correct dictionary for whichever language you are
    doing spelling correction on. `''en_US''` can give you different results than
    `''en_GB''`, such as for the word "theater". "Theater" is the American English
    spelling, whereas the British English spelling is "Theatre":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Personal word lists
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Enchant also supports personal word lists. These can be combined with an existing
    dictionary, allowing you to augment the dictionary with your own words. So let
    us say you had a file named `mywords.txt` that had `nltk` on one line. You could
    then create a dictionary augmented with your personal word list as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: To use an augmented dictionary with our `SpellingReplacer`, we can create a
    subclass in `replacers.py` that takes an existing spelling dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This `CustomSpellingReplacer` will not replace any words that you put into `mywords.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous recipe covered an extreme form of spelling correction by replacing
    repeating characters. You could also do spelling correction by simple word replacement
    as discussed in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Replacing synonyms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is often useful to reduce the vocabulary of a text by replacing words with
    common synonyms. By compressing the vocabulary without losing meaning, you can
    save memory in cases such as *frequency analysis* and *text indexing*. Vocabulary
    reduction can also increase the occurrence of significant collocations, which
    was covered in the *Discovering word collocations* recipe of [Chapter 1](ch01.html
    "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing Text and WordNet
    Basics*.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will need to have a defined mapping of a word to its synonym. This is a
    simple *controlled vocabulary*. We will start by hardcoding the synonyms as a
    Python dictionary, then explore other options for storing synonym maps.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll first create a `WordReplacer` class in `replacers.py` that takes a word
    replacement mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can demonstrate its usage for simple word replacement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`WordReplacer` is simply a class wrapper around a Python dictionary. The `replace()`
    method looks up the given word in its `word_map` and returns the replacement synonym
    if it exists. Otherwise, the given word is returned as is.'
  prefs: []
  type: TYPE_NORMAL
- en: If you were only using the `word_map` dictionary, you would have no need for
    the `WordReplacer` class, and could instead call `word_map.get()` directly. But
    `WordReplacer` can act as a base class for other classes that construct the `word_map`
    from various file formats. Read on for more information.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hardcoding synonyms as a Python dictionary is not a good long-term solution.
    Two better alternatives are to store the synonyms in a CSV file or in a YAML file.
    Choose whichever format is easiest for whoever will be maintaining your synonym
    vocabulary. Both of the classes outlined in the following section inherit the
    `replace()` method from `WordReplacer`.
  prefs: []
  type: TYPE_NORMAL
- en: CSV synonym replacement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `CsvWordReplacer` class extends `WordReplacer` in `replacers.py` in order
    to construct the `word_map` from a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Your CSV file should be two columns, where the first column is the word, and
    the second column is the synonym meant to replace it. If this file is called `synonyms.csv`
    and the first line is `bday`, `birthday`, then you can do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: YAML synonym replacement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have PyYAML installed, you can create a `YamlWordReplacer` in `replacers.py`.
    Download and installation instructions for PyYAML are located at [http://pyyaml.org/wiki/PyYAML](http://pyyaml.org/wiki/PyYAML).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Your YAML file should be a simple mapping of "word: synonym", such as `bday:
    birthday`. Note that the YAML syntax is very particular, and the space after the
    colon is required. If the file is named `synonyms.yaml`, you can do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use the `WordReplacer` to do any kind of word replacement, even spelling
    correction for more complicated words that can't be automatically corrected, as
    we did in the previous recipe. In the next recipe, we will cover antonym replacement.
  prefs: []
  type: TYPE_NORMAL
- en: Replacing negations with antonyms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The opposite of synonym replacement is *antonym* replacement. An **antonym**
    is the opposite meaning of a word. This time, instead of creating custom word
    mappings, we can use WordNet to replace words with unambiguous antonyms. Refer
    to the *Looking up lemmas and synonyms in WordNet* recipe in [Chapter 1](ch01.html
    "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing Text and WordNet
    Basics* for more details on antonym lookups.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us say you have a sentence such as "let''s not uglify our code". With antonym
    replacement, you can replace "not uglify" with "beautify", resulting in the sentence
    "let''s beautify our code". To do this, we will need to create an `AntonymReplacer`
    in `replacers.py` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can tokenize the original sentence into `["let''s", ''not'', ''uglify'',
    ''our'', ''code'']`, and pass this to the `replace_negations()` function. Here
    are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `AntonymReplacer` has two methods: `replace()` and `replace_negations()`.
    The `replace()` method takes a single `word` and an optional part of speech tag,
    then looks up the synsets for the word in WordNet. Going through all the synsets
    and every lemma of each synset, it creates a `set` of all antonyms found. If only
    one antonym is found, then it is an *unambiguous replacement*. If there is more
    than one antonym found, which can happen quite often, then we don''t know for
    sure which antonym is correct. In the case of multiple antonyms (or no antonyms),
    `replace()` returns `None` since it cannot make a decision.'
  prefs: []
  type: TYPE_NORMAL
- en: In `replace_negations()`, we look through a tokenized sentence for the word
    "`not`". If "`not`" is found, then we try to find an antonym for the next word
    using `replace()`. If we find an antonym, then it is appended to the list of `words`,
    replacing "`not`" and the original word. All other words are appended as it is,
    resulting in a tokenized sentence with unambiguous negations replaced by their
    antonyms.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since unambiguous antonyms aren''t very common in WordNet, you may want to
    create a custom antonym mapping the same way we did for synonyms. This `AntonymWordReplacer`
    could be constructed by inheriting from both `WordReplacer` and `AntonymReplacer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The order of inheritance is very important, as we want the initialization and
    `replace()` function of `WordReplacer` combined with the `replace_negations()`
    function from `AntonymReplacer`. The result is a replacer that can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Of course, you could also inherit from `CsvWordReplacer` or `YamlWordReplacer`
    instead of `WordReplacer` if you want to load the antonym word mappings from a
    file.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous recipe covers the `WordReplacer` from the perspective of synonym
    replacement. And in [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet
    Basics"), *Tokenizing Text and WordNet Basics* Wordnet usage is covered in detail
    in the *Looking up synsets for a word in Wordnet* and *Looking up lemmas and synonyms
    in Wordnet* recipes.
  prefs: []
  type: TYPE_NORMAL
