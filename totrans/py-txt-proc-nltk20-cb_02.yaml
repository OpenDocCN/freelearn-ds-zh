- en: Chapter 2. Replacing and Correcting Words
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章 替换和纠正单词
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下内容：
- en: Stemming words
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词干提取
- en: Lemmatizing words with WordNet
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用WordNet词元化单词
- en: Translating text with Babelfish
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Babelfish翻译文本
- en: Replacing words matching regular expressions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 替换匹配正则表达式的单词
- en: Removing repeating characters
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除重复字符
- en: Spelling correction with Enchant
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Enchant进行拼写纠正
- en: Replacing synonyms
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 替换同义词
- en: Replacing negations with antonyms
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将否定词替换为反义词
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: In this chapter, we will go over various word replacement and correction techniques.
    The recipes cover the gamut of linguistic compression, spelling correction, and
    text normalization. All of these methods can be very useful for pre-processing
    text before search indexing, document classification, and text analysis.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍各种单词替换和纠正技术。这些配方涵盖了语言压缩、拼写纠正和文本归一化的范围。所有这些方法在文本搜索索引、文档分类和文本分析之前的预处理中都非常有用。
- en: Stemming words
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词干提取
- en: '**Stemming** is a technique for removing *affixes* from a word, ending up with
    the *stem*. For example, the stem of "cooking" is "cook", and a good stemming
    algorithm knows that the "ing" *suffix* can be removed. Stemming is most commonly
    used by search engines for indexing words. Instead of storing all forms of a word,
    a search engine can store only the stems, greatly reducing the size of index while
    increasing retrieval accuracy.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**词干提取**是从单词中移除*词缀*的技术，最终得到*词干*。例如，“cooking”的词干是“cook”，一个好的词干提取算法知道“ing”*后缀*可以被移除。词干提取最常由搜索引擎用于索引单词。搜索引擎可以存储单词的所有形式，而不是存储词干，这可以大大减少索引的大小，同时提高检索的准确性。'
- en: One of the most common stemming algorithms is the **Porter Stemming Algorithm**,
    by Martin Porter. It is designed to remove and replace well known suffixes of
    English words, and its usage in NLTK will be covered next.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的词干提取算法之一是Martin Porter的**Porter词干提取算法**。它旨在移除和替换英语单词的已知后缀，NLTK中的使用将在下一节中介绍。
- en: Note
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The resulting stem is not always a valid word. For example, the stem of "cookery"
    is "cookeri". This is a feature, not a bug.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 结果词干不总是有效的单词。例如，“cookery”的词干是“cookeri”。这是一个特性，而不是错误。
- en: How to do it...
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到...
- en: NLTK comes with an implementation of the Porter Stemming Algorithm, which is
    very easy to use. Simply instantiate the `PorterStemmer` class and call the `stem()`
    method with the word you want to stem.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK包含Porter词干提取算法的实现，使用起来非常简单。只需实例化`PorterStemmer`类，并使用要提取词干的单词调用`stem()`方法。
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How it works...
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `PorterStemmer` knows a number of regular word forms and suffixes, and uses
    that knowledge to transform your input word to a final stem through a series of
    steps. The resulting stem is often a shorter word, or at least a common form of
    the word, that has the same root meaning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`PorterStemmer`知道许多常见的单词形式和后缀，并使用这些知识通过一系列步骤将输入单词转换为最终的词干。结果词干通常是更短的单词，或者至少是单词的常见形式，具有相同的词根意义。'
- en: There's more...
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: There are other stemming algorithms out there besides the Porter Stemming Algorithm,
    such as the **Lancaster Stemming Algorithm**, developed at Lancaster University.
    NLTK includes it as the `LancasterStemmer` class. At the time of writing, there
    is no definitive research demonstrating the superiority of one algorithm over
    the other. However, Porter Stemming is generally the default choice.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Porter词干提取算法之外，还有其他词干提取算法，例如在兰开斯特大学开发的**Lancaster词干提取算法**。NLTK将其作为`LancasterStemmer`类包含在内。在撰写本文时，没有确凿的研究表明一个算法优于另一个算法。然而，Porter词干提取通常是默认选择。
- en: 'All the stemmers covered next inherit from the `StemmerI` interface, which
    defines the `stem()` method. The following is an inheritance diagram showing this:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的所有词干提取器都继承自`StemmerI`接口，该接口定义了`stem()`方法。以下是一个继承图，展示了这一点：
- en: '![There''s more...](img/3609OS_02_01.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![还有更多...](img/3609OS_02_01.jpg)'
- en: LancasterStemmer
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LancasterStemmer
- en: The `LancasterStemmer` functions just like the `PorterStemmer`, but can produce
    slightly different results. It is known to be slightly more aggressive than the
    `P` `orterStemmer`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`LancasterStemmer`的功能与`PorterStemmer`类似，但可以产生略微不同的结果。它被认为是比`P` `orterStemmer`更激进。'
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: RegexpStemmer
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RegexpStemmer
- en: You can also construct your own stemmer using the `RegexpStemmer`. It takes
    a single regular expression (either compiled or as a string) and will remove any
    prefix or suffix that matches.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用`RegexpStemmer`构建自己的词干提取器。它接受单个正则表达式（可以是编译后的或字符串形式），并将移除任何匹配的前缀或后缀。
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A `RegexpStemmer` should only be used in very specific cases that are not covered
    by the `PorterStemmer` or `LancasterStemmer`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`RegexpStemmer` 应仅用于 `PorterStemmer` 或 `LancasterStemmer` 无法覆盖的非常特定的情况。'
- en: SnowballStemmer
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SnowballStemmer
- en: 'New in NLTK 2.0b9 is the `SnowballStemmer`, which supports 13 non-English languages.
    To use it, you create an instance with the name of the language you are using,
    and then call the `s` `tem()` method. Here is a list of all the supported languages,
    and an example using the Spanish `SnowballStemmer`:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 2.0b9 新增了 `SnowballStemmer`，它支持 13 种非英语语言。要使用它，你需要创建一个以你使用的语言命名的实例，然后调用
    `stem()` 方法。以下是所有支持的语言列表，以及使用西班牙语 `SnowballStemmer` 的示例：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: See also
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: In the next recipe, we will cover lemmatization, which is quite similar to stemming,
    but subtly different.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个菜谱中，我们将介绍词元化，它与词干提取非常相似，但有一些细微的差别。
- en: Lemmatizing words with WordNet
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 WordNet 词元化单词
- en: '**Lemmatization** is very similar to stemming, but is more akin to synonym
    replacement. A *lemma* is a root word, as opposed to the root *stem*. So unlike
    stemming, you are always left with a valid word which means the same thing. But
    the word you end up with can be completely different. A few examples will explain
    lemmatization...'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**词元化** 与词干提取非常相似，但更类似于同义词替换。一个 *词元* 是一个词根，与根 *词干* 相反。所以与词干提取不同，你总是留下一个有效的单词，它意味着相同的事情。但最终你得到的单词可能完全不同。一些例子将解释词元化...'
- en: Getting ready
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Be sure you have unzipped the `wordnet` corpus in `nltk_data/corpora/wordnet`.
    This will allow the `WordNetLemmatizer` to access WordNet. You should also be
    somewhat familiar with the part-of-speech tags covered in the *Looking up synsets
    for a word in WordNet* recipe of [Chapter 1](ch01.html "Chapter 1. Tokenizing
    Text and WordNet Basics"), *Tokenizing Text and WordNet Basics*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你已经解压缩了 `wordnet` 语料库在 `nltk_data/corpora/wordnet` 中。这将允许 `WordNetLemmatizer`
    访问 WordNet。你还应该对 [第 1 章](ch01.html "第 1 章。文本分词和 WordNet 基础") 中 *在 WordNet 中查找单词的词义集*
    菜单中涵盖的词性标签有所了解，*文本分词和 WordNet 基础*。
- en: How to do it...
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We will use the `WordNetLemmatizer` to find lemmas:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `WordNetLemmatizer` 来查找词元：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How it works...
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `WordNetLemmatizer` is a thin wrapper around the WordNet corpus, and uses
    the `morphy()` function of the `W` `ordNetCorpusReader` to find a lemma. If no
    lemma is found, the word is returned as it is. Unlike with stemming, knowing the
    part of speech of the word is important. As demonstrated previously, "cooking"
    does not have a lemma unless you specify that the part of speech (`pos`) is a
    *verb*. This is because the default part of speech is a *noun*, and since "cooking"
    is not a noun, no lemma is found. "Cookbooks", on the other hand, is a noun, and
    its lemma is the singular form, "cookbook".
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`WordNetLemmatizer` 是围绕 WordNet 语料库的一个薄包装，并使用 `W` `ordNetCorpusReader` 的 `morphy()`
    函数来查找词元。如果没有找到词元，则将单词按原样返回。与词干提取不同，了解单词的词性很重要。如前所述，“cooking”没有词元，除非你指定词性（`pos`）是动词。这是因为默认的词性是名词，而“cooking”不是名词，因此找不到词元。“Cookbooks”，另一方面，是名词，其词元是单数形式，“cookbook”。'
- en: There's more...
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多...
- en: 'Here''s an example that illustrates one of the major differences between stemming
    and lemmatization:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例，说明了词干提取和词元化之间的一大主要区别：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Instead of just chopping off the "es" like the `PorterStemmer`, the `WordNetLemmatizer`
    finds a valid root word. Where a stemmer only looks at the form of the word, the
    lemmatizer looks at the meaning of the word. And by returning a lemma, you will
    always get a valid word.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `PorterStemmer` 不同，`WordNetLemmatizer` 会找到一个有效的词根。词干提取器只关注单词的形式，而词元化器关注单词的意义。通过返回一个词元，你将始终得到一个有效的单词。
- en: Combining stemming with lemmatization
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将词干提取与词元化结合
- en: 'S temming and lemmatization can be combined to compress words more than either
    process can by itself. These cases are somewhat rare, but they do exist:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取和词元化可以结合起来压缩单词，比单独的任何过程都能压缩更多。这些情况相对较少，但它们确实存在：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: I n this example, stemming saves one character, lemmatizing saves two characters,
    and stemming the lemma saves a total of three characters out of five characters.
    That is nearly a 60% compression rate! This level of word compression over many
    thousands of words, while unlikely to always produce such high gains, can still
    make a huge difference.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，词干提取节省了一个字符，词元化节省了两个字符，而词干提取词元总共节省了五个字符中的三个字符。这几乎是 60% 的压缩率！在成千上万的单词中，这种程度的单词压缩虽然不太可能总是产生如此高的收益，但仍然可以产生巨大的差异。
- en: See also
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: In the previous recipe, we covered stemming basics and WordNet was introduced
    in the *Looking up synsets for a word in WordNet* and *Looking up lemmas* and
    *synonyms in WordNet* recipes of [Chapter 1](ch01.html "Chapter 1. Tokenizing
    Text and WordNet Basics"), *Tokenizing Text and WordNet Basics*. Looking forward,
    we will cover the *Using WordNet for Tagging* recipe in [Chapter 4](ch04.html
    "Chapter 4. Part-of-Speech Tagging"), *Part-of-Speech Tagging*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的食谱中，我们介绍了词干提取的基础知识，并在[第1章](ch01.html "第1章. 文本分词和WordNet基础知识")的*在WordNet中查找单词的synsets*和*在WordNet中查找词元和同义词*食谱中介绍了WordNet。展望未来，我们将在[第4章](ch04.html
    "第4章. 词性标注")的*使用WordNet进行词性标注*食谱中介绍。
- en: Translating text with Babelfish
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Babelfish翻译文本
- en: '**Babelfish** is an online language translation API provided by Yahoo. With
    it, you can translate text in a *source language* to a *target language*. NLTK
    comes with a simple interface for using it.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**Babelfish**是Yahoo提供的一个在线语言翻译API。使用它，你可以将*源语言*的文本翻译成*目标语言*。NLTK提供了一个简单的接口来使用它。'
- en: Getting ready
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Be sure you are connected to the internet first. The `babelfish.translate()`
    function requires access to Yahoo's online API in order to work.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先确保你已经连接到互联网。`babelfish.translate()`函数需要访问Yahoo的在线API才能工作。
- en: How to do it...
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'To translate your text, you first need to know two things:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要翻译你的文本，你首先需要知道两件事：
- en: The language of your text or source language.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你的文本或源语言的语言。
- en: The language you want to translate to or target language.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你想要翻译到的语言或目标语言。
- en: Language detection is outside the scope of this recipe, so we will assume you
    already know the source and target languages.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 语言检测不在这个食谱的范围内，所以我们将假设你已经知道源语言和目标语言。
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: You cannot translate using the same language for both source and target. Attempting
    to do so will raise a `BabelfishChangedError`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你不能使用相同的语言来翻译源语言和目标语言。尝试这样做将会引发一个`BabelfishChangedError`错误。
- en: How it works...
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `translate()` function is a small function that sends a `urllib` request
    to [http://babelfish.yahoo.com/translate_txt](http://babelfish.yahoo.com/translate_txt),
    and then searches the response for the translated text.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`translate()`函数是一个小的函数，它向[http://babelfish.yahoo.com/translate_txt](http://babelfish.yahoo.com/translate_txt)发送`urllib`请求，然后搜索响应以找到翻译后的文本。'
- en: Note
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If Yahoo, for whatever reason, had changed their HTML response to the point
    that `translate()` cannot identify the translated text, a `BabelfishChangedError`
    will be raised. This is unlikely to happen, but if it does, you may need to upgrade
    to a newer version of NLTK and/or report the error.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果由于某种原因Yahoo改变了他们的HTML响应，以至于`translate()`无法识别翻译后的文本，将会引发一个`BabelfishChangedError`错误。这种情况不太可能发生，但如果真的发生了，你可能需要升级到NLTK的新版本，或者报告这个错误。
- en: There's more...
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: There is also a fun function called `babelize()` that translates back and forth
    between the source and target language until there are no more changes.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个有趣的功能叫做`babelize()`，它可以在源语言和目标语言之间来回翻译，直到没有更多变化。
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Available languages
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可用语言
- en: You can see all the languages available for translation by examining the `available_languages`
    attribute.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过检查`available_languages`属性来查看所有可用的翻译语言。
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The lowercased version of each of these languages can be used as a source or
    target language for translation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这些语言的低档版本可以用作翻译的源语言或目标语言。
- en: Replacing words matching regular expressions
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 替换匹配正则表达式的单词
- en: Now we are going to get into the process of replacing words. Where stemming
    and lemmatization are a kind of *linguistic compression*, and word replacement
    can be thought of as *error correction*, or *text normalization*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将进入替换单词的过程。其中词干提取和词形还原是一种*语言压缩*，而单词替换可以被视为*错误纠正*，或*文本规范化*。
- en: For this recipe, we will be replacing words based on regular expressions, with
    a focus on *expanding contractions*. Remember when we were tokenizing words in
    [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing
    Text and WordNet Basics* and it was clear that most tokenizers had trouble with
    contractions? This recipe aims to fix that by replacing contractions with their
    expanded forms, such as by replacing "can't" with "cannot", or "would've" with
    "would have".
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个食谱，我们将根据正则表达式替换单词，重点是*扩展缩写词*。记得我们在[第1章](ch01.html "第1章. 文本分词和WordNet基础知识")中分词单词时，*文本分词和WordNet基础知识*，很明显大多数分词器在处理缩写词时都有困难吗？这个食谱旨在通过将缩写词替换为其扩展形式来解决这个问题，例如将"can't"替换为"cannot"，或将"would've"替换为"would
    have"。
- en: Getting ready
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Understanding how this recipe works will require a basic knowledge of regular
    expressions and the `re` module. The key things to know are *matching patterns*
    and the `re.subn()` function.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这个食谱的工作原理需要具备正则表达式和`re`模块的基本知识。关键要知道的是*匹配模式*和`re.subn()`函数。
- en: How to do it...
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: First, we need to define a number of replacement patterns. This will be a list
    of tuple pairs, where the first element is the pattern to match on, and the second
    element is the replacement.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要定义一系列替换模式。这将是一个元组对的列表，其中第一个元素是要匹配的模式，第二个元素是替换内容。
- en: Next, we will create a `RegexpReplacer` class that will compile the patterns,
    and provide a `replace()` method to substitute all found patterns with their replacements.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个`RegexpReplacer`类，该类将编译模式，并提供一个`replace()`方法来替换所有找到的模式。
- en: 'The following code can be found in the `replacers.py` module and is meant to
    be imported, not typed into the console:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码可以在`replacers.py`模块中找到，并打算导入，而不是在控制台中输入：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: How it works...
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Here is a simple usage example:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个简单的使用示例：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`RegexpReplacer.replace()` works by replacing every instance of a replacement
    pattern with its corresponding substitution pattern. In `replacement_patterns`,
    we have defined tuples such as (`r''(\w+)\''ve'', ''\g<1> have''`). The first
    element matches a group of ASCII characters followed by `''ve`. By grouping the
    characters before the `''ve` in parenthesis, a match group is found and can be
    used in the substitution pattern with the `\g<1>` reference. So we keep everything
    before `''ve`, then replace `''ve` with the word `have`. This is how "should''ve"
    can become "should have".'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`RegexpReplacer.replace()`通过将每个替换模式的所有实例替换为其相应的替换模式来工作。在`replacement_patterns`中，我们定义了如(`r''(\w+)\''ve'',
    ''\g<1> have''`)这样的元组。第一个元素匹配一组ASCII字符后跟`''ve''`。通过在`''ve''`之前将字符分组放在括号中，我们找到了一个匹配组，并可以使用`\g<1>`引用在替换模式中使用。因此，我们保留`''ve''`之前的所有内容，然后将`''ve''`替换为单词`have`。这就是“should''ve”可以变成“should
    have”的方式。'
- en: There's more...
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: This replacement technique can work with any kind of regular expression, not
    just contractions. So you could replace any occurrence of "&" with "and", or eliminate
    all occurrences of "-" by replacing it with the empty string. The `RegexpReplacer`
    can take any list of replacement patterns for whatever purpose.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这种替换技术可以与任何类型的正则表达式一起工作，而不仅仅是缩写。因此，你可以将任何“&”的出现替换为“and”，或者通过将其替换为空字符串来消除所有“-”的出现。`RegexpReplacer`可以接受任何用于任何目的的替换模式列表。
- en: Replacement before tokenization
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分词前的替换
- en: 'Let us try using the `RegexpReplacer` as a preliminary step before tokenization:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试在分词之前使用`RegexpReplacer`作为初步步骤：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Much better! By eliminating the contractions in the first place, the tokenizer
    will produce cleaner results. Cleaning up text before processing is a common pattern
    in natural language processing.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 更好！通过首先消除缩写，分词器将产生更干净的结果。在处理文本之前进行清理是自然语言处理中的常见模式。
- en: See also
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: For more information on tokenization, see the first three recipes in [Chapter
    1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing Text
    and WordNet Basics*. For more replacement techniques, continue reading the rest
    of this chapter.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 关于分词的更多信息，请参阅[第1章](ch01.html "第1章。文本分词和WordNet基础知识")的前三个食谱，“文本分词和WordNet基础知识”。有关更多替换技术，请继续阅读本章的其余部分。
- en: Removing repeating characters
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移除重复字符
- en: In everyday language, people are often not strictly grammatical. They will write
    things like "I looooooove it" in order to emphasize the word "love". But computers
    don't know that "looooooove" is a variation of "love" unless they are told. This
    recipe presents a method for removing those annoying repeating characters in order
    to end up with a "proper" English word.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在日常语言中，人们往往并不严格遵守语法。他们会写出像“我 looooooove 它”这样的句子来强调“爱”这个词。但除非有人告诉它们，“looooooove”是“love”的变体，否则计算机并不知道。这个方法提供了一种去除那些令人烦恼的重复字符的方法，以便最终得到一个“正确”的英语单词。
- en: Getting ready
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As in the previous recipe, we will be making use of the `re` module, and more
    specifically, backreferences. A **backreference** is a way to refer to a previously
    matched group in a regular expression. This is what will allow us to match and
    remove repeating characters.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正如上一个食谱中一样，我们将使用`re`模块，特别是回溯引用。**回溯引用**是在正则表达式中引用之前匹配的组的一种方式。这将使我们能够匹配和删除重复字符。
- en: How to do it...
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We will create a class that has the same form as the `RegexpReplacer` from
    the previous recipe. It will have a `replace()` method that takes a single word
    and returns a more correct version of that word, with dubious repeating characters
    removed. The following code can be found in `replacers.py` and is meant to be
    imported:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个类，其形式与之前菜谱中的 `RegexpReplacer` 相同。它将有一个 `replace()` 方法，该方法接受一个单词并返回该单词的正确版本，移除了可疑的重复字符。以下代码可以在
    `replacers.py` 中找到，并打算导入：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And now some example use cases:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是一些示例用法：
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: How it works...
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '`RepeatReplacer` starts by compiling a regular expression for matching and
    defining a replacement string with backreferences. The `repeat_regexp` matches
    three groups:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`RepeatReplacer` 首先编译一个用于匹配的正则表达式，并定义一个带有回溯引用的替换字符串。`repeat_regexp` 匹配三个组：'
- en: Zero or more starting characters `(\w*)`.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 零个或多个起始字符 `(\w*)`。
- en: A single character `(\w)`, followed by another instance of that character `\2`.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个字符 `(\w)`，后跟该字符的另一个实例 `\2`。
- en: Zero or more ending characters `(\w*)`.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 零个或多个结尾字符 `(\w*)`。
- en: The *replacement string* is then used to keep all the matched groups, while
    discarding the backreference to the second group. So the word "looooove" gets
    split into `(l)(o)o(ooove)` and then recombined as "loooove", discarding the second
    "o". This continues until only one "o" remains, when `repeat_regexp` no longer
    matches the string, and no more characters are removed.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，*替换字符串* 用于保留所有匹配的组，同时丢弃对第二个组的引用。因此，单词 "looooove" 被分割成 `(l)(o)o(ooove)`，然后重新组合为
    "loooove"，丢弃第二个 "o"。这会一直持续到只剩下一个 "o"，此时 `repeat_regexp` 不再匹配字符串，不再移除更多字符。
- en: There's more...
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'In the preceding examples, you can see that the `RepeatReplacer` is a bit too
    greedy and ends up changing "goose" into "gose". To correct this issue, we can
    augment the `replace()` function with a WordNet lookup. If WordNet recognizes
    the word, then we can stop replacing characters. Here is the WordNet augmented
    version:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，您可以看到 `RepeatReplacer` 稍微有点贪婪，最终将 "goose" 改成了 "gose"。为了纠正这个问题，我们可以在
    `replace()` 函数中增加 WordNet 查找。如果 WordNet 识别该单词，那么我们可以停止替换字符。以下是 WordNet 增强版本：
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now, "goose" will be found in WordNet, and no character replacement will take
    place. And "oooooh" will become "ooh" instead of "oh", because "ooh" is actually
    a word in WordNet, defined as an expression of admiration or pleasure.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，"goose" 将在 WordNet 中找到，不会进行字符替换。而 "oooooh" 将变成 "ooh" 而不是 "oh"，因为 "ooh" 实际上是一个单词，在
    WordNet 中定义为表示钦佩或愉悦的表达。
- en: See also
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考信息
- en: Read the next recipe to learn how to correct misspellings. And for more on WordNet,
    refer to the WordNet recipes in [Chapter 1](ch01.html "Chapter 1. Tokenizing Text
    and WordNet Basics"), *Tokenizing Text and WordNet Basics*. We will also be using
    WordNet for antonym replacement later in this chapter.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 读取下一菜谱了解如何纠正拼写错误。有关 WordNet 的更多信息，请参阅第 1 章 [Tokenizing Text and WordNet Basics](ch01.html
    "第 1 章。文本分词和 WordNet 基础") 中的 WordNet 菜谱。我们还将在本章后面使用 WordNet 进行反义词替换。
- en: Spelling correction with Enchant
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Enchant 进行拼写纠正
- en: R eplacing repeating characters is actually an extreme form of spelling correction.
    In this recipe, we will take on the less extreme case of correcting minor spelling
    issues using **Enchant**—a spelling correction API.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 替换重复字符实际上是拼写纠正的一种极端形式。在这个菜谱中，我们将处理不那么极端的情况，即使用 **Enchant**（一个拼写纠正 API）纠正轻微的拼写错误。
- en: Getting ready
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will need to install Enchant, and a dictionary for it to use. Enchant is
    an offshoot of the "Abiword" open source word processor, and more information
    can be found at [http://www.abisource.com/projects/enchant/](http://www.abisource.com/projects/enchant/).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要安装 Enchant 以及为其使用的词典。Enchant 是 "Abiword" 开源文字处理器的分支，更多信息可以在 [http://www.abisource.com/projects/enchant/](http://www.abisource.com/projects/enchant/)
    找到。
- en: For dictionaries, **aspell** is a good open source spellchecker and dictionary
    that can be found at [http://aspell.net/](http://aspell.net/).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于词典，**aspell** 是一个优秀的开源拼写检查器和词典，可以在 [http://aspell.net/](http://aspell.net/)
    找到。
- en: Finally, you will need the **pyenchant** library, which can be found at [http://www.rfk.id.au/software/pyenchant/](http://www.rfk.id.au/software/pyenchant/).
    You should be able to install it with the `easy_install` command that comes with
    *python-setuptools*, such as by doing `sudo easy_install pyenchant` on Linux or
    Unix.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您还需要 **pyenchant** 库，可以在 [http://www.rfk.id.au/software/pyenchant/](http://www.rfk.id.au/software/pyenchant/)
    找到。您应该能够使用随 *python-setuptools* 一起提供的 `easy_install` 命令安装它，例如在 Linux 或 Unix 上执行
    `sudo easy_install pyenchant`。
- en: How to do it...
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We will create a new class called `SpellingReplacer` in `replacers.py`, and
    this time the `replace()` method will check Enchant to see whether the word is
    valid or not. If not, we will look up suggested alternatives and return the best
    match using `nltk.metrics.edit_distance()`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 `replacers.py` 中创建一个新的类 `SpellingReplacer`，这次 `replace()` 方法将检查 Enchant
    以查看单词是否有效。如果不是，我们将查找建议的替代方案，并使用 `nltk.metrics.edit_distance()` 返回最佳匹配：
- en: '[PRE16]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The preceding class can be used to correct English spellings as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的类可以用来如下修正英语拼写：
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: How it works...
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '`SpellingReplacer` starts by creating a reference to an `enchant` dictionary.
    Then, in the `replace()` method, it first checks whether the given `word` is present
    in the dictionary or not. If it is, no spelling correction is necessary, and the
    word is returned. But if the word is not found, it looks up a list of suggestions
    and returns the first suggestion, as long as its edit distance is less than or
    equal to `max_dist`. The **edit distance** is the number of character changes
    necessary to transform the given word into the suggested word. `max_dist` then
    acts as a constraint on the Enchant `suggest()` function to ensure that no unlikely
    replacement words are returned. Here is an example showing all the suggestions
    for "languege", a misspelling of "language":'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`SpellingReplacer` 首先创建了一个对 `enchant` 字典的引用。然后，在 `replace()` 方法中，它首先检查给定的 `word`
    是否存在于字典中。如果存在，则不需要拼写修正，并返回该单词。但如果单词未找到，它会查找一个建议列表，并返回第一个建议，只要其编辑距离小于或等于 `max_dist`。**编辑距离**是将给定单词转换为建议单词所需的字符更改数。`max_dist`
    作为对 Enchant `suggest()` 函数的约束，以确保不会返回不太可能的替换词。以下是一个显示 "languege"（"language" 的拼写错误）的所有建议的示例：'
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Except for the correct suggestion, "language", all the other words have an edit
    distance of three or greater.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 除了正确的建议 "language" 之外，所有其他单词的编辑距离都为三个或更大。
- en: There's more...
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'You can use language dictionaries other than `''` `en''`, such as `''en_GB''`,
    assuming the dictionary has already been installed. To check which other languages
    are available, use `enchant.list_languages()`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用除 `'en'` 之外的语言字典，例如 `'en_GB'`，假设字典已经安装。要检查哪些其他语言可用，请使用 `enchant.list_languages()`：
- en: '[PRE19]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you try to use a dictionary that doesn't exist, you will get `enchant.DictNotFoundError`.
    You can first check whether the dictionary exists using `enchant.dict_exists()`,
    which will return `True` if the named dictionary exists, or `False` otherwise.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尝试使用不存在的字典，您将得到 `enchant.DictNotFoundError`。您可以使用 `enchant.dict_exists()`
    首先检查字典是否存在，如果存在，它将返回 `True`，否则返回 `False`。
- en: en_GB dictionary
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: en_GB 字典
- en: 'Always be sure to use the correct dictionary for whichever language you are
    doing spelling correction on. `''en_US''` can give you different results than
    `''en_GB''`, such as for the word "theater". "Theater" is the American English
    spelling, whereas the British English spelling is "Theatre":'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 总是要确保使用正确的字典来对您正在进行的拼写修正的语言进行操作。`'en_US'` 可能会给出与 `'en_GB'` 不同的结果，例如对于单词 "theater"。
    "Theater" 是美式英语的拼写，而英式英语的拼写是 "Theatre"：
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Personal word lists
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 个人单词列表
- en: 'Enchant also supports personal word lists. These can be combined with an existing
    dictionary, allowing you to augment the dictionary with your own words. So let
    us say you had a file named `mywords.txt` that had `nltk` on one line. You could
    then create a dictionary augmented with your personal word list as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Enchant 还支持个人单词列表。这些可以与现有字典结合使用，允许您通过自己的单词来扩展字典。所以假设您有一个名为 `mywords.txt` 的文件，其中有一行是
    `nltk`。您然后可以创建一个包含您的个人单词列表的扩展字典，如下所示：
- en: '[PRE21]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: To use an augmented dictionary with our `SpellingReplacer`, we can create a
    subclass in `replacers.py` that takes an existing spelling dictionary.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `SpellingReplacer` 的扩展字典，我们可以在 `replacers.py` 中创建一个子类，它接受现有的拼写字典。
- en: '[PRE22]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This `CustomSpellingReplacer` will not replace any words that you put into `mywords.txt`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 `CustomSpellingReplacer` 不会替换您放入 `mywords.txt` 中的任何单词。
- en: '[PRE23]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: See also
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关内容
- en: The previous recipe covered an extreme form of spelling correction by replacing
    repeating characters. You could also do spelling correction by simple word replacement
    as discussed in the next recipe.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的配方涉及通过替换重复字符的极端形式的拼写修正。您还可以通过简单的单词替换来进行拼写修正，如下一配方中讨论的那样。
- en: Replacing synonyms
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 替换同义词
- en: It is often useful to reduce the vocabulary of a text by replacing words with
    common synonyms. By compressing the vocabulary without losing meaning, you can
    save memory in cases such as *frequency analysis* and *text indexing*. Vocabulary
    reduction can also increase the occurrence of significant collocations, which
    was covered in the *Discovering word collocations* recipe of [Chapter 1](ch01.html
    "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing Text and WordNet
    Basics*.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 通过用常用同义词替换单词来减少文本的词汇量通常很有用。通过在不失去意义的情况下压缩词汇量，您可以在 *频率分析* 和 *文本索引* 等情况下节省内存。词汇量减少还可以增加重要搭配的出现频率，这在
    [第 1 章](ch01.html "第 1 章。文本分词和 WordNet 基础") 的 *发现单词搭配* 食谱中已有介绍，*文本分词和 WordNet
    基础*。
- en: Getting ready
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will need to have a defined mapping of a word to its synonym. This is a
    simple *controlled vocabulary*. We will start by hardcoding the synonyms as a
    Python dictionary, then explore other options for storing synonym maps.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要定义一个单词与其同义词的映射。这是一个简单的 *受控词汇表*。我们将首先将同义词硬编码为 Python 字典，然后探讨存储同义词映射的其他选项。
- en: How to do it...
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We''ll first create a `WordReplacer` class in `replacers.py` that takes a word
    replacement mapping:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在 `replacers.py` 中创建一个 `WordReplacer` 类，它接受一个单词替换映射：
- en: '[PRE24]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then we can demonstrate its usage for simple word replacement:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以演示其用于简单单词替换的使用方法：
- en: '[PRE25]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: How it works...
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '`WordReplacer` is simply a class wrapper around a Python dictionary. The `replace()`
    method looks up the given word in its `word_map` and returns the replacement synonym
    if it exists. Otherwise, the given word is returned as is.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`WordReplacer` 类简单地封装了一个 Python 字典。`replace()` 方法在其 `word_map` 中查找给定的单词，如果存在替换同义词，则返回该同义词。否则，返回给定的单词。'
- en: If you were only using the `word_map` dictionary, you would have no need for
    the `WordReplacer` class, and could instead call `word_map.get()` directly. But
    `WordReplacer` can act as a base class for other classes that construct the `word_map`
    from various file formats. Read on for more information.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只使用 `word_map` 字典，您就不需要 `WordReplacer` 类，可以直接调用 `word_map.get()`。但是 `WordReplacer`
    可以作为从各种文件格式构建 `word_map` 的其他类的基类。继续阅读以获取更多信息。
- en: There's more...
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Hardcoding synonyms as a Python dictionary is not a good long-term solution.
    Two better alternatives are to store the synonyms in a CSV file or in a YAML file.
    Choose whichever format is easiest for whoever will be maintaining your synonym
    vocabulary. Both of the classes outlined in the following section inherit the
    `replace()` method from `WordReplacer`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 将同义词硬编码为 Python 字典不是一个好的长期解决方案。两种更好的替代方案是将同义词存储在 CSV 文件或 YAML 文件中。选择对维护同义词词汇表的人来说最简单的格式。以下部分概述的两个类都从
    `WordReplacer` 继承了 `replace()` 方法。
- en: CSV synonym replacement
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CSV 同义词替换
- en: 'The `CsvWordReplacer` class extends `WordReplacer` in `replacers.py` in order
    to construct the `word_map` from a CSV file:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`CsvWordReplacer` 类在 `replacers.py` 中扩展了 `WordReplacer`，以便从 CSV 文件中构建 `word_map`：'
- en: '[PRE26]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Your CSV file should be two columns, where the first column is the word, and
    the second column is the synonym meant to replace it. If this file is called `synonyms.csv`
    and the first line is `bday`, `birthday`, then you can do:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 您的 CSV 文件应该有两列，其中第一列是单词，第二列是要替换的单词的同义词。如果此文件名为 `synonyms.csv` 且第一行是 `bday`，`birthday`，则可以这样做：
- en: '[PRE27]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: YAML synonym replacement
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: YAML 同义词替换
- en: If you have PyYAML installed, you can create a `YamlWordReplacer` in `replacers.py`.
    Download and installation instructions for PyYAML are located at [http://pyyaml.org/wiki/PyYAML](http://pyyaml.org/wiki/PyYAML).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已安装 PyYAML，您可以在 `replacers.py` 中创建一个 `YamlWordReplacer`。PyYAML 的下载和安装说明位于
    [http://pyyaml.org/wiki/PyYAML](http://pyyaml.org/wiki/PyYAML)。
- en: '[PRE28]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Your YAML file should be a simple mapping of "word: synonym", such as `bday:
    birthday`. Note that the YAML syntax is very particular, and the space after the
    colon is required. If the file is named `synonyms.yaml`, you can do:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '您的 YAML 文件应该是一个简单的 "单词：同义词" 映射，例如 `bday: birthday`。请注意，YAML 语法非常特别，冒号后面的空格是必需的。如果文件名为
    `synonyms.yaml`，则可以这样做：'
- en: '[PRE29]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: See also
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: You can use the `WordReplacer` to do any kind of word replacement, even spelling
    correction for more complicated words that can't be automatically corrected, as
    we did in the previous recipe. In the next recipe, we will cover antonym replacement.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `WordReplacer` 来进行任何类型的单词替换，甚至是对更复杂的单词进行拼写纠正，这些单词无法自动纠正，就像我们在前面的食谱中所做的那样。在下一个食谱中，我们将介绍反义词替换。
- en: Replacing negations with antonyms
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用反义词替换否定词
- en: The opposite of synonym replacement is *antonym* replacement. An **antonym**
    is the opposite meaning of a word. This time, instead of creating custom word
    mappings, we can use WordNet to replace words with unambiguous antonyms. Refer
    to the *Looking up lemmas and synonyms in WordNet* recipe in [Chapter 1](ch01.html
    "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing Text and WordNet
    Basics* for more details on antonym lookups.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 同义词替换的相反是 *反义词替换*。**反义词**是一个词的相反含义。这次，我们不再创建自定义的词映射，而是可以使用 WordNet 用明确的反义词替换词。有关反义词查找的更多详细信息，请参阅
    [第 1 章](ch01.html "第 1 章。文本分词和 WordNet 基础") 中的 *在 Wordnet 中查找词元和同义词* 配方。
- en: How to do it...
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Let us say you have a sentence such as "let''s not uglify our code". With antonym
    replacement, you can replace "not uglify" with "beautify", resulting in the sentence
    "let''s beautify our code". To do this, we will need to create an `AntonymReplacer`
    in `replacers.py` as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个句子，例如 "let's not uglify our code"。使用反义词替换，你可以将 "not uglify" 替换为 "beautify"，从而得到句子
    "let's beautify our code"。为此，我们需要在 `replacers.py` 中创建一个 `AntonymReplacer`，如下所示：
- en: '[PRE30]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now we can tokenize the original sentence into `["let''s", ''not'', ''uglify'',
    ''our'', ''code'']`, and pass this to the `replace_negations()` function. Here
    are some examples:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将原始句子分词为 `["let's", 'not', 'uglify', 'our', 'code']`，并将其传递给 `replace_negations()`
    函数。以下是一些示例：
- en: '[PRE31]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: How it works...
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The `AntonymReplacer` has two methods: `replace()` and `replace_negations()`.
    The `replace()` method takes a single `word` and an optional part of speech tag,
    then looks up the synsets for the word in WordNet. Going through all the synsets
    and every lemma of each synset, it creates a `set` of all antonyms found. If only
    one antonym is found, then it is an *unambiguous replacement*. If there is more
    than one antonym found, which can happen quite often, then we don''t know for
    sure which antonym is correct. In the case of multiple antonyms (or no antonyms),
    `replace()` returns `None` since it cannot make a decision.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`AntonymReplacer` 有两种方法：`replace()` 和 `replace_negations()`。`replace()` 方法接受一个单个的
    `word` 和一个可选的词性标签，然后查找 WordNet 中该词的 synsets。遍历所有 synsets 以及每个 synset 的每个词元，它创建一个包含所有找到的反义词的
    `set`。如果只找到一个反义词，那么它是一个 *明确的替换*。如果找到多个反义词（这种情况相当常见），那么我们无法确定哪个反义词是正确的。在存在多个反义词（或没有反义词）的情况下，`replace()`
    返回 `None`，因为它无法做出决定。'
- en: In `replace_negations()`, we look through a tokenized sentence for the word
    "`not`". If "`not`" is found, then we try to find an antonym for the next word
    using `replace()`. If we find an antonym, then it is appended to the list of `words`,
    replacing "`not`" and the original word. All other words are appended as it is,
    resulting in a tokenized sentence with unambiguous negations replaced by their
    antonyms.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `replace_negations()` 中，我们遍历一个分词句子以查找单词 "`not`"。如果找到 "`not`"，则尝试使用 `replace()`
    查找下一个词的反义词。如果我们找到一个反义词，则将其追加到 `words` 列表中，替换掉 "`not`" 和原始词。所有其他词都按原样追加，结果是一个分词句子，其中明确的否定被其反义词替换。
- en: There's more...
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Since unambiguous antonyms aren''t very common in WordNet, you may want to
    create a custom antonym mapping the same way we did for synonyms. This `AntonymWordReplacer`
    could be constructed by inheriting from both `WordReplacer` and `AntonymReplacer`:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在 WordNet 中明确的反义词并不常见，您可能需要创建一个与同义词相同的自定义反义词映射。这个 `AntonymWordReplacer` 可以通过从
    `WordReplacer` 和 `AntonymReplacer` 继承来构建：
- en: '[PRE32]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The order of inheritance is very important, as we want the initialization and
    `replace()` function of `WordReplacer` combined with the `replace_negations()`
    function from `AntonymReplacer`. The result is a replacer that can do the following:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 继承顺序非常重要，因为我们希望 `WordReplacer` 的初始化和 `replace()` 函数与 `AntonymReplacer` 的 `replace_negations()`
    函数结合。结果是这样一个替换器，它可以执行以下操作：
- en: '[PRE33]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Of course, you could also inherit from `CsvWordReplacer` or `YamlWordReplacer`
    instead of `WordReplacer` if you want to load the antonym word mappings from a
    file.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果您想从文件中加载反义词词映射，您也可以从 `CsvWordReplacer` 或 `YamlWordReplacer` 继承而不是 `WordReplacer`。
- en: See also
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The previous recipe covers the `WordReplacer` from the perspective of synonym
    replacement. And in [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet
    Basics"), *Tokenizing Text and WordNet Basics* Wordnet usage is covered in detail
    in the *Looking up synsets for a word in Wordnet* and *Looking up lemmas and synonyms
    in Wordnet* recipes.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的配方从同义词替换的角度介绍了 `WordReplacer`。在 [第 1 章](ch01.html "第 1 章。文本分词和 WordNet 基础")
    中，*文本分词和 WordNet 基础* 详细介绍了 Wordnet 的使用，包括 *在 Wordnet 中查找词的 synsets* 和 *在 Wordnet
    中查找词元和同义词* 配方。
