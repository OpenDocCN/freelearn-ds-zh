- en: '*Chapter 7*: Features of a Production Pipeline'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第七章*：生产管道的特性'
- en: In this chapter, you will learn several features that make a data pipeline ready
    for production. You will learn about building data pipelines that can be run multiple
    times without changing the results (idempotent). You will also learn what to do
    if transactions fail (atomicity). And you will learn about validating data in
    a staging environment. This chapter will use a sample data pipeline that I currently
    run in production.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习到使数据管道为生产准备就绪的几个特性。你将了解如何构建可以多次运行而不改变结果（幂等）的数据管道。你还将了解在事务失败时应该做什么（原子性）。此外，你还将学习在预演环境中验证数据。本章将使用一个我在生产中当前运行的数据管道示例。
- en: For me, this pipeline is a bonus, and I am not concerned with errors, or missing
    data. Because of this, there are elements missing in this pipeline that should
    be present in a mission critical, or production, pipeline. Every data pipeline
    will have different acceptable rates of errors – missing data – but in production,
    your pipelines should have some extra features that you have yet to learn.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我来说，这个管道是一个额外的收获，我对错误或缺失数据并不关心。正因为如此，这个管道中缺少了一些在关键任务或生产管道中应该存在的元素。每个数据管道都会有不同的可接受错误率——缺失数据——但在生产中，你的管道应该有一些你尚未学习的额外功能。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Staging and validating data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预演和验证
- en: Building idempotent data pipelines
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建幂等数据管道
- en: Building atomic data pipelines
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建原子数据管道
- en: Staging and validating data
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预演和验证
- en: When building production data pipelines, staging and validating data become
    extremely important. While you have seen basic data validation and cleaning in
    [*Chapter 5*](B15739_05_ePub_AM.xhtml#_idTextAnchor063)*, Cleaning, Transforming,
    and Enriching Data*, in production, you will need a more formal and automated
    way of performing these tasks. The next two sections will walk you through how
    to accomplish staging and validating data in production.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当构建生产数据管道时，数据预演和验证变得极其重要。虽然你在[*第五章*](B15739_05_ePub_AM.xhtml#_idTextAnchor063)*，数据清洗、转换和丰富*中已经看到了基本的数据验证和清理，但在生产中，你需要一种更正式和自动化的方式来执行这些任务。接下来的两个部分将指导你如何在生产中完成数据预演和验证。
- en: Staging data
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'In the NiFi data pipeline examples, data was extracted, and then passed along
    a series of connected processors. These processors performed some tasks on the
    data and sent the results to the next processor. But what happens if a processor
    fails? Do you start all over from the beginning? Depending on the source data,
    that may be impossible. This is where staging comes in to play. We will divide
    staging in to two different types: the staging of files or database dumps, and
    the staging of data in a database that is ready to be loaded into a warehouse.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在NiFi数据管道示例中，数据被提取，然后通过一系列连接的处理器传递。这些处理器对数据进行了一些操作，并将结果发送到下一个处理器。但如果处理器失败会发生什么？你是否需要从头开始？根据源数据，这可能是不可能的。这就是预演发挥作用的地方。我们将预演分为两种不同类型：文件或数据库转储的预演，以及将数据预演到准备加载到仓库的数据库中。
- en: Staging of files
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文件预演
- en: The first type of staging we will discuss is the staging of data in files following
    extraction from a source, usually a transactional database. Let's walk through
    a common scenario to see why we would need this type of staging.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要讨论的第一种预演类型是在从源（通常是事务型数据库）提取后，在文件中的数据预演。让我们通过一个常见的场景来了解一下为什么我们需要这种类型的预演。
- en: You are a data engineering at Widget Co – a company that has disrupted widget
    making and is the only online retailer of widgets. Every day, people from all
    over the world order widgets on the company website. Your boss has instructed
    you to build a data pipeline that takes sales from the website and puts them in
    a data warehouse every hour so that analysts can query the data and create reports.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你是Widget Co公司的数据工程师——这家公司颠覆了小部件制造行业，是唯一一家在线小部件零售商。每天，来自世界各地的人们都会在公司网站上订购小部件。你的老板指示你构建一个数据管道，每小时将网站的销售数据放入数据仓库，以便分析师可以查询数据并创建报告。
- en: 'Since sales are worldwide, let''s assume the only data transformation required
    is the conversion of the local sales date and time to be in GMT. This data pipeline
    should be straightforward and is shown in the following screenshot:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于销售是全球性的，让我们假设唯一需要的数据转换是将本地销售日期和时间转换为GMT。这个数据管道应该是直接的，如下面的截图所示：
- en: '![Figure 7.1 – A data pipeline to load widget sales into a warehouse'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 7.1 – 将小部件销售加载到仓库的数据管道'
- en: '](img/Figure_7.1_B15739.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure_7.1_B15739.jpg](img/Figure_7.1_B15739.jpg)'
- en: Figure 7.1 – A data pipeline to load widget sales into a warehouse
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 将小部件销售加载到仓库的数据管道
- en: The preceding data pipeline queries the widget database. It passes the records
    as a single flowfile to the `SplitText` processor, which sends each record to
    the processor, which will convert the date and time to GMT. Lastly, it loads the
    results in the data warehouse.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的数据管道查询小部件数据库。它将记录作为一个单独的flowfile传递给`SplitText`处理器，该处理器将每个记录发送到处理器，该处理器将日期和时间转换为GMT。最后，它将结果加载到数据仓库中。
- en: But what happens when you split the records, and then a date conversion fails?
    You can just re-query the database, right? No, you can't, because transactions
    are happening every minute and the transaction that failed was canceled and is
    no longer in the database, or they changed their order and now want a red widget
    and not the five blue widgets they initially ordered. Your marketing team will
    not be happy because they no longer know about these changes and cannot plan for
    how to convert these sales.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当你分割记录，然后日期转换失败会发生什么？你可以重新查询数据库，对吧？不，你不能，因为每分钟都在发生事务，失败的事务已被取消并且不再在数据库中，或者他们改变了顺序，现在想要一个红色的部件而不是最初订购的五个蓝色部件。你的营销团队不会高兴，因为他们不再了解这些变化，也无法计划如何转化这些销售。
- en: The point of the example is to demonstrate that in a transactional database,
    transactions are constantly happening, and data is being modified. Running a query
    produces a set of results that may be completely different if you run the same
    query 5 minutes later, and you have now lost that original data. This is why you
    need to stage your extracts.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 示例的目的是为了说明在一个事务型数据库中，事务是持续发生的，数据正在被修改。运行一个查询会产生一组结果，如果你5分钟后再次运行相同的查询，结果可能会完全不同，你现在已经失去了原始数据。这就是为什么你需要分阶段提取数据的原因。
- en: 'If the preceding pipeline example is used for staging, you will end up with
    a pipeline like the example shown in the following screenshot:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用前面的管道示例进行分阶段处理，你最终会得到以下截图所示的管道：
- en: '![Figure 7.2 – A data pipeline to load widget sales into a warehouse using
    staging'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 7.2 – 使用分阶段将小部件销售加载到仓库的数据管道'
- en: '](img/Figure_7.2_B15739.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure_7.2_B15739.jpg](img/Figure_7.2_B15739.jpg)'
- en: Figure 7.2 – A data pipeline to load widget sales into a warehouse using staging
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 使用分阶段将小部件销售加载到仓库的数据管道
- en: The preceding data pipeline is displayed as two graphs. The first graph queries
    the widget database and puts the results in a file on disk. This is the staging
    step. From here, the next graph will load the data from the staging file, split
    the records into flowfiles, convert the dates and times, and finally, load it
    into the warehouse. If this portion of the pipeline crashes, or you need to replay
    your pipeline for any reason, you can then just reload the CSV by restarting the
    second half of the data pipeline. You have a copy of the database at the time
    of the original query. If, 3 months from now, your warehouse is corrupted, you
    could replay your data pipeline with the data at every query, even though the
    database is completely different.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '前面的数据管道以两个图表的形式显示。第一个图表查询小部件数据库并将结果放在磁盘上的文件中。这是分阶段步骤。从这里，下一个图表将加载数据，将记录分割成flowfiles，转换日期和时间，最后将其加载到仓库中。如果这部分管道崩溃，或者你需要出于任何原因重放你的管道，你只需重新加载CSV文件，方法是重新启动数据管道的第二部分。你在原始查询时有一个数据库的副本。如果3个月后你的仓库被损坏，你可以使用每次查询的数据重放你的数据管道，即使数据库已经完全不同。 '
- en: Another benefit of having copies of database extracts in CSV files is that it
    reduces the load in terms of replaying your pipeline. If your queries are resource
    intensive, perhaps they can only be run at night, or if the systems you query
    belong to another department, agency, or company. Instead of having to use their
    resources again to fix a mistake, you can just use the copy.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在CSV文件中保留数据库提取副本的另一个好处是，它减少了重放管道的负载。如果你的查询资源密集型，可能只能在夜间运行，或者如果你查询的系统属于另一个部门、机构或公司。你不必再次使用他们的资源来修复错误，你只需使用副本即可。
- en: In the Airflow data pipelines you have built up to this point, you have staged
    your queries. The way Airflow works encourages good practices. Each task has saved
    the results to a file, and then you have loaded that file in the next task. In
    NiFi, however, your queries have been sent, usually to the `SplitRecords` or `Text`
    processor, to the next processor in the pipeline. This is not good practice for
    running pipelines in production and will no longer be the case in examples from
    here on in.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在你到目前为止构建的Airflow数据管道中，你已经进行了查询预演。Airflow的工作方式鼓励良好的实践。每个任务都已将结果保存到文件中，然后你在下一个任务中加载该文件。然而，在NiFi中，你的查询通常被发送到`SplitRecords`或`Text`处理器，到管道中的下一个处理器。这在生产中运行管道不是好的做法，并且从现在起，示例将不再是这样。
- en: Staging in databases
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据库中的预演
- en: Staging data in files is helpful during the extract phase of a data pipeline.
    On the other end of the pipeline, the load stage, it is better to stage your data
    in a database, and preferably, the same database as the warehouse. Let's walk
    through another example to see why.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据管道的提取阶段，将数据存储在文件中是有帮助的。在管道的另一端，即加载阶段，最好将数据存储在数据库中，最好是仓库相同的数据库。让我们通过另一个例子来看看原因。
- en: You have queried your data widget database and staged the data. The next data
    pipeline picks up the data, transforms it, and then loads it into the warehouse.
    But now what happens if loading does not work properly? Perhaps records went in
    and everything looks successful, but the mapping is wrong, and dates are strings.
    Notice I didn't say the load failed. You will learn about handling load failures
    later in this chapter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经查询了你的数据小部件数据库并进行了预演。下一个数据管道拾取数据，对其进行转换，然后将其加载到仓库中。但现在如果加载不正确会发生什么？也许记录已经进入并且一切看起来都很成功，但映射错误，日期是字符串。请注意，我没有说加载失败。你将在本章后面学习如何处理加载失败。
- en: Without actually loading the data into a database, you will only be able to
    guess what issues you may experience. By staging, you will load the data into
    a replica of your data warehouse. Then you can run validation suites and queries
    to see whether you get the results you expect – for example, you could run a `select
    count(*)` query from the table to see whether you get the correct number of records
    back. This will allow you to know exactly what issues you may have, or don't have,
    if all went well.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 没有将数据实际加载到数据库中，你只能猜测可能会遇到的问题。通过预演，你将数据加载到数据仓库的副本中。然后你可以运行验证套件和查询，以查看是否得到预期的结果——例如，你可以从表中运行一个`select
    count(*)`查询，以查看是否得到正确的记录数。这将帮助你确切地知道你可能遇到的问题，或者如果没有问题，你不会遇到什么问题。
- en: 'A data pipeline for Widget Co that uses staging at both ends of the pipeline
    should look like the pipeline in the following screenshot:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用管道两端都进行预演的Widget Co数据管道应该看起来像以下截图中的管道：
- en: '![Figure 7.3 – A production using staging at both ends of the pipeline'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 7.3 – 在管道两端使用预演的生产'
- en: '](img/Figure_7.3_B15739.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_7.3_B15739.jpg]'
- en: Figure 7.3 – A production using staging at both ends of the pipeline
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 7.3 – 在管道两端使用预演的生产
- en: The data pipeline in the preceding screenshot queries the widget database and
    stages the results in a file. The next stage picks up the file and converts the
    dates and times. The point of departure from the earlier example is that the data
    pipeline now loads the data into a replica of the data warehouse. The new segment
    of the data pipeline then queries this replica, performs some validation, and
    then loads it into the final database or warehouse.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个截图中的数据管道查询了小部件数据库，并将结果存储在文件中。下一个阶段拾取该文件，并转换日期和时间。与早期示例的不同之处在于，数据管道现在将数据加载到数据仓库的副本中。数据管道的新部分随后查询此副本，执行一些验证，然后将它加载到最终的数据库或仓库中。
- en: ETL versus ELT
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ETL与ELT
- en: So far, you have seen Extract, Transform, and Load. However, there is a growing
    shift toward an Extract, Load, and Transform process. In the ELT process, data
    is staged in a database immediately after the extract ion without any transformations.
    You handle all of the transformations in the database. This is very helpful if
    you are using SQL-based transformation tools. There is no right or wrong way,
    only preferences and use cases.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经看到了提取、转换和加载。然而，越来越多的人转向提取、加载和转换的过程。在ELT过程中，数据在提取后立即存储在数据库中，而不进行任何转换。你将在数据库中处理所有的转换。如果你使用基于SQL的转换工具，这将非常有帮助。没有正确或错误的方式，只有偏好和用例。
- en: By staging data at the front and end of your data pipeline, you are now better
    suited for handling errors and for validating the data as it moves through your
    pipeline. Do not think that these are the only two places where data can be staged,
    or that data must be staged in files. You can stage your data after every transformation
    in your data pipeline. Doing so will make debugging errors easier and allow you
    to pick up at any point in the data pipeline after an error. As your transformations
    become more time consuming, this may become more helpful.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在数据管道的前端和末端进行数据阶段，您现在更适合处理错误，并在数据通过管道移动时验证数据。不要认为这些是数据阶段放置的唯一两个地方，或者数据必须放置在文件中。您可以在数据管道中的每个转换之后阶段数据。这样做将使调试错误更容易，并在错误发生后在任何数据管道点继续。随着转换变得更加耗时，这可能更有帮助。
- en: You staged the extraction from the widget database in a file, but there is no
    reason to prevent you from extracting the data to a relational or noSQL database.
    Dumping data to files is slightly less complicated than loading it into a database
    – you don't need to handle schemas or build any additional infrastructure.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 您将小部件数据库的提取放置在文件中，但没有理由阻止您将数据提取到关系型或 noSQL 数据库中。将数据转储到文件中比将其加载到数据库中稍微简单一些 –
    您不需要处理模式或构建任何额外的基础设施。
- en: While staging data is helpful for replaying pipelines, handling errors, and
    debugging your pipeline, it is also helpful in the validation stages of your pipeline.
    In the next section, you will learn how to use Great Expectations to build validation
    suites on both file and database staged data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然阶段数据对于回放管道、处理错误和调试管道很有帮助，但它也有助于管道的验证阶段。在下一节中，您将学习如何使用 Great Expectations 在文件和数据库阶段数据上构建验证套件。
- en: Validating data with Great Expectations
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Great Expectations 验证数据
- en: With your data staged in either a file or a database, you have the perfect opportunity
    to validate it. In [*Chapter 5*](B15739_05_ePub_AM.xhtml#_idTextAnchor063)*, Cleaning,
    Transforming, and Enriching Data*, you used pandas to perform exploratory data
    analysis and gain insight into what columns existed, find counts of null values,
    look at ranges of values within columns, and examine the data types in each column.
    Pandas is powerful and, by using methods such as `value_counts` and `describe`,
    you can gain a lot of insight into your data, but there are tools that make validation
    much cleaner and make your expectations of the data much more obvious.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的数据放置在文件或数据库中时，您有完美的机会来验证它。在[*第 5 章*](B15739_05_ePub_AM.xhtml#_idTextAnchor063)*，清理、转换和丰富数据*中，您使用了
    pandas 来执行探索性数据分析，了解哪些列存在，查找空值的数量，查看列内值的范围，并检查每个列的数据类型。Pandas 功能强大，通过使用 `value_counts`
    和 `describe` 等方法，您可以获得大量关于数据的见解，但有一些工具可以使验证更加清晰，并使您对数据的期望更加明显。
- en: 'The library you will learn about in this section is **Great Expectations**.
    The following is a screenshot of the Great Expectations home page, where you can
    join and get involved with it:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中您将学习的库是 **Great Expectations**。以下是大纲页面的截图，您可以在其中加入并参与其中：
- en: '![Figure 7.4 – Great Expectations Python library for validating your data,
    and more'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.4 – 用于验证数据的 Great Expectations Python 库，以及其他功能'
- en: '](img/Figure_7.4_B15739.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.4_B15739.jpg)'
- en: Figure 7.4 – Great Expectations Python library for validating your data, and
    more
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – 用于验证数据的 Great Expectations Python 库，以及其他功能
- en: 'Why Great Expectations? Because with Great Expectations, you can specify human-readable
    expectations and let the library handle the implementation. For example, you can
    specify that the `age` column should not have null values, in your code, with
    the following line:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么选择 Great Expectations？因为使用 Great Expectations，您可以指定人类可读的期望，并让库处理实现。例如，您可以在代码中指定
    `age` 列不应包含空值，如下所示：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Great Expectations will handle the logic behind doing this irrespective of whether
    your data is in a DataFrame or in a database. The same expectation will run on
    either data context.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您的数据是在 DataFrame 中还是在数据库中，Great Expectations 都将处理执行此操作的逻辑。相同的期望将在任何数据上下文中运行。
- en: Getting started with Great Expectations
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开始使用 Great Expectations
- en: 'Installing Great Expectations can be done with `pip3` as shown:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `pip3` 安装 Great Expectations 可以按照以下步骤进行：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To view the documents that Great Expectations generates, you will also need
    to have Jupyter Notebook available on your machine. You can install Notebook with
    `pip3` as well:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看 Great Expectations 生成的文档，您还需要在您的机器上安装 Jupyter Notebook。您也可以使用 `pip3` 安装
    Notebook：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With the requirements installed, you can now set up a project. Create a directory
    at `$HOME/peoplepipeline` and press *Enter*. You can do this on Linux using the
    following commands:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完所需的要求后，你现在可以设置一个项目了。在 `$HOME/peoplepipeline` 目录下创建一个目录并按 *Enter* 键。你可以在 Linux
    上使用以下命令来完成此操作：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that you are in the project directory, before you set up Great Expectations,
    we will dump a sample of the data we will be working with. Using the code from
    [*Chapter 3*](B15739_03_ePub_AM.xhtml#_idTextAnchor039)*, Reading and Writing
    Files*, we will generate 1,000 records relating to people. The code is as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经进入了项目目录，在设置 Great Expectations 之前，我们将导出我们将要处理的数据样本。使用来自 [*第 3 章*](B15739_03_ePub_AM.xhtml#_idTextAnchor039)*，阅读和写入文件*
    的代码，我们将生成 1,000 条与人物相关的记录。代码如下：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The preceding code creates a CSV file with records about people. We will put
    this CSV file into the project directory.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码创建了一个包含人物记录的 CSV 文件。我们将把这个 CSV 文件放入项目目录中。
- en: 'Now you can set up Great Expectations on this project by using the command-line
    interface. The following line will initialize your project:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以通过使用命令行界面来设置这个项目上的 Great Expectations。以下命令将初始化你的项目：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You will now walk through a series of steps to configure Great Expectations.
    First, Great Expectations will ask you whether you are ready to proceed. Your
    terminal should look like the following screenshot:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在将经历一系列步骤来配置 Great Expectations。首先，Great Expectations 将询问你是否准备好继续。你的终端应该看起来如下截图所示：
- en: '![Figure 7.5 – Initializing Great Expectations on a project'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.5 – 在项目上初始化 Great Expectations](img/Figure_7.5_B15739.jpg)'
- en: '](img/Figure_7.5_B15739.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.5 – 在项目上初始化 Great Expectations](img/Figure_7.5_B15739.jpg)'
- en: Figure 7.5 – Initializing Great Expectations on a project
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 在项目上初始化 Great Expectations
- en: 'Having entered *Y* and pressed *Enter*, you will be prompted with a series
    of questions:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 输入 *Y* 并按 *Enter* 键后，你将收到一系列问题的提示：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The answers to the questions are shown in the following screenshot, but it
    should be `Files`, `Pandas`, where you put your file, and whatever you would like
    to name it:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 问题的答案显示在以下截图中，但应该是 `Files`、`Pandas`，即你放置文件的位置，以及你希望为其命名的任何名称：
- en: '![Figure 7.6 – Initializing Great Expectations by answering questions'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.6 – 通过回答问题初始化 Great Expectations](img/Figure_7.6_B15739.jpg)'
- en: '](img/Figure_7.6_B15739.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.6 – 通过回答问题初始化 Great Expectations](img/Figure_7.6_B15739.jpg)'
- en: Figure 7.6 – Initializing Great Expectations by answering questions
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 通过回答问题初始化 Great Expectations
- en: 'When Great Expectations has finished running, it will tell you it''s done,
    give you a path to the document it has generated, and open the document in your
    browser. The documents will look like the following screenshot:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Great Expectations 运行完成后，它会告诉你已经完成，并给出它生成的文档的路径，并在你的浏览器中打开该文档。文档看起来如下截图所示：
- en: '![Figure 7.7 – Documentation generated by Great Expectations'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.7 – Great Expectations 生成的文档](img/Figure_7.7_B15739.jpg)'
- en: '](img/Figure_7.7_B15739.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.7 – Great Expectations 生成的文档](img/Figure_7.7_B15739.jpg)'
- en: Figure 7.7 – Documentation generated by Great Expectations
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – Great Expectations 生成的文档
- en: 'The preceding screenshot shows the documentation generated for the Great Expectations
    Suite. You can see there are **11** expectations and we have passed all of them.
    The expectations are very basic, specifying how many records should exist and
    what columns should exist in what order. Also, in the code I specified an age
    range. So, **age** has a minimum and maximum value. Ages have to be greater than
    17 and less than 81 to pass the validation. You can see a sample of the expectations
    generated by scrolling. I have shown some of mine in the following screenshot:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 上一张截图显示了为 Great Expectations 套件生成的文档。你可以看到有 **11** 个预期，并且我们已经通过了所有的预期。这些预期非常基础，指定了应该存在多少条记录以及哪些列应该以什么顺序存在。此外，在代码中我指定了一个年龄范围。因此，**年龄**
    有一个最小值和最大值。年龄必须大于 17 且小于 81 才能通过验证。你可以通过滚动查看生成的预期样本。以下截图显示了其中的一些：
- en: '![Figure 7.8 – Sample generated expectations'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.8 – 生成的预期样本](img/Figure_7.8_B15739.jpg)'
- en: '](img/Figure_7.8_B15739.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.8 – Great Expectations 生成的文档](img/Figure_7.8_B15739.jpg)'
- en: Figure 7.8 – Sample generated expectations
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 生成的预期样本
- en: 'As you can see, the expectations are very rigid – age must never be null, for
    example. Let''s edit the expectations. You have installed Jupyter Notebook, so
    you can run the following command to launch your expectation suite in a single
    step:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，预期非常严格——例如，年龄不能为空。让我们编辑预期。因为你已经安装了 Jupyter Notebook，所以你可以运行以下命令来一次性启动你的预期套件：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Your browser will open a Jupyter notebook and should look like the following
    screenshot:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你的浏览器将打开一个 Jupyter notebook，应该看起来如下截图所示：
- en: '![Figure 7.9 – Your expectation suite in a Jupyter notebook'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.9 – 在 Jupyter notebook 中的预期套件](img/Figure_7.9_B15739.jpg)'
- en: '](img/Figure_7.9_B15739.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.9_B15739.jpg)'
- en: Figure 7.9 – Your expectation suite in a Jupyter notebook
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – 您的期望套件在 Jupyter 笔记本中
- en: 'Some items should stand out in the code – the expectation suite name, and the
    path to your data file in the `batch_kwargs` variable. As you scroll through,
    you will see the expectations with headers for their type. If you scroll to the
    `Table_Expectation(s)` header, I will remove the row count expectation by deleting
    the cell, or by deleting the code in the cell, as shown in the following screenshot:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一些项目应在代码中突出显示 – 期望套件名称，以及`batch_kwargs`变量中您的数据文件路径。当您滚动浏览时，您将看到带有其类型标题的期望。如果您滚动到`Table_Expectation(s)`标题，我将通过删除单元格或删除单元格中的代码来删除行计数期望，如下面的截图所示：
- en: '![Figure 7.10 – Table Expectation(s)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.10 – 表期望'
- en: '](img/Figure_7.10_B15739.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.10_B15739.jpg)'
- en: Figure 7.10 – Table Expectation(s)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 – 表期望
- en: 'The other expectation to edit is under the `age` header. I will remove an expectation,
    specifically, the `expect_quantile_values_to_be_between` expectation. The exact
    line is shown in the following screenshot:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要编辑的期望是在`age`标题下。我将删除一个期望，具体来说是`expect_quantile_values_to_be_between`期望。确切的行如下面的截图所示：
- en: '![Figure 7.11 – Age expectations with the quantile expectations to be removed'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.11 – 要删除的量分期望的年龄期望'
- en: '](img/Figure_7.11_B15739.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.11_B15739.jpg)'
- en: Figure 7.11 – Age expectations with the quantile expectations to be removed
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 – 要删除的量分期望的年龄期望
- en: You can continue to remove expectations, or you can add new ones, or even just
    modify the values of existing expectations. You can find a glossary of available
    expectations at [https://docs.greatexpectations.io/en/latest/reference/glossary_of_expectations.html](https://docs.greatexpectations.io/en/latest/reference/glossary_of_expectations.html).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以继续删除期望，或者添加新的期望，或者甚至只是修改现有期望的值。您可以在[https://docs.greatexpectations.io/en/latest/reference/glossary_of_expectations.html](https://docs.greatexpectations.io/en/latest/reference/glossary_of_expectations.html)找到可用期望的词汇表。
- en: 'Once you have made all of the changes and are satisfied, you can run the entire
    notebook to save the changes to your expectation suite. The following screenshot
    shows how to do that – select **Cell** | **Run All**:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您完成所有更改并且满意，您可以通过运行整个笔记本来保存对您的期望套件的更改。以下截图显示了如何操作 – 选择**单元格** | **运行所有**：
- en: '![Figure 7.12 – Saving the changes to your expectation suite by running the
    notebook'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.12 – 通过运行笔记本保存您的期望套件中的更改'
- en: '](img/Figure_7.12_B15739.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.12_B15739.jpg)'
- en: Figure 7.12 – Saving the changes to your expectation suite by running the notebook
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 – 通过运行笔记本保存您的期望套件中的更改
- en: Now that you have an expectation suite, it is time to add it to your pipeline.
    In the next two sections, you will learn how to add it alongside your pipeline
    for use with NiFi or embed the code into your pipeline for use with Airflow.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您有了期望套件，是时候将其添加到您的管道中。在接下来的两个部分中，您将学习如何将其添加到您的管道中以供 NiFi 使用，或者将代码嵌入到您的管道中以供
    Airflow 使用。
- en: Great Expectations outside the pipeline
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管道外部的 Great Expectations
- en: So far, you have validated data while you edited the expectation suite inside
    a Jupyter notebook. You could continue to do that using a library such as Papermill,
    but that is beyond the scope of this book. In this section, however, you will
    create a tap and run it from NiFi.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您在编辑 Jupyter 笔记本内的期望套件时验证了数据。您可以使用像 Papermill 这样的库继续这样做，但这超出了本书的范围。然而，在本节中，您将创建一个
    Tap 并从 NiFi 中运行它。
- en: Papermill
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Papermill
- en: Papermill is a library created at Netflix that allows you to create parameterized
    Jupyter notebooks and run them from the command line. You can change parameters
    and specify an output directory for the resultant notebook. It pairs well with
    another Netflix library, Scrapbook. Find them both, along with other interesting
    projects, including Hydrogen, at [https://github.com/nteract](https://github.com/nteract).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Papermill 是 Netflix 创建的一个库，允许您创建参数化的 Jupyter 笔记本，并通过命令行运行它们。您可以为结果笔记本更改参数并指定输出目录。它与另一个
    Netflix 库 Scrapbook 配合良好。您可以在 [https://github.com/nteract](https://github.com/nteract)
    找到它们，以及其他有趣的项目，包括 Hydrogen。
- en: 'A tap is how Great Expectations creates executable Python files to run against
    your expectation suite. You can create a new tap using the command-line interface,
    as shown:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Tap 是 Great Expectations 创建可执行 Python 文件以运行您的期望套件的方式。您可以使用命令行界面创建一个新的 Tap，如下所示：
- en: '[PRE8]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding command takes an expectation suite and the name of a Python file
    to create. When it runs, it will ask you for a data file. I have pointed it to
    the `people.csv` file that you used in the preceding section when creating the
    suite. This is the file that the data pipeline will overwrite as it stages data:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令接受一个期望套件和一个 Python 文件名来创建。当它运行时，它会要求你提供一个数据文件。我已经将其指向你在上一节创建套件时使用的 `people.csv`
    文件。这是数据管道在阶段数据时将覆盖的文件：
- en: '![Figure 7.13 – Result of the Python file at the specified location'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.13 – 指定位置的 Python 文件的结果'
- en: '](img/Figure_7.13_B15739.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.13_B15739.jpg)'
- en: Figure 7.13 – Result of the Python file at the specified location
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 – 指定位置的 Python 文件的结果
- en: 'If you run the tap, you should see that it succeeded, as shown in the following
    screenshot:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行 tap，你应该会看到它成功了，如下面的截图所示：
- en: '![Figure 7.14 – Great Expectation tap run'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.14 – Great Expectation tap 运行'
- en: '](img/Figure_7.14_B15739.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.14_B15739.jpg)'
- en: Figure 7.14 – Great Expectation tap run
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 – Great Expectation tap 运行
- en: You are now ready to build a pipeline in NiFi and validate your data using Great
    Expectations. The next section will walk you through the process.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以开始在 NiFi 中构建管道并使用 Great Expectations 验证你的数据了。下一节将带你了解这个过程。
- en: Great Expectations in NiFi
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NiFi 中的 Great Expectations
- en: 'Combining NiFi and Great Expectations requires a few modifications to the tap
    you created in the previous section. First, you will need to change all the exits
    to be `0`. If you have a `system.exit(1)` exit, NiFi processors will crash because
    the script failed. We want the script to close successfully, even if the results
    are not, because the second thing you will change are the `print` statements.
    Change the `print` statements to be a JSON string with a result key and a pass
    or fail value. Now, even though the script exits successfully, we will know in
    NiFi whether it actually passed or failed. The code of the tap is shown in the
    following code block, with the modifications in bold:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 结合 NiFi 和 Great Expectations 需要对上一节中创建的 tap 进行一些修改。首先，你需要将所有的出口都改为 `0`。如果你有一个
    `system.exit(1)` 的出口，NiFi 处理器会因为脚本失败而崩溃。我们希望脚本能够成功关闭，即使结果不成功，因为你要更改的第二件事是 `print`
    语句。将 `print` 语句改为带有结果键和通过或失败值的 JSON 字符串。现在，即使脚本成功退出，我们也会在 NiFi 中知道它实际上是否通过。以下代码块显示了
    tap 的代码，其中修改的部分用粗体表示：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'With the changes to the tap complete, you can now build a data pipeline in
    NiFi. The following screenshot is the start of a data pipeline using the tap:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成对 tap 的更改后，你现在可以在 NiFi 中构建数据管道。以下截图是使用 tap 开始的数据管道：
- en: '![Figure 7.15 – A NiFi data pipeline using Great Expectations'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.15 – 使用 Great Expectations 的 NiFi 数据管道'
- en: '](img/Figure_7.15_B15739.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.15_B15739.jpg)'
- en: Figure 7.15 – A NiFi data pipeline using Great Expectations
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.15 – 使用 Great Expectations 的 NiFi 数据管道
- en: 'The preceding data pipeline creates 1,000 records and saves it as a CSV file.
    It then runs the tap on the data and reads in the result — the pass or fail JSON
    from the script. Lastly, it extracts the result and routes the flowfile to either
    a pass or fail processor. From there, your data pipeline can continue, or it can
    log the error. You will walk through the pipeline in the following steps:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 上述数据管道创建了 1,000 条记录并将其保存为 CSV 文件。然后它在数据上运行 tap 并读取结果——来自脚本的通过或失败 JSON。最后，它提取结果并将
    flowfile 路由到通过或失败处理器。从那里，你的数据管道可以继续，或者它可以记录错误。你将在以下步骤中走过这个管道：
- en: The data pipeline starts by generating a fake flowfile without any data to trigger
    the next processor. You could replace this processor with one that queries your
    transactional database, or that reads files from your data lake. I have scheduled
    this processor to run every hour.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据管道首先生成一个没有任何数据的假 flowfile 来触发下一个处理器。你可以用查询你的事务数据库或从你的数据湖读取文件的处理器来替换这个处理器。我已经安排这个处理器每小时运行一次。
- en: Once the empty flowfile is received, the `ExecuteStreamCommand` processor calls
    the `loadcsv.py` Python script. This file is from [*Chapter 3*](B15739_03_ePub_AM.xhtml#_idTextAnchor039)*,
    Reading and Writing Files*, and uses `Faker` to create 1,000 fake people records.
    The `ExecuteStreamCommand` processor will read the output from the script. If
    you had print statements, each line would become a flowfile. The script has one
    output, and that is `{"status":"Complete"}`.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦收到空流文件，`ExecuteStreamCommand` 处理器会调用 `loadcsv.py` Python 脚本。此文件来自 [*第 3 章*](B15739_03_ePub_AM.xhtml#_idTextAnchor039)*，读取和写入文件*，并使用
    `Faker` 创建 1,000 条虚假人物记录。`ExecuteStreamCommand` 处理器将读取脚本的输出。如果你有打印语句，每一行都会成为一个流文件。脚本有一个输出，那就是
    `{"status":"Complete"}`。
- en: To configure the processor to run the script, you can set the `python3` – if
    you can run the command with the full path, you do not need to enter it all. Lastly,
    set `loadcsv.py`. When the processor runs, the output flowfile is shown in the
    following screenshot:![Figure 7.16 – The flowfile shows the JSON string
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要配置处理器以运行脚本，你可以设置 `python3` – 如果你可以使用完整路径运行命令，你不需要全部输入。最后，设置 `loadcsv.py`。当处理器运行时，输出流文件将在以下屏幕截图显示：![图
    7.16 – 流文件显示 JSON 字符串
- en: '](img/Figure_7.16_B15739.jpg)'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_7.16_B15739.jpg)'
- en: Figure 7.16 – The flowfile shows the JSON string
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.16 – 流文件显示 JSON 字符串
- en: The next processor is also an `ExecuteStreamCommand` processor. This time, the
    script will be your tap. The configuration should be the same as in the previous
    step, except `peoplevalidatescript.py`. Once the processor completes, the flowfile
    will contain JSON with a result of pass or fail. The `pass` flowfile is shown
    in the following screenshot:![Figure 7.17 – Result of the tap, validation passed
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个处理器也是一个 `ExecuteStreamCommand` 处理器。这次，脚本将是你的 tap。配置应该与上一步相同，除了 `peoplevalidatescript.py`。处理器完成后，流文件将包含带有通过或失败结果的
    JSON。`pass` 流文件在以下屏幕截图显示：![图 7.17 – tap 的结果，验证通过
- en: '](img/Figure_7.17_B15739.jpg)'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_7.17_B15739.jpg)'
- en: Figure 7.17 – Result of the tap, validation passed
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.17 – 拨号结果，验证通过
- en: The value of the result is extracted in the next processor – `EvaluateJsonPath`.
    Adding a new property with the plus button, name it `result` and set the value
    to `$.result`. This will extract the `pass` or `fail` value and send it as a flowfile
    attribute.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个处理器中提取了结果值 – `EvaluateJsonPath`。通过点击加号添加一个新属性，命名为 `result` 并将其值设置为 `$.result`。这将提取
    `pass` 或 `fail` 值并将其作为流文件属性发送。
- en: 'The next process is `RouteOnAttribute`. This processor allows you to create
    properties that can be used as a relationship in a connection to another processor,
    meaning you can send each property to a different path. Creating two new properties
    – `pass` and `fail`, the values are shown in the following code snippet:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个过程是 `RouteOnAttribute`。此处理器允许你创建可以用于连接到另一个处理器的属性的属性，这意味着你可以将每个属性发送到不同的路径。创建两个新属性
    – `pass` 和 `fail`，其值在以下代码片段中显示：
- en: '[PRE10]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preceding command uses the NiFi expression language to read the value of
    the result attribute in the flowfile.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的命令使用 NiFi 表达式语言读取流文件中结果属性的值。
- en: From here, I have terminated the data pipeline at a `PutFile` processor. But
    you would now be able to continue by connecting a `pass` and `fail` path to their
    respective relationships in the previous processor. If it passed, you could read
    the staged file and insert the data into the warehouse.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这里，我在 `PutFile` 处理器处终止了数据管道。但现在你可以通过将 `pass` 和 `fail` 路径连接到上一个处理器中的相应关系来继续。如果通过了，你可以读取暂存文件并将数据插入到仓库中。
- en: In this section, you connected Great Expectations to your data pipeline. The
    tap was generated using your data, and because of this, the test passed. The pipeline
    ended with the file being written to disk. However, you could continue the data
    pipeline to route success to a data warehouse. In the real world, your tests will
    fail on occasion. In the next section, you will learn how to handle failed tests.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将 Great Expectations 连接到你的数据管道。tap 是使用你的数据生成的，因此测试通过了。管道以将文件写入磁盘结束。然而，你可以继续数据管道，将成功路由到数据仓库。在现实世界中，你的测试有时会失败。在下一节中，你将学习如何处理失败的测试。
- en: Failing the validation
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证失败
- en: 'The validation will always pass because the script we are using generates records
    that meet the validations rules. What if we changed the script? If you edit the
    `loadcsv.py` script and change the minimum and maximum age, we can make the validation
    fail. The edit is shown as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 验证总是会通过，因为我们使用的脚本生成的记录都符合验证规则。如果我们更改了脚本怎么办？如果你编辑`loadcsv.py`脚本并更改最小和最大年龄，我们可以使验证失败。编辑如下所示：
- en: '[PRE11]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will create records that are below the minimum and above the maximum—hopefully,
    because it is random, but 1,000 records should get us there. Once you have edited
    the script, you can rerun the data pipeline. The final flowfile should have been
    routed to the `fail` path. Great Expectations creates documents for your validations.
    If you remember, you saw them initially when you created the validation suite.
    Now you will have a record of both the passed and failed runs. Using your browser,
    open the documents. The path is within your project folder. For example, my docs
    are at the following path:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建低于最小值和高于最大值的记录——希望如此，因为它是随机的，但1000条记录应该能让我们达到目标。一旦你编辑了脚本，你可以重新运行数据管道。最终的flowfile应该已经被路由到`fail`路径。Great
    Expectations为你的验证创建文档。如果你记得，你最初在创建验证套件时看到了它们。现在你将有一个记录了通过和失败的运行。使用你的浏览器打开这些文档。路径位于你的项目文件夹中。例如，我的文档位于以下路径：
- en: '`file:///home/paulcrickard/peoplepipeline/great_expectations/uncommitted/data_docs/local_site/validations/people/validate/20200505T145722.862661Z/6f1eb7a06079eb9cab8de404c6faa
    b62.html`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`file:///home/paulcrickard/peoplepipeline/great_expectations/uncommitted/data_docs/local_site/validations/people/validate/20200505T145722.862661Z/6f1eb7a06079eb9cab8de404c6faa
    b62.html`'
- en: 'The documents should show all your validations runs. The documents will look
    like the following screenshot:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 文档应该显示所有的验证运行。文档将看起来像以下截图：
- en: '![Figure 7.18 – Results of multiple validation runs'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.18 – 多次验证运行的结果'
- en: '](img/Figure_7.18_B15739.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.18_B15739.jpg)'
- en: Figure 7.18 – Results of multiple validation runs
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18 – 多次验证运行的结果
- en: 'The preceding screenshot shows all of the validation runs. You can see the
    red **x** indicating failures. Click on one of the failed runs to see which expectations
    were not met. The results should be that both the minimum and maximum age were
    not met. You should see that this is the case, as shown in the following screenshot:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的截图显示了所有的验证运行。你可以看到表示失败的红色**x**。点击一个失败的运行，查看哪些期望未满足。结果应该是最小和最大年龄都没有达到。你应该看到这一点，如下面的截图所示：
- en: '![Figure 7.19 – Age expectations have not been met'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.19 – 年龄期望未达到'
- en: '](img/Figure_7.19_B15739.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_7.19_B15739.jpg)'
- en: Figure 7.19 – Age expectations have not been met
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.19 – 年龄期望未达到
- en: In this section, you have created a Great Expectations suite and specified expectations
    for your data. Previously, you would have had to do this manually using DataFrames
    and a significant amount of code. Now you can use human-readable statements and
    allow Great Expectations to do the work. You have created a tap that you can run
    inside your NiFi data pipeline — or that you can schedule using Cron or any other
    tool.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你创建了一个Great Expectations套件并指定了你的数据期望。以前，你将不得不手动使用DataFrames和大量的代码来完成这项工作。现在你可以使用可读性强的语句，并让Great
    Expectations来做这项工作。你已经创建了一个可以在你的NiFi数据管道内运行或使用Cron或其他工具调度的tap。
- en: A quick note on Airflow
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于Airflow的简要说明
- en: 'In the preceding example, you ran the validation suite outside of your pipeline
    – the script ran in the pipeline, but was called by a processor. You can also
    run the code inside the pipeline without having to call it. In Apache Airflow,
    you can create a validation task that has the code from the tap. To handle the
    failure, you would need to raise an exception. To do that, import the library
    in your Airflow code. I have included the libraries that you need to include on
    top of your standard boilerplate in the following code block:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，你在管道外部运行了验证套件 – 脚本在管道中运行，但被处理器调用。你还可以在管道内运行代码，而不需要调用它。在Apache Airflow中，你可以创建一个具有来自tap的代码的验证任务。为了处理失败，你需要抛出一个异常。为此，在你的Airflow代码中导入库。我在下面的代码块中包含了你需要包含在你标准样板之上的库：
- en: '[PRE12]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After importing all of the libraries, you can write your task, as shown in
    the following code block:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入所有库之后，你可以编写你的任务，如下面的代码块所示：
- en: '[PRE13]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The preceding code will throw an error, or it will end if the validation succeeded.
    However, choosing to handle the failure is up to you. All you need to do is check
    whether `results["success"]` is `True`. You can now code the other functions,
    create the tasks using `PythonOperator`, and then set the downstream relationships
    as you have in all the other Airflow examples.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将引发错误，或者如果验证成功，则结束。然而，选择处理失败取决于您。您需要做的只是检查`results["success"]`是否为`True`。您现在可以编写其他函数，使用`PythonOperator`创建任务，然后设置与所有其他Airflow示例相同的下游关系。
- en: The following sections will discuss two other features of a production data
    pipeline – idempotence and atomicity.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 以下几节将讨论生产数据管道的两个其他特性——幂等性和原子性。
- en: Building idempotent data pipelines
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建幂等数据管道
- en: A crucial feature of a production data pipeline is that it is idempotent. Idempotent
    is defined as *denoting an element of a set that is unchanged in value when multiplied
    or otherwise operated on by itself*.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 生产数据管道的一个关键特性是它必须是幂等的。幂等被定义为*表示一个集合的元素，当它自身乘以或以其他方式操作时，其值不变*。
- en: In data science, this means that when your pipeline fails, which is not a matter
    of *if*, but *when*, it can be rerun and the results are the same. Or, if you
    accidently click run on your pipeline three times in a row by mistake, there are
    not duplicate records – even if you accidently click run multiple times in a row.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学中，这意味着当您的管道失败时，这不是一个“是否”的问题，而是一个“何时”的问题，它可以重新运行，并且结果相同。或者，如果您不小心连续三次错误地点击了管道的运行按钮，则不会有重复的记录——即使您连续多次错误地点击运行按钮。
- en: In [*Chapter 3*](B15739_03_ePub_AM.xhtml#_idTextAnchor039)*, Reading and Writing
    Files*, you created a data pipeline that generated 1,000 records of people and
    put that data in an Elasticsearch database. If you let that pipeline run every
    5 minutes, you would have 2,000 records after 10 minutes. In this example, the
    records are all random and you may be OK. But what if the records were rows queried
    from another system?
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第3章*](B15739_03_ePub_AM.xhtml#_idTextAnchor039)*，读取和写入文件*中，您创建了一个数据管道，生成了1,000条人员记录，并将这些数据放入Elasticsearch数据库中。如果您让这个管道每5分钟运行一次，那么10分钟后您将会有2,000条记录。在这个例子中，记录都是随机的，您可能没问题。但如果记录是从另一个系统中查询的行呢？
- en: Every time the pipeline runs, it would insert the same records over and over
    again. How you create idempotent data pipelines depends on what systems you are
    using and how you want to store your data.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 每次管道运行时，它都会反复插入相同的记录。您如何创建幂等数据管道取决于您使用的是哪些系统和您希望如何存储您的数据。
- en: In the SeeClickFix data pipeline from the previous chapter, you queried the
    SeeClickFix API. You did not specify any rolling time frame that would only grab
    the most recent records, and your backfill code grabbed all the archived issues.
    If you run this data pipeline every 8 hours, as it was scheduled, you will grab
    new issues, but also issues you already have.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章的SeeClickFix数据管道中，您查询了SeeClickFix API。您没有指定任何滚动时间范围，以仅获取最新的记录，并且您的回填代码抓取了所有归档的问题。如果您按照计划每8小时运行此数据管道一次，那么您将抓取新问题，但也会抓取您已经有的问题。
- en: The SeeClickFix data pipeline used the `upsert` method in Elasticsearch to make
    the pipeline idempotent. Using the `EvaluteJsonPath` processor, you extracted
    the issue ID and then used that as the `Identifier Attribute` in the `PutElasticsearchHttp`
    processor. You also set the `upsert`. This is the equivalent of using an update
    in SQL. No records will be duplicated, and records will only be modified if there
    have been changes.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: SeeClickFix数据管道使用Elasticsearch中的`upsert`方法来使管道幂等。使用`EvaluteJsonPath`处理器，您提取了问题ID，然后将其用作`PutElasticsearchHttp`处理器中的`Identifier
    Attribute`。您还设置了`upsert`。这相当于在SQL中使用更新。不会重复记录，并且只有在有更改的情况下才会修改记录。
- en: Another way to make the data pipeline idempotent, and one that is advocated
    by some functional data engineering advocates, is to create a new index or partition
    every time your data pipeline is run. If you named your index with the datetime
    stamped as a suffix, you would get a new index with distinct records every time
    the pipeline runs. This not only makes the data pipeline idempotent; it creates
    an immutable object out of your database indexes. An index will never change;
    just new indexes will be added.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 使数据管道幂等的另一种方法，一些功能数据工程倡导者所推崇的方法，是在每次运行数据管道时创建一个新的索引或分区。如果您将索引命名为带有时间戳后缀的名称，那么每次管道运行时都会得到一个新的具有不同记录的索引。这不仅使数据管道幂等，还从您的数据库索引中创建了一个不可变对象。索引永远不会改变；只是会添加新的索引。
- en: Building atomic data pipelines
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建原子数据管道
- en: The final feature of a production data pipeline that we will discuss in this
    chapter is atomicity. Atomicity means that if a single operation in a transaction
    fails, then all of the operations fail. If you are inserting 1,000 records into
    the database, as you did in [*Chapter 3*](B15739_03_ePub_AM.xhtml#_idTextAnchor039)*,
    Reading and Writing Files*, if one record fails, then all 1,000 fail.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论生产数据管道的最后一个特性，即原子性。原子性意味着如果事务中的单个操作失败，那么所有操作都会失败。如果你正在将1,000条记录插入数据库，就像你在[*第3章*](B15739_03_ePub_AM.xhtml#_idTextAnchor039)“读取和写入文件”中所做的那样，如果有一条记录失败，那么所有1,000条都会失败。
- en: In SQL databases, the database will roll back all the changes if record number
    500 fails, and it will no longer attempt to continue. You are now free to retry
    the transaction. Failures can occur for many reasons, some of which are beyond
    your control. If the power or the network goes down while you are inserting records,
    do you want those records to be saved to the database? You would then need to
    determine which records in a transaction succeeded and which failed and then retry
    only the failed records. This would be much easier than retrying the entire transaction.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQL数据库中，如果记录编号500失败，数据库将回滚所有更改，并且将不再尝试继续。你现在可以自由地重试事务。失败可能由许多原因引起，其中一些是你无法控制的。如果你在插入记录时电源或网络中断，你希望这些记录被保存到数据库中吗？那么你需要确定事务中哪些记录成功，哪些失败，然后只重试失败的记录。这将比重试整个事务容易得多。
- en: In the NiFi data pipelines you have built, there was no atomicity. In the SeeClickFix
    example, each issue was sent as a flowfile and upserted in Elasticsearch. The
    only atomicity that existed is that every field in the document (issue) succeeded
    or failed. But we could have had a situation where all the issues failed except
    one, and that would have resulted in the data pipeline succeeding.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在你构建的NiFi数据管道中，没有原子性。在SeeClickFix示例中，每个问题都作为一个flowfile发送，并在Elasticsearch中更新。唯一存在的原子性是文档（问题）中的每个字段都成功或失败。但可能存在这样的情况：所有问题都失败了，只有一个成功了，这会导致数据管道成功。
- en: 'Elasticsearch does not have atomic transactions, so any data pipeline that
    implements Elasticsearch would need to handle that within the logic. For example,
    you could track every record that is indexed in Elasticsearch as well as every
    failure relationship. If there is a failure relationship during the run, you would
    then delete all the successfully indexed issues. An example data pipeline is shown
    in the following screenshot:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch没有原子事务，因此任何实现Elasticsearch的数据管道都需要在逻辑中处理这一点。例如，你可以跟踪在Elasticsearch中索引的每条记录以及每个失败关系。如果在运行过程中出现失败关系，那么就会删除所有成功索引的问题。以下截图显示了示例数据管道：
- en: '![Figure 7.20 – Building atomicity into a data pipeline'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.20 – 将原子性构建到数据管道中'
- en: '](img/Figure_7.20_B15739.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 7.20](img/Figure_7.20_B15739.jpg)'
- en: Figure 7.20 – Building atomicity into a data pipeline
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.20 – 将原子性构建到数据管道中
- en: The preceding data pipeline created two flowfiles; one succeeded and one failed.
    The contents of both are put in files on disk. From here, your data pipeline could
    list the files in the failed directory. If there was one or more, it could then
    read the success files and remove them from Elasticsearch.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的数据管道创建了两个flowfiles；一个成功，一个失败。两个的内容都放在磁盘上的文件中。从这里，你的数据管道可以列出失败目录中的文件。如果有零个或多个，那么可以读取成功文件并将它们从Elasticsearch中删除。
- en: This is not elegant, but atomicity is important. Debugging data pipeline failures
    when the failure is only partial is extremely difficult and time consuming. The
    extra work required to incorporate atomicity is well worth it.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不优雅，但原子性很重要。当失败只是部分时，调试数据管道失败非常困难且耗时。为了引入原子性所需的额外工作是非常值得的。
- en: SQL databases have atomicity built into the transactions. Using a library such
    as `psycopg2`, you can roll multiple inserts, updates, or deletes into a single
    transaction and guarantee that the results will either be that all operations
    were successful, or the transaction failed.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: SQL数据库在事务中内置了原子性。使用如`psycopg2`这样的库，你可以将多个插入、更新或删除操作合并为一个事务，并保证结果要么是所有操作都成功，要么是事务失败。
- en: Creating data pipelines that are idempotent and atomic requires additional work
    when creating your data pipeline. But without these two features, you will have
    data pipelines that will make changes to your results if accidently run multiple
    times (not idempotent) or if there are records that are missing (not atomic).
    Debugging these issues is difficult, so the time spent on making your data pipelines
    idempotent and atomic is well spent.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 创建幂等性和原子性的数据管道在创建数据管道时需要额外的工作。但是，如果没有这两个特性，你的数据管道在意外多次运行时（非幂等）或存在缺失记录时（非原子）会对结果进行更改。调试这些问题是困难的，因此，在使你的数据管道幂等和原子性上花费的时间是值得的。
- en: Summary
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, you learned three key features of production data pipelines:
    staging and validation, idempotency, and atomicity. You learned how to use Great
    Expectations to add production-grade validation to your data pipeline staged data.
    You also learned how you could incorporate idempotency and atomicity into your
    pipelines. With these skills, you can build more robust, production-ready pipelines.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了生产数据管道的三个关键特性：预演和验证、幂等性和原子性。你学习了如何使用Great Expectations为你的数据管道中的预演数据添加生产级验证。你还学习了如何将幂等性和原子性融入你的管道中。掌握这些技能后，你可以构建更健壮、适用于生产环境的管道。
- en: In the next chapter, you will learn how to use version control with the NiFi
    registry.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习如何使用NiFi注册表进行版本控制。
