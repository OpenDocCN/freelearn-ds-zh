<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Python Geoprocessing with Hadoop</h1>
                </header>
            
            <article>
                
<p>Most of the examples in this book worked with relatively small datasets using a single computer. But as data gets larger, the datasets and even individual files may be spread out over a cluster of machines. Working with big data requires different tools. In this chapter, you will learn how to use Apache Hadoop to work with big data, and the Esri GIS tools for Hadoop to work with the big data spatially.</p>
<p>This chapter will teach you how to:</p>
<ul>
<li>Install Linux</li>
<li>Install and run Docker</li>
<li>Install and configure a Hadoop environment</li>
<li>Work with files in HDFS</li>
<li>Basic queries using Hive</li>
<li>Install the Esri GIS tools for Hadoop</li>
<li>Perform spatial queries in Hive</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is Hadoop?</h1>
                </header>
            
            <article>
                
<p>Hadoop is an open-source framework for working with large quantities of data spread across a single computer to thousands of computers. Hadoop is composed of four modules:</p>
<ul>
<li><strong>Hadoop Core</strong></li>
<li><strong>Hadoop Distributed File System</strong> (<strong>HDFS</strong>)</li>
<li><strong>Yet Another Resource Negotiator</strong> (<strong>YARN</strong>)</li>
<li><strong>MapReduce</strong></li>
</ul>
<p>The Hadoop Core makes up the components needed to run the other three modules. HDFS is a Java-based file system that has been designed to be distributed and is capable of storing large files across many machines. By large files, we are talking terabytes. YARN manages the resources and scheduling in your Hadoop framework. The MapReduce engine allows you to process data in parallel.</p>
<p>There are several other projects that can be installed to work with the Hadoop framework. In this chapter, you will use Hive and Ambari. Hive allows you to read and write data using SQL. You will use Hive to run the spatial queries on your data at the end of the chapter. Ambari provides a web user interface to Hadoop and Hive. In this chapter, you will use it to upload files and to enter your queries.  </p>
<p>Now that you have an overview of Hadoop, the next section will show you how to set up your environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing the Hadoop framework</h1>
                </header>
            
            <article>
                
<p>In this chapter, you will not configure each of the Hadoop framework components yourself. You will run a Docker image, which requires you to install Docker. Currently, Docker runs on Windows 10 Pro or Enterprise, but it runs much better on Linux or Mac. Hadoop also runs on Windows but requires you to build it from source, and so it will be much easier to run it on Linux. Also, the Docker image you will use is running Linux, so getting familiar with Linux may be beneficial. In this section, you will learn how to install Linux.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Linux</h1>
                </header>
            
            <article>
                
<p>The first step to set up the Hadoop framework is to install Linux. You will need to get a copy of a Linux operating system. There are many flavors of Linux. You can choose whichever version you like, however, this chapter was written using CentOS 7 because most of the tools you will be installing have also been tested on CentOS. CentOS is a Red Hat-based version of Linux. You can download an ISO at: <a href="https://www.centos.org/">https://www.centos.org/</a>. Select <span class="packt_screen">Get CentOS Now</span>. Then, select DVD image. Choose a mirror to download the ISO.</p>
<p>After downloading the image, you can burn it to a disk using Windows. Once you have burned the disk, place it in the machine that will run Linux and start it. The installation will prompt you along the way. Two steps to pay attention to are the software selection step and the partitioning. For software selection, choose <span class="packt_screen">GNOME Desktop</span>. This will provide a sufficient base system with a popular GUI. If you have another file system on the computer, you can overwrite it or select the free space on the partitioning screen.</p>
<p>For a more detailed explanation of how to install Linux, Google is your friend. There are many excellent walkthroughs and YouTube videos that will walk you through it. Unfortunately, it looks as though the CentOS website does not have an installation manual for CentOS 7.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Docker</h1>
                </header>
            
            <article>
                
<p>Docker provides the software so that you can run containers. A <strong>container</strong> is an executable that contains everything you need to run the software it contains. For example, if I have a Linux system configured to run Hadoop, Hive, and Ambari, and I create a container from it, I can give you the container, and when you run it, it will contain everything you need for that system to work, no matter the configuration or software installed on your computer. The same applies if I give that container image to any other person. It will always run the same. A container is not a virtual machine. A virtual machine is an abstraction at the hardware level and a container is an abstraction at the application layer. A container has everything you need to run a piece of software. For this chapter, that is all you need to know.<br/>
<br/>
Now that you have Linux installed and have an understanding of what Docker is, you can install a copy of Docker. Using your terminal, enter the following command:</p>
<pre><strong>curl -fsSL https://get.docker.com/ | sh</strong></pre>
<p>The preceding command uses the <kbd>curl</kbd> application to download and install the latest version of Docker. The parameters tell <kbd>curl</kbd> to, in order, fail silently on server errors, do not show progress, report any errors, and redirect if the server says the location has changed. The output of the <kbd>curl</kbd> command is piped - <kbd>|</kbd> - to the <kbd>sh</kbd> (Bash shell) to execute.</p>
<p>When Docker has installed, you can run it by executing the following command:</p>
<pre><strong>sudo systemctl start docker</strong></pre>
<p>The previous command uses <kbd>sudo</kbd> to run the command as an administrator (<kbd>root</kbd>). Think of this as right-clicking in Windows and selecting the <span class="packt_screen">Run as administrator</span> option. The next command is <kbd>systemctl</kbd>. This is how you start services in Linux. Lastly, <kbd>start docker</kbd> does exactly that, it starts <kbd>docker</kbd>. If you receive an error when executing the earlier mentioned command that mentions sudoers, your user may not have permission to run applications as the <kbd>root</kbd>. You will need to log in as the <kbd>root</kbd> (or use the <kbd>su</kbd> command) and edit the text file at <kbd>/etc/sudoers</kbd>. Add the following line:</p>
<pre><strong>your username  ALL=(ALL) ALL</strong></pre>
<p class="mce-root CDPAlignLeft CDPAlign">The previous line will give you permission to use <kbd>sudo</kbd>. Your <kbd>/etc/sudoers</kbd> file should look like the following screenshot:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/737f6f46-0fa2-4f2d-94ce-b99d42273c68.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">Now that you have <kbd>docker</kbd> running, you can download the image we will load which contains the Hadoop framework.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Install Hortonworks</h1>
                </header>
            
            <article>
                
<p>Instead of installing Hadoop and all the other components, you will use a preconfigured Docker image. Hortonworks has a Data Platform Sandbox that already has a container which you can load in Docker. To download it, go to <a href="https://hortonworks.com/downloads/#sandbox">https://hortonworks.com/downloads/#sandbox</a> and select <span class="packt_screen">DOWNLOAD FOR DOCKER</span>.</p>
<p>You will also need to install the <kbd>start_sandox_hdp_version.sh</kbd> script. This will simplify the launching of the container in Docker. You can download the script from GitHub at: <a href="https://gist.github.com/orendain/8d05c5ac0eecf226a6fed24a79e5d71a">https://gist.github.com/orendain/8d05c5ac0eecf226a6fed24a79e5d71a</a><a href="https://raw.githubusercontent.com/hortonworks/data-tutorials/master/tutorials/hdp/sandbox-deployment-and-install-guide/assets/start_sandbox-hdp.sh.">.</a></p>
<p>Now you will need to load the image in Docker. The following command will show you how:</p>
<pre><strong>docker load -i &lt;image name&gt;</strong></pre>
<p>The previous command loads the image into Docker. The image name will be similar to <kbd>HDP_2.6.3_docker_10_11_2017.tar</kbd>, but it will change depending on your version. To see that the sandbox has been loaded, run the following command:</p>
<pre><strong>docker images</strong></pre>
<p class="mce-root CDPAlignLeft CDPAlign">The output, if you have no other containers, should look as it does in the following screenshot:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/56dc5a44-976b-4eb5-9a74-9b5f075fc8f2.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">In order to use the web-based GUI Ambari, you will want to have a domain name established for the sandbox. To do that, you will need the IP address of the container. You can get it by running two commands:</p>
<pre><strong>docker ps<br/></strong><strong>docker inspect &lt;container ID&gt;</strong> </pre>
<p>The first command will have the <kbd>container ID</kbd>, and the second command will take the <kbd>container ID</kbd> and return a lot of information, with the IP address being towards the end. Or, you can take advantage of the Linux command line and just get the IP address by using the following command:</p>
<pre><strong>docker inspect $(docker ps --format "{{.ID}}") --format="{{json .NetworkSettings.IPAddress}}"</strong></pre>
<p>The previous command wraps the previously mentioned commands into a single command. The <kbd>docker inspect</kbd> command takes the output of <kbd>docker ps</kbd> as the <kbd>container ID</kbd>. It does so by wrapping it in <kbd>$()</kbd>, but it also passes a filter so that only the <kbd>ID</kbd> is returned. Then, the <kbd>inspect</kbd> command also includes a filter to only return the IP address. The text between the <kbd>{{}}</kbd> is a Go template. The output of this command should be an IP address, for example, 172.17.0.2.</p>
<p>Now that you have the IP address of the image, you should update your host's file using the following command:</p>
<pre><strong>echo '172.17.0.2 sandbox.hortonworks.com sandbox-hdp.hortonworks.com sandbox-hdf.hortonworks.com' | sudo tee -a /etc/hosts</strong></pre>
<p>The previous command redirects the output of the <kbd>echo</kbd>—which is the text you want in your <kbd>/etc/hosts</kbd><span><span> </span></span><span>file and sends it to the</span> <kbd>sudo tee -a /etc/hosts</kbd><span> command. This second command uses</span> <kbd>sudo</kbd> <span>to run as</span> <kbd>root</kbd><span>. The</span> <kbd>tee</kbd> <span>command sends the output to a file and to the terminal (</span><kbd>STDOUT</kbd><span>). The</span> <kbd>-a</kbd> <span>tells</span> <kbd>tee</kbd> <span>to append to the file, and</span> <kbd>/etc/hosts</kbd> <span>is the file you want to append. Now, in your browser, you will be able to use names instead of the IP address.</span></p>
<p>Now you are ready to launch the image and browse to your Hadoop framework.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hadoop basics</h1>
                </header>
            
            <article>
                
<p>In this section, you will launch your Hadoop image and learn how to connect using <kbd>ssh</kbd> and Ambari. You will also move files and perform a basic Hive query. Once you understand how to interact with the framework, the next section will show you how to use a spatial query.</p>
<p>First, from the terminal, launch the Hortonworks Sandbox using the provided Bash script. The following command will show you how:</p>
<pre><strong>sudo sh start_sandbox-hdp.sh</strong></pre>
<p>The previous command executes the script you downloaded with the sandbox. Again, it used <kbd>sudo</kbd> to run as <kbd>root</kbd>. Depending on your machine, it may take some time to completely load and start all the services. When it is done, your terminal should look like it does in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/da573ba6-8cc1-4b01-b171-4d716cd3555b.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Connecting via Secure Shell</h1>
                </header>
            
            <article>
                
<p>Now that the sandbox is running, you can connect using Secure Shell (SSH). The secure shell allows you to log in remotely to another machine. Open a new terminal and enter the following command:</p>
<pre><strong>ssh raj_ops@127.0.0.1 -p2222</strong></pre>
<p>The previous command uses <kbd>ssh</kbd> to connect as user <kbd>raj_ops</kbd> to the <kbd>localhost</kbd> (<kbd>127.0.0.1</kbd>) on port <kbd>2222</kbd>. You will get a warning that the authenticity of the host cannot be established. We did not create any keys for <kbd>ssh</kbd>. Just type <kbd>yes</kbd> and you will be prompted for the password. The user <kbd>raj_ops</kbd> has the password <kbd>raj_ops</kbd>. Your terminal prompt should now look like the following line:</p>
<pre><strong>[raj_ops@sandbox-hdp ~]$</strong></pre>
<p>If your terminal is like it is in the previous code, you are now logged into the container.</p>
<div class="packt_infobox">For more information on users, their permissions, and configuring the sandbox, go to the following page: <a href="https://hortonworks.com/tutorial/learning-the-ropes-of-the-hortonworks-sandbox/">https://hortonworks.com/tutorial/learning-the-ropes-of-the-hortonworks-sandbox/</a></div>
<p>You can now use most Linux commands to navigate the container. You can now download and move files around, run Hive, and all your other tools from the command line. This section has already been heavy enough on Linux, so you will not use the command line exclusively in this chapter. Instead, the next section will show you how to execute these tasks in Ambari, a web-based GUI for executing tasks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ambari</h1>
                </header>
            
            <article>
                
<p>Ambari is a UI for making the management of Hadoop easier. In the previous section, you learned how to implement <kbd>ssh</kbd> into the container. From there you could manage Hadoop, run Hive queries, download data, and add it to the HDFS file system. Ambari makes all of this much simpler, especially if you are not familiar with the command line. To open Ambari, browse to the URL as: <a href="http://sandbox.hortonworks.com:8080/">http://sandbox.hortonworks.com:8080/</a>.</p>
<div class="packt_infobox">The Ambari URL depends on your installation. If you have followed the instructions in this chapter, then this will be your URL. You must also have started the server from the Docker image.</div>
<p class="mce-root CDPAlignLeft CDPAlign">You will be directed to the Ambari login page. Enter the user/password combination of <kbd>raj_ops</kbd>/<kbd>raj_ops</kbd>, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/65c4ceb9-cceb-4d6a-b092-ff998fcf28ee.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">After logging in, you will see the Ambari <span class="packt_screen">Dashboard</span>. It will look like it does in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3eae97d4-ff3a-4642-86be-336f5f71d124.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">On the left, you have a list of services. The main portion of the window contains the metrics, and the top menu bar has tabs for different functions. In this chapter, you will use the square comprised of nine smaller squares. Hover over the square icon and you will see a drop-down for the files view.</p>
<p class="mce-root CDPAlignLeft CDPAlign">This is the <kbd>root</kbd> directory of the HDFS file system.</p>
<div class="packt_infobox">When connected to the container via <kbd>ssh</kbd>, run the <kbd>hdfs dfs -ls /</kbd> command and you will see the same directory structure.</div>
<p>From here, you can upload files. To try it out, open a text editor and create a simple CSV. This example will use the following data:</p>
<pre>40, Paul<br/>23, Fred<br/>72, Mary<br/>16, Helen<br/>16, Steve </pre>
<p class="mce-root CDPAlignLeft CDPAlign">Save the CSV file and then click the <span class="packt_screen">Upload</span> button in Ambari. You will be able to drag and drop the CSV to the browser. Ambari added the file to the HDFS file system on the container:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/bdd196ed-7da5-4458-afca-231aea3689d5.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">Now that you have data loaded in the container, you can query it in Hive using SQL. Using the square icon again, select the drop-down for <span class="packt_screen">Hive View 2.0</span>. You should see a workspace as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/384475d8-8bd7-4fc7-93e9-3d5d53389ba0.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">In <span class="packt_screen">Hive</span>, you have worksheets. On the worksheet, you have the database you are connected to, which in this case is the <span class="packt_screen">default</span>. Underneath that, you have the main query window. To the right, you have a list of existing tables. Lastly, scrolling down, you will see the <span class="packt_screen">Execute</span> button, and under that is where the results will be loaded.<br/>
<br/>
In the query pane, enter the SQL query as follows:</p>
<pre>SELECT * FROM sample_07</pre>
<p class="mce-root CDPAlignLeft CDPAlign">The previous query is a basic select all in SQL. The results will be shown as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5d00e72c-b222-454c-93bc-ca3253e11597.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Esri GIS tools for Hadoop</h1>
                </header>
            
            <article>
                
<p class="mce-root">With your environment set up and some basic knowledge of Ambari, HDFS, and Hive, you will now learn how to add a spatial component to your queries. To do so, we will use the Esri GIS tools for Hadoop.</p>
<p class="mce-root">The first step is to download the files located at the GitHub repository, which is located at: <a href="https://github.com/Esri/gis-tools-for-hadoop">https://github.com/Esri/gis-tools-for-hadoop</a>. You will be using Ambari to move the files to HDFS not the container, so download these files to your local machine.</p>
<div class="mce-root packt_infobox">Esri has a tutorial for downloading the files by using <kbd>ssh</kbd> to connect to the container and then using <kbd>git</kbd> to clone the repository. You can follow these instructions here: <a href="https://github.com/Esri/gis-tools-for-hadoop/wiki/GIS-Tools-for-Hadoop-for-Beginners">https://github.com/Esri/gis-tools-for-hadoop/wiki/GIS-Tools-for-Hadoop-for-Beginners</a>.</div>
<p class="mce-root">You can download the files by using the GitHub <span class="packt_screen">Clone or download</span> button on the right-hand side of the repository. To unzip the archive, use one of the following commands:</p>
<pre class="mce-root">unzip gis-tools-for-hadoop-master.zip<br/>unzip gis-tools-for-hadoop-master.zip -d /home/pcrickard</pre>
<p class="mce-root">The first command will unzip the file in the current directory, which is most likely the <kbd>Downloads</kbd> folder of your home directory. The second command will unzip the file, but by passing <kbd>-d</kbd> and a path, it will unzip to that location. In this case, this is the <kbd>root</kbd> of my home directory.</p>
<p class="mce-root">Now that you have the files unzipped, you can open the <span class="packt_screen">Files View</span> in Ambari by selecting it from the box icon drop-down menu. Select <span class="packt_screen">Upload</span> and a modal will open, allowing you to drop a file. On your local machine, browse to the location of the Esri <strong>Java ARchive</strong> (JAR) files. If you moved the zip to your home directory, the path will be similar to <kbd>/home/pcrickard/gis-tools-for-hadoop-master/samples/lib</kbd>. You will have three JAR files:</p>
<ul>
<li class="mce-root"><kbd>esri-geometry-api-2.0.0.jar</kbd></li>
<li class="mce-root"><kbd>spatial-sdk-hive-2.0.0.jar</kbd></li>
<li class="mce-root"><kbd>spatial-sdk-json-2.0.0.jar</kbd></li>
</ul>
<p class="mce-root">Move each of these three files to the <kbd>root</kbd> folder in Ambari. This is the <kbd>/</kbd> directory, which is the default location that opens when you launch <span class="packt_screen">Files View</span>.</p>
<p class="mce-root">Next, you would normally move the data to HDFS as well, however, you did that in the previous example. In this example, you will leave the data files on your local machine and you will learn how you can load them into a Hive table without being on HDFS.</p>
<p class="mce-root">Now you are ready to execute the spatial query in Hive. From the box icon drop-down, select <span class="packt_screen">Hive View 2.0</span>. In the query pane, enter the following code:</p>
<pre class="mce-root">add jar hdfs:///esri-geometry-api-2.0.0.jar;<br/>add jar hdfs:///spatial-sdk-json-2.0.0.jar;<br/>add jar hdfs:///spatial-sdk-hive-2.0.0.jar;<br/> <br/>create temporary function ST_Point as 'com.esri.hadoop.hive.ST_Point';<br/>create temporary function ST_Contains as 'com.esri.hadoop.hive.ST_Contains';<br/> <br/>drop table earthquakes;<br/>drop table counties;<br/> <br/>CREATE TABLE earthquakes (earthquake_date STRING, latitude DOUBLE, longitude DOUBLE, depth DOUBLE, magnitude DOUBLE,magtype string, mbstations string, gap string, distance string, rms string, source string, eventid string)<br/>ROW FORMAT DELIMITED FIELDS TERMINATED BY ','<br/>STORED AS TEXTFILE;<br/> <br/>CREATE TABLE counties (Area string, Perimeter string, State string, County string, Name string, BoundaryShape binary)                  <br/>ROW FORMAT SERDE 'com.esri.hadoop.hive.serde.EsriJsonSerDe'<br/>STORED AS INPUTFORMAT 'com.esri.json.hadoop.EnclosedEsriJsonInputFormat'<br/>OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat';<br/> <br/>LOAD DATA LOCAL INPATH '/gis-tools-for-hadoop-master/samples/data/earthquake-data/earthquakes.csv' OVERWRITE INTO TABLE earthquakes;<br/> <br/>LOAD DATA LOCAL INPATH '/gis-tools-for-hadoop-master/samples/data/counties-data/california-counties.json' OVERWRITE INTO TABLE counties;<br/> <br/>SELECT counties.name, count(*) cnt FROM counties<br/>JOIN earthquakes<br/>WHERE ST_Contains(counties.boundaryshape, ST_Point(earthquakes.longitude, earthquakes.latitude))<br/>GROUP BY counties.name<br/>ORDER BY cnt desc;</pre>
<p class="mce-root CDPAlignLeft CDPAlign">Running the preceding code will take some time depending on your machine. The end result will look like it does in the following image:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/6d09ff04-db16-420f-8c00-85d0ebc55456.png" style="width:13.92em;height:42.08em;"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">The previous code and results were presented without explanation so that you could get the example to work and see the output. Following that, the code will be explained block by block.</p>
<p class="mce-root CDPAlignLeft CDPAlign">The first block of code is shown as follows:</p>
<pre class="mce-root">add jar hdfs:///esri-geometry-api-2.0.0.jar;<br/>add jar hdfs:///spatial-sdk-json-2.0.0.jar;<br/>add jar hdfs:///spatial-sdk-hive-2.0.0.jar;<br/><br/>create temporary function ST_Point as 'com.esri.hadoop.hive.ST_Point';<br/>create temporary function ST_Contains as 'com.esri.hadoop.hive.ST_Contains';<br/> </pre>
<p class="mce-root">This block adds the JAR files from the HDFS location. In this case, it is the <kbd>/</kbd> folder. Once the code loads the JAR files, it can then create the functions <kbd>ST_Point</kbd> and <kbd>ST_Contains</kbd> by calling the classes in the JAR files. A JAR file may contain many Java files (classes). The order of the <kbd>add jar</kbd> statements matter.</p>
<p class="mce-root">The following block drops two tables—<kbd>earthquakes</kbd> and <kbd>counties</kbd>. If you had never run the example, you could skip these lines:</p>
<pre class="mce-root">drop table earthquakes;<br/>drop table counties; </pre>
<p class="mce-root">Next, the code creates the tables for <kbd>earthquakes</kbd> and <kbd>counties</kbd>. The <kbd>earthquakes</kbd> table is created and each field and type are passed to <kbd>CREATE</kbd>. The row format is specified as CSV—the <kbd>','</kbd>. Lastly, it is in a text file:</p>
<pre class="mce-root">CREATE TABLE earthquakes (earthquake_date STRING, latitude DOUBLE, longitude DOUBLE, depth DOUBLE, magnitude DOUBLE,magtype string, mbstations string, gap string, distance string, rms string, source <br/>string, eventid string)<br/>ROW FORMAT DELIMITED FIELDS TERMINATED BY ','<br/>STORED AS TEXTFILE;</pre>
<p class="mce-root">The <kbd>counties</kbd> table is created in a similar fashion by passing the field names and types to <kbd>CREATE</kbd>, but the data is in JSON format and will use the <kbd>com.esri.hadoop.hive.serde.EsriJSonSerDe</kbd> class in the JAR <kbd>spatial-sdk-json-2.0.0</kbd> that you imported. <kbd>STORED AS INPUTFORMAT</kbd> and <kbd>OUTPUTFORMAT</kbd> are required for Hive to know how to parse and work with the JSON data:</p>
<pre class="mce-root">CREATE TABLE counties (Area string, Perimeter string, State string, County string, Name string, BoundaryShape binary)                  <br/>ROW FORMAT SERDE 'com.esri.hadoop.hive.serde.EsriJsonSerDe'<br/>STORED AS INPUTFORMAT 'com.esri.json.hadoop.EnclosedEsriJsonInputFormat'<br/>OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat';</pre>
<p class="mce-root">The next two blocks load the data into the created tables. The data exists on your local machine and not on HDFS. To use the local data without first loading it in HDFS, you can use the <kbd>LOCAL</kbd> command with <kbd>LOAD DATA INPATH</kbd> and specify the local path of the data:</p>
<pre class="mce-root">LOAD DATA LOCAL INPATH '/gis-tools-for-hadoop-master/samples/data/earthquake-data/earthquakes.csv' OVERWRITE INTO TABLE earthquakes;<br/> <br/>LOAD DATA LOCAL INPATH '/gis-tools-for-hadoop-master/samples/data/counties-data/california-counties.json' OVERWRITE INTO TABLE counties;<br/> </pre>
<p class="mce-root">With the JAR files loaded and the tables created and populated with data, you can now run a spatial query using the two defined functions—<kbd>ST_Point</kbd> and <kbd>ST_Contains</kbd>. These are used as in the examples from <a href="42c1ea5a-7372-4688-bb7f-fc3822248562.xhtml" target="_blank">Chapter 3</a>, <em>Introduction to Geodatabases</em>:</p>
<pre class="mce-root"> SELECT counties.name, count(*) cnt FROM counties<br/> JOIN earthquakes<br/> WHERE ST_Contains(counties.boundaryshape, <br/> ST_Point(earthquakes.longitude, earthquakes.latitude))<br/> GROUP BY counties.name<br/> ORDER BY cnt desc;</pre>
<p class="mce-root">The previous query selects the <kbd>name</kbd> of the county and the <kbd>count</kbd> of earthquakes by passing the county geometry and the location of each earthquake as a point to <kbd>ST_Contains</kbd>. The results are shown as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6d09ff04-db16-420f-8c00-85d0ebc55456.png" style="width:13.42em;height:40.58em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">HDFS and Hive in Python</h1>
                </header>
            
            <article>
                
<p>This book is about Python for geospatial development, so in this section, you will learn how to use Python for HDFS operations and Hive queries. There are several database wrapper libraries with Python and Hadoop, but it does not seem like a single library has become a standout go-to library, and others, like Snakebite, don't appear ready to run on Python 3. In this section, you will learn how to use two libraries—PyHive and PyWebHDFS. You will also learn how you can use the Python subprocess module to execute HDFS and Hive commands.</p>
<p>To get PyHive, you can use <kbd>conda</kbd> and the following command:</p>
<pre><strong>conda install -c blaze pyhive</strong></pre>
<p>You may also need to install the <kbd>sasl</kbd> library:</p>
<pre><strong>conda install -c blaze sasl</strong></pre>
<p>The previous libraries will give you the ability to run Hive queries from Python. You will also want to be able to move files to HDFS. To do so, you can install <kbd>pywebhdfs</kbd>:</p>
<pre><strong>conda install -c conda-forge pywebhdfs</strong></pre>
<p>The preceding command will install the library, and as always, you can also use <kbd>pip</kbd> install or use any other method.</p>
<p>With the libraries installed, let's first look at <kbd>pywebhdfs</kbd>.</p>
<div class="packt_infobox">The documentation for <kbd>pywebhdfs</kbd> is located at: <a href="http://pythonhosted.org/pywebhdfs/">http://pythonhosted.org/pywebhdfs/</a></div>
<p>To make a connection in Python, you need to know the location of your Hive server. If you have followed this chapter, particularly the configuration changes in <kbd>/etc/hosts</kbd>—you can do so using the following code:</p>
<pre>from pywebhdfs.webhdfs import PyWebHdfsClient as h<br/>hdfs=h(host='sandbox.hortonworks.com',port='50070',user_name='raj_ops')</pre>
<p>The previous code imports the <kbd>PyWebHdfsClient</kbd> as <kbd>h</kbd>. It then creates the connection to the HDFS file system running in the container. The container is mapped to <kbd>sandbox. hortonworks.com</kbd>, and HDFS is on port <kbd>50070</kbd>. Since the examples have been using the <kbd>raj_ops</kbd> user, the code did so as well.</p>
<p>The functions now available to the <kbd>hdfs</kbd> variable are similar to your standard terminal commands, but with a different name—<kbd>mkdir</kbd> is now <kbd>make_dir</kbd> and <kbd>ls</kbd> is <kbd>list_dir</kbd>. To delete a file or directory, you will use <kbd>delete_file_dir</kbd>. The <kbd>make</kbd> and <kbd>delete</kbd> commands will return <kbd>True</kbd> if they are successful.</p>
<p>Let's look at the <kbd>root</kbd> directory of our HDFS file system using Python:</p>
<pre>ls=hdfs.list_dir('/')</pre>
<p>The previous code issued the <kbd>list_dir</kbd> command (<kbd>ls</kbd> equivalent) and assigned it to <kbd>ls</kbd>. The result is a dictionary with all the files and folders in the directory.</p>
<p>To see a single record, you can use the following code:</p>
<pre>ls['FileStatuses']['FileStatus'][0]</pre>
<p>The previous code gets to the individual records by using the dictionary keys <kbd>FileStatuses</kbd> and <kbd>FileStatus</kbd>.</p>
<div class="packt_tip">To get the keys in a dictionary, you can use <kbd>.keys(). ls.keys()</kbd> which returns <kbd>[FileStatuses]</kbd>, and <kbd>ls['FileStatuses'].keys()</kbd> which returns <kbd>['FileStatus']</kbd>.</div>
<p>The output of the previous code is shown as follows:</p>
<pre><strong>{'accessTime': 0, 'blockSize': 0, 'childrenNum': 1, 'fileId': 16404, 'group': 'hadoop', 'length': 0, 'modificationTime': 1510325976603, 'owner': 'yarn', 'pathSuffix': 'app-logs', 'permission': '777', 'replication': 0, 'storagePolicy': 0, 'type': 'DIRECTORY'}</strong></pre>
<p>Each file or directory contains several pieces of data, but most importantly the type, owner, and permissions.</p>
<p>The first step to running a Hive query example is to move our data files from the local machine to HDFS. Using Python, you can accomplish this using the following code:</p>
<pre>hdfs.make_dir('/samples',permission=755)<br/>f=open('/home/pcrickard/sample.csv')<br/>d=f.read()<br/>hdfs.create_file('/samples/sample.csv',d)</pre>
<p>The previous code creates a directory called <kbd>samples</kbd> with the permissions <kbd>755</kbd>. In Linux, permissions are based on read (<kbd>4</kbd>), write (<kbd>2</kbd>), and execute (<kbd>1</kbd>) for three types of users—owner, and group, other. So, permissions of <kbd>755</kbd> mean that the owner has read, write, and execute permissions (4+2+1 =7), and that the group and others have read and execute (4+1=5).</p>
<p>Next, the code opens and reads the CSV file we want to transfer to HDFS and assigns it to the variable <kbd>d</kbd>. The code then creates the <kbd>sample.csv</kbd> file in the <kbd>samples</kbd> directory, passing the contents of <kbd>d</kbd>.</p>
<p>To verify that the file was created, you can read the contents of the file using the following code:</p>
<pre>hdfs.read_file('/samples/sample.csv')</pre>
<p>The output of the previous code will be a string of the CSV file. It was created successfully.</p>
<p>Or, you can use the following code to get the <kbd>status</kbd> and details of the file:</p>
<pre>hdfs.get_file_dir_status('/samples/sample.csv')</pre>
<p>The previous code will return the details as follows, but only if the file or directory exists. If it does not, the preceding code will raise a <kbd>FileNotFound</kbd> error. You can wrap the preceding code in a <kbd>try</kbd>...<kbd>except</kbd> block:</p>
<pre><strong>{'FileStatus': {'accessTime': 1517929744092, 'blockSize': 134217728, 'childrenNum': 0, 'fileId': 22842, 'group': 'hdfs', 'length': 47, 'modificationTime': 1517929744461, 'owner': 'raj_ops', 'pathSuffix': '', 'permission': '755', 'replication': 1, 'storagePolicy': 0, 'type': 'FILE'}}</strong></pre>
<p>With the data file transferred to HDFS, you can move on to querying the data with Hive.</p>
<div class="packt_infobox"><br/>
The documentation for PyHive is located at: <a href="https://github.com/dropbox/PyHive">https://github.com/dropbox/PyHive</a></div>
<p>Using <kbd>pyhive</kbd>, the following code will create a table:</p>
<pre>from pyhive import hive<br/>c=hive.connect('sandbox.hortonworks.com').cursor()<br/>c.execute('CREATE TABLE FromPython (age int, name string)  ROW FORMAT DELIMITED FIELDS TERMINATED BY ","')</pre>
<p>The previous code imports <kbd>pyhive</kbd> as <kbd>hive</kbd>. It creates a connection and gets the <kbd>cursor</kbd>. Lastly, it executes a Hive statement. Once you have the connection and the <kbd>cursor</kbd>, you can make your SQL queries by wrapping them in the <kbd>.execute()</kbd> method. To load the data from the CSV in HDFS into the table and then to select all, you would use the following code:</p>
<pre>c.execute("LOAD DATA INPATH '/samples/sample.csv' OVERWRITE INTO TABLE FromPython")<br/>c.execute("SELECT * FROM FromPython")<br/>result=c.fetchall()</pre>
<p>The preceding code uses the <kbd>execute()</kbd> method two more times to load the data and then executes select all. Using <kbd>fetchall()</kbd>, the results are passed to the <kbd>result</kbd> variable and will look like they do in the following output:</p>
<pre><strong>[(40, ' Paul'), (23, ' Fred'), (72, ' Mary'), (16, ' Helen'), (16, ' Steve')]</strong></pre>
<p>Working with <kbd>pyhive</kbd> is just like working with <kbd>psycopg2</kbd> <span>—</span> the Python library for connecting to PostgreSQL. Most database wrapper libraries are very similar in that you make a connection, get a <kbd>cursor</kbd>, and then execute statements. Results can be retrieved by grabbing all, one, or next (iterable).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you learned how to set up a Hadoop environment. This required you to install Linux and Docker to download an image from Hortonworks, and to learn the ropes of that environment. Much of this chapter was spent on the environment and how to perform a spatial query using the GUI tools provided. This is because the Hadoop environment is complex and without a proper understanding, it would be hard to fully understand how to use it with Python. Lastly, you learned how to use HDFS and Hive in Python. The Python libraries for working with Hadoop, Hive, and HDFS are still developing. This chapter provided you with a foundation so that when these libraries improve, you will have enough knowledge of Hadoop and the accompanying technologies to implement these new Python libraries.</p>


            </article>

            
        </section>
    </body></html>