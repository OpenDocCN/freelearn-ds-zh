- en: 5\. Getting Comfortable with Different Kinds of Data Sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will provide you with the skills to read CSV, Excel, and JSON files
    into pandas DataFrames. You will learn how to read PDF documents and HTML tables
    into pandas DataFrames and perform basic web scraping operations using powerful
    yet easy-to-use libraries such as Beautiful Soup. You will also see how to extract
    structured and textual information from portals. By the end of this chapter, you
    will be able to implement data wrangling techniques such as web scraping in the
    real world.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this book, we have focused on studying pandas DataFrame objects as
    the main data structure for the application of wrangling techniques. In this chapter,
    we will learn about various techniques by which we can read data into a DataFrame
    from external sources. Some of these sources could be text-based (such as CSV,
    HTML, and JSON), whereas others could be binary (that is, not in ASCII format;
    for example, from Excel or PDFs). We will also learn how to deal with data that
    is present in web pages or HTML documents.
  prefs: []
  type: TYPE_NORMAL
- en: Being able to deal with and extract meaningful data from various sources is
    of paramount interest to a data practitioner. Data can, and often does, come in
    various forms and flavors. It is essential to be able to get the data into a form
    that is useful for performing predictive or other kinds of downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: As we have gone through detailed examples of basic operations with NumPy and
    pandas, in this chapter, we will often skip trivial code snippets such as viewing
    a table, selecting a column, and plotting. Instead, we will focus on showing code
    examples for the new topics we aim to learn about here.
  prefs: []
  type: TYPE_NORMAL
- en: Reading Data from Different Sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most valued and widely used skills of a data wrangling professional
    is the ability to extract and read data from a diverse array of sources into a
    structured format. Modern analytics pipelines depend on the ability and skills
    of those professionals to build a robust system that can scan and absorb a variety
    of data sources to build and analyze a pattern-rich model. Such kinds of feature-rich,
    multi-dimensional models will have high predictive and generalization accuracy.
    They will be valued by stakeholders and end users alike in any data-driven product.
    In the first part of this chapter, we will go through various data sources and
    how they can be imported into `pandas` DataFrames, thus imbuing data wrangling
    professionals with extremely valuable data ingestion knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Data Files Provided with This Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As this topic is about reading from various data sources, we will use small
    files of various types in the following exercises. All the data files are provided,
    along with the Jupyter notebook, in the code repository.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'All the data files can be accessed from the following link: [https://packt.live/3fAWg3f](https://packt.live/3fAWg3f).'
  prefs: []
  type: TYPE_NORMAL
- en: Libraries to Install for This Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As this chapter deals with reading files of various formats, we need to have
    the support of additional libraries and software platforms to accomplish our goals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we install these libraries, ensure that **Java Development Kit (JDK)**
    is installed on your system. If not, go to the following link to install it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.oracle.com/in/java/technologies/javase-downloads.html](https://www.oracle.com/in/java/technologies/javase-downloads.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Once you are on the website, click the link that says `JDK Download`. Then,
    proceed to download and install JDK based on your operating system. Once installation
    completes, ensure that you restart your system, especially if you are using Windows.
    In case you face issues after installation, check if you've set the PATH system
    variable correctly. To learn how to do that, refer [https://www.java.com/en/download/help/path.xml](https://www.java.com/en/download/help/path.xml).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once JDK is installed, go ahead and install the necessary libraries. Execute
    the following command in your Jupyter Notebook cells:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Don't forget the `!` before each line of code. This little exclamation sign
    in front of each command lets the Jupyter runtime know that what is in the cell
    is a `bash` command and not a line of Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Reading Data Using Pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `pandas` library provides a simple method called `read_csv` to read data
    in a tabular format from a comma-separated text file, or `.csv`. This is particularly
    useful because `.csv` is a lightweight yet extremely handy data exchange format
    for many applications, including such domains where machine-generated data is
    involved. It is not a proprietary format and therefore is universally used by
    a variety of data-generating sources.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, a `.csv` file has two sections. The first line of a `.csv` file is
    usually treated as a header line. So, each column (each word, or words, between
    two consecutive commas) in the first line should indicate the name of the column.
    This is very valuable information because without it, it would often be impossible
    to say what kind of data each of them represents. After the first line, we have
    data rows where each line represents one data point and each column represents
    values of those data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 5.01: Working with Headers When Reading Data from a CSV File'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, you will see how to read data from a `.csv` file. The file
    can be found here [https://packt.live/3fDMCNp](https://packt.live/3fDMCNp). This
    exercise acts as a demonstration of how to work with headers and what to do when
    the headers are missing. At times, you will encounter situations where headers
    are not present, and you may have to add proper headers or column names of your
    own. Let''s have a look at how this can be done:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a new Jupyter Notebook and read the example `.csv` file (with a header)
    using the following code and examine the resulting DataFrame, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Throughout this exercise, don't forget to change the path (highlighted) of the
    CSV file based on its location on your system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.1: Output of the example CSV file'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15780_05_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.1: Output of the example CSV file'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Read a `.csv` file with no header using a `pandas` DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read the `.csv` file by setting the `header` to `None`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the `names` argument to get the correct headers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, you will get a DataFrame that will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.4: CSV file with correct column header'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15780_05_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.4: CSV file with correct column header'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding figure, the headers have been added in the right
    places.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3hxmAgm](https://packt.live/3hxmAgm).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3eaToda](https://packt.live/3eaToda).
  prefs: []
  type: TYPE_NORMAL
- en: Up until now, we've been comfortable reading from files where a comma acts as
    a delimiter. Let's look at the following exercise, where we will be reading from
    a CSV file where the values are not separated by commas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 5.02: Reading from a CSV File Where Delimiters Are Not Commas'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is fairly common to encounter raw data files where the separator/delimiter
    is a character and not a comma. This exercise will demonstrate how you can read
    data from a file in such a case.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The file can be found here: [https://packt.live/2YPEJgO](https://packt.live/2YPEJgO).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a `.csv` file using `pandas` DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify the delimiter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.6: Semicolons removed from the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15780_05_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.6: Semicolons removed from the DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, it is fairly simple to read from a csv file when the delimiter
    is specified in the `read_csv` function.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2Na4oM0](https://packt.live/2Na4oM0).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3fvdm2g](https://packt.live/3fvdm2g).
  prefs: []
  type: TYPE_NORMAL
- en: In the following exercise, we will see how to bypass the headers if your CSV
    file already comes with headers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 5.03: Bypassing and Renaming the Headers of a CSV File'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This exercise will demonstrate how to bypass the headers of a CSV file and
    put in your own. To do that, you have to specifically set `header=0`. If you try
    to set the `names` variable to your `header` list, unexpected things can happen.
    Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add `names` to a `.csv` file that has headers, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Throughout this exercise, don't forget to change the path (highlighted) of the
    CSV file based on its location on your system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.7: A CSV file with headers overlapped'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15780_05_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.7: A CSV file with headers overlapped'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To avoid this, set `header` to zero and provide a `names` list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.8: A CSV file with defined headers'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15780_05_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.8: A CSV file with defined headers'
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that this representation is just in memory at the moment and only
    available in the present session of the notebook; it is not reflected in the physical
    CSV file. The original file has not changed due to our manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/30QwEeA](https://packt.live/30QwEeA).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/315gOgr](https://packt.live/315gOgr).
  prefs: []
  type: TYPE_NORMAL
- en: We observed some operations that we can do on the headers in a file. However,
    some CSV files may have an even more complex structure than the simple ones that
    we have been using so far. In the following exercise, we will discover some tricks
    to deal with such complex structures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 5.04: Skipping Initial Rows and Footers When Reading a CSV File'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will skip the first few rows because, most of the time,
    the first few rows of a CSV data file are metadata about the data source or similar
    information, which is not read into the table. Also, we will go ahead and remove
    the footer of the file, which might sometimes contain information that''s not
    very useful. Let''s see how we can do that using the example shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9: Contents of the CSV file'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15780_05_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.9: Contents of the CSV file'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The first two lines in the CSV file are irrelevant data. The file can be found
    here [https://packt.live/30SdvJh](https://packt.live/30SdvJh)
  prefs: []
  type: TYPE_NORMAL
- en: 'Read the CSV file and examine the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Throughout this exercise, don't forget to change the path (highlighted) of the
    CSV file based on its location on your system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.10: DataFrame with an unexpected error'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15780_05_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.10: DataFrame with an unexpected error'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Skip the first two rows and read the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `skipfooter` option in Python:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.13: DataFrame without a footer'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15780_05_013.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.13: DataFrame without a footer'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2CbehGO](https://packt.live/2CbehGO).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Ycw0Gp](https://packt.live/2Ycw0Gp).
  prefs: []
  type: TYPE_NORMAL
- en: We've now seen how to read values skipping the headers and footers from a file.
    It can very often be very handy while dealing with data collected from several
    different sources, especially in situations where a file contains unnecessary
    and junk information.
  prefs: []
  type: TYPE_NORMAL
- en: Reading Only the First N Rows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many situations, we may not want to read a whole data file but only the first
    few rows. This is particularly useful for extremely large data files, where we
    may just want to read the first couple of hundred rows to check an initial pattern
    and then decide to read the whole of the data afterward. Reading the entire file
    can take a long time and can slow down the entire data wrangling pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple option, called `nrows`, in the `read_csv` function, enables us to
    do just that. We will specify the number of rows we want to read and pass it as
    an argument to `nrows` like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The path (highlighted) would need to be changed based on where the file is saved
    on your system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14: DataFrame with the first few rows of the CSV file'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15780_05_014.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.14: DataFrame with the first few rows of the CSV file'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to be able to read only a selected number of rows is useful, specifically
    if you are dealing with large CSV files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 5.05: Combining skiprows and nrows to Read Data in Small Chunks'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This exercise will demonstrate how we can read from a very large data file.
    To do that, we can cleverly combine `skiprows` and `nrows` to read in a large
    file in small chunks of pre-determined sizes. We will read from the `Boston_housing.csv`
    file, which contains data about the pricing of houses in the Boston area in the
    US. It contains information such as per capita crime rate by town and the average
    number of rooms per dwelling. To do this, let''s go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Each exercise continues directly from the previous one. You do not need to open
    a new Jupyter Notebook each time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset can be found here: [https://packt.live/3fEIH2z](https://packt.live/3fEIH2z)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a list where DataFrames will be stored:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Store the number of rows to be read into a variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a variable to store the number of chunks to be read:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a dummy DataFrame to get the column names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Throughout this exercise, don't forget to change the path (highlighted) of the
    CSV file based on its location on your system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Loop over the CSV file to read only a fixed number of rows at a time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This particular step will not show any output as the values are getting appended
    to the list.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note how the `iterator` variable is set up inside the `range` function to break
    it into chunks. Say the number of chunks is `5` and the rows per chunk is `10`,
    then the iterator will have a range of `(0,5*10,10)`, where the final `10` is
    step-size, that is, it will iterate with indices of `(0,9,19,29,39,49)`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3fGmBwZ](https://packt.live/3fGmBwZ).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3hDbVAJ](https://packt.live/3hDbVAJ).
  prefs: []
  type: TYPE_NORMAL
- en: Setting the skip_blank_lines Option
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By default, `read_csv` ignores blank lines, which means if there are row entries
    with `NaN` values, the `read_csv` function will not read that data. However, in
    some situations, you may want to read them in as `NaN` so that you can count how
    many blank entries were present in the raw data file. In some situations, this
    is an indicator of the default data streaming quality and consistency. For this,
    you have to disable the `skip_blank_lines` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The path (highlighted) would need to be changed based on where the file is located
    on your system.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15: DataFrame of a .csv file that has blank rows'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15780_05_015.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.15: DataFrame of a .csv file that has blank rows'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to read CSV data from a zip file.
  prefs: []
  type: TYPE_NORMAL
- en: Reading CSV Data from a Zip File
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is an awesome feature of `pandas`, and it allows you to read directly from
    a compressed file, such as `.zip`, `.gz`, `.bz2`, or `.xz`. The only requirement
    is that the intended data file (`CSV`) should be the only file inside the compressed
    file. For example, we might need to compress a large csv file, and in that case,
    it will be the only file inside the `.zip` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we compressed the example CSV file with the `7-Zip` program
    and read from it directly using the `read_csv` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16: DataFrame of a compressed CSV file'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15780_05_016.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.16: DataFrame of a compressed CSV file'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will turn our attention to a Microsoft Excel file. It turns out that
    most of the options and methods we learned about in the previous exercises with
    the CSV file apply directly to reading Excel files too.
  prefs: []
  type: TYPE_NORMAL
- en: Reading from an Excel File Using sheet_name and Handling a Distinct sheet_name
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will focus on the differences between the methods of reading
    from an Excel file. An Excel file can consist of multiple worksheets, and we can
    read a specific sheet by passing in a particular argument, that is, `sheet_name`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the `Housing_data.xlsx` file, we have three worksheets. The
    following code reads them one by one into three separate DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: If the Excel file has multiple distinct worksheets but the `sheet_name` argument
    is set to `None`, then an ordered dictionary will be returned by the `read_excel`
    function. That ordered `dict` will have the data from all the worksheets, and
    the top-level keys will indicate the name of the worksheet. Thereafter, we can
    simply iterate over that dictionary or its keys to retrieve individual DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, we can access these individual worksheets using the distinct keys.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 5.06: Reading a General Delimited Text File'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will read from general delimited text files and see that
    this can be done as easily as reading from CSV files. However, we will have to
    use the right separator if it is anything other than a whitespace or a tab. To
    see this in action, let''s go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read the data from a `.txt` file using the `read_table` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the separator as a comma in the `sep` variable as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.18: A DataFrame read using a comma separator'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15780_05_018.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.18: A DataFrame read using a comma separator'
  prefs: []
  type: TYPE_NORMAL
- en: We can see in the figure that the data is read as expected from the `.txt` file.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/30UUdD8](https://packt.live/30UUdD8).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/37F57ho](https://packt.live/37F57ho).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen the various ways of reading data from `csv` files, in
    the next section, let's focus on reading data directly from a URL.
  prefs: []
  type: TYPE_NORMAL
- en: Reading HTML Tables Directly from a URL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `pandas` library allows us to read HTML tables directly from a URL. This
    means that the library already has some kind of built-in HTML parser that processes
    the HTML content of a given page and tries to extract various tables from the
    page.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `read_html` method from the `pandas` library returns a list of DataFrames
    (even if the page has a single DataFrame) and you have to extract the relevant
    tables from the list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'These results are shown in the following DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.19: Results of reading HTML tables'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15780_05_019.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.19: Results of reading HTML tables'
  prefs: []
  type: TYPE_NORMAL
- en: In the following exercise, we'll explore some more wrangling techniques to get
    the data in the desired format. As discussed in the preceding exercise, `read_html`,
    the HTML-reading function, almost always returns more than one table for a given
    HTML page, and we have to further parse through the list to extract the particular
    table we are interested in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 5.07: Further Wrangling to Get the Desired Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will work with the table of the 2016 Summer Olympics medal
    tally (by nation). We can easily search to get a page on Wikipedia containing
    this data that we can pass on to `pandas`. We will apply a few wrangling techniques
    on this data to get the output. To do so, let''s go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `read_html` command to read from the Wikipedia page containing Summer
    Olympics records from 2016:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the length of the list returned. We will see that it is `7`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To look for the particular table, run a simple loop. We are using the `shape`
    property of a DataFrame to examine the number of rows and the number of columns
    of each of them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It looks like the second element in this list is the table we are looking for.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Extract the second element from the table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.20: Output of the data in the second table'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15780_05_020.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.20: Output of the data in the second table'
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe from the preceding table, data containing the records of the
    Wikipedia page has been read in a table format.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3de6LYw](https://packt.live/3de6LYw).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Bk9e6q](https://packt.live/2Bk9e6q).
  prefs: []
  type: TYPE_NORMAL
- en: Reading from a JSON file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over the last 15 years, JSON has become ubiquitous for data exchange on the
    web. Today, it is the format of choice for almost every publicly available web
    API, and it is frequently used for private web APIs as well. It is a schema-less,
    text-based representation of structured data that is based on key-value pairs
    and ordered lists. The `pandas` library provides excellent support for reading
    data from a JSON file directly into a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 5.08: Reading from a JSON File'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will read data from the `movies.json` file. This file
    contains the cast, genre, title, and year (of release) information for almost
    all major movies since `1900`. Let''s go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `.json` file could be found at [https://packt.live/3d7DO0l](https://packt.live/3d7DO0l).
  prefs: []
  type: TYPE_NORMAL
- en: Extract the list of movies from the file into a DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Don't forget to change the path (highlighted) of the JSON file based on its
    location on your system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.21: DataFrame displaying the movie list'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15780_05_021.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.21: DataFrame displaying the movie list'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To look for the cast where the title is `Avengers`, use filtering:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/37ISQJ8](https://packt.live/37ISQJ8).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2YeymVv](https://packt.live/2YeymVv).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Reading a PDF File
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Among the various types of data sources, the PDF format is probably the most
    difficult to parse in general. While there are some popular packages in Python
    for working with PDF files for general page formatting, the best library to use
    for table extraction from PDF files is `tabula-py`.
  prefs: []
  type: TYPE_NORMAL
- en: From the GitHub page of this package, `tabula-py` is a simple Python wrapper
    of `tabula-java`, which can read a table from a PDF. You can read tables from
    PDFs and convert them into `pandas` DataFrames. The `tabula-py` library also enables
    you to convert a PDF file into a CSV/TSV/JSON file.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you've installed `tabula` based on the instructions detailed in the
    section titled *Libraries to Install for This Chapter*.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will also need the following packages installed on your system before you
    can run this, but they are free and easy to install; you can use `pip install`
    to install them from the notebook session as you did in the past:'
  prefs: []
  type: TYPE_NORMAL
- en: '`urllib3`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytest`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flake8`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`distro`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pathlib`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise 5.09: Reading Tabular Data from a PDF File'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will first read from two different pages of a PDF file
    from [https://packt.live/2Ygj4j7](https://packt.live/2Ygj4j7) in tabular format,
    and then we will perform a few simple operations to handle the headers of these
    files.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Before proceeding, make sure you've installed `tabula` based on the instructions
    detailed in an earlier section titled *Libraries to Install for This Chapter*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go through the following steps to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code retrieves the tables from two pages and joins them to make
    one table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Throughout this exercise, don't forget to change the path (highlighted) of the
    PDF based on its location on your system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.22: DataFrame with a table derived by merging a table flowing over'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: two pages in a PDF
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15780_05_022.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.22: DataFrame with a table derived by merging a table flowing over
    two pages in a PDF'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Retrieve the table from another page of the same PDF by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.23: DataFrame displaying a table from another page'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15780_05_023.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.23: DataFrame displaying a table from another page'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To concatenate the tables that were derived from the first two steps, execute
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.24: DataFrame derived by concatenating two tables'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15780_05_024.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.24: DataFrame derived by concatenating two tables'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With PDF extraction, most of the time, headers will be difficult to extract automatically.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Pass on the list of headers with the `names` argument in the `read-pdf` function
    set to `pandas_option`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.25: DataFrame with the correct column headers for PDF data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15780_05_025.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.25: DataFrame with the correct column headers for PDF data'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2YcHz0v](https://packt.live/2YcHz0v).
  prefs: []
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example, and will
    need to be run locally.
  prefs: []
  type: TYPE_NORMAL
- en: We will have a full activity on reading tables from a PDF report and processing
    them at the end of this chapter. Let's dive into web page scraping and the library
    used to do that, Beautiful Soup 4\.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Beautiful Soup 4 and Web Page Parsing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ability to read and understand web pages is of paramount interest to a person
    collecting and formatting data. For example, consider the task of gathering data
    about movies and then formatting it for a downstream system. Data from movie databases
    is best obtained from websites such as IMDb, and that data does not come pre-packaged
    in nice forms (such as CSV or JSON), so you need to know how to download and read
    a web page.
  prefs: []
  type: TYPE_NORMAL
- en: You also need to be equipped with the knowledge of the structure of a web page
    so that you can design a system that can search for (query) a particular piece
    of information from a whole web page and get the value from it. This involves
    understanding the grammar of markup languages and being able to write something
    that can parse them. Doing this, and keeping all the edge cases in mind, for something
    like HTML is already incredibly complex, and if you extend the scope of the bespoke
    markup language to include XML as well, then it becomes full-time work for a team
    of people.
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, we are using Python, and Python has a very mature and stable library
    that does all the complicated tasks for us. This library is called `BeautifulSoup`
    (it is, at present, in version 4, and thus we will call it `bs4` for short from
    now on). `bs4` is a library for getting data from HTML or XML documents, and it
    gives you a nice, normalized, idiomatic way of navigating and querying a document.
    It does not include a parser but it supports different ones.
  prefs: []
  type: TYPE_NORMAL
- en: Structure of HTML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we jump into `bs4` and start working with it, we need to examine the
    structure of an HTML document. **H**yper **T**ext **M**arkup **L**anguage is a
    structured way of telling web browsers about the organization of a web page, meaning
    which kinds of elements (text, image, video, and so on) come from where, where
    inside the page they should appear, what they look like, what they contain, and
    how they will behave with user input. HTML5 is the latest version of HTML. An
    HTML document can be viewed as a tree, as we can see in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.26: HTML structure'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15780_05_026.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.26: HTML structure'
  prefs: []
  type: TYPE_NORMAL
- en: Each node of the tree represents one element in the document. An element is
    anything that starts with `<` and ends with `>`. For example, `<html>`, `<head>`,
    `<p>`, `<br>`, `<img>`, and so on are various HTML elements. Some elements have
    a start and end element, where the end element begins with `</` and has the same
    name as the start element, such as `<p>` and `</p>`, and they can contain an arbitrary
    number of elements of other types in them. Some elements do not have an ending
    part, such as the `<br/>` element, and they cannot contain anything within them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only other thing that we need to know about an element at this point is
    the fact that elements can have attributes, which are there to modify the default
    behavior of an element. For example, an `<a>` anchor element requires a `href`
    attribute to tell the browser which website it should navigate to when that particular
    `<a>` is clicked, like this: `<a href="http://cnn.com">`. `The CNN news channel`,
    `</a>`, will take you to [cnn.com](http://cnn.com) when clicked:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.27: CNN news channel hyperlink'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15780_05_027.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.27: CNN news channel hyperlink'
  prefs: []
  type: TYPE_NORMAL
- en: So, when you are at a particular element of the tree, you can visit all the
    children of that element to get their contents and attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Equipped with this knowledge, let's see how we can read and query data from
    an HTML document.
  prefs: []
  type: TYPE_NORMAL
- en: In this topic, we will cover the reading and parsing of web pages, but we do
    not request them from a live website. Instead, we read them from disk. A section
    on reading them from the internet will follow in a future chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 5.10: Reading an HTML File and Extracting Its Contents Using Beautiful
    Soup'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will do the simplest thing possible. We will import the
    `Beautiful Soup` or `bs4` library and then use it to read an HTML document. Then,
    we will examine the different kinds of objects it returns. While doing the exercises
    for this topic, you should have the example HTML file (called `test.html`) open
    in a text editor so that you can check for the different tags and their attributes
    and contents:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `bs4` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Please download the following test HTML file and save it on your disk, and
    then use `bs4` to read it from the disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the contents of the file in a nice way, by which we mean that the printing
    will keep some kind of nice indentation by using the `prettify` method from the
    class, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.28: Contents of the HTML file'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15780_05_028.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.28: Contents of the HTML file'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The same information can also be obtained by using the `soup.contents` member
    variable. The differences are: first, it won''t print anything pretty and, second,
    it is essentially a list.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If we look carefully at the contents of the HTML file in a separate text editor,
    we will see that there are many paragraph tags, or `<p>` tags. Let's read content
    from one such `<p>` tag. We can do that using the simple `.` access modifier as
    we would have done for a normal member variable of a class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The magic of `bs4` is the fact that it gives us this excellent way to dereference
    tags as member variables of the `BeautifulSoup` class instance. In the following
    few steps, we are going to read an HTML file and then pass the file handler returned
    by Python's `open` call directly to the constructor of the `BeautifulSoup` class.
    It does a lot of things (including reading the content and then parsing it) and
    returns an instance of the class that we can then use.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Read the HTML file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `findall` method to extract the content from the tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we will see how to get the contents of a particular HTML tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first way is by using the `children` generator from any `bs4` instance,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To do that, we use the `descendants` generator from the `bs4` instance, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The comparison print at the end of the code block will show us the difference
    between `children` and `descendants`. The length of the list we got from `children`
    is only `9`, whereas the length of the list we got from `descendants` is `61`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2N994l6](https://packt.live/2N994l6).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2UT2p2K](https://packt.live/2UT2p2K).
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have seen some basic ways to navigate the tags inside an HTML document
    using `bs4`. Now, we are going to go one step further and use the power of `bs4`
    combined with the power of `pandas` to generate a DataFrame out of a plain HTML
    table. With the knowledge we will acquire now, it will be fairly easy for us to
    prepare a `pandas` DataFrame to perform `BeautifulSoup` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 5.11: DataFrames and BeautifulSoup'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will extract the data from the `test.html` page using
    the `BeautifulSoup` library. We will then perform a few operations for data preparation
    and display the data in an easily readable tabular format. To do that, let''s
    go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pandas` and read the document, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the original table structure in the HTML source. You will see that the
    first row is the column heading and all of the following rows are the data from
    the HTML source. We''ll assign two different variables for the two sections, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Keep in mind that the art of scraping an HTML page goes hand in hand with an
    understanding of the source HTML structure. So, whenever you want to scrape a
    page, the first thing you need to do is right-click on it and then use `View Source`
    from the browser to see the source HTML.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once we have separated the two sections, we need two list comprehensions to
    make them ready to go in a DataFrame. For the header, this is easy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Data preparation is a bit tricky for a `pandas` DataFrame. You need to have
    a two-dimensional list, which is a list of lists. We accomplish that in the following
    way, using the tricks we learned earlier about list comprehension.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the `for…in` loop to iterate over the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.32: Output as a two-dimensional list'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15780_05_032.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.32: Output as a two-dimensional list'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Invoke the `pd.DataFrame` method and supply the right arguments by using the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.33: Output in tabular format with column headers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15780_05_033.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.33: Output in tabular format with column headers'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we conclude our exercise on creating a data frame from an HTML table.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/30QyE6A](https://packt.live/30QyE6A).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3hBPFY5](https://packt.live/3hBPFY5).
  prefs: []
  type: TYPE_NORMAL
- en: In the following exercise, we'll export a DataFrame as an Excel file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 5.12: Exporting a DataFrame as an Excel File'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will see how we can save a DataFrame as an Excel file.
    `Pandas` can do this natively, but it needs the help of the `openpyxl` library
    to achieve this goal. `openpyxl` is a Python library for reading/writing Excel
    2010 `xlsx/xlsm/xltx/xltm` files.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This exercise is continued from the previous exercise. You'll need to continue
    in the same Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the `openpyxl` library by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To save the DataFrame as an Excel file, use the following command from inside
    of the Jupyter notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is the way in which we can export a `pandas` DataFrame to Excel. Given
    that Excel is a very popular format among many types of users, this is a very
    important trick you need to master.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2YcSdV6](https://packt.live/2YcSdV6).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2YZTXjJ](https://packt.live/2YZTXjJ).
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, when we were discussing the stack, we explained how
    important it is to have a stack that we can push the URLs from a web page to so
    that we can pop them at a later time to follow each of them. The following exercise
    will demonstrate how to do that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 5.13: Stacking URLs from a Document Using bs4'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will append the URLs one after the other from the `test.html`
    web page. In that file, HTML file links or `<a>` tags are under a `<ul>` tag,
    and each of them is contained inside a `</li>` tag. We are going to find all the
    `<a>` tags and create a stack with them.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This exercise is continued from the previous exercise. You'll need to continue
    in the same Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, let''s go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Find all the `<a>` tags by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Don't forget to change the path (highlighted) of the HTML file based on its
    location on your system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define a stack before you start the loop. Then, inside the loop, use the `append`
    method to push the links in the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3hCCAOj](https://packt.live/3hCCAOj).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3fCYNd0](https://packt.live/3fCYNd0).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's put together everything we have learned so far in this chapter and get
    started with an activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 5.01: Reading Tabular Data from a Web Page and Creating DataFrames'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this activity, you have been given a Wikipedia page where you have the GDP
    of all countries listed. You have to create three `DataFrames` from the three
    sources mentioned on the page ([https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal](https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal))).
  prefs: []
  type: TYPE_NORMAL
- en: 'You will have to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the page in a separate Chrome/Firefox tab and use something like an `Inspect
    Element` tool to view the source HTML and understand its structure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the page using `bs4`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the table structure you will need to deal with (how many tables are there?).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the right table using `bs4`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Separate the source names and their corresponding data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the source names from the list of sources you have created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Separate the header and data from the data that you separated before for the
    first source only, and then create a DataFrame using that.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the last task for the other two data sources.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.34: Final output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15780_05_034.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.34: Final output'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution for this activity can be found via [this link](B15780_Solution_Final_RK.xhtml#_idTextAnchor317).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have looked into several different types of data formats
    and how to work with them. These formats include CSV, PDF, Excel, Plain Text,
    and HTML. HTML documents are the cornerstone of the World Wide Web and, given
    the amount of data that's contained in it, we can easily infer the importance
    of HTML as a data source.
  prefs: []
  type: TYPE_NORMAL
- en: We learned about `bs4` (`BeautifulSoup 4`), a Python library that gives us Pythonic
    ways to read and query HTML documents. We used bs4 to load an HTML document and
    explored several different ways to navigate the loaded document.
  prefs: []
  type: TYPE_NORMAL
- en: We also looked at how we can create a `pandas` DataFrame from an HTML document
    (which contains a table). Although there are some built-in ways to do this job
    in `pandas`, they fail as soon as the target table is encoded inside a complex
    hierarchy of elements. So, the knowledge we gathered in this topic to transform
    an HTML table into a `pandas` DataFrame in a step-by-step manner is invaluable.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we looked at how we can create a stack in our code, where we push all
    the URLs that we encounter while reading the HTML file and then use them at a
    later time. In the next chapter, we will discuss list comprehensions, the `.zip`
    format, and outlier detection and cleaning.
  prefs: []
  type: TYPE_NORMAL
