- en: Chapter 3. Juggling Data with Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章 使用Spark处理数据
- en: As per the batch and streaming architecture laid out in the previous chapter,
    we need data to fuel our applications. We will harvest data focused on Apache
    Spark from Twitter. The objective of this chapter is to prepare data to be further
    used by the machine learning and streaming applications. This chapter focuses
    on how to exchange code and data across the distributed network. We will get practical
    insights into serialization, persistence, marshaling, and caching. We will get
    to grips with on Spark SQL, the key Spark module to interactively explore structured
    and semi-structured data. The fundamental data structure powering Spark SQL is
    the Spark dataframe. The Spark dataframe is inspired by the Python Pandas dataframe
    and the R dataframe. It is a powerful data structure, well understood and appreciated
    by data scientists with a background in R or Python.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上一章中概述的批处理和流式架构，我们需要数据来为我们的应用程序提供动力。我们将从Twitter收集专注于Apache Spark的数据。本章的目标是为机器学习和流式应用程序准备数据。本章重点介绍如何在分布式网络中交换代码和数据。我们将获得关于序列化、持久化、打包和缓存的实用见解。我们将掌握Spark
    SQL，这是Spark中用于交互式探索结构化和半结构化数据的关键模块。驱动Spark SQL的基本数据结构是Spark数据框。Spark数据框受到Python
    Pandas数据框和R数据框的启发。它是一个强大的数据结构，被具有R或Python背景的数据科学家广泛理解和欣赏。
- en: 'In this chapter, we will cover the following points:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Connect to Twitter, collect the relevant data, and then persist it in various
    formats such as JSON and CSV and data stores such as MongoDB
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接到Twitter，收集相关数据，并将其以JSON和CSV等格式以及MongoDB等数据存储方式持久化
- en: Analyze the data using Blaze and Odo, a spin-off library from Blaze, in order
    to connect and transfer data from various sources and destinations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Blaze和Odo（Blaze的一个衍生库）分析数据，以便从各种来源和目的地连接和传输数据
- en: Introduce Spark dataframes as the foundation for data interchange between the
    various Spark modules and explore data interactively using Spark SQL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Spark数据框作为Spark模块之间数据交换的基础，并使用Spark SQL交互式地探索数据
- en: Revisiting the data-intensive app architecture
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新审视数据密集型应用架构
- en: Let's first put in context the focus of this chapter with respect to the data-intensive
    app architecture. We will concentrate our attention on the integration layer and
    essentially run through iterative cycles of the acquisition, refinement, and persistence
    of the data. This cycle was termed the five Cs. The five Cs stand for *connect*,
    *collect*, *correct*, *compose*, and *consume*. They are the essential processes
    we run through in the integration layer in order to get to the right quality and
    quantity of data retrieved from Twitter. We will also delve deeper in the persistence
    layer and set up a data store such as MongoDB to collect our data for processing
    later.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先从数据密集型应用架构的角度来界定本章的重点。我们将集中注意力于集成层，并基本上运行数据获取、精炼和持久化的迭代周期。这个周期被称为五个C，即*连接*、*收集*、*纠正*、*组合*和*消费*。它们是我们为了从Twitter获取正确质量和数量的数据而在集成层运行的基本过程。我们还将深入了解持久化层，并设置一个如MongoDB这样的数据存储来收集我们的数据以便后续处理。
- en: We will explore the data with Blaze, a Python library for data manipulation,
    and Spark SQL, the interactive module of Spark for data discovery powered by the
    Spark dataframe. The dataframe paradigm is shared by Python Pandas, Python Blaze,
    and Spark SQL. We will get a feel for the nuances of the three dataframe flavors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Blaze（一个用于数据操作的Python库）和Spark SQL（Spark的交互式数据发现模块，由Spark数据框驱动）探索数据。数据框范式由Python
    Pandas、Python Blaze和Spark SQL共享。我们将了解三种数据框风味的细微差别。
- en: 'The following diagram sets the context of the chapter''s focus, highlighting
    the integration layer and the persistence layer:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表设置了本章重点的上下文，突出了集成层和持久化层：
- en: '![Revisiting the data-intensive app architecture](img/B03986_03_01.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![重新审视数据密集型应用架构](img/B03986_03_01.jpg)'
- en: Serializing and deserializing data
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列化和反序列化数据
- en: As we are harvesting data from web APIs under rate limit constraints, we need
    to store them. As the data is processed on a distributed cluster, we need consistent
    ways to save state and retrieve it for later usage.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在速率限制约束下从Web API收集数据，我们需要存储它们。由于数据在分布式集群上处理，我们需要一致的方式来保存状态并在以后使用时检索它。
- en: Let's now define serialization, persistence, marshaling, and caching or memorization.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义序列化、持久化、打包和缓存或记忆化。
- en: Serializing a Python object converts it into a stream of bytes. The Python object
    needs to be retrieved beyond the scope of its existence, when the program is shut.
    The serialized Python object can be transferred over a network or stored in a
    persistent storage. Deserialization is the opposite and converts the stream of
    bytes into the original Python object so the program can carry on from the saved
    state. The most popular serialization library in Python is Pickle. As a matter
    of fact, the PySpark commands are transferred over the wire to the worker nodes
    via pickled data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 序列化Python对象将其转换为字节流。Python对象需要在程序关闭时从其存在范围之外检索。序列化的Python对象可以通过网络传输或存储在持久化存储中。反序列化是相反的过程，它将字节流转换为原始Python对象，以便程序可以从保存的状态继续执行。Python中最流行的序列化库是Pickle。实际上，PySpark命令是通过序列化数据通过网络传输到工作节点的。
- en: Persistence saves a program's state data to disk or memory so that it can carry
    on where it left off upon restart. It saves a Python object from memory to a file
    or a database and loads it later with the same state.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 持久化将程序的状态数据保存到磁盘或内存中，以便在重启时从上次停止的地方继续。它将Python对象从内存保存到文件或数据库，并在稍后以相同的状态加载它。
- en: Marshalling sends Python code or data over a network TCP connection in a multicore
    or distributed system.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Marshalling在多核或分布式系统中通过TCP连接在网络中发送Python代码或数据。
- en: Caching converts a Python object to a string in memory so that it can be used
    as a dictionary key later on. Spark supports pulling a dataset into a cluster-wide,
    in-memory cache. This is very useful when data is accessed repeatedly such as
    when querying a small reference dataset or running an iterative algorithm such
    as Google PageRank.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存将Python对象转换为内存中的字符串，以便以后用作字典键。Spark支持将数据集拉入集群范围内的内存缓存。当数据被重复访问时，例如查询小型参考数据集或运行迭代算法（如Google
    PageRank）时，这非常有用。
- en: Caching is a crucial concept for Spark as it allows us to save RDDs in memory
    or with a spillage to disk. The caching strategy can be selected based on the
    lineage of the data or the **DAG** (short for **Directed Acyclic Graph**) of transformations
    applied to the RDDs in order to minimize shuffle or cross network heavy data exchange.
    In order to achieve good performance with Spark, beware of data shuffling. A good
    partitioning policy and use of RDD caching, coupled with avoiding unnecessary
    action operations, leads to better performance with Spark.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存对于Spark来说是一个关键概念，因为它允许我们将RDDs存储在内存中或通过溢出到磁盘。可以根据数据的血缘关系或应用于RDDs的**DAG**（代表**有向无环图**）来选择缓存策略，以最小化洗牌或跨网络重数据交换。为了在Spark中获得良好的性能，请注意数据洗牌。良好的分区策略和RDD缓存的合理使用，以及避免不必要的行动操作，可以带来更好的Spark性能。
- en: Harvesting and storing data
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集和存储数据
- en: 'Before delving into database persistent storage such as MongoDB, we will look
    at some useful file storages that are widely used: **CSV** (short for **comma-separated
    values**) and **JSON** (short for **JavaScript Object Notation**) file storage.
    The enduring popularity of these two file formats lies in a few key reasons: they
    are human readable, simple, relatively lightweight, and easy to use.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究数据库持久化存储，如MongoDB之前，我们将探讨一些广泛使用的有用文件存储：**CSV**（代表**逗号分隔值**）和**JSON**（代表**JavaScript对象表示法**）文件存储。这两种文件格式的持久流行有几个关键原因：它们是可读的，简单，相对轻量级，且易于使用。
- en: Persisting data in CSV
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在CSV中持久化数据
- en: The CSV format is lightweight, human readable, and easy to use. It has delimited
    text columns with an inherent tabular schema.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: CSV格式轻量级，可读，易于使用。它具有带固有表格模式的分隔文本列。
- en: Python offers a robust `csv` library that can serialize a `csv` file into a
    Python dictionary. For the purpose of our program, we have written a `python`
    class that manages to persist data in CSV format and read from a given CSV.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Python提供了一个强大的`csv`库，可以将`csv`文件序列化为Python字典。为了我们程序的目的，我们已经编写了一个`python`类，该类能够以CSV格式持久化数据并从给定的CSV中读取。
- en: 'Let''s run through the code of the class `IO_csv` object. The `__init__` section
    of the class basically instantiates the file path, the filename, and the file
    suffix (in this case, `.csv`):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行`IO_csv`对象类的代码。类的`__init__`部分基本上实例化了文件路径、文件名和文件后缀（在这种情况下，为`.csv`）：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `save` method of the class uses a Python named tuple and the header fields
    of the `csv` file in order to impart a schema while persisting the rows of the
    CSV. If the `csv` file already exists, it will be appended and not overwritten
    otherwise; it will be created:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 类的`save`方法使用Python命名元组和`csv`文件的标题字段来在持久化CSV行的同时赋予一个模式。如果`csv`文件已经存在，它将被追加而不是覆盖；否则，它将被创建：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `load` method of the class also uses a Python named tuple and the header
    fields of the `csv` file in order to retrieve the data using a consistent schema.
    The `load` method is a memory-efficient generator to avoid loading a huge file
    in memory: hence we use `yield` in place of `return`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 类的`load`方法也使用Python命名元组和`csv`文件的标题字段来使用一致的架构检索数据。`load`方法是一个内存高效的生成器，以避免在内存中加载一个巨大的文件：因此我们使用`yield`代替`return`：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here''s the named tuple. We are using it to parse the tweet in order to save
    or retrieve them to and from the `csv` file:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是命名元组。我们使用它来解析推文，以便将其保存或从`csv`文件中检索：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Persisting data in JSON
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在JSON中持久化数据
- en: JSON is one of the most popular data formats for Internet-based applications.
    All the APIs we are dealing with, Twitter, GitHub, and Meetup, deliver their data
    in JSON format. The JSON format is relatively lightweight compared to XML and
    human readable, and the schema is embedded in JSON. As opposed to the CSV format,
    where all records follow exactly the same tabular structure, JSON records can
    vary in their structure. JSON is semi-structured. A JSON record can be mapped
    into a Python dictionary of dictionaries.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: JSON是互联网应用程序中最流行的数据格式之一。我们处理的所有API，包括Twitter、GitHub和Meetup，都以JSON格式提供数据。与相对较重的XML相比，JSON格式相对轻量级，且易于阅读，其模式嵌入在JSON中。与所有记录都遵循完全相同的表格结构的CSV格式不同，JSON记录的结构可以不同。JSON是半结构化的。一个JSON记录可以被映射到一个Python字典的字典。
- en: 'Let''s run through the code of the class `IO_json` object. The `__init__` section
    of the class basically instantiates the file path, the filename, and the file
    suffix (in this case, `.json`):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们遍历`IO_json`类对象的代码。类的`__init__`部分基本上实例化了文件路径、文件名和文件后缀（在这种情况下，是`.json`）：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `save` method of the class uses `utf-8` encoding in order to ensure read
    and write compatibility of the data. If the JSON file already exists, it will
    be appended and not overwritten; otherwise it will be created:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 类的`save`方法使用`utf-8`编码以确保数据的读写兼容性。如果JSON文件已经存在，它将被追加而不是覆盖；否则，它将被创建：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `load` method of the class just returns the file that has been read. A
    further `json.loads` function needs to be applied in order to retrieve the `json`
    out of the file read:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 类的`load`方法仅返回已读取的文件。需要应用进一步的`json.loads`函数来从读取的文件中检索`json`：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Setting up MongoDB
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置MongoDB
- en: It is crucial to store the information harvested. Thus, we set up MongoDB as
    our main document data store. As all the information collected is in JSON format
    and MongoDB stores information in **BSON** (short for **Binary JSON**), it is
    therefore a natural choice.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 存储收集到的信息至关重要。因此，我们将MongoDB设置为我们的主要文档数据存储。由于所有收集到的信息都是JSON格式，而MongoDB以**BSON**（即**Binary
    JSON**）格式存储信息，因此这是一个自然的选择。
- en: 'We will run through the following steps now:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将执行以下步骤：
- en: Installing the MongoDB server and client
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装MongoDB服务器和客户端
- en: Running the MongoDB server
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行MongoDB服务器
- en: Running the Mongo client
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行Mongo客户端
- en: Installing the PyMongo driver
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装PyMongo驱动程序
- en: Creating the Python Mongo client
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建Python Mongo客户端
- en: Installing the MongoDB server and client
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装MongoDB服务器和客户端
- en: 'In order to install the MongoDB package, perform through the following steps:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安装MongoDB软件包，执行以下步骤：
- en: 'Import the public key used by the package management system (in our case, Ubuntu''s
    `apt`). To import the MongoDB public key, we issue the following command:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入包管理系统中使用的公钥（在我们的例子中，是Ubuntu的`apt`）。为了导入MongoDB的公钥，我们执行以下命令：
- en: '[PRE7]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Create a list file for MongoDB. To create the list file, we use the following
    command:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为MongoDB创建一个列表文件。要创建列表文件，我们使用以下命令：
- en: '[PRE8]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Update the local package database as `sudo`:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`sudo`更新本地软件包数据库：
- en: '[PRE9]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Install the MongoDB packages. We install the latest stable version of MongoDB
    with the following command:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装MongoDB软件包。我们使用以下命令安装MongoDB的最新稳定版本：
- en: '[PRE10]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Running the MongoDB server
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行MongoDB服务器
- en: 'Let''s start the MongoDB server:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们启动MongoDB服务器：
- en: 'To start MongoDB server, we issue the following command to start `mongod`:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要启动MongoDB服务器，我们执行以下命令以启动`mongod`：
- en: '[PRE11]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To check whether `mongod` has started properly, we issue the command:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了检查 `mongod` 是否已正确启动，我们发出以下命令：
- en: '[PRE12]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this case, we see that `mongodb` is running in process `967`.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，我们看到 `mongodb` 正在进程 `967` 中运行。
- en: The `mongod` server sends a message to the effect that it is waiting for connection
    on `port 27017`. This is the default port for MongoDB. It can be changed in the
    configuration file.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`mongod` 服务器发送一条消息，表明它正在等待连接到 `port 27017`。这是 MongoDB 的默认端口。它可以在配置文件中更改。'
- en: 'We can check the contents of the log file at `/var/log/mongod/mongod.log`:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以检查 `/var/log/mongod/mongod.log` 中的日志文件内容：
- en: '[PRE13]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In order to stop the `mongodb` server, just issue the following command:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了停止 `mongodb` 服务器，只需发出以下命令：
- en: '[PRE14]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Running the Mongo client
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行 Mongo 客户端
- en: 'Running the Mongo client in the console is as easy as calling `mongo`, as highlighted
    in the following command:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台中运行 Mongo 客户端就像调用 `mongo` 一样简单，如下所示：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'At the mongo client console prompt, we can see the databases with the following
    commands:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在 mongo 客户端控制台提示符下，我们可以使用以下命令查看数据库：
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We select the test database using `use test`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `use test` 选择测试数据库：
- en: '[PRE17]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We display the collections within the test database:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们显示测试数据库中的集合：
- en: '[PRE18]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We check a sample record in the restaurant collection listed previously:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查之前列出的餐厅集合中的一个样本记录：
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Installing the PyMongo driver
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 PyMongo 驱动程序
- en: 'Installing the Python driver with anaconda is easy. Just run the following
    command at the terminal:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 anaconda 安装 Python 驱动程序非常简单。只需在终端运行以下命令：
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Creating the Python client for MongoDB
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 MongoDB 的 Python 客户端
- en: 'We are creating a `IO_mongo` class that will be used in our harvesting and
    processing programs to store the data collected and retrieved saved information.
    In order to create the `mongo` client, we will import the `MongoClient` module
    from `pymongo`. We connect to the `mongodb` server on localhost at port 27017\.
    The command is as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在创建一个 `IO_mongo` 类，它将在我们的收集和处理程序中使用，以存储收集和检索保存的信息。为了创建 `mongo` 客户端，我们将从 `pymongo`
    中导入 `MongoClient` 模块。我们连接到本地主机上的 `mongodb` 服务器，端口为 27017。命令如下：
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We initialize our class with the client connection, the database (in this case,
    `twtr_db`), and the collection (in this case, `twtr_coll`) to be accessed:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用客户端连接、数据库（在这种情况下为 `twtr_db`）和要访问的集合（在这种情况下为 `twtr_coll`）来初始化我们的类：
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `save` method inserts new records in the preinitialized collection and
    database:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`save` 方法将新记录插入到预先初始化的集合和数据库中：'
- en: '[PRE23]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The `load` method allows the retrieval of specific records according to criteria
    and projection. In the case of large amount of data, it returns a cursor:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`load` 方法允许根据标准和投影检索特定记录。在大量数据的情况下，它返回一个游标：'
- en: '[PRE24]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Harvesting data from Twitter
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 Twitter 收集数据
- en: Each social network poses its limitations and challenges. One of the main obstacles
    for harvesting data is an imposed rate limit. While running repeated or long-running
    connections between rates limit pauses, we have to be careful to avoid collecting
    duplicate data.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 每个社交网络都存在其局限性和挑战。在收集数据时，一个主要的障碍是施加的速率限制。在运行重复或长时间运行的连接时，我们必须要小心，以避免收集重复数据。
- en: We have redesigned our connection programs outlined in the previous chapter
    to take care of the rate limits.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经重新设计了上一章中概述的连接程序，以处理速率限制。
- en: 'In this `TwitterAPI` class that connects and collects the tweets according
    to the search query we specify, we have added the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 `TwitterAPI` 类中，我们连接并收集我们指定的搜索查询的推文，我们添加了以下内容：
- en: Logging capability using the Python logging library with the aim of collecting
    any errors or warning in the case of program failure
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Python 日志库的日志功能，以收集程序失败时的任何错误或警告
- en: Persistence capability using MongoDB, with the `IO_mongo` class exposed previously
    as well as JSON file using the `IO_json` class
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 MongoDB 的持久性功能，包括之前公开的 `IO_mongo` 类以及使用 `IO_json` 类的 JSON 文件
- en: API rate limit and error management capability, so we can ensure more resilient
    calls to Twitter without getting barred for tapping into the firehose
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API 速率限制和错误管理功能，这样我们就可以确保对 Twitter 的调用更加健壮，而不会因为访问数据流而被禁止。
- en: 'Let''s go through the steps:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步来：
- en: 'We initialize by instantiating the Twitter API with our credentials:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过实例化 Twitter API 并提供我们的凭据来初始化：
- en: '[PRE25]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We initialize the logger by providing the log level:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过提供日志级别来初始化日志记录器：
- en: '`logger.debug`(debug message)'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logger.debug`(调试信息)'
- en: '`logger.info`(info message)'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logger.info`(信息消息)'
- en: '`logger.warn`(warn message)'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logger.warn`(警告消息)'
- en: '`logger.error`(error message)'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logger.error`(错误消息)'
- en: '`logger.critical`(critical message)'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logger.critical`(关键信息)'
- en: 'We set the log path and the message format:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置日志路径和消息格式：
- en: '[PRE26]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We initialize the JSON file persistence instruction:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们初始化 JSON 文件持久化指令：
- en: '[PRE27]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We initialize the MongoDB database and collection for persistence:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们初始化 MongoDB 数据库和集合以实现持久化：
- en: '[PRE28]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The method `searchTwitter` launches the search according to the query specified:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方法 `searchTwitter` 根据指定的查询启动搜索：
- en: '[PRE29]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `saveTweets` method actually saves the collected tweets in JSON and in
    MongoDB:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`saveTweets` 方法实际上将收集到的推文保存为 JSON 和 MongoDB：'
- en: '[PRE30]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `parseTweets` method allows us to extract the key tweet information from
    the vast amount of information provided by the Twitter API:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`parseTweets` 方法允许我们从 Twitter API 提供的大量信息中提取关键推文信息：'
- en: '[PRE31]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `getTweets` method calls the `searchTwitter` method described previously.
    The `getTweets` method ensures that API calls are made reliably whilst respecting
    the imposed rate limit. The code is as follows:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`getTweets` 方法调用前面描述的 `searchTwitter` 方法。`getTweets` 方法确保 API 调用可靠，同时尊重施加的速率限制。代码如下：'
- en: '[PRE32]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here, we are calling the `searchTwitter` API with the relevant query based
    on the parameters specified. If we encounter any error such as rate limitation
    from the provider, this will be processed by the `handleError` method:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们根据指定的参数调用 `searchTwitter` API 的相关查询。如果遇到任何错误，例如来自提供者的速率限制，这将由 `handleError`
    方法处理：
- en: '[PRE33]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Exploring data using Blaze
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Blaze 探索数据
- en: Blaze is an open source Python library, primarily developed by Continuum.io,
    leveraging Python Numpy arrays and Pandas dataframe. Blaze extends to out-of-core
    computing, while Pandas and Numpy are single-core.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Blaze 是一个开源的 Python 库，主要由 Continuum.io 开发，利用 Python Numpy 数组和 Pandas dataframe。Blaze
    扩展到离核计算，而 Pandas 和 Numpy 是单核。
- en: 'Blaze offers an adaptable, unified, and consistent user interface across various
    backends. Blaze orchestrates the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Blaze 在各种后端提供可适应的、统一的和一致的用户界面。Blaze 协调以下操作：
- en: '**Data**: Seamless exchange of data across storages such as CSV, JSON, HDF5,
    HDFS, and Bcolz files.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据**：在 CSV、JSON、HDF5、HDFS 和 Bcolz 文件等存储之间无缝交换数据。'
- en: '**Computation**: Using the same query processing against computational backends
    such as Spark, MongoDB, Pandas, or SQL Alchemy.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算**：使用相同的查询处理计算后端，如 Spark、MongoDB、Pandas 或 SQL Alchemy。'
- en: '**Symbolic expressions**: Abstract expressions such as join, group-by, filter,
    selection, and projection with a syntax similar to Pandas but limited in scope.
    Implements the split-apply-combine methods pioneered by the R language.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**符号表达式**：类似于 Pandas 的语法，但作用域有限的抽象表达式，如连接、分组、过滤、选择和投影。实现了 R 语言开创的拆分-应用-组合方法。'
- en: Blaze expressions are lazily evaluated and in that respect share a similar processing
    paradigm with Spark RDDs transformations.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Blaze 表达式是惰性评估的，在这方面与 Spark RDDs 转换具有相似的处理范式。
- en: 'Let''s dive into Blaze by first importing the necessary libraries: `numpy`,
    `pandas`, `blaze` and `odo`. Odo is a spin-off of Blaze and ensures data migration
    from various backends. The commands are as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先导入必要的库：`numpy`、`pandas`、`blaze` 和 `odo`。Odo 是 Blaze 的衍生品，确保从各种后端进行数据迁移。命令如下：
- en: '[PRE34]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We create a Pandas `Dataframe` by reading the parsed tweets saved in a CSV
    file, `twts_csv`:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过读取保存在 CSV 文件 `twts_csv` 中的解析后的推文来创建 Pandas `Dataframe`：
- en: '[PRE35]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We run the Tweets Panda `Dataframe` to the `describe()` function to get some
    overall information on the dataset:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 Tweets Panda `Dataframe` 运行到 `describe()` 函数以获取有关数据集的一些总体信息：
- en: '[PRE36]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We convert the Pandas `dataframe` into a Blaze `dataframe` by simply passing
    it through the `Data()` function:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过简单地通过 `Data()` 函数传递 Pandas `dataframe` 来将其转换为 Blaze `dataframe`：
- en: '[PRE37]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can retrieve the schema representation of the Blaze `dataframe` by passing
    the `schema` function:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过传递 `schema` 函数来检索 Blaze `dataframe` 的模式表示：
- en: '[PRE38]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The `.dshape` function gives a record count and the schema:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`.dshape` 函数提供了记录数和模式：'
- en: '[PRE39]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We can print the Blaze `dataframe` content:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以打印 Blaze `dataframe` 的内容：
- en: '[PRE40]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We extract the column `tweet_text` and take the unique values:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提取列 `tweet_text` 并取其唯一值：
- en: '[PRE41]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We extract multiple columns `[''id'', ''user_name'',''tweet_text'']` from the
    `dataframe` and take the unique records:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 `dataframe` 中提取多个列 `['id', 'user_name','tweet_text']` 并取其唯一记录：
- en: '[PRE42]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Transferring data using Odo
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Odo 转移数据
- en: 'Odo is a spin-off project of Blaze. Odo allows the interchange of data. Odo
    ensures the migration of data across different formats (CSV, JSON, HDFS, and more)
    and across different databases (SQL databases, MongoDB, and so on) using a very
    simple predicate:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Odo 是 Blaze 的一个衍生项目。Odo 允许数据交换。Odo 确保数据在不同格式（CSV、JSON、HDFS 等）和不同数据库（SQL 数据库、MongoDB
    等）之间迁移，使用一个非常简单的谓词：
- en: '[PRE43]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'To transfer to a database, the address is specified using a URL. For example,
    for a MongoDB database, it would look like this:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要传输到数据库，使用 URL 指定地址。例如，对于 MongoDB 数据库，它看起来会是这样：
- en: '[PRE44]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let''s run some examples of using Odo. Here, we illustrate `odo` by reading
    a CSV file and creating a Blaze `dataframe`:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行一些使用 Odo 的示例。在这里，我们通过读取 CSV 文件并创建 Blaze `dataframe` 来展示 `odo`：
- en: '[PRE45]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Count the number of records in the `dataframe`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 计算数据框中的记录数：
- en: '[PRE46]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Display the five initial records of the `dataframe`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 显示 `dataframe` 的前五个记录：
- en: '[PRE47]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Get `dshape` information from the `dataframe`, which gives us the number of
    records and the schema:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `dataframe` 获取 `dshape` 信息，它给我们记录数和 schema：
- en: '[PRE48]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Save a processed Blaze `dataframe` into JSON:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 将处理后的 Blaze `dataframe` 保存到 JSON：
- en: '[PRE49]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Convert a JSON file to a CSV file:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 将 JSON 文件转换为 CSV 文件：
- en: '[PRE50]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Exploring data using Spark SQL
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spark SQL 探索数据
- en: Spark SQL is a relational query engine built on top of Spark Core. Spark SQL
    uses a query optimizer called **Catalyst**.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 是建立在 Spark Core 之上的关系型查询引擎。Spark SQL 使用一个名为 **Catalyst** 的查询优化器。
- en: Relational queries can be expressed using SQL or HiveQL and executed against
    JSON, CSV, and various databases. Spark SQL gives us the full expressiveness of
    declarative programing with Spark dataframes on top of functional programming
    with RDDs.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 关系型查询可以使用 SQL 或 HiveQL 表达，并针对 JSON、CSV 和各种数据库执行。Spark SQL 给我们在 RDD 函数式编程的基础上，提供了
    Spark 数据框的声明性编程的完整表达能力。
- en: Understanding Spark dataframes
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Spark 数据框
- en: Here's a tweet from `@bigdata` announcing Spark 1.3.0, the advent of Spark SQL
    and dataframes. It also highlights the various data sources in the lower part
    of the diagram. On the top part, we can notice R as the new language that will
    be gradually supported on top of Scala, Java, and Python. Ultimately, the Data
    Frame philosophy is pervasive between R, Python, and Spark.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一条来自 `@bigdata` 的推文，宣布 Spark 1.3.0 的发布，Spark SQL 和数据框的出现。它还突出了图中下方的各种数据源。在上部，我们可以注意到
    R 作为将在 Scala、Java 和 Python 之上逐步支持的新语言。最终，数据框哲学在 R、Python 和 Spark 之间无处不在。
- en: '![Understanding Spark dataframes](img/B03986_03_02.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![理解 Spark 数据框](img/B03986_03_02.jpg)'
- en: Spark dataframes originate from SchemaRDDs. It combines RDD with a schema that
    can be inferred by Spark, if requested, when registering the dataframe. It allows
    us to query complex nested JSON data with plain SQL. Lazy evaluation, lineage,
    partitioning, and persistence apply to dataframes.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 数据框起源于 SchemaRDDs。它结合了 RDD 和 Spark 可以推断的 schema，如果需要，在注册 dataframe 时可以请求。它允许我们使用普通的
    SQL 查询复杂的嵌套 JSON 数据。延迟评估、血缘、分区和持久化适用于数据框。
- en: 'Let''s query the data with Spark SQL, by first importing `SparkContext` and
    `SQLContext`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Spark SQL 查询数据，首先导入 `SparkContext` 和 `SQLContext`：
- en: '[PRE51]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We read in the JSON file we saved with Odo:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们读取使用 Odo 保存的 JSON 文件：
- en: '[PRE52]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We print the schema of the Spark dataframe:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们打印 Spark 数据框的模式：
- en: '[PRE53]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We select the `user_name` column from the dataframe:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 dataframe 中选择 `user_name` 列：
- en: '[PRE54]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We register the dataframe as a table, so we can execute a SQL query on it:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 dataframe 注册为表，因此可以在其上执行 SQL 查询：
- en: '[PRE55]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We execute a SQL statement against the dataframe:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 dataframe 上执行 SQL 语句：
- en: '[PRE56]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let''s process some more complex JSON; we read the original Twitter JSON file:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们处理一些更复杂的 JSON；我们读取原始的 Twitter JSON 文件：
- en: '[PRE57]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Spark SQL is able to infer the schema of a complex nested JSON file:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 能够推断复杂嵌套 JSON 文件的 schema：
- en: '[PRE58]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We extract the key information of interest from the wall of data by selecting
    specific columns in the dataframe (in this case, `[''created_at'', ''id'', ''text'',
    ''user.id'', ''user.name'', ''entities.urls.expanded_url'']`):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在 dataframe 中选择特定的列（在这种情况下，`['created_at', 'id', 'text', 'user.id', 'user.name',
    'entities.urls.expanded_url']`）从数据墙中提取感兴趣的关键信息：
- en: '[PRE59]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Understanding the Spark SQL query optimizer
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Spark SQL 查询优化器
- en: 'We execute a SQL statement against the dataframe:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 dataframe 上执行 SQL 语句：
- en: '[PRE60]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We get a detailed view of the query plans executed by Spark SQL:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以详细查看 Spark SQL 执行的查询计划：
- en: Parsed logical plan
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析的逻辑计划
- en: Analyzed logical plan
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析的逻辑计划
- en: Optimized logical plan
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化的逻辑计划
- en: Physical plan
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理计划
- en: The query plan uses Spark SQL's Catalyst optimizer. In order to generate the
    compiled bytecode from the query parts, the Catalyst optimizer runs through logical
    plan parsing and optimization followed by physical plan evaluation and optimization
    based on cost.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 查询计划使用 Spark SQL 的 Catalyst 优化器。为了从查询部分生成编译后的字节码，Catalyst 优化器会运行逻辑计划解析和优化，然后基于成本进行物理计划评估和优化。
- en: 'This is illustrated in the following tweet:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这在下述推文中得到了说明：
- en: '![Understanding the Spark SQL query optimizer](img/B03986_03_03.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![理解 Spark SQL 查询优化器](img/B03986_03_03.jpg)'
- en: 'Looking back at our code, we call the `.explain` function on the Spark SQL
    query we just executed, and it delivers the full details of the steps taken by
    the Catalyst optimizer in order to assess and optimize the logical plan and the
    physical plan and get to the result RDD:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们的代码，我们在刚刚执行的 Spark SQL 查询上调用 `.explain` 函数，它提供了 Catalyst 优化器在评估和优化逻辑计划和物理计划以及得到结果
    RDD 所采取的步骤的完整细节：
- en: '[PRE61]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Finally, here''s the result of the query:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这是查询的结果：
- en: '[PRE62]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Loading and processing CSV files with Spark SQL
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Spark SQL 加载和处理 CSV 文件
- en: 'We will use the Spark package `spark-csv_2.11:1.2.0`. The command to be used
    to launch PySpark with the IPython Notebook and the `spark-csv` package should
    explicitly state the `–packages` argument:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Spark 包 `spark-csv_2.11:1.2.0`。用于启动 PySpark 并使用 `spark-csv` 包的命令应明确指定
    `–packages` 参数：
- en: '[PRE63]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'This will trigger the following output; we can see that the `spark-csv` package
    is installed with all its dependencies:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这将触发以下输出；我们可以看到 `spark-csv` 包及其所有依赖项已安装：
- en: '[PRE64]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We are now ready to load our `csv` file and process it. Let''s first import
    the `SQLContext`:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备加载我们的 `csv` 文件并处理它。首先，我们导入 `SQLContext`：
- en: '[PRE66]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We access the schema of the dataframe created from the loaded `csv`:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们访问从加载的 `csv` 创建的数据框的架构：
- en: '[PRE67]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'We check the columns of the dataframe:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查数据框的列：
- en: '[PRE68]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We introspect the dataframe content:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查数据框的内容：
- en: '[PRE69]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Querying MongoDB from Spark SQL
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 Spark SQL 查询 MongoDB
- en: 'There are two major ways to interact with MongoDB from Spark: the first is
    through the Hadoop MongoDB connector, and the second one is directly from Spark
    to MongoDB.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 与 MongoDB 交互有两种主要方式：第一种是通过 Hadoop MongoDB 连接器，第二种是直接从 Spark 到 MongoDB。
- en: 'The first approach to interact with MongoDB from Spark is to set up a Hadoop
    environment and query through the Hadoop MongoDB connector. The connector details
    are hosted on GitHub at [https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage](https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage).
    An actual use case is described in the series of blog posts from MongoDB:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Spark 交互 MongoDB 的第一种方法是设置 Hadoop 环境，并通过 Hadoop MongoDB 连接器进行查询。连接器详细信息托管在
    GitHub 上，网址为 [https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage](https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage)。MongoDB
    系列博客文章中描述了实际用例：
- en: '*Using MongoDB with Hadoop & Spark: Part 1 - Introduction & Setup* ([https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-1-introduction-setup](https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-1-introduction-setup))'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 Hadoop 和 Spark 与 MongoDB：第 1 部分 - 简介与设置* ([https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-1-introduction-setup](https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-1-introduction-setup))'
- en: '*Using MongoDB with Hadoop and Spark: Part 2 - Hive Example* ([https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-2-hive-example](https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-2-hive-example))'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 Hadoop 和 Spark 与 MongoDB：第 2 部分 - Hive 示例* ([https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-2-hive-example](https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-2-hive-example))'
- en: '*Using MongoDB with Hadoop & Spark: Part 3 - Spark Example & Key Takeaways*
    ([https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-3-spark-example-key-takeaways](https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-3-spark-example-key-takeaways))'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 Hadoop 和 Spark 与 MongoDB：第 3 部分 - Spark 示例与关键要点* ([https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-3-spark-example-key-takeaways](https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-3-spark-example-key-takeaways))'
- en: 'Setting up a full Hadoop environment is bit elaborate. We will favor the second
    approach. We will use the `spark-mongodb` connector developed and maintained by
    Stratio. We are using the `Stratio spark-mongodb` package hosted at `spark.packages.org`.
    The packages information and version can be found in `spark.packages.org`:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 设置完整的 Hadoop 环境有些复杂。我们将优先考虑第二种方法。我们将使用由 Stratio 开发和维护的 `spark-mongodb` 连接器。我们使用托管在
    `spark.packages.org` 的 `Stratio spark-mongodb` 包。包信息和版本可以在 `spark.packages.org`
    中找到：
- en: Note
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Releases**'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**版本**'
- en: 'Version: 0.10.1 ( 8263c8 | zip | jar ) / Date: 2015-11-18 / License: Apache-2.0
    / Scala version: 2.10'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 版本：0.10.1 ( 8263c8 | zip | jar ) / 日期：2015-11-18 / 许可证：Apache-2.0 / Scala版本：2.10
- en: ([http://spark-packages.org/package/Stratio/spark-mongodb](http://spark-packages.org/package/Stratio/spark-mongodb))
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ([http://spark-packages.org/package/Stratio/spark-mongodb](http://spark-packages.org/package/Stratio/spark-mongodb))
- en: 'The command to launch PySpark with the IPython Notebook and the `spark-mongodb`
    package should explicitly state the packages argument:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 启动PySpark与IPython Notebook以及`spark-mongodb`包的命令应明确指定包参数：
- en: '[PRE70]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'This will trigger the following output; we can see that the `spark-mongodb`
    package is installed with all its dependencies:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这将触发以下输出；我们可以看到`spark-mongodb`包及其所有依赖项已安装：
- en: '[PRE71]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: We are now ready to query MongoDB on `localhost:27017` from the collection `twtr01_coll`
    in the database `twtr01_db`.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已准备好从数据库`twtr01_db`中的集合`twtr01_coll`在`localhost:27017`上查询MongoDB：
- en: 'We first import the `SQLContext`:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入`SQLContext`：
- en: '[PRE72]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Here''s the output of our query:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的查询输出：
- en: '[PRE73]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Summary
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we harvested data from Twitter. Once the data was acquired,
    we explored the information using `Continuum.io's` Blaze and Odo libraries. Spark
    SQL is an important module for interactive data exploration, analysis, and transformation,
    leveraging the Spark dataframe datastructure. The dataframe concept originates
    from R and then was adopted by Python Pandas with great success. The dataframe
    is the workhorse of the data scientist. The combination of Spark SQL and dataframe
    creates a powerful engine for data processing.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从Twitter收集了数据。一旦数据被获取，我们就使用`Continuum.io's`的Blaze和Odo库来探索这些信息。Spark SQL是交互式数据探索、分析和转换的重要模块，它利用了Spark
    dataframe数据结构。dataframe概念起源于R，然后被Python Pandas成功采用。dataframe是数据科学家的得力助手。Spark
    SQL和dataframe的结合为数据处理创建了一个强大的引擎。
- en: We are now gearing up for extracting the insights from the datasets using machine
    learning from Spark MLlib.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在正在准备使用Spark MLlib的机器学习从数据集中提取洞察：
