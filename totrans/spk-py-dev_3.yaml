- en: Chapter 3. Juggling Data with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As per the batch and streaming architecture laid out in the previous chapter,
    we need data to fuel our applications. We will harvest data focused on Apache
    Spark from Twitter. The objective of this chapter is to prepare data to be further
    used by the machine learning and streaming applications. This chapter focuses
    on how to exchange code and data across the distributed network. We will get practical
    insights into serialization, persistence, marshaling, and caching. We will get
    to grips with on Spark SQL, the key Spark module to interactively explore structured
    and semi-structured data. The fundamental data structure powering Spark SQL is
    the Spark dataframe. The Spark dataframe is inspired by the Python Pandas dataframe
    and the R dataframe. It is a powerful data structure, well understood and appreciated
    by data scientists with a background in R or Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Connect to Twitter, collect the relevant data, and then persist it in various
    formats such as JSON and CSV and data stores such as MongoDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the data using Blaze and Odo, a spin-off library from Blaze, in order
    to connect and transfer data from various sources and destinations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduce Spark dataframes as the foundation for data interchange between the
    various Spark modules and explore data interactively using Spark SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revisiting the data-intensive app architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's first put in context the focus of this chapter with respect to the data-intensive
    app architecture. We will concentrate our attention on the integration layer and
    essentially run through iterative cycles of the acquisition, refinement, and persistence
    of the data. This cycle was termed the five Cs. The five Cs stand for *connect*,
    *collect*, *correct*, *compose*, and *consume*. They are the essential processes
    we run through in the integration layer in order to get to the right quality and
    quantity of data retrieved from Twitter. We will also delve deeper in the persistence
    layer and set up a data store such as MongoDB to collect our data for processing
    later.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore the data with Blaze, a Python library for data manipulation,
    and Spark SQL, the interactive module of Spark for data discovery powered by the
    Spark dataframe. The dataframe paradigm is shared by Python Pandas, Python Blaze,
    and Spark SQL. We will get a feel for the nuances of the three dataframe flavors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram sets the context of the chapter''s focus, highlighting
    the integration layer and the persistence layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Revisiting the data-intensive app architecture](img/B03986_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Serializing and deserializing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we are harvesting data from web APIs under rate limit constraints, we need
    to store them. As the data is processed on a distributed cluster, we need consistent
    ways to save state and retrieve it for later usage.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now define serialization, persistence, marshaling, and caching or memorization.
  prefs: []
  type: TYPE_NORMAL
- en: Serializing a Python object converts it into a stream of bytes. The Python object
    needs to be retrieved beyond the scope of its existence, when the program is shut.
    The serialized Python object can be transferred over a network or stored in a
    persistent storage. Deserialization is the opposite and converts the stream of
    bytes into the original Python object so the program can carry on from the saved
    state. The most popular serialization library in Python is Pickle. As a matter
    of fact, the PySpark commands are transferred over the wire to the worker nodes
    via pickled data.
  prefs: []
  type: TYPE_NORMAL
- en: Persistence saves a program's state data to disk or memory so that it can carry
    on where it left off upon restart. It saves a Python object from memory to a file
    or a database and loads it later with the same state.
  prefs: []
  type: TYPE_NORMAL
- en: Marshalling sends Python code or data over a network TCP connection in a multicore
    or distributed system.
  prefs: []
  type: TYPE_NORMAL
- en: Caching converts a Python object to a string in memory so that it can be used
    as a dictionary key later on. Spark supports pulling a dataset into a cluster-wide,
    in-memory cache. This is very useful when data is accessed repeatedly such as
    when querying a small reference dataset or running an iterative algorithm such
    as Google PageRank.
  prefs: []
  type: TYPE_NORMAL
- en: Caching is a crucial concept for Spark as it allows us to save RDDs in memory
    or with a spillage to disk. The caching strategy can be selected based on the
    lineage of the data or the **DAG** (short for **Directed Acyclic Graph**) of transformations
    applied to the RDDs in order to minimize shuffle or cross network heavy data exchange.
    In order to achieve good performance with Spark, beware of data shuffling. A good
    partitioning policy and use of RDD caching, coupled with avoiding unnecessary
    action operations, leads to better performance with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Harvesting and storing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before delving into database persistent storage such as MongoDB, we will look
    at some useful file storages that are widely used: **CSV** (short for **comma-separated
    values**) and **JSON** (short for **JavaScript Object Notation**) file storage.
    The enduring popularity of these two file formats lies in a few key reasons: they
    are human readable, simple, relatively lightweight, and easy to use.'
  prefs: []
  type: TYPE_NORMAL
- en: Persisting data in CSV
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The CSV format is lightweight, human readable, and easy to use. It has delimited
    text columns with an inherent tabular schema.
  prefs: []
  type: TYPE_NORMAL
- en: Python offers a robust `csv` library that can serialize a `csv` file into a
    Python dictionary. For the purpose of our program, we have written a `python`
    class that manages to persist data in CSV format and read from a given CSV.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run through the code of the class `IO_csv` object. The `__init__` section
    of the class basically instantiates the file path, the filename, and the file
    suffix (in this case, `.csv`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `save` method of the class uses a Python named tuple and the header fields
    of the `csv` file in order to impart a schema while persisting the rows of the
    CSV. If the `csv` file already exists, it will be appended and not overwritten
    otherwise; it will be created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The `load` method of the class also uses a Python named tuple and the header
    fields of the `csv` file in order to retrieve the data using a consistent schema.
    The `load` method is a memory-efficient generator to avoid loading a huge file
    in memory: hence we use `yield` in place of `return`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the named tuple. We are using it to parse the tweet in order to save
    or retrieve them to and from the `csv` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Persisting data in JSON
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JSON is one of the most popular data formats for Internet-based applications.
    All the APIs we are dealing with, Twitter, GitHub, and Meetup, deliver their data
    in JSON format. The JSON format is relatively lightweight compared to XML and
    human readable, and the schema is embedded in JSON. As opposed to the CSV format,
    where all records follow exactly the same tabular structure, JSON records can
    vary in their structure. JSON is semi-structured. A JSON record can be mapped
    into a Python dictionary of dictionaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run through the code of the class `IO_json` object. The `__init__` section
    of the class basically instantiates the file path, the filename, and the file
    suffix (in this case, `.json`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `save` method of the class uses `utf-8` encoding in order to ensure read
    and write compatibility of the data. If the JSON file already exists, it will
    be appended and not overwritten; otherwise it will be created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `load` method of the class just returns the file that has been read. A
    further `json.loads` function needs to be applied in order to retrieve the `json`
    out of the file read:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Setting up MongoDB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is crucial to store the information harvested. Thus, we set up MongoDB as
    our main document data store. As all the information collected is in JSON format
    and MongoDB stores information in **BSON** (short for **Binary JSON**), it is
    therefore a natural choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will run through the following steps now:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing the MongoDB server and client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the MongoDB server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the Mongo client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing the PyMongo driver
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the Python Mongo client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing the MongoDB server and client
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to install the MongoDB package, perform through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the public key used by the package management system (in our case, Ubuntu''s
    `apt`). To import the MongoDB public key, we issue the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a list file for MongoDB. To create the list file, we use the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Update the local package database as `sudo`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install the MongoDB packages. We install the latest stable version of MongoDB
    with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Running the MongoDB server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s start the MongoDB server:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start MongoDB server, we issue the following command to start `mongod`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To check whether `mongod` has started properly, we issue the command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this case, we see that `mongodb` is running in process `967`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `mongod` server sends a message to the effect that it is waiting for connection
    on `port 27017`. This is the default port for MongoDB. It can be changed in the
    configuration file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can check the contents of the log file at `/var/log/mongod/mongod.log`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In order to stop the `mongodb` server, just issue the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Running the Mongo client
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Running the Mongo client in the console is as easy as calling `mongo`, as highlighted
    in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'At the mongo client console prompt, we can see the databases with the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We select the test database using `use test`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We display the collections within the test database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We check a sample record in the restaurant collection listed previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Installing the PyMongo driver
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Installing the Python driver with anaconda is easy. Just run the following
    command at the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Creating the Python client for MongoDB
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are creating a `IO_mongo` class that will be used in our harvesting and
    processing programs to store the data collected and retrieved saved information.
    In order to create the `mongo` client, we will import the `MongoClient` module
    from `pymongo`. We connect to the `mongodb` server on localhost at port 27017\.
    The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We initialize our class with the client connection, the database (in this case,
    `twtr_db`), and the collection (in this case, `twtr_coll`) to be accessed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `save` method inserts new records in the preinitialized collection and
    database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `load` method allows the retrieval of specific records according to criteria
    and projection. In the case of large amount of data, it returns a cursor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Harvesting data from Twitter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each social network poses its limitations and challenges. One of the main obstacles
    for harvesting data is an imposed rate limit. While running repeated or long-running
    connections between rates limit pauses, we have to be careful to avoid collecting
    duplicate data.
  prefs: []
  type: TYPE_NORMAL
- en: We have redesigned our connection programs outlined in the previous chapter
    to take care of the rate limits.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this `TwitterAPI` class that connects and collects the tweets according
    to the search query we specify, we have added the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Logging capability using the Python logging library with the aim of collecting
    any errors or warning in the case of program failure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persistence capability using MongoDB, with the `IO_mongo` class exposed previously
    as well as JSON file using the `IO_json` class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: API rate limit and error management capability, so we can ensure more resilient
    calls to Twitter without getting barred for tapping into the firehose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s go through the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We initialize by instantiating the Twitter API with our credentials:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We initialize the logger by providing the log level:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`logger.debug`(debug message)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logger.info`(info message)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logger.warn`(warn message)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logger.error`(error message)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logger.critical`(critical message)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We set the log path and the message format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We initialize the JSON file persistence instruction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We initialize the MongoDB database and collection for persistence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The method `searchTwitter` launches the search according to the query specified:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `saveTweets` method actually saves the collected tweets in JSON and in
    MongoDB:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `parseTweets` method allows us to extract the key tweet information from
    the vast amount of information provided by the Twitter API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `getTweets` method calls the `searchTwitter` method described previously.
    The `getTweets` method ensures that API calls are made reliably whilst respecting
    the imposed rate limit. The code is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we are calling the `searchTwitter` API with the relevant query based
    on the parameters specified. If we encounter any error such as rate limitation
    from the provider, this will be processed by the `handleError` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Exploring data using Blaze
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Blaze is an open source Python library, primarily developed by Continuum.io,
    leveraging Python Numpy arrays and Pandas dataframe. Blaze extends to out-of-core
    computing, while Pandas and Numpy are single-core.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blaze offers an adaptable, unified, and consistent user interface across various
    backends. Blaze orchestrates the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data**: Seamless exchange of data across storages such as CSV, JSON, HDF5,
    HDFS, and Bcolz files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computation**: Using the same query processing against computational backends
    such as Spark, MongoDB, Pandas, or SQL Alchemy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Symbolic expressions**: Abstract expressions such as join, group-by, filter,
    selection, and projection with a syntax similar to Pandas but limited in scope.
    Implements the split-apply-combine methods pioneered by the R language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blaze expressions are lazily evaluated and in that respect share a similar processing
    paradigm with Spark RDDs transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s dive into Blaze by first importing the necessary libraries: `numpy`,
    `pandas`, `blaze` and `odo`. Odo is a spin-off of Blaze and ensures data migration
    from various backends. The commands are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a Pandas `Dataframe` by reading the parsed tweets saved in a CSV
    file, `twts_csv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We run the Tweets Panda `Dataframe` to the `describe()` function to get some
    overall information on the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We convert the Pandas `dataframe` into a Blaze `dataframe` by simply passing
    it through the `Data()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can retrieve the schema representation of the Blaze `dataframe` by passing
    the `schema` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The `.dshape` function gives a record count and the schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We can print the Blaze `dataframe` content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We extract the column `tweet_text` and take the unique values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We extract multiple columns `[''id'', ''user_name'',''tweet_text'']` from the
    `dataframe` and take the unique records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Transferring data using Odo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Odo is a spin-off project of Blaze. Odo allows the interchange of data. Odo
    ensures the migration of data across different formats (CSV, JSON, HDFS, and more)
    and across different databases (SQL databases, MongoDB, and so on) using a very
    simple predicate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'To transfer to a database, the address is specified using a URL. For example,
    for a MongoDB database, it would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run some examples of using Odo. Here, we illustrate `odo` by reading
    a CSV file and creating a Blaze `dataframe`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Count the number of records in the `dataframe`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the five initial records of the `dataframe`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Get `dshape` information from the `dataframe`, which gives us the number of
    records and the schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Save a processed Blaze `dataframe` into JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert a JSON file to a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Exploring data using Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark SQL is a relational query engine built on top of Spark Core. Spark SQL
    uses a query optimizer called **Catalyst**.
  prefs: []
  type: TYPE_NORMAL
- en: Relational queries can be expressed using SQL or HiveQL and executed against
    JSON, CSV, and various databases. Spark SQL gives us the full expressiveness of
    declarative programing with Spark dataframes on top of functional programming
    with RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Spark dataframes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here's a tweet from `@bigdata` announcing Spark 1.3.0, the advent of Spark SQL
    and dataframes. It also highlights the various data sources in the lower part
    of the diagram. On the top part, we can notice R as the new language that will
    be gradually supported on top of Scala, Java, and Python. Ultimately, the Data
    Frame philosophy is pervasive between R, Python, and Spark.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Spark dataframes](img/B03986_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Spark dataframes originate from SchemaRDDs. It combines RDD with a schema that
    can be inferred by Spark, if requested, when registering the dataframe. It allows
    us to query complex nested JSON data with plain SQL. Lazy evaluation, lineage,
    partitioning, and persistence apply to dataframes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s query the data with Spark SQL, by first importing `SparkContext` and
    `SQLContext`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We read in the JSON file we saved with Odo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We print the schema of the Spark dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We select the `user_name` column from the dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We register the dataframe as a table, so we can execute a SQL query on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We execute a SQL statement against the dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s process some more complex JSON; we read the original Twitter JSON file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Spark SQL is able to infer the schema of a complex nested JSON file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We extract the key information of interest from the wall of data by selecting
    specific columns in the dataframe (in this case, `[''created_at'', ''id'', ''text'',
    ''user.id'', ''user.name'', ''entities.urls.expanded_url'']`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Understanding the Spark SQL query optimizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We execute a SQL statement against the dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We get a detailed view of the query plans executed by Spark SQL:'
  prefs: []
  type: TYPE_NORMAL
- en: Parsed logical plan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzed logical plan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimized logical plan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Physical plan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The query plan uses Spark SQL's Catalyst optimizer. In order to generate the
    compiled bytecode from the query parts, the Catalyst optimizer runs through logical
    plan parsing and optimization followed by physical plan evaluation and optimization
    based on cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is illustrated in the following tweet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Spark SQL query optimizer](img/B03986_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Looking back at our code, we call the `.explain` function on the Spark SQL
    query we just executed, and it delivers the full details of the steps taken by
    the Catalyst optimizer in order to assess and optimize the logical plan and the
    physical plan and get to the result RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, here''s the result of the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Loading and processing CSV files with Spark SQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the Spark package `spark-csv_2.11:1.2.0`. The command to be used
    to launch PySpark with the IPython Notebook and the `spark-csv` package should
    explicitly state the `–packages` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'This will trigger the following output; we can see that the `spark-csv` package
    is installed with all its dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to load our `csv` file and process it. Let''s first import
    the `SQLContext`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'We access the schema of the dataframe created from the loaded `csv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'We check the columns of the dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We introspect the dataframe content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Querying MongoDB from Spark SQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two major ways to interact with MongoDB from Spark: the first is
    through the Hadoop MongoDB connector, and the second one is directly from Spark
    to MongoDB.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first approach to interact with MongoDB from Spark is to set up a Hadoop
    environment and query through the Hadoop MongoDB connector. The connector details
    are hosted on GitHub at [https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage](https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage).
    An actual use case is described in the series of blog posts from MongoDB:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Using MongoDB with Hadoop & Spark: Part 1 - Introduction & Setup* ([https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-1-introduction-setup](https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-1-introduction-setup))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Using MongoDB with Hadoop and Spark: Part 2 - Hive Example* ([https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-2-hive-example](https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-2-hive-example))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Using MongoDB with Hadoop & Spark: Part 3 - Spark Example & Key Takeaways*
    ([https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-3-spark-example-key-takeaways](https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-3-spark-example-key-takeaways))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Setting up a full Hadoop environment is bit elaborate. We will favor the second
    approach. We will use the `spark-mongodb` connector developed and maintained by
    Stratio. We are using the `Stratio spark-mongodb` package hosted at `spark.packages.org`.
    The packages information and version can be found in `spark.packages.org`:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Releases**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Version: 0.10.1 ( 8263c8 | zip | jar ) / Date: 2015-11-18 / License: Apache-2.0
    / Scala version: 2.10'
  prefs: []
  type: TYPE_NORMAL
- en: ([http://spark-packages.org/package/Stratio/spark-mongodb](http://spark-packages.org/package/Stratio/spark-mongodb))
  prefs: []
  type: TYPE_NORMAL
- en: 'The command to launch PySpark with the IPython Notebook and the `spark-mongodb`
    package should explicitly state the packages argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'This will trigger the following output; we can see that the `spark-mongodb`
    package is installed with all its dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to query MongoDB on `localhost:27017` from the collection `twtr01_coll`
    in the database `twtr01_db`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first import the `SQLContext`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the output of our query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we harvested data from Twitter. Once the data was acquired,
    we explored the information using `Continuum.io's` Blaze and Odo libraries. Spark
    SQL is an important module for interactive data exploration, analysis, and transformation,
    leveraging the Spark dataframe datastructure. The dataframe concept originates
    from R and then was adopted by Python Pandas with great success. The dataframe
    is the workhorse of the data scientist. The combination of Spark SQL and dataframe
    creates a powerful engine for data processing.
  prefs: []
  type: TYPE_NORMAL
- en: We are now gearing up for extracting the insights from the datasets using machine
    learning from Spark MLlib.
  prefs: []
  type: TYPE_NORMAL
