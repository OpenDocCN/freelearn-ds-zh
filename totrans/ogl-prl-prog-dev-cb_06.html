<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Developing a Sobel Edge Detection Filter</h1></div></div></div><p>In this chapter, we'll cover the following recipes:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding the convolution Theory</li><li class="listitem" style="list-style-type: disc">Understanding convolution in 1D</li><li class="listitem" style="list-style-type: disc">Understanding convolution in 2D</li><li class="listitem" style="list-style-type: disc">OpenCL implementation of the Sobel edge filter</li><li class="listitem" style="list-style-type: disc">Understanding profiling in OpenCL</li></ul></div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec45"/>Introduction</h1></div></div></div><p>In this chapter, we are going to take a look at how to develop a popular image processing algorithm known as<a id="id458" class="indexterm"/> edge detection. This problem happens to be a part of solving a more general problem in image segmentation.</p><div><div><h3 class="title"><a id="note27"/>Note</h3><p>Image segmentation<a id="id459" class="indexterm"/> is the process of partitioning a digital image into multiple segments (sets of pixels, also known as super pixels). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze. Image segmentation is typically used to locate objects and boundaries (lines, curves, and so on) in images.</p></div></div><p>The Sobel operator<a id="id460" class="indexterm"/> is a discrete differentiation operator, computing an approximation of the gradient of the image density function. The Sobel operator is based on convolving the image with a small, separable, and an integer-value filter in both horizontal and vertical directions. Thus, it is relatively inexpensive in terms of computations.</p><p>Don't worry if you don't understand these notations right away, we are going to step through enough theory and math, and help you realize the application in OpenCL.</p><p>Briefly, the Sobel filtering<a id="id461" class="indexterm"/> is a three-step process. Two 3 x 3 filters are applied separately and independently on every pixel and the idea is to use these two filters to approximate the derivatives of x and y, respectively. Using the results of these filters, we can finally approximate the magnitude of the gradient.</p><p>The gradient computed by running Sobel's edge detector through each pixel (which also uses its neighboring eight pixels) will inform us whether there are changes in the vertical and horizontal axes (where the neighboring pixels reside).</p><p>For those who are already familiar with the convolution theory, in general, may skip to the <em>How to do it</em> section of this recipe.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec46"/>Understanding the convolution theory</h1></div></div></div><p>In the past, mathematicians developed <a id="id462" class="indexterm"/>calculus so that there's a systematic way to reason about how things change, and the convolution theory is really about measuring how these changes affect one another. At that time, the convolution integral was born.</p><div><img src="img/4520OT_06_24.jpg" alt="Understanding the convolution theory"/></div><p>And the <img src="img/4520OT_06_23.jpg" alt="Understanding the convolution theory"/> operator<a id="id463" class="indexterm"/> is the convolution operator used in conventional math. An astute reader will notice immediately that we have replaced one function with the other, and the reason why this is done is because of the fact that the convolution operator is commutative, that is, the order of computation does not matter. The computation of the integral can be done in discrete form, and without loss of generality, we can replace the integral sign <img src="img/4520OT_06_22.jpg" alt="Understanding the convolution theory"/> with the summation sign <img src="img/4520OT_06_21.jpg" alt="Understanding the convolution theory"/>, and with that, let's see the mathematical definition of convolution in discrete time domain.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec105"/>Getting ready</h2></div></div></div><p>Later we will walk through what the following equation tells us over a discrete time domain:</p><div><img src="img/4520OT_06_20.jpg" alt="Getting ready"/></div><p>where <em>x[n]</em> is an input signal, <em>h[n]</em> is an impulse response, and <em>y[n]</em> is the output. The asterisk (<em>*</em>) denotes convolution. Notice that we multiply the terms of <em>x[k]</em> by the terms of a time-shifted <em>h[n]</em> and add them up. The key to understanding convolution lies behind impulse response and impulse decomposition.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec106"/>How to do it…</h2></div></div></div><p>In order to understand the<a id="id464" class="indexterm"/> meaning of convolution, we are going to start from the concept of signal decomposition. The input signal can be broken down into additive components, and the system response of the input signal results in by adding the output of these components passed through the system.</p><p>The following section will illustrate on how convolution works in 1D, and once you're proficient in that, we will build on that concept and illustrate how in convolution works 2D and we'll see the Sobel edge detector in action!</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec47"/>Understanding convolution in 1D</h1></div></div></div><p>Let's imagine that a burst <a id="id465" class="indexterm"/>of <a id="id466" class="indexterm"/>energy (signal) have arrived into our system and it looks similar to the following diagram with <em>x[n] = {1,3,4,2,1}, for n = 0,1,2,3,4</em>.</p><div><img src="img/4520OT_06_01.jpg" alt="Understanding convolution in 1D"/></div><p>And let's assume that our impulse function has a non-zero value whenever <strong>n</strong> = <strong>0</strong> or <strong>1</strong>, while it'll have a zero value for all other values of <strong>n</strong>.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec107"/>How to do it...</h2></div></div></div><p>Using the preceding <a id="id467" class="indexterm"/>information, let's work out what the output signal would be by <a id="id468" class="indexterm"/>quickly recalling the following equation:</p><div><img src="img/4520OT_06_19.jpg" alt="How to do it..."/></div><p>Following this equation faithfully, we realize that the output signal is amplified initially and quickly tapers off, and after solving this manually (yes, I mean evaluating the equation on a pencil and paper) we would see the following final output signal:</p><div><img src="img/4520OT_06_18.jpg" alt="How to do it..."/></div><div><img src="img/4520OT_06_02.jpg" alt="How to do it..."/></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec108"/>How it works…</h2></div></div></div><p>Looking at the preceding equation again, this time we rearrange them and remove all terms that evaluate to zero. Let's try to see whether we can discover a pattern:</p><div><img src="img/4520OT_06_17.jpg" alt="How it works…"/></div><p>And I believe you can see that each output value is computed from its previous two output values (taking into account the impulse function)! And now we may conclude, quite comfortably, that the general formula for computing the convolution in 1D is in fact the following:</p><div><img src="img/4520OT_06_16.jpg" alt="How it works…"/></div><p>Finally, you should be<a id="id469" class="indexterm"/> aware that (by convention) any value that is not defined for<a id="id470" class="indexterm"/> any <em>x[i-k]</em> is automatically given the value zero. This seemingly small, subtle fact will play a role in our eventual understanding of the Sobel edge detection filter which we'll describe next.</p><p>Finally for this section, let's take a look at how a sequential convolution code in 1D might look like:</p><div><pre class="programlisting">// dataCount is size of elements in the 1D array
// kernelCount is the pre-defined kernel/filter e.g. h[0]=2,h[1]=1 
// h[x]=0 for x ={…,-1,2,3,…}
for(int i = 0; i &lt; dataCount; ++i) {
  y[i] = 0;
  for(int j = 0; j &lt; kernelCount; ++j) {
    y[i] += x[i – j] * h[j]; // statement 1
  }
}</pre></div><p>Examining the code again, you will probably notice that we are iterating over the 1D array and the most interesting code would be in <code class="literal">statement 1</code>, as this is where the action really lies. Let's put that new knowledge aside and move on to extending this to a 2D space.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec48"/>Understanding convolution in 2D</h1></div></div></div><p>Convolution in 2D is<a id="id471" class="indexterm"/> actually <a id="id472" class="indexterm"/>an extension of the previously described <em>Understanding convolution in 1D</em> section, and we do so by computing the convolution in two dimensions.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec109"/>Getting ready</h2></div></div></div><p>The impulse function also exists in a 2D spatial domain, so let's call this function. <em>b[x,y]</em> has the value 1, where x and y are zero, and zero where x,y<sup>1</sup>0. The impulse function is also referred to as filter or kernel when it's being used in image processing.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec110"/>How to do it…</h2></div></div></div><p>Using the previous <a id="id473" class="indexterm"/>example as a guide, let's start thinking from the perspective of a signal <a id="id474" class="indexterm"/>which can be decomposed into the sum of its components and impulse functions, and their double summation accounts to the fact that this runs over both vertical and horizontal axes in our 2D space.</p><div><img src="img/4520OT_06_15.jpg" alt="How to do it…"/></div><p>Next, I think it's very helpful if we use an example to illustrate how it works when we have two convolution kernels to represent the filters we like to apply on the elements in a 2D array. Let's give them names, <strong>Sx</strong> and <strong>Sy</strong>. The next thing is to try out how the equation would develop itself in a 2D setting, where the element we want to convolve is at <em>x[1,1]</em> and we make a note of its surrounding eight elements and then see what happens.</p><p>If you think about why we are choosing the surrounding eight elements, it's the only way we can measure how big a change is with respect to every other element.</p><div><img src="img/4520OT_06_14.jpg" alt="How to do it…"/></div><div><img src="img/4520OT_06_03.jpg" alt="How to do it…"/></div><p>Let's give it a go:</p><div><img src="img/4520OT_06_13.jpg" alt="How to do it…"/></div><div><img src="img/4520OT_06_12.jpg" alt="How to do it…"/></div><p>This results in the<a id="id475" class="indexterm"/> summation of nine elements (including the element we're interested in), <a id="id476" class="indexterm"/>and this process is repeated for all elements in the 2D array. The following diagram illustrates how convolution in 2D works in a 2D space.</p><div><div><h3 class="title"><a id="note28"/>Note</h3><p>You may wish to read Irwin Sobel's 1964 original doctoral thesis since he's the inventor, and this author had a good fortune of meeting the man himself.</p></div></div><p>What happens when you attempt to convolve around the elements that border the 2D array or in image processing, are they referred to as edge pixels? If you use this formula for computation, you will notice that the results will be inaccurate, because those elements are undefined and hence they're in general discounted from the final computation. In general, you can imagine a 3 x 3 filtering operation being applied to each element of the 2D array and all such computations will result in a new value for that element in the output data array.</p><p>Next, you may wonder what is being done to this output array? Remember that this array now contains values, which basically shows how big is the change detected in a particular element is. And when you obtain a bunch of them in the vicinity, then it usually tells you major color changes, that is, edges.</p><div><img src="img/4520OT_06_04.jpg" alt="How to do it…"/></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec111"/>How it works…</h2></div></div></div><p>With this understanding, you can probably begin to appreciate why we took this effort to illustrate the theory behind a concept.</p><p>When you want to build non-trivial<a id="id477" class="indexterm"/> OpenCL applications for your customers, one of the things you<a id="id478" class="indexterm"/> have to deal with is learning how to interpret a problem and convert it to a solution. And what that means is mostly about formulating an algorithm (or picking existing algorithms to suit your case) and verifying that it works. Most of the problems you're likely to encounter are going to involve some sort of mathematical understanding and your ability to learn about it. You should treat this as an adventure!</p><p>Now that we've armed ourselves with what convolution is in a 2D space, Let's begin by taking a look at how convolution in 2D would work in regular C/C++ code with the following snippet:</p><div><pre class="programlisting">// find centre position of kernel (assuming a 2D array of equal
// dimensions)
int centerX = kernelCols/2;
int centerY = kernelRows/2;
for(int i = 0; i &lt; numRows2D; ++i) {
  for(int j = 0; j &lt; numCols2D; ++j) {
    for(m = 0; m &lt; kernelRows; ++m) {
          mm = kernelRows - 1 – m;
    for(n = 0; n &lt; kernelCols; ++n) {
              nn = kernelCols - 1 – n;
        ii = i + (m – centerX);
        jj = j + (n – centerY);
        if (ii &gt;= 0 &amp;&amp; ii &lt; rows &amp;&amp; jj &gt;= 0 &amp;&amp; jj &lt; numCols)
       out[i][j] += in[ii][jj] * kernel[mm][nn]; // statement 1
    }
    }
  }
}</pre></div><p>This <a id="id479" class="indexterm"/>implementation is probably the most direct for the purpose of understanding the<a id="id480" class="indexterm"/> concept, although it may not be the fastest (since it's not many-core aware). But it works, as there are conceptually two major loops where the two outer <code class="literal">fo</code>r loops are for iterating over the entire 2D array space, while the two inner <code class="literal">for</code> loops are for iterating the filter/kernel over the element, that is, convoluting and storing the final value into an appropriate output array.</p><p>Putting on our parallel algorithm developer hat now, we discover that <code class="literal">statement 1</code> appears to be a nice target for work items to execute over. Next, let's take a look at how we can take what we've learnt and build the same program in OpenCL.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec49"/>OpenCL implementation of the Sobel edge filter</h1></div></div></div><p>Now that you've been<a id="id481" class="indexterm"/> armed with how convolution actually works, you<a id="id482" class="indexterm"/> should be able to imagine how our algorithm might look like. Briefly, we will read an input image assuming that it's going to be in the Windows BMP format.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec112"/>Getting ready</h2></div></div></div><p>Next we'll construct the necessary data structures for transporting this image file in the OpenCL device for convolution, and once that's done we'll read and write the data out to another image file, so that we can compare the two.</p><div><div><h3 class="title"><a id="note29"/>Note</h3><p>Optionally, you can choose to implement this using the <code class="literal">clCreateImage(...)</code> APIs provided by OpenCL, and we'll leave it as an exercise for the reader to make the attempt.</p></div></div><p>In the following sections, you will be shown with an implementation from what is translated, what we have learnt so far. It won't be the most efficient algorithm, and that's really not our intention here. Rather, we want to show you how you can get this done quickly and we'll let you inject those optimizations which include the not withstanding, following data binning, data tiling, shared memory optimization, warp / wavefront-level programming, implementing 2D-convolution using fast fourier transformations, and so many other features.</p><div><div><h3 class="title"><a id="tip19"/>Tip</h3><p>A possible avenue from where I derived a lot of the latest techniques about solving convolution was by reading academic research papers published by AMD and NVIDIA, and also by visiting <a class="ulink" href="http://gpgpu.org">gpgpu.org</a>, <a class="ulink" href="http://developer.amd.com">developer.amd.com</a>, <a class="ulink" href="http://developer.nvidia.com">developer.nvidia.com</a>, and <a class="ulink" href="http://developer.intel.com">developer.intel.com</a>. Another good resource I can think of are books on image processing and computer vision from your favorite local bookstores. Also, books on processor and memory structure released by Intel are also good resources if you like.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec113"/>How to do it…</h2></div></div></div><p>We only show<a id="id483" class="indexterm"/> the code for the kernel found in <code class="literal">Ch6/sobelfilter/sobel_detector.cl</code>, since <a id="id484" class="indexterm"/>this is where our algorithm translation will reach its Xenith. And we've not shown the host code in <code class="literal">Ch6/sobelfilter/SobelFilter.c</code>, since we believe that you would be confident to know what typically resides in there:</p><div><pre class="programlisting">__kernel void SobelDetector(__global uchar4* input, 
                            __global uchar4* output) {
      uint x = get_global_id(0);
      uint y = get_global_id(1);

  uint width = get_global_size(0);
  uint height = get_global_size(1);

  float4 Gx = (float4)(0);
  float4 Gy = (float4)(0);

    // Given that we know the (x,y) coordinates of the pixel we're 
    // looking at, its natural to use (x,y) to look at its
    // neighbouring pixels
    // Convince yourself that the indexing operation below is
    // doing exactly that

    // the variables i00 through to i22 seek to identify the pixels
    // following the naming convention in graphics programming e.g.   
    // OpenGL where i00 refers
    // to the top-left-hand corner and iterates through to the bottom
    // right-hand corner

  if( x &gt;= 1 &amp;&amp; x &lt; (width-1) &amp;&amp; y &gt;= 1 &amp;&amp; y &lt; height - 1)
  {
    float4 i00 = convert_float4(input[(x - 1) + (y - 1) * width]);
    float4 i10 = convert_float4(input[x + (y - 1) * width]);
    float4 i20 = convert_float4(input[(x + 1) + (y - 1) * width]);
    float4 i01 = convert_float4(input[(x - 1) + y * width]);
    float4 i11 = convert_float4(input[x + y * width]);
    float4 i21 = convert_float4(input[(x + 1) + y * width]);
    float4 i02 = convert_float4(input[(x - 1) + (y + 1) * width]);
    float4 i12 = convert_float4(input[x + (y + 1) * width]);
    float4 i22 = convert_float4(input[(x + 1) + (y + 1) * width]);

        // To understand why the masks are applied this way, look
        // at the mask for Gy and Gx which are respectively equal 
        // to the matrices:
        // { {-1, 0, 1}, { {-1,-2,-1},
        //   {-2, 0, 2},   { 0, 0, 0},
        //   {-1, 0, 1}}   { 1, 2, 1}}

Gx = i00 + (float4)(2) * i10 + i20 - i02  - (float4)(2) * i12 -i22;
Gy = i00 - i20  + (float4)(2)*i01 - (float4)(2)*i21 + i02  -  i22;

        // The math operation here is applied to each element of
        // the unsigned char vector and the final result is applied 
        // back to the output image
  output[x + y *width] = convert_uchar4(hypot(Gx, Gy)/(float4)(2));
  }  
}</pre></div><p>An astute reader <a id="id485" class="indexterm"/>will probably figure out by reading the code, that<a id="id486" class="indexterm"/> the derived values for <code class="literal">Gx</code> and <code class="literal">Gy</code> should have been as follows:</p><div><pre class="programlisting">Gx = i00 + (float4)(2) * i10 + i20 - i02  - (float4)(2) * i12 - i22+ 0*i01+0*i11+0*i21;
Gy = i00 - i20  + (float4)(2)*i01 - (float4)(2)*i21 + i02 - i22+ 0*i10+0*i11+0*i12;</pre></div><p>But since we know their values will be zero, there is no need for us to include the computation inside it. Although we did, it's really a minor optimization. It shaved off some GPU processing cycles!</p><p>As before, the compilation steps are similar to that in <code class="literal">Ch6/sobelfilter/SobelFilter.c</code> with the following command:</p><div><pre class="programlisting">
<strong>gcc -std=c99 -Wall -DUNIX -g -DDEBUG -DAPPLE -arch i386 -o SobelFilter SobelFilter.c -framework OpenCL</strong>
</pre></div><p>To execute the program, simply execute the executable file (<code class="literal">SobelFilter</code>) on the <code class="literal">Ch6/sobelfilter</code> directory, and an output image file named <code class="literal">OutputImage.bmp</code> would be presented (it's the output of reading in <code class="literal">InputImage.bmp</code> and conducting the convolution process against it).</p><p>The net effect is that<a id="id487" class="indexterm"/> the output contains an image that outlines <a id="id488" class="indexterm"/>the edges of the original input image, and you can even refer to the picture images in the <em>How it works…</em> section of this recipe to see how these two images are different from one another.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec114"/>How it works…</h2></div></div></div><p>At first, we create a representation of a pixel to represent each of the channels in the RGBA fashion. That structure is given a simple name, <code class="literal">uchar4</code>, where it consists of four unsigned char data types which will correctly represent each color's range from [0..255] or [0x00..0xFF], since that's how each color's range is defined by convention.</p><p>We omit the description of the mechanism behind pulling the pixel information from the input image to how we construct the final in-memory representation of the image. Interested readers can search on the Internet regarding the Windows BMP format to understand how we parse the image data or read the source code in the <code class="literal">bmp.h</code> file via the <code class="literal">load</code> function, and we write out the image using the <code class="literal">write</code> function.</p><p>Skipping the OpenCL device memory allocation, since that by now is standard fare we arrived quickly at the portion where we look at how the kernel processes each pixel of the input data.</p><p>Before we do that, let's quickly recall from the kernel launching code how many global work-items have been assigned and whether the work-group composition is like:</p><div><pre class="programlisting">clEnqueueNDRangeKernel(command, queue, 2, NULL, globalThreads, localThreads, 0, NULL, NULL);</pre></div><p><code class="literal">localThreads</code> is configured to have work-groups of sizes {256,1}, work-items processing a portion of the input 2D image data array.</p><p>When the image is loaded into the device memory, the image is processed in blocks. Each block has a number of work-items or threads if you process the image. Each work-item proceeds the next to perform the convolution process on the center of the pixel and also on its eight neighbors. The resultant value generated by each work-item will be outputed as pixel value into the device memory. Pictorially, the following diagram illustrates what a typical work-item will perform.</p><div><div><h3 class="title"><a id="tip20"/>Tip</h3><p>You need to watch out and that is we actually used the data type conversion function, <code class="literal">convert_float4</code> to apply our unsigned char data values encapsulated within each pixel, which effectively widens the data type so that it doesn't overflow when the Sobel operator is applied on them.</p></div></div><p>Finally, once we have <a id="id489" class="indexterm"/>the masked the values we need to compute the<a id="id490" class="indexterm"/> magnitude of this gradient and the standard way of computing that is to apply <img src="img/4520OT_06_12a.jpg" alt="How it works…"/> where <strong>Gx</strong> = <img src="img/4520OT_06_11.jpg" alt="How it works…"/> and <strong>Gy</strong> = <img src="img/4520OT_06_10.jpg" alt="How it works…"/>.</p><div><img src="img/4520OT_06_05.jpg" alt="How it works…"/></div><p>Whether this algorithm works, the only way is to check it through an image. The following is the side-by-side comparison, where the first image is before the Sobel operator is applied and the <a id="id491" class="indexterm"/>second one is after it's <a id="id492" class="indexterm"/>being applied.</p><div><img src="img/4520OT_06_06.jpg" alt="How it works…"/></div><div><img src="img/4520OT_06_07.jpg" alt="How it works…"/></div><p>However, there is another nice optimization which we could have done, and it would have helped if we understood that a 3 X 3 convolution kernel (for example, the Sobel operator) is actually equivalent to the product of two vectors. This realization is behind the optimization algorithm also known as separable convolution.</p><p>Technically, a<a id="id493" class="indexterm"/> two-dimensional filter is considered to be<a id="id494" class="indexterm"/> separable if it can be expressed as an outer product of two vectors. Considering the Sobel operator here, we can actually write <img src="img/4520OT_06_08.jpg" alt="How it works…"/> and <img src="img/4520OT_06_09.jpg" alt="How it works…"/>.</p><div><div><h3 class="title"><a id="tip300"/>Tip</h3><p>The superscript T is the transpose of a row vector, which is equivalent to the column-vector and vice versa. Note that convolution is itself associative, so it doesn't really matter in which way you multiply the vectors against the input image matrix.</p></div></div><p>Why is this important? The main reason is because we actually save processing cycles by using this separable convolution kernel. Let's imagine we have a X-by-Y image and a convolution kernel of M-by-N. Using the original method, we would have conducted XYMN multiples and adds while using the separable convolution technique, we would have actually done XY (M + N) multiples and adds. Theoretically speaking, applying this to our 3-by-3 convolution kernel we would have increased our performance to 50 percent or 1.5 times and when we use a 9-by-9 convolution kernel, we would have increased our performance to 81 / 18 = 4.5 or 450 percent.</p><p>Next, we are going to talk about how you can profile your algorithms and their runtimes so that you can make your algorithms not only run faster, but also deepen your understanding of how the algorithm works and more often than not, help the developer develop a better intuition on how to make better use of the OpenCL device's capabilities.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec50"/>Understanding profiling in OpenCL</h1></div></div></div><p>Profiling <a id="id495" class="indexterm"/>is a relatively simple operation from the perspective of an OpenCL developer, since it basically means that he/she wishes to measure how long a particular operation took. This is important because during any software development, users of the system would often specify the latencies which are considered acceptable, and as you develop bigger and more complex systems, profiling the application becomes important in helping you understand the bottlenecks of the application. The profiling we are going to take is a look done programmatically by the developer to explicitly measure the pockets of code. Of course, there is another class of profilers which profiles your OpenCL operations on a deeper level with various breakdowns on the running times measured and displayed, but that is out of the scope of the book. But we encourage readers to download the profilers from AMD and Intel to check them out.</p><div><div><h3 class="title"><a id="note31"/>Note</h3><p>While writing this book, AMD has made its OpenCL profiler and a generally available debugger named CodeXL found at <a class="ulink" href="http://developer.amd.com/tools-and-sdks/heterogeneous-computing/codexl/">http://developer.amd.com/tools-and-sdks/heterogeneous-computing/codexl/</a>. Intel has a similar package offered separately and you can refer to the following URL for more details:</p><p><a class="ulink" href="http://software.intel.com/en-us/vcsource/tools/opencl-sdk-2013">http://software.intel.com/en-us/vcsource/tools/opencl-sdk-2013</a>. As for NVIDIA GPGPUs, you can only use the APIs provided by OpenCL.</p></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec115"/>Getting ready</h2></div></div></div><p>The two operations that <a id="id496" class="indexterm"/>OpenCL allows the developer to have such insight into their <a id="id497" class="indexterm"/>runtimes are data transfer operations and kernel execution operations; the times are all measured in nanoseconds.</p><div><div><h3 class="title"><a id="note32"/>Note</h3><p>Since all devices cannot resolve to a nanosecond, it's important to determine what is the level of resolution, and you can know this by passing the <code class="literal">CL_DEVICE_PROFILING_TIMER_RESOLUTION</code> flag to <code class="literal">clGetDeviceInfo</code> for the appropriate device ID.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec116"/>How to do it…</h2></div></div></div><p>All you have to do is to pass the <code class="literal">CL_QUEUE_PROFILING_ENABLE</code> flag as part of the <code class="literal">properties</code> argument, when you create the command queue via <code class="literal">clCreateCommandQueue</code>. The API looks like this:</p><div><pre class="programlisting">cl_command_queue 
clCreateCommandQueue(cl_context context,
                     cl_device_id device,
                     cl_command_queue_properties properties, cl_int* error_ret);</pre></div><p>Once the profiling is enabled, the next thing you need to do is to inject OpenCL events into areas of the code, where you want to know how the runtimes fare. To achieve this, you need to create a <code class="literal">cl_event</code> variable for the regions of code you wish to monitor and associate this variable with one of the following APIs:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Data transfer operations:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">clEnqueue{Read|Write|Map}Buffer</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">clEnqueue{Read|Write|Map}BufferRect</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">clEnqueue{Read|Write|Map}Image</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">clEnqueueUnmapMemObject</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">clEnqueuCopyBuffer</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">clEnqueueCopyBufferRect</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">clEnqueueCopyImage</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">clEnqueueCopyImageToBuffer</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">clEnqueueCopyBufferToImage</code></li></ul></div></li><li class="listitem" style="list-style-type: disc">Kernel operations:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">clEnqueueNDRangeKernel</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">clEnqueueTask</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">clEnqueueNativeTask</code></li></ul></div></li></ul></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec117"/>How it works…</h2></div></div></div><p>The way to obtain <a id="id498" class="indexterm"/>the runtimes <a id="id499" class="indexterm"/>for these operations is to invoke the <code class="literal">clGetEventProfilingInfo</code> API, passing in one of these flags: <code class="literal">CL_PROFILING_COMMAND_QUEUED</code>, <code class="literal">CL_PROFILING_COMMAND_SUBMIT</code>, <code class="literal">CL_PROFILING_COMMAND_START</code>, or <code class="literal">CL_PROFILING_COMMAND_END</code>. The API looks like this:</p><div><pre class="programlisting">cl_int
clGetEventProfilingInfo(cl_event event,
                        cl_profiling_info param_name,               
                        size_t param_value_size, 
                        void* param_value,
                        size_t* param_value_size_ret);</pre></div><p>To obtain the time spent by the command in the queue, you invoke <code class="literal">clGetEventProfilingInfo</code> with <code class="literal">CL_PROFILING_COMMAND_SUBMIT</code> once, and at the end of the code region invoke <code class="literal">clGetEventProfilingInfo</code> with <code class="literal">CL_PROFILING_COMMAND_QUEUED</code> again to get the difference in time.</p><p>To obtain the duration that the command took to execute, invoke <code class="literal">clGetEventProfilingInfo</code> once with <code class="literal">CL_PROFILING_COMMAND_START</code> and invoke the same API with <code class="literal">CL_PROFILING_COMMAND_END</code>, from the difference in the runtimes you will obtain the value.</p><p>The following is a small code snippet which illustrates the basic mechanism:</p><div><pre class="programlisting">
<strong>cl_event readEvt;</strong>
<strong>cl_ulong startTime;</strong>
<strong>cl_ulong endTime;</strong>
<strong>cl_ulong timeToRead;</strong>
<strong>cl_command_queue queue = clCreateCommandQueue(context, device, CL_QUEUE_PROFILING_ENABLE, NULL);</strong>
<strong>clEnqueueReadBuffer(queue, some_buffer, TRUE, 0, sizeof(data), data,0, NULL, &amp;readEvt);</strong>
<strong>clGetEventProfilingInfo(readEvt, CL_PROFILING_COMMAND_START,sizeof(startTime),&amp;startTime, NULL);</strong>
<strong>clGetEventProfilingInfo(readEvt, CL_PROFILING_COMMAND_END,sizeof(endTime),&amp;endTime, NULL);</strong>
<strong>timeToRead = endTime – startTim;</strong>
</pre></div></div></div></body></html>