- en: '*Chapter 3*: Reading and Writing Files'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第三章*：读取和写入文件'
- en: In the previous chapter, we looked at how to install various tools, such as
    NiFi, Airflow, PostgreSQL, and Elasticsearch. In this chapter, you will be learning
    how to use these tools. One of the most basic tasks in data engineering is moving
    data from a text file to a database. In this chapter, you will read data from
    and write data to several different text-based formats, such as CSV and JSON.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了如何安装各种工具，例如 NiFi、Airflow、PostgreSQL 和 Elasticsearch。在本章中，你将学习如何使用这些工具。数据工程中最基本的一项任务是将数据从文本文件移动到数据库。在本章中，你将读取来自不同基于文本格式的数据，例如
    CSV 和 JSON，并将数据写入这些格式。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Reading and writing files in Python
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 中读取和写入文件
- en: Processing files in Airflow
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Airflow 中处理文件
- en: NiFi processors for handling files
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NiFi 处理器用于处理文件
- en: Reading and writing data to databases in Python
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 中读取和写入数据库中的数据
- en: Databases in Airflow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Airflow 中的数据库
- en: Database processors in NiFi
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NiFi 中的数据库处理器
- en: Writing and reading files in Python
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Python 中编写和读取文件
- en: The title of this section may sound strange as you are probably used to seeing
    it written as reading and writing, but in this section, you will write data to
    files first, then read it. By writing it, you will understand the structure of
    the data and you will know what it is you are trying to read.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小节的标题可能听起来有些奇怪，因为你可能习惯于看到它被写成读取和写入，但在这个小节中，你将首先写入数据到文件，然后读取它。通过写入，你将了解数据的结构，并且你会知道你试图读取的内容是什么。
- en: To write data, you will use a library named `faker`. `faker` allows you to easily
    create fake data for common fields. You can generate an address by simply calling
    `address()`, or a female name using `name_female()`. This will simplify the creation
    of fake data while at the same time making it more realistic.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要写入数据，你将使用一个名为 `faker` 的库。`faker` 允许你轻松地为常见字段创建假数据。你可以通过调用 `address()` 生成地址，或者使用
    `name_female()` 生成女性名字。这将简化假数据的创建，同时使其更加真实。
- en: 'To install `faker`, you can use `pip`:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 `faker`，你可以使用 `pip`：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With `faker` now installed, you are ready to start writing files. The next section
    will start with CSV files.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经安装了 `faker`，你可以开始编写文件了。下一节将从 CSV 文件开始。
- en: Writing and reading CSVs
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写和读取 CSV 文件
- en: The most common file type you will encounter is **Comma-Separated Values** (**CSV**).
    A CSV is a file made up of fields separated by commas. Because commas are fairly
    common in text, you need to be able to handle them in CSV files. This can be accomplished
    by using escape characters, usually a pair of quotes around text strings that
    could contain a comma that is not used to signify a new field. These quotes are
    called escape characters. The Python standard library for handling CSVs simplifies
    the process of handling CSV data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你最常遇到的文件类型是 **逗号分隔值**（**CSV**）。CSV 是由逗号分隔的字段组成的文件。因为逗号在文本中相当常见，所以你需要能够在 CSV
    文件中处理它们。这可以通过使用转义字符来实现，通常是在可能包含逗号的文本字符串周围使用一对引号，而这个逗号不是用来表示新字段的。这些引号被称为转义字符。Python
    标准库简化了处理 CSV 数据的过程。
- en: Writing CSVs using the Python CSV Library
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Python CSV 库编写 CSV 文件
- en: 'To write a CSV with the CSV library, you need to use the following steps:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CSV 库编写 CSV 文件时，你需要遵循以下步骤：
- en: 'Open a file in writing mode. To open a file, you need to specify a filename
    and a mode. The mode for writing is `w`, but you can also open a file for reading
    with `r`, appending with `a`, or reading and writing with `r+`. Lastly, if you
    are handling files that are not text, you can add `b`, for binary mode, to any
    of the preceding modes to write in bytes; for example, `wb` will allow you to
    write in bytes:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以写入模式打开文件。要打开文件，你需要指定一个文件名和模式。写入模式为 `w`，但你也可以使用 `r` 打开文件进行读取，使用 `a` 进行追加，或者使用
    `r+` 进行读写。最后，如果你正在处理非文本文件，你可以在任何前面提到的模式中添加 `b`，以二进制模式写入；例如，`wb` 将允许你以字节形式写入：
- en: '[PRE1]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create `CSV_writer`. At a minimum, you must specify a file to write to, but
    you can also pass additional parameters, such as a dialect. A dialect can be a
    defined CSV type, such as Excel, or it can be options such as the delimiter to
    use or the level of quoting. The defaults are usually what you will need; for
    example, the delimiter defaults to a comma (it is a CSV writer after all) and
    quoting defaults to `QUOTE_MINIMAL`, which will only add quotes when there are
    special characters or the delimiter within a field. So, you can create the writer
    as shown:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`CSV_writer`。至少，你必须指定一个要写入的文件，但你也可以传递额外的参数，例如方言。方言可以是定义好的CSV类型，如Excel，或者是一些选项，如要使用的分隔符或引号级别。默认值通常是您所需要的；例如，分隔符默认为逗号（毕竞这是一个CSV写入器），引号默认为`QUOTE_MINIMAL`，这意味着只有当字段中有特殊字符或分隔符时才会添加引号。因此，你可以像下面这样创建写入器：
- en: '[PRE2]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Include a header. You might be able to remember what the fields are in your
    CSV, but it is best to include a header. Writing a header is the same as writing
    any other row: define the values, then you will use `writerow()`, as shown:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 包含标题。你可能能够记住CSV中的字段，但最好还是包含标题。写入标题与写入任何其他行相同：定义值，然后使用`writerow()`，如下所示：
- en: '[PRE3]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Write the data to a file. You can now write a data row by using `writerow(0)`
    and passing some data, as shown:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据写入文件。现在，你可以通过使用`writerow(0)`并传递一些数据来写入一行数据，如下所示：
- en: '[PRE4]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, if you look in the directory, you will have a CSV file named `myCSV.CSV`
    and the contents should look as in the following screenshot:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你查看目录，你将有一个名为`myCSV.CSV`的CSV文件，其内容应如下截图所示：
- en: '![Figure 3.1 – The contents of mycsv.csv'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.1 – mycsv.csv的内容'
- en: '](img/B15739_03_01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B15739_03_01.jpg](img/B15739_03_01.jpg)'
- en: Figure 3.1 – The contents of mycsv.csv
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – mycsv.csv的内容
- en: Notice that when you used `cat` to view the file, the newlines were added. By
    default, `CSV_writer` uses a return and a newline (`'\r\n'`).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当你使用`cat`查看文件时，新行被添加了。默认情况下，`CSV_writer`使用回车和换行符（`'\r\n'`）。
- en: 'The preceding example was very basic. However, if you are trying to write a
    lot of data, you would most likely want to loop through some condition or iterate
    through existing data. In the following example, you will use `Faker` to generate
    1,000 records:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例非常基础。然而，如果你试图写入大量数据，你很可能希望通过某种条件循环或遍历现有数据。在以下示例中，你将使用`Faker`生成1,000条记录：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You should now have a `data.CSV` file with 1,000 rows of names and ages.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该有一个包含1,000行姓名和年龄的`data.CSV`文件。
- en: Now that you have written a CSV, the next section will walk you through reading
    it using Python.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经写入了CSV，下一节将指导你使用Python读取它。
- en: Reading CSVs
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 读取CSV
- en: 'Reading a CSV is somewhat similar to writing one. The same steps are followed
    with slight modifications:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 读取CSV与写入CSV有些类似。遵循相同的步骤，但略有修改：
- en: 'Open a file using `with`. Using `with` has some additional benefits, but for
    now, the one you will reap is not having to use `close()` on the file. If you
    do not specify a mode, `open` defaults to read (`r`). After `open`, you will need
    to specify what to refer to the file as; in this case, you will open the `data.CSV`
    file and refer to it as `f`:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`with`打开文件。使用`with`有一些额外的优势，但就目前而言，你将获得的好处是不必在文件上使用`close()`。如果你没有指定模式，`open`默认为读取模式（`r`）。在`open`之后，你需要指定要引用的文件名；在这种情况下，你将打开`data.CSV`文件并将其命名为`f`：
- en: '[PRE6]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Create the reader. Instead of just using `reader()`, you will use `DictReader()`.
    By using the dictionary reader, you will be able to call fields in the data by
    name instead of position. For example, instead of calling the first item in a
    row as `row[0]`, you can now call it as `row[''name'']`. Just like the writer,
    the defaults are usually sufficient, and you will only need to specify a file
    to read. The following code opens `data.CSV` using the `f` variable name:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建读取器。你将使用`DictReader()`而不是仅仅使用`reader()`。通过使用字典读取器，你将能够通过名称而不是位置来调用数据中的字段。例如，你不再需要将行中的第一个项目称为`row[0]`，现在你可以将其称为`row['name']`。就像写入器一样，默认值通常足够，你只需要指定一个要读取的文件。以下代码使用`f`变量名打开`data.CSV`：
- en: '[PRE7]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Grab the headers by reading a single line with `next()`:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用`next()`读取一行来获取标题：
- en: '[PRE8]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, you can iterate through the rest of the rows using the following:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你可以使用以下方式遍历其余的行：
- en: '[PRE9]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Lastly, you can print the names using the following:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你可以使用以下方式打印名称：
- en: '[PRE10]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You should only see the 1,000 names scroll by. Now you have a Python dictionary
    that you can manipulate any way you need. There is another way to handle CSV data
    in Python and that requires `pandas`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Reading and writing CSVs using pandas DataFrames
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`pandas` DataFrames are a powerful tool not only for reading and writing data
    but also for the querying and manipulation of data. It does require a larger overhead
    than the built-in CSV library, but there are times when it may be worth the trade-off.
    You may already have `pandas` installed, depending on your Python environment,
    but if you do not, you can install it with the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You can think of a `pandas` DataFrame as an Excel sheet or a table. You will
    have rows, columns, and an index. To load CSV data into a DataFrame, the following
    steps must be followed:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pandas` (usually as `pd`):'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, read the file using `read_csv()`. The `read_csv()` method takes several
    optional parameters, and one required parameter – the file or file-like buffer.
    The two optional parameters that may be of interest are `header`, which by defaultattempts
    to infer the headers. If you set `header=0`, then you can use the `names` parameter
    with an array of column names. If you have a large file and you just want to look
    at a piece of it, you can use `nrows` to specify the number of rows to read, so
    `nrows=100` means it will only read 100 rows for the data. In the following snippet,
    you will load the entire file using the defaults:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s now look at the first 10 records by using the following:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Because you used `Faker` to generate data, you will have the same schema as
    in the following screenshot, but will have different values:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Reading a CSV into a DataFrame and printing head()'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15739_03_02.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 – Reading a CSV into a DataFrame and printing head()
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create a DataFrame in Python with the following steps:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a dictionary of data. A dictionary is a data structure that stores data
    as a key:value pair. The value can be of any Python data type – for example, an
    array. Dictionaries have methods for finding `keys()`, `values()`, and `items()`.
    They also allow you to find the value of a key by using the key name in brackets
    – for example, `dictionary[''key'']` will return the value for that key:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Pass the data to the DataFrame:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The columns are specified as the keys in the dictionary. Now that you have
    a DataFrame, you can write the contents to a CSV using `to_csv()` and passing
    a filename. In the example, we did not set an index, which means the row names
    will be a number from 0 to *n*, where *n* is the length of the DataFrame. When
    you export to CSV, these values will be written to the file, but the column name
    will be blank. So, in a case where you do not need the row names or index to be
    written to the file, pass the `index` parameter to `to_csv()`, as shown:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You will now have a CSV file with the contents of the DataFrame. How we can
    use the contents of this DataFrame for executing SQL queries will be covered in
    the next chapter. They will become an important tool in your toolbox and the rest
    of the book will lean on them heavily.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你将有一个包含 DataFrame 内容的 CSV 文件。我们将在下一章中介绍如何使用此 DataFrame 的内容来执行 SQL 查询。它们将成为你工具箱中的重要工具，本书的其余部分将大量依赖它们。
- en: For now, let's move on to the next section, where you will learn about another
    common text format – **JSON**.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续下一节，你将学习另一种常见的文本格式 - **JSON**。
- en: Writing JSON with Python
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Python 写入 JSON
- en: Another common data format you will probably deal with is `JSON–JSON`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会处理另一种常见的数据格式 - `JSON–JSON`。
- en: 'To write JSON using Python and the standard library, the following steps need
    to be observed:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Python 和标准库写入 JSON，需要遵循以下步骤：
- en: 'Import the library and open the file you will write to. You also create the
    `Faker` object:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库并打开你要写入的文件。你还需要创建 `Faker` 对象：
- en: '[PRE18]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We will create 1,000 records, just as we did in the CSV example, so you will
    need to create a dictionary to hold the data. As mentioned earlier, the value
    of a key can be any Python data type – including an array of values. After creating
    the dictionary to hold the records, add a `''records''` key and initialize it
    with a blank array, as shown:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将创建 1,000 条记录，就像在 CSV 示例中做的那样，因此你需要创建一个字典来保存数据。如前所述，键的值可以是任何 Python 数据类型 -
    包括值的数组。在创建保存记录的字典后，添加一个 `'records'` 键，并用一个空数组初始化，如下所示：
- en: '[PRE19]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To write the records, you use `Faker` to create a dictionary, then append it
    to the array:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要写入记录，你使用 `Faker` 创建一个字典，然后将其追加到数组中：
- en: '[PRE20]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Lastly, to write the JSON to a file, use the `JSON.dump()` method. Pass the
    data that you want to write and a file to write to:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，要将 JSON 写入文件，使用 `JSON.dump()` 方法。传递要写入的数据和要写入的文件：
- en: '[PRE21]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You now have a `data.JSON` file that has an array with 1,000 records. You can
    read this file by taking the following steps:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你有一个包含 1,000 条记录的数组 `data.JSON` 文件。你可以通过以下步骤读取此文件：
- en: 'Open the file using the following:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下方式打开文件：
- en: '[PRE22]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Use `JSON.load()` and pass the file reference to the method:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `JSON.load()` 并将文件引用传递给该方法：
- en: '[PRE23]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Inspect the json by looking at the first record using the following:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过查看第一条记录来检查 json，使用以下方式：
- en: '[PRE24]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Or just use the name:'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者直接使用文件名：
- en: '[PRE25]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: When you `loads` and `dumps` are different than `load` and `dump`. Both are
    valid methods of the JSON library. The difference is that `loads` and `dumps`
    are for strings – they do not serialize the JSON.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用 `loads` 和 `dumps` 时，与 `load` 和 `dump` 不同。它们都是 JSON 库的有效方法。区别在于 `loads`
    和 `dumps` 用于字符串 - 它们不会序列化 JSON。
- en: pandas DataFrames
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: pandas DataFrame
- en: Reading and writing JSON with DataFrames is similar to what we did with CSV.
    The only difference is that you change `to_csv` to `to_json()` and `read_csv()`
    to `read_json()`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DataFrame 读取和写入 JSON 与我们处理 CSV 类似。唯一的区别是将 `to_csv` 改为 `to_json()`，将 `read_csv()`
    改为 `read_json()`。
- en: 'If you have a clean, well-formatted JSON file, you can read it using the following
    code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个干净、格式良好的 JSON 文件，你可以使用以下代码读取它：
- en: '[PRE26]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In the case of the `data.JSON` file, the records are nested in a `records`
    dictionary. So, loading the JSON is not as straightforward as the preceding code.
    You will need a few extra steps, which are as follows. To load JSON data from
    the file, do the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `data.JSON` 文件的情况下，记录嵌套在一个 `records` 字典中。因此，加载 JSON 不像前面的代码那样直接。你需要额外的几个步骤，如下所示。要从文件加载
    JSON 数据，请执行以下操作：
- en: 'Use the `pandas` `JSON` library:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `pandas` 的 `JSON` 库：
- en: '[PRE27]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Open the file and load it with the `pandas` version of `JSON.loads()`:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `pandas` 版本的 `JSON.loads()` 打开文件并加载：
- en: '[PRE28]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'To create the DataFrame, you need to normalize the JSON. Normalizing is how
    you can flatten the JSON to fit in a table. In this case, you want to grab the
    individual JSON records held in the `records` dictionary. Pass that path – `records`
    – to the `record_path` parameter of `json_normalize()`:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建 DataFrame，你需要规范化 JSON。规范化是将 JSON 展平以适应表格的过程。在这种情况下，你想要获取存储在 `records` 字典中的单个
    JSON 记录。将此路径 - `records` - 传递给 `json_normalize()` 的 `record_path` 参数：
- en: '[PRE29]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You will now have a DataFrame that contains all the records in the `data.JSON`
    file. You can now write them back to JSON, or CSV, using DataFrames.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你将有一个包含 `data.JSON` 文件中所有记录的 DataFrame。你现在可以使用 DataFrame 将它们写回 JSON 或 CSV。
- en: 'When writing to JSON, you can pass the `orient` parameter, which determines
    the format of the JSON that is returned. The default is columns, which for the
    `data.JSON` file you created in the previous section would look like the following
    data:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在写入 JSON 时，你可以传递 `orient` 参数，该参数确定返回的 JSON 格式。默认为列，对于你在上一节中创建的 `data.JSON` 文件，它看起来如下所示的数据：
- en: '[PRE30]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'By changing the `orient` value to `records`, you get each row as a record in
    the JSON, as shown:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 `orient` 值更改为 `records`，你将得到 JSON 中的每一行作为一个记录，如下所示：
- en: '[PRE31]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: I find that working with JSON that is oriented around `records` makes processing
    it in tools such as Airflow much easier than JSON in other formats, such as split,
    index, columns, values, or table. Now that you know how to handle CSV and JSON
    files in Python, it is time to learn how to combine tasks into a data pipeline
    using Airflow and NiFi. In the next section, you will learn how to build pipelines
    in Apache Airflow.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现，围绕 `records` 定向的 JSON 在 Airflow 等工具中处理起来比其他格式（如 split、index、columns、values
    或 table）的 JSON 要容易得多。现在你已知道如何在 Python 中处理 CSV 和 JSON 文件，是时候学习如何使用 Airflow 和 NiFi
    将任务组合成数据管道了。在下一节中，你将学习如何在 Apache Airflow 中构建管道。
- en: Building data pipelines in Apache Airflow
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 Apache Airflow 中的数据管道
- en: Apache Airflow uses Python functions, as well as Bash or other operators, to
    create tasks that can be combined into a **Directed Acyclic Graph** (**DAG**)
    – meaning each task moves in one direction when completed. Airflow allows you
    to combine Python functions to create tasks. You can specify the order in which
    the tasks will run, and which tasks depend on others. This order and dependency
    are what make it a DAG. Then, you can schedule your DAG in Airflow to specify
    when, and how frequently, your DAG should run. Using the Airflow GUI, you can
    monitor and manage your DAG. By using what you learned in the preceding sections,
    you will now make a data pipeline in Airflow.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Airflow 使用 Python 函数以及 Bash 或其他操作符来创建可以组合成 **有向无环图**（**DAG**）的任务——这意味着每个任务在完成时都朝一个方向移动。Airflow
    允许你组合 Python 函数来创建任务。你可以指定任务的运行顺序以及哪些任务依赖于其他任务。这种顺序和依赖性使其成为 DAG。然后，你可以在 Airflow
    中安排你的 DAG 以指定 DAG 应该何时以及多久运行一次。使用 Airflow GUI，你可以监控和管理你的 DAG。通过使用前面章节中学到的知识，你现在将在
    Airflow 中构建数据管道。
- en: Building a CSV to a JSON data pipeline
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 构建 CSV 到 JSON 数据管道
- en: 'Starting with a simple DAG will help you understand how Airflow works and will
    help you to add more functions to build a better data pipeline. The DAG you build
    will print out a message using Bash, then read the CSV and print a list of all
    the names. The following steps will walk you through building the data pipeline:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个简单的 DAG 开始将帮助你理解 Airflow 的工作原理，并有助于你添加更多函数来构建更好的数据管道。你构建的 DAG 将使用 Bash 打印一条消息，然后读取
    CSV 并打印所有名称的列表。以下步骤将指导你构建数据管道：
- en: 'Open a new file using the Python IDE or any text editor. Import the required
    libraries, as shown:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Python IDE 或任何文本编辑器打开一个新文件。导入所需的库，如下所示：
- en: '[PRE32]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The first two imports bring in `datetime` and `timedelta`. These libraries are
    used for scheduling the DAG. The three Airflow imports bring in the required libraries
    for building the DAG and using the Bash and Python operators. These are the operators
    you will use to build tasks. Lastly, you import `pandas` so that you can easily
    convert between CSV and JSON.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前两个导入引入了 `datetime` 和 `timedelta`。这些库用于安排 DAG。三个 Airflow 导入引入了构建 DAG 和使用 Bash
    和 Python 操作符所需的库。这些是你将用于构建任务的操作符。最后，你导入 `pandas` 以便你可以轻松地在 CSV 和 JSON 之间进行转换。
- en: 'Next, write a function to read a CSV file and print out the names. By combining
    the steps for reading CSV data and writing JSON data from the previous sections,
    you can create a function that reads in the `data.CSV` file and writes it out
    to JSON, as shown in the following code:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，编写一个函数来读取 CSV 文件并打印出名称。通过结合之前章节中读取 CSV 数据和写入 JSON 数据的步骤，你可以创建一个函数，该函数读取
    `data.CSV` 文件并将其写入 JSON，如下面的代码所示：
- en: '[PRE33]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This function opens the file in a DataFrame. Then, it iterates through the rows,
    printing only the names, and lastly, it writes the CSV to a JSON file.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此函数在 DataFrame 中打开文件。然后，它遍历行，仅打印名称，最后将 CSV 写入 JSON 文件。
- en: 'Now, you need to implement the Airflow portion of the pipeline. Specify the
    arguments that will be passed to `DAG()`. In this book, you will use a minimal
    set of parameters. The arguments in this example assign an owner, a start date,
    the number of retries in the event of a failure, and how long to wait before retrying.
    They are shown in the following dictionary:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您需要实现管道的 Airflow 部分。指定将传递给 `DAG()` 的参数。在这本书中，您将使用一组最小的参数。本例中的参数分配了一个所有者、开始日期、失败时的重试次数以及重试前的等待时间。它们在以下字典中显示：
- en: '[PRE34]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, pass the arguments dictionary to `DAG()`. Create the DAG ID, which is
    set to `MyCSVDAG`, the dictionary of arguments (the `default_args` variable in
    the preceding code), and the schedule interval (how often to run the data pipeline).
    The schedule interval can be set using `timedelts`, or you can use a crontab format
    with the following presets or crontab:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将参数字典传递给 `DAG()`。创建 DAG ID，设置为 `MyCSVDAG`，参数字典（前述代码中的 `default_args` 变量），以及调度间隔（数据管道的运行频率）。调度间隔可以使用
    `timedelta` 设置，或者您可以使用以下预设或 crontab 格式：
- en: a) `@once`
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) `@once`
- en: b) `@hourly` – `0 * * * *`
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `@hourly` – `0 * * * *`
- en: c) `@daily` – `0 0 * * *`
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) `@daily` – `0 0 * * *`
- en: d) `@weekly` – `0 0 * * 0`
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) `@weekly` – `0 0 * * 0`
- en: e) `@monthly` – `0 0 1 * *`
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) `@monthly` – `0 0 1 * *`
- en: f) `@yearly` – `0 0 1 1 *`
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: f) `@yearly` – `0 0 1 1 *`
- en: crontab uses the format minute, hour, day of month, month, day of week. The
    value for `@yearly` is `0 0 1 1 *`, which means run yearly on January 1 (`1 1`),
    at 0:0 (midnight), on any day of the week (`*`).
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: crontab 使用分钟、小时、月份中的天、月份、星期的格式。`@yearly` 的值为 `0 0 1 1 *`，这意味着在每年的 1 月 1 日（`1
    1`）午夜运行，在星期的任何一天（`*`）。
- en: '[PRE35]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'You can now create your tasks using operators. Airflow has several prebuilt
    operators. You can view them all in the documentation at [https://airflow.apache.org/docs/stable/_api/airflow/operators/index.html](https://airflow.apache.org/docs/stable/_api/airflow/operators/index.html).
    In this book, you will mostly use the Bash, Python, and Postgres operators. The
    operators allow you to remove most of the boilerplate code that is required to
    perform common tasks. In the following snippet, you will create two tasks using
    the Bash and Python operators:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您现在可以使用运算符来创建任务。Airflow 有几个预构建的运算符。您可以在文档中查看它们，链接为 [https://airflow.apache.org/docs/stable/_api/airflow/operators/index.html](https://airflow.apache.org/docs/stable/_api/airflow/operators/index.html)。在这本书中，您将主要使用
    Bash、Python 和 Postgres 运算符。这些运算符允许您移除执行常见任务所需的大部分样板代码。在下面的代码片段中，您将使用 Bash 和 Python
    运算符创建两个任务：
- en: '[PRE36]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The preceding snippet creates a task using the `BashOperator` operator, which
    prints out a statement to let you know it is running. This task serves no purpose
    other than to allow you to see how to connect multiple tasks together. The next
    task, `CSVJson`, uses the `PythonOperator` operator to call the function you defined
    at the beginning of the file (`CSVToJson()`). The function reads the `data.CSV`
    file and prints the `name` field in every row.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下代码片段使用 `BashOperator` 运算符创建了一个任务，该任务打印一条语句以通知您它正在运行。此任务除了允许您看到如何连接多个任务外，没有其他用途。下一个任务
    `CSVJson` 使用 `PythonOperator` 运算符调用文件开头定义的函数（`CSVToJson()`）。该函数读取 `data.CSV` 文件并打印每一行的
    `name` 字段。
- en: 'With the tasks defined, you now need to make the connections between the tasks.
    You can do this using the `set_upstream()` and `set_downstream()` methods or with
    the bit shift operator. By using upstream and downstream, you can make the graph
    go from the Bash task to the Python task using either of two snippets; the following
    is the first snippet:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义了任务后，您现在需要建立任务之间的连接。您可以使用 `set_upstream()` 和 `set_downstream()` 方法或使用位移运算符来完成此操作。通过使用上游和下游，您可以使用两个代码片段中的任何一个将图从
    Bash 任务转换为 Python 任务；以下是最初的代码片段：
- en: '[PRE37]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The following is the second snippet:'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面的代码片段是第二个：
- en: '[PRE38]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Using the bit shift operator, you can do the same; the following is the first
    option:'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用位移运算符，您可以执行相同的操作；以下是一个选项：
- en: '[PRE39]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The following is the second option:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是一个选项：
- en: '[PRE40]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Which method you choose is up to you; however, you should be consistent. In
    this book, you will see the bit shift operator setting the downstream.
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您选择哪种方法取决于您；然而，您应该保持一致。在这本书中，您将看到位移运算符设置下游。
- en: To use Airflow and Scheduler in the GUI, you first need to make a directory
    for your DAGs. During the install and configuration of Apache Airflow, in the
    previous chapter, we removed the samples and so the DAG directory is missing.
    If you look at `airflow.cfg`, you will see the setting for `dags_folder`. It is
    in the format of `$AIRFLOW_HOME/dags`. On my machine, `$AIRFLOW_HOME` is `home/paulcrickard/airflow`.
    This is the directory in which you will make the `dags folder.e` configuration
    file showing where the folder should be.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Copy your DAG code to the folder, then run the following:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Launch the GUI by opening your web browser and going to `http://localhost:8080`.
    You will see your DAG, as shown in the following screenshot:![Figure 3.3 – The
    main screen of the Airflow GUI showing MyCSVDAG
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15739_03_03.jpg)'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.3 – The main screen of the Airflow GUI showing MyCSVDAG
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on **DAGs** and select **Tree View**. Turn the DAG on, and then click
    **Go**. As the tasks start running, you will see the status of each run, as shown
    in the following screenshot:![Figure 3.4 – Multiple runs of the DAG and the status
    of each task
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15739_03_04.jpg)'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.4 – Multiple runs of the DAG and the status of each task
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You will see that there have been successful runs – each task ran and did so
    successfully. But there is no output or results. To see the results, click on
    one of the completed squares, as shown in the following screenshot:![Figure 3.5
    – Checking results by hovering over the completed task
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15739_03_05.jpg)'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.5 – Checking results by hovering over the completed task
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You will see a popup with several options. Click the **View Log** button, as
    shown in the following screenshot:![Figure 3.6 – Selecting View Log to see what
    happened in your task
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15739_03_06.jpg)'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.6 – Selecting View Log to see what happened in your task
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will be redirected to the log screen for the task. Looking at a successful
    run of the CSV task, you should see a log file similar to the one in the following
    screenshot:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Log of the Python task showing the names being printed'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15739_03_07.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.7 – Log of the Python task showing the names being printed
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have built a data pipeline with Python and ran it in Airflow.
    The result of your pipeline is a JSON file in your `dags` directory that was created
    from your `data.CSV` file. You can leave it running and it will continue to run
    at the specified `schedule_interval` time. Building more advanced pipelines will
    only require you to write more functions and connect them with the same process.
    But before you move on to more advanced techniques, you will need to learn how
    to use Apache NiFi to build data pipelines.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Handling files using NiFi processors
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, you learned how to read and write CSV and JSON files
    using Python. Reading files is such a common task that tools such as NiFi have
    prebuilt processors to handle it. In this section, you will learn how to handle
    files using NiFi processors.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，您学习了如何使用Python读取和写入CSV和JSON文件。读取文件是一项如此常见的任务，以至于像NiFi这样的工具已经预置了处理器来处理它。在本节中，您将学习如何使用NiFi处理器处理文件。
- en: Working with CSV in NiFi
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在NiFi中处理CSV
- en: Working with files in NiFi requires many more steps than you had to use when
    doing the same tasks in Python. There are benefits to using more steps and using
    Nifi, including that someone who does not know code can look at your data pipeline
    and understand what it is you are doing. You may even find it easier to remember
    what it is you were trying to do when you come back to your pipeline in the future.
    Also, changes to the data pipeline do not require refactoring a lot of code; rather,
    you can reorder processors via drag and drop.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在NiFi中处理文件比在Python中执行相同任务需要更多步骤。使用更多步骤和NiFi的好处包括，即使不知道代码的人也能查看您的数据管道并理解您正在做什么。您甚至可能会发现，当您将来回到管道时，更容易记住您当时试图做什么。此外，对数据管道的更改不需要重构大量代码；相反，您可以通过拖放重新排序处理器。
- en: In this section, you will create a data pipeline that reads in the `data.CSV`
    file you created in Python. It will run a query for people over the age of 40,
    then write out that record to a file.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将创建一个数据管道，该管道读取您在Python中创建的`data.CSV`文件。它将运行针对40岁以上人群的查询，然后将该记录写入文件。
- en: 'The result of this section is shown in the following screenshot:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的结果如图所示：
- en: '![Figure 3.8 – The data pipeline you will build in this section'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.8 – 本节中您将构建的数据管道'
- en: '](img/B15739_03_08.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15739_03_08.jpg)'
- en: Figure 3.8 – The data pipeline you will build in this section
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 – 本节中您将构建的数据管道
- en: The following sections will walk you through building a data pipeline.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节将指导您构建数据管道。
- en: Reading a file with GetFile
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用GetFile读取文件
- en: 'The first step in your data pipeline is to read in the `data.csv` file. To
    do that, take the following steps:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数据管道的第一步是读取`data.csv`文件。为此，请执行以下步骤：
- en: Drag the **Processor** icon from the NiFi toolbar to the canvas. Search for
    **GetFile** and then select it.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从NiFi工具栏拖动**处理器**图标到画布上。搜索**GetFile**并选择它。
- en: To configure the `GetFile` processor, you must specify the input directory.
    In the Python examples earlier in this chapter, I wrote the `data.CSV` file to
    my home directory, which is `home/paulcrickard`, so this is what I will use for
    the input directory.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要配置`GetFile`处理器，您必须指定输入目录。在本章前面的Python示例中，我将`data.CSV`文件写入我的家目录，即`home/paulcrickard`，因此我将使用它作为输入目录。
- en: Next, you will need to specify a file filter. This field allows the NiFi expression
    language, so you could use `[^\.].*\.CSV` – but for this example, you can just
    set the value to `data.csv`.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您需要指定一个文件过滤器。该字段允许使用NiFi表达式语言，因此您可以使用`[^\.].*\.CSV`——但在这个例子中，您可以将值设置为`data.csv`。
- en: 'Lastly, the **Keep Source File** property should be set to **true**. If you
    leave it as **false**, NiFi will delete the file once it has processed it. The
    complete configuration is shown in the following screenshot:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将**保留源文件**属性设置为**true**。如果您将其保留为**false**，NiFi在处理完文件后将其删除。完整的配置如图所示：
- en: '![Figure 3.9 – GetFile processor configuration'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.9 – GetFile处理器配置'
- en: '](img/B15739_03_09.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15739_03_09.jpg)'
- en: Figure 3.9 – GetFile processor configuration
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 – GetFile处理器配置
- en: Splitting records into distinct flowfiles
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将记录分割成独立的flowfiles
- en: 'Now you can pass the success relationship from the `GetFile` processor to the
    `SplitRecord` processor:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以将`GetFile`处理器到`SplitRecord`处理器的成功关系传递：
- en: The `SplitRecord` processor will allow you to separate each row into a separate
    flowfile. Drag and drop it on the canvas. You need to create a record reader and
    a record writer – NiFi already has several that you can configure. Click on the
    box next to **Record Reader** and select **Create new service**, as shown in the
    following screenshot:![Figure 3.10 – A list of available readers
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`SplitRecord`处理器将允许您将每一行分离成单独的flowfile。将其拖放到画布上。您需要创建一个记录读取器和记录写入器——NiFi已经预置了几个您可以选择配置的。点击**Record
    Reader**旁边的框，选择**创建新服务**，如图所示：![图3.10 – 可用读取器列表'
- en: '](img/B15739_03_10.jpg)'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15739_03_10.jpg)'
- en: Figure 3.10 – A list of available readers
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.10 – 可用读取器列表
- en: You will need to choose the type of reader. Select **CSVReader** from the dropdown.
    Select the dropdown for **Record Writer** and choose **CSVRecordSetWriter**:![Figure
    3.11 – A list of available readers
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15739_03_11.jpg)'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.11 – A list of available readers
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To configure **CSVReader** and **CSVRecordSetWriter**, click the arrow to the
    right of either one. This will open the **Files Configuration** window on the
    **CONTROLLER SERVICES** tab. You will see the screen shown in the following screenshot:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Configuring the reader and writer'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15739_03_12.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.12 – Configuring the reader and writer
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'The three icons to the right are as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: A gear for settings
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A lightning bolt for enabling and disabling the service (it is currently disabled)
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A trash can to delete it
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select the gear for **CSVReader**. The default configuration will work, except
    for the **Treat First Line as Header** property, which should be set to **true**.
    Click the gear for **CSVRecordSetWriter** and you can see the available properties.
    The defaults are sufficient in this example. Now, click the lightning bolt to
    enable the services.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Filtering records with the QueryRecord processor
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You now have a pipeline that will read a CSV and split the rows into individual
    flowfiles. Now you can process each row with the `QueryRecord` processor. This
    processor will allow you to execute a SQL command against the flowfile. The contents
    of the new flowfile will be the results of the SQL query. In this example, you
    will select all records where the age of the person is over 40:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Drag and drop the `QueryRecord` processor to the canvas. To query the flowfile,
    you need to specify a record reader and writer. You have already created one of
    each of these and they are available in the dropdown now. The **Include Zero Record
    FlowFiles** property should be set to **false**. This property will route records
    that do not meet the criteria to the same relationship (which you do not want).
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Lastly, click the plus sign in the right-hand corner and specify a property
    name in the popup. The name of the property will become a relationship when you
    create a connection from this processor. Name the property `over.40`. Then, the
    value popup will appear. This is where you will enter the SQL query. The results
    of the query will become the contents of the flowfile. Since you want the records
    of people over 40 years of age, the query is as follows:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The `Select` `*` query is what returns the entire flowfile. If you only wanted
    the name of the person and for the field to be `full_name`, you could run the
    following SQL:'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The point I am attempting to drive home here is that you can execute SQL and
    modify the flowfile to something other than the contents of the row – for example,
    running and aggregation and a group by.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Extracting data from a flowfile
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next processor will extract a value from the flowfile. That processer is
    `ExtractText`. The processor can be used on any flowfile containing text and uses
    regex to pull any data from the flowfile and assign it to an attribute.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个处理器将从 flowfile 中提取一个值。该处理器是 `ExtractText`。该处理器可用于包含文本的任何 flowfile，并使用正则表达式从
    flowfile 中提取任何数据并将其分配给属性。
- en: 'To configure the processor, click the plus sign and name the property. You
    will extract the person name from the flowfile, so you can name the property name.
    The value will be regex and should be as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置处理器，点击加号并命名属性。你将从 flowfile 中提取人名，因此你可以命名属性为 name。值将是正则表达式，应该如下所示：
- en: '[PRE44]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Without a full tutorial on regex, the preceding regex statement looks for a
    newline and a comma – `\n` and the comma at the end – and grabs the text inside.
    The parentheses say to take the text and return any characters that are not `^`
    or a comma. This regex returns the person's name. The flowfile contains a header
    of field names in CSV, a new line, followed by values in CSV. The `name` field
    is the first field on the second line – after the newline and before the first
    comma that specifies the end of the `name` field. This is why the regex looks
    for the text between the newline and the comma.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 没有完整的正则表达式教程，前面的正则表达式语句查找换行符和逗号 – `\n` 和结尾的逗号 – 并抓取其间的文本。括号表示取文本并返回任何不是 `^`
    或逗号的字符。这个正则表达式返回的是人的名字。Flowfile 包含 CSV 字段名的标题，一个换行符，然后是 CSV 中的值。`name` 字段是第二行的第一个字段
    – 在换行符之后，在指定 `name` 字段结束的第一个逗号之前。这就是为什么正则表达式查找换行符和逗号之间的文本。
- en: Modifying flowfile attributes
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修改 flowfile 属性
- en: Now that you have pulled out the person name as an attribute, you can use the
    `UpdateAttribute` processor to change the value of existing attributes. By using
    this processor, you will modify the default filename attribute that NiFi has provided
    the flowfile all the way at the beginning in the `GetFile` processor. Every flowfile
    will have the filename `data.CSV`. If you try to write the flowfiles out to CSV,
    they will all have the same name and will either overwrite or fail.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经将人名作为属性提取出来，你可以使用 `UpdateAttribute` 处理器来更改现有属性值。通过使用此处理器，你将修改 NiFi 在 `GetFile`
    处理器一开始提供的默认文件名属性。每个 flowfile 都将具有 `data.CSV` 的文件名。如果你尝试将 flowfiles 写入 CSV，它们都将具有相同的名称，并且可能会覆盖或失败。
- en: Click the plus sign in the configuration for the `UpdateAttribute` processor
    and name the new property filename. The value will use the NiFi Expression Language.
    In the Expression Language, you can grab the value of an attribute using the format
    `${attribute name}`. So, to use the name attribute, set the value to `${name}`.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `UpdateAttribute` 处理器的配置中点击加号并命名新属性文件名。值将使用 NiFi 表达式语言。在表达式语言中，你可以使用格式 `${attribute
    name}` 来获取属性的值。因此，要使用 name 属性，将值设置为 `${name}`。
- en: Saving a flowfile to disk
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 flowfile 保存到磁盘
- en: Using the `PutFile` processor, you can write the contents of a flowfile to disk.
    To configure the processor, you need to specify a directory in which to write
    the files. I will again use my home directory.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `PutFile` 处理器，你可以将 flowfile 的内容写入磁盘。要配置处理器，你需要指定一个写入文件的目录。我将继续使用我的家目录。
- en: Next, you can specify a conflict resolution strategy. By default, it will be
    set to fail, but it allows you to overwrite an existing file. If you were running
    this data pipeline, aggregating data every hour and writing the results to files,
    maybe you would set the property to overwrite so that the file always holds the
    most current data. By default, the flowfile will write to a file on disk with
    the property filename as the filename.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以指定一个冲突解决策略。默认情况下，它将被设置为失败，但它允许你覆盖现有文件。如果你运行这个数据管道，每小时聚合数据并将结果写入文件，你可能将属性设置为覆盖，以便文件始终包含最新的数据。默认情况下，flowfile
    将写入磁盘上的文件，文件名为属性文件名。
- en: Creating relationships between the processors
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在处理器之间建立关系
- en: 'The last step is to make connections for specified relationships between the
    processors:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是为处理器之间的指定关系建立连接：
- en: Grab the `GetFile` processor, drag the arrow to the `SplitRecord` processor,
    and check the relationship success in the popup.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 捕获 `GetFile` 处理器，将箭头拖到 `SplitRecord` 处理器，并在弹出窗口中检查关系成功。
- en: From the `SplitRecord` processor, make a connection to the `QueryRecord` processor
    and select the relationship splits. This means that any record that was split
    will be sent to the next processor.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `SplitRecord` 处理器连接到 `QueryRecord` 处理器，并选择关系拆分。这意味着任何被拆分的记录将被发送到下一个处理器。
- en: From `QueryRecord`, connect to the `ExtractText` processor. Notice the relationship
    you created is named `over.40`. If you added more SQL queries, you would get additional
    relationships. For this example, use the `over.40` relationship.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `QueryRecord` 连接到 `ExtractText` 处理器。注意你创建的关系被命名为 `over.40`。如果你添加了更多的 SQL 查询，你会得到更多的关系。对于这个例子，使用
    `over.40` 关系。
- en: Connect `ExtractText` to the `UpdateAttribute` processor for the relationship
    matched.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `ExtractText` 连接到匹配关系的 `UpdateAttribute` 处理器。
- en: Lastly, connect `UpdateAttribute` to the `PutFile` processor for the relationship
    success.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将 `UpdateAttribute` 连接到 `PutFile` 处理器以成功处理关系。
- en: The data pipeline is now complete. You can click on each processor and select
    **Run** to start it – or click the run icon in the operate window to start them
    all at once.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道现在已完成。你可以点击每个处理器并选择**运行**来启动它 – 或者点击操作窗口中的运行图标来一次性启动所有处理器。
- en: When the pipeline is completed, you will have a directory with all the rows
    where the person was over 40\. Of the 1,000 records, I have 635 CSVs named for
    each person. You will have different results based on what `Faker` used as the
    age value.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 当管道完成时，你将有一个包含所有超过 40 岁的人的行的目录。在 1,000 条记录中，我有 635 个以每个人命名的 CSV 文件。你将根据 `Faker`
    使用的年龄值得到不同的结果。
- en: This section showed you how to read in a CSV file. You also learned how you
    can split the file into rows and then run queries against them, as well as how
    to modify attributes of a flowfile and use it in another processor. In the next
    section, you will build another data pipeline using JSON.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 本节向你展示了如何读取 CSV 文件。你还学习了如何将文件拆分为行并对它们进行查询，以及如何修改流文件的属性并在另一个处理器中使用它。在下一节中，你将使用
    JSON 构建另一个数据管道。
- en: Working with JSON in NiFi
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 NiFi 中处理 JSON
- en: 'While having a different structure, working with JSON in NiFi is very similar
    to working with CSV. There are, however, a few processors for dealing exclusively
    with JSON. In this section, you will build a flow similar to the CSV example –
    read a file, split it into rows, and write each row to a file – but you will perform
    some more modifications of the data within the pipeline so that the rows you write
    to disk are different than what was in the original file. The following diagram
    shows the completed data pipeline:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管结构不同，在 NiFi 中处理 JSON 与处理 CSV 非常相似。然而，有几个处理器专门用于处理 JSON。在本节中，你将构建一个类似于 CSV
    示例的流程 – 读取一个文件，将其拆分为行，并将每一行写入一个文件 – 但你将在管道中对数据进行一些修改，以便写入磁盘的行与原始文件中的不同。以下图显示了完成的数据管道：
- en: '![Figure 3.13 – The completed JSON data pipeline'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.13 – 完成的 JSON 数据管道'
- en: '](img/B15739_03_13.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B15739_03_13.jpg](img/B15739_03_13.jpg)'
- en: Figure 3.13 – The completed JSON data pipeline
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13 – 完成的 JSON 数据管道
- en: 'To build the data pipeline, take the following steps:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建数据管道，请按照以下步骤操作：
- en: Place the `GetFile` processor on to the canvas. To configure the processor,
    specify the `home/paulcrickard` – and the `data.JSON`.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `GetFile` 处理器放置到画布上。要配置处理器，指定 `home/paulcrickard` – 和 `data.JSON`。
- en: 'In the CSV example, you used the `SplitRecord` processor. Here, for JSON, you
    can use the `SplitJson` processor. You will need to configure the **JsonPath Expression**
    property. This property is looking for an array that contains JSON elements. The
    JSON file is in the following format:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 CSV 示例中，你使用了 `SplitRecord` 处理器。这里，对于 JSON，你可以使用 `SplitJson` 处理器。你需要配置 **JsonPath
    表达式**属性。此属性正在寻找包含 JSON 元素的数组。JSON 文件格式如下：
- en: '[PRE45]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Because each record is in an array, you can pass the following value to the
    **JsonPath Expression** property:'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因为每个记录都在一个数组中，你可以将以下值传递给 **JsonPath 表达式**属性：
- en: '[PRE46]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This will split records inside of the array, which is the result you want.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将拆分数组内的记录，这是你想要的结果。
- en: The records will now become individual flowfiles. You will pass the files to
    the `EvaluateJsonPath` processor. This processor allows you to extract values
    from the flowfile. You can either pass the results to the flowfile content or
    to an attribute. Set the value of the `flowfile-attribute`. You can then select
    attributes to create using the plus sign. You will name the attribute, then specify
    the value. The value is the JSON path, and you use the format `$.key`. The configured
    processor is shown in the following screenshot:![Figure 3.14 – Configuration for
    extracting values from the flowfile
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录现在将成为单独的flowfile。你将文件传递给`EvaluateJsonPath`处理器。此处理器允许你从flowfile中提取值。你可以将结果传递到flowfile内容或属性中。设置`flowfile-attribute`的值。然后你可以选择使用加号创建的属性。你需要为属性命名，然后指定值。值是JSON路径，你使用格式`$.key`。配置的处理器如下面的截图所示：![图3.14
    – 从flowfile中提取值的配置
- en: '](img/B15739_03_14.jpg)'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/B15739_03_14.jpg](img/B15739_03_14.jpg)'
- en: Figure 3.14 – Configuration for extracting values from the flowfile
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.14 – 从flowfile中提取值的配置
- en: These attributes will not be passed down the data pipeline with the flowfile.
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些属性不会随着flowfile流经数据管道。
- en: 'Now, you can use the `QueryRecord` processor, just like you did with the CSV
    example. The difference with JSON is that you need to create a new record reader
    and recordset writer. Select the option to create a new service. Select `over.40`
    and set the value to the following:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你可以使用`QueryRecord`处理器，就像你在CSV示例中所做的那样。与JSON的区别在于你需要创建一个新的记录读取器和记录集写入器。选择创建新服务的选项。选择`over.40`并将值设置为以下内容：
- en: '[PRE47]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The next processor is the `AttributesToJSON` processor. This processor allows
    you to replace the flowfile content with the attributes you extracted in the `EvaluateJsonPath`
    processor shown in *step 3*. Set the `flowfile-content`. This processor also allows
    you to specify a comma-separated list of attributes in the **Attributes List**
    property. This can come in handy if you only want certain attributes. In this
    example, you leave it blank and several attributes you do not extract will be
    added to the flowfile content. All of the metadata attributes that NiFi writes
    will now be a part of the flowfile. The flowfile will now look as in the following
    snippet:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个处理器是`AttributesToJSON`处理器。此处理器允许你用在*步骤3*中显示的`EvaluateJsonPath`处理器中提取的属性替换flowfile内容。设置`flowfile-content`。此处理器还允许你在**属性列表**属性中指定以逗号分隔的属性列表。如果你只想使用某些属性，这可能会很有用。在这个例子中，你将其留空，并且一些你没有提取的属性将被添加到flowfile内容中。NiFi写入的所有元数据属性现在将成为flowfile的一部分。flowfile现在将看起来如下面的片段所示：
- en: '[PRE48]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Using the `EvalueJsonPath` processor again, you will create an attribute named
    `uuid`. Now that the metadata from NiFi is in the flowfile, you have the unique
    ID of the flowfile. Make sure to set `flowfile-attribute`. You will extract it
    now so that you can pass it to the next processor – `UpdateAttribute`.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次使用`EvalueJsonPath`处理器，你将创建一个名为`uuid`的属性。现在NiFi的元数据已经在flowfile中，你有flowfile的唯一ID。请确保设置`flowfile-attribute`。你现在将提取它，以便将其传递给下一个处理器`UpdateAttribute`。
- en: In the CSV example, you updated the filename using the `UpdateAttribute` processor.
    You will do the same here. Click on the plus sign and add an attribute named `filename`.
    Set the value to `${uuid}`.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在CSV示例中，你使用`UpdateAttribute`处理器更新了文件名。你在这里也将这样做。点击加号并添加一个名为`filename`的属性。将值设置为`${uuid}`。
- en: One way to modify JSON using NiFi is through `zip` field from the flowfile.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用NiFi修改JSON的一种方法是通过flowfile中的`zip`字段。
- en: Lastly, use the `PutFile` processor to write each row to disk. Configure the
    **Directory** and **Conflict Resolution Strategy** properties. By setting the
    **Conflict Resolution Strategy** property to **ignore**, the processor will not
    warn you if it has already processed a file with the same name.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用`PutFile`处理器将每一行写入磁盘。配置**目录**和**冲突解决策略**属性。通过将**冲突解决策略**属性设置为**忽略**，处理器在已经处理了具有相同名称的文件时不会警告你。
- en: 'Create the connections and relationships between the processors:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 创建处理器之间的连接和关系：
- en: Connect `GetFile` to `SplitJson` for relationship success.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`GetFile`连接到`SplitJson`以实现关系成功。
- en: Connect `SplitJson` to `EvaluateJsonPath` for relationship splits.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`SplitJson`连接到`EvaluateJsonPath`以实现关系拆分。
- en: Connect `EvaluateJsonPath` to `QueryRecord` for relationship matched.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`EvaluateJsonPath`连接到`QueryRecord`以实现关系匹配。
- en: Connect `QueryRecord` to `AttributesToJSON` for relationship `over.40`.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`QueryRecord`连接到`AttributesToJSON`以实现关系`over.40`。
- en: Connect `AttributesToJSON` to `UpdateAttribute` for relationship success.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`AttributesToJSON`连接到`UpdateAttribute`以实现关系成功。
- en: Connect `UpdateAttributes` to `JoltTransformJSON` for relationship success.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`UpdateAttributes`连接到`JoltTransformJSON`以实现关系成功。
- en: Connect `JoltTransformJSON` to `PutFile` for relationship success.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`JoltTransformJSON`连接到`PutFile`以实现关系成功。
- en: Run the data pipeline by starting each processor or clicking **Run** in the
    operate box. When complete, you will have a subset of 1,000 files – all people
    over 40 – on disk and named by their unique ID.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 通过启动每个处理器或在操作框中点击**运行**来运行数据管道。完成后，你将在磁盘上拥有一个包含1,000个文件的子集——所有40岁以上的人——并且按其唯一ID命名。
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to process CSV and JSON files using Python.
    Using this new skill, you have created a data pipeline in Apache Airflow by creating
    a Python function to process a CSV and transform it into JSON. You should now
    have a basic understanding of the Airflow GUI and how to run DAGs. You also learned
    how to build data pipelines in Apache NiFi using processors. The process for building
    more advanced data pipelines is the same, and you will learn the skills needed
    to accomplish this throughout the rest of this book.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何使用Python处理CSV和JSON文件。通过这项新技能，你通过创建一个Python函数来处理CSV并将其转换为JSON，在Apache
    Airflow中创建了一个数据管道。你现在应该对Airflow GUI以及如何运行DAGs有一个基本的了解。你还学习了如何使用处理器在Apache NiFi中构建数据管道。构建更高级数据管道的过程是相同的，你将在本书的其余部分学习完成此任务所需的所有技能。
- en: In the next chapter, you will learn how to use Python, Airflow, and NiFi to
    read and write data to databases. You will learn how to use PostgreSQL and Elasticsearch.
    Using both will expose you to standard relational databases that can be queried
    using SQL and NoSQL databases that allow you to store documents and use their
    own query languages.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习如何使用Python、Airflow和NiFi读取和写入数据库中的数据。你将学习如何使用PostgreSQL和Elasticsearch。使用两者将让你接触到可以使用SQL查询的标准关系型数据库，以及允许你存储文档并使用它们自己的查询语言的NoSQL数据库。
