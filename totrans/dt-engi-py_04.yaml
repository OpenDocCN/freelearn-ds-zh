- en: '*Chapter 3*: Reading and Writing Files'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at how to install various tools, such as
    NiFi, Airflow, PostgreSQL, and Elasticsearch. In this chapter, you will be learning
    how to use these tools. One of the most basic tasks in data engineering is moving
    data from a text file to a database. In this chapter, you will read data from
    and write data to several different text-based formats, such as CSV and JSON.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Reading and writing files in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing files in Airflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NiFi processors for handling files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading and writing data to databases in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databases in Airflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Database processors in NiFi
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing and reading files in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The title of this section may sound strange as you are probably used to seeing
    it written as reading and writing, but in this section, you will write data to
    files first, then read it. By writing it, you will understand the structure of
    the data and you will know what it is you are trying to read.
  prefs: []
  type: TYPE_NORMAL
- en: To write data, you will use a library named `faker`. `faker` allows you to easily
    create fake data for common fields. You can generate an address by simply calling
    `address()`, or a female name using `name_female()`. This will simplify the creation
    of fake data while at the same time making it more realistic.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `faker`, you can use `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: With `faker` now installed, you are ready to start writing files. The next section
    will start with CSV files.
  prefs: []
  type: TYPE_NORMAL
- en: Writing and reading CSVs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most common file type you will encounter is **Comma-Separated Values** (**CSV**).
    A CSV is a file made up of fields separated by commas. Because commas are fairly
    common in text, you need to be able to handle them in CSV files. This can be accomplished
    by using escape characters, usually a pair of quotes around text strings that
    could contain a comma that is not used to signify a new field. These quotes are
    called escape characters. The Python standard library for handling CSVs simplifies
    the process of handling CSV data.
  prefs: []
  type: TYPE_NORMAL
- en: Writing CSVs using the Python CSV Library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To write a CSV with the CSV library, you need to use the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a file in writing mode. To open a file, you need to specify a filename
    and a mode. The mode for writing is `w`, but you can also open a file for reading
    with `r`, appending with `a`, or reading and writing with `r+`. Lastly, if you
    are handling files that are not text, you can add `b`, for binary mode, to any
    of the preceding modes to write in bytes; for example, `wb` will allow you to
    write in bytes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create `CSV_writer`. At a minimum, you must specify a file to write to, but
    you can also pass additional parameters, such as a dialect. A dialect can be a
    defined CSV type, such as Excel, or it can be options such as the delimiter to
    use or the level of quoting. The defaults are usually what you will need; for
    example, the delimiter defaults to a comma (it is a CSV writer after all) and
    quoting defaults to `QUOTE_MINIMAL`, which will only add quotes when there are
    special characters or the delimiter within a field. So, you can create the writer
    as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Include a header. You might be able to remember what the fields are in your
    CSV, but it is best to include a header. Writing a header is the same as writing
    any other row: define the values, then you will use `writerow()`, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Write the data to a file. You can now write a data row by using `writerow(0)`
    and passing some data, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, if you look in the directory, you will have a CSV file named `myCSV.CSV`
    and the contents should look as in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – The contents of mycsv.csv'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15739_03_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – The contents of mycsv.csv
  prefs: []
  type: TYPE_NORMAL
- en: Notice that when you used `cat` to view the file, the newlines were added. By
    default, `CSV_writer` uses a return and a newline (`'\r\n'`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding example was very basic. However, if you are trying to write a
    lot of data, you would most likely want to loop through some condition or iterate
    through existing data. In the following example, you will use `Faker` to generate
    1,000 records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You should now have a `data.CSV` file with 1,000 rows of names and ages.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have written a CSV, the next section will walk you through reading
    it using Python.
  prefs: []
  type: TYPE_NORMAL
- en: Reading CSVs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Reading a CSV is somewhat similar to writing one. The same steps are followed
    with slight modifications:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a file using `with`. Using `with` has some additional benefits, but for
    now, the one you will reap is not having to use `close()` on the file. If you
    do not specify a mode, `open` defaults to read (`r`). After `open`, you will need
    to specify what to refer to the file as; in this case, you will open the `data.CSV`
    file and refer to it as `f`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the reader. Instead of just using `reader()`, you will use `DictReader()`.
    By using the dictionary reader, you will be able to call fields in the data by
    name instead of position. For example, instead of calling the first item in a
    row as `row[0]`, you can now call it as `row[''name'']`. Just like the writer,
    the defaults are usually sufficient, and you will only need to specify a file
    to read. The following code opens `data.CSV` using the `f` variable name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Grab the headers by reading a single line with `next()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, you can iterate through the rest of the rows using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can print the names using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should only see the 1,000 names scroll by. Now you have a Python dictionary
    that you can manipulate any way you need. There is another way to handle CSV data
    in Python and that requires `pandas`.
  prefs: []
  type: TYPE_NORMAL
- en: Reading and writing CSVs using pandas DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`pandas` DataFrames are a powerful tool not only for reading and writing data
    but also for the querying and manipulation of data. It does require a larger overhead
    than the built-in CSV library, but there are times when it may be worth the trade-off.
    You may already have `pandas` installed, depending on your Python environment,
    but if you do not, you can install it with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You can think of a `pandas` DataFrame as an Excel sheet or a table. You will
    have rows, columns, and an index. To load CSV data into a DataFrame, the following
    steps must be followed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pandas` (usually as `pd`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, read the file using `read_csv()`. The `read_csv()` method takes several
    optional parameters, and one required parameter – the file or file-like buffer.
    The two optional parameters that may be of interest are `header`, which by defaultattempts
    to infer the headers. If you set `header=0`, then you can use the `names` parameter
    with an array of column names. If you have a large file and you just want to look
    at a piece of it, you can use `nrows` to specify the number of rows to read, so
    `nrows=100` means it will only read 100 rows for the data. In the following snippet,
    you will load the entire file using the defaults:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now look at the first 10 records by using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Because you used `Faker` to generate data, you will have the same schema as
    in the following screenshot, but will have different values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Reading a CSV into a DataFrame and printing head()'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15739_03_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 – Reading a CSV into a DataFrame and printing head()
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create a DataFrame in Python with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a dictionary of data. A dictionary is a data structure that stores data
    as a key:value pair. The value can be of any Python data type – for example, an
    array. Dictionaries have methods for finding `keys()`, `values()`, and `items()`.
    They also allow you to find the value of a key by using the key name in brackets
    – for example, `dictionary[''key'']` will return the value for that key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pass the data to the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The columns are specified as the keys in the dictionary. Now that you have
    a DataFrame, you can write the contents to a CSV using `to_csv()` and passing
    a filename. In the example, we did not set an index, which means the row names
    will be a number from 0 to *n*, where *n* is the length of the DataFrame. When
    you export to CSV, these values will be written to the file, but the column name
    will be blank. So, in a case where you do not need the row names or index to be
    written to the file, pass the `index` parameter to `to_csv()`, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You will now have a CSV file with the contents of the DataFrame. How we can
    use the contents of this DataFrame for executing SQL queries will be covered in
    the next chapter. They will become an important tool in your toolbox and the rest
    of the book will lean on them heavily.
  prefs: []
  type: TYPE_NORMAL
- en: For now, let's move on to the next section, where you will learn about another
    common text format – **JSON**.
  prefs: []
  type: TYPE_NORMAL
- en: Writing JSON with Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another common data format you will probably deal with is `JSON–JSON`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To write JSON using Python and the standard library, the following steps need
    to be observed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the library and open the file you will write to. You also create the
    `Faker` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will create 1,000 records, just as we did in the CSV example, so you will
    need to create a dictionary to hold the data. As mentioned earlier, the value
    of a key can be any Python data type – including an array of values. After creating
    the dictionary to hold the records, add a `''records''` key and initialize it
    with a blank array, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To write the records, you use `Faker` to create a dictionary, then append it
    to the array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, to write the JSON to a file, use the `JSON.dump()` method. Pass the
    data that you want to write and a file to write to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You now have a `data.JSON` file that has an array with 1,000 records. You can
    read this file by taking the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the file using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `JSON.load()` and pass the file reference to the method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inspect the json by looking at the first record using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Or just use the name:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When you `loads` and `dumps` are different than `load` and `dump`. Both are
    valid methods of the JSON library. The difference is that `loads` and `dumps`
    are for strings – they do not serialize the JSON.
  prefs: []
  type: TYPE_NORMAL
- en: pandas DataFrames
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reading and writing JSON with DataFrames is similar to what we did with CSV.
    The only difference is that you change `to_csv` to `to_json()` and `read_csv()`
    to `read_json()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have a clean, well-formatted JSON file, you can read it using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In the case of the `data.JSON` file, the records are nested in a `records`
    dictionary. So, loading the JSON is not as straightforward as the preceding code.
    You will need a few extra steps, which are as follows. To load JSON data from
    the file, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `pandas` `JSON` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Open the file and load it with the `pandas` version of `JSON.loads()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To create the DataFrame, you need to normalize the JSON. Normalizing is how
    you can flatten the JSON to fit in a table. In this case, you want to grab the
    individual JSON records held in the `records` dictionary. Pass that path – `records`
    – to the `record_path` parameter of `json_normalize()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You will now have a DataFrame that contains all the records in the `data.JSON`
    file. You can now write them back to JSON, or CSV, using DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: 'When writing to JSON, you can pass the `orient` parameter, which determines
    the format of the JSON that is returned. The default is columns, which for the
    `data.JSON` file you created in the previous section would look like the following
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'By changing the `orient` value to `records`, you get each row as a record in
    the JSON, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: I find that working with JSON that is oriented around `records` makes processing
    it in tools such as Airflow much easier than JSON in other formats, such as split,
    index, columns, values, or table. Now that you know how to handle CSV and JSON
    files in Python, it is time to learn how to combine tasks into a data pipeline
    using Airflow and NiFi. In the next section, you will learn how to build pipelines
    in Apache Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: Building data pipelines in Apache Airflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Airflow uses Python functions, as well as Bash or other operators, to
    create tasks that can be combined into a **Directed Acyclic Graph** (**DAG**)
    – meaning each task moves in one direction when completed. Airflow allows you
    to combine Python functions to create tasks. You can specify the order in which
    the tasks will run, and which tasks depend on others. This order and dependency
    are what make it a DAG. Then, you can schedule your DAG in Airflow to specify
    when, and how frequently, your DAG should run. Using the Airflow GUI, you can
    monitor and manage your DAG. By using what you learned in the preceding sections,
    you will now make a data pipeline in Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: Building a CSV to a JSON data pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with a simple DAG will help you understand how Airflow works and will
    help you to add more functions to build a better data pipeline. The DAG you build
    will print out a message using Bash, then read the CSV and print a list of all
    the names. The following steps will walk you through building the data pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a new file using the Python IDE or any text editor. Import the required
    libraries, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The first two imports bring in `datetime` and `timedelta`. These libraries are
    used for scheduling the DAG. The three Airflow imports bring in the required libraries
    for building the DAG and using the Bash and Python operators. These are the operators
    you will use to build tasks. Lastly, you import `pandas` so that you can easily
    convert between CSV and JSON.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, write a function to read a CSV file and print out the names. By combining
    the steps for reading CSV data and writing JSON data from the previous sections,
    you can create a function that reads in the `data.CSV` file and writes it out
    to JSON, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This function opens the file in a DataFrame. Then, it iterates through the rows,
    printing only the names, and lastly, it writes the CSV to a JSON file.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, you need to implement the Airflow portion of the pipeline. Specify the
    arguments that will be passed to `DAG()`. In this book, you will use a minimal
    set of parameters. The arguments in this example assign an owner, a start date,
    the number of retries in the event of a failure, and how long to wait before retrying.
    They are shown in the following dictionary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, pass the arguments dictionary to `DAG()`. Create the DAG ID, which is
    set to `MyCSVDAG`, the dictionary of arguments (the `default_args` variable in
    the preceding code), and the schedule interval (how often to run the data pipeline).
    The schedule interval can be set using `timedelts`, or you can use a crontab format
    with the following presets or crontab:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) `@once`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) `@hourly` – `0 * * * *`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) `@daily` – `0 0 * * *`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) `@weekly` – `0 0 * * 0`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: e) `@monthly` – `0 0 1 * *`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: f) `@yearly` – `0 0 1 1 *`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: crontab uses the format minute, hour, day of month, month, day of week. The
    value for `@yearly` is `0 0 1 1 *`, which means run yearly on January 1 (`1 1`),
    at 0:0 (midnight), on any day of the week (`*`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can now create your tasks using operators. Airflow has several prebuilt
    operators. You can view them all in the documentation at [https://airflow.apache.org/docs/stable/_api/airflow/operators/index.html](https://airflow.apache.org/docs/stable/_api/airflow/operators/index.html).
    In this book, you will mostly use the Bash, Python, and Postgres operators. The
    operators allow you to remove most of the boilerplate code that is required to
    perform common tasks. In the following snippet, you will create two tasks using
    the Bash and Python operators:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding snippet creates a task using the `BashOperator` operator, which
    prints out a statement to let you know it is running. This task serves no purpose
    other than to allow you to see how to connect multiple tasks together. The next
    task, `CSVJson`, uses the `PythonOperator` operator to call the function you defined
    at the beginning of the file (`CSVToJson()`). The function reads the `data.CSV`
    file and prints the `name` field in every row.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'With the tasks defined, you now need to make the connections between the tasks.
    You can do this using the `set_upstream()` and `set_downstream()` methods or with
    the bit shift operator. By using upstream and downstream, you can make the graph
    go from the Bash task to the Python task using either of two snippets; the following
    is the first snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the second snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the bit shift operator, you can do the same; the following is the first
    option:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the second option:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which method you choose is up to you; however, you should be consistent. In
    this book, you will see the bit shift operator setting the downstream.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To use Airflow and Scheduler in the GUI, you first need to make a directory
    for your DAGs. During the install and configuration of Apache Airflow, in the
    previous chapter, we removed the samples and so the DAG directory is missing.
    If you look at `airflow.cfg`, you will see the setting for `dags_folder`. It is
    in the format of `$AIRFLOW_HOME/dags`. On my machine, `$AIRFLOW_HOME` is `home/paulcrickard/airflow`.
    This is the directory in which you will make the `dags folder.e` configuration
    file showing where the folder should be.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Copy your DAG code to the folder, then run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Launch the GUI by opening your web browser and going to `http://localhost:8080`.
    You will see your DAG, as shown in the following screenshot:![Figure 3.3 – The
    main screen of the Airflow GUI showing MyCSVDAG
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15739_03_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.3 – The main screen of the Airflow GUI showing MyCSVDAG
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on **DAGs** and select **Tree View**. Turn the DAG on, and then click
    **Go**. As the tasks start running, you will see the status of each run, as shown
    in the following screenshot:![Figure 3.4 – Multiple runs of the DAG and the status
    of each task
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15739_03_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.4 – Multiple runs of the DAG and the status of each task
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You will see that there have been successful runs – each task ran and did so
    successfully. But there is no output or results. To see the results, click on
    one of the completed squares, as shown in the following screenshot:![Figure 3.5
    – Checking results by hovering over the completed task
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15739_03_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.5 – Checking results by hovering over the completed task
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You will see a popup with several options. Click the **View Log** button, as
    shown in the following screenshot:![Figure 3.6 – Selecting View Log to see what
    happened in your task
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15739_03_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.6 – Selecting View Log to see what happened in your task
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will be redirected to the log screen for the task. Looking at a successful
    run of the CSV task, you should see a log file similar to the one in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Log of the Python task showing the names being printed'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15739_03_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.7 – Log of the Python task showing the names being printed
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have built a data pipeline with Python and ran it in Airflow.
    The result of your pipeline is a JSON file in your `dags` directory that was created
    from your `data.CSV` file. You can leave it running and it will continue to run
    at the specified `schedule_interval` time. Building more advanced pipelines will
    only require you to write more functions and connect them with the same process.
    But before you move on to more advanced techniques, you will need to learn how
    to use Apache NiFi to build data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Handling files using NiFi processors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, you learned how to read and write CSV and JSON files
    using Python. Reading files is such a common task that tools such as NiFi have
    prebuilt processors to handle it. In this section, you will learn how to handle
    files using NiFi processors.
  prefs: []
  type: TYPE_NORMAL
- en: Working with CSV in NiFi
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Working with files in NiFi requires many more steps than you had to use when
    doing the same tasks in Python. There are benefits to using more steps and using
    Nifi, including that someone who does not know code can look at your data pipeline
    and understand what it is you are doing. You may even find it easier to remember
    what it is you were trying to do when you come back to your pipeline in the future.
    Also, changes to the data pipeline do not require refactoring a lot of code; rather,
    you can reorder processors via drag and drop.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you will create a data pipeline that reads in the `data.CSV`
    file you created in Python. It will run a query for people over the age of 40,
    then write out that record to a file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of this section is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – The data pipeline you will build in this section'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15739_03_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.8 – The data pipeline you will build in this section
  prefs: []
  type: TYPE_NORMAL
- en: The following sections will walk you through building a data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Reading a file with GetFile
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step in your data pipeline is to read in the `data.csv` file. To
    do that, take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Drag the **Processor** icon from the NiFi toolbar to the canvas. Search for
    **GetFile** and then select it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To configure the `GetFile` processor, you must specify the input directory.
    In the Python examples earlier in this chapter, I wrote the `data.CSV` file to
    my home directory, which is `home/paulcrickard`, so this is what I will use for
    the input directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, you will need to specify a file filter. This field allows the NiFi expression
    language, so you could use `[^\.].*\.CSV` – but for this example, you can just
    set the value to `data.csv`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Lastly, the **Keep Source File** property should be set to **true**. If you
    leave it as **false**, NiFi will delete the file once it has processed it. The
    complete configuration is shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.9 – GetFile processor configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15739_03_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.9 – GetFile processor configuration
  prefs: []
  type: TYPE_NORMAL
- en: Splitting records into distinct flowfiles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now you can pass the success relationship from the `GetFile` processor to the
    `SplitRecord` processor:'
  prefs: []
  type: TYPE_NORMAL
- en: The `SplitRecord` processor will allow you to separate each row into a separate
    flowfile. Drag and drop it on the canvas. You need to create a record reader and
    a record writer – NiFi already has several that you can configure. Click on the
    box next to **Record Reader** and select **Create new service**, as shown in the
    following screenshot:![Figure 3.10 – A list of available readers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15739_03_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.10 – A list of available readers
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You will need to choose the type of reader. Select **CSVReader** from the dropdown.
    Select the dropdown for **Record Writer** and choose **CSVRecordSetWriter**:![Figure
    3.11 – A list of available readers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15739_03_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.11 – A list of available readers
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To configure **CSVReader** and **CSVRecordSetWriter**, click the arrow to the
    right of either one. This will open the **Files Configuration** window on the
    **CONTROLLER SERVICES** tab. You will see the screen shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Configuring the reader and writer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15739_03_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.12 – Configuring the reader and writer
  prefs: []
  type: TYPE_NORMAL
- en: 'The three icons to the right are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A gear for settings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A lightning bolt for enabling and disabling the service (it is currently disabled)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A trash can to delete it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select the gear for **CSVReader**. The default configuration will work, except
    for the **Treat First Line as Header** property, which should be set to **true**.
    Click the gear for **CSVRecordSetWriter** and you can see the available properties.
    The defaults are sufficient in this example. Now, click the lightning bolt to
    enable the services.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering records with the QueryRecord processor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You now have a pipeline that will read a CSV and split the rows into individual
    flowfiles. Now you can process each row with the `QueryRecord` processor. This
    processor will allow you to execute a SQL command against the flowfile. The contents
    of the new flowfile will be the results of the SQL query. In this example, you
    will select all records where the age of the person is over 40:'
  prefs: []
  type: TYPE_NORMAL
- en: Drag and drop the `QueryRecord` processor to the canvas. To query the flowfile,
    you need to specify a record reader and writer. You have already created one of
    each of these and they are available in the dropdown now. The **Include Zero Record
    FlowFiles** property should be set to **false**. This property will route records
    that do not meet the criteria to the same relationship (which you do not want).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Lastly, click the plus sign in the right-hand corner and specify a property
    name in the popup. The name of the property will become a relationship when you
    create a connection from this processor. Name the property `over.40`. Then, the
    value popup will appear. This is where you will enter the SQL query. The results
    of the query will become the contents of the flowfile. Since you want the records
    of people over 40 years of age, the query is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `Select` `*` query is what returns the entire flowfile. If you only wanted
    the name of the person and for the field to be `full_name`, you could run the
    following SQL:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The point I am attempting to drive home here is that you can execute SQL and
    modify the flowfile to something other than the contents of the row – for example,
    running and aggregation and a group by.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting data from a flowfile
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next processor will extract a value from the flowfile. That processer is
    `ExtractText`. The processor can be used on any flowfile containing text and uses
    regex to pull any data from the flowfile and assign it to an attribute.
  prefs: []
  type: TYPE_NORMAL
- en: 'To configure the processor, click the plus sign and name the property. You
    will extract the person name from the flowfile, so you can name the property name.
    The value will be regex and should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Without a full tutorial on regex, the preceding regex statement looks for a
    newline and a comma – `\n` and the comma at the end – and grabs the text inside.
    The parentheses say to take the text and return any characters that are not `^`
    or a comma. This regex returns the person's name. The flowfile contains a header
    of field names in CSV, a new line, followed by values in CSV. The `name` field
    is the first field on the second line – after the newline and before the first
    comma that specifies the end of the `name` field. This is why the regex looks
    for the text between the newline and the comma.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying flowfile attributes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you have pulled out the person name as an attribute, you can use the
    `UpdateAttribute` processor to change the value of existing attributes. By using
    this processor, you will modify the default filename attribute that NiFi has provided
    the flowfile all the way at the beginning in the `GetFile` processor. Every flowfile
    will have the filename `data.CSV`. If you try to write the flowfiles out to CSV,
    they will all have the same name and will either overwrite or fail.
  prefs: []
  type: TYPE_NORMAL
- en: Click the plus sign in the configuration for the `UpdateAttribute` processor
    and name the new property filename. The value will use the NiFi Expression Language.
    In the Expression Language, you can grab the value of an attribute using the format
    `${attribute name}`. So, to use the name attribute, set the value to `${name}`.
  prefs: []
  type: TYPE_NORMAL
- en: Saving a flowfile to disk
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using the `PutFile` processor, you can write the contents of a flowfile to disk.
    To configure the processor, you need to specify a directory in which to write
    the files. I will again use my home directory.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you can specify a conflict resolution strategy. By default, it will be
    set to fail, but it allows you to overwrite an existing file. If you were running
    this data pipeline, aggregating data every hour and writing the results to files,
    maybe you would set the property to overwrite so that the file always holds the
    most current data. By default, the flowfile will write to a file on disk with
    the property filename as the filename.
  prefs: []
  type: TYPE_NORMAL
- en: Creating relationships between the processors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The last step is to make connections for specified relationships between the
    processors:'
  prefs: []
  type: TYPE_NORMAL
- en: Grab the `GetFile` processor, drag the arrow to the `SplitRecord` processor,
    and check the relationship success in the popup.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the `SplitRecord` processor, make a connection to the `QueryRecord` processor
    and select the relationship splits. This means that any record that was split
    will be sent to the next processor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From `QueryRecord`, connect to the `ExtractText` processor. Notice the relationship
    you created is named `over.40`. If you added more SQL queries, you would get additional
    relationships. For this example, use the `over.40` relationship.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect `ExtractText` to the `UpdateAttribute` processor for the relationship
    matched.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, connect `UpdateAttribute` to the `PutFile` processor for the relationship
    success.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The data pipeline is now complete. You can click on each processor and select
    **Run** to start it – or click the run icon in the operate window to start them
    all at once.
  prefs: []
  type: TYPE_NORMAL
- en: When the pipeline is completed, you will have a directory with all the rows
    where the person was over 40\. Of the 1,000 records, I have 635 CSVs named for
    each person. You will have different results based on what `Faker` used as the
    age value.
  prefs: []
  type: TYPE_NORMAL
- en: This section showed you how to read in a CSV file. You also learned how you
    can split the file into rows and then run queries against them, as well as how
    to modify attributes of a flowfile and use it in another processor. In the next
    section, you will build another data pipeline using JSON.
  prefs: []
  type: TYPE_NORMAL
- en: Working with JSON in NiFi
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While having a different structure, working with JSON in NiFi is very similar
    to working with CSV. There are, however, a few processors for dealing exclusively
    with JSON. In this section, you will build a flow similar to the CSV example –
    read a file, split it into rows, and write each row to a file – but you will perform
    some more modifications of the data within the pipeline so that the rows you write
    to disk are different than what was in the original file. The following diagram
    shows the completed data pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – The completed JSON data pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15739_03_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.13 – The completed JSON data pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'To build the data pipeline, take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Place the `GetFile` processor on to the canvas. To configure the processor,
    specify the `home/paulcrickard` – and the `data.JSON`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the CSV example, you used the `SplitRecord` processor. Here, for JSON, you
    can use the `SplitJson` processor. You will need to configure the **JsonPath Expression**
    property. This property is looking for an array that contains JSON elements. The
    JSON file is in the following format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Because each record is in an array, you can pass the following value to the
    **JsonPath Expression** property:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will split records inside of the array, which is the result you want.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The records will now become individual flowfiles. You will pass the files to
    the `EvaluateJsonPath` processor. This processor allows you to extract values
    from the flowfile. You can either pass the results to the flowfile content or
    to an attribute. Set the value of the `flowfile-attribute`. You can then select
    attributes to create using the plus sign. You will name the attribute, then specify
    the value. The value is the JSON path, and you use the format `$.key`. The configured
    processor is shown in the following screenshot:![Figure 3.14 – Configuration for
    extracting values from the flowfile
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15739_03_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.14 – Configuration for extracting values from the flowfile
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These attributes will not be passed down the data pipeline with the flowfile.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, you can use the `QueryRecord` processor, just like you did with the CSV
    example. The difference with JSON is that you need to create a new record reader
    and recordset writer. Select the option to create a new service. Select `over.40`
    and set the value to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next processor is the `AttributesToJSON` processor. This processor allows
    you to replace the flowfile content with the attributes you extracted in the `EvaluateJsonPath`
    processor shown in *step 3*. Set the `flowfile-content`. This processor also allows
    you to specify a comma-separated list of attributes in the **Attributes List**
    property. This can come in handy if you only want certain attributes. In this
    example, you leave it blank and several attributes you do not extract will be
    added to the flowfile content. All of the metadata attributes that NiFi writes
    will now be a part of the flowfile. The flowfile will now look as in the following
    snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using the `EvalueJsonPath` processor again, you will create an attribute named
    `uuid`. Now that the metadata from NiFi is in the flowfile, you have the unique
    ID of the flowfile. Make sure to set `flowfile-attribute`. You will extract it
    now so that you can pass it to the next processor – `UpdateAttribute`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the CSV example, you updated the filename using the `UpdateAttribute` processor.
    You will do the same here. Click on the plus sign and add an attribute named `filename`.
    Set the value to `${uuid}`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One way to modify JSON using NiFi is through `zip` field from the flowfile.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, use the `PutFile` processor to write each row to disk. Configure the
    **Directory** and **Conflict Resolution Strategy** properties. By setting the
    **Conflict Resolution Strategy** property to **ignore**, the processor will not
    warn you if it has already processed a file with the same name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create the connections and relationships between the processors:'
  prefs: []
  type: TYPE_NORMAL
- en: Connect `GetFile` to `SplitJson` for relationship success.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connect `SplitJson` to `EvaluateJsonPath` for relationship splits.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connect `EvaluateJsonPath` to `QueryRecord` for relationship matched.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connect `QueryRecord` to `AttributesToJSON` for relationship `over.40`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connect `AttributesToJSON` to `UpdateAttribute` for relationship success.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connect `UpdateAttributes` to `JoltTransformJSON` for relationship success.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connect `JoltTransformJSON` to `PutFile` for relationship success.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the data pipeline by starting each processor or clicking **Run** in the
    operate box. When complete, you will have a subset of 1,000 files – all people
    over 40 – on disk and named by their unique ID.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to process CSV and JSON files using Python.
    Using this new skill, you have created a data pipeline in Apache Airflow by creating
    a Python function to process a CSV and transform it into JSON. You should now
    have a basic understanding of the Airflow GUI and how to run DAGs. You also learned
    how to build data pipelines in Apache NiFi using processors. The process for building
    more advanced data pipelines is the same, and you will learn the skills needed
    to accomplish this throughout the rest of this book.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to use Python, Airflow, and NiFi to
    read and write data to databases. You will learn how to use PostgreSQL and Elasticsearch.
    Using both will expose you to standard relational databases that can be queried
    using SQL and NoSQL databases that allow you to store documents and use their
    own query languages.
  prefs: []
  type: TYPE_NORMAL
