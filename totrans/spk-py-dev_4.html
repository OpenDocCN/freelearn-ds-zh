<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Learning from Data Using Spark"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Learning from Data Using Spark</h1></div></div></div><p>As we have laid the foundation for data to be harvested in the previous chapter, we are now ready to learn from the data. Machine learning is about drawing insights from data. Our <a id="id209" class="indexterm"/>objective is to give an overview of the Spark <span class="strong"><strong>MLlib</strong></span> (short for <span class="strong"><strong>Machine Learning library</strong></span>) and apply the appropriate algorithms to our dataset in order to derive insights. From the Twitter dataset, we will be applying an unsupervised clustering algorithm in order to distinguish between Apache Spark-relevant tweets versus the rest. We have as initial input a mixed bag of tweets. We first need to preprocess the data in order to extract the relevant features, then apply the machine learning algorithm to our dataset, and finally evaluate the results and the performance of our model.</p><p>In this chapter, we will cover the following points:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Providing an overview of the Spark MLlib module with its algorithms and the typical machine learning workflow.</li><li class="listitem" style="list-style-type: disc">Preprocessing the Twitter harvested dataset to extract the relevant features, applying an unsupervised clustering algorithm to identify <span class="emphasis"><em>Apache Spark</em></span>-relevant tweets. Then, evaluating the model and the results obtained.</li><li class="listitem" style="list-style-type: disc">Describing the Spark machine learning pipeline.</li></ul></div><div class="section" title="Contextualizing Spark MLlib in the app architecture"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec28"/>Contextualizing Spark MLlib in the app architecture</h1></div></div></div><p>Let's first <a id="id210" class="indexterm"/>contextualize the focus of this chapter on data-intensive app architecture. We will concentrate our attention on the analytics layer and more precisely machine learning. This will serve as a foundation for streaming apps as we want to apply the learning from the batch processing of data as inference rules for the streaming analysis.</p><p>The following diagram sets the context of the chapter's focus, highlighting the machine learning module within the analytics layer while using tools for exploratory data analysis, Spark SQL, and Pandas.</p><div class="mediaobject"><img src="graphics/B03986_04_01.jpg" alt="Contextualizing Spark MLlib in the app architecture"/></div></div></div>
<div class="section" title="Classifying Spark MLlib algorithms"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec29"/>Classifying Spark MLlib algorithms</h1></div></div></div><p>Spark MLlib is a<a id="id211" class="indexterm"/> rapidly evolving module of Spark with new algorithms added with each release of Spark.</p><p>The following diagram provides a high-level overview of Spark MLlib algorithms grouped in the traditional broad machine learning techniques and following the categorical or continuous nature of the data:</p><div class="mediaobject"><img src="graphics/B03986_04_02.jpg" alt="Classifying Spark MLlib algorithms"/></div><p>We categorize the Spark MLlib algorithms in two columns, categorical or continuous, depending on the type of data. We distinguish between data that is categorical or more qualitative in nature versus continuous data, which is quantitative in nature. An example of qualitative data is predicting the weather; given the atmospheric pressure, the temperature, and the presence and type of clouds, the weather will be sunny, dry, rainy, or overcast. These are discrete values. On the other hand, let's say we want to predict house prices, given the location, square meterage, and the number of beds; the real estate value can be predicted using linear regression. In this case, we are talking about continuous or quantitative values.</p><p>The horizontal grouping reflects the types of machine learning method used. Unsupervised versus supervised machine learning techniques are dependent on whether the training data is labeled. In an unsupervised learning challenge, no labels are given to the learning algorithm. The goal is to find the hidden structure in its input. In the case of supervised learning, the data is labeled. The focus is on making predictions using regression if the data is continuous or classification if the data is categorical.</p><p>An important category of machine learning is recommender systems, which leverage collaborative filtering techniques. The Amazon web store and Netflix have very powerful recommender systems powering their recommendations.</p><p>
<span class="strong"><strong>Stochastic Gradient Descent</strong></span> is<a id="id212" class="indexterm"/> one of the machine learning optimization techniques that is well suited for Spark distributed computation.</p><p>For<a id="id213" class="indexterm"/> processing large amounts of text, Spark offers crucial <a id="id214" class="indexterm"/>libraries for feature extraction and transformation such as <span class="strong"><strong>TF-IDF</strong></span> (short for <span class="strong"><strong>Term Frequency – Inverse Document Frequency</strong></span>), Word2Vec, standard scaler, and normalizer.</p><div class="section" title="Supervised and unsupervised learning"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec37"/>Supervised and unsupervised learning</h2></div></div></div><p>We delve<a id="id215" class="indexterm"/> more deeply here in to the traditional<a id="id216" class="indexterm"/> machine learning algorithms offered by Spark MLlib. We distinguish between supervised and unsupervised learning depending on whether the data is labeled. We distinguish between categorical or continuous depending on whether the data is discrete or continuous.</p><p>The following diagram explains the Spark MLlib supervised and unsupervised machine learning algorithms and preprocessing techniques:</p><div class="mediaobject"><img src="graphics/B03986_04_03.jpg" alt="Supervised and unsupervised learning"/></div><p>The following supervised and unsupervised MLlib algorithms and preprocessing techniques are currently available in Spark:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Clustering</strong></span>: This is an unsupervised machine learning technique where the data is<a id="id217" class="indexterm"/> not labeled. The aim is to extract structure from the data:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>K-Means</strong></span>: This<a id="id218" class="indexterm"/> partitions the data in K distinct clusters</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Gaussian Mixture</strong></span>: Clusters are assigned based on the maximum<a id="id219" class="indexterm"/> posterior probability of the component</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Power Iteration Clustering (PIC)</strong></span>: This <a id="id220" class="indexterm"/>groups vertices of a graph based on pairwise edge similarities</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Latent Dirichlet Allocation</strong></span> (<span class="strong"><strong>LDA</strong></span>): This<a id="id221" class="indexterm"/> is used to group collections of text documents into topics</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Streaming K-Means</strong></span>: This<a id="id222" class="indexterm"/> means clusters dynamically streaming data using a windowing function on the incoming data</li></ul></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Dimensionality Reduction</strong></span>: This aims to reduce the number of features under <a id="id223" class="indexterm"/>consideration. Essentially, this<a id="id224" class="indexterm"/> reduces noise in the data and focuses on the key features:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Singular Value Decomposition</strong></span> (<span class="strong"><strong>SVD</strong></span>): This<a id="id225" class="indexterm"/> breaks the matrix that contains the data into simpler meaningful pieces. It factorizes the initial matrix into three matrices.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Principal Component Analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>): This <a id="id226" class="indexterm"/>approximates a high dimensional dataset with a low dimensional sub space.</li></ul></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Regression and Classification</strong></span>: Regression predicts output values using labeled training data, while<a id="id227" class="indexterm"/> Classification groups the results into classes. Classification has dependent variables that are categorical or unordered whilst Regression has dependent variables that are continuous and ordered:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Linear Regression Models</strong></span> (linear regression, logistic regression, and support vector machines): Linear regression algorithms<a id="id228" class="indexterm"/> can be expressed as convex optimization problems that aim to minimize an objective function based on a vector of weight variables. The objective function controls the complexity of the model through the regularized part of the function and the error of the model through the loss part of the function.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Naive Bayes</strong></span>: This makes predictions based on the conditional probability <a id="id229" class="indexterm"/>distribution of a label given an observation. It assumes that features are mutually independent of each other.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Decision Trees</strong></span>: This performs recursive binary partitioning of the feature <a id="id230" class="indexterm"/>space. The<a id="id231" class="indexterm"/> information gain at the tree node level is maximized in order to determine the best split for the partition.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Ensembles of trees</strong></span> (Random Forests and Gradient-Boosted Trees): Tree ensemble algorithms combine base decision tree models in order to build a performant model. They are intuitive and <a id="id232" class="indexterm"/>very successful <a id="id233" class="indexterm"/>for classification and regression tasks.</li></ul></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Isotonic Regression</strong></span>: This<a id="id234" class="indexterm"/> minimizes the mean squared error between given data<a id="id235" class="indexterm"/> and observed responses.</li></ul></div></div><div class="section" title="Additional learning algorithms"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec38"/>Additional learning algorithms</h2></div></div></div><p>Spark MLlib<a id="id236" class="indexterm"/> offers more algorithms than the supervised and unsupervised learning ones. We have broadly three more additional types of machine learning methods: recommender systems, optimization algorithms, and feature extraction.</p><div class="mediaobject"><img src="graphics/B03986_04_04.jpg" alt="Additional learning algorithms"/></div><p>The following <a id="id237" class="indexterm"/>additional MLlib algorithms are currently available in Spark:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Collaborative filtering</strong></span>: This is the basis for recommender systems. It creates <a id="id238" class="indexterm"/>a user-item association matrix and aims to fill the gaps. Based on other users and items along with their ratings, it recommends an item that the target user has no ratings for. In distributed computing, one of the most successful algorithms is <span class="strong"><strong>ALS</strong></span> (short for <span class="strong"><strong>Alternating Least Square</strong></span>):<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Alternating Least Squares</strong></span>: This matrix factorization technique incorporates implicit feedback, temporal effects, and confidence levels. It decomposes the large user item matrix into a lower dimensional user and item factors. It minimizes a quadratic loss function by fixing alternatively its factors.</li></ul></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Feature extraction and transformation</strong></span>: These are essential techniques for large text document processing. It<a id="id239" class="indexterm"/> includes<a id="id240" class="indexterm"/> the following techniques:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Term Frequency</strong></span>: Search engines use TF-IDF to score and rank document relevance in a vast corpus. It is also used in machine learning to determine the importance of a word in a document or corpus. Term frequency statistically determines the weight of a term relative to its frequency in the corpus. Term frequency on its own can be misleading as it overemphasizes words such as <span class="emphasis"><em>the</em></span>, <span class="emphasis"><em>of</em></span>, or <span class="emphasis"><em>and</em></span> that give little information. Inverse Document Frequency provides the specificity or the measure of the amount of information, whether the term is rare or common across all documents in the corpus.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Word2Vec</strong></span>: This includes two models, <span class="strong"><strong>Skip-Gram</strong></span> and <span class="strong"><strong>Continuous Bag of Word</strong></span>. The Skip-Gram predicts neighboring words given a word, based on sliding windows of words, while Continuous Bag of Words predicts the current word given the neighboring words.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Standard Scaler</strong></span>: As part of preprocessing, the dataset must often be standardized by mean removal and variance scaling. We compute the mean and standard deviation on the training data and apply the same transformation to the test data.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Normalizer</strong></span>: We scale the samples to have unit norm. It is useful for quadratic forms such as the dot product or kernel methods.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Feature selection</strong></span>: This reduces the dimensionality of the vector space by selecting the most relevant features for the model.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Chi-Square Selector</strong></span>: This is a statistical method to measure the independence of two events.</li></ul></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Optimization</strong></span>: These specific Spark MLlib optimization algorithms focus on various<a id="id241" class="indexterm"/> techniques of gradient descent. Spark provides very efficient implementation of gradient descent on a distributed cluster of machines. It looks for the local minima by iteratively going down the steepest descent. It is compute-intensive as it iterates through all the data available:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Stochastic Gradient Descent</strong></span>: We minimize an objective function that is the sum of differentiable functions. Stochastic Gradient Descent<a id="id242" class="indexterm"/> uses only a sample of the training data in order to update a parameter in a particular iteration. It is used for large-scale and sparse machine learning problems such as text classification.</li></ul></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Limited-memory BFGS</strong></span> (<span class="strong"><strong>L-BFGS</strong></span>): As<a id="id243" class="indexterm"/> the name says, L-BFGS uses limited memory and suits the distributed optimization algorithm implementation of Spark MLlib.</li></ul></div></div></div>
<div class="section" title="Spark MLlib data types"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec30"/>Spark MLlib data types</h1></div></div></div><p>MLlib<a id="id244" class="indexterm"/> supports four essential data types: <span class="strong"><strong>local vector</strong></span>, <span class="strong"><strong>labeled point</strong></span>, <span class="strong"><strong>local matrix</strong></span>, and <span class="strong"><strong>distributed matrix</strong></span>. These data types are widely used in Spark MLlib algorithms:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Local vector</strong></span>: This <a id="id245" class="indexterm"/>resides in a single machine. It can be dense or sparse:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Dense vector is a traditional array of doubles. An example of dense vector is <code class="literal">[5.0, 0.0, 1.0, 7.0]</code>.</li><li class="listitem" style="list-style-type: disc">Sparse vector uses integer indices and double values. So the sparse representation of the vector <code class="literal">[5.0, 0.0, 1.0, 7.0]</code> would be <code class="literal">(4, [0, 2, 3], [5.0, 1.0, 7.0])</code>, where represent the dimension of the vector.<p>Here's an example of local vector in PySpark:</p><div class="informalexample"><pre class="programlisting">import numpy as np
import scipy.sparse as sps
from pyspark.mllib.linalg import Vectors

# NumPy array for dense vector.
dvect1 = np.array([5.0, 0.0, 1.0, 7.0])
# Python list for dense vector.
dvect2 = [5.0, 0.0, 1.0, 7.0]
# SparseVector creation
svect1 = Vectors.sparse(4, [0, 2, 3], [5.0, 1.0, 7.0])
# Sparse vector using a single-column SciPy csc_matrix
svect2 = sps.csc_matrix((np.array([5.0, 1.0, 7.0]), np.array([0, 2, 3])), shape = (4, 1))</pre></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Labeled point</strong></span>. A labeled point is a dense or sparse vector with a label used in supervised <a id="id246" class="indexterm"/>learning. In the case of binary labels, 0.0 represents the negative label whilst 1.0 represents the positive value.<p>Here's an example of a labeled point in PySpark:</p><div class="informalexample"><pre class="programlisting">from pyspark.mllib.linalg import SparseVector
from pyspark.mllib.regression import LabeledPoint

# Labeled point with a positive label and a dense feature vector.
lp_pos = LabeledPoint(1.0, [5.0, 0.0, 1.0, 7.0])

# Labeled point with a negative label and a sparse feature vector.
lp_neg = LabeledPoint(0.0, SparseVector(4, [0, 2, 3], [5.0, 1.0, 7.0]))</pre></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Local Matrix</strong></span>: This local matrix resides in a single machine with integer-type indices<a id="id248" class="indexterm"/> and values of type double.<p>Here's an example of a local matrix in PySpark:</p><div class="informalexample"><pre class="programlisting">from pyspark.mllib.linalg import Matrix, Matrices

# Dense matrix ((1.0, 2.0, 3.0), (4.0, 5.0, 6.0))
dMatrix = Matrices.dense(2, 3, [1, 2, 3, 4, 5, 6])

# Sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))
sMatrix = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])</pre></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Distributed Matrix</strong></span>: Leveraging the distributed mature of the RDD, distributed <a id="id249" class="indexterm"/>matrices can be shared in a cluster of machines. We distinguish four distributed matrix types: <code class="literal">RowMatrix</code>, <code class="literal">IndexedRowMatrix</code>, <code class="literal">CoordinateMatrix</code>, and <code class="literal">BlockMatrix</code>:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">RowMatrix</code>: This takes an RDD of vectors and creates a distributed matrix of rows with meaningless indices, called <code class="literal">RowMatrix</code>, from the RDD of vectors.</li><li class="listitem" style="list-style-type: disc"><code class="literal">IndexedRowMatrix</code>: In this case, row indices are meaningful. First, we create an RDD of indexed rows using the class <code class="literal">IndexedRow</code> and then create an <code class="literal">IndexedRowMatrix</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">CoordinateMatrix</code>: This is useful to represent very large and very sparse matrices. <code class="literal">CoordinateMatrix</code> is created from RDDs of <a id="id250" class="indexterm"/>the <code class="literal">MatrixEntry</code> points, represented by a tuple of type (long, long, or float)</li><li class="listitem" style="list-style-type: disc"><code class="literal">BlockMatrix</code>: These are created from RDDs of sub-matrix blocks, where a sub-matrix block is <code class="literal">((blockRowIndex, blockColIndex), sub-matrix)</code>.</li></ul></div></li></ul></div></li></ul></div></div>
<div class="section" title="Machine learning workflows and data flows"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec31"/>Machine learning workflows and data flows</h1></div></div></div><p>Beyond<a id="id251" class="indexterm"/> algorithms, machine learning is also about <a id="id252" class="indexterm"/>processes. We will discuss the typical workflows and data flows of supervised and unsupervised machine learning.</p><div class="section" title="Supervised machine learning workflows"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec39"/>Supervised machine learning workflows</h2></div></div></div><p>In <a id="id253" class="indexterm"/>supervised machine learning, the input training dataset is labeled. One of the key data practices is to split input data into training and test sets, and validate the mode accordingly.</p><p>We typically go through a six-step process flow in supervised learning:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Collect the data</strong></span>: This step essentially ties in with the previous chapter and ensures we collect the right data with the right volume and granularity in order to enable the machine learning algorithm to provide reliable answers.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Preprocess the data</strong></span>: This step is about checking the data quality by sampling, filling in the missing values if any, scaling and normalizing the data. We also define the feature extraction process. Typically, in the case of large text-based datasets, we apply tokenization, stop words removal, stemming, and TF-IDF.<p>In the case of supervised learning, we separate the input data into a training and test set. We can also implement various strategies of sampling and splitting the dataset for cross-validation purposes.</p></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Ready the data</strong></span>: In this step, we get the data in the format or data type expected by the algorithms. In the case of Spark MLlib, this includes local vector, dense or sparse vectors, labeled points, local matrix, distributed matrix with row matrix, indexed row matrix, coordinate matrix, and block matrix.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Model</strong></span>: In this step, we apply the algorithms that are suitable for the problem at hand and get the results for evaluation of the most suitable algorithm in the evaluate step. We might have multiple algorithms suitable for the problem; their respective performance will be scored in the evaluate step to select the best preforming ones. We can implement an ensemble or combination of models in order to reach the best results.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Optimize</strong></span>: We may need to run a grid search for the optimal parameters of certain algorithms. These parameters are determined during training, and fine-tuned during the testing and production phase.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Evaluate</strong></span>: We ultimately score the models and select the best one in terms of accuracy, performance, reliability, and scalability. We move the best performing model to test with the held out test data in order to ascertain the prediction accuracy of our model. Once satisfied with the fine-tuned model, we move it to production to process live data.</li></ul></div><p>The supervised machine learning workflow and dataflow are represented in the following diagram:</p><div class="mediaobject"><img src="graphics/B03986_04_05.jpg" alt="Supervised machine learning workflows"/></div></div><div class="section" title="Unsupervised machine learning workflows"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec40"/>Unsupervised machine learning workflows</h2></div></div></div><p>As <a id="id254" class="indexterm"/>opposed to supervised learning, our initial data is not labeled in the case of unsupervised learning, which is most often the case in real life. We will extract the structure from the data by using clustering or dimensionality reduction algorithms. In the unsupervised learning case, we do not split the data into training and test, as we cannot make any prediction because the data is not labeled. We will train the data along six steps similar to those in supervised learning. Once the model is trained, we will evaluate the results and fine-tune the model and then release it for production.</p><p>Unsupervised learning can be a preliminary step to supervised learning. Namely, we look at reducing the dimensionality of the data prior to attacking the learning phase.</p><p>The unsupervised machine learning workflows and dataflow are represented as follows:</p><div class="mediaobject"><img src="graphics/B03986_04_06.jpg" alt="Unsupervised machine learning workflows"/></div></div></div>
<div class="section" title="Clustering the Twitter dataset"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec32"/>Clustering the Twitter dataset</h1></div></div></div><p>Let's first get <a id="id255" class="indexterm"/>a feel for the data extracted from Twitter and get an understanding of the data structure in order to prepare and run it through the K-Means clustering algorithms. Our plan of attack uses the process and dataflow depicted earlier for unsupervised learning. The steps are as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Combine all tweet files into a single dataframe.</li><li class="listitem">Parse the tweets, remove stop words, extract emoticons, extract URL, and finally normalize the words (for example, mapping them to lowercase and removing punctuation and numbers).</li><li class="listitem">Feature extraction includes the following:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Tokenization</strong></span>: This breaks down the parsed tweet text into individual words or tokens</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>TF-IDF</strong></span>: This applies the TF-IDF algorithm to create feature vectors from the tokenized tweet texts</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Hash TF-IDF</strong></span>: This applies a hashing function to the token vectors</li></ul></div></li><li class="listitem">Run the K-Means clustering algorithm.</li><li class="listitem">Evaluate<a id="id256" class="indexterm"/> the results of the K-Means clustering:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Identify tweet membership to clusters</li><li class="listitem" style="list-style-type: disc">Perform dimensionality reduction to two dimensions with the Multi-Dimensional Scaling or the Principal Component Analysis algorithm</li><li class="listitem" style="list-style-type: disc">Plot the clusters</li></ul></div></li><li class="listitem">Pipeline:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Fine-tune the number of relevant clusters K</li><li class="listitem" style="list-style-type: disc">Measure the model cost</li><li class="listitem" style="list-style-type: disc">Select the optimal model</li></ul></div></li></ol></div><div class="section" title="Applying Scikit-Learn on the Twitter dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec41"/>Applying Scikit-Learn on the Twitter dataset</h2></div></div></div><p>Python's <a id="id257" class="indexterm"/>own Scikit-Learn machine learning library is one of the most reliable, intuitive, and robust tools around. Let's run through a preprocessing and unsupervised learning using Pandas and Scikit-Learn. It is often beneficial to explore a sample of the data using Scikit-Learn before spinning off clusters with Spark MLlib.</p><p>We have a mixed bag of 7,540 tweets. It contains tweets related to Apache Spark, Python, the upcoming presidential election with Hillary Clinton and Donald Trump as protagonists, and some tweets related to fashion and music with Lady Gaga and Justin Bieber. We are running the K-Means clustering algorithm using Python Scikit-Learn on the Twitter dataset harvested. We first load the sample data into a Pandas dataframe:</p><div class="informalexample"><pre class="programlisting">import pandas as pd

csv_in = 'C:\\Users\\Amit\\Documents\\IPython Notebooks\\AN00_Data\\unq_tweetstxt.csv'
twts_df01 = pd.read_csv(csv_in, sep =';', encoding='utf-8')

In [24]:

twts_df01.count()
Out[24]:
Unnamed: 0    7540
id            7540
created_at    7540
user_id       7540
user_name     7538
tweet_text    7540
dtype: int64

#
# Introspecting the tweets text
#
In [82]:

twtstxt_ls01[6910:6920]
Out[82]:
['RT @deroach_Ismoke: I am NOT voting for #hilaryclinton http://t.co/jaZZpcHkkJ',
 'RT @AnimalRightsJen: #HilaryClinton What do Bernie Sanders and Donald Trump Have in Common?: He has so far been th... http://t.co/t2YRcGCh6…',
 'I understand why Bill was out banging other chicks........I mean look at what he is married to.....\n@HilaryClinton',
 '#HilaryClinton What do Bernie Sanders and Donald Trump Have in Common?: He has so far been th... http://t.co/t2YRcGCh67 #Tcot #UniteBlue']</pre></div><p>We<a id="id258" class="indexterm"/> first perform a feature extraction from the tweets' text. We apply a sparse vectorizer to the dataset using a TF-IDF vectorizer with 10,000 features and English stop words:</p><div class="informalexample"><pre class="programlisting">In [37]:

print("Extracting features from the training dataset using a sparse vectorizer")
t0 = time()
Extracting features from the training dataset using a sparse vectorizer
In [38]:

vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000,
                                 min_df=2, stop_words='english',
                                 use_idf=True)
X = vectorizer.fit_transform(twtstxt_ls01)
#
# Output of the TFIDF Feature vectorizer
#
print("done in %fs" % (time() - t0))
print("n_samples: %d, n_features: %d" % X.shape)
print()
done in 5.232165s
n_samples: 7540, n_features: 6638</pre></div><p>As the <a id="id259" class="indexterm"/>dataset is now broken into a 7540 sample with vectors of 6,638 features, we are ready to feed this sparse matrix to the K-Means clustering algorithm. We will choose seven clusters and 100 maximum iterations initially:</p><div class="informalexample"><pre class="programlisting">In [47]:

km = KMeans(n_clusters=7, init='k-means++', max_iter=100, n_init=1,
            verbose=1)

print("Clustering sparse data with %s" % km)
t0 = time()
km.fit(X)
print("done in %0.3fs" % (time() - t0))

Clustering sparse data with KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=7, n_init=1,
    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,
    verbose=1)
Initialization complete
Iteration  0, inertia 13635.141
Iteration  1, inertia 6943.485
Iteration  2, inertia 6924.093
Iteration  3, inertia 6915.004
Iteration  4, inertia 6909.212
Iteration  5, inertia 6903.848
Iteration  6, inertia 6888.606
Iteration  7, inertia 6863.226
Iteration  8, inertia 6860.026
Iteration  9, inertia 6859.338
Iteration 10, inertia 6859.213
Iteration 11, inertia 6859.102
Iteration 12, inertia 6859.080
Iteration 13, inertia 6859.060
Iteration 14, inertia 6859.047
Iteration 15, inertia 6859.039
Iteration 16, inertia 6859.032
Iteration 17, inertia 6859.031
Iteration 18, inertia 6859.029
Converged at iteration 18
done in 1.701s</pre></div><p>The K-Means clustering algorithm converged after 18 iterations. We see in the following results the seven clusters with their respective key words. Clusters <code class="literal">0</code> and <code class="literal">6</code> are about music and <a id="id260" class="indexterm"/>fashion with Justin Bieber and Lady Gaga-related tweets. Clusters <code class="literal">1</code> and <code class="literal">5</code> are related to the U.S.A. presidential elections with Donald Trump-and Hilary Clinton-related tweets. Clusters <code class="literal">2</code> and <code class="literal">3</code> are the ones of interest to us as they are about Apache Spark and Python. Cluster <code class="literal">4</code> contains Thailand-related tweets:</p><div class="informalexample"><pre class="programlisting">#
# Introspect top terms per cluster
#

In [49]:

print("Top terms per cluster:")
order_centroids = km.cluster_centers_.argsort()[:, ::-1]
terms = vectorizer.get_feature_names()
for i in range(7):
    print("Cluster %d:" % i, end='')
    for ind in order_centroids[i, :20]:
        print(' %s' % terms[ind], end='')
    print()
Top terms per cluster:
Cluster 0: justinbieber love mean rt follow thank hi https whatdoyoumean video wanna hear whatdoyoumeanviral rorykramer happy lol making person dream justin
Cluster 1: donaldtrump hilaryclinton rt https trump2016 realdonaldtrump trump gop amp justinbieber president clinton emails oy8ltkstze tcot like berniesanders hilary people email
Cluster 2: bigdata apachespark hadoop analytics rt spark training chennai ibm datascience apache processing cloudera mapreduce data sap https vora transforming development
Cluster 3: apachespark python https rt spark data amp databricks using new learn hadoop ibm big apache continuumio bluemix learning join open
Cluster 4: ernestsgantt simbata3 jdhm2015 elsahel12 phuketdailynews dreamintentions beyhiveinfrance almtorta18 civipartnership 9_a_6 25whu72ep0 k7erhvu7wn fdmxxxcm3h osxuh2fxnt 5o5rmb0xhp jnbgkqn0dj ovap57ujdh dtzsz3lb6x sunnysai12345 sdcvulih6g
Cluster 5: trump donald donaldtrump starbucks trumpquote trumpforpresident oy8ltkstze https zfns7pxysx silly goy stump trump2016 news jeremy coffee corbyn ok7vc8aetz rt tonight
Cluster 6: ladygaga gaga lady rt https love follow horror cd story ahshotel american japan hotel human trafficking music fashion diet queen ahs</pre></div><p>We will <a id="id261" class="indexterm"/>visualize the results by plotting the cluster. We have 7,540 samples with 6,638 features. It will be impossible to visualize that many dimensions. We will use the <span class="strong"><strong>Multi-Dimensional Scaling</strong></span> (<span class="strong"><strong>MDS</strong></span>) algorithm to bring down the multidimensional features of the clusters into two tractable dimensions<a id="id262" class="indexterm"/> to be able to picture them:</p><div class="informalexample"><pre class="programlisting">import matplotlib.pyplot as plt
import matplotlib as mpl
from sklearn.manifold import MDS

MDS()

#
# Bring down the MDS to two dimensions (components) as we will plot 
# the clusters
#
mds = MDS(n_components=2, dissimilarity="precomputed", random_state=1)

pos = mds.fit_transform(dist)  # shape (n_components, n_samples)

xs, ys = pos[:, 0], pos[:, 1]

In [67]:

#
# Set up colors per clusters using a dict
#
cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e', 5: '#9990b3', 6: '#e8888a'}

#
#set up cluster names using a dict
#
cluster_names = {0: 'Music, Pop', 
                 1: 'USA Politics, Election', 
                 2: 'BigData, Spark', 
                 3: 'Spark, Python',
                 4: 'Thailand', 
                 5: 'USA Politics, Election', 
                 6: 'Music, Pop'}
In [115]:
#
# ipython magic to show the matplotlib plots inline
#
%matplotlib inline 

#
# Create data frame which includes MDS results, cluster numbers and tweet texts to be displayed
#
df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, txt=twtstxt_ls02_utf8))
ix_start = 2000
ix_stop  = 2050
df01 = df[ix_start:ix_stop]

print(df01[['label','txt']])
print(len(df01))
print()

# Group by cluster

groups = df.groupby('label')
groups01 = df01.groupby('label')

# Set up the plot

fig, ax = plt.subplots(figsize=(17, 10)) 
ax.margins(0.05) 

#
# Build the plot object
#
for name, group in groups01:
    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, 
            label=cluster_names[name], color=cluster_colors[name], 
            mec='none')
    ax.set_aspect('auto')
    ax.tick_params(\
        axis= 'x',         # settings for x-axis
        which='both',      # 
        bottom='off',      # 
        top='off',         # 
        labelbottom='off')
    ax.tick_params(\
        axis= 'y',         # settings for y-axis
        which='both',      # 
        left='off',        # 
        top='off',         # 
        labelleft='off')
    
ax.legend(numpoints=1)     #
#
# Add label in x,y position with tweet text
#
for i in range(ix_start, ix_stop):
    ax.text(df01.ix[i]['x'], df01.ix[i]['y'], df01.ix[i]['txt'], size=10)  
    
plt.show()                 # Display the plot


      label       text
2000      2       b'RT @BigDataTechCon: '
2001      3       b"@4Quant 's presentat"
2002      2       b'Cassandra Summit 201'</pre></div><p>Here's <a id="id263" class="indexterm"/>a plot of Cluster <code class="literal">2</code>, <span class="emphasis"><em>Big Data</em></span> and<span class="emphasis"><em> Spark</em></span>, represented by blue dots along with Cluster <code class="literal">3</code>, <span class="emphasis"><em>Spark</em></span> and <span class="emphasis"><em>Python</em></span>, represented by red dots, and some sample tweets related to the respective clusters:</p><div class="mediaobject"><img src="graphics/B03986_04_07.jpg" alt="Applying Scikit-Learn on the Twitter dataset"/></div><p>We have gained some good insights into the data with the exploration and processing done with <a id="id264" class="indexterm"/>Scikit-Learn. We will now focus our attention on Spark MLlib and take it for a ride on the Twitter dataset.</p></div><div class="section" title="Preprocessing the dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec42"/>Preprocessing the dataset</h2></div></div></div><p>Now, we <a id="id265" class="indexterm"/>will focus on feature extraction and engineering in order to ready the data for the clustering algorithm run. We instantiate the Spark Context and read the Twitter dataset into a Spark dataframe. We will then successively tokenize the tweet text data, apply a hashing Term frequency algorithm to the tokens, and finally apply the Inverse Document Frequency algorithm and rescale the data. The code is as follows:</p><div class="informalexample"><pre class="programlisting">In [3]:
#
# Read csv in a Panda DF
#
#
import pandas as pd
csv_in = '/home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/data/unq_tweetstxt.csv'
pddf_in = pd.read_csv(csv_in, index_col=None, header=0, sep=';', encoding='utf-8')

In [4]:

sqlContext = SQLContext(sc)

In [5]:

#
# Convert a Panda DF to a Spark DF
#
#

spdf_02 = sqlContext.createDataFrame(pddf_in[['id', 'user_id', 'user_name', 'tweet_text']])

In [8]:

spdf_02.show()

In [7]:

spdf_02.take(3)

Out[7]:

[Row(id=638830426971181057, user_id=3276255125, user_name=u'True Equality', tweet_text=u'ernestsgantt: BeyHiveInFrance: 9_A_6: dreamintentions: elsahel12: simbata3: JDHM2015: almtorta18: dreamintentions:\u2026 http://t.co/VpD7FoqMr0'),
 Row(id=638830426727911424, user_id=3276255125, user_name=u'True Equality', tweet_text=u'ernestsgantt: BeyHiveInFrance: PhuketDailyNews: dreamintentions: elsahel12: simbata3: JDHM2015: almtorta18: CiviPa\u2026 http://t.co/VpD7FoqMr0'),
 Row(id=638830425402556417, user_id=3276255125, user_name=u'True Equality', tweet_text=u'ernestsgantt: BeyHiveInFrance: 9_A_6: ernestsgantt: elsahel12: simbata3: JDHM2015: almtorta18: CiviPartnership: dr\u2026 http://t.co/EMDOn8chPK')]

In [9]:

from pyspark.ml.feature import HashingTF, IDF, Tokenizer

In [10]:

#
# Tokenize the tweet_text 
#
tokenizer = Tokenizer(inputCol="tweet_text", outputCol="tokens")
tokensData = tokenizer.transform(spdf_02)

In [11]:

tokensData.take(1)

Out[11]:

[Row(id=638830426971181057, user_id=3276255125, user_name=u'True Equality', tweet_text=u'ernestsgantt: BeyHiveInFrance: 9_A_6: dreamintentions: elsahel12: simbata3: JDHM2015: almtorta18: dreamintentions:\u2026 http://t.co/VpD7FoqMr0', tokens=[u'ernestsgantt:', u'beyhiveinfrance:', u'9_a_6:', u'dreamintentions:', u'elsahel12:', u'simbata3:', u'jdhm2015:', u'almtorta18:', u'dreamintentions:\u2026', u'http://t.co/vpd7foqmr0'])]

In [14]:

#
# Apply Hashing TF to the tokens
#
hashingTF = HashingTF(inputCol="tokens", outputCol="rawFeatures", numFeatures=2000)
featuresData = hashingTF.transform(tokensData)

In [15]:

featuresData.take(1)

Out[15]:

[Row(id=638830426971181057, user_id=3276255125, user_name=u'True Equality', tweet_text=u'ernestsgantt: BeyHiveInFrance: 9_A_6: dreamintentions: elsahel12: simbata3: JDHM2015: almtorta18: dreamintentions:\u2026 http://t.co/VpD7FoqMr0', tokens=[u'ernestsgantt:', u'beyhiveinfrance:', u'9_a_6:', u'dreamintentions:', u'elsahel12:', u'simbata3:', u'jdhm2015:', u'almtorta18:', u'dreamintentions:\u2026', u'http://t.co/vpd7foqmr0'], rawFeatures=SparseVector(2000, {74: 1.0, 97: 1.0, 100: 1.0, 160: 1.0, 185: 1.0, 742: 1.0, 856: 1.0, 991: 1.0, 1383: 1.0, 1620: 1.0}))]

In [16]:

#
# Apply IDF to the raw features and rescale the data
#
idf = IDF(inputCol="rawFeatures", outputCol="features")
idfModel = idf.fit(featuresData)
rescaledData = idfModel.transform(featuresData)

for features in rescaledData.select("features").take(3):
  print(features)

In [17]:

rescaledData.take(2)

Out[17]:

[Row(id=638830426971181057, user_id=3276255125, user_name=u'True Equality', tweet_text=u'ernestsgantt: BeyHiveInFrance: 9_A_6: dreamintentions: elsahel12: simbata3: JDHM2015: almtorta18: dreamintentions:\u2026 http://t.co/VpD7FoqMr0', tokens=[u'ernestsgantt:', u'beyhiveinfrance:', u'9_a_6:', u'dreamintentions:', u'elsahel12:', u'simbata3:', u'jdhm2015:', u'almtorta18:', u'dreamintentions:\u2026', u'http://t.co/vpd7foqmr0'], rawFeatures=SparseVector(2000, {74: 1.0, 97: 1.0, 100: 1.0, 160: 1.0, 185: 1.0, 742: 1.0, 856: 1.0, 991: 1.0, 1383: 1.0, 1620: 1.0}), features=SparseVector(2000, {74: 2.6762, 97: 1.8625, 100: 2.6384, 160: 2.9985, 185: 2.7481, 742: 5.5269, 856: 4.1406, 991: 2.9518, 1383: 4.694, 1620: 3.073})),
 Row(id=638830426727911424, user_id=3276255125, user_name=u'True Equality', tweet_text=u'ernestsgantt: BeyHiveInFrance: PhuketDailyNews: dreamintentions: elsahel12: simbata3: JDHM2015: almtorta18: CiviPa\u2026 http://t.co/VpD7FoqMr0', tokens=[u'ernestsgantt:', u'beyhiveinfrance:', u'phuketdailynews:', u'dreamintentions:', u'elsahel12:', u'simbata3:', u'jdhm2015:', u'almtorta18:', u'civipa\u2026', u'http://t.co/vpd7foqmr0'], rawFeatures=SparseVector(2000, {74: 1.0, 97: 1.0, 100: 1.0, 160: 1.0, 185: 1.0, 460: 1.0, 987: 1.0, 991: 1.0, 1383: 1.0, 1620: 1.0}), features=SparseVector(2000, {74: 2.6762, 97: 1.8625, 100: 2.6384, 160: 2.9985, 185: 2.7481, 460: 6.4432, 987: 2.9959, 991: 2.9518, 1383: 4.694, 1620: 3.073}))]

In [21]:

rs_pddf = rescaledData.toPandas()

In [22]:

rs_pddf.count()

Out[22]:

id             7540
user_id        7540
user_name      7540
tweet_text     7540
tokens         7540
rawFeatures    7540
features       7540
dtype: int64


In [27]:

feat_lst = rs_pddf.features.tolist()

In [28]:

feat_lst[:2]

Out[28]:

[SparseVector(2000, {74: 2.6762, 97: 1.8625, 100: 2.6384, 160: 2.9985, 185: 2.7481, 742: 5.5269, 856: 4.1406, 991: 2.9518, 1383: 4.694, 1620: 3.073}),
 SparseVector(2000, {74: 2.6762, 97: 1.8625, 100: 2.6384, 160: 2.9985, 185: 2.7481, 460: 6.4432, 987: 2.9959, 991: 2.9518, 1383: 4.694, 1620: 3.073})]</pre></div></div><div class="section" title="Running the clustering algorithm"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec43"/>Running the clustering algorithm</h2></div></div></div><p>We will<a id="id266" class="indexterm"/> use the K-Means algorithm against the Twitter dataset. As an unlabeled and shuffled bag of tweets, we want to see if the <span class="emphasis"><em>Apache Spark</em></span> tweets are grouped in a single cluster. From the previous steps, the TF-IDF sparse vector of features is converted into an RDD that will be the input to the Spark MLlib program. We initialize the K-Means model with 5 clusters, 10 iterations of 10 runs:</p><div class="informalexample"><pre class="programlisting">In [32]:

from pyspark.mllib.clustering import KMeans, KMeansModel
from numpy import array
from math import sqrt

In [34]:

# Load and parse the data


in_Data = sc.parallelize(feat_lst)

In [35]:

in_Data.take(3)

Out[35]:

[SparseVector(2000, {74: 2.6762, 97: 1.8625, 100: 2.6384, 160: 2.9985, 185: 2.7481, 742: 5.5269, 856: 4.1406, 991: 2.9518, 1383: 4.694, 1620: 3.073}),
 SparseVector(2000, {74: 2.6762, 97: 1.8625, 100: 2.6384, 160: 2.9985, 185: 2.7481, 460: 6.4432, 987: 2.9959, 991: 2.9518, 1383: 4.694, 1620: 3.073}),
 SparseVector(2000, {20: 4.3534, 74: 2.6762, 97: 1.8625, 100: 5.2768, 185: 2.7481, 856: 4.1406, 991: 2.9518, 1039: 3.073, 1620: 3.073, 1864: 4.6377})]

In [37]:

in_Data.count()

Out[37]:

7540

In [38]:

# Build the model (cluster the data)

clusters = KMeans.train(in_Data, 5, maxIterations=10,
        runs=10, initializationMode="random")

In [53]:

# Evaluate clustering by computing Within Set Sum of Squared Errors

def error(point):
    center = clusters.centers[clusters.predict(point)]
    return sqrt(sum([x**2 for x in (point - center)]))

WSSSE = in_Data.map(lambda point: error(point)).reduce(lambda x, y: x + y)
print("Within Set Sum of Squared Error = " + str(WSSSE))</pre></div></div><div class="section" title="Evaluating the model and the results"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec44"/>Evaluating the model and the results</h2></div></div></div><p>One <a id="id267" class="indexterm"/>way to fine-tune the clustering algorithm is by varying the number of clusters and verifying the output. Let's check the clusters and get a feel for the clustering results so far:</p><div class="informalexample"><pre class="programlisting">In [43]:

cluster_membership = in_Data.map(lambda x: clusters.predict(x))

In [54]:

cluster_idx = cluster_membership.zipWithIndex()

In [55]:

type(cluster_idx)

Out[55]:

pyspark.rdd.PipelinedRDD

In [58]:

cluster_idx.take(20)

Out[58]:

[(3, 0),
 (3, 1),
 (3, 2),
 (3, 3),
 (3, 4),
 (3, 5),
 (1, 6),
 (3, 7),
 (3, 8),
 (3, 9),
 (3, 10),
 (3, 11),
 (3, 12),
 (3, 13),
 (3, 14),
 (1, 15),
 (3, 16),
 (3, 17),
 (1, 18),
 (1, 19)]

In [59]:

cluster_df = cluster_idx.toDF()

In [65]:

pddf_with_cluster = pd.concat([pddf_in, cluster_pddf],axis=1)

In [76]:

pddf_with_cluster._1.unique()

Out[76]:

array([3, 1, 4, 0, 2])

In [79]:

pddf_with_cluster[pddf_with_cluster['_1'] == 0].head(10)

Out[79]:
  Unnamed: 0   id   created_at   user_id   user_name   tweet_text   _1   _2
6227   3   642418116819988480   Fri Sep 11 19:23:09 +0000 2015   49693598   Ajinkya Kale   RT @bigdata: Distributed Matrix Computations i...   0   6227
6257   45   642391207205859328   Fri Sep 11 17:36:13 +0000 2015   937467860   Angela Bassa   [Auto] I'm reading ""Distributed Matrix Comput...   0   6257
6297   119   642348577147064320   Fri Sep 11 14:46:49 +0000 2015   18318677   Ben Lorica   Distributed Matrix Computations in @ApacheSpar...   0   6297
In [80]:

pddf_with_cluster[pddf_with_cluster['_1'] == 1].head(10)

Out[80]:
  Unnamed: 0   id   created_at   user_id   user_name   tweet_text   _1   _2
6   6   638830419090079746   Tue Sep 01 21:46:55 +0000 2015   2241040634   Massimo Carrisi   Python:Python: Removing \xa0 from string? - I ...   1   6
15   17   638830380578045953   Tue Sep 01 21:46:46 +0000 2015   57699376   Rafael Monnerat   RT @ramalhoorg: Noite de autógrafos do Fluent ...   1   15
18   41   638830280988426250   Tue Sep 01 21:46:22 +0000 2015   951081582   Jack Baldwin   RT @cloudaus: We are 3/4 full! 2-day @swcarpen...   1   18
19   42   638830276626399232   Tue Sep 01 21:46:21 +0000 2015   6525302   Masayoshi Nakamura   PynamoDB #AWS #DynamoDB #Python http://...   1   19
20   43   638830213288235008   Tue Sep 01 21:46:06 +0000 2015   3153874869   Baltimore Python   Flexx: Python UI tookit based on web technolog...   1   20
21   44   638830117645516800   Tue Sep 01 21:45:43 +0000 2015   48474625   Radio Free Denali   Hmm, emerge --depclean wants to remove somethi...   1   21
22   46   638829977014636544   Tue Sep 01 21:45:10 +0000 2015   154915461   Luciano Ramalho   Noite de autógrafos do Fluent Python no Garoa ...   1   22
23   47   638829882928070656   Tue Sep 01 21:44:47 +0000 2015   917320920   bsbafflesbrains   @DanSWright Harper channeling Monty Python. "...   1   23
24   48   638829868679954432   Tue Sep 01 21:44:44 +0000 2015   134280898   Lannick Technology   RT @SergeyKalnish: I am #hiring: Senior Back e...   1   24
25   49   638829707484508161   Tue Sep 01 21:44:05 +0000 2015   2839203454   Joshua Jones   RT @LindseyPelas: Surviving Monty Python in Fl...   1   25
In [81]:

pddf_with_cluster[pddf_with_cluster['_1'] == 2].head(10)

Out[81]:
  Unnamed: 0   id   created_at   user_id   user_name   tweet_text   _1   _2
7280   688   639056941592014848   Wed Sep 02 12:47:02 +0000 2015   2735137484   Chris   A true gay icon when will @ladygaga @Madonna @...   2   7280
In [82]:

pddf_with_cluster[pddf_with_cluster['_1'] == 3].head(10)

Out[82]:
  Unnamed: 0   id   created_at   user_id   user_name   tweet_text   _1   _2
0   0   638830426971181057   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: dreamint...   3   0
1   1   638830426727911424   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...   3   1
2   2   638830425402556417   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: ernestsg...   3   2
3   3   638830424563716097   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...   3   3
4   4   638830422256816132   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: elsahel12: 9_A_6: dreamintention...   3   4
5   5   638830420159655936   Tue Sep 01 21:46:55 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...   3   5
7   7   638830418330980352   Tue Sep 01 21:46:55 +0000 2015   3276255125   True Equality   ernestsgantt: elsahel12: 9_A_6: dreamintention...   3   7
8   8   638830397648822272   Tue Sep 01 21:46:50 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...   3   8
9   9   638830395375529984   Tue Sep 01 21:46:49 +0000 2015   3276255125   True Equality   ernestsgantt: elsahel12: 9_A_6: dreamintention...   3   9
10   10   638830392389177344   Tue Sep 01 21:46:49 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...   3   10
In [83]:

pddf_with_cluster[pddf_with_cluster['_1'] == 4].head(10)

Out[83]:
  Unnamed: 0   id   created_at   user_id   user_name   tweet_text   _1   _2
1361   882   642648214454317056   Sat Sep 12 10:37:28 +0000 2015   27415756   Raymond Enisuoh   LA Chosen For US 2024 Olympic Bid - LA2016 See...   4   1361
1363   885   642647848744583168   Sat Sep 12 10:36:01 +0000 2015   27415756   Raymond Enisuoh   Prison See: https://t.co/x3EKAExeFi … … … … … ...   4   1363
5412   11   640480770369286144   Sun Sep 06 11:04:49 +0000 2015   3242403023   Donald Trump 2016   " igiboooy! @ Starbucks https://t.co/97wdL...   4   5412
5428   27   640477140660518912   Sun Sep 06 10:50:24 +0000 2015   3242403023   Donald Trump 2016   "  @ Starbucks https://t.co/wsEYFIefk7 " - D...   4   5428
5455   61   640469542272110592   Sun Sep 06 10:20:12 +0000 2015   3242403023   Donald Trump 2016   " starbucks @ Starbucks Mam Plaza https://t.co...   4   5455
5456   62   640469541370372096   Sun Sep 06 10:20:12 +0000 2015   3242403023   Donald Trump 2016   " Aaahhh the pumpkin spice latte is back, fall...   4   5456
5457   63   640469539524898817   Sun Sep 06 10:20:12 +0000 2015   3242403023   Donald Trump 2016   " RT kayyleighferry: Oh my goddd Harry Potter ...   4   5457
5458   64   640469537176031232   Sun Sep 06 10:20:11 +0000 2015   3242403023   Donald Trump 2016   " Starbucks https://t.co/3xYYXlwNkf " - Donald...   4   5458
5459   65   640469536119070720   Sun Sep 06 10:20:11 +0000 2015   3242403023   Donald Trump 2016   " A Starbucks is under construction in my neig...   4   5459
5460   66   640469530435813376   Sun Sep 06 10:20:10 +0000 2015   3242403023   Donald Trump 2016   " Babam starbucks'tan fotogtaf atıyor bende du...   4   5460</pre></div><p>We<a id="id268" class="indexterm"/> map the <code class="literal">5</code> clusters with some sample tweets. Cluster <code class="literal">0</code> is about Spark. Cluster <code class="literal">1</code> is about Python. Cluster <code class="literal">2</code> is about Lady Gaga. Cluster <code class="literal">3</code> is about Thailand's Phuket News. Cluster <code class="literal">4</code> is about Donald Trump.</p></div></div>
<div class="section" title="Building machine learning pipelines"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec33"/>Building machine learning pipelines</h1></div></div></div><p>We<a id="id269" class="indexterm"/> want to compose the feature extraction, preparatory activities, training, testing, and prediction activities while optimizing the best tuning parameter to get the best performing model.</p><p>The following tweet captures perfectly in five lines of code a powerful machine learning Pipeline implemented in Spark MLlib:</p><div class="mediaobject"><img src="graphics/B03986_04_08.jpg" alt="Building machine learning pipelines"/></div><p>The<a id="id270" class="indexterm"/> Spark ML pipeline is inspired by Python's Scikit-Learn and creates a succinct, declarative statement of the successive transformations to the data in order to quickly deliver a tunable model.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec34"/>Summary</h1></div></div></div><p>In this chapter, we got an overview of Spark MLlib's ever-expanding library of algorithms Spark MLlib. We discussed supervised and unsupervised learning, recommender systems, optimization, and feature extraction algorithms. We then put the harvested data from Twitter into the machine learning process, algorithms, and evaluation to derive insights from the data. We put the Twitter-harvested dataset through a Python Scikit-Learn and Spark MLlib K-means clustering in order to segregate the tweets relevant to <span class="emphasis"><em>Apache Spark</em></span>. We also evaluated the performance of the model.</p><p>This gets us ready for the next chapter, which will cover Streaming Analytics using Spark. Let's jump right in.</p></div></body></html>