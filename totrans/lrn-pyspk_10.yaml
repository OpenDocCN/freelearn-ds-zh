- en: Chapter 10. Structured Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will provide a jump-start on the concepts behind Spark Streaming
    and how this has evolved into Structured Streaming. An important aspect of Structured
    Streaming is that it utilizes Spark DataFrames. This shift in paradigm will make
    it easier for Python developers to start working with Spark Streaming.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, your will learn:'
  prefs: []
  type: TYPE_NORMAL
- en: What is Spark Streaming?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why do we need Spark Streaming?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the Spark Streaming application data flow?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple streaming application using DStream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A quick primer on Spark Streaming global aggregations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Structured Streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note, for the initial sections of this chapter, the example code used will be
    in Scala, as this was how most Spark Streaming code was written. When we start
    focusing on Structured Streaming, we will work with Python examples.
  prefs: []
  type: TYPE_NORMAL
- en: What is Spark Streaming?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At its core, Spark Streaming is a scalable, fault-tolerant streaming system
    that takes the RDD batch paradigm (that is, processing data in batches) and speeds
    it up. While it is a slight over-simplification, basically Spark Streaming operates
    in mini-batches or batch intervals (from 500ms to larger interval windows).
  prefs: []
  type: TYPE_NORMAL
- en: As noted in the following diagram, Spark Streaming receives an input data stream
    and internally breaks that data stream into multiple smaller batches (the size
    of which is based on the *batch interval*). The Spark engine processes those batches
    of input data to a result set of batches of processed data.
  prefs: []
  type: TYPE_NORMAL
- en: '![What is Spark Streaming?](img/B05793_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Apache Spark Streaming Programming Guide at: [http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html)'
  prefs: []
  type: TYPE_NORMAL
- en: The key abstraction for Spark Streaming is Discretized Stream (DStream), which
    represents the previously mentioned small batches that make up the stream of data.
    DStreams are built on RDDs, allowing Spark developers to work within the same
    context of RDDs and batches, only now applying it to their streaming problems.
    Also, an important aspect is that, because you are using Apache Spark, Spark Streaming
    integrates with MLlib, SQL, DataFrames, and GraphX.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure denotes the basic components of Spark Streaming:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is Spark Streaming?](img/B05793_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Apache Spark Streaming Programming Guide at: [http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark Streaming is a high-level API that provides fault-tolerant *exactly-once*
    semantics for stateful operations. Spark Streaming has built in *receivers* that
    can take on many sources, with the most common being Apache Kafka, Flume, HDFS/S3,
    Kinesis, and Twitter. For example, the most commonly used integration between
    Kafka and Spark Streaming is well documented in the Spark Streaming + Kafka Integration
    Guide found at: [https://spark.apache.org/docs/latest/streaming-kafka-integration.html](https://spark.apache.org/docs/latest/streaming-kafka-integration.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Also, you can create your own *custom receiver*, such as the Meetup Receiver
    ([https://github.com/actions/meetup-stream/blob/master/src/main/scala/receiver/MeetupReceiver.scala](https://github.com/actions/meetup-stream/blob/master/src/main/scala/receiver/MeetupReceiver.scala)),
    which allows you to read the Meetup Streaming API ([https://www.meetup.com/meetup_api/docs/stream/2/rsvps/)](https://www.meetup.com/meetup_api/docs/stream/2/rsvps/))
    using Spark Streaming.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Watch the Meetup Receiver in Action**'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are interested in seeking the Spark Streaming Meetup Receiver in action,
    you can refer to the Databricks notebooks at: [https://github.com/dennyglee/databricks/tree/master/notebooks/Users/denny%40databricks.com/content/Streaming%20Meetup%20RSVPs](https://github.com/dennyglee/databricks/tree/master/notebooks/Users/denny%40databricks.com/content/Streaming%20Meetup%20RSVPs)
    which utilize the previously mentioned Meetup Receiver.'
  prefs: []
  type: TYPE_NORMAL
- en: The following is a screenshot of the notebook in action left window, while viewing
    the Spark UI (Streaming Tab) on the right.
  prefs: []
  type: TYPE_NORMAL
- en: '![What is Spark Streaming?](img/B05793_10_03a.jpg)![What is Spark Streaming?](img/B05793_10_03b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You will be able to use Spark Streaming to receive Meetup RSVPs from around
    the country (or world) and get a near real-time summary of Meetup RSVPs by state
    (or country). Note, these notebooks are currently written in `Scala`.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need Spark Streaming?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As noted by Tathagata Das – committer and member of the project management
    committee (PMC) to the Apache Spark project and lead developer of Spark Streaming
    – in the Datanami article *Spark Streaming: What is It and Who''s Using it* ([https://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/](https://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/)),
    there is a *business need* for streaming. With the prevalence of online transactions
    and social media, as well as sensors and devices, companies are generating and
    processing more data at a faster rate.'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to develop actionable insight at scale and in real time provides
    those businesses with a competitive advantage. Whether you are detecting fraudulent
    transactions, providing real-time detection of sensor anomalies, or reacting to
    the next viral tweet, streaming analytics is becoming increasingly important in
    data scientists' and data engineer's toolbox.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason Spark Streaming is itself being rapidly adopted is because Apache
    Spark unifies all of these disparate data processing paradigms (Machine Learning
    via ML and MLlib, Spark SQL, and Streaming) within the same framework. So, you
    can go from training machine learning models (ML or MLlib), to scoring data with
    these models (Streaming) and perform analysis using your favourite BI tool (SQL)
    – all within the same framework. Companies including Uber, Netflix, and Pinterest
    often showcase their Spark Streaming use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '*How Uber Uses Spark and Hadoop to Optimize Customer Experience*: [https://www.datanami.com/2015/10/05/how-uber-uses-spark-and-hadoop-to-optimize-customer-experience/](https://www.datanami.com/2015/10/05/how-uber-uses-spark-and-hadoop-to-optimize-customer-experience/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Spark and Spark Streaming at Netflix*: [https://spark-summit.org/2015/events/spark-and-spark-streaming-at-netflix/](https://spark-summit.org/2015/events/spark-and-spark-streaming-at-netflix/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Can Spark Streaming survive Chaos Monkey?* [http://techblog.netflix.com/2015/03/can-spark-streaming-survive-chaos-monkey.html](http://techblog.netflix.com/2015/03/can-spark-streaming-survive-chaos-monkey.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Real-time analytics at Pinterest*: [https://engineering.pinterest.com/blog/real-time-analytics-pinterest](https://engineering.pinterest.com/blog/real-time-analytics-pinterest)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Currently, there are four broad use cases surrounding Spark Streaming:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Streaming ETL**: Data is continuously being cleansed and aggregated prior
    to being pushed downstream. This is commonly done to reduce the amount of data
    to be stored in the final data store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Triggers**: Real-time detection of behavioral or anomaly events trigger immediate
    and downstream actions. For example, a device that is within the proximity of
    a detector or beacon will trigger an alert.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data enrichment**: Real-time data joined to other datasets allowing for richer
    analysis. For example, including real-time weather information with flight information
    to build better travel alerts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex sessions and continuous learning**: Multiple sets of events associated
    with real-time streams are continuously analyzed and/or updating machine learning
    models. For example, the stream of user activity associated with an online game
    that allows us to better segment the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the Spark Streaming application data flow?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following figure provides the data flow between the Spark driver, workers,
    streaming sources and targets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is the Spark Streaming application data flow?](img/B05793_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It all starts with the Spark Streaming Context, represented by `ssc.start()`
    in the preceding figure:'
  prefs: []
  type: TYPE_NORMAL
- en: When the Spark Streaming Context starts, the driver will execute a long-running
    task on the executors (that is, the Spark workers).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Receiver** on the executors (**Executor 1** in this diagram) receives
    a data stream from the Streaming Sources. With the incoming data stream, the receiver
    divides the stream into blocks and keeps these blocks in memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These blocks are also replicated to another executor to avoid data loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The block ID information is transmitted to the **Block Management Master** on
    the driver.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For every batch interval configured within Spark Streaming Context (commonly
    this is every 1 second), the driver will launch Spark tasks to process the blocks.
    Those blocks are then persisted to any number of target data stores, including
    cloud storage (for example, S3, WASB, and so on), relational data stores (for
    example, MySQL, PostgreSQL, and so on), and NoSQL stores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suffice it to say, there are a lot of moving parts for a streaming application
    that need to be continually optimized and configured. Most of the documentation
    for Spark Streaming is more complete in Scala, so, as you are working with the
    Python APIs, you may sometimes need to reference the Scala version of the documentation
    instead. If this happens to you, please file a bug and/or fill out a PR if you
    have a proposed fix ([https://issues.apache.org/jira/browse/spark/](https://issues.apache.org/jira/browse/spark/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'For a deeper dive on this topic, please refer to:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Spark 1.6 Streaming Programming Guide*: [https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html](https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Tathagata Das'' Deep Dive with Spark Streaming (Spark Meetup 2013-06-17)*:
    [http://www.slideshare.net/spark-project/deep-divewithsparkstreaming-tathagatadassparkmeetup20130617](http://www.slideshare.net/spark-project/deep-divewithsparkstreaming-tathagatadassparkmeetup20130617)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Simple streaming application using DStreams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s create a simple word count example using Spark Streaming in Python.
    For this example, we will be working with DStream – the Discretized Stream of
    small batches that make up the stream of data. The example used for this section
    of the book can be found in its entirety at: [https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/streaming_word_count.py](https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/streaming_word_count.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This word count example will use the Linux / Unix `nc` command – it is a simple
    utility that reads and writes data across network connections. We will use two
    different bash terminals, one using the `nc` command to send words to our computer''s
    local port (`9999`) and one terminal that will run Spark Streaming to receive
    those words and count them. The initial set of commands for our script are noted
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are some important call outs for the preceding commands:'
  prefs: []
  type: TYPE_NORMAL
- en: The `StreamingContext` on line 9 is the entry point into Spark Streaming
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `1` of `...(sc, 1)` on line 9 is the *batch interval*; in this case, we
    are running micro-batches every second.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `lines` on line 12 is the `DStream` representing the data stream extracted
    via the `ssc.socketTextStream`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As noted in the description, the `ssc.socketTextStream` is the Spark Streaming
    method to review a text stream for a particular socket; in this case, your local
    computer on socket `9999`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next few lines of code (as described in the comments), split the lines
    DStream into words and then, using RDDs, count each word in each batch of data
    and print this information out to the console (line number 9):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The final set of lines of the code start Spark Streaming (`ssc.start()`) and
    then await a termination command to stop running (for example, `<Ctrl><C>`). If
    no termination command is sent, then the Spark Streaming program will continue
    running.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that you have your script, as noted earlier, open two terminal windows
    – one for your `nc` command, and one for your Spark Streaming Program. To start
    the `nc` command, from one of your terminals, type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Everything you type from this point onwards in that terminal will be transmitted
    to port `9999`, as noted in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple streaming application using DStreams](img/B05793_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this example (as noted previously), I typed the words **green** three times
    and **blue** five times. From the other terminal screen, let's run the Python
    streaming script you just created. In this example, I named the script `streaming_word_count.py../bin/spark-submit
    streaming_word_count.py localhost 9999`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command will run the `streaming_word_count.py` script, reading your local
    computer (that is, `localhost`) port `9999` to receive any words sent to that
    socket. As you have already sent information to the port on the first screen,
    shortly after starting up the script, your Spark Streaming program will read the
    words sent to port `9999` and perform a word count as noted in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple streaming application using DStreams](img/B05793_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `streaming_word_count.py` script will continue to read and print any new
    information to the console. Going back to our first terminal (with the `nc` command),
    we now can type our next set of words, as noted in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple streaming application using DStreams](img/B05793_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Reviewing the streaming script in the second terminal, you will notice that
    this script continues to run every second (that is, the configured *batch interval*),
    and you will notice the calculated word count for `gohawks` a few seconds later:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple streaming application using DStreams](img/B05793_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With this relatively simple script, now you can see Spark Streaming in action
    with Python. But if you continue typing words into the `nc` terminal, you will
    notice that this information is not aggregated. For example, if we continue to
    write green in the `nc` terminal (as noted here):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple streaming application using DStreams](img/B05793_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Spark Streaming terminal will report the current snapshot of data; that
    is, the two additional green values (as noted here):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple streaming application using DStreams](img/B05793_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: What did not happen was the concept of global aggregations, where we would keep
    *state* for this information. What this means is that, instead of reporting 2
    new greens, we could get Spark Streaming to give us the overall counts of green,
    for example, 7 greens, 5 blues, and 1 gohawks. We will talk about global aggregations
    in the form of `UpdateStateByKey /` `mapWithState` in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For other good PySpark Streaming examples, check out:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Network Wordcount (in Apache Spark GitHub repo): [https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/network_wordcount.py](https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/network_wordcount.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Streaming Examples: [https://github.com/apache/spark/tree/master/examples/src/main/python/streaming](https://github.com/apache/spark/tree/master/examples/src/main/python/streaming)'
  prefs: []
  type: TYPE_NORMAL
- en: 'S3 FileStream Wordcount (Databricks notebook): [https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#07%20Spark%20Streaming/06%20FileStream%20Word%20Count%20-%20Python.html](https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#07%20Spark%20Streaming/06%20FileStream%20Word%20Count%20-%20Python.html)'
  prefs: []
  type: TYPE_NORMAL
- en: A quick primer on global aggregations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As noted in the previous section, so far, our script has performed a point
    in time streaming word count. The following diagram denotes the **lines DStream**
    and its micro-batches as per how our script had executed in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A quick primer on global aggregations](img/B05793_10_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: At the 1 second mark, our Python Spark Streaming script returned the value of
    `{(blue, 5), (green, 3)}`, at the 2 second mark it returned `{(gohawks, 1)}`,
    and at the 4 second mark, it returned `{(green, 2)}`. But what if you had wanted
    the aggregate word count over a specific time window?
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure represents us calculating a stateful aggregation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A quick primer on global aggregations](img/B05793_10_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, we have a time window between 0-5 seconds. Note, that in our
    script we have not got the specified time window: each second, we calculate the
    cumulative sum of the words. Therefore, at the 2 second mark, the output is not
    just the `green` and `blue` from the 1 second mark, but it also includes the `gohawks`
    from the 2 second mark: `{(blue, 5), (green, 3), (gohawks, 1)}`. At the 4 second
    mark, the additional 2 `greens` provide us a total of `{(blue, 5), (green, 5),
    (gohawks, 1)}`.'
  prefs: []
  type: TYPE_NORMAL
- en: For those of you who regularly work with relational databases, this seems to
    be just a `GROUP BY, SUM()` statement. Yet, in the case of streaming analytics,
    the duration to persist the data long enough to run a `GROUP BY, SUM()` statement
    is longer than the *batch interval* (for example, 1 second). This means that we
    would constantly be running behind and trying to catch up with the data stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you were to run the *1\. Streaming and DataFrames.scala* Databricks
    notebook at [https://github.com/dennyglee/databricks/blob/master/notebooks/Users/denny%40databricks.com/content/Streaming%20Meetup%20RSVPs/1.%20Streaming%20and%20DataFrames.scala](https://github.com/dennyglee/databricks/blob/master/notebooks/Users/denny%40databricks.com/content/Streaming%20Meetup%20RSVPs/1.%20Streaming%20and%20DataFrames.scala),
    and you were to view the Streaming jobs in the Spark UI, you would get something
    like the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A quick primer on global aggregations](img/B05793_10_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice in the graph that the **Scheduling Delay** and **Total Delay** numbers
    are rapidly increasing (for example, average Total Delay is **54 seconds 254 ms**
    and the actual Total Delay is > 2min) and way outside the *batch interval* threshold
    of 1 second. The reason we see this delay is because, inside the streaming code
    for that notebook, we had also run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: That is, inserting any new chunks of data (that is, 1 second RDD micro-batches),
    converting them into a DataFrame (`meetup_stream_json` table), and inserting the
    data into a persistent table (`meetup_stream` table). Persisting the data in this
    fashion led to slow streaming performance with the ever-increasing scheduling
    delays. To solve this problem via *streaming analytics*, this is where creating
    global aggregations via `UpdateStateByKey` (Spark 1.5 and before) or `mapWithState`
    (Spark 1.6 onwards) come in.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on Spark Streaming visualizations, please take the time
    to review *New Visualizations for Understanding Apache Spark Streaming Applications:*
    [https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-apache-spark-streaming-applications.html](https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-apache-spark-streaming-applications.html).
  prefs: []
  type: TYPE_NORMAL
- en: Knowing this, let's re-write the original `streaming_word_count.py` so that
    we now have a *stateful* version called `stateful_streaming_word_count.py`; you
    can get the full version of this script at [https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/stateful_streaming_word_count.py](https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/stateful_streaming_word_count.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'The initial set of commands for our script are noted here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If you recall `streaming_word_count.py`, the primary differences start at line
    11:'
  prefs: []
  type: TYPE_NORMAL
- en: The `ssc.checkpoint("checkpoint")` on line 12 configures a Spark Streaming *checkpoint*.
    To ensure that Spark Streaming is fault tolerant due to its continual operation,
    it needs to checkpoint enough information to fault-tolerant storage, so it can
    recover from failures. Note, we will not dive deep into this concept (though more
    information is available in the following *Tip* section), as many of these configurations
    will be abstracted away with Structured Streaming.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `updateFunc` on line 15 tells the program to update the application's *state*
    (later in the code) via `UpdateStateByKey`. In this case, it is returning a sum
    of the previous value (`last_sum`) and the sum of the new values (`sum(new_values)
    + (last_sum or 0)`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At line 19, we have the same `ssc.socketTextStream` as the previous script.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For more information on Spark Streaming *checkpoint*, some good references
    are:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Spark Streaming Programming Guide > Checkpoint: [https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html#checkpointing](https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html#checkpointing)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Exploring Stateful Streaming with Apache Spark: [http://asyncified.io/2016/07/31/exploring-stateful-streaming-with-apache-spark/](http://asyncified.io/2016/07/31/exploring-stateful-streaming-with-apache-spark/)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The final section of the code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: While lines 10-14 are identical to the previous script, the difference is that
    we now have a `running_counts` variable that splits to get the words and runs
    a map function to count each word in each batch (in the previous script this was
    the `words` and `pairs` variables).
  prefs: []
  type: TYPE_NORMAL
- en: The primary difference is the use of the `updateStateByKey` method, which will
    execute the previously noted `updateFunc` that performs the sum. `updateStateByKey`
    is Spark Streaming's method to perform calculations against your stream of data
    and update the state for each key in a performant manner. It is important to note
    that you would typically use `updateStateByKey` for Spark 1.5 and earlier; the
    performance of these *stateful* global aggregations is proportional to the *size
    of the state*. From Spark 1.6 onwards, you should use `mapWithState`, as the performance
    is proportional to the *size of the batch*.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note, there is more code typically involved with `mapWithState` (in comparison
    to `updateStateByKey`), hence the examples were written using `updateStateByKey`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information about stateful Spark Streaming, including the use of `mapWithState`,
    please refer to:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stateful Network Wordcount Python example: [https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/stateful_network_wordcount.py](https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/stateful_network_wordcount.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Global Aggregation using mapWithState (Scala): [https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#07%20Spark%20Streaming/12%20Global%20Aggregations%20-%20mapWithState.html](https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#07%20Spark%20Streaming/12%20Global%20Aggregations%20-%20mapWithState.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Word count using mapWithState (Scala): [https://docs.cloud.databricks.com/docs/spark/1.6/examples/Streaming%20mapWithState.html](https://docs.cloud.databricks.com/docs/spark/1.6/examples/Streaming%20mapWithState.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Faster Stateful Stream Processing in Apache Spark Streaming: [https://databricks.com/blog/2016/02/01/faster-stateful-stream-processing-in-apache-spark-streaming.html](https://databricks.com/blog/2016/02/01/faster-stateful-stream-processing-in-apache-spark-streaming.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Structured Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With Spark 2.0, the Apache Spark community is working on simplifying streaming
    by introducing the concept of *structured streaming* which bridges the concepts
    of streaming with Datasets/DataFrames (as noted in the following diagram):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing Structured Streaming](img/B05793_10_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As noted in earlier chapters on DataFrames, the execution of SQL and/or DataFrame
    queries within the Spark SQL Engine (and Catalyst Optimizer) revolves around building
    a logical plan, building numerous physical plans, the engine choosing the correct
    physical plan based on its cost optimizer, and then generating the code (i.e.
    *code gen*) that will deliver the results in a performant manner. What *Structured
    Streaming* introduces is the concept of an **Incremental Execution Plan**. When
    working with blocks of data, structured streaming repeatedly applies the execution
    plan for every new set of blocks it receives. By running in this manner, the engine
    can take advantage of the optimizations included within Spark DataFrames/Datasets
    and apply them to an incoming data stream. It will also be easier to integrate
    other DataFrame optimized components of Spark, including ML Pipelines, GraphFrames,
    TensorFrames, and many others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using structured streaming will also simplify your code. For example, the following
    is a pseudo-code example *batch aggregation* that reads a data stream from S3
    and saves it to a MySQL database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a pseudo-code example for a *continous aggregation*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The reason for creating the `sq` variable is that it allows you to check the
    status of your structured streaming job and terminate it, as per the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take the stateful streaming word count script that had used `updateStateByKey`
    and make it a structured streaming word count script; you can get the complete
    `structured_streaming_word_count.py` script at: [https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/structured_streaming_word_count.py](https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/structured_streaming_word_count.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As opposed to the previous scripts, we are now working with the more familiar
    DataFrames code as noted here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The first lines of the script import the necessary classes and establish the
    current `SparkSession`. But, as opposed to the previous streaming scripts, as
    in the next lines of the script noted here, you do not need to establish a Streaming
    Context as this is already included within the `SparkSession`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Instead, the streaming portion of the code is initiated by calling `readStream`
    in line 4.
  prefs: []
  type: TYPE_NORMAL
- en: Lines 3-8 initiate the *reading* of the data stream from port `9999`, just like
    the previous two scripts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of running RDD `flatMap`, `map`, and `reduceByKey` functions to split
    the lines read into words and count each word in each batch, we can use the PySpark
    SQL functions `explode` and `split` as noted in lines 10-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of running `updateStateByKey` or creating an `updateFunc` as per the
    stateful streaming word count script, we can generate the running word count with
    a familiar DataFrame `groupBy` statement and `count()`, as noted in lines 17-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To output this data to the console, we will use `writeStream`, as noted here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Instead of using `pprint()`, we're explicitly calling out `writeStream` to write
    the stream, and defining the format and output mode. While it is a little longer
    to write, these methods and properties are syntactically similar with other DataFrame
    calls and you would only need to change the `outputMode` and `format` properties
    to save it to a Database, file system, console, and so on. Finally, as noted in
    line 10, we will run `awaitTermination` to await to cancel this streaming job.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go back and run our `nc` job in the first terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the following output. As you can see, you get the advantages of stateful
    streaming but using the more familiar DataFrame API:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing Structured Streaming](img/B05793_10_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to note that Structured Streaming is currently (at the time
    of writing) not production-ready. It is, however, a paradigm shift in Spark that
    will hopefully make it easier for data scientists and data engineers to build
    **continuous applications**. While not explicitly called out in the previous sections,
    when working with streaming applications, there are many potential problems that
    you will need to design for, such as late events, partial outputs, state recovery
    on failure, distributed reads and writes, and so on. With structured streaming,
    many of these issues will be abstracted away to make it easier for you to build
    *continuous applications*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We encourage you to try Spark Structured Streaming so you will be able to easily
    build streaming applications as structured streaming matures. As Reynold Xin noted
    in his Spark Summit 2016 East presentation *The Future of Real-Time in Spark*
    ([http://www.slideshare.net/rxin/the-future-of-realtime-in-spark](http://www.slideshare.net/rxin/the-future-of-realtime-in-spark)):'
  prefs: []
  type: TYPE_NORMAL
- en: '"The simplest way to perform streaming analytics is not having to *reason*
    about streaming."'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For more information, here are some additional Structured Streaming resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*PySpark 2.1 Documentation: pyspark.sql.module*: [http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Introducing Apache Spark 2.1*: [https://databricks.com/blog/2016/12/29/introducing-apache-spark-2-1.html](https://databricks.com/blog/2016/12/29/introducing-apache-spark-2-1.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Structuring Apache Spark 2.0: SQL, DataFrames, Datasets and Streaming - by
    Michael Armbrust*: [http://www.slideshare.net/databricks/structuring-spark-dataframes-datasets-and-streaming-62871797](http://www.slideshare.net/databricks/structuring-spark-dataframes-datasets-and-streaming-62871797)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Structured Streaming Programming Guide*: [http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Structured Streaming (aka Streaming DataFrames) [SPARK-8360]*: [https://issues.apache.org/jira/browse/SPARK-8360](https://issues.apache.org/jira/browse/SPARK-8360)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Structured Streaming Programming Abstraction, Semantics, and APIs ­Apache
    JIRA*: [https://issues.apache.org/jira/secure/attachment/12793410/StructuredStreamingProgrammingAbstractionSemanticsandAPIs-ApacheJIRA.pdf](https://issues.apache.org/jira/secure/attachment/12793410/StructuredStreamingProgrammingAbstractionSemanticsandAPIs-ApacheJIRA.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next chapter we will show you how to modularize and package up your PySpark
    application and submit it for execution programmatically.
  prefs: []
  type: TYPE_NORMAL
