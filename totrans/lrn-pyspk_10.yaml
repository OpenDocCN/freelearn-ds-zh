- en: Chapter 10. Structured Streaming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 10 章：结构化流
- en: This chapter will provide a jump-start on the concepts behind Spark Streaming
    and how this has evolved into Structured Streaming. An important aspect of Structured
    Streaming is that it utilizes Spark DataFrames. This shift in paradigm will make
    it easier for Python developers to start working with Spark Streaming.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍 Spark Streaming 背后的概念以及它如何演变成结构化流（Structured Streaming）。结构化流的一个重要方面是它利用
    Spark DataFrames。这种范式转变将使 Python 开发者更容易开始使用 Spark Streaming。
- en: 'In this chapter, your will learn:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习：
- en: What is Spark Streaming?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 Spark Streaming？
- en: Why do we need Spark Streaming?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们为什么需要 Spark Streaming？
- en: What is the Spark Streaming application data flow?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Streaming 应用程序数据流是什么？
- en: Simple streaming application using DStream
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DStream 的简单流应用程序
- en: A quick primer on Spark Streaming global aggregations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Streaming 全球聚合的快速入门
- en: Introducing Structured Streaming
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍结构化流
- en: Note, for the initial sections of this chapter, the example code used will be
    in Scala, as this was how most Spark Streaming code was written. When we start
    focusing on Structured Streaming, we will work with Python examples.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，本章的前几节将使用 Scala 代码示例，因为这是大多数 Spark Streaming 代码的编写方式。当我们开始关注结构化流时，我们将使用 Python
    示例。
- en: What is Spark Streaming?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 Spark Streaming？
- en: At its core, Spark Streaming is a scalable, fault-tolerant streaming system
    that takes the RDD batch paradigm (that is, processing data in batches) and speeds
    it up. While it is a slight over-simplification, basically Spark Streaming operates
    in mini-batches or batch intervals (from 500ms to larger interval windows).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，Spark Streaming 是一个可扩展、容错的流处理系统，它采用了 RDD 批处理范式（即批量处理数据）并加快了处理速度。虽然这是一个轻微的简化，但基本上
    Spark Streaming 在微批或批处理间隔（从 500ms 到更大的间隔窗口）中运行。
- en: As noted in the following diagram, Spark Streaming receives an input data stream
    and internally breaks that data stream into multiple smaller batches (the size
    of which is based on the *batch interval*). The Spark engine processes those batches
    of input data to a result set of batches of processed data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下图所示，Spark Streaming 接收一个输入数据流，并将其内部分解成多个更小的批量（其大小基于 *批处理间隔*）。Spark 引擎处理这些输入数据批量，生成处理后的数据批量结果集。
- en: '![What is Spark Streaming?](img/B05793_10_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![什么是 Spark Streaming？](img/B05793_10_01.jpg)'
- en: 'Source: Apache Spark Streaming Programming Guide at: [http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：Apache Spark Streaming 编程指南，请参阅：[http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html)
- en: The key abstraction for Spark Streaming is Discretized Stream (DStream), which
    represents the previously mentioned small batches that make up the stream of data.
    DStreams are built on RDDs, allowing Spark developers to work within the same
    context of RDDs and batches, only now applying it to their streaming problems.
    Also, an important aspect is that, because you are using Apache Spark, Spark Streaming
    integrates with MLlib, SQL, DataFrames, and GraphX.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 的关键抽象是离散流（Discretized Stream，DStream），它代表了之前提到的构成数据流的小批量。DStreams
    建立在 RDD 之上，允许 Spark 开发者在 RDD 和批处理相同的上下文中工作，现在只是将其应用于他们的流处理问题。此外，一个重要的方面是，因为你使用
    Apache Spark，Spark Streaming 与 MLlib、SQL、DataFrames 和 GraphX 集成。
- en: 'The following figure denotes the basic components of Spark Streaming:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表示 Spark Streaming 的基本组件：
- en: '![What is Spark Streaming?](img/B05793_10_02.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![什么是 Spark Streaming？](img/B05793_10_02.jpg)'
- en: 'Source: Apache Spark Streaming Programming Guide at: [http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：Apache Spark Streaming 编程指南，请参阅：[http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html)
- en: 'Spark Streaming is a high-level API that provides fault-tolerant *exactly-once*
    semantics for stateful operations. Spark Streaming has built in *receivers* that
    can take on many sources, with the most common being Apache Kafka, Flume, HDFS/S3,
    Kinesis, and Twitter. For example, the most commonly used integration between
    Kafka and Spark Streaming is well documented in the Spark Streaming + Kafka Integration
    Guide found at: [https://spark.apache.org/docs/latest/streaming-kafka-integration.html](https://spark.apache.org/docs/latest/streaming-kafka-integration.html).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming是一个高级API，为有状态操作提供容错**精确一次**语义。Spark Streaming内置**接收器**，可以处理许多来源，其中最常见的是Apache
    Kafka、Flume、HDFS/S3、Kinesis和Twitter。例如，Kafka和Spark Streaming之间最常用的集成在Spark Streaming
    + Kafka集成指南中有很好的文档记录，该指南可在[https://spark.apache.org/docs/latest/streaming-kafka-integration.html](https://spark.apache.org/docs/latest/streaming-kafka-integration.html)找到。
- en: Also, you can create your own *custom receiver*, such as the Meetup Receiver
    ([https://github.com/actions/meetup-stream/blob/master/src/main/scala/receiver/MeetupReceiver.scala](https://github.com/actions/meetup-stream/blob/master/src/main/scala/receiver/MeetupReceiver.scala)),
    which allows you to read the Meetup Streaming API ([https://www.meetup.com/meetup_api/docs/stream/2/rsvps/)](https://www.meetup.com/meetup_api/docs/stream/2/rsvps/))
    using Spark Streaming.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还可以创建自己的**自定义接收器**，例如Meetup接收器([https://github.com/actions/meetup-stream/blob/master/src/main/scala/receiver/MeetupReceiver.scala](https://github.com/actions/meetup-stream/blob/master/src/main/scala/receiver/MeetupReceiver.scala))，它允许您使用Spark
    Streaming读取Meetup流式API([https://www.meetup.com/meetup_api/docs/stream/2/rsvps/](https://www.meetup.com/meetup_api/docs/stream/2/rsvps/))。
- en: Note
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Watch the Meetup Receiver in Action**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**观看Meetup接收器在实际操作中的表现**'
- en: 'If you are interested in seeking the Spark Streaming Meetup Receiver in action,
    you can refer to the Databricks notebooks at: [https://github.com/dennyglee/databricks/tree/master/notebooks/Users/denny%40databricks.com/content/Streaming%20Meetup%20RSVPs](https://github.com/dennyglee/databricks/tree/master/notebooks/Users/denny%40databricks.com/content/Streaming%20Meetup%20RSVPs)
    which utilize the previously mentioned Meetup Receiver.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对查看Spark Streaming Meetup接收器在实际操作中的表现感兴趣，您可以参考以下Databricks笔记本：[https://github.com/dennyglee/databricks/tree/master/notebooks/Users/denny%40databricks.com/content/Streaming%20Meetup%20RSVPs](https://github.com/dennyglee/databricks/tree/master/notebooks/Users/denny%40databricks.com/content/Streaming%20Meetup%20RSVPs)，这些笔记本使用了之前提到的Meetup接收器。
- en: The following is a screenshot of the notebook in action left window, while viewing
    the Spark UI (Streaming Tab) on the right.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在左侧窗口中查看Spark UI（流式处理选项卡）时，笔记本的实际截图。
- en: '![What is Spark Streaming?](img/B05793_10_03a.jpg)![What is Spark Streaming?](img/B05793_10_03b.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![什么是Spark Streaming?](img/B05793_10_03a.jpg)![什么是Spark Streaming?](img/B05793_10_03b.jpg)'
- en: You will be able to use Spark Streaming to receive Meetup RSVPs from around
    the country (or world) and get a near real-time summary of Meetup RSVPs by state
    (or country). Note, these notebooks are currently written in `Scala`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 您将能够使用Spark Streaming接收来自全国（或世界）的Meetup RSVP，并通过州（或国家）获得近乎实时的Meetup RSVP摘要。注意，这些笔记本目前是用`Scala`编写的。
- en: Why do we need Spark Streaming?
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们为什么需要Spark Streaming？
- en: 'As noted by Tathagata Das – committer and member of the project management
    committee (PMC) to the Apache Spark project and lead developer of Spark Streaming
    – in the Datanami article *Spark Streaming: What is It and Who''s Using it* ([https://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/](https://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/)),
    there is a *business need* for streaming. With the prevalence of online transactions
    and social media, as well as sensors and devices, companies are generating and
    processing more data at a faster rate.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如Tathagata Das所述——Apache Spark项目的提交者和项目管理委员会（PMC）成员，以及Spark Streaming的首席开发者——在Datanami文章《Spark
    Streaming：它是什么以及谁在使用它》([https://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/](https://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/))中提到，对于流式处理存在**业务需求**。随着在线交易、社交媒体、传感器和设备的普及，公司正在以更快的速度生成和处理更多数据。
- en: The ability to develop actionable insight at scale and in real time provides
    those businesses with a competitive advantage. Whether you are detecting fraudulent
    transactions, providing real-time detection of sensor anomalies, or reacting to
    the next viral tweet, streaming analytics is becoming increasingly important in
    data scientists' and data engineer's toolbox.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 能够在规模和实时性上开发可操作的见解，为这些企业提供竞争优势。无论您是检测欺诈交易、提供传感器异常的实时检测，还是对下一个病毒式推文做出反应，流式分析正在成为数据科学家和数据工程师工具箱中越来越重要的组成部分。
- en: 'The reason Spark Streaming is itself being rapidly adopted is because Apache
    Spark unifies all of these disparate data processing paradigms (Machine Learning
    via ML and MLlib, Spark SQL, and Streaming) within the same framework. So, you
    can go from training machine learning models (ML or MLlib), to scoring data with
    these models (Streaming) and perform analysis using your favourite BI tool (SQL)
    – all within the same framework. Companies including Uber, Netflix, and Pinterest
    often showcase their Spark Streaming use cases:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming之所以被迅速采用，是因为Apache Spark将所有这些不同的数据处理范式（通过ML和MLlib进行机器学习、Spark
    SQL和流式处理）统一在同一个框架中。因此，你可以从训练机器学习模型（ML或MLlib）开始，到使用这些模型评分数据（流式处理），再到使用你喜欢的BI工具进行数据分析（SQL）——所有这些都在同一个框架内完成。包括Uber、Netflix和Pinterest在内的公司经常展示他们的Spark
    Streaming应用案例：
- en: '*How Uber Uses Spark and Hadoop to Optimize Customer Experience*: [https://www.datanami.com/2015/10/05/how-uber-uses-spark-and-hadoop-to-optimize-customer-experience/](https://www.datanami.com/2015/10/05/how-uber-uses-spark-and-hadoop-to-optimize-customer-experience/)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Uber如何使用Spark和Hadoop优化客户体验*：[https://www.datanami.com/2015/10/05/how-uber-uses-spark-and-hadoop-to-optimize-customer-experience/](https://www.datanami.com/2015/10/05/how-uber-uses-spark-and-hadoop-to-optimize-customer-experience/)'
- en: '*Spark and Spark Streaming at Netflix*: [https://spark-summit.org/2015/events/spark-and-spark-streaming-at-netflix/](https://spark-summit.org/2015/events/spark-and-spark-streaming-at-netflix/)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Netflix的Spark和Spark Streaming*：[https://spark-summit.org/2015/events/spark-and-spark-streaming-at-netflix/](https://spark-summit.org/2015/events/spark-and-spark-streaming-at-netflix/)'
- en: '*Can Spark Streaming survive Chaos Monkey?* [http://techblog.netflix.com/2015/03/can-spark-streaming-survive-chaos-monkey.html](http://techblog.netflix.com/2015/03/can-spark-streaming-survive-chaos-monkey.html)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Spark Streaming能否在混沌猴子测试中生存？* [http://techblog.netflix.com/2015/03/can-spark-streaming-survive-chaos-monkey.html](http://techblog.netflix.com/2015/03/can-spark-streaming-survive-chaos-monkey.html)'
- en: '*Real-time analytics at Pinterest*: [https://engineering.pinterest.com/blog/real-time-analytics-pinterest](https://engineering.pinterest.com/blog/real-time-analytics-pinterest)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Pinterest的实时分析*：[https://engineering.pinterest.com/blog/real-time-analytics-pinterest](https://engineering.pinterest.com/blog/real-time-analytics-pinterest)'
- en: 'Currently, there are four broad use cases surrounding Spark Streaming:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，围绕Spark Streaming有四个广泛的应用场景：
- en: '**Streaming ETL**: Data is continuously being cleansed and aggregated prior
    to being pushed downstream. This is commonly done to reduce the amount of data
    to be stored in the final data store.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流式ETL**：在将数据推送到下游之前，数据会持续进行清洗和聚合。这通常是为了减少最终数据存储中需要存储的数据量。'
- en: '**Triggers**: Real-time detection of behavioral or anomaly events trigger immediate
    and downstream actions. For example, a device that is within the proximity of
    a detector or beacon will trigger an alert.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**触发器**：实时检测行为或异常事件会触发立即和下游操作。例如，一个位于检测器或信标附近的设备将触发一个警报。'
- en: '**Data enrichment**: Real-time data joined to other datasets allowing for richer
    analysis. For example, including real-time weather information with flight information
    to build better travel alerts.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据丰富**：将实时数据与其他数据集合并，以进行更深入的分析。例如，将实时天气信息与航班信息结合，以构建更好的旅行警报。'
- en: '**Complex sessions and continuous learning**: Multiple sets of events associated
    with real-time streams are continuously analyzed and/or updating machine learning
    models. For example, the stream of user activity associated with an online game
    that allows us to better segment the user.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂会话和持续学习**：与实时流相关联的多组事件持续进行分析和/或更新机器学习模型。例如，与在线游戏相关的用户活动流，使我们能够更好地细分用户。'
- en: What is the Spark Streaming application data flow?
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Streaming应用数据流是什么？
- en: 'The following figure provides the data flow between the Spark driver, workers,
    streaming sources and targets:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 下图提供了Spark驱动程序、工作节点、流式数据源和目标之间的数据流：
- en: '![What is the Spark Streaming application data flow?](img/B05793_10_04.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![Spark Streaming应用数据流是什么？](img/B05793_10_04.jpg)'
- en: 'It all starts with the Spark Streaming Context, represented by `ssc.start()`
    in the preceding figure:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这一切都始于Spark Streaming上下文，如前图所示`ssc.start()`：
- en: When the Spark Streaming Context starts, the driver will execute a long-running
    task on the executors (that is, the Spark workers).
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当Spark Streaming上下文启动时，驱动程序将在executors（即Spark工作节点）上执行一个长时间运行的任务。
- en: The **Receiver** on the executors (**Executor 1** in this diagram) receives
    a data stream from the Streaming Sources. With the incoming data stream, the receiver
    divides the stream into blocks and keeps these blocks in memory.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器上的**接收器**（此图中的**Executor 1**）从流式源接收数据流。随着数据流的到来，接收器将流分成块，并将这些块保存在内存中。
- en: These blocks are also replicated to another executor to avoid data loss.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些块也被复制到另一个执行器，以避免数据丢失。
- en: The block ID information is transmitted to the **Block Management Master** on
    the driver.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 块ID信息被传输到驱动程序上的**块管理主节点**。
- en: For every batch interval configured within Spark Streaming Context (commonly
    this is every 1 second), the driver will launch Spark tasks to process the blocks.
    Those blocks are then persisted to any number of target data stores, including
    cloud storage (for example, S3, WASB, and so on), relational data stores (for
    example, MySQL, PostgreSQL, and so on), and NoSQL stores.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于在Spark Streaming上下文中配置的每个批次间隔（通常这是每秒一次），驱动程序将启动Spark任务来处理这些块。然后，这些块被持久化到任意数量的目标数据存储中，包括云存储（例如，S3、WASB等）、关系型数据存储（例如，MySQL、PostgreSQL等）和NoSQL存储。
- en: Suffice it to say, there are a lot of moving parts for a streaming application
    that need to be continually optimized and configured. Most of the documentation
    for Spark Streaming is more complete in Scala, so, as you are working with the
    Python APIs, you may sometimes need to reference the Scala version of the documentation
    instead. If this happens to you, please file a bug and/or fill out a PR if you
    have a proposed fix ([https://issues.apache.org/jira/browse/spark/](https://issues.apache.org/jira/browse/spark/)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于流式应用程序来说，有很多动态部分需要不断优化和配置。Spark Streaming的大部分文档在Scala中更为完整，因此，当您使用Python API时，您可能有时需要参考Scala版本的文档。如果这种情况发生在您身上，请提交一个错误报告，并且/或者如果您有一个建议的修复方案，请填写一个PR
    ([https://issues.apache.org/jira/browse/spark/](https://issues.apache.org/jira/browse/spark/))。
- en: 'For a deeper dive on this topic, please refer to:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个主题的更深入探讨，请参阅：
- en: '*Spark 1.6 Streaming Programming Guide*: [https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html](https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html)'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Spark 1.6流式编程指南*: [https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html](https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html)'
- en: '*Tathagata Das'' Deep Dive with Spark Streaming (Spark Meetup 2013-06-17)*:
    [http://www.slideshare.net/spark-project/deep-divewithsparkstreaming-tathagatadassparkmeetup20130617](http://www.slideshare.net/spark-project/deep-divewithsparkstreaming-tathagatadassparkmeetup20130617)'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*《达沙塔·达斯深入浅出Spark Streaming（Spark Meetup 2013-06-17）》*: [http://www.slideshare.net/spark-project/deep-divewithsparkstreaming-tathagatadassparkmeetup20130617](http://www.slideshare.net/spark-project/deep-divewithsparkstreaming-tathagatadassparkmeetup20130617)'
- en: Simple streaming application using DStreams
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DStreams的简单流式应用程序
- en: 'Let''s create a simple word count example using Spark Streaming in Python.
    For this example, we will be working with DStream – the Discretized Stream of
    small batches that make up the stream of data. The example used for this section
    of the book can be found in its entirety at: [https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/streaming_word_count.py](https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/streaming_word_count.py).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Python中的Spark Streaming创建一个简单的词频统计示例。对于这个示例，我们将使用DStream——组成数据流的小批次的离散流。本书本节使用的示例可以在以下位置找到：[https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/streaming_word_count.py](https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/streaming_word_count.py)。
- en: 'This word count example will use the Linux / Unix `nc` command – it is a simple
    utility that reads and writes data across network connections. We will use two
    different bash terminals, one using the `nc` command to send words to our computer''s
    local port (`9999`) and one terminal that will run Spark Streaming to receive
    those words and count them. The initial set of commands for our script are noted
    here:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个词频统计示例将使用Linux/Unix的`nc`命令——这是一个简单的工具，可以在网络连接中读取和写入数据。我们将使用两个不同的bash终端，一个使用`nc`命令将单词发送到我们计算机的本地端口（`9999`），另一个终端将运行Spark
    Streaming以接收这些单词并计数。我们脚本的初始命令集在此处记录：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here are some important call outs for the preceding commands:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些关于前面命令的重要说明：
- en: The `StreamingContext` on line 9 is the entry point into Spark Streaming
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第9行的`StreamingContext`是Spark Streaming的入口点
- en: The `1` of `...(sc, 1)` on line 9 is the *batch interval*; in this case, we
    are running micro-batches every second.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第9行`...(sc, 1)`中的`1`是*批次间隔*；在这种情况下，我们每秒运行微批次。
- en: The `lines` on line 12 is the `DStream` representing the data stream extracted
    via the `ssc.socketTextStream`.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第 12 行的 `lines` 是通过 `ssc.socketTextStream` 提取的数据流的 `DStream`。
- en: As noted in the description, the `ssc.socketTextStream` is the Spark Streaming
    method to review a text stream for a particular socket; in this case, your local
    computer on socket `9999`.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如描述中所述，`ssc.socketTextStream` 是 Spark Streaming 方法，用于审查特定套接字的文本流；在这种情况下，你的本地计算机在套接字
    `9999` 上。
- en: 'The next few lines of code (as described in the comments), split the lines
    DStream into words and then, using RDDs, count each word in each batch of data
    and print this information out to the console (line number 9):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下几行代码（如注释中所述），将行 DStream 分割成单词，然后使用 RDDs，对每个数据批次中的每个单词进行计数，并将此信息打印到控制台（第 9 行）：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The final set of lines of the code start Spark Streaming (`ssc.start()`) and
    then await a termination command to stop running (for example, `<Ctrl><C>`). If
    no termination command is sent, then the Spark Streaming program will continue
    running.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的最后一行启动了 Spark Streaming (`ssc.start()`)，然后等待一个终止命令来停止运行（例如，`<Ctrl><C>`）。如果没有发送终止命令，那么
    Spark Streaming 程序将继续运行。
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that you have your script, as noted earlier, open two terminal windows
    – one for your `nc` command, and one for your Spark Streaming Program. To start
    the `nc` command, from one of your terminals, type:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了脚本，如之前所述，打开两个终端窗口——一个用于你的 `nc` 命令，另一个用于 Spark Streaming 程序。要启动 `nc` 命令，在你的一个终端中输入：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Everything you type from this point onwards in that terminal will be transmitted
    to port `9999`, as noted in the following screenshot:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，你在这个终端中输入的所有内容都将被传输到端口 `9999`，如下面的截图所示：
- en: '![Simple streaming application using DStreams](img/B05793_10_05.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![使用 DStreams 的简单流应用程序](img/B05793_10_05.jpg)'
- en: In this example (as noted previously), I typed the words **green** three times
    and **blue** five times. From the other terminal screen, let's run the Python
    streaming script you just created. In this example, I named the script `streaming_word_count.py../bin/spark-submit
    streaming_word_count.py localhost 9999`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子（如之前所述）中，我输入了三次单词 **green** 和五次 **blue**。从另一个终端屏幕，让我们运行你刚刚创建的 Python 流脚本。在这个例子中，我将脚本命名为
    `streaming_word_count.py` 并使用命令 `../bin/spark-submit streaming_word_count.py localhost
    9999`。
- en: 'The command will run the `streaming_word_count.py` script, reading your local
    computer (that is, `localhost`) port `9999` to receive any words sent to that
    socket. As you have already sent information to the port on the first screen,
    shortly after starting up the script, your Spark Streaming program will read the
    words sent to port `9999` and perform a word count as noted in the following screenshot:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令将运行 `streaming_word_count.py` 脚本，读取你的本地计算机（即 `localhost`）端口 `9999` 以接收发送到该套接字的所有单词。由于你已经在第一个屏幕上向该端口发送了信息，脚本启动后不久，Spark
    Streaming 程序将读取发送到端口 `9999` 的单词并执行单词计数，如下面的截图所示：
- en: '![Simple streaming application using DStreams](img/B05793_10_06.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![使用 DStreams 的简单流应用程序](img/B05793_10_06.jpg)'
- en: 'The `streaming_word_count.py` script will continue to read and print any new
    information to the console. Going back to our first terminal (with the `nc` command),
    we now can type our next set of words, as noted in the following screenshot:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`streaming_word_count.py` 脚本将继续读取并打印任何新的信息到控制台。回到我们第一个终端（使用 `nc` 命令），我们现在可以输入下一组单词，如下面的截图所示：'
- en: '![Simple streaming application using DStreams](img/B05793_10_07.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![使用 DStreams 的简单流应用程序](img/B05793_10_07.jpg)'
- en: 'Reviewing the streaming script in the second terminal, you will notice that
    this script continues to run every second (that is, the configured *batch interval*),
    and you will notice the calculated word count for `gohawks` a few seconds later:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 查看第二个终端中的流脚本，你会注意到这个脚本每秒继续运行（即配置的 *批处理间隔*），你会在几秒钟后注意到计算出的 `gohawks` 单词计数：
- en: '![Simple streaming application using DStreams](img/B05793_10_08.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![使用 DStreams 的简单流应用程序](img/B05793_10_08.jpg)'
- en: 'With this relatively simple script, now you can see Spark Streaming in action
    with Python. But if you continue typing words into the `nc` terminal, you will
    notice that this information is not aggregated. For example, if we continue to
    write green in the `nc` terminal (as noted here):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个相对简单的脚本，现在你可以看到 Spark Streaming 使用 Python 的实际应用。但是，如果你继续在 `nc` 终端中输入单词，你会注意到这些信息没有被聚合。例如，如果我们继续在
    `nc` 终端中写入绿色（如下所示）：
- en: '![Simple streaming application using DStreams](img/B05793_10_09.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![使用 DStreams 的简单流应用程序](img/B05793_10_09.jpg)'
- en: 'The Spark Streaming terminal will report the current snapshot of data; that
    is, the two additional green values (as noted here):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 终端将报告当前数据快照；即，这里提到的两个额外的 `green` 值：
- en: '![Simple streaming application using DStreams](img/B05793_10_10.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![使用 DStreams 的简单流式应用程序](img/B05793_10_10.jpg)'
- en: What did not happen was the concept of global aggregations, where we would keep
    *state* for this information. What this means is that, instead of reporting 2
    new greens, we could get Spark Streaming to give us the overall counts of green,
    for example, 7 greens, 5 blues, and 1 gohawks. We will talk about global aggregations
    in the form of `UpdateStateByKey /` `mapWithState` in the next section.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 没有发生的是全局聚合的概念，其中我们会保留该信息的 *状态*。这意味着，而不是报告 2 个新的 `green`，我们可以让 Spark Streaming
    给我们提供绿色的总体计数，例如，7 个 `green`，5 个 `blue`，和 1 个 `gohawks`。我们将在下一节以 `UpdateStateByKey`
    / `mapWithState` 的形式讨论全局聚合。
- en: Tip
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'For other good PySpark Streaming examples, check out:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他好的 PySpark 流式处理示例，请查看：
- en: 'Network Wordcount (in Apache Spark GitHub repo): [https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/network_wordcount.py](https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/network_wordcount.py)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 网络单词计数（在 Apache Spark GitHub 仓库中）：[https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/network_wordcount.py](https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/network_wordcount.py)
- en: 'Python Streaming Examples: [https://github.com/apache/spark/tree/master/examples/src/main/python/streaming](https://github.com/apache/spark/tree/master/examples/src/main/python/streaming)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Python 流式处理示例：[https://github.com/apache/spark/tree/master/examples/src/main/python/streaming](https://github.com/apache/spark/tree/master/examples/src/main/python/streaming)
- en: 'S3 FileStream Wordcount (Databricks notebook): [https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#07%20Spark%20Streaming/06%20FileStream%20Word%20Count%20-%20Python.html](https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#07%20Spark%20Streaming/06%20FileStream%20Word%20Count%20-%20Python.html)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: S3 FileStream Wordcount（Databricks 笔记本）：[https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#07%20Spark%20Streaming/06%20FileStream%20Word%20Count%20-%20Python.html](https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#07%20Spark%20Streaming/06%20FileStream%20Word%20Count%20-%20Python.html)
- en: A quick primer on global aggregations
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全球聚合的快速入门
- en: 'As noted in the previous section, so far, our script has performed a point
    in time streaming word count. The following diagram denotes the **lines DStream**
    and its micro-batches as per how our script had executed in the previous section:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如前节所述，到目前为止，我们的脚本已经执行了点时间流式单词计数。以下图表示了 **lines DStream** 及其微批处理，正如我们在前一节中脚本执行的那样：
- en: '![A quick primer on global aggregations](img/B05793_10_11.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![全球聚合的快速入门](img/B05793_10_11.jpg)'
- en: At the 1 second mark, our Python Spark Streaming script returned the value of
    `{(blue, 5), (green, 3)}`, at the 2 second mark it returned `{(gohawks, 1)}`,
    and at the 4 second mark, it returned `{(green, 2)}`. But what if you had wanted
    the aggregate word count over a specific time window?
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在 1 秒的标记处，我们的 Python Spark Streaming 脚本返回了 `{(blue, 5), (green, 3)}` 的值，在 2
    秒的标记处返回了 `{(gohawks, 1)}`，在 4 秒的标记处返回了 `{(green, 2)}`。但如果你想要特定时间窗口内的聚合单词计数呢？
- en: 'The following figure represents us calculating a stateful aggregation:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了我们计算状态聚合的过程：
- en: '![A quick primer on global aggregations](img/B05793_10_12.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![全球聚合的快速入门](img/B05793_10_12.jpg)'
- en: 'In this case, we have a time window between 0-5 seconds. Note, that in our
    script we have not got the specified time window: each second, we calculate the
    cumulative sum of the words. Therefore, at the 2 second mark, the output is not
    just the `green` and `blue` from the 1 second mark, but it also includes the `gohawks`
    from the 2 second mark: `{(blue, 5), (green, 3), (gohawks, 1)}`. At the 4 second
    mark, the additional 2 `greens` provide us a total of `{(blue, 5), (green, 5),
    (gohawks, 1)}`.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有一个 0-5 秒的时间窗口。注意，在我们的脚本中，我们没有得到指定的时间窗口：每秒钟，我们计算单词的累积总和。因此，在 2 秒的标记处，输出不仅仅是
    1 秒标记处的 `green` 和 `blue`，还包括 2 秒标记处的 `gohawks`：`{(blue, 5), (green, 3), (gohawks,
    1)}`。在 4 秒的标记处，额外的 2 个 `green` 使总数达到 `{(blue, 5), (green, 5), (gohawks, 1)}`。
- en: For those of you who regularly work with relational databases, this seems to
    be just a `GROUP BY, SUM()` statement. Yet, in the case of streaming analytics,
    the duration to persist the data long enough to run a `GROUP BY, SUM()` statement
    is longer than the *batch interval* (for example, 1 second). This means that we
    would constantly be running behind and trying to catch up with the data stream.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些经常与关系型数据库工作的人来说，这似乎只是一个 `GROUP BY, SUM()` 语句。然而，在流式分析的情况下，持久化数据以运行 `GROUP
    BY, SUM()` 语句的时间比 *批处理间隔*（例如，1秒）要长。这意味着我们将会不断落后并试图赶上数据流。
- en: 'For example, if you were to run the *1\. Streaming and DataFrames.scala* Databricks
    notebook at [https://github.com/dennyglee/databricks/blob/master/notebooks/Users/denny%40databricks.com/content/Streaming%20Meetup%20RSVPs/1.%20Streaming%20and%20DataFrames.scala](https://github.com/dennyglee/databricks/blob/master/notebooks/Users/denny%40databricks.com/content/Streaming%20Meetup%20RSVPs/1.%20Streaming%20and%20DataFrames.scala),
    and you were to view the Streaming jobs in the Spark UI, you would get something
    like the following figure:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你要运行 [https://github.com/dennyglee/databricks/blob/master/notebooks/Users/denny%40databricks.com/content/Streaming%20Meetup%20RSVPs/1.%20Streaming%20and%20DataFrames.scala](https://github.com/dennyglee/databricks/blob/master/notebooks/Users/denny%40databricks.com/content/Streaming%20Meetup%20RSVPs/1.%20Streaming%20and%20DataFrames.scala)
    的 *1\. Streaming and DataFrames.scala* Databricks笔记本，并且查看Spark UI中的流式作业，你会得到以下类似图示：
- en: '![A quick primer on global aggregations](img/B05793_10_13.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![关于全局聚合的快速入门](img/B05793_10_13.jpg)'
- en: 'Notice in the graph that the **Scheduling Delay** and **Total Delay** numbers
    are rapidly increasing (for example, average Total Delay is **54 seconds 254 ms**
    and the actual Total Delay is > 2min) and way outside the *batch interval* threshold
    of 1 second. The reason we see this delay is because, inside the streaming code
    for that notebook, we had also run the following code:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在图中，**调度延迟**和**总延迟**的数字正在迅速增加（例如，平均总延迟为**54秒254毫秒**，实际总延迟大于2分钟）并且远远超出1秒的 *批处理间隔*
    阈值。我们看到这种延迟的原因是因为，在那个笔记本的流式代码内部，我们也运行了以下代码：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: That is, inserting any new chunks of data (that is, 1 second RDD micro-batches),
    converting them into a DataFrame (`meetup_stream_json` table), and inserting the
    data into a persistent table (`meetup_stream` table). Persisting the data in this
    fashion led to slow streaming performance with the ever-increasing scheduling
    delays. To solve this problem via *streaming analytics*, this is where creating
    global aggregations via `UpdateStateByKey` (Spark 1.5 and before) or `mapWithState`
    (Spark 1.6 onwards) come in.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 即，插入任何新的数据块（即，1秒RDD微批），将它们转换为DataFrame（`meetup_stream_json` 表），并将数据插入到持久表中（`meetup_stream`
    表）。以这种方式持久化数据导致了缓慢的流式性能和不断增长的调度延迟。为了通过 *流式分析* 解决这个问题，这就是通过 `UpdateStateByKey`（Spark
    1.5及之前）或 `mapWithState`（Spark 1.6及以后）创建全局聚合的地方。
- en: Tip
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: For more information on Spark Streaming visualizations, please take the time
    to review *New Visualizations for Understanding Apache Spark Streaming Applications:*
    [https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-apache-spark-streaming-applications.html](https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-apache-spark-streaming-applications.html).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Spark Streaming的可视化信息，请花时间查看 *理解Apache Spark Streaming应用程序的新可视化*：[https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-apache-spark-streaming-applications.html](https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-apache-spark-streaming-applications.html)。
- en: Knowing this, let's re-write the original `streaming_word_count.py` so that
    we now have a *stateful* version called `stateful_streaming_word_count.py`; you
    can get the full version of this script at [https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/stateful_streaming_word_count.py](https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/stateful_streaming_word_count.py).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这一点后，让我们重新编写原始的 `streaming_word_count.py`，现在我们有一个名为 `stateful_streaming_word_count.py`
    的 *有状态* 版本；你可以在这个脚本的全版本在 [https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/stateful_streaming_word_count.py](https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/stateful_streaming_word_count.py)。
- en: 'The initial set of commands for our script are noted here:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们脚本的初始命令集合如下所示：
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If you recall `streaming_word_count.py`, the primary differences start at line
    11:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得 `streaming_word_count.py`，主要的不同之处从第11行开始：
- en: The `ssc.checkpoint("checkpoint")` on line 12 configures a Spark Streaming *checkpoint*.
    To ensure that Spark Streaming is fault tolerant due to its continual operation,
    it needs to checkpoint enough information to fault-tolerant storage, so it can
    recover from failures. Note, we will not dive deep into this concept (though more
    information is available in the following *Tip* section), as many of these configurations
    will be abstracted away with Structured Streaming.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第12行的`ssc.checkpoint("checkpoint")`配置了一个Spark Streaming *检查点*。为了确保Spark Streaming由于持续运行而具有容错性，它需要将足够的信息检查点到容错存储中，以便在发生故障时恢复。注意，我们不会深入探讨这个概念（尽管在下面的*小贴士*部分有更多信息），因为许多这些配置都将通过Structured
    Streaming抽象化。
- en: The `updateFunc` on line 15 tells the program to update the application's *state*
    (later in the code) via `UpdateStateByKey`. In this case, it is returning a sum
    of the previous value (`last_sum`) and the sum of the new values (`sum(new_values)
    + (last_sum or 0)`).
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第15行的`updateFunc`告诉程序通过`UpdateStateByKey`更新应用程序的*状态*（在代码的后面部分）。在这种情况下，它返回前一个值（`last_sum`）和新的值的总和（`sum(new_values)
    + (last_sum or 0)`）。
- en: At line 19, we have the same `ssc.socketTextStream` as the previous script.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第19行，我们有与上一个脚本相同的`ssc.socketTextStream`。
- en: Tip
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'For more information on Spark Streaming *checkpoint*, some good references
    are:'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于Spark Streaming *检查点*的更多信息，以下是一些好的参考资料：
- en: 'Spark Streaming Programming Guide > Checkpoint: [https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html#checkpointing](https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html#checkpointing)'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Spark Streaming编程指南 > 检查点: [https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html#checkpointing](https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html#checkpointing)'
- en: 'Exploring Stateful Streaming with Apache Spark: [http://asyncified.io/2016/07/31/exploring-stateful-streaming-with-apache-spark/](http://asyncified.io/2016/07/31/exploring-stateful-streaming-with-apache-spark/)'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '探索Apache Spark中的有状态流: [http://asyncified.io/2016/07/31/exploring-stateful-streaming-with-apache-spark/](http://asyncified.io/2016/07/31/exploring-stateful-streaming-with-apache-spark/)'
- en: 'The final section of the code is as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的最后一部分如下：
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: While lines 10-14 are identical to the previous script, the difference is that
    we now have a `running_counts` variable that splits to get the words and runs
    a map function to count each word in each batch (in the previous script this was
    the `words` and `pairs` variables).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当第10-14行的代码与上一个脚本相同，但区别在于我们现在有一个`running_counts`变量，它将数据拆分以获取单词，并在每个批次中运行一个映射函数来计算每个单词（在之前的脚本中这是`words`和`pairs`变量）。
- en: The primary difference is the use of the `updateStateByKey` method, which will
    execute the previously noted `updateFunc` that performs the sum. `updateStateByKey`
    is Spark Streaming's method to perform calculations against your stream of data
    and update the state for each key in a performant manner. It is important to note
    that you would typically use `updateStateByKey` for Spark 1.5 and earlier; the
    performance of these *stateful* global aggregations is proportional to the *size
    of the state*. From Spark 1.6 onwards, you should use `mapWithState`, as the performance
    is proportional to the *size of the batch*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 主要区别在于使用`updateStateByKey`方法，该方法将执行之前提到的`updateFunc`，该函数执行求和操作。`updateStateByKey`是Spark
    Streaming执行对数据流进行计算并高效更新每个键的状态的方法。需要注意的是，你通常会在Spark 1.5及更早版本中使用`updateStateByKey`；这些*有状态*的全局聚合的性能与*状态的大小*成正比。从Spark
    1.6版本开始，你应该使用`mapWithState`，因为其性能与*批次的大小*成正比。
- en: Tip
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Note, there is more code typically involved with `mapWithState` (in comparison
    to `updateStateByKey`), hence the examples were written using `updateStateByKey`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与`updateStateByKey`相比，`mapWithState`通常涉及更多的代码，因此示例是使用`updateStateByKey`编写的。
- en: 'For more information about stateful Spark Streaming, including the use of `mapWithState`,
    please refer to:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 关于有状态Spark Streaming的更多信息，包括`mapWithState`的使用，请参阅：
- en: 'Stateful Network Wordcount Python example: [https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/stateful_network_wordcount.py](https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/stateful_network_wordcount.py)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '有状态网络词频Python示例: [https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/stateful_network_wordcount.py](https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/stateful_network_wordcount.py)'
- en: 'Global Aggregation using mapWithState (Scala): [https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#07%20Spark%20Streaming/12%20Global%20Aggregations%20-%20mapWithState.html](https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#07%20Spark%20Streaming/12%20Global%20Aggregations%20-%20mapWithState.html)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 mapWithState 在 Scala 中进行全局聚合：[https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#07%20Spark%20Streaming/12%20Global%20Aggregations%20-%20mapWithState.html](https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#07%20Spark%20Streaming/12%20Global%20Aggregations%20-%20mapWithState.html)
- en: 'Word count using mapWithState (Scala): [https://docs.cloud.databricks.com/docs/spark/1.6/examples/Streaming%20mapWithState.html](https://docs.cloud.databricks.com/docs/spark/1.6/examples/Streaming%20mapWithState.html)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 mapWithState 在 Scala 中计算单词数量：[https://docs.cloud.databricks.com/docs/spark/1.6/examples/Streaming%20mapWithState.html](https://docs.cloud.databricks.com/docs/spark/1.6/examples/Streaming%20mapWithState.html)
- en: 'Faster Stateful Stream Processing in Apache Spark Streaming: [https://databricks.com/blog/2016/02/01/faster-stateful-stream-processing-in-apache-spark-streaming.html](https://databricks.com/blog/2016/02/01/faster-stateful-stream-processing-in-apache-spark-streaming.html)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Apache Spark Streaming 中实现更快的有状态流处理：[https://databricks.com/blog/2016/02/01/faster-stateful-stream-processing-in-apache-spark-streaming.html](https://databricks.com/blog/2016/02/01/faster-stateful-stream-processing-in-apache-spark-streaming.html)
- en: Introducing Structured Streaming
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍结构化流
- en: 'With Spark 2.0, the Apache Spark community is working on simplifying streaming
    by introducing the concept of *structured streaming* which bridges the concepts
    of streaming with Datasets/DataFrames (as noted in the following diagram):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 2.0 中，Apache Spark 社区正在通过引入 *结构化流* 的概念来简化流处理，该概念将流的概念与 Datasets/DataFrames
    相结合（如下图中所示）：
- en: '![Introducing Structured Streaming](img/B05793_10_14.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![介绍结构化流](img/B05793_10_14.jpg)'
- en: As noted in earlier chapters on DataFrames, the execution of SQL and/or DataFrame
    queries within the Spark SQL Engine (and Catalyst Optimizer) revolves around building
    a logical plan, building numerous physical plans, the engine choosing the correct
    physical plan based on its cost optimizer, and then generating the code (i.e.
    *code gen*) that will deliver the results in a performant manner. What *Structured
    Streaming* introduces is the concept of an **Incremental Execution Plan**. When
    working with blocks of data, structured streaming repeatedly applies the execution
    plan for every new set of blocks it receives. By running in this manner, the engine
    can take advantage of the optimizations included within Spark DataFrames/Datasets
    and apply them to an incoming data stream. It will also be easier to integrate
    other DataFrame optimized components of Spark, including ML Pipelines, GraphFrames,
    TensorFrames, and many others.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章关于 DataFrames 所述，在 Spark SQL 引擎（和 Catalyst 优化器）中执行 SQL 和/或 DataFrame 查询的过程是围绕构建逻辑计划、构建多个物理计划、引擎根据其成本优化器选择正确的物理计划，然后生成代码（即
    *代码生成*）以高效地提供结果。*结构化流* 引入的概念是 **增量执行计划**。当处理数据块时，结构化流会对其接收到的每一组新数据块重复应用执行计划。通过这种方式运行，引擎可以利用
    Spark DataFrames/Datasets 中的优化，并将它们应用于传入的数据流。这将更容易集成 Spark 的其他 DataFrame 优化组件，包括
    ML 流水线、GraphFrames、TensorFrames 以及许多其他组件。
- en: 'Using structured streaming will also simplify your code. For example, the following
    is a pseudo-code example *batch aggregation* that reads a data stream from S3
    and saves it to a MySQL database:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用结构化流也将简化你的代码。例如，以下是一个读取数据流从 S3 并将其保存到 MySQL 数据库的 *批量聚合* 的伪代码示例：
- en: '[PRE7]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following is a pseudo-code example for a *continous aggregation*:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个 *连续聚合* 的伪代码示例：
- en: '[PRE8]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The reason for creating the `sq` variable is that it allows you to check the
    status of your structured streaming job and terminate it, as per the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 `sq` 变量的原因是它允许你检查你的结构化流作业的状态并终止它，如下所示：
- en: '[PRE9]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s take the stateful streaming word count script that had used `updateStateByKey`
    and make it a structured streaming word count script; you can get the complete
    `structured_streaming_word_count.py` script at: [https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/structured_streaming_word_count.py](https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/structured_streaming_word_count.py).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将使用 `updateStateByKey` 的有状态流式单词计数脚本转换为结构化流式单词计数脚本；你可以从以下链接获取完整的 `structured_streaming_word_count.py`
    脚本：[https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/structured_streaming_word_count.py](https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/structured_streaming_word_count.py)。
- en: 'As opposed to the previous scripts, we are now working with the more familiar
    DataFrames code as noted here:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的脚本相反，我们现在使用更熟悉的 DataFrame 代码，如以下所示：
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The first lines of the script import the necessary classes and establish the
    current `SparkSession`. But, as opposed to the previous streaming scripts, as
    in the next lines of the script noted here, you do not need to establish a Streaming
    Context as this is already included within the `SparkSession`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的最初几行导入必要的类并建立当前的 `SparkSession`。但是，与之前的流脚本相反，在脚本的下一行中注意到的这里，你不需要建立流上下文，因为这已经包含在
    `SparkSession` 中：
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Instead, the streaming portion of the code is initiated by calling `readStream`
    in line 4.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，代码中的流部分是通过在第 4 行调用 `readStream` 来启动的。
- en: Lines 3-8 initiate the *reading* of the data stream from port `9999`, just like
    the previous two scripts
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 3-8 行启动从端口 `9999` 的数据流 *读取*，就像前两个脚本一样
- en: Instead of running RDD `flatMap`, `map`, and `reduceByKey` functions to split
    the lines read into words and count each word in each batch, we can use the PySpark
    SQL functions `explode` and `split` as noted in lines 10-15
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不需要运行 RDD 的 `flatMap`、`map` 和 `reduceByKey` 函数来分割读取的行到单词并计算每个批次中的每个单词，我们可以使用
    PySpark SQL 函数 `explode` 和 `split`，如第 10-15 行所示
- en: Instead of running `updateStateByKey` or creating an `updateFunc` as per the
    stateful streaming word count script, we can generate the running word count with
    a familiar DataFrame `groupBy` statement and `count()`, as noted in lines 17-18
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不需要运行 `updateStateByKey` 或创建 `updateFunc`，就像状态流词频脚本中所做的那样，我们可以使用熟悉的 DataFrame
    `groupBy` 语句和 `count()` 来生成运行词频，如第 17-18 行所示
- en: 'To output this data to the console, we will use `writeStream`, as noted here:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 要将此数据输出到控制台，我们将使用 `writeStream`，如以下所示：
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Instead of using `pprint()`, we're explicitly calling out `writeStream` to write
    the stream, and defining the format and output mode. While it is a little longer
    to write, these methods and properties are syntactically similar with other DataFrame
    calls and you would only need to change the `outputMode` and `format` properties
    to save it to a Database, file system, console, and so on. Finally, as noted in
    line 10, we will run `awaitTermination` to await to cancel this streaming job.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是使用 `pprint()`，而是明确调用 `writeStream` 来写入流，并定义格式和输出模式。虽然写起来稍微长一些，但这些方法和属性与
    DataFrame 调用的语法相似，你只需要更改 `outputMode` 和 `format` 属性来将其保存到数据库、文件系统、控制台等。最后，正如第
    10 行所注明的，我们将运行 `awaitTermination` 来等待取消此流作业。
- en: 'Let''s go back and run our `nc` job in the first terminal:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到第一个终端并运行我们的 `nc` 作业：
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Check the following output. As you can see, you get the advantages of stateful
    streaming but using the more familiar DataFrame API:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 检查以下输出。正如你所见，你得到了具有状态流的优势，但使用了更熟悉的 DataFrame API：
- en: '![Introducing Structured Streaming](img/B05793_10_15.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![介绍结构化流](img/B05793_10_15.jpg)'
- en: Summary
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: It is important to note that Structured Streaming is currently (at the time
    of writing) not production-ready. It is, however, a paradigm shift in Spark that
    will hopefully make it easier for data scientists and data engineers to build
    **continuous applications**. While not explicitly called out in the previous sections,
    when working with streaming applications, there are many potential problems that
    you will need to design for, such as late events, partial outputs, state recovery
    on failure, distributed reads and writes, and so on. With structured streaming,
    many of these issues will be abstracted away to make it easier for you to build
    *continuous applications*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，结构化流目前（在撰写本文时）尚未准备好投入生产。然而，它却是 Spark 中的一个范式转变，有望使数据科学家和数据工程师更容易构建 **持续应用程序**。虽然在前面的章节中没有明确指出，但在处理流应用程序时，你将需要为许多潜在问题进行设计，例如迟到事件、部分输出、失败时的状态恢复、分布式读写等。使用结构化流，许多这些问题将被抽象化，以便你更容易构建
    *持续应用程序*。
- en: 'We encourage you to try Spark Structured Streaming so you will be able to easily
    build streaming applications as structured streaming matures. As Reynold Xin noted
    in his Spark Summit 2016 East presentation *The Future of Real-Time in Spark*
    ([http://www.slideshare.net/rxin/the-future-of-realtime-in-spark](http://www.slideshare.net/rxin/the-future-of-realtime-in-spark)):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励你尝试 Spark 结构化流，这样你将能够轻松构建随着结构化流成熟的应用程序。正如 Reynold Xin 在他的 Spark Summit 2016
    East 演讲 *Spark 的实时未来* 中指出（[http://www.slideshare.net/rxin/the-future-of-realtime-in-spark](http://www.slideshare.net/rxin/the-future-of-realtime-in-spark)）：
- en: '"The simplest way to perform streaming analytics is not having to *reason*
    about streaming."'
  id: totrans-153
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '"执行流式分析最简单的方法就是无需对流进行*推理*。"'
- en: 'For more information, here are some additional Structured Streaming resources:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，以下是一些额外的结构化流资源：
- en: '*PySpark 2.1 Documentation: pyspark.sql.module*: [http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*PySpark 2.1 文档: pyspark.sql.module*: [http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html)'
- en: '*Introducing Apache Spark 2.1*: [https://databricks.com/blog/2016/12/29/introducing-apache-spark-2-1.html](https://databricks.com/blog/2016/12/29/introducing-apache-spark-2-1.html)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*介绍Apache Spark 2.1*: [https://databricks.com/blog/2016/12/29/introducing-apache-spark-2-1.html](https://databricks.com/blog/2016/12/29/introducing-apache-spark-2-1.html)'
- en: '*Structuring Apache Spark 2.0: SQL, DataFrames, Datasets and Streaming - by
    Michael Armbrust*: [http://www.slideshare.net/databricks/structuring-spark-dataframes-datasets-and-streaming-62871797](http://www.slideshare.net/databricks/structuring-spark-dataframes-datasets-and-streaming-62871797)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*结构化Apache Spark 2.0：SQL、数据框、数据集和流式处理 - 作者：Michael Armbrust*: [http://www.slideshare.net/databricks/structuring-spark-dataframes-datasets-and-streaming-62871797](http://www.slideshare.net/databricks/structuring-spark-dataframes-datasets-and-streaming-62871797)'
- en: '*Structured Streaming Programming Guide*: [http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*结构化流编程指南*: [http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html)'
- en: '*Structured Streaming (aka Streaming DataFrames) [SPARK-8360]*: [https://issues.apache.org/jira/browse/SPARK-8360](https://issues.apache.org/jira/browse/SPARK-8360)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*结构化流（又称流式数据框）[SPARK-8360]*: [https://issues.apache.org/jira/browse/SPARK-8360](https://issues.apache.org/jira/browse/SPARK-8360)'
- en: '*Structured Streaming Programming Abstraction, Semantics, and APIs ­Apache
    JIRA*: [https://issues.apache.org/jira/secure/attachment/12793410/StructuredStreamingProgrammingAbstractionSemanticsandAPIs-ApacheJIRA.pdf](https://issues.apache.org/jira/secure/attachment/12793410/StructuredStreamingProgrammingAbstractionSemanticsandAPIs-ApacheJIRA.pdf)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*结构化流编程抽象、语义和API - Apache JIRA*: [https://issues.apache.org/jira/secure/attachment/12793410/StructuredStreamingProgrammingAbstractionSemanticsandAPIs-ApacheJIRA.pdf](https://issues.apache.org/jira/secure/attachment/12793410/StructuredStreamingProgrammingAbstractionSemanticsandAPIs-ApacheJIRA.pdf)'
- en: In the next chapter we will show you how to modularize and package up your PySpark
    application and submit it for execution programmatically.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将向您展示如何模块化并打包您的PySpark应用程序，并程序化地提交它以执行。
