<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer157">
<h1 class="chapter-number" id="_idParaDest-195"><a id="_idTextAnchor195"/>6</h1>
<h1 id="_idParaDest-196"><a id="_idTextAnchor196"/>Using PySpark with Deﬁned and Non-Deﬁned Schemas</h1>
<p>Generally, schemas are forms used to create or apply structures to data. As someone who works or will work with large volumes of data, it is essential to understand how to manipulate DataFrames and apply structure when it is necessary to bring more context to the <span class="No-Break">information involved.</span></p>
<p>However, as seen in the previous chapters, data can come from different sources or be present without a well-defined structure, and applying a schema can be challenging. Here, we will see how to create schemas and standard formats using PySpark with structured and <span class="No-Break">unstructured data.</span></p>
<p>In this chapter, we will cover the <span class="No-Break">following recipes:</span></p>
<ul>
<li>Applying schemas to <span class="No-Break">data ingestion</span></li>
<li>Importing structured data using a <span class="No-Break">well-deﬁned schema</span></li>
<li>Importing unstructured data with an <span class="No-Break">undefined schema</span></li>
<li>Ingesting unstructured data with a well-deﬁned schema <span class="No-Break">and format</span></li>
<li>Inserting formatted SparkSession logs to facilitate <span class="No-Break">your work</span></li>
</ul>
<h1 id="_idParaDest-197"><a id="_idTextAnchor197"/>Technical requirements</h1>
<p>You can also find the code for this chapter in the GitHub repository <span class="No-Break">here: </span><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook"><span class="No-Break">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook</span></a><span class="No-Break">.</span></p>
<p>Using <strong class="bold">Jupyter Notebook</strong> is not mandatory but can help you see how the code works interactively. Since we will execute Python and PySpark code, it can help us understand the scripts better. Once you have it installed, you can execute Jupyter using the <span class="No-Break">following line:</span></p>
<pre class="source-code">
$ jupyter Notebook</pre>
<p>It is recommended to create a separate folder to store the Python files or Notebooks we will cover in this chapter; however, feel free to organize the files in the best way that <span class="No-Break">fits you.</span></p>
<p>In this chapter, all recipes will need a <strong class="source-inline">SparkSession</strong> instance initialized, and you can use the same session for all of them. You can use the following code to create <span class="No-Break">your session:</span></p>
<pre class="source-code">
from pyspark.sql import SparkSession
spark = SparkSession.builder \
      .master("local[1]") \
      .appName("chapter6_schemas") \
      .config("spark.executor.memory", <strong class="source-inline">'3g'</strong>) \
      .config("spark.executor.cores", '1') \
      .config("spark.cores.max", '1') \
      .getOrCreate()</pre>
<p class="callout-heading">Note</p>
<p class="callout">A <strong class="source-inline">WARN</strong> message as output is expected in some cases, especially if you are using WSL on Windows, so you don’t need <span class="No-Break">to worry.</span></p>
<h1 id="_idParaDest-198"><a id="_idTextAnchor198"/>Applying schemas to data ingestion</h1>
<p>The application of schemas is common practice when ingesting data, and PySpark natively supports <a id="_idIndexMarker429"/>applying them to DataFrames. To <a id="_idIndexMarker430"/>define and apply schemas to our DataFrames, we need to understand some concepts <span class="No-Break">of Spark.</span></p>
<p>This recipe introduces the basic concept of working with schemas using PySpark and its best practices so that we can later apply them to structured and <span class="No-Break">unstructured data.</span></p>
<h2 id="_idParaDest-199"><a id="_idTextAnchor199"/>Getting ready</h2>
<p>Make sure PySpark is installed and working on your machine for this recipe. You can run the following code on your command line to check <span class="No-Break">this requirement:</span></p>
<pre class="source-code">
$ pyspark --version</pre>
<p>You should see the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer139">
<img alt="Figure 6.1 – PySpark version console output" height="346" src="image/Figure_6.1_B19453.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – PySpark version console output</p>
<p>If don’t have PySpark installed on your local machine, please refer to the <em class="italic">Installing PySpark</em> recipe in <a href="B19453_01.xhtml#_idTextAnchor022"><span class="No-Break"><em class="italic">Chapter 1</em></span></a><span class="No-Break">.</span></p>
<p>I will <a id="_idIndexMarker431"/>use Jupyter Notebook to execute the code to make it <a id="_idIndexMarker432"/>more interactive. You can use this link and follow the instructions on the screen to install <span class="No-Break">it: </span><a href="https://jupyter.org/install"><span class="No-Break">https://jupyter.org/install</span></a><span class="No-Break">.</span></p>
<p>If you already have it installed, check the version using the following code on your <span class="No-Break">command line:</span></p>
<pre class="source-code">
$ jupyter --version</pre>
<p>The following screenshot shows the <span class="No-Break">expected output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer140">
<img alt="Figure 6.2 – Jupyter package versions" height="320" src="image/Figure_6.2_B19453.jpg" width="367"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Jupyter package versions</p>
<p>As you can see, the Notebook version was <strong class="source-inline">6.4.4</strong> at the time this book was written. Make sure to always use the <span class="No-Break">latest version.</span></p>
<h2 id="_idParaDest-200"><a id="_idTextAnchor200"/>How to do it…</h2>
<p>Here are the steps to carry out <span class="No-Break">the recipe:</span></p>
<ol>
<li><strong class="bold">Creating mock data</strong>: Before applying the schema to our DataFrame, we need to create a simple <a id="_idIndexMarker433"/>set containing simulated data <a id="_idIndexMarker434"/>of people’s information in this format—ID, name, last name, age, <span class="No-Break">and gender:</span><pre class="source-code">
my_data = [("3456","Cristian","Rayner",30,"M"),
            ("3567","Guto","Flower",35,"M"),
            ("9867","Yasmin","Novak",23,"F"),
            ("3342","Tayla","Mejia",45,"F"),
            ("8890","Barbara","Kumar",20,"F")
            ]</pre></li>
<li><strong class="bold">Importing and structuring the schema</strong>: The next step is to import the types and create the structure of <span class="No-Break">our schema:</span><pre class="source-code">
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
schema = StructType([ \
    StructField("id",StringType(),True), \
    StructField("name",StringType(),True), \
    StructField("lastname",StringType(),True), \
    StructField("age", IntegerType(), True), \
    StructField("gender", StringType(), True), \
  ])</pre></li>
<li><strong class="bold">Creating the DataFrame</strong>: Then, we make the DataFrame, applying the schema we <span class="No-Break">have created:</span><pre class="source-code">
df = spark.createDataFrame(data=my_data,schema=schema)</pre></li>
</ol>
<p>When printing <a id="_idIndexMarker435"/>our DataFrame schema <a id="_idIndexMarker436"/>using the <strong class="source-inline">.printSchema()</strong> method, this is the <span class="No-Break">expected output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer141">
<img alt="Figure 6.3 – The DataFrame schema" height="116" src="image/Figure_6.3_B19453.jpg" width="341"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – The DataFrame schema</p>
<h2 id="_idParaDest-201"><a id="_idTextAnchor201"/>How it works…</h2>
<p>Before understanding the methods in <em class="italic">step 2</em>, let’s step back a bit and understand the concept of <span class="No-Break">a DataFrame.</span></p>
<p>DataFrame is like a table with data stored and organized in a two-dimensional array, which resembles a table from a relational database such as MySQL or Postgres. Each line corresponds to a record, and libraries such as pandas and PySpark, by default, assign a record number internally to each line (<span class="No-Break">or index).</span></p>
<div>
<div class="IMG---Figure" id="_idContainer142">
<img alt="Figure 6.4 – GeeksforGeeks DataFrame explanation" height="462" src="image/Figure_6.4_B19453.jpg" width="751"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – GeeksforGeeks DataFrame explanation</p>
<p class="callout-heading">Note</p>
<p class="callout">Speaking of the Pandas library, it is common to refer to a column in a Pandas DataFrame as a series and expect it to behave like a Python list. It makes it easier to manipulate data for analysis <span class="No-Break">and visualization.</span></p>
<p>The objective of <a id="_idIndexMarker437"/>using a DataFrame is to utilize the several <a id="_idIndexMarker438"/>optimizations for data processing under the hood that Spark brings, which are directly linked to <span class="No-Break">parallel processing.</span></p>
<p>Back to the schema definition in our <strong class="source-inline">schema</strong> variable, let’s take a look at the code <span class="No-Break">we created:</span></p>
<pre class="source-code">
schema = StructType([ \
    StructField("id",StringType(),True), \
    StructField("name",StringType(),True), \
    StructField("lastname",StringType(),True), \
    StructField("age", IntegerType(), True), \
    StructField("gender", StringType(), True), \
  ])</pre>
<p>The first object we declare is the <strong class="source-inline">StructType</strong> class. This class will create an object of collection or our rows. Next, we declare a <strong class="source-inline">StructField</strong> instance, representing our column with its name, data type, whether it is nullable, and its metadata when applicable. <strong class="source-inline">StructField</strong> must be in the same order as the columns in the DataFrame; otherwise, it can generate errors due to the incompatibility of a data type (for example, the <a id="_idIndexMarker439"/>column has string values, and we are setting it <a id="_idIndexMarker440"/>as an integer) or the presence of null values. Defining <strong class="source-inline">StructField</strong> is an excellent opportunity to standardize the name of the DataFrame and, therefore, the <span class="No-Break">analytical data.</span></p>
<p>Finally, <strong class="source-inline">StringType</strong> and <strong class="source-inline">IntegerType</strong> are the methods that will cast the data type into the respective columns. They were imported at the beginning of the recipe and derived from the SQL types inside PySpark. In our mock data example, we defined the <strong class="source-inline">id</strong> and <strong class="source-inline">age</strong> columns as <strong class="source-inline">IntegerType</strong> since we expect no other kind of data in them. However, there are many situations where the <strong class="source-inline">id</strong> column is referred to as a string type, usually when the data comes from <span class="No-Break">different systems.</span></p>
<p>Here we used <strong class="source-inline">StringType</strong> and <strong class="source-inline">IntegerType</strong>, but many others can be used to create context and standardize our data. Refer to the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer143">
<img alt="Figure 6.5 – SparkbyExample table of data types in Spark" height="415" src="image/Figure_6.5_B19453.jpg" width="338"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – SparkbyExample table of data types in Spark</p>
<p>You can understand more about how to apply the <strong class="bold">Spark data types</strong> in the Spark official <a id="_idIndexMarker441"/>documentation <span class="No-Break">here: </span><a href="https://spark.apache.org/docs/latest/sql-ref-datatypes.xhtml"><span class="No-Break">https://spark.apache.org/docs/latest/sql-ref-datatypes.xhtml</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-202"><a id="_idTextAnchor202"/>There’s more…</h2>
<p>When handling terminology, there needs to be a common understanding about using a dataset or DataFrame, especially if you are a newcomer to the <span class="No-Break">data world.</span></p>
<p>A dataset is a collection <a id="_idIndexMarker442"/>of data containing rows and columns (for example, relational data) or documents and files (for example, non-relational data). It comes from a source and is available in different <span class="No-Break">file formats.</span></p>
<p>On the other hand, a DataFrame is derived from a dataset, presenting the data in a tabular form even <a id="_idIndexMarker443"/>if that is not the primary format. The DataFrame can transform a MongoDB document collection into a tabular organization based on the configurations set when it <span class="No-Break">is created.</span></p>
<h2 id="_idParaDest-203"><a id="_idTextAnchor203"/>See also</h2>
<p>More examples on the <em class="italic">SparkbyExample</em> site can be found <span class="No-Break">here: </span><a href="https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/"><span class="No-Break">https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-204"><a id="_idTextAnchor204"/>Importing structured data using a well-deﬁned schema</h1>
<p>As seen in the previous chapter, <em class="italic">Ingesting Data from Structured and Unstructured Databases</em>, structured <a id="_idIndexMarker444"/>data has a standard format presented in rows and columns and is often stored inside <span class="No-Break">a database.</span></p>
<p>Due to its format, the application of a DataFrame schema tends to be less complex and has several benefits, such as ensuring the ingested information is the same as the data source or follows <span class="No-Break">a rule.</span></p>
<p>In this recipe, we will ingest data from a structured file such as a CSV file and apply a DataFrame schema to understand better how it is used in a <span class="No-Break">real-world scenario.</span></p>
<h2 id="_idParaDest-205"><a id="_idTextAnchor205"/>Getting ready</h2>
<p>This exercise requires the <strong class="source-inline">listings.csv</strong> file found inside the GitHub repository for this book. Also, make sure your <strong class="source-inline">SparkSession</strong> <span class="No-Break">is initialized.</span></p>
<p>All the code in this recipe can be executed in Jupyter Notebook cells or a <span class="No-Break">PySpark shell.</span></p>
<h2 id="_idParaDest-206"><a id="_idTextAnchor206"/>How to do it…</h2>
<p>Here are <a id="_idIndexMarker445"/>the steps to perform <span class="No-Break">this recipe:</span></p>
<ol>
<li><strong class="bold">Importing Spark data types</strong>: Besides <strong class="source-inline">StringType</strong> and <strong class="source-inline">IntegerType</strong>, we will include two more data types in our import, <strong class="source-inline">FloatType</strong> and <strong class="source-inline">DateType</strong>, shown <span class="No-Break">as follows:</span><pre class="source-code">
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, DoubleType</pre></li>
<li><strong class="bold">Creating the schema</strong>: Based on the columns inside the CSV file, let’s insert their names into each <strong class="source-inline">StructField</strong> and assign them to a respective <span class="No-Break">data type:</span><pre class="source-code">
schema = StructType([ \
    StructField("id",IntegerType(),True), \
    StructField("name",StringType(),True), \
    StructField("host_id",IntegerType(),True), \
    StructField("host_name",StringType(),True), \
    StructField("neighbourhood_group",StringType(),True), \
    StructField("neighbourhood",StringType(),True), \
    StructField("latitude",DoubleType(),True), \
    StructField("longitude",DoubleType(),True), \
    StructField("room_type",StringType(),True), \
    StructField("price",FloatType(),True), \
    StructField("minimum_nights",IntegerType(),True), \
    StructField("number_of_reviews",IntegerType(),True), \
    StructField("last_review",DateType(),True), \
    StructField("reviews_per_month",FloatType(),True), \
      StructField("calculated_host_listings_count",IntegerType(),True), \
    StructField("availability_365",IntegerType(),True), \
    StructField("number_of_reviews_ltm",IntegerType(),True), \
    StructField("license",StringType(),True)
  ])</pre></li>
<li><strong class="bold">Reading the CSV file with the schema defined</strong>: Now let’s read our <strong class="source-inline">listings.csv</strong> file <a id="_idIndexMarker446"/>with the <strong class="source-inline">.options()</strong> configurations and add the <strong class="source-inline">.schema()</strong> method with the <strong class="source-inline">schema</strong> variable seen in <span class="No-Break"><em class="italic">step 2</em></span><span class="No-Break">.</span><pre class="source-code">
df = spark.read.options(header=True, sep=',',
                          multiLine=True, escape='"')\
                .schema(schema) \
                .csv('listings.csv')</pre></li>
</ol>
<p>If everything is well set, you should see no output from <span class="No-Break">this execution.</span></p>
<ol>
<li value="4"><strong class="bold">Checking the read DataFrame</strong>: We can check the schema of our DataFrame by executing the <span class="No-Break">following code:</span><pre class="source-code">
df.printSchema()</pre></li>
</ol>
<p>You should see the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer144">
<img alt="Figure 6.6 – listings.csv DataFrame schema" height="364" src="image/Figure_6.6_B19453.jpg" width="533"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – listings.csv DataFrame schema</p>
<h2 id="_idParaDest-207"><a id="_idTextAnchor207"/>How it works…</h2>
<p>We made a few additions in this exercise that differ from the last recipe, <em class="italic">Applying schemas to </em><span class="No-Break"><em class="italic">data ingestion.</em></span></p>
<p>As usual, we started by importing the required methods to make the script work. We added <a id="_idIndexMarker447"/>three more data types: float, double, and date. The choice was made based on what the CSV file contained. Let’s look at the first lines of our file, as you can see in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer145">
<img alt="Figure 6.7 – listings.csv view from Microsoft Excel" height="163" src="image/Figure_6.7_B19453.jpg" width="1302"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – listings.csv view from Microsoft Excel</p>
<p>We can observe different types of numerical fields; some require more decimal places, and <strong class="source-inline">last_review</strong> is in a date format. Because of this, we made additional library imports, as you can see in the following piece <span class="No-Break">of code:</span></p>
<pre class="source-code">
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType, DateType</pre>
<p><em class="italic">Step 2</em> is similar to what we did before, where we attributed the column names and their respective data types. It is in <em class="italic">step 3</em> that we made the schema attribution using the <strong class="source-inline">schema()</strong> method from the <strong class="source-inline">SparkSession</strong> class. If the schema contains the same number of columns as the file, we should expect no output here; otherwise, this message <span class="No-Break">will appear:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer146">
<img alt="Figure 6.8 – Output warning message when the schema does not match" height="74" src="image/Figure_6.8_B19453.jpg" width="1166"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – Output warning message when the schema does not match</p>
<p>The content is important, even if it is a <strong class="source-inline">WARN</strong> log message. Looking closely, it says the schema <a id="_idIndexMarker448"/>does not match the number of columns in the file. This could be a problem later in the ETL pipeline when loading data into a data warehouse or any other <span class="No-Break">analytical database.</span></p>
<h2 id="_idParaDest-208"><a id="_idTextAnchor208"/>There’s more…</h2>
<p>If you go back to <a href="B19453_04.xhtml#_idTextAnchor127"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, you will observe that one of our CSV readings contains an <strong class="source-inline">inferSchema</strong> parameter inserted into the <strong class="source-inline">options()</strong> method. Refer to the <span class="No-Break">following code:</span></p>
<pre class="source-code">
df_2 = spark.read.options(header=True, sep=',',
                          multiLine=True, escape='"',
                         inferSchema=True) \
                .csv('listings.csv')</pre>
<p>This parameter tells Spark to infer the data types based on the traits of the rows. For example, if a row has a value without quotation marks and is a number, there is a good chance this is an integer. However, if it contains a quotation mark, Spark can interpret it as a string, and any numerical operation <span class="No-Break">will break.</span></p>
<p>In the recipe, if we use <strong class="source-inline">inferSchema</strong>, we will see a very similar <strong class="source-inline">printSchema</strong> output from the <a id="_idIndexMarker449"/>schema we defined, except for some fields that were interpreted as <strong class="source-inline">DoubleType</strong> and that we declared as <strong class="source-inline">FloatType</strong> or <strong class="source-inline">DateType</strong>. This is shown in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer147">
<img alt="Figure 6.9 – Schema comparison when using a schema with inferSchema" height="414" src="image/Figure_6.9_B19453.jpg" width="1086"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – Schema comparison when using a schema with inferSchema</p>
<p>Even though it seems like a tiny detail, when dealing with streaming or a large dataset and few computational <a id="_idIndexMarker450"/>resources, it can make a difference. Float types have a small range and <a id="_idIndexMarker451"/>bring in high processing power. Double data types bring more precision and are used to decrease mathematical errors or values being rounded by decimal <span class="No-Break">data types.</span></p>
<h2 id="_idParaDest-209"><a id="_idTextAnchor209"/>See also</h2>
<p>Read more <a id="_idIndexMarker452"/>about float and double types on the <em class="italic">Hackr IO</em> website <span class="No-Break">here: </span><a href="https://hackr.io/blog/float-vs-double"><span class="No-Break">https://hackr.io/blog/float-vs-double</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-210"><a id="_idTextAnchor210"/>Importing unstructured data without a schema</h1>
<p>As seen before, <a id="_idIndexMarker453"/>unstructured data or <strong class="bold">NoSQL</strong> is a <a id="_idIndexMarker454"/>group of information that does not follow a format, such as relational or tabular data. It can be presented as an image, video, metadata, transcripts, and so on. The data ingestion process usually involves a JSON file or a document collection, as we previously saw when ingesting data <span class="No-Break">from </span><span class="No-Break"><strong class="bold">MongoDB</strong></span><span class="No-Break">.</span></p>
<p>In this recipe, we will read a JSON file and transform it into a DataFrame without a schema. Although unstructured data is supposed to have a more flexible design, we will see some implications of not having any schema or structure in <span class="No-Break">our DataFrame.</span></p>
<h2 id="_idParaDest-211"><a id="_idTextAnchor211"/>Getting ready…</h2>
<p>Here, we will use the <strong class="source-inline">holiday_brazil.json</strong> file to create the DataFrame. You can find it in the GitHub repository <span class="No-Break">here: </span><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook"><span class="No-Break">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook</span></a><span class="No-Break">.</span></p>
<p>We will use <strong class="source-inline">SparkSession</strong> to read the JSON file and create a DataFrame to ensure the session is up <span class="No-Break">and running.</span></p>
<p>All code <a id="_idIndexMarker455"/>can be executed in a Jupyter Notebook <a id="_idIndexMarker456"/>or at <span class="No-Break">PySpark shell.</span></p>
<h2 id="_idParaDest-212"><a id="_idTextAnchor212"/>How to do it…</h2>
<p>Let’s now read our <strong class="source-inline">holiday_brazil.json</strong> file, observing how Spark <span class="No-Break">handles it:</span></p>
<ol>
<li><strong class="bold">Reading the JSON file</strong>: Since our file has nested objects, we will pass <strong class="source-inline">multiline</strong> as a parameter to the <strong class="source-inline">options()</strong> method. We will also let PySpark infer the data types <span class="No-Break">in it:</span><pre class="source-code">
df_json = spark.read.option("multiline","true") \
                    .json('holiday_brazil.json')</pre></li>
</ol>
<p>If all goes right, you should see <span class="No-Break">no output.</span></p>
<ol>
<li value="2"><strong class="bold">Printing the inferred schema</strong>: Using the <strong class="source-inline">printSchema()</strong> method, we can see how PySpark interpreted the data types of <span class="No-Break">each key:</span><pre class="source-code">
df_json.printSchema()</pre></li>
</ol>
<p>The following screenshot is the <span class="No-Break">expected output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer148">
<img alt="Figure 6.10 – holiday_brazil.json DataFrame schema" height="477" src="image/Figure_6.10_B19453.jpg" width="571"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – holiday_brazil.json DataFrame schema</p>
<ol>
<li value="3"><strong class="bold">Visualizing it with Pandas</strong>: Let’s <a id="_idIndexMarker457"/>use the <strong class="source-inline">toPandas()</strong> function to visualize our <span class="No-Break">DataFrame better:</span><pre class="source-code">
df_json.toPandas()</pre></li>
</ol>
<p>You should see <span class="No-Break">this output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer149">
<img alt="Figure 6.11 – Output of toPandas() vision from the DataFrame" height="79" src="image/Figure_6.11_B19453.jpg" width="987"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – Output of toPandas() vision from the DataFrame</p>
<h2 id="_idParaDest-213"><a id="_idTextAnchor213"/>How it works…</h2>
<p>Let’s <a id="_idIndexMarker458"/>take a look at the output from <span class="No-Break"><em class="italic">step 2</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer150">
<img alt="Figure 6.12 – Holiday_brasil.json DataFrame schema" height="468" src="image/Figure_6.12_B19453.jpg" width="580"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – Holiday_brasil.json DataFrame schema</p>
<p>As we can observe, Spark only brought four columns, the first four keys in the JSON file, and ignored the rest of the other nested keys by keeping them inside the main four ones. This happens because Spark needs to handle the parameter better by flattening values from nested fields, even though we passed <strong class="source-inline">multiline</strong> as a parameter in the <span class="No-Break"><strong class="source-inline">options()</strong></span><span class="No-Break"> configuration.</span></p>
<p>Another important <a id="_idIndexMarker459"/>point is that data types inferred <a id="_idIndexMarker460"/>for the <strong class="source-inline">numeric</strong> keys inside the <strong class="source-inline">weekday</strong> array are string values and should be integers. This happened because those values have quotation marks, as you can see in the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer151">
<img alt="Figure 6.13 – weekday objects" height="224" src="image/Figure_6.13_B19453.jpg" width="230"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.13 – weekday objects</p>
<p>Applying this to a real-world scenario where it is crucial to have a qualitative evaluation, these unformatted and schemaless DataFrame can create problems later when uploaded to a data warehouse. If a field suddenly changes its name or is unavailable in the source, it can lead to data inconsistency. There are some ways to solve this, and we will cover this further in the following recipe, <em class="italic">Ingesting unstructured data with a well-deﬁned schema </em><span class="No-Break"><em class="italic">and format.</em></span></p>
<p>However, there <a id="_idIndexMarker461"/>are plenty of other scenarios where a <a id="_idIndexMarker462"/>schema is not needed to be defined or unstructured data doesn’t need to be standardized. An excellent example is application logs or metadata, where data is often linked to the application’s or system’s availability to send the information. In that case, solutions such as <strong class="bold">ElasticSearch</strong>, <strong class="bold">DynamoDB</strong>, and many others are good storage options that provide query support. In other words, most of the issues here will be more inclined to generate a <span class="No-Break">quantitative output.</span></p>
<h1 id="_idParaDest-214"><a id="_idTextAnchor214"/>Ingesting unstructured data with a well-deﬁned schema and format</h1>
<p>In the previous recipe, <em class="italic">Importing unstructured data without schema</em>, we read a JSON file without any <a id="_idIndexMarker463"/>schema or formatting application. This led us to an odd output, which could bring confusion and require additional work later in the data pipeline. While this example pertains specifically to a JSON file, it also applies to all other NoSQL or unstructured data that needs to be converted into <span class="No-Break">analytical data.</span></p>
<p>The objective is to continue the last recipe and apply a schema and standard to our data, making it more legible and easy to process in the subsequent phases <span class="No-Break">of </span><span class="No-Break"><strong class="bold">ETL</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-215"><a id="_idTextAnchor215"/>Getting ready</h2>
<p>This recipe has the exact same requirements as the <em class="italic">Importing unstructured data without a </em><span class="No-Break"><em class="italic">schema</em></span><span class="No-Break"> recipe.</span></p>
<h2 id="_idParaDest-216"><a id="_idTextAnchor216"/>How to do it…</h2>
<p>We will perform the following steps to perform <span class="No-Break">this recipe:</span></p>
<ol>
<li><strong class="bold">Importing data types</strong>: As usual, let’s start by importing our data types from the <span class="No-Break">PySpark library:</span><pre class="source-code">
from pyspark.sql.types import StructType, ArrayType, StructField, StringType, IntegerType, MapType</pre></li>
<li><strong class="bold">Structuring the JSON schema</strong>: Next, we set the schema based on how the JSON <a id="_idIndexMarker464"/><span class="No-Break">is structured:</span><pre class="source-code">
schema = StructType([ \
        StructField('status', StringType(), True),
        StructField('holidays', ArrayType(
            StructType([
                StructField('name', StringType(), True),
                StructField('date', DateType(), True),
                StructField('observed', StringType(), True),
                StructField('public', StringType(), True),
                StructField('country', StringType(), True),
                StructField('uuid', StringType(), True),
                StructField('weekday', MapType(StringType(), MapType(StringType(),StringType(),True),True))
            ])
        ))
    ])</pre></li>
<li><strong class="bold">Applying the schema</strong>: Similar to the CSV reading, we add the <strong class="source-inline">schema()</strong> method to apply the schema created in the <span class="No-Break">previous step:</span><pre class="source-code">
df_json = spark.read.option("multiline","true") \
                    .schema(schema) \
                    .json('holiday_brazil.json')</pre></li>
<li><strong class="bold">Expanding the columns</strong>: Using the <strong class="source-inline">explode()</strong> method, let’s broaden the fields inside the <span class="No-Break"><strong class="source-inline">holidays</strong></span><span class="No-Break"> column:</span><pre class="source-code">
from pyspark.sql.functions import explode
exploded_json = df_json.select('status', explode("holidays").alias("holidaysExplode"))\
        .select("status", "holidaysExplode.*")</pre></li>
<li><strong class="bold">Expanding more columns</strong>: This is an optional step, but if needed, we can keep <a id="_idIndexMarker465"/>growing and flattening the other columns with nested <span class="No-Break">content inside:</span><pre class="source-code">
exploded_json2 = exploded_json.select("*", explode('weekday').alias('weekday_type', 'weekday_objects'))</pre></li>
<li><strong class="bold">Seeing our DataFrame</strong>: Using the <strong class="source-inline">toPandas()</strong> function, we can better view what our DataFrame looks <span class="No-Break">like now:</span><pre class="source-code">
exploded_json2.toPandas()</pre></li>
</ol>
<p>You should see the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer152">
<img alt="Figure 6.14 – The DataFrame with expanded columns" height="546" src="image/Figure_6.14_B19453.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.14 – The DataFrame with expanded columns</p>
<p>The final DataFrame can be saved as a Parquet file, which can make the next steps in the data pipeline easier <span class="No-Break">to handle.</span></p>
<h2 id="_idParaDest-217"><a id="_idTextAnchor217"/>How it works…</h2>
<p>As you can observe, this JSON file has some complexity due to the number of nested objects. When handling a file such as this, we can use several approaches. When coding, there are many approaches to reaching a solution. Let’s understand how this <span class="No-Break">recipe works.</span></p>
<p>In <em class="italic">step 1</em>, two new data types were imported—<strong class="source-inline">ArrayType</strong> and <strong class="source-inline">MapType</strong>. Even though there is some confusion when using each type, it is somewhat simple to understand when looking at the <span class="No-Break">JSON structure.</span></p>
<p>We used <strong class="source-inline">ArrayType</strong> for the <strong class="source-inline">holiday</strong> key because its structure looks <span class="No-Break">like this:</span></p>
<pre class="source-code">
holiday : [{},{}...]</pre>
<p>It is an array (or a list if we use Python) of objects, each representing a holiday in Brazil. When <strong class="source-inline">ArrayType</strong> has other objects inside it, we need to re-use <strong class="source-inline">StructType</strong> to inform <a id="_idIndexMarker466"/>Spark of the structure of objects that reside inside. That’s why in <em class="italic">step 2</em>, our schema started to look <span class="No-Break">like this:</span></p>
<pre class="source-code">
StructField('holidays', ArrayType(
            StructType([
  ...
])
))</pre>
<p>The following new data type is <strong class="source-inline">MapType</strong>. This type refers to an object with other objects inside. If we are only using Python, it can be referred to as a dictionary. PySpark extends this data type from a <strong class="bold">superclass</strong> in Spark, and you can read more about that <span class="No-Break">here: </span><a href="https://spark.apache.org/docs/2.0.1/api/java/index.xhtml?org/apache/spark/sql/types/MapType.xhtml"><span class="No-Break">https://spark.apache.org/docs/2.0.1/api/java/index.xhtml?org/apache/spark/sql/types/MapType.xhtml</span></a><span class="No-Break">.</span></p>
<p>The syntax of <strong class="source-inline">MapType</strong> requires the key-value type, the value type, and whether it accepts null values. We used this type in the <strong class="source-inline">weekday</strong> field, as you can <span class="No-Break">see here:</span></p>
<pre class="source-code">
StructField('weekday', MapType(
StringType(),MapType(
StringType(),StringType(),True)
,True))</pre>
<p>Maybe this is the most complex structure we have seen so far, and that was due to how JSON gets structured <span class="No-Break">through it:</span></p>
<pre class="source-code">
weekday : {
 day : {{}, {}},
 observed : {{}, {}}
}</pre>
<p>The structure <a id="_idIndexMarker467"/>in our schema created <strong class="source-inline">MapType</strong> for <strong class="source-inline">weekday</strong> and the subsequent keys, <strong class="source-inline">day</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">observed</strong></span><span class="No-Break">.</span></p>
<p>Once our schema is defined and applied to our DataFrame, we can make a preview of it using <strong class="source-inline">df_json.show()</strong>. If the schema does not match the JSON structure, you should see <span class="No-Break">this output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer153">
<img alt="Figure 6.15 – df_json print" height="169" src="image/Figure_6.15_B19453.jpg" width="306"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.15 – df_json print</p>
<p>This demonstrates that Spark was unable to create the DataFrame correctly. The best action in this situation is to apply the schema step by step until the problem <span class="No-Break">is resolved.</span></p>
<p>To flatten our fields inside the <strong class="source-inline">holidays</strong> column, we need to use a function from PySpark called <strong class="source-inline">explode</strong>, as you can <span class="No-Break">see here:</span></p>
<pre class="source-code">
from pyspark.sql.functions import explode</pre>
<p>To apply the changes, we need to attribute the results to a variable where it will create a <span class="No-Break">new DataFrame:</span></p>
<pre class="source-code">
exploded_json = df_json.select('status', explode("holidays").alias("holidaysExplode"))\
        .select("holidaysExplode.*")</pre>
<p class="callout-heading">Note</p>
<p class="callout">Although it seems redundant, preventing the original DataFrame from being modified is a good practice. If anything goes wrong, we don’t need to reread the file because we have the intact state of the <span class="No-Break">original DataFrame.</span></p>
<p>Using the <strong class="source-inline">select()</strong> method, we select the column that will remain intact and expand the desired <a id="_idIndexMarker468"/>one. We do so because Spark requires at least one column flattened as a reference. As you can observe, the other columns regarding the status of the <strong class="bold">API</strong> ingestion were removed from <span class="No-Break">this schema.</span></p>
<p>The second parameter of <strong class="source-inline">select()</strong> is the <strong class="source-inline">explode()</strong> method, where we pass the <strong class="source-inline">holiday</strong> column and attribute an alias. The second chain, <strong class="source-inline">select()</strong>, will only retrieve the <strong class="source-inline">holidaysExplode</strong> column. <em class="italic">Step 5</em> follows the same process but for the <span class="No-Break"><strong class="source-inline">weekdays</strong></span><span class="No-Break"> column.</span></p>
<h2 id="_idParaDest-218"><a id="_idTextAnchor218"/>There’s more…</h2>
<p>As we previously discussed, there are many ways of flattening a JSON and applying a schema. You can see an example from Thomas at his <em class="italic">Medium blog</em> <span class="No-Break">here: </span><a href="mailto:https://medium.com/@thomaspt748/how-to-flatten-json-files-dynamically-using-apache-pyspark-c6b1b5fd4777"><span class="No-Break">https://medium.com/@thomaspt748/how-to-flatten-json-files-dynamically-using-apache-pyspark-c6b1b5fd4777</span></a><span class="No-Break">.</span></p>
<p>He uses Python functions to decouple the nested fields and then apply the <span class="No-Break">PySpark code.</span></p>
<p><em class="italic">Towards Data Science</em> also offers a solution using Python’s lambda function. You can see it <span class="No-Break">here: </span><a href="https://towardsdatascience.com/flattening-json-records-using-pyspark-b83137669def"><span class="No-Break">https://towardsdatascience.com/flattening-json-records-using-pyspark-b83137669def</span></a><span class="No-Break">.</span></p>
<p>Flattening the JSON file can be a fantastic approach but requires more knowledge of complex Python functions. Again, there is no right way of doing it; it is important to bring a solution that you and your team <span class="No-Break">can support.</span></p>
<h2 id="_idParaDest-219"><a id="_idTextAnchor219"/>See also</h2>
<p>You can <a id="_idIndexMarker469"/>read more about the PySpark data structures <span class="No-Break">here: </span><a href="https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/"><span class="No-Break">https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-220"><a id="_idTextAnchor220"/>Inserting formatted SparkSession logs to facilitate your work</h1>
<p>A commonly <a id="_idIndexMarker470"/>underestimated best practice is how to create valuable logs. Applications that log information and small code files can save a significant amount of debugging time. This is also true when ingesting or <span class="No-Break">processing data.</span></p>
<p>This recipe approaches the best practice of logging events in our PySpark scripts. The examples here will give a more generic overview, which can be applied to any other piece of code and will even be used later in <span class="No-Break">this book.</span></p>
<h2 id="_idParaDest-221"><a id="_idTextAnchor221"/>Getting ready</h2>
<p>We will use the <strong class="source-inline">listings.csv</strong> file to execute the <strong class="source-inline">read</strong> method from Spark. You can find this dataset inside the GitHub repository for this book. Make sure your <strong class="source-inline">SparkSession</strong> is up <span class="No-Break">and running.</span></p>
<h2 id="_idParaDest-222"><a id="_idTextAnchor222"/>How to do it…</h2>
<p>Here are the steps to perform <span class="No-Break">this recipe:</span></p>
<ol>
<li><strong class="bold">Setting the log level</strong>: Now, using <strong class="source-inline">sparkContext</strong>, we will assign the <span class="No-Break">log level:</span><pre class="source-code">
spark.sparkContext.setLogLevel("INFO")</pre></li>
<li><strong class="bold">Instantiating the log4j logger</strong>: The next step is to create a variable that instantiates the <span class="No-Break"><strong class="source-inline">getLogger()</strong></span><span class="No-Break"> method:</span><pre class="source-code">
Logger= spark._jvm.org.apache.log4j.Logger
syslogger = Logger.getLogger(__name__)</pre></li>
<li><strong class="bold">Creating our logs</strong>: With <strong class="source-inline">getLogger()</strong> instantiated, we can now call the internal methods representing the log levels, such as <strong class="source-inline">ERROR</strong> <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">INFO</strong></span><span class="No-Break">:</span><pre class="source-code">
syslogger.error("Error message sample")
syslogger.info("Info message sample")</pre></li>
</ol>
<p>This gives us the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer154">
<img alt="Figure 6.16 – Log message output example from Spark" height="44" src="image/Figure_6.16_B19453.jpg" width="467"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.16 – Log message output example from Spark</p>
<ol>
<li value="4"><strong class="bold">Creating a DataFrame</strong>: Now, let’s create a DataFrame using a file we have already <a id="_idIndexMarker471"/>seen in this chapter and observe <span class="No-Break">the output:</span><pre class="source-code">
try:
    df = spark.read.options(header=True, sep=',',
                              multiLine=True, escape='"',
                             inferSchema=True) \
                    .csv('listings.csv')
except Exception as e:
    syslogger.error(f"Error message: {e}")</pre></li>
</ol>
<p>You should see the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer155">
<img alt="Figure 6.17 – PySpark logs output" height="209" src="image/Figure_6.17_B19453.jpg" width="1085"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.17 – PySpark logs output</p>
<p>Don’t worry if more lines are showing in your console; the image was cut to make it <span class="No-Break">more readable.</span></p>
<h2 id="_idParaDest-223"><a id="_idTextAnchor223"/>How it works…</h2>
<p>Let’s explore a bit of what was done in this recipe. Spark has a native library called <strong class="source-inline">log4j</strong>, or <strong class="source-inline">rootLogger</strong>. <strong class="source-inline">log4j</strong> is the default logging mechanism Spark uses to throw all log messages such as <strong class="source-inline">TRACE</strong>, <strong class="source-inline">DEBUG</strong>, <strong class="source-inline">INFO</strong>, <strong class="source-inline">WARN</strong>, <strong class="source-inline">ERROR</strong>, and <strong class="source-inline">FATAL</strong>. The severity of the message increases with each level. By default, Spark has logs at the <strong class="source-inline">WARN</strong> level, so new messages started to appear when we set it to <strong class="source-inline">INFO</strong>, such as memory <span class="No-Break">store information.</span></p>
<p>We will observe more <strong class="source-inline">INFO</strong> logs when we execute a <strong class="source-inline">spark-submit</strong> command to run PySpark (we will cover this later in <a href="B19453_11.xhtml#_idTextAnchor402"><span class="No-Break"><em class="italic">Chapter 11</em></span></a><span class="No-Break">).</span></p>
<p>Depending on how big our script is and which environment it belongs to, it is a good practice to set it only to show <strong class="source-inline">ERROR</strong> messages. We can change the log level with the <span class="No-Break">following code:</span></p>
<pre class="source-code">
spark.sparkContext.setLogLevel("ERROR")</pre>
<p>As you can observe, we used <strong class="source-inline">sparkContext</strong> to set the log level. <strong class="source-inline">sparkContext</strong> is a crucial <a id="_idIndexMarker472"/>component in Spark applications. It manages the cluster resources, coordinates the execution of tasks, and provides an interface for interacting with distributed datasets and performing computations in a distributed and parallel manner. Defining the log level of our code will prevent levels below <strong class="source-inline">ERROR</strong> from appearing on the console, making <span class="No-Break">it cleaner:</span></p>
<pre class="source-code">
Logger= spark._jvm.org.apache.log4j.Logger
syslogger = Logger.getLogger(__name__)</pre>
<p>Next, we retrieved our <strong class="source-inline">log4j</strong> module and its <strong class="source-inline">Logger</strong> class from the instantiated session. This class has a method to show logs already formatted as Spark does. The <strong class="source-inline">__name__</strong> parameter will retrieve the name of the current module from the Python internals; in our case, it <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">main</strong></span><span class="No-Break">.</span></p>
<p>With this, we can create customized logs <span class="No-Break">as follows:</span></p>
<pre class="source-code">
syslogger.error("Error message sample")
syslogger.info("Info message sample")</pre>
<p>And, of course, you can ally the PySpark logs with your Python output for a complete solution <a id="_idIndexMarker473"/>using a <strong class="source-inline">try…except</strong> exception-handling closure. Let’s simulate an error by passing the wrong filename to our reading function <span class="No-Break">as follows:</span></p>
<pre class="source-code">
try:
    df = spark.read.options(header=True, sep=',',
                              multiLine=True, escape='"',
                             inferSchema=True) \
                    .csv('listings.cs') # Error here
except Exception as e:
    syslogger.error(f"Error message: {e}")</pre>
<p>You should see the following <span class="No-Break">output message:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer156">
<img alt="Figure 6.18 – Error message formatted by log4j" height="29" src="image/Figure_6.18_B19453.jpg" width="882"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.18 – Error message formatted by log4j</p>
<h2 id="_idParaDest-224"><a id="_idTextAnchor224"/>There’s more…</h2>
<p>Many more customizations are available in <strong class="source-inline">log4j</strong>, but this might require a little more work. For example, you can change some <strong class="source-inline">WARN</strong> messages to appear as <strong class="source-inline">ERROR</strong> messages and prevent the script from continuing to be processed. A practical example in this chapter would be the <em class="italic">Importing structured data using a well-deﬁned schema</em> recipe, when the number of columns in the schema does not match <span class="No-Break">the file.</span></p>
<p>You can read more about it on <em class="italic">Ivan Trusov</em>’s blog page <span class="No-Break">here: </span><a href="https://polarpersonal.medium.com/writing-pyspark-logs-in-apache-spark-and-databricks-8590c28d1d51"><span class="No-Break">https://polarpersonal.medium.com/writing-pyspark-logs-in-apache-spark-and-databricks-8590c28d1d51</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-225"><a id="_idTextAnchor225"/>See also</h2>
<ul>
<li>You can find <a id="_idIndexMarker474"/>more about the PySpark best practices <span class="No-Break">here: </span><a href="https://climbtheladder.com/10-pyspark-logging-best-practices/"><span class="No-Break">https://climbtheladder.com/10-pyspark-logging-best-practices/</span></a><span class="No-Break">.</span></li>
<li>Read more about <strong class="source-inline">sparkContext</strong> and how it works in the Spark official documentation <span class="No-Break">here: </span><a href="https://spark.apache.org/docs/3.2.0/api/java/org/apache/spark/SparkContext.xhtml"><span class="No-Break">https://spark.apache.org/docs/3.2.0/api/java/org/apache/spark/SparkContext.xhtml</span></a><span class="No-Break">.</span></li>
</ul>
<h1 id="_idParaDest-226"><a id="_idTextAnchor226"/>Further reading</h1>
<ul>
<li><a href="https://www.tibco.com/reference-center/what-is-structured-data"><span class="No-Break">https://www.tibco.com/reference-center/what-is-structured-data</span></a></li>
<li><a href="https://spark.apache.org/docs/latest/sql-programming-guide.xhtml"><span class="No-Break">https://spark.apache.org/docs/latest/sql-programming-guide.xhtml</span></a></li>
<li><a href="https://mungingdata.com/pyspark/schema-structtype-structfield/"><span class="No-Break">https://mungingdata.com/pyspark/schema-structtype-structfield/</span></a></li>
<li><a href="https://sparkbyexamples.com/pyspark/pyspark-select-nested-struct-columns/"><span class="No-Break">https://sparkbyexamples.com/pyspark/pyspark-select-nested-struct-columns/</span></a></li>
<li><a href="https://benalexkeen.com/using-pyspark-to-read-and-flatten-json-data-using-an-enforced-schema/"><span class="No-Break">https://benalexkeen.com/using-pyspark-to-read-and-flatten-json-data-using-an-enforced-schema/</span></a></li>
<li><a href="https://towardsdatascience.com/json-in-databricks-and-pyspark-26437352f0e9"><span class="No-Break">https://towardsdatascience.com/json-in-databricks-and-pyspark-26437352f0e9</span></a></li>
</ul>
</div>
</div></body></html>