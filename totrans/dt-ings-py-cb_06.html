<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-195"><a id="_idTextAnchor195"/>6</h1>
<h1 id="_idParaDest-196"><a id="_idTextAnchor196"/>Using PySpark with Deﬁned and Non-Deﬁned Schemas</h1>
<p>Generally, schemas are forms used to create or apply structures to data. As someone who works or will work with large volumes of data, it is essential to understand how to manipulate DataFrames and apply structure when it is necessary to bring more context to the information involved.</p>
<p>However, as seen in the previous chapters, data can come from different sources or be present without a well-defined structure, and applying a schema can be challenging. Here, we will see how to create schemas and standard formats using PySpark with structured and unstructured data.</p>
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Applying schemas to data ingestion</li>
<li>Importing structured data using a well-deﬁned schema</li>
<li>Importing unstructured data with an undefined schema</li>
<li>Ingesting unstructured data with a well-deﬁned schema and format</li>
<li>Inserting formatted SparkSession logs to facilitate your work</li>
</ul>
<h1 id="_idParaDest-197"><a id="_idTextAnchor197"/>Technical requirements</h1>
<p>You can also find the code for this chapter in the GitHub repository here: <a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook</a>.</p>
<p>Using <strong class="bold">Jupyter Notebook</strong> is not mandatory but can help you see how the code works interactively. Since we will execute Python and PySpark code, it can help us understand the scripts better. Once you have it installed, you can execute Jupyter using the following line:</p>
<pre class="source-code">
$ jupyter Notebook</pre>
<p>It is recommended to create a separate folder to store the Python files or Notebooks we will cover in this chapter; however, feel free to organize the files in the best way that fits you.</p>
<p>In this chapter, all recipes will need a <code>SparkSession</code> instance initialized, and you can use the same session for all of them. You can use the following code to create your session:</p>
<pre class="source-code">
from pyspark.sql import SparkSession
spark = SparkSession.builder \
      .master("local[1]") \
      .appName("chapter6_schemas") \
      .config("spark.executor.memory", <code>'3g'</code>) \
      .config("spark.executor.cores", '1') \
      .config("spark.cores.max", '1') \
      .getOrCreate()</pre>
<p class="callout-heading">Note</p>
<p class="callout">A <code>WARN</code> message as output is expected in some cases, especially if you are using WSL on Windows, so you don’t need to worry.</p>
<h1 id="_idParaDest-198"><a id="_idTextAnchor198"/>Applying schemas to data ingestion</h1>
<p>The application of schemas is common practice when ingesting data, and PySpark natively supports <a id="_idIndexMarker429"/>applying them to DataFrames. To <a id="_idIndexMarker430"/>define and apply schemas to our DataFrames, we need to understand some concepts of Spark.</p>
<p>This recipe introduces the basic concept of working with schemas using PySpark and its best practices so that we can later apply them to structured and unstructured data.</p>
<h2 id="_idParaDest-199"><a id="_idTextAnchor199"/>Getting ready</h2>
<p>Make sure PySpark is installed and working on your machine for this recipe. You can run the following code on your command line to check this requirement:</p>
<pre class="source-code">
$ pyspark --version</pre>
<p>You should see the following output:</p>
<div><div><img alt="Figure 6.1 – PySpark version console output" height="346" src="img/Figure_6.1_B19453.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – PySpark version console output</p>
<p>If don’t have PySpark installed on your local machine, please refer to the <em class="italic">Installing PySpark</em> recipe in <a href="B19453_01.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a>.</p>
<p>I will <a id="_idIndexMarker431"/>use Jupyter Notebook to execute the code to make it <a id="_idIndexMarker432"/>more interactive. You can use this link and follow the instructions on the screen to install it: <a href="https://jupyter.org/install">https://jupyter.org/install</a>.</p>
<p>If you already have it installed, check the version using the following code on your command line:</p>
<pre class="source-code">
$ jupyter --version</pre>
<p>The following screenshot shows the expected output:</p>
<div><div><img alt="Figure 6.2 – Jupyter package versions" height="320" src="img/Figure_6.2_B19453.jpg" width="367"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Jupyter package versions</p>
<p>As you can see, the Notebook version was <code>6.4.4</code> at the time this book was written. Make sure to always use the latest version.</p>
<h2 id="_idParaDest-200"><a id="_idTextAnchor200"/>How to do it…</h2>
<p>Here are the steps to carry out the recipe:</p>
<ol>
<li><strong class="bold">Creating mock data</strong>: Before applying the schema to our DataFrame, we need to create a simple <a id="_idIndexMarker433"/>set containing simulated data <a id="_idIndexMarker434"/>of people’s information in this format—ID, name, last name, age, and gender:<pre class="source-code">
my_data = [("3456","Cristian","Rayner",30,"M"),
            ("3567","Guto","Flower",35,"M"),
            ("9867","Yasmin","Novak",23,"F"),
            ("3342","Tayla","Mejia",45,"F"),
            ("8890","Barbara","Kumar",20,"F")
            ]</pre></li>
<li><strong class="bold">Importing and structuring the schema</strong>: The next step is to import the types and create the structure of our schema:<pre class="source-code">
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
schema = StructType([ \
    StructField("id",StringType(),True), \
    StructField("name",StringType(),True), \
    StructField("lastname",StringType(),True), \
    StructField("age", IntegerType(), True), \
    StructField("gender", StringType(), True), \
  ])</pre></li>
<li><strong class="bold">Creating the DataFrame</strong>: Then, we make the DataFrame, applying the schema we have created:<pre class="source-code">
df = spark.createDataFrame(data=my_data,schema=schema)</pre></li>
</ol>
<p>When printing <a id="_idIndexMarker435"/>our DataFrame schema <a id="_idIndexMarker436"/>using the <code>.printSchema()</code> method, this is the expected output:</p>
<div><div><img alt="Figure 6.3 – The DataFrame schema" height="116" src="img/Figure_6.3_B19453.jpg" width="341"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – The DataFrame schema</p>
<h2 id="_idParaDest-201"><a id="_idTextAnchor201"/>How it works…</h2>
<p>Before understanding the methods in <em class="italic">step 2</em>, let’s step back a bit and understand the concept of a DataFrame.</p>
<p>DataFrame is like a table with data stored and organized in a two-dimensional array, which resembles a table from a relational database such as MySQL or Postgres. Each line corresponds to a record, and libraries such as pandas and PySpark, by default, assign a record number internally to each line (or index).</p>
<div><div><img alt="Figure 6.4 – GeeksforGeeks DataFrame explanation" height="462" src="img/Figure_6.4_B19453.jpg" width="751"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – GeeksforGeeks DataFrame explanation</p>
<p class="callout-heading">Note</p>
<p class="callout">Speaking of the Pandas library, it is common to refer to a column in a Pandas DataFrame as a series and expect it to behave like a Python list. It makes it easier to manipulate data for analysis and visualization.</p>
<p>The objective of <a id="_idIndexMarker437"/>using a DataFrame is to utilize the several <a id="_idIndexMarker438"/>optimizations for data processing under the hood that Spark brings, which are directly linked to parallel processing.</p>
<p>Back to the schema definition in our <code>schema</code> variable, let’s take a look at the code we created:</p>
<pre class="source-code">
schema = StructType([ \
    StructField("id",StringType(),True), \
    StructField("name",StringType(),True), \
    StructField("lastname",StringType(),True), \
    StructField("age", IntegerType(), True), \
    StructField("gender", StringType(), True), \
  ])</pre>
<p>The first object we declare is the <code>StructType</code> class. This class will create an object of collection or our rows. Next, we declare a <code>StructField</code> instance, representing our column with its name, data type, whether it is nullable, and its metadata when applicable. <code>StructField</code> must be in the same order as the columns in the DataFrame; otherwise, it can generate errors due to the incompatibility of a data type (for example, the <a id="_idIndexMarker439"/>column has string values, and we are setting it <a id="_idIndexMarker440"/>as an integer) or the presence of null values. Defining <code>StructField</code> is an excellent opportunity to standardize the name of the DataFrame and, therefore, the analytical data.</p>
<p>Finally, <code>StringType</code> and <code>IntegerType</code> are the methods that will cast the data type into the respective columns. They were imported at the beginning of the recipe and derived from the SQL types inside PySpark. In our mock data example, we defined the <code>id</code> and <code>age</code> columns as <code>IntegerType</code> since we expect no other kind of data in them. However, there are many situations where the <code>id</code> column is referred to as a string type, usually when the data comes from different systems.</p>
<p>Here we used <code>StringType</code> and <code>IntegerType</code>, but many others can be used to create context and standardize our data. Refer to the following figure:</p>
<div><div><img alt="Figure 6.5 – SparkbyExample table of data types in Spark" height="415" src="img/Figure_6.5_B19453.jpg" width="338"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – SparkbyExample table of data types in Spark</p>
<p>You can understand more about how to apply the <strong class="bold">Spark data types</strong> in the Spark official <a id="_idIndexMarker441"/>documentation here: <a href="https://spark.apache.org/docs/latest/sql-ref-datatypes.xhtml">https://spark.apache.org/docs/latest/sql-ref-datatypes.xhtml</a>.</p>
<h2 id="_idParaDest-202"><a id="_idTextAnchor202"/>There’s more…</h2>
<p>When handling terminology, there needs to be a common understanding about using a dataset or DataFrame, especially if you are a newcomer to the data world.</p>
<p>A dataset is a collection <a id="_idIndexMarker442"/>of data containing rows and columns (for example, relational data) or documents and files (for example, non-relational data). It comes from a source and is available in different file formats.</p>
<p>On the other hand, a DataFrame is derived from a dataset, presenting the data in a tabular form even <a id="_idIndexMarker443"/>if that is not the primary format. The DataFrame can transform a MongoDB document collection into a tabular organization based on the configurations set when it is created.</p>
<h2 id="_idParaDest-203"><a id="_idTextAnchor203"/>See also</h2>
<p>More examples on the <em class="italic">SparkbyExample</em> site can be found here: <a href="https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/">https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/</a>.</p>
<h1 id="_idParaDest-204"><a id="_idTextAnchor204"/>Importing structured data using a well-deﬁned schema</h1>
<p>As seen in the previous chapter, <em class="italic">Ingesting Data from Structured and Unstructured Databases</em>, structured <a id="_idIndexMarker444"/>data has a standard format presented in rows and columns and is often stored inside a database.</p>
<p>Due to its format, the application of a DataFrame schema tends to be less complex and has several benefits, such as ensuring the ingested information is the same as the data source or follows a rule.</p>
<p>In this recipe, we will ingest data from a structured file such as a CSV file and apply a DataFrame schema to understand better how it is used in a real-world scenario.</p>
<h2 id="_idParaDest-205"><a id="_idTextAnchor205"/>Getting ready</h2>
<p>This exercise requires the <code>listings.csv</code> file found inside the GitHub repository for this book. Also, make sure your <code>SparkSession</code> is initialized.</p>
<p>All the code in this recipe can be executed in Jupyter Notebook cells or a PySpark shell.</p>
<h2 id="_idParaDest-206"><a id="_idTextAnchor206"/>How to do it…</h2>
<p>Here are <a id="_idIndexMarker445"/>the steps to perform this recipe:</p>
<ol>
<li><code>StringType</code> and <code>IntegerType</code>, we will include two more data types in our import, <code>FloatType</code> and <code>DateType</code>, shown as follows:<pre class="source-code">
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, DoubleType</pre></li>
<li><code>StructField</code> and assign them to a respective data type:<pre class="source-code">
schema = StructType([ \
    StructField("id",IntegerType(),True), \
    StructField("name",StringType(),True), \
    StructField("host_id",IntegerType(),True), \
    StructField("host_name",StringType(),True), \
    StructField("neighbourhood_group",StringType(),True), \
    StructField("neighbourhood",StringType(),True), \
    StructField("latitude",DoubleType(),True), \
    StructField("longitude",DoubleType(),True), \
    StructField("room_type",StringType(),True), \
    StructField("price",FloatType(),True), \
    StructField("minimum_nights",IntegerType(),True), \
    StructField("number_of_reviews",IntegerType(),True), \
    StructField("last_review",DateType(),True), \
    StructField("reviews_per_month",FloatType(),True), \
      StructField("calculated_host_listings_count",IntegerType(),True), \
    StructField("availability_365",IntegerType(),True), \
    StructField("number_of_reviews_ltm",IntegerType(),True), \
    StructField("license",StringType(),True)
  ])</pre></li>
<li><code>listings.csv</code> file <a id="_idIndexMarker446"/>with the <code>.options()</code> configurations and add the <code>.schema()</code> method with the <code>schema</code> variable seen in <em class="italic">step 2</em>.<pre class="source-code">
df = spark.read.options(header=True, sep=',',
                          multiLine=True, escape='"')\
                .schema(schema) \
                .csv('listings.csv')</pre></li>
</ol>
<p>If everything is well set, you should see no output from this execution.</p>
<ol>
<li value="4"><strong class="bold">Checking the read DataFrame</strong>: We can check the schema of our DataFrame by executing the following code:<pre class="source-code">
df.printSchema()</pre></li>
</ol>
<p>You should see the following output:</p>
<div><div><img alt="Figure 6.6 – listings.csv DataFrame schema" height="364" src="img/Figure_6.6_B19453.jpg" width="533"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – listings.csv DataFrame schema</p>
<h2 id="_idParaDest-207"><a id="_idTextAnchor207"/>How it works…</h2>
<p>We made a few additions in this exercise that differ from the last recipe, <em class="italic">Applying schemas to </em><em class="italic">data ingestion.</em></p>
<p>As usual, we started by importing the required methods to make the script work. We added <a id="_idIndexMarker447"/>three more data types: float, double, and date. The choice was made based on what the CSV file contained. Let’s look at the first lines of our file, as you can see in the following screenshot:</p>
<div><div><img alt="Figure 6.7 – listings.csv view from Microsoft Excel" height="163" src="img/Figure_6.7_B19453.jpg" width="1302"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – listings.csv view from Microsoft Excel</p>
<p>We can observe different types of numerical fields; some require more decimal places, and <code>last_review</code> is in a date format. Because of this, we made additional library imports, as you can see in the following piece of code:</p>
<pre class="source-code">
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType, DateType</pre>
<p><em class="italic">Step 2</em> is similar to what we did before, where we attributed the column names and their respective data types. It is in <em class="italic">step 3</em> that we made the schema attribution using the <code>schema()</code> method from the <code>SparkSession</code> class. If the schema contains the same number of columns as the file, we should expect no output here; otherwise, this message will appear:</p>
<div><div><img alt="Figure 6.8 – Output warning message when the schema does not match" height="74" src="img/Figure_6.8_B19453.jpg" width="1166"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – Output warning message when the schema does not match</p>
<p>The content is important, even if it is a <code>WARN</code> log message. Looking closely, it says the schema <a id="_idIndexMarker448"/>does not match the number of columns in the file. This could be a problem later in the ETL pipeline when loading data into a data warehouse or any other analytical database.</p>
<h2 id="_idParaDest-208"><a id="_idTextAnchor208"/>There’s more…</h2>
<p>If you go back to <a href="B19453_04.xhtml#_idTextAnchor127"><em class="italic">Chapter 4</em></a>, you will observe that one of our CSV readings contains an <code>inferSchema</code> parameter inserted into the <code>options()</code> method. Refer to the following code:</p>
<pre class="source-code">
df_2 = spark.read.options(header=True, sep=',',
                          multiLine=True, escape='"',
                         inferSchema=True) \
                .csv('listings.csv')</pre>
<p>This parameter tells Spark to infer the data types based on the traits of the rows. For example, if a row has a value without quotation marks and is a number, there is a good chance this is an integer. However, if it contains a quotation mark, Spark can interpret it as a string, and any numerical operation will break.</p>
<p>In the recipe, if we use <code>inferSchema</code>, we will see a very similar <code>printSchema</code> output from the <a id="_idIndexMarker449"/>schema we defined, except for some fields that were interpreted as <code>DoubleType</code> and that we declared as <code>FloatType</code> or <code>DateType</code>. This is shown in the following screenshot:</p>
<div><div><img alt="Figure 6.9 – Schema comparison when using a schema with inferSchema" height="414" src="img/Figure_6.9_B19453.jpg" width="1086"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – Schema comparison when using a schema with inferSchema</p>
<p>Even though it seems like a tiny detail, when dealing with streaming or a large dataset and few computational <a id="_idIndexMarker450"/>resources, it can make a difference. Float types have a small range and <a id="_idIndexMarker451"/>bring in high processing power. Double data types bring more precision and are used to decrease mathematical errors or values being rounded by decimal data types.</p>
<h2 id="_idParaDest-209"><a id="_idTextAnchor209"/>See also</h2>
<p>Read more <a id="_idIndexMarker452"/>about float and double types on the <em class="italic">Hackr IO</em> website here: <a href="https://hackr.io/blog/float-vs-double">https://hackr.io/blog/float-vs-double</a>.</p>
<h1 id="_idParaDest-210"><a id="_idTextAnchor210"/>Importing unstructured data without a schema</h1>
<p>As seen before, <a id="_idIndexMarker453"/>unstructured data or <strong class="bold">NoSQL</strong> is a <a id="_idIndexMarker454"/>group of information that does not follow a format, such as relational or tabular data. It can be presented as an image, video, metadata, transcripts, and so on. The data ingestion process usually involves a JSON file or a document collection, as we previously saw when ingesting data from <strong class="bold">MongoDB</strong>.</p>
<p>In this recipe, we will read a JSON file and transform it into a DataFrame without a schema. Although unstructured data is supposed to have a more flexible design, we will see some implications of not having any schema or structure in our DataFrame.</p>
<h2 id="_idParaDest-211"><a id="_idTextAnchor211"/>Getting ready…</h2>
<p>Here, we will use the <code>holiday_brazil.json</code> file to create the DataFrame. You can find it in the GitHub repository here: <a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook</a>.</p>
<p>We will use <code>SparkSession</code> to read the JSON file and create a DataFrame to ensure the session is up and running.</p>
<p>All code <a id="_idIndexMarker455"/>can be executed in a Jupyter Notebook <a id="_idIndexMarker456"/>or at PySpark shell.</p>
<h2 id="_idParaDest-212"><a id="_idTextAnchor212"/>How to do it…</h2>
<p>Let’s now read our <code>holiday_brazil.json</code> file, observing how Spark handles it:</p>
<ol>
<li><code>multiline</code> as a parameter to the <code>options()</code> method. We will also let PySpark infer the data types in it:<pre class="source-code">
df_json = spark.read.option("multiline","true") \
                    .json('holiday_brazil.json')</pre></li>
</ol>
<p>If all goes right, you should see no output.</p>
<ol>
<li value="2"><code>printSchema()</code> method, we can see how PySpark interpreted the data types of each key:<pre class="source-code">
df_json.printSchema()</pre></li>
</ol>
<p>The following screenshot is the expected output:</p>
<div><div><img alt="Figure 6.10 – holiday_brazil.json DataFrame schema" height="477" src="img/Figure_6.10_B19453.jpg" width="571"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – holiday_brazil.json DataFrame schema</p>
<ol>
<li value="3"><code>toPandas()</code> function to visualize our DataFrame better:<pre class="source-code">
df_json.toPandas()</pre></li>
</ol>
<p>You should see this output:</p>
<div><div><img alt="Figure 6.11 – Output of toPandas() vision from the DataFrame" height="79" src="img/Figure_6.11_B19453.jpg" width="987"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – Output of toPandas() vision from the DataFrame</p>
<h2 id="_idParaDest-213"><a id="_idTextAnchor213"/>How it works…</h2>
<p>Let’s <a id="_idIndexMarker458"/>take a look at the output from <em class="italic">step 2</em>:</p>
<div><div><img alt="Figure 6.12 – Holiday_brasil.json DataFrame schema" height="468" src="img/Figure_6.12_B19453.jpg" width="580"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – Holiday_brasil.json DataFrame schema</p>
<p>As we can observe, Spark only brought four columns, the first four keys in the JSON file, and ignored the rest of the other nested keys by keeping them inside the main four ones. This happens because Spark needs to handle the parameter better by flattening values from nested fields, even though we passed <code>multiline</code> as a parameter in the <code>options()</code> configuration.</p>
<p>Another important <a id="_idIndexMarker459"/>point is that data types inferred <a id="_idIndexMarker460"/>for the <code>numeric</code> keys inside the <code>weekday</code> array are string values and should be integers. This happened because those values have quotation marks, as you can see in the following figure:</p>
<div><div><img alt="Figure 6.13 – weekday objects" height="224" src="img/Figure_6.13_B19453.jpg" width="230"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.13 – weekday objects</p>
<p>Applying this to a real-world scenario where it is crucial to have a qualitative evaluation, these unformatted and schemaless DataFrame can create problems later when uploaded to a data warehouse. If a field suddenly changes its name or is unavailable in the source, it can lead to data inconsistency. There are some ways to solve this, and we will cover this further in the following recipe, <em class="italic">Ingesting unstructured data with a well-deﬁned schema </em><em class="italic">and format.</em></p>
<p>However, there <a id="_idIndexMarker461"/>are plenty of other scenarios where a <a id="_idIndexMarker462"/>schema is not needed to be defined or unstructured data doesn’t need to be standardized. An excellent example is application logs or metadata, where data is often linked to the application’s or system’s availability to send the information. In that case, solutions such as <strong class="bold">ElasticSearch</strong>, <strong class="bold">DynamoDB</strong>, and many others are good storage options that provide query support. In other words, most of the issues here will be more inclined to generate a quantitative output.</p>
<h1 id="_idParaDest-214"><a id="_idTextAnchor214"/>Ingesting unstructured data with a well-deﬁned schema and format</h1>
<p>In the previous recipe, <em class="italic">Importing unstructured data without schema</em>, we read a JSON file without any <a id="_idIndexMarker463"/>schema or formatting application. This led us to an odd output, which could bring confusion and require additional work later in the data pipeline. While this example pertains specifically to a JSON file, it also applies to all other NoSQL or unstructured data that needs to be converted into analytical data.</p>
<p>The objective is to continue the last recipe and apply a schema and standard to our data, making it more legible and easy to process in the subsequent phases of <strong class="bold">ETL</strong>.</p>
<h2 id="_idParaDest-215"><a id="_idTextAnchor215"/>Getting ready</h2>
<p>This recipe has the exact same requirements as the <em class="italic">Importing unstructured data without a </em><em class="italic">schema</em> recipe.</p>
<h2 id="_idParaDest-216"><a id="_idTextAnchor216"/>How to do it…</h2>
<p>We will perform the following steps to perform this recipe:</p>
<ol>
<li><strong class="bold">Importing data types</strong>: As usual, let’s start by importing our data types from the PySpark library:<pre class="source-code">
from pyspark.sql.types import StructType, ArrayType, StructField, StringType, IntegerType, MapType</pre></li>
<li><strong class="bold">Structuring the JSON schema</strong>: Next, we set the schema based on how the JSON <a id="_idIndexMarker464"/>is structured:<pre class="source-code">
schema = StructType([ \
        StructField('status', StringType(), True),
        StructField('holidays', ArrayType(
            StructType([
                StructField('name', StringType(), True),
                StructField('date', DateType(), True),
                StructField('observed', StringType(), True),
                StructField('public', StringType(), True),
                StructField('country', StringType(), True),
                StructField('uuid', StringType(), True),
                StructField('weekday', MapType(StringType(), MapType(StringType(),StringType(),True),True))
            ])
        ))
    ])</pre></li>
<li><code>schema()</code> method to apply the schema created in the previous step:<pre class="source-code">
df_json = spark.read.option("multiline","true") \
                    .schema(schema) \
                    .json('holiday_brazil.json')</pre></li>
<li><code>explode()</code> method, let’s broaden the fields inside the <code>holidays</code> column:<pre class="source-code">
from pyspark.sql.functions import explode
exploded_json = df_json.select('status', explode("holidays").alias("holidaysExplode"))\
        .select("status", "holidaysExplode.*")</pre></li>
<li><strong class="bold">Expanding more columns</strong>: This is an optional step, but if needed, we can keep <a id="_idIndexMarker465"/>growing and flattening the other columns with nested content inside:<pre class="source-code">
exploded_json2 = exploded_json.select("*", explode('weekday').alias('weekday_type', 'weekday_objects'))</pre></li>
<li><code>toPandas()</code> function, we can better view what our DataFrame looks like now:<pre class="source-code">
exploded_json2.toPandas()</pre></li>
</ol>
<p>You should see the following output:</p>
<div><div><img alt="Figure 6.14 – The DataFrame with expanded columns" height="546" src="img/Figure_6.14_B19453.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.14 – The DataFrame with expanded columns</p>
<p>The final DataFrame can be saved as a Parquet file, which can make the next steps in the data pipeline easier to handle.</p>
<h2 id="_idParaDest-217"><a id="_idTextAnchor217"/>How it works…</h2>
<p>As you can observe, this JSON file has some complexity due to the number of nested objects. When handling a file such as this, we can use several approaches. When coding, there are many approaches to reaching a solution. Let’s understand how this recipe works.</p>
<p>In <em class="italic">step 1</em>, two new data types were imported—<code>ArrayType</code> and <code>MapType</code>. Even though there is some confusion when using each type, it is somewhat simple to understand when looking at the JSON structure.</p>
<p>We used <code>ArrayType</code> for the <code>holiday</code> key because its structure looks like this:</p>
<pre class="source-code">
holiday : [{},{}...]</pre>
<p>It is an array (or a list if we use Python) of objects, each representing a holiday in Brazil. When <code>ArrayType</code> has other objects inside it, we need to re-use <code>StructType</code> to inform <a id="_idIndexMarker466"/>Spark of the structure of objects that reside inside. That’s why in <em class="italic">step 2</em>, our schema started to look like this:</p>
<pre class="source-code">
StructField('holidays', ArrayType(
            StructType([
  ...
])
))</pre>
<p>The following new data type is <code>MapType</code>. This type refers to an object with other objects inside. If we are only using Python, it can be referred to as a dictionary. PySpark extends this data type from a <strong class="bold">superclass</strong> in Spark, and you can read more about that here: <a href="https://spark.apache.org/docs/2.0.1/api/java/index.xhtml?org/apache/spark/sql/types/MapType.xhtml">https://spark.apache.org/docs/2.0.1/api/java/index.xhtml?org/apache/spark/sql/types/MapType.xhtml</a>.</p>
<p>The syntax of <code>MapType</code> requires the key-value type, the value type, and whether it accepts null values. We used this type in the <code>weekday</code> field, as you can see here:</p>
<pre class="source-code">
StructField('weekday', MapType(
StringType(),MapType(
StringType(),StringType(),True)
,True))</pre>
<p>Maybe this is the most complex structure we have seen so far, and that was due to how JSON gets structured through it:</p>
<pre class="source-code">
weekday : {
 day : {{}, {}},
 observed : {{}, {}}
}</pre>
<p>The structure <a id="_idIndexMarker467"/>in our schema created <code>MapType</code> for <code>weekday</code> and the subsequent keys, <code>day</code> and <code>observed</code>.</p>
<p>Once our schema is defined and applied to our DataFrame, we can make a preview of it using <code>df_json.show()</code>. If the schema does not match the JSON structure, you should see this output:</p>
<div><div><img alt="Figure 6.15 – df_json print" height="169" src="img/Figure_6.15_B19453.jpg" width="306"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.15 – df_json print</p>
<p>This demonstrates that Spark was unable to create the DataFrame correctly. The best action in this situation is to apply the schema step by step until the problem is resolved.</p>
<p>To flatten our fields inside the <code>holidays</code> column, we need to use a function from PySpark called <code>explode</code>, as you can see here:</p>
<pre class="source-code">
from pyspark.sql.functions import explode</pre>
<p>To apply the changes, we need to attribute the results to a variable where it will create a new DataFrame:</p>
<pre class="source-code">
exploded_json = df_json.select('status', explode("holidays").alias("holidaysExplode"))\
        .select("holidaysExplode.*")</pre>
<p class="callout-heading">Note</p>
<p class="callout">Although it seems redundant, preventing the original DataFrame from being modified is a good practice. If anything goes wrong, we don’t need to reread the file because we have the intact state of the original DataFrame.</p>
<p>Using the <code>select()</code> method, we select the column that will remain intact and expand the desired <a id="_idIndexMarker468"/>one. We do so because Spark requires at least one column flattened as a reference. As you can observe, the other columns regarding the status of the <strong class="bold">API</strong> ingestion were removed from this schema.</p>
<p>The second parameter of <code>select()</code> is the <code>explode()</code> method, where we pass the <code>holiday</code> column and attribute an alias. The second chain, <code>select()</code>, will only retrieve the <code>holidaysExplode</code> column. <em class="italic">Step 5</em> follows the same process but for the <code>weekdays</code> column.</p>
<h2 id="_idParaDest-218"><a id="_idTextAnchor218"/>There’s more…</h2>
<p>As we previously discussed, there are many ways of flattening a JSON and applying a schema. You can see an example from Thomas at his <em class="italic">Medium blog</em> here: <a href="mailto:https://medium.com/@thomaspt748/how-to-flatten-json-files-dynamically-using-apache-pyspark-c6b1b5fd4777">https://medium.com/@thomaspt748/how-to-flatten-json-files-dynamically-using-apache-pyspark-c6b1b5fd4777</a>.</p>
<p>He uses Python functions to decouple the nested fields and then apply the PySpark code.</p>
<p><em class="italic">Towards Data Science</em> also offers a solution using Python’s lambda function. You can see it here: <a href="https://towardsdatascience.com/flattening-json-records-using-pyspark-b83137669def">https://towardsdatascience.com/flattening-json-records-using-pyspark-b83137669def</a>.</p>
<p>Flattening the JSON file can be a fantastic approach but requires more knowledge of complex Python functions. Again, there is no right way of doing it; it is important to bring a solution that you and your team can support.</p>
<h2 id="_idParaDest-219"><a id="_idTextAnchor219"/>See also</h2>
<p>You can <a id="_idIndexMarker469"/>read more about the PySpark data structures here: <a href="https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/">https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/</a>.</p>
<h1 id="_idParaDest-220"><a id="_idTextAnchor220"/>Inserting formatted SparkSession logs to facilitate your work</h1>
<p>A commonly <a id="_idIndexMarker470"/>underestimated best practice is how to create valuable logs. Applications that log information and small code files can save a significant amount of debugging time. This is also true when ingesting or processing data.</p>
<p>This recipe approaches the best practice of logging events in our PySpark scripts. The examples here will give a more generic overview, which can be applied to any other piece of code and will even be used later in this book.</p>
<h2 id="_idParaDest-221"><a id="_idTextAnchor221"/>Getting ready</h2>
<p>We will use the <code>listings.csv</code> file to execute the <code>read</code> method from Spark. You can find this dataset inside the GitHub repository for this book. Make sure your <code>SparkSession</code> is up and running.</p>
<h2 id="_idParaDest-222"><a id="_idTextAnchor222"/>How to do it…</h2>
<p>Here are the steps to perform this recipe:</p>
<ol>
<li><code>sparkContext</code>, we will assign the log level:<pre class="source-code">
spark.sparkContext.setLogLevel("INFO")</pre></li>
<li><code>getLogger()</code> method:<pre class="source-code">
Logger= spark._jvm.org.apache.log4j.Logger
syslogger = Logger.getLogger(__name__)</pre></li>
<li><code>getLogger()</code> instantiated, we can now call the internal methods representing the log levels, such as <code>ERROR</code> or <code>INFO</code>:<pre class="source-code">
syslogger.error("Error message sample")
syslogger.info("Info message sample")</pre></li>
</ol>
<p>This gives us the following output:</p>
<div><div><img alt="Figure 6.16 – Log message output example from Spark" height="44" src="img/Figure_6.16_B19453.jpg" width="467"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.16 – Log message output example from Spark</p>
<ol>
<li value="4"><strong class="bold">Creating a DataFrame</strong>: Now, let’s create a DataFrame using a file we have already <a id="_idIndexMarker471"/>seen in this chapter and observe the output:<pre class="source-code">
try:
    df = spark.read.options(header=True, sep=',',
                              multiLine=True, escape='"',
                             inferSchema=True) \
                    .csv('listings.csv')
except Exception as e:
    syslogger.error(f"Error message: {e}")</pre></li>
</ol>
<p>You should see the following output:</p>
<div><div><img alt="Figure 6.17 – PySpark logs output" height="209" src="img/Figure_6.17_B19453.jpg" width="1085"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.17 – PySpark logs output</p>
<p>Don’t worry if more lines are showing in your console; the image was cut to make it more readable.</p>
<h2 id="_idParaDest-223"><a id="_idTextAnchor223"/>How it works…</h2>
<p>Let’s explore a bit of what was done in this recipe. Spark has a native library called <code>log4j</code>, or <code>rootLogger</code>. <code>log4j</code> is the default logging mechanism Spark uses to throw all log messages such as <code>TRACE</code>, <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code>, <code>ERROR</code>, and <code>FATAL</code>. The severity of the message increases with each level. By default, Spark has logs at the <code>WARN</code> level, so new messages started to appear when we set it to <code>INFO</code>, such as memory store information.</p>
<p>We will observe more <code>INFO</code> logs when we execute a <code>spark-submit</code> command to run PySpark (we will cover this later in <a href="B19453_11.xhtml#_idTextAnchor402"><em class="italic">Chapter 11</em></a>).</p>
<p>Depending on how big our script is and which environment it belongs to, it is a good practice to set it only to show <code>ERROR</code> messages. We can change the log level with the following code:</p>
<pre class="source-code">
spark.sparkContext.setLogLevel("ERROR")</pre>
<p>As you can observe, we used <code>sparkContext</code> to set the log level. <code>sparkContext</code> is a crucial <a id="_idIndexMarker472"/>component in Spark applications. It manages the cluster resources, coordinates the execution of tasks, and provides an interface for interacting with distributed datasets and performing computations in a distributed and parallel manner. Defining the log level of our code will prevent levels below <code>ERROR</code> from appearing on the console, making it cleaner:</p>
<pre class="source-code">
Logger= spark._jvm.org.apache.log4j.Logger
syslogger = Logger.getLogger(__name__)</pre>
<p>Next, we retrieved our <code>log4j</code> module and its <code>Logger</code> class from the instantiated session. This class has a method to show logs already formatted as Spark does. The <code>__name__</code> parameter will retrieve the name of the current module from the Python internals; in our case, it is <code>main</code>.</p>
<p>With this, we can create customized logs as follows:</p>
<pre class="source-code">
syslogger.error("Error message sample")
syslogger.info("Info message sample")</pre>
<p>And, of course, you can ally the PySpark logs with your Python output for a complete solution <a id="_idIndexMarker473"/>using a <code>try…except</code> exception-handling closure. Let’s simulate an error by passing the wrong filename to our reading function as follows:</p>
<pre class="source-code">
try:
    df = spark.read.options(header=True, sep=',',
                              multiLine=True, escape='"',
                             inferSchema=True) \
                    .csv('listings.cs') # Error here
except Exception as e:
    syslogger.error(f"Error message: {e}")</pre>
<p>You should see the following output message:</p>
<div><div><img alt="Figure 6.18 – Error message formatted by log4j" height="29" src="img/Figure_6.18_B19453.jpg" width="882"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.18 – Error message formatted by log4j</p>
<h2 id="_idParaDest-224"><a id="_idTextAnchor224"/>There’s more…</h2>
<p>Many more customizations are available in <code>log4j</code>, but this might require a little more work. For example, you can change some <code>WARN</code> messages to appear as <code>ERROR</code> messages and prevent the script from continuing to be processed. A practical example in this chapter would be the <em class="italic">Importing structured data using a well-deﬁned schema</em> recipe, when the number of columns in the schema does not match the file.</p>
<p>You can read more about it on <em class="italic">Ivan Trusov</em>’s blog page here: <a href="https://polarpersonal.medium.com/writing-pyspark-logs-in-apache-spark-and-databricks-8590c28d1d51">https://polarpersonal.medium.com/writing-pyspark-logs-in-apache-spark-and-databricks-8590c28d1d51</a>.</p>
<h2 id="_idParaDest-225"><a id="_idTextAnchor225"/>See also</h2>
<ul>
<li>You can find <a id="_idIndexMarker474"/>more about the PySpark best practices here: <a href="https://climbtheladder.com/10-pyspark-logging-best-practices/">https://climbtheladder.com/10-pyspark-logging-best-practices/</a>.</li>
<li>Read more about <code>sparkContext</code> and how it works in the Spark official documentation here: <a href="https://spark.apache.org/docs/3.2.0/api/java/org/apache/spark/SparkContext.xhtml">https://spark.apache.org/docs/3.2.0/api/java/org/apache/spark/SparkContext.xhtml</a>.</li>
</ul>
<h1 id="_idParaDest-226"><a id="_idTextAnchor226"/>Further reading</h1>
<ul>
<li><a href="https://www.tibco.com/reference-center/what-is-structured-data">https://www.tibco.com/reference-center/what-is-structured-data</a></li>
<li><a href="https://spark.apache.org/docs/latest/sql-programming-guide.xhtml">https://spark.apache.org/docs/latest/sql-programming-guide.xhtml</a></li>
<li><a href="https://mungingdata.com/pyspark/schema-structtype-structfield/">https://mungingdata.com/pyspark/schema-structtype-structfield/</a></li>
<li><a href="https://sparkbyexamples.com/pyspark/pyspark-select-nested-struct-columns/">https://sparkbyexamples.com/pyspark/pyspark-select-nested-struct-columns/</a></li>
<li><a href="https://benalexkeen.com/using-pyspark-to-read-and-flatten-json-data-using-an-enforced-schema/">https://benalexkeen.com/using-pyspark-to-read-and-flatten-json-data-using-an-enforced-schema/</a></li>
<li><a href="https://towardsdatascience.com/json-in-databricks-and-pyspark-26437352f0e9">https://towardsdatascience.com/json-in-databricks-and-pyspark-26437352f0e9</a></li>
</ul>
</div>
</div></body></html>